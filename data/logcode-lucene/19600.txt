GitDiffStart: 367b35f0cb0dbea897a2c0083508a287200efd57 | Mon Aug 24 12:44:13 2009 +0000
diff --git a/contrib/analyzers/common/src/java/org/apache/lucene/analysis/miscellaneous/SingleTokenTokenStream.java b/contrib/analyzers/common/src/java/org/apache/lucene/analysis/miscellaneous/SingleTokenTokenStream.java
index 7205f46..8b9aebe 100644
--- a/contrib/analyzers/common/src/java/org/apache/lucene/analysis/miscellaneous/SingleTokenTokenStream.java
+++ b/contrib/analyzers/common/src/java/org/apache/lucene/analysis/miscellaneous/SingleTokenTokenStream.java
@@ -19,14 +19,10 @@ package org.apache.lucene.analysis.miscellaneous;
 
 import java.io.IOException;
 
+import org.apache.lucene.util.AttributeImpl;
 import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.FlagsAttribute;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
 
 /**
  * A {@link TokenStream} containing a single token.
@@ -34,45 +30,37 @@ import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
 public class SingleTokenTokenStream extends TokenStream {
 
   private boolean exhausted = false;
+  
   // The token needs to be immutable, so work with clones!
   private Token singleToken;
+  private final AttributeImpl tokenAtt;
 
-  private TermAttribute termAtt;
-  private OffsetAttribute offsetAtt;
-  private FlagsAttribute flagsAtt;
-  private PositionIncrementAttribute posIncAtt;
-  private TypeAttribute typeAtt;
-  private PayloadAttribute payloadAtt;
+  private static final AttributeFactory TOKEN_ATTRIBUTE_FACTORY = new AttributeFactory() {
+    public AttributeImpl createAttributeInstance(Class attClass) {
+      return attClass.isAssignableFrom(Token.class)
+        ? new Token() : DEFAULT_ATTRIBUTE_FACTORY.createAttributeInstance(attClass);
+    }
+  };
 
   public SingleTokenTokenStream(Token token) {
+    super(TOKEN_ATTRIBUTE_FACTORY);
+    
     assert token != null;
     this.singleToken = (Token) token.clone();
-     
-    termAtt = (TermAttribute) addAttribute(TermAttribute.class);
-    offsetAtt = (OffsetAttribute) addAttribute(OffsetAttribute.class);
-    flagsAtt = (FlagsAttribute) addAttribute(FlagsAttribute.class);
-    posIncAtt = (PositionIncrementAttribute) addAttribute(PositionIncrementAttribute.class);
-    typeAtt = (TypeAttribute) addAttribute(TypeAttribute.class);
-    payloadAtt = (PayloadAttribute) addAttribute(PayloadAttribute.class);
+    
+    tokenAtt = (AttributeImpl) addAttribute(TermAttribute.class);
+    assert (tokenAtt instanceof Token || tokenAtt.getClass().getName().equals("org.apache.lucene.analysis.TokenWrapper"));
   }
 
-
   public final boolean incrementToken() throws IOException {
     if (exhausted) {
       return false;
+    } else {
+      clearAttributes();
+      singleToken.copyTo(tokenAtt);
+      exhausted = true;
+      return true;
     }
-    
-    Token clone = (Token) singleToken.clone();
-    
-    clearAttributes();
-    termAtt.setTermBuffer(clone.termBuffer(), 0, clone.termLength());
-    offsetAtt.setOffset(clone.startOffset(), clone.endOffset());
-    flagsAtt.setFlags(clone.getFlags());
-    typeAtt.setType(clone.type());
-    posIncAtt.setPositionIncrement(clone.getPositionIncrement());
-    payloadAtt.setPayload(clone.getPayload());
-    exhausted = true;
-    return true;
   }
   
   /** @deprecated Will be removed in Lucene 3.0. This method is final, as it should
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ar/TestArabicAnalyzer.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ar/TestArabicAnalyzer.java
index a1a546a..e26d302 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ar/TestArabicAnalyzer.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ar/TestArabicAnalyzer.java
@@ -21,15 +21,13 @@ import java.io.StringReader;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-
-import junit.framework.TestCase;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 
 /**
  * Test the Arabic Analyzer
  *
  */
-public class TestArabicAnalyzer extends TestCase {
+public class TestArabicAnalyzer extends BaseTokenStreamTestCase {
   
   /** This test fails with NPE when the 
    * stopwords file is missing in classpath */
@@ -74,33 +72,4 @@ public class TestArabicAnalyzer extends TestCase {
     assertAnalyzesTo(new ArabicAnalyzer(), "English text.", new String[] {
         "english", "text" });
   }
-  
-  private void assertAnalyzesTo(Analyzer a, String input, String[] output)
-      throws Exception {
-    TokenStream ts = a.tokenStream("dummy", new StringReader(input));
-    TermAttribute termAtt = (TermAttribute) ts
-        .getAttribute(TermAttribute.class);
-
-    for (int i = 0; i < output.length; i++) {
-      assertTrue(ts.incrementToken());
-      assertEquals(output[i], termAtt.term());
-    }
-
-    assertFalse(ts.incrementToken());
-    ts.close();
-  }
-  
-  private void assertAnalyzesToReuse(Analyzer a, String input, String[] output)
-      throws Exception {
-    TokenStream ts = a.reusableTokenStream("dummy", new StringReader(input));
-    TermAttribute termAtt = (TermAttribute) ts
-        .getAttribute(TermAttribute.class);
-
-    for (int i = 0; i < output.length; i++) {
-      assertTrue(ts.incrementToken());
-      assertEquals(output[i], termAtt.term());
-    }
-
-    assertFalse(ts.incrementToken());
-  }
 }
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ar/TestArabicNormalizationFilter.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ar/TestArabicNormalizationFilter.java
index 99d170e..9663cba 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ar/TestArabicNormalizationFilter.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ar/TestArabicNormalizationFilter.java
@@ -20,15 +20,14 @@ package org.apache.lucene.analysis.ar;
 import java.io.IOException;
 import java.io.StringReader;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
 /**
  * Test the Arabic Normalization Filter
  *
  */
-public class TestArabicNormalizationFilter extends TestCase {
+public class TestArabicNormalizationFilter extends BaseTokenStreamTestCase {
 
   public void testAlifMadda() throws IOException {
     check("آج?", "اج?");
@@ -89,11 +88,7 @@ public class TestArabicNormalizationFilter extends TestCase {
   private void check(final String input, final String expected) throws IOException {
     ArabicLetterTokenizer tokenStream = new ArabicLetterTokenizer(new StringReader(input));
     ArabicNormalizationFilter filter = new ArabicNormalizationFilter(tokenStream);
-    TermAttribute termAtt = (TermAttribute) filter.getAttribute(TermAttribute.class);
-    
-    assertTrue(filter.incrementToken());
-    assertEquals(expected, termAtt.term());
-    filter.close();
+    assertTokenStreamContents(filter, new String[]{expected});
   }
 
 }
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ar/TestArabicStemFilter.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ar/TestArabicStemFilter.java
index 6c0685f..47f7444 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ar/TestArabicStemFilter.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ar/TestArabicStemFilter.java
@@ -20,15 +20,14 @@ package org.apache.lucene.analysis.ar;
 import java.io.IOException;
 import java.io.StringReader;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
 /**
  * Test the Arabic Normalization Filter
  *
  */
-public class TestArabicStemFilter extends TestCase {
+public class TestArabicStemFilter extends BaseTokenStreamTestCase {
   
   public void testAlPrefix() throws IOException {
     check("ا?حس?", "حس?");
@@ -117,11 +116,7 @@ public class TestArabicStemFilter extends TestCase {
   private void check(final String input, final String expected) throws IOException {
     ArabicLetterTokenizer tokenStream  = new ArabicLetterTokenizer(new StringReader(input));
     ArabicStemFilter filter = new ArabicStemFilter(tokenStream);
-    TermAttribute termAtt = (TermAttribute) filter.getAttribute(TermAttribute.class);
-    
-    assertTrue(filter.incrementToken());
-    assertEquals(expected, termAtt.term());
-    filter.close();
+    assertTokenStreamContents(filter, new String[]{expected});
   }
 
 }
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/br/TestBrazilianStemmer.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/br/TestBrazilianStemmer.java
index cb659cd..a20c9e0 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/br/TestBrazilianStemmer.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/br/TestBrazilianStemmer.java
@@ -17,11 +17,7 @@ package org.apache.lucene.analysis.br;
  * limitations under the License.
  */
 
-import java.io.IOException;
-import java.io.StringReader;
-
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
@@ -32,9 +28,9 @@ import org.apache.lucene.analysis.tokenattributes.TermAttribute;
  * It is very similar to the snowball portuguese algorithm but not exactly the same.
  *
  */
-public class TestBrazilianStemmer extends TestCase {
+public class TestBrazilianStemmer extends BaseTokenStreamTestCase {
   
-  public void testWithSnowballExamples() throws IOException {
+  public void testWithSnowballExamples() throws Exception {
 	 check("boa", "boa");
 	 check("boainain", "boainain");
 	 check("boas", "boas");
@@ -150,23 +146,13 @@ public class TestBrazilianStemmer extends TestCase {
     a.setStemExclusionTable(new String[] { "quintessência" });
     checkReuse(a, "quintessência", "quintessência");
   }
-
-  private void check(final String input, final String expected) throws IOException {
-    Analyzer analyzer = new BrazilianAnalyzer(); 
-    TokenStream stream = analyzer.tokenStream("dummy", new StringReader(input));
-    TermAttribute text = (TermAttribute) stream.getAttribute(TermAttribute.class);
-    assertTrue(stream.incrementToken());
-    assertEquals(expected, text.term());
-    assertFalse(stream.incrementToken());
-    stream.close();
+  
+  private void check(final String input, final String expected) throws Exception {
+    checkOneTerm(new BrazilianAnalyzer(), input, expected);
   }
   
-  private void checkReuse(Analyzer analyzer, final String input, final String expected) throws IOException {
-    TokenStream stream = analyzer.reusableTokenStream("dummy", new StringReader(input));
-    TermAttribute text = (TermAttribute) stream.getAttribute(TermAttribute.class);
-    assertTrue(stream.incrementToken());
-    assertEquals(expected, text.term());
-    assertFalse(stream.incrementToken());
+  private void checkReuse(Analyzer a, String input, String expected) throws Exception {
+    checkOneTermReuse(a, input, expected);
   }
 
 }
\ No newline at end of file
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer.java
index 0addd87..92f187f 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer.java
@@ -20,8 +20,7 @@ package org.apache.lucene.analysis.cjk;
 import java.io.IOException;
 import java.io.StringReader;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
@@ -29,7 +28,7 @@ import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
 
 
-public class TestCJKTokenizer extends TestCase{
+public class TestCJKTokenizer extends BaseTokenStreamTestCase {
   
   class TestToken {
     String termText;
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/cn/TestChineseTokenizer.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/cn/TestChineseTokenizer.java
index 2477b90..d19d2ca 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/cn/TestChineseTokenizer.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/cn/TestChineseTokenizer.java
@@ -21,17 +21,15 @@ import java.io.IOException;
 import java.io.Reader;
 import java.io.StringReader;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
 
-public class TestChineseTokenizer extends TestCase
+public class TestChineseTokenizer extends BaseTokenStreamTestCase
 {
     public void testOtherLetterOffset() throws IOException
     {
@@ -116,34 +114,5 @@ public class TestChineseTokenizer extends TestCase
       assertAnalyzesTo(justFilter, "This is a Test. b c d", 
           new String[] { "This", "Test." });
     }
-    
-    private void assertAnalyzesTo(Analyzer a, String input, String[] output)
-      throws Exception {
-      TokenStream ts = a.tokenStream("dummy", new StringReader(input));
-      TermAttribute termAtt = (TermAttribute) ts
-      .getAttribute(TermAttribute.class);
-
-     for (int i = 0; i < output.length; i++) {
-       assertTrue(ts.incrementToken());
-       assertEquals(output[i], termAtt.term());
-     }
 
-     assertFalse(ts.incrementToken());
-     ts.close();
-    }
-    
-    private void assertAnalyzesToReuse(Analyzer a, String input, String[] output,
-      int startOffsets[], int endOffsets[])
-      throws Exception {
-      TokenStream ts = a.reusableTokenStream("dummy", new StringReader(input));
-      TermAttribute termAtt = (TermAttribute) ts
-        .getAttribute(TermAttribute.class);
-
-      for (int i = 0; i < output.length; i++) {
-        assertTrue(ts.incrementToken());
-        assertEquals(output[i], termAtt.term());
-      }
-
-      assertFalse(ts.incrementToken());
-    }
 }
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java
index a865634..2a1150b 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java
@@ -31,8 +31,7 @@ import java.util.List;
 import java.util.zip.ZipEntry;
 import java.util.zip.ZipInputStream;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
@@ -41,7 +40,7 @@ import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
-public class TestCompoundWordTokenFilter extends TestCase {
+public class TestCompoundWordTokenFilter extends BaseTokenStreamTestCase {
   private static String[] locations = {
       "http://dfn.dl.sourceforge.net/sourceforge/offo/offo-hyphenation.zip",
       "http://surfnet.dl.sourceforge.net/sourceforge/offo/offo-hyphenation.zip",
@@ -76,7 +75,7 @@ public class TestCompoundWordTokenFilter extends TestCase {
         dict, CompoundWordTokenFilterBase.DEFAULT_MIN_WORD_SIZE,
         CompoundWordTokenFilterBase.DEFAULT_MIN_SUBWORD_SIZE,
         CompoundWordTokenFilterBase.DEFAULT_MAX_SUBWORD_SIZE, false);
-    assertFiltersTo(tf, new String[] { "Rindfleischüberwachungsgesetz", "Rind",
+    assertTokenStreamContents(tf, new String[] { "Rindfleischüberwachungsgesetz", "Rind",
         "fleisch", "überwachung", "gesetz", "Drahtschere", "Draht", "schere",
         "abba" }, new int[] { 0, 0, 4, 11, 23, 30, 30, 35, 42 }, new int[] {
         29, 4, 11, 22, 29, 41, 35, 41, 46 }, new int[] { 1, 0, 0, 0, 0, 1, 0,
@@ -101,7 +100,7 @@ public class TestCompoundWordTokenFilter extends TestCase {
             "Rindfleischüberwachungsgesetz")), hyphenator, dict,
         CompoundWordTokenFilterBase.DEFAULT_MIN_WORD_SIZE,
         CompoundWordTokenFilterBase.DEFAULT_MIN_SUBWORD_SIZE, 40, true);
-    assertFiltersTo(tf, new String[] { "Rindfleischüberwachungsgesetz",
+    assertTokenStreamContents(tf, new String[] { "Rindfleischüberwachungsgesetz",
         "Rindfleisch", "fleisch", "überwachungsgesetz", "gesetz" }, new int[] {
         0, 0, 4, 11, 23 }, new int[] { 29, 11, 11, 29, 29 }, new int[] { 1, 0,
         0, 0, 0 });
@@ -118,7 +117,7 @@ public class TestCompoundWordTokenFilter extends TestCase {
                 "Bildörr Bilmotor Biltak Slagborr Hammarborr Pelarborr Glasögonfodral Basfiolsfodral Basfiolsfodralmakaregesäll Skomakare Vindrutetorkare Vindrutetorkarblad abba")),
         dict);
 
-    assertFiltersTo(tf, new String[] { "Bildörr", "Bil", "dörr", "Bilmotor",
+    assertTokenStreamContents(tf, new String[] { "Bildörr", "Bil", "dörr", "Bilmotor",
         "Bil", "motor", "Biltak", "Bil", "tak", "Slagborr", "Slag", "borr",
         "Hammarborr", "Hammar", "borr", "Pelarborr", "Pelar", "borr",
         "Glasögonfodral", "Glas", "ögon", "fodral", "Basfiolsfodral", "Bas",
@@ -147,7 +146,7 @@ public class TestCompoundWordTokenFilter extends TestCase {
         CompoundWordTokenFilterBase.DEFAULT_MIN_SUBWORD_SIZE,
         CompoundWordTokenFilterBase.DEFAULT_MAX_SUBWORD_SIZE, true);
 
-    assertFiltersTo(tf, new String[] { "Basfiolsfodralmakaregesäll", "Bas",
+    assertTokenStreamContents(tf, new String[] { "Basfiolsfodralmakaregesäll", "Bas",
         "fiolsfodral", "fodral", "makare", "gesäll" }, new int[] { 0, 0, 3, 8,
         14, 20 }, new int[] { 26, 3, 14, 14, 20, 26 }, new int[] { 1, 0, 0, 0,
         0, 0 });
@@ -185,22 +184,6 @@ public class TestCompoundWordTokenFilter extends TestCase {
     assertEquals("Rindfleischüberwachungsgesetz", termAtt.term());
   }
 
-  private void assertFiltersTo(TokenFilter tf, String[] s, int[] startOffset,
-      int[] endOffset, int[] posIncr) throws Exception {
-    TermAttribute termAtt = (TermAttribute) tf.getAttribute(TermAttribute.class);
-    OffsetAttribute offsetAtt = (OffsetAttribute) tf.getAttribute(OffsetAttribute.class);
-    PositionIncrementAttribute posIncAtt = (PositionIncrementAttribute) tf.getAttribute(PositionIncrementAttribute.class);
-    
-    for (int i = 0; i < s.length; ++i) {
-      assertTrue(tf.incrementToken());
-      assertEquals(s[i], termAtt.term());
-      assertEquals(startOffset[i], offsetAtt.startOffset());
-      assertEquals(endOffset[i], offsetAtt.endOffset());
-      assertEquals(posIncr[i], posIncAtt.getPositionIncrement());
-    }
-    assertFalse(tf.incrementToken());
-  }
-
   private void getHyphenationPatternFileContents() {
     if (patternsFileContent == null) {
       try {
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/cz/TestCzechAnalyzer.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/cz/TestCzechAnalyzer.java
index 1516e6d..9d7c2ef 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/cz/TestCzechAnalyzer.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/cz/TestCzechAnalyzer.java
@@ -21,13 +21,10 @@ import java.io.File;
 import java.io.FileInputStream;
 import java.io.IOException;
 import java.io.InputStream;
-import java.io.StringReader;
-
-import junit.framework.TestCase;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
 /**
  * Test the CzechAnalyzer
@@ -35,7 +32,7 @@ import org.apache.lucene.analysis.tokenattributes.TermAttribute;
  * CzechAnalyzer is like a StandardAnalyzer with a custom stopword list.
  *
  */
-public class TestCzechAnalyzer extends TestCase {
+public class TestCzechAnalyzer extends BaseTokenStreamTestCase {
   File dataDir = new File(System.getProperty("dataDir", "./bin"));
   File customStopFile = new File(dataDir, "org/apache/lucene/analysis/cz/customStopWordFile.txt");
   
@@ -85,24 +82,4 @@ public class TestCzechAnalyzer extends TestCase {
     assertAnalyzesToReuse(cz, "?eská Republika", new String[] { "?eská" });
   }
 
-  private void assertAnalyzesTo(Analyzer a, String input, String[] output) throws Exception {
-    TokenStream ts = a.tokenStream("dummy", new StringReader(input));
-    TermAttribute text = (TermAttribute) ts.getAttribute(TermAttribute.class);
-    for (int i=0; i<output.length; i++) {
-      assertTrue(ts.incrementToken());
-      assertEquals(text.term(), output[i]);
-    }
-    assertFalse(ts.incrementToken());
-    ts.close();
-  }
-  
-  private void assertAnalyzesToReuse(Analyzer a, String input, String[] output) throws Exception {
-    TokenStream ts = a.reusableTokenStream("dummy", new StringReader(input));
-    TermAttribute text = (TermAttribute) ts.getAttribute(TermAttribute.class);
-    for (int i=0; i<output.length; i++) {
-      assertTrue(ts.incrementToken());
-      assertEquals(text.term(), output[i]);
-    }
-    assertFalse(ts.incrementToken());
-  }
 }
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter.java
index a2cc203..c64c5bc 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter.java
@@ -20,18 +20,14 @@ package org.apache.lucene.analysis.de;
 import java.io.BufferedReader;
 import java.io.File;
 import java.io.FileInputStream;
-import java.io.IOException;
 import java.io.InputStreamReader;
 import java.io.Reader;
-import java.io.StringReader;
-
-import junit.framework.TestCase;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
 /**
  * Test the German stemmer. The stemming algorithm is known to work less 
@@ -39,34 +35,29 @@ import org.apache.lucene.analysis.tokenattributes.TermAttribute;
  * also check some of the cases where the algorithm is wrong.
  *
  */
-public class TestGermanStemFilter extends TestCase {
+public class TestGermanStemFilter extends BaseTokenStreamTestCase {
 
-  public void testStemming() {
-    try {
-      // read test cases from external file:
-      File dataDir = new File(System.getProperty("dataDir", "./bin"));
-      File testFile = new File(dataDir, "org/apache/lucene/analysis/de/data.txt");
-      FileInputStream fis = new FileInputStream(testFile);
-      InputStreamReader isr = new InputStreamReader(fis, "iso-8859-1");
-      BufferedReader breader = new BufferedReader(isr);
-      while(true) {
-        String line = breader.readLine();
-        if (line == null)
-          break;
-        line = line.trim();
-        if (line.startsWith("#") || line.equals(""))
-          continue;    // ignore comments and empty lines
-        String[] parts = line.split(";");
-        //System.out.println(parts[0] + " -- " + parts[1]);
-        check(parts[0], parts[1]);
-      }
-      breader.close();
-      isr.close();
-      fis.close();
-    } catch (IOException e) {
-       e.printStackTrace();
-       fail();
+  public void testStemming() throws Exception {
+    // read test cases from external file:
+    File dataDir = new File(System.getProperty("dataDir", "./bin"));
+    File testFile = new File(dataDir, "org/apache/lucene/analysis/de/data.txt");
+    FileInputStream fis = new FileInputStream(testFile);
+    InputStreamReader isr = new InputStreamReader(fis, "iso-8859-1");
+    BufferedReader breader = new BufferedReader(isr);
+    while(true) {
+      String line = breader.readLine();
+      if (line == null)
+        break;
+      line = line.trim();
+      if (line.startsWith("#") || line.equals(""))
+        continue;    // ignore comments and empty lines
+      String[] parts = line.split(";");
+      //System.out.println(parts[0] + " -- " + parts[1]);
+      check(parts[0], parts[1]);
     }
+    breader.close();
+    isr.close();
+    fis.close();
   }
   
   public void testReusableTokenStream() throws Exception {
@@ -100,20 +91,11 @@ public class TestGermanStemFilter extends TestCase {
     checkReuse(a, "tischen", "tischen");
   }
   
-  private void check(final String input, final String expected) throws IOException {
-    Analyzer a = new GermanAnalyzer();
-    TokenStream tokenStream = a.tokenStream("dummy", new StringReader(input));
-    TermAttribute termAtt = (TermAttribute) tokenStream.getAttribute(TermAttribute.class);
-    assertTrue(tokenStream.incrementToken());
-    assertEquals(expected, termAtt.term());
-    tokenStream.close();
+  private void check(final String input, final String expected) throws Exception {
+    checkOneTerm(new GermanAnalyzer(), input, expected);
   }
   
-  private void checkReuse(Analyzer a, String input, String expected) throws IOException {
-    TokenStream stream = a.reusableTokenStream("dummy", new StringReader(input));
-    TermAttribute text = (TermAttribute) stream.getAttribute(TermAttribute.class);
-    assertTrue(stream.incrementToken());
-    assertEquals(expected, text.term());
-    assertFalse(stream.incrementToken());
+  private void checkReuse(Analyzer a, String input, String expected) throws Exception {
+    checkOneTermReuse(a, input, expected);
   }
 }
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java
index 92c1193..49f8cbf 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java
@@ -16,49 +16,16 @@ package org.apache.lucene.analysis.el;
  * limitations under the License.
  */
 
-import java.io.StringReader;
-
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
 
 /**
  * A unit test class for verifying the correct operation of the GreekAnalyzer.
  *
  */
-public class GreekAnalyzerTest extends TestCase {
-
-	/**
-	 * A helper method copied from org.apache.lucene.analysis.TestAnalyzers.
-	 *
-	 * @param a			the Analyzer to test
-	 * @param input		an input String to analyze
-	 * @param output	a String[] with the results of the analysis
-	 * @throws Exception in case an error occurs
-	 */
-	private void assertAnalyzesTo(Analyzer a, String input, String[] output) throws Exception {
-		TokenStream ts = a.tokenStream("dummy", new StringReader(input));
-		TermAttribute termAtt = (TermAttribute) ts.getAttribute(TermAttribute.class);
-		for (int i=0; i<output.length; i++) {
-			assertTrue(ts.incrementToken());
-			assertEquals(termAtt.term(), output[i]);
-		}
-		assertFalse(ts.incrementToken());
-		ts.close();
-	}
-	
-	private void assertAnalyzesToReuse(Analyzer a, String input, String[] output) throws Exception {
-	    TokenStream ts = a.reusableTokenStream("dummy", new StringReader(input));
-	    TermAttribute termAtt = (TermAttribute) ts.getAttribute(TermAttribute.class);
-	    for (int i=0; i<output.length; i++) {
-	        assertTrue(ts.incrementToken());
-	        assertEquals(termAtt.term(), output[i]);
-	    }
-	    assertFalse(ts.incrementToken());
-	}
+public class GreekAnalyzerTest extends BaseTokenStreamTestCase {
 
 	/**
 	 * Test the analysis of various greek strings.
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/fa/TestPersianAnalyzer.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/fa/TestPersianAnalyzer.java
index e067254..a6629e2 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/fa/TestPersianAnalyzer.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/fa/TestPersianAnalyzer.java
@@ -19,17 +19,15 @@ package org.apache.lucene.analysis.fa;
 
 import java.io.StringReader;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
 /**
  * Test the Persian Analyzer
  * 
  */
-public class TestPersianAnalyzer extends TestCase {
+public class TestPersianAnalyzer extends BaseTokenStreamTestCase {
 
   /**
    * This test fails with NPE when the stopwords file is missing in classpath
@@ -216,33 +214,4 @@ public class TestPersianAnalyzer extends TestCase {
     assertAnalyzesToReuse(a, "برگ????", new String[] { "برگ" });
   }
 
-  private void assertAnalyzesTo(Analyzer a, String input, String[] output)
-      throws Exception {
-	TokenStream ts = a.tokenStream("dummy", new StringReader(input));
-	TermAttribute termAtt = (TermAttribute) ts.getAttribute(TermAttribute.class);
-
-	for (int i = 0; i < output.length; i++) {
-		assertTrue(ts.incrementToken());
-		assertEquals(output[i], termAtt.term());
-	}
-	
-	assertFalse(ts.incrementToken());
-    ts.close();
-  }
-  
-  private void assertAnalyzesToReuse(Analyzer a, String input, String[] output)
-      throws Exception {
-    TokenStream ts = a.reusableTokenStream("dummy", new StringReader(input));
-    TermAttribute termAtt = (TermAttribute) ts
-        .getAttribute(TermAttribute.class);
-
-    for (int i = 0; i < output.length; i++) {
-      assertTrue(ts.incrementToken());
-      assertEquals(output[i], termAtt.term());
-    }
-
-    assertFalse(ts.incrementToken());
-    ts.close();
-  }
-
 }
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/fa/TestPersianNormalizationFilter.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/fa/TestPersianNormalizationFilter.java
index 195cd82..1b4814f 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/fa/TestPersianNormalizationFilter.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/fa/TestPersianNormalizationFilter.java
@@ -20,16 +20,14 @@ package org.apache.lucene.analysis.fa;
 import java.io.IOException;
 import java.io.StringReader;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.ar.ArabicLetterTokenizer;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
 /**
  * Test the Arabic Normalization Filter
  * 
  */
-public class TestPersianNormalizationFilter extends TestCase {
+public class TestPersianNormalizationFilter extends BaseTokenStreamTestCase {
 
   public void testFarsiYeh() throws IOException {
     check("?ا?", "?ا?");
@@ -55,17 +53,12 @@ public class TestPersianNormalizationFilter extends TestCase {
     check("زاد?", "زاد?");
   }
 
-  private void check(final String input, final String expected)
-      throws IOException {
+  private void check(final String input, final String expected) throws IOException {
     ArabicLetterTokenizer tokenStream = new ArabicLetterTokenizer(
         new StringReader(input));
     PersianNormalizationFilter filter = new PersianNormalizationFilter(
         tokenStream);
-    TermAttribute termAtt = (TermAttribute) filter.getAttribute(TermAttribute.class);
-    assertTrue(filter.incrementToken());
-    assertEquals(expected, termAtt.term());
-    assertFalse(filter.incrementToken());
-    filter.close();
+    assertTokenStreamContents(filter, new String[]{expected});
   }
 
 }
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/fr/TestElision.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/fr/TestElision.java
index e123dd1..4b988b3 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/fr/TestElision.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/fr/TestElision.java
@@ -24,8 +24,7 @@ import java.util.HashSet;
 import java.util.List;
 import java.util.Set;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
@@ -34,9 +33,9 @@ import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 /**
  * 
  */
-public class TestElision extends TestCase {
+public class TestElision extends BaseTokenStreamTestCase {
 
-  public void testElision() {
+  public void testElision() throws Exception {
     String test = "Plop, juste pour voir l'embrouille avec O'brian. M'enfin.";
     Tokenizer tokenizer = new StandardTokenizer(new StringReader(test));
     Set articles = new HashSet();
@@ -49,15 +48,11 @@ public class TestElision extends TestCase {
     assertEquals("enfin", tas.get(7));
   }
 
-  private List filtre(TokenFilter filter) {
+  private List filtre(TokenFilter filter) throws IOException {
     List tas = new ArrayList();
-    try {
-      TermAttribute termAtt = (TermAttribute) filter.getAttribute(TermAttribute.class);
-      while (filter.incrementToken()) {
-        tas.add(termAtt.term());
-      }
-    } catch (IOException e) {
-      e.printStackTrace();
+    TermAttribute termAtt = (TermAttribute) filter.getAttribute(TermAttribute.class);
+    while (filter.incrementToken()) {
+      tas.add(termAtt.term());
     }
     return tas;
   }
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/fr/TestFrenchAnalyzer.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/fr/TestFrenchAnalyzer.java
index b3c2b1c..0b5354e 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/fr/TestFrenchAnalyzer.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/fr/TestFrenchAnalyzer.java
@@ -56,11 +56,9 @@ package org.apache.lucene.analysis.fr;
 
 import java.io.StringReader;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
 /**
  * Test case for FrenchAnalyzer.
@@ -68,35 +66,7 @@ import org.apache.lucene.analysis.tokenattributes.TermAttribute;
  * @version   $version$
  */
 
-public class TestFrenchAnalyzer extends TestCase {
-
-	// Method copied from TestAnalyzers, maybe should be refactored
-	public void assertAnalyzesTo(Analyzer a, String input, String[] output)
-		throws Exception {
-
-		TokenStream ts = a.tokenStream("dummy", new StringReader(input));
-
-		TermAttribute termAtt = (TermAttribute) ts.getAttribute(TermAttribute.class);
-		for (int i = 0; i < output.length; i++) {
-			assertTrue(ts.incrementToken());
-			assertEquals(termAtt.term(), output[i]);
-		}
-		assertFalse(ts.incrementToken());
-		ts.close();
-	}
-	
-   public void assertAnalyzesToReuse(Analyzer a, String input, String[] output)
-       throws Exception {
-
-       TokenStream ts = a.reusableTokenStream("dummy", new StringReader(input));
-
-       TermAttribute termAtt = (TermAttribute) ts.getAttribute(TermAttribute.class);
-       for (int i = 0; i < output.length; i++) {
-           assertTrue(ts.incrementToken());
-           assertEquals(termAtt.term(), output[i]);
-       }
-       assertFalse(ts.incrementToken());
-   }
+public class TestFrenchAnalyzer extends BaseTokenStreamTestCase {
 
 	public void testAnalyzer() throws Exception {
 		FrenchAnalyzer fa = new FrenchAnalyzer();
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestEmptyTokenStream.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestEmptyTokenStream.java
index bfb9f0e..b8b1270 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestEmptyTokenStream.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestEmptyTokenStream.java
@@ -19,11 +19,10 @@ package org.apache.lucene.analysis.miscellaneous;
 
 import java.io.IOException;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.analysis.TokenStream;
 
-public class TestEmptyTokenStream extends TestCase {
+public class TestEmptyTokenStream extends LuceneTestCase {
 
   public void test() throws IOException {
     TokenStream ts = new EmptyTokenStream();
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAndSuffixAwareTokenFilter.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAndSuffixAwareTokenFilter.java
index 7447db7..0f0197e 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAndSuffixAwareTokenFilter.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAndSuffixAwareTokenFilter.java
@@ -17,7 +17,7 @@ package org.apache.lucene.analysis.miscellaneous;
  * limitations under the License.
  */
 
-import junit.framework.TestCase;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
@@ -27,7 +27,7 @@ import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import java.io.IOException;
 import java.io.StringReader;
 
-public class TestPrefixAndSuffixAwareTokenFilter extends TestCase {
+public class TestPrefixAndSuffixAwareTokenFilter extends BaseTokenStreamTestCase {
 
   public void test() throws IOException {
 
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAwareTokenFilter.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAwareTokenFilter.java
index c15b76b..bf27102 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAwareTokenFilter.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAwareTokenFilter.java
@@ -17,7 +17,7 @@ package org.apache.lucene.analysis.miscellaneous;
  * limitations under the License.
  */
 
-import junit.framework.TestCase;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
@@ -27,7 +27,7 @@ import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import java.io.IOException;
 import java.io.StringReader;
 
-public class TestPrefixAwareTokenFilter extends TestCase {
+public class TestPrefixAwareTokenFilter extends BaseTokenStreamTestCase {
 
   public void test() throws IOException {
 
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestSingleTokenTokenFilter.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestSingleTokenTokenFilter.java
index 90df886..71a0b2b 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestSingleTokenTokenFilter.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestSingleTokenTokenFilter.java
@@ -17,20 +17,27 @@ package org.apache.lucene.analysis.miscellaneous;
  * limitations under the License.
  */
 
-import junit.framework.TestCase;
-
 import java.io.IOException;
 
+import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.analysis.Token;
 
-public class TestSingleTokenTokenFilter extends TestCase {
+public class TestSingleTokenTokenFilter extends LuceneTestCase {
 
   public void test() throws IOException {
+    final Token reusableToken = new Token();
+    
     Token token = new Token();
-
     SingleTokenTokenStream ts = new SingleTokenTokenStream(token);
+    ts.reset();
+
+    assertEquals(token, ts.next(reusableToken));
+    assertNull(ts.next(reusableToken));
+    
+    token = new Token("hallo", 10, 20, "someType");
+    ts.setToken(token);
+    ts.reset();
 
-    final Token reusableToken = new Token();
     assertEquals(token, ts.next(reusableToken));
     assertNull(ts.next(reusableToken));
   }
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java
index 80af6fc..f3f970d 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java
@@ -19,19 +19,18 @@ package org.apache.lucene.analysis.ngram;
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 
 import java.io.StringReader;
 
-import junit.framework.TestCase;
-
 /**
  * Tests {@link EdgeNGramTokenFilter} for correctness.
  */
-public class EdgeNGramTokenFilterTest extends TestCase {
+public class EdgeNGramTokenFilterTest extends BaseTokenStreamTestCase {
   private TokenStream input;
 
-  public void setUp() {
+  public void setUp() throws Exception {
+    super.setUp();
     input = new WhitespaceTokenizer(new StringReader("abcde"));
   }
 
@@ -67,71 +66,40 @@ public class EdgeNGramTokenFilterTest extends TestCase {
 
   public void testFrontUnigram() throws Exception {
     EdgeNGramTokenFilter tokenizer = new EdgeNGramTokenFilter(input, EdgeNGramTokenFilter.Side.FRONT, 1, 1);
-    TermAttribute termAtt = (TermAttribute) tokenizer.addAttribute(TermAttribute.class);
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(a,0,1)", termAtt.toString());
-    assertFalse(tokenizer.incrementToken());
+    assertTokenStreamContents(tokenizer, new String[]{"a"}, new int[]{0}, new int[]{1});
   }
 
   public void testBackUnigram() throws Exception {
     EdgeNGramTokenFilter tokenizer = new EdgeNGramTokenFilter(input, EdgeNGramTokenFilter.Side.BACK, 1, 1);
-    TermAttribute termAtt = (TermAttribute) tokenizer.addAttribute(TermAttribute.class);
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(e,4,5)", termAtt.toString());
-    assertFalse(tokenizer.incrementToken());
+    assertTokenStreamContents(tokenizer, new String[]{"e"}, new int[]{4}, new int[]{5});
   }
 
   public void testOversizedNgrams() throws Exception {
     EdgeNGramTokenFilter tokenizer = new EdgeNGramTokenFilter(input, EdgeNGramTokenFilter.Side.FRONT, 6, 6);
-    assertFalse(tokenizer.incrementToken());
+    assertTokenStreamContents(tokenizer, new String[0], new int[0], new int[0]);
   }
 
   public void testFrontRangeOfNgrams() throws Exception {
     EdgeNGramTokenFilter tokenizer = new EdgeNGramTokenFilter(input, EdgeNGramTokenFilter.Side.FRONT, 1, 3);
-    TermAttribute termAtt = (TermAttribute) tokenizer.addAttribute(TermAttribute.class);
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(a,0,1)", termAtt.toString());
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(ab,0,2)", termAtt.toString());
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(abc,0,3)", termAtt.toString());
-    assertFalse(tokenizer.incrementToken());
+    assertTokenStreamContents(tokenizer, new String[]{"a","ab","abc"}, new int[]{0,0,0}, new int[]{1,2,3});
   }
 
   public void testBackRangeOfNgrams() throws Exception {
     EdgeNGramTokenFilter tokenizer = new EdgeNGramTokenFilter(input, EdgeNGramTokenFilter.Side.BACK, 1, 3);
-    TermAttribute termAtt = (TermAttribute) tokenizer.addAttribute(TermAttribute.class);
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(e,4,5)", termAtt.toString());
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(de,3,5)", termAtt.toString());
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(cde,2,5)", termAtt.toString());
-    assertFalse(tokenizer.incrementToken());
+    assertTokenStreamContents(tokenizer, new String[]{"e","de","cde"}, new int[]{4,3,2}, new int[]{5,5,5});
   }
   
   public void testSmallTokenInStream() throws Exception {
     input = new WhitespaceTokenizer(new StringReader("abc de fgh"));
     EdgeNGramTokenFilter tokenizer = new EdgeNGramTokenFilter(input, EdgeNGramTokenFilter.Side.FRONT, 3, 3);
-    TermAttribute termAtt = (TermAttribute) tokenizer.addAttribute(TermAttribute.class);
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(abc,0,3)", termAtt.toString());
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(fgh,0,3)", termAtt.toString());
-    assertFalse(tokenizer.incrementToken());
+    assertTokenStreamContents(tokenizer, new String[]{"abc","fgh"}, new int[]{0,0}, new int[]{3,3});
   }
   
   public void testReset() throws Exception {
     WhitespaceTokenizer tokenizer = new WhitespaceTokenizer(new StringReader("abcde"));
     EdgeNGramTokenFilter filter = new EdgeNGramTokenFilter(tokenizer, EdgeNGramTokenFilter.Side.FRONT, 1, 3);
-    TermAttribute termAtt = (TermAttribute) filter.getAttribute(TermAttribute.class);
-    assertTrue(filter.incrementToken());
-    assertEquals("(a,0,1)", termAtt.toString());
-    assertTrue(filter.incrementToken());
-    assertEquals("(ab,0,2)", termAtt.toString());
+    assertTokenStreamContents(filter, new String[]{"a","ab","abc"}, new int[]{0,0,0}, new int[]{1,2,3});
     tokenizer.reset(new StringReader("abcde"));
-    filter.reset();
-    assertTrue(filter.incrementToken());
-    assertEquals("(a,0,1)", termAtt.toString());
+    assertTokenStreamContents(filter, new String[]{"a","ab","abc"}, new int[]{0,0,0}, new int[]{1,2,3});
   }
 }
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenizerTest.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenizerTest.java
index 3048e73..c2d267e 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenizerTest.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenizerTest.java
@@ -20,17 +20,16 @@ package org.apache.lucene.analysis.ngram;
 
 import java.io.StringReader;
 
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-
-import junit.framework.TestCase;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 
 /**
  * Tests {@link EdgeNGramTokenizer} for correctness.
  */
-public class EdgeNGramTokenizerTest extends TestCase {
+public class EdgeNGramTokenizerTest extends BaseTokenStreamTestCase {
   private StringReader input;
 
-  public void setUp() {
+  public void setUp() throws Exception {
+    super.setUp();
     input = new StringReader("abcde");
   }
 
@@ -66,58 +65,33 @@ public class EdgeNGramTokenizerTest extends TestCase {
 
   public void testFrontUnigram() throws Exception {
     EdgeNGramTokenizer tokenizer = new EdgeNGramTokenizer(input, EdgeNGramTokenizer.Side.FRONT, 1, 1);
-    TermAttribute termAtt = (TermAttribute) tokenizer.addAttribute(TermAttribute.class);
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(a,0,1)", termAtt.toString());
-    assertFalse(tokenizer.incrementToken());
+    assertTokenStreamContents(tokenizer, new String[]{"a"}, new int[]{0}, new int[]{1});
   }
 
   public void testBackUnigram() throws Exception {
     EdgeNGramTokenizer tokenizer = new EdgeNGramTokenizer(input, EdgeNGramTokenizer.Side.BACK, 1, 1);
-    TermAttribute termAtt = (TermAttribute) tokenizer.addAttribute(TermAttribute.class);
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(e,4,5)", termAtt.toString());
-    assertFalse(tokenizer.incrementToken());
+    assertTokenStreamContents(tokenizer, new String[]{"e"}, new int[]{4}, new int[]{5});
   }
 
   public void testOversizedNgrams() throws Exception {
     EdgeNGramTokenizer tokenizer = new EdgeNGramTokenizer(input, EdgeNGramTokenizer.Side.FRONT, 6, 6);
-    assertFalse(tokenizer.incrementToken());
+    assertTokenStreamContents(tokenizer, new String[0], new int[0], new int[0]);
   }
 
   public void testFrontRangeOfNgrams() throws Exception {
     EdgeNGramTokenizer tokenizer = new EdgeNGramTokenizer(input, EdgeNGramTokenizer.Side.FRONT, 1, 3);
-    TermAttribute termAtt = (TermAttribute) tokenizer.addAttribute(TermAttribute.class);
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(a,0,1)", termAtt.toString());
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(ab,0,2)", termAtt.toString());
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(abc,0,3)", termAtt.toString());
-    assertFalse(tokenizer.incrementToken());
+    assertTokenStreamContents(tokenizer, new String[]{"a","ab","abc"}, new int[]{0,0,0}, new int[]{1,2,3});
   }
 
   public void testBackRangeOfNgrams() throws Exception {
     EdgeNGramTokenizer tokenizer = new EdgeNGramTokenizer(input, EdgeNGramTokenizer.Side.BACK, 1, 3);
-    TermAttribute termAtt = (TermAttribute) tokenizer.addAttribute(TermAttribute.class);
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(e,4,5)", termAtt.toString());
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(de,3,5)", termAtt.toString());
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(cde,2,5)", termAtt.toString());
-    assertFalse(tokenizer.incrementToken());
+    assertTokenStreamContents(tokenizer, new String[]{"e","de","cde"}, new int[]{4,3,2}, new int[]{5,5,5});
   }
   
   public void testReset() throws Exception {
     EdgeNGramTokenizer tokenizer = new EdgeNGramTokenizer(input, EdgeNGramTokenizer.Side.FRONT, 1, 3);
-    TermAttribute termAtt = (TermAttribute) tokenizer.addAttribute(TermAttribute.class);
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(a,0,1)", termAtt.toString());
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(ab,0,2)", termAtt.toString());
+    assertTokenStreamContents(tokenizer, new String[]{"a","ab","abc"}, new int[]{0,0,0}, new int[]{1,2,3});
     tokenizer.reset(new StringReader("abcde"));
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(a,0,1)", termAtt.toString());
+    assertTokenStreamContents(tokenizer, new String[]{"a","ab","abc"}, new int[]{0,0,0}, new int[]{1,2,3});
   }
 }
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java
index 15ef287..d6d4b93 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java
@@ -19,20 +19,19 @@ package org.apache.lucene.analysis.ngram;
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 
 import java.io.IOException;
 import java.io.StringReader;
 
-import junit.framework.TestCase;
-
 /**
  * Tests {@link NGramTokenFilter} for correctness.
  */
-public class NGramTokenFilterTest extends TestCase {
+public class NGramTokenFilterTest extends BaseTokenStreamTestCase {
     private TokenStream input;
     
-    public void setUp() {
+    public void setUp() throws Exception {
+        super.setUp();
         input = new WhitespaceTokenizer(new StringReader("abcde"));
     }
 
@@ -56,70 +55,41 @@ public class NGramTokenFilterTest extends TestCase {
         assertTrue(gotException);
     }
 
-    private void checkStream(TokenStream stream, String[] exp) throws IOException {
-      TermAttribute termAtt = (TermAttribute) stream.addAttribute(TermAttribute.class);
-      for (int i = 0; i < exp.length; i++) {
-        assertTrue(stream.incrementToken());
-        assertEquals(exp[i], termAtt.toString());
-      }
-      assertFalse(stream.incrementToken());
-    }
-    
     public void testUnigrams() throws Exception {
       NGramTokenFilter filter = new NGramTokenFilter(input, 1, 1);
-      String[] exp = new String[] {
-        "(a,0,1)", "(b,1,2)", "(c,2,3)", "(d,3,4)", "(e,4,5)"
-      };
-      
-      checkStream(filter, exp);
+      assertTokenStreamContents(filter, new String[]{"a","b","c","d","e"}, new int[]{0,1,2,3,4}, new int[]{1,2,3,4,5});
     }
 
     public void testBigrams() throws Exception {
       NGramTokenFilter filter = new NGramTokenFilter(input, 2, 2);
-      String[] exp = new String[] {
-          "(ab,0,2)", "(bc,1,3)", "(cd,2,4)", "(de,3,5)"
-        };
-        
-      checkStream(filter, exp);
+      assertTokenStreamContents(filter, new String[]{"ab","bc","cd","de"}, new int[]{0,1,2,3}, new int[]{2,3,4,5});
     }
 
     public void testNgrams() throws Exception {
       NGramTokenFilter filter = new NGramTokenFilter(input, 1, 3);
-      String[] exp = new String[] {
-          "(a,0,1)", "(b,1,2)", "(c,2,3)", "(d,3,4)", "(e,4,5)",
-          "(ab,0,2)", "(bc,1,3)", "(cd,2,4)", "(de,3,5)",
-          "(abc,0,3)", "(bcd,1,4)", "(cde,2,5)"
-      };
-        
-      checkStream(filter, exp);
+      assertTokenStreamContents(filter,
+        new String[]{"a","b","c","d","e", "ab","bc","cd","de", "abc","bcd","cde"}, 
+        new int[]{0,1,2,3,4, 0,1,2,3, 0,1,2},
+        new int[]{1,2,3,4,5, 2,3,4,5, 3,4,5}
+      );
     }
 
     public void testOversizedNgrams() throws Exception {
       NGramTokenFilter filter = new NGramTokenFilter(input, 6, 7);
-      assertFalse(filter.incrementToken());
+      assertTokenStreamContents(filter, new String[0], new int[0], new int[0]);
     }
     
     public void testSmallTokenInStream() throws Exception {
       input = new WhitespaceTokenizer(new StringReader("abc de fgh"));
       NGramTokenFilter filter = new NGramTokenFilter(input, 3, 3);
-      String[] exp = new String[] {
-          "(abc,0,3)", "(fgh,0,3)"
-        };
-        
-      checkStream(filter, exp);
+      assertTokenStreamContents(filter, new String[]{"abc","fgh"}, new int[]{0,0}, new int[]{3,3});
     }
     
     public void testReset() throws Exception {
       WhitespaceTokenizer tokenizer = new WhitespaceTokenizer(new StringReader("abcde"));
-      NGramTokenFilter filter = new NGramTokenFilter(tokenizer, 1, 3);
-      TermAttribute termAtt = (TermAttribute) filter.addAttribute(TermAttribute.class);
-      assertTrue(filter.incrementToken());
-      assertEquals("(a,0,1)", termAtt.toString());
-      assertTrue(filter.incrementToken());
-      assertEquals("(b,1,2)", termAtt.toString());
+      NGramTokenFilter filter = new NGramTokenFilter(tokenizer, 1, 1);
+      assertTokenStreamContents(filter, new String[]{"a","b","c","d","e"}, new int[]{0,1,2,3,4}, new int[]{1,2,3,4,5});
       tokenizer.reset(new StringReader("abcde"));
-      filter.reset();
-      assertTrue(filter.incrementToken());
-      assertEquals("(a,0,1)", termAtt.toString());
+      assertTokenStreamContents(filter, new String[]{"a","b","c","d","e"}, new int[]{0,1,2,3,4}, new int[]{1,2,3,4,5});
     }
 }
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest.java
index 5e08dac..ab7d92f 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest.java
@@ -22,17 +22,16 @@ import java.io.IOException;
 import java.io.StringReader;
 
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-
-import junit.framework.TestCase;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 
 /**
  * Tests {@link NGramTokenizer} for correctness.
  */
-public class NGramTokenizerTest extends TestCase {
+public class NGramTokenizerTest extends BaseTokenStreamTestCase {
     private StringReader input;
     
-    public void setUp() {
+    public void setUp() throws Exception {
+        super.setUp();
         input = new StringReader("abcde");
     }
 
@@ -55,60 +54,35 @@ public class NGramTokenizerTest extends TestCase {
         }
         assertTrue(gotException);
     }
-    
-    private void checkStream(TokenStream stream, String[] exp) throws IOException {
-      TermAttribute termAtt = (TermAttribute) stream.addAttribute(TermAttribute.class);
-      for (int i = 0; i < exp.length; i++) {
-        assertTrue(stream.incrementToken());
-        assertEquals(exp[i], termAtt.toString());
-      }
-      assertFalse(stream.incrementToken());
-    }
 
     public void testUnigrams() throws Exception {
         NGramTokenizer tokenizer = new NGramTokenizer(input, 1, 1);
-        
-        String[] exp = new String[] {
-            "(a,0,1)", "(b,1,2)", "(c,2,3)", "(d,3,4)", "(e,4,5)"
-          };
-          
-        checkStream(tokenizer, exp);
+        assertTokenStreamContents(tokenizer, new String[]{"a","b","c","d","e"}, new int[]{0,1,2,3,4}, new int[]{1,2,3,4,5});
     }
 
     public void testBigrams() throws Exception {
         NGramTokenizer tokenizer = new NGramTokenizer(input, 2, 2);
-        String[] exp = new String[] {
-            "(ab,0,2)", "(bc,1,3)", "(cd,2,4)", "(de,3,5)"
-          };
-          
-        checkStream(tokenizer, exp);
+        assertTokenStreamContents(tokenizer, new String[]{"ab","bc","cd","de"}, new int[]{0,1,2,3}, new int[]{2,3,4,5});
     }
 
     public void testNgrams() throws Exception {
         NGramTokenizer tokenizer = new NGramTokenizer(input, 1, 3);
-        String[] exp = new String[] {
-            "(a,0,1)", "(b,1,2)", "(c,2,3)", "(d,3,4)", "(e,4,5)",
-            "(ab,0,2)", "(bc,1,3)", "(cd,2,4)", "(de,3,5)",
-            "(abc,0,3)", "(bcd,1,4)", "(cde,2,5)"
-        };
-          
-        checkStream(tokenizer, exp);
+        assertTokenStreamContents(tokenizer,
+          new String[]{"a","b","c","d","e", "ab","bc","cd","de", "abc","bcd","cde"}, 
+          new int[]{0,1,2,3,4, 0,1,2,3, 0,1,2},
+          new int[]{1,2,3,4,5, 2,3,4,5, 3,4,5}
+        );
     }
 
     public void testOversizedNgrams() throws Exception {
         NGramTokenizer tokenizer = new NGramTokenizer(input, 6, 7);
-        assertFalse(tokenizer.incrementToken());
+        assertTokenStreamContents(tokenizer, new String[0], new int[0], new int[0]);
     }
     
     public void testReset() throws Exception {
-      NGramTokenizer tokenizer = new NGramTokenizer(input, 1, 3);
-      TermAttribute termAtt = (TermAttribute) tokenizer.getAttribute(TermAttribute.class);
-      assertTrue(tokenizer.incrementToken());
-      assertEquals("(a,0,1)", termAtt.toString());
-      assertTrue(tokenizer.incrementToken());
-      assertEquals("(b,1,2)", termAtt.toString());
+      NGramTokenizer tokenizer = new NGramTokenizer(input, 1, 1);
+      assertTokenStreamContents(tokenizer, new String[]{"a","b","c","d","e"}, new int[]{0,1,2,3,4}, new int[]{1,2,3,4,5});
       tokenizer.reset(new StringReader("abcde"));
-      assertTrue(tokenizer.incrementToken());
-      assertEquals("(a,0,1)", termAtt.toString());
+      assertTokenStreamContents(tokenizer, new String[]{"a","b","c","d","e"}, new int[]{0,1,2,3,4}, new int[]{1,2,3,4,5});
     }
 }
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/nl/TestDutchStemmer.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/nl/TestDutchStemmer.java
index fde17ae..3919d8a 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/nl/TestDutchStemmer.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/nl/TestDutchStemmer.java
@@ -18,16 +18,12 @@ package org.apache.lucene.analysis.nl;
  */
 
 import java.io.File;
-import java.io.IOException;
 import java.io.Reader;
-import java.io.StringReader;
-
-import junit.framework.TestCase;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
 /**
  * Test the Dutch Stem Filter, which only modifies the term text.
@@ -35,11 +31,11 @@ import org.apache.lucene.analysis.tokenattributes.TermAttribute;
  * The code states that it uses the snowball algorithm, but tests reveal some differences.
  * 
  */
-public class TestDutchStemmer extends TestCase {
+public class TestDutchStemmer extends BaseTokenStreamTestCase {
   File dataDir = new File(System.getProperty("dataDir", "./bin"));
   File customDictFile = new File(dataDir, "org/apache/lucene/analysis/nl/customStemDict.txt");
   
-  public void testWithSnowballExamples() throws IOException {
+  public void testWithSnowballExamples() throws Exception {
 	 check("lichaamsziek", "lichaamsziek");
 	 check("lichamelijk", "licham");
 	 check("lichamelijke", "licham");
@@ -124,10 +120,10 @@ public class TestDutchStemmer extends TestCase {
   
   public void testReusableTokenStream() throws Exception {
     Analyzer a = new DutchAnalyzer(); 
-    checkReuse(a, "lichaamsziek", "lichaamsziek");
-    checkReuse(a, "lichamelijk", "licham");
-    checkReuse(a, "lichamelijke", "licham");
-    checkReuse(a, "lichamelijkheden", "licham");
+    checkOneTermReuse(a, "lichaamsziek", "lichaamsziek");
+    checkOneTermReuse(a, "lichamelijk", "licham");
+    checkOneTermReuse(a, "lichamelijke", "licham");
+    checkOneTermReuse(a, "lichamelijkheden", "licham");
   }
   
   /**
@@ -141,10 +137,10 @@ public class TestDutchStemmer extends TestCase {
   
   public void testLUCENE1678BWComp() throws Exception {
     Analyzer a = new DutchSubclassAnalyzer();
-    checkReuse(a, "lichaamsziek", "lichaamsziek");
-    checkReuse(a, "lichamelijk", "lichamelijk");
-    checkReuse(a, "lichamelijke", "lichamelijke");
-    checkReuse(a, "lichamelijkheden", "lichamelijkheden");
+    checkOneTermReuse(a, "lichaamsziek", "lichaamsziek");
+    checkOneTermReuse(a, "lichamelijk", "lichamelijk");
+    checkOneTermReuse(a, "lichamelijke", "lichamelijke");
+    checkOneTermReuse(a, "lichamelijkheden", "lichamelijkheden");
   }
  
   /* 
@@ -153,9 +149,9 @@ public class TestDutchStemmer extends TestCase {
    */
   public void testExclusionTableReuse() throws Exception {
     DutchAnalyzer a = new DutchAnalyzer();
-    checkReuse(a, "lichamelijk", "licham");
+    checkOneTermReuse(a, "lichamelijk", "licham");
     a.setStemExclusionTable(new String[] { "lichamelijk" });
-    checkReuse(a, "lichamelijk", "lichamelijk");
+    checkOneTermReuse(a, "lichamelijk", "lichamelijk");
   }
   
   /* 
@@ -164,30 +160,13 @@ public class TestDutchStemmer extends TestCase {
    */
   public void testStemDictionaryReuse() throws Exception {
     DutchAnalyzer a = new DutchAnalyzer();
-    checkReuse(a, "lichamelijk", "licham");
+    checkOneTermReuse(a, "lichamelijk", "licham");
     a.setStemDictionary(customDictFile);
-    checkReuse(a, "lichamelijk", "somethingentirelydifferent");
+    checkOneTermReuse(a, "lichamelijk", "somethingentirelydifferent");
   }
   
-  private void check(final String input, final String expected) throws IOException {
-    Analyzer analyzer = new DutchAnalyzer(); 
-    TokenStream stream = analyzer.tokenStream("dummy", new StringReader(input));
-    TermAttribute text = (TermAttribute) stream.getAttribute(TermAttribute.class);
-    assertTrue(stream.incrementToken());
-    assertEquals(expected, text.term());
-    assertFalse(stream.incrementToken());
-    stream.close();
+  private void check(final String input, final String expected) throws Exception {
+    checkOneTerm(new DutchAnalyzer(), input, expected); 
   }
   
-  private void checkReuse(Analyzer a, final String input, final String expected)
-      throws IOException {
-    TokenStream stream = a
-        .reusableTokenStream("dummy", new StringReader(input));
-    TermAttribute text = (TermAttribute) stream
-        .getAttribute(TermAttribute.class);
-    assertTrue(stream.incrementToken());
-    assertEquals(expected, text.term());
-    assertFalse(stream.incrementToken());
-  }
-
 }
\ No newline at end of file
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/payloads/NumericPayloadTokenFilterTest.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/payloads/NumericPayloadTokenFilterTest.java
index 1d4c53c..c24e768 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/payloads/NumericPayloadTokenFilterTest.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/payloads/NumericPayloadTokenFilterTest.java
@@ -16,7 +16,7 @@ package org.apache.lucene.analysis.payloads;
  * limitations under the License.
  */
 
-import junit.framework.TestCase;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
@@ -27,20 +27,13 @@ import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
 import java.io.IOException;
 import java.io.StringReader;
 
-public class NumericPayloadTokenFilterTest extends TestCase {
+public class NumericPayloadTokenFilterTest extends BaseTokenStreamTestCase {
 
 
   public NumericPayloadTokenFilterTest(String s) {
     super(s);
   }
 
-  protected void setUp() {
-  }
-
-  protected void tearDown() {
-
-  }
-
   public void test() throws IOException {
     String test = "The quick red fox jumped over the lazy brown dogs";
 
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/payloads/TokenOffsetPayloadTokenFilterTest.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/payloads/TokenOffsetPayloadTokenFilterTest.java
index 51dd143..2ee83b1 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/payloads/TokenOffsetPayloadTokenFilterTest.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/payloads/TokenOffsetPayloadTokenFilterTest.java
@@ -16,7 +16,7 @@ package org.apache.lucene.analysis.payloads;
  * limitations under the License.
  */
 
-import junit.framework.TestCase;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
@@ -25,20 +25,13 @@ import org.apache.lucene.index.Payload;
 import java.io.IOException;
 import java.io.StringReader;
 
-public class TokenOffsetPayloadTokenFilterTest extends TestCase {
+public class TokenOffsetPayloadTokenFilterTest extends BaseTokenStreamTestCase {
 
 
   public TokenOffsetPayloadTokenFilterTest(String s) {
     super(s);
   }
 
-  protected void setUp() {
-  }
-
-  protected void tearDown() {
-
-  }
-
   public void test() throws IOException {
     String test = "The quick red fox jumped over the lazy brown dogs";
 
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/payloads/TypeAsPayloadTokenFilterTest.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/payloads/TypeAsPayloadTokenFilterTest.java
index c7d5f60..ad397de 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/payloads/TypeAsPayloadTokenFilterTest.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/payloads/TypeAsPayloadTokenFilterTest.java
@@ -16,7 +16,7 @@ package org.apache.lucene.analysis.payloads;
  * limitations under the License.
  */
 
-import junit.framework.TestCase;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
@@ -27,21 +27,13 @@ import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
 import java.io.IOException;
 import java.io.StringReader;
 
-public class TypeAsPayloadTokenFilterTest extends TestCase {
+public class TypeAsPayloadTokenFilterTest extends BaseTokenStreamTestCase {
 
 
   public TypeAsPayloadTokenFilterTest(String s) {
     super(s);
   }
 
-  protected void setUp() {
-  }
-
-  protected void tearDown() {
-
-  }
-
-
   public void test() throws IOException {
     String test = "The quick red fox jumped over the lazy brown dogs";
 
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/position/PositionFilterTest.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/position/PositionFilterTest.java
index 0f7c96c..49baa3f 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/position/PositionFilterTest.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/position/PositionFilterTest.java
@@ -19,13 +19,12 @@ package org.apache.lucene.analysis.position;
 
 import java.io.IOException;
 
-import junit.framework.TestCase;
-import org.apache.lucene.analysis.Token;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.shingle.ShingleFilter;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
-public class PositionFilterTest extends TestCase {
+public class PositionFilterTest extends BaseTokenStreamTestCase {
 
   public class TestTokenStream extends TokenStream {
 
@@ -40,6 +39,7 @@ public class PositionFilterTest extends TestCase {
     }
 
     public final boolean incrementToken() throws IOException {
+      clearAttributes();
       if (index < testToken.length) {
         termAtt.setTermBuffer(testToken[index++]);
         return true;
@@ -52,9 +52,6 @@ public class PositionFilterTest extends TestCase {
     }
   }
 
-  public static void main(String[] args) {
-    junit.textui.TestRunner.run(PositionFilterTest.class);
-  }
   public static final String[] TEST_TOKEN = new String[]{
     "please",
     "divide",
@@ -105,65 +102,39 @@ public class PositionFilterTest extends TestCase {
     "word"
   };
 
-  public void testFilter() throws IOException {
+  public void testFilter() throws Exception {
 
-    filterTest(new PositionFilter(new TestTokenStream(TEST_TOKEN)),
+    assertTokenStreamContents(new PositionFilter(new TestTokenStream(TEST_TOKEN)),
                TEST_TOKEN,
                TEST_TOKEN_POSITION_INCREMENTS);
   }
 
-  public void testNonZeroPositionIncrement() throws IOException {
+  public void testNonZeroPositionIncrement() throws Exception {
     
-    filterTest(new PositionFilter(new TestTokenStream(TEST_TOKEN), 5),
+    assertTokenStreamContents(new PositionFilter(new TestTokenStream(TEST_TOKEN), 5),
                TEST_TOKEN,
                TEST_TOKEN_NON_ZERO_POSITION_INCREMENTS);
   }
   
-  public void testReset() throws IOException {
+  public void testReset() throws Exception {
 
     PositionFilter filter = new PositionFilter(new TestTokenStream(TEST_TOKEN));
-    filterTest(filter, TEST_TOKEN, TEST_TOKEN_POSITION_INCREMENTS);
+    assertTokenStreamContents(filter, TEST_TOKEN, TEST_TOKEN_POSITION_INCREMENTS);
     filter.reset();
     // Make sure that the reset filter provides correct position increments
-    filterTest(filter, TEST_TOKEN, TEST_TOKEN_POSITION_INCREMENTS);
+    assertTokenStreamContents(filter, TEST_TOKEN, TEST_TOKEN_POSITION_INCREMENTS);
   }
   
   /** Tests ShingleFilter up to six shingles against six terms.
    *  Tests PositionFilter setting all but the first positionIncrement to zero.
    * @throws java.io.IOException @see Token#next(Token)
    */
-  public void test6GramFilterNoPositions() throws IOException {
+  public void test6GramFilterNoPositions() throws Exception {
 
     ShingleFilter filter = new ShingleFilter(new TestTokenStream(TEST_TOKEN), 6);
-    filterTest(new PositionFilter(filter),
+    assertTokenStreamContents(new PositionFilter(filter),
                SIX_GRAM_NO_POSITIONS_TOKENS,
                SIX_GRAM_NO_POSITIONS_INCREMENTS);
   }
 
-  protected TokenStream filterTest(final TokenStream filter,
-                                   final String[] tokensToCompare,
-                                   final int[] positionIncrements)
-      throws IOException {
-
-    int i = 0;
-    final Token reusableToken = new Token();
-
-    for (Token nextToken = filter.next(reusableToken)
-        ; i < tokensToCompare.length
-        ; nextToken = filter.next(reusableToken)) {
-
-      if (null != nextToken) {
-        final String termText = nextToken.term();
-        final String goldText = tokensToCompare[i];
-
-        assertEquals("Wrong termText", goldText, termText);
-        assertEquals("Wrong positionIncrement for token \"" + termText + "\"",
-                     positionIncrements[i], nextToken.getPositionIncrement());
-      }else{
-        assertNull(tokensToCompare[i]);
-      }
-      i++;
-    }
-    return filter;
-  }
 }
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzerTest.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzerTest.java
index ec154b3..37a265b 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzerTest.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzerTest.java
@@ -16,7 +16,7 @@ package org.apache.lucene.analysis.query;
  * limitations under the License.
  */
 
-import junit.framework.TestCase;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.LetterTokenizer;
 import org.apache.lucene.analysis.TokenStream;
@@ -39,7 +39,7 @@ import java.io.IOException;
 import java.io.Reader;
 import java.io.StringReader;
 
-public class QueryAutoStopWordAnalyzerTest extends TestCase {
+public class QueryAutoStopWordAnalyzerTest extends BaseTokenStreamTestCase {
   String variedFieldValues[] = {"the", "quick", "brown", "fox", "jumped", "over", "the", "lazy", "boring", "dog"};
   String repetitiveFieldValues[] = {"boring", "boring", "vaguelyboring"};
   RAMDirectory dir;
@@ -67,8 +67,8 @@ public class QueryAutoStopWordAnalyzerTest extends TestCase {
   }
 
   protected void tearDown() throws Exception {
-    super.tearDown();
     reader.close();
+    super.tearDown();
   }
 
   //Helper method to query
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/reverse/TestReverseStringFilter.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/reverse/TestReverseStringFilter.java
index 4f261c8..f181e55 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/reverse/TestReverseStringFilter.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/reverse/TestReverseStringFilter.java
@@ -22,9 +22,9 @@ import java.io.StringReader;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 
-public class TestReverseStringFilter extends LuceneTestCase {
+public class TestReverseStringFilter extends BaseTokenStreamTestCase {
   public void testFilter() throws Exception {
     TokenStream stream = new WhitespaceTokenizer(
         new StringReader("Do have a nice day"));     // 1-4 length string
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ru/TestRussianAnalyzer.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ru/TestRussianAnalyzer.java
index 0683d0b..77bca28 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ru/TestRussianAnalyzer.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ru/TestRussianAnalyzer.java
@@ -24,8 +24,7 @@ import java.io.InputStreamReader;
 import java.io.Reader;
 import java.io.StringReader;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
@@ -37,7 +36,7 @@ import org.apache.lucene.analysis.tokenattributes.TermAttribute;
  * @version   $Id$
  */
 
-public class TestRussianAnalyzer extends TestCase
+public class TestRussianAnalyzer extends BaseTokenStreamTestCase
 {
     private InputStreamReader inWords;
 
@@ -55,6 +54,7 @@ public class TestRussianAnalyzer extends TestCase
 
     protected void setUp() throws Exception
     {
+      super.setUp();
       dataDir = new File(System.getProperty("dataDir", "./bin"));
     }
 
@@ -195,14 +195,4 @@ public class TestRussianAnalyzer extends TestCase
       assertAnalyzesToReuse(a, "?о знание ??о ??анило?? в ?айне",
           new String[] { "знан", "??ан", "?айн" });
     }
-
-    private void assertAnalyzesToReuse(Analyzer a, String input, String[] output) throws Exception {
-      TokenStream ts = a.reusableTokenStream("dummy", new StringReader(input));
-      TermAttribute termAtt = (TermAttribute) ts.getAttribute(TermAttribute.class);
-      for (int i=0; i<output.length; i++) {
-          assertTrue(ts.incrementToken());
-          assertEquals(termAtt.term(), output[i]);
-      }
-      assertFalse(ts.incrementToken());
-    }
 }
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ru/TestRussianStem.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ru/TestRussianStem.java
index f91fcc6..df3f219 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ru/TestRussianStem.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ru/TestRussianStem.java
@@ -17,15 +17,14 @@ package org.apache.lucene.analysis.ru;
  * limitations under the License.
  */
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.util.LuceneTestCase;
 import java.io.BufferedReader;
 import java.io.File;
 import java.io.InputStreamReader;
 import java.io.FileInputStream;
 import java.util.ArrayList;
 
-public class TestRussianStem extends TestCase
+public class TestRussianStem extends LuceneTestCase
 {
     private ArrayList words = new ArrayList();
     private ArrayList stems = new ArrayList();
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java
index ed28aba..8df472b 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java
@@ -20,15 +20,14 @@ package org.apache.lucene.analysis.shingle;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.LetterTokenizer;
 import org.apache.lucene.analysis.WhitespaceAnalyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
+import org.apache.lucene.analysis.tokenattributes.*;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexWriter;
@@ -44,19 +43,13 @@ import org.apache.lucene.search.BooleanClause;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.RAMDirectory;
 
-import junit.framework.TestCase;
-
 /**
  * A test class for ShingleAnalyzerWrapper as regards queries and scoring.
  */
-public class ShingleAnalyzerWrapperTest extends TestCase {
+public class ShingleAnalyzerWrapperTest extends BaseTokenStreamTestCase {
 
   public IndexSearcher searcher;
 
-  public static void main(String[] args) {
-    junit.textui.TestRunner.run(ShingleAnalyzerWrapperTest.class);
-  }
-
   /**
    * Set up a new index in RAM with three test phrases and the supplied Analyzer.
    *
@@ -233,8 +226,7 @@ public class ShingleAnalyzerWrapperTest extends TestCase {
     assertAnalyzesToReuse(a, "this is a test",
         new String[] { "this", "is", "a", "test" },
         new int[] { 0, 5, 8, 10 },
-        new int[] { 4, 7, 9, 14 },
-        new int[] { 1, 1, 1, 1 });
+        new int[] { 4, 7, 9, 14 });
   }
   
   /*
@@ -269,25 +261,4 @@ public class ShingleAnalyzerWrapperTest extends TestCase {
         new int[] { 6, 13, 13, 18, 18, 27, 27 },
         new int[] { 1, 0, 1, 0, 1, 0, 1 });
   }
-  
-  private void assertAnalyzesToReuse(Analyzer a, String input, String[] output,
-      int[] startOffsets, int[] endOffsets, int[] posIncr) throws Exception {
-    TokenStream ts = a.reusableTokenStream("dummy", new StringReader(input));
-    TermAttribute termAtt = (TermAttribute) ts
-        .getAttribute(TermAttribute.class);
-    OffsetAttribute offsetAtt = (OffsetAttribute) ts
-        .getAttribute(OffsetAttribute.class);
-    PositionIncrementAttribute posIncAtt = (PositionIncrementAttribute) ts
-        .getAttribute(PositionIncrementAttribute.class);
-
-    for (int i = 0; i < output.length; i++) {
-      assertTrue(ts.incrementToken());
-      assertEquals(output[i], termAtt.term());
-      assertEquals(startOffsets[i], offsetAtt.startOffset());
-      assertEquals(endOffsets[i], offsetAtt.endOffset());
-      assertEquals(posIncr[i], posIncAtt.getPositionIncrement());
-    }
-
-    assertFalse(ts.incrementToken());
-  }
 }
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest.java
index a82f9a7..ad62297 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest.java
@@ -20,18 +20,14 @@ package org.apache.lucene.analysis.shingle;
 import java.io.IOException;
 import java.io.StringReader;
 
-import junit.framework.TestCase;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
-import org.apache.lucene.analysis.tokenattributes.TypeAttributeImpl;
+import org.apache.lucene.analysis.tokenattributes.*;
 
-public class ShingleFilterTest extends TestCase {
+public class ShingleFilterTest extends BaseTokenStreamTestCase {
 
   public class TestTokenStream extends TokenStream {
 
@@ -53,6 +49,7 @@ public class ShingleFilterTest extends TestCase {
     }
 
     public final boolean incrementToken() throws IOException {
+      clearAttributes();
       if (index < testToken.length) {
         Token t = testToken[index++];
         termAtt.setTermBuffer(t.termBuffer(), 0, t.termLength());
@@ -66,10 +63,6 @@ public class ShingleFilterTest extends TestCase {
     }
   }
 
-  public static void main(String[] args) {
-    junit.textui.TestRunner.run(ShingleFilterTest.class);
-  }
-
   public static final Token[] TEST_TOKEN = new Token[] {
       createToken("please", 0, 6),
       createToken("divide", 7, 13),
@@ -188,15 +181,19 @@ public class ShingleFilterTest extends TestCase {
   public void testReset() throws Exception {
     Tokenizer wsTokenizer = new WhitespaceTokenizer(new StringReader("please divide this sentence"));
     TokenStream filter = new ShingleFilter(wsTokenizer, 2);
-    TermAttribute termAtt = (TermAttribute) filter.getAttribute(TermAttribute.class);
-    assertTrue(filter.incrementToken());
-    assertEquals("(please,0,6)", termAtt.toString());
-    assertTrue(filter.incrementToken());
-    assertEquals("(please divide,0,13,type=shingle,posIncr=0)", termAtt.toString());
+    assertTokenStreamContents(filter,
+      new String[]{"please","please divide","divide","divide this","this","this sentence","sentence"},
+      new int[]{0,0,7,7,14,14,19}, new int[]{6,13,13,18,18,27,27},
+      new String[]{TypeAttributeImpl.DEFAULT_TYPE,"shingle",TypeAttributeImpl.DEFAULT_TYPE,"shingle",TypeAttributeImpl.DEFAULT_TYPE,"shingle",TypeAttributeImpl.DEFAULT_TYPE},
+      new int[]{1,0,1,0,1,0,1}
+    );
     wsTokenizer.reset(new StringReader("please divide this sentence"));
-    filter.reset();
-    assertTrue(filter.incrementToken());
-    assertEquals("(please,0,6)", termAtt.toString());
+    assertTokenStreamContents(filter,
+      new String[]{"please","please divide","divide","divide this","this","this sentence","sentence"},
+      new int[]{0,0,7,7,14,14,19}, new int[]{6,13,13,18,18,27,27},
+      new String[]{TypeAttributeImpl.DEFAULT_TYPE,"shingle",TypeAttributeImpl.DEFAULT_TYPE,"shingle",TypeAttributeImpl.DEFAULT_TYPE,"shingle",TypeAttributeImpl.DEFAULT_TYPE},
+      new int[]{1,0,1,0,1,0,1}
+    );
   }
   
   protected void shingleFilterTest(int maxSize, Token[] tokensToShingle, Token[] tokensToCompare,
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/TestShingleMatrixFilter.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/TestShingleMatrixFilter.java
index cb80ad9..068c24a 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/TestShingleMatrixFilter.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/TestShingleMatrixFilter.java
@@ -21,9 +21,10 @@ import java.io.IOException;
 import java.util.Collection;
 import java.util.Iterator;
 import java.util.LinkedList;
+import java.util.HashSet;
+import java.util.Arrays;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.CachingTokenFilter;
 import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
@@ -33,15 +34,16 @@ import org.apache.lucene.analysis.miscellaneous.SingleTokenTokenStream;
 import org.apache.lucene.analysis.payloads.PayloadHelper;
 import org.apache.lucene.analysis.shingle.ShingleMatrixFilter.Matrix;
 import org.apache.lucene.analysis.shingle.ShingleMatrixFilter.Matrix.Column;
-import org.apache.lucene.analysis.tokenattributes.FlagsAttribute;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
+import org.apache.lucene.analysis.tokenattributes.*;
 
-public class TestShingleMatrixFilter extends TestCase {
+public class TestShingleMatrixFilter extends BaseTokenStreamTestCase {
 
+  public TestShingleMatrixFilter(String name) {
+    // use this ctor, because SingleTokenTokenStream only uses next(Token), so exclude it
+    super(name, new HashSet(Arrays.asList(new String[]{
+      "testBehavingAsShingleFilter", "testMatrix"
+    })));
+  }
 
   public void testBehavingAsShingleFilter() throws IOException {
 
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/sinks/DateRecognizerSinkTokenizerTest.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/sinks/DateRecognizerSinkTokenizerTest.java
index baad73d..271b733 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/sinks/DateRecognizerSinkTokenizerTest.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/sinks/DateRecognizerSinkTokenizerTest.java
@@ -21,26 +21,18 @@ import java.io.StringReader;
 import java.text.SimpleDateFormat;
 import java.util.Locale;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TeeSinkTokenFilter;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.TeeSinkTokenFilter.SinkTokenStream;
 
-public class DateRecognizerSinkTokenizerTest extends TestCase {
+public class DateRecognizerSinkTokenizerTest extends BaseTokenStreamTestCase {
 
 
   public DateRecognizerSinkTokenizerTest(String s) {
     super(s);
   }
 
-  protected void setUp() {
-  }
-
-  protected void tearDown() {
-
-  }
-
   public void test() throws IOException {
     DateRecognizerSinkFilter sinkFilter = new DateRecognizerSinkFilter(new SimpleDateFormat("MM/dd/yyyy", Locale.US));
     String test = "The quick red fox jumped over the lazy brown dogs on 7/11/2006  The dogs finally reacted on 7/12/2006";
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/sinks/TokenRangeSinkTokenizerTest.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/sinks/TokenRangeSinkTokenizerTest.java
index c2205a1..902bdfc 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/sinks/TokenRangeSinkTokenizerTest.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/sinks/TokenRangeSinkTokenizerTest.java
@@ -19,26 +19,18 @@ package org.apache.lucene.analysis.sinks;
 import java.io.IOException;
 import java.io.StringReader;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TeeSinkTokenFilter;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.TeeSinkTokenFilter.SinkTokenStream;
 
-public class TokenRangeSinkTokenizerTest extends TestCase {
+public class TokenRangeSinkTokenizerTest extends BaseTokenStreamTestCase {
 
 
   public TokenRangeSinkTokenizerTest(String s) {
     super(s);
   }
 
-  protected void setUp() {
-  }
-
-  protected void tearDown() {
-
-  }
-
   public void test() throws IOException {
     TokenRangeSinkFilter sinkFilter = new TokenRangeSinkFilter(2, 4);
     String test = "The quick red fox jumped over the lazy brown dogs";
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/sinks/TokenTypeSinkTokenizerTest.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/sinks/TokenTypeSinkTokenizerTest.java
index fdce59c..eb9d6f6 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/sinks/TokenTypeSinkTokenizerTest.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/sinks/TokenTypeSinkTokenizerTest.java
@@ -19,8 +19,7 @@ package org.apache.lucene.analysis.sinks;
 import java.io.IOException;
 import java.io.StringReader;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TeeSinkTokenFilter;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
@@ -29,20 +28,13 @@ import org.apache.lucene.analysis.TeeSinkTokenFilter.SinkTokenStream;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
 
-public class TokenTypeSinkTokenizerTest extends TestCase {
+public class TokenTypeSinkTokenizerTest extends BaseTokenStreamTestCase {
 
 
   public TokenTypeSinkTokenizerTest(String s) {
     super(s);
   }
 
-  protected void setUp() {
-  }
-
-  protected void tearDown() {
-
-  }
-
   public void test() throws IOException {
     TokenTypeSinkFilter sinkFilter = new TokenTypeSinkFilter("D");
     String test = "The quick red fox jumped over the lazy brown dogs";
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/th/TestThaiAnalyzer.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/th/TestThaiAnalyzer.java
index 2458bd0..dd36fa4 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/th/TestThaiAnalyzer.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/th/TestThaiAnalyzer.java
@@ -18,16 +18,11 @@ package org.apache.lucene.analysis.th;
  */
 
 import java.io.Reader;
-import java.io.StringReader;
-
-import junit.framework.TestCase;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
 
 /**
  * Test case for ThaiAnalyzer, modified from TestFrenchAnalyzer
@@ -35,7 +30,7 @@ import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
  * @version   0.1
  */
 
-public class TestThaiAnalyzer extends TestCase {
+public class TestThaiAnalyzer extends BaseTokenStreamTestCase {
 	
 	/* 
 	 * testcase for offsets
@@ -71,56 +66,6 @@ public class TestThaiAnalyzer extends TestCase {
 				new String[] { "<ALPHANUM>", "<ALPHANUM>", "<ALPHANUM>", "<ALPHANUM>", "<ALPHANUM>", "<NUM>" });
 	}
 	*/
-	
-	public void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[])
-		throws Exception {
-
-		TokenStream ts = a.tokenStream("dummy", new StringReader(input));
-		TermAttribute termAtt = (TermAttribute) ts.addAttribute(TermAttribute.class);
-		OffsetAttribute offsetAtt = (OffsetAttribute) ts.addAttribute(OffsetAttribute.class);
-		TypeAttribute typeAtt = (TypeAttribute) ts.addAttribute(TypeAttribute.class);
-		for (int i = 0; i < output.length; i++) {
-			assertTrue(ts.incrementToken());
-			assertEquals(termAtt.term(), output[i]);
-			if (startOffsets != null)
-				assertEquals(offsetAtt.startOffset(), startOffsets[i]);
-			if (endOffsets != null)
-				assertEquals(offsetAtt.endOffset(), endOffsets[i]);
-			if (types != null)
-				assertEquals(typeAtt.type(), types[i]);
-		}
-		assertFalse(ts.incrementToken());
-		ts.close();
-	}
-	
-	public void assertAnalyzesToReuse(Analyzer a, String input, String[] output)
-      throws Exception {
-
-      TokenStream ts = a.reusableTokenStream("dummy", new StringReader(input));
-      TermAttribute termAtt = (TermAttribute) ts
-        .addAttribute(TermAttribute.class);
-      OffsetAttribute offsetAtt = (OffsetAttribute) ts
-        .addAttribute(OffsetAttribute.class);
-      TypeAttribute typeAtt = (TypeAttribute) ts
-        .addAttribute(TypeAttribute.class);
-      for (int i = 0; i < output.length; i++) {
-        assertTrue(ts.incrementToken());
-        assertEquals(termAtt.term(), output[i]);
-      }
-      assertFalse(ts.incrementToken());
-    }
-	
-	public void assertAnalyzesTo(Analyzer a, String input, String[] output) throws Exception {
-		assertAnalyzesTo(a, input, output, null, null, null);
-	}
-	
-	public void assertAnalyzesTo(Analyzer a, String input, String[] output, String[] types) throws Exception {
-		assertAnalyzesTo(a, input, output, null, null, types);
-	}
-	
-	public void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[]) throws Exception {
-		assertAnalyzesTo(a, input, output, startOffsets, endOffsets, null);
-	}
 
 	public void testAnalyzer() throws Exception {
 		ThaiAnalyzer analyzer = new ThaiAnalyzer();
diff --git a/contrib/analyzers/smartcn/src/test/org/apache/lucene/analysis/cn/TestSmartChineseAnalyzer.java b/contrib/analyzers/smartcn/src/test/org/apache/lucene/analysis/cn/TestSmartChineseAnalyzer.java
index 025596e..8fce34a 100644
--- a/contrib/analyzers/smartcn/src/test/org/apache/lucene/analysis/cn/TestSmartChineseAnalyzer.java
+++ b/contrib/analyzers/smartcn/src/test/org/apache/lucene/analysis/cn/TestSmartChineseAnalyzer.java
@@ -20,20 +20,13 @@ package org.apache.lucene.analysis.cn;
 import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.io.Reader;
-import java.io.StringReader;
 import java.io.UnsupportedEncodingException;
 import java.util.Date;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.Token;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
 
-public class TestSmartChineseAnalyzer extends TestCase {
+public class TestSmartChineseAnalyzer extends BaseTokenStreamTestCase {
   
   public void testChineseStopWordsDefault() throws Exception {
     Analyzer ca = new SmartChineseAnalyzer(); /* will load stopwords */
@@ -77,20 +70,11 @@ public class TestSmartChineseAnalyzer extends TestCase {
     assertAnalyzesTo(ca, sentence, result);
   }
   
-  public void testChineseAnalyzer() throws IOException {
-    Token nt = new Token();
+  public void testChineseAnalyzer() throws Exception {
     Analyzer ca = new SmartChineseAnalyzer(true);
-    Reader sentence = new StringReader("??买??????????");
+    String sentence = "??买??????????";
     String[] result = { "??", "?", "?", "???", "??", "???" };
-    TokenStream ts = ca.tokenStream("sentence", sentence);
-    int i = 0;
-    nt = ts.next(nt);
-    while (nt != null) {
-      assertEquals(result[i], nt.term());
-      i++;
-      nt = ts.next(nt);
-    }
-    ts.close();
+    assertAnalyzesTo(ca, sentence, result);
   }
   
   /*
@@ -165,90 +149,4 @@ public class TestSmartChineseAnalyzer extends TestCase {
         new int[] { 0, 1, 3, 4, 6, 7 },
         new int[] { 1, 3, 4, 6, 7, 9 });
   }
-  
-  public void assertAnalyzesToReuse(Analyzer a, String input, String[] output,
-      int startOffsets[], int endOffsets[]) throws Exception {
-
-    TokenStream ts = a.reusableTokenStream("dummy", new StringReader(input));
-    TermAttribute termAtt = (TermAttribute) ts.getAttribute(TermAttribute.class);
-    OffsetAttribute offsetAtt = (OffsetAttribute) ts.getAttribute(OffsetAttribute.class);
-    for (int i = 0; i < output.length; i++) {
-      assertTrue(ts.incrementToken());
-      assertEquals(termAtt.term(), output[i]);
-      assertEquals(offsetAtt.startOffset(), startOffsets[i]);
-      assertEquals(offsetAtt.endOffset(), endOffsets[i]);
-    }
-    assertFalse(ts.incrementToken());
-  }
-  
-  public void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[])
-  throws Exception {
-
-    TokenStream ts = a.tokenStream("dummy", new StringReader(input));
-    TermAttribute termAtt = (TermAttribute) ts.getAttribute(TermAttribute.class);
-    OffsetAttribute offsetAtt = (OffsetAttribute) ts.getAttribute(OffsetAttribute.class);
-    TypeAttribute typeAtt = (TypeAttribute) ts.getAttribute(TypeAttribute.class);
-    for (int i = 0; i < output.length; i++) {
-      assertTrue(ts.incrementToken());
-      assertEquals(termAtt.term(), output[i]);
-      if (startOffsets != null)
-        assertEquals(offsetAtt.startOffset(), startOffsets[i]);
-      if (endOffsets != null)
-        assertEquals(offsetAtt.endOffset(), endOffsets[i]);
-      if (types != null)
-        assertEquals(typeAtt.type(), types[i]);
-    }
-    assertFalse(ts.incrementToken());
-    ts.close();
-  }
-
-public void assertAnalyzesTo(Analyzer a, String input, String[] output) throws Exception {
-  assertAnalyzesTo(a, input, output, null, null, null);
-}
-
-public void assertAnalyzesTo(Analyzer a, String input, String[] output, String[] types) throws Exception {
-  assertAnalyzesTo(a, input, output, null, null, types);
-}
-
-public void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[]) throws Exception {
-  assertAnalyzesTo(a, input, output, startOffsets, endOffsets, null);
-}
-
-
-  /**
-   * @param args
-   * @throws IOException
-   */
-  public static void main(String[] args) throws IOException {
-    new TestSmartChineseAnalyzer().sampleMethod();
-  }
-
-  /**
-   * @throws UnsupportedEncodingException
-   * @throws FileNotFoundException
-   * @throws IOException
-   */
-  private void sampleMethod() throws UnsupportedEncodingException,
-      FileNotFoundException, IOException {
-    Token nt = new Token();
-    Analyzer ca = new SmartChineseAnalyzer(true);
-    Reader sentence = new StringReader(
-        "??????????为?己?大以????????????亲?????, ????????移????影?????????????为?家?????????????????????????????????????????????中年???????????????????????????????信????????????????????????????????????????家?大?????????己??????????????????好???????常????????"
-            + "幸?????????????????????????????????????????????就?????????????????趣??????????大???????????????????????????????她??????????????常???????"
-            + "??????????????????触???????????????????趣?????????????????????趣??????????????????级???? ??????????????????究????????项???以她???????????????????以????????????????????????家?????????????大?????????她??2???????????????????????"
-            + "??????????????"
-            + "????欢????人?? ?????????????????????????????????????人?????????????????????工??????????????信????????????强??????????????????  ??欢?????工?????????欢??????西?? ??欢帮??人?????????人???????????工?????????????导????????活????????????????");
-    TokenStream ts = ca.tokenStream("sentence", sentence);
-
-    System.out.println("start: " + (new Date()));
-    long before = System.currentTimeMillis();
-    nt = ts.next(nt);
-    while (nt != null) {
-      System.out.println(nt.term());
-      nt = ts.next(nt);
-    }
-    ts.close();
-    long now = System.currentTimeMillis();
-    System.out.println("time: " + (now - before) / 1000.0 + " s");
-  }
 }
diff --git a/contrib/memory/src/test/org/apache/lucene/index/memory/TestSynonymTokenFilter.java b/contrib/memory/src/test/org/apache/lucene/index/memory/TestSynonymTokenFilter.java
index 3005656..bdd9729 100644
--- a/contrib/memory/src/test/org/apache/lucene/index/memory/TestSynonymTokenFilter.java
+++ b/contrib/memory/src/test/org/apache/lucene/index/memory/TestSynonymTokenFilter.java
@@ -28,9 +28,6 @@ import org.apache.lucene.analysis.LowerCaseFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 
 public class TestSynonymTokenFilter extends BaseTokenStreamTestCase {
@@ -117,44 +114,4 @@ public class TestSynonymTokenFilter extends BaseTokenStreamTestCase {
     }
   }
   
-  public void assertAnalyzesTo(Analyzer a, String input, String[] output,
-      int startOffsets[], int endOffsets[], int posIncs[]) throws Exception {
-
-    TokenStream ts = a.tokenStream("dummy", new StringReader(input));
-    TermAttribute termAtt = (TermAttribute) ts
-        .getAttribute(TermAttribute.class);
-    OffsetAttribute offsetAtt = (OffsetAttribute) ts
-        .getAttribute(OffsetAttribute.class);
-    PositionIncrementAttribute posIncAtt = (PositionIncrementAttribute) ts
-        .getAttribute(PositionIncrementAttribute.class);
-    for (int i = 0; i < output.length; i++) {
-      assertTrue(ts.incrementToken());
-      assertEquals(termAtt.term(), output[i]);
-      assertEquals(offsetAtt.startOffset(), startOffsets[i]);
-      assertEquals(offsetAtt.endOffset(), endOffsets[i]);
-      assertEquals(posIncAtt.getPositionIncrement(), posIncs[i]);
-    }
-    assertFalse(ts.incrementToken());
-    ts.close();
-  }
-
-  public void assertAnalyzesToReuse(Analyzer a, String input, String[] output,
-      int startOffsets[], int endOffsets[], int posIncs[]) throws Exception {
-
-    TokenStream ts = a.reusableTokenStream("dummy", new StringReader(input));
-    TermAttribute termAtt = (TermAttribute) ts
-        .getAttribute(TermAttribute.class);
-    OffsetAttribute offsetAtt = (OffsetAttribute) ts
-        .getAttribute(OffsetAttribute.class);
-    PositionIncrementAttribute posIncAtt = (PositionIncrementAttribute) ts
-        .getAttribute(PositionIncrementAttribute.class);
-    for (int i = 0; i < output.length; i++) {
-      assertTrue(ts.incrementToken());
-      assertEquals(termAtt.term(), output[i]);
-      assertEquals(offsetAtt.startOffset(), startOffsets[i]);
-      assertEquals(offsetAtt.endOffset(), endOffsets[i]);
-      assertEquals(posIncAtt.getPositionIncrement(), posIncs[i]);
-    }
-    assertFalse(ts.incrementToken());
-  }
 }
diff --git a/contrib/snowball/src/test/org/apache/lucene/analysis/snowball/TestSnowball.java b/contrib/snowball/src/test/org/apache/lucene/analysis/snowball/TestSnowball.java
index caaf5e9..2b2e393 100644
--- a/contrib/snowball/src/test/org/apache/lucene/analysis/snowball/TestSnowball.java
+++ b/contrib/snowball/src/test/org/apache/lucene/analysis/snowball/TestSnowball.java
@@ -20,8 +20,7 @@ package org.apache.lucene.analysis.snowball;
 import java.io.Reader;
 import java.io.StringReader;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.index.Payload;
@@ -33,32 +32,7 @@ import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
 
-public class TestSnowball extends TestCase {
-
-  public void assertAnalyzesTo(Analyzer a,
-                               String input,
-                               String[] output) throws Exception {
-    TokenStream ts = a.tokenStream("dummy", new StringReader(input));
-    TermAttribute termAtt = (TermAttribute) ts.getAttribute(TermAttribute.class);
-    for (int i = 0; i < output.length; i++) {
-      assertTrue(ts.incrementToken());
-      assertEquals(output[i], termAtt.term());
-    }
-    assertFalse(ts.incrementToken());
-    ts.close();
-  }
-  
-  public void assertAnalyzesToReuse(Analyzer a,
-                               String input,
-                               String[] output) throws Exception {
-    TokenStream ts = a.reusableTokenStream("dummy", new StringReader(input));
-    TermAttribute termAtt = (TermAttribute) ts.getAttribute(TermAttribute.class);
-    for (int i = 0; i < output.length; i++) {
-      assertTrue(ts.incrementToken());
-      assertEquals(output[i], termAtt.term());
-    }
-    assertFalse(ts.incrementToken());
-  }
+public class TestSnowball extends BaseTokenStreamTestCase {
 
   public void testEnglish() throws Exception {
     Analyzer a = new SnowballAnalyzer("English");
diff --git a/contrib/wikipedia/src/test/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerTest.java b/contrib/wikipedia/src/test/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerTest.java
index a594335..5720edd 100644
--- a/contrib/wikipedia/src/test/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerTest.java
+++ b/contrib/wikipedia/src/test/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerTest.java
@@ -27,6 +27,7 @@ import java.util.Map;
 import java.util.Set;
 import java.util.HashSet;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.tokenattributes.FlagsAttribute;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
@@ -38,22 +39,13 @@ import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
  *
  *
  **/
-public class WikipediaTokenizerTest extends TestCase {
+public class WikipediaTokenizerTest extends BaseTokenStreamTestCase {
   protected static final String LINK_PHRASES = "click [[link here again]] click [http://lucene.apache.org here again] [[Category:a b c d]]";
 
-
   public WikipediaTokenizerTest(String s) {
     super(s);
   }
 
-  protected void setUp() {
-  }
-
-  protected void tearDown() {
-
-  }
-
-
   public void testHandwritten() throws Exception {
     //make sure all tokens are in only one type
     String test = "[[link]] This is a [[Category:foo]] Category  This is a linked [[:Category:bar none withstanding]] " +
diff --git a/src/java/org/apache/lucene/analysis/Token.java b/src/java/org/apache/lucene/analysis/Token.java
index 9cc6ac7..67f752b 100644
--- a/src/java/org/apache/lucene/analysis/Token.java
+++ b/src/java/org/apache/lucene/analysis/Token.java
@@ -866,6 +866,9 @@ public class Token extends AttributeImpl
       if (payload !=null) {
         to.payload = (Payload) payload.clone();
       }
+    // remove the following optimization in 3.0 when old TokenStream API removed:
+    } else if (target instanceof TokenWrapper) {
+      ((TokenWrapper) target).delegate = (Token) this.clone();
     } else {
       initTermBuffer();
       ((TermAttribute) target).setTermBuffer(termBuffer, 0, termLength);
diff --git a/src/test/org/apache/lucene/analysis/BaseTokenStreamTestCase.java b/src/test/org/apache/lucene/analysis/BaseTokenStreamTestCase.java
index 1bd2c10..452a0eb 100644
--- a/src/test/org/apache/lucene/analysis/BaseTokenStreamTestCase.java
+++ b/src/test/org/apache/lucene/analysis/BaseTokenStreamTestCase.java
@@ -18,7 +18,10 @@ package org.apache.lucene.analysis;
  */
 
 import java.util.Set;
+import java.io.StringReader;
+import java.io.IOException;
  
+import org.apache.lucene.analysis.tokenattributes.*;
 import org.apache.lucene.util.LuceneTestCase;
 
 /** 
@@ -59,12 +62,6 @@ public abstract class BaseTokenStreamTestCase extends LuceneTestCase {
   }
 
   // @Override
-  protected void tearDown() throws Exception {
-    TokenStream.setOnlyUseNewAPI(false);
-    super.tearDown();
-  }
-
-  // @Override
   public void runBare() throws Throwable {
     // Do the test with onlyUseNewAPI=false (default)
     try {
@@ -86,5 +83,127 @@ public abstract class BaseTokenStreamTestCase extends LuceneTestCase {
       }
     }
   }
+  
+  // some helpers to test Analyzers and TokenStreams:
+  
+  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[]) throws IOException {
+    assertNotNull(output);
+    assertTrue("has TermAttribute", ts.hasAttribute(TermAttribute.class));
+    TermAttribute termAtt = (TermAttribute) ts.getAttribute(TermAttribute.class);
+    
+    OffsetAttribute offsetAtt = null;
+    if (startOffsets != null || endOffsets != null) {
+      assertTrue("has OffsetAttribute", ts.hasAttribute(OffsetAttribute.class));
+      offsetAtt = (OffsetAttribute) ts.getAttribute(OffsetAttribute.class);
+    }
+    
+    TypeAttribute typeAtt = null;
+    if (types != null) {
+      assertTrue("has TypeAttribute", ts.hasAttribute(TypeAttribute.class));
+      typeAtt = (TypeAttribute) ts.getAttribute(TypeAttribute.class);
+    }
+    
+    PositionIncrementAttribute posIncrAtt = null;
+    if (posIncrements != null) {
+      assertTrue("has PositionIncrementAttribute", ts.hasAttribute(PositionIncrementAttribute.class));
+      posIncrAtt = (PositionIncrementAttribute) ts.getAttribute(PositionIncrementAttribute.class);
+    }
+    
+    ts.reset();
+    for (int i = 0; i < output.length; i++) {
+      assertTrue("token "+i+" exists", ts.incrementToken());
+      assertEquals("term "+i, output[i], termAtt.term());
+      if (startOffsets != null)
+        assertEquals("startOffset "+i, startOffsets[i], offsetAtt.startOffset());
+      if (endOffsets != null)
+        assertEquals("endOffset "+i, endOffsets[i], offsetAtt.endOffset());
+      if (types != null)
+        assertEquals("type "+i, types[i], typeAtt.type());
+      if (posIncrements != null)
+        assertEquals("posIncrement "+i, posIncrements[i], posIncrAtt.getPositionIncrement());
+    }
+    assertFalse("end of stream", ts.incrementToken());
+    ts.close();
+  }
+  
+  public static void assertTokenStreamContents(TokenStream ts, String[] output) throws IOException {
+    assertTokenStreamContents(ts, output, null, null, null, null);
+  }
+  
+  public static void assertTokenStreamContents(TokenStream ts, String[] output, String[] types) throws IOException {
+    assertTokenStreamContents(ts, output, null, null, types, null);
+  }
+  
+  public static void assertTokenStreamContents(TokenStream ts, String[] output, int[] posIncrements) throws IOException {
+    assertTokenStreamContents(ts, output, null, null, null, posIncrements);
+  }
+  
+  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[]) throws IOException {
+    assertTokenStreamContents(ts, output, startOffsets, endOffsets, null, null);
+  }
+  
+  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], int[] posIncrements) throws IOException {
+    assertTokenStreamContents(ts, output, startOffsets, endOffsets, null, posIncrements);
+  }
 
+  
+  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[]) throws IOException {
+    assertTokenStreamContents(a.tokenStream("dummy", new StringReader(input)), output, startOffsets, endOffsets, types, posIncrements);
+  }
+  
+  public static void assertAnalyzesTo(Analyzer a, String input, String[] output) throws IOException {
+    assertAnalyzesTo(a, input, output, null, null, null, null);
+  }
+  
+  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, String[] types) throws IOException {
+    assertAnalyzesTo(a, input, output, null, null, types, null);
+  }
+  
+  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int[] posIncrements) throws IOException {
+    assertAnalyzesTo(a, input, output, null, null, null, posIncrements);
+  }
+  
+  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[]) throws IOException {
+    assertAnalyzesTo(a, input, output, startOffsets, endOffsets, null, null);
+  }
+  
+  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], int[] posIncrements) throws IOException {
+    assertAnalyzesTo(a, input, output, startOffsets, endOffsets, null, posIncrements);
+  }
+  
+
+  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[]) throws IOException {
+    assertTokenStreamContents(a.reusableTokenStream("dummy", new StringReader(input)), output, startOffsets, endOffsets, types, posIncrements);
+  }
+  
+  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output) throws IOException {
+    assertAnalyzesToReuse(a, input, output, null, null, null, null);
+  }
+  
+  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, String[] types) throws IOException {
+    assertAnalyzesToReuse(a, input, output, null, null, types, null);
+  }
+  
+  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, int[] posIncrements) throws IOException {
+    assertAnalyzesToReuse(a, input, output, null, null, null, posIncrements);
+  }
+  
+  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[]) throws IOException {
+    assertAnalyzesToReuse(a, input, output, startOffsets, endOffsets, null, null);
+  }
+  
+  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], int[] posIncrements) throws IOException {
+    assertAnalyzesToReuse(a, input, output, startOffsets, endOffsets, null, posIncrements);
+  }
+
+  // simple utility method for testing stemmers
+  
+  public static void checkOneTerm(Analyzer a, final String input, final String expected) throws IOException {
+    assertAnalyzesTo(a, input, new String[]{expected});
+  }
+  
+  public static void checkOneTermReuse(Analyzer a, final String input, final String expected) throws IOException {
+    assertAnalyzesToReuse(a, input, new String[]{expected});
+  }
+  
 }
diff --git a/src/test/org/apache/lucene/analysis/BaseTokenTestCase.java b/src/test/org/apache/lucene/analysis/BaseTokenTestCase.java
deleted file mode 100644
index d61482e..0000000
--- a/src/test/org/apache/lucene/analysis/BaseTokenTestCase.java
+++ /dev/null
@@ -1,157 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.lucene.analysis;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-
-import org.apache.lucene.util.AttributeImpl;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-
-public abstract class BaseTokenTestCase extends BaseTokenStreamTestCase {
-
-  public static String tsToString(TokenStream in) throws IOException {
-    final TermAttribute termAtt = (TermAttribute) in.addAttribute(TermAttribute.class);
-    final StringBuffer out = new StringBuffer();
-    in.reset();
-    while (in.incrementToken()) {
-      if (out.length()>0) out.append(' ');
-      out.append(termAtt.term());
-    }
-    in.close();
-    return out.toString();
-  }
-
-  public void assertTokEqual(List/*<Token>*/ a, List/*<Token>*/ b) {
-    assertTokEq(a,b,false);
-    assertTokEq(b,a,false);
-  }
-
-  public void assertTokEqualOff(List/*<Token>*/ a, List/*<Token>*/ b) {
-    assertTokEq(a,b,true);
-    assertTokEq(b,a,true);
-  }
-
-  private void assertTokEq(List/*<Token>*/ a, List/*<Token>*/ b, boolean checkOff) {
-    int pos=0;
-    for (Iterator iter = a.iterator(); iter.hasNext();) {
-      Token tok = (Token)iter.next();
-      pos += tok.getPositionIncrement();
-      if (!tokAt(b, tok.term(), pos
-              , checkOff ? tok.startOffset() : -1
-              , checkOff ? tok.endOffset() : -1
-              )) 
-      {
-        fail(a + "!=" + b);
-      }
-    }
-  }
-
-  public boolean tokAt(List/*<Token>*/ lst, String val, int tokPos, int startOff, int endOff) {
-    int pos=0;
-    for (Iterator iter = lst.iterator(); iter.hasNext();) {
-      Token tok = (Token)iter.next();
-      pos += tok.getPositionIncrement();
-      if (pos==tokPos && tok.term().equals(val)
-          && (startOff==-1 || tok.startOffset()==startOff)
-          && (endOff  ==-1 || tok.endOffset()  ==endOff  )
-           )
-      {
-        return true;
-      }
-    }
-    return false;
-  }
-
-
-  /***
-   * Return a list of tokens according to a test string format:
-   * a b c  =>  returns List<Token> [a,b,c]
-   * a/b   => tokens a and b share the same spot (b.positionIncrement=0)
-   * a,3/b/c => a,b,c all share same position (a.positionIncrement=3, b.positionIncrement=0, c.positionIncrement=0)
-   * a,1,10,11  => "a" with positionIncrement=1, startOffset=10, endOffset=11
-   */
-  public List/*<Token>*/ tokens(String str) {
-    String[] arr = str.split(" ");
-    List/*<Token>*/ result = new ArrayList/*<Token>*/();
-    for (int i=0; i<arr.length; i++) {
-      String[] toks = arr[i].split("/");
-      String[] params = toks[0].split(",");
-
-      int posInc;
-      int start;
-      int end;
-
-      if (params.length > 1) {
-        posInc = Integer.parseInt(params[1]);
-      } else {
-        posInc = 1;
-      }
-
-      if (params.length > 2) {
-        start = Integer.parseInt(params[2]);
-      } else {
-        start = 0;
-      }
-
-      if (params.length > 3) {
-        end = Integer.parseInt(params[3]);
-      } else {
-        end = start + params[0].length();
-      }
-
-      Token t = new Token(params[0],start,end,"TEST");
-      t.setPositionIncrement(posInc);
-      
-      result.add(t);
-      for (int j=1; j<toks.length; j++) {
-        t = new Token(toks[j],0,0,"TEST");
-        t.setPositionIncrement(0);
-        result.add(t);
-      }
-    }
-    return result;
-  }
-
-  //------------------------------------------------------------------------
-  // These may be useful beyond test cases...
-  //------------------------------------------------------------------------
-
-  static List/*<Token>*/ getTokens(TokenStream tstream) throws IOException {
-    List/*<Token>*/ tokens = new ArrayList/*<Token>*/();
-    tstream.reset();
-    while (tstream.incrementToken()) {
-      final Token t = new Token();
-      for (Iterator it = tstream.getAttributeImplsIterator(); it.hasNext();) {
-        final AttributeImpl att = (AttributeImpl) it.next();
-        try {
-          att.copyTo(t);
-        } catch (ClassCastException ce) {
-          // ignore Attributes unsupported by Token
-        }
-      }
-      tokens.add(t);
-    }
-    tstream.close();
-    
-    return tokens;
-  }
-
-}
diff --git a/src/test/org/apache/lucene/analysis/TestAnalyzers.java b/src/test/org/apache/lucene/analysis/TestAnalyzers.java
index c43d3bf..447b614 100644
--- a/src/test/org/apache/lucene/analysis/TestAnalyzers.java
+++ b/src/test/org/apache/lucene/analysis/TestAnalyzers.java
@@ -33,19 +33,6 @@ public class TestAnalyzers extends BaseTokenStreamTestCase {
       super(name);
    }
 
-  public void assertAnalyzesTo(Analyzer a, 
-                               String input, 
-                               String[] output) throws Exception {
-    TokenStream ts = a.tokenStream("dummy", new StringReader(input));
-    TermAttribute termAtt = (TermAttribute) ts.getAttribute(TermAttribute.class);
-    for (int i=0; i<output.length; i++) {
-      assertTrue(ts.incrementToken());
-      assertEquals(termAtt.term(), output[i]);
-    }
-    assertFalse(ts.incrementToken());
-    ts.close();
-  }
-
   public void testSimple() throws Exception {
     Analyzer a = new SimpleAnalyzer();
     assertAnalyzesTo(a, "foo bar FOO BAR", 
diff --git a/src/test/org/apache/lucene/analysis/TestMappingCharFilter.java b/src/test/org/apache/lucene/analysis/TestMappingCharFilter.java
index 520b67c..3f64d1e 100644
--- a/src/test/org/apache/lucene/analysis/TestMappingCharFilter.java
+++ b/src/test/org/apache/lucene/analysis/TestMappingCharFilter.java
@@ -20,7 +20,7 @@ package org.apache.lucene.analysis;
 import java.io.StringReader;
 import java.util.List;
 
-public class TestMappingCharFilter extends BaseTokenTestCase {
+public class TestMappingCharFilter extends BaseTokenStreamTestCase {
 
   NormalizeCharMap normMap;
 
@@ -59,72 +59,55 @@ public class TestMappingCharFilter extends BaseTokenTestCase {
   public void testNothingChange() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, CharReader.get( new StringReader( "x" ) ) );
     TokenStream ts = new WhitespaceTokenizer( cs );
-    List real = getTokens( ts );
-    List expect = tokens( "x" );
-    assertTokEqualOff( expect, real );
+    assertTokenStreamContents(ts, new String[]{"x"}, new int[]{0}, new int[]{1});
   }
 
   public void test1to1() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, CharReader.get( new StringReader( "h" ) ) );
     TokenStream ts = new WhitespaceTokenizer( cs );
-    List real = getTokens( ts );
-    List expect = tokens( "i" );
-    assertTokEqualOff( expect, real );
+    assertTokenStreamContents(ts, new String[]{"i"}, new int[]{0}, new int[]{1});
   }
 
   public void test1to2() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, CharReader.get( new StringReader( "j" ) ) );
     TokenStream ts = new WhitespaceTokenizer( cs );
-    List real = getTokens( ts );
-    List expect = tokens( "jj,1,0,1" );
-    assertTokEqualOff( expect, real );
+    assertTokenStreamContents(ts, new String[]{"jj"}, new int[]{0}, new int[]{1});
   }
 
   public void test1to3() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, CharReader.get( new StringReader( "k" ) ) );
     TokenStream ts = new WhitespaceTokenizer( cs );
-    List real = getTokens( ts );
-    List expect = tokens( "kkk,1,0,1" );
-    assertTokEqualOff( expect, real );
+    assertTokenStreamContents(ts, new String[]{"kkk"}, new int[]{0}, new int[]{1});
   }
 
   public void test2to4() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, CharReader.get( new StringReader( "ll" ) ) );
     TokenStream ts = new WhitespaceTokenizer( cs );
-    List real = getTokens( ts );
-    List expect = tokens( "llll,1,0,2" );
-    assertTokEqualOff( expect, real );
+    assertTokenStreamContents(ts, new String[]{"llll"}, new int[]{0}, new int[]{2});
   }
 
   public void test2to1() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, CharReader.get( new StringReader( "aa" ) ) );
     TokenStream ts = new WhitespaceTokenizer( cs );
-    List real = getTokens( ts );
-    List expect = tokens( "a,1,0,2" );
-    assertTokEqualOff( expect, real );
+    assertTokenStreamContents(ts, new String[]{"a"}, new int[]{0}, new int[]{2});
   }
 
   public void test3to1() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, CharReader.get( new StringReader( "bbb" ) ) );
     TokenStream ts = new WhitespaceTokenizer( cs );
-    List real = getTokens( ts );
-    List expect = tokens( "b,1,0,3" );
-    assertTokEqualOff( expect, real );
+    assertTokenStreamContents(ts, new String[]{"b"}, new int[]{0}, new int[]{3});
   }
 
   public void test4to2() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, CharReader.get( new StringReader( "cccc" ) ) );
     TokenStream ts = new WhitespaceTokenizer( cs );
-    List real = getTokens( ts );
-    List expect = tokens( "cc,1,0,4" );
-    assertTokEqualOff( expect, real );
+    assertTokenStreamContents(ts, new String[]{"cc"}, new int[]{0}, new int[]{4});
   }
 
   public void test5to0() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, CharReader.get( new StringReader( "empty" ) ) );
     TokenStream ts = new WhitespaceTokenizer( cs );
-    List real = getTokens( ts );
-    assertEquals( 0, real.size() );
+    assertTokenStreamContents(ts, new String[0]);
   }
 
   //
@@ -148,9 +131,11 @@ public class TestMappingCharFilter extends BaseTokenTestCase {
   public void testTokenStream() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, CharReader.get( new StringReader( "h i j k ll cccc bbb aa" ) ) );
     TokenStream ts = new WhitespaceTokenizer( cs );
-    List real = getTokens( ts );
-    List expect = tokens( "i,1,0,1 i,1,2,3 jj,1,4,5 kkk,1,6,7 llll,1,8,10 cc,1,11,15 b,1,16,19 a,1,20,22" );
-    assertTokEqualOff( expect, real );
+    assertTokenStreamContents(ts,
+      new String[]{"i","i","jj","kkk","llll","cc","b","a"},
+      new int[]{0,2,4,6,8,11,16,20},
+      new int[]{1,3,5,7,10,15,19,22}
+    );
   }
 
   //
@@ -167,8 +152,10 @@ public class TestMappingCharFilter extends BaseTokenTestCase {
     CharStream cs = new MappingCharFilter( normMap,
         new MappingCharFilter( normMap, CharReader.get( new StringReader( "aaaa ll h" ) ) ) );
     TokenStream ts = new WhitespaceTokenizer( cs );
-    List real = getTokens( ts );
-    List expect = tokens( "a,1,0,4 llllllll,1,5,7 i,1,8,9" );
-    assertTokEqualOff( expect, real );
+    assertTokenStreamContents(ts,
+      new String[]{"a","llllllll","i"},
+      new int[]{0,5,8},
+      new int[]{4,7,9}
+    );
   }
 }
diff --git a/src/test/org/apache/lucene/analysis/TestStandardAnalyzer.java b/src/test/org/apache/lucene/analysis/TestStandardAnalyzer.java
index c0f2396..3ae4092 100644
--- a/src/test/org/apache/lucene/analysis/TestStandardAnalyzer.java
+++ b/src/test/org/apache/lucene/analysis/TestStandardAnalyzer.java
@@ -28,39 +28,6 @@ public class TestStandardAnalyzer extends BaseTokenStreamTestCase {
 
   private Analyzer a = new StandardAnalyzer();
 
-  public void assertAnalyzesTo(Analyzer a, String input, String[] expected) throws Exception {
-    assertAnalyzesTo(a, input, expected, null);
-  }
-
-  public void assertAnalyzesTo(Analyzer a, String input, String[] expectedImages, String[] expectedTypes) throws Exception {
-    assertAnalyzesTo(a, input, expectedImages, expectedTypes, null);
-  }
-
-  public void assertAnalyzesTo(Analyzer a, String input, String[] expectedImages, String[] expectedTypes, int[] expectedPosIncrs) throws Exception {
-    TokenStream ts = a.tokenStream("dummy", new StringReader(input));
-    // TODO Java 1.5
-    //final TypeAttribute typeAtt = reusableToken.getAttribute(TypeAttribute.class);
-    //final PositionIncrementAttribute posIncrAtt = reusableToken.getAttribute(PositionIncrementAttribute.class);
-
-    final TermAttribute termAtt = (TermAttribute) ts.getAttribute(TermAttribute.class);
-    final TypeAttribute typeAtt = (TypeAttribute) ts.getAttribute(TypeAttribute.class);
-    final PositionIncrementAttribute posIncrAtt = (PositionIncrementAttribute) ts.getAttribute(PositionIncrementAttribute.class);
-    
-    for (int i = 0; i < expectedImages.length; i++) {
-      assertTrue(ts.incrementToken());
-      assertEquals(expectedImages[i], new String(termAtt.termBuffer(), 0, termAtt.termLength()));
-      if (expectedTypes != null) {
-        assertEquals(expectedTypes[i], typeAtt.type());
-      }
-      if (expectedPosIncrs != null) {
-        assertEquals(expectedPosIncrs[i], posIncrAtt.getPositionIncrement());
-      }
-    }
-    assertFalse(ts.incrementToken());
-    ts.close();
-  }
-
-
   public void testMaxTermLength() throws Exception {
     StandardAnalyzer sa = new StandardAnalyzer();
     sa.setMaxTokenLength(5);
@@ -72,7 +39,7 @@ public class TestStandardAnalyzer extends BaseTokenStreamTestCase {
     assertAnalyzesTo(sa, "ab cd toolong xy z", new String[]{"ab", "cd", "toolong", "xy", "z"});
     sa.setMaxTokenLength(5);
     
-    assertAnalyzesTo(sa, "ab cd toolong xy z", new String[]{"ab", "cd", "xy", "z"}, null, new int[]{1, 1, 2, 1});
+    assertAnalyzesTo(sa, "ab cd toolong xy z", new String[]{"ab", "cd", "xy", "z"}, new int[]{1, 1, 2, 1});
   }
 
   public void testMaxTermLength3() throws Exception {
diff --git a/src/test/org/apache/lucene/analysis/TestToken.java b/src/test/org/apache/lucene/analysis/TestToken.java
index 2a802ea..bc000c4 100644
--- a/src/test/org/apache/lucene/analysis/TestToken.java
+++ b/src/test/org/apache/lucene/analysis/TestToken.java
@@ -185,11 +185,16 @@ public class TestToken extends LuceneTestCase {
   }
   
   public void testCopyTo() throws Exception {
-    Token t = new Token(0, 5);
+    Token t = new Token();
+    Token copy = (Token) TestSimpleAttributeImpls.assertCopyIsEqual(t);
+    assertEquals("", t.term());
+    assertEquals("", copy.term());
+
+    t = new Token(0, 5);
     char[] content = "hello".toCharArray();
     t.setTermBuffer(content, 0, 5);
     char[] buf = t.termBuffer();
-    Token copy = (Token) TestSimpleAttributeImpls.assertCopyIsEqual(t);
+    copy = (Token) TestSimpleAttributeImpls.assertCopyIsEqual(t);
     assertEquals(t.term(), copy.term());
     assertNotSame(buf, copy.termBuffer());
 
diff --git a/src/test/org/apache/lucene/analysis/tokenattributes/TestTermAttributeImpl.java b/src/test/org/apache/lucene/analysis/tokenattributes/TestTermAttributeImpl.java
index 141ba05..eb333dc 100644
--- a/src/test/org/apache/lucene/analysis/tokenattributes/TestTermAttributeImpl.java
+++ b/src/test/org/apache/lucene/analysis/tokenattributes/TestTermAttributeImpl.java
@@ -148,10 +148,15 @@ public class TestTermAttributeImpl extends LuceneTestCase {
   
   public void testCopyTo() throws Exception {
     TermAttributeImpl t = new TermAttributeImpl();
+    TermAttributeImpl copy = (TermAttributeImpl) TestSimpleAttributeImpls.assertCopyIsEqual(t);
+    assertEquals("", t.term());
+    assertEquals("", copy.term());
+
+    t = new TermAttributeImpl();
     char[] content = "hello".toCharArray();
     t.setTermBuffer(content, 0, 5);
     char[] buf = t.termBuffer();
-    TermAttributeImpl copy = (TermAttributeImpl) TestSimpleAttributeImpls.assertCopyIsEqual(t);
+    copy = (TermAttributeImpl) TestSimpleAttributeImpls.assertCopyIsEqual(t);
     assertEquals(t.term(), copy.term());
     assertNotSame(buf, copy.termBuffer());
   }
diff --git a/src/test/org/apache/lucene/util/LuceneTestCase.java b/src/test/org/apache/lucene/util/LuceneTestCase.java
index 3b93806..02c6673 100644
--- a/src/test/org/apache/lucene/util/LuceneTestCase.java
+++ b/src/test/org/apache/lucene/util/LuceneTestCase.java
@@ -50,6 +50,8 @@ import org.apache.lucene.util.FieldCacheSanityChecker.Insanity;
  */
 public abstract class LuceneTestCase extends TestCase {
 
+  private boolean savedAPISetting = false;
+
   public LuceneTestCase() {
     super();
   }
@@ -61,6 +63,8 @@ public abstract class LuceneTestCase extends TestCase {
   protected void setUp() throws Exception {
     super.setUp();
     ConcurrentMergeScheduler.setTestMode();
+    
+    savedAPISetting = TokenStream.getOnlyUseNewAPI();
     TokenStream.setOnlyUseNewAPI(false);
   }
 
@@ -99,6 +103,8 @@ public abstract class LuceneTestCase extends TestCase {
     } finally {
       purgeFieldCache(FieldCache.DEFAULT);
     }
+    
+    TokenStream.setOnlyUseNewAPI(savedAPISetting);
     super.tearDown();
   }
 

