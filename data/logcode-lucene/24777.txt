GitDiffStart: c0b059bf978edfadc681a3166ea7e3a87ce0de46 | Sun Dec 5 18:21:58 2004 +0000
diff --git a/src/test-deprecated/org/apache/lucene/analysis/TestAnalyzers.java b/src/test-deprecated/org/apache/lucene/analysis/TestAnalyzers.java
new file mode 100644
index 0000000..dfdefe2
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/analysis/TestAnalyzers.java
@@ -0,0 +1,92 @@
+package org.apache.lucene.analysis;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.*;
+import junit.framework.*;
+
+import org.apache.lucene.*;
+import org.apache.lucene.analysis.*;
+
+public class TestAnalyzers extends TestCase {
+
+   public TestAnalyzers(String name) {
+      super(name);
+   }
+
+  public void assertAnalyzesTo(Analyzer a, 
+                               String input, 
+                               String[] output) throws Exception {
+    TokenStream ts = a.tokenStream("dummy", new StringReader(input));
+    for (int i=0; i<output.length; i++) {
+      Token t = ts.next();
+      assertNotNull(t);
+      assertEquals(t.termText(), output[i]);
+    }
+    assertNull(ts.next());
+    ts.close();
+  }
+
+  public void testSimple() throws Exception {
+    Analyzer a = new SimpleAnalyzer();
+    assertAnalyzesTo(a, "foo bar FOO BAR", 
+                     new String[] { "foo", "bar", "foo", "bar" });
+    assertAnalyzesTo(a, "foo      bar .  FOO <> BAR", 
+                     new String[] { "foo", "bar", "foo", "bar" });
+    assertAnalyzesTo(a, "foo.bar.FOO.BAR", 
+                     new String[] { "foo", "bar", "foo", "bar" });
+    assertAnalyzesTo(a, "U.S.A.", 
+                     new String[] { "u", "s", "a" });
+    assertAnalyzesTo(a, "C++", 
+                     new String[] { "c" });
+    assertAnalyzesTo(a, "B2B", 
+                     new String[] { "b", "b" });
+    assertAnalyzesTo(a, "2B", 
+                     new String[] { "b" });
+    assertAnalyzesTo(a, "\"QUOTED\" word", 
+                     new String[] { "quoted", "word" });
+  }
+
+  public void testNull() throws Exception {
+    Analyzer a = new WhitespaceAnalyzer();
+    assertAnalyzesTo(a, "foo bar FOO BAR", 
+                     new String[] { "foo", "bar", "FOO", "BAR" });
+    assertAnalyzesTo(a, "foo      bar .  FOO <> BAR", 
+                     new String[] { "foo", "bar", ".", "FOO", "<>", "BAR" });
+    assertAnalyzesTo(a, "foo.bar.FOO.BAR", 
+                     new String[] { "foo.bar.FOO.BAR" });
+    assertAnalyzesTo(a, "U.S.A.", 
+                     new String[] { "U.S.A." });
+    assertAnalyzesTo(a, "C++", 
+                     new String[] { "C++" });
+    assertAnalyzesTo(a, "B2B", 
+                     new String[] { "B2B" });
+    assertAnalyzesTo(a, "2B", 
+                     new String[] { "2B" });
+    assertAnalyzesTo(a, "\"QUOTED\" word", 
+                     new String[] { "\"QUOTED\"", "word" });
+  }
+
+  public void testStop() throws Exception {
+    Analyzer a = new StopAnalyzer();
+    assertAnalyzesTo(a, "foo bar FOO BAR", 
+                     new String[] { "foo", "bar", "foo", "bar" });
+    assertAnalyzesTo(a, "foo a bar such FOO THESE BAR", 
+                     new String[] { "foo", "bar", "foo", "bar" });
+  }
+}
+
diff --git a/src/test-deprecated/org/apache/lucene/analysis/TestPerFieldAnalzyerWrapper.java b/src/test-deprecated/org/apache/lucene/analysis/TestPerFieldAnalzyerWrapper.java
new file mode 100644
index 0000000..81a80f1
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/analysis/TestPerFieldAnalzyerWrapper.java
@@ -0,0 +1,43 @@
+package org.apache.lucene.analysis;
+
+import junit.framework.TestCase;
+import java.io.StringReader;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class TestPerFieldAnalzyerWrapper extends TestCase {
+  public void testPerField() throws Exception {
+    String text = "Qwerty";
+    PerFieldAnalyzerWrapper analyzer =
+              new PerFieldAnalyzerWrapper(new WhitespaceAnalyzer());
+    analyzer.addAnalyzer("special", new SimpleAnalyzer());
+
+    TokenStream tokenStream = analyzer.tokenStream("field",
+                                            new StringReader(text));
+    Token token = tokenStream.next();
+    assertEquals("WhitespaceAnalyzer does not lowercase",
+                 "Qwerty",
+                 token.termText());
+
+    tokenStream = analyzer.tokenStream("special",
+                                            new StringReader(text));
+    token = tokenStream.next();
+    assertEquals("SimpleAnalyzer lowercases",
+                 "qwerty",
+                 token.termText());
+  }
+}
diff --git a/src/test-deprecated/org/apache/lucene/analysis/TestStopAnalyzer.java b/src/test-deprecated/org/apache/lucene/analysis/TestStopAnalyzer.java
new file mode 100644
index 0000000..acf7bd7
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/analysis/TestStopAnalyzer.java
@@ -0,0 +1,76 @@
+package org.apache.lucene.analysis;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import junit.framework.TestCase;
+
+import java.io.StringReader;
+import java.io.IOException;
+import java.util.Set;
+import java.util.HashSet;
+
+public class TestStopAnalyzer extends TestCase {
+  private StopAnalyzer stop = new StopAnalyzer();
+
+  private Set inValidTokens = new HashSet();
+  public TestStopAnalyzer(String s) {
+    super(s);
+  }
+
+  protected void setUp() {
+    for (int i = 0; i < StopAnalyzer.ENGLISH_STOP_WORDS.length; i++) {
+      inValidTokens.add(StopAnalyzer.ENGLISH_STOP_WORDS[i]);
+    }
+  }
+
+  public void testDefaults() {
+    assertTrue(stop != null);
+    StringReader reader = new StringReader("This is a test of the english stop analyzer");
+    TokenStream stream = stop.tokenStream("test", reader);
+    assertTrue(stream != null);
+    Token token = null;
+    try {
+      while ((token = stream.next()) != null)
+      {
+        assertTrue(inValidTokens.contains(token.termText()) == false);
+      }
+    } catch (IOException e) {
+      assertTrue(false);
+    }
+  }
+
+  public void testStopList() {
+    Set stopWordsSet = new HashSet();
+    stopWordsSet.add("good");
+    stopWordsSet.add("test");
+    stopWordsSet.add("analyzer");
+    StopAnalyzer newStop = new StopAnalyzer((String[])stopWordsSet.toArray(new String[3]));
+    StringReader reader = new StringReader("This is a good test of the english stop analyzer");
+    TokenStream stream = newStop.tokenStream("test", reader);
+    assertTrue(stream != null);
+    Token token = null;
+    try {
+      while ((token = stream.next()) != null)
+      {
+        String text = token.termText();
+        assertTrue(stopWordsSet.contains(text) == false);
+      }
+    } catch (IOException e) {
+      assertTrue(false);
+    }
+  }
+}
diff --git a/src/test-deprecated/org/apache/lucene/document/TestDocument.java b/src/test-deprecated/org/apache/lucene/document/TestDocument.java
new file mode 100644
index 0000000..09f2193
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/document/TestDocument.java
@@ -0,0 +1,168 @@
+package org.apache.lucene.document;
+
+import junit.framework.TestCase;
+
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Searcher;
+import org.apache.lucene.search.Hits;
+
+import java.io.IOException;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Tests {@link Document} class.
+ *
+ * @author Otis Gospodnetic
+ * @version $Id$
+ */
+public class TestDocument extends TestCase
+{
+
+  /**
+   * Tests {@link Document#remove()} method for a brand new Document
+   * that has not been indexed yet.
+   *
+   * @throws Exception on error
+   */
+  public void testRemoveForNewDocument() throws Exception
+  {
+    Document doc = makeDocumentWithFields();
+    assertEquals(8, doc.fields.size());
+    doc.removeFields("keyword");
+    assertEquals(6, doc.fields.size());
+    doc.removeFields("doesnotexists");      // removing non-existing fields is siltenlty ignored
+    doc.removeFields("keyword");		// removing a field more than once
+    assertEquals(6, doc.fields.size());
+    doc.removeField("text");
+    assertEquals(5, doc.fields.size());
+    doc.removeField("text");
+    assertEquals(4, doc.fields.size());
+    doc.removeField("text");
+    assertEquals(4, doc.fields.size());
+    doc.removeField("doesnotexists");       // removing non-existing fields is siltenlty ignored
+    assertEquals(4, doc.fields.size());
+    doc.removeFields("unindexed");
+    assertEquals(2, doc.fields.size());
+    doc.removeFields("unstored");
+    assertEquals(0, doc.fields.size());
+    doc.removeFields("doesnotexists");	// removing non-existing fields is siltenlty ignored
+    assertEquals(0, doc.fields.size());
+  }
+
+    /**
+     * Tests {@link Document#getValues()} method for a brand new Document
+     * that has not been indexed yet.
+     *
+     * @throws Exception on error
+     */
+    public void testGetValuesForNewDocument() throws Exception
+    {
+        doAssert(makeDocumentWithFields(), false);
+    }
+
+    /**
+     * Tests {@link Document#getValues()} method for a Document retrieved from
+     * an index.
+     *
+     * @throws Exception on error
+     */
+    public void testGetValuesForIndexedDocument() throws Exception
+    {
+        RAMDirectory dir = new RAMDirectory();
+        IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(), true);
+        writer.addDocument(makeDocumentWithFields());
+        writer.close();
+
+        Searcher searcher = new IndexSearcher(dir);
+
+	// search for something that does exists
+	Query query = new TermQuery(new Term("keyword", "test1"));
+
+	// ensure that queries return expected results without DateFilter first
+        Hits hits = searcher.search(query);
+	assertEquals(1, hits.length());
+
+        try
+        {
+            doAssert(hits.doc(0), true);
+        }
+        catch (Exception e)
+        {
+            e.printStackTrace(System.err);
+            System.err.print("\n");
+        }
+        finally
+        {
+            searcher.close();
+        }
+    }
+
+    private Document makeDocumentWithFields() throws IOException
+    {
+        Document doc = new Document();
+        doc.add(Field.Keyword(  "keyword",   "test1"));
+        doc.add(Field.Keyword(  "keyword",   "test2"));
+        doc.add(Field.Text(     "text",      "test1"));
+        doc.add(Field.Text(     "text",      "test2"));
+        doc.add(Field.UnIndexed("unindexed", "test1"));
+        doc.add(Field.UnIndexed("unindexed", "test2"));
+        doc.add(Field.UnStored( "unstored",  "test1"));
+        doc.add(Field.UnStored( "unstored",  "test2"));
+        return doc;
+    }
+
+    private void doAssert(Document doc, boolean fromIndex)
+    {
+        String[] keywordFieldValues   = doc.getValues("keyword");
+        String[] textFieldValues      = doc.getValues("text");
+        String[] unindexedFieldValues = doc.getValues("unindexed");
+        String[] unstoredFieldValues  = doc.getValues("unstored");
+
+        assertTrue(keywordFieldValues.length   == 2);
+        assertTrue(textFieldValues.length      == 2);
+        assertTrue(unindexedFieldValues.length == 2);
+        // this test cannot work for documents retrieved from the index
+        // since unstored fields will obviously not be returned
+        if (! fromIndex)
+        {
+            assertTrue(unstoredFieldValues.length  == 2);
+        }
+
+        assertTrue(keywordFieldValues[0].equals("test1"));
+        assertTrue(keywordFieldValues[1].equals("test2"));
+        assertTrue(textFieldValues[0].equals("test1"));
+        assertTrue(textFieldValues[1].equals("test2"));
+        assertTrue(unindexedFieldValues[0].equals("test1"));
+        assertTrue(unindexedFieldValues[1].equals("test2"));
+        // this test cannot work for documents retrieved from the index
+        // since unstored fields will obviously not be returned
+        if (! fromIndex)
+        {
+            assertTrue(unstoredFieldValues[0].equals("test1"));
+            assertTrue(unstoredFieldValues[1].equals("test2"));
+        }
+    }
+}
diff --git a/src/test-deprecated/org/apache/lucene/index/DocHelper.java b/src/test-deprecated/org/apache/lucene/index/DocHelper.java
new file mode 100644
index 0000000..2a8c387
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/index/DocHelper.java
@@ -0,0 +1,159 @@
+package org.apache.lucene.index;
+
+/**
+ * Created by IntelliJ IDEA.
+ * User: Grant Ingersoll
+ * Date: Feb 2, 2004
+ * Time: 6:16:12 PM
+ * $Id$
+ * Copyright 2004.  Center For Natural Language Processing
+ */
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.search.Similarity;
+import org.apache.lucene.store.Directory;
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Enumeration;
+
+/**
+ *
+ *
+ **/
+class DocHelper {
+  public static final String FIELD_1_TEXT = "field one text";
+  public static final String TEXT_FIELD_1_KEY = "textField1";
+  public static Field textField1 = Field.Text(TEXT_FIELD_1_KEY, FIELD_1_TEXT, false);
+  
+  public static final String FIELD_2_TEXT = "field field field two text";
+  //Fields will be lexicographically sorted.  So, the order is: field, text, two
+  public static final int [] FIELD_2_FREQS = {3, 1, 1}; 
+  public static final String TEXT_FIELD_2_KEY = "textField2";
+  public static Field textField2 = Field.Text(TEXT_FIELD_2_KEY, FIELD_2_TEXT, true);
+  
+  public static final String KEYWORD_TEXT = "Keyword";
+  public static final String KEYWORD_FIELD_KEY = "keyField";
+  public static Field keyField = Field.Keyword(KEYWORD_FIELD_KEY, KEYWORD_TEXT);
+  
+  public static final String UNINDEXED_FIELD_TEXT = "unindexed field text";
+  public static final String UNINDEXED_FIELD_KEY = "unIndField";
+  public static Field unIndField = Field.UnIndexed(UNINDEXED_FIELD_KEY, UNINDEXED_FIELD_TEXT);
+  
+  public static final String UNSTORED_1_FIELD_TEXT = "unstored field text";
+  public static final String UNSTORED_FIELD_1_KEY = "unStoredField1";
+  public static Field unStoredField1 = Field.UnStored(UNSTORED_FIELD_1_KEY, UNSTORED_1_FIELD_TEXT, false);
+
+  public static final String UNSTORED_2_FIELD_TEXT = "unstored field text";
+  public static final String UNSTORED_FIELD_2_KEY = "unStoredField2";
+  public static Field unStoredField2 = Field.UnStored(UNSTORED_FIELD_2_KEY, UNSTORED_2_FIELD_TEXT, true);
+
+//  public static Set fieldNamesSet = null;
+//  public static Set fieldValuesSet = null;
+  public static Map nameValues = null;
+  
+  static
+  {
+    
+    nameValues = new HashMap();
+    nameValues.put(TEXT_FIELD_1_KEY, FIELD_1_TEXT);
+    nameValues.put(TEXT_FIELD_2_KEY, FIELD_2_TEXT);
+    nameValues.put(KEYWORD_FIELD_KEY, KEYWORD_TEXT);
+    nameValues.put(UNINDEXED_FIELD_KEY, UNINDEXED_FIELD_TEXT);
+    nameValues.put(UNSTORED_FIELD_1_KEY, UNSTORED_1_FIELD_TEXT);
+    nameValues.put(UNSTORED_FIELD_2_KEY, UNSTORED_2_FIELD_TEXT);
+  }
+  
+  /**
+   * Adds the fields above to a document 
+   * @param doc The document to write
+   */ 
+  public static void setupDoc(Document doc) {
+    doc.add(textField1);
+    doc.add(textField2);
+    doc.add(keyField);
+    doc.add(unIndField);
+    doc.add(unStoredField1);
+    doc.add(unStoredField2);
+  }                         
+  /**
+   * Writes the document to the directory using a segment named "test"
+   * @param dir
+   * @param doc
+   */ 
+  public static void writeDoc(Directory dir, Document doc)
+  {
+    
+    writeDoc(dir, "test", doc);
+  }
+  /**
+   * Writes the document to the directory in the given segment
+   * @param dir
+   * @param segment
+   * @param doc
+   */ 
+  public static void writeDoc(Directory dir, String segment, Document doc)
+  {
+    Analyzer analyzer = new WhitespaceAnalyzer();
+    Similarity similarity = Similarity.getDefault();
+    writeDoc(dir, analyzer, similarity, segment, doc);
+  }
+  /**
+   * Writes the document to the directory segment named "test" using the specified analyzer and similarity
+   * @param dir
+   * @param analyzer
+   * @param similarity
+   * @param doc
+   */ 
+  public static void writeDoc(Directory dir, Analyzer analyzer, Similarity similarity, Document doc)
+  {
+    writeDoc(dir, analyzer, similarity, "test", doc);
+  }
+  /**
+   * Writes the document to the directory segment using the analyzer and the similarity score
+   * @param dir
+   * @param analyzer
+   * @param similarity
+   * @param segment
+   * @param doc
+   */ 
+  public static void writeDoc(Directory dir, Analyzer analyzer, Similarity similarity, String segment, Document doc)
+  {
+    DocumentWriter writer = new DocumentWriter(dir, analyzer, similarity, 50);
+    try {
+      writer.addDocument(segment, doc);
+    } catch (IOException e) {
+      e.printStackTrace();
+    }
+  }
+
+  public static int numFields(Document doc) {
+    Enumeration fields = doc.fields();
+    int result = 0;
+    while (fields.hasMoreElements()) {
+      fields.nextElement();
+      result++;
+    }
+    return result;
+  }
+}
+/*
+    fieldNamesSet = new HashSet();
+    fieldNamesSet.add(TEXT_FIELD_1_KEY);
+    fieldNamesSet.add(TEXT_FIELD_2_KEY);
+    fieldNamesSet.add(KEYWORD_FIELD_KEY);
+    fieldNamesSet.add(UNINDEXED_FIELD_KEY);
+    fieldNamesSet.add(UNSTORED_FIELD_1_KEY);
+    fieldNamesSet.add(UNSTORED_FIELD_2_KEY);
+    fieldValuesSet = new HashSet();
+    fieldValuesSet.add(FIELD_1_TEXT);
+    fieldValuesSet.add(FIELD_2_TEXT);
+    fieldValuesSet.add(KEYWORD_TEXT);
+    fieldValuesSet.add(UNINDEXED_FIELD_TEXT);
+    fieldValuesSet.add(UNSTORED_1_FIELD_TEXT);
+    fieldValuesSet.add(UNSTORED_2_FIELD_TEXT);
+*/
diff --git a/src/test-deprecated/org/apache/lucene/index/MockInputStream.java b/src/test-deprecated/org/apache/lucene/index/MockInputStream.java
new file mode 100644
index 0000000..a506967
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/index/MockInputStream.java
@@ -0,0 +1,56 @@
+package org.apache.lucene.index;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.store.InputStream;
+
+import java.io.IOException;
+
+public class MockInputStream extends InputStream {
+    private byte[] buffer;
+    private int pointer = 0;
+
+    public MockInputStream(byte[] bytes) {
+        buffer = bytes;
+        length = bytes.length;
+    }
+
+    protected void readInternal(byte[] dest, int destOffset, int len)
+            throws IOException {
+        int remainder = len;
+        int start = pointer;
+        while (remainder != 0) {
+//          int bufferNumber = start / buffer.length;
+          int bufferOffset = start % buffer.length;
+          int bytesInBuffer = buffer.length - bufferOffset;
+          int bytesToCopy = bytesInBuffer >= remainder ? remainder : bytesInBuffer;
+          System.arraycopy(buffer, bufferOffset, dest, destOffset, bytesToCopy);
+          destOffset += bytesToCopy;
+          start += bytesToCopy;
+          remainder -= bytesToCopy;
+        }
+        pointer += len;
+    }
+
+    public void close() throws IOException {
+        // ignore
+    }
+
+    protected void seekInternal(long pos) throws IOException {
+        pointer = (int) pos;
+    }
+}
diff --git a/src/test-deprecated/org/apache/lucene/index/TermInfosTest.java b/src/test-deprecated/org/apache/lucene/index/TermInfosTest.java
new file mode 100644
index 0000000..4535e20
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/index/TermInfosTest.java
@@ -0,0 +1,183 @@
+package org.apache.lucene.index;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.FSDirectory;
+
+import java.util.Date;
+import java.util.Random;
+import java.util.Vector;
+import java.io.BufferedReader;
+import java.io.InputStreamReader;
+import java.io.File;
+import java.io.FileInputStream;
+
+class TermInfosTest {
+  public static void main(String[] args) {
+    try {
+      test();
+    } catch (Exception e) {
+      System.out.println(" caught a " + e.getClass() +
+			 "\n with message: " + e.getMessage());
+    }
+  }
+
+  // FIXME: OG: remove hard-coded file names
+  public static void test()
+       throws Exception {
+
+    File file = new File("words.txt");
+    System.out.println(" reading word file containing " +
+		       file.length() + " bytes");
+
+    Date start = new Date();
+
+    Vector keys = new Vector();
+    FileInputStream ws = new FileInputStream(file);
+    BufferedReader wr = new BufferedReader(new InputStreamReader(ws));
+
+    for (String key = wr.readLine(); key!=null; key = wr.readLine())
+      keys.addElement(new Term("word", key));
+    wr.close();
+
+    Date end = new Date();
+
+    System.out.print(end.getTime() - start.getTime());
+    System.out.println(" milliseconds to read " + keys.size() + " words");
+
+    start = new Date();
+
+    Random gen = new Random(1251971);
+    long fp = (gen.nextInt() & 0xF) + 1;
+    long pp = (gen.nextInt() & 0xF) + 1;
+    int[] docFreqs = new int[keys.size()];
+    long[] freqPointers = new long[keys.size()];
+    long[] proxPointers = new long[keys.size()];
+    for (int i = 0; i < keys.size(); i++) {
+      docFreqs[i] = (gen.nextInt() & 0xF) + 1;
+      freqPointers[i] = fp;
+      proxPointers[i] = pp;
+      fp += (gen.nextInt() & 0xF) + 1;;
+      pp += (gen.nextInt() & 0xF) + 1;;
+    }
+
+    end = new Date();
+
+    System.out.print(end.getTime() - start.getTime());
+    System.out.println(" milliseconds to generate values");
+
+    start = new Date();
+
+    Directory store = FSDirectory.getDirectory("test.store", true);
+    FieldInfos fis = new FieldInfos();
+
+    TermInfosWriter writer = new TermInfosWriter(store, "words", fis);
+    fis.add("word", false);
+
+    for (int i = 0; i < keys.size(); i++)
+      writer.add((Term)keys.elementAt(i),
+		 new TermInfo(docFreqs[i], freqPointers[i], proxPointers[i]));
+
+    writer.close();
+
+    end = new Date();
+
+    System.out.print(end.getTime() - start.getTime());
+    System.out.println(" milliseconds to write table");
+
+    System.out.println(" table occupies " +
+		       store.fileLength("words.tis") + " bytes");
+
+    start = new Date();
+
+    TermInfosReader reader = new TermInfosReader(store, "words", fis);
+
+    end = new Date();
+
+    System.out.print(end.getTime() - start.getTime());
+    System.out.println(" milliseconds to open table");
+
+    start = new Date();
+
+    SegmentTermEnum enumerator = reader.terms();
+    for (int i = 0; i < keys.size(); i++) {
+      enumerator.next();
+      Term key = (Term)keys.elementAt(i);
+      if (!key.equals(enumerator.term()))
+	throw new Exception("wrong term: " + enumerator.term()
+			    + ", expected: " + key
+			    + " at " + i);
+      TermInfo ti = enumerator.termInfo();
+      if (ti.docFreq != docFreqs[i])
+	throw
+	  new Exception("wrong value: " + Long.toString(ti.docFreq, 16)
+			+ ", expected: " + Long.toString(docFreqs[i], 16)
+			+ " at " + i);
+      if (ti.freqPointer != freqPointers[i])
+	throw
+	  new Exception("wrong value: " + Long.toString(ti.freqPointer, 16)
+			+ ", expected: " + Long.toString(freqPointers[i], 16)
+			+ " at " + i);
+      if (ti.proxPointer != proxPointers[i])
+	throw
+	  new Exception("wrong value: " + Long.toString(ti.proxPointer, 16)
+			+ ", expected: " + Long.toString(proxPointers[i], 16)
+			+ " at " + i);
+    }
+
+    end = new Date();
+
+    System.out.print(end.getTime() - start.getTime());
+    System.out.println(" milliseconds to iterate over " +
+		       keys.size() + " words");
+
+    start = new Date();
+
+    for (int i = 0; i < keys.size(); i++) {
+      Term key = (Term)keys.elementAt(i);
+      TermInfo ti = reader.get(key);
+      if (ti.docFreq != docFreqs[i])
+	throw
+	  new Exception("wrong value: " + Long.toString(ti.docFreq, 16)
+			+ ", expected: " + Long.toString(docFreqs[i], 16)
+			+ " at " + i);
+      if (ti.freqPointer != freqPointers[i])
+	throw
+	  new Exception("wrong value: " + Long.toString(ti.freqPointer, 16)
+			+ ", expected: " + Long.toString(freqPointers[i], 16)
+			+ " at " + i);
+      if (ti.proxPointer != proxPointers[i])
+	throw
+	  new Exception("wrong value: " + Long.toString(ti.proxPointer, 16)
+			+ ", expected: " + Long.toString(proxPointers[i], 16)
+			+ " at " + i);
+    }
+
+    end = new Date();
+
+    System.out.print((end.getTime() - start.getTime()) / (float)keys.size());
+    System.out.println(" average milliseconds per lookup");
+
+    TermEnum e = reader.terms(new Term("word", "azz"));
+    System.out.println("Word after azz is " + e.term().text);
+
+    reader.close();
+
+    store.close();
+  }
+}
diff --git a/src/test-deprecated/org/apache/lucene/index/TestFieldsReader.java b/src/test-deprecated/org/apache/lucene/index/TestFieldsReader.java
new file mode 100644
index 0000000..cea9d94
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/index/TestFieldsReader.java
@@ -0,0 +1,77 @@
+package org.apache.lucene.index;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import junit.framework.TestCase;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.search.Similarity;
+
+import java.util.Map;
+import java.io.IOException;
+
+public class TestFieldsReader extends TestCase {
+  private RAMDirectory dir = new RAMDirectory();
+  private Document testDoc = new Document();
+  private FieldInfos fieldInfos = null;
+
+  public TestFieldsReader(String s) {
+    super(s);
+  }
+
+  protected void setUp() {
+    fieldInfos = new FieldInfos();
+    DocHelper.setupDoc(testDoc);
+    fieldInfos.add(testDoc);
+    DocumentWriter writer = new DocumentWriter(dir, new WhitespaceAnalyzer(),
+            Similarity.getDefault(), 50);
+    assertTrue(writer != null);
+    try {
+      writer.addDocument("test", testDoc);
+    }
+    catch (IOException e)
+    {
+      
+    }
+  }
+
+  protected void tearDown() {
+
+  }
+
+  public void test() {
+    assertTrue(dir != null);
+    assertTrue(fieldInfos != null);
+    try {
+      FieldsReader reader = new FieldsReader(dir, "test", fieldInfos);
+      assertTrue(reader != null);
+      assertTrue(reader.size() == 1);
+      Document doc = reader.doc(0);
+      assertTrue(doc != null);
+      assertTrue(doc.getField("textField1") != null);
+      Field field = doc.getField("textField2");
+      assertTrue(field != null);
+      assertTrue(field.isTermVectorStored() == true);
+      reader.close();
+    } catch (IOException e) {
+      e.printStackTrace();
+      assertTrue(false);
+    }
+  }
+}
diff --git a/src/test-deprecated/org/apache/lucene/index/TestFilterIndexReader.java b/src/test-deprecated/org/apache/lucene/index/TestFilterIndexReader.java
new file mode 100644
index 0000000..aff30ce
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/index/TestFilterIndexReader.java
@@ -0,0 +1,137 @@
+package org.apache.lucene.index;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+import junit.framework.TestCase;
+import junit.framework.TestSuite;
+import junit.textui.TestRunner;
+import junit.framework.TestResult;
+
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Searcher;
+import org.apache.lucene.search.Hits;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.store.FSDirectory;
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+
+import java.util.Collection;
+import java.io.IOException;
+
+public class TestFilterIndexReader extends TestCase {
+
+  private static class TestReader extends FilterIndexReader {
+
+     /** Filter that only permits terms containing 'e'.*/
+    private static class TestTermEnum extends FilterTermEnum {
+      public TestTermEnum(TermEnum termEnum)
+        throws IOException {
+        super(termEnum);
+      }
+
+      /** Scan for terms containing the letter 'e'.*/
+      public boolean next() throws IOException {
+        while (in.next()) {
+          if (in.term().text().indexOf('e') != -1)
+            return true;
+        }
+        return false;
+      }
+    }
+    
+    /** Filter that only returns odd numbered documents. */
+    private static class TestTermPositions extends FilterTermPositions {
+      public TestTermPositions(TermPositions in)
+        throws IOException {
+        super(in);
+      }
+
+      /** Scan for odd numbered documents. */
+      public boolean next() throws IOException {
+        while (in.next()) {
+          if ((in.doc() % 2) == 1)
+            return true;
+        }
+        return false;
+      }
+    }
+    
+    public TestReader(IndexReader reader) {
+      super(reader);
+    }
+
+    /** Filter terms with TestTermEnum. */
+    public TermEnum terms() throws IOException {
+      return new TestTermEnum(in.terms());
+    }
+
+    /** Filter positions with TestTermPositions. */
+    public TermPositions termPositions() throws IOException {
+      return new TestTermPositions(in.termPositions());
+    }
+  }
+
+
+  /** Main for running test case by itself. */
+  public static void main(String args[]) {
+    TestRunner.run (new TestSuite(TestIndexReader.class));
+  }
+    
+  /**
+   * Tests the IndexReader.getFieldNames implementation
+   * @throws Exception on error
+   */
+  public void testFilterIndexReader() throws Exception {
+    RAMDirectory directory = new RAMDirectory();
+    IndexWriter writer =
+      new IndexWriter(directory, new WhitespaceAnalyzer(), true);
+
+    Document d1 = new Document();
+    d1.add(Field.Text("default","one two"));
+    writer.addDocument(d1);
+
+    Document d2 = new Document();
+    d2.add(Field.Text("default","one three"));
+    writer.addDocument(d2);
+
+    Document d3 = new Document();
+    d3.add(Field.Text("default","two four"));
+    writer.addDocument(d3);
+
+    writer.close();
+
+    IndexReader reader = new TestReader(IndexReader.open(directory));
+
+    TermEnum terms = reader.terms();
+    while (terms.next()) {
+      assertTrue(terms.term().text().indexOf('e') != -1);
+    }
+    terms.close();
+    
+    TermPositions positions = reader.termPositions(new Term("default", "one"));
+    while (positions.next()) {
+      assertTrue((positions.doc() % 2) == 1);
+    }
+
+    reader.close();
+  }
+}
diff --git a/src/test-deprecated/org/apache/lucene/index/TestIndexReader.java b/src/test-deprecated/org/apache/lucene/index/TestIndexReader.java
new file mode 100644
index 0000000..5678cef
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/index/TestIndexReader.java
@@ -0,0 +1,448 @@
+package org.apache.lucene.index;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+import junit.framework.TestCase;
+import junit.framework.TestSuite;
+import junit.textui.TestRunner;
+
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.store.FSDirectory;
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+
+import java.util.Collection;
+import java.io.IOException;
+import java.io.File;
+
+public class TestIndexReader extends TestCase
+{
+    /** Main for running test case by itself. */
+    public static void main(String args[]) {
+        TestRunner.run (new TestSuite(TestIndexReader.class));
+//        TestRunner.run (new TestIndexReader("testBasicDelete"));
+//        TestRunner.run (new TestIndexReader("testDeleteReaderWriterConflict"));
+//        TestRunner.run (new TestIndexReader("testDeleteReaderReaderConflict"));
+//        TestRunner.run (new TestIndexReader("testFilesOpenClose"));
+    }
+
+    public TestIndexReader(String name) {
+        super(name);
+    }
+
+
+    /**
+     * Tests the IndexReader.getFieldNames implementation
+     * @throws Exception on error
+     */
+    public void testGetFieldNames() throws Exception
+    {
+        RAMDirectory d = new RAMDirectory();
+        // set up writer
+        IndexWriter writer = new IndexWriter(d, new StandardAnalyzer(), true);
+        addDocumentWithFields(writer);
+        writer.close();
+        // set up reader
+        IndexReader reader = IndexReader.open(d);
+        Collection fieldNames = reader.getFieldNames();
+        assertTrue(fieldNames.contains("keyword"));
+        assertTrue(fieldNames.contains("text"));
+        assertTrue(fieldNames.contains("unindexed"));
+        assertTrue(fieldNames.contains("unstored"));
+        // add more documents
+        writer = new IndexWriter(d, new StandardAnalyzer(), false);
+        // want to get some more segments here
+        for (int i = 0; i < 5*writer.mergeFactor; i++)
+        {
+            addDocumentWithFields(writer);
+        }
+        // new fields are in some different segments (we hope)
+        for (int i = 0; i < 5*writer.mergeFactor; i++)
+        {
+            addDocumentWithDifferentFields(writer);
+        }
+        writer.close();
+        // verify fields again
+        reader = IndexReader.open(d);
+        fieldNames = reader.getFieldNames();
+        assertEquals(9, fieldNames.size());    // the following fields + an empty one (bug?!)
+        assertTrue(fieldNames.contains("keyword"));
+        assertTrue(fieldNames.contains("text"));
+        assertTrue(fieldNames.contains("unindexed"));
+        assertTrue(fieldNames.contains("unstored"));
+        assertTrue(fieldNames.contains("keyword2"));
+        assertTrue(fieldNames.contains("text2"));
+        assertTrue(fieldNames.contains("unindexed2"));
+        assertTrue(fieldNames.contains("unstored2"));
+
+        // verify that only indexed fields were returned
+        Collection indexedFieldNames = reader.getFieldNames(true);
+        assertEquals(6, indexedFieldNames.size());
+        assertTrue(indexedFieldNames.contains("keyword"));
+        assertTrue(indexedFieldNames.contains("text"));
+        assertTrue(indexedFieldNames.contains("unstored"));
+        assertTrue(indexedFieldNames.contains("keyword2"));
+        assertTrue(indexedFieldNames.contains("text2"));
+        assertTrue(indexedFieldNames.contains("unstored2"));
+
+        // verify that only unindexed fields were returned
+        Collection unindexedFieldNames = reader.getFieldNames(false);
+        assertEquals(3, unindexedFieldNames.size());    // the following fields + an empty one
+        assertTrue(unindexedFieldNames.contains("unindexed"));
+        assertTrue(unindexedFieldNames.contains("unindexed2"));
+    }
+
+
+    private void assertTermDocsCount(String msg,
+                                     IndexReader reader,
+                                     Term term,
+                                     int expected)
+    throws IOException
+    {
+        TermDocs tdocs = null;
+
+        try {
+            tdocs = reader.termDocs(term);
+            assertNotNull(msg + ", null TermDocs", tdocs);
+            int count = 0;
+            while(tdocs.next()) {
+                count++;
+            }
+            assertEquals(msg + ", count mismatch", expected, count);
+
+        } finally {
+            if (tdocs != null)
+                try { tdocs.close(); } catch (Exception e) { }
+        }
+
+    }
+
+
+
+    public void testBasicDelete() throws IOException
+    {
+        Directory dir = new RAMDirectory();
+
+        IndexWriter writer = null;
+        IndexReader reader = null;
+        Term searchTerm = new Term("content", "aaa");
+
+        //  add 100 documents with term : aaa
+        writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true);
+        for (int i = 0; i < 100; i++)
+        {
+            addDoc(writer, searchTerm.text());
+        }
+        writer.close();
+
+        // OPEN READER AT THIS POINT - this should fix the view of the
+        // index at the point of having 100 "aaa" documents and 0 "bbb"
+        reader = IndexReader.open(dir);
+        assertEquals("first docFreq", 100, reader.docFreq(searchTerm));
+        assertTermDocsCount("first reader", reader, searchTerm, 100);
+
+        // DELETE DOCUMENTS CONTAINING TERM: aaa
+        int deleted = 0;
+        reader = IndexReader.open(dir);
+        deleted = reader.delete(searchTerm);
+        assertEquals("deleted count", 100, deleted);
+        assertEquals("deleted docFreq", 100, reader.docFreq(searchTerm));
+        assertTermDocsCount("deleted termDocs", reader, searchTerm, 0);
+        reader.close();
+
+        // CREATE A NEW READER and re-test
+        reader = IndexReader.open(dir);
+        assertEquals("deleted docFreq", 100, reader.docFreq(searchTerm));
+        assertTermDocsCount("deleted termDocs", reader, searchTerm, 0);
+        reader.close();
+    }
+
+
+    public void testDeleteReaderWriterConflictUnoptimized() throws IOException{
+      deleteReaderWriterConflict(false);
+    }
+    
+    public void testDeleteReaderWriterConflictOptimized() throws IOException{
+        deleteReaderWriterConflict(true);
+    }
+
+    private void deleteReaderWriterConflict(boolean optimize) throws IOException
+    {
+        //Directory dir = new RAMDirectory();
+        Directory dir = getDirectory(true);
+
+        Term searchTerm = new Term("content", "aaa");
+        Term searchTerm2 = new Term("content", "bbb");
+
+        //  add 100 documents with term : aaa
+        IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true);
+        for (int i = 0; i < 100; i++)
+        {
+            addDoc(writer, searchTerm.text());
+        }
+        writer.close();
+
+        // OPEN READER AT THIS POINT - this should fix the view of the
+        // index at the point of having 100 "aaa" documents and 0 "bbb"
+        IndexReader reader = IndexReader.open(dir);
+        assertEquals("first docFreq", 100, reader.docFreq(searchTerm));
+        assertEquals("first docFreq", 0, reader.docFreq(searchTerm2));
+        assertTermDocsCount("first reader", reader, searchTerm, 100);
+        assertTermDocsCount("first reader", reader, searchTerm2, 0);
+
+        // add 100 documents with term : bbb
+        writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), false);
+        for (int i = 0; i < 100; i++)
+        {
+            addDoc(writer, searchTerm2.text());
+        }
+
+        // REQUEST OPTIMIZATION
+        // This causes a new segment to become current for all subsequent
+        // searchers. Because of this, deletions made via a previously open
+        // reader, which would be applied to that reader's segment, are lost
+        // for subsequent searchers/readers
+        if(optimize)
+          writer.optimize();
+        writer.close();
+
+        // The reader should not see the new data
+        assertEquals("first docFreq", 100, reader.docFreq(searchTerm));
+        assertEquals("first docFreq", 0, reader.docFreq(searchTerm2));
+        assertTermDocsCount("first reader", reader, searchTerm, 100);
+        assertTermDocsCount("first reader", reader, searchTerm2, 0);
+
+
+        // DELETE DOCUMENTS CONTAINING TERM: aaa
+        // NOTE: the reader was created when only "aaa" documents were in
+        int deleted = 0;
+        try {
+            deleted = reader.delete(searchTerm);
+            fail("Delete allowed on an index reader with stale segment information");
+        } catch (IOException e) {
+            /* success */
+        }
+
+        // Re-open index reader and try again. This time it should see
+        // the new data.
+        reader.close();
+        reader = IndexReader.open(dir);
+        assertEquals("first docFreq", 100, reader.docFreq(searchTerm));
+        assertEquals("first docFreq", 100, reader.docFreq(searchTerm2));
+        assertTermDocsCount("first reader", reader, searchTerm, 100);
+        assertTermDocsCount("first reader", reader, searchTerm2, 100);
+
+        deleted = reader.delete(searchTerm);
+        assertEquals("deleted count", 100, deleted);
+        assertEquals("deleted docFreq", 100, reader.docFreq(searchTerm));
+        assertEquals("deleted docFreq", 100, reader.docFreq(searchTerm2));
+        assertTermDocsCount("deleted termDocs", reader, searchTerm, 0);
+        assertTermDocsCount("deleted termDocs", reader, searchTerm2, 100);
+        reader.close();
+
+        // CREATE A NEW READER and re-test
+        reader = IndexReader.open(dir);
+        assertEquals("deleted docFreq", 100, reader.docFreq(searchTerm));
+        assertEquals("deleted docFreq", 100, reader.docFreq(searchTerm2));
+        assertTermDocsCount("deleted termDocs", reader, searchTerm, 0);
+        assertTermDocsCount("deleted termDocs", reader, searchTerm2, 100);
+        reader.close();
+    }
+
+  private Directory getDirectory(boolean create) throws IOException {
+    return FSDirectory.getDirectory(new File(System.getProperty("tempDir"), "testIndex"), create);
+  }
+
+  public void testFilesOpenClose() throws IOException
+    {
+        // Create initial data set
+        Directory dir = getDirectory(true);
+        IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true);
+        addDoc(writer, "test");
+        writer.close();
+        dir.close();
+
+        // Try to erase the data - this ensures that the writer closed all files
+        dir = getDirectory(true);
+
+        // Now create the data set again, just as before
+        writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true);
+        addDoc(writer, "test");
+        writer.close();
+        dir.close();
+
+        // Now open existing directory and test that reader closes all files
+        dir = getDirectory(false);
+        IndexReader reader1 = IndexReader.open(dir);
+        reader1.close();
+        dir.close();
+
+        // The following will fail if reader did not close all files
+        dir = getDirectory(true);
+    }
+
+    public void testDeleteReaderReaderConflictUnoptimized() throws IOException{
+      deleteReaderReaderConflict(false);
+    }
+    
+    public void testDeleteReaderReaderConflictOptimized() throws IOException{
+      deleteReaderReaderConflict(true);
+    }
+    
+    private void deleteReaderReaderConflict(boolean optimize) throws IOException
+    {
+        Directory dir = getDirectory(true);
+
+        Term searchTerm1 = new Term("content", "aaa");
+        Term searchTerm2 = new Term("content", "bbb");
+        Term searchTerm3 = new Term("content", "ccc");
+
+        //  add 100 documents with term : aaa
+        //  add 100 documents with term : bbb
+        //  add 100 documents with term : ccc
+        IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true);
+        for (int i = 0; i < 100; i++)
+        {
+            addDoc(writer, searchTerm1.text());
+            addDoc(writer, searchTerm2.text());
+            addDoc(writer, searchTerm3.text());
+        }
+        if(optimize)
+          writer.optimize();
+        writer.close();
+
+        // OPEN TWO READERS
+        // Both readers get segment info as exists at this time
+        IndexReader reader1 = IndexReader.open(dir);
+        assertEquals("first opened", 100, reader1.docFreq(searchTerm1));
+        assertEquals("first opened", 100, reader1.docFreq(searchTerm2));
+        assertEquals("first opened", 100, reader1.docFreq(searchTerm3));
+        assertTermDocsCount("first opened", reader1, searchTerm1, 100);
+        assertTermDocsCount("first opened", reader1, searchTerm2, 100);
+        assertTermDocsCount("first opened", reader1, searchTerm3, 100);
+
+        IndexReader reader2 = IndexReader.open(dir);
+        assertEquals("first opened", 100, reader2.docFreq(searchTerm1));
+        assertEquals("first opened", 100, reader2.docFreq(searchTerm2));
+        assertEquals("first opened", 100, reader2.docFreq(searchTerm3));
+        assertTermDocsCount("first opened", reader2, searchTerm1, 100);
+        assertTermDocsCount("first opened", reader2, searchTerm2, 100);
+        assertTermDocsCount("first opened", reader2, searchTerm3, 100);
+
+        // DELETE DOCS FROM READER 2 and CLOSE IT
+        // delete documents containing term: aaa
+        // when the reader is closed, the segment info is updated and
+        // the first reader is now stale
+        reader2.delete(searchTerm1);
+        assertEquals("after delete 1", 100, reader2.docFreq(searchTerm1));
+        assertEquals("after delete 1", 100, reader2.docFreq(searchTerm2));
+        assertEquals("after delete 1", 100, reader2.docFreq(searchTerm3));
+        assertTermDocsCount("after delete 1", reader2, searchTerm1, 0);
+        assertTermDocsCount("after delete 1", reader2, searchTerm2, 100);
+        assertTermDocsCount("after delete 1", reader2, searchTerm3, 100);
+        reader2.close();
+
+        // Make sure reader 1 is unchanged since it was open earlier
+        assertEquals("after delete 1", 100, reader1.docFreq(searchTerm1));
+        assertEquals("after delete 1", 100, reader1.docFreq(searchTerm2));
+        assertEquals("after delete 1", 100, reader1.docFreq(searchTerm3));
+        assertTermDocsCount("after delete 1", reader1, searchTerm1, 100);
+        assertTermDocsCount("after delete 1", reader1, searchTerm2, 100);
+        assertTermDocsCount("after delete 1", reader1, searchTerm3, 100);
+
+
+        // ATTEMPT TO DELETE FROM STALE READER
+        // delete documents containing term: bbb
+        try {
+            reader1.delete(searchTerm2);
+            fail("Delete allowed from a stale index reader");
+        } catch (IOException e) {
+            /* success */
+        }
+
+        // RECREATE READER AND TRY AGAIN
+        reader1.close();
+        reader1 = IndexReader.open(dir);
+        assertEquals("reopened", 100, reader1.docFreq(searchTerm1));
+        assertEquals("reopened", 100, reader1.docFreq(searchTerm2));
+        assertEquals("reopened", 100, reader1.docFreq(searchTerm3));
+        assertTermDocsCount("reopened", reader1, searchTerm1, 0);
+        assertTermDocsCount("reopened", reader1, searchTerm2, 100);
+        assertTermDocsCount("reopened", reader1, searchTerm3, 100);
+
+        reader1.delete(searchTerm2);
+        assertEquals("deleted 2", 100, reader1.docFreq(searchTerm1));
+        assertEquals("deleted 2", 100, reader1.docFreq(searchTerm2));
+        assertEquals("deleted 2", 100, reader1.docFreq(searchTerm3));
+        assertTermDocsCount("deleted 2", reader1, searchTerm1, 0);
+        assertTermDocsCount("deleted 2", reader1, searchTerm2, 0);
+        assertTermDocsCount("deleted 2", reader1, searchTerm3, 100);
+        reader1.close();
+
+        // Open another reader to confirm that everything is deleted
+        reader2 = IndexReader.open(dir);
+        assertEquals("reopened 2", 100, reader2.docFreq(searchTerm1));
+        assertEquals("reopened 2", 100, reader2.docFreq(searchTerm2));
+        assertEquals("reopened 2", 100, reader2.docFreq(searchTerm3));
+        assertTermDocsCount("reopened 2", reader2, searchTerm1, 0);
+        assertTermDocsCount("reopened 2", reader2, searchTerm2, 0);
+        assertTermDocsCount("reopened 2", reader2, searchTerm3, 100);
+        reader2.close();
+
+        dir.close();
+    }
+
+
+    private void addDocumentWithFields(IndexWriter writer) throws IOException
+    {
+        Document doc = new Document();
+        doc.add(Field.Keyword("keyword","test1"));
+        doc.add(Field.Text("text","test1"));
+        doc.add(Field.UnIndexed("unindexed","test1"));
+        doc.add(Field.UnStored("unstored","test1"));
+        writer.addDocument(doc);
+    }
+
+    private void addDocumentWithDifferentFields(IndexWriter writer) throws IOException
+    {
+        Document doc = new Document();
+        doc.add(Field.Keyword("keyword2","test1"));
+        doc.add(Field.Text("text2","test1"));
+        doc.add(Field.UnIndexed("unindexed2","test1"));
+        doc.add(Field.UnStored("unstored2","test1"));
+        writer.addDocument(doc);
+    }
+
+    private void addDoc(IndexWriter writer, String value)
+    {
+        Document doc = new Document();
+        doc.add(Field.UnStored("content", value));
+
+        try
+        {
+            writer.addDocument(doc);
+        }
+        catch (IOException e)
+        {
+            e.printStackTrace();
+        }
+    }
+}
diff --git a/src/test-deprecated/org/apache/lucene/index/TestIndexWriter.java b/src/test-deprecated/org/apache/lucene/index/TestIndexWriter.java
new file mode 100644
index 0000000..7b65990
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/index/TestIndexWriter.java
@@ -0,0 +1,86 @@
+package org.apache.lucene.index;
+
+import java.io.IOException;
+
+import junit.framework.TestCase;
+
+import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.RAMDirectory;
+
+
+/**
+ * @author goller
+ * @version $Id$
+ */
+public class TestIndexWriter extends TestCase
+{
+    public void testDocCount()
+    {
+        Directory dir = new RAMDirectory();
+
+        IndexWriter writer = null;
+        IndexReader reader = null;
+        int i;
+
+        try {
+            writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true);
+
+            // add 100 documents
+            for (i = 0; i < 100; i++) {
+                addDoc(writer);
+            }
+            assertEquals(100, writer.docCount());
+            writer.close();
+
+            // delete 40 documents
+            reader = IndexReader.open(dir);
+            for (i = 0; i < 40; i++) {
+                reader.delete(i);
+            }
+            reader.close();
+
+            // test doc count before segments are merged/index is optimized
+            writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false);
+            assertEquals(100, writer.docCount());
+            writer.close();
+
+            reader = IndexReader.open(dir);
+            assertEquals(100, reader.maxDoc());
+            assertEquals(60, reader.numDocs());
+            reader.close();
+
+            // optimize the index and check that the new doc count is correct
+            writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false);
+            writer.optimize();
+            assertEquals(60, writer.docCount());
+            writer.close();
+
+            // check that the index reader gives the same numbers.
+            reader = IndexReader.open(dir);
+            assertEquals(60, reader.maxDoc());
+            assertEquals(60, reader.numDocs());
+            reader.close();
+        }
+        catch (IOException e) {
+            e.printStackTrace();
+        }
+    }
+
+    private void addDoc(IndexWriter writer)
+    {
+        Document doc = new Document();
+        doc.add(Field.UnStored("content", "aaa"));
+
+        try {
+            writer.addDocument(doc);
+        }
+        catch (IOException e) {
+            e.printStackTrace();
+        }
+    }
+}
diff --git a/src/test-deprecated/org/apache/lucene/index/TestInputStream.java b/src/test-deprecated/org/apache/lucene/index/TestInputStream.java
new file mode 100644
index 0000000..9fa6a94
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/index/TestInputStream.java
@@ -0,0 +1,37 @@
+package org.apache.lucene.index;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import junit.framework.TestCase;
+import org.apache.lucene.store.InputStream;
+
+import java.io.IOException;
+
+public class TestInputStream extends TestCase {
+    public void testRead() throws IOException {
+        InputStream is = new MockInputStream(new byte[] { (byte) 0x80, 0x01,
+                                                          (byte) 0xFF, 0x7F,
+                                                          (byte) 0x80, (byte) 0x80, 0x01,
+                                                          (byte) 0x81, (byte) 0x80, 0x01,
+                                                          0x06, 'L', 'u', 'c', 'e', 'n', 'e'});
+        assertEquals(128,is.readVInt());
+        assertEquals(16383,is.readVInt());
+        assertEquals(16384,is.readVInt());
+        assertEquals(16385,is.readVInt());
+        assertEquals("Lucene",is.readString());
+    }
+}
diff --git a/src/test-deprecated/org/apache/lucene/index/TestSegmentTermEnum.java b/src/test-deprecated/org/apache/lucene/index/TestSegmentTermEnum.java
new file mode 100644
index 0000000..1985d3b
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/index/TestSegmentTermEnum.java
@@ -0,0 +1,110 @@
+package org.apache.lucene.index;
+
+import java.io.IOException;
+
+import junit.framework.TestCase;
+
+import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.TermEnum;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.RAMDirectory;
+
+/**
+ * @author goller
+ */
+public class TestSegmentTermEnum extends TestCase
+{
+  Directory dir = new RAMDirectory();
+
+  public void testTermEnum()
+  {
+    IndexWriter writer = null;
+
+    try {
+      writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true);
+
+      // add 100 documents with term : aaa
+      // add 100 documents with terms: aaa bbb
+      // Therefore, term 'aaa' has document frequency of 200 and term 'bbb' 100
+      for (int i = 0; i < 100; i++) {
+        addDoc(writer, "aaa");
+        addDoc(writer, "aaa bbb");
+      }
+
+      writer.close();
+    }
+    catch (IOException e) {
+      e.printStackTrace();
+    }
+
+    try {
+      // verify document frequency of terms in an unoptimized index
+      verifyDocFreq();
+
+      // merge segments by optimizing the index
+      writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false);
+      writer.optimize();
+      writer.close();
+
+      // verify document frequency of terms in an optimized index
+      verifyDocFreq();
+    }
+    catch (IOException e2) {
+      e2.printStackTrace();
+    }
+  }
+
+  private void verifyDocFreq()
+      throws IOException
+  {
+      IndexReader reader = IndexReader.open(dir);
+      TermEnum termEnum = null;
+
+    // create enumeration of all terms
+    termEnum = reader.terms();
+    // go to the first term (aaa)
+    termEnum.next();
+    // assert that term is 'aaa'
+    assertEquals("aaa", termEnum.term().text());
+    assertEquals(200, termEnum.docFreq());
+    // go to the second term (bbb)
+    termEnum.next();
+    // assert that term is 'bbb'
+    assertEquals("bbb", termEnum.term().text());
+    assertEquals(100, termEnum.docFreq());
+
+    termEnum.close();
+
+
+    // create enumeration of terms after term 'aaa', including 'aaa'
+    termEnum = reader.terms(new Term("content", "aaa"));
+    // assert that term is 'aaa'
+    assertEquals("aaa", termEnum.term().text());
+    assertEquals(200, termEnum.docFreq());
+    // go to term 'bbb'
+    termEnum.next();
+    // assert that term is 'bbb'
+    assertEquals("bbb", termEnum.term().text());
+    assertEquals(100, termEnum.docFreq());
+
+    termEnum.close();
+  }
+
+  private void addDoc(IndexWriter writer, String value)
+  {
+    Document doc = new Document();
+    doc.add(Field.UnStored("content", value));
+
+    try {
+      writer.addDocument(doc);
+    }
+    catch (IOException e) {
+      e.printStackTrace();
+    }
+  }
+}
diff --git a/src/test-deprecated/org/apache/lucene/index/TestTermVectorsReader.java b/src/test-deprecated/org/apache/lucene/index/TestTermVectorsReader.java
new file mode 100644
index 0000000..c14a089
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/index/TestTermVectorsReader.java
@@ -0,0 +1,106 @@
+package org.apache.lucene.index;
+
+
+import junit.framework.TestCase;
+import org.apache.lucene.store.RAMDirectory;
+
+import java.io.IOException;
+import java.util.Arrays;
+
+public class TestTermVectorsReader extends TestCase {
+  private TermVectorsWriter writer = null;
+  //Must be lexicographically sorted, will do in setup, versus trying to maintain here
+  private String [] testFields = {"f1", "f2", "f3"};
+  private String [] testTerms = {"this", "is", "a", "test"};
+  private RAMDirectory dir = new RAMDirectory();
+  private String seg = "testSegment";
+  private FieldInfos fieldInfos = new FieldInfos();
+
+  public TestTermVectorsReader(String s) {
+    super(s);
+  }
+
+  protected void setUp() {
+    for (int i = 0; i < testFields.length; i++) {
+      fieldInfos.add(testFields[i], true, true);
+    }
+    
+    try {
+      Arrays.sort(testTerms);
+      for (int j = 0; j < 5; j++) {
+        writer = new TermVectorsWriter(dir, seg, fieldInfos);
+        writer.openDocument();
+
+        for (int k = 0; k < testFields.length; k++) {
+          writer.openField(testFields[k]);
+          for (int i = 0; i < testTerms.length; i++) {
+            writer.addTerm(testTerms[i], i);      
+          }
+          writer.closeField();
+        }
+        writer.closeDocument();
+        writer.close();
+      }
+
+    } catch (IOException e) {
+      e.printStackTrace();
+      assertTrue(false);
+    }    
+  }
+
+  protected void tearDown() {
+
+  }
+
+  public void test() {
+      //Check to see the files were created properly in setup
+      assertTrue(writer.isDocumentOpen() == false);          
+      assertTrue(dir.fileExists(seg + TermVectorsWriter.TVD_EXTENSION));
+      assertTrue(dir.fileExists(seg + TermVectorsWriter.TVX_EXTENSION));
+  }
+  
+  public void testReader() {
+    try {
+      TermVectorsReader reader = new TermVectorsReader(dir, seg, fieldInfos);
+      assertTrue(reader != null);
+      TermFreqVector vector = reader.get(0, testFields[0]);
+      assertTrue(vector != null);
+      String [] terms = vector.getTerms();
+      assertTrue(terms != null);
+      assertTrue(terms.length == testTerms.length);
+      for (int i = 0; i < terms.length; i++) {
+        String term = terms[i];
+        //System.out.println("Term: " + term);
+        assertTrue(term.equals(testTerms[i]));
+      }
+      
+    } catch (IOException e) {
+      e.printStackTrace();
+      assertTrue(false);
+    }
+  }  
+
+  /**
+   * Make sure exceptions and bad params are handled appropriately
+   */ 
+  public void testBadParams() {
+    try {
+      TermVectorsReader reader = new TermVectorsReader(dir, seg, fieldInfos);
+      assertTrue(reader != null);
+      //Bad document number, good field number
+      TermFreqVector vector = reader.get(50, testFields[0]);
+      assertTrue(false);      
+    } catch (IOException e) {
+      assertTrue(true);
+    }
+    try {
+      TermVectorsReader reader = new TermVectorsReader(dir, seg, fieldInfos);
+      assertTrue(reader != null);
+      //good document number, bad field number
+      TermFreqVector vector = reader.get(0, "f50");
+      assertTrue(vector == null);      
+    } catch (IOException e) {
+      assertTrue(false);
+    }
+  }    
+}
diff --git a/src/test-deprecated/org/apache/lucene/index/TestTermVectorsWriter.java b/src/test-deprecated/org/apache/lucene/index/TestTermVectorsWriter.java
new file mode 100644
index 0000000..2318427
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/index/TestTermVectorsWriter.java
@@ -0,0 +1,202 @@
+package org.apache.lucene.index;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import junit.framework.TestCase;
+import org.apache.lucene.store.RAMDirectory;
+
+import java.io.IOException;
+
+public class TestTermVectorsWriter extends TestCase {
+
+  private String[] testTerms = {"this", "is", "a", "test"};
+  private String [] testFields = {"f1", "f2", "f3"};
+  private int[][] positions = new int[testTerms.length][];
+  private RAMDirectory dir = new RAMDirectory();
+  private String seg = "testSegment";
+  private FieldInfos fieldInfos = new FieldInfos();
+
+  public TestTermVectorsWriter(String s) {
+    super(s);
+  }
+
+  protected void setUp() {
+
+    for (int i = 0; i < testFields.length; i++) {
+      fieldInfos.add(testFields[i], true, true);
+    }
+    
+
+    for (int i = 0; i < testTerms.length; i++) {
+      positions[i] = new int[5];
+      for (int j = 0; j < positions[i].length; j++) {
+        positions[i][j] = i * 100;
+      }
+    }
+  }
+
+  protected void tearDown() {
+  }
+
+  public void test() {
+    assertTrue(dir != null);
+    assertTrue(positions != null);
+  }
+  
+  /*public void testWriteNoPositions() {
+    try {
+      TermVectorsWriter writer = new TermVectorsWriter(dir, seg, 50);
+      writer.openDocument();
+      assertTrue(writer.isDocumentOpen() == true);
+      writer.openField(0);
+      assertTrue(writer.isFieldOpen() == true);
+      for (int i = 0; i < testTerms.length; i++) {
+        writer.addTerm(testTerms[i], i);
+      }
+      writer.closeField();
+      
+      writer.closeDocument();
+      writer.close();
+      assertTrue(writer.isDocumentOpen() == false);
+      //Check to see the files were created
+      assertTrue(dir.fileExists(seg + TermVectorsWriter.TVD_EXTENSION));
+      assertTrue(dir.fileExists(seg + TermVectorsWriter.TVX_EXTENSION));
+      //Now read it back in
+      TermVectorsReader reader = new TermVectorsReader(dir, seg);
+      assertTrue(reader != null);
+      checkTermVector(reader, 0, 0);
+    } catch (IOException e) {
+      e.printStackTrace();
+      assertTrue(false);
+    }
+  }  */  
+
+  public void testWriter() {
+    try {
+      TermVectorsWriter writer = new TermVectorsWriter(dir, seg, fieldInfos);
+      writer.openDocument();
+      assertTrue(writer.isDocumentOpen() == true);
+      writeField(writer, testFields[0]);
+      writer.closeDocument();
+      writer.close();
+      assertTrue(writer.isDocumentOpen() == false);
+      //Check to see the files were created
+      assertTrue(dir.fileExists(seg + TermVectorsWriter.TVD_EXTENSION));
+      assertTrue(dir.fileExists(seg + TermVectorsWriter.TVX_EXTENSION));
+      //Now read it back in
+      TermVectorsReader reader = new TermVectorsReader(dir, seg, fieldInfos);
+      assertTrue(reader != null);
+      checkTermVector(reader, 0, testFields[0]);
+    } catch (IOException e) {
+      e.printStackTrace();
+      assertTrue(false);
+    }
+  }
+  private void checkTermVector(TermVectorsReader reader, int docNum, String field) throws IOException {
+    TermFreqVector vector = reader.get(docNum, field);
+    assertTrue(vector != null);
+    String[] terms = vector.getTerms();
+    assertTrue(terms != null);
+    assertTrue(terms.length == testTerms.length);
+    for (int i = 0; i < terms.length; i++) {
+      String term = terms[i];
+      assertTrue(term.equals(testTerms[i]));
+    }
+  }
+
+  /**
+   * Test one document, multiple fields
+   */
+  public void testMultipleFields() {
+    try {
+      TermVectorsWriter writer = new TermVectorsWriter(dir, seg, fieldInfos);
+      writeDocument(writer, testFields.length);
+
+      writer.close();
+
+      assertTrue(writer.isDocumentOpen() == false);
+      //Check to see the files were created
+      assertTrue(dir.fileExists(seg + TermVectorsWriter.TVD_EXTENSION));
+      assertTrue(dir.fileExists(seg + TermVectorsWriter.TVX_EXTENSION));
+      //Now read it back in
+      TermVectorsReader reader = new TermVectorsReader(dir, seg, fieldInfos);
+      assertTrue(reader != null);
+
+      for (int j = 0; j < testFields.length; j++) {
+        checkTermVector(reader, 0, testFields[j]);
+      }
+    } catch (IOException e) {
+      e.printStackTrace();
+      assertTrue(false);
+    }
+  }
+
+  private void writeDocument(TermVectorsWriter writer, int numFields) throws IOException {
+    writer.openDocument();
+    assertTrue(writer.isDocumentOpen() == true);
+
+    for (int j = 0; j < numFields; j++) {
+      writeField(writer, testFields[j]);
+    }
+    writer.closeDocument();
+    assertTrue(writer.isDocumentOpen() == false);
+  }
+
+  /**
+   * 
+   * @param writer The writer to write to
+   * @param j The field number
+   * @throws IOException
+   */
+  private void writeField(TermVectorsWriter writer, String f) throws IOException {
+    writer.openField(f);
+    assertTrue(writer.isFieldOpen() == true);
+    for (int i = 0; i < testTerms.length; i++) {
+      writer.addTerm(testTerms[i], i);
+    }
+    writer.closeField();
+  }
+
+
+  public void testMultipleDocuments() {
+
+    try {
+      TermVectorsWriter writer = new TermVectorsWriter(dir, seg, fieldInfos);
+      assertTrue(writer != null);
+      for (int i = 0; i < 10; i++) {
+        writeDocument(writer, testFields.length);
+      }
+      writer.close();
+    } catch (IOException e) {
+      e.printStackTrace();
+      assertTrue(false);
+    }      
+    //Do some arbitrary tests
+    try {
+      TermVectorsReader reader = new TermVectorsReader(dir, seg, fieldInfos);
+      for (int i = 0; i < 10; i++) {        
+        assertTrue(reader != null);
+        checkTermVector(reader, 5, testFields[0]);
+        checkTermVector(reader, 2, testFields[2]);
+      }
+    } catch (IOException e) {
+      e.printStackTrace();
+      assertTrue(false);
+    }
+  }
+
+}
diff --git a/src/test-deprecated/org/apache/lucene/index/store/FSDirectoryTestCase.java b/src/test-deprecated/org/apache/lucene/index/store/FSDirectoryTestCase.java
new file mode 100644
index 0000000..04572e8
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/index/store/FSDirectoryTestCase.java
@@ -0,0 +1,21 @@
+package org.apache.lucene.index.store;
+
+import junit.framework.TestCase;
+import org.apache.lucene.store.FSDirectory;
+import java.io.IOException;
+
+abstract public class FSDirectoryTestCase extends TestCase {
+  private FSDirectory directory;
+
+  protected final FSDirectory getDirectory() throws IOException {
+    return getDirectory(false);
+  }
+
+  protected final FSDirectory getDirectory(boolean create) throws IOException {
+    if (directory == null) {
+      directory = FSDirectory.getDirectory(System.getProperty("test.index.dir"), create);
+    }
+
+    return directory;
+  }
+}
diff --git a/src/test-deprecated/org/apache/lucene/queryParser/TestQueryParser.java b/src/test-deprecated/org/apache/lucene/queryParser/TestQueryParser.java
new file mode 100644
index 0000000..8332dba
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/queryParser/TestQueryParser.java
@@ -0,0 +1,495 @@
+package org.apache.lucene.queryParser;
+
+/**
+ * Copyright 2002-2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import junit.framework.TestCase;
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.LowerCaseTokenizer;
+import org.apache.lucene.analysis.SimpleAnalyzer;
+import org.apache.lucene.analysis.Token;
+import org.apache.lucene.analysis.TokenFilter;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.document.DateField;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.FuzzyQuery;
+import org.apache.lucene.search.PhraseQuery;
+import org.apache.lucene.search.PrefixQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.RangeQuery;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.WildcardQuery;
+import java.io.IOException;
+import java.io.Reader;
+import java.text.DateFormat;
+import java.util.Calendar;
+
+/**
+ * Tests QueryParser.
+ */
+public class TestQueryParser extends TestCase {
+
+  public static Analyzer qpAnalyzer = new QPTestAnalyzer();
+
+  public static class QPTestFilter extends TokenFilter {
+    /**
+     * Filter which discards the token 'stop' and which expands the
+     * token 'phrase' into 'phrase1 phrase2'
+     */
+    public QPTestFilter(TokenStream in) {
+      super(in);
+    }
+
+    boolean inPhrase = false;
+    int savedStart = 0, savedEnd = 0;
+
+    public Token next() throws IOException {
+      if (inPhrase) {
+        inPhrase = false;
+        return new Token("phrase2", savedStart, savedEnd);
+      } else
+        for (Token token = input.next(); token != null; token = input.next()) {
+          if (token.termText().equals("phrase")) {
+            inPhrase = true;
+            savedStart = token.startOffset();
+            savedEnd = token.endOffset();
+            return new Token("phrase1", savedStart, savedEnd);
+          } else if (!token.termText().equals("stop"))
+            return token;
+        }
+      return null;
+    }
+  }
+
+  public static class QPTestAnalyzer extends Analyzer {
+
+    /** Filters LowerCaseTokenizer with StopFilter. */
+    public final TokenStream tokenStream(String fieldName, Reader reader) {
+      return new QPTestFilter(new LowerCaseTokenizer(reader));
+    }
+  }
+
+  public static class QPTestParser extends QueryParser {
+    public QPTestParser(String f, Analyzer a) {
+      super(f, a);
+    }
+
+    protected Query getFuzzyQuery(String field, String termStr, float minSimilarity) throws ParseException {
+      throw new ParseException("Fuzzy queries not allowed");
+    }
+
+    protected Query getWildcardQuery(String field, String termStr) throws ParseException {
+      throw new ParseException("Wildcard queries not allowed");
+    }
+  }
+
+  private int originalMaxClauses;
+
+  public void setUp() {
+    originalMaxClauses = BooleanQuery.getMaxClauseCount();
+  }
+
+  public QueryParser getParser(Analyzer a) throws Exception {
+    if (a == null)
+      a = new SimpleAnalyzer();
+    QueryParser qp = new QueryParser("field", a);
+    qp.setOperator(QueryParser.DEFAULT_OPERATOR_OR);
+    return qp;
+  }
+
+  public Query getQuery(String query, Analyzer a) throws Exception {
+    return getParser(a).parse(query);
+  }
+
+  public void assertQueryEquals(String query, Analyzer a, String result)
+    throws Exception {
+    Query q = getQuery(query, a);
+    String s = q.toString("field");
+    if (!s.equals(result)) {
+      fail("Query /" + query + "/ yielded /" + s
+           + "/, expecting /" + result + "/");
+    }
+  }
+
+  public void assertWildcardQueryEquals(String query, boolean lowercase, String result)
+    throws Exception {
+    QueryParser qp = getParser(null);
+    qp.setLowercaseWildcardTerms(lowercase);
+    Query q = qp.parse(query);
+    String s = q.toString("field");
+    if (!s.equals(result)) {
+      fail("WildcardQuery /" + query + "/ yielded /" + s
+           + "/, expecting /" + result + "/");
+    }
+  }
+
+  public Query getQueryDOA(String query, Analyzer a)
+    throws Exception {
+    if (a == null)
+      a = new SimpleAnalyzer();
+    QueryParser qp = new QueryParser("field", a);
+    qp.setOperator(QueryParser.DEFAULT_OPERATOR_AND);
+    return qp.parse(query);
+  }
+
+  public void assertQueryEqualsDOA(String query, Analyzer a, String result)
+    throws Exception {
+    Query q = getQueryDOA(query, a);
+    String s = q.toString("field");
+    if (!s.equals(result)) {
+      fail("Query /" + query + "/ yielded /" + s
+           + "/, expecting /" + result + "/");
+    }
+  }
+
+  public void testSimple() throws Exception {
+    assertQueryEquals("term term term", null, "term term term");
+    assertQueryEquals("trm term term", null, "trm term term");
+    assertQueryEquals("mlaut", null, "mlaut");
+
+    assertQueryEquals("a AND b", null, "+a +b");
+    assertQueryEquals("(a AND b)", null, "+a +b");
+    assertQueryEquals("c OR (a AND b)", null, "c (+a +b)");
+    assertQueryEquals("a AND NOT b", null, "+a -b");
+    assertQueryEquals("a AND -b", null, "+a -b");
+    assertQueryEquals("a AND !b", null, "+a -b");
+    assertQueryEquals("a && b", null, "+a +b");
+    assertQueryEquals("a && ! b", null, "+a -b");
+
+    assertQueryEquals("a OR b", null, "a b");
+    assertQueryEquals("a || b", null, "a b");
+    assertQueryEquals("a OR !b", null, "a -b");
+    assertQueryEquals("a OR ! b", null, "a -b");
+    assertQueryEquals("a OR -b", null, "a -b");
+
+    assertQueryEquals("+term -term term", null, "+term -term term");
+    assertQueryEquals("foo:term AND field:anotherTerm", null,
+                      "+foo:term +anotherterm");
+    assertQueryEquals("term AND \"phrase phrase\"", null,
+                      "+term +\"phrase phrase\"");
+    assertQueryEquals("\"hello there\"", null, "\"hello there\"");
+    assertTrue(getQuery("a AND b", null) instanceof BooleanQuery);
+    assertTrue(getQuery("hello", null) instanceof TermQuery);
+    assertTrue(getQuery("\"hello there\"", null) instanceof PhraseQuery);
+
+    assertQueryEquals("germ term^2.0", null, "germ term^2.0");
+    assertQueryEquals("(term)^2.0", null, "term^2.0");
+    assertQueryEquals("(germ term)^2.0", null, "(germ term)^2.0");
+    assertQueryEquals("term^2.0", null, "term^2.0");
+    assertQueryEquals("term^2", null, "term^2.0");
+    assertQueryEquals("\"germ term\"^2.0", null, "\"germ term\"^2.0");
+    assertQueryEquals("\"term germ\"^2", null, "\"term germ\"^2.0");
+
+    assertQueryEquals("(foo OR bar) AND (baz OR boo)", null,
+                      "+(foo bar) +(baz boo)");
+    assertQueryEquals("((a OR b) AND NOT c) OR d", null,
+                      "(+(a b) -c) d");
+    assertQueryEquals("+(apple \"steve jobs\") -(foo bar baz)", null,
+                      "+(apple \"steve jobs\") -(foo bar baz)");
+    assertQueryEquals("+title:(dog OR cat) -author:\"bob dole\"", null,
+                      "+(title:dog title:cat) -author:\"bob dole\"");
+  }
+
+  public void testPunct() throws Exception {
+    Analyzer a = new WhitespaceAnalyzer();
+    assertQueryEquals("a&b", a, "a&b");
+    assertQueryEquals("a&&b", a, "a&&b");
+    assertQueryEquals(".NET", a, ".NET");
+  }
+
+  public void testSlop() throws Exception {
+    assertQueryEquals("\"term germ\"~2", null, "\"term germ\"~2");
+    assertQueryEquals("\"term germ\"~2 flork", null, "\"term germ\"~2 flork");
+    assertQueryEquals("\"term\"~2", null, "term");
+    assertQueryEquals("\" \"~2 germ", null, "germ");
+    assertQueryEquals("\"term germ\"~2^2", null, "\"term germ\"~2^2.0");
+  }
+
+  public void testNumber() throws Exception {
+// The numbers go away because SimpleAnalzyer ignores them
+    assertQueryEquals("3", null, "");
+    assertQueryEquals("term 1.0 1 2", null, "term");
+    assertQueryEquals("term term1 term2", null, "term term term");
+
+    Analyzer a = new StandardAnalyzer();
+    assertQueryEquals("3", a, "3");
+    assertQueryEquals("term 1.0 1 2", a, "term 1.0 1 2");
+    assertQueryEquals("term term1 term2", a, "term term1 term2");
+  }
+
+  public void testWildcard() throws Exception {
+    assertQueryEquals("term*", null, "term*");
+    assertQueryEquals("term*^2", null, "term*^2.0");
+    assertQueryEquals("term~", null, "term~0.5");
+    assertQueryEquals("term~0.7", null, "term~0.7");
+    assertQueryEquals("term~^2", null, "term^2.0~0.5");
+    assertQueryEquals("term^2~", null, "term^2.0~0.5");
+    assertQueryEquals("term*germ", null, "term*germ");
+    assertQueryEquals("term*germ^3", null, "term*germ^3.0");
+
+    assertTrue(getQuery("term*", null) instanceof PrefixQuery);
+    assertTrue(getQuery("term*^2", null) instanceof PrefixQuery);
+    assertTrue(getQuery("term~", null) instanceof FuzzyQuery);
+    assertTrue(getQuery("term~0.7", null) instanceof FuzzyQuery);
+    FuzzyQuery fq = (FuzzyQuery)getQuery("term~0.7", null);
+    assertEquals(0.7f, fq.getMinSimilarity(), 0.1f);
+    assertEquals(0, fq.getPrefixLength());
+    fq = (FuzzyQuery)getQuery("term~", null);
+    assertEquals(0.5f, fq.getMinSimilarity(), 0.1f);
+    assertEquals(0, fq.getPrefixLength());
+    try {
+      getQuery("term~1.1", null);   // value > 1, throws exception
+      fail();
+    } catch(ParseException pe) {
+      // expected exception
+    }
+    assertTrue(getQuery("term*germ", null) instanceof WildcardQuery);
+
+/* Tests to see that wild card terms are (or are not) properly
+	 * lower-cased with propery parser configuration
+	 */
+// First prefix queries:
+    assertWildcardQueryEquals("term*", true, "term*");
+    assertWildcardQueryEquals("Term*", true, "term*");
+    assertWildcardQueryEquals("TERM*", true, "term*");
+    assertWildcardQueryEquals("term*", false, "term*");
+    assertWildcardQueryEquals("Term*", false, "Term*");
+    assertWildcardQueryEquals("TERM*", false, "TERM*");
+// Then 'full' wildcard queries:
+    assertWildcardQueryEquals("te?m", true, "te?m");
+    assertWildcardQueryEquals("Te?m", true, "te?m");
+    assertWildcardQueryEquals("TE?M", true, "te?m");
+    assertWildcardQueryEquals("Te?m*gerM", true, "te?m*germ");
+    assertWildcardQueryEquals("te?m", false, "te?m");
+    assertWildcardQueryEquals("Te?m", false, "Te?m");
+    assertWildcardQueryEquals("TE?M", false, "TE?M");
+    assertWildcardQueryEquals("Te?m*gerM", false, "Te?m*gerM");
+  }
+
+  public void testQPA() throws Exception {
+    assertQueryEquals("term term term", qpAnalyzer, "term term term");
+    assertQueryEquals("term +stop term", qpAnalyzer, "term term");
+    assertQueryEquals("term -stop term", qpAnalyzer, "term term");
+    assertQueryEquals("drop AND stop AND roll", qpAnalyzer, "+drop +roll");
+    assertQueryEquals("term phrase term", qpAnalyzer,
+                      "term \"phrase1 phrase2\" term");
+    assertQueryEquals("term AND NOT phrase term", qpAnalyzer,
+                      "+term -\"phrase1 phrase2\" term");
+    assertQueryEquals("stop", qpAnalyzer, "");
+    assertTrue(getQuery("term term term", qpAnalyzer) instanceof BooleanQuery);
+    assertTrue(getQuery("term +stop", qpAnalyzer) instanceof TermQuery);
+  }
+
+  public void testRange() throws Exception {
+    assertQueryEquals("[ a TO z]", null, "[a TO z]");
+    assertTrue(getQuery("[ a TO z]", null) instanceof RangeQuery);
+    assertQueryEquals("[ a TO z ]", null, "[a TO z]");
+    assertQueryEquals("{ a TO z}", null, "{a TO z}");
+    assertQueryEquals("{ a TO z }", null, "{a TO z}");
+    assertQueryEquals("{ a TO z }^2.0", null, "{a TO z}^2.0");
+    assertQueryEquals("[ a TO z] OR bar", null, "[a TO z] bar");
+    assertQueryEquals("[ a TO z] AND bar", null, "+[a TO z] +bar");
+    assertQueryEquals("( bar blar { a TO z}) ", null, "bar blar {a TO z}");
+    assertQueryEquals("gack ( bar blar { a TO z}) ", null, "gack (bar blar {a TO z})");
+  }
+
+  public String getDate(String s) throws Exception {
+    DateFormat df = DateFormat.getDateInstance(DateFormat.SHORT);
+    return DateField.dateToString(df.parse(s));
+  }
+
+  public String getLocalizedDate(int year, int month, int day) {
+    DateFormat df = DateFormat.getDateInstance(DateFormat.SHORT);
+    Calendar calendar = Calendar.getInstance();
+    calendar.set(year, month, day);
+    return df.format(calendar.getTime());
+  }
+
+  public void testDateRange() throws Exception {
+    String startDate = getLocalizedDate(2002, 1, 1);
+    String endDate = getLocalizedDate(2002, 1, 4);
+    assertQueryEquals("[ " + startDate + " TO " + endDate + "]", null,
+                      "[" + getDate(startDate) + " TO " + getDate(endDate) + "]");
+    assertQueryEquals("{  " + startDate + "    " + endDate + "   }", null,
+                      "{" + getDate(startDate) + " TO " + getDate(endDate) + "}");
+  }
+
+  public void testEscaped() throws Exception {
+    Analyzer a = new WhitespaceAnalyzer();
+    
+    /*assertQueryEquals("\\[brackets", a, "\\[brackets");
+    assertQueryEquals("\\[brackets", null, "brackets");
+    assertQueryEquals("\\\\", a, "\\\\");
+    assertQueryEquals("\\+blah", a, "\\+blah");
+    assertQueryEquals("\\(blah", a, "\\(blah");
+
+    assertQueryEquals("\\-blah", a, "\\-blah");
+    assertQueryEquals("\\!blah", a, "\\!blah");
+    assertQueryEquals("\\{blah", a, "\\{blah");
+    assertQueryEquals("\\}blah", a, "\\}blah");
+    assertQueryEquals("\\:blah", a, "\\:blah");
+    assertQueryEquals("\\^blah", a, "\\^blah");
+    assertQueryEquals("\\[blah", a, "\\[blah");
+    assertQueryEquals("\\]blah", a, "\\]blah");
+    assertQueryEquals("\\\"blah", a, "\\\"blah");
+    assertQueryEquals("\\(blah", a, "\\(blah");
+    assertQueryEquals("\\)blah", a, "\\)blah");
+    assertQueryEquals("\\~blah", a, "\\~blah");
+    assertQueryEquals("\\*blah", a, "\\*blah");
+    assertQueryEquals("\\?blah", a, "\\?blah");
+    //assertQueryEquals("foo \\&\\& bar", a, "foo \\&\\& bar");
+    //assertQueryEquals("foo \\|| bar", a, "foo \\|| bar");
+    //assertQueryEquals("foo \\AND bar", a, "foo \\AND bar");*/
+
+    assertQueryEquals("a\\-b:c", a, "a-b:c");
+    assertQueryEquals("a\\+b:c", a, "a+b:c");
+    assertQueryEquals("a\\:b:c", a, "a:b:c");
+    assertQueryEquals("a\\\\b:c", a, "a\\b:c");
+
+    assertQueryEquals("a:b\\-c", a, "a:b-c");
+    assertQueryEquals("a:b\\+c", a, "a:b+c");
+    assertQueryEquals("a:b\\:c", a, "a:b:c");
+    assertQueryEquals("a:b\\\\c", a, "a:b\\c");
+
+    assertQueryEquals("a:b\\-c*", a, "a:b-c*");
+    assertQueryEquals("a:b\\+c*", a, "a:b+c*");
+    assertQueryEquals("a:b\\:c*", a, "a:b:c*");
+
+    assertQueryEquals("a:b\\\\c*", a, "a:b\\c*");
+
+    assertQueryEquals("a:b\\-?c", a, "a:b-?c");
+    assertQueryEquals("a:b\\+?c", a, "a:b+?c");
+    assertQueryEquals("a:b\\:?c", a, "a:b:?c");
+
+    assertQueryEquals("a:b\\\\?c", a, "a:b\\?c");
+
+    assertQueryEquals("a:b\\-c~", a, "a:b-c~0.5");
+    assertQueryEquals("a:b\\+c~", a, "a:b+c~0.5");
+    assertQueryEquals("a:b\\:c~", a, "a:b:c~0.5");
+    assertQueryEquals("a:b\\\\c~", a, "a:b\\c~0.5");
+
+    assertQueryEquals("[ a\\- TO a\\+ ]", null, "[a- TO a+]");
+    assertQueryEquals("[ a\\: TO a\\~ ]", null, "[a: TO a~]");
+    assertQueryEquals("[ a\\\\ TO a\\* ]", null, "[a\\ TO a*]");
+  }
+
+  public void testTabNewlineCarriageReturn()
+    throws Exception {
+    assertQueryEqualsDOA("+weltbank +worlbank", null,
+      "+weltbank +worlbank");
+
+    assertQueryEqualsDOA("+weltbank\n+worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \n+worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \n +worlbank", null,
+      "+weltbank +worlbank");
+
+    assertQueryEqualsDOA("+weltbank\r+worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \r+worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \r +worlbank", null,
+      "+weltbank +worlbank");
+
+    assertQueryEqualsDOA("+weltbank\r\n+worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \r\n+worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \r\n +worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \r \n +worlbank", null,
+      "+weltbank +worlbank");
+
+    assertQueryEqualsDOA("+weltbank\t+worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \t+worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \t +worlbank", null,
+      "+weltbank +worlbank");
+  }
+
+  public void testSimpleDAO()
+    throws Exception {
+    assertQueryEqualsDOA("term term term", null, "+term +term +term");
+    assertQueryEqualsDOA("term +term term", null, "+term +term +term");
+    assertQueryEqualsDOA("term term +term", null, "+term +term +term");
+    assertQueryEqualsDOA("term +term +term", null, "+term +term +term");
+    assertQueryEqualsDOA("-term term term", null, "-term +term +term");
+  }
+
+  public void testBoost()
+    throws Exception {
+    StandardAnalyzer oneStopAnalyzer = new StandardAnalyzer(new String[]{"on"});
+    QueryParser qp = new QueryParser("field", oneStopAnalyzer);
+    Query q = qp.parse("on^1.0");
+    assertNotNull(q);
+    q = qp.parse("\"hello\"^2.0");
+    assertNotNull(q);
+    assertEquals(q.getBoost(), (float) 2.0, (float) 0.5);
+    q = qp.parse("hello^2.0");
+    assertNotNull(q);
+    assertEquals(q.getBoost(), (float) 2.0, (float) 0.5);
+    q = qp.parse("\"on\"^1.0");
+    assertNotNull(q);
+
+    q = QueryParser.parse("the^3", "field", new StandardAnalyzer());
+    assertNotNull(q);
+  }
+
+  public void testException() throws Exception {
+    try {
+      assertQueryEquals("\"some phrase", null, "abc");
+      fail("ParseException expected, not thrown");
+    } catch (ParseException expected) {
+    }
+  }
+
+  public void testCustomQueryParserWildcard() {
+    try {
+      new QPTestParser("contents", new WhitespaceAnalyzer()).parse("a?t");
+    } catch (ParseException expected) {
+      return;
+    }
+    fail("Wildcard queries should not be allowed");
+  }
+
+  public void testCustomQueryParserFuzzy() throws Exception {
+    try {
+      new QPTestParser("contents", new WhitespaceAnalyzer()).parse("xunit~");
+    } catch (ParseException expected) {
+      return;
+    }
+    fail("Fuzzy queries should not be allowed");
+  }
+
+  public void testBooleanQuery() throws Exception {
+    BooleanQuery.setMaxClauseCount(2);
+    try {
+      QueryParser.parse("one two three", "field", new WhitespaceAnalyzer());
+      fail("ParseException expected due to too many boolean clauses");
+    } catch (ParseException expected) {
+      // too many boolean clauses, so ParseException is expected
+    }
+  }
+
+  public void tearDown() {
+    BooleanQuery.setMaxClauseCount(originalMaxClauses);
+  }
+
+}
diff --git a/src/test-deprecated/org/apache/lucene/search/CheckHits.java b/src/test-deprecated/org/apache/lucene/search/CheckHits.java
new file mode 100644
index 0000000..9cabec5
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/search/CheckHits.java
@@ -0,0 +1,65 @@
+package org.apache.lucene.search;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/* 20 May 2004:   Factored out of spans tests. Please leave this comment
+                  until this class is evt. also used by tests in search package.
+ */
+
+import org.apache.lucene.search.Searcher;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.Hits;
+import junit.framework.TestCase;
+
+import java.io.IOException;
+import java.util.Set;
+import java.util.TreeSet;
+
+public class CheckHits {
+  public static void checkHits(
+        Query query,
+        String defaultFieldName,
+        Searcher searcher,
+        int[] results,
+        TestCase testCase)
+          throws IOException {
+    Hits hits = searcher.search(query);
+
+    Set correct = new TreeSet();
+    for (int i = 0; i < results.length; i++) {
+      correct.add(new Integer(results[i]));
+    }
+
+    Set actual = new TreeSet();
+    for (int i = 0; i < hits.length(); i++) {
+      actual.add(new Integer(hits.id(i)));
+    }
+
+    testCase.assertEquals(query.toString(defaultFieldName), correct, actual);
+  }
+
+  public static void printDocNrs(Hits hits) throws IOException {
+    System.out.print("new int[] {");
+    for (int i = 0; i < hits.length(); i++) {
+      System.out.print(hits.id(i));
+      if (i != hits.length()-1)
+        System.out.print(", ");
+    }
+    System.out.println("}");
+  }
+}
+
diff --git a/src/test-deprecated/org/apache/lucene/search/MockFilter.java b/src/test-deprecated/org/apache/lucene/search/MockFilter.java
new file mode 100644
index 0000000..5bdd584
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/search/MockFilter.java
@@ -0,0 +1,37 @@
+package org.apache.lucene.search;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.index.IndexReader;
+import java.util.BitSet;
+
+public class MockFilter extends Filter {
+  private boolean wasCalled;
+
+  public BitSet bits(IndexReader reader) {
+    wasCalled = true;
+    return new BitSet();
+  }
+
+  public void clear() {
+    wasCalled = false;
+  }
+
+  public boolean wasCalled() {
+    return wasCalled;
+  }
+}
diff --git a/src/test-deprecated/org/apache/lucene/search/SampleComparable.java b/src/test-deprecated/org/apache/lucene/search/SampleComparable.java
new file mode 100644
index 0000000..3d0ce6d
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/search/SampleComparable.java
@@ -0,0 +1,143 @@
+package org.apache.lucene.search;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.TermDocs;
+import org.apache.lucene.index.TermEnum;
+
+import java.io.IOException;
+import java.io.Serializable;
+
+/**
+ * An example Comparable for use with the custom sort tests.
+ * It implements a comparable for "id" sort of values which
+ * consist of an alphanumeric part and a numeric part, such as:
+ * <p/>
+ * <P>ABC-123, A-1, A-7, A-100, B-99999
+ * <p/>
+ * <p>Such values cannot be sorted as strings, since A-100 needs
+ * to come after A-7.
+ * <p/>
+ * <p>It could be argued that the "ids" should be rewritten as
+ * A-0001, A-0100, etc. so they will sort as strings.  That is
+ * a valid alternate way to solve it - but
+ * this is only supposed to be a simple test case.
+ * <p/>
+ * <p>Created: Apr 21, 2004 5:34:47 PM
+ *
+ * @author Tim Jones
+ * @version $Id$
+ * @since 1.4
+ */
+public class SampleComparable
+implements Comparable, Serializable {
+
+  String string_part;
+  Integer int_part;
+
+  public SampleComparable (String s) {
+    int i = s.indexOf ("-");
+    string_part = s.substring (0, i);
+    int_part = new Integer (s.substring (i + 1));
+  }
+
+  public int compareTo (Object o) {
+    SampleComparable otherid = (SampleComparable) o;
+    int i = string_part.compareTo (otherid.string_part);
+    if (i == 0) return int_part.compareTo (otherid.int_part);
+    return i;
+  }
+
+  public static SortComparatorSource getComparatorSource () {
+    return new SortComparatorSource () {
+      public ScoreDocComparator newComparator (final IndexReader reader, String fieldname)
+      throws IOException {
+        final String field = fieldname.intern ();
+        final TermEnum enumerator = reader.terms (new Term (fieldname, ""));
+        try {
+          return new ScoreDocComparator () {
+            protected Comparable[] cachedValues = fillCache (reader, enumerator, field);
+
+            public int compare (ScoreDoc i, ScoreDoc j) {
+              return cachedValues[i.doc].compareTo (cachedValues[j.doc]);
+            }
+
+            public Comparable sortValue (ScoreDoc i) {
+              return cachedValues[i.doc];
+            }
+
+            public int sortType () {
+              return SortField.CUSTOM;
+            }
+          };
+        } finally {
+          enumerator.close ();
+        }
+      }
+
+      /**
+       * Returns an array of objects which represent that natural order
+       * of the term values in the given field.
+       *
+       * @param reader     Terms are in this index.
+       * @param enumerator Use this to get the term values and TermDocs.
+       * @param fieldname  Comparables should be for this field.
+       * @return Array of objects representing natural order of terms in field.
+       * @throws IOException If an error occurs reading the index.
+       */
+      protected Comparable[] fillCache (IndexReader reader, TermEnum enumerator, String fieldname)
+      throws IOException {
+        final String field = fieldname.intern ();
+        Comparable[] retArray = new Comparable[reader.maxDoc ()];
+        if (retArray.length > 0) {
+          TermDocs termDocs = reader.termDocs ();
+          try {
+            if (enumerator.term () == null) {
+              throw new RuntimeException ("no terms in field " + field);
+            }
+            do {
+              Term term = enumerator.term ();
+              if (term.field () != field) break;
+              Comparable termval = getComparable (term.text ());
+              termDocs.seek (enumerator);
+              while (termDocs.next ()) {
+                retArray[termDocs.doc ()] = termval;
+              }
+            } while (enumerator.next ());
+          } finally {
+            termDocs.close ();
+          }
+        }
+        return retArray;
+      }
+
+      Comparable getComparable (String termtext) {
+        return new SampleComparable (termtext);
+      }
+    };
+  }
+
+  public static SortComparator getComparator() {
+    return new SortComparator() {
+      protected Comparable getComparable (String termtext) {
+        return new SampleComparable (termtext);
+      }
+    };
+  }
+}
\ No newline at end of file
diff --git a/src/test-deprecated/org/apache/lucene/search/TestBooleanPrefixQuery.java b/src/test-deprecated/org/apache/lucene/search/TestBooleanPrefixQuery.java
new file mode 100644
index 0000000..e114412
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/search/TestBooleanPrefixQuery.java
@@ -0,0 +1,104 @@
+package org.apache.lucene.search;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import junit.framework.TestCase;
+import junit.framework.Test;
+import junit.framework.TestSuite;
+import junit.textui.TestRunner;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.search.PrefixQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.BooleanQuery;
+
+import java.io.IOException;
+
+/**
+ * @author schnee
+ * @version $Id$
+ **/
+
+public class TestBooleanPrefixQuery extends TestCase {
+
+  public static void main(String[] args) {
+    TestRunner.run(suite());
+  }
+
+  public static Test suite() {
+    return new TestSuite(TestBooleanPrefixQuery.class);
+  }
+
+  public TestBooleanPrefixQuery(String name) {
+    super(name);
+  }
+
+  public void testMethod() {
+    RAMDirectory directory = new RAMDirectory();
+
+    String[] categories = new String[]{"food",
+                                       "foodanddrink",
+                                       "foodanddrinkandgoodtimes",
+                                       "food and drink"};
+
+    Query rw1 = null;
+    Query rw2 = null;
+    try {
+      IndexWriter writer = new IndexWriter(directory, new
+                                           WhitespaceAnalyzer(), true);
+      for (int i = 0; i < categories.length; i++) {
+        Document doc = new Document();
+        doc.add(Field.Keyword("category", categories[i]));
+        writer.addDocument(doc);
+      }
+      writer.close();
+      
+      IndexReader reader = IndexReader.open(directory);
+      PrefixQuery query = new PrefixQuery(new Term("category", "foo"));
+      
+      rw1 = query.rewrite(reader);
+      
+      BooleanQuery bq = new BooleanQuery();
+      bq.add(query, true, false);
+      
+      rw2 = bq.rewrite(reader);
+    } catch (IOException e) {
+      fail(e.getMessage());
+    }
+
+    BooleanQuery bq1 = null;
+    if (rw1 instanceof BooleanQuery) {
+      bq1 = (BooleanQuery) rw1;
+    }
+
+    BooleanQuery bq2 = null;
+    if (rw2 instanceof BooleanQuery) {
+        bq2 = (BooleanQuery) rw2;
+    } else {
+      fail("Rewrite");
+    }
+
+    assertEquals("Number of Clauses Mismatch", bq1.getClauses().length,
+                 bq2.getClauses().length);
+  }
+}
+
diff --git a/src/test-deprecated/org/apache/lucene/search/TestCachingWrapperFilter.java b/src/test-deprecated/org/apache/lucene/search/TestCachingWrapperFilter.java
new file mode 100644
index 0000000..9a76a85
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/search/TestCachingWrapperFilter.java
@@ -0,0 +1,48 @@
+package org.apache.lucene.search;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import junit.framework.TestCase;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+
+public class TestCachingWrapperFilter extends TestCase {
+  public void testCachingWorks() throws Exception {
+    Directory dir = new RAMDirectory();
+    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(), true);
+    writer.close();
+
+    IndexReader reader = IndexReader.open(dir);
+
+    MockFilter filter = new MockFilter();
+    CachingWrapperFilter cacher = new CachingWrapperFilter(filter);
+
+    // first time, nested filter is called
+    cacher.bits(reader);
+    assertTrue("first time", filter.wasCalled());
+
+    // second time, nested filter should not be called
+    filter.clear();
+    cacher.bits(reader);
+    assertFalse("second time", filter.wasCalled());
+
+    reader.close();
+ }
+}
diff --git a/src/test-deprecated/org/apache/lucene/search/TestDateFilter.java b/src/test-deprecated/org/apache/lucene/search/TestDateFilter.java
new file mode 100644
index 0000000..c1a8df4
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/search/TestDateFilter.java
@@ -0,0 +1,163 @@
+package org.apache.lucene.search;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.Hits;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.analysis.SimpleAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.DateField;
+
+import java.io.IOException;
+
+import junit.framework.TestCase;
+
+ /**
+  * DateFilter JUnit tests.
+  *
+  * @author Otis Gospodnetic
+  * @version $Revision$
+  */
+public class TestDateFilter
+    extends TestCase
+{
+    public TestDateFilter(String name)
+    {
+	super(name);
+    }
+
+    /**
+     *
+     */
+    public static void testBefore()
+	throws IOException
+    {
+	// create an index
+        RAMDirectory indexStore = new RAMDirectory();
+        IndexWriter writer = new IndexWriter(indexStore, new SimpleAnalyzer(), true);
+
+ 	long now = System.currentTimeMillis();
+
+ 	Document doc = new Document();
+ 	// add time that is in the past
+ 	doc.add(Field.Keyword("datefield", DateField.timeToString(now - 1000)));
+ 	doc.add(Field.Text("body", "Today is a very sunny day in New York City"));
+  	writer.addDocument(doc);
+ 	writer.optimize();
+	writer.close();
+
+	IndexSearcher searcher = new IndexSearcher(indexStore);
+
+	// filter that should preserve matches
+	DateFilter df1 = DateFilter.Before("datefield", now);
+
+	// filter that should discard matches
+	DateFilter df2 = DateFilter.Before("datefield", now - 999999);
+
+	// search something that doesn't exist with DateFilter
+	Query query1 = new TermQuery(new Term("body", "NoMatchForThis"));
+
+	// search for something that does exists
+	Query query2 = new TermQuery(new Term("body", "sunny"));
+
+	Hits result;
+
+	// ensure that queries return expected results without DateFilter first
+	result = searcher.search(query1);
+	assertEquals(0, result.length());
+
+	result = searcher.search(query2);
+	assertEquals(1, result.length());
+
+
+	// run queries with DateFilter
+	result = searcher.search(query1, df1);
+	assertEquals(0, result.length());
+
+	result = searcher.search(query1, df2);
+	assertEquals(0, result.length());
+
+ 	result = searcher.search(query2, df1);
+ 	assertEquals(1, result.length());
+
+	result = searcher.search(query2, df2);
+	assertEquals(0, result.length());
+    }
+
+    /**
+     *
+     */
+    public static void testAfter()
+	throws IOException
+    {
+	// create an index
+        RAMDirectory indexStore = new RAMDirectory();
+        IndexWriter writer = new IndexWriter(indexStore, new SimpleAnalyzer(), true);
+
+ 	long now = System.currentTimeMillis();
+
+ 	Document doc = new Document();
+ 	// add time that is in the future
+ 	doc.add(Field.Keyword("datefield", DateField.timeToString(now + 888888)));
+ 	doc.add(Field.Text("body", "Today is a very sunny day in New York City"));
+  	writer.addDocument(doc);
+ 	writer.optimize();
+	writer.close();
+
+	IndexSearcher searcher = new IndexSearcher(indexStore);
+
+	// filter that should preserve matches
+	DateFilter df1 = DateFilter.After("datefield", now);
+
+	// filter that should discard matches
+	DateFilter df2 = DateFilter.After("datefield", now + 999999);
+
+	// search something that doesn't exist with DateFilter
+	Query query1 = new TermQuery(new Term("body", "NoMatchForThis"));
+
+	// search for something that does exists
+	Query query2 = new TermQuery(new Term("body", "sunny"));
+
+	Hits result;
+
+	// ensure that queries return expected results without DateFilter first
+	result = searcher.search(query1);
+	assertEquals(0, result.length());
+
+	result = searcher.search(query2);
+	assertEquals(1, result.length());
+
+
+	// run queries with DateFilter
+	result = searcher.search(query1, df1);
+	assertEquals(0, result.length());
+
+	result = searcher.search(query1, df2);
+	assertEquals(0, result.length());
+
+ 	result = searcher.search(query2, df1);
+ 	assertEquals(1, result.length());
+
+	result = searcher.search(query2, df2);
+	assertEquals(0, result.length());
+    }
+}
diff --git a/src/test-deprecated/org/apache/lucene/search/TestDocBoost.java b/src/test-deprecated/org/apache/lucene/search/TestDocBoost.java
new file mode 100644
index 0000000..a3bb315
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/search/TestDocBoost.java
@@ -0,0 +1,83 @@
+package org.apache.lucene.search;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import junit.framework.TestCase;
+
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.analysis.SimpleAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+
+/** Document boost unit test.
+ *
+ * @author Doug Cutting
+ * @version $Revision$
+ */
+public class TestDocBoost extends TestCase {
+  public TestDocBoost(String name) {
+    super(name);
+  }
+  
+  public void testDocBoost() throws Exception {
+    RAMDirectory store = new RAMDirectory();
+    IndexWriter writer = new IndexWriter(store, new SimpleAnalyzer(), true);
+    
+    Field f1 = Field.Text("field", "word");
+    Field f2 = Field.Text("field", "word");
+    f2.setBoost(2.0f);
+    
+    Document d1 = new Document();
+    Document d2 = new Document();
+    Document d3 = new Document();
+    Document d4 = new Document();
+    d3.setBoost(3.0f);
+    d4.setBoost(2.0f);
+    
+    d1.add(f1);                                 // boost = 1
+    d2.add(f2);                                 // boost = 2
+    d3.add(f1);                                 // boost = 3
+    d4.add(f2);                                 // boost = 4
+    
+    writer.addDocument(d1);
+    writer.addDocument(d2);
+    writer.addDocument(d3);
+    writer.addDocument(d4);
+    writer.optimize();
+    writer.close();
+
+    final float[] scores = new float[4];
+
+    new IndexSearcher(store).search
+      (new TermQuery(new Term("field", "word")),
+       new HitCollector() {
+         public final void collect(int doc, float score) {
+           scores[doc] = score;
+         }
+       });
+    
+    float lastScore = 0.0f;
+
+    for (int i = 0; i < 4; i++) {
+      assertTrue(scores[i] > lastScore);
+      lastScore = scores[i];
+    }
+  }
+}
diff --git a/src/test-deprecated/org/apache/lucene/search/TestFilteredQuery.java b/src/test-deprecated/org/apache/lucene/search/TestFilteredQuery.java
new file mode 100644
index 0000000..5864c88
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/search/TestFilteredQuery.java
@@ -0,0 +1,132 @@
+package org.apache.lucene.search;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import junit.framework.TestCase;
+import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.store.RAMDirectory;
+import java.util.BitSet;
+import java.io.IOException;
+
+
+/**
+ * FilteredQuery JUnit tests.
+ *
+ * <p>Created: Apr 21, 2004 1:21:46 PM
+ *
+ * @author  Tim Jones
+ * @version $Id$
+ * @since   1.4
+ */
+public class TestFilteredQuery
+extends TestCase {
+
+  private IndexSearcher searcher;
+  private RAMDirectory directory;
+  private Query query;
+  private Filter filter;
+
+  public void setUp()
+  throws Exception {
+    directory = new RAMDirectory();
+    IndexWriter writer = new IndexWriter (directory, new WhitespaceAnalyzer(), true);
+
+    Document doc = new Document();
+    doc.add (Field.Text ("field", "one two three four five"));
+    doc.add (Field.Text ("sorter", "b"));
+    writer.addDocument (doc);
+
+    doc = new Document();
+    doc.add (Field.Text ("field", "one two three four"));
+    doc.add (Field.Text ("sorter", "d"));
+    writer.addDocument (doc);
+
+    doc = new Document();
+    doc.add (Field.Text ("field", "one two three y"));
+    doc.add (Field.Text ("sorter", "a"));
+    writer.addDocument (doc);
+
+    doc = new Document();
+    doc.add (Field.Text ("field", "one two x"));
+    doc.add (Field.Text ("sorter", "c"));
+    writer.addDocument (doc);
+
+    writer.optimize ();
+    writer.close ();
+
+    searcher = new IndexSearcher (directory);
+    query = new TermQuery (new Term ("field", "three"));
+    filter = new Filter() {
+      public BitSet bits (IndexReader reader) throws IOException {
+        BitSet bitset = new BitSet(5);
+        bitset.set (1);
+        bitset.set (3);
+        return bitset;
+      }
+    };
+  }
+
+  public void tearDown()
+  throws Exception {
+    searcher.close();
+    directory.close();
+  }
+
+  public void testFilteredQuery()
+  throws Exception {
+    Query filteredquery = new FilteredQuery (query, filter);
+    Hits hits = searcher.search (filteredquery);
+    assertEquals (1, hits.length());
+    assertEquals (1, hits.id(0));
+
+    hits = searcher.search (filteredquery, new Sort("sorter"));
+    assertEquals (1, hits.length());
+    assertEquals (1, hits.id(0));
+
+    filteredquery = new FilteredQuery (new TermQuery (new Term ("field", "one")), filter);
+    hits = searcher.search (filteredquery);
+    assertEquals (2, hits.length());
+
+    filteredquery = new FilteredQuery (new TermQuery (new Term ("field", "x")), filter);
+    hits = searcher.search (filteredquery);
+    assertEquals (1, hits.length());
+    assertEquals (3, hits.id(0));
+
+    filteredquery = new FilteredQuery (new TermQuery (new Term ("field", "y")), filter);
+    hits = searcher.search (filteredquery);
+    assertEquals (0, hits.length());
+  }
+
+  /**
+   * This tests FilteredQuery's rewrite correctness
+   */
+  public void testRangeQuery() throws Exception {
+    RangeQuery rq = new RangeQuery(
+        new Term("sorter", "b"), new Term("sorter", "d"), true);
+
+    Query filteredquery = new FilteredQuery(rq, filter);
+    Hits hits = searcher.search(filteredquery);
+    assertEquals(2, hits.length());
+  }
+
+}
+
diff --git a/src/test-deprecated/org/apache/lucene/search/TestFuzzyQuery.java b/src/test-deprecated/org/apache/lucene/search/TestFuzzyQuery.java
new file mode 100644
index 0000000..9091b24
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/search/TestFuzzyQuery.java
@@ -0,0 +1,137 @@
+package org.apache.lucene.search;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import junit.framework.TestCase;
+import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.store.RAMDirectory;
+
+/**
+ * Tests {@link FuzzyQuery}.
+ *
+ * @author Daniel Naber
+ */
+public class TestFuzzyQuery extends TestCase {
+
+  public void testDefaultFuzziness() throws Exception {
+    RAMDirectory directory = new RAMDirectory();
+    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true);
+    addDoc("aaaaa", writer);
+    addDoc("aaaab", writer);
+    addDoc("aaabb", writer);
+    addDoc("aabbb", writer);
+    addDoc("abbbb", writer);
+    addDoc("bbbbb", writer);
+    addDoc("ddddd", writer);
+    writer.optimize();
+    writer.close();
+    IndexSearcher searcher = new IndexSearcher(directory);
+
+    FuzzyQuery query = new FuzzyQuery(new Term("field", "aaaaa"));   
+    Hits hits = searcher.search(query);
+    assertEquals(3, hits.length());
+
+    // not similar enough:
+    query = new FuzzyQuery(new Term("field", "xxxxx"));  	
+    hits = searcher.search(query);
+    assertEquals(0, hits.length());
+    query = new FuzzyQuery(new Term("field", "aaccc"));   // edit distance to "aaaaa" = 3
+    hits = searcher.search(query);
+    assertEquals(0, hits.length());
+
+    // query identical to a word in the index:
+    query = new FuzzyQuery(new Term("field", "aaaaa"));   
+    hits = searcher.search(query);
+    assertEquals(3, hits.length());
+    assertEquals(hits.doc(0).get("field"), ("aaaaa"));
+    // default allows for up to two edits:
+    assertEquals(hits.doc(1).get("field"), ("aaaab"));
+    assertEquals(hits.doc(2).get("field"), ("aaabb"));
+
+    // query similar to a word in the index:
+    query = new FuzzyQuery(new Term("field", "aaaac"));   
+    hits = searcher.search(query);
+    assertEquals(3, hits.length());
+    assertEquals(hits.doc(0).get("field"), ("aaaaa"));
+    assertEquals(hits.doc(1).get("field"), ("aaaab"));
+    assertEquals(hits.doc(2).get("field"), ("aaabb"));
+
+    query = new FuzzyQuery(new Term("field", "ddddX"));   
+    hits = searcher.search(query);
+    assertEquals(1, hits.length());
+    assertEquals(hits.doc(0).get("field"), ("ddddd"));
+
+    // different field = no match:
+    query = new FuzzyQuery(new Term("anotherfield", "ddddX"));   
+    hits = searcher.search(query);
+    assertEquals(0, hits.length());
+
+    searcher.close();
+    directory.close();
+  }
+
+  public void testDefaultFuzzinessLong() throws Exception {
+    RAMDirectory directory = new RAMDirectory();
+    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true);
+    addDoc("aaaaaaa", writer);
+    addDoc("segment", writer);
+    writer.optimize();
+    writer.close();
+    IndexSearcher searcher = new IndexSearcher(directory);
+
+    FuzzyQuery query;
+    // not similar enough:
+    query = new FuzzyQuery(new Term("field", "xxxxx"));   
+    Hits hits = searcher.search(query);
+    assertEquals(0, hits.length());
+    // edit distance to "aaaaaaa" = 3, this matches because the string is longer than
+    // in testDefaultFuzziness so a bigger difference is allowed:
+    query = new FuzzyQuery(new Term("field", "aaaaccc"));   
+    hits = searcher.search(query);
+    assertEquals(1, hits.length());
+    assertEquals(hits.doc(0).get("field"), ("aaaaaaa"));
+
+    // no match, more than half of the characters is wrong:
+    query = new FuzzyQuery(new Term("field", "aaacccc"));   
+    hits = searcher.search(query);
+    assertEquals(0, hits.length());
+
+    // "student" and "stellent" are indeed similar to "segment" by default:
+    query = new FuzzyQuery(new Term("field", "student"));   
+    hits = searcher.search(query);
+    assertEquals(1, hits.length());
+    query = new FuzzyQuery(new Term("field", "stellent"));   
+    hits = searcher.search(query);
+    assertEquals(1, hits.length());
+
+    searcher.close();
+    directory.close();
+  }
+  
+  private void addDoc(String text, IndexWriter writer) throws IOException {
+    Document doc = new Document();
+    doc.add(Field.Text("field", text));
+    writer.addDocument(doc);
+  }
+
+}
diff --git a/src/test-deprecated/org/apache/lucene/search/TestMultiSearcher.java b/src/test-deprecated/org/apache/lucene/search/TestMultiSearcher.java
new file mode 100644
index 0000000..f7e7c83
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/search/TestMultiSearcher.java
@@ -0,0 +1,206 @@
+package org.apache.lucene.search;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.search.Searcher;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.RAMDirectory;
+
+import junit.framework.TestCase;
+
+import java.io.IOException;
+
+/**
+ * Tests {@link MultiSearcher} class.
+ *
+ * @version $Id$
+ */
+public class TestMultiSearcher extends TestCase
+{
+    public TestMultiSearcher(String name)
+    {
+        super(name);
+    }
+
+	/**
+	 * ReturnS a new instance of the concrete MultiSearcher class
+	 * used in this test.
+	 */
+	protected MultiSearcher getMultiSearcherInstance(Searcher[] searchers) throws IOException {
+		return new MultiSearcher(searchers);
+	}
+
+    public void testEmptyIndex()
+        throws Exception
+    {
+        // creating two directories for indices
+        Directory indexStoreA = new RAMDirectory();
+        Directory indexStoreB = new RAMDirectory();
+
+        // creating a document to store
+        Document lDoc = new Document();
+        lDoc.add(Field.Text("fulltext", "Once upon a time....."));
+        lDoc.add(Field.Keyword("id", "doc1"));
+        lDoc.add(Field.Keyword("handle", "1"));
+
+        // creating a document to store
+        Document lDoc2 = new Document();
+        lDoc2.add(Field.Text("fulltext", "in a galaxy far far away....."));
+        lDoc2.add(Field.Keyword("id", "doc2"));
+        lDoc2.add(Field.Keyword("handle", "1"));
+
+        // creating a document to store
+        Document lDoc3 = new Document();
+        lDoc3.add(Field.Text("fulltext", "a bizarre bug manifested itself...."));
+        lDoc3.add(Field.Keyword("id", "doc3"));
+        lDoc3.add(Field.Keyword("handle", "1"));
+
+        // creating an index writer for the first index
+        IndexWriter writerA = new IndexWriter(indexStoreA, new StandardAnalyzer(), true);
+        // creating an index writer for the second index, but writing nothing
+        IndexWriter writerB = new IndexWriter(indexStoreB, new StandardAnalyzer(), true);
+
+        //--------------------------------------------------------------------
+        // scenario 1
+        //--------------------------------------------------------------------
+
+        // writing the documents to the first index
+        writerA.addDocument(lDoc);
+        writerA.addDocument(lDoc2);
+        writerA.addDocument(lDoc3);
+        writerA.optimize();
+        writerA.close();
+
+        // closing the second index
+        writerB.close();
+
+        // creating the query
+        Query query = QueryParser.parse("handle:1", "fulltext", new StandardAnalyzer());
+
+        // building the searchables
+        Searcher[] searchers = new Searcher[2];
+        // VITAL STEP:adding the searcher for the empty index first, before the searcher for the populated index
+        searchers[0] = new IndexSearcher(indexStoreB);
+        searchers[1] = new IndexSearcher(indexStoreA);
+        // creating the multiSearcher
+        Searcher mSearcher = getMultiSearcherInstance(searchers);
+        // performing the search
+        Hits hits = mSearcher.search(query);
+
+        assertEquals(3, hits.length());
+
+        try {
+            // iterating over the hit documents
+            for (int i = 0; i < hits.length(); i++) {
+                Document d = hits.doc(i);
+            }
+        }
+        catch (ArrayIndexOutOfBoundsException e)
+        {
+            fail("ArrayIndexOutOfBoundsException thrown: " + e.getMessage());
+            e.printStackTrace();
+        } finally{
+            mSearcher.close();
+        }
+
+
+        //--------------------------------------------------------------------
+        // scenario 2
+        //--------------------------------------------------------------------
+
+        // adding one document to the empty index
+        writerB = new IndexWriter(indexStoreB, new StandardAnalyzer(), false);
+        writerB.addDocument(lDoc);
+        writerB.optimize();
+        writerB.close();
+
+        // building the searchables
+        Searcher[] searchers2 = new Searcher[2];
+        // VITAL STEP:adding the searcher for the empty index first, before the searcher for the populated index
+        searchers2[0] = new IndexSearcher(indexStoreB);
+        searchers2[1] = new IndexSearcher(indexStoreA);
+        // creating the mulitSearcher
+        Searcher mSearcher2 = getMultiSearcherInstance(searchers2);
+        // performing the same search
+        Hits hits2 = mSearcher2.search(query);
+
+        assertEquals(4, hits2.length());
+
+        try {
+            // iterating over the hit documents
+            for (int i = 0; i < hits2.length(); i++) {
+                // no exception should happen at this point
+                Document d = hits2.doc(i);
+            }
+        }
+        catch (Exception e)
+        {
+            fail("Exception thrown: " + e.getMessage());
+            e.printStackTrace();
+        } finally{
+            mSearcher2.close();
+        }
+
+        //--------------------------------------------------------------------
+        // scenario 3
+        //--------------------------------------------------------------------
+
+        // deleting the document just added, this will cause a different exception to take place
+        Term term = new Term("id", "doc1");
+        IndexReader readerB = IndexReader.open(indexStoreB);
+        readerB.delete(term);
+        readerB.close();
+
+        // optimizing the index with the writer
+        writerB = new IndexWriter(indexStoreB, new StandardAnalyzer(), false);
+        writerB.optimize();
+        writerB.close();
+
+        // building the searchables
+        Searcher[] searchers3 = new Searcher[2];
+
+        searchers3[0] = new IndexSearcher(indexStoreB);
+        searchers3[1] = new IndexSearcher(indexStoreA);
+        // creating the mulitSearcher
+        Searcher mSearcher3 = getMultiSearcherInstance(searchers3);
+        // performing the same search
+        Hits hits3 = mSearcher3.search(query);
+
+        assertEquals(3, hits3.length());
+
+        try {
+            // iterating over the hit documents
+            for (int i = 0; i < hits3.length(); i++) {
+                Document d = hits3.doc(i);
+            }
+        }
+        catch (IOException e)
+        {
+            fail("IOException thrown: " + e.getMessage());
+            e.printStackTrace();
+        } finally{
+            mSearcher3.close();
+        }
+    }
+}
diff --git a/src/test-deprecated/org/apache/lucene/search/TestNot.java b/src/test-deprecated/org/apache/lucene/search/TestNot.java
new file mode 100644
index 0000000..87547b6
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/search/TestNot.java
@@ -0,0 +1,58 @@
+package org.apache.lucene.search;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import junit.framework.TestCase;
+
+import java.util.Vector;
+
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.analysis.SimpleAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+
+/** Similarity unit test.
+ *
+ * @author Doug Cutting
+ * @version $Revision$
+ */
+public class TestNot extends TestCase {
+  public TestNot(String name) {
+    super(name);
+  }
+
+  public void testNot() throws Exception {
+    RAMDirectory store = new RAMDirectory();
+    IndexWriter writer = new IndexWriter(store, new SimpleAnalyzer(), true);
+
+    Document d1 = new Document();
+    d1.add(Field.Text("field", "a b"));
+
+    writer.addDocument(d1);
+    writer.optimize();
+    writer.close();
+
+    Searcher searcher = new IndexSearcher(store);
+    Query query = QueryParser.parse("a NOT b", "field", new SimpleAnalyzer());
+    //System.out.println(query);
+    Hits hits = searcher.search(query);
+    assertEquals(0, hits.length());
+  }
+}
diff --git a/src/test-deprecated/org/apache/lucene/search/TestParallelMultiSearcher.java b/src/test-deprecated/org/apache/lucene/search/TestParallelMultiSearcher.java
new file mode 100644
index 0000000..d06269d
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/search/TestParallelMultiSearcher.java
@@ -0,0 +1,35 @@
+package org.apache.lucene.search;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+ 
+import java.io.IOException;
+
+/**
+ * Unit tests for the ParallelMultiSearcher 
+ */
+public class TestParallelMultiSearcher extends TestMultiSearcher {
+
+	public TestParallelMultiSearcher(String name) {
+		super(name);
+	}
+
+	protected MultiSearcher getMultiSearcherInstance(Searcher[] searchers)
+		throws IOException {
+		return new ParallelMultiSearcher(searchers);
+	}
+
+}
diff --git a/src/test-deprecated/org/apache/lucene/search/TestPhrasePrefixQuery.java b/src/test-deprecated/org/apache/lucene/search/TestPhrasePrefixQuery.java
new file mode 100644
index 0000000..512f3de
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/search/TestPhrasePrefixQuery.java
@@ -0,0 +1,104 @@
+package org.apache.lucene.search;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.TermEnum;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.analysis.SimpleAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+
+import junit.framework.TestCase;
+
+import java.io.IOException;
+import java.util.LinkedList;
+
+/**
+ * This class tests PhrasePrefixQuery class.
+ *
+ * @author Otis Gospodnetic
+ * @version $Id$
+ */
+public class TestPhrasePrefixQuery
+    extends TestCase
+{
+    public TestPhrasePrefixQuery(String name)
+    {
+        super(name);
+    }
+
+    /**
+     *
+     */
+    public void testPhrasePrefix()
+        throws IOException
+    {
+        RAMDirectory indexStore = new RAMDirectory();
+        IndexWriter writer = new IndexWriter(indexStore, new SimpleAnalyzer(), true);
+        Document doc1 = new Document();
+        Document doc2 = new Document();
+        Document doc3 = new Document();
+        Document doc4 = new Document();
+        Document doc5 = new Document();
+        doc1.add(Field.Text("body", "blueberry pie"));
+        doc2.add(Field.Text("body", "blueberry strudel"));
+        doc3.add(Field.Text("body", "blueberry pizza"));
+        doc4.add(Field.Text("body", "blueberry chewing gum"));
+        doc5.add(Field.Text("body", "piccadilly circus"));
+        writer.addDocument(doc1);
+        writer.addDocument(doc2);
+        writer.addDocument(doc3);
+        writer.addDocument(doc4);
+        writer.addDocument(doc5);
+        writer.optimize();
+        writer.close();
+
+        IndexSearcher searcher = new IndexSearcher(indexStore);
+
+        PhrasePrefixQuery query1 = new PhrasePrefixQuery();
+        PhrasePrefixQuery query2 = new PhrasePrefixQuery();
+        query1.add(new Term("body", "blueberry"));
+        query2.add(new Term("body", "strawberry"));
+
+        LinkedList termsWithPrefix = new LinkedList();
+        IndexReader ir = IndexReader.open(indexStore);
+
+        // this TermEnum gives "piccadilly", "pie" and "pizza".
+        String prefix = "pi";
+        TermEnum te = ir.terms(new Term("body", prefix + "*"));
+        do {
+            if (te.term().text().startsWith(prefix))
+            {
+                termsWithPrefix.add(te.term());
+            }
+        } while (te.next());
+
+        query1.add((Term[])termsWithPrefix.toArray(new Term[0]));
+        query2.add((Term[])termsWithPrefix.toArray(new Term[0]));
+
+        Hits result;
+        result = searcher.search(query1);
+        assertEquals(2, result.length());
+
+        result = searcher.search(query2);
+        assertEquals(0, result.length());
+    }
+}
diff --git a/src/test-deprecated/org/apache/lucene/search/TestPhraseQuery.java b/src/test-deprecated/org/apache/lucene/search/TestPhraseQuery.java
new file mode 100644
index 0000000..fb98217
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/search/TestPhraseQuery.java
@@ -0,0 +1,257 @@
+package org.apache.lucene.search;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import junit.framework.TestCase;
+import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.StopAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.store.RAMDirectory;
+
+/**
+ * Tests {@link PhraseQuery}.
+ *
+ * @see TestPositionIncrement
+ * @author Erik Hatcher
+ */
+public class TestPhraseQuery extends TestCase {
+  private IndexSearcher searcher;
+  private PhraseQuery query;
+  private RAMDirectory directory;
+
+  public void setUp() throws Exception {
+    directory = new RAMDirectory();
+    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true);
+    
+    Document doc = new Document();
+    doc.add(Field.Text("field", "one two three four five"));
+    writer.addDocument(doc);
+    
+    writer.optimize();
+    writer.close();
+
+    searcher = new IndexSearcher(directory);
+    query = new PhraseQuery();
+  }
+
+  public void tearDown() throws Exception {
+    searcher.close();
+    directory.close();
+  }
+
+  public void testNotCloseEnough() throws Exception {
+    query.setSlop(2);
+    query.add(new Term("field", "one"));
+    query.add(new Term("field", "five"));
+    Hits hits = searcher.search(query);
+    assertEquals(0, hits.length());
+  }
+
+  public void testBarelyCloseEnough() throws Exception {
+    query.setSlop(3);
+    query.add(new Term("field", "one"));
+    query.add(new Term("field", "five"));
+    Hits hits = searcher.search(query);
+    assertEquals(1, hits.length());
+  }
+
+  /**
+   * Ensures slop of 0 works for exact matches, but not reversed
+   */
+  public void testExact() throws Exception {
+    // slop is zero by default
+    query.add(new Term("field", "four"));
+    query.add(new Term("field", "five"));
+    Hits hits = searcher.search(query);
+    assertEquals("exact match", 1, hits.length());
+
+    query = new PhraseQuery();
+    query.add(new Term("field", "two"));
+    query.add(new Term("field", "one"));
+    hits = searcher.search(query);
+    assertEquals("reverse not exact", 0, hits.length());
+  }
+
+  public void testSlop1() throws Exception {
+    // Ensures slop of 1 works with terms in order.
+    query.setSlop(1);
+    query.add(new Term("field", "one"));
+    query.add(new Term("field", "two"));
+    Hits hits = searcher.search(query);
+    assertEquals("in order", 1, hits.length());
+
+    // Ensures slop of 1 does not work for phrases out of order;
+    // must be at least 2.
+    query = new PhraseQuery();
+    query.setSlop(1);
+    query.add(new Term("field", "two"));
+    query.add(new Term("field", "one"));
+    hits = searcher.search(query);
+    assertEquals("reversed, slop not 2 or more", 0, hits.length());
+  }
+
+  /**
+   * As long as slop is at least 2, terms can be reversed
+   */
+  public void testOrderDoesntMatter() throws Exception {
+    query.setSlop(2); // must be at least two for reverse order match
+    query.add(new Term("field", "two"));
+    query.add(new Term("field", "one"));
+    Hits hits = searcher.search(query);
+    assertEquals("just sloppy enough", 1, hits.length());
+
+    query = new PhraseQuery();
+    query.setSlop(2);
+    query.add(new Term("field", "three"));
+    query.add(new Term("field", "one"));
+    hits = searcher.search(query);
+    assertEquals("not sloppy enough", 0, hits.length());
+  }
+
+  /**
+   * slop is the total number of positional moves allowed
+   * to line up a phrase
+   */
+  public void testMulipleTerms() throws Exception {
+    query.setSlop(2);
+    query.add(new Term("field", "one"));
+    query.add(new Term("field", "three"));
+    query.add(new Term("field", "five"));
+    Hits hits = searcher.search(query);
+    assertEquals("two total moves", 1, hits.length());
+
+    query = new PhraseQuery();
+    query.setSlop(5); // it takes six moves to match this phrase
+    query.add(new Term("field", "five"));
+    query.add(new Term("field", "three"));
+    query.add(new Term("field", "one"));
+    hits = searcher.search(query);
+    assertEquals("slop of 5 not close enough", 0, hits.length());
+
+    query.setSlop(6);
+    hits = searcher.search(query);
+    assertEquals("slop of 6 just right", 1, hits.length());
+  }
+  
+  public void testPhraseQueryWithStopAnalyzer() throws Exception {
+    RAMDirectory directory = new RAMDirectory();
+    StopAnalyzer stopAnalyzer = new StopAnalyzer();
+    IndexWriter writer = new IndexWriter(directory, stopAnalyzer, true);
+    Document doc = new Document();
+    doc.add(Field.Text("field", "the stop words are here"));
+    writer.addDocument(doc);
+    writer.close();
+
+    IndexSearcher searcher = new IndexSearcher(directory);
+
+    // valid exact phrase query
+    PhraseQuery query = new PhraseQuery();
+    query.add(new Term("field","stop"));
+    query.add(new Term("field","words"));
+    Hits hits = searcher.search(query);
+    assertEquals(1, hits.length());
+
+    // currently StopAnalyzer does not leave "holes", so this matches.
+    query = new PhraseQuery();
+    query.add(new Term("field", "words"));
+    query.add(new Term("field", "here"));
+    hits = searcher.search(query);
+    assertEquals(1, hits.length());
+
+    searcher.close();
+  }
+  
+  public void testPhraseQueryInConjunctionScorer() throws Exception {
+    RAMDirectory directory = new RAMDirectory();
+    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true);
+    
+    Document doc = new Document();
+    doc.add(new Field("source", "marketing info", true, true, true));
+    writer.addDocument(doc);
+    
+    doc = new Document();
+    doc.add(new Field("contents", "foobar", true, true, true));
+    doc.add(new Field("source", "marketing info", true, true, true)); 
+    writer.addDocument(doc);
+    
+    writer.optimize();
+    writer.close();
+    
+    IndexSearcher searcher = new IndexSearcher(directory);
+    
+    PhraseQuery phraseQuery = new PhraseQuery();
+    phraseQuery.add(new Term("source", "marketing"));
+    phraseQuery.add(new Term("source", "info"));
+    Hits hits = searcher.search(phraseQuery);
+    assertEquals(2, hits.length());
+    
+    TermQuery termQuery = new TermQuery(new Term("contents","foobar"));
+    BooleanQuery booleanQuery = new BooleanQuery();
+    booleanQuery.add(termQuery, true, false);
+    booleanQuery.add(phraseQuery, true, false);
+    hits = searcher.search(booleanQuery);
+    assertEquals(1, hits.length());
+    
+    searcher.close();
+    
+    writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true);
+    doc = new Document();
+    doc.add(new Field("contents", "map entry woo", true, true, true));
+    writer.addDocument(doc);
+
+    doc = new Document();
+    doc.add(new Field("contents", "woo map entry", true, true, true));
+    writer.addDocument(doc);
+
+    doc = new Document();
+    doc.add(new Field("contents", "map foobarword entry woo", true, true, true));
+    writer.addDocument(doc);
+
+    writer.optimize();
+    writer.close();
+    
+    searcher = new IndexSearcher(directory);
+    
+    termQuery = new TermQuery(new Term("contents","woo"));
+    phraseQuery = new PhraseQuery();
+    phraseQuery.add(new Term("contents","map"));
+    phraseQuery.add(new Term("contents","entry"));
+    
+    hits = searcher.search(termQuery);
+    assertEquals(3, hits.length());
+    hits = searcher.search(phraseQuery);
+    assertEquals(2, hits.length());
+    
+    booleanQuery = new BooleanQuery();
+    booleanQuery.add(termQuery, true, false);
+    booleanQuery.add(phraseQuery, true, false);
+    hits = searcher.search(booleanQuery);
+    assertEquals(2, hits.length());
+    
+    booleanQuery = new BooleanQuery();
+    booleanQuery.add(phraseQuery, true, false);
+    booleanQuery.add(termQuery, true, false);
+    hits = searcher.search(booleanQuery);
+    assertEquals(2, hits.length());
+    
+    searcher.close();
+    directory.close();
+  }
+}
diff --git a/src/test-deprecated/org/apache/lucene/search/TestPositionIncrement.java b/src/test-deprecated/org/apache/lucene/search/TestPositionIncrement.java
new file mode 100644
index 0000000..5e78ac8
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/search/TestPositionIncrement.java
@@ -0,0 +1,135 @@
+package org.apache.lucene.search;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.search.PhraseQuery;
+import org.apache.lucene.search.Hits;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.Token;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+
+import java.io.Reader;
+import java.io.IOException;
+import java.io.StringReader;
+
+import junit.framework.TestCase;
+
+/**
+ * Term position unit test.
+ *
+ * @author Doug Cutting
+ * @version $Revision$
+ */
+public class TestPositionIncrement extends TestCase {
+
+  public void testSetPosition() throws Exception {
+    Analyzer analyzer = new Analyzer() {
+      public TokenStream tokenStream(String fieldName, Reader reader) {
+        return new TokenStream() {
+          private final String[] TOKENS = {"1", "2", "3", "4", "5"};
+          private final int[] INCREMENTS = {1, 2, 1, 0, 1};
+          private int i = 0;
+
+          public Token next() throws IOException {
+            if (i == TOKENS.length)
+              return null;
+            Token t = new Token(TOKENS[i], i, i);
+            t.setPositionIncrement(INCREMENTS[i]);
+            i++;
+            return t;
+          }
+        };
+      }
+    };
+    RAMDirectory store = new RAMDirectory();
+    IndexWriter writer = new IndexWriter(store, analyzer, true);
+    Document d = new Document();
+    d.add(Field.Text("field", "bogus"));
+    writer.addDocument(d);
+    writer.optimize();
+    writer.close();
+
+    IndexSearcher searcher = new IndexSearcher(store);
+    PhraseQuery q;
+    Hits hits;
+
+    q = new PhraseQuery();
+    q.add(new Term("field", "1"));
+    q.add(new Term("field", "2"));
+    hits = searcher.search(q);
+    assertEquals(0, hits.length());
+
+    q = new PhraseQuery();
+    q.add(new Term("field", "2"));
+    q.add(new Term("field", "3"));
+    hits = searcher.search(q);
+    assertEquals(1, hits.length());
+
+    q = new PhraseQuery();
+    q.add(new Term("field", "3"));
+    q.add(new Term("field", "4"));
+    hits = searcher.search(q);
+    assertEquals(0, hits.length());
+
+    q = new PhraseQuery();
+    q.add(new Term("field", "2"));
+    q.add(new Term("field", "4"));
+    hits = searcher.search(q);
+    assertEquals(1, hits.length());
+
+    q = new PhraseQuery();
+    q.add(new Term("field", "3"));
+    q.add(new Term("field", "5"));
+    hits = searcher.search(q);
+    assertEquals(1, hits.length());
+
+    q = new PhraseQuery();
+    q.add(new Term("field", "4"));
+    q.add(new Term("field", "5"));
+    hits = searcher.search(q);
+    assertEquals(1, hits.length());
+
+    q = new PhraseQuery();
+    q.add(new Term("field", "2"));
+    q.add(new Term("field", "5"));
+    hits = searcher.search(q);
+    assertEquals(0, hits.length());
+  }
+
+  /**
+   * Basic analyzer behavior should be to keep sequential terms in one
+   * increment from one another.
+   */
+  public void testIncrementingPositions() throws Exception {
+    Analyzer analyzer = new WhitespaceAnalyzer();
+    TokenStream ts = analyzer.tokenStream("field",
+                                new StringReader("one two three four five"));
+
+    while (true) {
+      Token token = ts.next();
+      if (token == null) break;
+      assertEquals(token.termText(), 1, token.getPositionIncrement());
+    }
+  }
+}
diff --git a/src/test-deprecated/org/apache/lucene/search/TestPrefixQuery.java b/src/test-deprecated/org/apache/lucene/search/TestPrefixQuery.java
new file mode 100644
index 0000000..f628600
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/search/TestPrefixQuery.java
@@ -0,0 +1,56 @@
+package org.apache.lucene.search;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import junit.framework.TestCase;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+
+/**
+ * Tests {@link PrefixQuery} class.
+ *
+ * @author Erik Hatcher
+ */
+public class TestPrefixQuery extends TestCase {
+  public void testPrefixQuery() throws Exception {
+    RAMDirectory directory = new RAMDirectory();
+
+    String[] categories = new String[] {"/Computers",
+                                        "/Computers/Mac",
+                                        "/Computers/Windows"};
+    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true);
+    for (int i = 0; i < categories.length; i++) {
+      Document doc = new Document();
+      doc.add(Field.Keyword("category", categories[i]));
+      writer.addDocument(doc);
+    }
+    writer.close();
+
+    PrefixQuery query = new PrefixQuery(new Term("category", "/Computers"));
+    IndexSearcher searcher = new IndexSearcher(directory);
+    Hits hits = searcher.search(query);
+    assertEquals("All documents in /Computers category and below", 3, hits.length());
+
+    query = new PrefixQuery(new Term("category", "/Computers/Mac"));
+    hits = searcher.search(query);
+    assertEquals("One in /Computers/Mac", 1, hits.length());
+  }
+}
diff --git a/src/test-deprecated/org/apache/lucene/search/TestQueryTermVector.java b/src/test-deprecated/org/apache/lucene/search/TestQueryTermVector.java
new file mode 100644
index 0000000..e47274e
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/search/TestQueryTermVector.java
@@ -0,0 +1,66 @@
+package org.apache.lucene.search;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import junit.framework.TestCase;
+import org.apache.lucene.analysis.WhitespaceAnalyzer;
+
+public class TestQueryTermVector extends TestCase {
+
+
+  public TestQueryTermVector(String s) {
+    super(s);
+  }
+
+  protected void setUp() {
+  }
+
+  protected void tearDown() {
+
+  }
+
+  public void testConstructor() {
+    String [] queryTerm = {"foo", "bar", "foo", "again", "foo", "bar", "go", "go", "go"};
+    //Items are sorted lexicographically
+    String [] gold = {"again", "bar", "foo", "go"};
+    int [] goldFreqs = {1, 2, 3, 3};
+    QueryTermVector result = new QueryTermVector(queryTerm);
+    assertTrue(result != null);
+    String [] terms = result.getTerms();
+    assertTrue(terms.length == 4);
+    int [] freq = result.getTermFrequencies();
+    assertTrue(freq.length == 4);
+    checkGold(terms, gold, freq, goldFreqs);
+    result = new QueryTermVector(null);
+    assertTrue(result.getTerms().length == 0);
+    
+    result = new QueryTermVector("foo bar foo again foo bar go go go", new WhitespaceAnalyzer());
+    assertTrue(result != null);
+    terms = result.getTerms();
+    assertTrue(terms.length == 4);
+    freq = result.getTermFrequencies();
+    assertTrue(freq.length == 4);
+    checkGold(terms, gold, freq, goldFreqs);
+  }
+
+  private void checkGold(String[] terms, String[] gold, int[] freq, int[] goldFreqs) {
+    for (int i = 0; i < terms.length; i++) {
+      assertTrue(terms[i].equals(gold[i]));
+      assertTrue(freq[i] == goldFreqs[i]);
+    }
+  }
+}
diff --git a/src/test-deprecated/org/apache/lucene/search/TestRangeQuery.java b/src/test-deprecated/org/apache/lucene/search/TestRangeQuery.java
new file mode 100644
index 0000000..c685ab6
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/search/TestRangeQuery.java
@@ -0,0 +1,96 @@
+package org.apache.lucene.search;
+
+import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.store.RAMDirectory;
+
+import junit.framework.TestCase;
+import java.io.IOException;
+
+/**
+ * @author goller
+ */
+public class TestRangeQuery extends TestCase {
+
+  private int docCount = 0;
+  private RAMDirectory dir;
+
+  public void setUp() {
+    dir = new RAMDirectory();
+  }
+
+  public void testExclusive() throws Exception {
+    Query query = new RangeQuery(new Term("content", "A"),
+                                 new Term("content", "C"),
+                                 false);
+    initializeIndex(new String[] {"A", "B", "C", "D"});
+    IndexSearcher searcher = new IndexSearcher(dir);
+    Hits hits = searcher.search(query);
+    assertEquals("A,B,C,D, only B in range", 1, hits.length());
+    searcher.close();
+
+    initializeIndex(new String[] {"A", "B", "D"});
+    searcher = new IndexSearcher(dir);
+    hits = searcher.search(query);
+    assertEquals("A,B,D, only B in range", 1, hits.length());
+    searcher.close();
+
+    addDoc("C");
+    searcher = new IndexSearcher(dir);
+    hits = searcher.search(query);
+    assertEquals("C added, still only B in range", 1, hits.length());
+    searcher.close();
+  }
+
+  public void testInclusive() throws Exception {
+    Query query = new RangeQuery(new Term("content", "A"),
+                                 new Term("content", "C"),
+                                 true);
+
+    initializeIndex(new String[]{"A", "B", "C", "D"});
+    IndexSearcher searcher = new IndexSearcher(dir);
+    Hits hits = searcher.search(query);
+    assertEquals("A,B,C,D - A,B,C in range", 3, hits.length());
+    searcher.close();
+
+    initializeIndex(new String[]{"A", "B", "D"});
+    searcher = new IndexSearcher(dir);
+    hits = searcher.search(query);
+    assertEquals("A,B,D - A and B in range", 2, hits.length());
+    searcher.close();
+
+    addDoc("C");
+    searcher = new IndexSearcher(dir);
+    hits = searcher.search(query);
+    assertEquals("C added - A, B, C in range", 3, hits.length());
+    searcher.close();
+  }
+
+  private void initializeIndex(String[] values) throws IOException {
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true);
+    for (int i = 0; i < values.length; i++) {
+      insertDoc(writer, values[i]);
+    }
+    writer.close();
+  }
+
+  private void addDoc(String content) throws IOException {
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false);
+    insertDoc(writer, content);
+    writer.close();
+  }
+
+  private void insertDoc(IndexWriter writer, String content) throws IOException {
+    Document doc = new Document();
+
+    doc.add(Field.Keyword("id", "id" + docCount));
+    doc.add(Field.UnStored("content", content));
+
+    writer.addDocument(doc);
+    docCount++;
+  }
+}
+
diff --git a/src/test-deprecated/org/apache/lucene/search/TestRemoteSearchable.java b/src/test-deprecated/org/apache/lucene/search/TestRemoteSearchable.java
new file mode 100644
index 0000000..8a1b8d1
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/search/TestRemoteSearchable.java
@@ -0,0 +1,109 @@
+package org.apache.lucene.search;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import junit.framework.TestCase;
+
+import java.rmi.Naming;
+import java.rmi.registry.LocateRegistry;
+
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.analysis.SimpleAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+
+/**
+ * @version $Id$
+ */
+public class TestRemoteSearchable extends TestCase {
+  public TestRemoteSearchable(String name) {
+    super(name);
+  }
+
+  private static Searchable getRemote() throws Exception {
+    try {
+      return lookupRemote();
+    } catch (Throwable e) {
+      startServer();
+      return lookupRemote();
+    }
+  }
+
+  private static Searchable lookupRemote() throws Exception {
+    return (Searchable)Naming.lookup("//localhost/Searchable");
+  }
+
+  private static void startServer() throws Exception {
+    // construct an index
+    RAMDirectory indexStore = new RAMDirectory();
+    IndexWriter writer = new IndexWriter(indexStore,new SimpleAnalyzer(),true);
+    Document doc = new Document();
+    doc.add(Field.Text("test", "test text"));
+    writer.addDocument(doc);
+    writer.optimize();
+    writer.close();
+
+    // publish it
+    LocateRegistry.createRegistry(1099);
+    Searchable local = new IndexSearcher(indexStore);
+    RemoteSearchable impl = new RemoteSearchable(local);
+    Naming.rebind("//localhost/Searchable", impl);
+  }
+
+  private static void search(Query query) throws Exception {
+    // try to search the published index
+    Searchable[] searchables = { getRemote() };
+    Searcher searcher = new MultiSearcher(searchables);
+    Hits result = searcher.search(query);
+
+    assertEquals(1, result.length());
+    assertEquals("test text", result.doc(0).get("test"));
+  }
+
+  public void testTermQuery() throws Exception {
+    search(new TermQuery(new Term("test", "test")));
+  }
+
+  public void testBooleanQuery() throws Exception {
+    BooleanQuery query = new BooleanQuery();
+    query.add(new TermQuery(new Term("test", "test")), true, false);
+    search(query);
+  }
+
+  public void testPhraseQuery() throws Exception {
+    PhraseQuery query = new PhraseQuery();
+    query.add(new Term("test", "test"));
+    query.add(new Term("test", "text"));
+    search(query);
+  }
+
+  // Tests bug fix at http://nagoya.apache.org/bugzilla/show_bug.cgi?id=20290
+  public void testQueryFilter() throws Exception {
+    // try to search the published index
+    Searchable[] searchables = { getRemote() };
+    Searcher searcher = new MultiSearcher(searchables);
+    Hits hits = searcher.search(
+          new TermQuery(new Term("test", "text")),
+          new QueryFilter(new TermQuery(new Term("test", "test"))));
+    Hits nohits = searcher.search(
+          new TermQuery(new Term("test", "text")),
+          new QueryFilter(new TermQuery(new Term("test", "non-existent-term"))));
+    assertEquals(0, nohits.length());
+  }
+}
diff --git a/src/test-deprecated/org/apache/lucene/search/TestSetNorm.java b/src/test-deprecated/org/apache/lucene/search/TestSetNorm.java
new file mode 100644
index 0000000..a11b526
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/search/TestSetNorm.java
@@ -0,0 +1,80 @@
+package org.apache.lucene.search;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import junit.framework.TestCase;
+
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.analysis.SimpleAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+
+/** Document boost unit test.
+ *
+ * @author Doug Cutting
+ * @version $Revision$
+ */
+public class TestSetNorm extends TestCase {
+  public TestSetNorm(String name) {
+    super(name);
+  }
+  
+  public void testSetNorm() throws Exception {
+    RAMDirectory store = new RAMDirectory();
+    IndexWriter writer = new IndexWriter(store, new SimpleAnalyzer(), true);
+    
+    // add the same document four times
+    Field f1 = Field.Text("field", "word");
+    Document d1 = new Document();
+    d1.add(f1);
+    writer.addDocument(d1);
+    writer.addDocument(d1);
+    writer.addDocument(d1);
+    writer.addDocument(d1);
+    writer.close();
+
+    // reset the boost of each instance of this document
+    IndexReader reader = IndexReader.open(store);
+    reader.setNorm(0, "field", 1.0f);
+    reader.setNorm(1, "field", 2.0f);
+    reader.setNorm(2, "field", 4.0f);
+    reader.setNorm(3, "field", 16.0f);
+    reader.close();
+
+    // check that searches are ordered by this boost
+    final float[] scores = new float[4];
+
+    new IndexSearcher(store).search
+      (new TermQuery(new Term("field", "word")),
+       new HitCollector() {
+         public final void collect(int doc, float score) {
+           scores[doc] = score;
+         }
+       });
+    
+    float lastScore = 0.0f;
+
+    for (int i = 0; i < 4; i++) {
+      assertTrue(scores[i] > lastScore);
+      lastScore = scores[i];
+    }
+  }
+}
diff --git a/src/test-deprecated/org/apache/lucene/search/TestSimilarity.java b/src/test-deprecated/org/apache/lucene/search/TestSimilarity.java
new file mode 100644
index 0000000..72a0419
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/search/TestSimilarity.java
@@ -0,0 +1,121 @@
+package org.apache.lucene.search;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import junit.framework.TestCase;
+
+import java.util.Collection;
+
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.analysis.SimpleAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+
+/** Similarity unit test.
+ *
+ * @author Doug Cutting
+ * @version $Revision$
+ */
+public class TestSimilarity extends TestCase {
+  public TestSimilarity(String name) {
+    super(name);
+  }
+  
+  public static class SimpleSimilarity extends Similarity {
+    public float lengthNorm(String field, int numTerms) { return 1.0f; }
+    public float queryNorm(float sumOfSquaredWeights) { return 1.0f; }
+    public float tf(float freq) { return freq; }
+    public float sloppyFreq(int distance) { return 2.0f; }
+    public float idf(Collection terms, Searcher searcher) { return 1.0f; }
+    public float idf(int docFreq, int numDocs) { return 1.0f; }
+    public float coord(int overlap, int maxOverlap) { return 1.0f; }
+  }
+
+  public void testSimilarity() throws Exception {
+    RAMDirectory store = new RAMDirectory();
+    IndexWriter writer = new IndexWriter(store, new SimpleAnalyzer(), true);
+    writer.setSimilarity(new SimpleSimilarity());
+    
+    Document d1 = new Document();
+    d1.add(Field.Text("field", "a c"));
+
+    Document d2 = new Document();
+    d2.add(Field.Text("field", "a b c"));
+    
+    writer.addDocument(d1);
+    writer.addDocument(d2);
+    writer.optimize();
+    writer.close();
+
+    final float[] scores = new float[4];
+
+    Searcher searcher = new IndexSearcher(store);
+    searcher.setSimilarity(new SimpleSimilarity());
+
+    Term a = new Term("field", "a");
+    Term b = new Term("field", "b");
+    Term c = new Term("field", "c");
+
+    searcher.search
+      (new TermQuery(b),
+       new HitCollector() {
+         public final void collect(int doc, float score) {
+           assertTrue(score == 1.0f);
+         }
+       });
+
+    BooleanQuery bq = new BooleanQuery();
+    bq.add(new TermQuery(a), false, false);
+    bq.add(new TermQuery(b), false, false);
+    //System.out.println(bq.toString("field"));
+    searcher.search
+      (bq,
+       new HitCollector() {
+         public final void collect(int doc, float score) {
+           //System.out.println("Doc=" + doc + " score=" + score);
+           assertTrue(score == (float)doc+1);
+         }
+       });
+
+    PhraseQuery pq = new PhraseQuery();
+    pq.add(a);
+    pq.add(c);
+    //System.out.println(pq.toString("field"));
+    searcher.search
+      (pq,
+       new HitCollector() {
+         public final void collect(int doc, float score) {
+           //System.out.println("Doc=" + doc + " score=" + score);
+           assertTrue(score == 1.0f);
+         }
+       });
+
+    pq.setSlop(2);
+    //System.out.println(pq.toString("field"));
+    searcher.search
+      (pq,
+       new HitCollector() {
+         public final void collect(int doc, float score) {
+           //System.out.println("Doc=" + doc + " score=" + score);
+           assertTrue(score == 2.0f);
+         }
+       });
+  }
+}
diff --git a/src/test-deprecated/org/apache/lucene/search/TestSort.java b/src/test-deprecated/org/apache/lucene/search/TestSort.java
new file mode 100644
index 0000000..1c3ec13
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/search/TestSort.java
@@ -0,0 +1,588 @@
+package org.apache.lucene.search;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.index.*;
+import org.apache.lucene.analysis.SimpleAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+
+import java.rmi.Naming;
+import java.rmi.registry.LocateRegistry;
+import java.rmi.registry.Registry;
+import java.io.IOException;
+import java.io.Serializable;
+import java.util.regex.Pattern;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.Locale;
+
+import junit.framework.TestCase;
+import junit.framework.Test;
+import junit.framework.TestSuite;
+import junit.textui.TestRunner;
+
+/**
+ * Unit tests for sorting code.
+ *
+ * <p>Created: Feb 17, 2004 4:55:10 PM
+ *
+ * @author  Tim Jones (Nacimiento Software)
+ * @since   lucene 1.4
+ * @version $Id$
+ */
+
+public class TestSort
+extends TestCase
+implements Serializable {
+
+	private Searcher full;
+	private Searcher searchX;
+	private Searcher searchY;
+	private Query queryX;
+	private Query queryY;
+	private Query queryA;
+	private Query queryF;
+	private Sort sort;
+
+
+	public TestSort (String name) {
+		super (name);
+	}
+
+	public static void main (String[] argv) {
+		if (argv == null || argv.length < 1)
+			TestRunner.run (suite());
+		else if ("server".equals (argv[0])) {
+			TestSort test = new TestSort (null);
+			try {
+				test.startServer();
+				Thread.sleep (500000);
+			} catch (Exception e) {
+				System.out.println (e);
+				e.printStackTrace();
+			}
+		}
+	}
+
+	public static Test suite() {
+		return new TestSuite (TestSort.class);
+	}
+
+
+	// document data:
+	// the tracer field is used to determine which document was hit
+	// the contents field is used to search and sort by relevance
+	// the int field to sort by int
+	// the float field to sort by float
+	// the string field to sort by string
+	private String[][] data = new String[][] {
+	// tracer  contents         int            float           string   custom
+	{   "A",   "x a",           "5",           "4f",           "c",     "A-3"   },
+	{   "B",   "y a",           "5",           "3.4028235E38", "i",     "B-10"  },
+	{   "C",   "x a b c",       "2147483647",  "1.0",          "j",     "A-2"   },
+	{   "D",   "y a b c",       "-1",          "0.0f",         "a",     "C-0"   },
+	{   "E",   "x a b c d",     "5",           "2f",           "h",     "B-8"   },
+	{   "F",   "y a b c d",     "2",           "3.14159f",     "g",     "B-1"   },
+	{   "G",   "x a b c d",     "3",           "-1.0",         "f",     "C-100" },
+	{   "H",   "y a b c d",     "0",           "1.4E-45",      "e",     "C-88"  },
+	{   "I",   "x a b c d e f", "-2147483648", "1.0e+0",       "d",     "A-10"  },
+	{   "J",   "y a b c d e f", "4",           ".5",           "b",     "C-7"   },
+	{   "Z",   "f",             null,          null,           null,    null    }
+	};
+
+	// create an index of all the documents, or just the x, or just the y documents
+	private Searcher getIndex (boolean even, boolean odd)
+	throws IOException {
+		RAMDirectory indexStore = new RAMDirectory ();
+		IndexWriter writer = new IndexWriter (indexStore, new SimpleAnalyzer(), true);
+		for (int i=0; i<data.length; ++i) {
+			if (((i%2)==0 && even) || ((i%2)==1 && odd)) {
+				Document doc = new Document();          // store, index, token
+				doc.add (new Field ("tracer",   data[i][0], true, false, false));
+				doc.add (new Field ("contents", data[i][1], false, true, true));
+				if (data[i][2] != null) doc.add (new Field ("int",      data[i][2], false, true, false));
+				if (data[i][3] != null) doc.add (new Field ("float",    data[i][3], false, true, false));
+				if (data[i][4] != null) doc.add (new Field ("string",   data[i][4], false, true, false));
+				if (data[i][5] != null) doc.add (new Field ("custom",   data[i][5], false, true, false));
+				writer.addDocument (doc);
+			}
+		}
+		writer.optimize ();
+		writer.close ();
+		return new IndexSearcher (indexStore);
+	}
+
+	private Searcher getFullIndex()
+	throws IOException {
+		return getIndex (true, true);
+	}
+
+	private Searcher getXIndex()
+	throws IOException {
+		return getIndex (true, false);
+	}
+
+	private Searcher getYIndex()
+	throws IOException {
+		return getIndex (false, true);
+	}
+
+	private Searcher getEmptyIndex()
+	throws IOException {
+		return getIndex (false, false);
+	}
+
+	public void setUp() throws Exception {
+		full = getFullIndex();
+		searchX = getXIndex();
+		searchY = getYIndex();
+		queryX = new TermQuery (new Term ("contents", "x"));
+		queryY = new TermQuery (new Term ("contents", "y"));
+		queryA = new TermQuery (new Term ("contents", "a"));
+		queryF = new TermQuery (new Term ("contents", "f"));
+		sort = new Sort();
+	}
+
+	// test the sorts by score and document number
+	public void testBuiltInSorts() throws Exception {
+		sort = new Sort();
+		assertMatches (full, queryX, sort, "ACEGI");
+		assertMatches (full, queryY, sort, "BDFHJ");
+
+		sort.setSort(SortField.FIELD_DOC);
+		assertMatches (full, queryX, sort, "ACEGI");
+		assertMatches (full, queryY, sort, "BDFHJ");
+	}
+
+	// test sorts where the type of field is specified
+	public void testTypedSort() throws Exception {
+		sort.setSort (new SortField[] { new SortField ("int", SortField.INT), SortField.FIELD_DOC });
+		assertMatches (full, queryX, sort, "IGAEC");
+		assertMatches (full, queryY, sort, "DHFJB");
+
+		sort.setSort (new SortField[] { new SortField ("float", SortField.FLOAT), SortField.FIELD_DOC });
+		assertMatches (full, queryX, sort, "GCIEA");
+		assertMatches (full, queryY, sort, "DHJFB");
+
+		sort.setSort (new SortField[] { new SortField ("string", SortField.STRING), SortField.FIELD_DOC });
+		assertMatches (full, queryX, sort, "AIGEC");
+		assertMatches (full, queryY, sort, "DJHFB");
+	}
+
+	// test sorts when there's nothing in the index
+	public void testEmptyIndex() throws Exception {
+		Searcher empty = getEmptyIndex();
+
+		sort = new Sort();
+		assertMatches (empty, queryX, sort, "");
+
+		sort.setSort(SortField.FIELD_DOC);
+		assertMatches (empty, queryX, sort, "");
+
+		sort.setSort (new SortField[] { new SortField ("int", SortField.INT), SortField.FIELD_DOC });
+		assertMatches (empty, queryX, sort, "");
+
+		sort.setSort (new SortField[] { new SortField ("string", SortField.STRING, true), SortField.FIELD_DOC });
+		assertMatches (empty, queryX, sort, "");
+
+		sort.setSort (new SortField[] { new SortField ("float", SortField.FLOAT), new SortField ("string", SortField.STRING) });
+		assertMatches (empty, queryX, sort, "");
+	}
+
+	// test sorts where the type of field is determined dynamically
+	public void testAutoSort() throws Exception {
+		sort.setSort("int");
+		assertMatches (full, queryX, sort, "IGAEC");
+		assertMatches (full, queryY, sort, "DHFJB");
+
+		sort.setSort("float");
+		assertMatches (full, queryX, sort, "GCIEA");
+		assertMatches (full, queryY, sort, "DHJFB");
+
+		sort.setSort("string");
+		assertMatches (full, queryX, sort, "AIGEC");
+		assertMatches (full, queryY, sort, "DJHFB");
+	}
+
+	// test sorts in reverse
+	public void testReverseSort() throws Exception {
+		sort.setSort (new SortField[] { new SortField (null, SortField.SCORE, true), SortField.FIELD_DOC });
+		assertMatches (full, queryX, sort, "IEGCA");
+		assertMatches (full, queryY, sort, "JFHDB");
+
+		sort.setSort (new SortField (null, SortField.DOC, true));
+		assertMatches (full, queryX, sort, "IGECA");
+		assertMatches (full, queryY, sort, "JHFDB");
+
+		sort.setSort ("int", true);
+		assertMatches (full, queryX, sort, "CAEGI");
+		assertMatches (full, queryY, sort, "BJFHD");
+
+		sort.setSort ("float", true);
+		assertMatches (full, queryX, sort, "AECIG");
+		assertMatches (full, queryY, sort, "BFJHD");
+
+		sort.setSort ("string", true);
+		assertMatches (full, queryX, sort, "CEGIA");
+		assertMatches (full, queryY, sort, "BFHJD");
+	}
+
+	// test sorting when the sort field is empty (undefined) for some of the documents
+	public void testEmptyFieldSort() throws Exception {
+		sort.setSort ("string");
+		assertMatches (full, queryF, sort, "ZJI");
+
+		sort.setSort ("string", true);
+		assertMatches (full, queryF, sort, "IJZ");
+
+		sort.setSort ("int");
+		assertMatches (full, queryF, sort, "IZJ");
+
+		sort.setSort ("int", true);
+		assertMatches (full, queryF, sort, "JZI");
+
+		sort.setSort ("float");
+		assertMatches (full, queryF, sort, "ZJI");
+
+		sort.setSort ("float", true);
+		assertMatches (full, queryF, sort, "IJZ");
+	}
+
+	// test sorts using a series of fields
+	public void testSortCombos() throws Exception {
+		sort.setSort (new String[] {"int","float"});
+		assertMatches (full, queryX, sort, "IGEAC");
+
+		sort.setSort (new SortField[] { new SortField ("int", true), new SortField (null, SortField.DOC, true) });
+		assertMatches (full, queryX, sort, "CEAGI");
+
+		sort.setSort (new String[] {"float","string"});
+		assertMatches (full, queryX, sort, "GICEA");
+	}
+
+	// test using a Locale for sorting strings
+	public void testLocaleSort() throws Exception {
+		sort.setSort (new SortField[] { new SortField ("string", Locale.US) });
+		assertMatches (full, queryX, sort, "AIGEC");
+		assertMatches (full, queryY, sort, "DJHFB");
+
+		sort.setSort (new SortField[] { new SortField ("string", Locale.US, true) });
+		assertMatches (full, queryX, sort, "CEGIA");
+		assertMatches (full, queryY, sort, "BFHJD");
+	}
+
+	// test a custom sort function
+	public void testCustomSorts() throws Exception {
+		sort.setSort (new SortField ("custom", SampleComparable.getComparatorSource()));
+		assertMatches (full, queryX, sort, "CAIEG");
+		sort.setSort (new SortField ("custom", SampleComparable.getComparatorSource(), true));
+		assertMatches (full, queryY, sort, "HJDBF");
+		SortComparator custom = SampleComparable.getComparator();
+		sort.setSort (new SortField ("custom", custom));
+		assertMatches (full, queryX, sort, "CAIEG");
+		sort.setSort (new SortField ("custom", custom, true));
+		assertMatches (full, queryY, sort, "HJDBF");
+	}
+
+	// test a variety of sorts using more than one searcher
+	public void testMultiSort() throws Exception {
+		MultiSearcher searcher = new MultiSearcher (new Searchable[] { searchX, searchY });
+		runMultiSorts (searcher);
+	}
+
+	// test a variety of sorts using a parallel multisearcher
+	public void testParallelMultiSort() throws Exception {
+		Searcher searcher = new ParallelMultiSearcher (new Searchable[] { searchX, searchY });
+		runMultiSorts (searcher);
+	}
+
+	// test a variety of sorts using a remote searcher
+	public void testRemoteSort() throws Exception {
+		Searchable searcher = getRemote();
+		MultiSearcher multi = new MultiSearcher (new Searchable[] { searcher });
+		runMultiSorts (multi);
+	}
+
+	// test custom search when remote
+	public void testRemoteCustomSort() throws Exception {
+		Searchable searcher = getRemote();
+		MultiSearcher multi = new MultiSearcher (new Searchable[] { searcher });
+		sort.setSort (new SortField ("custom", SampleComparable.getComparatorSource()));
+		assertMatches (multi, queryX, sort, "CAIEG");
+		sort.setSort (new SortField ("custom", SampleComparable.getComparatorSource(), true));
+		assertMatches (multi, queryY, sort, "HJDBF");
+		SortComparator custom = SampleComparable.getComparator();
+		sort.setSort (new SortField ("custom", custom));
+		assertMatches (multi, queryX, sort, "CAIEG");
+		sort.setSort (new SortField ("custom", custom, true));
+		assertMatches (multi, queryY, sort, "HJDBF");
+	}
+
+	// test that the relevancy scores are the same even if
+	// hits are sorted
+	public void testNormalizedScores() throws Exception {
+
+		// capture relevancy scores
+		HashMap scoresX = getScores (full.search (queryX));
+		HashMap scoresY = getScores (full.search (queryY));
+		HashMap scoresA = getScores (full.search (queryA));
+
+		// we'll test searching locally, remote and multi
+		// note: the multi test depends on each separate index containing
+		// the same documents as our local index, so the computed normalization
+		// will be the same.  so we make a multi searcher over two equal document
+		// sets - not realistic, but necessary for testing.
+		MultiSearcher remote = new MultiSearcher (new Searchable[] { getRemote() });
+		MultiSearcher multi  = new MultiSearcher (new Searchable[] { full, full });
+
+		// change sorting and make sure relevancy stays the same
+
+		sort = new Sort();
+		assertSameValues (scoresX, getScores(full.search(queryX,sort)));
+		assertSameValues (scoresX, getScores(remote.search(queryX,sort)));
+		assertSameValues (scoresX, getScores(multi.search(queryX,sort)));
+		assertSameValues (scoresY, getScores(full.search(queryY,sort)));
+		assertSameValues (scoresY, getScores(remote.search(queryY,sort)));
+		assertSameValues (scoresY, getScores(multi.search(queryY,sort)));
+		assertSameValues (scoresA, getScores(full.search(queryA,sort)));
+		assertSameValues (scoresA, getScores(remote.search(queryA,sort)));
+		assertSameValues (scoresA, getScores(multi.search(queryA,sort)));
+
+		sort.setSort(SortField.FIELD_DOC);
+		assertSameValues (scoresX, getScores(full.search(queryX,sort)));
+		assertSameValues (scoresX, getScores(remote.search(queryX,sort)));
+		assertSameValues (scoresX, getScores(multi.search(queryX,sort)));
+		assertSameValues (scoresY, getScores(full.search(queryY,sort)));
+		assertSameValues (scoresY, getScores(remote.search(queryY,sort)));
+		assertSameValues (scoresY, getScores(multi.search(queryY,sort)));
+		assertSameValues (scoresA, getScores(full.search(queryA,sort)));
+		assertSameValues (scoresA, getScores(remote.search(queryA,sort)));
+		assertSameValues (scoresA, getScores(multi.search(queryA,sort)));
+
+		sort.setSort ("int");
+		assertSameValues (scoresX, getScores(full.search(queryX,sort)));
+		assertSameValues (scoresX, getScores(remote.search(queryX,sort)));
+		assertSameValues (scoresX, getScores(multi.search(queryX,sort)));
+		assertSameValues (scoresY, getScores(full.search(queryY,sort)));
+		assertSameValues (scoresY, getScores(remote.search(queryY,sort)));
+		assertSameValues (scoresY, getScores(multi.search(queryY,sort)));
+		assertSameValues (scoresA, getScores(full.search(queryA,sort)));
+		assertSameValues (scoresA, getScores(remote.search(queryA,sort)));
+		assertSameValues (scoresA, getScores(multi.search(queryA,sort)));
+
+		sort.setSort ("float");
+		assertSameValues (scoresX, getScores(full.search(queryX,sort)));
+		assertSameValues (scoresX, getScores(remote.search(queryX,sort)));
+		assertSameValues (scoresX, getScores(multi.search(queryX,sort)));
+		assertSameValues (scoresY, getScores(full.search(queryY,sort)));
+		assertSameValues (scoresY, getScores(remote.search(queryY,sort)));
+		assertSameValues (scoresY, getScores(multi.search(queryY,sort)));
+		assertSameValues (scoresA, getScores(full.search(queryA,sort)));
+		assertSameValues (scoresA, getScores(remote.search(queryA,sort)));
+		assertSameValues (scoresA, getScores(multi.search(queryA,sort)));
+
+		sort.setSort ("string");
+		assertSameValues (scoresX, getScores(full.search(queryX,sort)));
+		assertSameValues (scoresX, getScores(remote.search(queryX,sort)));
+		assertSameValues (scoresX, getScores(multi.search(queryX,sort)));
+		assertSameValues (scoresY, getScores(full.search(queryY,sort)));
+		assertSameValues (scoresY, getScores(remote.search(queryY,sort)));
+		assertSameValues (scoresY, getScores(multi.search(queryY,sort)));
+		assertSameValues (scoresA, getScores(full.search(queryA,sort)));
+		assertSameValues (scoresA, getScores(remote.search(queryA,sort)));
+		assertSameValues (scoresA, getScores(multi.search(queryA,sort)));
+
+		sort.setSort (new String[] {"int","float"});
+		assertSameValues (scoresX, getScores(full.search(queryX,sort)));
+		assertSameValues (scoresX, getScores(remote.search(queryX,sort)));
+		assertSameValues (scoresX, getScores(multi.search(queryX,sort)));
+		assertSameValues (scoresY, getScores(full.search(queryY,sort)));
+		assertSameValues (scoresY, getScores(remote.search(queryY,sort)));
+		assertSameValues (scoresY, getScores(multi.search(queryY,sort)));
+		assertSameValues (scoresA, getScores(full.search(queryA,sort)));
+		assertSameValues (scoresA, getScores(remote.search(queryA,sort)));
+		assertSameValues (scoresA, getScores(multi.search(queryA,sort)));
+
+		sort.setSort (new SortField[] { new SortField ("int", true), new SortField (null, SortField.DOC, true) });
+		assertSameValues (scoresX, getScores(full.search(queryX,sort)));
+		assertSameValues (scoresX, getScores(remote.search(queryX,sort)));
+		assertSameValues (scoresX, getScores(multi.search(queryX,sort)));
+		assertSameValues (scoresY, getScores(full.search(queryY,sort)));
+		assertSameValues (scoresY, getScores(remote.search(queryY,sort)));
+		assertSameValues (scoresY, getScores(multi.search(queryY,sort)));
+		assertSameValues (scoresA, getScores(full.search(queryA,sort)));
+		assertSameValues (scoresA, getScores(remote.search(queryA,sort)));
+		assertSameValues (scoresA, getScores(multi.search(queryA,sort)));
+
+		sort.setSort (new String[] {"float","string"});
+		assertSameValues (scoresX, getScores(full.search(queryX,sort)));
+		assertSameValues (scoresX, getScores(remote.search(queryX,sort)));
+		assertSameValues (scoresX, getScores(multi.search(queryX,sort)));
+		assertSameValues (scoresY, getScores(full.search(queryY,sort)));
+		assertSameValues (scoresY, getScores(remote.search(queryY,sort)));
+		assertSameValues (scoresY, getScores(multi.search(queryY,sort)));
+		assertSameValues (scoresA, getScores(full.search(queryA,sort)));
+		assertSameValues (scoresA, getScores(remote.search(queryA,sort)));
+		assertSameValues (scoresA, getScores(multi.search(queryA,sort)));
+
+	}
+
+	// runs a variety of sorts useful for multisearchers
+	private void runMultiSorts (Searcher multi) throws Exception {
+		sort.setSort (SortField.FIELD_DOC);
+		assertMatchesPattern (multi, queryA, sort, "[AB]{2}[CD]{2}[EF]{2}[GH]{2}[IJ]{2}");
+
+		sort.setSort (new SortField ("int", SortField.INT));
+		assertMatchesPattern (multi, queryA, sort, "IDHFGJ[ABE]{3}C");
+
+		sort.setSort (new SortField[] {new SortField ("int", SortField.INT), SortField.FIELD_DOC});
+		assertMatchesPattern (multi, queryA, sort, "IDHFGJ[AB]{2}EC");
+
+		sort.setSort ("int");
+		assertMatchesPattern (multi, queryA, sort, "IDHFGJ[AB]{2}EC");
+
+		sort.setSort (new SortField[] {new SortField ("float", SortField.FLOAT), SortField.FIELD_DOC});
+		assertMatchesPattern (multi, queryA, sort, "GDHJ[CI]{2}EFAB");
+
+		sort.setSort ("float");
+		assertMatchesPattern (multi, queryA, sort, "GDHJ[CI]{2}EFAB");
+
+		sort.setSort ("string");
+		assertMatches (multi, queryA, sort, "DJAIHGFEBC");
+
+		sort.setSort ("int", true);
+		assertMatchesPattern (multi, queryA, sort, "C[AB]{2}EJGFHDI");
+
+		sort.setSort ("float", true);
+		assertMatchesPattern (multi, queryA, sort, "BAFE[IC]{2}JHDG");
+
+		sort.setSort ("string", true);
+		assertMatches (multi, queryA, sort, "CBEFGHIAJD");
+
+		sort.setSort (new SortField[] { new SortField ("string", Locale.US) });
+		assertMatches (multi, queryA, sort, "DJAIHGFEBC");
+
+		sort.setSort (new SortField[] { new SortField ("string", Locale.US, true) });
+		assertMatches (multi, queryA, sort, "CBEFGHIAJD");
+
+		sort.setSort (new String[] {"int","float"});
+		assertMatches (multi, queryA, sort, "IDHFGJEABC");
+
+		sort.setSort (new String[] {"float","string"});
+		assertMatches (multi, queryA, sort, "GDHJICEFAB");
+
+		sort.setSort ("int");
+		assertMatches (multi, queryF, sort, "IZJ");
+
+		sort.setSort ("int", true);
+		assertMatches (multi, queryF, sort, "JZI");
+
+		sort.setSort ("float");
+		assertMatches (multi, queryF, sort, "ZJI");
+
+		sort.setSort ("string");
+		assertMatches (multi, queryF, sort, "ZJI");
+
+		sort.setSort ("string", true);
+		assertMatches (multi, queryF, sort, "IJZ");
+	}
+
+	// make sure the documents returned by the search match the expected list
+	private void assertMatches (Searcher searcher, Query query, Sort sort, String expectedResult)
+	throws IOException {
+		Hits result = searcher.search (query, sort);
+		StringBuffer buff = new StringBuffer(10);
+		int n = result.length();
+		for (int i=0; i<n; ++i) {
+			Document doc = result.doc(i);
+			String[] v = doc.getValues("tracer");
+			for (int j=0; j<v.length; ++j) {
+				buff.append (v[j]);
+			}
+		}
+		assertEquals (expectedResult, buff.toString());
+	}
+
+	// make sure the documents returned by the search match the expected list pattern
+	private void assertMatchesPattern (Searcher searcher, Query query, Sort sort, String pattern)
+	throws IOException {
+		Hits result = searcher.search (query, sort);
+		StringBuffer buff = new StringBuffer(10);
+		int n = result.length();
+		for (int i=0; i<n; ++i) {
+			Document doc = result.doc(i);
+			String[] v = doc.getValues("tracer");
+			for (int j=0; j<v.length; ++j) {
+				buff.append (v[j]);
+			}
+		}
+		// System.out.println ("matching \""+buff+"\" against pattern \""+pattern+"\"");
+		assertTrue (Pattern.compile(pattern).matcher(buff.toString()).matches());
+	}
+
+	private HashMap getScores (Hits hits)
+	throws IOException {
+		HashMap scoreMap = new HashMap();
+		int n = hits.length();
+		for (int i=0; i<n; ++i) {
+			Document doc = hits.doc(i);
+			String[] v = doc.getValues("tracer");
+			assertEquals (v.length, 1);
+			scoreMap.put (v[0], new Float(hits.score(i)));
+		}
+		return scoreMap;
+	}
+
+	// make sure all the values in the maps match
+	private void assertSameValues (HashMap m1, HashMap m2) {
+		int n = m1.size();
+		int m = m2.size();
+		assertEquals (n, m);
+		Iterator iter = m1.keySet().iterator();
+		while (iter.hasNext()) {
+			Object key = iter.next();
+			assertEquals (m1.get(key), m2.get(key));
+		}
+	}
+
+	private Searchable getRemote () throws Exception {
+		try {
+			return lookupRemote ();
+		} catch (Throwable e) {
+			startServer ();
+			return lookupRemote ();
+		}
+	}
+
+	private Searchable lookupRemote () throws Exception {
+		return (Searchable) Naming.lookup ("//localhost/SortedSearchable");
+	}
+
+	private void startServer () throws Exception {
+		// construct an index
+		Searcher local = getFullIndex();
+		// local.search (queryA, new Sort());
+
+		// publish it
+		Registry reg = LocateRegistry.createRegistry (1099);
+		RemoteSearchable impl = new RemoteSearchable (local);
+		Naming.rebind ("//localhost/SortedSearchable", impl);
+	}
+
+}
diff --git a/src/test-deprecated/org/apache/lucene/search/TestTermVectors.java b/src/test-deprecated/org/apache/lucene/search/TestTermVectors.java
new file mode 100644
index 0000000..4b0590e
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/search/TestTermVectors.java
@@ -0,0 +1,223 @@
+package org.apache.lucene.search;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import junit.framework.TestCase;
+import org.apache.lucene.analysis.SimpleAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.index.*;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.util.English;
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+public class TestTermVectors extends TestCase {
+  private IndexSearcher searcher;
+  private RAMDirectory directory = new RAMDirectory();
+  public TestTermVectors(String s) {
+    super(s);
+  }
+
+  public void setUp() throws Exception {                  
+    IndexWriter writer
+            = new IndexWriter(directory, new SimpleAnalyzer(), true);
+    //writer.setUseCompoundFile(true);
+    //writer.infoStream = System.out;
+    StringBuffer buffer = new StringBuffer();
+    for (int i = 0; i < 1000; i++) {
+      Document doc = new Document();
+      doc.add(Field.Text("field", English.intToEnglish(i), true));
+      writer.addDocument(doc);
+    }
+    writer.close();
+    searcher = new IndexSearcher(directory);
+  }
+
+  protected void tearDown() {
+
+  }
+
+  public void test() {
+    assertTrue(searcher != null);
+  }
+
+  public void testTermVectors() {
+    Query query = new TermQuery(new Term("field", "seventy"));
+    try {
+      Hits hits = searcher.search(query);
+      assertEquals(100, hits.length());
+      
+      for (int i = 0; i < hits.length(); i++)
+      {
+        TermFreqVector [] vector = searcher.reader.getTermFreqVectors(hits.id(i));
+        assertTrue(vector != null);
+        assertTrue(vector.length == 1);
+        //assertTrue();
+      }
+      TermFreqVector [] vector = searcher.reader.getTermFreqVectors(hits.id(50));
+      //System.out.println("Explain: " + searcher.explain(query, hits.id(50)));
+      //System.out.println("Vector: " + vector[0].toString());
+    } catch (IOException e) {
+      assertTrue(false);
+    }
+  }
+  
+  public void testTermPositionVectors() {
+    Query query = new TermQuery(new Term("field", "fifty"));
+    try {
+      Hits hits = searcher.search(query);
+      assertEquals(100, hits.length());
+      
+      for (int i = 0; i < hits.length(); i++)
+      {
+        TermFreqVector [] vector = searcher.reader.getTermFreqVectors(hits.id(i));
+        assertTrue(vector != null);
+        assertTrue(vector.length == 1);
+        //assertTrue();
+      }
+    } catch (IOException e) {
+      assertTrue(false);
+    }
+  }
+  
+  public void testKnownSetOfDocuments() {
+    String [] termArray = {"eating", "chocolate", "in", "a", "computer", "lab", "grows", "old", "colored",
+                      "with", "an"};
+    String test1 = "eating chocolate in a computer lab"; //6 terms
+    String test2 = "computer in a computer lab"; //5 terms
+    String test3 = "a chocolate lab grows old"; //5 terms
+    String test4 = "eating chocolate with a chocolate lab in an old chocolate colored computer lab"; //13 terms
+    Map test4Map = new HashMap();
+    test4Map.put("chocolate", new Integer(3));
+    test4Map.put("lab", new Integer(2));
+    test4Map.put("eating", new Integer(1));
+    test4Map.put("computer", new Integer(1));
+    test4Map.put("with", new Integer(1));
+    test4Map.put("a", new Integer(1));
+    test4Map.put("colored", new Integer(1));
+    test4Map.put("in", new Integer(1));
+    test4Map.put("an", new Integer(1));
+    test4Map.put("computer", new Integer(1));
+    test4Map.put("old", new Integer(1));
+    
+    Document testDoc1 = new Document();
+    setupDoc(testDoc1, test1);
+    Document testDoc2 = new Document();
+    setupDoc(testDoc2, test2);
+    Document testDoc3 = new Document();
+    setupDoc(testDoc3, test3);
+    Document testDoc4 = new Document();
+    setupDoc(testDoc4, test4);
+        
+    Directory dir = new RAMDirectory();
+    
+    try {
+      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), true);
+      assertTrue(writer != null);
+      writer.addDocument(testDoc1);
+      writer.addDocument(testDoc2);
+      writer.addDocument(testDoc3);
+      writer.addDocument(testDoc4);
+      writer.close();
+      IndexSearcher knownSearcher = new IndexSearcher(dir);
+      TermEnum termEnum = knownSearcher.reader.terms();
+      TermDocs termDocs = knownSearcher.reader.termDocs();
+      //System.out.println("Terms: " + termEnum.size() + " Orig Len: " + termArray.length);
+      
+      Similarity sim = knownSearcher.getSimilarity();
+      while (termEnum.next() == true)
+      {
+        Term term = termEnum.term();
+        //System.out.println("Term: " + term);
+        termDocs.seek(term);
+        while (termDocs.next())
+        {
+          int docId = termDocs.doc();
+          int freq = termDocs.freq();
+          //System.out.println("Doc Id: " + docId + " freq " + freq);
+          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, "field");
+          float tf = sim.tf(freq);
+          float idf = sim.idf(term, knownSearcher);
+          //float qNorm = sim.queryNorm()
+          //This is fine since we don't have stop words
+          float lNorm = sim.lengthNorm("field", vector.getTerms().length);
+          //float coord = sim.coord()
+          //System.out.println("TF: " + tf + " IDF: " + idf + " LenNorm: " + lNorm);
+          assertTrue(vector != null);
+          String[] vTerms = vector.getTerms();
+          int [] freqs = vector.getTermFrequencies();
+          for (int i = 0; i < vTerms.length; i++)
+          {
+            if (term.text().equals(vTerms[i]) == true)
+            {
+              assertTrue(freqs[i] == freq);
+            }
+          }
+          
+        }
+        //System.out.println("--------");
+      }
+      Query query = new TermQuery(new Term("field", "chocolate"));
+      Hits hits = knownSearcher.search(query);
+      //doc 3 should be the first hit b/c it is the shortest match
+      assertTrue(hits.length() == 3);
+      float score = hits.score(0);
+      /*System.out.println("Hit 0: " + hits.id(0) + " Score: " + hits.score(0) + " String: " + hits.doc(0).toString());
+      System.out.println("Explain: " + knownSearcher.explain(query, hits.id(0)));
+      System.out.println("Hit 1: " + hits.id(1) + " Score: " + hits.score(1) + " String: " + hits.doc(1).toString());
+      System.out.println("Explain: " + knownSearcher.explain(query, hits.id(1)));
+      System.out.println("Hit 2: " + hits.id(2) + " Score: " + hits.score(2) + " String: " +  hits.doc(2).toString());
+      System.out.println("Explain: " + knownSearcher.explain(query, hits.id(2)));*/
+      assertTrue(testDoc3.toString().equals(hits.doc(0).toString()));
+      assertTrue(testDoc4.toString().equals(hits.doc(1).toString()));
+      assertTrue(testDoc1.toString().equals(hits.doc(2).toString()));
+      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits.id(1), "field");
+      assertTrue(vector != null);
+      //System.out.println("Vector: " + vector);
+      String[] terms = vector.getTerms();
+      int [] freqs = vector.getTermFrequencies();
+      assertTrue(terms != null && terms.length == 10);
+      for (int i = 0; i < terms.length; i++) {
+        String term = terms[i];
+        //System.out.println("Term: " + term);
+        int freq = freqs[i];
+        assertTrue(test4.indexOf(term) != -1);
+        Integer freqInt = (Integer)test4Map.get(term);
+        assertTrue(freqInt != null);
+        assertTrue(freqInt.intValue() == freq);        
+      } 
+      knownSearcher.close();
+    } catch (IOException e) {
+      e.printStackTrace();
+      assertTrue(false);
+    }
+
+
+  } 
+  
+  private void setupDoc(Document doc, String text)
+  {
+    doc.add(Field.Text("field", text, true));
+    //System.out.println("Document: " + doc);
+  }
+  
+  
+}
diff --git a/src/test-deprecated/org/apache/lucene/search/TestWildcard.java b/src/test-deprecated/org/apache/lucene/search/TestWildcard.java
new file mode 100644
index 0000000..499b8c0
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/search/TestWildcard.java
@@ -0,0 +1,133 @@
+package org.apache.lucene.search;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.analysis.SimpleAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+
+import junit.framework.TestCase;
+
+import java.io.IOException;
+
+/**
+ * TestWildcard tests the '*' and '?' wildard characters.
+ *
+ * @author Otis Gospodnetic
+ */
+public class TestWildcard
+    extends TestCase
+{
+    /**
+     * Creates a new <code>TestWildcard</code> instance.
+     *
+     * @param name the name of the test
+     */
+    public TestWildcard(String name)
+    {
+	super(name);
+    }
+
+    /**
+     * Tests Wildcard queries with an asterisk.
+     *
+     */
+    public void testAsterisk()
+        throws IOException
+    {
+        RAMDirectory indexStore = getIndexStore("body", new String[]
+	    { "metal", "metals" }
+						);
+	IndexSearcher searcher = new IndexSearcher(indexStore);
+	Query query1 = new TermQuery(new Term("body", "metal"));
+        Query query2 = new WildcardQuery(new Term("body", "metal*"));
+        Query query3 = new WildcardQuery(new Term("body", "m*tal"));
+        Query query4 = new WildcardQuery(new Term("body", "m*tal*"));
+        Query query5 = new WildcardQuery(new Term("body", "m*tals"));
+
+        BooleanQuery query6 = new BooleanQuery();
+        query6.add(query5, false, false);
+
+        BooleanQuery query7 = new BooleanQuery();
+        query7.add(query3, false, false);
+        query7.add(query5, false, false);
+
+	// Queries do not automatically lower-case search terms:
+        Query query8 = new WildcardQuery(new Term("body", "M*tal*"));
+
+	assertMatches(searcher, query1, 1);
+	assertMatches(searcher, query2, 2);
+	assertMatches(searcher, query3, 1);
+	assertMatches(searcher, query4, 2);
+	assertMatches(searcher, query5, 1);
+	assertMatches(searcher, query6, 1);
+	assertMatches(searcher, query7, 2);
+	assertMatches(searcher, query8, 0);
+    }
+
+    /**
+     * Tests Wildcard queries with a question mark.
+     *
+     * @exception IOException if an error occurs
+     */
+    public void testQuestionmark()
+	throws IOException
+    {
+        RAMDirectory indexStore = getIndexStore("body", new String[]
+	    { "metal", "metals", "mXtals", "mXtXls" }
+						);
+	IndexSearcher searcher = new IndexSearcher(indexStore);
+        Query query1 = new WildcardQuery(new Term("body", "m?tal"));
+        Query query2 = new WildcardQuery(new Term("body", "metal?"));
+        Query query3 = new WildcardQuery(new Term("body", "metals?"));
+        Query query4 = new WildcardQuery(new Term("body", "m?t?ls"));
+        Query query5 = new WildcardQuery(new Term("body", "M?t?ls"));
+
+	assertMatches(searcher, query1, 1);
+	assertMatches(searcher, query2, 2);
+	assertMatches(searcher, query3, 1);
+	assertMatches(searcher, query4, 3);
+	assertMatches(searcher, query5, 0);
+    }
+
+    private RAMDirectory getIndexStore(String field, String[] contents)
+	throws IOException
+    {
+        RAMDirectory indexStore = new RAMDirectory();
+        IndexWriter writer = new IndexWriter(indexStore, new SimpleAnalyzer(), true);
+	for (int i = 0; i < contents.length; ++i) {
+	    Document doc = new Document();
+	    doc.add(Field.Text(field, contents[i]));
+	    writer.addDocument(doc);
+	}
+	writer.optimize();
+	writer.close();
+
+	return indexStore;
+    }
+
+    private void assertMatches(IndexSearcher searcher, Query q, int expectedMatches)
+	throws IOException
+    {
+	Hits result = searcher.search(q);
+	assertEquals(expectedMatches, result.length());
+    }
+}
diff --git a/src/test-deprecated/org/apache/lucene/search/spans/TestBasics.java b/src/test-deprecated/org/apache/lucene/search/spans/TestBasics.java
new file mode 100644
index 0000000..7ac36fb
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/search/spans/TestBasics.java
@@ -0,0 +1,268 @@
+package org.apache.lucene.search.spans;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import junit.framework.TestCase;
+
+import java.io.IOException;
+
+import org.apache.lucene.util.English;
+import org.apache.lucene.analysis.SimpleAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.store.RAMDirectory;
+
+import org.apache.lucene.search.*;
+
+/**
+ * Tests basic search capabilities.
+ *
+ * <p>Uses a collection of 1000 documents, each the english rendition of their
+ * document number.  For example, the document numbered 333 has text "three
+ * hundred thirty three".
+ *
+ * <p>Tests are each a single query, and its hits are checked to ensure that
+ * all and only the correct documents are returned, thus providing end-to-end
+ * testing of the indexing and search code.
+ *
+ * @author Doug Cutting
+ */
+public class TestBasics extends TestCase {
+  private IndexSearcher searcher;
+
+  public void setUp() throws Exception {
+    RAMDirectory directory = new RAMDirectory();
+    IndexWriter writer
+      = new IndexWriter(directory, new SimpleAnalyzer(), true);
+    //writer.infoStream = System.out;
+    for (int i = 0; i < 1000; i++) {
+      Document doc = new Document();
+      doc.add(Field.Text("field", English.intToEnglish(i)));
+      writer.addDocument(doc);
+    }
+
+    writer.close();
+
+    searcher = new IndexSearcher(directory);
+  }
+  
+  public void testTerm() throws Exception {
+    Query query = new TermQuery(new Term("field", "seventy"));
+    checkHits(query, new int[]
+      {70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 170, 171, 172, 173, 174, 175,
+       176, 177, 178, 179, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,
+       370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 470, 471, 472, 473,
+       474, 475, 476, 477, 478, 479, 570, 571, 572, 573, 574, 575, 576, 577,
+       578, 579, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 770, 771,
+       772, 773, 774, 775, 776, 777, 778, 779, 870, 871, 872, 873, 874, 875,
+       876, 877, 878, 879, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979});
+  }
+
+  public void testTerm2() throws Exception {
+    Query query = new TermQuery(new Term("field", "seventish"));
+    checkHits(query, new int[] {});
+  }
+
+  public void testPhrase() throws Exception {
+    PhraseQuery query = new PhraseQuery();
+    query.add(new Term("field", "seventy"));
+    query.add(new Term("field", "seven"));
+    checkHits(query, new int[]
+      {77, 177, 277, 377, 477, 577, 677, 777, 877, 977});
+  }
+
+  public void testPhrase2() throws Exception {
+    PhraseQuery query = new PhraseQuery();
+    query.add(new Term("field", "seventish"));
+    query.add(new Term("field", "sevenon"));
+    checkHits(query, new int[] {});
+  }
+
+  public void testBoolean() throws Exception {
+    BooleanQuery query = new BooleanQuery();
+    query.add(new TermQuery(new Term("field", "seventy")), true, false);
+    query.add(new TermQuery(new Term("field", "seven")), true, false);
+    checkHits(query, new int[]
+      {77, 777, 177, 277, 377, 477, 577, 677, 770, 771, 772, 773, 774, 775,
+       776, 778, 779, 877, 977});
+  }
+
+  public void testBoolean2() throws Exception {
+    BooleanQuery query = new BooleanQuery();
+    query.add(new TermQuery(new Term("field", "sevento")), true, false);
+    query.add(new TermQuery(new Term("field", "sevenly")), true, false);
+    checkHits(query, new int[] {});
+  }
+
+  public void testSpanNearExact() throws Exception {
+    SpanTermQuery term1 = new SpanTermQuery(new Term("field", "seventy"));
+    SpanTermQuery term2 = new SpanTermQuery(new Term("field", "seven"));
+    SpanNearQuery query = new SpanNearQuery(new SpanQuery[] {term1, term2},
+                                            0, true);
+    checkHits(query, new int[]
+      {77, 177, 277, 377, 477, 577, 677, 777, 877, 977});
+
+    assertTrue(searcher.explain(query, 77).getValue() > 0.0f);
+    assertTrue(searcher.explain(query, 977).getValue() > 0.0f);
+  }
+
+  public void testSpanNearUnordered() throws Exception {
+    SpanTermQuery term1 = new SpanTermQuery(new Term("field", "nine"));
+    SpanTermQuery term2 = new SpanTermQuery(new Term("field", "six"));
+    SpanNearQuery query = new SpanNearQuery(new SpanQuery[] {term1, term2},
+                                            4, false);
+
+    checkHits(query, new int[]
+      {609, 629, 639, 649, 659, 669, 679, 689, 699,
+       906, 926, 936, 946, 956, 966, 976, 986, 996});
+  }
+
+  public void testSpanNearOrdered() throws Exception {
+    SpanTermQuery term1 = new SpanTermQuery(new Term("field", "nine"));
+    SpanTermQuery term2 = new SpanTermQuery(new Term("field", "six"));
+    SpanNearQuery query = new SpanNearQuery(new SpanQuery[] {term1, term2},
+                                            4, true);
+    checkHits(query, new int[]
+      {906, 926, 936, 946, 956, 966, 976, 986, 996});
+  }
+
+  public void testSpanNot() throws Exception {
+    SpanTermQuery term1 = new SpanTermQuery(new Term("field", "eight"));
+    SpanTermQuery term2 = new SpanTermQuery(new Term("field", "one"));
+    SpanNearQuery near = new SpanNearQuery(new SpanQuery[] {term1, term2},
+                                           4, true);
+    SpanTermQuery term3 = new SpanTermQuery(new Term("field", "forty"));
+    SpanNotQuery query = new SpanNotQuery(near, term3);
+
+    checkHits(query, new int[]
+      {801, 821, 831, 851, 861, 871, 881, 891});
+
+    assertTrue(searcher.explain(query, 801).getValue() > 0.0f);
+    assertTrue(searcher.explain(query, 891).getValue() > 0.0f);
+  }
+
+  public void testSpanFirst() throws Exception {
+    SpanTermQuery term1 = new SpanTermQuery(new Term("field", "five"));
+    SpanFirstQuery query = new SpanFirstQuery(term1, 1);
+
+    checkHits(query, new int[]
+      {5, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513,
+       514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527,
+       528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541,
+       542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555,
+       556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569,
+       570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583,
+       584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597,
+       598, 599});
+
+    assertTrue(searcher.explain(query, 5).getValue() > 0.0f);
+    assertTrue(searcher.explain(query, 599).getValue() > 0.0f);
+
+  }
+
+  public void testSpanOr() throws Exception {
+    SpanTermQuery term1 = new SpanTermQuery(new Term("field", "thirty"));
+    SpanTermQuery term2 = new SpanTermQuery(new Term("field", "three"));
+    SpanNearQuery near1 = new SpanNearQuery(new SpanQuery[] {term1, term2},
+                                            0, true);
+    SpanTermQuery term3 = new SpanTermQuery(new Term("field", "forty"));
+    SpanTermQuery term4 = new SpanTermQuery(new Term("field", "seven"));
+    SpanNearQuery near2 = new SpanNearQuery(new SpanQuery[] {term3, term4},
+                                            0, true);
+
+    SpanOrQuery query = new SpanOrQuery(new SpanQuery[] {near1, near2});
+
+    checkHits(query, new int[]
+      {33, 47, 133, 147, 233, 247, 333, 347, 433, 447, 533, 547, 633, 647, 733,
+       747, 833, 847, 933, 947});
+
+    assertTrue(searcher.explain(query, 33).getValue() > 0.0f);
+    assertTrue(searcher.explain(query, 947).getValue() > 0.0f);
+  }
+
+  public void testSpanExactNested() throws Exception {
+    SpanTermQuery term1 = new SpanTermQuery(new Term("field", "three"));
+    SpanTermQuery term2 = new SpanTermQuery(new Term("field", "hundred"));
+    SpanNearQuery near1 = new SpanNearQuery(new SpanQuery[] {term1, term2},
+                                            0, true);
+    SpanTermQuery term3 = new SpanTermQuery(new Term("field", "thirty"));
+    SpanTermQuery term4 = new SpanTermQuery(new Term("field", "three"));
+    SpanNearQuery near2 = new SpanNearQuery(new SpanQuery[] {term3, term4},
+                                            0, true);
+
+    SpanNearQuery query = new SpanNearQuery(new SpanQuery[] {near1, near2},
+                                            0, true);
+
+    checkHits(query, new int[] {333});
+
+    assertTrue(searcher.explain(query, 333).getValue() > 0.0f);
+  }
+
+  public void testSpanNearOr() throws Exception {
+
+    SpanTermQuery t1 = new SpanTermQuery(new Term("field","six"));
+    SpanTermQuery t3 = new SpanTermQuery(new Term("field","seven"));
+    
+    SpanTermQuery t5 = new SpanTermQuery(new Term("field","seven"));
+    SpanTermQuery t6 = new SpanTermQuery(new Term("field","six"));
+
+    SpanOrQuery to1 = new SpanOrQuery(new SpanQuery[] {t1, t3});
+    SpanOrQuery to2 = new SpanOrQuery(new SpanQuery[] {t5, t6});
+    
+    SpanNearQuery query = new SpanNearQuery(new SpanQuery[] {to1, to2},
+                                            10, true);
+
+    checkHits(query, new int[]
+      {606, 607, 626, 627, 636, 637, 646, 647, 
+       656, 657, 666, 667, 676, 677, 686, 687, 696, 697,
+       706, 707, 726, 727, 736, 737, 746, 747, 
+       756, 757, 766, 767, 776, 777, 786, 787, 796, 797});
+  }
+
+  public void testSpanComplex1() throws Exception {
+      
+    SpanTermQuery t1 = new SpanTermQuery(new Term("field","six"));
+    SpanTermQuery t2 = new SpanTermQuery(new Term("field","hundred"));
+    SpanNearQuery tt1 = new SpanNearQuery(new SpanQuery[] {t1, t2}, 0,true);
+
+    SpanTermQuery t3 = new SpanTermQuery(new Term("field","seven"));
+    SpanTermQuery t4 = new SpanTermQuery(new Term("field","hundred"));
+    SpanNearQuery tt2 = new SpanNearQuery(new SpanQuery[] {t3, t4}, 0,true);
+    
+    SpanTermQuery t5 = new SpanTermQuery(new Term("field","seven"));
+    SpanTermQuery t6 = new SpanTermQuery(new Term("field","six"));
+
+    SpanOrQuery to1 = new SpanOrQuery(new SpanQuery[] {tt1, tt2});
+    SpanOrQuery to2 = new SpanOrQuery(new SpanQuery[] {t5, t6});
+    
+    SpanNearQuery query = new SpanNearQuery(new SpanQuery[] {to1, to2},
+                                            100, true);
+    
+    checkHits(query, new int[]
+      {606, 607, 626, 627, 636, 637, 646, 647, 
+       656, 657, 666, 667, 676, 677, 686, 687, 696, 697,
+       706, 707, 726, 727, 736, 737, 746, 747, 
+       756, 757, 766, 767, 776, 777, 786, 787, 796, 797});
+  }
+
+
+  private void checkHits(Query query, int[] results) throws IOException {
+    CheckHits.checkHits(query, "field", searcher, results, this);
+  }
+}
diff --git a/src/test-deprecated/org/apache/lucene/search/spans/TestSpans.java b/src/test-deprecated/org/apache/lucene/search/spans/TestSpans.java
new file mode 100644
index 0000000..883b203
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/search/spans/TestSpans.java
@@ -0,0 +1,97 @@
+package org.apache.lucene.search.spans;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.Hits;
+import org.apache.lucene.search.CheckHits;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import junit.framework.TestCase;
+
+import java.io.IOException;
+import java.util.Set;
+import java.util.TreeSet;
+
+public class TestSpans extends TestCase {
+  private IndexSearcher searcher;
+
+  public static final String field = "field";
+
+  public void setUp() throws Exception {
+    RAMDirectory directory = new RAMDirectory();
+    IndexWriter writer= new IndexWriter(directory, new WhitespaceAnalyzer(), true);
+    StringBuffer buffer = new StringBuffer();
+    for (int i = 0; i < docFields.length; i++) {
+      Document doc = new Document();
+      doc.add(Field.Text(field, docFields[i]));
+      writer.addDocument(doc);
+    }
+    writer.close();
+    searcher = new IndexSearcher(directory);
+  }
+
+  private String[] docFields = {
+    "w1 w2 w3 w4 w5",
+    "w1 w3 w2 w3",
+    "w1 xx w2 yy w3",
+    "w1 w3 xx w2 yy w3",
+    ""
+  };
+
+  public SpanTermQuery makeSpanTermQuery(String text) {
+    return new SpanTermQuery(new Term(field, text));
+  }
+  
+  private void checkHits(Query query, int[] results) throws IOException {
+    CheckHits.checkHits(query, field, searcher, results, this);
+  }
+  
+  public void orderedSlopTest3(int slop, int[] expectedDocs) throws IOException {
+    SpanTermQuery w1 = makeSpanTermQuery("w1");
+    SpanTermQuery w2 = makeSpanTermQuery("w2");
+    SpanTermQuery w3 = makeSpanTermQuery("w3");
+    boolean ordered = true;
+    SpanNearQuery snq = new SpanNearQuery( new SpanQuery[]{w1,w2,w3}, slop, ordered);
+    checkHits(snq, expectedDocs);
+  }
+  
+  public void testSpanNearOrdered01() throws Exception {
+    orderedSlopTest3(0, new int[] {0});
+  }
+
+  public void testSpanNearOrdered02() throws Exception {
+    orderedSlopTest3(1, new int[] {0,1});
+  }
+
+  public void testSpanNearOrdered03() throws Exception {
+    orderedSlopTest3(2, new int[] {0,1,2});
+  }
+
+  public void testSpanNearOrdered04() throws Exception {
+    orderedSlopTest3(3, new int[] {0,1,2,3});
+  }
+
+  public void testSpanNearOrdered05() throws Exception {
+    orderedSlopTest3(4, new int[] {0,1,2,3});
+  }
+}
diff --git a/src/test-deprecated/org/apache/lucene/util/English.java b/src/test-deprecated/org/apache/lucene/util/English.java
new file mode 100644
index 0000000..a1d650d
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/util/English.java
@@ -0,0 +1,102 @@
+package org.apache.lucene.util;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+public class English {
+
+  public static String intToEnglish(int i) {
+    StringBuffer result = new StringBuffer();
+    intToEnglish(i, result);
+    return result.toString();
+  }
+
+  public static void intToEnglish(int i, StringBuffer result) {
+    if (i == 0) {
+      result.append("zero");
+      return;
+    }
+    if (i < 0) {
+      result.append("minus ");
+      i = -i;
+    }
+    if (i >= 1000000000) {			  // billions
+      intToEnglish(i/1000000000, result);
+      result.append("billion, ");
+      i = i%1000000000;
+    }
+    if (i >= 1000000) {				  // millions
+      intToEnglish(i/1000000, result);
+      result.append("million, ");
+      i = i%1000000;
+    }
+    if (i >= 1000) {				  // thousands
+      intToEnglish(i/1000, result);
+      result.append("thousand, ");
+      i = i%1000;
+    }
+    if (i >= 100) {				  // hundreds
+      intToEnglish(i/100, result);
+      result.append("hundred ");
+      i = i%100;
+    }
+    if (i >= 20) {
+      switch (i/10) {
+      case 9 : result.append("ninety"); break;
+      case 8 : result.append("eighty"); break;
+      case 7 : result.append("seventy"); break;
+      case 6 : result.append("sixty"); break;
+      case 5 : result.append("fifty"); break;
+      case 4 : result.append("forty"); break;
+      case 3 : result.append("thirty"); break;
+      case 2 : result.append("twenty"); break;
+      }
+      i = i%10;
+      if (i == 0)
+        result.append(" ");
+      else 
+        result.append("-");
+    }
+    switch (i) {
+    case 19 : result.append("nineteen "); break;
+    case 18 : result.append("eighteen "); break;
+    case 17 : result.append("seventeen "); break;
+    case 16 : result.append("sixteen "); break;
+    case 15 : result.append("fifteen "); break;
+    case 14 : result.append("fourteen "); break;
+    case 13 : result.append("thirteen "); break;
+    case 12 : result.append("twelve "); break;
+    case 11 : result.append("eleven "); break;
+    case 10 : result.append("ten "); break;
+    case 9 : result.append("nine "); break;
+    case 8 : result.append("eight "); break;
+    case 7 : result.append("seven "); break;
+    case 6 : result.append("six "); break;
+    case 5 : result.append("five "); break;
+    case 4 : result.append("four "); break;
+    case 3 : result.append("three "); break;
+    case 2 : result.append("two "); break;
+    case 1 : result.append("one "); break;
+    case 0 : result.append(""); break;
+    }
+  }
+
+  public static void main(String[] args) {
+    System.out.println(intToEnglish(Integer.parseInt(args[0])));
+  }
+
+}
diff --git a/src/test-deprecated/org/apache/lucene/util/StringHelperTest.java b/src/test-deprecated/org/apache/lucene/util/StringHelperTest.java
new file mode 100644
index 0000000..d5e5d13
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/util/StringHelperTest.java
@@ -0,0 +1,50 @@
+package org.apache.lucene.util;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import junit.framework.TestCase;
+
+public class StringHelperTest extends TestCase {
+
+
+  public StringHelperTest(String s) {
+    super(s);
+  }
+
+  protected void setUp() {
+  }
+
+  protected void tearDown() {
+
+  }
+
+  public void testStringDifference() {
+    String test1 = "test";
+    String test2 = "testing";
+    
+    int result = StringHelper.stringDifference(test1, test2);
+    assertTrue(result == 4);
+    
+    test2 = "foo";
+    result = StringHelper.stringDifference(test1, test2);
+    assertTrue(result == 0);
+    
+    test2 = "test";
+    result = StringHelper.stringDifference(test1, test2);
+    assertTrue(result == 4);
+  }
+}
diff --git a/src/test-deprecated/org/apache/lucene/util/TestBitVector.java b/src/test-deprecated/org/apache/lucene/util/TestBitVector.java
new file mode 100644
index 0000000..097129a
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/util/TestBitVector.java
@@ -0,0 +1,177 @@
+package org.apache.lucene.util;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import junit.framework.TestCase;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.RAMDirectory;
+
+/**
+ * <code>TestBitVector</code> tests the <code>BitVector</code>, obviously.
+ *
+ * @author "Peter Mularien" <pmularien@deploy.com>
+ * @version $Id$
+ */
+public class TestBitVector extends TestCase
+{
+    public TestBitVector(String s) {
+        super(s);
+    }
+
+    /**
+     * Test the default constructor on BitVectors of various sizes.
+     * @throws Exception
+     */
+    public void testConstructSize() throws Exception {
+        doTestConstructOfSize(8);
+        doTestConstructOfSize(20);
+        doTestConstructOfSize(100);
+        doTestConstructOfSize(1000);
+    }
+
+    private void doTestConstructOfSize(int n) {
+        BitVector bv = new BitVector(n);
+        assertEquals(n,bv.size());
+    }
+
+    /**
+     * Test the get() and set() methods on BitVectors of various sizes.
+     * @throws Exception
+     */
+    public void testGetSet() throws Exception {
+        doTestGetSetVectorOfSize(8);
+        doTestGetSetVectorOfSize(20);
+        doTestGetSetVectorOfSize(100);
+        doTestGetSetVectorOfSize(1000);
+    }
+
+    private void doTestGetSetVectorOfSize(int n) {
+        BitVector bv = new BitVector(n);
+        for(int i=0;i<bv.size();i++) {
+            // ensure a set bit can be git'
+            assertFalse(bv.get(i));
+            bv.set(i);
+            assertTrue(bv.get(i));
+        }
+    }
+
+    /**
+     * Test the clear() method on BitVectors of various sizes.
+     * @throws Exception
+     */
+    public void testClear() throws Exception {
+        doTestClearVectorOfSize(8);
+        doTestClearVectorOfSize(20);
+        doTestClearVectorOfSize(100);
+        doTestClearVectorOfSize(1000);
+    }
+
+    private void doTestClearVectorOfSize(int n) {
+        BitVector bv = new BitVector(n);
+        for(int i=0;i<bv.size();i++) {
+            // ensure a set bit is cleared
+            assertFalse(bv.get(i));
+            bv.set(i);
+            assertTrue(bv.get(i));
+            bv.clear(i);
+            assertFalse(bv.get(i));
+        }
+    }
+
+    /**
+     * Test the count() method on BitVectors of various sizes.
+     * @throws Exception
+     */
+    public void testCount() throws Exception {
+        doTestCountVectorOfSize(8);
+        doTestCountVectorOfSize(20);
+        doTestCountVectorOfSize(100);
+        doTestCountVectorOfSize(1000);
+    }
+
+    private void doTestCountVectorOfSize(int n) {
+        BitVector bv = new BitVector(n);
+        // test count when incrementally setting bits
+        for(int i=0;i<bv.size();i++) {
+            assertFalse(bv.get(i));
+            assertEquals(i,bv.count());
+            bv.set(i);
+            assertTrue(bv.get(i));
+            assertEquals(i+1,bv.count());
+        }
+
+        bv = new BitVector(n);
+        // test count when setting then clearing bits
+        for(int i=0;i<bv.size();i++) {
+            assertFalse(bv.get(i));
+            assertEquals(0,bv.count());
+            bv.set(i);
+            assertTrue(bv.get(i));
+            assertEquals(1,bv.count());
+            bv.clear(i);
+            assertFalse(bv.get(i));
+            assertEquals(0,bv.count());
+        }
+    }
+
+    /**
+     * Test writing and construction to/from Directory.
+     * @throws Exception
+     */
+    public void testWriteRead() throws Exception {
+        doTestWriteRead(8);
+        doTestWriteRead(20);
+        doTestWriteRead(100);
+        doTestWriteRead(1000);
+    }
+
+    private void doTestWriteRead(int n) throws Exception {
+        Directory d = new  RAMDirectory();
+
+        BitVector bv = new BitVector(n);
+        // test count when incrementally setting bits
+        for(int i=0;i<bv.size();i++) {
+            assertFalse(bv.get(i));
+            assertEquals(i,bv.count());
+            bv.set(i);
+            assertTrue(bv.get(i));
+            assertEquals(i+1,bv.count());
+            bv.write(d, "TESTBV");
+            BitVector compare = new BitVector(d, "TESTBV");
+            // compare bit vectors with bits set incrementally
+            assertTrue(doCompare(bv,compare));
+        }
+    }
+
+    /**
+     * Compare two BitVectors.
+     * This should really be an equals method on the BitVector itself.
+     * @param bv One bit vector
+     * @param compare The second to compare
+     */
+    private boolean doCompare(BitVector bv, BitVector compare) {
+        boolean equal = true;
+        for(int i=0;i<bv.size();i++) {
+            // bits must be equal
+            if(bv.get(i)!=compare.get(i)) {
+                equal = false;
+                break;
+            }
+        }
+        return equal;
+    }
+}
diff --git a/src/test-deprecated/org/apache/lucene/util/TestPriorityQueue.java b/src/test-deprecated/org/apache/lucene/util/TestPriorityQueue.java
new file mode 100644
index 0000000..5aee8c5
--- /dev/null
+++ b/src/test-deprecated/org/apache/lucene/util/TestPriorityQueue.java
@@ -0,0 +1,112 @@
+package org.apache.lucene.util;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Date;
+import java.util.Random;
+import junit.framework.TestCase;
+
+public class TestPriorityQueue
+    extends TestCase
+{
+    public TestPriorityQueue(String name)
+    {
+	super(name);
+    }
+
+    private static class IntegerQueue
+	extends PriorityQueue
+    {
+	public IntegerQueue(int count)
+	{
+	    super();
+	    initialize(count);
+	}
+
+	protected boolean lessThan(Object a, Object b)
+	{
+	    return ((Integer) a).intValue() < ((Integer) b).intValue();
+	}
+    }
+
+    public void testPQ()
+	throws Exception
+    {
+	testPQ(10000);
+    }
+
+    public static void testPQ(int count)
+    {
+	PriorityQueue pq = new IntegerQueue(count);
+	Random gen = new Random();
+	int sum = 0, sum2 = 0;
+
+	Date start = new Date();
+
+	for (int i = 0; i < count; i++)
+	{
+	    int next = gen.nextInt();
+	    sum += next;
+	    pq.put(new Integer(next));
+	}
+
+	//      Date end = new Date();
+
+	//      System.out.print(((float)(end.getTime()-start.getTime()) / count) * 1000);
+	//      System.out.println(" microseconds/put");
+
+	//      start = new Date();
+
+	int last = Integer.MIN_VALUE;
+	for (int i = 0; i < count; i++)
+	{
+	    Integer next = (Integer)pq.pop();
+	    assertTrue(next.intValue() >= last);
+	    last = next.intValue();
+	    sum2 += last;
+	}
+
+	assertEquals(sum, sum2);
+	//      end = new Date();
+
+	//      System.out.print(((float)(end.getTime()-start.getTime()) / count) * 1000);
+	//      System.out.println(" microseconds/pop");
+    }
+
+    public void testClear()
+    {
+	PriorityQueue pq = new IntegerQueue(3);
+	pq.put(new Integer(2));
+	pq.put(new Integer(3));
+	pq.put(new Integer(1));
+	assertEquals(3, pq.size());
+	pq.clear();
+	assertEquals(0, pq.size());
+    }
+    
+    public void testFixedSize(){
+        PriorityQueue pq = new IntegerQueue(3);
+        pq.insert(new Integer(2));
+        pq.insert(new Integer(3));
+        pq.insert(new Integer(1));
+        pq.insert(new Integer(5));
+        pq.insert(new Integer(7));
+        pq.insert(new Integer(1));
+        assertEquals(3, pq.size());
+        assertEquals(3, ((Integer) pq.top()).intValue());
+    }
+}

