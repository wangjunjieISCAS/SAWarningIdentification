GitDiffStart: ece152480567fc6f6cf9f0a245873e39ae7f850f | Sun Feb 6 19:48:54 2011 +0000
diff --git a/dev-tools/eclipse/dot.classpath b/dev-tools/eclipse/dot.classpath
index b633d43..9f2c328 100644
--- a/dev-tools/eclipse/dot.classpath
+++ b/dev-tools/eclipse/dot.classpath
@@ -1,6 +1,7 @@
 <?xml version="1.0" encoding="UTF-8"?>
 <classpath>
 	<classpathentry kind="src" path="lucene/src/java"/>
+	<classpathentry kind="src" path="lucene/src/test-framework"/>
 	<classpathentry kind="src" path="lucene/src/test"/>
 	<classpathentry kind="src" path="lucene/contrib/ant/src/java"/>
 	<classpathentry kind="src" path="lucene/contrib/ant/src/resources"/>
diff --git a/dev-tools/idea/lucene/lucene.iml b/dev-tools/idea/lucene/lucene.iml
index c6dd927..80dfc2d 100644
--- a/dev-tools/idea/lucene/lucene.iml
+++ b/dev-tools/idea/lucene/lucene.iml
@@ -6,6 +6,7 @@
     <exclude-output />
     <content url="file://$MODULE_DIR$">
       <sourceFolder url="file://$MODULE_DIR$/src/java" isTestSource="false" />
+      <sourceFolder url="file://$MODULE_DIR$/src/test-framework" isTestSource="true" />
       <sourceFolder url="file://$MODULE_DIR$/src/test" isTestSource="true" />
       <excludeFolder url="file://$MODULE_DIR$/build" />
     </content>
diff --git a/dev-tools/maven/lucene/contrib/ant/pom.xml.template b/dev-tools/maven/lucene/contrib/ant/pom.xml.template
index cc03f19..da8a45f 100644
--- a/dev-tools/maven/lucene/contrib/ant/pom.xml.template
+++ b/dev-tools/maven/lucene/contrib/ant/pom.xml.template
@@ -43,7 +43,7 @@
     </dependency>
     <dependency>
       <groupId>${project.groupId}</groupId>
-      <artifactId>lucene-core</artifactId>
+      <artifactId>lucene-test-framework</artifactId>
       <version>${project.version}</version>
       <type>test-jar</type>
       <scope>test</scope>
diff --git a/dev-tools/maven/lucene/contrib/db/bdb-je/pom.xml.template b/dev-tools/maven/lucene/contrib/db/bdb-je/pom.xml.template
index fd5934f..3f17208 100644
--- a/dev-tools/maven/lucene/contrib/db/bdb-je/pom.xml.template
+++ b/dev-tools/maven/lucene/contrib/db/bdb-je/pom.xml.template
@@ -43,7 +43,7 @@
     </dependency>
     <dependency>
       <groupId>${project.groupId}</groupId>
-      <artifactId>lucene-core</artifactId>
+      <artifactId>lucene-test-framework</artifactId>
       <version>${project.version}</version>
       <type>test-jar</type>
       <scope>test</scope>
diff --git a/dev-tools/maven/lucene/contrib/db/bdb/pom.xml.template b/dev-tools/maven/lucene/contrib/db/bdb/pom.xml.template
index cf17c67..6a7d4a0 100644
--- a/dev-tools/maven/lucene/contrib/db/bdb/pom.xml.template
+++ b/dev-tools/maven/lucene/contrib/db/bdb/pom.xml.template
@@ -43,7 +43,7 @@
     </dependency>
     <dependency>
       <groupId>${project.groupId}</groupId>
-      <artifactId>lucene-core</artifactId>
+      <artifactId>lucene-test-framework</artifactId>
       <version>${project.version}</version>
       <type>test-jar</type>
       <scope>test</scope>
diff --git a/dev-tools/maven/lucene/contrib/demo/pom.xml.template b/dev-tools/maven/lucene/contrib/demo/pom.xml.template
index c5d2132..0efced3 100644
--- a/dev-tools/maven/lucene/contrib/demo/pom.xml.template
+++ b/dev-tools/maven/lucene/contrib/demo/pom.xml.template
@@ -43,7 +43,7 @@
     </dependency>
     <dependency>
       <groupId>${project.groupId}</groupId>
-      <artifactId>lucene-core</artifactId>
+      <artifactId>lucene-test-framework</artifactId>
       <version>${project.version}</version>
       <type>test-jar</type>
       <scope>test</scope>
diff --git a/dev-tools/maven/lucene/contrib/highlighter/pom.xml.template b/dev-tools/maven/lucene/contrib/highlighter/pom.xml.template
index 8cf32e3..8259493 100644
--- a/dev-tools/maven/lucene/contrib/highlighter/pom.xml.template
+++ b/dev-tools/maven/lucene/contrib/highlighter/pom.xml.template
@@ -45,7 +45,7 @@
     </dependency>
     <dependency>
       <groupId>${project.groupId}</groupId>
-      <artifactId>lucene-core</artifactId>
+      <artifactId>lucene-test-framework</artifactId>
       <version>${project.version}</version>
       <type>test-jar</type>
       <scope>test</scope>
diff --git a/dev-tools/maven/lucene/contrib/instantiated/pom.xml.template b/dev-tools/maven/lucene/contrib/instantiated/pom.xml.template
index c2a9e2c..13f8cb7 100644
--- a/dev-tools/maven/lucene/contrib/instantiated/pom.xml.template
+++ b/dev-tools/maven/lucene/contrib/instantiated/pom.xml.template
@@ -43,7 +43,7 @@
     </dependency>
     <dependency>
       <groupId>${project.groupId}</groupId>
-      <artifactId>lucene-core</artifactId>
+      <artifactId>lucene-test-framework</artifactId>
       <version>${project.version}</version>
       <type>test-jar</type>
       <scope>test</scope>
diff --git a/dev-tools/maven/lucene/contrib/lucli/pom.xml.template b/dev-tools/maven/lucene/contrib/lucli/pom.xml.template
index f792841..391178a 100644
--- a/dev-tools/maven/lucene/contrib/lucli/pom.xml.template
+++ b/dev-tools/maven/lucene/contrib/lucli/pom.xml.template
@@ -43,7 +43,7 @@
     </dependency>
     <dependency>
       <groupId>${project.groupId}</groupId>
-      <artifactId>lucene-core</artifactId>
+      <artifactId>lucene-test-framework</artifactId>
       <version>${project.version}</version>
       <type>test-jar</type>
       <scope>test</scope>
diff --git a/dev-tools/maven/lucene/contrib/memory/pom.xml.template b/dev-tools/maven/lucene/contrib/memory/pom.xml.template
index 5922bf5..b82f713 100644
--- a/dev-tools/maven/lucene/contrib/memory/pom.xml.template
+++ b/dev-tools/maven/lucene/contrib/memory/pom.xml.template
@@ -45,7 +45,7 @@
     </dependency>
     <dependency>
       <groupId>${project.groupId}</groupId>
-      <artifactId>lucene-core</artifactId>
+      <artifactId>lucene-test-framework</artifactId>
       <version>${project.version}</version>
       <type>test-jar</type>
       <scope>test</scope>
diff --git a/dev-tools/maven/lucene/contrib/misc/pom.xml.template b/dev-tools/maven/lucene/contrib/misc/pom.xml.template
index f2ba799..d076f68 100644
--- a/dev-tools/maven/lucene/contrib/misc/pom.xml.template
+++ b/dev-tools/maven/lucene/contrib/misc/pom.xml.template
@@ -50,6 +50,13 @@
     </dependency>
     <dependency>
       <groupId>${project.groupId}</groupId>
+      <artifactId>lucene-test-framework</artifactId>
+      <version>${project.version}</version>
+      <type>test-jar</type>
+      <scope>test</scope>
+    </dependency>
+    <dependency>
+      <groupId>${project.groupId}</groupId>
       <artifactId>lucene-analyzers-common</artifactId>
       <version>${project.version}</version>
     </dependency>
diff --git a/dev-tools/maven/lucene/contrib/queries/pom.xml.template b/dev-tools/maven/lucene/contrib/queries/pom.xml.template
index be2a216..8feea4b 100644
--- a/dev-tools/maven/lucene/contrib/queries/pom.xml.template
+++ b/dev-tools/maven/lucene/contrib/queries/pom.xml.template
@@ -45,7 +45,7 @@
     </dependency>
     <dependency>
       <groupId>${project.groupId}</groupId>
-      <artifactId>lucene-core</artifactId>
+      <artifactId>lucene-test-framework</artifactId>
       <version>${project.version}</version>
       <type>test-jar</type>
       <scope>test</scope>
diff --git a/dev-tools/maven/lucene/contrib/queryparser/pom.xml.template b/dev-tools/maven/lucene/contrib/queryparser/pom.xml.template
index 1640c94..ecbaf9e 100644
--- a/dev-tools/maven/lucene/contrib/queryparser/pom.xml.template
+++ b/dev-tools/maven/lucene/contrib/queryparser/pom.xml.template
@@ -51,6 +51,13 @@
       <scope>test</scope>
     </dependency>
     <dependency>
+      <groupId>${project.groupId}</groupId>
+      <artifactId>lucene-test-framework</artifactId>
+      <version>${project.version}</version>
+      <type>test-jar</type>
+      <scope>test</scope>
+    </dependency>
+    <dependency>
       <groupId>junit</groupId>
       <artifactId>junit</artifactId>
       <scope>test</scope>
diff --git a/dev-tools/maven/lucene/contrib/spatial/pom.xml.template b/dev-tools/maven/lucene/contrib/spatial/pom.xml.template
index da42c87..0db411f 100644
--- a/dev-tools/maven/lucene/contrib/spatial/pom.xml.template
+++ b/dev-tools/maven/lucene/contrib/spatial/pom.xml.template
@@ -43,7 +43,7 @@
     </dependency>
     <dependency>
       <groupId>${project.groupId}</groupId>
-      <artifactId>lucene-core</artifactId>
+      <artifactId>lucene-test-framework</artifactId>
       <version>${project.version}</version>
       <type>test-jar</type>
       <scope>test</scope>
diff --git a/dev-tools/maven/lucene/contrib/spellchecker/pom.xml.template b/dev-tools/maven/lucene/contrib/spellchecker/pom.xml.template
index a65a107..845e196 100644
--- a/dev-tools/maven/lucene/contrib/spellchecker/pom.xml.template
+++ b/dev-tools/maven/lucene/contrib/spellchecker/pom.xml.template
@@ -43,7 +43,7 @@
     </dependency>
     <dependency>
       <groupId>${project.groupId}</groupId>
-      <artifactId>lucene-core</artifactId>
+      <artifactId>lucene-test-framework</artifactId>
       <version>${project.version}</version>
       <type>test-jar</type>
       <scope>test</scope>
diff --git a/dev-tools/maven/lucene/contrib/swing/pom.xml.template b/dev-tools/maven/lucene/contrib/swing/pom.xml.template
index 31af986..649e422 100644
--- a/dev-tools/maven/lucene/contrib/swing/pom.xml.template
+++ b/dev-tools/maven/lucene/contrib/swing/pom.xml.template
@@ -43,7 +43,7 @@
     </dependency>
     <dependency>
       <groupId>${project.groupId}</groupId>
-      <artifactId>lucene-core</artifactId>
+      <artifactId>lucene-test-framework</artifactId>
       <version>${project.version}</version>
       <type>test-jar</type>
       <scope>test</scope>
diff --git a/dev-tools/maven/lucene/contrib/wordnet/pom.xml.template b/dev-tools/maven/lucene/contrib/wordnet/pom.xml.template
index 6a4604f..ed72a97 100644
--- a/dev-tools/maven/lucene/contrib/wordnet/pom.xml.template
+++ b/dev-tools/maven/lucene/contrib/wordnet/pom.xml.template
@@ -43,7 +43,7 @@
     </dependency>
     <dependency>
       <groupId>${project.groupId}</groupId>
-      <artifactId>lucene-core</artifactId>
+      <artifactId>lucene-test-framework</artifactId>
       <version>${project.version}</version>
       <type>test-jar</type>
       <scope>test</scope>
diff --git a/dev-tools/maven/lucene/contrib/xml-query-parser/pom.xml.template b/dev-tools/maven/lucene/contrib/xml-query-parser/pom.xml.template
index 66efaab..7a54371 100644
--- a/dev-tools/maven/lucene/contrib/xml-query-parser/pom.xml.template
+++ b/dev-tools/maven/lucene/contrib/xml-query-parser/pom.xml.template
@@ -43,7 +43,7 @@
     </dependency>
     <dependency>
       <groupId>${project.groupId}</groupId>
-      <artifactId>lucene-core</artifactId>
+      <artifactId>lucene-test-framework</artifactId>
       <version>${project.version}</version>
       <type>test-jar</type>
       <scope>test</scope>
diff --git a/dev-tools/maven/lucene/pom.xml.template b/dev-tools/maven/lucene/pom.xml.template
index b6c2fdf..19fda15 100644
--- a/dev-tools/maven/lucene/pom.xml.template
+++ b/dev-tools/maven/lucene/pom.xml.template
@@ -33,6 +33,7 @@
   <description>Lucene parent POM</description>
   <modules>
     <module>src</module>
+    <module>src/test-framework</module>
     <module>contrib</module>
   </modules>
   <build>
diff --git a/dev-tools/maven/lucene/src/pom.xml.template b/dev-tools/maven/lucene/src/pom.xml.template
index 4f65081..6488755 100644
--- a/dev-tools/maven/lucene/src/pom.xml.template
+++ b/dev-tools/maven/lucene/src/pom.xml.template
@@ -121,6 +121,24 @@
           </programs>
         </configuration>
       </plugin>
+      <plugin>
+        <groupId>org.codehaus.mojo</groupId>
+        <artifactId>build-helper-maven-plugin</artifactId>
+        <executions>
+          <execution>
+            <id>add-test-source</id>
+            <phase>generate-test-sources</phase>
+            <goals>
+              <goal>add-test-source</goal>
+            </goals>
+            <configuration>
+              <sources>
+                <source>test-framework</source>
+              </sources>
+            </configuration>
+          </execution>
+        </executions>
+      </plugin>
     </plugins>
   </build>
 </project>
diff --git a/dev-tools/maven/lucene/src/test-framework/pom.xml.template b/dev-tools/maven/lucene/src/test-framework/pom.xml.template
new file mode 100644
index 0000000..e391e5f
--- /dev/null
+++ b/dev-tools/maven/lucene/src/test-framework/pom.xml.template
@@ -0,0 +1,89 @@
+<project xmlns="http://maven.apache.org/POM/4.0.0"
+         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+  <!--
+    Licensed to the Apache Software Foundation (ASF) under one
+    or more contributor license agreements.  See the NOTICE file
+    distributed with this work for additional information
+    regarding copyright ownership.  The ASF licenses this file
+    to you under the Apache License, Version 2.0 (the
+    "License"); you may not use this file except in compliance
+    with the License.  You may obtain a copy of the License at
+    
+    http://www.apache.org/licenses/LICENSE-2.0
+    
+    Unless required by applicable law or agreed to in writing,
+    software distributed under the License is distributed on an
+    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+    KIND, either express or implied.  See the License for the
+    specific language governing permissions and limitations
+    under the License.
+  -->
+  <modelVersion>4.0.0</modelVersion>
+  <parent>
+    <groupId>org.apache.lucene</groupId>
+    <artifactId>lucene-parent</artifactId>
+    <version>@version@</version>
+    <relativePath>../../pom.xml</relativePath>
+  </parent>
+  <groupId>org.apache.lucene</groupId>
+  <artifactId>lucene-test-framework</artifactId>
+  <packaging>jar</packaging>
+  <name>Lucene Test Framework</name>
+  <description>Apache Lucene Java Test Framework</description>
+  <properties>
+    <module-directory>lucene/src/test-framework</module-directory>
+    <build-directory>../../build</build-directory>
+  </properties>
+  <dependencies>
+    <dependency>
+      <groupId>${project.groupId}</groupId>
+      <artifactId>lucene-core</artifactId>
+      <version>${project.version}</version>
+      <scope>test</scope>
+    </dependency>
+    <dependency>
+      <groupId>junit</groupId>
+      <artifactId>junit</artifactId>
+      <scope>test</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.ant</groupId>
+      <artifactId>ant-junit</artifactId>
+      <scope>test</scope>
+    </dependency>
+  </dependencies>
+  <build>
+    <directory>${build-directory}/classes/test-framework</directory>
+    <testOutputDirectory>${build-directory}/classes/test-framework</testOutputDirectory>
+    <testSourceDirectory>.</testSourceDirectory>
+    <testResources>
+      <testResource>
+        <directory>${project.build.testSourceDirectory}</directory>
+        <excludes>
+          <exclude>**/*.java</exclude>
+        </excludes>
+      </testResource>
+    </testResources>
+    <plugins>
+      <plugin>
+        <groupId>org.apache.maven.plugins</groupId>
+        <artifactId>maven-jar-plugin</artifactId>
+        <executions>
+          <execution>
+            <goals>
+              <goal>test-jar</goal>
+            </goals>
+          </execution>
+        </executions>
+      </plugin>
+      <plugin>
+        <groupId>org.apache.maven.plugins</groupId>
+        <artifactId>maven-surefire-plugin</artifactId>
+        <configuration>
+          <skip>true</skip>
+        </configuration>
+      </plugin>
+    </plugins>
+  </build>
+</project>
diff --git a/dev-tools/maven/modules/analysis/common/pom.xml.template b/dev-tools/maven/modules/analysis/common/pom.xml.template
index 99eb404..19f8615 100644
--- a/dev-tools/maven/modules/analysis/common/pom.xml.template
+++ b/dev-tools/maven/modules/analysis/common/pom.xml.template
@@ -49,6 +49,13 @@
       <scope>test</scope>
     </dependency>
     <dependency>
+      <groupId>${project.groupId}</groupId>
+      <artifactId>lucene-test-framework</artifactId>
+      <version>${project.version}</version>
+      <type>test-jar</type>
+      <scope>test</scope>
+    </dependency>
+    <dependency>
       <groupId>junit</groupId>
       <artifactId>junit</artifactId>
       <scope>test</scope>
diff --git a/dev-tools/maven/modules/analysis/icu/pom.xml.template b/dev-tools/maven/modules/analysis/icu/pom.xml.template
index e98fff3..363364c 100644
--- a/dev-tools/maven/modules/analysis/icu/pom.xml.template
+++ b/dev-tools/maven/modules/analysis/icu/pom.xml.template
@@ -53,6 +53,13 @@
     </dependency>
     <dependency>
       <groupId>${project.groupId}</groupId>
+      <artifactId>lucene-test-framework</artifactId>
+      <version>${project.version}</version>
+      <type>test-jar</type>
+      <scope>test</scope>
+    </dependency>
+    <dependency>
+      <groupId>${project.groupId}</groupId>
       <artifactId>lucene-analyzers-common</artifactId>
       <version>${project.version}</version>
     </dependency>
diff --git a/dev-tools/maven/modules/analysis/smartcn/pom.xml.template b/dev-tools/maven/modules/analysis/smartcn/pom.xml.template
index 10f1e91..26a7e4f 100644
--- a/dev-tools/maven/modules/analysis/smartcn/pom.xml.template
+++ b/dev-tools/maven/modules/analysis/smartcn/pom.xml.template
@@ -43,7 +43,7 @@
     </dependency>
     <dependency>
       <groupId>${project.groupId}</groupId>
-      <artifactId>lucene-core</artifactId>
+      <artifactId>lucene-test-framework</artifactId>
       <version>${project.version}</version>
       <type>test-jar</type>
       <scope>test</scope>
diff --git a/dev-tools/maven/modules/analysis/stempel/pom.xml.template b/dev-tools/maven/modules/analysis/stempel/pom.xml.template
index 979498f..cacb74d 100644
--- a/dev-tools/maven/modules/analysis/stempel/pom.xml.template
+++ b/dev-tools/maven/modules/analysis/stempel/pom.xml.template
@@ -43,7 +43,7 @@
     </dependency>
     <dependency>
       <groupId>${project.groupId}</groupId>
-      <artifactId>lucene-core</artifactId>
+      <artifactId>lucene-test-framework</artifactId>
       <version>${project.version}</version>
       <type>test-jar</type>
       <scope>test</scope>
diff --git a/dev-tools/maven/modules/benchmark/pom.xml.template b/dev-tools/maven/modules/benchmark/pom.xml.template
index 263a38f..184c190 100755
--- a/dev-tools/maven/modules/benchmark/pom.xml.template
+++ b/dev-tools/maven/modules/benchmark/pom.xml.template
@@ -43,7 +43,7 @@
     </dependency>
     <dependency>
       <groupId>${project.groupId}</groupId>
-      <artifactId>lucene-core</artifactId>
+      <artifactId>lucene-test-framework</artifactId>
       <version>${project.version}</version>
       <type>test-jar</type>
       <scope>test</scope>
diff --git a/dev-tools/maven/solr/contrib/analysis-extras/pom.xml.template b/dev-tools/maven/solr/contrib/analysis-extras/pom.xml.template
index ff4e14d..0ff4b13 100644
--- a/dev-tools/maven/solr/contrib/analysis-extras/pom.xml.template
+++ b/dev-tools/maven/solr/contrib/analysis-extras/pom.xml.template
@@ -76,7 +76,7 @@
     </dependency>
     <dependency>
       <groupId>org.apache.lucene</groupId>
-      <artifactId>lucene-core</artifactId>
+      <artifactId>lucene-test-framework</artifactId>
       <version>${project.version}</version>
       <type>test-jar</type>
       <scope>test</scope>
diff --git a/dev-tools/maven/solr/contrib/clustering/pom.xml.template b/dev-tools/maven/solr/contrib/clustering/pom.xml.template
index 18afe9e..3b47b21 100644
--- a/dev-tools/maven/solr/contrib/clustering/pom.xml.template
+++ b/dev-tools/maven/solr/contrib/clustering/pom.xml.template
@@ -61,7 +61,7 @@
     </dependency>
     <dependency>
       <groupId>org.apache.lucene</groupId>
-      <artifactId>lucene-core</artifactId>
+      <artifactId>lucene-test-framework</artifactId>
       <version>${project.version}</version>
       <type>test-jar</type>
       <scope>test</scope>
diff --git a/dev-tools/maven/solr/contrib/dataimporthandler/src/extras/pom.xml.template b/dev-tools/maven/solr/contrib/dataimporthandler/src/extras/pom.xml.template
index 6a93cfb..739465a 100644
--- a/dev-tools/maven/solr/contrib/dataimporthandler/src/extras/pom.xml.template
+++ b/dev-tools/maven/solr/contrib/dataimporthandler/src/extras/pom.xml.template
@@ -68,7 +68,7 @@
     </dependency>
     <dependency>
       <groupId>org.apache.lucene</groupId>
-      <artifactId>lucene-core</artifactId>
+      <artifactId>lucene-test-framework</artifactId>
       <version>${project.version}</version>
       <type>test-jar</type>
       <scope>test</scope>
diff --git a/dev-tools/maven/solr/contrib/dataimporthandler/src/pom.xml.template b/dev-tools/maven/solr/contrib/dataimporthandler/src/pom.xml.template
index ccf9242..e0ea149 100644
--- a/dev-tools/maven/solr/contrib/dataimporthandler/src/pom.xml.template
+++ b/dev-tools/maven/solr/contrib/dataimporthandler/src/pom.xml.template
@@ -61,7 +61,7 @@
     </dependency>
     <dependency>
       <groupId>org.apache.lucene</groupId>
-      <artifactId>lucene-core</artifactId>
+      <artifactId>lucene-test-framework</artifactId>
       <version>${project.version}</version>
       <type>test-jar</type>
       <scope>test</scope>
diff --git a/dev-tools/maven/solr/contrib/extraction/pom.xml.template b/dev-tools/maven/solr/contrib/extraction/pom.xml.template
index 8bd8317..6d76eae 100644
--- a/dev-tools/maven/solr/contrib/extraction/pom.xml.template
+++ b/dev-tools/maven/solr/contrib/extraction/pom.xml.template
@@ -64,7 +64,7 @@
     </dependency>
     <dependency>
       <groupId>org.apache.lucene</groupId>
-      <artifactId>lucene-core</artifactId>
+      <artifactId>lucene-test-framework</artifactId>
       <version>${project.version}</version>
       <type>test-jar</type>
       <scope>test</scope>
diff --git a/dev-tools/maven/solr/contrib/uima/pom.xml.template b/dev-tools/maven/solr/contrib/uima/pom.xml.template
index a7802c5..ef31460 100644
--- a/dev-tools/maven/solr/contrib/uima/pom.xml.template
+++ b/dev-tools/maven/solr/contrib/uima/pom.xml.template
@@ -56,7 +56,7 @@
     </dependency>
     <dependency>
       <groupId>org.apache.lucene</groupId>
-      <artifactId>lucene-core</artifactId>
+      <artifactId>lucene-test-framework</artifactId>
       <version>${project.version}</version>
       <type>test-jar</type>
       <scope>test</scope>
diff --git a/dev-tools/maven/solr/src/pom.xml.template b/dev-tools/maven/solr/src/pom.xml.template
index df9bb50..ec3d625 100644
--- a/dev-tools/maven/solr/src/pom.xml.template
+++ b/dev-tools/maven/solr/src/pom.xml.template
@@ -48,7 +48,7 @@
     </dependency>
     <dependency>
       <groupId>org.apache.lucene</groupId>
-      <artifactId>lucene-core</artifactId>
+      <artifactId>lucene-test-framework</artifactId>
       <version>${project.version}</version>
       <type>test-jar</type>
       <scope>test</scope>
diff --git a/dev-tools/maven/solr/src/solrj/pom.xml.template b/dev-tools/maven/solr/src/solrj/pom.xml.template
index dcf961b..e4ed4c7 100644
--- a/dev-tools/maven/solr/src/solrj/pom.xml.template
+++ b/dev-tools/maven/solr/src/solrj/pom.xml.template
@@ -44,7 +44,7 @@
     </dependency>
     <dependency>
       <groupId>org.apache.lucene</groupId>
-      <artifactId>lucene-core</artifactId>
+      <artifactId>lucene-test-framework</artifactId>
       <version>${project.version}</version>
       <type>test-jar</type>
       <scope>test</scope>
diff --git a/dev-tools/testjar/testfiles b/dev-tools/testjar/testfiles
deleted file mode 100755
index 84d8bfb..0000000
--- a/dev-tools/testjar/testfiles
+++ /dev/null
@@ -1,24 +0,0 @@
-core.test.files=\
-	org/apache/lucene/util/_TestUtil.java,\
-	org/apache/lucene/util/LineFileDocs.java,\
-	org/apache/lucene/util/LuceneJUnitDividingSelector.java,\
-	org/apache/lucene/util/LuceneJUnitResultFormatter.java,\
-	org/apache/lucene/util/LuceneTestCase.java,\
-	org/apache/lucene/util/automaton/AutomatonTestUtil.java,\
-	org/apache/lucene/search/QueryUtils.java,\
-	org/apache/lucene/analysis/BaseTokenStreamTestCase.java,\
-	org/apache/lucene/analysis/MockAnalyzer.java,\
-	org/apache/lucene/analysis/MockPayloadAnalyzer.java,\
-	org/apache/lucene/analysis/MockTokenFilter.java,\
-	org/apache/lucene/analysis/MockTokenizer.java,\
-	org/apache/lucene/index/MockIndexInput.java,\
-	org/apache/lucene/index/RandomIndexWriter.java,\
-	org/apache/lucene/index/DocHelper.java,\
-	org/apache/lucene/codecs/preflexrw/PreFlexFieldsWriter.java,\
-	org/apache/lucene/codecs/preflexrw/PreFlexRWCodec.java,\
-	org/apache/lucene/codecs/preflexrw/TermInfosWriter.java,\
-	org/apache/lucene/codecs/mockrandom/MockRandomCodec.java,\
-	org/apache/lucene/store/_TestHelper.java,\
-	org/apache/lucene/store/MockDirectoryWrapper.java,\
-	org/apache/lucene/store/MockIndexInputWrapper.java,\
-	org/apache/lucene/store/MockIndexOutputWrapper.java,\
diff --git a/lucene/CHANGES.txt b/lucene/CHANGES.txt
index 32b9ede..bdd4940 100644
--- a/lucene/CHANGES.txt
+++ b/lucene/CHANGES.txt
@@ -978,6 +978,10 @@ Build
 * LUCENE-2657: Switch from using Maven POM templates to full POMs when
   generating Maven artifacts (Steven Rowe)
 
+* LUCENE-2609: Added jar-test-framework Ant target which packages Lucene's
+  tests' framework classes. (Drew Farris, Grant Ingersoll, Shai Erera, Steven 
+  Rowe)
+
 Test Cases
 
 * LUCENE-2037 Allow Junit4 tests in our environment (Erick Erickson
diff --git a/lucene/build.xml b/lucene/build.xml
index 3fe5b81..4cb7a82 100644
--- a/lucene/build.xml
+++ b/lucene/build.xml
@@ -17,7 +17,8 @@
     limitations under the License.
  -->
 
-<project name="core" default="default" basedir=".">
+<project name="core" default="default" basedir="."
+         xmlns:artifact="antlib:org.apache.maven.artifact.ant">
 
   <property name="junit.includes" value="**/Test*.java"/>
 
@@ -32,12 +33,13 @@
   	<path refid="classpath"/>
     <path refid="junit-path"/>
     <path refid="ant-path"/>
+    <pathelement location="${build.dir}/classes/test-framework"/>
     <pathelement location="${build.dir}/classes/test"/>
   </path>
 
-
   <path id="junit.classpath">
     <path refid="junit-path"/>
+    <pathelement location="${build.dir}/classes/test-framework"/>
     <pathelement location="${build.dir}/classes/test"/>
     <pathelement location="${build.dir}/classes/java"/>
     <pathelement path="${java.class.path}"/>
@@ -153,7 +155,7 @@
   </target>
 
   <target name="javadocs" description="Generate javadoc" 
-          depends="javadocs-all, javadocs-core, javadocs-contrib">
+          depends="javadocs-all, javadocs-core, javadocs-contrib, javadocs-test-framework">
     <echo file="${javadoc.dir}/index.html" append="false">
 <![CDATA[<html><head><title>${Name} ${version} Javadoc Index</title></head>
 <body>
@@ -259,7 +261,7 @@
   <!-- ================================================================== -->
   <!--                                                                    -->
   <!-- ================================================================== -->
-  <target name="package" depends="jar-core, jar-core-test, javadocs, build-contrib, init-dist, changes-to-html"/>
+  <target name="package" depends="jar-core, jar-test-framework, javadocs, build-contrib, init-dist, changes-to-html"/>
 
   <target name="nightly" depends="test, package-tgz">
   </target>
@@ -389,7 +391,7 @@
 
   <target name="dist-all" depends="dist, dist-src"/>
 
-  <target name="generate-maven-artifacts" depends="maven.ant.tasks-check, package, jar-src, javadocs">
+  <target name="generate-maven-artifacts" depends="maven.ant.tasks-check, package, jar-src, jar-test-framework-src, javadocs">
     <sequential>
 	  <ant target="get-maven-poms" dir=".."/>
  
@@ -405,6 +407,22 @@
                   classifier="tests"/>
         </artifact-attachments>
       </m2-deploy>
+
+      <artifact:install-provider artifactId="wagon-ssh" version="1.0-beta-7"/>
+      <artifact:pom id="test-framework-pom" file="src/test-framework/pom.xml"/>
+      <artifact:deploy>
+        <attach file="${build.dir}/${final.name}-tests.jar" 
+                classifier="tests" />
+        <attach file="${build.dir}/${final.name}-tests-src.jar"
+                classifier="test-sources"/>
+        <attach file="${build.dir}/${final.name}-tests-javadoc.jar"
+                classifier="test-javadoc"/>
+        <remoteRepository url="${m2.repository.url}">
+          <authentication username="${m2.repository.username}" privateKey="${m2.repository.private.key}"/>
+        </remoteRepository>
+        <pom refid="test-framework-pom"/>
+      </artifact:deploy>
+
       <contrib-crawl target="dist-maven"/>
     </sequential>
   </target>
@@ -602,16 +620,30 @@
     <patch patchfile="${patch.file}" strip="0"/>
   </target>
 
-  <target name="jar-core-test" depends="compile-test">
-    <!-- load the list of test files into a property named core.test.files -->
-  	<property file="../dev-tools/testjar/testfiles" />
-    <!-- copy the files so that we can compile and include both .java and .class in the jar --> 
-    <copy todir="${build.dir}/testjar" >
-        <fileset dir="src/test" includes="${core.test.files}" />
-    </copy>
-  	<compile-test-macro srcdir="${build.dir}/testjar" destdir="${build.dir}/testjar"
-						test.classpath="test.classpath"/>
-    <jarify basedir="${build.dir}/testjar" destfile="${build.dir}/${final.name}-tests.jar"/>
+  <target name="jar-test-framework" depends="compile-test-framework">
+    <jarify basedir="${build.dir}/classes/test-framework" destfile="${build.dir}/${final.name}-tests.jar"
+            title="Lucene Search Engine: Test Framework" />
+  </target>
+
+  <target name="javadocs-test-framework">
+	<sequential>
+      <mkdir dir="${javadoc.dir}/test-framework"/>
+      <invoke-javadoc
+          destdir="${javadoc.dir}/test-framework"
+          title="${Name} ${version} Test Framework API">
+        <sources>
+          <packageset dir="src/test-framework"/>
+          <link href=""/>
+        </sources>
+      </invoke-javadoc>
+      <jarify basedir="${javadoc.dir}/test-framework" destfile="${build.dir}/${final.name}-tests-javadoc.jar"
+	          title="Lucene Search Engine: Test Framework" />
+    </sequential>
+  </target>
+
+  <target name="jar-test-framework-src" depends="init">
+    <jarify basedir="${tests-framework.src.dir}" destfile="${build.dir}/${final.name}-tests-src.jar"
+            title="Lucene Search Engine: Test Framework" />
   </target>
 
 </project>
diff --git a/lucene/common-build.xml b/lucene/common-build.xml
index b98368c..70baa82 100644
--- a/lucene/common-build.xml
+++ b/lucene/common-build.xml
@@ -113,6 +113,7 @@
 
   <property name="src.dir" location="src/java"/>
   <property name="tests.src.dir" location="src/test"/>
+  <property name="tests-framework.src.dir" location="${common.dir}/src/test-framework"/>
   <property name="build.dir" location="build"/>
   <property name="dist.dir" location="dist"/>
   <property name="maven.dist.dir" location="dist/maven"/>
@@ -363,6 +364,8 @@
   <macrodef name="jarify" description="Builds a JAR file">
   	<attribute name="basedir" default="${build.dir}/classes/java"/>
   	<attribute name="destfile" default="${build.dir}/${final.name}.jar"/>
+  	<attribute name="title" default="Lucene Search Engine: ${ant.project.name}"/>
+    <attribute name="excludes" default="**/pom.xml"/>
     <element name="manifest-attributes" optional="yes"/>
   	<element name="metainf-includes" optional="yes"/>
     <sequential>
@@ -372,12 +375,13 @@
         <arg line="."/>
       </exec>
       
-      <build-manifest/>
+      <build-manifest title="@{title}"/>
     	
       <jar
         destfile="@{destfile}"
         basedir="@{basedir}"
-      	manifest="${manifest.file}">
+      	manifest="${manifest.file}"
+        excludes="@{excludes}">
         <manifest>
         	<manifest-attributes/>
         </manifest>
@@ -390,7 +394,12 @@
     </sequential>
   </macrodef>
 
-  <target name="compile-test" depends="compile-core">
+  <target name="compile-test-framework" depends="compile-core">
+  	<compile-test-macro srcdir="${tests-framework.src.dir}" destdir="${common.dir}/build/classes/test-framework"
+  						test.classpath="test.classpath"/>
+  </target>
+
+  <target name="compile-test" depends="compile-test-framework">
   	<compile-test-macro srcdir="${tests.src.dir}" destdir="${build.dir}/classes/test"
   						test.classpath="test.classpath"/>
   </target>
@@ -553,6 +562,9 @@
       <fileset dir="${src.dir}">
         <include name="org/apache/**/*.java" />
       </fileset>
+      <testsources dir="${tests-framework.src.dir}">
+        <include name="org/apache/**/*.java" />
+      </testsources>
       <testsources dir="${tests.src.dir}">
         <include name="org/apache/**/*.java" />
       </testsources>
@@ -583,6 +595,9 @@
     <fileset dir="contrib" id="clover.contrib.test.src.files">
       <include name="**/test/**/*.java"/>
     </fileset>
+    <fileset dir="${tests-framework.src.dir}" id="clover.test.src.files">
+      <include name="**/*.java" />
+    </fileset>
     <fileset dir="${tests.src.dir}" id="clover.test.src.files">
       <include name="**/*.java" />
     </fileset>
@@ -653,6 +668,7 @@
 	  description="runs the tasks over source and test files">
     <rat:report xmlns:rat="antlib:org.apache.rat.anttasks">
       <fileset dir="${src.dir}"/>
+      <fileset dir="${tests-framework.src.dir}"/>
       <fileset dir="${tests.src.dir}"/>
     </rat:report>
   </target>
diff --git a/lucene/contrib/contrib-build.xml b/lucene/contrib/contrib-build.xml
index 77b1dd0..bbd3508 100644
--- a/lucene/contrib/contrib-build.xml
+++ b/lucene/contrib/contrib-build.xml
@@ -40,6 +40,7 @@
   
   <path id="test.base.classpath">
     <path refid="classpath"/>
+    <pathelement location="${common.dir}/build/classes/test-framework"/>
     <pathelement location="${common.dir}/build/classes/test"/>
     <path refid="junit-path"/>
     <pathelement location="${build.dir}/classes/java"/>
@@ -50,7 +51,6 @@
   <path id="junit.classpath">
     <path refid="test.classpath"/>
     <pathelement location="${build.dir}/classes/test"/>
-    <pathelement location="${build.dir}/classes/java"/>
     <pathelement path="${java.class.path}"/>
   </path>
 
diff --git a/lucene/contrib/db/bdb-je/build.xml b/lucene/contrib/db/bdb-je/build.xml
index b751d84..cc8c1c8 100644
--- a/lucene/contrib/db/bdb-je/build.xml
+++ b/lucene/contrib/db/bdb-je/build.xml
@@ -39,13 +39,6 @@
 	
   <import file="../../contrib-build.xml" />
 
-  <path id="test.classpath">
-    <path refid="classpath"/>
-    <pathelement location="../../../build/classes/test/"/>
-    <path refid="junit-path"/>
-    <pathelement location="${build.dir}/classes/java"/>
-  </path>
-
   <target name="get-je-jar" unless="je.jar.exists">
     <mkdir dir="lib" />
     <get src="http://download.oracle.com/maven/com/sleepycat/je/${je.version}/je-${je.version}.jar"
diff --git a/lucene/contrib/db/bdb/build.xml b/lucene/contrib/db/bdb/build.xml
index 39d22d0..4f7c742 100644
--- a/lucene/contrib/db/bdb/build.xml
+++ b/lucene/contrib/db/bdb/build.xml
@@ -39,13 +39,6 @@
 
   <import file="../../contrib-build.xml" />
 
-  <path id="test.classpath">
-    <path refid="classpath"/>
-    <pathelement location="../../../build/classes/test/"/>
-    <path refid="junit-path"/>
-    <pathelement location="${build.dir}/classes/java"/>
-  </path>
-
   <target name="get-db-jar" unless="db.jar.exists">
     <mkdir dir="lib" />
     <get src="http://downloads.osafoundation.org/db/db-${db.version}.jar"
diff --git a/lucene/src/test-framework/org/apache/lucene/analysis/BaseTokenStreamTestCase.java b/lucene/src/test-framework/org/apache/lucene/analysis/BaseTokenStreamTestCase.java
new file mode 100644
index 0000000..aac0351
--- /dev/null
+++ b/lucene/src/test-framework/org/apache/lucene/analysis/BaseTokenStreamTestCase.java
@@ -0,0 +1,219 @@
+package org.apache.lucene.analysis;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.StringReader;
+import java.io.IOException;
+ 
+import org.apache.lucene.analysis.tokenattributes.*;
+import org.apache.lucene.util.Attribute;
+import org.apache.lucene.util.AttributeImpl;
+import org.apache.lucene.util.LuceneTestCase;
+
+/** 
+ * Base class for all Lucene unit tests that use TokenStreams.  
+ */
+public abstract class BaseTokenStreamTestCase extends LuceneTestCase {
+  // some helpers to test Analyzers and TokenStreams:
+  
+  public static interface CheckClearAttributesAttribute extends Attribute {
+    boolean getAndResetClearCalled();
+  }
+
+  public static final class CheckClearAttributesAttributeImpl extends AttributeImpl implements CheckClearAttributesAttribute {
+    private boolean clearCalled = false;
+    
+    public boolean getAndResetClearCalled() {
+      try {
+        return clearCalled;
+      } finally {
+        clearCalled = false;
+      }
+    }
+
+    @Override
+    public void clear() {
+      clearCalled = true;
+    }
+
+    @Override
+    public boolean equals(Object other) {
+      return (
+        other instanceof CheckClearAttributesAttributeImpl &&
+        ((CheckClearAttributesAttributeImpl) other).clearCalled == this.clearCalled
+      );
+    }
+
+    @Override
+    public int hashCode() {
+      return 76137213 ^ Boolean.valueOf(clearCalled).hashCode();
+    }
+    
+    @Override
+    public void copyTo(AttributeImpl target) {
+      ((CheckClearAttributesAttributeImpl) target).clear();
+    }
+  }
+
+  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], Integer finalOffset) throws IOException {
+    assertNotNull(output);
+    CheckClearAttributesAttribute checkClearAtt = ts.addAttribute(CheckClearAttributesAttribute.class);
+    
+    assertTrue("has no CharTermAttribute", ts.hasAttribute(CharTermAttribute.class));
+    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);
+    
+    OffsetAttribute offsetAtt = null;
+    if (startOffsets != null || endOffsets != null || finalOffset != null) {
+      assertTrue("has no OffsetAttribute", ts.hasAttribute(OffsetAttribute.class));
+      offsetAtt = ts.getAttribute(OffsetAttribute.class);
+    }
+    
+    TypeAttribute typeAtt = null;
+    if (types != null) {
+      assertTrue("has no TypeAttribute", ts.hasAttribute(TypeAttribute.class));
+      typeAtt = ts.getAttribute(TypeAttribute.class);
+    }
+    
+    PositionIncrementAttribute posIncrAtt = null;
+    if (posIncrements != null) {
+      assertTrue("has no PositionIncrementAttribute", ts.hasAttribute(PositionIncrementAttribute.class));
+      posIncrAtt = ts.getAttribute(PositionIncrementAttribute.class);
+    }
+    
+    ts.reset();
+    for (int i = 0; i < output.length; i++) {
+      // extra safety to enforce, that the state is not preserved and also assign bogus values
+      ts.clearAttributes();
+      termAtt.setEmpty().append("bogusTerm");
+      if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);
+      if (typeAtt != null) typeAtt.setType("bogusType");
+      if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);
+      
+      checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before
+      assertTrue("token "+i+" does not exist", ts.incrementToken());
+      assertTrue("clearAttributes() was not called correctly in TokenStream chain", checkClearAtt.getAndResetClearCalled());
+      
+      assertEquals("term "+i, output[i], termAtt.toString());
+      if (startOffsets != null)
+        assertEquals("startOffset "+i, startOffsets[i], offsetAtt.startOffset());
+      if (endOffsets != null)
+        assertEquals("endOffset "+i, endOffsets[i], offsetAtt.endOffset());
+      if (types != null)
+        assertEquals("type "+i, types[i], typeAtt.type());
+      if (posIncrements != null)
+        assertEquals("posIncrement "+i, posIncrements[i], posIncrAtt.getPositionIncrement());
+    }
+    assertFalse("end of stream", ts.incrementToken());
+    ts.end();
+    if (finalOffset != null)
+      assertEquals("finalOffset ", finalOffset.intValue(), offsetAtt.endOffset());
+    ts.close();
+  }
+  
+  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[]) throws IOException {
+    assertTokenStreamContents(ts, output, startOffsets, endOffsets, types, posIncrements, null);
+  }
+
+  public static void assertTokenStreamContents(TokenStream ts, String[] output) throws IOException {
+    assertTokenStreamContents(ts, output, null, null, null, null, null);
+  }
+  
+  public static void assertTokenStreamContents(TokenStream ts, String[] output, String[] types) throws IOException {
+    assertTokenStreamContents(ts, output, null, null, types, null, null);
+  }
+  
+  public static void assertTokenStreamContents(TokenStream ts, String[] output, int[] posIncrements) throws IOException {
+    assertTokenStreamContents(ts, output, null, null, null, posIncrements, null);
+  }
+  
+  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[]) throws IOException {
+    assertTokenStreamContents(ts, output, startOffsets, endOffsets, null, null, null);
+  }
+  
+  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], Integer finalOffset) throws IOException {
+    assertTokenStreamContents(ts, output, startOffsets, endOffsets, null, null, finalOffset);
+  }
+  
+  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], int[] posIncrements) throws IOException {
+    assertTokenStreamContents(ts, output, startOffsets, endOffsets, null, posIncrements, null);
+  }
+
+  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], int[] posIncrements, Integer finalOffset) throws IOException {
+    assertTokenStreamContents(ts, output, startOffsets, endOffsets, null, posIncrements, finalOffset);
+  }
+  
+  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[]) throws IOException {
+    assertTokenStreamContents(a.tokenStream("dummy", new StringReader(input)), output, startOffsets, endOffsets, types, posIncrements, input.length());
+  }
+  
+  public static void assertAnalyzesTo(Analyzer a, String input, String[] output) throws IOException {
+    assertAnalyzesTo(a, input, output, null, null, null, null);
+  }
+  
+  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, String[] types) throws IOException {
+    assertAnalyzesTo(a, input, output, null, null, types, null);
+  }
+  
+  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int[] posIncrements) throws IOException {
+    assertAnalyzesTo(a, input, output, null, null, null, posIncrements);
+  }
+  
+  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[]) throws IOException {
+    assertAnalyzesTo(a, input, output, startOffsets, endOffsets, null, null);
+  }
+  
+  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], int[] posIncrements) throws IOException {
+    assertAnalyzesTo(a, input, output, startOffsets, endOffsets, null, posIncrements);
+  }
+  
+
+  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[]) throws IOException {
+    assertTokenStreamContents(a.reusableTokenStream("dummy", new StringReader(input)), output, startOffsets, endOffsets, types, posIncrements, input.length());
+  }
+  
+  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output) throws IOException {
+    assertAnalyzesToReuse(a, input, output, null, null, null, null);
+  }
+  
+  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, String[] types) throws IOException {
+    assertAnalyzesToReuse(a, input, output, null, null, types, null);
+  }
+  
+  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, int[] posIncrements) throws IOException {
+    assertAnalyzesToReuse(a, input, output, null, null, null, posIncrements);
+  }
+  
+  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[]) throws IOException {
+    assertAnalyzesToReuse(a, input, output, startOffsets, endOffsets, null, null);
+  }
+  
+  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], int[] posIncrements) throws IOException {
+    assertAnalyzesToReuse(a, input, output, startOffsets, endOffsets, null, posIncrements);
+  }
+
+  // simple utility method for testing stemmers
+  
+  public static void checkOneTerm(Analyzer a, final String input, final String expected) throws IOException {
+    assertAnalyzesTo(a, input, new String[]{expected});
+  }
+  
+  public static void checkOneTermReuse(Analyzer a, final String input, final String expected) throws IOException {
+    assertAnalyzesToReuse(a, input, new String[]{expected});
+  }
+  
+}
diff --git a/lucene/src/test-framework/org/apache/lucene/analysis/MockAnalyzer.java b/lucene/src/test-framework/org/apache/lucene/analysis/MockAnalyzer.java
new file mode 100644
index 0000000..d23b093
--- /dev/null
+++ b/lucene/src/test-framework/org/apache/lucene/analysis/MockAnalyzer.java
@@ -0,0 +1,156 @@
+package org.apache.lucene.analysis;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.io.Reader;
+
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
+import org.apache.lucene.index.Payload;
+import org.apache.lucene.util.automaton.CharacterRunAutomaton;
+
+/**
+ * Analyzer for testing
+ */
+public final class MockAnalyzer extends Analyzer { 
+  private final CharacterRunAutomaton runAutomaton;
+  private final boolean lowerCase;
+  private final CharacterRunAutomaton filter;
+  private final boolean enablePositionIncrements;
+  private final boolean payload;
+  private int positionIncrementGap;
+
+  public MockAnalyzer(CharacterRunAutomaton runAutomaton, boolean lowerCase, CharacterRunAutomaton filter, boolean enablePositionIncrements) {
+    this(runAutomaton, lowerCase, filter, enablePositionIncrements, true);    
+  }
+
+  /**
+   * Creates a new MockAnalyzer.
+   * 
+   * @param runAutomaton DFA describing how tokenization should happen (e.g. [a-zA-Z]+)
+   * @param lowerCase true if the tokenizer should lowercase terms
+   * @param filter DFA describing how terms should be filtered (set of stopwords, etc)
+   * @param enablePositionIncrements true if position increments should reflect filtered terms.
+   * @param payload if payloads should be added
+   */
+  public MockAnalyzer(CharacterRunAutomaton runAutomaton, boolean lowerCase, CharacterRunAutomaton filter, boolean enablePositionIncrements, boolean payload) {
+    this.runAutomaton = runAutomaton;
+    this.lowerCase = lowerCase;
+    this.filter = filter;
+    this.enablePositionIncrements = enablePositionIncrements;
+    this.payload = payload;
+  }
+
+  /**
+   * Creates a new MockAnalyzer, with no filtering.
+   * 
+   * @param runAutomaton DFA describing how tokenization should happen (e.g. [a-zA-Z]+)
+   * @param lowerCase true if the tokenizer should lowercase terms
+   */
+  public MockAnalyzer(CharacterRunAutomaton runAutomaton, boolean lowerCase) {
+    this(runAutomaton, lowerCase, MockTokenFilter.EMPTY_STOPSET, false, true);
+  }
+
+  public MockAnalyzer(CharacterRunAutomaton runAutomaton, boolean lowerCase, boolean payload) {
+    this(runAutomaton, lowerCase, MockTokenFilter.EMPTY_STOPSET, false, payload);
+  }
+  
+  /** 
+   * Create a Whitespace-lowercasing analyzer with no stopwords removal 
+   */
+  public MockAnalyzer() {
+    this(MockTokenizer.WHITESPACE, true);
+  }
+
+  @Override
+  public TokenStream tokenStream(String fieldName, Reader reader) {
+    MockTokenizer tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase);
+    TokenFilter filt = new MockTokenFilter(tokenizer, filter, enablePositionIncrements);
+    if (payload){
+      filt = new SimplePayloadFilter(filt, fieldName);
+    }
+    return filt;
+  }
+
+  private class SavedStreams {
+    MockTokenizer tokenizer;
+    TokenFilter filter;
+  }
+
+  @Override
+  public TokenStream reusableTokenStream(String fieldName, Reader reader)
+      throws IOException {
+    SavedStreams saved = (SavedStreams) getPreviousTokenStream();
+    if (saved == null) {
+      saved = new SavedStreams();
+      saved.tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase);
+      saved.filter = new MockTokenFilter(saved.tokenizer, filter, enablePositionIncrements);
+      if (payload){
+        saved.filter = new SimplePayloadFilter(saved.filter, fieldName);
+      }
+      setPreviousTokenStream(saved);
+      return saved.filter;
+    } else {
+      saved.tokenizer.reset(reader);
+      saved.filter.reset();
+      return saved.filter;
+    }
+  }
+  
+  public void setPositionIncrementGap(int positionIncrementGap){
+    this.positionIncrementGap = positionIncrementGap;
+  }
+  
+  @Override
+  public int getPositionIncrementGap(String fieldName){
+    return positionIncrementGap;
+  }
+}
+
+final class SimplePayloadFilter extends TokenFilter {
+  String fieldName;
+  int pos;
+  final PayloadAttribute payloadAttr;
+  final CharTermAttribute termAttr;
+
+  public SimplePayloadFilter(TokenStream input, String fieldName) {
+    super(input);
+    this.fieldName = fieldName;
+    pos = 0;
+    payloadAttr = input.addAttribute(PayloadAttribute.class);
+    termAttr = input.addAttribute(CharTermAttribute.class);
+  }
+
+  @Override
+  public boolean incrementToken() throws IOException {
+    if (input.incrementToken()) {
+      payloadAttr.setPayload(new Payload(("pos: " + pos).getBytes()));
+      pos++;
+      return true;
+    } else {
+      return false;
+    }
+  }
+
+  @Override
+  public void reset() throws IOException {
+    super.reset();
+    pos = 0;
+  }
+}
diff --git a/lucene/src/test-framework/org/apache/lucene/analysis/MockPayloadAnalyzer.java b/lucene/src/test-framework/org/apache/lucene/analysis/MockPayloadAnalyzer.java
new file mode 100644
index 0000000..63d99af
--- /dev/null
+++ b/lucene/src/test-framework/org/apache/lucene/analysis/MockPayloadAnalyzer.java
@@ -0,0 +1,93 @@
+package org.apache.lucene.analysis;
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
+import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.index.Payload;
+
+import java.io.IOException;
+import java.io.Reader;
+
+
+/**
+ *
+ *
+ **/
+public final class MockPayloadAnalyzer extends Analyzer {
+
+  @Override
+  public TokenStream tokenStream(String fieldName, Reader reader) {
+    TokenStream result = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);
+    return new MockPayloadFilter(result, fieldName);
+  }
+}
+
+
+/**
+ *
+ *
+ **/
+final class MockPayloadFilter extends TokenFilter {
+  String fieldName;
+
+  int pos;
+
+  int i;
+
+  final PositionIncrementAttribute posIncrAttr;
+  final PayloadAttribute payloadAttr;
+  final CharTermAttribute termAttr;
+
+  public MockPayloadFilter(TokenStream input, String fieldName) {
+    super(input);
+    this.fieldName = fieldName;
+    pos = 0;
+    i = 0;
+    posIncrAttr = input.addAttribute(PositionIncrementAttribute.class);
+    payloadAttr = input.addAttribute(PayloadAttribute.class);
+    termAttr = input.addAttribute(CharTermAttribute.class);
+  }
+
+  @Override
+  public boolean incrementToken() throws IOException {
+    if (input.incrementToken()) {
+      payloadAttr.setPayload(new Payload(("pos: " + pos).getBytes()));
+      int posIncr;
+      if (i % 2 == 1) {
+        posIncr = 1;
+      } else {
+        posIncr = 0;
+      }
+      posIncrAttr.setPositionIncrement(posIncr);
+      pos += posIncr;
+      i++;
+      return true;
+    } else {
+      return false;
+    }
+  }
+
+  @Override
+  public void reset() throws IOException {
+    i = 0;
+    pos = 0;
+  }
+}
+
diff --git a/lucene/src/test-framework/org/apache/lucene/analysis/MockTokenFilter.java b/lucene/src/test-framework/org/apache/lucene/analysis/MockTokenFilter.java
new file mode 100644
index 0000000..f16165b
--- /dev/null
+++ b/lucene/src/test-framework/org/apache/lucene/analysis/MockTokenFilter.java
@@ -0,0 +1,101 @@
+package org.apache.lucene.analysis;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import static org.apache.lucene.util.automaton.BasicAutomata.makeEmpty;
+import static org.apache.lucene.util.automaton.BasicAutomata.makeString;
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.util.automaton.BasicOperations;
+import org.apache.lucene.util.automaton.CharacterRunAutomaton;
+
+/**
+ * A tokenfilter for testing that removes terms accepted by a DFA.
+ * <ul>
+ *  <li>Union a list of singletons to act like a stopfilter.
+ *  <li>Use the complement to act like a keepwordfilter
+ *  <li>Use a regex like <code>.{12,}</code> to act like a lengthfilter
+ * </ul>
+ */
+public final class MockTokenFilter extends TokenFilter {
+  /** Empty set of stopwords */
+  public static final CharacterRunAutomaton EMPTY_STOPSET =
+    new CharacterRunAutomaton(makeEmpty());
+  
+  /** Set of common english stopwords */
+  public static final CharacterRunAutomaton ENGLISH_STOPSET = 
+    new CharacterRunAutomaton(BasicOperations.union(Arrays.asList(
+      makeString("a"), makeString("an"), makeString("and"), makeString("are"),
+      makeString("as"), makeString("at"), makeString("be"), makeString("but"), 
+      makeString("by"), makeString("for"), makeString("if"), makeString("in"), 
+      makeString("into"), makeString("is"), makeString("it"), makeString("no"),
+      makeString("not"), makeString("of"), makeString("on"), makeString("or"), 
+      makeString("such"), makeString("that"), makeString("the"), makeString("their"), 
+      makeString("then"), makeString("there"), makeString("these"), makeString("they"), 
+      makeString("this"), makeString("to"), makeString("was"), makeString("will"), 
+      makeString("with"))));
+  
+  private final CharacterRunAutomaton filter;
+  private boolean enablePositionIncrements = false;
+
+  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
+  private final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);
+  
+  public MockTokenFilter(TokenStream input, CharacterRunAutomaton filter, boolean enablePositionIncrements) {
+    super(input);
+    this.filter = filter;
+    this.enablePositionIncrements = enablePositionIncrements;
+  }
+  
+  @Override
+  public boolean incrementToken() throws IOException {
+    // return the first non-stop word found
+    int skippedPositions = 0;
+    while (input.incrementToken()) {
+      if (!filter.run(termAtt.buffer(), 0, termAtt.length())) {
+        if (enablePositionIncrements) {
+          posIncrAtt.setPositionIncrement(posIncrAtt.getPositionIncrement() + skippedPositions);
+        }
+        return true;
+      }
+      skippedPositions += posIncrAtt.getPositionIncrement();
+    }
+    // reached EOS -- return false
+    return false;
+  }
+  
+  /**
+   * @see #setEnablePositionIncrements(boolean)
+   */
+  public boolean getEnablePositionIncrements() {
+    return enablePositionIncrements;
+  }
+
+  /**
+   * If <code>true</code>, this Filter will preserve
+   * positions of the incoming tokens (ie, accumulate and
+   * set position increments of the removed stop tokens).
+   */
+  public void setEnablePositionIncrements(boolean enable) {
+    this.enablePositionIncrements = enable;
+  }
+}
diff --git a/lucene/src/test-framework/org/apache/lucene/analysis/MockTokenizer.java b/lucene/src/test-framework/org/apache/lucene/analysis/MockTokenizer.java
new file mode 100644
index 0000000..017f828
--- /dev/null
+++ b/lucene/src/test-framework/org/apache/lucene/analysis/MockTokenizer.java
@@ -0,0 +1,83 @@
+package org.apache.lucene.analysis;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.io.Reader;
+
+import org.apache.lucene.util.Version;
+import org.apache.lucene.util.automaton.CharacterRunAutomaton;
+import org.apache.lucene.util.automaton.RegExp;
+
+/**
+ * Automaton-based tokenizer for testing. Optionally lowercases.
+ */
+public class MockTokenizer extends CharTokenizer {
+  /** Acts Similar to WhitespaceTokenizer */
+  public static final CharacterRunAutomaton WHITESPACE = 
+    new CharacterRunAutomaton(new RegExp("[^ \t\r\n]+").toAutomaton());
+  /** Acts Similar to KeywordTokenizer.
+   * TODO: Keyword returns an "empty" token for an empty reader... 
+   */
+  public static final CharacterRunAutomaton KEYWORD =
+    new CharacterRunAutomaton(new RegExp(".*").toAutomaton());
+  /** Acts like LetterTokenizer. */
+  // the ugly regex below is Unicode 5.2 [:Letter:]
+  public static final CharacterRunAutomaton SIMPLE =
+    new CharacterRunAutomaton(new RegExp("[A-Za-z?-??--??-??---??-???---??--??-??---?-??-??-?-?-??-??????-??--?-????--?-?????-?-????--?-???-?-??-??-?-??-????-????-?-??-????-???-??-??-????????-??-??-??-??---????-??-??---???-??-??--??-?-??--?-??-?-?????????-??-?-?-?-??????-??-??-???-????-???-???????-??-?????-???-????-???-???-?????-???-???-???-??-??-????-???-???-???-???-???-???-??-???-??-???-???-???-???-??-???-???-??-?????-?-??-?-??-?-?-??-??-??-???-?-??-????-?-??--?-?-?-??-??-??-??-?????-?--?-??-??-??-??-?--???-??????-?????-????????-???-??-??-???????-?-??--??--?-??-?-?--?-??-??-??-???????-????-???-???-??-??-???-???-??-??-?-???-???-??-???-??????-???-???-???-??-???-??????-???-???-???-??-?--?--??-?-??-??-??-??-?-????-???-??-?-??-???-??-?-?-??-??-???-??-?-??????-?-?-??-?----?-?-?-??-??-??-???-?????-?????-??????-????-????-????-????-????-????-????-????-????-????-????-????-??????-??????????-????-????-?????-????-????-???-???-????-????-????-????-?????-?????-????-????-??????????????-????-??????-????-????-????-????-????-????-????-??????-????-????-????-????-????-????-????-????-????-????-????-????-?????-??-??-?]+").toAutomaton());
+
+  private final CharacterRunAutomaton runAutomaton;
+  private final boolean lowerCase;
+  private int state;
+
+  public MockTokenizer(AttributeFactory factory, Reader input, CharacterRunAutomaton runAutomaton, boolean lowerCase) {
+    super(Version.LUCENE_CURRENT, factory, input);
+    this.runAutomaton = runAutomaton;
+    this.lowerCase = lowerCase;
+    this.state = runAutomaton.getInitialState();
+  }
+
+  public MockTokenizer(Reader input, CharacterRunAutomaton runAutomaton, boolean lowerCase) {
+    super(Version.LUCENE_CURRENT, input);
+    this.runAutomaton = runAutomaton;
+    this.lowerCase = lowerCase;
+    this.state = runAutomaton.getInitialState();
+  }
+  
+  @Override
+  protected boolean isTokenChar(int c) {
+    state = runAutomaton.step(state, c);
+    if (state < 0) {
+      state = runAutomaton.getInitialState();
+      return false;
+    } else {
+      return true;
+    }
+  }
+  
+  @Override
+  protected int normalize(int c) {
+    return lowerCase ? Character.toLowerCase(c) : c;
+  }
+
+  @Override
+  public void reset() throws IOException {
+    super.reset();
+    state = runAutomaton.getInitialState();
+  }
+}
diff --git a/lucene/src/test-framework/org/apache/lucene/index/DocHelper.java b/lucene/src/test-framework/org/apache/lucene/index/DocHelper.java
new file mode 100644
index 0000000..28bcdff
--- /dev/null
+++ b/lucene/src/test-framework/org/apache/lucene/index/DocHelper.java
@@ -0,0 +1,250 @@
+package org.apache.lucene.index;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.io.UnsupportedEncodingException;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.Fieldable;
+import org.apache.lucene.search.SimilarityProvider;
+import org.apache.lucene.store.Directory;
+import static org.apache.lucene.util.LuceneTestCase.TEST_VERSION_CURRENT;
+
+class DocHelper {
+  public static final String FIELD_1_TEXT = "field one text";
+  public static final String TEXT_FIELD_1_KEY = "textField1";
+  public static Field textField1 = new Field(TEXT_FIELD_1_KEY, FIELD_1_TEXT,
+      Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO);
+  
+  public static final String FIELD_2_TEXT = "field field field two text";
+  //Fields will be lexicographically sorted.  So, the order is: field, text, two
+  public static final int [] FIELD_2_FREQS = {3, 1, 1}; 
+  public static final String TEXT_FIELD_2_KEY = "textField2";
+  public static Field textField2 = new Field(TEXT_FIELD_2_KEY, FIELD_2_TEXT, Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS);
+  
+  public static final String FIELD_3_TEXT = "aaaNoNorms aaaNoNorms bbbNoNorms";
+  public static final String TEXT_FIELD_3_KEY = "textField3";
+  public static Field textField3 = new Field(TEXT_FIELD_3_KEY, FIELD_3_TEXT, Field.Store.YES, Field.Index.ANALYZED);
+  static { textField3.setOmitNorms(true); }
+
+  public static final String KEYWORD_TEXT = "Keyword";
+  public static final String KEYWORD_FIELD_KEY = "keyField";
+  public static Field keyField = new Field(KEYWORD_FIELD_KEY, KEYWORD_TEXT,
+      Field.Store.YES, Field.Index.NOT_ANALYZED);
+
+  public static final String NO_NORMS_TEXT = "omitNormsText";
+  public static final String NO_NORMS_KEY = "omitNorms";
+  public static Field noNormsField = new Field(NO_NORMS_KEY, NO_NORMS_TEXT,
+      Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS);
+
+  public static final String NO_TF_TEXT = "analyzed with no tf and positions";
+  public static final String NO_TF_KEY = "omitTermFreqAndPositions";
+  public static Field noTFField = new Field(NO_TF_KEY, NO_TF_TEXT,
+      Field.Store.YES, Field.Index.ANALYZED);
+  static {
+    noTFField.setOmitTermFreqAndPositions(true);
+  }
+
+  public static final String UNINDEXED_FIELD_TEXT = "unindexed field text";
+  public static final String UNINDEXED_FIELD_KEY = "unIndField";
+  public static Field unIndField = new Field(UNINDEXED_FIELD_KEY, UNINDEXED_FIELD_TEXT,
+      Field.Store.YES, Field.Index.NO);
+
+
+  public static final String UNSTORED_1_FIELD_TEXT = "unstored field text";
+  public static final String UNSTORED_FIELD_1_KEY = "unStoredField1";
+  public static Field unStoredField1 = new Field(UNSTORED_FIELD_1_KEY, UNSTORED_1_FIELD_TEXT,
+      Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO);
+
+  public static final String UNSTORED_2_FIELD_TEXT = "unstored field text";
+  public static final String UNSTORED_FIELD_2_KEY = "unStoredField2";
+  public static Field unStoredField2 = new Field(UNSTORED_FIELD_2_KEY, UNSTORED_2_FIELD_TEXT,
+      Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES);
+
+  public static final String LAZY_FIELD_BINARY_KEY = "lazyFieldBinary";
+  public static byte [] LAZY_FIELD_BINARY_BYTES;
+  public static Field lazyFieldBinary;
+  
+  public static final String LAZY_FIELD_KEY = "lazyField";
+  public static final String LAZY_FIELD_TEXT = "These are some field bytes";
+  public static Field lazyField = new Field(LAZY_FIELD_KEY, LAZY_FIELD_TEXT, Field.Store.YES, Field.Index.ANALYZED);
+  
+  public static final String LARGE_LAZY_FIELD_KEY = "largeLazyField";
+  public static String LARGE_LAZY_FIELD_TEXT;
+  public static Field largeLazyField;
+  
+  //From Issue 509
+  public static final String FIELD_UTF1_TEXT = "field one \u4e00text";
+  public static final String TEXT_FIELD_UTF1_KEY = "textField1Utf8";
+  public static Field textUtfField1 = new Field(TEXT_FIELD_UTF1_KEY, FIELD_UTF1_TEXT,
+      Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO);
+
+  public static final String FIELD_UTF2_TEXT = "field field field \u4e00two text";
+  //Fields will be lexicographically sorted.  So, the order is: field, text, two
+  public static final int [] FIELD_UTF2_FREQS = {3, 1, 1};
+  public static final String TEXT_FIELD_UTF2_KEY = "textField2Utf8";
+  public static Field textUtfField2 = new Field(TEXT_FIELD_UTF2_KEY, FIELD_UTF2_TEXT, Field.Store.YES, 
+          Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS);
+ 
+  
+  
+  
+  public static Map<String,Object> nameValues = null;
+
+  // ordered list of all the fields...
+  // could use LinkedHashMap for this purpose if Java1.4 is OK
+  public static Field[] fields = new Field[] {
+    textField1,
+    textField2,
+    textField3,
+    keyField,
+    noNormsField,
+    noTFField,
+    unIndField,
+    unStoredField1,
+    unStoredField2,
+    textUtfField1,
+    textUtfField2,
+    lazyField,
+    lazyFieldBinary,//placeholder for binary field, since this is null.  It must be second to last.
+    largeLazyField//placeholder for large field, since this is null.  It must always be last
+  };
+
+  public static Map<String,Fieldable> all     =new HashMap<String,Fieldable>();
+  public static Map<String,Fieldable> indexed =new HashMap<String,Fieldable>();
+  public static Map<String,Fieldable> stored  =new HashMap<String,Fieldable>();
+  public static Map<String,Fieldable> unstored=new HashMap<String,Fieldable>();
+  public static Map<String,Fieldable> unindexed=new HashMap<String,Fieldable>();
+  public static Map<String,Fieldable> termvector=new HashMap<String,Fieldable>();
+  public static Map<String,Fieldable> notermvector=new HashMap<String,Fieldable>();
+  public static Map<String,Fieldable> lazy= new HashMap<String,Fieldable>();
+  public static Map<String,Fieldable> noNorms=new HashMap<String,Fieldable>();
+  public static Map<String,Fieldable> noTf=new HashMap<String,Fieldable>();
+
+  static {
+    //Initialize the large Lazy Field
+    StringBuilder buffer = new StringBuilder();
+    for (int i = 0; i < 10000; i++)
+    {
+      buffer.append("Lazily loading lengths of language in lieu of laughing ");
+    }
+    
+    try {
+      LAZY_FIELD_BINARY_BYTES = "These are some binary field bytes".getBytes("UTF8");
+    } catch (UnsupportedEncodingException e) {
+    }
+    lazyFieldBinary = new Field(LAZY_FIELD_BINARY_KEY, LAZY_FIELD_BINARY_BYTES);
+    fields[fields.length - 2] = lazyFieldBinary;
+    LARGE_LAZY_FIELD_TEXT = buffer.toString();
+    largeLazyField = new Field(LARGE_LAZY_FIELD_KEY, LARGE_LAZY_FIELD_TEXT, Field.Store.YES, Field.Index.ANALYZED);
+    fields[fields.length - 1] = largeLazyField;
+    for (int i=0; i<fields.length; i++) {
+      Fieldable f = fields[i];
+      add(all,f);
+      if (f.isIndexed()) add(indexed,f);
+      else add(unindexed,f);
+      if (f.isTermVectorStored()) add(termvector,f);
+      if (f.isIndexed() && !f.isTermVectorStored()) add(notermvector,f);
+      if (f.isStored()) add(stored,f);
+      else add(unstored,f);
+      if (f.getOmitNorms()) add(noNorms,f);
+      if (f.getOmitTermFreqAndPositions()) add(noTf,f);
+      if (f.isLazy()) add(lazy, f);
+    }
+  }
+
+
+  private static void add(Map<String,Fieldable> map, Fieldable field) {
+    map.put(field.name(), field);
+  }
+
+
+  static
+  {
+    nameValues = new HashMap<String,Object>();
+    nameValues.put(TEXT_FIELD_1_KEY, FIELD_1_TEXT);
+    nameValues.put(TEXT_FIELD_2_KEY, FIELD_2_TEXT);
+    nameValues.put(TEXT_FIELD_3_KEY, FIELD_3_TEXT);
+    nameValues.put(KEYWORD_FIELD_KEY, KEYWORD_TEXT);
+    nameValues.put(NO_NORMS_KEY, NO_NORMS_TEXT);
+    nameValues.put(NO_TF_KEY, NO_TF_TEXT);
+    nameValues.put(UNINDEXED_FIELD_KEY, UNINDEXED_FIELD_TEXT);
+    nameValues.put(UNSTORED_FIELD_1_KEY, UNSTORED_1_FIELD_TEXT);
+    nameValues.put(UNSTORED_FIELD_2_KEY, UNSTORED_2_FIELD_TEXT);
+    nameValues.put(LAZY_FIELD_KEY, LAZY_FIELD_TEXT);
+    nameValues.put(LAZY_FIELD_BINARY_KEY, LAZY_FIELD_BINARY_BYTES);
+    nameValues.put(LARGE_LAZY_FIELD_KEY, LARGE_LAZY_FIELD_TEXT);
+    nameValues.put(TEXT_FIELD_UTF1_KEY, FIELD_UTF1_TEXT);
+    nameValues.put(TEXT_FIELD_UTF2_KEY, FIELD_UTF2_TEXT);
+  }   
+  
+  /**
+   * Adds the fields above to a document 
+   * @param doc The document to write
+   */ 
+  public static void setupDoc(Document doc) {
+    for (int i=0; i<fields.length; i++) {
+      doc.add(fields[i]);
+    }
+  }                         
+
+  /**
+   * Writes the document to the directory using a segment
+   * named "test"; returns the SegmentInfo describing the new
+   * segment 
+   * @param dir
+   * @param doc
+   * @throws IOException
+   */ 
+  public static SegmentInfo writeDoc(Directory dir, Document doc) throws IOException
+  {
+    return writeDoc(dir, new MockAnalyzer(MockTokenizer.WHITESPACE, false), null, doc);
+  }
+
+  /**
+   * Writes the document to the directory using the analyzer
+   * and the similarity score; returns the SegmentInfo
+   * describing the new segment
+   * @param dir
+   * @param analyzer
+   * @param similarity
+   * @param doc
+   * @throws IOException
+   */ 
+  public static SegmentInfo writeDoc(Directory dir, Analyzer analyzer, SimilarityProvider similarity, Document doc) throws IOException {
+    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(
+        TEST_VERSION_CURRENT, analyzer).setSimilarityProvider(similarity));
+    //writer.setUseCompoundFile(false);
+    writer.addDocument(doc);
+    writer.commit();
+    SegmentInfo info = writer.newestSegment();
+    writer.close();
+    return info;
+  }
+
+  public static int numFields(Document doc) {
+    return doc.getFields().size();
+  }
+}
diff --git a/lucene/src/test-framework/org/apache/lucene/index/MockIndexInput.java b/lucene/src/test-framework/org/apache/lucene/index/MockIndexInput.java
new file mode 100644
index 0000000..1e2346c
--- /dev/null
+++ b/lucene/src/test-framework/org/apache/lucene/index/MockIndexInput.java
@@ -0,0 +1,64 @@
+package org.apache.lucene.index;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.store.BufferedIndexInput;
+
+public class MockIndexInput extends BufferedIndexInput {
+    private byte[] buffer;
+    private int pointer = 0;
+    private long length;
+
+    public MockIndexInput(byte[] bytes) {
+        buffer = bytes;
+        length = bytes.length;
+    }
+
+    @Override
+    protected void readInternal(byte[] dest, int destOffset, int len) {
+        int remainder = len;
+        int start = pointer;
+        while (remainder != 0) {
+//          int bufferNumber = start / buffer.length;
+          int bufferOffset = start % buffer.length;
+          int bytesInBuffer = buffer.length - bufferOffset;
+          int bytesToCopy = bytesInBuffer >= remainder ? remainder : bytesInBuffer;
+          System.arraycopy(buffer, bufferOffset, dest, destOffset, bytesToCopy);
+          destOffset += bytesToCopy;
+          start += bytesToCopy;
+          remainder -= bytesToCopy;
+        }
+        pointer += len;
+    }
+
+    @Override
+    public void close() {
+        // ignore
+    }
+
+    @Override
+    protected void seekInternal(long pos) {
+        pointer = (int) pos;
+    }
+
+    @Override
+    public long length() {
+      return length;
+    }
+
+}
diff --git a/lucene/src/test-framework/org/apache/lucene/index/MockRandomMergePolicy.java b/lucene/src/test-framework/org/apache/lucene/index/MockRandomMergePolicy.java
new file mode 100644
index 0000000..e8bc977
--- /dev/null
+++ b/lucene/src/test-framework/org/apache/lucene/index/MockRandomMergePolicy.java
@@ -0,0 +1,95 @@
+package org.apache.lucene.index;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.Random;
+import java.util.Set;
+
+import org.apache.lucene.util._TestUtil;
+
+public class MockRandomMergePolicy extends MergePolicy {
+  private final Random random;
+
+  public MockRandomMergePolicy(Random random) {
+    // fork a private random, since we are called
+    // unpredictably from threads:
+    this.random = new Random(random.nextLong());
+  }
+
+  @Override
+  public MergeSpecification findMerges(SegmentInfos segmentInfos) {
+    MergeSpecification mergeSpec = null;
+    //System.out.println("MRMP: findMerges sis=" + segmentInfos);
+
+    if (segmentInfos.size() > 1 && random.nextInt(5) == 3) {
+      
+      SegmentInfos segmentInfos2 = new SegmentInfos();
+      segmentInfos2.addAll(segmentInfos);
+      Collections.shuffle(segmentInfos2, random);
+
+      // TODO: sometimes make more than 1 merge?
+      mergeSpec = new MergeSpecification();
+      final int segsToMerge = _TestUtil.nextInt(random, 1, segmentInfos.size());
+      mergeSpec.add(new OneMerge(segmentInfos2.range(0, segsToMerge)));
+    }
+
+    return mergeSpec;
+  }
+
+  @Override
+  public MergeSpecification findMergesForOptimize(
+      SegmentInfos segmentInfos, int maxSegmentCount, Set<SegmentInfo> segmentsToOptimize)
+    throws CorruptIndexException, IOException {
+
+    //System.out.println("MRMP: findMergesForOptimize sis=" + segmentInfos);
+    MergeSpecification mergeSpec = null;
+    if (segmentInfos.size() > 1 || (segmentInfos.size() == 1 && segmentInfos.info(0).hasDeletions())) {
+      mergeSpec = new MergeSpecification();
+      SegmentInfos segmentInfos2 = new SegmentInfos();
+      segmentInfos2.addAll(segmentInfos);
+      Collections.shuffle(segmentInfos2, random);
+      int upto = 0;
+      while(upto < segmentInfos.size()) {
+        int max = Math.min(10, segmentInfos.size()-upto);
+        int inc = max <= 2 ? max : _TestUtil.nextInt(random, 2, max);
+        mergeSpec.add(new OneMerge(segmentInfos2.range(upto, upto+inc)));
+        upto += inc;
+      }
+    }
+    return mergeSpec;
+  }
+
+  @Override
+  public MergeSpecification findMergesToExpungeDeletes(
+      SegmentInfos segmentInfos)
+    throws CorruptIndexException, IOException {
+    return findMerges(segmentInfos);
+  }
+
+  @Override
+  public void close() {
+  }
+
+  @Override
+  public boolean useCompoundFile(SegmentInfos infos, SegmentInfo mergedInfo) throws IOException {
+    // 80% of the time we create CFS:
+    return random.nextInt(5) != 1;
+  }
+}
diff --git a/lucene/src/test-framework/org/apache/lucene/index/RandomIndexWriter.java b/lucene/src/test-framework/org/apache/lucene/index/RandomIndexWriter.java
new file mode 100644
index 0000000..2faf22b
--- /dev/null
+++ b/lucene/src/test-framework/org/apache/lucene/index/RandomIndexWriter.java
@@ -0,0 +1,162 @@
+package org.apache.lucene.index;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.Random;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.Version;
+import org.apache.lucene.util._TestUtil;
+
+/** Silly class that randomizes the indexing experience.  EG
+ *  it may swap in a different merge policy/scheduler; may
+ *  commit periodically; may or may not optimize in the end,
+ *  may flush by doc count instead of RAM, etc. 
+ */
+
+public class RandomIndexWriter implements Closeable {
+
+  public IndexWriter w;
+  private final Random r;
+  int docCount;
+  int flushAt;
+  private boolean getReaderCalled;
+
+  // Randomly calls Thread.yield so we mixup thread scheduling
+  private static final class MockIndexWriter extends IndexWriter {
+
+    private final Random r;
+
+    public MockIndexWriter(Random r,Directory dir, IndexWriterConfig conf) throws IOException {
+      super(dir, conf);
+      // must make a private random since our methods are
+      // called from different threads; else test failures may
+      // not be reproducible from the original seed
+      this.r = new Random(r.nextInt());
+    }
+
+    @Override
+    boolean testPoint(String name) {
+      if (r.nextInt(4) == 2)
+        Thread.yield();
+      return true;
+    }
+  }
+
+  /** create a RandomIndexWriter with a random config: Uses TEST_VERSION_CURRENT and MockAnalyzer */
+  public RandomIndexWriter(Random r, Directory dir) throws IOException {
+    this(r, dir, LuceneTestCase.newIndexWriterConfig(r, LuceneTestCase.TEST_VERSION_CURRENT, new MockAnalyzer()));
+  }
+  
+  /** create a RandomIndexWriter with a random config: Uses TEST_VERSION_CURRENT */
+  public RandomIndexWriter(Random r, Directory dir, Analyzer a) throws IOException {
+    this(r, dir, LuceneTestCase.newIndexWriterConfig(r, LuceneTestCase.TEST_VERSION_CURRENT, a));
+  }
+  
+  /** create a RandomIndexWriter with a random config */
+  public RandomIndexWriter(Random r, Directory dir, Version v, Analyzer a) throws IOException {
+    this(r, dir, LuceneTestCase.newIndexWriterConfig(r, v, a));
+  }
+  
+  /** create a RandomIndexWriter with the provided config */
+  public RandomIndexWriter(Random r, Directory dir, IndexWriterConfig c) throws IOException {
+    this.r = r;
+    w = new MockIndexWriter(r, dir, c);
+    flushAt = _TestUtil.nextInt(r, 10, 1000);
+    if (LuceneTestCase.VERBOSE) {
+      System.out.println("RIW config=" + w.getConfig());
+      System.out.println("codec default=" + w.getConfig().getCodecProvider().getDefaultFieldCodec());
+      w.setInfoStream(System.out);
+    }
+  } 
+
+  public void addDocument(Document doc) throws IOException {
+    w.addDocument(doc);
+    if (docCount++ == flushAt) {
+      if (LuceneTestCase.VERBOSE) {
+        System.out.println("RIW.addDocument: now doing a commit");
+      }
+      w.commit();
+      flushAt += _TestUtil.nextInt(r, 10, 1000);
+    }
+  }
+  
+  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {
+    w.addIndexes(dirs);
+  }
+  
+  public void deleteDocuments(Term term) throws CorruptIndexException, IOException {
+    w.deleteDocuments(term);
+  }
+  
+  public void commit() throws CorruptIndexException, IOException {
+    w.commit();
+  }
+  
+  public int numDocs() throws IOException {
+    return w.numDocs();
+  }
+
+  public int maxDoc() {
+    return w.maxDoc();
+  }
+
+  public void deleteAll() throws IOException {
+    w.deleteAll();
+  }
+
+  public IndexReader getReader() throws IOException {
+    getReaderCalled = true;
+    if (r.nextInt(4) == 2)
+      w.optimize();
+    // If we are writing with PreFlexRW, force a full
+    // IndexReader.open so terms are sorted in codepoint
+    // order during searching:
+    if (!w.codecs.getDefaultFieldCodec().equals("PreFlex") && r.nextBoolean()) {
+      if (LuceneTestCase.VERBOSE) {
+        System.out.println("RIW.getReader: use NRT reader");
+      }
+      return w.getReader();
+    } else {
+      if (LuceneTestCase.VERBOSE) {
+        System.out.println("RIW.getReader: open new reader");
+      }
+      w.commit();
+      return IndexReader.open(w.getDirectory(), new KeepOnlyLastCommitDeletionPolicy(), r.nextBoolean(), _TestUtil.nextInt(r, 1, 10));
+    }
+  }
+
+  public void close() throws IOException {
+    // if someone isn't using getReader() API, we want to be sure to
+    // maybeOptimize since presumably they might open a reader on the dir.
+    if (getReaderCalled == false && r.nextInt(4) == 2) {
+      w.optimize();
+    }
+    w.close();
+  }
+
+  public void optimize() throws IOException {
+    w.optimize();
+  }
+}
diff --git a/lucene/src/test-framework/org/apache/lucene/index/codecs/mockintblock/MockFixedIntBlockCodec.java b/lucene/src/test-framework/org/apache/lucene/index/codecs/mockintblock/MockFixedIntBlockCodec.java
new file mode 100644
index 0000000..fc50b4a
--- /dev/null
+++ b/lucene/src/test-framework/org/apache/lucene/index/codecs/mockintblock/MockFixedIntBlockCodec.java
@@ -0,0 +1,202 @@
+package org.apache.lucene.index.codecs.mockintblock;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.codecs.Codec;
+import org.apache.lucene.index.codecs.FieldsConsumer;
+import org.apache.lucene.index.codecs.FieldsProducer;
+import org.apache.lucene.index.codecs.sep.IntStreamFactory;
+import org.apache.lucene.index.codecs.sep.IntIndexInput;
+import org.apache.lucene.index.codecs.sep.IntIndexOutput;
+import org.apache.lucene.index.codecs.sep.SepPostingsReaderImpl;
+import org.apache.lucene.index.codecs.sep.SepPostingsWriterImpl;
+import org.apache.lucene.index.codecs.intblock.FixedIntBlockIndexInput;
+import org.apache.lucene.index.codecs.intblock.FixedIntBlockIndexOutput;
+import org.apache.lucene.index.codecs.FixedGapTermsIndexReader;
+import org.apache.lucene.index.codecs.FixedGapTermsIndexWriter;
+import org.apache.lucene.index.codecs.PostingsWriterBase;
+import org.apache.lucene.index.codecs.PostingsReaderBase;
+import org.apache.lucene.index.codecs.BlockTermsReader;
+import org.apache.lucene.index.codecs.BlockTermsWriter;
+import org.apache.lucene.index.codecs.TermsIndexReaderBase;
+import org.apache.lucene.index.codecs.TermsIndexWriterBase;
+import org.apache.lucene.index.codecs.standard.StandardCodec;
+import org.apache.lucene.store.*;
+import org.apache.lucene.util.BytesRef;
+
+/**
+ * A silly test codec to verify core support for fixed
+ * sized int block encoders is working.  The int encoder
+ * used here just writes each block as a series of vInt.
+ */
+
+public class MockFixedIntBlockCodec extends Codec {
+
+  private final int blockSize;
+
+  public MockFixedIntBlockCodec(int blockSize) {
+    this.blockSize = blockSize;
+    name = "MockFixedIntBlock";
+  }
+
+  @Override
+  public String toString() {
+    return name + "(blockSize=" + blockSize + ")";
+  }
+
+  // only for testing
+  public IntStreamFactory getIntFactory() {
+    return new MockIntFactory(blockSize);
+  }
+
+  public static class MockIntFactory extends IntStreamFactory {
+    private final int blockSize;
+
+    public MockIntFactory(int blockSize) {
+      this.blockSize = blockSize;
+    }
+
+    @Override
+    public IntIndexInput openInput(Directory dir, String fileName, int readBufferSize) throws IOException {
+      return new FixedIntBlockIndexInput(dir.openInput(fileName, readBufferSize)) {
+
+        @Override
+        protected BlockReader getBlockReader(final IndexInput in, final int[] buffer) throws IOException {
+          return new BlockReader() {
+            public void seek(long pos) {}
+            public void readBlock() throws IOException {
+              for(int i=0;i<buffer.length;i++) {
+                buffer[i] = in.readVInt();
+              }
+            }
+          };
+        }
+      };
+    }
+
+    @Override
+    public IntIndexOutput createOutput(Directory dir, String fileName) throws IOException {
+      return new FixedIntBlockIndexOutput(dir.createOutput(fileName), blockSize) {
+        @Override
+        protected void flushBlock() throws IOException {
+          for(int i=0;i<buffer.length;i++) {
+            assert buffer[i] >= 0;
+            out.writeVInt(buffer[i]);
+          }
+        }
+      };
+    }
+  }
+
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    PostingsWriterBase postingsWriter = new SepPostingsWriterImpl(state, new MockIntFactory(blockSize));
+
+    boolean success = false;
+    TermsIndexWriterBase indexWriter;
+    try {
+      indexWriter = new FixedGapTermsIndexWriter(state);
+      success = true;
+    } finally {
+      if (!success) {
+        postingsWriter.close();
+      }
+    }
+
+    success = false;
+    try {
+      FieldsConsumer ret = new BlockTermsWriter(indexWriter, state, postingsWriter, BytesRef.getUTF8SortedAsUnicodeComparator());
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        try {
+          postingsWriter.close();
+        } finally {
+          indexWriter.close();
+        }
+      }
+    }
+  }
+
+  @Override
+  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+    PostingsReaderBase postingsReader = new SepPostingsReaderImpl(state.dir,
+                                                                      state.segmentInfo,
+                                                                      state.readBufferSize,
+                                                                      new MockIntFactory(blockSize), state.codecId);
+
+    TermsIndexReaderBase indexReader;
+    boolean success = false;
+    try {
+      indexReader = new FixedGapTermsIndexReader(state.dir,
+                                                       state.fieldInfos,
+                                                       state.segmentInfo.name,
+                                                       state.termsIndexDivisor,
+                                                       BytesRef.getUTF8SortedAsUnicodeComparator(), state.codecId);
+      success = true;
+    } finally {
+      if (!success) {
+        postingsReader.close();
+      }
+    }
+
+    success = false;
+    try {
+      FieldsProducer ret = new BlockTermsReader(indexReader,
+                                                state.dir,
+                                                state.fieldInfos,
+                                                state.segmentInfo.name,
+                                                postingsReader,
+                                                state.readBufferSize,
+                                                BytesRef.getUTF8SortedAsUnicodeComparator(),
+                                                StandardCodec.TERMS_CACHE_SIZE,
+                                                state.codecId);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        try {
+          postingsReader.close();
+        } finally {
+          indexReader.close();
+        }
+      }
+    }
+  }
+
+  @Override
+  public void files(Directory dir, SegmentInfo segmentInfo, String codecId, Set<String> files) {
+    SepPostingsReaderImpl.files(segmentInfo, codecId, files);
+    BlockTermsReader.files(dir, segmentInfo, codecId, files);
+    FixedGapTermsIndexReader.files(dir, segmentInfo, codecId, files);
+  }
+
+  @Override
+  public void getExtensions(Set<String> extensions) {
+    SepPostingsWriterImpl.getExtensions(extensions);
+    BlockTermsReader.getExtensions(extensions);
+    FixedGapTermsIndexReader.getIndexExtensions(extensions);
+  }
+}
diff --git a/lucene/src/test-framework/org/apache/lucene/index/codecs/mockintblock/MockVariableIntBlockCodec.java b/lucene/src/test-framework/org/apache/lucene/index/codecs/mockintblock/MockVariableIntBlockCodec.java
new file mode 100644
index 0000000..82b8615
--- /dev/null
+++ b/lucene/src/test-framework/org/apache/lucene/index/codecs/mockintblock/MockVariableIntBlockCodec.java
@@ -0,0 +1,227 @@
+package org.apache.lucene.index.codecs.mockintblock;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.codecs.Codec;
+import org.apache.lucene.index.codecs.FieldsConsumer;
+import org.apache.lucene.index.codecs.FieldsProducer;
+import org.apache.lucene.index.codecs.sep.IntStreamFactory;
+import org.apache.lucene.index.codecs.sep.IntIndexInput;
+import org.apache.lucene.index.codecs.sep.IntIndexOutput;
+import org.apache.lucene.index.codecs.sep.SepPostingsReaderImpl;
+import org.apache.lucene.index.codecs.sep.SepPostingsWriterImpl;
+import org.apache.lucene.index.codecs.intblock.VariableIntBlockIndexInput;
+import org.apache.lucene.index.codecs.intblock.VariableIntBlockIndexOutput;
+import org.apache.lucene.index.codecs.FixedGapTermsIndexReader;
+import org.apache.lucene.index.codecs.FixedGapTermsIndexWriter;
+import org.apache.lucene.index.codecs.PostingsWriterBase;
+import org.apache.lucene.index.codecs.PostingsReaderBase;
+import org.apache.lucene.index.codecs.BlockTermsReader;
+import org.apache.lucene.index.codecs.BlockTermsWriter;
+import org.apache.lucene.index.codecs.TermsIndexReaderBase;
+import org.apache.lucene.index.codecs.TermsIndexWriterBase;
+import org.apache.lucene.index.codecs.standard.StandardCodec;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+
+/**
+ * A silly test codec to verify core support for variable
+ * sized int block encoders is working.  The int encoder
+ * used here writes baseBlockSize ints at once, if the first
+ * int is <= 3, else 2*baseBlockSize.
+ */
+
+public class MockVariableIntBlockCodec extends Codec {
+  private final int baseBlockSize;
+
+  public MockVariableIntBlockCodec(int baseBlockSize) {
+    name = "MockVariableIntBlock";
+    this.baseBlockSize = baseBlockSize;
+  }
+
+  @Override
+  public String toString() {
+    return name + "(baseBlockSize="+ baseBlockSize + ")";
+  }
+
+  public static class MockIntFactory extends IntStreamFactory {
+
+    private final int baseBlockSize;
+
+    public MockIntFactory(int baseBlockSize) {
+      this.baseBlockSize = baseBlockSize;
+    }
+
+    @Override
+    public IntIndexInput openInput(Directory dir, String fileName, int readBufferSize) throws IOException {
+      final IndexInput in = dir.openInput(fileName, readBufferSize);
+      final int baseBlockSize = in.readInt();
+      return new VariableIntBlockIndexInput(in) {
+
+        @Override
+        protected BlockReader getBlockReader(final IndexInput in, final int[] buffer) throws IOException {
+          return new BlockReader() {
+            public void seek(long pos) {}
+            public int readBlock() throws IOException {
+              buffer[0] = in.readVInt();
+              final int count = buffer[0] <= 3 ? baseBlockSize-1 : 2*baseBlockSize-1;
+              assert buffer.length >= count: "buffer.length=" + buffer.length + " count=" + count;
+              for(int i=0;i<count;i++) {
+                buffer[i+1] = in.readVInt();
+              }
+              return 1+count;
+            }
+          };
+        }
+      };
+    }
+
+    @Override
+    public IntIndexOutput createOutput(Directory dir, String fileName) throws IOException {
+      final IndexOutput out = dir.createOutput(fileName);
+      out.writeInt(baseBlockSize);
+      return new VariableIntBlockIndexOutput(out, 2*baseBlockSize) {
+
+        int pendingCount;
+        final int[] buffer = new int[2+2*baseBlockSize];
+
+        @Override
+        protected int add(int value) throws IOException {
+          assert value >= 0;
+          buffer[pendingCount++] = value;
+          // silly variable block length int encoder: if
+          // first value <= 3, we write N vints at once;
+          // else, 2*N
+          final int flushAt = buffer[0] <= 3 ? baseBlockSize : 2*baseBlockSize;
+
+          // intentionally be non-causal here:
+          if (pendingCount == flushAt+1) {
+            for(int i=0;i<flushAt;i++) {
+              out.writeVInt(buffer[i]);
+            }
+            buffer[0] = buffer[flushAt];
+            pendingCount = 1;
+            return flushAt;
+          } else {
+            return 0;
+          }
+        }
+      };
+    }
+  }
+
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    PostingsWriterBase postingsWriter = new SepPostingsWriterImpl(state, new MockIntFactory(baseBlockSize));
+
+    boolean success = false;
+    TermsIndexWriterBase indexWriter;
+    try {
+      indexWriter = new FixedGapTermsIndexWriter(state);
+      success = true;
+    } finally {
+      if (!success) {
+        postingsWriter.close();
+      }
+    }
+
+    success = false;
+    try {
+      FieldsConsumer ret = new BlockTermsWriter(indexWriter, state, postingsWriter, BytesRef.getUTF8SortedAsUnicodeComparator());
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        try {
+          postingsWriter.close();
+        } finally {
+          indexWriter.close();
+        }
+      }
+    }
+  }
+
+  @Override
+  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+    PostingsReaderBase postingsReader = new SepPostingsReaderImpl(state.dir,
+                                                                      state.segmentInfo,
+                                                                      state.readBufferSize,
+                                                                      new MockIntFactory(baseBlockSize), state.codecId);
+
+    TermsIndexReaderBase indexReader;
+    boolean success = false;
+    try {
+      indexReader = new FixedGapTermsIndexReader(state.dir,
+                                                       state.fieldInfos,
+                                                       state.segmentInfo.name,
+                                                       state.termsIndexDivisor,
+                                                       BytesRef.getUTF8SortedAsUnicodeComparator(),
+                                                       state.codecId);
+      success = true;
+    } finally {
+      if (!success) {
+        postingsReader.close();
+      }
+    }
+
+    success = false;
+    try {
+      FieldsProducer ret = new BlockTermsReader(indexReader,
+                                                state.dir,
+                                                state.fieldInfos,
+                                                state.segmentInfo.name,
+                                                postingsReader,
+                                                state.readBufferSize,
+                                                BytesRef.getUTF8SortedAsUnicodeComparator(),
+                                                StandardCodec.TERMS_CACHE_SIZE,
+                                                state.codecId);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        try {
+          postingsReader.close();
+        } finally {
+          indexReader.close();
+        }
+      }
+    }
+  }
+
+  @Override
+  public void files(Directory dir, SegmentInfo segmentInfo, String codecId, Set<String> files) {
+    SepPostingsReaderImpl.files(segmentInfo, codecId, files);
+    BlockTermsReader.files(dir, segmentInfo, codecId, files);
+    FixedGapTermsIndexReader.files(dir, segmentInfo, codecId, files);
+  }
+
+  @Override
+  public void getExtensions(Set<String> extensions) {
+    SepPostingsWriterImpl.getExtensions(extensions);
+    BlockTermsReader.getExtensions(extensions);
+    FixedGapTermsIndexReader.getIndexExtensions(extensions);
+  }
+}
diff --git a/lucene/src/test-framework/org/apache/lucene/index/codecs/mockrandom/MockRandomCodec.java b/lucene/src/test-framework/org/apache/lucene/index/codecs/mockrandom/MockRandomCodec.java
new file mode 100644
index 0000000..745c619
--- /dev/null
+++ b/lucene/src/test-framework/org/apache/lucene/index/codecs/mockrandom/MockRandomCodec.java
@@ -0,0 +1,335 @@
+package org.apache.lucene.index.codecs.mockrandom;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Iterator;
+import java.util.Random;
+import java.util.Set;
+
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.codecs.BlockTermsReader;
+import org.apache.lucene.index.codecs.BlockTermsWriter;
+import org.apache.lucene.index.codecs.Codec;
+import org.apache.lucene.index.codecs.FieldsConsumer;
+import org.apache.lucene.index.codecs.FieldsProducer;
+import org.apache.lucene.index.codecs.FixedGapTermsIndexReader;
+import org.apache.lucene.index.codecs.FixedGapTermsIndexWriter;
+import org.apache.lucene.index.codecs.PostingsReaderBase;
+import org.apache.lucene.index.codecs.PostingsWriterBase;
+import org.apache.lucene.index.codecs.TermStats;
+import org.apache.lucene.index.codecs.TermsIndexReaderBase;
+import org.apache.lucene.index.codecs.TermsIndexWriterBase;
+import org.apache.lucene.index.codecs.VariableGapTermsIndexReader;
+import org.apache.lucene.index.codecs.VariableGapTermsIndexWriter;
+import org.apache.lucene.index.codecs.mockintblock.MockFixedIntBlockCodec;
+import org.apache.lucene.index.codecs.mockintblock.MockVariableIntBlockCodec;
+import org.apache.lucene.index.codecs.mocksep.MockSingleIntFactory;
+import org.apache.lucene.index.codecs.pulsing.PulsingPostingsReaderImpl;
+import org.apache.lucene.index.codecs.pulsing.PulsingPostingsWriterImpl;
+import org.apache.lucene.index.codecs.sep.SepPostingsReaderImpl;
+import org.apache.lucene.index.codecs.sep.SepPostingsWriterImpl;
+import org.apache.lucene.index.codecs.standard.StandardPostingsReader;
+import org.apache.lucene.index.codecs.standard.StandardPostingsWriter;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
+
+/**
+ * Randomly combines terms index impl w/ postings impls.
+ */
+
+public class MockRandomCodec extends Codec {
+
+  private final Random seedRandom;
+  private final String SEED_EXT = "sd";
+
+  public MockRandomCodec(Random random) {
+    name = "MockRandom";
+    this.seedRandom = new Random(random.nextLong());
+  }
+
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+
+    final long seed = seedRandom.nextLong();
+
+    final String seedFileName = IndexFileNames.segmentFileName(state.segmentName, state.codecId, SEED_EXT);
+    final IndexOutput out = state.directory.createOutput(seedFileName);
+    out.writeLong(seed);
+    out.close();
+
+    final Random random = new Random(seed);
+    PostingsWriterBase postingsWriter;
+    final int n = random.nextInt(4);
+
+    if (n == 0) {
+      if (LuceneTestCase.VERBOSE) {
+        System.out.println("MockRandomCodec: writing MockSep postings");
+      }
+      postingsWriter = new SepPostingsWriterImpl(state, new MockSingleIntFactory());
+    } else if (n == 1) {
+      final int blockSize = _TestUtil.nextInt(random, 1, 2000);
+      if (LuceneTestCase.VERBOSE) {
+        System.out.println("MockRandomCodec: writing MockFixedIntBlock(" + blockSize + ") postings");
+      }
+      postingsWriter = new SepPostingsWriterImpl(state, new MockFixedIntBlockCodec.MockIntFactory(blockSize));
+    } else if (n == 2) {
+      final int baseBlockSize = _TestUtil.nextInt(random, 1, 127);
+      if (LuceneTestCase.VERBOSE) {
+        System.out.println("MockRandomCodec: writing MockVariableIntBlock(" + baseBlockSize + ") postings");
+      }
+      postingsWriter = new SepPostingsWriterImpl(state, new MockVariableIntBlockCodec.MockIntFactory(baseBlockSize));
+    } else {
+      if (LuceneTestCase.VERBOSE) {
+        System.out.println("MockRandomCodec: writing Standard postings");
+      }
+      postingsWriter = new StandardPostingsWriter(state);
+    }
+
+    if (random.nextBoolean()) {
+      final int totTFCutoff = _TestUtil.nextInt(random, 1, 20);
+      if (LuceneTestCase.VERBOSE) {
+        System.out.println("MockRandomCodec: pulsing postings with totTFCutoff=" + totTFCutoff);
+      }
+      postingsWriter = new PulsingPostingsWriterImpl(totTFCutoff, postingsWriter);
+    }
+
+    final TermsIndexWriterBase indexWriter;
+    boolean success = false;
+
+    try {
+      if (random.nextBoolean()) {
+        state.termIndexInterval = _TestUtil.nextInt(random, 1, 100);
+        if (LuceneTestCase.VERBOSE) {
+          System.out.println("MockRandomCodec: fixed-gap terms index (tii=" + state.termIndexInterval + ")");
+        }
+        indexWriter = new FixedGapTermsIndexWriter(state);
+      } else {
+        final VariableGapTermsIndexWriter.IndexTermSelector selector;
+        final int n2 = random.nextInt(3);
+        if (n2 == 0) {
+          final int tii = _TestUtil.nextInt(random, 1, 100);
+          selector = new VariableGapTermsIndexWriter.EveryNTermSelector(tii);
+          if (LuceneTestCase.VERBOSE) {
+            System.out.println("MockRandomCodec: variable-gap terms index (tii=" + tii + ")");
+          }
+        } else if (n2 == 1) {
+          final int docFreqThresh = _TestUtil.nextInt(random, 2, 100);
+          final int tii = _TestUtil.nextInt(random, 1, 100);
+          selector = new VariableGapTermsIndexWriter.EveryNOrDocFreqTermSelector(docFreqThresh, tii);
+        } else {
+          final long seed2 = random.nextLong();
+          final int gap = _TestUtil.nextInt(random, 2, 40);
+          if (LuceneTestCase.VERBOSE) {
+            System.out.println("MockRandomCodec: random-gap terms index (max gap=" + gap + ")");
+          }
+          selector = new VariableGapTermsIndexWriter.IndexTermSelector() {
+              final Random rand = new Random(seed2);
+
+              @Override
+              public boolean isIndexTerm(BytesRef term, TermStats stats) {
+                return random.nextInt(gap) == 17;
+              }
+
+              @Override
+              public void newField(FieldInfo fieldInfo) {
+              }
+            };
+        }
+        indexWriter = new VariableGapTermsIndexWriter(state, selector);
+      }
+      success = true;
+    } finally {
+      if (!success) {
+        postingsWriter.close();
+      }
+    }
+
+    success = false;
+    try {
+      FieldsConsumer ret = new BlockTermsWriter(indexWriter, state, postingsWriter, BytesRef.getUTF8SortedAsUnicodeComparator());
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        try {
+          postingsWriter.close();
+        } finally {
+          indexWriter.close();
+        }
+      }
+    }
+  }
+
+  @Override
+  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+
+    final String seedFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.codecId, SEED_EXT);
+    final IndexInput in = state.dir.openInput(seedFileName);
+    final long seed = in.readLong();
+    in.close();
+
+    final Random random = new Random(seed);
+    PostingsReaderBase postingsReader;
+    final int n = random.nextInt(4);
+
+    if (n == 0) {
+      if (LuceneTestCase.VERBOSE) {
+        System.out.println("MockRandomCodec: reading MockSep postings");
+      }
+      postingsReader = new SepPostingsReaderImpl(state.dir, state.segmentInfo,
+                                                 state.readBufferSize, new MockSingleIntFactory(), state.codecId);
+    } else if (n == 1) {
+      final int blockSize = _TestUtil.nextInt(random, 1, 2000);
+      if (LuceneTestCase.VERBOSE) {
+        System.out.println("MockRandomCodec: reading MockFixedIntBlock(" + blockSize + ") postings");
+      }
+      postingsReader = new SepPostingsReaderImpl(state.dir, state.segmentInfo,
+                                                 state.readBufferSize, new MockFixedIntBlockCodec.MockIntFactory(blockSize), state.codecId);
+    } else if (n == 2) {
+      final int baseBlockSize = _TestUtil.nextInt(random, 1, 127);
+      if (LuceneTestCase.VERBOSE) {
+        System.out.println("MockRandomCodec: reading MockVariableIntBlock(" + baseBlockSize + ") postings");
+      }
+      postingsReader = new SepPostingsReaderImpl(state.dir, state.segmentInfo,
+                                                 state.readBufferSize, new MockVariableIntBlockCodec.MockIntFactory(baseBlockSize), state.codecId);
+    } else {
+      if (LuceneTestCase.VERBOSE) {
+        System.out.println("MockRandomCodec: reading Standard postings");
+      }
+      postingsReader = new StandardPostingsReader(state.dir, state.segmentInfo, state.readBufferSize, state.codecId);
+    }
+
+    if (random.nextBoolean()) {
+      final int totTFCutoff = _TestUtil.nextInt(random, 1, 20);
+      if (LuceneTestCase.VERBOSE) {
+        System.out.println("MockRandomCodec: reading pulsing postings with totTFCutoff=" + totTFCutoff);
+      }
+      postingsReader = new PulsingPostingsReaderImpl(postingsReader);
+    }
+
+    final TermsIndexReaderBase indexReader;
+    boolean success = false;
+
+    try {
+      if (random.nextBoolean()) {
+        // if termsIndexDivisor is set to -1, we should not touch it. It means a
+        // test explicitly instructed not to load the terms index.
+        if (state.termsIndexDivisor != -1) {
+          state.termsIndexDivisor = _TestUtil.nextInt(random, 1, 10);
+        }
+        if (LuceneTestCase.VERBOSE) {
+          System.out.println("MockRandomCodec: fixed-gap terms index (divisor=" + state.termsIndexDivisor + ")");
+        }
+        indexReader = new FixedGapTermsIndexReader(state.dir,
+                                                   state.fieldInfos,
+                                                   state.segmentInfo.name,
+                                                   state.termsIndexDivisor,
+                                                   BytesRef.getUTF8SortedAsUnicodeComparator(),
+                                                   state.codecId);
+      } else {
+        final int n2 = random.nextInt(3);
+        if (n2 == 1) {
+          random.nextInt();
+        } else if (n2 == 2) {
+          random.nextLong();
+        }
+        if (LuceneTestCase.VERBOSE) {
+          System.out.println("MockRandomCodec: variable-gap terms index (divisor=" + state.termsIndexDivisor + ")");
+        }
+        if (state.termsIndexDivisor != -1) {
+          state.termsIndexDivisor = _TestUtil.nextInt(random, 1, 10);
+        }
+        indexReader = new VariableGapTermsIndexReader(state.dir,
+                                                      state.fieldInfos,
+                                                      state.segmentInfo.name,
+                                                      state.termsIndexDivisor,
+                                                      state.codecId);
+      }
+      success = true;
+    } finally {
+      if (!success) {
+        postingsReader.close();
+      }
+    }
+
+    final int termsCacheSize = _TestUtil.nextInt(random, 1, 1024);
+
+    success = false;
+    try {
+      FieldsProducer ret = new BlockTermsReader(indexReader,
+                                                state.dir,
+                                                state.fieldInfos,
+                                                state.segmentInfo.name,
+                                                postingsReader,
+                                                state.readBufferSize,
+                                                BytesRef.getUTF8SortedAsUnicodeComparator(),
+                                                termsCacheSize,
+                                                state.codecId);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        try {
+          postingsReader.close();
+        } finally {
+          indexReader.close();
+        }
+      }
+    }
+  }
+
+  @Override
+  public void files(Directory dir, SegmentInfo segmentInfo, String codecId, Set<String> files) throws IOException {
+    final String seedFileName = IndexFileNames.segmentFileName(segmentInfo.name, codecId, SEED_EXT);    
+    files.add(seedFileName);
+    SepPostingsReaderImpl.files(segmentInfo, codecId, files);
+    StandardPostingsReader.files(dir, segmentInfo, codecId, files);
+    BlockTermsReader.files(dir, segmentInfo, codecId, files);
+    FixedGapTermsIndexReader.files(dir, segmentInfo, codecId, files);
+    VariableGapTermsIndexReader.files(dir, segmentInfo, codecId, files);
+    
+    // hackish!
+    Iterator<String> it = files.iterator();
+    while(it.hasNext()) {
+      final String file = it.next();
+      if (!dir.fileExists(file)) {
+        it.remove();
+      }
+    }
+    //System.out.println("MockRandom.files return " + files);
+  }
+
+  @Override
+  public void getExtensions(Set<String> extensions) {
+    SepPostingsWriterImpl.getExtensions(extensions);
+    BlockTermsReader.getExtensions(extensions);
+    FixedGapTermsIndexReader.getIndexExtensions(extensions);
+    VariableGapTermsIndexReader.getIndexExtensions(extensions);
+    extensions.add(SEED_EXT);
+    //System.out.println("MockRandom.getExtensions return " + extensions);
+  }
+}
diff --git a/lucene/src/test-framework/org/apache/lucene/index/codecs/mocksep/MockSepCodec.java b/lucene/src/test-framework/org/apache/lucene/index/codecs/mocksep/MockSepCodec.java
new file mode 100644
index 0000000..e1e9358
--- /dev/null
+++ b/lucene/src/test-framework/org/apache/lucene/index/codecs/mocksep/MockSepCodec.java
@@ -0,0 +1,150 @@
+package org.apache.lucene.index.codecs.mocksep;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.codecs.Codec;
+import org.apache.lucene.index.codecs.FieldsConsumer;
+import org.apache.lucene.index.codecs.FieldsProducer;
+import org.apache.lucene.index.codecs.FixedGapTermsIndexReader;
+import org.apache.lucene.index.codecs.FixedGapTermsIndexWriter;
+import org.apache.lucene.index.codecs.PostingsReaderBase;
+import org.apache.lucene.index.codecs.PostingsWriterBase;
+import org.apache.lucene.index.codecs.BlockTermsReader;
+import org.apache.lucene.index.codecs.BlockTermsWriter;
+import org.apache.lucene.index.codecs.TermsIndexReaderBase;
+import org.apache.lucene.index.codecs.TermsIndexWriterBase;
+import org.apache.lucene.index.codecs.standard.StandardCodec;
+import org.apache.lucene.index.codecs.sep.SepPostingsWriterImpl;
+import org.apache.lucene.index.codecs.sep.SepPostingsReaderImpl;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
+
+/**
+ * A silly codec that simply writes each file separately as
+ * single vInts.  Don't use this (performance will be poor)!
+ * This is here just to test the core sep codec
+ * classes.
+ */
+public class MockSepCodec extends Codec {
+
+  public MockSepCodec() {
+    name = "MockSep";
+  }
+
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+
+    PostingsWriterBase postingsWriter = new SepPostingsWriterImpl(state, new MockSingleIntFactory());
+
+    boolean success = false;
+    TermsIndexWriterBase indexWriter;
+    try {
+      indexWriter = new FixedGapTermsIndexWriter(state);
+      success = true;
+    } finally {
+      if (!success) {
+        postingsWriter.close();
+      }
+    }
+
+    success = false;
+    try {
+      FieldsConsumer ret = new BlockTermsWriter(indexWriter, state, postingsWriter, BytesRef.getUTF8SortedAsUnicodeComparator());
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        try {
+          postingsWriter.close();
+        } finally {
+          indexWriter.close();
+        }
+      }
+    }
+  }
+
+  @Override
+  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+
+    PostingsReaderBase postingsReader = new SepPostingsReaderImpl(state.dir, state.segmentInfo,
+        state.readBufferSize, new MockSingleIntFactory(), state.codecId);
+
+    TermsIndexReaderBase indexReader;
+    boolean success = false;
+    try {
+      indexReader = new FixedGapTermsIndexReader(state.dir,
+                                                       state.fieldInfos,
+                                                       state.segmentInfo.name,
+                                                       state.termsIndexDivisor,
+                                                       BytesRef.getUTF8SortedAsUnicodeComparator(),
+                                                       state.codecId);
+      success = true;
+    } finally {
+      if (!success) {
+        postingsReader.close();
+      }
+    }
+
+    success = false;
+    try {
+      FieldsProducer ret = new BlockTermsReader(indexReader,
+                                                state.dir,
+                                                state.fieldInfos,
+                                                state.segmentInfo.name,
+                                                postingsReader,
+                                                state.readBufferSize,
+                                                BytesRef.getUTF8SortedAsUnicodeComparator(),
+                                                StandardCodec.TERMS_CACHE_SIZE,
+                                                state.codecId);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        try {
+          postingsReader.close();
+        } finally {
+          indexReader.close();
+        }
+      }
+    }
+  }
+
+  @Override
+  public void files(Directory dir, SegmentInfo segmentInfo, String codecId, Set<String> files) {
+    SepPostingsReaderImpl.files(segmentInfo, codecId, files);
+    BlockTermsReader.files(dir, segmentInfo, codecId, files);
+    FixedGapTermsIndexReader.files(dir, segmentInfo, codecId, files);
+  }
+
+  @Override
+  public void getExtensions(Set<String> extensions) {
+    getSepExtensions(extensions);
+  }
+
+  public static void getSepExtensions(Set<String> extensions) {
+    SepPostingsWriterImpl.getExtensions(extensions);
+    BlockTermsReader.getExtensions(extensions);
+    FixedGapTermsIndexReader.getIndexExtensions(extensions);
+  }
+}
diff --git a/lucene/src/test-framework/org/apache/lucene/index/codecs/mocksep/MockSingleIntFactory.java b/lucene/src/test-framework/org/apache/lucene/index/codecs/mocksep/MockSingleIntFactory.java
new file mode 100644
index 0000000..092db12
--- /dev/null
+++ b/lucene/src/test-framework/org/apache/lucene/index/codecs/mocksep/MockSingleIntFactory.java
@@ -0,0 +1,37 @@
+package org.apache.lucene.index.codecs.mocksep;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.index.codecs.sep.IntStreamFactory;
+import org.apache.lucene.index.codecs.sep.IntIndexInput;
+import org.apache.lucene.index.codecs.sep.IntIndexOutput;
+
+import java.io.IOException;
+
+/** @lucene.experimental */
+public class MockSingleIntFactory extends IntStreamFactory {
+  @Override
+  public IntIndexInput openInput(Directory dir, String fileName, int readBufferSize) throws IOException {
+    return new MockSingleIntIndexInput(dir, fileName, readBufferSize);
+  }
+  @Override
+  public IntIndexOutput createOutput(Directory dir, String fileName) throws IOException {
+    return new MockSingleIntIndexOutput(dir, fileName);
+  }
+}
diff --git a/lucene/src/test-framework/org/apache/lucene/index/codecs/mocksep/MockSingleIntIndexInput.java b/lucene/src/test-framework/org/apache/lucene/index/codecs/mocksep/MockSingleIntIndexInput.java
new file mode 100644
index 0000000..031794d
--- /dev/null
+++ b/lucene/src/test-framework/org/apache/lucene/index/codecs/mocksep/MockSingleIntIndexInput.java
@@ -0,0 +1,123 @@
+package org.apache.lucene.index.codecs.mocksep;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.codecs.sep.IntIndexInput;
+import org.apache.lucene.store.DataInput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.CodecUtil;
+
+/** Reads IndexInputs written with {@link
+ *  SingleIntIndexOutput}.  NOTE: this class is just for
+ *  demonstration puprposes (it is a very slow way to read a
+ *  block of ints).
+ *
+ * @lucene.experimental
+ */
+public class MockSingleIntIndexInput extends IntIndexInput {
+  private final IndexInput in;
+
+  public MockSingleIntIndexInput(Directory dir, String fileName, int readBufferSize)
+    throws IOException {
+    in = dir.openInput(fileName, readBufferSize);
+    CodecUtil.checkHeader(in, MockSingleIntIndexOutput.CODEC,
+                          MockSingleIntIndexOutput.VERSION_START,
+                          MockSingleIntIndexOutput.VERSION_START);
+  }
+
+  @Override
+  public Reader reader() throws IOException {
+    return new Reader((IndexInput) in.clone());
+  }
+
+  @Override
+  public void close() throws IOException {
+    in.close();
+  }
+
+  public static class Reader extends IntIndexInput.Reader {
+    // clone:
+    private final IndexInput in;
+
+    public Reader(IndexInput in) {
+      this.in = in;
+    }
+
+    /** Reads next single int */
+    @Override
+    public int next() throws IOException {
+      //System.out.println("msii.next() fp=" + in.getFilePointer() + " vs " + in.length());
+      return in.readVInt();
+    }
+  }
+  
+  class Index extends IntIndexInput.Index {
+    private long fp;
+
+    @Override
+    public void read(DataInput indexIn, boolean absolute)
+      throws IOException {
+      if (absolute) {
+        fp = indexIn.readVLong();
+      } else {
+        fp += indexIn.readVLong();
+      }
+    }
+
+    @Override
+    public void read(IntIndexInput.Reader indexIn, boolean absolute)
+      throws IOException {
+      if (absolute) {
+        fp = indexIn.readVLong();
+      } else {
+        fp += indexIn.readVLong();
+      }
+    }
+
+    @Override
+    public void set(IntIndexInput.Index other) {
+      fp = ((Index) other).fp;
+    }
+
+    @Override
+    public void seek(IntIndexInput.Reader other) throws IOException {
+      ((Reader) other).in.seek(fp);
+    }
+
+    @Override
+    public String toString() {
+      return Long.toString(fp);
+    }
+
+    @Override
+    public Object clone() {
+      Index other = new Index();
+      other.fp = fp;
+      return other;
+    }
+  }
+
+  @Override
+  public Index index() {
+    return new Index();
+  }
+}
+
diff --git a/lucene/src/test-framework/org/apache/lucene/index/codecs/mocksep/MockSingleIntIndexOutput.java b/lucene/src/test-framework/org/apache/lucene/index/codecs/mocksep/MockSingleIntIndexOutput.java
new file mode 100644
index 0000000..98ba2b4
--- /dev/null
+++ b/lucene/src/test-framework/org/apache/lucene/index/codecs/mocksep/MockSingleIntIndexOutput.java
@@ -0,0 +1,97 @@
+package org.apache.lucene.index.codecs.mocksep;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.CodecUtil;
+import org.apache.lucene.index.codecs.sep.IntIndexOutput;
+import java.io.IOException;
+
+/** Writes ints directly to the file (not in blocks) as
+ *  vInt.
+ * 
+ * @lucene.experimental
+*/
+public class MockSingleIntIndexOutput extends IntIndexOutput {
+  private final IndexOutput out;
+  final static String CODEC = "SINGLE_INTS";
+  final static int VERSION_START = 0;
+  final static int VERSION_CURRENT = VERSION_START;
+
+  public MockSingleIntIndexOutput(Directory dir, String fileName) throws IOException {
+    out = dir.createOutput(fileName);
+    CodecUtil.writeHeader(out, CODEC, VERSION_CURRENT);
+  }
+
+  /** Write an int to the primary file */
+  @Override
+  public void write(int v) throws IOException {
+    assert v >= 0;
+    out.writeVInt(v);
+  }
+
+  @Override
+  public Index index() {
+    return new Index();
+  }
+
+  @Override
+  public void close() throws IOException {
+    out.close();
+  }
+
+  private class Index extends IntIndexOutput.Index {
+    long fp;
+    long lastFP;
+    @Override
+    public void mark() {
+      fp = out.getFilePointer();
+    }
+    @Override
+    public void set(IntIndexOutput.Index other) {
+      lastFP = fp = ((Index) other).fp;
+    }
+    @Override
+    public void write(IndexOutput indexOut, boolean absolute)
+      throws IOException {
+      if (absolute) {
+        indexOut.writeVLong(fp);
+      } else {
+        indexOut.writeVLong(fp - lastFP);
+      }
+      lastFP = fp;
+    }
+
+    @Override
+    public void write(IntIndexOutput indexOut, boolean absolute) 
+      throws IOException {
+      if (absolute) {
+        indexOut.writeVLong(fp);
+      } else {
+        indexOut.writeVLong(fp - lastFP);
+      }
+      lastFP = fp;
+    }
+      
+    @Override
+    public String toString() {
+      return Long.toString(fp);
+    }
+  }
+}
diff --git a/lucene/src/test-framework/org/apache/lucene/index/codecs/preflexrw/PreFlexFieldsWriter.java b/lucene/src/test-framework/org/apache/lucene/index/codecs/preflexrw/PreFlexFieldsWriter.java
new file mode 100644
index 0000000..00b6e01
--- /dev/null
+++ b/lucene/src/test-framework/org/apache/lucene/index/codecs/preflexrw/PreFlexFieldsWriter.java
@@ -0,0 +1,209 @@
+package org.apache.lucene.index.codecs.preflexrw;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.index.codecs.FieldsConsumer;
+import org.apache.lucene.index.codecs.TermsConsumer;
+import org.apache.lucene.index.codecs.PostingsConsumer;
+import org.apache.lucene.index.codecs.TermStats;
+import org.apache.lucene.index.codecs.standard.DefaultSkipListWriter;
+import org.apache.lucene.index.codecs.preflex.PreFlexCodec;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.codecs.preflex.TermInfo;
+import org.apache.lucene.store.IndexOutput;
+
+import java.io.IOException;
+import java.util.Comparator;
+
+class PreFlexFieldsWriter extends FieldsConsumer {
+
+  private final TermInfosWriter termsOut;
+  private final IndexOutput freqOut;
+  private final IndexOutput proxOut;
+  private final DefaultSkipListWriter skipListWriter;
+  private final int totalNumDocs;
+
+  public PreFlexFieldsWriter(SegmentWriteState state) throws IOException {
+    termsOut = new TermInfosWriter(state.directory,
+                                   state.segmentName,
+                                   state.fieldInfos,
+                                   state.termIndexInterval);
+
+    final String freqFile = IndexFileNames.segmentFileName(state.segmentName, "", PreFlexCodec.FREQ_EXTENSION);
+    freqOut = state.directory.createOutput(freqFile);
+    totalNumDocs = state.numDocs;
+
+    if (state.fieldInfos.hasProx()) {
+      final String proxFile = IndexFileNames.segmentFileName(state.segmentName, "", PreFlexCodec.PROX_EXTENSION);
+      proxOut = state.directory.createOutput(proxFile);
+    } else {
+      proxOut = null;
+    }
+
+    skipListWriter = new DefaultSkipListWriter(termsOut.skipInterval,
+                                               termsOut.maxSkipLevels,
+                                               totalNumDocs,
+                                               freqOut,
+                                               proxOut);
+    //System.out.println("\nw start seg=" + segment);
+  }
+
+  @Override
+  public TermsConsumer addField(FieldInfo field) throws IOException {
+    assert field.number != -1;
+    //System.out.println("w field=" + field.name + " storePayload=" + field.storePayloads + " number=" + field.number);
+    return new PreFlexTermsWriter(field);
+  }
+
+  @Override
+  public void close() throws IOException {
+    termsOut.close();
+    freqOut.close();
+    if (proxOut != null) {
+      proxOut.close();
+    }
+  }
+
+  private class PreFlexTermsWriter extends TermsConsumer {
+    private final FieldInfo fieldInfo;
+    private final boolean omitTF;
+    private final boolean storePayloads;
+    
+    private final TermInfo termInfo = new TermInfo();
+    private final PostingsWriter postingsWriter = new PostingsWriter();
+
+    public PreFlexTermsWriter(FieldInfo fieldInfo) {
+      this.fieldInfo = fieldInfo;
+      omitTF = fieldInfo.omitTermFreqAndPositions;
+      storePayloads = fieldInfo.storePayloads;
+    }
+
+    private class PostingsWriter extends PostingsConsumer {
+      private int lastDocID;
+      private int lastPayloadLength = -1;
+      private int lastPosition;
+      private int df;
+
+      public PostingsWriter reset() {
+        df = 0;
+        lastDocID = 0;
+        lastPayloadLength = -1;
+        return this;
+      }
+
+      @Override
+      public void startDoc(int docID, int termDocFreq) throws IOException {
+        //System.out.println("    w doc=" + docID);
+
+        final int delta = docID - lastDocID;
+        if (docID < 0 || (df > 0 && delta <= 0)) {
+          throw new CorruptIndexException("docs out of order (" + docID + " <= " + lastDocID + " )");
+        }
+
+        if ((++df % termsOut.skipInterval) == 0) {
+          skipListWriter.setSkipData(lastDocID, storePayloads, lastPayloadLength);
+          skipListWriter.bufferSkip(df);
+        }
+
+        lastDocID = docID;
+
+        assert docID < totalNumDocs: "docID=" + docID + " totalNumDocs=" + totalNumDocs;
+
+        if (omitTF) {
+          freqOut.writeVInt(delta);
+        } else {
+          final int code = delta << 1;
+          if (termDocFreq == 1) {
+            freqOut.writeVInt(code|1);
+          } else {
+            freqOut.writeVInt(code);
+            freqOut.writeVInt(termDocFreq);
+          }
+        }
+        lastPosition = 0;
+      }
+
+      @Override
+      public void addPosition(int position, BytesRef payload) throws IOException {
+        assert proxOut != null;
+
+        //System.out.println("      w pos=" + position + " payl=" + payload);
+        final int delta = position - lastPosition;
+        lastPosition = position;
+
+        if (storePayloads) {
+          final int payloadLength = payload == null ? 0 : payload.length;
+          if (payloadLength != lastPayloadLength) {
+            //System.out.println("        write payload len=" + payloadLength);
+            lastPayloadLength = payloadLength;
+            proxOut.writeVInt((delta<<1)|1);
+            proxOut.writeVInt(payloadLength);
+          } else {
+            proxOut.writeVInt(delta << 1);
+          }
+          if (payloadLength > 0) {
+            proxOut.writeBytes(payload.bytes, payload.offset, payload.length);
+          }
+        } else {
+          proxOut.writeVInt(delta);
+        }
+      }
+
+      @Override
+      public void finishDoc() throws IOException {
+      }
+    }
+
+    @Override
+    public PostingsConsumer startTerm(BytesRef text) throws IOException {
+      //System.out.println("  w term=" + text.utf8ToString());
+      skipListWriter.resetSkip();
+      termInfo.freqPointer = freqOut.getFilePointer();
+      if (proxOut != null) {
+        termInfo.proxPointer = proxOut.getFilePointer();
+      }
+      return postingsWriter.reset();
+    }
+
+    @Override
+    public void finishTerm(BytesRef text, TermStats stats) throws IOException {
+      if (stats.docFreq > 0) {
+        long skipPointer = skipListWriter.writeSkip(freqOut);
+        termInfo.docFreq = stats.docFreq;
+        termInfo.skipOffset = (int) (skipPointer - termInfo.freqPointer);
+        //System.out.println("  w finish term=" + text.utf8ToString() + " fnum=" + fieldInfo.number);
+        termsOut.add(fieldInfo.number,
+                     text,
+                     termInfo);
+      }
+    }
+
+    @Override
+    public void finish(long sumTotalTermCount) throws IOException {
+    }
+
+    @Override
+    public Comparator<BytesRef> getComparator() throws IOException {
+      return BytesRef.getUTF8SortedAsUTF16Comparator();
+    }
+  }
+}
\ No newline at end of file
diff --git a/lucene/src/test-framework/org/apache/lucene/index/codecs/preflexrw/PreFlexRWCodec.java b/lucene/src/test-framework/org/apache/lucene/index/codecs/preflexrw/PreFlexRWCodec.java
new file mode 100644
index 0000000..5a2d947
--- /dev/null
+++ b/lucene/src/test-framework/org/apache/lucene/index/codecs/preflexrw/PreFlexRWCodec.java
@@ -0,0 +1,77 @@
+package org.apache.lucene.index.codecs.preflexrw;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.codecs.preflex.PreFlexCodec;
+import org.apache.lucene.index.codecs.preflex.PreFlexFields;
+import org.apache.lucene.index.codecs.FieldsConsumer;
+import org.apache.lucene.index.codecs.FieldsProducer;
+import org.apache.lucene.util.LuceneTestCase;
+
+/** Codec, only for testing, that can write and read the
+ *  pre-flex index format.
+ *
+ * @lucene.experimental
+ */
+public class PreFlexRWCodec extends PreFlexCodec {
+
+  public PreFlexRWCodec() {
+    // NOTE: we impersonate the PreFlex codec so that it can
+    // read the segments we write!
+    super();
+  }
+  
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    return new PreFlexFieldsWriter(state);
+  }
+
+  @Override
+  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+
+    // Whenever IW opens readers, eg for merging, we have to
+    // keep terms order in UTF16:
+
+    return new PreFlexFields(state.dir, state.fieldInfos, state.segmentInfo, state.readBufferSize, state.termsIndexDivisor) {
+      @Override
+      protected boolean sortTermsByUnicode() {
+        // We carefully peek into stack track above us: if
+        // we are part of a "merge", we must sort by UTF16:
+        boolean unicodeSortOrder = true;
+
+        StackTraceElement[] trace = new Exception().getStackTrace();
+        for (int i = 0; i < trace.length; i++) {
+          //System.out.println(trace[i].getClassName());
+          if ("merge".equals(trace[i].getMethodName())) {
+            unicodeSortOrder = false;
+            if (LuceneTestCase.VERBOSE) {
+              System.out.println("NOTE: PreFlexRW codec: forcing legacy UTF16 term sort order");
+            }
+            break;
+          }
+        }
+
+        return unicodeSortOrder;
+      }
+    };
+  }
+}
diff --git a/lucene/src/test-framework/org/apache/lucene/index/codecs/preflexrw/TermInfosWriter.java b/lucene/src/test-framework/org/apache/lucene/index/codecs/preflexrw/TermInfosWriter.java
new file mode 100644
index 0000000..782cd3a
--- /dev/null
+++ b/lucene/src/test-framework/org/apache/lucene/index/codecs/preflexrw/TermInfosWriter.java
@@ -0,0 +1,227 @@
+package org.apache.lucene.index.codecs.preflexrw;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+import java.io.IOException;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.UnicodeUtil;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.index.codecs.preflex.TermInfo;
+
+
+/** This stores a monotonically increasing set of <Term, TermInfo> pairs in a
+  Directory.  A TermInfos can be written once, in order.  */
+
+final class TermInfosWriter {
+  /** The file format version, a negative number. */
+  public static final int FORMAT = -3;
+
+  // Changed strings to true utf8 with length-in-bytes not
+  // length-in-chars
+  public static final int FORMAT_VERSION_UTF8_LENGTH_IN_BYTES = -4;
+
+  // NOTE: always change this if you switch to a new format!
+  public static final int FORMAT_CURRENT = FORMAT_VERSION_UTF8_LENGTH_IN_BYTES;
+
+  private FieldInfos fieldInfos;
+  private IndexOutput output;
+  private TermInfo lastTi = new TermInfo();
+  private long size;
+
+  // TODO: the default values for these two parameters should be settable from
+  // IndexWriter.  However, once that's done, folks will start setting them to
+  // ridiculous values and complaining that things don't work well, as with
+  // mergeFactor.  So, let's wait until a number of folks find that alternate
+  // values work better.  Note that both of these values are stored in the
+  // segment, so that it's safe to change these w/o rebuilding all indexes.
+
+  /** Expert: The fraction of terms in the "dictionary" which should be stored
+   * in RAM.  Smaller values use more memory, but make searching slightly
+   * faster, while larger values use less memory and make searching slightly
+   * slower.  Searching is typically not dominated by dictionary lookup, so
+   * tweaking this is rarely useful.*/
+  int indexInterval = 128;
+
+  /** Expert: The fraction of {@link TermDocs} entries stored in skip tables,
+   * used to accelerate {@link TermDocs#skipTo(int)}.  Larger values result in
+   * smaller indexes, greater acceleration, but fewer accelerable cases, while
+   * smaller values result in bigger indexes, less acceleration and more
+   * accelerable cases. More detailed experiments would be useful here. */
+  int skipInterval = 16;
+  
+  /** Expert: The maximum number of skip levels. Smaller values result in 
+   * slightly smaller indexes, but slower skipping in big posting lists.
+   */
+  int maxSkipLevels = 10;
+
+  private long lastIndexPointer;
+  private boolean isIndex;
+  private final BytesRef lastTerm = new BytesRef();
+  private int lastFieldNumber = -1;
+
+  private TermInfosWriter other;
+
+  TermInfosWriter(Directory directory, String segment, FieldInfos fis,
+                  int interval)
+       throws IOException {
+    initialize(directory, segment, fis, interval, false);
+    other = new TermInfosWriter(directory, segment, fis, interval, true);
+    other.other = this;
+  }
+
+  private TermInfosWriter(Directory directory, String segment, FieldInfos fis,
+                          int interval, boolean isIndex) throws IOException {
+    initialize(directory, segment, fis, interval, isIndex);
+  }
+
+  private void initialize(Directory directory, String segment, FieldInfos fis,
+                          int interval, boolean isi) throws IOException {
+    indexInterval = interval;
+    fieldInfos = fis;
+    isIndex = isi;
+    output = directory.createOutput(segment + (isIndex ? ".tii" : ".tis"));
+    output.writeInt(FORMAT_CURRENT);              // write format
+    output.writeLong(0);                          // leave space for size
+    output.writeInt(indexInterval);               // write indexInterval
+    output.writeInt(skipInterval);                // write skipInterval
+    output.writeInt(maxSkipLevels);               // write maxSkipLevels
+    assert initUTF16Results();
+  }
+
+  // Currently used only by assert statements
+  UnicodeUtil.UTF16Result utf16Result1;
+  UnicodeUtil.UTF16Result utf16Result2;
+  private final BytesRef scratchBytes = new BytesRef();
+
+  // Currently used only by assert statements
+  private boolean initUTF16Results() {
+    utf16Result1 = new UnicodeUtil.UTF16Result();
+    utf16Result2 = new UnicodeUtil.UTF16Result();
+    return true;
+  }
+
+  // Currently used only by assert statement
+  private int compareToLastTerm(int fieldNumber, BytesRef term) {
+
+    if (lastFieldNumber != fieldNumber) {
+      final int cmp = fieldInfos.fieldName(lastFieldNumber).compareTo(fieldInfos.fieldName(fieldNumber));
+      // If there is a field named "" (empty string) then we
+      // will get 0 on this comparison, yet, it's "OK".  But
+      // it's not OK if two different field numbers map to
+      // the same name.
+      if (cmp != 0 || lastFieldNumber != -1)
+        return cmp;
+    }
+
+    scratchBytes.copy(term);
+    assert lastTerm.offset == 0;
+    UnicodeUtil.UTF8toUTF16(lastTerm.bytes, 0, lastTerm.length, utf16Result1);
+
+    assert scratchBytes.offset == 0;
+    UnicodeUtil.UTF8toUTF16(scratchBytes.bytes, 0, scratchBytes.length, utf16Result2);
+
+    final int len;
+    if (utf16Result1.length < utf16Result2.length)
+      len = utf16Result1.length;
+    else
+      len = utf16Result2.length;
+
+    for(int i=0;i<len;i++) {
+      final char ch1 = utf16Result1.result[i];
+      final char ch2 = utf16Result2.result[i];
+      if (ch1 != ch2)
+        return ch1-ch2;
+    }
+    return utf16Result1.length - utf16Result2.length;
+  }
+
+  /** Adds a new <<fieldNumber, termBytes>, TermInfo> pair to the set.
+    Term must be lexicographically greater than all previous Terms added.
+    TermInfo pointers must be positive and greater than all previous.*/
+  public void add(int fieldNumber, BytesRef term, TermInfo ti)
+    throws IOException {
+
+    assert compareToLastTerm(fieldNumber, term) < 0 ||
+      (isIndex && term.length == 0 && lastTerm.length == 0) :
+      "Terms are out of order: field=" + fieldInfos.fieldName(fieldNumber) + " (number " + fieldNumber + ")" +
+        " lastField=" + fieldInfos.fieldName(lastFieldNumber) + " (number " + lastFieldNumber + ")" +
+        " text=" + term.utf8ToString() + " lastText=" + lastTerm.utf8ToString();
+
+    assert ti.freqPointer >= lastTi.freqPointer: "freqPointer out of order (" + ti.freqPointer + " < " + lastTi.freqPointer + ")";
+    assert ti.proxPointer >= lastTi.proxPointer: "proxPointer out of order (" + ti.proxPointer + " < " + lastTi.proxPointer + ")";
+
+    if (!isIndex && size % indexInterval == 0)
+      other.add(lastFieldNumber, lastTerm, lastTi);                      // add an index term
+
+    writeTerm(fieldNumber, term);                        // write term
+
+    output.writeVInt(ti.docFreq);                       // write doc freq
+    output.writeVLong(ti.freqPointer - lastTi.freqPointer); // write pointers
+    output.writeVLong(ti.proxPointer - lastTi.proxPointer);
+
+    if (ti.docFreq >= skipInterval) {
+      output.writeVInt(ti.skipOffset);
+    }
+
+    if (isIndex) {
+      output.writeVLong(other.output.getFilePointer() - lastIndexPointer);
+      lastIndexPointer = other.output.getFilePointer(); // write pointer
+    }
+
+    lastFieldNumber = fieldNumber;
+    lastTi.set(ti);
+    size++;
+  }
+
+  private void writeTerm(int fieldNumber, BytesRef term)
+       throws IOException {
+
+    //System.out.println("  tiw.write field=" + fieldNumber + " term=" + term.utf8ToString());
+
+    // TODO: UTF16toUTF8 could tell us this prefix
+    // Compute prefix in common with last term:
+    int start = 0;
+    final int limit = term.length < lastTerm.length ? term.length : lastTerm.length;
+    while(start < limit) {
+      if (term.bytes[start+term.offset] != lastTerm.bytes[start+lastTerm.offset])
+        break;
+      start++;
+    }
+
+    final int length = term.length - start;
+    output.writeVInt(start);                     // write shared prefix length
+    output.writeVInt(length);                  // write delta length
+    output.writeBytes(term.bytes, start+term.offset, length);  // write delta bytes
+    output.writeVInt(fieldNumber); // write field num
+    lastTerm.copy(term);
+  }
+
+  /** Called to complete TermInfos creation. */
+  void close() throws IOException {
+    output.seek(4);          // write size after format
+    output.writeLong(size);
+    output.close();
+
+    if (!isIndex)
+      other.close();
+  }
+
+}
diff --git a/lucene/src/test-framework/org/apache/lucene/search/CheckHits.java b/lucene/src/test-framework/org/apache/lucene/search/CheckHits.java
new file mode 100644
index 0000000..6846e59
--- /dev/null
+++ b/lucene/src/test-framework/org/apache/lucene/search/CheckHits.java
@@ -0,0 +1,500 @@
+package org.apache.lucene.search;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+import java.util.TreeSet;
+import java.util.Random;
+
+import junit.framework.Assert;
+
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.store.Directory;
+
+public class CheckHits {
+  
+  /**
+   * Some explains methods calculate their values though a slightly
+   * different  order of operations from the actual scoring method ...
+   * this allows for a small amount of variation
+   */
+  public static float EXPLAIN_SCORE_TOLERANCE_DELTA = 0.0002f;
+    
+  /**
+   * Tests that all documents up to maxDoc which are *not* in the
+   * expected result set, have an explanation which indicates no match
+   * (ie: Explanation value of 0.0f)
+   */
+  public static void checkNoMatchExplanations(Query q, String defaultFieldName,
+                                              IndexSearcher searcher, int[] results)
+    throws IOException {
+
+    String d = q.toString(defaultFieldName);
+    Set<Integer> ignore = new TreeSet<Integer>();
+    for (int i = 0; i < results.length; i++) {
+      ignore.add(Integer.valueOf(results[i]));
+    }
+    
+    int maxDoc = searcher.maxDoc();
+    for (int doc = 0; doc < maxDoc; doc++) {
+      if (ignore.contains(Integer.valueOf(doc))) continue;
+
+      Explanation exp = searcher.explain(q, doc);
+      Assert.assertNotNull("Explanation of [["+d+"]] for #"+doc+" is null",
+                             exp);
+      Assert.assertEquals("Explanation of [["+d+"]] for #"+doc+
+                            " doesn't indicate non-match: " + exp.toString(),
+                            0.0f, exp.getValue(), 0.0f);
+    }
+    
+  }
+  
+  /**
+   * Tests that a query matches the an expected set of documents using a
+   * HitCollector.
+   *
+   * <p>
+   * Note that when using the HitCollector API, documents will be collected
+   * if they "match" regardless of what their score is.
+   * </p>
+   * @param query the query to test
+   * @param searcher the searcher to test the query against
+   * @param defaultFieldName used for displaying the query in assertion messages
+   * @param results a list of documentIds that must match the query
+   * @see Searcher#search(Query,Collector)
+   * @see #checkHits
+   */
+  public static void checkHitCollector(Random random, Query query, String defaultFieldName,
+                                       IndexSearcher searcher, int[] results)
+    throws IOException {
+
+    QueryUtils.check(random,query,searcher);
+    
+    Set<Integer> correct = new TreeSet<Integer>();
+    for (int i = 0; i < results.length; i++) {
+      correct.add(Integer.valueOf(results[i]));
+    }
+    final Set<Integer> actual = new TreeSet<Integer>();
+    final Collector c = new SetCollector(actual);
+
+    searcher.search(query, c);
+    Assert.assertEquals("Simple: " + query.toString(defaultFieldName), 
+                        correct, actual);
+
+    for (int i = -1; i < 2; i++) {
+      actual.clear();
+      IndexSearcher s = QueryUtils.wrapUnderlyingReader
+        (random, searcher, i);
+      s.search(query, c);
+      Assert.assertEquals("Wrap Reader " + i + ": " +
+                          query.toString(defaultFieldName),
+                          correct, actual);
+      s.close();
+    }
+  }
+
+  public static class SetCollector extends Collector {
+    final Set<Integer> bag;
+    public SetCollector(Set<Integer> bag) {
+      this.bag = bag;
+    }
+    private int base = 0;
+    @Override
+    public void setScorer(Scorer scorer) throws IOException {}
+    @Override
+    public void collect(int doc) {
+      bag.add(Integer.valueOf(doc + base));
+    }
+    @Override
+    public void setNextReader(AtomicReaderContext context) {
+      base = context.docBase;
+    }
+    @Override
+    public boolean acceptsDocsOutOfOrder() {
+      return true;
+    }
+  }
+
+  /**
+   * Tests that a query matches the an expected set of documents using Hits.
+   *
+   * <p>
+   * Note that when using the Hits API, documents will only be returned
+   * if they have a positive normalized score.
+   * </p>
+   * @param query the query to test
+   * @param searcher the searcher to test the query against
+   * @param defaultFieldName used for displaing the query in assertion messages
+   * @param results a list of documentIds that must match the query
+   * @see Searcher#search(Query, int)
+   * @see #checkHitCollector
+   */
+  public static void checkHits(
+        Random random,
+        Query query,
+        String defaultFieldName,
+        IndexSearcher searcher,
+        int[] results)
+          throws IOException {
+
+    ScoreDoc[] hits = searcher.search(query, 1000).scoreDocs;
+
+    Set<Integer> correct = new TreeSet<Integer>();
+    for (int i = 0; i < results.length; i++) {
+      correct.add(Integer.valueOf(results[i]));
+    }
+
+    Set<Integer> actual = new TreeSet<Integer>();
+    for (int i = 0; i < hits.length; i++) {
+      actual.add(Integer.valueOf(hits[i].doc));
+    }
+
+    Assert.assertEquals(query.toString(defaultFieldName), correct, actual);
+
+    QueryUtils.check(random, query,searcher);
+  }
+
+  /** Tests that a Hits has an expected order of documents */
+  public static void checkDocIds(String mes, int[] results, ScoreDoc[] hits)
+  throws IOException {
+    Assert.assertEquals(mes + " nr of hits", hits.length, results.length);
+    for (int i = 0; i < results.length; i++) {
+      Assert.assertEquals(mes + " doc nrs for hit " + i, results[i], hits[i].doc);
+    }
+  }
+
+  /** Tests that two queries have an expected order of documents,
+   * and that the two queries have the same score values.
+   */
+  public static void checkHitsQuery(
+        Query query,
+        ScoreDoc[] hits1,
+        ScoreDoc[] hits2,
+        int[] results)
+          throws IOException {
+
+    checkDocIds("hits1", results, hits1);
+    checkDocIds("hits2", results, hits2);
+    checkEqual(query, hits1, hits2);
+  }
+
+  public static void checkEqual(Query query, ScoreDoc[] hits1, ScoreDoc[] hits2) throws IOException {
+     final float scoreTolerance = 1.0e-6f;
+     if (hits1.length != hits2.length) {
+       Assert.fail("Unequal lengths: hits1="+hits1.length+",hits2="+hits2.length);
+     }
+    for (int i = 0; i < hits1.length; i++) {
+      if (hits1[i].doc != hits2[i].doc) {
+        Assert.fail("Hit " + i + " docnumbers don't match\n"
+                + hits2str(hits1, hits2,0,0)
+                + "for query:" + query.toString());
+      }
+
+      if ((hits1[i].doc != hits2[i].doc)
+          || Math.abs(hits1[i].score -  hits2[i].score) > scoreTolerance)
+      {
+        Assert.fail("Hit " + i + ", doc nrs " + hits1[i].doc + " and " + hits2[i].doc
+                      + "\nunequal       : " + hits1[i].score
+                      + "\n           and: " + hits2[i].score
+                      + "\nfor query:" + query.toString());
+      }
+    }
+  }
+
+  public static String hits2str(ScoreDoc[] hits1, ScoreDoc[] hits2, int start, int end) throws IOException {
+    StringBuilder sb = new StringBuilder();
+    int len1=hits1==null ? 0 : hits1.length;
+    int len2=hits2==null ? 0 : hits2.length;
+    if (end<=0) {
+      end = Math.max(len1,len2);
+    }
+
+      sb.append("Hits length1=").append(len1).append("\tlength2=").append(len2);
+
+    sb.append('\n');
+    for (int i=start; i<end; i++) {
+        sb.append("hit=").append(i).append(':');
+      if (i<len1) {
+          sb.append(" doc").append(hits1[i].doc).append('=').append(hits1[i].score);
+      } else {
+        sb.append("               ");
+      }
+      sb.append(",\t");
+      if (i<len2) {
+        sb.append(" doc").append(hits2[i].doc).append('=').append(hits2[i].score);
+      }
+      sb.append('\n');
+    }
+    return sb.toString();
+  }
+
+
+  public static String topdocsString(TopDocs docs, int start, int end) {
+    StringBuilder sb = new StringBuilder();
+      sb.append("TopDocs totalHits=").append(docs.totalHits).append(" top=").append(docs.scoreDocs.length).append('\n');
+    if (end<=0) end=docs.scoreDocs.length;
+    else end=Math.min(end,docs.scoreDocs.length);
+    for (int i=start; i<end; i++) {
+      sb.append('\t');
+      sb.append(i);
+      sb.append(") doc=");
+      sb.append(docs.scoreDocs[i].doc);
+      sb.append("\tscore=");
+      sb.append(docs.scoreDocs[i].score);
+      sb.append('\n');
+    }
+    return sb.toString();
+  }
+
+  /**
+   * Asserts that the explanation value for every document matching a
+   * query corresponds with the true score. 
+   *
+   * @see ExplanationAsserter
+   * @see #checkExplanations(Query, String, Searcher, boolean) for a
+   * "deep" testing of the explanation details.
+   *   
+   * @param query the query to test
+   * @param searcher the searcher to test the query against
+   * @param defaultFieldName used for displaing the query in assertion messages
+   */
+  public static void checkExplanations(Query query,
+                                       String defaultFieldName,
+                                       IndexSearcher searcher) throws IOException {
+    checkExplanations(query, defaultFieldName, searcher, false);
+  }
+
+  /**
+   * Asserts that the explanation value for every document matching a
+   * query corresponds with the true score.  Optionally does "deep" 
+   * testing of the explanation details.
+   *
+   * @see ExplanationAsserter
+   * @param query the query to test
+   * @param searcher the searcher to test the query against
+   * @param defaultFieldName used for displaing the query in assertion messages
+   * @param deep indicates whether a deep comparison of sub-Explanation details should be executed
+   */
+  public static void checkExplanations(Query query,
+                                       String defaultFieldName,
+                                       IndexSearcher searcher, 
+                                       boolean deep) throws IOException {
+
+    searcher.search(query,
+                    new ExplanationAsserter
+                    (query, defaultFieldName, searcher, deep));
+
+  }
+
+  /** 
+   * Assert that an explanation has the expected score, and optionally that its
+   * sub-details max/sum/factor match to that score.
+   *
+   * @param q String representation of the query for assertion messages
+   * @param doc Document ID for assertion messages
+   * @param score Real score value of doc with query q
+   * @param deep indicates whether a deep comparison of sub-Explanation details should be executed
+   * @param expl The Explanation to match against score
+   */
+  public static void verifyExplanation(String q, 
+                                       int doc, 
+                                       float score,
+                                       boolean deep,
+                                       Explanation expl) {
+    float value = expl.getValue();
+    Assert.assertEquals(q+": score(doc="+doc+")="+score+
+        " != explanationScore="+value+" Explanation: "+expl,
+        score,value,EXPLAIN_SCORE_TOLERANCE_DELTA);
+
+    if (!deep) return;
+
+    Explanation detail[] = expl.getDetails();
+    if (detail!=null) {
+      if (detail.length==1) {
+        // simple containment, no matter what the description says, 
+        // just verify contained expl has same score
+        verifyExplanation(q,doc,score,deep,detail[0]);
+      } else {
+        // explanation must either:
+        // - end with one of: "product of:", "sum of:", "max of:", or
+        // - have "max plus <x> times others" (where <x> is float).
+        float x = 0;
+        String descr = expl.getDescription().toLowerCase();
+        boolean productOf = descr.endsWith("product of:");
+        boolean sumOf = descr.endsWith("sum of:");
+        boolean maxOf = descr.endsWith("max of:");
+        boolean maxTimesOthers = false;
+        if (!(productOf || sumOf || maxOf)) {
+          // maybe 'max plus x times others'
+          int k1 = descr.indexOf("max plus ");
+          if (k1>=0) {
+            k1 += "max plus ".length();
+            int k2 = descr.indexOf(" ",k1);
+            try {
+              x = Float.parseFloat(descr.substring(k1,k2).trim());
+              if (descr.substring(k2).trim().equals("times others of:")) {
+                maxTimesOthers = true;
+              }
+            } catch (NumberFormatException e) {
+            }
+          }
+        }
+        Assert.assertTrue(
+            q+": multi valued explanation description=\""+descr
+            +"\" must be 'max of plus x times others' or end with 'product of'"
+            +" or 'sum of:' or 'max of:' - "+expl,
+            productOf || sumOf || maxOf || maxTimesOthers);
+        float sum = 0;
+        float product = 1;
+        float max = 0;
+        for (int i=0; i<detail.length; i++) {
+          float dval = detail[i].getValue();
+          verifyExplanation(q,doc,dval,deep,detail[i]);
+          product *= dval;
+          sum += dval;
+          max = Math.max(max,dval);
+        }
+        float combined = 0;
+        if (productOf) {
+          combined = product;
+        } else if (sumOf) {
+          combined = sum;
+        } else if (maxOf) {
+          combined = max;
+        } else if (maxTimesOthers) {
+          combined = max + x * (sum - max);
+        } else {
+            Assert.assertTrue("should never get here!",false);
+        }
+        Assert.assertEquals(q+": actual subDetails combined=="+combined+
+            " != value="+value+" Explanation: "+expl,
+            combined,value,EXPLAIN_SCORE_TOLERANCE_DELTA);
+      }
+    }
+  }
+
+  /**
+   * an IndexSearcher that implicitly checks hte explanation of every match
+   * whenever it executes a search.
+   *
+   * @see ExplanationAsserter
+   */
+  public static class ExplanationAssertingSearcher extends IndexSearcher {
+    public ExplanationAssertingSearcher(Directory d) throws IOException {
+      super(d, true);
+    }
+    public ExplanationAssertingSearcher(IndexReader r) throws IOException {
+      super(r);
+    }
+    protected void checkExplanations(Query q) throws IOException {
+      super.search(q, null,
+                   new ExplanationAsserter
+                   (q, null, this));
+    }
+    @Override
+    public TopFieldDocs search(Query query,
+                               Filter filter,
+                               int n,
+                               Sort sort) throws IOException {
+      
+      checkExplanations(query);
+      return super.search(query,filter,n,sort);
+    }
+    @Override
+    public void search(Query query, Collector results) throws IOException {
+      checkExplanations(query);
+      super.search(query, results);
+    }
+    @Override
+    public void search(Query query, Filter filter, Collector results) throws IOException {
+      checkExplanations(query);
+      super.search(query, filter, results);
+    }
+    @Override
+    public TopDocs search(Query query, Filter filter,
+                          int n) throws IOException {
+
+      checkExplanations(query);
+      return super.search(query,filter, n);
+    }
+  }
+    
+  /**
+   * Asserts that the score explanation for every document matching a
+   * query corresponds with the true score.
+   *
+   * NOTE: this HitCollector should only be used with the Query and Searcher
+   * specified at when it is constructed.
+   *
+   * @see CheckHits#verifyExplanation
+   */
+  public static class ExplanationAsserter extends Collector {
+
+    Query q;
+    IndexSearcher s;
+    String d;
+    boolean deep;
+    
+    Scorer scorer;
+    private int base = 0;
+
+    /** Constructs an instance which does shallow tests on the Explanation */
+    public ExplanationAsserter(Query q, String defaultFieldName, IndexSearcher s) {
+      this(q,defaultFieldName,s,false);
+    }      
+    public ExplanationAsserter(Query q, String defaultFieldName, IndexSearcher s, boolean deep) {
+      this.q=q;
+      this.s=s;
+      this.d = q.toString(defaultFieldName);
+      this.deep=deep;
+    }      
+    
+    @Override
+    public void setScorer(Scorer scorer) throws IOException {
+      this.scorer = scorer;     
+    }
+    
+    @Override
+    public void collect(int doc) throws IOException {
+      Explanation exp = null;
+      doc = doc + base;
+      try {
+        exp = s.explain(q, doc);
+      } catch (IOException e) {
+        throw new RuntimeException
+          ("exception in hitcollector of [["+d+"]] for #"+doc, e);
+      }
+      
+      Assert.assertNotNull("Explanation of [["+d+"]] for #"+doc+" is null", exp);
+      verifyExplanation(d,doc,scorer.score(),deep,exp);
+    }
+    @Override
+    public void setNextReader(AtomicReaderContext context) {
+      base = context.docBase;
+    }
+    @Override
+    public boolean acceptsDocsOutOfOrder() {
+      return true;
+    }
+  }
+
+}
+
+
diff --git a/lucene/src/test-framework/org/apache/lucene/search/QueryUtils.java b/lucene/src/test-framework/org/apache/lucene/search/QueryUtils.java
new file mode 100644
index 0000000..e84b2f9
--- /dev/null
+++ b/lucene/src/test-framework/org/apache/lucene/search/QueryUtils.java
@@ -0,0 +1,444 @@
+package org.apache.lucene.search;
+
+import java.io.ByteArrayInputStream;
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+import java.io.ObjectInputStream;
+import java.io.ObjectOutputStream;
+import java.util.Random;
+import java.lang.reflect.Method;
+
+import junit.framework.Assert;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.MultiReader;
+import org.apache.lucene.search.Weight.ScorerContext;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.MockDirectoryWrapper;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.ReaderUtil;
+
+import static org.apache.lucene.util.LuceneTestCase.TEST_VERSION_CURRENT;
+
+/**
+ * Copyright 2005 Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+
+public class QueryUtils {
+
+  /** Check the types of things query objects should be able to do. */
+  public static void check(Query q) {
+    checkHashEquals(q);
+  }
+
+  /** check very basic hashCode and equals */
+  public static void checkHashEquals(Query q) {
+    Query q2 = (Query)q.clone();
+    checkEqual(q,q2);
+
+    Query q3 = (Query)q.clone();
+    q3.setBoost(7.21792348f);
+    checkUnequal(q,q3);
+
+    // test that a class check is done so that no exception is thrown
+    // in the implementation of equals()
+    Query whacky = new Query() {
+      @Override
+      public String toString(String field) {
+        return "My Whacky Query";
+      }
+    };
+    whacky.setBoost(q.getBoost());
+    checkUnequal(q, whacky);
+    
+    // null test
+    Assert.assertFalse(q.equals(null));
+  }
+
+  public static void checkEqual(Query q1, Query q2) {
+    Assert.assertEquals(q1, q2);
+    Assert.assertEquals(q1.hashCode(), q2.hashCode());
+  }
+
+  public static void checkUnequal(Query q1, Query q2) {
+    Assert.assertTrue(!q1.equals(q2));
+    Assert.assertTrue(!q2.equals(q1));
+
+    // possible this test can fail on a hash collision... if that
+    // happens, please change test to use a different example.
+    Assert.assertTrue(q1.hashCode() != q2.hashCode());
+  }
+  
+  /** deep check that explanations of a query 'score' correctly */
+  public static void checkExplanations (final Query q, final IndexSearcher s) throws IOException {
+    CheckHits.checkExplanations(q, null, s, true);
+  }
+  
+  /** 
+   * Various query sanity checks on a searcher, some checks are only done for
+   * instanceof IndexSearcher.
+   *
+   * @see #check(Query)
+   * @see #checkFirstSkipTo
+   * @see #checkSkipTo
+   * @see #checkExplanations
+   * @see #checkSerialization
+   * @see #checkEqual
+   */
+  public static void check(Random random, Query q1, IndexSearcher s) {
+    check(random, q1, s, true);
+  }
+  private static void check(Random random, Query q1, IndexSearcher s, boolean wrap) {
+    try {
+      check(q1);
+      if (s!=null) {
+        checkFirstSkipTo(q1,s);
+        checkSkipTo(q1,s);
+        if (wrap) {
+          IndexSearcher wrapped;
+          check(random, q1, wrapped = wrapUnderlyingReader(random, s, -1), false);
+          wrapped.close();
+          check(random, q1, wrapped = wrapUnderlyingReader(random, s,  0), false);
+          wrapped.close();
+          check(random, q1, wrapped = wrapUnderlyingReader(random, s, +1), false);
+          wrapped.close();
+        }
+        checkExplanations(q1,s);
+        checkSerialization(q1,s);
+        
+        Query q2 = (Query)q1.clone();
+        checkEqual(s.rewrite(q1),
+                   s.rewrite(q2));
+      }
+    } catch (IOException e) {
+      throw new RuntimeException(e);
+    }
+  }
+
+  /**
+   * Given an IndexSearcher, returns a new IndexSearcher whose IndexReader 
+   * is a MultiReader containing the Reader of the original IndexSearcher, 
+   * as well as several "empty" IndexReaders -- some of which will have 
+   * deleted documents in them.  This new IndexSearcher should 
+   * behave exactly the same as the original IndexSearcher.
+   * @param s the searcher to wrap
+   * @param edge if negative, s will be the first sub; if 0, s will be in the middle, if positive s will be the last sub
+   */
+  public static IndexSearcher wrapUnderlyingReader(Random random, final IndexSearcher s, final int edge) 
+    throws IOException {
+
+    IndexReader r = s.getIndexReader();
+
+    // we can't put deleted docs before the nested reader, because
+    // it will throw off the docIds
+    IndexReader[] readers = new IndexReader[] {
+      edge < 0 ? r : IndexReader.open(makeEmptyIndex(random, 0), true),
+      IndexReader.open(makeEmptyIndex(random, 0), true),
+      new MultiReader(IndexReader.open(makeEmptyIndex(random, edge < 0 ? 4 : 0), true),
+          IndexReader.open(makeEmptyIndex(random, 0), true),
+          0 == edge ? r : IndexReader.open(makeEmptyIndex(random, 0), true)),
+      IndexReader.open(makeEmptyIndex(random, 0 < edge ? 0 : 7), true),
+      IndexReader.open(makeEmptyIndex(random, 0), true),
+      new MultiReader(IndexReader.open(makeEmptyIndex(random, 0 < edge ? 0 : 5), true),
+          IndexReader.open(makeEmptyIndex(random, 0), true),
+          0 < edge ? r : IndexReader.open(makeEmptyIndex(random, 0), true))
+    };
+    IndexSearcher out = LuceneTestCase.newSearcher(new MultiReader(readers));
+    out.setSimilarityProvider(s.getSimilarityProvider());
+    return out;
+  }
+
+  private static Directory makeEmptyIndex(Random random, final int numDeletedDocs) 
+    throws IOException {
+    Directory d = new MockDirectoryWrapper(random, new RAMDirectory());
+      IndexWriter w = new IndexWriter(d, new IndexWriterConfig(
+        TEST_VERSION_CURRENT, new MockAnalyzer()));
+      for (int i = 0; i < numDeletedDocs; i++) {
+        w.addDocument(new Document());
+      }
+      w.commit();
+      w.deleteDocuments( new MatchAllDocsQuery() );
+      try {
+        // Carefully invoke what is a package-private (test
+        // only, internal) method on IndexWriter:
+        Method m = IndexWriter.class.getDeclaredMethod("keepFullyDeletedSegments");
+        m.setAccessible(true);
+        m.invoke(w);
+      } catch (Exception e) {
+        // Should not happen?
+        throw new RuntimeException(e);
+      }
+      w.commit();
+
+      if (0 < numDeletedDocs)
+        Assert.assertTrue("writer has no deletions", w.hasDeletions());
+
+      Assert.assertEquals("writer is missing some deleted docs", 
+                          numDeletedDocs, w.maxDoc());
+      Assert.assertEquals("writer has non-deleted docs", 
+                          0, w.numDocs());
+      w.close();
+      IndexReader r = IndexReader.open(d, true);
+      Assert.assertEquals("reader has wrong number of deleted docs", 
+                          numDeletedDocs, r.numDeletedDocs());
+      r.close();
+      return d;
+  }
+  
+
+  /** check that the query weight is serializable. 
+   * @throws IOException if serialization check fail. 
+   */
+  private static void checkSerialization(Query q, IndexSearcher s) throws IOException {
+    Weight w = q.weight(s);
+    try {
+      ByteArrayOutputStream bos = new ByteArrayOutputStream();
+      ObjectOutputStream oos = new ObjectOutputStream(bos);
+      oos.writeObject(w);
+      oos.close();
+      ObjectInputStream ois = new ObjectInputStream(new ByteArrayInputStream(bos.toByteArray()));
+      ois.readObject();
+      ois.close();
+      
+      //skip equals() test for now - most weights don't override equals() and we won't add this just for the tests.
+      //TestCase.assertEquals("writeObject(w) != w.  ("+w+")",w2,w);   
+      
+    } catch (Exception e) {
+      IOException e2 = new IOException("Serialization failed for "+w);
+      e2.initCause(e);
+      throw e2;
+    }
+  }
+
+  /** alternate scorer skipTo(),skipTo(),next(),next(),skipTo(),skipTo(), etc
+   * and ensure a hitcollector receives same docs and scores
+   */
+  public static void checkSkipTo(final Query q, final IndexSearcher s) throws IOException {
+    //System.out.println("Checking "+q);
+    final AtomicReaderContext[] readerContextArray = ReaderUtil.leaves(s.getTopReaderContext());
+    if (q.weight(s).scoresDocsOutOfOrder()) return;  // in this case order of skipTo() might differ from that of next().
+
+    final int skip_op = 0;
+    final int next_op = 1;
+    final int orders [][] = {
+        {next_op},
+        {skip_op},
+        {skip_op, next_op},
+        {next_op, skip_op},
+        {skip_op, skip_op, next_op, next_op},
+        {next_op, next_op, skip_op, skip_op},
+        {skip_op, skip_op, skip_op, next_op, next_op},
+    };
+    for (int k = 0; k < orders.length; k++) {
+
+        final int order[] = orders[k];
+        // System.out.print("Order:");for (int i = 0; i < order.length; i++)
+        // System.out.print(order[i]==skip_op ? " skip()":" next()");
+        // System.out.println();
+        final int opidx[] = { 0 };
+        final int lastDoc[] = {-1};
+
+        // FUTURE: ensure scorer.doc()==-1
+
+        final float maxDiff = 1e-5f;
+        final IndexReader lastReader[] = {null};
+
+        s.search(q, new Collector() {
+          private Scorer sc;
+          private Scorer scorer;
+          private int leafPtr;
+
+          @Override
+          public void setScorer(Scorer scorer) throws IOException {
+            this.sc = scorer;
+          }
+
+          @Override
+          public void collect(int doc) throws IOException {
+            float score = sc.score();
+            lastDoc[0] = doc;
+            try {
+              if (scorer == null) {
+                Weight w = q.weight(s);
+                scorer = w.scorer(readerContextArray[leafPtr], ScorerContext.def());
+              }
+              
+              int op = order[(opidx[0]++) % order.length];
+              // System.out.println(op==skip_op ?
+              // "skip("+(sdoc[0]+1)+")":"next()");
+              boolean more = op == skip_op ? scorer.advance(scorer.docID() + 1) != DocIdSetIterator.NO_MORE_DOCS
+                  : scorer.nextDoc() != DocIdSetIterator.NO_MORE_DOCS;
+              int scorerDoc = scorer.docID();
+              float scorerScore = scorer.score();
+              float scorerScore2 = scorer.score();
+              float scoreDiff = Math.abs(score - scorerScore);
+              float scorerDiff = Math.abs(scorerScore2 - scorerScore);
+              if (!more || doc != scorerDoc || scoreDiff > maxDiff
+                  || scorerDiff > maxDiff) {
+                StringBuilder sbord = new StringBuilder();
+                for (int i = 0; i < order.length; i++)
+                  sbord.append(order[i] == skip_op ? " skip()" : " next()");
+                throw new RuntimeException("ERROR matching docs:" + "\n\t"
+                    + (doc != scorerDoc ? "--> " : "") + "doc=" + doc + ", scorerDoc=" + scorerDoc
+                    + "\n\t" + (!more ? "--> " : "") + "tscorer.more=" + more
+                    + "\n\t" + (scoreDiff > maxDiff ? "--> " : "")
+                    + "scorerScore=" + scorerScore + " scoreDiff=" + scoreDiff
+                    + " maxDiff=" + maxDiff + "\n\t"
+                    + (scorerDiff > maxDiff ? "--> " : "") + "scorerScore2="
+                    + scorerScore2 + " scorerDiff=" + scorerDiff
+                    + "\n\thitCollector.doc=" + doc + " score=" + score
+                    + "\n\t Scorer=" + scorer + "\n\t Query=" + q + "  "
+                    + q.getClass().getName() + "\n\t Searcher=" + s
+                    + "\n\t Order=" + sbord + "\n\t Op="
+                    + (op == skip_op ? " skip()" : " next()"));
+              }
+            } catch (IOException e) {
+              throw new RuntimeException(e);
+            }
+          }
+
+          @Override
+          public void setNextReader(AtomicReaderContext context) throws IOException {
+            // confirm that skipping beyond the last doc, on the
+            // previous reader, hits NO_MORE_DOCS
+            if (lastReader[0] != null) {
+              final IndexReader previousReader = lastReader[0];
+              IndexSearcher indexSearcher = LuceneTestCase.newSearcher(previousReader);
+              Weight w = q.weight(indexSearcher);
+              Scorer scorer = w.scorer((AtomicReaderContext)indexSearcher.getTopReaderContext(), ScorerContext.def());
+              if (scorer != null) {
+                boolean more = scorer.advance(lastDoc[0] + 1) != DocIdSetIterator.NO_MORE_DOCS;
+                Assert.assertFalse("query's last doc was "+ lastDoc[0] +" but skipTo("+(lastDoc[0]+1)+") got to "+scorer.docID(),more);
+              }
+              leafPtr++;
+              indexSearcher.close();
+            }
+            lastReader[0] = context.reader;
+            assert readerContextArray[leafPtr].reader == context.reader;
+            this.scorer = null;
+            lastDoc[0] = -1;
+          }
+
+          @Override
+          public boolean acceptsDocsOutOfOrder() {
+            return true;
+          }
+        });
+
+        if (lastReader[0] != null) {
+          // confirm that skipping beyond the last doc, on the
+          // previous reader, hits NO_MORE_DOCS
+          final IndexReader previousReader = lastReader[0];
+          IndexSearcher indexSearcher = LuceneTestCase.newSearcher(previousReader);
+          Weight w = q.weight(indexSearcher);
+          Scorer scorer = w.scorer((AtomicReaderContext)previousReader.getTopReaderContext(), ScorerContext.def());
+          if (scorer != null) {
+            boolean more = scorer.advance(lastDoc[0] + 1) != DocIdSetIterator.NO_MORE_DOCS;
+            Assert.assertFalse("query's last doc was "+ lastDoc[0] +" but skipTo("+(lastDoc[0]+1)+") got to "+scorer.docID(),more);
+          }
+          indexSearcher.close();
+        }
+      }
+  }
+    
+  // check that first skip on just created scorers always goes to the right doc
+  private static void checkFirstSkipTo(final Query q, final IndexSearcher s) throws IOException {
+    //System.out.println("checkFirstSkipTo: "+q);
+    final float maxDiff = 1e-3f;
+    final int lastDoc[] = {-1};
+    final IndexReader lastReader[] = {null};
+    final AtomicReaderContext[] context = ReaderUtil.leaves(s.getTopReaderContext());
+    s.search(q,new Collector() {
+      private Scorer scorer;
+      private int leafPtr;
+      @Override
+      public void setScorer(Scorer scorer) throws IOException {
+        this.scorer = scorer;
+      }
+      @Override
+      public void collect(int doc) throws IOException {
+        float score = scorer.score();
+        try {
+          long startMS = System.currentTimeMillis();
+          for (int i=lastDoc[0]+1; i<=doc; i++) {
+            Weight w = q.weight(s);
+            Scorer scorer = w.scorer(context[leafPtr], ScorerContext.def());
+            Assert.assertTrue("query collected "+doc+" but skipTo("+i+") says no more docs!",scorer.advance(i) != DocIdSetIterator.NO_MORE_DOCS);
+            Assert.assertEquals("query collected "+doc+" but skipTo("+i+") got to "+scorer.docID(),doc,scorer.docID());
+            float skipToScore = scorer.score();
+            Assert.assertEquals("unstable skipTo("+i+") score!",skipToScore,scorer.score(),maxDiff); 
+            Assert.assertEquals("query assigned doc "+doc+" a score of <"+score+"> but skipTo("+i+") has <"+skipToScore+">!",score,skipToScore,maxDiff);
+            
+            // Hurry things along if they are going slow (eg
+            // if you got SimpleText codec this will kick in):
+            if (i < doc && System.currentTimeMillis() - startMS > 5) {
+              i = doc-1;
+            }
+          }
+          lastDoc[0] = doc;
+        } catch (IOException e) {
+          throw new RuntimeException(e);
+        }
+      }
+
+      @Override
+      public void setNextReader(AtomicReaderContext context) throws IOException {
+        // confirm that skipping beyond the last doc, on the
+        // previous reader, hits NO_MORE_DOCS
+        if (lastReader[0] != null) {
+          final IndexReader previousReader = lastReader[0];
+          IndexSearcher indexSearcher = LuceneTestCase.newSearcher(previousReader);
+          Weight w = q.weight(indexSearcher);
+          Scorer scorer = w.scorer((AtomicReaderContext)indexSearcher.getTopReaderContext(), ScorerContext.def());
+          if (scorer != null) {
+            boolean more = scorer.advance(lastDoc[0] + 1) != DocIdSetIterator.NO_MORE_DOCS;
+            Assert.assertFalse("query's last doc was "+ lastDoc[0] +" but skipTo("+(lastDoc[0]+1)+") got to "+scorer.docID(),more);
+          }
+          indexSearcher.close();
+          leafPtr++;
+        }
+
+        lastReader[0] = context.reader;
+        lastDoc[0] = -1;
+      }
+      @Override
+      public boolean acceptsDocsOutOfOrder() {
+        return false;
+      }
+    });
+
+    if (lastReader[0] != null) {
+      // confirm that skipping beyond the last doc, on the
+      // previous reader, hits NO_MORE_DOCS
+      final IndexReader previousReader = lastReader[0];
+      IndexSearcher indexSearcher = LuceneTestCase.newSearcher(previousReader);
+      Weight w = q.weight(indexSearcher);
+      Scorer scorer = w.scorer((AtomicReaderContext)indexSearcher.getTopReaderContext(), ScorerContext.def());
+      if (scorer != null) {
+        boolean more = scorer.advance(lastDoc[0] + 1) != DocIdSetIterator.NO_MORE_DOCS;
+        Assert.assertFalse("query's last doc was "+ lastDoc[0] +" but skipTo("+(lastDoc[0]+1)+") got to "+scorer.docID(),more);
+      }
+      indexSearcher.close();
+    }
+  }
+}
diff --git a/lucene/src/test-framework/org/apache/lucene/store/MockDirectoryWrapper.java b/lucene/src/test-framework/org/apache/lucene/store/MockDirectoryWrapper.java
new file mode 100644
index 0000000..bb9552b
--- /dev/null
+++ b/lucene/src/test-framework/org/apache/lucene/store/MockDirectoryWrapper.java
@@ -0,0 +1,550 @@
+package org.apache.lucene.store;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable;
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.IdentityHashMap;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.Random;
+import java.util.Set;
+
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
+
+/**
+ * This is a Directory Wrapper that adds methods
+ * intended to be used only by unit tests.
+ */
+
+public class MockDirectoryWrapper extends Directory {
+  final Directory delegate;
+  long maxSize;
+
+  // Max actual bytes used. This is set by MockRAMOutputStream:
+  long maxUsedSize;
+  double randomIOExceptionRate;
+  Random randomState;
+  boolean noDeleteOpenFile = true;
+  boolean preventDoubleWrite = true;
+  boolean checkIndexOnClose = true;
+  boolean trackDiskUsage = false;
+  private Set<String> unSyncedFiles;
+  private Set<String> createdFiles;
+  Set<String> openFilesForWrite = new HashSet<String>();
+  volatile boolean crashed;
+
+  // use this for tracking files for crash.
+  // additionally: provides debugging information in case you leave one open
+  Map<Closeable,Exception> openFileHandles = Collections.synchronizedMap(new IdentityHashMap<Closeable,Exception>());
+
+  // NOTE: we cannot initialize the Map here due to the
+  // order in which our constructor actually does this
+  // member initialization vs when it calls super.  It seems
+  // like super is called, then our members are initialized:
+  Map<String,Integer> openFiles;
+
+  // Only tracked if noDeleteOpenFile is true: if an attempt
+  // is made to delete an open file, we enroll it here.
+  Set<String> openFilesDeleted;
+
+  private synchronized void init() {
+    if (openFiles == null) {
+      openFiles = new HashMap<String,Integer>();
+      openFilesDeleted = new HashSet<String>();
+    }
+
+    if (createdFiles == null)
+      createdFiles = new HashSet<String>();
+    if (unSyncedFiles == null)
+      unSyncedFiles = new HashSet<String>();
+  }
+
+  public MockDirectoryWrapper(Random random, Directory delegate) {
+    this.delegate = delegate;
+    // must make a private random since our methods are
+    // called from different threads; else test failures may
+    // not be reproducible from the original seed
+    this.randomState = new Random(random.nextInt());
+    init();
+  }
+
+  public void setTrackDiskUsage(boolean v) {
+    trackDiskUsage = v;
+  }
+
+  /** If set to true, we throw an IOException if the same
+   *  file is opened by createOutput, ever. */
+  public void setPreventDoubleWrite(boolean value) {
+    preventDoubleWrite = value;
+  }
+
+  @Override
+  public synchronized void sync(Collection<String> names) throws IOException {
+    maybeYield();
+    for (String name : names)
+      maybeThrowDeterministicException();
+    if (crashed)
+      throw new IOException("cannot sync after crash");
+    unSyncedFiles.removeAll(names);
+    delegate.sync(names);
+  }
+  
+  @Override
+  public String toString() {
+    maybeYield();
+    return "MockDirWrapper(" + delegate + ")";
+  }
+
+  public synchronized final long sizeInBytes() throws IOException {
+    if (delegate instanceof RAMDirectory)
+      return ((RAMDirectory) delegate).sizeInBytes();
+    else {
+      // hack
+      long size = 0;
+      for (String file : delegate.listAll())
+        size += delegate.fileLength(file);
+      return size;
+    }
+  }
+
+  /** Simulates a crash of OS or machine by overwriting
+   *  unsynced files. */
+  public synchronized void crash() throws IOException {
+    crashed = true;
+    openFiles = new HashMap<String,Integer>();
+    openFilesForWrite = new HashSet<String>();
+    openFilesDeleted = new HashSet<String>();
+    Iterator<String> it = unSyncedFiles.iterator();
+    unSyncedFiles = new HashSet<String>();
+    // first force-close all files, so we can corrupt on windows etc.
+    // clone the file map, as these guys want to remove themselves on close.
+    Map<Closeable,Exception> m = new IdentityHashMap<Closeable,Exception>(openFileHandles);
+    for (Closeable f : m.keySet())
+      try {
+        f.close();
+      } catch (Exception ignored) {}
+    
+    int count = 0;
+    while(it.hasNext()) {
+      String name = it.next();
+      if (count % 3 == 0) {
+        deleteFile(name, true);
+      } else if (count % 3 == 1) {
+        // Zero out file entirely
+        long length = fileLength(name);
+        byte[] zeroes = new byte[256];
+        long upto = 0;
+        IndexOutput out = delegate.createOutput(name);
+        while(upto < length) {
+          final int limit = (int) Math.min(length-upto, zeroes.length);
+          out.writeBytes(zeroes, 0, limit);
+          upto += limit;
+        }
+        out.close();
+      } else if (count % 3 == 2) {
+        // Truncate the file:
+        IndexOutput out = delegate.createOutput(name);
+        out.setLength(fileLength(name)/2);
+        out.close();
+      }
+      count++;
+    }
+  }
+
+  public synchronized void clearCrash() throws IOException {
+    crashed = false;
+  }
+
+  public void setMaxSizeInBytes(long maxSize) {
+    this.maxSize = maxSize;
+  }
+  public long getMaxSizeInBytes() {
+    return this.maxSize;
+  }
+
+  /**
+   * Returns the peek actual storage used (bytes) in this
+   * directory.
+   */
+  public long getMaxUsedSizeInBytes() {
+    return this.maxUsedSize;
+  }
+  public void resetMaxUsedSizeInBytes() throws IOException {
+    this.maxUsedSize = getRecomputedActualSizeInBytes();
+  }
+
+  /**
+   * Emulate windows whereby deleting an open file is not
+   * allowed (raise IOException).
+  */
+  public void setNoDeleteOpenFile(boolean value) {
+    this.noDeleteOpenFile = value;
+  }
+  public boolean getNoDeleteOpenFile() {
+    return noDeleteOpenFile;
+  }
+
+  /**
+   * Set whether or not checkindex should be run
+   * on close
+   */
+  public void setCheckIndexOnClose(boolean value) {
+    this.checkIndexOnClose = value;
+  }
+  
+  public boolean getCheckIndexOnClose() {
+    return checkIndexOnClose;
+  }
+  /**
+   * If 0.0, no exceptions will be thrown.  Else this should
+   * be a double 0.0 - 1.0.  We will randomly throw an
+   * IOException on the first write to an OutputStream based
+   * on this probability.
+   */
+  public void setRandomIOExceptionRate(double rate) {
+    randomIOExceptionRate = rate;
+  }
+  public double getRandomIOExceptionRate() {
+    return randomIOExceptionRate;
+  }
+
+  void maybeThrowIOException() throws IOException {
+    if (randomIOExceptionRate > 0.0) {
+      int number = Math.abs(randomState.nextInt() % 1000);
+      if (number < randomIOExceptionRate*1000) {
+        if (LuceneTestCase.VERBOSE) {
+          System.out.println(Thread.currentThread().getName() + ": MockDirectoryWrapper: now throw random exception");
+          new Throwable().printStackTrace(System.out);
+        }
+        throw new IOException("a random IOException");
+      }
+    }
+  }
+
+  @Override
+  public synchronized void deleteFile(String name) throws IOException {
+    maybeYield();
+    deleteFile(name, false);
+  }
+
+  // sets the cause of the incoming ioe to be the stack
+  // trace when the offending file name was opened
+  private synchronized IOException fillOpenTrace(IOException ioe, String name, boolean input) {
+    for(Map.Entry<Closeable,Exception> ent : openFileHandles.entrySet()) {
+      if (input && ent.getKey() instanceof MockIndexInputWrapper && ((MockIndexInputWrapper) ent.getKey()).name.equals(name)) {
+        ioe.initCause(ent.getValue());
+        break;
+      } else if (!input && ent.getKey() instanceof MockIndexOutputWrapper && ((MockIndexOutputWrapper) ent.getKey()).name.equals(name)) {
+        ioe.initCause(ent.getValue());
+        break;
+      }
+    }
+    return ioe;
+  }
+
+  private void maybeYield() {
+    if (randomState.nextBoolean()) {
+      Thread.yield();
+    }
+  }
+
+  private synchronized void deleteFile(String name, boolean forced) throws IOException {
+    maybeYield();
+
+    maybeThrowDeterministicException();
+
+    if (crashed && !forced)
+      throw new IOException("cannot delete after crash");
+
+    if (unSyncedFiles.contains(name))
+      unSyncedFiles.remove(name);
+    if (!forced && noDeleteOpenFile) {
+      if (openFiles.containsKey(name)) {
+        openFilesDeleted.add(name);
+        throw fillOpenTrace(new IOException("MockDirectoryWrapper: file \"" + name + "\" is still open: cannot delete"), name, true);
+      } else {
+        openFilesDeleted.remove(name);
+      }
+    }
+    delegate.deleteFile(name);
+  }
+
+  public synchronized Set<String> getOpenDeletedFiles() {
+    return new HashSet<String>(openFilesDeleted);
+  }
+
+  @Override
+  public synchronized IndexOutput createOutput(String name) throws IOException {
+    maybeYield();
+    if (crashed)
+      throw new IOException("cannot createOutput after crash");
+    init();
+    synchronized(this) {
+      if (preventDoubleWrite && createdFiles.contains(name) && !name.equals("segments.gen"))
+        throw new IOException("file \"" + name + "\" was already written to");
+    }
+    if (noDeleteOpenFile && openFiles.containsKey(name))
+      throw new IOException("MockDirectoryWrapper: file \"" + name + "\" is still open: cannot overwrite");
+    
+    if (crashed)
+      throw new IOException("cannot createOutput after crash");
+    unSyncedFiles.add(name);
+    createdFiles.add(name);
+    
+    if (delegate instanceof RAMDirectory) {
+      RAMDirectory ramdir = (RAMDirectory) delegate;
+      RAMFile file = new RAMFile(ramdir);
+      RAMFile existing = ramdir.fileMap.get(name);
+    
+      // Enforce write once:
+      if (existing!=null && !name.equals("segments.gen") && preventDoubleWrite)
+        throw new IOException("file " + name + " already exists");
+      else {
+        if (existing!=null) {
+          ramdir.sizeInBytes.getAndAdd(-existing.sizeInBytes);
+          existing.directory = null;
+        }
+        ramdir.fileMap.put(name, file);
+      }
+    }
+    //System.out.println(Thread.currentThread().getName() + ": MDW: create " + name);
+    IndexOutput io = new MockIndexOutputWrapper(this, delegate.createOutput(name), name);
+    openFileHandles.put(io, new RuntimeException("unclosed IndexOutput"));
+    openFilesForWrite.add(name);
+    return io;
+  }
+
+  @Override
+  public synchronized IndexInput openInput(String name) throws IOException {
+    maybeYield();
+    if (!delegate.fileExists(name))
+      throw new FileNotFoundException(name);
+
+    // cannot open a file for input if it's still open for
+    // output, except for segments.gen and segments_N
+    if (openFilesForWrite.contains(name) && !name.startsWith("segments")) {
+      throw fillOpenTrace(new IOException("MockDirectoryWrapper: file \"" + name + "\" is still open for writing"), name, false);
+    }
+
+    if (openFiles.containsKey(name)) {
+      Integer v =  openFiles.get(name);
+      v = Integer.valueOf(v.intValue()+1);
+      openFiles.put(name, v);
+    } else {
+      openFiles.put(name, Integer.valueOf(1));
+    }
+
+    IndexInput ii = new MockIndexInputWrapper(this, name, delegate.openInput(name));
+    openFileHandles.put(ii, new RuntimeException("unclosed IndexInput"));
+    return ii;
+  }
+
+  /** Provided for testing purposes.  Use sizeInBytes() instead. */
+  public synchronized final long getRecomputedSizeInBytes() throws IOException {
+    if (!(delegate instanceof RAMDirectory))
+      return sizeInBytes();
+    long size = 0;
+    for(final RAMFile file: ((RAMDirectory)delegate).fileMap.values()) {
+      size += file.getSizeInBytes();
+    }
+    return size;
+  }
+
+  /** Like getRecomputedSizeInBytes(), but, uses actual file
+   * lengths rather than buffer allocations (which are
+   * quantized up to nearest
+   * RAMOutputStream.BUFFER_SIZE (now 1024) bytes.
+   */
+
+  public final synchronized long getRecomputedActualSizeInBytes() throws IOException {
+    if (!(delegate instanceof RAMDirectory))
+      return sizeInBytes();
+    long size = 0;
+    for (final RAMFile file : ((RAMDirectory)delegate).fileMap.values())
+      size += file.length;
+    return size;
+  }
+
+  @Override
+  public synchronized void close() throws IOException {
+    maybeYield();
+    if (openFiles == null) {
+      openFiles = new HashMap<String,Integer>();
+      openFilesDeleted = new HashSet<String>();
+    }
+    if (noDeleteOpenFile && openFiles.size() > 0) {
+      // print the first one as its very verbose otherwise
+      Exception cause = null;
+      Iterator<Exception> stacktraces = openFileHandles.values().iterator();
+      if (stacktraces.hasNext())
+        cause = stacktraces.next();
+      // RuntimeException instead of IOException because
+      // super() does not throw IOException currently:
+      throw new RuntimeException("MockDirectoryWrapper: cannot close: there are still open files: " + openFiles, cause);
+    }
+    open = false;
+    if (checkIndexOnClose && IndexReader.indexExists(this)) {
+      _TestUtil.checkIndex(this);
+    }
+    delegate.close();
+  }
+
+  boolean open = true;
+  
+  public synchronized boolean isOpen() {
+    return open;
+  }
+  
+  /**
+   * Objects that represent fail-able conditions. Objects of a derived
+   * class are created and registered with the mock directory. After
+   * register, each object will be invoked once for each first write
+   * of a file, giving the object a chance to throw an IOException.
+   */
+  public static class Failure {
+    /**
+     * eval is called on the first write of every new file.
+     */
+    public void eval(MockDirectoryWrapper dir) throws IOException { }
+
+    /**
+     * reset should set the state of the failure to its default
+     * (freshly constructed) state. Reset is convenient for tests
+     * that want to create one failure object and then reuse it in
+     * multiple cases. This, combined with the fact that Failure
+     * subclasses are often anonymous classes makes reset difficult to
+     * do otherwise.
+     *
+     * A typical example of use is
+     * Failure failure = new Failure() { ... };
+     * ...
+     * mock.failOn(failure.reset())
+     */
+    public Failure reset() { return this; }
+
+    protected boolean doFail;
+
+    public void setDoFail() {
+      doFail = true;
+    }
+
+    public void clearDoFail() {
+      doFail = false;
+    }
+  }
+
+  ArrayList<Failure> failures;
+
+  /**
+   * add a Failure object to the list of objects to be evaluated
+   * at every potential failure point
+   */
+  synchronized public void failOn(Failure fail) {
+    if (failures == null) {
+      failures = new ArrayList<Failure>();
+    }
+    failures.add(fail);
+  }
+
+  /**
+   * Iterate through the failures list, giving each object a
+   * chance to throw an IOE
+   */
+  synchronized void maybeThrowDeterministicException() throws IOException {
+    if (failures != null) {
+      for(int i = 0; i < failures.size(); i++) {
+        failures.get(i).eval(this);
+      }
+    }
+  }
+
+  @Override
+  public synchronized String[] listAll() throws IOException {
+    maybeYield();
+    return delegate.listAll();
+  }
+
+  @Override
+  public synchronized boolean fileExists(String name) throws IOException {
+    maybeYield();
+    return delegate.fileExists(name);
+  }
+
+  @Override
+  public synchronized long fileModified(String name) throws IOException {
+    maybeYield();
+    return delegate.fileModified(name);
+  }
+
+  @Override
+  public synchronized void touchFile(String name) throws IOException {
+    maybeYield();
+    delegate.touchFile(name);
+  }
+
+  @Override
+  public synchronized long fileLength(String name) throws IOException {
+    maybeYield();
+    return delegate.fileLength(name);
+  }
+
+  @Override
+  public synchronized Lock makeLock(String name) {
+    maybeYield();
+    return delegate.makeLock(name);
+  }
+
+  @Override
+  public synchronized void clearLock(String name) throws IOException {
+    maybeYield();
+    delegate.clearLock(name);
+  }
+
+  @Override
+  public synchronized void setLockFactory(LockFactory lockFactory) throws IOException {
+    maybeYield();
+    delegate.setLockFactory(lockFactory);
+  }
+
+  @Override
+  public synchronized LockFactory getLockFactory() {
+    maybeYield();
+    return delegate.getLockFactory();
+  }
+
+  @Override
+  public synchronized String getLockID() {
+    maybeYield();
+    return delegate.getLockID();
+  }
+
+  @Override
+  public synchronized void copy(Directory to, String src, String dest) throws IOException {
+    maybeYield();
+    delegate.copy(to, src, dest);
+  }
+}
diff --git a/lucene/src/test-framework/org/apache/lucene/store/MockIndexInputWrapper.java b/lucene/src/test-framework/org/apache/lucene/store/MockIndexInputWrapper.java
new file mode 100644
index 0000000..5e14a36
--- /dev/null
+++ b/lucene/src/test-framework/org/apache/lucene/store/MockIndexInputWrapper.java
@@ -0,0 +1,148 @@
+package org.apache.lucene.store;
+
+import java.io.IOException;
+import java.util.Map;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Used by MockDirectoryWrapper to create an input stream that
+ * keeps track of when it's been closed.
+ */
+
+public class MockIndexInputWrapper extends IndexInput {
+  private MockDirectoryWrapper dir;
+  final String name;
+  private IndexInput delegate;
+  private boolean isClone;
+
+  /** Construct an empty output buffer. 
+   * @throws IOException */
+  public MockIndexInputWrapper(MockDirectoryWrapper dir, String name, IndexInput delegate) {
+    this.name = name;
+    this.dir = dir;
+    this.delegate = delegate;
+  }
+
+  @Override
+  public void close() throws IOException {
+    delegate.close();
+    // Pending resolution on LUCENE-686 we may want to
+    // remove the conditional check so we also track that
+    // all clones get closed:
+    if (!isClone) {
+      synchronized(dir) {
+        Integer v = dir.openFiles.get(name);
+        // Could be null when MockRAMDirectory.crash() was called
+        if (v != null) {
+          if (v.intValue() == 1) {
+            dir.openFiles.remove(name);
+            dir.openFilesDeleted.remove(name);
+          } else {
+            v = Integer.valueOf(v.intValue()-1);
+            dir.openFiles.put(name, v);
+          }
+        }
+        dir.openFileHandles.remove(this);
+      }
+    }
+  }
+
+  @Override
+  public Object clone() {
+    IndexInput iiclone = (IndexInput) delegate.clone();
+    MockIndexInputWrapper clone = new MockIndexInputWrapper(dir, name, iiclone);
+    clone.isClone = true;
+    // Pending resolution on LUCENE-686 we may want to
+    // uncomment this code so that we also track that all
+    // clones get closed:
+    /*
+    synchronized(dir.openFiles) {
+      if (dir.openFiles.containsKey(name)) {
+        Integer v = (Integer) dir.openFiles.get(name);
+        v = Integer.valueOf(v.intValue()+1);
+        dir.openFiles.put(name, v);
+      } else {
+        throw new RuntimeException("BUG: cloned file was not open?");
+      }
+    }
+    */
+    return clone;
+  }
+
+  @Override
+  public long getFilePointer() {
+    return delegate.getFilePointer();
+  }
+
+  @Override
+  public void seek(long pos) throws IOException {
+    delegate.seek(pos);
+  }
+
+  @Override
+  public long length() {
+    return delegate.length();
+  }
+
+  @Override
+  public byte readByte() throws IOException {
+    return delegate.readByte();
+  }
+
+  @Override
+  public void readBytes(byte[] b, int offset, int len) throws IOException {
+    delegate.readBytes(b, offset, len);
+  }
+
+  @Override
+  public void copyBytes(IndexOutput out, long numBytes) throws IOException {
+    delegate.copyBytes(out, numBytes);
+  }
+
+  @Override
+  public void readBytes(byte[] b, int offset, int len, boolean useBuffer)
+      throws IOException {
+    delegate.readBytes(b, offset, len, useBuffer);
+  }
+
+  @Override
+  public short readShort() throws IOException {
+    return delegate.readShort();
+  }
+
+  @Override
+  public int readInt() throws IOException {
+    return delegate.readInt();
+  }
+
+  @Override
+  public long readLong() throws IOException {
+    return delegate.readLong();
+  }
+
+  @Override
+  public String readString() throws IOException {
+    return delegate.readString();
+  }
+
+  @Override
+  public Map<String,String> readStringStringMap() throws IOException {
+    return delegate.readStringStringMap();
+  }
+}
diff --git a/lucene/src/test-framework/org/apache/lucene/store/MockIndexOutputWrapper.java b/lucene/src/test-framework/org/apache/lucene/store/MockIndexOutputWrapper.java
new file mode 100644
index 0000000..7e6e17d
--- /dev/null
+++ b/lucene/src/test-framework/org/apache/lucene/store/MockIndexOutputWrapper.java
@@ -0,0 +1,159 @@
+package org.apache.lucene.store;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.util.LuceneTestCase;
+
+/**
+ * Used by MockRAMDirectory to create an output stream that
+ * will throw an IOException on fake disk full, track max
+ * disk space actually used, and maybe throw random
+ * IOExceptions.
+ */
+
+public class MockIndexOutputWrapper extends IndexOutput {
+  private MockDirectoryWrapper dir;
+  private final IndexOutput delegate;
+  private boolean first=true;
+  final String name;
+  
+  byte[] singleByte = new byte[1];
+
+  /** Construct an empty output buffer. */
+  public MockIndexOutputWrapper(MockDirectoryWrapper dir, IndexOutput delegate, String name) {
+    this.dir = dir;
+    this.name = name;
+    this.delegate = delegate;
+  }
+
+  @Override
+  public void close() throws IOException {
+    dir.maybeThrowDeterministicException();
+    delegate.close();
+    if (dir.trackDiskUsage) {
+      // Now compute actual disk usage & track the maxUsedSize
+      // in the MockDirectoryWrapper:
+      long size = dir.getRecomputedActualSizeInBytes();
+      if (size > dir.maxUsedSize) {
+        dir.maxUsedSize = size;
+      }
+    }
+    synchronized(dir) {
+      dir.openFileHandles.remove(this);
+      dir.openFilesForWrite.remove(name);
+    }
+  }
+
+  @Override
+  public void flush() throws IOException {
+    dir.maybeThrowDeterministicException();
+    delegate.flush();
+  }
+
+  @Override
+  public void writeByte(byte b) throws IOException {
+    singleByte[0] = b;
+    writeBytes(singleByte, 0, 1);
+  }
+  
+  @Override
+  public void writeBytes(byte[] b, int offset, int len) throws IOException {
+    long freeSpace = dir.maxSize == 0 ? 0 : dir.maxSize - dir.sizeInBytes();
+    long realUsage = 0;
+
+    // If MockRAMDir crashed since we were opened, then
+    // don't write anything:
+    if (dir.crashed)
+      throw new IOException("MockRAMDirectory was crashed; cannot write to " + name);
+
+    // Enforce disk full:
+    if (dir.maxSize != 0 && freeSpace <= len) {
+      // Compute the real disk free.  This will greatly slow
+      // down our test but makes it more accurate:
+      realUsage = dir.getRecomputedActualSizeInBytes();
+      freeSpace = dir.maxSize - realUsage;
+    }
+
+    if (dir.maxSize != 0 && freeSpace <= len) {
+      if (freeSpace > 0) {
+        realUsage += freeSpace;
+        delegate.writeBytes(b, offset, (int) freeSpace);
+      }
+      if (realUsage > dir.maxUsedSize) {
+        dir.maxUsedSize = realUsage;
+      }
+      String message = "fake disk full at " + dir.getRecomputedActualSizeInBytes() + " bytes when writing " + name + " (file length=" + delegate.length();
+      if (freeSpace > 0) {
+        message += "; wrote " + freeSpace + " of " + len + " bytes";
+      }
+      message += ")";
+      if (LuceneTestCase.VERBOSE) {
+        System.out.println(Thread.currentThread().getName() + ": MDW: now throw fake disk full");
+        new Throwable().printStackTrace(System.out);
+      }
+      throw new IOException(message);
+    } else {
+      if (dir.randomState.nextBoolean()) {
+        final int half = len/2;
+        delegate.writeBytes(b, offset, half);
+        Thread.yield();
+        delegate.writeBytes(b, offset+half, len-half);
+      } else {
+        delegate.writeBytes(b, offset, len);
+      }
+    }
+
+    dir.maybeThrowDeterministicException();
+
+    if (first) {
+      // Maybe throw random exception; only do this on first
+      // write to a new file:
+      first = false;
+      dir.maybeThrowIOException();
+    }
+  }
+
+  @Override
+  public long getFilePointer() {
+    return delegate.getFilePointer();
+  }
+
+  @Override
+  public void seek(long pos) throws IOException {
+    delegate.seek(pos);
+  }
+
+  @Override
+  public long length() throws IOException {
+    return delegate.length();
+  }
+
+  @Override
+  public void setLength(long length) throws IOException {
+    delegate.setLength(length);
+  }
+
+  @Override
+  public void copyBytes(DataInput input, long numBytes) throws IOException {
+    delegate.copyBytes(input, numBytes);
+    // TODO: we may need to check disk full here as well
+    dir.maybeThrowDeterministicException();
+  }
+}
diff --git a/lucene/src/test-framework/org/apache/lucene/store/_TestHelper.java b/lucene/src/test-framework/org/apache/lucene/store/_TestHelper.java
new file mode 100644
index 0000000..fb90a87
--- /dev/null
+++ b/lucene/src/test-framework/org/apache/lucene/store/_TestHelper.java
@@ -0,0 +1,65 @@
+package org.apache.lucene.store;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.store.SimpleFSDirectory.SimpleFSIndexInput;
+
+/** This class provides access to package-level features defined in the
+ *  store package. It is used for testing only.
+ */
+public class _TestHelper {
+
+    /** Returns true if the instance of the provided input stream is actually
+     *  an SimpleFSIndexInput.
+     */
+    public static boolean isSimpleFSIndexInput(IndexInput is) {
+        return is instanceof SimpleFSIndexInput;
+    }
+
+    /** Returns true if the provided input stream is an SimpleFSIndexInput and
+     *  is a clone, that is it does not own its underlying file descriptor.
+     */
+    public static boolean isSimpleFSIndexInputClone(IndexInput is) {
+        if (isSimpleFSIndexInput(is)) {
+            return ((SimpleFSIndexInput) is).isClone;
+        } else {
+            return false;
+        }
+    }
+
+    /** Given an instance of SimpleFSDirectory.SimpleFSIndexInput, this method returns
+     *  true if the underlying file descriptor is valid, and false otherwise.
+     *  This can be used to determine if the OS file has been closed.
+     *  The descriptor becomes invalid when the non-clone instance of the
+     *  SimpleFSIndexInput that owns this descriptor is closed. However, the
+     *  descriptor may possibly become invalid in other ways as well.
+     */
+    public static boolean isSimpleFSIndexInputOpen(IndexInput is)
+    throws IOException
+    {
+        if (isSimpleFSIndexInput(is)) {
+            SimpleFSIndexInput fis = (SimpleFSIndexInput) is;
+            return fis.isFDValid();
+        } else {
+            return false;
+        }
+    }
+
+}
diff --git a/lucene/src/test-framework/org/apache/lucene/util/LineFileDocs.java b/lucene/src/test-framework/org/apache/lucene/util/LineFileDocs.java
new file mode 100644
index 0000000..56cb3e0
--- /dev/null
+++ b/lucene/src/test-framework/org/apache/lucene/util/LineFileDocs.java
@@ -0,0 +1,180 @@
+package org.apache.lucene.util;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.IOException;
+import java.io.BufferedReader;
+import java.io.InputStreamReader;
+import java.io.InputStream;
+import java.io.BufferedInputStream;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.zip.GZIPInputStream;
+import java.util.Random;
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+
+// Minimal port of contrib/benchmark's LneDocSource +
+// DocMaker, so tests can enum docs from a line file created
+// by contrib/benchmark's WriteLineDoc task
+public class LineFileDocs implements Closeable {
+
+  private BufferedReader reader;
+  private final static int BUFFER_SIZE = 1 << 16;     // 64K
+  private final AtomicInteger id = new AtomicInteger();
+  private final String path;
+
+  // If forever is true, we rewind the file at EOF (repeat
+  // the docs over and over)
+  public LineFileDocs(Random random, String path) throws IOException {
+    this.path = path;
+    open(random);
+  }
+
+  public LineFileDocs(Random random) throws IOException {
+    this(random, LuceneTestCase.TEST_LINE_DOCS_FILE);
+  }
+
+  public synchronized void close() throws IOException {
+    if (reader != null) {
+      reader.close();
+      reader = null;
+    }
+  }
+
+  private synchronized void open(Random random) throws IOException {
+    InputStream is = getClass().getResourceAsStream(path);
+    if (is == null) {
+      // if its not in classpath, we load it as absolute filesystem path (e.g. Hudson's home dir)
+      is = new FileInputStream(path);
+    }
+    File file = new File(path);
+    long size;
+    if (file.exists()) {
+      size = file.length();
+    } else {
+      size = is.available();
+    }
+    if (path.endsWith(".gz")) {
+      is = new GZIPInputStream(is);
+      // guestimate:
+      size *= 2.8;
+    }
+
+    final InputStream in = new BufferedInputStream(is, BUFFER_SIZE);
+    reader = new BufferedReader(new InputStreamReader(in, "UTF-8"), BUFFER_SIZE);
+
+    // Override sizes for currently "known" line files:
+    if (path.equals("europarl.lines.txt.gz")) {
+      size = 15129506L;
+    } else if (path.equals("/home/hudson/lucene-data/enwiki.random.lines.txt.gz")) {
+      size = 3038178822L;
+    }
+
+    // Randomly seek to starting point:
+    if (random != null && size > 3) {
+      final long seekTo = (random.nextLong()&Long.MAX_VALUE) % (size/3);
+      if (LuceneTestCase.VERBOSE) {
+        System.out.println("TEST: LineFileDocs: seek to fp=" + seekTo + " on open");
+      }
+      reader.skip(seekTo);
+      reader.readLine();
+    }
+  }
+
+  public synchronized void reset(Random random) throws IOException {
+    close();
+    open(random);
+    id.set(0);
+  }
+
+  private final static char SEP = '\t';
+
+  private static final class DocState {
+    final Document doc;
+    final Field titleTokenized;
+    final Field title;
+    final Field body;
+    final Field id;
+    final Field date;
+
+    public DocState() {
+      doc = new Document();
+      
+      title = new Field("title", "", Field.Store.NO, Field.Index.NOT_ANALYZED_NO_NORMS);
+      doc.add(title);
+
+      titleTokenized = new Field("titleTokenized", "", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS);
+      doc.add(titleTokenized);
+
+      body = new Field("body", "", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS);
+      doc.add(body);
+
+      id = new Field("id", "", Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS);
+      doc.add(id);
+
+      date = new Field("date", "", Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS);
+      doc.add(date);
+    }
+  }
+
+  private final ThreadLocal<DocState> threadDocs = new ThreadLocal<DocState>();
+
+  // Document instance is re-used per-thread
+  public Document nextDoc() throws IOException {
+    String line;
+    synchronized(this) {
+      line = reader.readLine();
+      if (line == null) {
+        // Always rewind at end:
+        if (LuceneTestCase.VERBOSE) {
+          System.out.println("TEST: LineFileDocs: now rewind file...");
+        }
+        close();
+        open(null);
+        line = reader.readLine();
+      }
+    }
+
+    DocState docState = threadDocs.get();
+    if (docState == null) {
+      docState = new DocState();
+      threadDocs.set(docState);
+    }
+
+    int spot = line.indexOf(SEP);
+    if (spot == -1) {
+      throw new RuntimeException("line: [" + line + "] is in an invalid format !");
+    }
+    int spot2 = line.indexOf(SEP, 1 + spot);
+    if (spot2 == -1) {
+      throw new RuntimeException("line: [" + line + "] is in an invalid format !");
+    }
+
+    docState.body.setValue(line.substring(1+spot2, line.length()));
+    final String title = line.substring(0, spot);
+    docState.title.setValue(title);
+    docState.titleTokenized.setValue(title);
+    docState.date.setValue(line.substring(1+spot, spot2));
+    docState.id.setValue(Integer.toString(id.getAndIncrement()));
+    return docState.doc;
+  }
+}
diff --git a/lucene/src/test-framework/org/apache/lucene/util/LuceneJUnitDividingSelector.java b/lucene/src/test-framework/org/apache/lucene/util/LuceneJUnitDividingSelector.java
new file mode 100644
index 0000000..5a9509c
--- /dev/null
+++ b/lucene/src/test-framework/org/apache/lucene/util/LuceneJUnitDividingSelector.java
@@ -0,0 +1,66 @@
+/**
+ *  Licensed to the Apache Software Foundation (ASF) under one or more
+ *  contributor license agreements.  See the NOTICE file distributed with
+ *  this work for additional information regarding copyright ownership.
+ *  The ASF licenses this file to You under the Apache License, Version 2.0
+ *  (the "License"); you may not use this file except in compliance with
+ *  the License.  You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *  Unless required by applicable law or agreed to in writing, software
+ *  distributed under the License is distributed on an "AS IS" BASIS,
+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *  See the License for the specific language governing permissions and
+ *  limitations under the License.
+ *
+ */
+package org.apache.lucene.util;
+import java.io.File;
+
+import org.apache.tools.ant.BuildException;
+import org.apache.tools.ant.types.Parameter;
+import org.apache.tools.ant.types.selectors.BaseExtendSelector;
+
+/** Divides filesets into equal groups */
+public class LuceneJUnitDividingSelector extends BaseExtendSelector {
+  private int counter;
+  /** Number of total parts to split. */
+  private int divisor;
+  /** Current part to accept. */
+  private int part;
+
+  @Override
+  public void setParameters(Parameter[] pParameters) {
+    super.setParameters(pParameters);
+    for (int j = 0; j < pParameters.length; j++) {
+      Parameter p = pParameters[j];
+      if ("divisor".equalsIgnoreCase(p.getName())) {
+        divisor = Integer.parseInt(p.getValue());
+      }
+      else if ("part".equalsIgnoreCase(p.getName())) {
+        part = Integer.parseInt(p.getValue());
+      }
+      else {
+        throw new BuildException("unknown " + p.getName());
+      }
+    }
+  }
+
+  @Override
+  public void verifySettings() {
+    super.verifySettings();
+    if (divisor <= 0 || part <= 0) {
+      throw new BuildException("part or divisor not set");
+    }
+    if (part > divisor) {
+      throw new BuildException("part must be <= divisor");
+    }
+  }
+
+  @Override
+  public boolean isSelected(File dir, String name, File path) {
+    counter = counter % divisor + 1;
+    return counter == part;
+  }
+}
diff --git a/lucene/src/test-framework/org/apache/lucene/util/LuceneJUnitResultFormatter.java b/lucene/src/test-framework/org/apache/lucene/util/LuceneJUnitResultFormatter.java
new file mode 100644
index 0000000..a03f780
--- /dev/null
+++ b/lucene/src/test-framework/org/apache/lucene/util/LuceneJUnitResultFormatter.java
@@ -0,0 +1,293 @@
+/**
+ *  Licensed to the Apache Software Foundation (ASF) under one or more
+ *  contributor license agreements.  See the NOTICE file distributed with
+ *  this work for additional information regarding copyright ownership.
+ *  The ASF licenses this file to You under the Apache License, Version 2.0
+ *  (the "License"); you may not use this file except in compliance with
+ *  the License.  You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *  Unless required by applicable law or agreed to in writing, software
+ *  distributed under the License is distributed on an "AS IS" BASIS,
+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *  See the License for the specific language governing permissions and
+ *  limitations under the License.
+ *
+ */
+
+package org.apache.lucene.util;
+
+import java.io.ByteArrayOutputStream;
+import java.io.File;
+import java.io.IOException;
+import java.io.OutputStream;
+import java.text.NumberFormat;
+import java.util.logging.LogManager;
+
+import junit.framework.AssertionFailedError;
+import junit.framework.Test;
+
+import org.apache.lucene.store.LockReleaseFailedException;
+import org.apache.lucene.store.NativeFSLockFactory;
+import org.apache.tools.ant.taskdefs.optional.junit.JUnitResultFormatter;
+import org.apache.tools.ant.taskdefs.optional.junit.JUnitTest;
+import org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner;
+import org.apache.tools.ant.util.FileUtils;
+import org.apache.tools.ant.util.StringUtils;
+import org.junit.Ignore;
+
+/**
+ * Just like BriefJUnitResultFormatter "brief" bundled with ant,
+ * except all formatted text is buffered until the test suite is finished.
+ * At this point, the output is written at once in synchronized fashion.
+ * This way tests can run in parallel without interleaving output.
+ */
+public class LuceneJUnitResultFormatter implements JUnitResultFormatter {
+  private static final double ONE_SECOND = 1000.0;
+  
+  private static final NativeFSLockFactory lockFactory;
+  
+  /** Where to write the log to. */
+  private OutputStream out;
+  
+  /** Formatter for timings. */
+  private NumberFormat numberFormat = NumberFormat.getInstance();
+  
+  /** Output suite has written to System.out */
+  private String systemOutput = null;
+  
+  /** Output suite has written to System.err */
+  private String systemError = null;
+  
+  /** Buffer output until the end of the test */
+  private ByteArrayOutputStream sb; // use a BOS for our mostly ascii-output
+
+  private static final org.apache.lucene.store.Lock lock;
+
+  static {
+    File lockDir = new File(System.getProperty("java.io.tmpdir"),
+        "lucene_junit_lock");
+    lockDir.mkdirs();
+    if (!lockDir.exists()) {
+      throw new RuntimeException("Could not make Lock directory:" + lockDir);
+    }
+    try {
+      lockFactory = new NativeFSLockFactory(lockDir);
+      lock = lockFactory.makeLock("junit_lock");
+    } catch (IOException e) {
+      throw new RuntimeException(e);
+    }
+  }
+
+  /** Constructor for LuceneJUnitResultFormatter. */
+  public LuceneJUnitResultFormatter() {
+  }
+  
+  /**
+   * Sets the stream the formatter is supposed to write its results to.
+   * @param out the output stream to write to
+   */
+  public void setOutput(OutputStream out) {
+    this.out = out;
+  }
+  
+  /**
+   * @see JUnitResultFormatter#setSystemOutput(String)
+   */
+  /** {@inheritDoc}. */
+  public void setSystemOutput(String out) {
+    systemOutput = out;
+  }
+  
+  /**
+   * @see JUnitResultFormatter#setSystemError(String)
+   */
+  /** {@inheritDoc}. */
+  public void setSystemError(String err) {
+    systemError = err;
+  }
+  
+  
+  /**
+   * The whole testsuite started.
+   * @param suite the test suite
+   */
+  public synchronized void startTestSuite(JUnitTest suite) {
+    if (out == null) {
+      return; // Quick return - no output do nothing.
+    }
+    sb = new ByteArrayOutputStream(); // don't reuse, so its gc'ed
+    try {
+      LogManager.getLogManager().readConfiguration();
+    } catch (Exception e) {}
+    append("Testsuite: ");
+    append(suite.getName());
+    append(StringUtils.LINE_SEP);
+  }
+  
+  /**
+   * The whole testsuite ended.
+   * @param suite the test suite
+   */
+  public synchronized void endTestSuite(JUnitTest suite) {
+    append("Tests run: ");
+    append(suite.runCount());
+    append(", Failures: ");
+    append(suite.failureCount());
+    append(", Errors: ");
+    append(suite.errorCount());
+    append(", Time elapsed: ");
+    append(numberFormat.format(suite.getRunTime() / ONE_SECOND));
+    append(" sec");
+    append(StringUtils.LINE_SEP);
+    append(StringUtils.LINE_SEP);
+    
+    // append the err and output streams to the log
+    if (systemOutput != null && systemOutput.length() > 0) {
+      append("------------- Standard Output ---------------")
+      .append(StringUtils.LINE_SEP)
+      .append(systemOutput)
+      .append("------------- ---------------- ---------------")
+      .append(StringUtils.LINE_SEP);
+    }
+    
+    // HACK: junit gives us no way to do this in LuceneTestCase
+    try {
+      Class<?> clazz = Class.forName(suite.getName());
+      Ignore ignore = clazz.getAnnotation(Ignore.class);
+      if (ignore != null) {
+        if (systemError == null) systemError = "";
+        systemError += "NOTE: Ignoring test class '" + clazz.getSimpleName() + "': " 
+                    + ignore.value() + StringUtils.LINE_SEP;
+      }
+    } catch (ClassNotFoundException e) { /* no problem */ }
+    // END HACK
+    
+    if (systemError != null && systemError.length() > 0) {
+      append("------------- Standard Error -----------------")
+      .append(StringUtils.LINE_SEP)
+      .append(systemError)
+      .append("------------- ---------------- ---------------")
+      .append(StringUtils.LINE_SEP);
+    }
+    
+    if (out != null) {
+      try {
+        lock.obtain(5000);
+        try {
+          sb.writeTo(out);
+          out.flush();
+        } finally {
+          try {
+            lock.release();
+          } catch(LockReleaseFailedException e) {
+            // well lets pretend its released anyway
+          }
+        }
+      } catch (IOException e) {
+        throw new RuntimeException("unable to write results", e);
+      } finally {
+        if (out != System.out && out != System.err) {
+          FileUtils.close(out);
+        }
+      }
+    }
+  }
+  
+  /**
+   * A test started.
+   * @param test a test
+   */
+  public void startTest(Test test) {
+  }
+  
+  /**
+   * A test ended.
+   * @param test a test
+   */
+  public void endTest(Test test) {
+  }
+  
+  /**
+   * Interface TestListener for JUnit &lt;= 3.4.
+   *
+   * <p>A Test failed.
+   * @param test a test
+   * @param t    the exception thrown by the test
+   */
+  public void addFailure(Test test, Throwable t) {
+    formatError("\tFAILED", test, t);
+  }
+  
+  /**
+   * Interface TestListener for JUnit &gt; 3.4.
+   *
+   * <p>A Test failed.
+   * @param test a test
+   * @param t    the assertion failed by the test
+   */
+  public void addFailure(Test test, AssertionFailedError t) {
+    addFailure(test, (Throwable) t);
+  }
+  
+  /**
+   * A test caused an error.
+   * @param test  a test
+   * @param error the error thrown by the test
+   */
+  public void addError(Test test, Throwable error) {
+    formatError("\tCaused an ERROR", test, error);
+  }
+  
+  /**
+   * Format the test for printing..
+   * @param test a test
+   * @return the formatted testname
+   */
+  protected String formatTest(Test test) {
+    if (test == null) {
+      return "Null Test: ";
+    } else {
+      return "Testcase: " + test.toString() + ":";
+    }
+  }
+  
+  /**
+   * Format an error and print it.
+   * @param type the type of error
+   * @param test the test that failed
+   * @param error the exception that the test threw
+   */
+  protected synchronized void formatError(String type, Test test,
+      Throwable error) {
+    if (test != null) {
+      endTest(test);
+    }
+    
+    append(formatTest(test) + type);
+    append(StringUtils.LINE_SEP);
+    append(error.getMessage());
+    append(StringUtils.LINE_SEP);
+    String strace = JUnitTestRunner.getFilteredTrace(error);
+    append(strace);
+    append(StringUtils.LINE_SEP);
+    append(StringUtils.LINE_SEP);
+  }
+
+  public LuceneJUnitResultFormatter append(String s) {
+    if (s == null)
+      s = "(null)";
+    try {
+      sb.write(s.getBytes()); // intentionally use default charset, its a console.
+    } catch (IOException e) {
+      throw new RuntimeException(e);
+    }
+    return this;
+  }
+  
+  public LuceneJUnitResultFormatter append(long l) {
+    return append(Long.toString(l));
+  }
+}
+
diff --git a/lucene/src/test-framework/org/apache/lucene/util/LuceneTestCase.java b/lucene/src/test-framework/org/apache/lucene/util/LuceneTestCase.java
new file mode 100644
index 0000000..a10689c
--- /dev/null
+++ b/lucene/src/test-framework/org/apache/lucene/util/LuceneTestCase.java
@@ -0,0 +1,1253 @@
+package org.apache.lucene.util;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.File;
+import java.io.IOException;
+import java.io.PrintStream;
+import java.lang.annotation.Documented;
+import java.lang.annotation.Inherited;
+import java.lang.annotation.Retention;
+import java.lang.annotation.RetentionPolicy;
+import java.lang.reflect.Constructor;
+import java.lang.reflect.Method;
+import java.lang.reflect.Modifier;
+import java.util.*;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.TimeUnit;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.Field.Index;
+import org.apache.lucene.document.Field.Store;
+import org.apache.lucene.document.Field.TermVector;
+import org.apache.lucene.index.*;
+import org.apache.lucene.index.codecs.Codec;
+import org.apache.lucene.index.codecs.CodecProvider;
+import org.apache.lucene.index.codecs.mockintblock.MockFixedIntBlockCodec;
+import org.apache.lucene.index.codecs.mockintblock.MockVariableIntBlockCodec;
+import org.apache.lucene.index.codecs.mocksep.MockSepCodec;
+import org.apache.lucene.index.codecs.mockrandom.MockRandomCodec;
+import org.apache.lucene.index.codecs.preflex.PreFlexCodec;
+import org.apache.lucene.index.codecs.preflexrw.PreFlexRWCodec;
+import org.apache.lucene.index.codecs.pulsing.PulsingCodec;
+import org.apache.lucene.index.codecs.simpletext.SimpleTextCodec;
+import org.apache.lucene.index.codecs.standard.StandardCodec;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.FieldCache;
+import org.apache.lucene.search.FieldCache.CacheEntry;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.FSDirectory;
+import org.apache.lucene.store.LockFactory;
+import org.apache.lucene.store.MockDirectoryWrapper;
+import org.apache.lucene.util.FieldCacheSanityChecker.Insanity;
+import org.junit.*;
+import org.junit.rules.TestWatchman;
+import org.junit.runner.Description;
+import org.junit.runner.RunWith;
+import org.junit.runner.manipulation.Filter;
+import org.junit.runner.manipulation.NoTestsRemainException;
+import org.junit.runner.notification.RunNotifier;
+import org.junit.runners.BlockJUnit4ClassRunner;
+import org.junit.runners.model.FrameworkMethod;
+import org.junit.runners.model.InitializationError;
+
+/**
+ * Base class for all Lucene unit tests, Junit3 or Junit4 variant.
+ * <p>
+ * </p>
+ * <p>
+ * If you
+ * override either <code>setUp()</code> or
+ * <code>tearDown()</code> in your unit test, make sure you
+ * call <code>super.setUp()</code> and
+ * <code>super.tearDown()</code>
+ * </p>
+ *
+ * @After - replaces setup
+ * @Before - replaces teardown
+ * @Test - any public method with this annotation is a test case, regardless
+ * of its name
+ * <p>
+ * <p>
+ * See Junit4 <a href="http://junit.org/junit/javadoc/4.7/">documentation</a> for a complete list of features.
+ * <p>
+ * Import from org.junit rather than junit.framework.
+ * <p>
+ * You should be able to use this class anywhere you used LuceneTestCase
+ * if you annotate your derived class correctly with the annotations above
+ * @see #assertSaneFieldCaches(String)
+ */
+
+@RunWith(LuceneTestCase.LuceneTestCaseRunner.class)
+public abstract class LuceneTestCase extends Assert {
+
+  /**
+   * true iff tests are run in verbose mode. Note: if it is false, tests are not
+   * expected to print any messages.
+   */
+  public static final boolean VERBOSE = Boolean.getBoolean("tests.verbose");
+
+  /** Use this constant when creating Analyzers and any other version-dependent stuff.
+   * <p><b>NOTE:</b> Change this when development starts for new Lucene version:
+   */
+  public static final Version TEST_VERSION_CURRENT = Version.LUCENE_40;
+
+  /**
+   * If this is set, it is the only method that should run.
+   */
+  static final String TEST_METHOD;
+  
+  /** Create indexes in this directory, optimally use a subdir, named after the test */
+  public static final File TEMP_DIR;
+  static {
+    String method = System.getProperty("testmethod", "").trim();
+    TEST_METHOD = method.length() == 0 ? null : method;
+    String s = System.getProperty("tempDir", System.getProperty("java.io.tmpdir"));
+    if (s == null)
+      throw new RuntimeException("To run tests, you need to define system property 'tempDir' or 'java.io.tmpdir'.");
+    TEMP_DIR = new File(s);
+    TEMP_DIR.mkdirs();
+  }
+
+  // by default we randomly pick a different codec for
+  // each test case (non-J4 tests) and each test class (J4
+  // tests)
+  /** Gets the codec to run tests with. */
+  public static final String TEST_CODEC = System.getProperty("tests.codec", "randomPerField");
+  /** Gets the locale to run tests with */
+  public static final String TEST_LOCALE = System.getProperty("tests.locale", "random");
+  /** Gets the timezone to run tests with */
+  public static final String TEST_TIMEZONE = System.getProperty("tests.timezone", "random");
+  /** Gets the directory to run tests with */
+  public static final String TEST_DIRECTORY = System.getProperty("tests.directory", "random");
+  /** Get the number of times to run tests */
+  public static final int TEST_ITER = Integer.parseInt(System.getProperty("tests.iter", "1"));
+  /** Get the random seed for tests */
+  public static final String TEST_SEED = System.getProperty("tests.seed", "random");
+  /** whether or not nightly tests should run */
+  public static final boolean TEST_NIGHTLY = Boolean.parseBoolean(System.getProperty("tests.nightly", "false"));
+  /** the line file used by LineFileDocs */
+  public static final String TEST_LINE_DOCS_FILE = System.getProperty("tests.linedocsfile", "europarl.lines.txt.gz");
+
+  private static final Pattern codecWithParam = Pattern.compile("(.*)\\(\\s*(\\d+)\\s*\\)");
+
+  /**
+   * A random multiplier which you should use when writing random tests:
+   * multiply it by the number of iterations
+   */
+  public static final int RANDOM_MULTIPLIER = Integer.parseInt(System.getProperty("tests.multiplier", "1"));
+  
+  private int savedBoolMaxClauseCount;
+
+  private volatile Thread.UncaughtExceptionHandler savedUncaughtExceptionHandler = null;
+  
+  /** Used to track if setUp and tearDown are called correctly from subclasses */
+  private boolean setup;
+
+  /**
+   * Some tests expect the directory to contain a single segment, and want to do tests on that segment's reader.
+   * This is an utility method to help them.
+   */
+  public static SegmentReader getOnlySegmentReader(IndexReader reader) {
+    if (reader instanceof SegmentReader)
+      return (SegmentReader) reader;
+
+    IndexReader[] subReaders = reader.getSequentialSubReaders();
+    if (subReaders.length != 1)
+      throw new IllegalArgumentException(reader + " has " + subReaders.length + " segments instead of exactly one");
+
+    return (SegmentReader) subReaders[0];
+  }
+
+  private static class UncaughtExceptionEntry {
+    public final Thread thread;
+    public final Throwable exception;
+    
+    public UncaughtExceptionEntry(Thread thread, Throwable exception) {
+      this.thread = thread;
+      this.exception = exception;
+    }
+  }
+  private List<UncaughtExceptionEntry> uncaughtExceptions = Collections.synchronizedList(new ArrayList<UncaughtExceptionEntry>());
+  
+  // saves default codec: we do this statically as many build indexes in @beforeClass
+  private static String savedDefaultCodec;
+  // default codec: not set when we use a per-field provider.
+  private static Codec codec;
+  // default codec provider
+  private static CodecProvider savedCodecProvider;
+  
+  private static Locale locale;
+  private static Locale savedLocale;
+  private static TimeZone timeZone;
+  private static TimeZone savedTimeZone;
+  
+  private static Map<MockDirectoryWrapper,StackTraceElement[]> stores;
+  
+  private static final String[] TEST_CODECS = new String[] {"MockSep", "MockFixedIntBlock", "MockVariableIntBlock", "MockRandom"};
+
+  private static void swapCodec(Codec c, CodecProvider cp) {
+    Codec prior = null;
+    try {
+      prior = cp.lookup(c.name);
+    } catch (IllegalArgumentException iae) {
+    }
+    if (prior != null) {
+      cp.unregister(prior);
+    }
+    cp.register(c);
+  }
+
+  // returns current default codec
+  static Codec installTestCodecs(String codec, CodecProvider cp) {
+    savedDefaultCodec = cp.getDefaultFieldCodec();
+
+    final boolean codecHasParam;
+    int codecParam = 0;
+    if (codec.equals("randomPerField")) {
+      // lie
+      codec = "Standard";
+      codecHasParam = false;
+    } else if (codec.equals("random")) {
+      codec = pickRandomCodec(random);
+      codecHasParam = false;
+    } else {
+      Matcher m = codecWithParam.matcher(codec);
+      if (m.matches()) {
+        // codec has a fixed param
+        codecHasParam = true;
+        codec = m.group(1);
+        codecParam = Integer.parseInt(m.group(2));
+      } else {
+        codecHasParam = false;
+      }
+    }
+
+    cp.setDefaultFieldCodec(codec);
+
+    if (codec.equals("PreFlex")) {
+      // If we're running w/ PreFlex codec we must swap in the
+      // test-only PreFlexRW codec (since core PreFlex can
+      // only read segments):
+      swapCodec(new PreFlexRWCodec(), cp);
+    }
+
+    swapCodec(new MockSepCodec(), cp);
+    swapCodec(new PulsingCodec(codecHasParam && "Pulsing".equals(codec) ? codecParam : _TestUtil.nextInt(random, 1, 20)), cp);
+    swapCodec(new MockFixedIntBlockCodec(codecHasParam && "MockFixedIntBlock".equals(codec) ? codecParam : _TestUtil.nextInt(random, 1, 2000)), cp);
+    // baseBlockSize cannot be over 127:
+    swapCodec(new MockVariableIntBlockCodec(codecHasParam && "MockVariableIntBlock".equals(codec) ? codecParam : _TestUtil.nextInt(random, 1, 127)), cp);
+    swapCodec(new MockRandomCodec(random), cp);
+
+    return cp.lookup(codec);
+  }
+
+  // returns current PreFlex codec
+  static void removeTestCodecs(Codec codec, CodecProvider cp) {
+    if (codec.name.equals("PreFlex")) {
+      final Codec preFlex = cp.lookup("PreFlex");
+      if (preFlex != null) {
+        cp.unregister(preFlex);
+      }
+      cp.register(new PreFlexCodec());
+    }
+    cp.unregister(cp.lookup("MockSep"));
+    cp.unregister(cp.lookup("MockFixedIntBlock"));
+    cp.unregister(cp.lookup("MockVariableIntBlock"));
+    cp.unregister(cp.lookup("MockRandom"));
+    swapCodec(new PulsingCodec(1), cp);
+    cp.setDefaultFieldCodec(savedDefaultCodec);
+  }
+
+  // randomly picks from core and test codecs
+  static String pickRandomCodec(Random rnd) {
+    int idx = rnd.nextInt(CodecProvider.CORE_CODECS.length + 
+                          TEST_CODECS.length);
+    if (idx < CodecProvider.CORE_CODECS.length) {
+      return CodecProvider.CORE_CODECS[idx];
+    } else {
+      return TEST_CODECS[idx - CodecProvider.CORE_CODECS.length];
+    }
+  }
+
+  private static class TwoLongs {
+    public final long l1, l2;
+
+    public TwoLongs(long l1, long l2) {
+      this.l1 = l1;
+      this.l2 = l2;
+    }
+
+    @Override
+    public String toString() {
+      return l1 + ":" + l2;
+    }
+
+    public static TwoLongs fromString(String s) {
+      final int i = s.indexOf(':');
+      assert i != -1;
+      return new TwoLongs(Long.parseLong(s.substring(0, i)),
+                          Long.parseLong(s.substring(1+i)));
+    }
+  }
+
+  /** @deprecated (4.0) until we fix no-fork problems in solr tests */
+  @Deprecated
+  private static List<String> testClassesRun = new ArrayList<String>();
+  
+  @BeforeClass
+  public static void beforeClassLuceneTestCaseJ4() {
+    staticSeed = "random".equals(TEST_SEED) ? seedRand.nextLong() : TwoLongs.fromString(TEST_SEED).l1;
+    random.setSeed(staticSeed);
+    stores = Collections.synchronizedMap(new IdentityHashMap<MockDirectoryWrapper,StackTraceElement[]>());
+    savedCodecProvider = CodecProvider.getDefault();
+    if ("randomPerField".equals(TEST_CODEC)) {
+      if (random.nextInt(4) == 0) { // preflex-only setup
+        codec = installTestCodecs("PreFlex", CodecProvider.getDefault());
+      } else { // per-field setup
+        CodecProvider.setDefault(new RandomCodecProvider(random));
+        codec = installTestCodecs(TEST_CODEC, CodecProvider.getDefault());
+      }
+    } else { // ordinary setup
+      codec = installTestCodecs(TEST_CODEC, CodecProvider.getDefault());
+    }
+    savedLocale = Locale.getDefault();
+    locale = TEST_LOCALE.equals("random") ? randomLocale(random) : localeForName(TEST_LOCALE);
+    Locale.setDefault(locale);
+    savedTimeZone = TimeZone.getDefault();
+    timeZone = TEST_TIMEZONE.equals("random") ? randomTimeZone(random) : TimeZone.getTimeZone(TEST_TIMEZONE);
+    TimeZone.setDefault(timeZone);
+    testsFailed = false;
+  }
+  
+  @AfterClass
+  public static void afterClassLuceneTestCaseJ4() {
+    int rogueThreads = threadCleanup("test class");
+    if (rogueThreads > 0) {
+      // TODO: fail here once the leaks are fixed.
+      System.err.println("RESOURCE LEAK: test class left " + rogueThreads + " thread(s) running");
+    }
+    String codecDescription;
+    CodecProvider cp = CodecProvider.getDefault();
+
+    if ("randomPerField".equals(TEST_CODEC)) {
+      if (cp instanceof RandomCodecProvider)
+        codecDescription = cp.toString();
+      else 
+        codecDescription = "PreFlex";
+    } else {
+      codecDescription = codec.toString();
+    }
+    
+    if (CodecProvider.getDefault() == savedCodecProvider)
+      removeTestCodecs(codec, CodecProvider.getDefault());
+    CodecProvider.setDefault(savedCodecProvider);
+    Locale.setDefault(savedLocale);
+    TimeZone.setDefault(savedTimeZone);
+    System.clearProperty("solr.solr.home");
+    System.clearProperty("solr.data.dir");
+    // now look for unclosed resources
+    if (!testsFailed)
+      for (MockDirectoryWrapper d : stores.keySet()) {
+        if (d.isOpen()) {
+          StackTraceElement elements[] = stores.get(d);
+          // Look for the first class that is not LuceneTestCase that requested
+          // a Directory. The first two items are of Thread's, so skipping over
+          // them.
+          StackTraceElement element = null;
+          for (int i = 2; i < elements.length; i++) {
+            StackTraceElement ste = elements[i];
+            if (ste.getClassName().indexOf("LuceneTestCase") == -1) {
+              element = ste;
+              break;
+            }
+          }
+          fail("directory of test was not closed, opened from: " + element);
+        }
+      }
+    stores = null;
+    // if verbose or tests failed, report some information back
+    if (VERBOSE || testsFailed)
+      System.err.println("NOTE: test params are: codec=" + codecDescription + 
+        ", locale=" + locale + 
+        ", timezone=" + (timeZone == null ? "(null)" : timeZone.getID()));
+    if (testsFailed) {
+      System.err.println("NOTE: all tests run in this JVM:");
+      System.err.println(Arrays.toString(testClassesRun.toArray()));
+      System.err.println("NOTE: " + System.getProperty("os.name") + " " 
+          + System.getProperty("os.version") + " " 
+          + System.getProperty("os.arch") + "/"
+          + System.getProperty("java.vendor") + " "
+          + System.getProperty("java.version") + " "
+          + (Constants.JRE_IS_64BIT ? "(64-bit)" : "(32-bit)") + "/"
+          + "cpus=" + Runtime.getRuntime().availableProcessors() + ","
+          + "threads=" + Thread.activeCount() + ","
+          + "free=" + Runtime.getRuntime().freeMemory() + ","
+          + "total=" + Runtime.getRuntime().totalMemory());
+    }
+  }
+
+  private static boolean testsFailed; /* true if any tests failed */
+  
+  // This is how we get control when errors occur.
+  // Think of this as start/end/success/failed
+  // events.
+  @Rule
+  public final TestWatchman intercept = new TestWatchman() {
+
+    @Override
+    public void failed(Throwable e, FrameworkMethod method) {
+      // org.junit.internal.AssumptionViolatedException in older releases
+      // org.junit.Assume.AssumptionViolatedException in recent ones
+      if (e.getClass().getName().endsWith("AssumptionViolatedException")) {
+        if (e.getCause() instanceof TestIgnoredException)
+          e = e.getCause();
+        System.err.print("NOTE: Assume failed in '" + method.getName() + "' (ignored):");
+        if (VERBOSE) {
+          System.err.println();
+          e.printStackTrace(System.err);
+        } else {
+          System.err.print(" ");
+          System.err.println(e.getMessage());
+        }
+      } else {
+        testsFailed = true;
+        reportAdditionalFailureInfo();
+      }
+      super.failed(e, method);
+    }
+
+    @Override
+    public void starting(FrameworkMethod method) {
+      // set current method name for logging
+      LuceneTestCase.this.name = method.getName();
+      super.starting(method);
+    }
+    
+  };
+
+  @Before
+  public void setUp() throws Exception {
+    seed = "random".equals(TEST_SEED) ? seedRand.nextLong() : TwoLongs.fromString(TEST_SEED).l2;
+    random.setSeed(seed);
+    assertFalse("ensure your tearDown() calls super.tearDown()!!!", setup);
+    setup = true;
+    savedUncaughtExceptionHandler = Thread.getDefaultUncaughtExceptionHandler();
+    Thread.setDefaultUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {
+      public void uncaughtException(Thread t, Throwable e) {
+        testsFailed = true;
+        uncaughtExceptions.add(new UncaughtExceptionEntry(t, e));
+        if (savedUncaughtExceptionHandler != null)
+          savedUncaughtExceptionHandler.uncaughtException(t, e);
+      }
+    });
+    
+    savedBoolMaxClauseCount = BooleanQuery.getMaxClauseCount();
+  }
+
+
+  /**
+   * Forcible purges all cache entries from the FieldCache.
+   * <p>
+   * This method will be called by tearDown to clean up FieldCache.DEFAULT.
+   * If a (poorly written) test has some expectation that the FieldCache
+   * will persist across test methods (ie: a static IndexReader) this
+   * method can be overridden to do nothing.
+   * </p>
+   *
+   * @see FieldCache#purgeAllCaches()
+   */
+  protected void purgeFieldCache(final FieldCache fc) {
+    fc.purgeAllCaches();
+  }
+
+  protected String getTestLabel() {
+    return getClass().getName() + "." + getName();
+  }
+
+  @After
+  public void tearDown() throws Exception {
+    assertTrue("ensure your setUp() calls super.setUp()!!!", setup);
+    setup = false;
+    BooleanQuery.setMaxClauseCount(savedBoolMaxClauseCount);
+    if (!getClass().getName().startsWith("org.apache.solr")) {
+      int rogueThreads = threadCleanup("test method: '" + getName() + "'");
+      if (rogueThreads > 0) {
+        System.err.println("RESOURCE LEAK: test method: '" + getName() 
+            + "' left " + rogueThreads + " thread(s) running");
+        // TODO: fail, but print seed for now.
+        if (!testsFailed && uncaughtExceptions.isEmpty()) {
+          reportAdditionalFailureInfo();
+        }
+      }
+    }
+    Thread.setDefaultUncaughtExceptionHandler(savedUncaughtExceptionHandler);
+    try {
+
+      if (!uncaughtExceptions.isEmpty()) {
+        testsFailed = true;
+        reportAdditionalFailureInfo();
+        System.err.println("The following exceptions were thrown by threads:");
+        for (UncaughtExceptionEntry entry : uncaughtExceptions) {
+          System.err.println("*** Thread: " + entry.thread.getName() + " ***");
+          entry.exception.printStackTrace(System.err);
+        }
+        fail("Some threads threw uncaught exceptions!");
+      }
+
+      // calling assertSaneFieldCaches here isn't as useful as having test 
+      // classes call it directly from the scope where the index readers 
+      // are used, because they could be gc'ed just before this tearDown 
+      // method is called.
+      //
+      // But it's better then nothing.
+      //
+      // If you are testing functionality that you know for a fact 
+      // "violates" FieldCache sanity, then you should either explicitly 
+      // call purgeFieldCache at the end of your test method, or refactor
+      // your Test class so that the inconsistant FieldCache usages are 
+      // isolated in distinct test methods  
+      assertSaneFieldCaches(getTestLabel());
+
+    } finally {
+      purgeFieldCache(FieldCache.DEFAULT);
+    }
+  }
+
+  private final static int THREAD_STOP_GRACE_MSEC = 1000;
+  // jvm-wide list of 'rogue threads' we found, so they only get reported once.
+  private final static IdentityHashMap<Thread,Boolean> rogueThreads = new IdentityHashMap<Thread,Boolean>();
+  
+  static {
+    // just a hack for things like eclipse test-runner threads
+    for (Thread t : Thread.getAllStackTraces().keySet()) {
+      rogueThreads.put(t, true);
+    }
+  }
+  
+  /**
+   * Looks for leftover running threads, trying to kill them off,
+   * so they don't fail future tests.
+   * returns the number of rogue threads that it found.
+   */
+  private static int threadCleanup(String context) {
+    // educated guess
+    Thread[] stillRunning = new Thread[Thread.activeCount()+1];
+    int threadCount = 0;
+    int rogueCount = 0;
+    
+    if ((threadCount = Thread.enumerate(stillRunning)) > 1) {
+      while (threadCount == stillRunning.length) {
+        // truncated response
+        stillRunning = new Thread[stillRunning.length*2];
+        threadCount = Thread.enumerate(stillRunning);
+      }
+      
+      for (int i = 0; i < threadCount; i++) {
+        Thread t = stillRunning[i];
+          
+        if (t.isAlive() && 
+            !rogueThreads.containsKey(t) && 
+            t != Thread.currentThread() && 
+            /* its ok to keep your searcher across test cases */
+            (t.getName().startsWith("LuceneTestCase") && context.startsWith("test method")) == false) {
+          System.err.println("WARNING: " + context  + " left thread running: " + t);
+          rogueThreads.put(t, true);
+          rogueCount++;
+          if (t.getName().startsWith("LuceneTestCase")) {
+            System.err.println("PLEASE CLOSE YOUR INDEXSEARCHERS IN YOUR TEST!!!!");
+            continue;
+          } else {
+            // wait on the thread to die of natural causes
+            try {
+              t.join(THREAD_STOP_GRACE_MSEC);
+            } catch (InterruptedException e) { e.printStackTrace(); }
+          }
+          // try to stop the thread:
+          t.setUncaughtExceptionHandler(null);
+          Thread.setDefaultUncaughtExceptionHandler(null);
+          t.interrupt();
+          try {
+            t.join(THREAD_STOP_GRACE_MSEC);
+          } catch (InterruptedException e) { e.printStackTrace(); }
+        }
+      }
+    }
+    return rogueCount;
+  }
+  
+  /**
+   * Asserts that FieldCacheSanityChecker does not detect any
+   * problems with FieldCache.DEFAULT.
+   * <p>
+   * If any problems are found, they are logged to System.err
+   * (allong with the msg) when the Assertion is thrown.
+   * </p>
+   * <p>
+   * This method is called by tearDown after every test method,
+   * however IndexReaders scoped inside test methods may be garbage
+   * collected prior to this method being called, causing errors to
+   * be overlooked. Tests are encouraged to keep their IndexReaders
+   * scoped at the class level, or to explicitly call this method
+   * directly in the same scope as the IndexReader.
+   * </p>
+   *
+   * @see FieldCacheSanityChecker
+   */
+  protected void assertSaneFieldCaches(final String msg) {
+    final CacheEntry[] entries = FieldCache.DEFAULT.getCacheEntries();
+    Insanity[] insanity = null;
+    try {
+      try {
+        insanity = FieldCacheSanityChecker.checkSanity(entries);
+      } catch (RuntimeException e) {
+        dumpArray(msg + ": FieldCache", entries, System.err);
+        throw e;
+      }
+
+      assertEquals(msg + ": Insane FieldCache usage(s) found",
+              0, insanity.length);
+      insanity = null;
+    } finally {
+
+      // report this in the event of any exception/failure
+      // if no failure, then insanity will be null anyway
+      if (null != insanity) {
+        dumpArray(msg + ": Insane FieldCache usage(s)", insanity, System.err);
+      }
+
+    }
+  }
+  
+  // @deprecated (4.0) These deprecated methods should be removed soon, when all tests using no Epsilon are fixed:
+  @Deprecated
+  static public void assertEquals(double expected, double actual) {
+    assertEquals(null, expected, actual);
+  }
+   
+  @Deprecated
+  static public void assertEquals(String message, double expected, double actual) {
+    assertEquals(message, Double.valueOf(expected), Double.valueOf(actual));
+  }
+
+  @Deprecated
+  static public void assertEquals(float expected, float actual) {
+    assertEquals(null, expected, actual);
+  }
+
+  @Deprecated
+  static public void assertEquals(String message, float expected, float actual) {
+    assertEquals(message, Float.valueOf(expected), Float.valueOf(actual));
+  }
+  
+  // Replacement for Assume jUnit class, so we can add a message with explanation:
+  
+  private static final class TestIgnoredException extends RuntimeException {
+    TestIgnoredException(String msg) {
+      super(msg);
+    }
+    
+    TestIgnoredException(String msg, Throwable t) {
+      super(msg, t);
+    }
+    
+    @Override
+    public String getMessage() {
+      StringBuilder sb = new StringBuilder(super.getMessage());
+      if (getCause() != null)
+        sb.append(" - ").append(getCause());
+      return sb.toString();
+    }
+    
+    // only this one is called by our code, exception is not used outside this class:
+    @Override
+    public void printStackTrace(PrintStream s) {
+      if (getCause() != null) {
+        s.println(super.toString() + " - Caused by:");
+        getCause().printStackTrace(s);
+      } else {
+        super.printStackTrace(s);
+      }
+    }
+  }
+  
+  public static void assumeTrue(String msg, boolean b) {
+    Assume.assumeNoException(b ? null : new TestIgnoredException(msg));
+  }
+ 
+  public static void assumeFalse(String msg, boolean b) {
+    assumeTrue(msg, !b);
+  }
+  
+  public static void assumeNoException(String msg, Exception e) {
+    Assume.assumeNoException(e == null ? null : new TestIgnoredException(msg, e));
+  }
+ 
+  public static <T> Set<T> asSet(T... args) {
+    return new HashSet<T>(Arrays.asList(args));
+  }
+
+  /**
+   * Convinience method for logging an iterator.
+   *
+   * @param label  String logged before/after the items in the iterator
+   * @param iter   Each next() is toString()ed and logged on it's own line. If iter is null this is logged differnetly then an empty iterator.
+   * @param stream Stream to log messages to.
+   */
+  public static void dumpIterator(String label, Iterator<?> iter,
+                                  PrintStream stream) {
+    stream.println("*** BEGIN " + label + " ***");
+    if (null == iter) {
+      stream.println(" ... NULL ...");
+    } else {
+      while (iter.hasNext()) {
+        stream.println(iter.next().toString());
+      }
+    }
+    stream.println("*** END " + label + " ***");
+  }
+
+  /**
+   * Convinience method for logging an array.  Wraps the array in an iterator and delegates
+   *
+   * @see #dumpIterator(String,Iterator,PrintStream)
+   */
+  public static void dumpArray(String label, Object[] objs,
+                               PrintStream stream) {
+    Iterator<?> iter = (null == objs) ? null : Arrays.asList(objs).iterator();
+    dumpIterator(label, iter, stream);
+  }
+
+  /** create a new index writer config with random defaults */
+  public static IndexWriterConfig newIndexWriterConfig(Version v, Analyzer a) {
+    return newIndexWriterConfig(random, v, a);
+  }
+  
+  public static IndexWriterConfig newIndexWriterConfig(Random r, Version v, Analyzer a) {
+    IndexWriterConfig c = new IndexWriterConfig(v, a);
+    if (r.nextBoolean()) {
+      c.setMergeScheduler(new SerialMergeScheduler());
+    }
+    if (r.nextBoolean()) {
+      if (r.nextInt(20) == 17) {
+        c.setMaxBufferedDocs(2);
+      } else {
+        c.setMaxBufferedDocs(_TestUtil.nextInt(r, 2, 1000));
+      }
+    }
+    if (r.nextBoolean()) {
+      c.setTermIndexInterval(_TestUtil.nextInt(r, 1, 1000));
+    }
+    if (r.nextBoolean()) {
+      c.setMaxThreadStates(_TestUtil.nextInt(r, 1, 20));
+    }
+
+    if (r.nextBoolean()) {
+      c.setMergePolicy(new MockRandomMergePolicy(r));
+    } else {
+      c.setMergePolicy(newLogMergePolicy());
+    }
+
+    c.setReaderPooling(r.nextBoolean());
+    c.setReaderTermsIndexDivisor(_TestUtil.nextInt(r, 1, 4));
+    return c;
+  }
+
+  public static LogMergePolicy newLogMergePolicy() {
+    return newLogMergePolicy(random);
+  }
+
+  public static LogMergePolicy newLogMergePolicy(Random r) {
+    LogMergePolicy logmp = r.nextBoolean() ? new LogDocMergePolicy() : new LogByteSizeMergePolicy();
+    logmp.setUseCompoundFile(r.nextBoolean());
+    logmp.setCalibrateSizeByDeletes(r.nextBoolean());
+    if (r.nextInt(3) == 2) {
+      logmp.setMergeFactor(2);
+    } else {
+      logmp.setMergeFactor(_TestUtil.nextInt(r, 2, 20));
+    }
+    return logmp;
+  }
+
+  public static LogMergePolicy newInOrderLogMergePolicy() {
+    LogMergePolicy logmp = newLogMergePolicy();
+    logmp.setRequireContiguousMerge(true);
+    return logmp;
+  }
+
+  public static LogMergePolicy newInOrderLogMergePolicy(int mergeFactor) {
+    LogMergePolicy logmp = newLogMergePolicy();
+    logmp.setMergeFactor(mergeFactor);
+    logmp.setRequireContiguousMerge(true);
+    return logmp;
+  }
+
+  public static LogMergePolicy newLogMergePolicy(boolean useCFS) {
+    LogMergePolicy logmp = newLogMergePolicy();
+    logmp.setUseCompoundFile(useCFS);
+    return logmp;
+  }
+
+  public static LogMergePolicy newLogMergePolicy(boolean useCFS, int mergeFactor) {
+    LogMergePolicy logmp = newLogMergePolicy();
+    logmp.setUseCompoundFile(useCFS);
+    logmp.setMergeFactor(mergeFactor);
+    return logmp;
+  }
+
+  public static LogMergePolicy newLogMergePolicy(int mergeFactor) {
+    LogMergePolicy logmp = newLogMergePolicy();
+    logmp.setMergeFactor(mergeFactor);
+    return logmp;
+  }
+
+  /**
+   * Returns a new Directory instance. Use this when the test does not
+   * care about the specific Directory implementation (most tests).
+   * <p>
+   * The Directory is wrapped with {@link MockDirectoryWrapper}.
+   * By default this means it will be picky, such as ensuring that you
+   * properly close it and all open files in your test. It will emulate
+   * some features of Windows, such as not allowing open files to be
+   * overwritten.
+   */
+  public static MockDirectoryWrapper newDirectory() throws IOException {
+    return newDirectory(random);
+  }
+  
+  public static MockDirectoryWrapper newDirectory(Random r) throws IOException {
+    Directory impl = newDirectoryImpl(r, TEST_DIRECTORY);
+    MockDirectoryWrapper dir = new MockDirectoryWrapper(r, impl);
+    stores.put(dir, Thread.currentThread().getStackTrace());
+    return dir;
+  }
+  
+  /**
+   * Returns a new Directory instance, with contents copied from the
+   * provided directory. See {@link #newDirectory()} for more
+   * information.
+   */
+  public static MockDirectoryWrapper newDirectory(Directory d) throws IOException {
+    return newDirectory(random, d);
+  }
+  
+  /** Returns a new FSDirectory instance over the given file, which must be a folder. */
+  public static MockDirectoryWrapper newFSDirectory(File f) throws IOException {
+    return newFSDirectory(f, null);
+  }
+  
+  /** Returns a new FSDirectory instance over the given file, which must be a folder. */
+  public static MockDirectoryWrapper newFSDirectory(File f, LockFactory lf) throws IOException {
+    String fsdirClass = TEST_DIRECTORY;
+    if (fsdirClass.equals("random")) {
+      fsdirClass = FS_DIRECTORIES[random.nextInt(FS_DIRECTORIES.length)];
+    }
+    
+    if (fsdirClass.indexOf(".") == -1) {// if not fully qualified, assume .store
+      fsdirClass = "org.apache.lucene.store." + fsdirClass;
+    }
+    
+    Class<? extends FSDirectory> clazz;
+    try {
+      try {
+        clazz = Class.forName(fsdirClass).asSubclass(FSDirectory.class);
+      } catch (ClassCastException e) {
+        // TEST_DIRECTORY is not a sub-class of FSDirectory, so draw one at random
+        fsdirClass = FS_DIRECTORIES[random.nextInt(FS_DIRECTORIES.length)];
+        
+        if (fsdirClass.indexOf(".") == -1) {// if not fully qualified, assume .store
+          fsdirClass = "org.apache.lucene.store." + fsdirClass;
+        }
+        
+        clazz = Class.forName(fsdirClass).asSubclass(FSDirectory.class);
+      }
+      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, newFSDirectoryImpl(clazz, f, lf));
+      stores.put(dir, Thread.currentThread().getStackTrace());
+      return dir;
+    } catch (Exception e) {
+      throw new RuntimeException(e);
+    }
+  }
+  
+  public static MockDirectoryWrapper newDirectory(Random r, Directory d) throws IOException {
+    Directory impl = newDirectoryImpl(r, TEST_DIRECTORY);
+    for (String file : d.listAll()) {
+     d.copy(impl, file, file);
+    }
+    MockDirectoryWrapper dir = new MockDirectoryWrapper(r, impl);
+    stores.put(dir, Thread.currentThread().getStackTrace());
+    return dir;
+  }
+  
+  public static Field newField(String name, String value, Index index) {
+    return newField(random, name, value, index);
+  }
+  
+  public static Field newField(String name, String value, Store store, Index index) {
+    return newField(random, name, value, store, index);
+  }
+  
+  public static Field newField(String name, String value, Store store, Index index, TermVector tv) {
+    return newField(random, name, value, store, index, tv);
+  }
+  
+  public static Field newField(Random random, String name, String value, Index index) {
+    return newField(random, name, value, Store.NO, index);
+  }
+  
+  public static Field newField(Random random, String name, String value, Store store, Index index) {
+    return newField(random, name, value, store, index, TermVector.NO);
+  }
+  
+  public static Field newField(Random random, String name, String value, Store store, Index index, TermVector tv) {
+    if (!index.isIndexed())
+      return new Field(name, value, store, index);
+    
+    if (!store.isStored() && random.nextBoolean())
+      store = Store.YES; // randomly store it
+    
+    tv = randomTVSetting(random, tv);
+    
+    return new Field(name, value, store, index, tv);
+  }
+  
+  static final TermVector tvSettings[] = { 
+    TermVector.NO, TermVector.YES, TermVector.WITH_OFFSETS, 
+    TermVector.WITH_POSITIONS, TermVector.WITH_POSITIONS_OFFSETS 
+  };
+  
+  private static TermVector randomTVSetting(Random random, TermVector minimum) {
+    switch(minimum) {
+      case NO: return tvSettings[_TestUtil.nextInt(random, 0, tvSettings.length-1)];
+      case YES: return tvSettings[_TestUtil.nextInt(random, 1, tvSettings.length-1)];
+      case WITH_OFFSETS: return random.nextBoolean() ? TermVector.WITH_OFFSETS 
+          : TermVector.WITH_POSITIONS_OFFSETS;
+      case WITH_POSITIONS: return random.nextBoolean() ? TermVector.WITH_POSITIONS 
+          : TermVector.WITH_POSITIONS_OFFSETS;
+      default: return TermVector.WITH_POSITIONS_OFFSETS;
+    }
+  }
+  
+  /** return a random Locale from the available locales on the system */
+  public static Locale randomLocale(Random random) {
+    Locale locales[] = Locale.getAvailableLocales();
+    return locales[random.nextInt(locales.length)];
+  }
+  
+  /** return a random TimeZone from the available timezones on the system */
+  public static TimeZone randomTimeZone(Random random) {
+    String tzIds[] = TimeZone.getAvailableIDs();
+    return TimeZone.getTimeZone(tzIds[random.nextInt(tzIds.length)]);
+  }
+  
+  /** return a Locale object equivalent to its programmatic name */
+  public static Locale localeForName(String localeName) {
+    String elements[] = localeName.split("\\_");
+    switch(elements.length) {
+      case 3: return new Locale(elements[0], elements[1], elements[2]);
+      case 2: return new Locale(elements[0], elements[1]);
+      case 1: return new Locale(elements[0]);
+      default: throw new IllegalArgumentException("Invalid Locale: " + localeName);
+    }
+  }
+
+  private static final String FS_DIRECTORIES[] = {
+    "SimpleFSDirectory",
+    "NIOFSDirectory",
+    "MMapDirectory"
+  };
+
+  private static final String CORE_DIRECTORIES[] = {
+    "RAMDirectory",
+    FS_DIRECTORIES[0], FS_DIRECTORIES[1], FS_DIRECTORIES[2]
+  };
+  
+  public static String randomDirectory(Random random) {
+    if (random.nextInt(10) == 0) {
+      return CORE_DIRECTORIES[random.nextInt(CORE_DIRECTORIES.length)];
+    } else {
+      return "RAMDirectory";
+    }
+  }
+
+  private static Directory newFSDirectoryImpl(
+      Class<? extends FSDirectory> clazz, File file, LockFactory lockFactory)
+      throws IOException {
+    try {
+      // Assuming every FSDirectory has a ctor(File), but not all may take a
+      // LockFactory too, so setting it afterwards.
+      Constructor<? extends FSDirectory> ctor = clazz.getConstructor(File.class);
+      FSDirectory d = ctor.newInstance(file);
+      if (lockFactory != null) {
+        d.setLockFactory(lockFactory);
+      }
+      return d;
+    } catch (Exception e) {
+      return FSDirectory.open(file);
+    }
+  }
+  
+  static Directory newDirectoryImpl(Random random, String clazzName) {
+    if (clazzName.equals("random"))
+      clazzName = randomDirectory(random);
+    if (clazzName.indexOf(".") == -1) // if not fully qualified, assume .store
+      clazzName = "org.apache.lucene.store." + clazzName;
+    try {
+      final Class<? extends Directory> clazz = Class.forName(clazzName).asSubclass(Directory.class);
+      // If it is a FSDirectory type, try its ctor(File)
+      if (FSDirectory.class.isAssignableFrom(clazz)) {
+        final File tmpFile = File.createTempFile("test", "tmp", TEMP_DIR);
+        tmpFile.delete();
+        tmpFile.mkdir();
+        return newFSDirectoryImpl(clazz.asSubclass(FSDirectory.class), tmpFile, null);
+      }
+
+      // try empty ctor
+      return clazz.newInstance();
+    } catch (Exception e) {
+      throw new RuntimeException(e);
+    } 
+  }
+  
+  /** create a new searcher over the reader */
+  public static IndexSearcher newSearcher(IndexReader r) throws IOException {
+    if (random.nextBoolean()) {
+      return new IndexSearcher(r);
+    } else {
+      int threads = 0;
+      final ExecutorService ex = (random.nextBoolean()) ? null 
+          : Executors.newFixedThreadPool(threads = _TestUtil.nextInt(random, 1, 8), 
+                      new NamedThreadFactory("LuceneTestCase"));
+      if (ex != null && VERBOSE) {
+        System.out.println("NOTE: newSearcher using ExecutorService with " + threads + " threads");
+      }
+      return new IndexSearcher(r.getTopReaderContext(), ex) {
+        @Override
+        public void close() throws IOException {
+          super.close();
+          if (ex != null) {
+            ex.shutdown();
+            try {
+              ex.awaitTermination(1000, TimeUnit.MILLISECONDS);
+            } catch (InterruptedException e) {
+              e.printStackTrace();
+            }
+          }
+        }
+      };
+    }
+  }
+
+  public String getName() {
+    return this.name;
+  }
+  
+  /** Gets a resource from the classpath as {@link File}. This method should only be used,
+   * if a real file is needed. To get a stream, code should prefer
+   * {@link Class#getResourceAsStream} using {@code this.getClass()}.
+   */
+  
+  protected File getDataFile(String name) throws IOException {
+    try {
+      return new File(this.getClass().getResource(name).toURI());
+    } catch (Exception e) {
+      throw new IOException("Cannot find resource: " + name);
+    }
+  }
+
+  // We get here from InterceptTestCaseEvents on the 'failed' event....
+  public void reportAdditionalFailureInfo() {
+    System.err.println("NOTE: reproduce with: ant test -Dtestcase=" + getClass().getSimpleName() 
+        + " -Dtestmethod=" + getName() + " -Dtests.seed=" + new TwoLongs(staticSeed, seed)
+        + reproduceWithExtraParams());
+  }
+  
+  // extra params that were overridden needed to reproduce the command
+  private String reproduceWithExtraParams() {
+    StringBuilder sb = new StringBuilder();
+    if (!TEST_CODEC.equals("randomPerField")) sb.append(" -Dtests.codec=").append(TEST_CODEC);
+    if (!TEST_LOCALE.equals("random")) sb.append(" -Dtests.locale=").append(TEST_LOCALE);
+    if (!TEST_TIMEZONE.equals("random")) sb.append(" -Dtests.timezone=").append(TEST_TIMEZONE);
+    if (!TEST_DIRECTORY.equals("random")) sb.append(" -Dtests.directory=").append(TEST_DIRECTORY);
+    if (RANDOM_MULTIPLIER > 1) sb.append(" -Dtests.multiplier=").append(RANDOM_MULTIPLIER);
+    return sb.toString();
+  }
+
+  // recorded seed: for beforeClass
+  private static long staticSeed;
+  // seed for individual test methods, changed in @before
+  private long seed;
+  
+  private static final Random seedRand = new Random();
+  protected static final Random random = new Random(0);
+
+  private String name = "<unknown>";
+  
+  /**
+   * Annotation for tests that should only be run during nightly builds.
+   */
+  @Documented
+  @Inherited
+  @Retention(RetentionPolicy.RUNTIME)
+  public @interface Nightly {}
+  
+  /** optionally filters the tests to be run by TEST_METHOD */
+  public static class LuceneTestCaseRunner extends BlockJUnit4ClassRunner {
+    private List<FrameworkMethod> testMethods;
+
+    @Override
+    protected List<FrameworkMethod> computeTestMethods() {
+      if (testMethods != null)
+        return testMethods;
+      testClassesRun.add(getTestClass().getJavaClass().getSimpleName());
+      testMethods = new ArrayList<FrameworkMethod>();
+      for (Method m : getTestClass().getJavaClass().getMethods()) {
+        // check if the current test's class has methods annotated with @Ignore
+        final Ignore ignored = m.getAnnotation(Ignore.class);
+        if (ignored != null && !m.getName().equals("alwaysIgnoredTestMethod")) {
+          System.err.println("NOTE: Ignoring test method '" + m.getName() + "': " + ignored.value());
+        }
+        // add methods starting with "test"
+        final int mod = m.getModifiers();
+        if (m.getAnnotation(Test.class) != null ||
+            (m.getName().startsWith("test") &&
+            !Modifier.isAbstract(mod) &&
+            m.getParameterTypes().length == 0 &&
+            m.getReturnType() == Void.TYPE))
+        {
+          if (Modifier.isStatic(mod))
+            throw new RuntimeException("Test methods must not be static.");
+          testMethods.add(new FrameworkMethod(m));
+        }
+      }
+      
+      if (testMethods.isEmpty()) {
+        throw new RuntimeException("No runnable methods!");
+      }
+      
+      if (TEST_NIGHTLY == false) {
+        if (getTestClass().getJavaClass().isAnnotationPresent(Nightly.class)) {
+          /* the test class is annotated with nightly, remove all methods */
+          String className = getTestClass().getJavaClass().getSimpleName();
+          System.err.println("NOTE: Ignoring nightly-only test class '" + className + "'");
+          testMethods.clear();
+        } else {
+          /* remove all nightly-only methods */
+          for (int i = 0; i < testMethods.size(); i++) {
+            final FrameworkMethod m = testMethods.get(i);
+            if (m.getAnnotation(Nightly.class) != null) {
+              System.err.println("NOTE: Ignoring nightly-only test method '" + m.getName() + "'");
+              testMethods.remove(i--);
+            }
+          }
+        }
+        /* dodge a possible "no-runnable methods" exception by adding a fake ignored test */
+        if (testMethods.isEmpty()) {
+          try {
+            testMethods.add(new FrameworkMethod(LuceneTestCase.class.getMethod("alwaysIgnoredTestMethod")));
+          } catch (Exception e) { throw new RuntimeException(e); }
+        }
+      }
+      return testMethods;
+    }
+
+    @Override
+    protected void runChild(FrameworkMethod arg0, RunNotifier arg1) {
+      if (VERBOSE) {
+        System.out.println("\nNOTE: running test " + arg0.getName());
+      }
+      for (int i = 0; i < TEST_ITER; i++) {
+        if (VERBOSE && TEST_ITER > 1) {
+          System.out.println("\nNOTE: running iter=" + (1+i) + " of " + TEST_ITER);
+        }
+        super.runChild(arg0, arg1);
+      }
+    }
+
+    public LuceneTestCaseRunner(Class<?> clazz) throws InitializationError {
+      super(clazz);
+      Filter f = new Filter() {
+
+        @Override
+        public String describe() { return "filters according to TEST_METHOD"; }
+
+        @Override
+        public boolean shouldRun(Description d) {
+          return TEST_METHOD == null || d.getMethodName().equals(TEST_METHOD);
+        }     
+      };
+      
+      try {
+        f.apply(this);
+      } catch (NoTestsRemainException e) {
+        throw new RuntimeException(e);
+      }
+    }
+  }
+  
+  private static class RandomCodecProvider extends CodecProvider {
+    private List<Codec> knownCodecs = new ArrayList<Codec>();
+    private Map<String,Codec> previousMappings = new HashMap<String,Codec>();
+    private final int perFieldSeed;
+    
+    RandomCodecProvider(Random random) {
+      this.perFieldSeed = random.nextInt();
+      register(new StandardCodec());
+      register(new PreFlexCodec());
+      register(new PulsingCodec(1));
+      register(new SimpleTextCodec());
+      Collections.shuffle(knownCodecs, random);
+    }
+
+    @Override
+    public synchronized void register(Codec codec) {
+      if (!codec.name.equals("PreFlex"))
+        knownCodecs.add(codec);
+      super.register(codec);
+    }
+
+    @Override
+    public synchronized void unregister(Codec codec) {
+      knownCodecs.remove(codec);
+      super.unregister(codec);
+    }
+
+    @Override
+    public synchronized String getFieldCodec(String name) {
+      Codec codec = previousMappings.get(name);
+      if (codec == null) {
+        codec = knownCodecs.get(Math.abs(perFieldSeed ^ name.hashCode()) % knownCodecs.size());
+        previousMappings.put(name, codec);
+      }
+      return codec.name;
+    }
+    
+    @Override
+    public String toString() {
+      return "RandomCodecProvider: " + previousMappings.toString();
+    }
+  }
+  
+  @Ignore("just a hack")
+  public final void alwaysIgnoredTestMethod() {}
+}
diff --git a/lucene/src/test-framework/org/apache/lucene/util/_TestUtil.java b/lucene/src/test-framework/org/apache/lucene/util/_TestUtil.java
new file mode 100644
index 0000000..1ad0380
--- /dev/null
+++ b/lucene/src/test-framework/org/apache/lucene/util/_TestUtil.java
@@ -0,0 +1,308 @@
+package org.apache.lucene.util;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.BufferedOutputStream;
+import java.io.ByteArrayOutputStream;
+import java.io.File;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.OutputStream;
+import java.io.PrintStream;
+import java.util.Enumeration;
+import java.util.Random;
+import java.util.Map;
+import java.util.HashMap;
+import java.util.zip.ZipEntry;
+import java.util.zip.ZipFile;
+
+import org.junit.Assert;
+
+import org.apache.lucene.index.CheckIndex;
+import org.apache.lucene.index.ConcurrentMergeScheduler;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.LogMergePolicy;
+import org.apache.lucene.index.MergeScheduler;
+import org.apache.lucene.index.codecs.Codec;
+import org.apache.lucene.index.codecs.CodecProvider;
+import org.apache.lucene.store.Directory;
+
+public class _TestUtil {
+
+  /** Returns temp dir, containing String arg in its name;
+   *  does not create the directory. */
+  public static File getTempDir(String desc) {
+    return new File(LuceneTestCase.TEMP_DIR, desc + "." + new Random().nextLong());
+  }
+
+  public static void rmDir(File dir) throws IOException {
+    if (dir.exists()) {
+      for (File f : dir.listFiles()) {
+        if (f.isDirectory()) {
+          rmDir(f);
+        } else {
+          if (!f.delete()) {
+            throw new IOException("could not delete " + f);
+          }
+        }
+      }
+      if (!dir.delete()) {
+        throw new IOException("could not delete " + dir);
+      }
+    }
+  }
+
+  /** 
+   * Convenience method: Unzip zipName + ".zip" under destDir, removing destDir first 
+   */
+  public static void unzip(File zipName, File destDir) throws IOException {
+    
+    ZipFile zipFile = new ZipFile(zipName);
+    
+    Enumeration<? extends ZipEntry> entries = zipFile.entries();
+    
+    rmDir(destDir);
+    
+    destDir.mkdir();
+    
+    while (entries.hasMoreElements()) {
+      ZipEntry entry = entries.nextElement();
+      
+      InputStream in = zipFile.getInputStream(entry);
+      File targetFile = new File(destDir, entry.getName());
+      if (entry.isDirectory()) {
+        // allow unzipping with directory structure
+        targetFile.mkdirs();
+      } else {
+        if (targetFile.getParentFile()!=null) {
+          // be on the safe side: do not rely on that directories are always extracted
+          // before their children (although this makes sense, but is it guaranteed?)
+          targetFile.getParentFile().mkdirs();   
+        }
+        OutputStream out = new BufferedOutputStream(new FileOutputStream(targetFile));
+        
+        byte[] buffer = new byte[8192];
+        int len;
+        while((len = in.read(buffer)) >= 0) {
+          out.write(buffer, 0, len);
+        }
+        
+        in.close();
+        out.close();
+      }
+    }
+    
+    zipFile.close();
+  }
+  
+  public static void syncConcurrentMerges(IndexWriter writer) {
+    syncConcurrentMerges(writer.getConfig().getMergeScheduler());
+  }
+
+  public static void syncConcurrentMerges(MergeScheduler ms) {
+    if (ms instanceof ConcurrentMergeScheduler)
+      ((ConcurrentMergeScheduler) ms).sync();
+  }
+
+  /** This runs the CheckIndex tool on the index in.  If any
+   *  issues are hit, a RuntimeException is thrown; else,
+   *  true is returned. */
+  public static CheckIndex.Status checkIndex(Directory dir) throws IOException {
+    return checkIndex(dir, CodecProvider.getDefault());
+  }
+  
+  /** This runs the CheckIndex tool on the index in.  If any
+   *  issues are hit, a RuntimeException is thrown; else,
+   *  true is returned. */
+  public static CheckIndex.Status checkIndex(Directory dir, CodecProvider codecs) throws IOException {
+    ByteArrayOutputStream bos = new ByteArrayOutputStream(1024);
+    CheckIndex checker = new CheckIndex(dir);
+    checker.setInfoStream(new PrintStream(bos));
+    CheckIndex.Status indexStatus = checker.checkIndex(null, codecs);
+    if (indexStatus == null || indexStatus.clean == false) {
+      System.out.println("CheckIndex failed");
+      System.out.println(bos.toString());
+      throw new RuntimeException("CheckIndex failed");
+    } else {
+      return indexStatus;
+    }
+  }
+
+  /** start and end are BOTH inclusive */
+  public static int nextInt(Random r, int start, int end) {
+    return start + r.nextInt(end-start+1);
+  }
+
+  /** Returns random string, including full unicode range. */
+  public static String randomUnicodeString(Random r) {
+    return randomUnicodeString(r, 20);
+  }
+
+  public static String randomUnicodeString(Random r, int maxLength) {
+    final int end = r.nextInt(maxLength);
+    if (end == 0) {
+      // allow 0 length
+      return "";
+    }
+    final char[] buffer = new char[end];
+    for (int i = 0; i < end; i++) {
+      int t = r.nextInt(5);
+
+      if (0 == t && i < end - 1) {
+        // Make a surrogate pair
+        // High surrogate
+        buffer[i++] = (char) nextInt(r, 0xd800, 0xdbff);
+        // Low surrogate
+        buffer[i] = (char) nextInt(r, 0xdc00, 0xdfff);
+      }
+      else if (t <= 1) buffer[i] = (char) r.nextInt(0x80);
+      else if (2 == t) buffer[i] = (char) nextInt(r, 0x80, 0x800);
+      else if (3 == t) buffer[i] = (char) nextInt(r, 0x800, 0xd7ff);
+      else if (4 == t) buffer[i] = (char) nextInt(r, 0xe000, 0xffff);
+    }
+    return new String(buffer, 0, end);
+  }
+
+  private static final int[] blockStarts = {
+    0x0000, 0x0080, 0x0100, 0x0180, 0x0250, 0x02B0, 0x0300, 0x0370, 0x0400, 
+    0x0500, 0x0530, 0x0590, 0x0600, 0x0700, 0x0750, 0x0780, 0x07C0, 0x0800, 
+    0x0900, 0x0980, 0x0A00, 0x0A80, 0x0B00, 0x0B80, 0x0C00, 0x0C80, 0x0D00, 
+    0x0D80, 0x0E00, 0x0E80, 0x0F00, 0x1000, 0x10A0, 0x1100, 0x1200, 0x1380, 
+    0x13A0, 0x1400, 0x1680, 0x16A0, 0x1700, 0x1720, 0x1740, 0x1760, 0x1780, 
+    0x1800, 0x18B0, 0x1900, 0x1950, 0x1980, 0x19E0, 0x1A00, 0x1A20, 0x1B00, 
+    0x1B80, 0x1C00, 0x1C50, 0x1CD0, 0x1D00, 0x1D80, 0x1DC0, 0x1E00, 0x1F00, 
+    0x2000, 0x2070, 0x20A0, 0x20D0, 0x2100, 0x2150, 0x2190, 0x2200, 0x2300, 
+    0x2400, 0x2440, 0x2460, 0x2500, 0x2580, 0x25A0, 0x2600, 0x2700, 0x27C0, 
+    0x27F0, 0x2800, 0x2900, 0x2980, 0x2A00, 0x2B00, 0x2C00, 0x2C60, 0x2C80, 
+    0x2D00, 0x2D30, 0x2D80, 0x2DE0, 0x2E00, 0x2E80, 0x2F00, 0x2FF0, 0x3000, 
+    0x3040, 0x30A0, 0x3100, 0x3130, 0x3190, 0x31A0, 0x31C0, 0x31F0, 0x3200, 
+    0x3300, 0x3400, 0x4DC0, 0x4E00, 0xA000, 0xA490, 0xA4D0, 0xA500, 0xA640, 
+    0xA6A0, 0xA700, 0xA720, 0xA800, 0xA830, 0xA840, 0xA880, 0xA8E0, 0xA900, 
+    0xA930, 0xA960, 0xA980, 0xAA00, 0xAA60, 0xAA80, 0xABC0, 0xAC00, 0xD7B0, 
+    0xE000, 0xF900, 0xFB00, 0xFB50, 0xFE00, 0xFE10, 
+    0xFE20, 0xFE30, 0xFE50, 0xFE70, 0xFF00, 0xFFF0, 
+    0x10000, 0x10080, 0x10100, 0x10140, 0x10190, 0x101D0, 0x10280, 0x102A0, 
+    0x10300, 0x10330, 0x10380, 0x103A0, 0x10400, 0x10450, 0x10480, 0x10800, 
+    0x10840, 0x10900, 0x10920, 0x10A00, 0x10A60, 0x10B00, 0x10B40, 0x10B60, 
+    0x10C00, 0x10E60, 0x11080, 0x12000, 0x12400, 0x13000, 0x1D000, 0x1D100, 
+    0x1D200, 0x1D300, 0x1D360, 0x1D400, 0x1F000, 0x1F030, 0x1F100, 0x1F200, 
+    0x20000, 0x2A700, 0x2F800, 0xE0000, 0xE0100, 0xF0000, 0x100000
+  };
+  
+  private static final int[] blockEnds = {
+    0x007F, 0x00FF, 0x017F, 0x024F, 0x02AF, 0x02FF, 0x036F, 0x03FF, 0x04FF, 
+    0x052F, 0x058F, 0x05FF, 0x06FF, 0x074F, 0x077F, 0x07BF, 0x07FF, 0x083F, 
+    0x097F, 0x09FF, 0x0A7F, 0x0AFF, 0x0B7F, 0x0BFF, 0x0C7F, 0x0CFF, 0x0D7F, 
+    0x0DFF, 0x0E7F, 0x0EFF, 0x0FFF, 0x109F, 0x10FF, 0x11FF, 0x137F, 0x139F, 
+    0x13FF, 0x167F, 0x169F, 0x16FF, 0x171F, 0x173F, 0x175F, 0x177F, 0x17FF, 
+    0x18AF, 0x18FF, 0x194F, 0x197F, 0x19DF, 0x19FF, 0x1A1F, 0x1AAF, 0x1B7F, 
+    0x1BBF, 0x1C4F, 0x1C7F, 0x1CFF, 0x1D7F, 0x1DBF, 0x1DFF, 0x1EFF, 0x1FFF, 
+    0x206F, 0x209F, 0x20CF, 0x20FF, 0x214F, 0x218F, 0x21FF, 0x22FF, 0x23FF, 
+    0x243F, 0x245F, 0x24FF, 0x257F, 0x259F, 0x25FF, 0x26FF, 0x27BF, 0x27EF, 
+    0x27FF, 0x28FF, 0x297F, 0x29FF, 0x2AFF, 0x2BFF, 0x2C5F, 0x2C7F, 0x2CFF, 
+    0x2D2F, 0x2D7F, 0x2DDF, 0x2DFF, 0x2E7F, 0x2EFF, 0x2FDF, 0x2FFF, 0x303F, 
+    0x309F, 0x30FF, 0x312F, 0x318F, 0x319F, 0x31BF, 0x31EF, 0x31FF, 0x32FF, 
+    0x33FF, 0x4DBF, 0x4DFF, 0x9FFF, 0xA48F, 0xA4CF, 0xA4FF, 0xA63F, 0xA69F, 
+    0xA6FF, 0xA71F, 0xA7FF, 0xA82F, 0xA83F, 0xA87F, 0xA8DF, 0xA8FF, 0xA92F, 
+    0xA95F, 0xA97F, 0xA9DF, 0xAA5F, 0xAA7F, 0xAADF, 0xABFF, 0xD7AF, 0xD7FF, 
+    0xF8FF, 0xFAFF, 0xFB4F, 0xFDFF, 0xFE0F, 0xFE1F, 
+    0xFE2F, 0xFE4F, 0xFE6F, 0xFEFF, 0xFFEF, 0xFFFF, 
+    0x1007F, 0x100FF, 0x1013F, 0x1018F, 0x101CF, 0x101FF, 0x1029F, 0x102DF, 
+    0x1032F, 0x1034F, 0x1039F, 0x103DF, 0x1044F, 0x1047F, 0x104AF, 0x1083F, 
+    0x1085F, 0x1091F, 0x1093F, 0x10A5F, 0x10A7F, 0x10B3F, 0x10B5F, 0x10B7F, 
+    0x10C4F, 0x10E7F, 0x110CF, 0x123FF, 0x1247F, 0x1342F, 0x1D0FF, 0x1D1FF, 
+    0x1D24F, 0x1D35F, 0x1D37F, 0x1D7FF, 0x1F02F, 0x1F09F, 0x1F1FF, 0x1F2FF, 
+    0x2A6DF, 0x2B73F, 0x2FA1F, 0xE007F, 0xE01EF, 0xFFFFF, 0x10FFFF
+  };
+  
+  /** Returns random string, all codepoints within the same unicode block. */
+  public static String randomRealisticUnicodeString(Random r) {
+    return randomRealisticUnicodeString(r, 20);
+  }
+  
+  /** Returns random string, all codepoints within the same unicode block. */
+  public static String randomRealisticUnicodeString(Random r, int maxLength) {
+    final int end = r.nextInt(maxLength);
+    final int block = r.nextInt(blockStarts.length);
+    StringBuilder sb = new StringBuilder();
+    for (int i = 0; i < end; i++)
+      sb.appendCodePoint(nextInt(r, blockStarts[block], blockEnds[block]));
+    return sb.toString();
+  }
+
+  public static CodecProvider alwaysCodec(final Codec c) {
+    CodecProvider p = new CodecProvider() {
+      @Override
+      public Codec lookup(String name) {
+        // can't do this until we fix PreFlexRW to not
+        //impersonate PreFlex:
+        if (name.equals(c.name)) {
+          return c;
+        } else {
+          return CodecProvider.getDefault().lookup(name);
+        }
+      }
+    };
+    p.setDefaultFieldCodec(c.name);
+    return p;
+  }
+
+  /** Return a CodecProvider that can read any of the
+   *  default codecs, but always writes in the specified
+   *  codec. */
+  public static CodecProvider alwaysCodec(final String codec) {
+    return alwaysCodec(CodecProvider.getDefault().lookup(codec));
+  }
+
+  public static boolean anyFilesExceptWriteLock(Directory dir) throws IOException {
+    String[] files = dir.listAll();
+    if (files.length > 1 || (files.length == 1 && !files[0].equals("write.lock"))) {
+      return true;
+    } else {
+      return false;
+    }
+  }
+
+  // just tries to configure things to keep the open file
+  // count lowish
+  public static void reduceOpenFiles(IndexWriter w) {
+    // keep number of open files lowish
+    LogMergePolicy lmp = (LogMergePolicy) w.getConfig().getMergePolicy();
+    lmp.setMergeFactor(Math.min(5, lmp.getMergeFactor()));
+
+    MergeScheduler ms = w.getConfig().getMergeScheduler();
+    if (ms instanceof ConcurrentMergeScheduler) {
+      ((ConcurrentMergeScheduler) ms).setMaxThreadCount(2);
+      ((ConcurrentMergeScheduler) ms).setMaxMergeCount(3);
+    }
+  }
+
+  /** Checks some basic behaviour of an AttributeImpl
+   * @param reflectedValues contains a map with "AttributeClass#key" as values
+   */
+  public static <T> void assertAttributeReflection(final AttributeImpl att, Map<String,T> reflectedValues) {
+    final Map<String,Object> map = new HashMap<String,Object>();
+    att.reflectWith(new AttributeReflector() {
+      public void reflect(Class<? extends Attribute> attClass, String key, Object value) {
+        map.put(attClass.getName() + '#' + key, value);
+      }
+    });
+    Assert.assertEquals("Reflection does not produce same map", reflectedValues, map);
+  }
+}
diff --git a/lucene/src/test-framework/org/apache/lucene/util/automaton/AutomatonTestUtil.java b/lucene/src/test-framework/org/apache/lucene/util/automaton/AutomatonTestUtil.java
new file mode 100644
index 0000000..6b9ef6e
--- /dev/null
+++ b/lucene/src/test-framework/org/apache/lucene/util/automaton/AutomatonTestUtil.java
@@ -0,0 +1,376 @@
+package org.apache.lucene.util.automaton;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.IdentityHashMap;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Map;
+import java.util.Random;
+import java.util.Set;
+
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.UnicodeUtil;
+import org.apache.lucene.util._TestUtil;
+
+public class AutomatonTestUtil {
+  /** Returns random string, including full unicode range. */
+  public static String randomRegexp(Random r) {
+    while (true) {
+      String regexp = randomRegexpString(r);
+      // we will also generate some undefined unicode queries
+      if (!UnicodeUtil.validUTF16String(regexp))
+        continue;
+      try {
+        new RegExp(regexp, RegExp.NONE);
+        return regexp;
+      } catch (Exception e) {}
+    }
+  }
+
+  private static String randomRegexpString(Random r) {
+    final int end = r.nextInt(20);
+    if (end == 0) {
+      // allow 0 length
+      return "";
+    }
+    final char[] buffer = new char[end];
+    for (int i = 0; i < end; i++) {
+      int t = r.nextInt(15);
+      if (0 == t && i < end - 1) {
+        // Make a surrogate pair
+        // High surrogate
+        buffer[i++] = (char) _TestUtil.nextInt(r, 0xd800, 0xdbff);
+        // Low surrogate
+        buffer[i] = (char) _TestUtil.nextInt(r, 0xdc00, 0xdfff);
+      }
+      else if (t <= 1) buffer[i] = (char) r.nextInt(0x80);
+      else if (2 == t) buffer[i] = (char) _TestUtil.nextInt(r, 0x80, 0x800);
+      else if (3 == t) buffer[i] = (char) _TestUtil.nextInt(r, 0x800, 0xd7ff);
+      else if (4 == t) buffer[i] = (char) _TestUtil.nextInt(r, 0xe000, 0xffff);
+      else if (5 == t) buffer[i] = '.';
+      else if (6 == t) buffer[i] = '?';
+      else if (7 == t) buffer[i] = '*';
+      else if (8 == t) buffer[i] = '+';
+      else if (9 == t) buffer[i] = '(';
+      else if (10 == t) buffer[i] = ')';
+      else if (11 == t) buffer[i] = '-';
+      else if (12 == t) buffer[i] = '[';
+      else if (13 == t) buffer[i] = ']';
+      else if (14 == t) buffer[i] = '|';
+    }
+    return new String(buffer, 0, end);
+  }
+  
+  // picks a random int code point, avoiding surrogates;
+  // throws IllegalArgumentException if this transition only
+  // accepts surrogates
+  private static int getRandomCodePoint(final Random r, final Transition t) {
+    final int code;
+    if (t.max < UnicodeUtil.UNI_SUR_HIGH_START ||
+        t.min > UnicodeUtil.UNI_SUR_HIGH_END) {
+      // easy: entire range is before or after surrogates
+      code = t.min+r.nextInt(t.max-t.min+1);
+    } else if (t.min >= UnicodeUtil.UNI_SUR_HIGH_START) {
+      if (t.max > UnicodeUtil.UNI_SUR_LOW_END) {
+        // after surrogates
+        code = 1+UnicodeUtil.UNI_SUR_LOW_END+r.nextInt(t.max-UnicodeUtil.UNI_SUR_LOW_END);
+      } else {
+        throw new IllegalArgumentException("transition accepts only surrogates: " + t);
+      }
+    } else if (t.max <= UnicodeUtil.UNI_SUR_LOW_END) {
+      if (t.min < UnicodeUtil.UNI_SUR_HIGH_START) {
+        // before surrogates
+        code = t.min + r.nextInt(UnicodeUtil.UNI_SUR_HIGH_START - t.min);
+      } else {
+        throw new IllegalArgumentException("transition accepts only surrogates: " + t);
+      }
+    } else {
+      // range includes all surrogates
+      int gap1 = UnicodeUtil.UNI_SUR_HIGH_START - t.min;
+      int gap2 = t.max - UnicodeUtil.UNI_SUR_LOW_END;
+      int c = r.nextInt(gap1+gap2);
+      if (c < gap1) {
+        code = t.min + c;
+      } else {
+        code = UnicodeUtil.UNI_SUR_LOW_END + c - gap1 + 1;
+      }
+    }
+
+    assert code >= t.min && code <= t.max && (code < UnicodeUtil.UNI_SUR_HIGH_START || code > UnicodeUtil.UNI_SUR_LOW_END):
+      "code=" + code + " min=" + t.min + " max=" + t.max;
+    return code;
+  }
+
+  public static class RandomAcceptedStrings {
+
+    private final Map<Transition,Boolean> leadsToAccept;
+    private final Automaton a;
+
+    private static class ArrivingTransition {
+      final State from;
+      final Transition t;
+      public ArrivingTransition(State from, Transition t) {
+        this.from = from;
+        this.t = t;
+      }
+    }
+
+    public RandomAcceptedStrings(Automaton a) {
+      this.a = a;
+      if (a.isSingleton()) {
+        leadsToAccept = null;
+        return;
+      }
+
+      // must use IdentityHashmap because two Transitions w/
+      // different start nodes can be considered the same
+      leadsToAccept = new IdentityHashMap<Transition,Boolean>();
+      final Map<State,List<ArrivingTransition>> allArriving = new HashMap<State,List<ArrivingTransition>>();
+
+      final LinkedList<State> q = new LinkedList<State>();
+      final Set<State> seen = new HashSet<State>();
+
+      // reverse map the transitions, so we can quickly look
+      // up all arriving transitions to a given state
+      for(State s: a.getNumberedStates()) {
+        for(int i=0;i<s.numTransitions;i++) {
+          final Transition t = s.transitionsArray[i];
+          List<ArrivingTransition> tl = allArriving.get(t.to);
+          if (tl == null) {
+            tl = new ArrayList<ArrivingTransition>();
+            allArriving.put(t.to, tl);
+          }
+          tl.add(new ArrivingTransition(s, t));
+        }
+        if (s.accept) {
+          q.add(s);
+          seen.add(s);
+        }
+      }
+
+      // Breadth-first search, from accept states,
+      // backwards:
+      while(!q.isEmpty()) {
+        final State s = q.removeFirst();
+        List<ArrivingTransition> arriving = allArriving.get(s);
+        if (arriving != null) {
+          for(ArrivingTransition at : arriving) {
+            final State from = at.from;
+            if (!seen.contains(from)) {
+              q.add(from);
+              seen.add(from);
+              leadsToAccept.put(at.t, Boolean.TRUE);
+            }
+          }
+        }
+      }
+    }
+
+    public int[] getRandomAcceptedString(Random r) {
+
+      final List<Integer> soFar = new ArrayList<Integer>();
+      if (a.isSingleton()) {
+        // accepts only one
+        final String s = a.singleton;
+      
+        int charUpto = 0;
+        while(charUpto < s.length()) {
+          final int cp = s.codePointAt(charUpto);
+          charUpto += Character.charCount(cp);
+          soFar.add(cp);
+        }
+      } else {
+
+        State s = a.initial;
+
+        while(true) {
+      
+          if (s.accept) {
+            if (s.numTransitions == 0) {
+              // stop now
+              break;
+            } else {
+              if (r.nextBoolean()) {
+                break;
+              }
+            }
+          }
+
+          if (s.numTransitions == 0) {
+            throw new RuntimeException("this automaton has dead states");
+          }
+
+          boolean cheat = r.nextBoolean();
+
+          final Transition t;
+          if (cheat) {
+            // pick a transition that we know is the fastest
+            // path to an accept state
+            List<Transition> toAccept = new ArrayList<Transition>();
+            for(int i=0;i<s.numTransitions;i++) {
+              final Transition t0 = s.transitionsArray[i];
+              if (leadsToAccept.containsKey(t0)) {
+                toAccept.add(t0);
+              }
+            }
+            if (toAccept.size() == 0) {
+              // this is OK -- it means we jumped into a cycle
+              t = s.transitionsArray[r.nextInt(s.numTransitions)];
+            } else {
+              t = toAccept.get(r.nextInt(toAccept.size()));
+            }
+          } else {
+            t = s.transitionsArray[r.nextInt(s.numTransitions)];
+          }
+          soFar.add(getRandomCodePoint(r, t));
+          s = t.to;
+        }
+      }
+
+      return ArrayUtil.toIntArray(soFar);
+    }
+  }
+  
+  /** return a random NFA/DFA for testing */
+  public static Automaton randomAutomaton(Random random) {
+    // get two random Automata from regexps
+    Automaton a1 = new RegExp(AutomatonTestUtil.randomRegexp(random), RegExp.NONE).toAutomaton();
+    if (random.nextBoolean())
+      a1 = BasicOperations.complement(a1);
+    
+    Automaton a2 = new RegExp(AutomatonTestUtil.randomRegexp(random), RegExp.NONE).toAutomaton();
+    if (random.nextBoolean()) 
+      a2 = BasicOperations.complement(a2);
+    
+    // combine them in random ways
+    switch(random.nextInt(4)) {
+      case 0: return BasicOperations.concatenate(a1, a2);
+      case 1: return BasicOperations.union(a1, a2);
+      case 2: return BasicOperations.intersection(a1, a2);
+      default: return BasicOperations.minus(a1, a2);
+    }
+  }
+  
+  /** 
+   * below are original, unoptimized implementations of DFA operations for testing.
+   * These are from brics automaton, full license (BSD) below:
+   */
+  
+  /*
+   * dk.brics.automaton
+   * 
+   * Copyright (c) 2001-2009 Anders Moeller
+   * All rights reserved.
+   * 
+   * Redistribution and use in source and binary forms, with or without
+   * modification, are permitted provided that the following conditions
+   * are met:
+   * 1. Redistributions of source code must retain the above copyright
+   *    notice, this list of conditions and the following disclaimer.
+   * 2. Redistributions in binary form must reproduce the above copyright
+   *    notice, this list of conditions and the following disclaimer in the
+   *    documentation and/or other materials provided with the distribution.
+   * 3. The name of the author may not be used to endorse or promote products
+   *    derived from this software without specific prior written permission.
+   * 
+   * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+   * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+   * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+   * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+   * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+   * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+   * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+   * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+   * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+   * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+   */
+
+  /**
+   * Simple, original brics implementation of Brzozowski minimize()
+   */
+  public static void minimizeSimple(Automaton a) {
+    if (a.isSingleton())
+      return;
+    determinizeSimple(a, SpecialOperations.reverse(a));
+    determinizeSimple(a, SpecialOperations.reverse(a));
+  }
+  
+  /**
+   * Simple, original brics implementation of determinize()
+   */
+  public static void determinizeSimple(Automaton a) {
+    if (a.deterministic || a.isSingleton())
+      return;
+    Set<State> initialset = new HashSet<State>();
+    initialset.add(a.initial);
+    determinizeSimple(a, initialset);
+  }
+  
+  /** 
+   * Simple, original brics implementation of determinize()
+   * Determinizes the given automaton using the given set of initial states. 
+   */
+  public static void determinizeSimple(Automaton a, Set<State> initialset) {
+    int[] points = a.getStartPoints();
+    // subset construction
+    Map<Set<State>, Set<State>> sets = new HashMap<Set<State>, Set<State>>();
+    LinkedList<Set<State>> worklist = new LinkedList<Set<State>>();
+    Map<Set<State>, State> newstate = new HashMap<Set<State>, State>();
+    sets.put(initialset, initialset);
+    worklist.add(initialset);
+    a.initial = new State();
+    newstate.put(initialset, a.initial);
+    while (worklist.size() > 0) {
+      Set<State> s = worklist.removeFirst();
+      State r = newstate.get(s);
+      for (State q : s)
+        if (q.accept) {
+          r.accept = true;
+          break;
+        }
+      for (int n = 0; n < points.length; n++) {
+        Set<State> p = new HashSet<State>();
+        for (State q : s)
+          for (Transition t : q.getTransitions())
+            if (t.min <= points[n] && points[n] <= t.max)
+              p.add(t.to);
+        if (!sets.containsKey(p)) {
+          sets.put(p, p);
+          worklist.add(p);
+          newstate.put(p, new State());
+        }
+        State q = newstate.get(p);
+        int min = points[n];
+        int max;
+        if (n + 1 < points.length)
+          max = points[n + 1] - 1;
+        else
+          max = Character.MAX_CODE_POINT;
+        r.addTransition(new Transition(min, max, q));
+      }
+    }
+    a.deterministic = true;
+    a.clearNumberedStates();
+    a.removeDeadTransitions();
+  }
+
+}
diff --git a/lucene/src/test/org/apache/lucene/analysis/BaseTokenStreamTestCase.java b/lucene/src/test/org/apache/lucene/analysis/BaseTokenStreamTestCase.java
deleted file mode 100644
index aac0351..0000000
--- a/lucene/src/test/org/apache/lucene/analysis/BaseTokenStreamTestCase.java
+++ /dev/null
@@ -1,219 +0,0 @@
-package org.apache.lucene.analysis;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.StringReader;
-import java.io.IOException;
- 
-import org.apache.lucene.analysis.tokenattributes.*;
-import org.apache.lucene.util.Attribute;
-import org.apache.lucene.util.AttributeImpl;
-import org.apache.lucene.util.LuceneTestCase;
-
-/** 
- * Base class for all Lucene unit tests that use TokenStreams.  
- */
-public abstract class BaseTokenStreamTestCase extends LuceneTestCase {
-  // some helpers to test Analyzers and TokenStreams:
-  
-  public static interface CheckClearAttributesAttribute extends Attribute {
-    boolean getAndResetClearCalled();
-  }
-
-  public static final class CheckClearAttributesAttributeImpl extends AttributeImpl implements CheckClearAttributesAttribute {
-    private boolean clearCalled = false;
-    
-    public boolean getAndResetClearCalled() {
-      try {
-        return clearCalled;
-      } finally {
-        clearCalled = false;
-      }
-    }
-
-    @Override
-    public void clear() {
-      clearCalled = true;
-    }
-
-    @Override
-    public boolean equals(Object other) {
-      return (
-        other instanceof CheckClearAttributesAttributeImpl &&
-        ((CheckClearAttributesAttributeImpl) other).clearCalled == this.clearCalled
-      );
-    }
-
-    @Override
-    public int hashCode() {
-      return 76137213 ^ Boolean.valueOf(clearCalled).hashCode();
-    }
-    
-    @Override
-    public void copyTo(AttributeImpl target) {
-      ((CheckClearAttributesAttributeImpl) target).clear();
-    }
-  }
-
-  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], Integer finalOffset) throws IOException {
-    assertNotNull(output);
-    CheckClearAttributesAttribute checkClearAtt = ts.addAttribute(CheckClearAttributesAttribute.class);
-    
-    assertTrue("has no CharTermAttribute", ts.hasAttribute(CharTermAttribute.class));
-    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);
-    
-    OffsetAttribute offsetAtt = null;
-    if (startOffsets != null || endOffsets != null || finalOffset != null) {
-      assertTrue("has no OffsetAttribute", ts.hasAttribute(OffsetAttribute.class));
-      offsetAtt = ts.getAttribute(OffsetAttribute.class);
-    }
-    
-    TypeAttribute typeAtt = null;
-    if (types != null) {
-      assertTrue("has no TypeAttribute", ts.hasAttribute(TypeAttribute.class));
-      typeAtt = ts.getAttribute(TypeAttribute.class);
-    }
-    
-    PositionIncrementAttribute posIncrAtt = null;
-    if (posIncrements != null) {
-      assertTrue("has no PositionIncrementAttribute", ts.hasAttribute(PositionIncrementAttribute.class));
-      posIncrAtt = ts.getAttribute(PositionIncrementAttribute.class);
-    }
-    
-    ts.reset();
-    for (int i = 0; i < output.length; i++) {
-      // extra safety to enforce, that the state is not preserved and also assign bogus values
-      ts.clearAttributes();
-      termAtt.setEmpty().append("bogusTerm");
-      if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);
-      if (typeAtt != null) typeAtt.setType("bogusType");
-      if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);
-      
-      checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before
-      assertTrue("token "+i+" does not exist", ts.incrementToken());
-      assertTrue("clearAttributes() was not called correctly in TokenStream chain", checkClearAtt.getAndResetClearCalled());
-      
-      assertEquals("term "+i, output[i], termAtt.toString());
-      if (startOffsets != null)
-        assertEquals("startOffset "+i, startOffsets[i], offsetAtt.startOffset());
-      if (endOffsets != null)
-        assertEquals("endOffset "+i, endOffsets[i], offsetAtt.endOffset());
-      if (types != null)
-        assertEquals("type "+i, types[i], typeAtt.type());
-      if (posIncrements != null)
-        assertEquals("posIncrement "+i, posIncrements[i], posIncrAtt.getPositionIncrement());
-    }
-    assertFalse("end of stream", ts.incrementToken());
-    ts.end();
-    if (finalOffset != null)
-      assertEquals("finalOffset ", finalOffset.intValue(), offsetAtt.endOffset());
-    ts.close();
-  }
-  
-  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[]) throws IOException {
-    assertTokenStreamContents(ts, output, startOffsets, endOffsets, types, posIncrements, null);
-  }
-
-  public static void assertTokenStreamContents(TokenStream ts, String[] output) throws IOException {
-    assertTokenStreamContents(ts, output, null, null, null, null, null);
-  }
-  
-  public static void assertTokenStreamContents(TokenStream ts, String[] output, String[] types) throws IOException {
-    assertTokenStreamContents(ts, output, null, null, types, null, null);
-  }
-  
-  public static void assertTokenStreamContents(TokenStream ts, String[] output, int[] posIncrements) throws IOException {
-    assertTokenStreamContents(ts, output, null, null, null, posIncrements, null);
-  }
-  
-  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[]) throws IOException {
-    assertTokenStreamContents(ts, output, startOffsets, endOffsets, null, null, null);
-  }
-  
-  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], Integer finalOffset) throws IOException {
-    assertTokenStreamContents(ts, output, startOffsets, endOffsets, null, null, finalOffset);
-  }
-  
-  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], int[] posIncrements) throws IOException {
-    assertTokenStreamContents(ts, output, startOffsets, endOffsets, null, posIncrements, null);
-  }
-
-  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], int[] posIncrements, Integer finalOffset) throws IOException {
-    assertTokenStreamContents(ts, output, startOffsets, endOffsets, null, posIncrements, finalOffset);
-  }
-  
-  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[]) throws IOException {
-    assertTokenStreamContents(a.tokenStream("dummy", new StringReader(input)), output, startOffsets, endOffsets, types, posIncrements, input.length());
-  }
-  
-  public static void assertAnalyzesTo(Analyzer a, String input, String[] output) throws IOException {
-    assertAnalyzesTo(a, input, output, null, null, null, null);
-  }
-  
-  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, String[] types) throws IOException {
-    assertAnalyzesTo(a, input, output, null, null, types, null);
-  }
-  
-  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int[] posIncrements) throws IOException {
-    assertAnalyzesTo(a, input, output, null, null, null, posIncrements);
-  }
-  
-  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[]) throws IOException {
-    assertAnalyzesTo(a, input, output, startOffsets, endOffsets, null, null);
-  }
-  
-  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], int[] posIncrements) throws IOException {
-    assertAnalyzesTo(a, input, output, startOffsets, endOffsets, null, posIncrements);
-  }
-  
-
-  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[]) throws IOException {
-    assertTokenStreamContents(a.reusableTokenStream("dummy", new StringReader(input)), output, startOffsets, endOffsets, types, posIncrements, input.length());
-  }
-  
-  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output) throws IOException {
-    assertAnalyzesToReuse(a, input, output, null, null, null, null);
-  }
-  
-  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, String[] types) throws IOException {
-    assertAnalyzesToReuse(a, input, output, null, null, types, null);
-  }
-  
-  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, int[] posIncrements) throws IOException {
-    assertAnalyzesToReuse(a, input, output, null, null, null, posIncrements);
-  }
-  
-  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[]) throws IOException {
-    assertAnalyzesToReuse(a, input, output, startOffsets, endOffsets, null, null);
-  }
-  
-  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], int[] posIncrements) throws IOException {
-    assertAnalyzesToReuse(a, input, output, startOffsets, endOffsets, null, posIncrements);
-  }
-
-  // simple utility method for testing stemmers
-  
-  public static void checkOneTerm(Analyzer a, final String input, final String expected) throws IOException {
-    assertAnalyzesTo(a, input, new String[]{expected});
-  }
-  
-  public static void checkOneTermReuse(Analyzer a, final String input, final String expected) throws IOException {
-    assertAnalyzesToReuse(a, input, new String[]{expected});
-  }
-  
-}
diff --git a/lucene/src/test/org/apache/lucene/analysis/MockAnalyzer.java b/lucene/src/test/org/apache/lucene/analysis/MockAnalyzer.java
deleted file mode 100644
index d23b093..0000000
--- a/lucene/src/test/org/apache/lucene/analysis/MockAnalyzer.java
+++ /dev/null
@@ -1,156 +0,0 @@
-package org.apache.lucene.analysis;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.Reader;
-
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
-import org.apache.lucene.index.Payload;
-import org.apache.lucene.util.automaton.CharacterRunAutomaton;
-
-/**
- * Analyzer for testing
- */
-public final class MockAnalyzer extends Analyzer { 
-  private final CharacterRunAutomaton runAutomaton;
-  private final boolean lowerCase;
-  private final CharacterRunAutomaton filter;
-  private final boolean enablePositionIncrements;
-  private final boolean payload;
-  private int positionIncrementGap;
-
-  public MockAnalyzer(CharacterRunAutomaton runAutomaton, boolean lowerCase, CharacterRunAutomaton filter, boolean enablePositionIncrements) {
-    this(runAutomaton, lowerCase, filter, enablePositionIncrements, true);    
-  }
-
-  /**
-   * Creates a new MockAnalyzer.
-   * 
-   * @param runAutomaton DFA describing how tokenization should happen (e.g. [a-zA-Z]+)
-   * @param lowerCase true if the tokenizer should lowercase terms
-   * @param filter DFA describing how terms should be filtered (set of stopwords, etc)
-   * @param enablePositionIncrements true if position increments should reflect filtered terms.
-   * @param payload if payloads should be added
-   */
-  public MockAnalyzer(CharacterRunAutomaton runAutomaton, boolean lowerCase, CharacterRunAutomaton filter, boolean enablePositionIncrements, boolean payload) {
-    this.runAutomaton = runAutomaton;
-    this.lowerCase = lowerCase;
-    this.filter = filter;
-    this.enablePositionIncrements = enablePositionIncrements;
-    this.payload = payload;
-  }
-
-  /**
-   * Creates a new MockAnalyzer, with no filtering.
-   * 
-   * @param runAutomaton DFA describing how tokenization should happen (e.g. [a-zA-Z]+)
-   * @param lowerCase true if the tokenizer should lowercase terms
-   */
-  public MockAnalyzer(CharacterRunAutomaton runAutomaton, boolean lowerCase) {
-    this(runAutomaton, lowerCase, MockTokenFilter.EMPTY_STOPSET, false, true);
-  }
-
-  public MockAnalyzer(CharacterRunAutomaton runAutomaton, boolean lowerCase, boolean payload) {
-    this(runAutomaton, lowerCase, MockTokenFilter.EMPTY_STOPSET, false, payload);
-  }
-  
-  /** 
-   * Create a Whitespace-lowercasing analyzer with no stopwords removal 
-   */
-  public MockAnalyzer() {
-    this(MockTokenizer.WHITESPACE, true);
-  }
-
-  @Override
-  public TokenStream tokenStream(String fieldName, Reader reader) {
-    MockTokenizer tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase);
-    TokenFilter filt = new MockTokenFilter(tokenizer, filter, enablePositionIncrements);
-    if (payload){
-      filt = new SimplePayloadFilter(filt, fieldName);
-    }
-    return filt;
-  }
-
-  private class SavedStreams {
-    MockTokenizer tokenizer;
-    TokenFilter filter;
-  }
-
-  @Override
-  public TokenStream reusableTokenStream(String fieldName, Reader reader)
-      throws IOException {
-    SavedStreams saved = (SavedStreams) getPreviousTokenStream();
-    if (saved == null) {
-      saved = new SavedStreams();
-      saved.tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase);
-      saved.filter = new MockTokenFilter(saved.tokenizer, filter, enablePositionIncrements);
-      if (payload){
-        saved.filter = new SimplePayloadFilter(saved.filter, fieldName);
-      }
-      setPreviousTokenStream(saved);
-      return saved.filter;
-    } else {
-      saved.tokenizer.reset(reader);
-      saved.filter.reset();
-      return saved.filter;
-    }
-  }
-  
-  public void setPositionIncrementGap(int positionIncrementGap){
-    this.positionIncrementGap = positionIncrementGap;
-  }
-  
-  @Override
-  public int getPositionIncrementGap(String fieldName){
-    return positionIncrementGap;
-  }
-}
-
-final class SimplePayloadFilter extends TokenFilter {
-  String fieldName;
-  int pos;
-  final PayloadAttribute payloadAttr;
-  final CharTermAttribute termAttr;
-
-  public SimplePayloadFilter(TokenStream input, String fieldName) {
-    super(input);
-    this.fieldName = fieldName;
-    pos = 0;
-    payloadAttr = input.addAttribute(PayloadAttribute.class);
-    termAttr = input.addAttribute(CharTermAttribute.class);
-  }
-
-  @Override
-  public boolean incrementToken() throws IOException {
-    if (input.incrementToken()) {
-      payloadAttr.setPayload(new Payload(("pos: " + pos).getBytes()));
-      pos++;
-      return true;
-    } else {
-      return false;
-    }
-  }
-
-  @Override
-  public void reset() throws IOException {
-    super.reset();
-    pos = 0;
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/analysis/MockPayloadAnalyzer.java b/lucene/src/test/org/apache/lucene/analysis/MockPayloadAnalyzer.java
deleted file mode 100644
index 63d99af..0000000
--- a/lucene/src/test/org/apache/lucene/analysis/MockPayloadAnalyzer.java
+++ /dev/null
@@ -1,93 +0,0 @@
-package org.apache.lucene.analysis;
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.index.Payload;
-
-import java.io.IOException;
-import java.io.Reader;
-
-
-/**
- *
- *
- **/
-public final class MockPayloadAnalyzer extends Analyzer {
-
-  @Override
-  public TokenStream tokenStream(String fieldName, Reader reader) {
-    TokenStream result = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);
-    return new MockPayloadFilter(result, fieldName);
-  }
-}
-
-
-/**
- *
- *
- **/
-final class MockPayloadFilter extends TokenFilter {
-  String fieldName;
-
-  int pos;
-
-  int i;
-
-  final PositionIncrementAttribute posIncrAttr;
-  final PayloadAttribute payloadAttr;
-  final CharTermAttribute termAttr;
-
-  public MockPayloadFilter(TokenStream input, String fieldName) {
-    super(input);
-    this.fieldName = fieldName;
-    pos = 0;
-    i = 0;
-    posIncrAttr = input.addAttribute(PositionIncrementAttribute.class);
-    payloadAttr = input.addAttribute(PayloadAttribute.class);
-    termAttr = input.addAttribute(CharTermAttribute.class);
-  }
-
-  @Override
-  public boolean incrementToken() throws IOException {
-    if (input.incrementToken()) {
-      payloadAttr.setPayload(new Payload(("pos: " + pos).getBytes()));
-      int posIncr;
-      if (i % 2 == 1) {
-        posIncr = 1;
-      } else {
-        posIncr = 0;
-      }
-      posIncrAttr.setPositionIncrement(posIncr);
-      pos += posIncr;
-      i++;
-      return true;
-    } else {
-      return false;
-    }
-  }
-
-  @Override
-  public void reset() throws IOException {
-    i = 0;
-    pos = 0;
-  }
-}
-
diff --git a/lucene/src/test/org/apache/lucene/analysis/MockTokenFilter.java b/lucene/src/test/org/apache/lucene/analysis/MockTokenFilter.java
deleted file mode 100644
index f16165b..0000000
--- a/lucene/src/test/org/apache/lucene/analysis/MockTokenFilter.java
+++ /dev/null
@@ -1,101 +0,0 @@
-package org.apache.lucene.analysis;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import static org.apache.lucene.util.automaton.BasicAutomata.makeEmpty;
-import static org.apache.lucene.util.automaton.BasicAutomata.makeString;
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.util.automaton.BasicOperations;
-import org.apache.lucene.util.automaton.CharacterRunAutomaton;
-
-/**
- * A tokenfilter for testing that removes terms accepted by a DFA.
- * <ul>
- *  <li>Union a list of singletons to act like a stopfilter.
- *  <li>Use the complement to act like a keepwordfilter
- *  <li>Use a regex like <code>.{12,}</code> to act like a lengthfilter
- * </ul>
- */
-public final class MockTokenFilter extends TokenFilter {
-  /** Empty set of stopwords */
-  public static final CharacterRunAutomaton EMPTY_STOPSET =
-    new CharacterRunAutomaton(makeEmpty());
-  
-  /** Set of common english stopwords */
-  public static final CharacterRunAutomaton ENGLISH_STOPSET = 
-    new CharacterRunAutomaton(BasicOperations.union(Arrays.asList(
-      makeString("a"), makeString("an"), makeString("and"), makeString("are"),
-      makeString("as"), makeString("at"), makeString("be"), makeString("but"), 
-      makeString("by"), makeString("for"), makeString("if"), makeString("in"), 
-      makeString("into"), makeString("is"), makeString("it"), makeString("no"),
-      makeString("not"), makeString("of"), makeString("on"), makeString("or"), 
-      makeString("such"), makeString("that"), makeString("the"), makeString("their"), 
-      makeString("then"), makeString("there"), makeString("these"), makeString("they"), 
-      makeString("this"), makeString("to"), makeString("was"), makeString("will"), 
-      makeString("with"))));
-  
-  private final CharacterRunAutomaton filter;
-  private boolean enablePositionIncrements = false;
-
-  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-  private final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);
-  
-  public MockTokenFilter(TokenStream input, CharacterRunAutomaton filter, boolean enablePositionIncrements) {
-    super(input);
-    this.filter = filter;
-    this.enablePositionIncrements = enablePositionIncrements;
-  }
-  
-  @Override
-  public boolean incrementToken() throws IOException {
-    // return the first non-stop word found
-    int skippedPositions = 0;
-    while (input.incrementToken()) {
-      if (!filter.run(termAtt.buffer(), 0, termAtt.length())) {
-        if (enablePositionIncrements) {
-          posIncrAtt.setPositionIncrement(posIncrAtt.getPositionIncrement() + skippedPositions);
-        }
-        return true;
-      }
-      skippedPositions += posIncrAtt.getPositionIncrement();
-    }
-    // reached EOS -- return false
-    return false;
-  }
-  
-  /**
-   * @see #setEnablePositionIncrements(boolean)
-   */
-  public boolean getEnablePositionIncrements() {
-    return enablePositionIncrements;
-  }
-
-  /**
-   * If <code>true</code>, this Filter will preserve
-   * positions of the incoming tokens (ie, accumulate and
-   * set position increments of the removed stop tokens).
-   */
-  public void setEnablePositionIncrements(boolean enable) {
-    this.enablePositionIncrements = enable;
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/analysis/MockTokenizer.java b/lucene/src/test/org/apache/lucene/analysis/MockTokenizer.java
deleted file mode 100644
index 017f828..0000000
--- a/lucene/src/test/org/apache/lucene/analysis/MockTokenizer.java
+++ /dev/null
@@ -1,83 +0,0 @@
-package org.apache.lucene.analysis;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.Reader;
-
-import org.apache.lucene.util.Version;
-import org.apache.lucene.util.automaton.CharacterRunAutomaton;
-import org.apache.lucene.util.automaton.RegExp;
-
-/**
- * Automaton-based tokenizer for testing. Optionally lowercases.
- */
-public class MockTokenizer extends CharTokenizer {
-  /** Acts Similar to WhitespaceTokenizer */
-  public static final CharacterRunAutomaton WHITESPACE = 
-    new CharacterRunAutomaton(new RegExp("[^ \t\r\n]+").toAutomaton());
-  /** Acts Similar to KeywordTokenizer.
-   * TODO: Keyword returns an "empty" token for an empty reader... 
-   */
-  public static final CharacterRunAutomaton KEYWORD =
-    new CharacterRunAutomaton(new RegExp(".*").toAutomaton());
-  /** Acts like LetterTokenizer. */
-  // the ugly regex below is Unicode 5.2 [:Letter:]
-  public static final CharacterRunAutomaton SIMPLE =
-    new CharacterRunAutomaton(new RegExp("[A-Za-z?-??--??-??---??-???---??--??-??---?-??-??-?-?-??-??????-??--?-????--?-?????-?-????--?-???-?-??-??-?-??-????-????-?-??-????-???-??-??-????????-??-??-??-??---????-??-??---???-??-??--??-?-??--?-??-?-?????????-??-?-?-?-??????-??-??-???-????-???-???????-??-?????-???-????-???-???-?????-???-???-???-??-??-????-???-???-???-???-???-???-??-???-??-???-???-???-???-??-???-???-??-?????-?-??-?-??-?-?-??-??-??-???-?-??-????-?-??--?-?-?-??-??-??-??-?????-?--?-??-??-??-??-?--???-??????-?????-????????-???-??-??-???????-?-??--??--?-??-?-?--?-??-??-??-???????-????-???-???-??-??-???-???-??-??-?-???-???-??-???-??????-???-???-???-??-???-??????-???-???-???-??-?--?--??-?-??-??-??-??-?-????-???-??-?-??-???-??-?-?-??-??-???-??-?-??????-?-?-??-?----?-?-?-??-??-??-???-?????-?????-??????-????-????-????-????-????-????-????-????-????-????-????-????-??????-??????????-????-????-?????-????-????-???-???-????-????-????-????-?????-?????-????-????-??????????????-????-??????-????-????-????-????-????-????-????-??????-????-????-????-????-????-????-????-????-????-????-????-????-?????-??-??-?]+").toAutomaton());
-
-  private final CharacterRunAutomaton runAutomaton;
-  private final boolean lowerCase;
-  private int state;
-
-  public MockTokenizer(AttributeFactory factory, Reader input, CharacterRunAutomaton runAutomaton, boolean lowerCase) {
-    super(Version.LUCENE_CURRENT, factory, input);
-    this.runAutomaton = runAutomaton;
-    this.lowerCase = lowerCase;
-    this.state = runAutomaton.getInitialState();
-  }
-
-  public MockTokenizer(Reader input, CharacterRunAutomaton runAutomaton, boolean lowerCase) {
-    super(Version.LUCENE_CURRENT, input);
-    this.runAutomaton = runAutomaton;
-    this.lowerCase = lowerCase;
-    this.state = runAutomaton.getInitialState();
-  }
-  
-  @Override
-  protected boolean isTokenChar(int c) {
-    state = runAutomaton.step(state, c);
-    if (state < 0) {
-      state = runAutomaton.getInitialState();
-      return false;
-    } else {
-      return true;
-    }
-  }
-  
-  @Override
-  protected int normalize(int c) {
-    return lowerCase ? Character.toLowerCase(c) : c;
-  }
-
-  @Override
-  public void reset() throws IOException {
-    super.reset();
-    state = runAutomaton.getInitialState();
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/index/DocHelper.java b/lucene/src/test/org/apache/lucene/index/DocHelper.java
deleted file mode 100644
index 28bcdff..0000000
--- a/lucene/src/test/org/apache/lucene/index/DocHelper.java
+++ /dev/null
@@ -1,250 +0,0 @@
-package org.apache.lucene.index;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.UnsupportedEncodingException;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.Fieldable;
-import org.apache.lucene.search.SimilarityProvider;
-import org.apache.lucene.store.Directory;
-import static org.apache.lucene.util.LuceneTestCase.TEST_VERSION_CURRENT;
-
-class DocHelper {
-  public static final String FIELD_1_TEXT = "field one text";
-  public static final String TEXT_FIELD_1_KEY = "textField1";
-  public static Field textField1 = new Field(TEXT_FIELD_1_KEY, FIELD_1_TEXT,
-      Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO);
-  
-  public static final String FIELD_2_TEXT = "field field field two text";
-  //Fields will be lexicographically sorted.  So, the order is: field, text, two
-  public static final int [] FIELD_2_FREQS = {3, 1, 1}; 
-  public static final String TEXT_FIELD_2_KEY = "textField2";
-  public static Field textField2 = new Field(TEXT_FIELD_2_KEY, FIELD_2_TEXT, Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS);
-  
-  public static final String FIELD_3_TEXT = "aaaNoNorms aaaNoNorms bbbNoNorms";
-  public static final String TEXT_FIELD_3_KEY = "textField3";
-  public static Field textField3 = new Field(TEXT_FIELD_3_KEY, FIELD_3_TEXT, Field.Store.YES, Field.Index.ANALYZED);
-  static { textField3.setOmitNorms(true); }
-
-  public static final String KEYWORD_TEXT = "Keyword";
-  public static final String KEYWORD_FIELD_KEY = "keyField";
-  public static Field keyField = new Field(KEYWORD_FIELD_KEY, KEYWORD_TEXT,
-      Field.Store.YES, Field.Index.NOT_ANALYZED);
-
-  public static final String NO_NORMS_TEXT = "omitNormsText";
-  public static final String NO_NORMS_KEY = "omitNorms";
-  public static Field noNormsField = new Field(NO_NORMS_KEY, NO_NORMS_TEXT,
-      Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS);
-
-  public static final String NO_TF_TEXT = "analyzed with no tf and positions";
-  public static final String NO_TF_KEY = "omitTermFreqAndPositions";
-  public static Field noTFField = new Field(NO_TF_KEY, NO_TF_TEXT,
-      Field.Store.YES, Field.Index.ANALYZED);
-  static {
-    noTFField.setOmitTermFreqAndPositions(true);
-  }
-
-  public static final String UNINDEXED_FIELD_TEXT = "unindexed field text";
-  public static final String UNINDEXED_FIELD_KEY = "unIndField";
-  public static Field unIndField = new Field(UNINDEXED_FIELD_KEY, UNINDEXED_FIELD_TEXT,
-      Field.Store.YES, Field.Index.NO);
-
-
-  public static final String UNSTORED_1_FIELD_TEXT = "unstored field text";
-  public static final String UNSTORED_FIELD_1_KEY = "unStoredField1";
-  public static Field unStoredField1 = new Field(UNSTORED_FIELD_1_KEY, UNSTORED_1_FIELD_TEXT,
-      Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO);
-
-  public static final String UNSTORED_2_FIELD_TEXT = "unstored field text";
-  public static final String UNSTORED_FIELD_2_KEY = "unStoredField2";
-  public static Field unStoredField2 = new Field(UNSTORED_FIELD_2_KEY, UNSTORED_2_FIELD_TEXT,
-      Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES);
-
-  public static final String LAZY_FIELD_BINARY_KEY = "lazyFieldBinary";
-  public static byte [] LAZY_FIELD_BINARY_BYTES;
-  public static Field lazyFieldBinary;
-  
-  public static final String LAZY_FIELD_KEY = "lazyField";
-  public static final String LAZY_FIELD_TEXT = "These are some field bytes";
-  public static Field lazyField = new Field(LAZY_FIELD_KEY, LAZY_FIELD_TEXT, Field.Store.YES, Field.Index.ANALYZED);
-  
-  public static final String LARGE_LAZY_FIELD_KEY = "largeLazyField";
-  public static String LARGE_LAZY_FIELD_TEXT;
-  public static Field largeLazyField;
-  
-  //From Issue 509
-  public static final String FIELD_UTF1_TEXT = "field one \u4e00text";
-  public static final String TEXT_FIELD_UTF1_KEY = "textField1Utf8";
-  public static Field textUtfField1 = new Field(TEXT_FIELD_UTF1_KEY, FIELD_UTF1_TEXT,
-      Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO);
-
-  public static final String FIELD_UTF2_TEXT = "field field field \u4e00two text";
-  //Fields will be lexicographically sorted.  So, the order is: field, text, two
-  public static final int [] FIELD_UTF2_FREQS = {3, 1, 1};
-  public static final String TEXT_FIELD_UTF2_KEY = "textField2Utf8";
-  public static Field textUtfField2 = new Field(TEXT_FIELD_UTF2_KEY, FIELD_UTF2_TEXT, Field.Store.YES, 
-          Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS);
- 
-  
-  
-  
-  public static Map<String,Object> nameValues = null;
-
-  // ordered list of all the fields...
-  // could use LinkedHashMap for this purpose if Java1.4 is OK
-  public static Field[] fields = new Field[] {
-    textField1,
-    textField2,
-    textField3,
-    keyField,
-    noNormsField,
-    noTFField,
-    unIndField,
-    unStoredField1,
-    unStoredField2,
-    textUtfField1,
-    textUtfField2,
-    lazyField,
-    lazyFieldBinary,//placeholder for binary field, since this is null.  It must be second to last.
-    largeLazyField//placeholder for large field, since this is null.  It must always be last
-  };
-
-  public static Map<String,Fieldable> all     =new HashMap<String,Fieldable>();
-  public static Map<String,Fieldable> indexed =new HashMap<String,Fieldable>();
-  public static Map<String,Fieldable> stored  =new HashMap<String,Fieldable>();
-  public static Map<String,Fieldable> unstored=new HashMap<String,Fieldable>();
-  public static Map<String,Fieldable> unindexed=new HashMap<String,Fieldable>();
-  public static Map<String,Fieldable> termvector=new HashMap<String,Fieldable>();
-  public static Map<String,Fieldable> notermvector=new HashMap<String,Fieldable>();
-  public static Map<String,Fieldable> lazy= new HashMap<String,Fieldable>();
-  public static Map<String,Fieldable> noNorms=new HashMap<String,Fieldable>();
-  public static Map<String,Fieldable> noTf=new HashMap<String,Fieldable>();
-
-  static {
-    //Initialize the large Lazy Field
-    StringBuilder buffer = new StringBuilder();
-    for (int i = 0; i < 10000; i++)
-    {
-      buffer.append("Lazily loading lengths of language in lieu of laughing ");
-    }
-    
-    try {
-      LAZY_FIELD_BINARY_BYTES = "These are some binary field bytes".getBytes("UTF8");
-    } catch (UnsupportedEncodingException e) {
-    }
-    lazyFieldBinary = new Field(LAZY_FIELD_BINARY_KEY, LAZY_FIELD_BINARY_BYTES);
-    fields[fields.length - 2] = lazyFieldBinary;
-    LARGE_LAZY_FIELD_TEXT = buffer.toString();
-    largeLazyField = new Field(LARGE_LAZY_FIELD_KEY, LARGE_LAZY_FIELD_TEXT, Field.Store.YES, Field.Index.ANALYZED);
-    fields[fields.length - 1] = largeLazyField;
-    for (int i=0; i<fields.length; i++) {
-      Fieldable f = fields[i];
-      add(all,f);
-      if (f.isIndexed()) add(indexed,f);
-      else add(unindexed,f);
-      if (f.isTermVectorStored()) add(termvector,f);
-      if (f.isIndexed() && !f.isTermVectorStored()) add(notermvector,f);
-      if (f.isStored()) add(stored,f);
-      else add(unstored,f);
-      if (f.getOmitNorms()) add(noNorms,f);
-      if (f.getOmitTermFreqAndPositions()) add(noTf,f);
-      if (f.isLazy()) add(lazy, f);
-    }
-  }
-
-
-  private static void add(Map<String,Fieldable> map, Fieldable field) {
-    map.put(field.name(), field);
-  }
-
-
-  static
-  {
-    nameValues = new HashMap<String,Object>();
-    nameValues.put(TEXT_FIELD_1_KEY, FIELD_1_TEXT);
-    nameValues.put(TEXT_FIELD_2_KEY, FIELD_2_TEXT);
-    nameValues.put(TEXT_FIELD_3_KEY, FIELD_3_TEXT);
-    nameValues.put(KEYWORD_FIELD_KEY, KEYWORD_TEXT);
-    nameValues.put(NO_NORMS_KEY, NO_NORMS_TEXT);
-    nameValues.put(NO_TF_KEY, NO_TF_TEXT);
-    nameValues.put(UNINDEXED_FIELD_KEY, UNINDEXED_FIELD_TEXT);
-    nameValues.put(UNSTORED_FIELD_1_KEY, UNSTORED_1_FIELD_TEXT);
-    nameValues.put(UNSTORED_FIELD_2_KEY, UNSTORED_2_FIELD_TEXT);
-    nameValues.put(LAZY_FIELD_KEY, LAZY_FIELD_TEXT);
-    nameValues.put(LAZY_FIELD_BINARY_KEY, LAZY_FIELD_BINARY_BYTES);
-    nameValues.put(LARGE_LAZY_FIELD_KEY, LARGE_LAZY_FIELD_TEXT);
-    nameValues.put(TEXT_FIELD_UTF1_KEY, FIELD_UTF1_TEXT);
-    nameValues.put(TEXT_FIELD_UTF2_KEY, FIELD_UTF2_TEXT);
-  }   
-  
-  /**
-   * Adds the fields above to a document 
-   * @param doc The document to write
-   */ 
-  public static void setupDoc(Document doc) {
-    for (int i=0; i<fields.length; i++) {
-      doc.add(fields[i]);
-    }
-  }                         
-
-  /**
-   * Writes the document to the directory using a segment
-   * named "test"; returns the SegmentInfo describing the new
-   * segment 
-   * @param dir
-   * @param doc
-   * @throws IOException
-   */ 
-  public static SegmentInfo writeDoc(Directory dir, Document doc) throws IOException
-  {
-    return writeDoc(dir, new MockAnalyzer(MockTokenizer.WHITESPACE, false), null, doc);
-  }
-
-  /**
-   * Writes the document to the directory using the analyzer
-   * and the similarity score; returns the SegmentInfo
-   * describing the new segment
-   * @param dir
-   * @param analyzer
-   * @param similarity
-   * @param doc
-   * @throws IOException
-   */ 
-  public static SegmentInfo writeDoc(Directory dir, Analyzer analyzer, SimilarityProvider similarity, Document doc) throws IOException {
-    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(
-        TEST_VERSION_CURRENT, analyzer).setSimilarityProvider(similarity));
-    //writer.setUseCompoundFile(false);
-    writer.addDocument(doc);
-    writer.commit();
-    SegmentInfo info = writer.newestSegment();
-    writer.close();
-    return info;
-  }
-
-  public static int numFields(Document doc) {
-    return doc.getFields().size();
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/index/MockIndexInput.java b/lucene/src/test/org/apache/lucene/index/MockIndexInput.java
deleted file mode 100644
index 1e2346c..0000000
--- a/lucene/src/test/org/apache/lucene/index/MockIndexInput.java
+++ /dev/null
@@ -1,64 +0,0 @@
-package org.apache.lucene.index;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.store.BufferedIndexInput;
-
-public class MockIndexInput extends BufferedIndexInput {
-    private byte[] buffer;
-    private int pointer = 0;
-    private long length;
-
-    public MockIndexInput(byte[] bytes) {
-        buffer = bytes;
-        length = bytes.length;
-    }
-
-    @Override
-    protected void readInternal(byte[] dest, int destOffset, int len) {
-        int remainder = len;
-        int start = pointer;
-        while (remainder != 0) {
-//          int bufferNumber = start / buffer.length;
-          int bufferOffset = start % buffer.length;
-          int bytesInBuffer = buffer.length - bufferOffset;
-          int bytesToCopy = bytesInBuffer >= remainder ? remainder : bytesInBuffer;
-          System.arraycopy(buffer, bufferOffset, dest, destOffset, bytesToCopy);
-          destOffset += bytesToCopy;
-          start += bytesToCopy;
-          remainder -= bytesToCopy;
-        }
-        pointer += len;
-    }
-
-    @Override
-    public void close() {
-        // ignore
-    }
-
-    @Override
-    protected void seekInternal(long pos) {
-        pointer = (int) pos;
-    }
-
-    @Override
-    public long length() {
-      return length;
-    }
-
-}
diff --git a/lucene/src/test/org/apache/lucene/index/MockRandomMergePolicy.java b/lucene/src/test/org/apache/lucene/index/MockRandomMergePolicy.java
deleted file mode 100644
index e8bc977..0000000
--- a/lucene/src/test/org/apache/lucene/index/MockRandomMergePolicy.java
+++ /dev/null
@@ -1,95 +0,0 @@
-package org.apache.lucene.index;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.Random;
-import java.util.Set;
-
-import org.apache.lucene.util._TestUtil;
-
-public class MockRandomMergePolicy extends MergePolicy {
-  private final Random random;
-
-  public MockRandomMergePolicy(Random random) {
-    // fork a private random, since we are called
-    // unpredictably from threads:
-    this.random = new Random(random.nextLong());
-  }
-
-  @Override
-  public MergeSpecification findMerges(SegmentInfos segmentInfos) {
-    MergeSpecification mergeSpec = null;
-    //System.out.println("MRMP: findMerges sis=" + segmentInfos);
-
-    if (segmentInfos.size() > 1 && random.nextInt(5) == 3) {
-      
-      SegmentInfos segmentInfos2 = new SegmentInfos();
-      segmentInfos2.addAll(segmentInfos);
-      Collections.shuffle(segmentInfos2, random);
-
-      // TODO: sometimes make more than 1 merge?
-      mergeSpec = new MergeSpecification();
-      final int segsToMerge = _TestUtil.nextInt(random, 1, segmentInfos.size());
-      mergeSpec.add(new OneMerge(segmentInfos2.range(0, segsToMerge)));
-    }
-
-    return mergeSpec;
-  }
-
-  @Override
-  public MergeSpecification findMergesForOptimize(
-      SegmentInfos segmentInfos, int maxSegmentCount, Set<SegmentInfo> segmentsToOptimize)
-    throws CorruptIndexException, IOException {
-
-    //System.out.println("MRMP: findMergesForOptimize sis=" + segmentInfos);
-    MergeSpecification mergeSpec = null;
-    if (segmentInfos.size() > 1 || (segmentInfos.size() == 1 && segmentInfos.info(0).hasDeletions())) {
-      mergeSpec = new MergeSpecification();
-      SegmentInfos segmentInfos2 = new SegmentInfos();
-      segmentInfos2.addAll(segmentInfos);
-      Collections.shuffle(segmentInfos2, random);
-      int upto = 0;
-      while(upto < segmentInfos.size()) {
-        int max = Math.min(10, segmentInfos.size()-upto);
-        int inc = max <= 2 ? max : _TestUtil.nextInt(random, 2, max);
-        mergeSpec.add(new OneMerge(segmentInfos2.range(upto, upto+inc)));
-        upto += inc;
-      }
-    }
-    return mergeSpec;
-  }
-
-  @Override
-  public MergeSpecification findMergesToExpungeDeletes(
-      SegmentInfos segmentInfos)
-    throws CorruptIndexException, IOException {
-    return findMerges(segmentInfos);
-  }
-
-  @Override
-  public void close() {
-  }
-
-  @Override
-  public boolean useCompoundFile(SegmentInfos infos, SegmentInfo mergedInfo) throws IOException {
-    // 80% of the time we create CFS:
-    return random.nextInt(5) != 1;
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/index/RandomIndexWriter.java b/lucene/src/test/org/apache/lucene/index/RandomIndexWriter.java
deleted file mode 100644
index 2faf22b..0000000
--- a/lucene/src/test/org/apache/lucene/index/RandomIndexWriter.java
+++ /dev/null
@@ -1,162 +0,0 @@
-package org.apache.lucene.index;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable;
-import java.io.IOException;
-import java.util.Random;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.Version;
-import org.apache.lucene.util._TestUtil;
-
-/** Silly class that randomizes the indexing experience.  EG
- *  it may swap in a different merge policy/scheduler; may
- *  commit periodically; may or may not optimize in the end,
- *  may flush by doc count instead of RAM, etc. 
- */
-
-public class RandomIndexWriter implements Closeable {
-
-  public IndexWriter w;
-  private final Random r;
-  int docCount;
-  int flushAt;
-  private boolean getReaderCalled;
-
-  // Randomly calls Thread.yield so we mixup thread scheduling
-  private static final class MockIndexWriter extends IndexWriter {
-
-    private final Random r;
-
-    public MockIndexWriter(Random r,Directory dir, IndexWriterConfig conf) throws IOException {
-      super(dir, conf);
-      // must make a private random since our methods are
-      // called from different threads; else test failures may
-      // not be reproducible from the original seed
-      this.r = new Random(r.nextInt());
-    }
-
-    @Override
-    boolean testPoint(String name) {
-      if (r.nextInt(4) == 2)
-        Thread.yield();
-      return true;
-    }
-  }
-
-  /** create a RandomIndexWriter with a random config: Uses TEST_VERSION_CURRENT and MockAnalyzer */
-  public RandomIndexWriter(Random r, Directory dir) throws IOException {
-    this(r, dir, LuceneTestCase.newIndexWriterConfig(r, LuceneTestCase.TEST_VERSION_CURRENT, new MockAnalyzer()));
-  }
-  
-  /** create a RandomIndexWriter with a random config: Uses TEST_VERSION_CURRENT */
-  public RandomIndexWriter(Random r, Directory dir, Analyzer a) throws IOException {
-    this(r, dir, LuceneTestCase.newIndexWriterConfig(r, LuceneTestCase.TEST_VERSION_CURRENT, a));
-  }
-  
-  /** create a RandomIndexWriter with a random config */
-  public RandomIndexWriter(Random r, Directory dir, Version v, Analyzer a) throws IOException {
-    this(r, dir, LuceneTestCase.newIndexWriterConfig(r, v, a));
-  }
-  
-  /** create a RandomIndexWriter with the provided config */
-  public RandomIndexWriter(Random r, Directory dir, IndexWriterConfig c) throws IOException {
-    this.r = r;
-    w = new MockIndexWriter(r, dir, c);
-    flushAt = _TestUtil.nextInt(r, 10, 1000);
-    if (LuceneTestCase.VERBOSE) {
-      System.out.println("RIW config=" + w.getConfig());
-      System.out.println("codec default=" + w.getConfig().getCodecProvider().getDefaultFieldCodec());
-      w.setInfoStream(System.out);
-    }
-  } 
-
-  public void addDocument(Document doc) throws IOException {
-    w.addDocument(doc);
-    if (docCount++ == flushAt) {
-      if (LuceneTestCase.VERBOSE) {
-        System.out.println("RIW.addDocument: now doing a commit");
-      }
-      w.commit();
-      flushAt += _TestUtil.nextInt(r, 10, 1000);
-    }
-  }
-  
-  public void addIndexes(Directory... dirs) throws CorruptIndexException, IOException {
-    w.addIndexes(dirs);
-  }
-  
-  public void deleteDocuments(Term term) throws CorruptIndexException, IOException {
-    w.deleteDocuments(term);
-  }
-  
-  public void commit() throws CorruptIndexException, IOException {
-    w.commit();
-  }
-  
-  public int numDocs() throws IOException {
-    return w.numDocs();
-  }
-
-  public int maxDoc() {
-    return w.maxDoc();
-  }
-
-  public void deleteAll() throws IOException {
-    w.deleteAll();
-  }
-
-  public IndexReader getReader() throws IOException {
-    getReaderCalled = true;
-    if (r.nextInt(4) == 2)
-      w.optimize();
-    // If we are writing with PreFlexRW, force a full
-    // IndexReader.open so terms are sorted in codepoint
-    // order during searching:
-    if (!w.codecs.getDefaultFieldCodec().equals("PreFlex") && r.nextBoolean()) {
-      if (LuceneTestCase.VERBOSE) {
-        System.out.println("RIW.getReader: use NRT reader");
-      }
-      return w.getReader();
-    } else {
-      if (LuceneTestCase.VERBOSE) {
-        System.out.println("RIW.getReader: open new reader");
-      }
-      w.commit();
-      return IndexReader.open(w.getDirectory(), new KeepOnlyLastCommitDeletionPolicy(), r.nextBoolean(), _TestUtil.nextInt(r, 1, 10));
-    }
-  }
-
-  public void close() throws IOException {
-    // if someone isn't using getReader() API, we want to be sure to
-    // maybeOptimize since presumably they might open a reader on the dir.
-    if (getReaderCalled == false && r.nextInt(4) == 2) {
-      w.optimize();
-    }
-    w.close();
-  }
-
-  public void optimize() throws IOException {
-    w.optimize();
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/index/codecs/mockintblock/MockFixedIntBlockCodec.java b/lucene/src/test/org/apache/lucene/index/codecs/mockintblock/MockFixedIntBlockCodec.java
deleted file mode 100644
index fc50b4a..0000000
--- a/lucene/src/test/org/apache/lucene/index/codecs/mockintblock/MockFixedIntBlockCodec.java
+++ /dev/null
@@ -1,202 +0,0 @@
-package org.apache.lucene.index.codecs.mockintblock;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.FieldsConsumer;
-import org.apache.lucene.index.codecs.FieldsProducer;
-import org.apache.lucene.index.codecs.sep.IntStreamFactory;
-import org.apache.lucene.index.codecs.sep.IntIndexInput;
-import org.apache.lucene.index.codecs.sep.IntIndexOutput;
-import org.apache.lucene.index.codecs.sep.SepPostingsReaderImpl;
-import org.apache.lucene.index.codecs.sep.SepPostingsWriterImpl;
-import org.apache.lucene.index.codecs.intblock.FixedIntBlockIndexInput;
-import org.apache.lucene.index.codecs.intblock.FixedIntBlockIndexOutput;
-import org.apache.lucene.index.codecs.FixedGapTermsIndexReader;
-import org.apache.lucene.index.codecs.FixedGapTermsIndexWriter;
-import org.apache.lucene.index.codecs.PostingsWriterBase;
-import org.apache.lucene.index.codecs.PostingsReaderBase;
-import org.apache.lucene.index.codecs.BlockTermsReader;
-import org.apache.lucene.index.codecs.BlockTermsWriter;
-import org.apache.lucene.index.codecs.TermsIndexReaderBase;
-import org.apache.lucene.index.codecs.TermsIndexWriterBase;
-import org.apache.lucene.index.codecs.standard.StandardCodec;
-import org.apache.lucene.store.*;
-import org.apache.lucene.util.BytesRef;
-
-/**
- * A silly test codec to verify core support for fixed
- * sized int block encoders is working.  The int encoder
- * used here just writes each block as a series of vInt.
- */
-
-public class MockFixedIntBlockCodec extends Codec {
-
-  private final int blockSize;
-
-  public MockFixedIntBlockCodec(int blockSize) {
-    this.blockSize = blockSize;
-    name = "MockFixedIntBlock";
-  }
-
-  @Override
-  public String toString() {
-    return name + "(blockSize=" + blockSize + ")";
-  }
-
-  // only for testing
-  public IntStreamFactory getIntFactory() {
-    return new MockIntFactory(blockSize);
-  }
-
-  public static class MockIntFactory extends IntStreamFactory {
-    private final int blockSize;
-
-    public MockIntFactory(int blockSize) {
-      this.blockSize = blockSize;
-    }
-
-    @Override
-    public IntIndexInput openInput(Directory dir, String fileName, int readBufferSize) throws IOException {
-      return new FixedIntBlockIndexInput(dir.openInput(fileName, readBufferSize)) {
-
-        @Override
-        protected BlockReader getBlockReader(final IndexInput in, final int[] buffer) throws IOException {
-          return new BlockReader() {
-            public void seek(long pos) {}
-            public void readBlock() throws IOException {
-              for(int i=0;i<buffer.length;i++) {
-                buffer[i] = in.readVInt();
-              }
-            }
-          };
-        }
-      };
-    }
-
-    @Override
-    public IntIndexOutput createOutput(Directory dir, String fileName) throws IOException {
-      return new FixedIntBlockIndexOutput(dir.createOutput(fileName), blockSize) {
-        @Override
-        protected void flushBlock() throws IOException {
-          for(int i=0;i<buffer.length;i++) {
-            assert buffer[i] >= 0;
-            out.writeVInt(buffer[i]);
-          }
-        }
-      };
-    }
-  }
-
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    PostingsWriterBase postingsWriter = new SepPostingsWriterImpl(state, new MockIntFactory(blockSize));
-
-    boolean success = false;
-    TermsIndexWriterBase indexWriter;
-    try {
-      indexWriter = new FixedGapTermsIndexWriter(state);
-      success = true;
-    } finally {
-      if (!success) {
-        postingsWriter.close();
-      }
-    }
-
-    success = false;
-    try {
-      FieldsConsumer ret = new BlockTermsWriter(indexWriter, state, postingsWriter, BytesRef.getUTF8SortedAsUnicodeComparator());
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        try {
-          postingsWriter.close();
-        } finally {
-          indexWriter.close();
-        }
-      }
-    }
-  }
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    PostingsReaderBase postingsReader = new SepPostingsReaderImpl(state.dir,
-                                                                      state.segmentInfo,
-                                                                      state.readBufferSize,
-                                                                      new MockIntFactory(blockSize), state.codecId);
-
-    TermsIndexReaderBase indexReader;
-    boolean success = false;
-    try {
-      indexReader = new FixedGapTermsIndexReader(state.dir,
-                                                       state.fieldInfos,
-                                                       state.segmentInfo.name,
-                                                       state.termsIndexDivisor,
-                                                       BytesRef.getUTF8SortedAsUnicodeComparator(), state.codecId);
-      success = true;
-    } finally {
-      if (!success) {
-        postingsReader.close();
-      }
-    }
-
-    success = false;
-    try {
-      FieldsProducer ret = new BlockTermsReader(indexReader,
-                                                state.dir,
-                                                state.fieldInfos,
-                                                state.segmentInfo.name,
-                                                postingsReader,
-                                                state.readBufferSize,
-                                                BytesRef.getUTF8SortedAsUnicodeComparator(),
-                                                StandardCodec.TERMS_CACHE_SIZE,
-                                                state.codecId);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        try {
-          postingsReader.close();
-        } finally {
-          indexReader.close();
-        }
-      }
-    }
-  }
-
-  @Override
-  public void files(Directory dir, SegmentInfo segmentInfo, String codecId, Set<String> files) {
-    SepPostingsReaderImpl.files(segmentInfo, codecId, files);
-    BlockTermsReader.files(dir, segmentInfo, codecId, files);
-    FixedGapTermsIndexReader.files(dir, segmentInfo, codecId, files);
-  }
-
-  @Override
-  public void getExtensions(Set<String> extensions) {
-    SepPostingsWriterImpl.getExtensions(extensions);
-    BlockTermsReader.getExtensions(extensions);
-    FixedGapTermsIndexReader.getIndexExtensions(extensions);
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/index/codecs/mockintblock/MockVariableIntBlockCodec.java b/lucene/src/test/org/apache/lucene/index/codecs/mockintblock/MockVariableIntBlockCodec.java
deleted file mode 100644
index 82b8615..0000000
--- a/lucene/src/test/org/apache/lucene/index/codecs/mockintblock/MockVariableIntBlockCodec.java
+++ /dev/null
@@ -1,227 +0,0 @@
-package org.apache.lucene.index.codecs.mockintblock;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.FieldsConsumer;
-import org.apache.lucene.index.codecs.FieldsProducer;
-import org.apache.lucene.index.codecs.sep.IntStreamFactory;
-import org.apache.lucene.index.codecs.sep.IntIndexInput;
-import org.apache.lucene.index.codecs.sep.IntIndexOutput;
-import org.apache.lucene.index.codecs.sep.SepPostingsReaderImpl;
-import org.apache.lucene.index.codecs.sep.SepPostingsWriterImpl;
-import org.apache.lucene.index.codecs.intblock.VariableIntBlockIndexInput;
-import org.apache.lucene.index.codecs.intblock.VariableIntBlockIndexOutput;
-import org.apache.lucene.index.codecs.FixedGapTermsIndexReader;
-import org.apache.lucene.index.codecs.FixedGapTermsIndexWriter;
-import org.apache.lucene.index.codecs.PostingsWriterBase;
-import org.apache.lucene.index.codecs.PostingsReaderBase;
-import org.apache.lucene.index.codecs.BlockTermsReader;
-import org.apache.lucene.index.codecs.BlockTermsWriter;
-import org.apache.lucene.index.codecs.TermsIndexReaderBase;
-import org.apache.lucene.index.codecs.TermsIndexWriterBase;
-import org.apache.lucene.index.codecs.standard.StandardCodec;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-
-/**
- * A silly test codec to verify core support for variable
- * sized int block encoders is working.  The int encoder
- * used here writes baseBlockSize ints at once, if the first
- * int is <= 3, else 2*baseBlockSize.
- */
-
-public class MockVariableIntBlockCodec extends Codec {
-  private final int baseBlockSize;
-
-  public MockVariableIntBlockCodec(int baseBlockSize) {
-    name = "MockVariableIntBlock";
-    this.baseBlockSize = baseBlockSize;
-  }
-
-  @Override
-  public String toString() {
-    return name + "(baseBlockSize="+ baseBlockSize + ")";
-  }
-
-  public static class MockIntFactory extends IntStreamFactory {
-
-    private final int baseBlockSize;
-
-    public MockIntFactory(int baseBlockSize) {
-      this.baseBlockSize = baseBlockSize;
-    }
-
-    @Override
-    public IntIndexInput openInput(Directory dir, String fileName, int readBufferSize) throws IOException {
-      final IndexInput in = dir.openInput(fileName, readBufferSize);
-      final int baseBlockSize = in.readInt();
-      return new VariableIntBlockIndexInput(in) {
-
-        @Override
-        protected BlockReader getBlockReader(final IndexInput in, final int[] buffer) throws IOException {
-          return new BlockReader() {
-            public void seek(long pos) {}
-            public int readBlock() throws IOException {
-              buffer[0] = in.readVInt();
-              final int count = buffer[0] <= 3 ? baseBlockSize-1 : 2*baseBlockSize-1;
-              assert buffer.length >= count: "buffer.length=" + buffer.length + " count=" + count;
-              for(int i=0;i<count;i++) {
-                buffer[i+1] = in.readVInt();
-              }
-              return 1+count;
-            }
-          };
-        }
-      };
-    }
-
-    @Override
-    public IntIndexOutput createOutput(Directory dir, String fileName) throws IOException {
-      final IndexOutput out = dir.createOutput(fileName);
-      out.writeInt(baseBlockSize);
-      return new VariableIntBlockIndexOutput(out, 2*baseBlockSize) {
-
-        int pendingCount;
-        final int[] buffer = new int[2+2*baseBlockSize];
-
-        @Override
-        protected int add(int value) throws IOException {
-          assert value >= 0;
-          buffer[pendingCount++] = value;
-          // silly variable block length int encoder: if
-          // first value <= 3, we write N vints at once;
-          // else, 2*N
-          final int flushAt = buffer[0] <= 3 ? baseBlockSize : 2*baseBlockSize;
-
-          // intentionally be non-causal here:
-          if (pendingCount == flushAt+1) {
-            for(int i=0;i<flushAt;i++) {
-              out.writeVInt(buffer[i]);
-            }
-            buffer[0] = buffer[flushAt];
-            pendingCount = 1;
-            return flushAt;
-          } else {
-            return 0;
-          }
-        }
-      };
-    }
-  }
-
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    PostingsWriterBase postingsWriter = new SepPostingsWriterImpl(state, new MockIntFactory(baseBlockSize));
-
-    boolean success = false;
-    TermsIndexWriterBase indexWriter;
-    try {
-      indexWriter = new FixedGapTermsIndexWriter(state);
-      success = true;
-    } finally {
-      if (!success) {
-        postingsWriter.close();
-      }
-    }
-
-    success = false;
-    try {
-      FieldsConsumer ret = new BlockTermsWriter(indexWriter, state, postingsWriter, BytesRef.getUTF8SortedAsUnicodeComparator());
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        try {
-          postingsWriter.close();
-        } finally {
-          indexWriter.close();
-        }
-      }
-    }
-  }
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    PostingsReaderBase postingsReader = new SepPostingsReaderImpl(state.dir,
-                                                                      state.segmentInfo,
-                                                                      state.readBufferSize,
-                                                                      new MockIntFactory(baseBlockSize), state.codecId);
-
-    TermsIndexReaderBase indexReader;
-    boolean success = false;
-    try {
-      indexReader = new FixedGapTermsIndexReader(state.dir,
-                                                       state.fieldInfos,
-                                                       state.segmentInfo.name,
-                                                       state.termsIndexDivisor,
-                                                       BytesRef.getUTF8SortedAsUnicodeComparator(),
-                                                       state.codecId);
-      success = true;
-    } finally {
-      if (!success) {
-        postingsReader.close();
-      }
-    }
-
-    success = false;
-    try {
-      FieldsProducer ret = new BlockTermsReader(indexReader,
-                                                state.dir,
-                                                state.fieldInfos,
-                                                state.segmentInfo.name,
-                                                postingsReader,
-                                                state.readBufferSize,
-                                                BytesRef.getUTF8SortedAsUnicodeComparator(),
-                                                StandardCodec.TERMS_CACHE_SIZE,
-                                                state.codecId);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        try {
-          postingsReader.close();
-        } finally {
-          indexReader.close();
-        }
-      }
-    }
-  }
-
-  @Override
-  public void files(Directory dir, SegmentInfo segmentInfo, String codecId, Set<String> files) {
-    SepPostingsReaderImpl.files(segmentInfo, codecId, files);
-    BlockTermsReader.files(dir, segmentInfo, codecId, files);
-    FixedGapTermsIndexReader.files(dir, segmentInfo, codecId, files);
-  }
-
-  @Override
-  public void getExtensions(Set<String> extensions) {
-    SepPostingsWriterImpl.getExtensions(extensions);
-    BlockTermsReader.getExtensions(extensions);
-    FixedGapTermsIndexReader.getIndexExtensions(extensions);
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/index/codecs/mockrandom/MockRandomCodec.java b/lucene/src/test/org/apache/lucene/index/codecs/mockrandom/MockRandomCodec.java
deleted file mode 100644
index 745c619..0000000
--- a/lucene/src/test/org/apache/lucene/index/codecs/mockrandom/MockRandomCodec.java
+++ /dev/null
@@ -1,335 +0,0 @@
-package org.apache.lucene.index.codecs.mockrandom;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Iterator;
-import java.util.Random;
-import java.util.Set;
-
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.codecs.BlockTermsReader;
-import org.apache.lucene.index.codecs.BlockTermsWriter;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.FieldsConsumer;
-import org.apache.lucene.index.codecs.FieldsProducer;
-import org.apache.lucene.index.codecs.FixedGapTermsIndexReader;
-import org.apache.lucene.index.codecs.FixedGapTermsIndexWriter;
-import org.apache.lucene.index.codecs.PostingsReaderBase;
-import org.apache.lucene.index.codecs.PostingsWriterBase;
-import org.apache.lucene.index.codecs.TermStats;
-import org.apache.lucene.index.codecs.TermsIndexReaderBase;
-import org.apache.lucene.index.codecs.TermsIndexWriterBase;
-import org.apache.lucene.index.codecs.VariableGapTermsIndexReader;
-import org.apache.lucene.index.codecs.VariableGapTermsIndexWriter;
-import org.apache.lucene.index.codecs.mockintblock.MockFixedIntBlockCodec;
-import org.apache.lucene.index.codecs.mockintblock.MockVariableIntBlockCodec;
-import org.apache.lucene.index.codecs.mocksep.MockSingleIntFactory;
-import org.apache.lucene.index.codecs.pulsing.PulsingPostingsReaderImpl;
-import org.apache.lucene.index.codecs.pulsing.PulsingPostingsWriterImpl;
-import org.apache.lucene.index.codecs.sep.SepPostingsReaderImpl;
-import org.apache.lucene.index.codecs.sep.SepPostingsWriterImpl;
-import org.apache.lucene.index.codecs.standard.StandardPostingsReader;
-import org.apache.lucene.index.codecs.standard.StandardPostingsWriter;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util._TestUtil;
-
-/**
- * Randomly combines terms index impl w/ postings impls.
- */
-
-public class MockRandomCodec extends Codec {
-
-  private final Random seedRandom;
-  private final String SEED_EXT = "sd";
-
-  public MockRandomCodec(Random random) {
-    name = "MockRandom";
-    this.seedRandom = new Random(random.nextLong());
-  }
-
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-
-    final long seed = seedRandom.nextLong();
-
-    final String seedFileName = IndexFileNames.segmentFileName(state.segmentName, state.codecId, SEED_EXT);
-    final IndexOutput out = state.directory.createOutput(seedFileName);
-    out.writeLong(seed);
-    out.close();
-
-    final Random random = new Random(seed);
-    PostingsWriterBase postingsWriter;
-    final int n = random.nextInt(4);
-
-    if (n == 0) {
-      if (LuceneTestCase.VERBOSE) {
-        System.out.println("MockRandomCodec: writing MockSep postings");
-      }
-      postingsWriter = new SepPostingsWriterImpl(state, new MockSingleIntFactory());
-    } else if (n == 1) {
-      final int blockSize = _TestUtil.nextInt(random, 1, 2000);
-      if (LuceneTestCase.VERBOSE) {
-        System.out.println("MockRandomCodec: writing MockFixedIntBlock(" + blockSize + ") postings");
-      }
-      postingsWriter = new SepPostingsWriterImpl(state, new MockFixedIntBlockCodec.MockIntFactory(blockSize));
-    } else if (n == 2) {
-      final int baseBlockSize = _TestUtil.nextInt(random, 1, 127);
-      if (LuceneTestCase.VERBOSE) {
-        System.out.println("MockRandomCodec: writing MockVariableIntBlock(" + baseBlockSize + ") postings");
-      }
-      postingsWriter = new SepPostingsWriterImpl(state, new MockVariableIntBlockCodec.MockIntFactory(baseBlockSize));
-    } else {
-      if (LuceneTestCase.VERBOSE) {
-        System.out.println("MockRandomCodec: writing Standard postings");
-      }
-      postingsWriter = new StandardPostingsWriter(state);
-    }
-
-    if (random.nextBoolean()) {
-      final int totTFCutoff = _TestUtil.nextInt(random, 1, 20);
-      if (LuceneTestCase.VERBOSE) {
-        System.out.println("MockRandomCodec: pulsing postings with totTFCutoff=" + totTFCutoff);
-      }
-      postingsWriter = new PulsingPostingsWriterImpl(totTFCutoff, postingsWriter);
-    }
-
-    final TermsIndexWriterBase indexWriter;
-    boolean success = false;
-
-    try {
-      if (random.nextBoolean()) {
-        state.termIndexInterval = _TestUtil.nextInt(random, 1, 100);
-        if (LuceneTestCase.VERBOSE) {
-          System.out.println("MockRandomCodec: fixed-gap terms index (tii=" + state.termIndexInterval + ")");
-        }
-        indexWriter = new FixedGapTermsIndexWriter(state);
-      } else {
-        final VariableGapTermsIndexWriter.IndexTermSelector selector;
-        final int n2 = random.nextInt(3);
-        if (n2 == 0) {
-          final int tii = _TestUtil.nextInt(random, 1, 100);
-          selector = new VariableGapTermsIndexWriter.EveryNTermSelector(tii);
-          if (LuceneTestCase.VERBOSE) {
-            System.out.println("MockRandomCodec: variable-gap terms index (tii=" + tii + ")");
-          }
-        } else if (n2 == 1) {
-          final int docFreqThresh = _TestUtil.nextInt(random, 2, 100);
-          final int tii = _TestUtil.nextInt(random, 1, 100);
-          selector = new VariableGapTermsIndexWriter.EveryNOrDocFreqTermSelector(docFreqThresh, tii);
-        } else {
-          final long seed2 = random.nextLong();
-          final int gap = _TestUtil.nextInt(random, 2, 40);
-          if (LuceneTestCase.VERBOSE) {
-            System.out.println("MockRandomCodec: random-gap terms index (max gap=" + gap + ")");
-          }
-          selector = new VariableGapTermsIndexWriter.IndexTermSelector() {
-              final Random rand = new Random(seed2);
-
-              @Override
-              public boolean isIndexTerm(BytesRef term, TermStats stats) {
-                return random.nextInt(gap) == 17;
-              }
-
-              @Override
-              public void newField(FieldInfo fieldInfo) {
-              }
-            };
-        }
-        indexWriter = new VariableGapTermsIndexWriter(state, selector);
-      }
-      success = true;
-    } finally {
-      if (!success) {
-        postingsWriter.close();
-      }
-    }
-
-    success = false;
-    try {
-      FieldsConsumer ret = new BlockTermsWriter(indexWriter, state, postingsWriter, BytesRef.getUTF8SortedAsUnicodeComparator());
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        try {
-          postingsWriter.close();
-        } finally {
-          indexWriter.close();
-        }
-      }
-    }
-  }
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-
-    final String seedFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.codecId, SEED_EXT);
-    final IndexInput in = state.dir.openInput(seedFileName);
-    final long seed = in.readLong();
-    in.close();
-
-    final Random random = new Random(seed);
-    PostingsReaderBase postingsReader;
-    final int n = random.nextInt(4);
-
-    if (n == 0) {
-      if (LuceneTestCase.VERBOSE) {
-        System.out.println("MockRandomCodec: reading MockSep postings");
-      }
-      postingsReader = new SepPostingsReaderImpl(state.dir, state.segmentInfo,
-                                                 state.readBufferSize, new MockSingleIntFactory(), state.codecId);
-    } else if (n == 1) {
-      final int blockSize = _TestUtil.nextInt(random, 1, 2000);
-      if (LuceneTestCase.VERBOSE) {
-        System.out.println("MockRandomCodec: reading MockFixedIntBlock(" + blockSize + ") postings");
-      }
-      postingsReader = new SepPostingsReaderImpl(state.dir, state.segmentInfo,
-                                                 state.readBufferSize, new MockFixedIntBlockCodec.MockIntFactory(blockSize), state.codecId);
-    } else if (n == 2) {
-      final int baseBlockSize = _TestUtil.nextInt(random, 1, 127);
-      if (LuceneTestCase.VERBOSE) {
-        System.out.println("MockRandomCodec: reading MockVariableIntBlock(" + baseBlockSize + ") postings");
-      }
-      postingsReader = new SepPostingsReaderImpl(state.dir, state.segmentInfo,
-                                                 state.readBufferSize, new MockVariableIntBlockCodec.MockIntFactory(baseBlockSize), state.codecId);
-    } else {
-      if (LuceneTestCase.VERBOSE) {
-        System.out.println("MockRandomCodec: reading Standard postings");
-      }
-      postingsReader = new StandardPostingsReader(state.dir, state.segmentInfo, state.readBufferSize, state.codecId);
-    }
-
-    if (random.nextBoolean()) {
-      final int totTFCutoff = _TestUtil.nextInt(random, 1, 20);
-      if (LuceneTestCase.VERBOSE) {
-        System.out.println("MockRandomCodec: reading pulsing postings with totTFCutoff=" + totTFCutoff);
-      }
-      postingsReader = new PulsingPostingsReaderImpl(postingsReader);
-    }
-
-    final TermsIndexReaderBase indexReader;
-    boolean success = false;
-
-    try {
-      if (random.nextBoolean()) {
-        // if termsIndexDivisor is set to -1, we should not touch it. It means a
-        // test explicitly instructed not to load the terms index.
-        if (state.termsIndexDivisor != -1) {
-          state.termsIndexDivisor = _TestUtil.nextInt(random, 1, 10);
-        }
-        if (LuceneTestCase.VERBOSE) {
-          System.out.println("MockRandomCodec: fixed-gap terms index (divisor=" + state.termsIndexDivisor + ")");
-        }
-        indexReader = new FixedGapTermsIndexReader(state.dir,
-                                                   state.fieldInfos,
-                                                   state.segmentInfo.name,
-                                                   state.termsIndexDivisor,
-                                                   BytesRef.getUTF8SortedAsUnicodeComparator(),
-                                                   state.codecId);
-      } else {
-        final int n2 = random.nextInt(3);
-        if (n2 == 1) {
-          random.nextInt();
-        } else if (n2 == 2) {
-          random.nextLong();
-        }
-        if (LuceneTestCase.VERBOSE) {
-          System.out.println("MockRandomCodec: variable-gap terms index (divisor=" + state.termsIndexDivisor + ")");
-        }
-        if (state.termsIndexDivisor != -1) {
-          state.termsIndexDivisor = _TestUtil.nextInt(random, 1, 10);
-        }
-        indexReader = new VariableGapTermsIndexReader(state.dir,
-                                                      state.fieldInfos,
-                                                      state.segmentInfo.name,
-                                                      state.termsIndexDivisor,
-                                                      state.codecId);
-      }
-      success = true;
-    } finally {
-      if (!success) {
-        postingsReader.close();
-      }
-    }
-
-    final int termsCacheSize = _TestUtil.nextInt(random, 1, 1024);
-
-    success = false;
-    try {
-      FieldsProducer ret = new BlockTermsReader(indexReader,
-                                                state.dir,
-                                                state.fieldInfos,
-                                                state.segmentInfo.name,
-                                                postingsReader,
-                                                state.readBufferSize,
-                                                BytesRef.getUTF8SortedAsUnicodeComparator(),
-                                                termsCacheSize,
-                                                state.codecId);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        try {
-          postingsReader.close();
-        } finally {
-          indexReader.close();
-        }
-      }
-    }
-  }
-
-  @Override
-  public void files(Directory dir, SegmentInfo segmentInfo, String codecId, Set<String> files) throws IOException {
-    final String seedFileName = IndexFileNames.segmentFileName(segmentInfo.name, codecId, SEED_EXT);    
-    files.add(seedFileName);
-    SepPostingsReaderImpl.files(segmentInfo, codecId, files);
-    StandardPostingsReader.files(dir, segmentInfo, codecId, files);
-    BlockTermsReader.files(dir, segmentInfo, codecId, files);
-    FixedGapTermsIndexReader.files(dir, segmentInfo, codecId, files);
-    VariableGapTermsIndexReader.files(dir, segmentInfo, codecId, files);
-    
-    // hackish!
-    Iterator<String> it = files.iterator();
-    while(it.hasNext()) {
-      final String file = it.next();
-      if (!dir.fileExists(file)) {
-        it.remove();
-      }
-    }
-    //System.out.println("MockRandom.files return " + files);
-  }
-
-  @Override
-  public void getExtensions(Set<String> extensions) {
-    SepPostingsWriterImpl.getExtensions(extensions);
-    BlockTermsReader.getExtensions(extensions);
-    FixedGapTermsIndexReader.getIndexExtensions(extensions);
-    VariableGapTermsIndexReader.getIndexExtensions(extensions);
-    extensions.add(SEED_EXT);
-    //System.out.println("MockRandom.getExtensions return " + extensions);
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/index/codecs/mocksep/MockSepCodec.java b/lucene/src/test/org/apache/lucene/index/codecs/mocksep/MockSepCodec.java
deleted file mode 100644
index e1e9358..0000000
--- a/lucene/src/test/org/apache/lucene/index/codecs/mocksep/MockSepCodec.java
+++ /dev/null
@@ -1,150 +0,0 @@
-package org.apache.lucene.index.codecs.mocksep;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.FieldsConsumer;
-import org.apache.lucene.index.codecs.FieldsProducer;
-import org.apache.lucene.index.codecs.FixedGapTermsIndexReader;
-import org.apache.lucene.index.codecs.FixedGapTermsIndexWriter;
-import org.apache.lucene.index.codecs.PostingsReaderBase;
-import org.apache.lucene.index.codecs.PostingsWriterBase;
-import org.apache.lucene.index.codecs.BlockTermsReader;
-import org.apache.lucene.index.codecs.BlockTermsWriter;
-import org.apache.lucene.index.codecs.TermsIndexReaderBase;
-import org.apache.lucene.index.codecs.TermsIndexWriterBase;
-import org.apache.lucene.index.codecs.standard.StandardCodec;
-import org.apache.lucene.index.codecs.sep.SepPostingsWriterImpl;
-import org.apache.lucene.index.codecs.sep.SepPostingsReaderImpl;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.BytesRef;
-
-/**
- * A silly codec that simply writes each file separately as
- * single vInts.  Don't use this (performance will be poor)!
- * This is here just to test the core sep codec
- * classes.
- */
-public class MockSepCodec extends Codec {
-
-  public MockSepCodec() {
-    name = "MockSep";
-  }
-
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-
-    PostingsWriterBase postingsWriter = new SepPostingsWriterImpl(state, new MockSingleIntFactory());
-
-    boolean success = false;
-    TermsIndexWriterBase indexWriter;
-    try {
-      indexWriter = new FixedGapTermsIndexWriter(state);
-      success = true;
-    } finally {
-      if (!success) {
-        postingsWriter.close();
-      }
-    }
-
-    success = false;
-    try {
-      FieldsConsumer ret = new BlockTermsWriter(indexWriter, state, postingsWriter, BytesRef.getUTF8SortedAsUnicodeComparator());
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        try {
-          postingsWriter.close();
-        } finally {
-          indexWriter.close();
-        }
-      }
-    }
-  }
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-
-    PostingsReaderBase postingsReader = new SepPostingsReaderImpl(state.dir, state.segmentInfo,
-        state.readBufferSize, new MockSingleIntFactory(), state.codecId);
-
-    TermsIndexReaderBase indexReader;
-    boolean success = false;
-    try {
-      indexReader = new FixedGapTermsIndexReader(state.dir,
-                                                       state.fieldInfos,
-                                                       state.segmentInfo.name,
-                                                       state.termsIndexDivisor,
-                                                       BytesRef.getUTF8SortedAsUnicodeComparator(),
-                                                       state.codecId);
-      success = true;
-    } finally {
-      if (!success) {
-        postingsReader.close();
-      }
-    }
-
-    success = false;
-    try {
-      FieldsProducer ret = new BlockTermsReader(indexReader,
-                                                state.dir,
-                                                state.fieldInfos,
-                                                state.segmentInfo.name,
-                                                postingsReader,
-                                                state.readBufferSize,
-                                                BytesRef.getUTF8SortedAsUnicodeComparator(),
-                                                StandardCodec.TERMS_CACHE_SIZE,
-                                                state.codecId);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        try {
-          postingsReader.close();
-        } finally {
-          indexReader.close();
-        }
-      }
-    }
-  }
-
-  @Override
-  public void files(Directory dir, SegmentInfo segmentInfo, String codecId, Set<String> files) {
-    SepPostingsReaderImpl.files(segmentInfo, codecId, files);
-    BlockTermsReader.files(dir, segmentInfo, codecId, files);
-    FixedGapTermsIndexReader.files(dir, segmentInfo, codecId, files);
-  }
-
-  @Override
-  public void getExtensions(Set<String> extensions) {
-    getSepExtensions(extensions);
-  }
-
-  public static void getSepExtensions(Set<String> extensions) {
-    SepPostingsWriterImpl.getExtensions(extensions);
-    BlockTermsReader.getExtensions(extensions);
-    FixedGapTermsIndexReader.getIndexExtensions(extensions);
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/index/codecs/mocksep/MockSingleIntFactory.java b/lucene/src/test/org/apache/lucene/index/codecs/mocksep/MockSingleIntFactory.java
deleted file mode 100644
index 092db12..0000000
--- a/lucene/src/test/org/apache/lucene/index/codecs/mocksep/MockSingleIntFactory.java
+++ /dev/null
@@ -1,37 +0,0 @@
-package org.apache.lucene.index.codecs.mocksep;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.index.codecs.sep.IntStreamFactory;
-import org.apache.lucene.index.codecs.sep.IntIndexInput;
-import org.apache.lucene.index.codecs.sep.IntIndexOutput;
-
-import java.io.IOException;
-
-/** @lucene.experimental */
-public class MockSingleIntFactory extends IntStreamFactory {
-  @Override
-  public IntIndexInput openInput(Directory dir, String fileName, int readBufferSize) throws IOException {
-    return new MockSingleIntIndexInput(dir, fileName, readBufferSize);
-  }
-  @Override
-  public IntIndexOutput createOutput(Directory dir, String fileName) throws IOException {
-    return new MockSingleIntIndexOutput(dir, fileName);
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/index/codecs/mocksep/MockSingleIntIndexInput.java b/lucene/src/test/org/apache/lucene/index/codecs/mocksep/MockSingleIntIndexInput.java
deleted file mode 100644
index 031794d..0000000
--- a/lucene/src/test/org/apache/lucene/index/codecs/mocksep/MockSingleIntIndexInput.java
+++ /dev/null
@@ -1,123 +0,0 @@
-package org.apache.lucene.index.codecs.mocksep;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.codecs.sep.IntIndexInput;
-import org.apache.lucene.store.DataInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.CodecUtil;
-
-/** Reads IndexInputs written with {@link
- *  SingleIntIndexOutput}.  NOTE: this class is just for
- *  demonstration puprposes (it is a very slow way to read a
- *  block of ints).
- *
- * @lucene.experimental
- */
-public class MockSingleIntIndexInput extends IntIndexInput {
-  private final IndexInput in;
-
-  public MockSingleIntIndexInput(Directory dir, String fileName, int readBufferSize)
-    throws IOException {
-    in = dir.openInput(fileName, readBufferSize);
-    CodecUtil.checkHeader(in, MockSingleIntIndexOutput.CODEC,
-                          MockSingleIntIndexOutput.VERSION_START,
-                          MockSingleIntIndexOutput.VERSION_START);
-  }
-
-  @Override
-  public Reader reader() throws IOException {
-    return new Reader((IndexInput) in.clone());
-  }
-
-  @Override
-  public void close() throws IOException {
-    in.close();
-  }
-
-  public static class Reader extends IntIndexInput.Reader {
-    // clone:
-    private final IndexInput in;
-
-    public Reader(IndexInput in) {
-      this.in = in;
-    }
-
-    /** Reads next single int */
-    @Override
-    public int next() throws IOException {
-      //System.out.println("msii.next() fp=" + in.getFilePointer() + " vs " + in.length());
-      return in.readVInt();
-    }
-  }
-  
-  class Index extends IntIndexInput.Index {
-    private long fp;
-
-    @Override
-    public void read(DataInput indexIn, boolean absolute)
-      throws IOException {
-      if (absolute) {
-        fp = indexIn.readVLong();
-      } else {
-        fp += indexIn.readVLong();
-      }
-    }
-
-    @Override
-    public void read(IntIndexInput.Reader indexIn, boolean absolute)
-      throws IOException {
-      if (absolute) {
-        fp = indexIn.readVLong();
-      } else {
-        fp += indexIn.readVLong();
-      }
-    }
-
-    @Override
-    public void set(IntIndexInput.Index other) {
-      fp = ((Index) other).fp;
-    }
-
-    @Override
-    public void seek(IntIndexInput.Reader other) throws IOException {
-      ((Reader) other).in.seek(fp);
-    }
-
-    @Override
-    public String toString() {
-      return Long.toString(fp);
-    }
-
-    @Override
-    public Object clone() {
-      Index other = new Index();
-      other.fp = fp;
-      return other;
-    }
-  }
-
-  @Override
-  public Index index() {
-    return new Index();
-  }
-}
-
diff --git a/lucene/src/test/org/apache/lucene/index/codecs/mocksep/MockSingleIntIndexOutput.java b/lucene/src/test/org/apache/lucene/index/codecs/mocksep/MockSingleIntIndexOutput.java
deleted file mode 100644
index 98ba2b4..0000000
--- a/lucene/src/test/org/apache/lucene/index/codecs/mocksep/MockSingleIntIndexOutput.java
+++ /dev/null
@@ -1,97 +0,0 @@
-package org.apache.lucene.index.codecs.mocksep;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.CodecUtil;
-import org.apache.lucene.index.codecs.sep.IntIndexOutput;
-import java.io.IOException;
-
-/** Writes ints directly to the file (not in blocks) as
- *  vInt.
- * 
- * @lucene.experimental
-*/
-public class MockSingleIntIndexOutput extends IntIndexOutput {
-  private final IndexOutput out;
-  final static String CODEC = "SINGLE_INTS";
-  final static int VERSION_START = 0;
-  final static int VERSION_CURRENT = VERSION_START;
-
-  public MockSingleIntIndexOutput(Directory dir, String fileName) throws IOException {
-    out = dir.createOutput(fileName);
-    CodecUtil.writeHeader(out, CODEC, VERSION_CURRENT);
-  }
-
-  /** Write an int to the primary file */
-  @Override
-  public void write(int v) throws IOException {
-    assert v >= 0;
-    out.writeVInt(v);
-  }
-
-  @Override
-  public Index index() {
-    return new Index();
-  }
-
-  @Override
-  public void close() throws IOException {
-    out.close();
-  }
-
-  private class Index extends IntIndexOutput.Index {
-    long fp;
-    long lastFP;
-    @Override
-    public void mark() {
-      fp = out.getFilePointer();
-    }
-    @Override
-    public void set(IntIndexOutput.Index other) {
-      lastFP = fp = ((Index) other).fp;
-    }
-    @Override
-    public void write(IndexOutput indexOut, boolean absolute)
-      throws IOException {
-      if (absolute) {
-        indexOut.writeVLong(fp);
-      } else {
-        indexOut.writeVLong(fp - lastFP);
-      }
-      lastFP = fp;
-    }
-
-    @Override
-    public void write(IntIndexOutput indexOut, boolean absolute) 
-      throws IOException {
-      if (absolute) {
-        indexOut.writeVLong(fp);
-      } else {
-        indexOut.writeVLong(fp - lastFP);
-      }
-      lastFP = fp;
-    }
-      
-    @Override
-    public String toString() {
-      return Long.toString(fp);
-    }
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/index/codecs/preflexrw/PreFlexFieldsWriter.java b/lucene/src/test/org/apache/lucene/index/codecs/preflexrw/PreFlexFieldsWriter.java
deleted file mode 100644
index 00b6e01..0000000
--- a/lucene/src/test/org/apache/lucene/index/codecs/preflexrw/PreFlexFieldsWriter.java
+++ /dev/null
@@ -1,209 +0,0 @@
-package org.apache.lucene.index.codecs.preflexrw;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.index.codecs.FieldsConsumer;
-import org.apache.lucene.index.codecs.TermsConsumer;
-import org.apache.lucene.index.codecs.PostingsConsumer;
-import org.apache.lucene.index.codecs.TermStats;
-import org.apache.lucene.index.codecs.standard.DefaultSkipListWriter;
-import org.apache.lucene.index.codecs.preflex.PreFlexCodec;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.codecs.preflex.TermInfo;
-import org.apache.lucene.store.IndexOutput;
-
-import java.io.IOException;
-import java.util.Comparator;
-
-class PreFlexFieldsWriter extends FieldsConsumer {
-
-  private final TermInfosWriter termsOut;
-  private final IndexOutput freqOut;
-  private final IndexOutput proxOut;
-  private final DefaultSkipListWriter skipListWriter;
-  private final int totalNumDocs;
-
-  public PreFlexFieldsWriter(SegmentWriteState state) throws IOException {
-    termsOut = new TermInfosWriter(state.directory,
-                                   state.segmentName,
-                                   state.fieldInfos,
-                                   state.termIndexInterval);
-
-    final String freqFile = IndexFileNames.segmentFileName(state.segmentName, "", PreFlexCodec.FREQ_EXTENSION);
-    freqOut = state.directory.createOutput(freqFile);
-    totalNumDocs = state.numDocs;
-
-    if (state.fieldInfos.hasProx()) {
-      final String proxFile = IndexFileNames.segmentFileName(state.segmentName, "", PreFlexCodec.PROX_EXTENSION);
-      proxOut = state.directory.createOutput(proxFile);
-    } else {
-      proxOut = null;
-    }
-
-    skipListWriter = new DefaultSkipListWriter(termsOut.skipInterval,
-                                               termsOut.maxSkipLevels,
-                                               totalNumDocs,
-                                               freqOut,
-                                               proxOut);
-    //System.out.println("\nw start seg=" + segment);
-  }
-
-  @Override
-  public TermsConsumer addField(FieldInfo field) throws IOException {
-    assert field.number != -1;
-    //System.out.println("w field=" + field.name + " storePayload=" + field.storePayloads + " number=" + field.number);
-    return new PreFlexTermsWriter(field);
-  }
-
-  @Override
-  public void close() throws IOException {
-    termsOut.close();
-    freqOut.close();
-    if (proxOut != null) {
-      proxOut.close();
-    }
-  }
-
-  private class PreFlexTermsWriter extends TermsConsumer {
-    private final FieldInfo fieldInfo;
-    private final boolean omitTF;
-    private final boolean storePayloads;
-    
-    private final TermInfo termInfo = new TermInfo();
-    private final PostingsWriter postingsWriter = new PostingsWriter();
-
-    public PreFlexTermsWriter(FieldInfo fieldInfo) {
-      this.fieldInfo = fieldInfo;
-      omitTF = fieldInfo.omitTermFreqAndPositions;
-      storePayloads = fieldInfo.storePayloads;
-    }
-
-    private class PostingsWriter extends PostingsConsumer {
-      private int lastDocID;
-      private int lastPayloadLength = -1;
-      private int lastPosition;
-      private int df;
-
-      public PostingsWriter reset() {
-        df = 0;
-        lastDocID = 0;
-        lastPayloadLength = -1;
-        return this;
-      }
-
-      @Override
-      public void startDoc(int docID, int termDocFreq) throws IOException {
-        //System.out.println("    w doc=" + docID);
-
-        final int delta = docID - lastDocID;
-        if (docID < 0 || (df > 0 && delta <= 0)) {
-          throw new CorruptIndexException("docs out of order (" + docID + " <= " + lastDocID + " )");
-        }
-
-        if ((++df % termsOut.skipInterval) == 0) {
-          skipListWriter.setSkipData(lastDocID, storePayloads, lastPayloadLength);
-          skipListWriter.bufferSkip(df);
-        }
-
-        lastDocID = docID;
-
-        assert docID < totalNumDocs: "docID=" + docID + " totalNumDocs=" + totalNumDocs;
-
-        if (omitTF) {
-          freqOut.writeVInt(delta);
-        } else {
-          final int code = delta << 1;
-          if (termDocFreq == 1) {
-            freqOut.writeVInt(code|1);
-          } else {
-            freqOut.writeVInt(code);
-            freqOut.writeVInt(termDocFreq);
-          }
-        }
-        lastPosition = 0;
-      }
-
-      @Override
-      public void addPosition(int position, BytesRef payload) throws IOException {
-        assert proxOut != null;
-
-        //System.out.println("      w pos=" + position + " payl=" + payload);
-        final int delta = position - lastPosition;
-        lastPosition = position;
-
-        if (storePayloads) {
-          final int payloadLength = payload == null ? 0 : payload.length;
-          if (payloadLength != lastPayloadLength) {
-            //System.out.println("        write payload len=" + payloadLength);
-            lastPayloadLength = payloadLength;
-            proxOut.writeVInt((delta<<1)|1);
-            proxOut.writeVInt(payloadLength);
-          } else {
-            proxOut.writeVInt(delta << 1);
-          }
-          if (payloadLength > 0) {
-            proxOut.writeBytes(payload.bytes, payload.offset, payload.length);
-          }
-        } else {
-          proxOut.writeVInt(delta);
-        }
-      }
-
-      @Override
-      public void finishDoc() throws IOException {
-      }
-    }
-
-    @Override
-    public PostingsConsumer startTerm(BytesRef text) throws IOException {
-      //System.out.println("  w term=" + text.utf8ToString());
-      skipListWriter.resetSkip();
-      termInfo.freqPointer = freqOut.getFilePointer();
-      if (proxOut != null) {
-        termInfo.proxPointer = proxOut.getFilePointer();
-      }
-      return postingsWriter.reset();
-    }
-
-    @Override
-    public void finishTerm(BytesRef text, TermStats stats) throws IOException {
-      if (stats.docFreq > 0) {
-        long skipPointer = skipListWriter.writeSkip(freqOut);
-        termInfo.docFreq = stats.docFreq;
-        termInfo.skipOffset = (int) (skipPointer - termInfo.freqPointer);
-        //System.out.println("  w finish term=" + text.utf8ToString() + " fnum=" + fieldInfo.number);
-        termsOut.add(fieldInfo.number,
-                     text,
-                     termInfo);
-      }
-    }
-
-    @Override
-    public void finish(long sumTotalTermCount) throws IOException {
-    }
-
-    @Override
-    public Comparator<BytesRef> getComparator() throws IOException {
-      return BytesRef.getUTF8SortedAsUTF16Comparator();
-    }
-  }
-}
\ No newline at end of file
diff --git a/lucene/src/test/org/apache/lucene/index/codecs/preflexrw/PreFlexRWCodec.java b/lucene/src/test/org/apache/lucene/index/codecs/preflexrw/PreFlexRWCodec.java
deleted file mode 100644
index 5a2d947..0000000
--- a/lucene/src/test/org/apache/lucene/index/codecs/preflexrw/PreFlexRWCodec.java
+++ /dev/null
@@ -1,77 +0,0 @@
-package org.apache.lucene.index.codecs.preflexrw;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.codecs.preflex.PreFlexCodec;
-import org.apache.lucene.index.codecs.preflex.PreFlexFields;
-import org.apache.lucene.index.codecs.FieldsConsumer;
-import org.apache.lucene.index.codecs.FieldsProducer;
-import org.apache.lucene.util.LuceneTestCase;
-
-/** Codec, only for testing, that can write and read the
- *  pre-flex index format.
- *
- * @lucene.experimental
- */
-public class PreFlexRWCodec extends PreFlexCodec {
-
-  public PreFlexRWCodec() {
-    // NOTE: we impersonate the PreFlex codec so that it can
-    // read the segments we write!
-    super();
-  }
-  
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    return new PreFlexFieldsWriter(state);
-  }
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-
-    // Whenever IW opens readers, eg for merging, we have to
-    // keep terms order in UTF16:
-
-    return new PreFlexFields(state.dir, state.fieldInfos, state.segmentInfo, state.readBufferSize, state.termsIndexDivisor) {
-      @Override
-      protected boolean sortTermsByUnicode() {
-        // We carefully peek into stack track above us: if
-        // we are part of a "merge", we must sort by UTF16:
-        boolean unicodeSortOrder = true;
-
-        StackTraceElement[] trace = new Exception().getStackTrace();
-        for (int i = 0; i < trace.length; i++) {
-          //System.out.println(trace[i].getClassName());
-          if ("merge".equals(trace[i].getMethodName())) {
-            unicodeSortOrder = false;
-            if (LuceneTestCase.VERBOSE) {
-              System.out.println("NOTE: PreFlexRW codec: forcing legacy UTF16 term sort order");
-            }
-            break;
-          }
-        }
-
-        return unicodeSortOrder;
-      }
-    };
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/index/codecs/preflexrw/TermInfosWriter.java b/lucene/src/test/org/apache/lucene/index/codecs/preflexrw/TermInfosWriter.java
deleted file mode 100644
index 782cd3a..0000000
--- a/lucene/src/test/org/apache/lucene/index/codecs/preflexrw/TermInfosWriter.java
+++ /dev/null
@@ -1,227 +0,0 @@
-package org.apache.lucene.index.codecs.preflexrw;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-import java.io.IOException;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.UnicodeUtil;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.index.codecs.preflex.TermInfo;
-
-
-/** This stores a monotonically increasing set of <Term, TermInfo> pairs in a
-  Directory.  A TermInfos can be written once, in order.  */
-
-final class TermInfosWriter {
-  /** The file format version, a negative number. */
-  public static final int FORMAT = -3;
-
-  // Changed strings to true utf8 with length-in-bytes not
-  // length-in-chars
-  public static final int FORMAT_VERSION_UTF8_LENGTH_IN_BYTES = -4;
-
-  // NOTE: always change this if you switch to a new format!
-  public static final int FORMAT_CURRENT = FORMAT_VERSION_UTF8_LENGTH_IN_BYTES;
-
-  private FieldInfos fieldInfos;
-  private IndexOutput output;
-  private TermInfo lastTi = new TermInfo();
-  private long size;
-
-  // TODO: the default values for these two parameters should be settable from
-  // IndexWriter.  However, once that's done, folks will start setting them to
-  // ridiculous values and complaining that things don't work well, as with
-  // mergeFactor.  So, let's wait until a number of folks find that alternate
-  // values work better.  Note that both of these values are stored in the
-  // segment, so that it's safe to change these w/o rebuilding all indexes.
-
-  /** Expert: The fraction of terms in the "dictionary" which should be stored
-   * in RAM.  Smaller values use more memory, but make searching slightly
-   * faster, while larger values use less memory and make searching slightly
-   * slower.  Searching is typically not dominated by dictionary lookup, so
-   * tweaking this is rarely useful.*/
-  int indexInterval = 128;
-
-  /** Expert: The fraction of {@link TermDocs} entries stored in skip tables,
-   * used to accelerate {@link TermDocs#skipTo(int)}.  Larger values result in
-   * smaller indexes, greater acceleration, but fewer accelerable cases, while
-   * smaller values result in bigger indexes, less acceleration and more
-   * accelerable cases. More detailed experiments would be useful here. */
-  int skipInterval = 16;
-  
-  /** Expert: The maximum number of skip levels. Smaller values result in 
-   * slightly smaller indexes, but slower skipping in big posting lists.
-   */
-  int maxSkipLevels = 10;
-
-  private long lastIndexPointer;
-  private boolean isIndex;
-  private final BytesRef lastTerm = new BytesRef();
-  private int lastFieldNumber = -1;
-
-  private TermInfosWriter other;
-
-  TermInfosWriter(Directory directory, String segment, FieldInfos fis,
-                  int interval)
-       throws IOException {
-    initialize(directory, segment, fis, interval, false);
-    other = new TermInfosWriter(directory, segment, fis, interval, true);
-    other.other = this;
-  }
-
-  private TermInfosWriter(Directory directory, String segment, FieldInfos fis,
-                          int interval, boolean isIndex) throws IOException {
-    initialize(directory, segment, fis, interval, isIndex);
-  }
-
-  private void initialize(Directory directory, String segment, FieldInfos fis,
-                          int interval, boolean isi) throws IOException {
-    indexInterval = interval;
-    fieldInfos = fis;
-    isIndex = isi;
-    output = directory.createOutput(segment + (isIndex ? ".tii" : ".tis"));
-    output.writeInt(FORMAT_CURRENT);              // write format
-    output.writeLong(0);                          // leave space for size
-    output.writeInt(indexInterval);               // write indexInterval
-    output.writeInt(skipInterval);                // write skipInterval
-    output.writeInt(maxSkipLevels);               // write maxSkipLevels
-    assert initUTF16Results();
-  }
-
-  // Currently used only by assert statements
-  UnicodeUtil.UTF16Result utf16Result1;
-  UnicodeUtil.UTF16Result utf16Result2;
-  private final BytesRef scratchBytes = new BytesRef();
-
-  // Currently used only by assert statements
-  private boolean initUTF16Results() {
-    utf16Result1 = new UnicodeUtil.UTF16Result();
-    utf16Result2 = new UnicodeUtil.UTF16Result();
-    return true;
-  }
-
-  // Currently used only by assert statement
-  private int compareToLastTerm(int fieldNumber, BytesRef term) {
-
-    if (lastFieldNumber != fieldNumber) {
-      final int cmp = fieldInfos.fieldName(lastFieldNumber).compareTo(fieldInfos.fieldName(fieldNumber));
-      // If there is a field named "" (empty string) then we
-      // will get 0 on this comparison, yet, it's "OK".  But
-      // it's not OK if two different field numbers map to
-      // the same name.
-      if (cmp != 0 || lastFieldNumber != -1)
-        return cmp;
-    }
-
-    scratchBytes.copy(term);
-    assert lastTerm.offset == 0;
-    UnicodeUtil.UTF8toUTF16(lastTerm.bytes, 0, lastTerm.length, utf16Result1);
-
-    assert scratchBytes.offset == 0;
-    UnicodeUtil.UTF8toUTF16(scratchBytes.bytes, 0, scratchBytes.length, utf16Result2);
-
-    final int len;
-    if (utf16Result1.length < utf16Result2.length)
-      len = utf16Result1.length;
-    else
-      len = utf16Result2.length;
-
-    for(int i=0;i<len;i++) {
-      final char ch1 = utf16Result1.result[i];
-      final char ch2 = utf16Result2.result[i];
-      if (ch1 != ch2)
-        return ch1-ch2;
-    }
-    return utf16Result1.length - utf16Result2.length;
-  }
-
-  /** Adds a new <<fieldNumber, termBytes>, TermInfo> pair to the set.
-    Term must be lexicographically greater than all previous Terms added.
-    TermInfo pointers must be positive and greater than all previous.*/
-  public void add(int fieldNumber, BytesRef term, TermInfo ti)
-    throws IOException {
-
-    assert compareToLastTerm(fieldNumber, term) < 0 ||
-      (isIndex && term.length == 0 && lastTerm.length == 0) :
-      "Terms are out of order: field=" + fieldInfos.fieldName(fieldNumber) + " (number " + fieldNumber + ")" +
-        " lastField=" + fieldInfos.fieldName(lastFieldNumber) + " (number " + lastFieldNumber + ")" +
-        " text=" + term.utf8ToString() + " lastText=" + lastTerm.utf8ToString();
-
-    assert ti.freqPointer >= lastTi.freqPointer: "freqPointer out of order (" + ti.freqPointer + " < " + lastTi.freqPointer + ")";
-    assert ti.proxPointer >= lastTi.proxPointer: "proxPointer out of order (" + ti.proxPointer + " < " + lastTi.proxPointer + ")";
-
-    if (!isIndex && size % indexInterval == 0)
-      other.add(lastFieldNumber, lastTerm, lastTi);                      // add an index term
-
-    writeTerm(fieldNumber, term);                        // write term
-
-    output.writeVInt(ti.docFreq);                       // write doc freq
-    output.writeVLong(ti.freqPointer - lastTi.freqPointer); // write pointers
-    output.writeVLong(ti.proxPointer - lastTi.proxPointer);
-
-    if (ti.docFreq >= skipInterval) {
-      output.writeVInt(ti.skipOffset);
-    }
-
-    if (isIndex) {
-      output.writeVLong(other.output.getFilePointer() - lastIndexPointer);
-      lastIndexPointer = other.output.getFilePointer(); // write pointer
-    }
-
-    lastFieldNumber = fieldNumber;
-    lastTi.set(ti);
-    size++;
-  }
-
-  private void writeTerm(int fieldNumber, BytesRef term)
-       throws IOException {
-
-    //System.out.println("  tiw.write field=" + fieldNumber + " term=" + term.utf8ToString());
-
-    // TODO: UTF16toUTF8 could tell us this prefix
-    // Compute prefix in common with last term:
-    int start = 0;
-    final int limit = term.length < lastTerm.length ? term.length : lastTerm.length;
-    while(start < limit) {
-      if (term.bytes[start+term.offset] != lastTerm.bytes[start+lastTerm.offset])
-        break;
-      start++;
-    }
-
-    final int length = term.length - start;
-    output.writeVInt(start);                     // write shared prefix length
-    output.writeVInt(length);                  // write delta length
-    output.writeBytes(term.bytes, start+term.offset, length);  // write delta bytes
-    output.writeVInt(fieldNumber); // write field num
-    lastTerm.copy(term);
-  }
-
-  /** Called to complete TermInfos creation. */
-  void close() throws IOException {
-    output.seek(4);          // write size after format
-    output.writeLong(size);
-    output.close();
-
-    if (!isIndex)
-      other.close();
-  }
-
-}
diff --git a/lucene/src/test/org/apache/lucene/search/CheckHits.java b/lucene/src/test/org/apache/lucene/search/CheckHits.java
deleted file mode 100644
index 6846e59..0000000
--- a/lucene/src/test/org/apache/lucene/search/CheckHits.java
+++ /dev/null
@@ -1,500 +0,0 @@
-package org.apache.lucene.search;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-import java.util.TreeSet;
-import java.util.Random;
-
-import junit.framework.Assert;
-
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.store.Directory;
-
-public class CheckHits {
-  
-  /**
-   * Some explains methods calculate their values though a slightly
-   * different  order of operations from the actual scoring method ...
-   * this allows for a small amount of variation
-   */
-  public static float EXPLAIN_SCORE_TOLERANCE_DELTA = 0.0002f;
-    
-  /**
-   * Tests that all documents up to maxDoc which are *not* in the
-   * expected result set, have an explanation which indicates no match
-   * (ie: Explanation value of 0.0f)
-   */
-  public static void checkNoMatchExplanations(Query q, String defaultFieldName,
-                                              IndexSearcher searcher, int[] results)
-    throws IOException {
-
-    String d = q.toString(defaultFieldName);
-    Set<Integer> ignore = new TreeSet<Integer>();
-    for (int i = 0; i < results.length; i++) {
-      ignore.add(Integer.valueOf(results[i]));
-    }
-    
-    int maxDoc = searcher.maxDoc();
-    for (int doc = 0; doc < maxDoc; doc++) {
-      if (ignore.contains(Integer.valueOf(doc))) continue;
-
-      Explanation exp = searcher.explain(q, doc);
-      Assert.assertNotNull("Explanation of [["+d+"]] for #"+doc+" is null",
-                             exp);
-      Assert.assertEquals("Explanation of [["+d+"]] for #"+doc+
-                            " doesn't indicate non-match: " + exp.toString(),
-                            0.0f, exp.getValue(), 0.0f);
-    }
-    
-  }
-  
-  /**
-   * Tests that a query matches the an expected set of documents using a
-   * HitCollector.
-   *
-   * <p>
-   * Note that when using the HitCollector API, documents will be collected
-   * if they "match" regardless of what their score is.
-   * </p>
-   * @param query the query to test
-   * @param searcher the searcher to test the query against
-   * @param defaultFieldName used for displaying the query in assertion messages
-   * @param results a list of documentIds that must match the query
-   * @see Searcher#search(Query,Collector)
-   * @see #checkHits
-   */
-  public static void checkHitCollector(Random random, Query query, String defaultFieldName,
-                                       IndexSearcher searcher, int[] results)
-    throws IOException {
-
-    QueryUtils.check(random,query,searcher);
-    
-    Set<Integer> correct = new TreeSet<Integer>();
-    for (int i = 0; i < results.length; i++) {
-      correct.add(Integer.valueOf(results[i]));
-    }
-    final Set<Integer> actual = new TreeSet<Integer>();
-    final Collector c = new SetCollector(actual);
-
-    searcher.search(query, c);
-    Assert.assertEquals("Simple: " + query.toString(defaultFieldName), 
-                        correct, actual);
-
-    for (int i = -1; i < 2; i++) {
-      actual.clear();
-      IndexSearcher s = QueryUtils.wrapUnderlyingReader
-        (random, searcher, i);
-      s.search(query, c);
-      Assert.assertEquals("Wrap Reader " + i + ": " +
-                          query.toString(defaultFieldName),
-                          correct, actual);
-      s.close();
-    }
-  }
-
-  public static class SetCollector extends Collector {
-    final Set<Integer> bag;
-    public SetCollector(Set<Integer> bag) {
-      this.bag = bag;
-    }
-    private int base = 0;
-    @Override
-    public void setScorer(Scorer scorer) throws IOException {}
-    @Override
-    public void collect(int doc) {
-      bag.add(Integer.valueOf(doc + base));
-    }
-    @Override
-    public void setNextReader(AtomicReaderContext context) {
-      base = context.docBase;
-    }
-    @Override
-    public boolean acceptsDocsOutOfOrder() {
-      return true;
-    }
-  }
-
-  /**
-   * Tests that a query matches the an expected set of documents using Hits.
-   *
-   * <p>
-   * Note that when using the Hits API, documents will only be returned
-   * if they have a positive normalized score.
-   * </p>
-   * @param query the query to test
-   * @param searcher the searcher to test the query against
-   * @param defaultFieldName used for displaing the query in assertion messages
-   * @param results a list of documentIds that must match the query
-   * @see Searcher#search(Query, int)
-   * @see #checkHitCollector
-   */
-  public static void checkHits(
-        Random random,
-        Query query,
-        String defaultFieldName,
-        IndexSearcher searcher,
-        int[] results)
-          throws IOException {
-
-    ScoreDoc[] hits = searcher.search(query, 1000).scoreDocs;
-
-    Set<Integer> correct = new TreeSet<Integer>();
-    for (int i = 0; i < results.length; i++) {
-      correct.add(Integer.valueOf(results[i]));
-    }
-
-    Set<Integer> actual = new TreeSet<Integer>();
-    for (int i = 0; i < hits.length; i++) {
-      actual.add(Integer.valueOf(hits[i].doc));
-    }
-
-    Assert.assertEquals(query.toString(defaultFieldName), correct, actual);
-
-    QueryUtils.check(random, query,searcher);
-  }
-
-  /** Tests that a Hits has an expected order of documents */
-  public static void checkDocIds(String mes, int[] results, ScoreDoc[] hits)
-  throws IOException {
-    Assert.assertEquals(mes + " nr of hits", hits.length, results.length);
-    for (int i = 0; i < results.length; i++) {
-      Assert.assertEquals(mes + " doc nrs for hit " + i, results[i], hits[i].doc);
-    }
-  }
-
-  /** Tests that two queries have an expected order of documents,
-   * and that the two queries have the same score values.
-   */
-  public static void checkHitsQuery(
-        Query query,
-        ScoreDoc[] hits1,
-        ScoreDoc[] hits2,
-        int[] results)
-          throws IOException {
-
-    checkDocIds("hits1", results, hits1);
-    checkDocIds("hits2", results, hits2);
-    checkEqual(query, hits1, hits2);
-  }
-
-  public static void checkEqual(Query query, ScoreDoc[] hits1, ScoreDoc[] hits2) throws IOException {
-     final float scoreTolerance = 1.0e-6f;
-     if (hits1.length != hits2.length) {
-       Assert.fail("Unequal lengths: hits1="+hits1.length+",hits2="+hits2.length);
-     }
-    for (int i = 0; i < hits1.length; i++) {
-      if (hits1[i].doc != hits2[i].doc) {
-        Assert.fail("Hit " + i + " docnumbers don't match\n"
-                + hits2str(hits1, hits2,0,0)
-                + "for query:" + query.toString());
-      }
-
-      if ((hits1[i].doc != hits2[i].doc)
-          || Math.abs(hits1[i].score -  hits2[i].score) > scoreTolerance)
-      {
-        Assert.fail("Hit " + i + ", doc nrs " + hits1[i].doc + " and " + hits2[i].doc
-                      + "\nunequal       : " + hits1[i].score
-                      + "\n           and: " + hits2[i].score
-                      + "\nfor query:" + query.toString());
-      }
-    }
-  }
-
-  public static String hits2str(ScoreDoc[] hits1, ScoreDoc[] hits2, int start, int end) throws IOException {
-    StringBuilder sb = new StringBuilder();
-    int len1=hits1==null ? 0 : hits1.length;
-    int len2=hits2==null ? 0 : hits2.length;
-    if (end<=0) {
-      end = Math.max(len1,len2);
-    }
-
-      sb.append("Hits length1=").append(len1).append("\tlength2=").append(len2);
-
-    sb.append('\n');
-    for (int i=start; i<end; i++) {
-        sb.append("hit=").append(i).append(':');
-      if (i<len1) {
-          sb.append(" doc").append(hits1[i].doc).append('=').append(hits1[i].score);
-      } else {
-        sb.append("               ");
-      }
-      sb.append(",\t");
-      if (i<len2) {
-        sb.append(" doc").append(hits2[i].doc).append('=').append(hits2[i].score);
-      }
-      sb.append('\n');
-    }
-    return sb.toString();
-  }
-
-
-  public static String topdocsString(TopDocs docs, int start, int end) {
-    StringBuilder sb = new StringBuilder();
-      sb.append("TopDocs totalHits=").append(docs.totalHits).append(" top=").append(docs.scoreDocs.length).append('\n');
-    if (end<=0) end=docs.scoreDocs.length;
-    else end=Math.min(end,docs.scoreDocs.length);
-    for (int i=start; i<end; i++) {
-      sb.append('\t');
-      sb.append(i);
-      sb.append(") doc=");
-      sb.append(docs.scoreDocs[i].doc);
-      sb.append("\tscore=");
-      sb.append(docs.scoreDocs[i].score);
-      sb.append('\n');
-    }
-    return sb.toString();
-  }
-
-  /**
-   * Asserts that the explanation value for every document matching a
-   * query corresponds with the true score. 
-   *
-   * @see ExplanationAsserter
-   * @see #checkExplanations(Query, String, Searcher, boolean) for a
-   * "deep" testing of the explanation details.
-   *   
-   * @param query the query to test
-   * @param searcher the searcher to test the query against
-   * @param defaultFieldName used for displaing the query in assertion messages
-   */
-  public static void checkExplanations(Query query,
-                                       String defaultFieldName,
-                                       IndexSearcher searcher) throws IOException {
-    checkExplanations(query, defaultFieldName, searcher, false);
-  }
-
-  /**
-   * Asserts that the explanation value for every document matching a
-   * query corresponds with the true score.  Optionally does "deep" 
-   * testing of the explanation details.
-   *
-   * @see ExplanationAsserter
-   * @param query the query to test
-   * @param searcher the searcher to test the query against
-   * @param defaultFieldName used for displaing the query in assertion messages
-   * @param deep indicates whether a deep comparison of sub-Explanation details should be executed
-   */
-  public static void checkExplanations(Query query,
-                                       String defaultFieldName,
-                                       IndexSearcher searcher, 
-                                       boolean deep) throws IOException {
-
-    searcher.search(query,
-                    new ExplanationAsserter
-                    (query, defaultFieldName, searcher, deep));
-
-  }
-
-  /** 
-   * Assert that an explanation has the expected score, and optionally that its
-   * sub-details max/sum/factor match to that score.
-   *
-   * @param q String representation of the query for assertion messages
-   * @param doc Document ID for assertion messages
-   * @param score Real score value of doc with query q
-   * @param deep indicates whether a deep comparison of sub-Explanation details should be executed
-   * @param expl The Explanation to match against score
-   */
-  public static void verifyExplanation(String q, 
-                                       int doc, 
-                                       float score,
-                                       boolean deep,
-                                       Explanation expl) {
-    float value = expl.getValue();
-    Assert.assertEquals(q+": score(doc="+doc+")="+score+
-        " != explanationScore="+value+" Explanation: "+expl,
-        score,value,EXPLAIN_SCORE_TOLERANCE_DELTA);
-
-    if (!deep) return;
-
-    Explanation detail[] = expl.getDetails();
-    if (detail!=null) {
-      if (detail.length==1) {
-        // simple containment, no matter what the description says, 
-        // just verify contained expl has same score
-        verifyExplanation(q,doc,score,deep,detail[0]);
-      } else {
-        // explanation must either:
-        // - end with one of: "product of:", "sum of:", "max of:", or
-        // - have "max plus <x> times others" (where <x> is float).
-        float x = 0;
-        String descr = expl.getDescription().toLowerCase();
-        boolean productOf = descr.endsWith("product of:");
-        boolean sumOf = descr.endsWith("sum of:");
-        boolean maxOf = descr.endsWith("max of:");
-        boolean maxTimesOthers = false;
-        if (!(productOf || sumOf || maxOf)) {
-          // maybe 'max plus x times others'
-          int k1 = descr.indexOf("max plus ");
-          if (k1>=0) {
-            k1 += "max plus ".length();
-            int k2 = descr.indexOf(" ",k1);
-            try {
-              x = Float.parseFloat(descr.substring(k1,k2).trim());
-              if (descr.substring(k2).trim().equals("times others of:")) {
-                maxTimesOthers = true;
-              }
-            } catch (NumberFormatException e) {
-            }
-          }
-        }
-        Assert.assertTrue(
-            q+": multi valued explanation description=\""+descr
-            +"\" must be 'max of plus x times others' or end with 'product of'"
-            +" or 'sum of:' or 'max of:' - "+expl,
-            productOf || sumOf || maxOf || maxTimesOthers);
-        float sum = 0;
-        float product = 1;
-        float max = 0;
-        for (int i=0; i<detail.length; i++) {
-          float dval = detail[i].getValue();
-          verifyExplanation(q,doc,dval,deep,detail[i]);
-          product *= dval;
-          sum += dval;
-          max = Math.max(max,dval);
-        }
-        float combined = 0;
-        if (productOf) {
-          combined = product;
-        } else if (sumOf) {
-          combined = sum;
-        } else if (maxOf) {
-          combined = max;
-        } else if (maxTimesOthers) {
-          combined = max + x * (sum - max);
-        } else {
-            Assert.assertTrue("should never get here!",false);
-        }
-        Assert.assertEquals(q+": actual subDetails combined=="+combined+
-            " != value="+value+" Explanation: "+expl,
-            combined,value,EXPLAIN_SCORE_TOLERANCE_DELTA);
-      }
-    }
-  }
-
-  /**
-   * an IndexSearcher that implicitly checks hte explanation of every match
-   * whenever it executes a search.
-   *
-   * @see ExplanationAsserter
-   */
-  public static class ExplanationAssertingSearcher extends IndexSearcher {
-    public ExplanationAssertingSearcher(Directory d) throws IOException {
-      super(d, true);
-    }
-    public ExplanationAssertingSearcher(IndexReader r) throws IOException {
-      super(r);
-    }
-    protected void checkExplanations(Query q) throws IOException {
-      super.search(q, null,
-                   new ExplanationAsserter
-                   (q, null, this));
-    }
-    @Override
-    public TopFieldDocs search(Query query,
-                               Filter filter,
-                               int n,
-                               Sort sort) throws IOException {
-      
-      checkExplanations(query);
-      return super.search(query,filter,n,sort);
-    }
-    @Override
-    public void search(Query query, Collector results) throws IOException {
-      checkExplanations(query);
-      super.search(query, results);
-    }
-    @Override
-    public void search(Query query, Filter filter, Collector results) throws IOException {
-      checkExplanations(query);
-      super.search(query, filter, results);
-    }
-    @Override
-    public TopDocs search(Query query, Filter filter,
-                          int n) throws IOException {
-
-      checkExplanations(query);
-      return super.search(query,filter, n);
-    }
-  }
-    
-  /**
-   * Asserts that the score explanation for every document matching a
-   * query corresponds with the true score.
-   *
-   * NOTE: this HitCollector should only be used with the Query and Searcher
-   * specified at when it is constructed.
-   *
-   * @see CheckHits#verifyExplanation
-   */
-  public static class ExplanationAsserter extends Collector {
-
-    Query q;
-    IndexSearcher s;
-    String d;
-    boolean deep;
-    
-    Scorer scorer;
-    private int base = 0;
-
-    /** Constructs an instance which does shallow tests on the Explanation */
-    public ExplanationAsserter(Query q, String defaultFieldName, IndexSearcher s) {
-      this(q,defaultFieldName,s,false);
-    }      
-    public ExplanationAsserter(Query q, String defaultFieldName, IndexSearcher s, boolean deep) {
-      this.q=q;
-      this.s=s;
-      this.d = q.toString(defaultFieldName);
-      this.deep=deep;
-    }      
-    
-    @Override
-    public void setScorer(Scorer scorer) throws IOException {
-      this.scorer = scorer;     
-    }
-    
-    @Override
-    public void collect(int doc) throws IOException {
-      Explanation exp = null;
-      doc = doc + base;
-      try {
-        exp = s.explain(q, doc);
-      } catch (IOException e) {
-        throw new RuntimeException
-          ("exception in hitcollector of [["+d+"]] for #"+doc, e);
-      }
-      
-      Assert.assertNotNull("Explanation of [["+d+"]] for #"+doc+" is null", exp);
-      verifyExplanation(d,doc,scorer.score(),deep,exp);
-    }
-    @Override
-    public void setNextReader(AtomicReaderContext context) {
-      base = context.docBase;
-    }
-    @Override
-    public boolean acceptsDocsOutOfOrder() {
-      return true;
-    }
-  }
-
-}
-
-
diff --git a/lucene/src/test/org/apache/lucene/search/QueryUtils.java b/lucene/src/test/org/apache/lucene/search/QueryUtils.java
deleted file mode 100644
index e84b2f9..0000000
--- a/lucene/src/test/org/apache/lucene/search/QueryUtils.java
+++ /dev/null
@@ -1,444 +0,0 @@
-package org.apache.lucene.search;
-
-import java.io.ByteArrayInputStream;
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.io.ObjectInputStream;
-import java.io.ObjectOutputStream;
-import java.util.Random;
-import java.lang.reflect.Method;
-
-import junit.framework.Assert;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.MultiReader;
-import org.apache.lucene.search.Weight.ScorerContext;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.MockDirectoryWrapper;
-import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.ReaderUtil;
-
-import static org.apache.lucene.util.LuceneTestCase.TEST_VERSION_CURRENT;
-
-/**
- * Copyright 2005 Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-
-public class QueryUtils {
-
-  /** Check the types of things query objects should be able to do. */
-  public static void check(Query q) {
-    checkHashEquals(q);
-  }
-
-  /** check very basic hashCode and equals */
-  public static void checkHashEquals(Query q) {
-    Query q2 = (Query)q.clone();
-    checkEqual(q,q2);
-
-    Query q3 = (Query)q.clone();
-    q3.setBoost(7.21792348f);
-    checkUnequal(q,q3);
-
-    // test that a class check is done so that no exception is thrown
-    // in the implementation of equals()
-    Query whacky = new Query() {
-      @Override
-      public String toString(String field) {
-        return "My Whacky Query";
-      }
-    };
-    whacky.setBoost(q.getBoost());
-    checkUnequal(q, whacky);
-    
-    // null test
-    Assert.assertFalse(q.equals(null));
-  }
-
-  public static void checkEqual(Query q1, Query q2) {
-    Assert.assertEquals(q1, q2);
-    Assert.assertEquals(q1.hashCode(), q2.hashCode());
-  }
-
-  public static void checkUnequal(Query q1, Query q2) {
-    Assert.assertTrue(!q1.equals(q2));
-    Assert.assertTrue(!q2.equals(q1));
-
-    // possible this test can fail on a hash collision... if that
-    // happens, please change test to use a different example.
-    Assert.assertTrue(q1.hashCode() != q2.hashCode());
-  }
-  
-  /** deep check that explanations of a query 'score' correctly */
-  public static void checkExplanations (final Query q, final IndexSearcher s) throws IOException {
-    CheckHits.checkExplanations(q, null, s, true);
-  }
-  
-  /** 
-   * Various query sanity checks on a searcher, some checks are only done for
-   * instanceof IndexSearcher.
-   *
-   * @see #check(Query)
-   * @see #checkFirstSkipTo
-   * @see #checkSkipTo
-   * @see #checkExplanations
-   * @see #checkSerialization
-   * @see #checkEqual
-   */
-  public static void check(Random random, Query q1, IndexSearcher s) {
-    check(random, q1, s, true);
-  }
-  private static void check(Random random, Query q1, IndexSearcher s, boolean wrap) {
-    try {
-      check(q1);
-      if (s!=null) {
-        checkFirstSkipTo(q1,s);
-        checkSkipTo(q1,s);
-        if (wrap) {
-          IndexSearcher wrapped;
-          check(random, q1, wrapped = wrapUnderlyingReader(random, s, -1), false);
-          wrapped.close();
-          check(random, q1, wrapped = wrapUnderlyingReader(random, s,  0), false);
-          wrapped.close();
-          check(random, q1, wrapped = wrapUnderlyingReader(random, s, +1), false);
-          wrapped.close();
-        }
-        checkExplanations(q1,s);
-        checkSerialization(q1,s);
-        
-        Query q2 = (Query)q1.clone();
-        checkEqual(s.rewrite(q1),
-                   s.rewrite(q2));
-      }
-    } catch (IOException e) {
-      throw new RuntimeException(e);
-    }
-  }
-
-  /**
-   * Given an IndexSearcher, returns a new IndexSearcher whose IndexReader 
-   * is a MultiReader containing the Reader of the original IndexSearcher, 
-   * as well as several "empty" IndexReaders -- some of which will have 
-   * deleted documents in them.  This new IndexSearcher should 
-   * behave exactly the same as the original IndexSearcher.
-   * @param s the searcher to wrap
-   * @param edge if negative, s will be the first sub; if 0, s will be in the middle, if positive s will be the last sub
-   */
-  public static IndexSearcher wrapUnderlyingReader(Random random, final IndexSearcher s, final int edge) 
-    throws IOException {
-
-    IndexReader r = s.getIndexReader();
-
-    // we can't put deleted docs before the nested reader, because
-    // it will throw off the docIds
-    IndexReader[] readers = new IndexReader[] {
-      edge < 0 ? r : IndexReader.open(makeEmptyIndex(random, 0), true),
-      IndexReader.open(makeEmptyIndex(random, 0), true),
-      new MultiReader(IndexReader.open(makeEmptyIndex(random, edge < 0 ? 4 : 0), true),
-          IndexReader.open(makeEmptyIndex(random, 0), true),
-          0 == edge ? r : IndexReader.open(makeEmptyIndex(random, 0), true)),
-      IndexReader.open(makeEmptyIndex(random, 0 < edge ? 0 : 7), true),
-      IndexReader.open(makeEmptyIndex(random, 0), true),
-      new MultiReader(IndexReader.open(makeEmptyIndex(random, 0 < edge ? 0 : 5), true),
-          IndexReader.open(makeEmptyIndex(random, 0), true),
-          0 < edge ? r : IndexReader.open(makeEmptyIndex(random, 0), true))
-    };
-    IndexSearcher out = LuceneTestCase.newSearcher(new MultiReader(readers));
-    out.setSimilarityProvider(s.getSimilarityProvider());
-    return out;
-  }
-
-  private static Directory makeEmptyIndex(Random random, final int numDeletedDocs) 
-    throws IOException {
-    Directory d = new MockDirectoryWrapper(random, new RAMDirectory());
-      IndexWriter w = new IndexWriter(d, new IndexWriterConfig(
-        TEST_VERSION_CURRENT, new MockAnalyzer()));
-      for (int i = 0; i < numDeletedDocs; i++) {
-        w.addDocument(new Document());
-      }
-      w.commit();
-      w.deleteDocuments( new MatchAllDocsQuery() );
-      try {
-        // Carefully invoke what is a package-private (test
-        // only, internal) method on IndexWriter:
-        Method m = IndexWriter.class.getDeclaredMethod("keepFullyDeletedSegments");
-        m.setAccessible(true);
-        m.invoke(w);
-      } catch (Exception e) {
-        // Should not happen?
-        throw new RuntimeException(e);
-      }
-      w.commit();
-
-      if (0 < numDeletedDocs)
-        Assert.assertTrue("writer has no deletions", w.hasDeletions());
-
-      Assert.assertEquals("writer is missing some deleted docs", 
-                          numDeletedDocs, w.maxDoc());
-      Assert.assertEquals("writer has non-deleted docs", 
-                          0, w.numDocs());
-      w.close();
-      IndexReader r = IndexReader.open(d, true);
-      Assert.assertEquals("reader has wrong number of deleted docs", 
-                          numDeletedDocs, r.numDeletedDocs());
-      r.close();
-      return d;
-  }
-  
-
-  /** check that the query weight is serializable. 
-   * @throws IOException if serialization check fail. 
-   */
-  private static void checkSerialization(Query q, IndexSearcher s) throws IOException {
-    Weight w = q.weight(s);
-    try {
-      ByteArrayOutputStream bos = new ByteArrayOutputStream();
-      ObjectOutputStream oos = new ObjectOutputStream(bos);
-      oos.writeObject(w);
-      oos.close();
-      ObjectInputStream ois = new ObjectInputStream(new ByteArrayInputStream(bos.toByteArray()));
-      ois.readObject();
-      ois.close();
-      
-      //skip equals() test for now - most weights don't override equals() and we won't add this just for the tests.
-      //TestCase.assertEquals("writeObject(w) != w.  ("+w+")",w2,w);   
-      
-    } catch (Exception e) {
-      IOException e2 = new IOException("Serialization failed for "+w);
-      e2.initCause(e);
-      throw e2;
-    }
-  }
-
-  /** alternate scorer skipTo(),skipTo(),next(),next(),skipTo(),skipTo(), etc
-   * and ensure a hitcollector receives same docs and scores
-   */
-  public static void checkSkipTo(final Query q, final IndexSearcher s) throws IOException {
-    //System.out.println("Checking "+q);
-    final AtomicReaderContext[] readerContextArray = ReaderUtil.leaves(s.getTopReaderContext());
-    if (q.weight(s).scoresDocsOutOfOrder()) return;  // in this case order of skipTo() might differ from that of next().
-
-    final int skip_op = 0;
-    final int next_op = 1;
-    final int orders [][] = {
-        {next_op},
-        {skip_op},
-        {skip_op, next_op},
-        {next_op, skip_op},
-        {skip_op, skip_op, next_op, next_op},
-        {next_op, next_op, skip_op, skip_op},
-        {skip_op, skip_op, skip_op, next_op, next_op},
-    };
-    for (int k = 0; k < orders.length; k++) {
-
-        final int order[] = orders[k];
-        // System.out.print("Order:");for (int i = 0; i < order.length; i++)
-        // System.out.print(order[i]==skip_op ? " skip()":" next()");
-        // System.out.println();
-        final int opidx[] = { 0 };
-        final int lastDoc[] = {-1};
-
-        // FUTURE: ensure scorer.doc()==-1
-
-        final float maxDiff = 1e-5f;
-        final IndexReader lastReader[] = {null};
-
-        s.search(q, new Collector() {
-          private Scorer sc;
-          private Scorer scorer;
-          private int leafPtr;
-
-          @Override
-          public void setScorer(Scorer scorer) throws IOException {
-            this.sc = scorer;
-          }
-
-          @Override
-          public void collect(int doc) throws IOException {
-            float score = sc.score();
-            lastDoc[0] = doc;
-            try {
-              if (scorer == null) {
-                Weight w = q.weight(s);
-                scorer = w.scorer(readerContextArray[leafPtr], ScorerContext.def());
-              }
-              
-              int op = order[(opidx[0]++) % order.length];
-              // System.out.println(op==skip_op ?
-              // "skip("+(sdoc[0]+1)+")":"next()");
-              boolean more = op == skip_op ? scorer.advance(scorer.docID() + 1) != DocIdSetIterator.NO_MORE_DOCS
-                  : scorer.nextDoc() != DocIdSetIterator.NO_MORE_DOCS;
-              int scorerDoc = scorer.docID();
-              float scorerScore = scorer.score();
-              float scorerScore2 = scorer.score();
-              float scoreDiff = Math.abs(score - scorerScore);
-              float scorerDiff = Math.abs(scorerScore2 - scorerScore);
-              if (!more || doc != scorerDoc || scoreDiff > maxDiff
-                  || scorerDiff > maxDiff) {
-                StringBuilder sbord = new StringBuilder();
-                for (int i = 0; i < order.length; i++)
-                  sbord.append(order[i] == skip_op ? " skip()" : " next()");
-                throw new RuntimeException("ERROR matching docs:" + "\n\t"
-                    + (doc != scorerDoc ? "--> " : "") + "doc=" + doc + ", scorerDoc=" + scorerDoc
-                    + "\n\t" + (!more ? "--> " : "") + "tscorer.more=" + more
-                    + "\n\t" + (scoreDiff > maxDiff ? "--> " : "")
-                    + "scorerScore=" + scorerScore + " scoreDiff=" + scoreDiff
-                    + " maxDiff=" + maxDiff + "\n\t"
-                    + (scorerDiff > maxDiff ? "--> " : "") + "scorerScore2="
-                    + scorerScore2 + " scorerDiff=" + scorerDiff
-                    + "\n\thitCollector.doc=" + doc + " score=" + score
-                    + "\n\t Scorer=" + scorer + "\n\t Query=" + q + "  "
-                    + q.getClass().getName() + "\n\t Searcher=" + s
-                    + "\n\t Order=" + sbord + "\n\t Op="
-                    + (op == skip_op ? " skip()" : " next()"));
-              }
-            } catch (IOException e) {
-              throw new RuntimeException(e);
-            }
-          }
-
-          @Override
-          public void setNextReader(AtomicReaderContext context) throws IOException {
-            // confirm that skipping beyond the last doc, on the
-            // previous reader, hits NO_MORE_DOCS
-            if (lastReader[0] != null) {
-              final IndexReader previousReader = lastReader[0];
-              IndexSearcher indexSearcher = LuceneTestCase.newSearcher(previousReader);
-              Weight w = q.weight(indexSearcher);
-              Scorer scorer = w.scorer((AtomicReaderContext)indexSearcher.getTopReaderContext(), ScorerContext.def());
-              if (scorer != null) {
-                boolean more = scorer.advance(lastDoc[0] + 1) != DocIdSetIterator.NO_MORE_DOCS;
-                Assert.assertFalse("query's last doc was "+ lastDoc[0] +" but skipTo("+(lastDoc[0]+1)+") got to "+scorer.docID(),more);
-              }
-              leafPtr++;
-              indexSearcher.close();
-            }
-            lastReader[0] = context.reader;
-            assert readerContextArray[leafPtr].reader == context.reader;
-            this.scorer = null;
-            lastDoc[0] = -1;
-          }
-
-          @Override
-          public boolean acceptsDocsOutOfOrder() {
-            return true;
-          }
-        });
-
-        if (lastReader[0] != null) {
-          // confirm that skipping beyond the last doc, on the
-          // previous reader, hits NO_MORE_DOCS
-          final IndexReader previousReader = lastReader[0];
-          IndexSearcher indexSearcher = LuceneTestCase.newSearcher(previousReader);
-          Weight w = q.weight(indexSearcher);
-          Scorer scorer = w.scorer((AtomicReaderContext)previousReader.getTopReaderContext(), ScorerContext.def());
-          if (scorer != null) {
-            boolean more = scorer.advance(lastDoc[0] + 1) != DocIdSetIterator.NO_MORE_DOCS;
-            Assert.assertFalse("query's last doc was "+ lastDoc[0] +" but skipTo("+(lastDoc[0]+1)+") got to "+scorer.docID(),more);
-          }
-          indexSearcher.close();
-        }
-      }
-  }
-    
-  // check that first skip on just created scorers always goes to the right doc
-  private static void checkFirstSkipTo(final Query q, final IndexSearcher s) throws IOException {
-    //System.out.println("checkFirstSkipTo: "+q);
-    final float maxDiff = 1e-3f;
-    final int lastDoc[] = {-1};
-    final IndexReader lastReader[] = {null};
-    final AtomicReaderContext[] context = ReaderUtil.leaves(s.getTopReaderContext());
-    s.search(q,new Collector() {
-      private Scorer scorer;
-      private int leafPtr;
-      @Override
-      public void setScorer(Scorer scorer) throws IOException {
-        this.scorer = scorer;
-      }
-      @Override
-      public void collect(int doc) throws IOException {
-        float score = scorer.score();
-        try {
-          long startMS = System.currentTimeMillis();
-          for (int i=lastDoc[0]+1; i<=doc; i++) {
-            Weight w = q.weight(s);
-            Scorer scorer = w.scorer(context[leafPtr], ScorerContext.def());
-            Assert.assertTrue("query collected "+doc+" but skipTo("+i+") says no more docs!",scorer.advance(i) != DocIdSetIterator.NO_MORE_DOCS);
-            Assert.assertEquals("query collected "+doc+" but skipTo("+i+") got to "+scorer.docID(),doc,scorer.docID());
-            float skipToScore = scorer.score();
-            Assert.assertEquals("unstable skipTo("+i+") score!",skipToScore,scorer.score(),maxDiff); 
-            Assert.assertEquals("query assigned doc "+doc+" a score of <"+score+"> but skipTo("+i+") has <"+skipToScore+">!",score,skipToScore,maxDiff);
-            
-            // Hurry things along if they are going slow (eg
-            // if you got SimpleText codec this will kick in):
-            if (i < doc && System.currentTimeMillis() - startMS > 5) {
-              i = doc-1;
-            }
-          }
-          lastDoc[0] = doc;
-        } catch (IOException e) {
-          throw new RuntimeException(e);
-        }
-      }
-
-      @Override
-      public void setNextReader(AtomicReaderContext context) throws IOException {
-        // confirm that skipping beyond the last doc, on the
-        // previous reader, hits NO_MORE_DOCS
-        if (lastReader[0] != null) {
-          final IndexReader previousReader = lastReader[0];
-          IndexSearcher indexSearcher = LuceneTestCase.newSearcher(previousReader);
-          Weight w = q.weight(indexSearcher);
-          Scorer scorer = w.scorer((AtomicReaderContext)indexSearcher.getTopReaderContext(), ScorerContext.def());
-          if (scorer != null) {
-            boolean more = scorer.advance(lastDoc[0] + 1) != DocIdSetIterator.NO_MORE_DOCS;
-            Assert.assertFalse("query's last doc was "+ lastDoc[0] +" but skipTo("+(lastDoc[0]+1)+") got to "+scorer.docID(),more);
-          }
-          indexSearcher.close();
-          leafPtr++;
-        }
-
-        lastReader[0] = context.reader;
-        lastDoc[0] = -1;
-      }
-      @Override
-      public boolean acceptsDocsOutOfOrder() {
-        return false;
-      }
-    });
-
-    if (lastReader[0] != null) {
-      // confirm that skipping beyond the last doc, on the
-      // previous reader, hits NO_MORE_DOCS
-      final IndexReader previousReader = lastReader[0];
-      IndexSearcher indexSearcher = LuceneTestCase.newSearcher(previousReader);
-      Weight w = q.weight(indexSearcher);
-      Scorer scorer = w.scorer((AtomicReaderContext)indexSearcher.getTopReaderContext(), ScorerContext.def());
-      if (scorer != null) {
-        boolean more = scorer.advance(lastDoc[0] + 1) != DocIdSetIterator.NO_MORE_DOCS;
-        Assert.assertFalse("query's last doc was "+ lastDoc[0] +" but skipTo("+(lastDoc[0]+1)+") got to "+scorer.docID(),more);
-      }
-      indexSearcher.close();
-    }
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/store/MockDirectoryWrapper.java b/lucene/src/test/org/apache/lucene/store/MockDirectoryWrapper.java
deleted file mode 100644
index bb9552b..0000000
--- a/lucene/src/test/org/apache/lucene/store/MockDirectoryWrapper.java
+++ /dev/null
@@ -1,550 +0,0 @@
-package org.apache.lucene.store;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable;
-import java.io.FileNotFoundException;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.IdentityHashMap;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.Random;
-import java.util.Set;
-
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util._TestUtil;
-
-/**
- * This is a Directory Wrapper that adds methods
- * intended to be used only by unit tests.
- */
-
-public class MockDirectoryWrapper extends Directory {
-  final Directory delegate;
-  long maxSize;
-
-  // Max actual bytes used. This is set by MockRAMOutputStream:
-  long maxUsedSize;
-  double randomIOExceptionRate;
-  Random randomState;
-  boolean noDeleteOpenFile = true;
-  boolean preventDoubleWrite = true;
-  boolean checkIndexOnClose = true;
-  boolean trackDiskUsage = false;
-  private Set<String> unSyncedFiles;
-  private Set<String> createdFiles;
-  Set<String> openFilesForWrite = new HashSet<String>();
-  volatile boolean crashed;
-
-  // use this for tracking files for crash.
-  // additionally: provides debugging information in case you leave one open
-  Map<Closeable,Exception> openFileHandles = Collections.synchronizedMap(new IdentityHashMap<Closeable,Exception>());
-
-  // NOTE: we cannot initialize the Map here due to the
-  // order in which our constructor actually does this
-  // member initialization vs when it calls super.  It seems
-  // like super is called, then our members are initialized:
-  Map<String,Integer> openFiles;
-
-  // Only tracked if noDeleteOpenFile is true: if an attempt
-  // is made to delete an open file, we enroll it here.
-  Set<String> openFilesDeleted;
-
-  private synchronized void init() {
-    if (openFiles == null) {
-      openFiles = new HashMap<String,Integer>();
-      openFilesDeleted = new HashSet<String>();
-    }
-
-    if (createdFiles == null)
-      createdFiles = new HashSet<String>();
-    if (unSyncedFiles == null)
-      unSyncedFiles = new HashSet<String>();
-  }
-
-  public MockDirectoryWrapper(Random random, Directory delegate) {
-    this.delegate = delegate;
-    // must make a private random since our methods are
-    // called from different threads; else test failures may
-    // not be reproducible from the original seed
-    this.randomState = new Random(random.nextInt());
-    init();
-  }
-
-  public void setTrackDiskUsage(boolean v) {
-    trackDiskUsage = v;
-  }
-
-  /** If set to true, we throw an IOException if the same
-   *  file is opened by createOutput, ever. */
-  public void setPreventDoubleWrite(boolean value) {
-    preventDoubleWrite = value;
-  }
-
-  @Override
-  public synchronized void sync(Collection<String> names) throws IOException {
-    maybeYield();
-    for (String name : names)
-      maybeThrowDeterministicException();
-    if (crashed)
-      throw new IOException("cannot sync after crash");
-    unSyncedFiles.removeAll(names);
-    delegate.sync(names);
-  }
-  
-  @Override
-  public String toString() {
-    maybeYield();
-    return "MockDirWrapper(" + delegate + ")";
-  }
-
-  public synchronized final long sizeInBytes() throws IOException {
-    if (delegate instanceof RAMDirectory)
-      return ((RAMDirectory) delegate).sizeInBytes();
-    else {
-      // hack
-      long size = 0;
-      for (String file : delegate.listAll())
-        size += delegate.fileLength(file);
-      return size;
-    }
-  }
-
-  /** Simulates a crash of OS or machine by overwriting
-   *  unsynced files. */
-  public synchronized void crash() throws IOException {
-    crashed = true;
-    openFiles = new HashMap<String,Integer>();
-    openFilesForWrite = new HashSet<String>();
-    openFilesDeleted = new HashSet<String>();
-    Iterator<String> it = unSyncedFiles.iterator();
-    unSyncedFiles = new HashSet<String>();
-    // first force-close all files, so we can corrupt on windows etc.
-    // clone the file map, as these guys want to remove themselves on close.
-    Map<Closeable,Exception> m = new IdentityHashMap<Closeable,Exception>(openFileHandles);
-    for (Closeable f : m.keySet())
-      try {
-        f.close();
-      } catch (Exception ignored) {}
-    
-    int count = 0;
-    while(it.hasNext()) {
-      String name = it.next();
-      if (count % 3 == 0) {
-        deleteFile(name, true);
-      } else if (count % 3 == 1) {
-        // Zero out file entirely
-        long length = fileLength(name);
-        byte[] zeroes = new byte[256];
-        long upto = 0;
-        IndexOutput out = delegate.createOutput(name);
-        while(upto < length) {
-          final int limit = (int) Math.min(length-upto, zeroes.length);
-          out.writeBytes(zeroes, 0, limit);
-          upto += limit;
-        }
-        out.close();
-      } else if (count % 3 == 2) {
-        // Truncate the file:
-        IndexOutput out = delegate.createOutput(name);
-        out.setLength(fileLength(name)/2);
-        out.close();
-      }
-      count++;
-    }
-  }
-
-  public synchronized void clearCrash() throws IOException {
-    crashed = false;
-  }
-
-  public void setMaxSizeInBytes(long maxSize) {
-    this.maxSize = maxSize;
-  }
-  public long getMaxSizeInBytes() {
-    return this.maxSize;
-  }
-
-  /**
-   * Returns the peek actual storage used (bytes) in this
-   * directory.
-   */
-  public long getMaxUsedSizeInBytes() {
-    return this.maxUsedSize;
-  }
-  public void resetMaxUsedSizeInBytes() throws IOException {
-    this.maxUsedSize = getRecomputedActualSizeInBytes();
-  }
-
-  /**
-   * Emulate windows whereby deleting an open file is not
-   * allowed (raise IOException).
-  */
-  public void setNoDeleteOpenFile(boolean value) {
-    this.noDeleteOpenFile = value;
-  }
-  public boolean getNoDeleteOpenFile() {
-    return noDeleteOpenFile;
-  }
-
-  /**
-   * Set whether or not checkindex should be run
-   * on close
-   */
-  public void setCheckIndexOnClose(boolean value) {
-    this.checkIndexOnClose = value;
-  }
-  
-  public boolean getCheckIndexOnClose() {
-    return checkIndexOnClose;
-  }
-  /**
-   * If 0.0, no exceptions will be thrown.  Else this should
-   * be a double 0.0 - 1.0.  We will randomly throw an
-   * IOException on the first write to an OutputStream based
-   * on this probability.
-   */
-  public void setRandomIOExceptionRate(double rate) {
-    randomIOExceptionRate = rate;
-  }
-  public double getRandomIOExceptionRate() {
-    return randomIOExceptionRate;
-  }
-
-  void maybeThrowIOException() throws IOException {
-    if (randomIOExceptionRate > 0.0) {
-      int number = Math.abs(randomState.nextInt() % 1000);
-      if (number < randomIOExceptionRate*1000) {
-        if (LuceneTestCase.VERBOSE) {
-          System.out.println(Thread.currentThread().getName() + ": MockDirectoryWrapper: now throw random exception");
-          new Throwable().printStackTrace(System.out);
-        }
-        throw new IOException("a random IOException");
-      }
-    }
-  }
-
-  @Override
-  public synchronized void deleteFile(String name) throws IOException {
-    maybeYield();
-    deleteFile(name, false);
-  }
-
-  // sets the cause of the incoming ioe to be the stack
-  // trace when the offending file name was opened
-  private synchronized IOException fillOpenTrace(IOException ioe, String name, boolean input) {
-    for(Map.Entry<Closeable,Exception> ent : openFileHandles.entrySet()) {
-      if (input && ent.getKey() instanceof MockIndexInputWrapper && ((MockIndexInputWrapper) ent.getKey()).name.equals(name)) {
-        ioe.initCause(ent.getValue());
-        break;
-      } else if (!input && ent.getKey() instanceof MockIndexOutputWrapper && ((MockIndexOutputWrapper) ent.getKey()).name.equals(name)) {
-        ioe.initCause(ent.getValue());
-        break;
-      }
-    }
-    return ioe;
-  }
-
-  private void maybeYield() {
-    if (randomState.nextBoolean()) {
-      Thread.yield();
-    }
-  }
-
-  private synchronized void deleteFile(String name, boolean forced) throws IOException {
-    maybeYield();
-
-    maybeThrowDeterministicException();
-
-    if (crashed && !forced)
-      throw new IOException("cannot delete after crash");
-
-    if (unSyncedFiles.contains(name))
-      unSyncedFiles.remove(name);
-    if (!forced && noDeleteOpenFile) {
-      if (openFiles.containsKey(name)) {
-        openFilesDeleted.add(name);
-        throw fillOpenTrace(new IOException("MockDirectoryWrapper: file \"" + name + "\" is still open: cannot delete"), name, true);
-      } else {
-        openFilesDeleted.remove(name);
-      }
-    }
-    delegate.deleteFile(name);
-  }
-
-  public synchronized Set<String> getOpenDeletedFiles() {
-    return new HashSet<String>(openFilesDeleted);
-  }
-
-  @Override
-  public synchronized IndexOutput createOutput(String name) throws IOException {
-    maybeYield();
-    if (crashed)
-      throw new IOException("cannot createOutput after crash");
-    init();
-    synchronized(this) {
-      if (preventDoubleWrite && createdFiles.contains(name) && !name.equals("segments.gen"))
-        throw new IOException("file \"" + name + "\" was already written to");
-    }
-    if (noDeleteOpenFile && openFiles.containsKey(name))
-      throw new IOException("MockDirectoryWrapper: file \"" + name + "\" is still open: cannot overwrite");
-    
-    if (crashed)
-      throw new IOException("cannot createOutput after crash");
-    unSyncedFiles.add(name);
-    createdFiles.add(name);
-    
-    if (delegate instanceof RAMDirectory) {
-      RAMDirectory ramdir = (RAMDirectory) delegate;
-      RAMFile file = new RAMFile(ramdir);
-      RAMFile existing = ramdir.fileMap.get(name);
-    
-      // Enforce write once:
-      if (existing!=null && !name.equals("segments.gen") && preventDoubleWrite)
-        throw new IOException("file " + name + " already exists");
-      else {
-        if (existing!=null) {
-          ramdir.sizeInBytes.getAndAdd(-existing.sizeInBytes);
-          existing.directory = null;
-        }
-        ramdir.fileMap.put(name, file);
-      }
-    }
-    //System.out.println(Thread.currentThread().getName() + ": MDW: create " + name);
-    IndexOutput io = new MockIndexOutputWrapper(this, delegate.createOutput(name), name);
-    openFileHandles.put(io, new RuntimeException("unclosed IndexOutput"));
-    openFilesForWrite.add(name);
-    return io;
-  }
-
-  @Override
-  public synchronized IndexInput openInput(String name) throws IOException {
-    maybeYield();
-    if (!delegate.fileExists(name))
-      throw new FileNotFoundException(name);
-
-    // cannot open a file for input if it's still open for
-    // output, except for segments.gen and segments_N
-    if (openFilesForWrite.contains(name) && !name.startsWith("segments")) {
-      throw fillOpenTrace(new IOException("MockDirectoryWrapper: file \"" + name + "\" is still open for writing"), name, false);
-    }
-
-    if (openFiles.containsKey(name)) {
-      Integer v =  openFiles.get(name);
-      v = Integer.valueOf(v.intValue()+1);
-      openFiles.put(name, v);
-    } else {
-      openFiles.put(name, Integer.valueOf(1));
-    }
-
-    IndexInput ii = new MockIndexInputWrapper(this, name, delegate.openInput(name));
-    openFileHandles.put(ii, new RuntimeException("unclosed IndexInput"));
-    return ii;
-  }
-
-  /** Provided for testing purposes.  Use sizeInBytes() instead. */
-  public synchronized final long getRecomputedSizeInBytes() throws IOException {
-    if (!(delegate instanceof RAMDirectory))
-      return sizeInBytes();
-    long size = 0;
-    for(final RAMFile file: ((RAMDirectory)delegate).fileMap.values()) {
-      size += file.getSizeInBytes();
-    }
-    return size;
-  }
-
-  /** Like getRecomputedSizeInBytes(), but, uses actual file
-   * lengths rather than buffer allocations (which are
-   * quantized up to nearest
-   * RAMOutputStream.BUFFER_SIZE (now 1024) bytes.
-   */
-
-  public final synchronized long getRecomputedActualSizeInBytes() throws IOException {
-    if (!(delegate instanceof RAMDirectory))
-      return sizeInBytes();
-    long size = 0;
-    for (final RAMFile file : ((RAMDirectory)delegate).fileMap.values())
-      size += file.length;
-    return size;
-  }
-
-  @Override
-  public synchronized void close() throws IOException {
-    maybeYield();
-    if (openFiles == null) {
-      openFiles = new HashMap<String,Integer>();
-      openFilesDeleted = new HashSet<String>();
-    }
-    if (noDeleteOpenFile && openFiles.size() > 0) {
-      // print the first one as its very verbose otherwise
-      Exception cause = null;
-      Iterator<Exception> stacktraces = openFileHandles.values().iterator();
-      if (stacktraces.hasNext())
-        cause = stacktraces.next();
-      // RuntimeException instead of IOException because
-      // super() does not throw IOException currently:
-      throw new RuntimeException("MockDirectoryWrapper: cannot close: there are still open files: " + openFiles, cause);
-    }
-    open = false;
-    if (checkIndexOnClose && IndexReader.indexExists(this)) {
-      _TestUtil.checkIndex(this);
-    }
-    delegate.close();
-  }
-
-  boolean open = true;
-  
-  public synchronized boolean isOpen() {
-    return open;
-  }
-  
-  /**
-   * Objects that represent fail-able conditions. Objects of a derived
-   * class are created and registered with the mock directory. After
-   * register, each object will be invoked once for each first write
-   * of a file, giving the object a chance to throw an IOException.
-   */
-  public static class Failure {
-    /**
-     * eval is called on the first write of every new file.
-     */
-    public void eval(MockDirectoryWrapper dir) throws IOException { }
-
-    /**
-     * reset should set the state of the failure to its default
-     * (freshly constructed) state. Reset is convenient for tests
-     * that want to create one failure object and then reuse it in
-     * multiple cases. This, combined with the fact that Failure
-     * subclasses are often anonymous classes makes reset difficult to
-     * do otherwise.
-     *
-     * A typical example of use is
-     * Failure failure = new Failure() { ... };
-     * ...
-     * mock.failOn(failure.reset())
-     */
-    public Failure reset() { return this; }
-
-    protected boolean doFail;
-
-    public void setDoFail() {
-      doFail = true;
-    }
-
-    public void clearDoFail() {
-      doFail = false;
-    }
-  }
-
-  ArrayList<Failure> failures;
-
-  /**
-   * add a Failure object to the list of objects to be evaluated
-   * at every potential failure point
-   */
-  synchronized public void failOn(Failure fail) {
-    if (failures == null) {
-      failures = new ArrayList<Failure>();
-    }
-    failures.add(fail);
-  }
-
-  /**
-   * Iterate through the failures list, giving each object a
-   * chance to throw an IOE
-   */
-  synchronized void maybeThrowDeterministicException() throws IOException {
-    if (failures != null) {
-      for(int i = 0; i < failures.size(); i++) {
-        failures.get(i).eval(this);
-      }
-    }
-  }
-
-  @Override
-  public synchronized String[] listAll() throws IOException {
-    maybeYield();
-    return delegate.listAll();
-  }
-
-  @Override
-  public synchronized boolean fileExists(String name) throws IOException {
-    maybeYield();
-    return delegate.fileExists(name);
-  }
-
-  @Override
-  public synchronized long fileModified(String name) throws IOException {
-    maybeYield();
-    return delegate.fileModified(name);
-  }
-
-  @Override
-  public synchronized void touchFile(String name) throws IOException {
-    maybeYield();
-    delegate.touchFile(name);
-  }
-
-  @Override
-  public synchronized long fileLength(String name) throws IOException {
-    maybeYield();
-    return delegate.fileLength(name);
-  }
-
-  @Override
-  public synchronized Lock makeLock(String name) {
-    maybeYield();
-    return delegate.makeLock(name);
-  }
-
-  @Override
-  public synchronized void clearLock(String name) throws IOException {
-    maybeYield();
-    delegate.clearLock(name);
-  }
-
-  @Override
-  public synchronized void setLockFactory(LockFactory lockFactory) throws IOException {
-    maybeYield();
-    delegate.setLockFactory(lockFactory);
-  }
-
-  @Override
-  public synchronized LockFactory getLockFactory() {
-    maybeYield();
-    return delegate.getLockFactory();
-  }
-
-  @Override
-  public synchronized String getLockID() {
-    maybeYield();
-    return delegate.getLockID();
-  }
-
-  @Override
-  public synchronized void copy(Directory to, String src, String dest) throws IOException {
-    maybeYield();
-    delegate.copy(to, src, dest);
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/store/MockIndexInputWrapper.java b/lucene/src/test/org/apache/lucene/store/MockIndexInputWrapper.java
deleted file mode 100644
index 5e14a36..0000000
--- a/lucene/src/test/org/apache/lucene/store/MockIndexInputWrapper.java
+++ /dev/null
@@ -1,148 +0,0 @@
-package org.apache.lucene.store;
-
-import java.io.IOException;
-import java.util.Map;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Used by MockDirectoryWrapper to create an input stream that
- * keeps track of when it's been closed.
- */
-
-public class MockIndexInputWrapper extends IndexInput {
-  private MockDirectoryWrapper dir;
-  final String name;
-  private IndexInput delegate;
-  private boolean isClone;
-
-  /** Construct an empty output buffer. 
-   * @throws IOException */
-  public MockIndexInputWrapper(MockDirectoryWrapper dir, String name, IndexInput delegate) {
-    this.name = name;
-    this.dir = dir;
-    this.delegate = delegate;
-  }
-
-  @Override
-  public void close() throws IOException {
-    delegate.close();
-    // Pending resolution on LUCENE-686 we may want to
-    // remove the conditional check so we also track that
-    // all clones get closed:
-    if (!isClone) {
-      synchronized(dir) {
-        Integer v = dir.openFiles.get(name);
-        // Could be null when MockRAMDirectory.crash() was called
-        if (v != null) {
-          if (v.intValue() == 1) {
-            dir.openFiles.remove(name);
-            dir.openFilesDeleted.remove(name);
-          } else {
-            v = Integer.valueOf(v.intValue()-1);
-            dir.openFiles.put(name, v);
-          }
-        }
-        dir.openFileHandles.remove(this);
-      }
-    }
-  }
-
-  @Override
-  public Object clone() {
-    IndexInput iiclone = (IndexInput) delegate.clone();
-    MockIndexInputWrapper clone = new MockIndexInputWrapper(dir, name, iiclone);
-    clone.isClone = true;
-    // Pending resolution on LUCENE-686 we may want to
-    // uncomment this code so that we also track that all
-    // clones get closed:
-    /*
-    synchronized(dir.openFiles) {
-      if (dir.openFiles.containsKey(name)) {
-        Integer v = (Integer) dir.openFiles.get(name);
-        v = Integer.valueOf(v.intValue()+1);
-        dir.openFiles.put(name, v);
-      } else {
-        throw new RuntimeException("BUG: cloned file was not open?");
-      }
-    }
-    */
-    return clone;
-  }
-
-  @Override
-  public long getFilePointer() {
-    return delegate.getFilePointer();
-  }
-
-  @Override
-  public void seek(long pos) throws IOException {
-    delegate.seek(pos);
-  }
-
-  @Override
-  public long length() {
-    return delegate.length();
-  }
-
-  @Override
-  public byte readByte() throws IOException {
-    return delegate.readByte();
-  }
-
-  @Override
-  public void readBytes(byte[] b, int offset, int len) throws IOException {
-    delegate.readBytes(b, offset, len);
-  }
-
-  @Override
-  public void copyBytes(IndexOutput out, long numBytes) throws IOException {
-    delegate.copyBytes(out, numBytes);
-  }
-
-  @Override
-  public void readBytes(byte[] b, int offset, int len, boolean useBuffer)
-      throws IOException {
-    delegate.readBytes(b, offset, len, useBuffer);
-  }
-
-  @Override
-  public short readShort() throws IOException {
-    return delegate.readShort();
-  }
-
-  @Override
-  public int readInt() throws IOException {
-    return delegate.readInt();
-  }
-
-  @Override
-  public long readLong() throws IOException {
-    return delegate.readLong();
-  }
-
-  @Override
-  public String readString() throws IOException {
-    return delegate.readString();
-  }
-
-  @Override
-  public Map<String,String> readStringStringMap() throws IOException {
-    return delegate.readStringStringMap();
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/store/MockIndexOutputWrapper.java b/lucene/src/test/org/apache/lucene/store/MockIndexOutputWrapper.java
deleted file mode 100644
index 7e6e17d..0000000
--- a/lucene/src/test/org/apache/lucene/store/MockIndexOutputWrapper.java
+++ /dev/null
@@ -1,159 +0,0 @@
-package org.apache.lucene.store;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.util.LuceneTestCase;
-
-/**
- * Used by MockRAMDirectory to create an output stream that
- * will throw an IOException on fake disk full, track max
- * disk space actually used, and maybe throw random
- * IOExceptions.
- */
-
-public class MockIndexOutputWrapper extends IndexOutput {
-  private MockDirectoryWrapper dir;
-  private final IndexOutput delegate;
-  private boolean first=true;
-  final String name;
-  
-  byte[] singleByte = new byte[1];
-
-  /** Construct an empty output buffer. */
-  public MockIndexOutputWrapper(MockDirectoryWrapper dir, IndexOutput delegate, String name) {
-    this.dir = dir;
-    this.name = name;
-    this.delegate = delegate;
-  }
-
-  @Override
-  public void close() throws IOException {
-    dir.maybeThrowDeterministicException();
-    delegate.close();
-    if (dir.trackDiskUsage) {
-      // Now compute actual disk usage & track the maxUsedSize
-      // in the MockDirectoryWrapper:
-      long size = dir.getRecomputedActualSizeInBytes();
-      if (size > dir.maxUsedSize) {
-        dir.maxUsedSize = size;
-      }
-    }
-    synchronized(dir) {
-      dir.openFileHandles.remove(this);
-      dir.openFilesForWrite.remove(name);
-    }
-  }
-
-  @Override
-  public void flush() throws IOException {
-    dir.maybeThrowDeterministicException();
-    delegate.flush();
-  }
-
-  @Override
-  public void writeByte(byte b) throws IOException {
-    singleByte[0] = b;
-    writeBytes(singleByte, 0, 1);
-  }
-  
-  @Override
-  public void writeBytes(byte[] b, int offset, int len) throws IOException {
-    long freeSpace = dir.maxSize == 0 ? 0 : dir.maxSize - dir.sizeInBytes();
-    long realUsage = 0;
-
-    // If MockRAMDir crashed since we were opened, then
-    // don't write anything:
-    if (dir.crashed)
-      throw new IOException("MockRAMDirectory was crashed; cannot write to " + name);
-
-    // Enforce disk full:
-    if (dir.maxSize != 0 && freeSpace <= len) {
-      // Compute the real disk free.  This will greatly slow
-      // down our test but makes it more accurate:
-      realUsage = dir.getRecomputedActualSizeInBytes();
-      freeSpace = dir.maxSize - realUsage;
-    }
-
-    if (dir.maxSize != 0 && freeSpace <= len) {
-      if (freeSpace > 0) {
-        realUsage += freeSpace;
-        delegate.writeBytes(b, offset, (int) freeSpace);
-      }
-      if (realUsage > dir.maxUsedSize) {
-        dir.maxUsedSize = realUsage;
-      }
-      String message = "fake disk full at " + dir.getRecomputedActualSizeInBytes() + " bytes when writing " + name + " (file length=" + delegate.length();
-      if (freeSpace > 0) {
-        message += "; wrote " + freeSpace + " of " + len + " bytes";
-      }
-      message += ")";
-      if (LuceneTestCase.VERBOSE) {
-        System.out.println(Thread.currentThread().getName() + ": MDW: now throw fake disk full");
-        new Throwable().printStackTrace(System.out);
-      }
-      throw new IOException(message);
-    } else {
-      if (dir.randomState.nextBoolean()) {
-        final int half = len/2;
-        delegate.writeBytes(b, offset, half);
-        Thread.yield();
-        delegate.writeBytes(b, offset+half, len-half);
-      } else {
-        delegate.writeBytes(b, offset, len);
-      }
-    }
-
-    dir.maybeThrowDeterministicException();
-
-    if (first) {
-      // Maybe throw random exception; only do this on first
-      // write to a new file:
-      first = false;
-      dir.maybeThrowIOException();
-    }
-  }
-
-  @Override
-  public long getFilePointer() {
-    return delegate.getFilePointer();
-  }
-
-  @Override
-  public void seek(long pos) throws IOException {
-    delegate.seek(pos);
-  }
-
-  @Override
-  public long length() throws IOException {
-    return delegate.length();
-  }
-
-  @Override
-  public void setLength(long length) throws IOException {
-    delegate.setLength(length);
-  }
-
-  @Override
-  public void copyBytes(DataInput input, long numBytes) throws IOException {
-    delegate.copyBytes(input, numBytes);
-    // TODO: we may need to check disk full here as well
-    dir.maybeThrowDeterministicException();
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/store/_TestHelper.java b/lucene/src/test/org/apache/lucene/store/_TestHelper.java
deleted file mode 100644
index fb90a87..0000000
--- a/lucene/src/test/org/apache/lucene/store/_TestHelper.java
+++ /dev/null
@@ -1,65 +0,0 @@
-package org.apache.lucene.store;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.store.SimpleFSDirectory.SimpleFSIndexInput;
-
-/** This class provides access to package-level features defined in the
- *  store package. It is used for testing only.
- */
-public class _TestHelper {
-
-    /** Returns true if the instance of the provided input stream is actually
-     *  an SimpleFSIndexInput.
-     */
-    public static boolean isSimpleFSIndexInput(IndexInput is) {
-        return is instanceof SimpleFSIndexInput;
-    }
-
-    /** Returns true if the provided input stream is an SimpleFSIndexInput and
-     *  is a clone, that is it does not own its underlying file descriptor.
-     */
-    public static boolean isSimpleFSIndexInputClone(IndexInput is) {
-        if (isSimpleFSIndexInput(is)) {
-            return ((SimpleFSIndexInput) is).isClone;
-        } else {
-            return false;
-        }
-    }
-
-    /** Given an instance of SimpleFSDirectory.SimpleFSIndexInput, this method returns
-     *  true if the underlying file descriptor is valid, and false otherwise.
-     *  This can be used to determine if the OS file has been closed.
-     *  The descriptor becomes invalid when the non-clone instance of the
-     *  SimpleFSIndexInput that owns this descriptor is closed. However, the
-     *  descriptor may possibly become invalid in other ways as well.
-     */
-    public static boolean isSimpleFSIndexInputOpen(IndexInput is)
-    throws IOException
-    {
-        if (isSimpleFSIndexInput(is)) {
-            SimpleFSIndexInput fis = (SimpleFSIndexInput) is;
-            return fis.isFDValid();
-        } else {
-            return false;
-        }
-    }
-
-}
diff --git a/lucene/src/test/org/apache/lucene/util/LineFileDocs.java b/lucene/src/test/org/apache/lucene/util/LineFileDocs.java
deleted file mode 100644
index 56cb3e0..0000000
--- a/lucene/src/test/org/apache/lucene/util/LineFileDocs.java
+++ /dev/null
@@ -1,180 +0,0 @@
-package org.apache.lucene.util;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable;
-import java.io.File;
-import java.io.FileInputStream;
-import java.io.IOException;
-import java.io.BufferedReader;
-import java.io.InputStreamReader;
-import java.io.InputStream;
-import java.io.BufferedInputStream;
-import java.util.concurrent.atomic.AtomicInteger;
-import java.util.zip.GZIPInputStream;
-import java.util.Random;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-
-// Minimal port of contrib/benchmark's LneDocSource +
-// DocMaker, so tests can enum docs from a line file created
-// by contrib/benchmark's WriteLineDoc task
-public class LineFileDocs implements Closeable {
-
-  private BufferedReader reader;
-  private final static int BUFFER_SIZE = 1 << 16;     // 64K
-  private final AtomicInteger id = new AtomicInteger();
-  private final String path;
-
-  // If forever is true, we rewind the file at EOF (repeat
-  // the docs over and over)
-  public LineFileDocs(Random random, String path) throws IOException {
-    this.path = path;
-    open(random);
-  }
-
-  public LineFileDocs(Random random) throws IOException {
-    this(random, LuceneTestCase.TEST_LINE_DOCS_FILE);
-  }
-
-  public synchronized void close() throws IOException {
-    if (reader != null) {
-      reader.close();
-      reader = null;
-    }
-  }
-
-  private synchronized void open(Random random) throws IOException {
-    InputStream is = getClass().getResourceAsStream(path);
-    if (is == null) {
-      // if its not in classpath, we load it as absolute filesystem path (e.g. Hudson's home dir)
-      is = new FileInputStream(path);
-    }
-    File file = new File(path);
-    long size;
-    if (file.exists()) {
-      size = file.length();
-    } else {
-      size = is.available();
-    }
-    if (path.endsWith(".gz")) {
-      is = new GZIPInputStream(is);
-      // guestimate:
-      size *= 2.8;
-    }
-
-    final InputStream in = new BufferedInputStream(is, BUFFER_SIZE);
-    reader = new BufferedReader(new InputStreamReader(in, "UTF-8"), BUFFER_SIZE);
-
-    // Override sizes for currently "known" line files:
-    if (path.equals("europarl.lines.txt.gz")) {
-      size = 15129506L;
-    } else if (path.equals("/home/hudson/lucene-data/enwiki.random.lines.txt.gz")) {
-      size = 3038178822L;
-    }
-
-    // Randomly seek to starting point:
-    if (random != null && size > 3) {
-      final long seekTo = (random.nextLong()&Long.MAX_VALUE) % (size/3);
-      if (LuceneTestCase.VERBOSE) {
-        System.out.println("TEST: LineFileDocs: seek to fp=" + seekTo + " on open");
-      }
-      reader.skip(seekTo);
-      reader.readLine();
-    }
-  }
-
-  public synchronized void reset(Random random) throws IOException {
-    close();
-    open(random);
-    id.set(0);
-  }
-
-  private final static char SEP = '\t';
-
-  private static final class DocState {
-    final Document doc;
-    final Field titleTokenized;
-    final Field title;
-    final Field body;
-    final Field id;
-    final Field date;
-
-    public DocState() {
-      doc = new Document();
-      
-      title = new Field("title", "", Field.Store.NO, Field.Index.NOT_ANALYZED_NO_NORMS);
-      doc.add(title);
-
-      titleTokenized = new Field("titleTokenized", "", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS);
-      doc.add(titleTokenized);
-
-      body = new Field("body", "", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS);
-      doc.add(body);
-
-      id = new Field("id", "", Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS);
-      doc.add(id);
-
-      date = new Field("date", "", Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS);
-      doc.add(date);
-    }
-  }
-
-  private final ThreadLocal<DocState> threadDocs = new ThreadLocal<DocState>();
-
-  // Document instance is re-used per-thread
-  public Document nextDoc() throws IOException {
-    String line;
-    synchronized(this) {
-      line = reader.readLine();
-      if (line == null) {
-        // Always rewind at end:
-        if (LuceneTestCase.VERBOSE) {
-          System.out.println("TEST: LineFileDocs: now rewind file...");
-        }
-        close();
-        open(null);
-        line = reader.readLine();
-      }
-    }
-
-    DocState docState = threadDocs.get();
-    if (docState == null) {
-      docState = new DocState();
-      threadDocs.set(docState);
-    }
-
-    int spot = line.indexOf(SEP);
-    if (spot == -1) {
-      throw new RuntimeException("line: [" + line + "] is in an invalid format !");
-    }
-    int spot2 = line.indexOf(SEP, 1 + spot);
-    if (spot2 == -1) {
-      throw new RuntimeException("line: [" + line + "] is in an invalid format !");
-    }
-
-    docState.body.setValue(line.substring(1+spot2, line.length()));
-    final String title = line.substring(0, spot);
-    docState.title.setValue(title);
-    docState.titleTokenized.setValue(title);
-    docState.date.setValue(line.substring(1+spot, spot2));
-    docState.id.setValue(Integer.toString(id.getAndIncrement()));
-    return docState.doc;
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/util/LuceneJUnitDividingSelector.java b/lucene/src/test/org/apache/lucene/util/LuceneJUnitDividingSelector.java
deleted file mode 100644
index 5a9509c..0000000
--- a/lucene/src/test/org/apache/lucene/util/LuceneJUnitDividingSelector.java
+++ /dev/null
@@ -1,66 +0,0 @@
-/**
- *  Licensed to the Apache Software Foundation (ASF) under one or more
- *  contributor license agreements.  See the NOTICE file distributed with
- *  this work for additional information regarding copyright ownership.
- *  The ASF licenses this file to You under the Apache License, Version 2.0
- *  (the "License"); you may not use this file except in compliance with
- *  the License.  You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- *  Unless required by applicable law or agreed to in writing, software
- *  distributed under the License is distributed on an "AS IS" BASIS,
- *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- *  See the License for the specific language governing permissions and
- *  limitations under the License.
- *
- */
-package org.apache.lucene.util;
-import java.io.File;
-
-import org.apache.tools.ant.BuildException;
-import org.apache.tools.ant.types.Parameter;
-import org.apache.tools.ant.types.selectors.BaseExtendSelector;
-
-/** Divides filesets into equal groups */
-public class LuceneJUnitDividingSelector extends BaseExtendSelector {
-  private int counter;
-  /** Number of total parts to split. */
-  private int divisor;
-  /** Current part to accept. */
-  private int part;
-
-  @Override
-  public void setParameters(Parameter[] pParameters) {
-    super.setParameters(pParameters);
-    for (int j = 0; j < pParameters.length; j++) {
-      Parameter p = pParameters[j];
-      if ("divisor".equalsIgnoreCase(p.getName())) {
-        divisor = Integer.parseInt(p.getValue());
-      }
-      else if ("part".equalsIgnoreCase(p.getName())) {
-        part = Integer.parseInt(p.getValue());
-      }
-      else {
-        throw new BuildException("unknown " + p.getName());
-      }
-    }
-  }
-
-  @Override
-  public void verifySettings() {
-    super.verifySettings();
-    if (divisor <= 0 || part <= 0) {
-      throw new BuildException("part or divisor not set");
-    }
-    if (part > divisor) {
-      throw new BuildException("part must be <= divisor");
-    }
-  }
-
-  @Override
-  public boolean isSelected(File dir, String name, File path) {
-    counter = counter % divisor + 1;
-    return counter == part;
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/util/LuceneJUnitResultFormatter.java b/lucene/src/test/org/apache/lucene/util/LuceneJUnitResultFormatter.java
deleted file mode 100644
index a03f780..0000000
--- a/lucene/src/test/org/apache/lucene/util/LuceneJUnitResultFormatter.java
+++ /dev/null
@@ -1,293 +0,0 @@
-/**
- *  Licensed to the Apache Software Foundation (ASF) under one or more
- *  contributor license agreements.  See the NOTICE file distributed with
- *  this work for additional information regarding copyright ownership.
- *  The ASF licenses this file to You under the Apache License, Version 2.0
- *  (the "License"); you may not use this file except in compliance with
- *  the License.  You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- *  Unless required by applicable law or agreed to in writing, software
- *  distributed under the License is distributed on an "AS IS" BASIS,
- *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- *  See the License for the specific language governing permissions and
- *  limitations under the License.
- *
- */
-
-package org.apache.lucene.util;
-
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.IOException;
-import java.io.OutputStream;
-import java.text.NumberFormat;
-import java.util.logging.LogManager;
-
-import junit.framework.AssertionFailedError;
-import junit.framework.Test;
-
-import org.apache.lucene.store.LockReleaseFailedException;
-import org.apache.lucene.store.NativeFSLockFactory;
-import org.apache.tools.ant.taskdefs.optional.junit.JUnitResultFormatter;
-import org.apache.tools.ant.taskdefs.optional.junit.JUnitTest;
-import org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner;
-import org.apache.tools.ant.util.FileUtils;
-import org.apache.tools.ant.util.StringUtils;
-import org.junit.Ignore;
-
-/**
- * Just like BriefJUnitResultFormatter "brief" bundled with ant,
- * except all formatted text is buffered until the test suite is finished.
- * At this point, the output is written at once in synchronized fashion.
- * This way tests can run in parallel without interleaving output.
- */
-public class LuceneJUnitResultFormatter implements JUnitResultFormatter {
-  private static final double ONE_SECOND = 1000.0;
-  
-  private static final NativeFSLockFactory lockFactory;
-  
-  /** Where to write the log to. */
-  private OutputStream out;
-  
-  /** Formatter for timings. */
-  private NumberFormat numberFormat = NumberFormat.getInstance();
-  
-  /** Output suite has written to System.out */
-  private String systemOutput = null;
-  
-  /** Output suite has written to System.err */
-  private String systemError = null;
-  
-  /** Buffer output until the end of the test */
-  private ByteArrayOutputStream sb; // use a BOS for our mostly ascii-output
-
-  private static final org.apache.lucene.store.Lock lock;
-
-  static {
-    File lockDir = new File(System.getProperty("java.io.tmpdir"),
-        "lucene_junit_lock");
-    lockDir.mkdirs();
-    if (!lockDir.exists()) {
-      throw new RuntimeException("Could not make Lock directory:" + lockDir);
-    }
-    try {
-      lockFactory = new NativeFSLockFactory(lockDir);
-      lock = lockFactory.makeLock("junit_lock");
-    } catch (IOException e) {
-      throw new RuntimeException(e);
-    }
-  }
-
-  /** Constructor for LuceneJUnitResultFormatter. */
-  public LuceneJUnitResultFormatter() {
-  }
-  
-  /**
-   * Sets the stream the formatter is supposed to write its results to.
-   * @param out the output stream to write to
-   */
-  public void setOutput(OutputStream out) {
-    this.out = out;
-  }
-  
-  /**
-   * @see JUnitResultFormatter#setSystemOutput(String)
-   */
-  /** {@inheritDoc}. */
-  public void setSystemOutput(String out) {
-    systemOutput = out;
-  }
-  
-  /**
-   * @see JUnitResultFormatter#setSystemError(String)
-   */
-  /** {@inheritDoc}. */
-  public void setSystemError(String err) {
-    systemError = err;
-  }
-  
-  
-  /**
-   * The whole testsuite started.
-   * @param suite the test suite
-   */
-  public synchronized void startTestSuite(JUnitTest suite) {
-    if (out == null) {
-      return; // Quick return - no output do nothing.
-    }
-    sb = new ByteArrayOutputStream(); // don't reuse, so its gc'ed
-    try {
-      LogManager.getLogManager().readConfiguration();
-    } catch (Exception e) {}
-    append("Testsuite: ");
-    append(suite.getName());
-    append(StringUtils.LINE_SEP);
-  }
-  
-  /**
-   * The whole testsuite ended.
-   * @param suite the test suite
-   */
-  public synchronized void endTestSuite(JUnitTest suite) {
-    append("Tests run: ");
-    append(suite.runCount());
-    append(", Failures: ");
-    append(suite.failureCount());
-    append(", Errors: ");
-    append(suite.errorCount());
-    append(", Time elapsed: ");
-    append(numberFormat.format(suite.getRunTime() / ONE_SECOND));
-    append(" sec");
-    append(StringUtils.LINE_SEP);
-    append(StringUtils.LINE_SEP);
-    
-    // append the err and output streams to the log
-    if (systemOutput != null && systemOutput.length() > 0) {
-      append("------------- Standard Output ---------------")
-      .append(StringUtils.LINE_SEP)
-      .append(systemOutput)
-      .append("------------- ---------------- ---------------")
-      .append(StringUtils.LINE_SEP);
-    }
-    
-    // HACK: junit gives us no way to do this in LuceneTestCase
-    try {
-      Class<?> clazz = Class.forName(suite.getName());
-      Ignore ignore = clazz.getAnnotation(Ignore.class);
-      if (ignore != null) {
-        if (systemError == null) systemError = "";
-        systemError += "NOTE: Ignoring test class '" + clazz.getSimpleName() + "': " 
-                    + ignore.value() + StringUtils.LINE_SEP;
-      }
-    } catch (ClassNotFoundException e) { /* no problem */ }
-    // END HACK
-    
-    if (systemError != null && systemError.length() > 0) {
-      append("------------- Standard Error -----------------")
-      .append(StringUtils.LINE_SEP)
-      .append(systemError)
-      .append("------------- ---------------- ---------------")
-      .append(StringUtils.LINE_SEP);
-    }
-    
-    if (out != null) {
-      try {
-        lock.obtain(5000);
-        try {
-          sb.writeTo(out);
-          out.flush();
-        } finally {
-          try {
-            lock.release();
-          } catch(LockReleaseFailedException e) {
-            // well lets pretend its released anyway
-          }
-        }
-      } catch (IOException e) {
-        throw new RuntimeException("unable to write results", e);
-      } finally {
-        if (out != System.out && out != System.err) {
-          FileUtils.close(out);
-        }
-      }
-    }
-  }
-  
-  /**
-   * A test started.
-   * @param test a test
-   */
-  public void startTest(Test test) {
-  }
-  
-  /**
-   * A test ended.
-   * @param test a test
-   */
-  public void endTest(Test test) {
-  }
-  
-  /**
-   * Interface TestListener for JUnit &lt;= 3.4.
-   *
-   * <p>A Test failed.
-   * @param test a test
-   * @param t    the exception thrown by the test
-   */
-  public void addFailure(Test test, Throwable t) {
-    formatError("\tFAILED", test, t);
-  }
-  
-  /**
-   * Interface TestListener for JUnit &gt; 3.4.
-   *
-   * <p>A Test failed.
-   * @param test a test
-   * @param t    the assertion failed by the test
-   */
-  public void addFailure(Test test, AssertionFailedError t) {
-    addFailure(test, (Throwable) t);
-  }
-  
-  /**
-   * A test caused an error.
-   * @param test  a test
-   * @param error the error thrown by the test
-   */
-  public void addError(Test test, Throwable error) {
-    formatError("\tCaused an ERROR", test, error);
-  }
-  
-  /**
-   * Format the test for printing..
-   * @param test a test
-   * @return the formatted testname
-   */
-  protected String formatTest(Test test) {
-    if (test == null) {
-      return "Null Test: ";
-    } else {
-      return "Testcase: " + test.toString() + ":";
-    }
-  }
-  
-  /**
-   * Format an error and print it.
-   * @param type the type of error
-   * @param test the test that failed
-   * @param error the exception that the test threw
-   */
-  protected synchronized void formatError(String type, Test test,
-      Throwable error) {
-    if (test != null) {
-      endTest(test);
-    }
-    
-    append(formatTest(test) + type);
-    append(StringUtils.LINE_SEP);
-    append(error.getMessage());
-    append(StringUtils.LINE_SEP);
-    String strace = JUnitTestRunner.getFilteredTrace(error);
-    append(strace);
-    append(StringUtils.LINE_SEP);
-    append(StringUtils.LINE_SEP);
-  }
-
-  public LuceneJUnitResultFormatter append(String s) {
-    if (s == null)
-      s = "(null)";
-    try {
-      sb.write(s.getBytes()); // intentionally use default charset, its a console.
-    } catch (IOException e) {
-      throw new RuntimeException(e);
-    }
-    return this;
-  }
-  
-  public LuceneJUnitResultFormatter append(long l) {
-    return append(Long.toString(l));
-  }
-}
-
diff --git a/lucene/src/test/org/apache/lucene/util/LuceneTestCase.java b/lucene/src/test/org/apache/lucene/util/LuceneTestCase.java
deleted file mode 100644
index a10689c..0000000
--- a/lucene/src/test/org/apache/lucene/util/LuceneTestCase.java
+++ /dev/null
@@ -1,1253 +0,0 @@
-package org.apache.lucene.util;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.File;
-import java.io.IOException;
-import java.io.PrintStream;
-import java.lang.annotation.Documented;
-import java.lang.annotation.Inherited;
-import java.lang.annotation.Retention;
-import java.lang.annotation.RetentionPolicy;
-import java.lang.reflect.Constructor;
-import java.lang.reflect.Method;
-import java.lang.reflect.Modifier;
-import java.util.*;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.Executors;
-import java.util.concurrent.TimeUnit;
-import java.util.regex.Matcher;
-import java.util.regex.Pattern;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.Field.Index;
-import org.apache.lucene.document.Field.Store;
-import org.apache.lucene.document.Field.TermVector;
-import org.apache.lucene.index.*;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.CodecProvider;
-import org.apache.lucene.index.codecs.mockintblock.MockFixedIntBlockCodec;
-import org.apache.lucene.index.codecs.mockintblock.MockVariableIntBlockCodec;
-import org.apache.lucene.index.codecs.mocksep.MockSepCodec;
-import org.apache.lucene.index.codecs.mockrandom.MockRandomCodec;
-import org.apache.lucene.index.codecs.preflex.PreFlexCodec;
-import org.apache.lucene.index.codecs.preflexrw.PreFlexRWCodec;
-import org.apache.lucene.index.codecs.pulsing.PulsingCodec;
-import org.apache.lucene.index.codecs.simpletext.SimpleTextCodec;
-import org.apache.lucene.index.codecs.standard.StandardCodec;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.FieldCache;
-import org.apache.lucene.search.FieldCache.CacheEntry;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.FSDirectory;
-import org.apache.lucene.store.LockFactory;
-import org.apache.lucene.store.MockDirectoryWrapper;
-import org.apache.lucene.util.FieldCacheSanityChecker.Insanity;
-import org.junit.*;
-import org.junit.rules.TestWatchman;
-import org.junit.runner.Description;
-import org.junit.runner.RunWith;
-import org.junit.runner.manipulation.Filter;
-import org.junit.runner.manipulation.NoTestsRemainException;
-import org.junit.runner.notification.RunNotifier;
-import org.junit.runners.BlockJUnit4ClassRunner;
-import org.junit.runners.model.FrameworkMethod;
-import org.junit.runners.model.InitializationError;
-
-/**
- * Base class for all Lucene unit tests, Junit3 or Junit4 variant.
- * <p>
- * </p>
- * <p>
- * If you
- * override either <code>setUp()</code> or
- * <code>tearDown()</code> in your unit test, make sure you
- * call <code>super.setUp()</code> and
- * <code>super.tearDown()</code>
- * </p>
- *
- * @After - replaces setup
- * @Before - replaces teardown
- * @Test - any public method with this annotation is a test case, regardless
- * of its name
- * <p>
- * <p>
- * See Junit4 <a href="http://junit.org/junit/javadoc/4.7/">documentation</a> for a complete list of features.
- * <p>
- * Import from org.junit rather than junit.framework.
- * <p>
- * You should be able to use this class anywhere you used LuceneTestCase
- * if you annotate your derived class correctly with the annotations above
- * @see #assertSaneFieldCaches(String)
- */
-
-@RunWith(LuceneTestCase.LuceneTestCaseRunner.class)
-public abstract class LuceneTestCase extends Assert {
-
-  /**
-   * true iff tests are run in verbose mode. Note: if it is false, tests are not
-   * expected to print any messages.
-   */
-  public static final boolean VERBOSE = Boolean.getBoolean("tests.verbose");
-
-  /** Use this constant when creating Analyzers and any other version-dependent stuff.
-   * <p><b>NOTE:</b> Change this when development starts for new Lucene version:
-   */
-  public static final Version TEST_VERSION_CURRENT = Version.LUCENE_40;
-
-  /**
-   * If this is set, it is the only method that should run.
-   */
-  static final String TEST_METHOD;
-  
-  /** Create indexes in this directory, optimally use a subdir, named after the test */
-  public static final File TEMP_DIR;
-  static {
-    String method = System.getProperty("testmethod", "").trim();
-    TEST_METHOD = method.length() == 0 ? null : method;
-    String s = System.getProperty("tempDir", System.getProperty("java.io.tmpdir"));
-    if (s == null)
-      throw new RuntimeException("To run tests, you need to define system property 'tempDir' or 'java.io.tmpdir'.");
-    TEMP_DIR = new File(s);
-    TEMP_DIR.mkdirs();
-  }
-
-  // by default we randomly pick a different codec for
-  // each test case (non-J4 tests) and each test class (J4
-  // tests)
-  /** Gets the codec to run tests with. */
-  public static final String TEST_CODEC = System.getProperty("tests.codec", "randomPerField");
-  /** Gets the locale to run tests with */
-  public static final String TEST_LOCALE = System.getProperty("tests.locale", "random");
-  /** Gets the timezone to run tests with */
-  public static final String TEST_TIMEZONE = System.getProperty("tests.timezone", "random");
-  /** Gets the directory to run tests with */
-  public static final String TEST_DIRECTORY = System.getProperty("tests.directory", "random");
-  /** Get the number of times to run tests */
-  public static final int TEST_ITER = Integer.parseInt(System.getProperty("tests.iter", "1"));
-  /** Get the random seed for tests */
-  public static final String TEST_SEED = System.getProperty("tests.seed", "random");
-  /** whether or not nightly tests should run */
-  public static final boolean TEST_NIGHTLY = Boolean.parseBoolean(System.getProperty("tests.nightly", "false"));
-  /** the line file used by LineFileDocs */
-  public static final String TEST_LINE_DOCS_FILE = System.getProperty("tests.linedocsfile", "europarl.lines.txt.gz");
-
-  private static final Pattern codecWithParam = Pattern.compile("(.*)\\(\\s*(\\d+)\\s*\\)");
-
-  /**
-   * A random multiplier which you should use when writing random tests:
-   * multiply it by the number of iterations
-   */
-  public static final int RANDOM_MULTIPLIER = Integer.parseInt(System.getProperty("tests.multiplier", "1"));
-  
-  private int savedBoolMaxClauseCount;
-
-  private volatile Thread.UncaughtExceptionHandler savedUncaughtExceptionHandler = null;
-  
-  /** Used to track if setUp and tearDown are called correctly from subclasses */
-  private boolean setup;
-
-  /**
-   * Some tests expect the directory to contain a single segment, and want to do tests on that segment's reader.
-   * This is an utility method to help them.
-   */
-  public static SegmentReader getOnlySegmentReader(IndexReader reader) {
-    if (reader instanceof SegmentReader)
-      return (SegmentReader) reader;
-
-    IndexReader[] subReaders = reader.getSequentialSubReaders();
-    if (subReaders.length != 1)
-      throw new IllegalArgumentException(reader + " has " + subReaders.length + " segments instead of exactly one");
-
-    return (SegmentReader) subReaders[0];
-  }
-
-  private static class UncaughtExceptionEntry {
-    public final Thread thread;
-    public final Throwable exception;
-    
-    public UncaughtExceptionEntry(Thread thread, Throwable exception) {
-      this.thread = thread;
-      this.exception = exception;
-    }
-  }
-  private List<UncaughtExceptionEntry> uncaughtExceptions = Collections.synchronizedList(new ArrayList<UncaughtExceptionEntry>());
-  
-  // saves default codec: we do this statically as many build indexes in @beforeClass
-  private static String savedDefaultCodec;
-  // default codec: not set when we use a per-field provider.
-  private static Codec codec;
-  // default codec provider
-  private static CodecProvider savedCodecProvider;
-  
-  private static Locale locale;
-  private static Locale savedLocale;
-  private static TimeZone timeZone;
-  private static TimeZone savedTimeZone;
-  
-  private static Map<MockDirectoryWrapper,StackTraceElement[]> stores;
-  
-  private static final String[] TEST_CODECS = new String[] {"MockSep", "MockFixedIntBlock", "MockVariableIntBlock", "MockRandom"};
-
-  private static void swapCodec(Codec c, CodecProvider cp) {
-    Codec prior = null;
-    try {
-      prior = cp.lookup(c.name);
-    } catch (IllegalArgumentException iae) {
-    }
-    if (prior != null) {
-      cp.unregister(prior);
-    }
-    cp.register(c);
-  }
-
-  // returns current default codec
-  static Codec installTestCodecs(String codec, CodecProvider cp) {
-    savedDefaultCodec = cp.getDefaultFieldCodec();
-
-    final boolean codecHasParam;
-    int codecParam = 0;
-    if (codec.equals("randomPerField")) {
-      // lie
-      codec = "Standard";
-      codecHasParam = false;
-    } else if (codec.equals("random")) {
-      codec = pickRandomCodec(random);
-      codecHasParam = false;
-    } else {
-      Matcher m = codecWithParam.matcher(codec);
-      if (m.matches()) {
-        // codec has a fixed param
-        codecHasParam = true;
-        codec = m.group(1);
-        codecParam = Integer.parseInt(m.group(2));
-      } else {
-        codecHasParam = false;
-      }
-    }
-
-    cp.setDefaultFieldCodec(codec);
-
-    if (codec.equals("PreFlex")) {
-      // If we're running w/ PreFlex codec we must swap in the
-      // test-only PreFlexRW codec (since core PreFlex can
-      // only read segments):
-      swapCodec(new PreFlexRWCodec(), cp);
-    }
-
-    swapCodec(new MockSepCodec(), cp);
-    swapCodec(new PulsingCodec(codecHasParam && "Pulsing".equals(codec) ? codecParam : _TestUtil.nextInt(random, 1, 20)), cp);
-    swapCodec(new MockFixedIntBlockCodec(codecHasParam && "MockFixedIntBlock".equals(codec) ? codecParam : _TestUtil.nextInt(random, 1, 2000)), cp);
-    // baseBlockSize cannot be over 127:
-    swapCodec(new MockVariableIntBlockCodec(codecHasParam && "MockVariableIntBlock".equals(codec) ? codecParam : _TestUtil.nextInt(random, 1, 127)), cp);
-    swapCodec(new MockRandomCodec(random), cp);
-
-    return cp.lookup(codec);
-  }
-
-  // returns current PreFlex codec
-  static void removeTestCodecs(Codec codec, CodecProvider cp) {
-    if (codec.name.equals("PreFlex")) {
-      final Codec preFlex = cp.lookup("PreFlex");
-      if (preFlex != null) {
-        cp.unregister(preFlex);
-      }
-      cp.register(new PreFlexCodec());
-    }
-    cp.unregister(cp.lookup("MockSep"));
-    cp.unregister(cp.lookup("MockFixedIntBlock"));
-    cp.unregister(cp.lookup("MockVariableIntBlock"));
-    cp.unregister(cp.lookup("MockRandom"));
-    swapCodec(new PulsingCodec(1), cp);
-    cp.setDefaultFieldCodec(savedDefaultCodec);
-  }
-
-  // randomly picks from core and test codecs
-  static String pickRandomCodec(Random rnd) {
-    int idx = rnd.nextInt(CodecProvider.CORE_CODECS.length + 
-                          TEST_CODECS.length);
-    if (idx < CodecProvider.CORE_CODECS.length) {
-      return CodecProvider.CORE_CODECS[idx];
-    } else {
-      return TEST_CODECS[idx - CodecProvider.CORE_CODECS.length];
-    }
-  }
-
-  private static class TwoLongs {
-    public final long l1, l2;
-
-    public TwoLongs(long l1, long l2) {
-      this.l1 = l1;
-      this.l2 = l2;
-    }
-
-    @Override
-    public String toString() {
-      return l1 + ":" + l2;
-    }
-
-    public static TwoLongs fromString(String s) {
-      final int i = s.indexOf(':');
-      assert i != -1;
-      return new TwoLongs(Long.parseLong(s.substring(0, i)),
-                          Long.parseLong(s.substring(1+i)));
-    }
-  }
-
-  /** @deprecated (4.0) until we fix no-fork problems in solr tests */
-  @Deprecated
-  private static List<String> testClassesRun = new ArrayList<String>();
-  
-  @BeforeClass
-  public static void beforeClassLuceneTestCaseJ4() {
-    staticSeed = "random".equals(TEST_SEED) ? seedRand.nextLong() : TwoLongs.fromString(TEST_SEED).l1;
-    random.setSeed(staticSeed);
-    stores = Collections.synchronizedMap(new IdentityHashMap<MockDirectoryWrapper,StackTraceElement[]>());
-    savedCodecProvider = CodecProvider.getDefault();
-    if ("randomPerField".equals(TEST_CODEC)) {
-      if (random.nextInt(4) == 0) { // preflex-only setup
-        codec = installTestCodecs("PreFlex", CodecProvider.getDefault());
-      } else { // per-field setup
-        CodecProvider.setDefault(new RandomCodecProvider(random));
-        codec = installTestCodecs(TEST_CODEC, CodecProvider.getDefault());
-      }
-    } else { // ordinary setup
-      codec = installTestCodecs(TEST_CODEC, CodecProvider.getDefault());
-    }
-    savedLocale = Locale.getDefault();
-    locale = TEST_LOCALE.equals("random") ? randomLocale(random) : localeForName(TEST_LOCALE);
-    Locale.setDefault(locale);
-    savedTimeZone = TimeZone.getDefault();
-    timeZone = TEST_TIMEZONE.equals("random") ? randomTimeZone(random) : TimeZone.getTimeZone(TEST_TIMEZONE);
-    TimeZone.setDefault(timeZone);
-    testsFailed = false;
-  }
-  
-  @AfterClass
-  public static void afterClassLuceneTestCaseJ4() {
-    int rogueThreads = threadCleanup("test class");
-    if (rogueThreads > 0) {
-      // TODO: fail here once the leaks are fixed.
-      System.err.println("RESOURCE LEAK: test class left " + rogueThreads + " thread(s) running");
-    }
-    String codecDescription;
-    CodecProvider cp = CodecProvider.getDefault();
-
-    if ("randomPerField".equals(TEST_CODEC)) {
-      if (cp instanceof RandomCodecProvider)
-        codecDescription = cp.toString();
-      else 
-        codecDescription = "PreFlex";
-    } else {
-      codecDescription = codec.toString();
-    }
-    
-    if (CodecProvider.getDefault() == savedCodecProvider)
-      removeTestCodecs(codec, CodecProvider.getDefault());
-    CodecProvider.setDefault(savedCodecProvider);
-    Locale.setDefault(savedLocale);
-    TimeZone.setDefault(savedTimeZone);
-    System.clearProperty("solr.solr.home");
-    System.clearProperty("solr.data.dir");
-    // now look for unclosed resources
-    if (!testsFailed)
-      for (MockDirectoryWrapper d : stores.keySet()) {
-        if (d.isOpen()) {
-          StackTraceElement elements[] = stores.get(d);
-          // Look for the first class that is not LuceneTestCase that requested
-          // a Directory. The first two items are of Thread's, so skipping over
-          // them.
-          StackTraceElement element = null;
-          for (int i = 2; i < elements.length; i++) {
-            StackTraceElement ste = elements[i];
-            if (ste.getClassName().indexOf("LuceneTestCase") == -1) {
-              element = ste;
-              break;
-            }
-          }
-          fail("directory of test was not closed, opened from: " + element);
-        }
-      }
-    stores = null;
-    // if verbose or tests failed, report some information back
-    if (VERBOSE || testsFailed)
-      System.err.println("NOTE: test params are: codec=" + codecDescription + 
-        ", locale=" + locale + 
-        ", timezone=" + (timeZone == null ? "(null)" : timeZone.getID()));
-    if (testsFailed) {
-      System.err.println("NOTE: all tests run in this JVM:");
-      System.err.println(Arrays.toString(testClassesRun.toArray()));
-      System.err.println("NOTE: " + System.getProperty("os.name") + " " 
-          + System.getProperty("os.version") + " " 
-          + System.getProperty("os.arch") + "/"
-          + System.getProperty("java.vendor") + " "
-          + System.getProperty("java.version") + " "
-          + (Constants.JRE_IS_64BIT ? "(64-bit)" : "(32-bit)") + "/"
-          + "cpus=" + Runtime.getRuntime().availableProcessors() + ","
-          + "threads=" + Thread.activeCount() + ","
-          + "free=" + Runtime.getRuntime().freeMemory() + ","
-          + "total=" + Runtime.getRuntime().totalMemory());
-    }
-  }
-
-  private static boolean testsFailed; /* true if any tests failed */
-  
-  // This is how we get control when errors occur.
-  // Think of this as start/end/success/failed
-  // events.
-  @Rule
-  public final TestWatchman intercept = new TestWatchman() {
-
-    @Override
-    public void failed(Throwable e, FrameworkMethod method) {
-      // org.junit.internal.AssumptionViolatedException in older releases
-      // org.junit.Assume.AssumptionViolatedException in recent ones
-      if (e.getClass().getName().endsWith("AssumptionViolatedException")) {
-        if (e.getCause() instanceof TestIgnoredException)
-          e = e.getCause();
-        System.err.print("NOTE: Assume failed in '" + method.getName() + "' (ignored):");
-        if (VERBOSE) {
-          System.err.println();
-          e.printStackTrace(System.err);
-        } else {
-          System.err.print(" ");
-          System.err.println(e.getMessage());
-        }
-      } else {
-        testsFailed = true;
-        reportAdditionalFailureInfo();
-      }
-      super.failed(e, method);
-    }
-
-    @Override
-    public void starting(FrameworkMethod method) {
-      // set current method name for logging
-      LuceneTestCase.this.name = method.getName();
-      super.starting(method);
-    }
-    
-  };
-
-  @Before
-  public void setUp() throws Exception {
-    seed = "random".equals(TEST_SEED) ? seedRand.nextLong() : TwoLongs.fromString(TEST_SEED).l2;
-    random.setSeed(seed);
-    assertFalse("ensure your tearDown() calls super.tearDown()!!!", setup);
-    setup = true;
-    savedUncaughtExceptionHandler = Thread.getDefaultUncaughtExceptionHandler();
-    Thread.setDefaultUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {
-      public void uncaughtException(Thread t, Throwable e) {
-        testsFailed = true;
-        uncaughtExceptions.add(new UncaughtExceptionEntry(t, e));
-        if (savedUncaughtExceptionHandler != null)
-          savedUncaughtExceptionHandler.uncaughtException(t, e);
-      }
-    });
-    
-    savedBoolMaxClauseCount = BooleanQuery.getMaxClauseCount();
-  }
-
-
-  /**
-   * Forcible purges all cache entries from the FieldCache.
-   * <p>
-   * This method will be called by tearDown to clean up FieldCache.DEFAULT.
-   * If a (poorly written) test has some expectation that the FieldCache
-   * will persist across test methods (ie: a static IndexReader) this
-   * method can be overridden to do nothing.
-   * </p>
-   *
-   * @see FieldCache#purgeAllCaches()
-   */
-  protected void purgeFieldCache(final FieldCache fc) {
-    fc.purgeAllCaches();
-  }
-
-  protected String getTestLabel() {
-    return getClass().getName() + "." + getName();
-  }
-
-  @After
-  public void tearDown() throws Exception {
-    assertTrue("ensure your setUp() calls super.setUp()!!!", setup);
-    setup = false;
-    BooleanQuery.setMaxClauseCount(savedBoolMaxClauseCount);
-    if (!getClass().getName().startsWith("org.apache.solr")) {
-      int rogueThreads = threadCleanup("test method: '" + getName() + "'");
-      if (rogueThreads > 0) {
-        System.err.println("RESOURCE LEAK: test method: '" + getName() 
-            + "' left " + rogueThreads + " thread(s) running");
-        // TODO: fail, but print seed for now.
-        if (!testsFailed && uncaughtExceptions.isEmpty()) {
-          reportAdditionalFailureInfo();
-        }
-      }
-    }
-    Thread.setDefaultUncaughtExceptionHandler(savedUncaughtExceptionHandler);
-    try {
-
-      if (!uncaughtExceptions.isEmpty()) {
-        testsFailed = true;
-        reportAdditionalFailureInfo();
-        System.err.println("The following exceptions were thrown by threads:");
-        for (UncaughtExceptionEntry entry : uncaughtExceptions) {
-          System.err.println("*** Thread: " + entry.thread.getName() + " ***");
-          entry.exception.printStackTrace(System.err);
-        }
-        fail("Some threads threw uncaught exceptions!");
-      }
-
-      // calling assertSaneFieldCaches here isn't as useful as having test 
-      // classes call it directly from the scope where the index readers 
-      // are used, because they could be gc'ed just before this tearDown 
-      // method is called.
-      //
-      // But it's better then nothing.
-      //
-      // If you are testing functionality that you know for a fact 
-      // "violates" FieldCache sanity, then you should either explicitly 
-      // call purgeFieldCache at the end of your test method, or refactor
-      // your Test class so that the inconsistant FieldCache usages are 
-      // isolated in distinct test methods  
-      assertSaneFieldCaches(getTestLabel());
-
-    } finally {
-      purgeFieldCache(FieldCache.DEFAULT);
-    }
-  }
-
-  private final static int THREAD_STOP_GRACE_MSEC = 1000;
-  // jvm-wide list of 'rogue threads' we found, so they only get reported once.
-  private final static IdentityHashMap<Thread,Boolean> rogueThreads = new IdentityHashMap<Thread,Boolean>();
-  
-  static {
-    // just a hack for things like eclipse test-runner threads
-    for (Thread t : Thread.getAllStackTraces().keySet()) {
-      rogueThreads.put(t, true);
-    }
-  }
-  
-  /**
-   * Looks for leftover running threads, trying to kill them off,
-   * so they don't fail future tests.
-   * returns the number of rogue threads that it found.
-   */
-  private static int threadCleanup(String context) {
-    // educated guess
-    Thread[] stillRunning = new Thread[Thread.activeCount()+1];
-    int threadCount = 0;
-    int rogueCount = 0;
-    
-    if ((threadCount = Thread.enumerate(stillRunning)) > 1) {
-      while (threadCount == stillRunning.length) {
-        // truncated response
-        stillRunning = new Thread[stillRunning.length*2];
-        threadCount = Thread.enumerate(stillRunning);
-      }
-      
-      for (int i = 0; i < threadCount; i++) {
-        Thread t = stillRunning[i];
-          
-        if (t.isAlive() && 
-            !rogueThreads.containsKey(t) && 
-            t != Thread.currentThread() && 
-            /* its ok to keep your searcher across test cases */
-            (t.getName().startsWith("LuceneTestCase") && context.startsWith("test method")) == false) {
-          System.err.println("WARNING: " + context  + " left thread running: " + t);
-          rogueThreads.put(t, true);
-          rogueCount++;
-          if (t.getName().startsWith("LuceneTestCase")) {
-            System.err.println("PLEASE CLOSE YOUR INDEXSEARCHERS IN YOUR TEST!!!!");
-            continue;
-          } else {
-            // wait on the thread to die of natural causes
-            try {
-              t.join(THREAD_STOP_GRACE_MSEC);
-            } catch (InterruptedException e) { e.printStackTrace(); }
-          }
-          // try to stop the thread:
-          t.setUncaughtExceptionHandler(null);
-          Thread.setDefaultUncaughtExceptionHandler(null);
-          t.interrupt();
-          try {
-            t.join(THREAD_STOP_GRACE_MSEC);
-          } catch (InterruptedException e) { e.printStackTrace(); }
-        }
-      }
-    }
-    return rogueCount;
-  }
-  
-  /**
-   * Asserts that FieldCacheSanityChecker does not detect any
-   * problems with FieldCache.DEFAULT.
-   * <p>
-   * If any problems are found, they are logged to System.err
-   * (allong with the msg) when the Assertion is thrown.
-   * </p>
-   * <p>
-   * This method is called by tearDown after every test method,
-   * however IndexReaders scoped inside test methods may be garbage
-   * collected prior to this method being called, causing errors to
-   * be overlooked. Tests are encouraged to keep their IndexReaders
-   * scoped at the class level, or to explicitly call this method
-   * directly in the same scope as the IndexReader.
-   * </p>
-   *
-   * @see FieldCacheSanityChecker
-   */
-  protected void assertSaneFieldCaches(final String msg) {
-    final CacheEntry[] entries = FieldCache.DEFAULT.getCacheEntries();
-    Insanity[] insanity = null;
-    try {
-      try {
-        insanity = FieldCacheSanityChecker.checkSanity(entries);
-      } catch (RuntimeException e) {
-        dumpArray(msg + ": FieldCache", entries, System.err);
-        throw e;
-      }
-
-      assertEquals(msg + ": Insane FieldCache usage(s) found",
-              0, insanity.length);
-      insanity = null;
-    } finally {
-
-      // report this in the event of any exception/failure
-      // if no failure, then insanity will be null anyway
-      if (null != insanity) {
-        dumpArray(msg + ": Insane FieldCache usage(s)", insanity, System.err);
-      }
-
-    }
-  }
-  
-  // @deprecated (4.0) These deprecated methods should be removed soon, when all tests using no Epsilon are fixed:
-  @Deprecated
-  static public void assertEquals(double expected, double actual) {
-    assertEquals(null, expected, actual);
-  }
-   
-  @Deprecated
-  static public void assertEquals(String message, double expected, double actual) {
-    assertEquals(message, Double.valueOf(expected), Double.valueOf(actual));
-  }
-
-  @Deprecated
-  static public void assertEquals(float expected, float actual) {
-    assertEquals(null, expected, actual);
-  }
-
-  @Deprecated
-  static public void assertEquals(String message, float expected, float actual) {
-    assertEquals(message, Float.valueOf(expected), Float.valueOf(actual));
-  }
-  
-  // Replacement for Assume jUnit class, so we can add a message with explanation:
-  
-  private static final class TestIgnoredException extends RuntimeException {
-    TestIgnoredException(String msg) {
-      super(msg);
-    }
-    
-    TestIgnoredException(String msg, Throwable t) {
-      super(msg, t);
-    }
-    
-    @Override
-    public String getMessage() {
-      StringBuilder sb = new StringBuilder(super.getMessage());
-      if (getCause() != null)
-        sb.append(" - ").append(getCause());
-      return sb.toString();
-    }
-    
-    // only this one is called by our code, exception is not used outside this class:
-    @Override
-    public void printStackTrace(PrintStream s) {
-      if (getCause() != null) {
-        s.println(super.toString() + " - Caused by:");
-        getCause().printStackTrace(s);
-      } else {
-        super.printStackTrace(s);
-      }
-    }
-  }
-  
-  public static void assumeTrue(String msg, boolean b) {
-    Assume.assumeNoException(b ? null : new TestIgnoredException(msg));
-  }
- 
-  public static void assumeFalse(String msg, boolean b) {
-    assumeTrue(msg, !b);
-  }
-  
-  public static void assumeNoException(String msg, Exception e) {
-    Assume.assumeNoException(e == null ? null : new TestIgnoredException(msg, e));
-  }
- 
-  public static <T> Set<T> asSet(T... args) {
-    return new HashSet<T>(Arrays.asList(args));
-  }
-
-  /**
-   * Convinience method for logging an iterator.
-   *
-   * @param label  String logged before/after the items in the iterator
-   * @param iter   Each next() is toString()ed and logged on it's own line. If iter is null this is logged differnetly then an empty iterator.
-   * @param stream Stream to log messages to.
-   */
-  public static void dumpIterator(String label, Iterator<?> iter,
-                                  PrintStream stream) {
-    stream.println("*** BEGIN " + label + " ***");
-    if (null == iter) {
-      stream.println(" ... NULL ...");
-    } else {
-      while (iter.hasNext()) {
-        stream.println(iter.next().toString());
-      }
-    }
-    stream.println("*** END " + label + " ***");
-  }
-
-  /**
-   * Convinience method for logging an array.  Wraps the array in an iterator and delegates
-   *
-   * @see #dumpIterator(String,Iterator,PrintStream)
-   */
-  public static void dumpArray(String label, Object[] objs,
-                               PrintStream stream) {
-    Iterator<?> iter = (null == objs) ? null : Arrays.asList(objs).iterator();
-    dumpIterator(label, iter, stream);
-  }
-
-  /** create a new index writer config with random defaults */
-  public static IndexWriterConfig newIndexWriterConfig(Version v, Analyzer a) {
-    return newIndexWriterConfig(random, v, a);
-  }
-  
-  public static IndexWriterConfig newIndexWriterConfig(Random r, Version v, Analyzer a) {
-    IndexWriterConfig c = new IndexWriterConfig(v, a);
-    if (r.nextBoolean()) {
-      c.setMergeScheduler(new SerialMergeScheduler());
-    }
-    if (r.nextBoolean()) {
-      if (r.nextInt(20) == 17) {
-        c.setMaxBufferedDocs(2);
-      } else {
-        c.setMaxBufferedDocs(_TestUtil.nextInt(r, 2, 1000));
-      }
-    }
-    if (r.nextBoolean()) {
-      c.setTermIndexInterval(_TestUtil.nextInt(r, 1, 1000));
-    }
-    if (r.nextBoolean()) {
-      c.setMaxThreadStates(_TestUtil.nextInt(r, 1, 20));
-    }
-
-    if (r.nextBoolean()) {
-      c.setMergePolicy(new MockRandomMergePolicy(r));
-    } else {
-      c.setMergePolicy(newLogMergePolicy());
-    }
-
-    c.setReaderPooling(r.nextBoolean());
-    c.setReaderTermsIndexDivisor(_TestUtil.nextInt(r, 1, 4));
-    return c;
-  }
-
-  public static LogMergePolicy newLogMergePolicy() {
-    return newLogMergePolicy(random);
-  }
-
-  public static LogMergePolicy newLogMergePolicy(Random r) {
-    LogMergePolicy logmp = r.nextBoolean() ? new LogDocMergePolicy() : new LogByteSizeMergePolicy();
-    logmp.setUseCompoundFile(r.nextBoolean());
-    logmp.setCalibrateSizeByDeletes(r.nextBoolean());
-    if (r.nextInt(3) == 2) {
-      logmp.setMergeFactor(2);
-    } else {
-      logmp.setMergeFactor(_TestUtil.nextInt(r, 2, 20));
-    }
-    return logmp;
-  }
-
-  public static LogMergePolicy newInOrderLogMergePolicy() {
-    LogMergePolicy logmp = newLogMergePolicy();
-    logmp.setRequireContiguousMerge(true);
-    return logmp;
-  }
-
-  public static LogMergePolicy newInOrderLogMergePolicy(int mergeFactor) {
-    LogMergePolicy logmp = newLogMergePolicy();
-    logmp.setMergeFactor(mergeFactor);
-    logmp.setRequireContiguousMerge(true);
-    return logmp;
-  }
-
-  public static LogMergePolicy newLogMergePolicy(boolean useCFS) {
-    LogMergePolicy logmp = newLogMergePolicy();
-    logmp.setUseCompoundFile(useCFS);
-    return logmp;
-  }
-
-  public static LogMergePolicy newLogMergePolicy(boolean useCFS, int mergeFactor) {
-    LogMergePolicy logmp = newLogMergePolicy();
-    logmp.setUseCompoundFile(useCFS);
-    logmp.setMergeFactor(mergeFactor);
-    return logmp;
-  }
-
-  public static LogMergePolicy newLogMergePolicy(int mergeFactor) {
-    LogMergePolicy logmp = newLogMergePolicy();
-    logmp.setMergeFactor(mergeFactor);
-    return logmp;
-  }
-
-  /**
-   * Returns a new Directory instance. Use this when the test does not
-   * care about the specific Directory implementation (most tests).
-   * <p>
-   * The Directory is wrapped with {@link MockDirectoryWrapper}.
-   * By default this means it will be picky, such as ensuring that you
-   * properly close it and all open files in your test. It will emulate
-   * some features of Windows, such as not allowing open files to be
-   * overwritten.
-   */
-  public static MockDirectoryWrapper newDirectory() throws IOException {
-    return newDirectory(random);
-  }
-  
-  public static MockDirectoryWrapper newDirectory(Random r) throws IOException {
-    Directory impl = newDirectoryImpl(r, TEST_DIRECTORY);
-    MockDirectoryWrapper dir = new MockDirectoryWrapper(r, impl);
-    stores.put(dir, Thread.currentThread().getStackTrace());
-    return dir;
-  }
-  
-  /**
-   * Returns a new Directory instance, with contents copied from the
-   * provided directory. See {@link #newDirectory()} for more
-   * information.
-   */
-  public static MockDirectoryWrapper newDirectory(Directory d) throws IOException {
-    return newDirectory(random, d);
-  }
-  
-  /** Returns a new FSDirectory instance over the given file, which must be a folder. */
-  public static MockDirectoryWrapper newFSDirectory(File f) throws IOException {
-    return newFSDirectory(f, null);
-  }
-  
-  /** Returns a new FSDirectory instance over the given file, which must be a folder. */
-  public static MockDirectoryWrapper newFSDirectory(File f, LockFactory lf) throws IOException {
-    String fsdirClass = TEST_DIRECTORY;
-    if (fsdirClass.equals("random")) {
-      fsdirClass = FS_DIRECTORIES[random.nextInt(FS_DIRECTORIES.length)];
-    }
-    
-    if (fsdirClass.indexOf(".") == -1) {// if not fully qualified, assume .store
-      fsdirClass = "org.apache.lucene.store." + fsdirClass;
-    }
-    
-    Class<? extends FSDirectory> clazz;
-    try {
-      try {
-        clazz = Class.forName(fsdirClass).asSubclass(FSDirectory.class);
-      } catch (ClassCastException e) {
-        // TEST_DIRECTORY is not a sub-class of FSDirectory, so draw one at random
-        fsdirClass = FS_DIRECTORIES[random.nextInt(FS_DIRECTORIES.length)];
-        
-        if (fsdirClass.indexOf(".") == -1) {// if not fully qualified, assume .store
-          fsdirClass = "org.apache.lucene.store." + fsdirClass;
-        }
-        
-        clazz = Class.forName(fsdirClass).asSubclass(FSDirectory.class);
-      }
-      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, newFSDirectoryImpl(clazz, f, lf));
-      stores.put(dir, Thread.currentThread().getStackTrace());
-      return dir;
-    } catch (Exception e) {
-      throw new RuntimeException(e);
-    }
-  }
-  
-  public static MockDirectoryWrapper newDirectory(Random r, Directory d) throws IOException {
-    Directory impl = newDirectoryImpl(r, TEST_DIRECTORY);
-    for (String file : d.listAll()) {
-     d.copy(impl, file, file);
-    }
-    MockDirectoryWrapper dir = new MockDirectoryWrapper(r, impl);
-    stores.put(dir, Thread.currentThread().getStackTrace());
-    return dir;
-  }
-  
-  public static Field newField(String name, String value, Index index) {
-    return newField(random, name, value, index);
-  }
-  
-  public static Field newField(String name, String value, Store store, Index index) {
-    return newField(random, name, value, store, index);
-  }
-  
-  public static Field newField(String name, String value, Store store, Index index, TermVector tv) {
-    return newField(random, name, value, store, index, tv);
-  }
-  
-  public static Field newField(Random random, String name, String value, Index index) {
-    return newField(random, name, value, Store.NO, index);
-  }
-  
-  public static Field newField(Random random, String name, String value, Store store, Index index) {
-    return newField(random, name, value, store, index, TermVector.NO);
-  }
-  
-  public static Field newField(Random random, String name, String value, Store store, Index index, TermVector tv) {
-    if (!index.isIndexed())
-      return new Field(name, value, store, index);
-    
-    if (!store.isStored() && random.nextBoolean())
-      store = Store.YES; // randomly store it
-    
-    tv = randomTVSetting(random, tv);
-    
-    return new Field(name, value, store, index, tv);
-  }
-  
-  static final TermVector tvSettings[] = { 
-    TermVector.NO, TermVector.YES, TermVector.WITH_OFFSETS, 
-    TermVector.WITH_POSITIONS, TermVector.WITH_POSITIONS_OFFSETS 
-  };
-  
-  private static TermVector randomTVSetting(Random random, TermVector minimum) {
-    switch(minimum) {
-      case NO: return tvSettings[_TestUtil.nextInt(random, 0, tvSettings.length-1)];
-      case YES: return tvSettings[_TestUtil.nextInt(random, 1, tvSettings.length-1)];
-      case WITH_OFFSETS: return random.nextBoolean() ? TermVector.WITH_OFFSETS 
-          : TermVector.WITH_POSITIONS_OFFSETS;
-      case WITH_POSITIONS: return random.nextBoolean() ? TermVector.WITH_POSITIONS 
-          : TermVector.WITH_POSITIONS_OFFSETS;
-      default: return TermVector.WITH_POSITIONS_OFFSETS;
-    }
-  }
-  
-  /** return a random Locale from the available locales on the system */
-  public static Locale randomLocale(Random random) {
-    Locale locales[] = Locale.getAvailableLocales();
-    return locales[random.nextInt(locales.length)];
-  }
-  
-  /** return a random TimeZone from the available timezones on the system */
-  public static TimeZone randomTimeZone(Random random) {
-    String tzIds[] = TimeZone.getAvailableIDs();
-    return TimeZone.getTimeZone(tzIds[random.nextInt(tzIds.length)]);
-  }
-  
-  /** return a Locale object equivalent to its programmatic name */
-  public static Locale localeForName(String localeName) {
-    String elements[] = localeName.split("\\_");
-    switch(elements.length) {
-      case 3: return new Locale(elements[0], elements[1], elements[2]);
-      case 2: return new Locale(elements[0], elements[1]);
-      case 1: return new Locale(elements[0]);
-      default: throw new IllegalArgumentException("Invalid Locale: " + localeName);
-    }
-  }
-
-  private static final String FS_DIRECTORIES[] = {
-    "SimpleFSDirectory",
-    "NIOFSDirectory",
-    "MMapDirectory"
-  };
-
-  private static final String CORE_DIRECTORIES[] = {
-    "RAMDirectory",
-    FS_DIRECTORIES[0], FS_DIRECTORIES[1], FS_DIRECTORIES[2]
-  };
-  
-  public static String randomDirectory(Random random) {
-    if (random.nextInt(10) == 0) {
-      return CORE_DIRECTORIES[random.nextInt(CORE_DIRECTORIES.length)];
-    } else {
-      return "RAMDirectory";
-    }
-  }
-
-  private static Directory newFSDirectoryImpl(
-      Class<? extends FSDirectory> clazz, File file, LockFactory lockFactory)
-      throws IOException {
-    try {
-      // Assuming every FSDirectory has a ctor(File), but not all may take a
-      // LockFactory too, so setting it afterwards.
-      Constructor<? extends FSDirectory> ctor = clazz.getConstructor(File.class);
-      FSDirectory d = ctor.newInstance(file);
-      if (lockFactory != null) {
-        d.setLockFactory(lockFactory);
-      }
-      return d;
-    } catch (Exception e) {
-      return FSDirectory.open(file);
-    }
-  }
-  
-  static Directory newDirectoryImpl(Random random, String clazzName) {
-    if (clazzName.equals("random"))
-      clazzName = randomDirectory(random);
-    if (clazzName.indexOf(".") == -1) // if not fully qualified, assume .store
-      clazzName = "org.apache.lucene.store." + clazzName;
-    try {
-      final Class<? extends Directory> clazz = Class.forName(clazzName).asSubclass(Directory.class);
-      // If it is a FSDirectory type, try its ctor(File)
-      if (FSDirectory.class.isAssignableFrom(clazz)) {
-        final File tmpFile = File.createTempFile("test", "tmp", TEMP_DIR);
-        tmpFile.delete();
-        tmpFile.mkdir();
-        return newFSDirectoryImpl(clazz.asSubclass(FSDirectory.class), tmpFile, null);
-      }
-
-      // try empty ctor
-      return clazz.newInstance();
-    } catch (Exception e) {
-      throw new RuntimeException(e);
-    } 
-  }
-  
-  /** create a new searcher over the reader */
-  public static IndexSearcher newSearcher(IndexReader r) throws IOException {
-    if (random.nextBoolean()) {
-      return new IndexSearcher(r);
-    } else {
-      int threads = 0;
-      final ExecutorService ex = (random.nextBoolean()) ? null 
-          : Executors.newFixedThreadPool(threads = _TestUtil.nextInt(random, 1, 8), 
-                      new NamedThreadFactory("LuceneTestCase"));
-      if (ex != null && VERBOSE) {
-        System.out.println("NOTE: newSearcher using ExecutorService with " + threads + " threads");
-      }
-      return new IndexSearcher(r.getTopReaderContext(), ex) {
-        @Override
-        public void close() throws IOException {
-          super.close();
-          if (ex != null) {
-            ex.shutdown();
-            try {
-              ex.awaitTermination(1000, TimeUnit.MILLISECONDS);
-            } catch (InterruptedException e) {
-              e.printStackTrace();
-            }
-          }
-        }
-      };
-    }
-  }
-
-  public String getName() {
-    return this.name;
-  }
-  
-  /** Gets a resource from the classpath as {@link File}. This method should only be used,
-   * if a real file is needed. To get a stream, code should prefer
-   * {@link Class#getResourceAsStream} using {@code this.getClass()}.
-   */
-  
-  protected File getDataFile(String name) throws IOException {
-    try {
-      return new File(this.getClass().getResource(name).toURI());
-    } catch (Exception e) {
-      throw new IOException("Cannot find resource: " + name);
-    }
-  }
-
-  // We get here from InterceptTestCaseEvents on the 'failed' event....
-  public void reportAdditionalFailureInfo() {
-    System.err.println("NOTE: reproduce with: ant test -Dtestcase=" + getClass().getSimpleName() 
-        + " -Dtestmethod=" + getName() + " -Dtests.seed=" + new TwoLongs(staticSeed, seed)
-        + reproduceWithExtraParams());
-  }
-  
-  // extra params that were overridden needed to reproduce the command
-  private String reproduceWithExtraParams() {
-    StringBuilder sb = new StringBuilder();
-    if (!TEST_CODEC.equals("randomPerField")) sb.append(" -Dtests.codec=").append(TEST_CODEC);
-    if (!TEST_LOCALE.equals("random")) sb.append(" -Dtests.locale=").append(TEST_LOCALE);
-    if (!TEST_TIMEZONE.equals("random")) sb.append(" -Dtests.timezone=").append(TEST_TIMEZONE);
-    if (!TEST_DIRECTORY.equals("random")) sb.append(" -Dtests.directory=").append(TEST_DIRECTORY);
-    if (RANDOM_MULTIPLIER > 1) sb.append(" -Dtests.multiplier=").append(RANDOM_MULTIPLIER);
-    return sb.toString();
-  }
-
-  // recorded seed: for beforeClass
-  private static long staticSeed;
-  // seed for individual test methods, changed in @before
-  private long seed;
-  
-  private static final Random seedRand = new Random();
-  protected static final Random random = new Random(0);
-
-  private String name = "<unknown>";
-  
-  /**
-   * Annotation for tests that should only be run during nightly builds.
-   */
-  @Documented
-  @Inherited
-  @Retention(RetentionPolicy.RUNTIME)
-  public @interface Nightly {}
-  
-  /** optionally filters the tests to be run by TEST_METHOD */
-  public static class LuceneTestCaseRunner extends BlockJUnit4ClassRunner {
-    private List<FrameworkMethod> testMethods;
-
-    @Override
-    protected List<FrameworkMethod> computeTestMethods() {
-      if (testMethods != null)
-        return testMethods;
-      testClassesRun.add(getTestClass().getJavaClass().getSimpleName());
-      testMethods = new ArrayList<FrameworkMethod>();
-      for (Method m : getTestClass().getJavaClass().getMethods()) {
-        // check if the current test's class has methods annotated with @Ignore
-        final Ignore ignored = m.getAnnotation(Ignore.class);
-        if (ignored != null && !m.getName().equals("alwaysIgnoredTestMethod")) {
-          System.err.println("NOTE: Ignoring test method '" + m.getName() + "': " + ignored.value());
-        }
-        // add methods starting with "test"
-        final int mod = m.getModifiers();
-        if (m.getAnnotation(Test.class) != null ||
-            (m.getName().startsWith("test") &&
-            !Modifier.isAbstract(mod) &&
-            m.getParameterTypes().length == 0 &&
-            m.getReturnType() == Void.TYPE))
-        {
-          if (Modifier.isStatic(mod))
-            throw new RuntimeException("Test methods must not be static.");
-          testMethods.add(new FrameworkMethod(m));
-        }
-      }
-      
-      if (testMethods.isEmpty()) {
-        throw new RuntimeException("No runnable methods!");
-      }
-      
-      if (TEST_NIGHTLY == false) {
-        if (getTestClass().getJavaClass().isAnnotationPresent(Nightly.class)) {
-          /* the test class is annotated with nightly, remove all methods */
-          String className = getTestClass().getJavaClass().getSimpleName();
-          System.err.println("NOTE: Ignoring nightly-only test class '" + className + "'");
-          testMethods.clear();
-        } else {
-          /* remove all nightly-only methods */
-          for (int i = 0; i < testMethods.size(); i++) {
-            final FrameworkMethod m = testMethods.get(i);
-            if (m.getAnnotation(Nightly.class) != null) {
-              System.err.println("NOTE: Ignoring nightly-only test method '" + m.getName() + "'");
-              testMethods.remove(i--);
-            }
-          }
-        }
-        /* dodge a possible "no-runnable methods" exception by adding a fake ignored test */
-        if (testMethods.isEmpty()) {
-          try {
-            testMethods.add(new FrameworkMethod(LuceneTestCase.class.getMethod("alwaysIgnoredTestMethod")));
-          } catch (Exception e) { throw new RuntimeException(e); }
-        }
-      }
-      return testMethods;
-    }
-
-    @Override
-    protected void runChild(FrameworkMethod arg0, RunNotifier arg1) {
-      if (VERBOSE) {
-        System.out.println("\nNOTE: running test " + arg0.getName());
-      }
-      for (int i = 0; i < TEST_ITER; i++) {
-        if (VERBOSE && TEST_ITER > 1) {
-          System.out.println("\nNOTE: running iter=" + (1+i) + " of " + TEST_ITER);
-        }
-        super.runChild(arg0, arg1);
-      }
-    }
-
-    public LuceneTestCaseRunner(Class<?> clazz) throws InitializationError {
-      super(clazz);
-      Filter f = new Filter() {
-
-        @Override
-        public String describe() { return "filters according to TEST_METHOD"; }
-
-        @Override
-        public boolean shouldRun(Description d) {
-          return TEST_METHOD == null || d.getMethodName().equals(TEST_METHOD);
-        }     
-      };
-      
-      try {
-        f.apply(this);
-      } catch (NoTestsRemainException e) {
-        throw new RuntimeException(e);
-      }
-    }
-  }
-  
-  private static class RandomCodecProvider extends CodecProvider {
-    private List<Codec> knownCodecs = new ArrayList<Codec>();
-    private Map<String,Codec> previousMappings = new HashMap<String,Codec>();
-    private final int perFieldSeed;
-    
-    RandomCodecProvider(Random random) {
-      this.perFieldSeed = random.nextInt();
-      register(new StandardCodec());
-      register(new PreFlexCodec());
-      register(new PulsingCodec(1));
-      register(new SimpleTextCodec());
-      Collections.shuffle(knownCodecs, random);
-    }
-
-    @Override
-    public synchronized void register(Codec codec) {
-      if (!codec.name.equals("PreFlex"))
-        knownCodecs.add(codec);
-      super.register(codec);
-    }
-
-    @Override
-    public synchronized void unregister(Codec codec) {
-      knownCodecs.remove(codec);
-      super.unregister(codec);
-    }
-
-    @Override
-    public synchronized String getFieldCodec(String name) {
-      Codec codec = previousMappings.get(name);
-      if (codec == null) {
-        codec = knownCodecs.get(Math.abs(perFieldSeed ^ name.hashCode()) % knownCodecs.size());
-        previousMappings.put(name, codec);
-      }
-      return codec.name;
-    }
-    
-    @Override
-    public String toString() {
-      return "RandomCodecProvider: " + previousMappings.toString();
-    }
-  }
-  
-  @Ignore("just a hack")
-  public final void alwaysIgnoredTestMethod() {}
-}
diff --git a/lucene/src/test/org/apache/lucene/util/_TestUtil.java b/lucene/src/test/org/apache/lucene/util/_TestUtil.java
deleted file mode 100644
index 1ad0380..0000000
--- a/lucene/src/test/org/apache/lucene/util/_TestUtil.java
+++ /dev/null
@@ -1,308 +0,0 @@
-package org.apache.lucene.util;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.BufferedOutputStream;
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.FileOutputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
-import java.io.PrintStream;
-import java.util.Enumeration;
-import java.util.Random;
-import java.util.Map;
-import java.util.HashMap;
-import java.util.zip.ZipEntry;
-import java.util.zip.ZipFile;
-
-import org.junit.Assert;
-
-import org.apache.lucene.index.CheckIndex;
-import org.apache.lucene.index.ConcurrentMergeScheduler;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.LogMergePolicy;
-import org.apache.lucene.index.MergeScheduler;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.CodecProvider;
-import org.apache.lucene.store.Directory;
-
-public class _TestUtil {
-
-  /** Returns temp dir, containing String arg in its name;
-   *  does not create the directory. */
-  public static File getTempDir(String desc) {
-    return new File(LuceneTestCase.TEMP_DIR, desc + "." + new Random().nextLong());
-  }
-
-  public static void rmDir(File dir) throws IOException {
-    if (dir.exists()) {
-      for (File f : dir.listFiles()) {
-        if (f.isDirectory()) {
-          rmDir(f);
-        } else {
-          if (!f.delete()) {
-            throw new IOException("could not delete " + f);
-          }
-        }
-      }
-      if (!dir.delete()) {
-        throw new IOException("could not delete " + dir);
-      }
-    }
-  }
-
-  /** 
-   * Convenience method: Unzip zipName + ".zip" under destDir, removing destDir first 
-   */
-  public static void unzip(File zipName, File destDir) throws IOException {
-    
-    ZipFile zipFile = new ZipFile(zipName);
-    
-    Enumeration<? extends ZipEntry> entries = zipFile.entries();
-    
-    rmDir(destDir);
-    
-    destDir.mkdir();
-    
-    while (entries.hasMoreElements()) {
-      ZipEntry entry = entries.nextElement();
-      
-      InputStream in = zipFile.getInputStream(entry);
-      File targetFile = new File(destDir, entry.getName());
-      if (entry.isDirectory()) {
-        // allow unzipping with directory structure
-        targetFile.mkdirs();
-      } else {
-        if (targetFile.getParentFile()!=null) {
-          // be on the safe side: do not rely on that directories are always extracted
-          // before their children (although this makes sense, but is it guaranteed?)
-          targetFile.getParentFile().mkdirs();   
-        }
-        OutputStream out = new BufferedOutputStream(new FileOutputStream(targetFile));
-        
-        byte[] buffer = new byte[8192];
-        int len;
-        while((len = in.read(buffer)) >= 0) {
-          out.write(buffer, 0, len);
-        }
-        
-        in.close();
-        out.close();
-      }
-    }
-    
-    zipFile.close();
-  }
-  
-  public static void syncConcurrentMerges(IndexWriter writer) {
-    syncConcurrentMerges(writer.getConfig().getMergeScheduler());
-  }
-
-  public static void syncConcurrentMerges(MergeScheduler ms) {
-    if (ms instanceof ConcurrentMergeScheduler)
-      ((ConcurrentMergeScheduler) ms).sync();
-  }
-
-  /** This runs the CheckIndex tool on the index in.  If any
-   *  issues are hit, a RuntimeException is thrown; else,
-   *  true is returned. */
-  public static CheckIndex.Status checkIndex(Directory dir) throws IOException {
-    return checkIndex(dir, CodecProvider.getDefault());
-  }
-  
-  /** This runs the CheckIndex tool on the index in.  If any
-   *  issues are hit, a RuntimeException is thrown; else,
-   *  true is returned. */
-  public static CheckIndex.Status checkIndex(Directory dir, CodecProvider codecs) throws IOException {
-    ByteArrayOutputStream bos = new ByteArrayOutputStream(1024);
-    CheckIndex checker = new CheckIndex(dir);
-    checker.setInfoStream(new PrintStream(bos));
-    CheckIndex.Status indexStatus = checker.checkIndex(null, codecs);
-    if (indexStatus == null || indexStatus.clean == false) {
-      System.out.println("CheckIndex failed");
-      System.out.println(bos.toString());
-      throw new RuntimeException("CheckIndex failed");
-    } else {
-      return indexStatus;
-    }
-  }
-
-  /** start and end are BOTH inclusive */
-  public static int nextInt(Random r, int start, int end) {
-    return start + r.nextInt(end-start+1);
-  }
-
-  /** Returns random string, including full unicode range. */
-  public static String randomUnicodeString(Random r) {
-    return randomUnicodeString(r, 20);
-  }
-
-  public static String randomUnicodeString(Random r, int maxLength) {
-    final int end = r.nextInt(maxLength);
-    if (end == 0) {
-      // allow 0 length
-      return "";
-    }
-    final char[] buffer = new char[end];
-    for (int i = 0; i < end; i++) {
-      int t = r.nextInt(5);
-
-      if (0 == t && i < end - 1) {
-        // Make a surrogate pair
-        // High surrogate
-        buffer[i++] = (char) nextInt(r, 0xd800, 0xdbff);
-        // Low surrogate
-        buffer[i] = (char) nextInt(r, 0xdc00, 0xdfff);
-      }
-      else if (t <= 1) buffer[i] = (char) r.nextInt(0x80);
-      else if (2 == t) buffer[i] = (char) nextInt(r, 0x80, 0x800);
-      else if (3 == t) buffer[i] = (char) nextInt(r, 0x800, 0xd7ff);
-      else if (4 == t) buffer[i] = (char) nextInt(r, 0xe000, 0xffff);
-    }
-    return new String(buffer, 0, end);
-  }
-
-  private static final int[] blockStarts = {
-    0x0000, 0x0080, 0x0100, 0x0180, 0x0250, 0x02B0, 0x0300, 0x0370, 0x0400, 
-    0x0500, 0x0530, 0x0590, 0x0600, 0x0700, 0x0750, 0x0780, 0x07C0, 0x0800, 
-    0x0900, 0x0980, 0x0A00, 0x0A80, 0x0B00, 0x0B80, 0x0C00, 0x0C80, 0x0D00, 
-    0x0D80, 0x0E00, 0x0E80, 0x0F00, 0x1000, 0x10A0, 0x1100, 0x1200, 0x1380, 
-    0x13A0, 0x1400, 0x1680, 0x16A0, 0x1700, 0x1720, 0x1740, 0x1760, 0x1780, 
-    0x1800, 0x18B0, 0x1900, 0x1950, 0x1980, 0x19E0, 0x1A00, 0x1A20, 0x1B00, 
-    0x1B80, 0x1C00, 0x1C50, 0x1CD0, 0x1D00, 0x1D80, 0x1DC0, 0x1E00, 0x1F00, 
-    0x2000, 0x2070, 0x20A0, 0x20D0, 0x2100, 0x2150, 0x2190, 0x2200, 0x2300, 
-    0x2400, 0x2440, 0x2460, 0x2500, 0x2580, 0x25A0, 0x2600, 0x2700, 0x27C0, 
-    0x27F0, 0x2800, 0x2900, 0x2980, 0x2A00, 0x2B00, 0x2C00, 0x2C60, 0x2C80, 
-    0x2D00, 0x2D30, 0x2D80, 0x2DE0, 0x2E00, 0x2E80, 0x2F00, 0x2FF0, 0x3000, 
-    0x3040, 0x30A0, 0x3100, 0x3130, 0x3190, 0x31A0, 0x31C0, 0x31F0, 0x3200, 
-    0x3300, 0x3400, 0x4DC0, 0x4E00, 0xA000, 0xA490, 0xA4D0, 0xA500, 0xA640, 
-    0xA6A0, 0xA700, 0xA720, 0xA800, 0xA830, 0xA840, 0xA880, 0xA8E0, 0xA900, 
-    0xA930, 0xA960, 0xA980, 0xAA00, 0xAA60, 0xAA80, 0xABC0, 0xAC00, 0xD7B0, 
-    0xE000, 0xF900, 0xFB00, 0xFB50, 0xFE00, 0xFE10, 
-    0xFE20, 0xFE30, 0xFE50, 0xFE70, 0xFF00, 0xFFF0, 
-    0x10000, 0x10080, 0x10100, 0x10140, 0x10190, 0x101D0, 0x10280, 0x102A0, 
-    0x10300, 0x10330, 0x10380, 0x103A0, 0x10400, 0x10450, 0x10480, 0x10800, 
-    0x10840, 0x10900, 0x10920, 0x10A00, 0x10A60, 0x10B00, 0x10B40, 0x10B60, 
-    0x10C00, 0x10E60, 0x11080, 0x12000, 0x12400, 0x13000, 0x1D000, 0x1D100, 
-    0x1D200, 0x1D300, 0x1D360, 0x1D400, 0x1F000, 0x1F030, 0x1F100, 0x1F200, 
-    0x20000, 0x2A700, 0x2F800, 0xE0000, 0xE0100, 0xF0000, 0x100000
-  };
-  
-  private static final int[] blockEnds = {
-    0x007F, 0x00FF, 0x017F, 0x024F, 0x02AF, 0x02FF, 0x036F, 0x03FF, 0x04FF, 
-    0x052F, 0x058F, 0x05FF, 0x06FF, 0x074F, 0x077F, 0x07BF, 0x07FF, 0x083F, 
-    0x097F, 0x09FF, 0x0A7F, 0x0AFF, 0x0B7F, 0x0BFF, 0x0C7F, 0x0CFF, 0x0D7F, 
-    0x0DFF, 0x0E7F, 0x0EFF, 0x0FFF, 0x109F, 0x10FF, 0x11FF, 0x137F, 0x139F, 
-    0x13FF, 0x167F, 0x169F, 0x16FF, 0x171F, 0x173F, 0x175F, 0x177F, 0x17FF, 
-    0x18AF, 0x18FF, 0x194F, 0x197F, 0x19DF, 0x19FF, 0x1A1F, 0x1AAF, 0x1B7F, 
-    0x1BBF, 0x1C4F, 0x1C7F, 0x1CFF, 0x1D7F, 0x1DBF, 0x1DFF, 0x1EFF, 0x1FFF, 
-    0x206F, 0x209F, 0x20CF, 0x20FF, 0x214F, 0x218F, 0x21FF, 0x22FF, 0x23FF, 
-    0x243F, 0x245F, 0x24FF, 0x257F, 0x259F, 0x25FF, 0x26FF, 0x27BF, 0x27EF, 
-    0x27FF, 0x28FF, 0x297F, 0x29FF, 0x2AFF, 0x2BFF, 0x2C5F, 0x2C7F, 0x2CFF, 
-    0x2D2F, 0x2D7F, 0x2DDF, 0x2DFF, 0x2E7F, 0x2EFF, 0x2FDF, 0x2FFF, 0x303F, 
-    0x309F, 0x30FF, 0x312F, 0x318F, 0x319F, 0x31BF, 0x31EF, 0x31FF, 0x32FF, 
-    0x33FF, 0x4DBF, 0x4DFF, 0x9FFF, 0xA48F, 0xA4CF, 0xA4FF, 0xA63F, 0xA69F, 
-    0xA6FF, 0xA71F, 0xA7FF, 0xA82F, 0xA83F, 0xA87F, 0xA8DF, 0xA8FF, 0xA92F, 
-    0xA95F, 0xA97F, 0xA9DF, 0xAA5F, 0xAA7F, 0xAADF, 0xABFF, 0xD7AF, 0xD7FF, 
-    0xF8FF, 0xFAFF, 0xFB4F, 0xFDFF, 0xFE0F, 0xFE1F, 
-    0xFE2F, 0xFE4F, 0xFE6F, 0xFEFF, 0xFFEF, 0xFFFF, 
-    0x1007F, 0x100FF, 0x1013F, 0x1018F, 0x101CF, 0x101FF, 0x1029F, 0x102DF, 
-    0x1032F, 0x1034F, 0x1039F, 0x103DF, 0x1044F, 0x1047F, 0x104AF, 0x1083F, 
-    0x1085F, 0x1091F, 0x1093F, 0x10A5F, 0x10A7F, 0x10B3F, 0x10B5F, 0x10B7F, 
-    0x10C4F, 0x10E7F, 0x110CF, 0x123FF, 0x1247F, 0x1342F, 0x1D0FF, 0x1D1FF, 
-    0x1D24F, 0x1D35F, 0x1D37F, 0x1D7FF, 0x1F02F, 0x1F09F, 0x1F1FF, 0x1F2FF, 
-    0x2A6DF, 0x2B73F, 0x2FA1F, 0xE007F, 0xE01EF, 0xFFFFF, 0x10FFFF
-  };
-  
-  /** Returns random string, all codepoints within the same unicode block. */
-  public static String randomRealisticUnicodeString(Random r) {
-    return randomRealisticUnicodeString(r, 20);
-  }
-  
-  /** Returns random string, all codepoints within the same unicode block. */
-  public static String randomRealisticUnicodeString(Random r, int maxLength) {
-    final int end = r.nextInt(maxLength);
-    final int block = r.nextInt(blockStarts.length);
-    StringBuilder sb = new StringBuilder();
-    for (int i = 0; i < end; i++)
-      sb.appendCodePoint(nextInt(r, blockStarts[block], blockEnds[block]));
-    return sb.toString();
-  }
-
-  public static CodecProvider alwaysCodec(final Codec c) {
-    CodecProvider p = new CodecProvider() {
-      @Override
-      public Codec lookup(String name) {
-        // can't do this until we fix PreFlexRW to not
-        //impersonate PreFlex:
-        if (name.equals(c.name)) {
-          return c;
-        } else {
-          return CodecProvider.getDefault().lookup(name);
-        }
-      }
-    };
-    p.setDefaultFieldCodec(c.name);
-    return p;
-  }
-
-  /** Return a CodecProvider that can read any of the
-   *  default codecs, but always writes in the specified
-   *  codec. */
-  public static CodecProvider alwaysCodec(final String codec) {
-    return alwaysCodec(CodecProvider.getDefault().lookup(codec));
-  }
-
-  public static boolean anyFilesExceptWriteLock(Directory dir) throws IOException {
-    String[] files = dir.listAll();
-    if (files.length > 1 || (files.length == 1 && !files[0].equals("write.lock"))) {
-      return true;
-    } else {
-      return false;
-    }
-  }
-
-  // just tries to configure things to keep the open file
-  // count lowish
-  public static void reduceOpenFiles(IndexWriter w) {
-    // keep number of open files lowish
-    LogMergePolicy lmp = (LogMergePolicy) w.getConfig().getMergePolicy();
-    lmp.setMergeFactor(Math.min(5, lmp.getMergeFactor()));
-
-    MergeScheduler ms = w.getConfig().getMergeScheduler();
-    if (ms instanceof ConcurrentMergeScheduler) {
-      ((ConcurrentMergeScheduler) ms).setMaxThreadCount(2);
-      ((ConcurrentMergeScheduler) ms).setMaxMergeCount(3);
-    }
-  }
-
-  /** Checks some basic behaviour of an AttributeImpl
-   * @param reflectedValues contains a map with "AttributeClass#key" as values
-   */
-  public static <T> void assertAttributeReflection(final AttributeImpl att, Map<String,T> reflectedValues) {
-    final Map<String,Object> map = new HashMap<String,Object>();
-    att.reflectWith(new AttributeReflector() {
-      public void reflect(Class<? extends Attribute> attClass, String key, Object value) {
-        map.put(attClass.getName() + '#' + key, value);
-      }
-    });
-    Assert.assertEquals("Reflection does not produce same map", reflectedValues, map);
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/util/automaton/AutomatonTestUtil.java b/lucene/src/test/org/apache/lucene/util/automaton/AutomatonTestUtil.java
deleted file mode 100644
index 6b9ef6e..0000000
--- a/lucene/src/test/org/apache/lucene/util/automaton/AutomatonTestUtil.java
+++ /dev/null
@@ -1,376 +0,0 @@
-package org.apache.lucene.util.automaton;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.IdentityHashMap;
-import java.util.LinkedList;
-import java.util.List;
-import java.util.Map;
-import java.util.Random;
-import java.util.Set;
-
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.UnicodeUtil;
-import org.apache.lucene.util._TestUtil;
-
-public class AutomatonTestUtil {
-  /** Returns random string, including full unicode range. */
-  public static String randomRegexp(Random r) {
-    while (true) {
-      String regexp = randomRegexpString(r);
-      // we will also generate some undefined unicode queries
-      if (!UnicodeUtil.validUTF16String(regexp))
-        continue;
-      try {
-        new RegExp(regexp, RegExp.NONE);
-        return regexp;
-      } catch (Exception e) {}
-    }
-  }
-
-  private static String randomRegexpString(Random r) {
-    final int end = r.nextInt(20);
-    if (end == 0) {
-      // allow 0 length
-      return "";
-    }
-    final char[] buffer = new char[end];
-    for (int i = 0; i < end; i++) {
-      int t = r.nextInt(15);
-      if (0 == t && i < end - 1) {
-        // Make a surrogate pair
-        // High surrogate
-        buffer[i++] = (char) _TestUtil.nextInt(r, 0xd800, 0xdbff);
-        // Low surrogate
-        buffer[i] = (char) _TestUtil.nextInt(r, 0xdc00, 0xdfff);
-      }
-      else if (t <= 1) buffer[i] = (char) r.nextInt(0x80);
-      else if (2 == t) buffer[i] = (char) _TestUtil.nextInt(r, 0x80, 0x800);
-      else if (3 == t) buffer[i] = (char) _TestUtil.nextInt(r, 0x800, 0xd7ff);
-      else if (4 == t) buffer[i] = (char) _TestUtil.nextInt(r, 0xe000, 0xffff);
-      else if (5 == t) buffer[i] = '.';
-      else if (6 == t) buffer[i] = '?';
-      else if (7 == t) buffer[i] = '*';
-      else if (8 == t) buffer[i] = '+';
-      else if (9 == t) buffer[i] = '(';
-      else if (10 == t) buffer[i] = ')';
-      else if (11 == t) buffer[i] = '-';
-      else if (12 == t) buffer[i] = '[';
-      else if (13 == t) buffer[i] = ']';
-      else if (14 == t) buffer[i] = '|';
-    }
-    return new String(buffer, 0, end);
-  }
-  
-  // picks a random int code point, avoiding surrogates;
-  // throws IllegalArgumentException if this transition only
-  // accepts surrogates
-  private static int getRandomCodePoint(final Random r, final Transition t) {
-    final int code;
-    if (t.max < UnicodeUtil.UNI_SUR_HIGH_START ||
-        t.min > UnicodeUtil.UNI_SUR_HIGH_END) {
-      // easy: entire range is before or after surrogates
-      code = t.min+r.nextInt(t.max-t.min+1);
-    } else if (t.min >= UnicodeUtil.UNI_SUR_HIGH_START) {
-      if (t.max > UnicodeUtil.UNI_SUR_LOW_END) {
-        // after surrogates
-        code = 1+UnicodeUtil.UNI_SUR_LOW_END+r.nextInt(t.max-UnicodeUtil.UNI_SUR_LOW_END);
-      } else {
-        throw new IllegalArgumentException("transition accepts only surrogates: " + t);
-      }
-    } else if (t.max <= UnicodeUtil.UNI_SUR_LOW_END) {
-      if (t.min < UnicodeUtil.UNI_SUR_HIGH_START) {
-        // before surrogates
-        code = t.min + r.nextInt(UnicodeUtil.UNI_SUR_HIGH_START - t.min);
-      } else {
-        throw new IllegalArgumentException("transition accepts only surrogates: " + t);
-      }
-    } else {
-      // range includes all surrogates
-      int gap1 = UnicodeUtil.UNI_SUR_HIGH_START - t.min;
-      int gap2 = t.max - UnicodeUtil.UNI_SUR_LOW_END;
-      int c = r.nextInt(gap1+gap2);
-      if (c < gap1) {
-        code = t.min + c;
-      } else {
-        code = UnicodeUtil.UNI_SUR_LOW_END + c - gap1 + 1;
-      }
-    }
-
-    assert code >= t.min && code <= t.max && (code < UnicodeUtil.UNI_SUR_HIGH_START || code > UnicodeUtil.UNI_SUR_LOW_END):
-      "code=" + code + " min=" + t.min + " max=" + t.max;
-    return code;
-  }
-
-  public static class RandomAcceptedStrings {
-
-    private final Map<Transition,Boolean> leadsToAccept;
-    private final Automaton a;
-
-    private static class ArrivingTransition {
-      final State from;
-      final Transition t;
-      public ArrivingTransition(State from, Transition t) {
-        this.from = from;
-        this.t = t;
-      }
-    }
-
-    public RandomAcceptedStrings(Automaton a) {
-      this.a = a;
-      if (a.isSingleton()) {
-        leadsToAccept = null;
-        return;
-      }
-
-      // must use IdentityHashmap because two Transitions w/
-      // different start nodes can be considered the same
-      leadsToAccept = new IdentityHashMap<Transition,Boolean>();
-      final Map<State,List<ArrivingTransition>> allArriving = new HashMap<State,List<ArrivingTransition>>();
-
-      final LinkedList<State> q = new LinkedList<State>();
-      final Set<State> seen = new HashSet<State>();
-
-      // reverse map the transitions, so we can quickly look
-      // up all arriving transitions to a given state
-      for(State s: a.getNumberedStates()) {
-        for(int i=0;i<s.numTransitions;i++) {
-          final Transition t = s.transitionsArray[i];
-          List<ArrivingTransition> tl = allArriving.get(t.to);
-          if (tl == null) {
-            tl = new ArrayList<ArrivingTransition>();
-            allArriving.put(t.to, tl);
-          }
-          tl.add(new ArrivingTransition(s, t));
-        }
-        if (s.accept) {
-          q.add(s);
-          seen.add(s);
-        }
-      }
-
-      // Breadth-first search, from accept states,
-      // backwards:
-      while(!q.isEmpty()) {
-        final State s = q.removeFirst();
-        List<ArrivingTransition> arriving = allArriving.get(s);
-        if (arriving != null) {
-          for(ArrivingTransition at : arriving) {
-            final State from = at.from;
-            if (!seen.contains(from)) {
-              q.add(from);
-              seen.add(from);
-              leadsToAccept.put(at.t, Boolean.TRUE);
-            }
-          }
-        }
-      }
-    }
-
-    public int[] getRandomAcceptedString(Random r) {
-
-      final List<Integer> soFar = new ArrayList<Integer>();
-      if (a.isSingleton()) {
-        // accepts only one
-        final String s = a.singleton;
-      
-        int charUpto = 0;
-        while(charUpto < s.length()) {
-          final int cp = s.codePointAt(charUpto);
-          charUpto += Character.charCount(cp);
-          soFar.add(cp);
-        }
-      } else {
-
-        State s = a.initial;
-
-        while(true) {
-      
-          if (s.accept) {
-            if (s.numTransitions == 0) {
-              // stop now
-              break;
-            } else {
-              if (r.nextBoolean()) {
-                break;
-              }
-            }
-          }
-
-          if (s.numTransitions == 0) {
-            throw new RuntimeException("this automaton has dead states");
-          }
-
-          boolean cheat = r.nextBoolean();
-
-          final Transition t;
-          if (cheat) {
-            // pick a transition that we know is the fastest
-            // path to an accept state
-            List<Transition> toAccept = new ArrayList<Transition>();
-            for(int i=0;i<s.numTransitions;i++) {
-              final Transition t0 = s.transitionsArray[i];
-              if (leadsToAccept.containsKey(t0)) {
-                toAccept.add(t0);
-              }
-            }
-            if (toAccept.size() == 0) {
-              // this is OK -- it means we jumped into a cycle
-              t = s.transitionsArray[r.nextInt(s.numTransitions)];
-            } else {
-              t = toAccept.get(r.nextInt(toAccept.size()));
-            }
-          } else {
-            t = s.transitionsArray[r.nextInt(s.numTransitions)];
-          }
-          soFar.add(getRandomCodePoint(r, t));
-          s = t.to;
-        }
-      }
-
-      return ArrayUtil.toIntArray(soFar);
-    }
-  }
-  
-  /** return a random NFA/DFA for testing */
-  public static Automaton randomAutomaton(Random random) {
-    // get two random Automata from regexps
-    Automaton a1 = new RegExp(AutomatonTestUtil.randomRegexp(random), RegExp.NONE).toAutomaton();
-    if (random.nextBoolean())
-      a1 = BasicOperations.complement(a1);
-    
-    Automaton a2 = new RegExp(AutomatonTestUtil.randomRegexp(random), RegExp.NONE).toAutomaton();
-    if (random.nextBoolean()) 
-      a2 = BasicOperations.complement(a2);
-    
-    // combine them in random ways
-    switch(random.nextInt(4)) {
-      case 0: return BasicOperations.concatenate(a1, a2);
-      case 1: return BasicOperations.union(a1, a2);
-      case 2: return BasicOperations.intersection(a1, a2);
-      default: return BasicOperations.minus(a1, a2);
-    }
-  }
-  
-  /** 
-   * below are original, unoptimized implementations of DFA operations for testing.
-   * These are from brics automaton, full license (BSD) below:
-   */
-  
-  /*
-   * dk.brics.automaton
-   * 
-   * Copyright (c) 2001-2009 Anders Moeller
-   * All rights reserved.
-   * 
-   * Redistribution and use in source and binary forms, with or without
-   * modification, are permitted provided that the following conditions
-   * are met:
-   * 1. Redistributions of source code must retain the above copyright
-   *    notice, this list of conditions and the following disclaimer.
-   * 2. Redistributions in binary form must reproduce the above copyright
-   *    notice, this list of conditions and the following disclaimer in the
-   *    documentation and/or other materials provided with the distribution.
-   * 3. The name of the author may not be used to endorse or promote products
-   *    derived from this software without specific prior written permission.
-   * 
-   * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
-   * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-   * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
-   * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
-   * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
-   * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
-   * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
-   * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
-   * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
-   * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-   */
-
-  /**
-   * Simple, original brics implementation of Brzozowski minimize()
-   */
-  public static void minimizeSimple(Automaton a) {
-    if (a.isSingleton())
-      return;
-    determinizeSimple(a, SpecialOperations.reverse(a));
-    determinizeSimple(a, SpecialOperations.reverse(a));
-  }
-  
-  /**
-   * Simple, original brics implementation of determinize()
-   */
-  public static void determinizeSimple(Automaton a) {
-    if (a.deterministic || a.isSingleton())
-      return;
-    Set<State> initialset = new HashSet<State>();
-    initialset.add(a.initial);
-    determinizeSimple(a, initialset);
-  }
-  
-  /** 
-   * Simple, original brics implementation of determinize()
-   * Determinizes the given automaton using the given set of initial states. 
-   */
-  public static void determinizeSimple(Automaton a, Set<State> initialset) {
-    int[] points = a.getStartPoints();
-    // subset construction
-    Map<Set<State>, Set<State>> sets = new HashMap<Set<State>, Set<State>>();
-    LinkedList<Set<State>> worklist = new LinkedList<Set<State>>();
-    Map<Set<State>, State> newstate = new HashMap<Set<State>, State>();
-    sets.put(initialset, initialset);
-    worklist.add(initialset);
-    a.initial = new State();
-    newstate.put(initialset, a.initial);
-    while (worklist.size() > 0) {
-      Set<State> s = worklist.removeFirst();
-      State r = newstate.get(s);
-      for (State q : s)
-        if (q.accept) {
-          r.accept = true;
-          break;
-        }
-      for (int n = 0; n < points.length; n++) {
-        Set<State> p = new HashSet<State>();
-        for (State q : s)
-          for (Transition t : q.getTransitions())
-            if (t.min <= points[n] && points[n] <= t.max)
-              p.add(t.to);
-        if (!sets.containsKey(p)) {
-          sets.put(p, p);
-          worklist.add(p);
-          newstate.put(p, new State());
-        }
-        State q = newstate.get(p);
-        int min = points[n];
-        int max;
-        if (n + 1 < points.length)
-          max = points[n + 1] - 1;
-        else
-          max = Character.MAX_CODE_POINT;
-        r.addTransition(new Transition(min, max, q));
-      }
-    }
-    a.deterministic = true;
-    a.clearNumberedStates();
-    a.removeDeadTransitions();
-  }
-
-}
diff --git a/modules/analysis/common/build.xml b/modules/analysis/common/build.xml
index 2e67866..75e93b5 100644
--- a/modules/analysis/common/build.xml
+++ b/modules/analysis/common/build.xml
@@ -30,6 +30,7 @@
 	
   <path id="test.classpath">
     <path refid="classpath"/>
+    <pathelement location="../../../lucene/build/classes/test-framework"/>
     <pathelement location="../../../lucene/build/classes/test/"/>
     <path refid="junit-path"/>
     <pathelement location="${build.dir}/classes/java"/>
diff --git a/modules/analysis/icu/build.xml b/modules/analysis/icu/build.xml
index db7969a..2b443ce 100644
--- a/modules/analysis/icu/build.xml
+++ b/modules/analysis/icu/build.xml
@@ -49,6 +49,7 @@
   <path id="test.classpath">
   	<pathelement path="${analyzers-common.jar}"/>
     <path refid="classpath"/>
+    <pathelement location="../../../lucene/build/classes/test-framework/"/>
     <pathelement location="../../../lucene/build/classes/test/"/>
   	<pathelement location="../build/common/classes/test/"/>
     <path refid="junit-path"/>
diff --git a/modules/analysis/phonetic/build.xml b/modules/analysis/phonetic/build.xml
index 9efd18a..e8625d4 100644
--- a/modules/analysis/phonetic/build.xml
+++ b/modules/analysis/phonetic/build.xml
@@ -48,6 +48,7 @@
   <path id="test.classpath">
   	<pathelement path="${analyzers-common.jar}"/>
     <path refid="classpath"/>
+    <pathelement location="../../../lucene/build/classes/test-framework/"/>
     <pathelement location="../../../lucene/build/classes/test/"/>
   	<pathelement location="../build/common/classes/test/"/>
     <path refid="junit-path"/>
diff --git a/modules/analysis/smartcn/build.xml b/modules/analysis/smartcn/build.xml
index 841e680..075f8f4 100644
--- a/modules/analysis/smartcn/build.xml
+++ b/modules/analysis/smartcn/build.xml
@@ -39,6 +39,7 @@
   <path id="test.classpath">
   	<pathelement path="${analyzers-common.jar}"/>
     <path refid="classpath"/>
+    <pathelement location="../../../lucene/build/classes/test-framework"/>
     <pathelement location="../../../lucene/build/classes/test/"/>
     <path refid="junit-path"/>
     <pathelement location="${build.dir}/classes/java"/>
diff --git a/modules/analysis/stempel/build.xml b/modules/analysis/stempel/build.xml
index 90c5065..517591f 100644
--- a/modules/analysis/stempel/build.xml
+++ b/modules/analysis/stempel/build.xml
@@ -38,6 +38,7 @@
 	
   <path id="test.classpath">
     <path refid="classpath"/>
+    <pathelement location="../../../lucene/build/classes/test-framework"/>
     <pathelement location="../../../lucene/build/classes/test/"/>
     <path refid="junit-path"/>
     <pathelement location="${build.dir}/classes/java"/>
diff --git a/solr/build.xml b/solr/build.xml
index 06fcb9d..d802127 100644
--- a/solr/build.xml
+++ b/solr/build.xml
@@ -342,7 +342,7 @@
     <path refid="compile.classpath.solrj" />
     <pathelement location="${dest}/solr"/>
     <pathelement location="${dest}/solrj"/> <!-- include solrj -->
-    <pathelement location="${common-solr.dir}/../lucene/build/classes/test" />  <!-- include some lucene test code -->
+    <pathelement location="${common-solr.dir}/../lucene/build/classes/test-framework" />  <!-- include some lucene test code -->
   </path>
 
   <path id="test.run.classpath">
@@ -350,7 +350,7 @@
     <pathelement location="${dest}/tests"/>
     <!-- include the solrj classpath and jetty files included in example -->
     <path refid="compile.classpath.solrj" />
-    <pathelement location="${common-solr.dir}/../lucene/build/classes/test" />  <!-- include some lucene test code -->
+    <pathelement location="${common-solr.dir}/../lucene/build/classes/test-framework" />  <!-- include some lucene test code -->
     <pathelement path="${java.class.path}"/>
   </path>
 
diff --git a/solr/contrib/analysis-extras/build.xml b/solr/contrib/analysis-extras/build.xml
index 2babe1a..1b135e3 100644
--- a/solr/contrib/analysis-extras/build.xml
+++ b/solr/contrib/analysis-extras/build.xml
@@ -73,7 +73,7 @@
     <pathelement path="${dest}/test-classes"/>
     <pathelement path="${java.class.path}"/>
     <pathelement location="${common-solr.dir}/build/tests"/> <!-- include solr test code -->
-    <pathelement location="${common-solr.dir}/../lucene/build/classes/test" />  <!-- include some lucene test code -->
+    <pathelement location="${common-solr.dir}/../lucene/build/classes/test-framework" />  <!-- include some lucene test code -->
     <path refid="common.classpath"/>
   </path>
 
diff --git a/solr/contrib/clustering/build.xml b/solr/contrib/clustering/build.xml
index a803642..0621df6 100644
--- a/solr/contrib/clustering/build.xml
+++ b/solr/contrib/clustering/build.xml
@@ -42,7 +42,7 @@
     <pathelement path="${dest}/test-classes"/>
     <pathelement path="${java.class.path}"/>
     <pathelement location="${common-solr.dir}/build/tests"/> <!-- include solr test code -->
-    <pathelement location="${common-solr.dir}/../lucene/build/classes/test" />  <!-- include some lucene test code -->
+    <pathelement location="${common-solr.dir}/../lucene/build/classes/test-framework" />  <!-- include some lucene test code -->
     <path refid="common.classpath"/>
     <!-- DistributedClusteringComponentTest uses Jetty -->
     <fileset dir="${solr-path}/example/lib">
diff --git a/solr/contrib/dataimporthandler/build.xml b/solr/contrib/dataimporthandler/build.xml
index c21c0f9..7772fcb 100644
--- a/solr/contrib/dataimporthandler/build.xml
+++ b/solr/contrib/dataimporthandler/build.xml
@@ -56,7 +56,7 @@
 	  <pathelement path="target/classes" />
   	<pathelement path="target/test-classes" />
     <pathelement location="${solr-path}/build/tests"/> <!-- include solr test code -->
-    <pathelement location="${solr-path}/../lucene/build/classes/test" />  <!-- include some lucene test code -->
+    <pathelement location="${solr-path}/../lucene/build/classes/test-framework" />  <!-- include some lucene test code -->
     <pathelement path="${java.class.path}"/>
   </path>
 
@@ -68,7 +68,7 @@
   	<pathelement path="target/test-classes" />
   	<pathelement path="target/extras/test-classes" />
     <pathelement location="${solr-path}/build/tests"/> <!-- include solr test code -->
-    <pathelement location="${solr-path}/../lucene/build/classes/test" />  <!-- include some lucene test code -->
+    <pathelement location="${solr-path}/../lucene/build/classes/test-framework" />  <!-- include some lucene test code -->
     <pathelement path="${java.class.path}"/>
   </path>
 	
diff --git a/solr/contrib/extraction/build.xml b/solr/contrib/extraction/build.xml
index 73182c4..de7542d 100644
--- a/solr/contrib/extraction/build.xml
+++ b/solr/contrib/extraction/build.xml
@@ -40,7 +40,7 @@
     <pathelement path="${dest}/classes" />
     <pathelement path="${dest}/test-classes" />
     <pathelement location="${solr-path}/build/tests"/> <!-- include solr test code -->
-    <pathelement location="${solr-path}/../lucene/build/classes/test" />  <!-- include some lucene test code -->
+    <pathelement location="${solr-path}/../lucene/build/classes/test-framework" />  <!-- include some lucene test code -->
     <pathelement path="${java.class.path}"/>
   </path>
 
diff --git a/solr/contrib/uima/build.xml b/solr/contrib/uima/build.xml
index 34b190b..34dbefe 100644
--- a/solr/contrib/uima/build.xml
+++ b/solr/contrib/uima/build.xml
@@ -41,7 +41,7 @@
     <pathelement path="${dest}/classes" />
     <pathelement path="${dest}/test-classes" />
     <pathelement location="${solr-path}/build/tests"/> <!-- include solr test code -->
-    <pathelement location="${solr-path}/../lucene/build/classes/test" />  <!-- include some lucene test code -->
+    <pathelement location="${solr-path}/../lucene/build/classes/test-framework" />  <!-- include some lucene test code -->
     <pathelement path="${java.class.path}"/>
   </path>
 

