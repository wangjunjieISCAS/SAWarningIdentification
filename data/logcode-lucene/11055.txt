GitDiffStart: ded01621a42085fdec95b98099da073ae9b3b5b7 | Mon Sep 17 16:01:56 2012 +0000
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/br/BrazilianAnalyzer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/br/BrazilianAnalyzer.java
index d8c4256..816df1e 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/br/BrazilianAnalyzer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/br/BrazilianAnalyzer.java
@@ -50,7 +50,7 @@ public final class BrazilianAnalyzer extends StopwordAnalyzerBase {
   /** File containing default Brazilian Portuguese stopwords. */
   public final static String DEFAULT_STOPWORD_FILE = "stopwords.txt";
   
-	/**
+  /**
    * Returns an unmodifiable instance of the default stop-words set.
    * @return an unmodifiable instance of the default stop-words set.
    */
@@ -74,19 +74,19 @@ public final class BrazilianAnalyzer extends StopwordAnalyzerBase {
   }
 
 
-	/**
-	 * Contains words that should be indexed but not stemmed.
-	 */
-	private CharArraySet excltable = CharArraySet.EMPTY_SET;
-	
-	/**
-	 * Builds an analyzer with the default stop words ({@link #getDefaultStopSet()}).
-	 */
-	public BrazilianAnalyzer(Version matchVersion) {
+  /**
+   * Contains words that should be indexed but not stemmed.
+   */
+  private CharArraySet excltable = CharArraySet.EMPTY_SET;
+
+  /**
+   * Builds an analyzer with the default stop words ({@link #getDefaultStopSet()}).
+   */
+  public BrazilianAnalyzer(Version matchVersion) {
     this(matchVersion, DefaultSetHolder.DEFAULT_STOP_SET);
-	}
-	
-	/**
+  }
+
+  /**
    * Builds an analyzer with the given stop words
    * 
    * @param matchVersion
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/br/BrazilianStemmer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/br/BrazilianStemmer.java
index 24c5590..0038bdb 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/br/BrazilianStemmer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/br/BrazilianStemmer.java
@@ -25,37 +25,37 @@ import java.util.Locale;
 public class BrazilianStemmer {
   private static final Locale locale = new Locale("pt", "BR");
 
-	/**
-	 * Changed term
-	 */
-	private   String TERM ;
-	private   String CT ;
-	private   String R1 ;
-	private   String R2 ;
-	private   String RV ;
-
-
-	public BrazilianStemmer() {
-	}
-
-	/**
-	 * Stems the given term to an unique <tt>discriminator</tt>.
-	 *
-	 * @param term  The term that should be stemmed.
-	 * @return      Discriminator for <tt>term</tt>
-	 */
-	protected String stem( String term ) {
+  /**
+   * Changed term
+   */
+  private   String TERM ;
+  private   String CT ;
+  private   String R1 ;
+  private   String R2 ;
+  private   String RV ;
+
+
+  public BrazilianStemmer() {
+  }
+
+  /**
+   * Stems the given term to an unique <tt>discriminator</tt>.
+   *
+   * @param term  The term that should be stemmed.
+   * @return      Discriminator for <tt>term</tt>
+   */
+  protected String stem( String term ) {
     boolean altered = false ; // altered the term
 
     // creates CT
     createCT(term) ;
 
-		if ( !isIndexable( CT ) ) {
-			return null;
-		}
-		if ( !isStemmable( CT ) ) {
-			return CT ;
-		}
+    if ( !isIndexable( CT ) ) {
+      return null;
+    }
+    if ( !isStemmable( CT ) ) {
+      return CT ;
+    }
 
     R1 = getR1(CT) ;
     R2 = getR1(R1) ;
@@ -76,38 +76,38 @@ public class BrazilianStemmer {
     step5() ;
 
     return CT ;
-	}
-
-	/**
-	 * Checks a term if it can be processed correctly.
-	 *
-	 * @return  true if, and only if, the given term consists in letters.
-	 */
-	private boolean isStemmable( String term ) {
-		for ( int c = 0; c < term.length(); c++ ) {
-			// Discard terms that contain non-letter characters.
-			if ( !Character.isLetter(term.charAt(c))) {
-				return false;
-			}
-		}
-		return true;
-	}
-
-	/**
-	 * Checks a term if it can be processed indexed.
-	 *
-	 * @return  true if it can be indexed
-	 */
-	private boolean isIndexable( String term ) {
-		return (term.length() < 30) && (term.length() > 2) ;
-	}
-
-	/**
-	 * See if string is 'a','e','i','o','u'
+  }
+
+  /**
+   * Checks a term if it can be processed correctly.
+   *
+   * @return  true if, and only if, the given term consists in letters.
+   */
+  private boolean isStemmable( String term ) {
+    for ( int c = 0; c < term.length(); c++ ) {
+      // Discard terms that contain non-letter characters.
+      if ( !Character.isLetter(term.charAt(c))) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  /**
+   * Checks a term if it can be processed indexed.
+   *
+   * @return  true if it can be indexed
+   */
+  private boolean isIndexable( String term ) {
+    return (term.length() < 30) && (term.length() > 2) ;
+  }
+
+  /**
+   * See if string is 'a','e','i','o','u'
    *
    * @return true if is vowel
-	 */
-	private boolean isVowel( char value ) {
+   */
+  private boolean isVowel( char value ) {
     return (value == 'a') ||
            (value == 'e') ||
            (value == 'i') ||
@@ -115,16 +115,16 @@ public class BrazilianStemmer {
            (value == 'u') ;
   }
 
-	/**
-	 * Gets R1
+  /**
+   * Gets R1
    *
    * R1 - is the region after the first non-vowel following a vowel,
    *      or is the null region at the end of the word if there is
    *      no such non-vowel.
    *
    * @return null or a string representing R1
-	 */
-	private String getR1( String value ) {
+   */
+  private String getR1( String value ) {
     int     i;
     int     j;
 
@@ -159,8 +159,8 @@ public class BrazilianStemmer {
     return value.substring(j+1) ;
   }
 
-	/**
-	 * Gets RV
+  /**
+   * Gets RV
    *
    * RV - IF the second letter is a consonant, RV is the region after
    *      the next following vowel,
@@ -175,8 +175,8 @@ public class BrazilianStemmer {
    *      found.
    *
    * @return null or a string representing RV
-	 */
-	private String getRV( String value ) {
+   */
+  private String getRV( String value ) {
     int     i;
     int     j;
 
@@ -229,15 +229,15 @@ public class BrazilianStemmer {
     return null ;
   }
 
-	/**
+  /**
    * 1) Turn to lowercase
    * 2) Remove accents
    * 3) ã -> a ; õ -> o
    * 4) ç -> c
    *
    * @return null or a string transformed
-	 */
-	private String changeTerm( String value ) {
+   */
+  private String changeTerm( String value ) {
     int     j;
     String  r = "" ;
 
@@ -282,12 +282,12 @@ public class BrazilianStemmer {
     return r ;
   }
 
-	/**
+  /**
    * Check if a string ends with a suffix
    *
    * @return true if the string ends with the specified suffix
-	 */
-	private boolean suffix( String value, String suffix ) {
+   */
+  private boolean suffix( String value, String suffix ) {
 
     // be-safe !!!
     if ((value == null) || (suffix == null)) {
@@ -301,12 +301,12 @@ public class BrazilianStemmer {
     return value.substring(value.length()-suffix.length()).equals(suffix);
   }
 
-	/**
+  /**
    * Replace a string suffix by another
    *
    * @return the replaced String
-	 */
-	private String replaceSuffix( String value, String toReplace, String changeTo ) {
+   */
+  private String replaceSuffix( String value, String toReplace, String changeTo ) {
     String vvalue ;
 
     // be-safe !!!
@@ -325,12 +325,12 @@ public class BrazilianStemmer {
     }
   }
 
-	/**
+  /**
    * Remove a string suffix
    *
    * @return the String without the suffix
-	 */
-	private String removeSuffix( String value, String toRemove ) {
+   */
+  private String removeSuffix( String value, String toRemove ) {
     // be-safe !!!
     if ((value == null) ||
         (toRemove == null) ||
@@ -341,12 +341,12 @@ public class BrazilianStemmer {
     return value.substring(0,value.length()-toRemove.length()) ;
   }
 
-	/**
+  /**
    * See if a suffix is preceded by a String
    *
    * @return true if the suffix is preceded
-	 */
-	private boolean suffixPreceded( String value, String suffix, String preceded ) {
+   */
+  private boolean suffixPreceded( String value, String suffix, String preceded ) {
     // be-safe !!!
     if ((value == null) ||
         (suffix == null) ||
@@ -358,10 +358,10 @@ public class BrazilianStemmer {
     return suffix(removeSuffix(value,suffix),preceded) ;
   }
 
-	/**
-	 * Creates CT (changed term) , substituting * 'ã' and 'õ' for 'a~' and 'o~'.
-	 */
-	private void createCT( String term ) {
+  /**
+   * Creates CT (changed term) , substituting * 'ã' and 'õ' for 'a~' and 'o~'.
+   */
+  private void createCT( String term ) {
     CT = changeTerm(term) ;
 
     if (CT.length() < 2) return ;
@@ -396,14 +396,14 @@ public class BrazilianStemmer {
   }
 
 
-	/**
-	 * Standard suffix removal.
+  /**
+   * Standard suffix removal.
    * Search for the longest among the following suffixes, and perform
    * the following actions:
    *
    * @return false if no ending was removed
-	 */
-	private boolean step1() {
+   */
+  private boolean step1() {
     if (CT == null) return false ;
 
     // suffix length = 7
@@ -559,15 +559,15 @@ public class BrazilianStemmer {
   }
 
 
-	/**
-	 * Verb suffixes.
+  /**
+   * Verb suffixes.
    *
    * Search for the longest among the following suffixes in RV,
    * and if found, delete.
    *
    * @return false if no ending was removed
-	*/
-	private boolean step2() {
+  */
+  private boolean step2() {
     if (RV == null) return false ;
 
     // suffix lenght = 7
@@ -941,11 +941,11 @@ public class BrazilianStemmer {
     return false ;
   }
 
-	/**
-	 * Delete suffix 'i' if in RV and preceded by 'c'
+  /**
+   * Delete suffix 'i' if in RV and preceded by 'c'
    *
-	*/
-	private void step3() {
+  */
+  private void step3() {
     if (RV == null) return ;
 
     if (suffix(RV,"i") && suffixPreceded(RV,"i","c")) {
@@ -954,14 +954,14 @@ public class BrazilianStemmer {
 
   }
 
-	/**
-	 * Residual suffix
+  /**
+   * Residual suffix
    *
    * If the word ends with one of the suffixes (os a i o á í ó)
    * in RV, delete it
    *
-	*/
-	private void step4() {
+  */
+  private void step4() {
     if (RV == null) return  ;
 
     if (suffix(RV,"os")) {
@@ -979,15 +979,15 @@ public class BrazilianStemmer {
 
   }
 
-	/**
-	 * If the word ends with one of ( e é ê) in RV,delete it,
+  /**
+   * If the word ends with one of ( e é ê) in RV,delete it,
    * and if preceded by 'gu' (or 'ci') with the 'u' (or 'i') in RV,
    * delete the 'u' (or 'i')
    *
    * Or if the word ends ç remove the cedilha
    *
-	*/
-	private void step5() {
+  */
+  private void step5() {
     if (RV == null) return  ;
 
     if (suffix(RV,"e")) {
@@ -1007,18 +1007,18 @@ public class BrazilianStemmer {
     }
   }
 
-	/**
-	 * For log and debug purpose
-	 *
-	 * @return  TERM, CT, RV, R1 and R2
-	 */
-	public String log() {
+  /**
+   * For log and debug purpose
+   *
+   * @return  TERM, CT, RV, R1 and R2
+   */
+  public String log() {
     return " (TERM = " + TERM + ")" +
            " (CT = " + CT +")" +
            " (RV = " + RV +")" +
            " (R1 = " + R1 +")" +
            " (R2 = " + R2 +")" ;
-	}
+  }
 
 }
 
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.java
index 7b74620..3b120d0 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.java
@@ -1,4 +1,4 @@
-/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 8/6/12 11:57 AM */
+/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 9/17/12 9:29 AM */
 
 package org.apache.lucene.analysis.charfilter;
 
@@ -40,8 +40,8 @@ import org.apache.lucene.analysis.util.OpenStringBuilder;
 /**
  * This class is a scanner generated by 
  * <a href="http://www.jflex.de/">JFlex</a> 1.5.0-SNAPSHOT
- * on 8/6/12 11:57 AM from the specification file
- * <tt>/home/rmuir/workspace/lucene-trunk/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.jflex</tt>
+ * on 9/17/12 9:29 AM from the specification file
+ * <tt>/Users/Erick/apache/trunk_4326/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.jflex</tt>
  */
 public final class HTMLStripCharFilter extends BaseCharFilter {
 
@@ -52,29 +52,29 @@ public final class HTMLStripCharFilter extends BaseCharFilter {
   private static final int ZZ_BUFFERSIZE = 16384;
 
   /** lexical states */
-  private static final int DOUBLE_QUOTED_STRING = 28;
+  private static final int YYINITIAL = 0;
+  private static final int AMPERSAND = 2;
+  private static final int NUMERIC_CHARACTER = 4;
   private static final int CHARACTER_REFERENCE_TAIL = 6;
-  private static final int START_TAG_TAIL_EXCLUDE = 38;
+  private static final int LEFT_ANGLE_BRACKET = 8;
+  private static final int BANG = 10;
+  private static final int COMMENT = 12;
   private static final int SCRIPT = 14;
+  private static final int SCRIPT_COMMENT = 16;
+  private static final int LEFT_ANGLE_BRACKET_SLASH = 18;
+  private static final int LEFT_ANGLE_BRACKET_SPACE = 20;
   private static final int CDATA = 22;
-  private static final int LEFT_ANGLE_BRACKET = 8;
-  private static final int END_TAG_TAIL_EXCLUDE = 32;
   private static final int SERVER_SIDE_INCLUDE = 24;
-  private static final int END_TAG_TAIL_SUBSTITUTE = 34;
   private static final int SINGLE_QUOTED_STRING = 26;
-  private static final int YYINITIAL = 0;
-  private static final int STYLE = 42;
+  private static final int DOUBLE_QUOTED_STRING = 28;
+  private static final int END_TAG_TAIL_INCLUDE = 30;
+  private static final int END_TAG_TAIL_EXCLUDE = 32;
+  private static final int END_TAG_TAIL_SUBSTITUTE = 34;
   private static final int START_TAG_TAIL_INCLUDE = 36;
-  private static final int AMPERSAND = 2;
-  private static final int BANG = 10;
-  private static final int LEFT_ANGLE_BRACKET_SLASH = 18;
+  private static final int START_TAG_TAIL_EXCLUDE = 38;
   private static final int START_TAG_TAIL_SUBSTITUTE = 40;
-  private static final int COMMENT = 12;
-  private static final int SCRIPT_COMMENT = 16;
-  private static final int LEFT_ANGLE_BRACKET_SPACE = 20;
+  private static final int STYLE = 42;
   private static final int STYLE_COMMENT = 44;
-  private static final int NUMERIC_CHARACTER = 4;
-  private static final int END_TAG_TAIL_INCLUDE = 30;
 
   /**
    * ZZ_LEXSTATE[l] is the state in the DFA for the lexical state l
@@ -30967,7 +30967,7 @@ public final class HTMLStripCharFilter extends BaseCharFilter {
       }     
     }
 
-	// numRead < 0
+    // numRead < 0
     return true;
   }
 
@@ -31247,57 +31247,79 @@ public final class HTMLStripCharFilter extends BaseCharFilter {
       zzMarkedPos = zzMarkedPosL;
 
       switch (zzAction < 0 ? zzAction : ZZ_ACTION[zzAction]) {
-        case 15: 
-          { 
+        case 1: 
+          { return zzBuffer[zzStartRead];
           }
         case 54: break;
-        case 39: 
-          { yybegin(STYLE);
+        case 2: 
+          { inputStart = yychar;
+  inputSegment.clear();
+  inputSegment.append('<');
+  yybegin(LEFT_ANGLE_BRACKET);
           }
         case 55: break;
-        case 27: 
-          { // add (previously matched input length) + (this match length) - (substitution length)
-    cumulativeDiff += inputSegment.length() + yylength() - 1;
-    // position the correction at (already output length) + (substitution length)
-    addOffCorrectMap(outputCharCount + 1, cumulativeDiff);
-    inputSegment.clear();
-    yybegin(YYINITIAL);
-    return BLOCK_LEVEL_START_TAG_REPLACEMENT;
+        case 3: 
+          { inputStart = yychar;
+  inputSegment.clear();
+  inputSegment.append('&');
+  yybegin(AMPERSAND);
           }
         case 56: break;
-        case 30: 
-          { int length = yylength();
-    inputSegment.write(zzBuffer, zzStartRead, length);
-    entitySegment.clear();
-    char ch = entityValues.get(zzBuffer, zzStartRead, length).charValue();
-    entitySegment.append(ch);
-    outputSegment = entitySegment;
-    yybegin(CHARACTER_REFERENCE_TAIL);
+        case 4: 
+          { yypushback(1);
+    outputSegment = inputSegment;
+    outputSegment.restart();
+    yybegin(YYINITIAL);
+    return outputSegment.nextChar();
           }
         case 57: break;
-        case 48: 
-          { inputSegment.clear();
-    yybegin(YYINITIAL);
-    // add (previously matched input length) -- current match and substitution handled below
-    cumulativeDiff += yychar - inputStart;
-    // position the offset correction at (already output length) -- substitution handled below
-    int offsetCorrectionPos = outputCharCount;
-    int returnValue;
-    if (escapeSTYLE) {
-      inputSegment.write(zzBuffer, zzStartRead, yylength());
-      outputSegment = inputSegment;
-      returnValue = outputSegment.nextChar();
+        case 5: 
+          { inputSegment.append('#'); yybegin(NUMERIC_CHARACTER);
+          }
+        case 58: break;
+        case 6: 
+          { int matchLength = yylength();
+    inputSegment.write(zzBuffer, zzStartRead, matchLength);
+    if (matchLength <= 7) { // 0x10FFFF = 1114111: max 7 decimal chars
+      String decimalCharRef = yytext();
+      int codePoint = 0;
+      try {
+        codePoint = Integer.parseInt(decimalCharRef);
+      } catch(Exception e) {
+        assert false: "Exception parsing code point '" + decimalCharRef + "'";
+      }
+      if (codePoint <= 0x10FFFF) {
+        outputSegment = entitySegment;
+        outputSegment.clear();
+        if (codePoint >= Character.MIN_SURROGATE
+            && codePoint <= Character.MAX_SURROGATE) {
+          outputSegment.unsafeWrite(REPLACEMENT_CHARACTER);
+        } else {
+          outputSegment.setLength
+              (Character.toChars(codePoint, outputSegment.getArray(), 0));
+        }
+        yybegin(CHARACTER_REFERENCE_TAIL);
+      } else {
+        outputSegment = inputSegment;
+        yybegin(YYINITIAL);
+        return outputSegment.nextChar();
+      }
     } else {
-      // add (this match length) - (substitution length)
-      cumulativeDiff += yylength() - 1;
-      // add (substitution length)
-      ++offsetCorrectionPos;
-      returnValue = STYLE_REPLACEMENT;
+      outputSegment = inputSegment;
+      yybegin(YYINITIAL);
+      return outputSegment.nextChar();
     }
-    addOffCorrectMap(offsetCorrectionPos, cumulativeDiff);
-    return returnValue;
           }
-        case 58: break;
+        case 59: break;
+        case 7: 
+          { // add (previously matched input length) + (this match length) - (substitution length)
+    cumulativeDiff += inputSegment.length() + yylength() - outputSegment.length();
+    // position the correction at (already output length) + (substitution length)
+    addOffCorrectMap(outputCharCount + outputSegment.length(), cumulativeDiff);
+    yybegin(YYINITIAL);
+    return outputSegment.nextChar();
+          }
+        case 60: break;
         case 8: 
           { inputSegment.write(zzBuffer, zzStartRead, yylength());
     if (null != escapedTags
@@ -31307,252 +31329,161 @@ public final class HTMLStripCharFilter extends BaseCharFilter {
       yybegin(START_TAG_TAIL_SUBSTITUTE);
     }
           }
-        case 59: break;
-        case 2: 
-          { inputStart = yychar;
-  inputSegment.clear();
-  inputSegment.append('<');
-  yybegin(LEFT_ANGLE_BRACKET);
-          }
-        case 60: break;
-        case 44: 
-          { restoreState = STYLE_COMMENT; yybegin(SERVER_SIDE_INCLUDE);
-          }
         case 61: break;
-        case 21: 
-          { previousRestoreState = restoreState;
-    restoreState = SERVER_SIDE_INCLUDE;
-    yybegin(SINGLE_QUOTED_STRING);
+        case 9: 
+          { inputSegment.write(zzBuffer, zzStartRead, yylength());
+    if (null != escapedTags
+        && escapedTags.contains(zzBuffer, zzStartRead, yylength())) {
+      yybegin(START_TAG_TAIL_INCLUDE);
+    } else {
+      yybegin(START_TAG_TAIL_EXCLUDE);
+    }
           }
         case 62: break;
+        case 10: 
+          { inputSegment.append('!'); yybegin(BANG);
+          }
+        case 63: break;
         case 11: 
           { inputSegment.write(zzBuffer, zzStartRead, yylength());
     yybegin(LEFT_ANGLE_BRACKET_SPACE);
           }
-        case 63: break;
-        case 35: 
-          { yybegin(SCRIPT);
-          }
         case 64: break;
-        case 42: 
-          { restoreState = COMMENT; yybegin(SERVER_SIDE_INCLUDE);
+        case 12: 
+          { inputSegment.append('/'); yybegin(LEFT_ANGLE_BRACKET_SLASH);
           }
         case 65: break;
-        case 10: 
-          { inputSegment.append('!'); yybegin(BANG);
+        case 13: 
+          { inputSegment.append(zzBuffer[zzStartRead]);
           }
         case 66: break;
-        case 51: 
-          { // Handle paired UTF-16 surrogates.
-    String surrogatePair = yytext();
-    char highSurrogate = '\u0000';
-    char lowSurrogate = '\u0000';
-    try {
-      highSurrogate = (char)Integer.parseInt(surrogatePair.substring(2, 6), 16);
-    } catch(Exception e) { // should never happen
-      assert false: "Exception parsing high surrogate '"
-                  + surrogatePair.substring(2, 6) + "'";
-    }
-    try { // Low surrogates are in decimal range [56320, 57343]
-      lowSurrogate = (char)Integer.parseInt(surrogatePair.substring(9, 14));
-    } catch(Exception e) { // should never happen
-      assert false: "Exception parsing low surrogate '"
-                  + surrogatePair.substring(9, 14) + "'";
-    }
-    if (Character.isLowSurrogate(lowSurrogate)) {
-      outputSegment = entitySegment;
-      outputSegment.clear();
-      outputSegment.unsafeWrite(lowSurrogate);
-      // add (previously matched input length) + (this match length) - (substitution length)
-      cumulativeDiff += inputSegment.length() + yylength() - 2;
-      // position the correction at (already output length) + (substitution length)
-      addOffCorrectMap(outputCharCount + 2, cumulativeDiff);
-      inputSegment.clear();
-      yybegin(YYINITIAL);
-      return highSurrogate;
-    }
-    yypushback(surrogatePair.length() - 1); // Consume only '#'
-    inputSegment.append('#');
-    yybegin(NUMERIC_CHARACTER);
+        case 14: 
+          { // add (previously matched input length) + (this match length) [ - (substitution length) = 0 ]
+    cumulativeDiff += inputSegment.length() + yylength();
+    // position the correction at (already output length) [ + (substitution length) = 0 ]
+    addOffCorrectMap(outputCharCount, cumulativeDiff);
+    inputSegment.clear();
+    yybegin(YYINITIAL);
           }
         case 67: break;
-        case 4: 
-          { yypushback(1);
-    outputSegment = inputSegment;
-    outputSegment.restart();
-    yybegin(YYINITIAL);
-    return outputSegment.nextChar();
+        case 15: 
+          { 
           }
         case 68: break;
-        case 43: 
-          { restoreState = SCRIPT_COMMENT; yybegin(SERVER_SIDE_INCLUDE);
+        case 16: 
+          { restoreState = SCRIPT_COMMENT; yybegin(SINGLE_QUOTED_STRING);
           }
         case 69: break;
-        case 52: 
-          { // Handle paired UTF-16 surrogates.
-    String surrogatePair = yytext();
-    char highSurrogate = '\u0000';
-    try { // High surrogates are in decimal range [55296, 56319]
-      highSurrogate = (char)Integer.parseInt(surrogatePair.substring(1, 6));
-    } catch(Exception e) { // should never happen
-      assert false: "Exception parsing high surrogate '"
-                  + surrogatePair.substring(1, 6) + "'";
-    }
-    if (Character.isHighSurrogate(highSurrogate)) {
-      outputSegment = entitySegment;
-      outputSegment.clear();
-      try {
-        outputSegment.unsafeWrite
-            ((char)Integer.parseInt(surrogatePair.substring(10, 14), 16));
-      } catch(Exception e) { // should never happen
-        assert false: "Exception parsing low surrogate '"
-                    + surrogatePair.substring(10, 14) + "'";
-      }
-      // add (previously matched input length) + (this match length) - (substitution length)
-      cumulativeDiff += inputSegment.length() + yylength() - 2;
-      // position the correction at (already output length) + (substitution length)
-      addOffCorrectMap(outputCharCount + 2, cumulativeDiff);
-      inputSegment.clear();
-      yybegin(YYINITIAL);
-      return highSurrogate;
-    }
-    yypushback(surrogatePair.length() - 1); // Consume only '#'
-    inputSegment.append('#');
-    yybegin(NUMERIC_CHARACTER);
+        case 17: 
+          { restoreState = SCRIPT_COMMENT; yybegin(DOUBLE_QUOTED_STRING);
           }
         case 70: break;
-        case 28: 
-          { restoreState = STYLE_COMMENT; yybegin(SINGLE_QUOTED_STRING);
+        case 18: 
+          { inputSegment.write(zzBuffer, zzStartRead, yylength());
+    if (null != escapedTags
+        && escapedTags.contains(zzBuffer, zzStartRead, yylength())) {
+      yybegin(END_TAG_TAIL_INCLUDE);
+    } else {
+      yybegin(END_TAG_TAIL_SUBSTITUTE);
+    }
           }
         case 71: break;
-        case 50: 
-          { // Handle paired UTF-16 surrogates.
-    outputSegment = entitySegment;
-    outputSegment.clear();
-    String surrogatePair = yytext();
-    char highSurrogate = '\u0000';
-    try {
-      highSurrogate = (char)Integer.parseInt(surrogatePair.substring(2, 6), 16);
-    } catch(Exception e) { // should never happen
-      assert false: "Exception parsing high surrogate '"
-                  + surrogatePair.substring(2, 6) + "'";
-    }
-    try {
-      outputSegment.unsafeWrite
-          ((char)Integer.parseInt(surrogatePair.substring(10, 14), 16));
-    } catch(Exception e) { // should never happen
-      assert false: "Exception parsing low surrogate '"
-                  + surrogatePair.substring(10, 14) + "'";
+        case 19: 
+          { inputSegment.write(zzBuffer, zzStartRead, yylength());
+    if (null != escapedTags
+        && escapedTags.contains(zzBuffer, zzStartRead, yylength())) {
+      yybegin(END_TAG_TAIL_INCLUDE);
+    } else {
+      yybegin(END_TAG_TAIL_EXCLUDE);
     }
-    // add (previously matched input length) + (this match length) - (substitution length)
-    cumulativeDiff += inputSegment.length() + yylength() - 2;
-    // position the correction at (already output length) + (substitution length)
-    addOffCorrectMap(outputCharCount + 2, cumulativeDiff);
-    inputSegment.clear();
-    yybegin(YYINITIAL);
-    return highSurrogate;
           }
         case 72: break;
-        case 16: 
-          { restoreState = SCRIPT_COMMENT; yybegin(SINGLE_QUOTED_STRING);
+        case 20: 
+          { inputSegment.write(zzBuffer, zzStartRead, yylength());
           }
         case 73: break;
-        case 22: 
+        case 21: 
           { previousRestoreState = restoreState;
     restoreState = SERVER_SIDE_INCLUDE;
-    yybegin(DOUBLE_QUOTED_STRING);
+    yybegin(SINGLE_QUOTED_STRING);
           }
         case 74: break;
-        case 26: 
-          { // add (previously matched input length) + (this match length) [ - (substitution length) = 0 ]
-    cumulativeDiff += inputSegment.length() + yylength();
-    // position the correction at (already output length) [ + (substitution length) = 0 ]
-    addOffCorrectMap(outputCharCount, cumulativeDiff);
-    inputSegment.clear();
-    outputSegment = inputSegment;
-    yybegin(YYINITIAL);
+        case 22: 
+          { previousRestoreState = restoreState;
+    restoreState = SERVER_SIDE_INCLUDE;
+    yybegin(DOUBLE_QUOTED_STRING);
           }
         case 75: break;
-        case 20: 
-          { inputSegment.write(zzBuffer, zzStartRead, yylength());
-          }
-        case 76: break;
-        case 47: 
-          { // add (previously matched input length) + (this match length) [ - (substitution length) = 0 ]
-    cumulativeDiff += inputSegment.length() + yylength();
-    // position the correction at (already output length) [ + (substitution length) = 0 ]
-    addOffCorrectMap(outputCharCount, cumulativeDiff);
-    inputSegment.clear();
-    yybegin(CDATA);
-          }
-        case 77: break;
-        case 33: 
-          { yybegin(YYINITIAL);
-    if (escapeBR) {
-      inputSegment.write(zzBuffer, zzStartRead, yylength());
-      outputSegment = inputSegment;
-      return outputSegment.nextChar();
-    } else {
-      // add (previously matched input length) + (this match length) - (substitution length)
-      cumulativeDiff += inputSegment.length() + yylength() - 1;
-      // position the correction at (already output length) + (substitution length)
-      addOffCorrectMap(outputCharCount + 1, cumulativeDiff);
-      inputSegment.reset();
-      return BR_START_TAG_REPLACEMENT;
-    }
-          }
-        case 78: break;
         case 23: 
           { yybegin(restoreState); restoreState = previousRestoreState;
           }
-        case 79: break;
-        case 32: 
-          { yybegin(COMMENT);
-          }
-        case 80: break;
+        case 76: break;
         case 24: 
           { inputSegment.write(zzBuffer, zzStartRead, yylength());
      outputSegment = inputSegment;
      yybegin(YYINITIAL);
      return outputSegment.nextChar();
           }
-        case 81: break;
-        case 3: 
-          { inputStart = yychar;
-  inputSegment.clear();
-  inputSegment.append('&');
-  yybegin(AMPERSAND);
-          }
-        case 82: break;
-        case 46: 
-          { yybegin(SCRIPT);
-    if (escapeSCRIPT) {
-      inputSegment.write(zzBuffer, zzStartRead, yylength());
-      outputSegment = inputSegment;
-      inputStart += 1 + yylength();
-      return outputSegment.nextChar();
-    }
+        case 77: break;
+        case 25: 
+          { // add (previously matched input length) + (this match length) - (substitution length)
+    cumulativeDiff += inputSegment.length() + yylength() - 1;
+    // position the correction at (already output length) + (substitution length)
+    addOffCorrectMap(outputCharCount + 1, cumulativeDiff);
+    inputSegment.clear();
+    yybegin(YYINITIAL);
+    return BLOCK_LEVEL_END_TAG_REPLACEMENT;
           }
-        case 83: break;
-        case 14: 
+        case 78: break;
+        case 26: 
           { // add (previously matched input length) + (this match length) [ - (substitution length) = 0 ]
     cumulativeDiff += inputSegment.length() + yylength();
     // position the correction at (already output length) [ + (substitution length) = 0 ]
     addOffCorrectMap(outputCharCount, cumulativeDiff);
     inputSegment.clear();
+    outputSegment = inputSegment;
     yybegin(YYINITIAL);
           }
-        case 84: break;
-        case 6: 
+        case 79: break;
+        case 27: 
+          { // add (previously matched input length) + (this match length) - (substitution length)
+    cumulativeDiff += inputSegment.length() + yylength() - 1;
+    // position the correction at (already output length) + (substitution length)
+    addOffCorrectMap(outputCharCount + 1, cumulativeDiff);
+    inputSegment.clear();
+    yybegin(YYINITIAL);
+    return BLOCK_LEVEL_START_TAG_REPLACEMENT;
+          }
+        case 80: break;
+        case 28: 
+          { restoreState = STYLE_COMMENT; yybegin(SINGLE_QUOTED_STRING);
+          }
+        case 81: break;
+        case 29: 
+          { restoreState = STYLE_COMMENT; yybegin(DOUBLE_QUOTED_STRING);
+          }
+        case 82: break;
+        case 30: 
+          { int length = yylength();
+    inputSegment.write(zzBuffer, zzStartRead, length);
+    entitySegment.clear();
+    char ch = entityValues.get(zzBuffer, zzStartRead, length).charValue();
+    entitySegment.append(ch);
+    outputSegment = entitySegment;
+    yybegin(CHARACTER_REFERENCE_TAIL);
+          }
+        case 83: break;
+        case 31: 
           { int matchLength = yylength();
     inputSegment.write(zzBuffer, zzStartRead, matchLength);
-    if (matchLength <= 7) { // 0x10FFFF = 1114111: max 7 decimal chars
-      String decimalCharRef = yytext();
+    if (matchLength <= 6) { // 10FFFF: max 6 hex chars
+      String hexCharRef
+          = new String(zzBuffer, zzStartRead + 1, matchLength - 1);
       int codePoint = 0;
       try {
-        codePoint = Integer.parseInt(decimalCharRef);
+        codePoint = Integer.parseInt(hexCharRef, 16);
       } catch(Exception e) {
-        assert false: "Exception parsing code point '" + decimalCharRef + "'";
+        assert false: "Exception parsing hex code point '" + hexCharRef + "'";
       }
       if (codePoint <= 0x10FFFF) {
         outputSegment = entitySegment;
@@ -31576,7 +31507,27 @@ public final class HTMLStripCharFilter extends BaseCharFilter {
       return outputSegment.nextChar();
     }
           }
+        case 84: break;
+        case 32: 
+          { yybegin(COMMENT);
+          }
         case 85: break;
+        case 33: 
+          { yybegin(YYINITIAL);
+    if (escapeBR) {
+      inputSegment.write(zzBuffer, zzStartRead, yylength());
+      outputSegment = inputSegment;
+      return outputSegment.nextChar();
+    } else {
+      // add (previously matched input length) + (this match length) - (substitution length)
+      cumulativeDiff += inputSegment.length() + yylength() - 1;
+      // position the correction at (already output length) + (substitution length)
+      addOffCorrectMap(outputCharCount + 1, cumulativeDiff);
+      inputSegment.reset();
+      return BR_START_TAG_REPLACEMENT;
+    }
+          }
+        case 86: break;
         case 34: 
           { // add (previously matched input length) + (this match length) [ - (substitution length) = 0]
     cumulativeDiff += yychar - inputStart + yylength();
@@ -31585,29 +31536,27 @@ public final class HTMLStripCharFilter extends BaseCharFilter {
     inputSegment.clear();
     yybegin(YYINITIAL);
           }
-        case 86: break;
-        case 5: 
-          { inputSegment.append('#'); yybegin(NUMERIC_CHARACTER);
-          }
         case 87: break;
-        case 13: 
-          { inputSegment.append(zzBuffer[zzStartRead]);
+        case 35: 
+          { yybegin(SCRIPT);
           }
         case 88: break;
-        case 18: 
-          { inputSegment.write(zzBuffer, zzStartRead, yylength());
-    if (null != escapedTags
-        && escapedTags.contains(zzBuffer, zzStartRead, yylength())) {
-      yybegin(END_TAG_TAIL_INCLUDE);
+        case 36: 
+          { yybegin(YYINITIAL);
+    if (escapeBR) {
+      inputSegment.write(zzBuffer, zzStartRead, yylength());
+      outputSegment = inputSegment;
+      return outputSegment.nextChar();
     } else {
-      yybegin(END_TAG_TAIL_SUBSTITUTE);
+      // add (previously matched input length) + (this match length) - (substitution length)
+      cumulativeDiff += inputSegment.length() + yylength() - 1;
+      // position the correction at (already output length) + (substitution length)
+      addOffCorrectMap(outputCharCount + 1, cumulativeDiff);
+      inputSegment.reset();
+      return BR_END_TAG_REPLACEMENT;
     }
           }
         case 89: break;
-        case 40: 
-          { yybegin(SCRIPT_COMMENT);
-          }
-        case 90: break;
         case 37: 
           { // add (this match length) [ - (substitution length) = 0 ]
     cumulativeDiff += yylength();
@@ -31615,21 +31564,87 @@ public final class HTMLStripCharFilter extends BaseCharFilter {
     addOffCorrectMap(outputCharCount, cumulativeDiff);
     yybegin(YYINITIAL);
           }
+        case 90: break;
+        case 38: 
+          { yybegin(restoreState);
+          }
         case 91: break;
-        case 12: 
-          { inputSegment.append('/'); yybegin(LEFT_ANGLE_BRACKET_SLASH);
+        case 39: 
+          { yybegin(STYLE);
           }
         case 92: break;
-        case 9: 
-          { inputSegment.write(zzBuffer, zzStartRead, yylength());
-    if (null != escapedTags
-        && escapedTags.contains(zzBuffer, zzStartRead, yylength())) {
-      yybegin(START_TAG_TAIL_INCLUDE);
+        case 40: 
+          { yybegin(SCRIPT_COMMENT);
+          }
+        case 93: break;
+        case 41: 
+          { yybegin(STYLE_COMMENT);
+          }
+        case 94: break;
+        case 42: 
+          { restoreState = COMMENT; yybegin(SERVER_SIDE_INCLUDE);
+          }
+        case 95: break;
+        case 43: 
+          { restoreState = SCRIPT_COMMENT; yybegin(SERVER_SIDE_INCLUDE);
+          }
+        case 96: break;
+        case 44: 
+          { restoreState = STYLE_COMMENT; yybegin(SERVER_SIDE_INCLUDE);
+          }
+        case 97: break;
+        case 45: 
+          { yybegin(STYLE);
+    if (escapeSTYLE) {
+      inputSegment.write(zzBuffer, zzStartRead, yylength());
+      outputSegment = inputSegment;
+      inputStart += 1 + yylength();
+      return outputSegment.nextChar();
+    }
+          }
+        case 98: break;
+        case 46: 
+          { yybegin(SCRIPT);
+    if (escapeSCRIPT) {
+      inputSegment.write(zzBuffer, zzStartRead, yylength());
+      outputSegment = inputSegment;
+      inputStart += 1 + yylength();
+      return outputSegment.nextChar();
+    }
+          }
+        case 99: break;
+        case 47: 
+          { // add (previously matched input length) + (this match length) [ - (substitution length) = 0 ]
+    cumulativeDiff += inputSegment.length() + yylength();
+    // position the correction at (already output length) [ + (substitution length) = 0 ]
+    addOffCorrectMap(outputCharCount, cumulativeDiff);
+    inputSegment.clear();
+    yybegin(CDATA);
+          }
+        case 100: break;
+        case 48: 
+          { inputSegment.clear();
+    yybegin(YYINITIAL);
+    // add (previously matched input length) -- current match and substitution handled below
+    cumulativeDiff += yychar - inputStart;
+    // position the offset correction at (already output length) -- substitution handled below
+    int offsetCorrectionPos = outputCharCount;
+    int returnValue;
+    if (escapeSTYLE) {
+      inputSegment.write(zzBuffer, zzStartRead, yylength());
+      outputSegment = inputSegment;
+      returnValue = outputSegment.nextChar();
     } else {
-      yybegin(START_TAG_TAIL_EXCLUDE);
+      // add (this match length) - (substitution length)
+      cumulativeDiff += yylength() - 1;
+      // add (substitution length)
+      ++offsetCorrectionPos;
+      returnValue = STYLE_REPLACEMENT;
     }
+    addOffCorrectMap(offsetCorrectionPos, cumulativeDiff);
+    return returnValue;
           }
-        case 93: break;
+        case 101: break;
         case 49: 
           { inputSegment.clear();
     yybegin(YYINITIAL);
@@ -31652,89 +31667,102 @@ public final class HTMLStripCharFilter extends BaseCharFilter {
     addOffCorrectMap(offsetCorrectionPos, cumulativeDiff);
     return returnValue;
           }
-        case 94: break;
-        case 29: 
-          { restoreState = STYLE_COMMENT; yybegin(DOUBLE_QUOTED_STRING);
-          }
-        case 95: break;
-        case 17: 
-          { restoreState = SCRIPT_COMMENT; yybegin(DOUBLE_QUOTED_STRING);
-          }
-        case 96: break;
-        case 45: 
-          { yybegin(STYLE);
-    if (escapeSTYLE) {
-      inputSegment.write(zzBuffer, zzStartRead, yylength());
-      outputSegment = inputSegment;
-      inputStart += 1 + yylength();
-      return outputSegment.nextChar();
+        case 102: break;
+        case 50: 
+          { // Handle paired UTF-16 surrogates.
+    outputSegment = entitySegment;
+    outputSegment.clear();
+    String surrogatePair = yytext();
+    char highSurrogate = '\u0000';
+    try {
+      highSurrogate = (char)Integer.parseInt(surrogatePair.substring(2, 6), 16);
+    } catch(Exception e) { // should never happen
+      assert false: "Exception parsing high surrogate '"
+                  + surrogatePair.substring(2, 6) + "'";
     }
-          }
-        case 97: break;
-        case 7: 
-          { // add (previously matched input length) + (this match length) - (substitution length)
-    cumulativeDiff += inputSegment.length() + yylength() - outputSegment.length();
-    // position the correction at (already output length) + (substitution length)
-    addOffCorrectMap(outputCharCount + outputSegment.length(), cumulativeDiff);
-    yybegin(YYINITIAL);
-    return outputSegment.nextChar();
-          }
-        case 98: break;
-        case 19: 
-          { inputSegment.write(zzBuffer, zzStartRead, yylength());
-    if (null != escapedTags
-        && escapedTags.contains(zzBuffer, zzStartRead, yylength())) {
-      yybegin(END_TAG_TAIL_INCLUDE);
-    } else {
-      yybegin(END_TAG_TAIL_EXCLUDE);
+    try {
+      outputSegment.unsafeWrite
+          ((char)Integer.parseInt(surrogatePair.substring(10, 14), 16));
+    } catch(Exception e) { // should never happen
+      assert false: "Exception parsing low surrogate '"
+                  + surrogatePair.substring(10, 14) + "'";
     }
-          }
-        case 99: break;
-        case 25: 
-          { // add (previously matched input length) + (this match length) - (substitution length)
-    cumulativeDiff += inputSegment.length() + yylength() - 1;
+    // add (previously matched input length) + (this match length) - (substitution length)
+    cumulativeDiff += inputSegment.length() + yylength() - 2;
     // position the correction at (already output length) + (substitution length)
-    addOffCorrectMap(outputCharCount + 1, cumulativeDiff);
+    addOffCorrectMap(outputCharCount + 2, cumulativeDiff);
     inputSegment.clear();
     yybegin(YYINITIAL);
-    return BLOCK_LEVEL_END_TAG_REPLACEMENT;
+    return highSurrogate;
           }
-        case 100: break;
-        case 31: 
-          { int matchLength = yylength();
-    inputSegment.write(zzBuffer, zzStartRead, matchLength);
-    if (matchLength <= 6) { // 10FFFF: max 6 hex chars
-      String hexCharRef
-          = new String(zzBuffer, zzStartRead + 1, matchLength - 1);
-      int codePoint = 0;
+        case 103: break;
+        case 51: 
+          { // Handle paired UTF-16 surrogates.
+    String surrogatePair = yytext();
+    char highSurrogate = '\u0000';
+    char lowSurrogate = '\u0000';
+    try {
+      highSurrogate = (char)Integer.parseInt(surrogatePair.substring(2, 6), 16);
+    } catch(Exception e) { // should never happen
+      assert false: "Exception parsing high surrogate '"
+                  + surrogatePair.substring(2, 6) + "'";
+    }
+    try { // Low surrogates are in decimal range [56320, 57343]
+      lowSurrogate = (char)Integer.parseInt(surrogatePair.substring(9, 14));
+    } catch(Exception e) { // should never happen
+      assert false: "Exception parsing low surrogate '"
+                  + surrogatePair.substring(9, 14) + "'";
+    }
+    if (Character.isLowSurrogate(lowSurrogate)) {
+      outputSegment = entitySegment;
+      outputSegment.clear();
+      outputSegment.unsafeWrite(lowSurrogate);
+      // add (previously matched input length) + (this match length) - (substitution length)
+      cumulativeDiff += inputSegment.length() + yylength() - 2;
+      // position the correction at (already output length) + (substitution length)
+      addOffCorrectMap(outputCharCount + 2, cumulativeDiff);
+      inputSegment.clear();
+      yybegin(YYINITIAL);
+      return highSurrogate;
+    }
+    yypushback(surrogatePair.length() - 1); // Consume only '#'
+    inputSegment.append('#');
+    yybegin(NUMERIC_CHARACTER);
+          }
+        case 104: break;
+        case 52: 
+          { // Handle paired UTF-16 surrogates.
+    String surrogatePair = yytext();
+    char highSurrogate = '\u0000';
+    try { // High surrogates are in decimal range [55296, 56319]
+      highSurrogate = (char)Integer.parseInt(surrogatePair.substring(1, 6));
+    } catch(Exception e) { // should never happen
+      assert false: "Exception parsing high surrogate '"
+                  + surrogatePair.substring(1, 6) + "'";
+    }
+    if (Character.isHighSurrogate(highSurrogate)) {
+      outputSegment = entitySegment;
+      outputSegment.clear();
       try {
-        codePoint = Integer.parseInt(hexCharRef, 16);
-      } catch(Exception e) {
-        assert false: "Exception parsing hex code point '" + hexCharRef + "'";
-      }
-      if (codePoint <= 0x10FFFF) {
-        outputSegment = entitySegment;
-        outputSegment.clear();
-        if (codePoint >= Character.MIN_SURROGATE
-            && codePoint <= Character.MAX_SURROGATE) {
-          outputSegment.unsafeWrite(REPLACEMENT_CHARACTER);
-        } else {
-          outputSegment.setLength
-              (Character.toChars(codePoint, outputSegment.getArray(), 0));
-        }
-        yybegin(CHARACTER_REFERENCE_TAIL);
-      } else {
-        outputSegment = inputSegment;
-        yybegin(YYINITIAL);
-        return outputSegment.nextChar();
+        outputSegment.unsafeWrite
+            ((char)Integer.parseInt(surrogatePair.substring(10, 14), 16));
+      } catch(Exception e) { // should never happen
+        assert false: "Exception parsing low surrogate '"
+                    + surrogatePair.substring(10, 14) + "'";
       }
-    } else {
-      outputSegment = inputSegment;
+      // add (previously matched input length) + (this match length) - (substitution length)
+      cumulativeDiff += inputSegment.length() + yylength() - 2;
+      // position the correction at (already output length) + (substitution length)
+      addOffCorrectMap(outputCharCount + 2, cumulativeDiff);
+      inputSegment.clear();
       yybegin(YYINITIAL);
-      return outputSegment.nextChar();
+      return highSurrogate;
     }
+    yypushback(surrogatePair.length() - 1); // Consume only '#'
+    inputSegment.append('#');
+    yybegin(NUMERIC_CHARACTER);
           }
-        case 101: break;
+        case 105: break;
         case 53: 
           { // Handle paired UTF-16 surrogates.
     String surrogatePair = yytext();
@@ -31770,34 +31798,6 @@ public final class HTMLStripCharFilter extends BaseCharFilter {
     inputSegment.append('#');
     yybegin(NUMERIC_CHARACTER);
           }
-        case 102: break;
-        case 36: 
-          { yybegin(YYINITIAL);
-    if (escapeBR) {
-      inputSegment.write(zzBuffer, zzStartRead, yylength());
-      outputSegment = inputSegment;
-      return outputSegment.nextChar();
-    } else {
-      // add (previously matched input length) + (this match length) - (substitution length)
-      cumulativeDiff += inputSegment.length() + yylength() - 1;
-      // position the correction at (already output length) + (substitution length)
-      addOffCorrectMap(outputCharCount + 1, cumulativeDiff);
-      inputSegment.reset();
-      return BR_END_TAG_REPLACEMENT;
-    }
-          }
-        case 103: break;
-        case 38: 
-          { yybegin(restoreState);
-          }
-        case 104: break;
-        case 41: 
-          { yybegin(STYLE_COMMENT);
-          }
-        case 105: break;
-        case 1: 
-          { return zzBuffer[zzStartRead];
-          }
         case 106: break;
         default: 
           if (zzInput == YYEOF && zzStartRead == zzCurrentPos) {
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.jflex b/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.jflex
index fc66755..5d36c91 100755
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.jflex
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.jflex
@@ -90,18 +90,18 @@ EventAttributeSuffixes = ( [aA][bB][oO][rR][tT]                 |
                            [bB][lL][uU][rR]                     |
                            [cC][hH][aA][nN][gG][eE]             |
                            [cC][lL][iI][cC][kK]                 |
-	                         [dD][bB][lL][cC][lL][iI][cC][kK]     |
+                           [dD][bB][lL][cC][lL][iI][cC][kK]     |
                            [eE][rR][rR][oO][rR]                 |
                            [fF][oO][cC][uU][sS]                 |
-	                         [kK][eE][yY][dD][oO][wW][nN]         |
-	                         [kK][eE][yY][pP][rR][eE][sS][sS]     |
-	                         [kK][eE][yY][uU][pP]                 |
+                           [kK][eE][yY][dD][oO][wW][nN]         |
+                           [kK][eE][yY][pP][rR][eE][sS][sS]     |
+                           [kK][eE][yY][uU][pP]                 |
                            [lL][oO][aA][dD]                     |
-	                         [mM][oO][uU][sS][eE][dD][oO][wW][nN] |
-	                         [mM][oO][uU][sS][eE][mM][oO][vV][eE] |
+                           [mM][oO][uU][sS][eE][dD][oO][wW][nN] |
+                           [mM][oO][uU][sS][eE][mM][oO][vV][eE] |
                            [mM][oO][uU][sS][eE][oO][uU][tT]     |
                            [mM][oO][uU][sS][eE][oO][vV][eE][rR] |
-	                         [mM][oO][uU][sS][eE][uU][pP]         |
+                           [mM][oO][uU][sS][eE][uU][pP]         |
                            [rR][eE][sS][eE][tT]                 |
                            [sS][eE][lL][eE][cC][tT]             |
                            [sS][uU][bB][mM][iI][tT]             |
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/DictionaryCompoundWordTokenFilterFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/DictionaryCompoundWordTokenFilterFactory.java
index 214bf61..d5b30ae 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/DictionaryCompoundWordTokenFilterFactory.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/DictionaryCompoundWordTokenFilterFactory.java
@@ -30,7 +30,7 @@ import java.io.IOException;
  *   &lt;analyzer&gt;
  *     &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
  *     &lt;filter class="solr.DictionaryCompoundWordTokenFilterFactory" dictionary="dictionary.txt"
- *     	     minWordSize="5" minSubwordSize="2" maxSubwordSize="15" onlyLongestMatch="true"/&gt;
+ *         minWordSize="5" minSubwordSize="2" maxSubwordSize="15" onlyLongestMatch="true"/&gt;
  *   &lt;/analyzer&gt;
  * &lt;/fieldType&gt;</pre>
  *
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/HyphenationCompoundWordTokenFilterFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/HyphenationCompoundWordTokenFilterFactory.java
index 9b6585a..9a932c7 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/HyphenationCompoundWordTokenFilterFactory.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/HyphenationCompoundWordTokenFilterFactory.java
@@ -50,7 +50,7 @@ import org.xml.sax.InputSource;
  *   &lt;analyzer&gt;
  *     &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
  *     &lt;filter class="solr.HyphenationCompoundWordTokenFilterFactory" hyphenator="hyphenator.xml" encoding="UTF-8"
- *     	     dictionary="dictionary.txt" minWordSize="5" minSubwordSize="2" maxSubwordSize="15" onlyLongestMatch="false"/&gt;
+ *         dictionary="dictionary.txt" minWordSize="5" minSubwordSize="2" maxSubwordSize="15" onlyLongestMatch="false"/&gt;
  *   &lt;/analyzer&gt;
  * &lt;/fieldType&gt;</pre>
  *
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/cz/CzechAnalyzer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/cz/CzechAnalyzer.java
index b630726..9decbf3 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/cz/CzechAnalyzer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/cz/CzechAnalyzer.java
@@ -50,24 +50,24 @@ public final class CzechAnalyzer extends StopwordAnalyzerBase {
    * 
    * @return a set of default Czech-stopwords
    */
-	public static final CharArraySet getDefaultStopSet(){
-	  return DefaultSetHolder.DEFAULT_SET;
-	}
-	
-	private static class DefaultSetHolder {
-	  private static final CharArraySet DEFAULT_SET;
-	  
-	  static {
-	    try {
-	      DEFAULT_SET = WordlistLoader.getWordSet(IOUtils.getDecodingReader(CzechAnalyzer.class, 
-	          DEFAULT_STOPWORD_FILE, IOUtils.CHARSET_UTF_8), "#", Version.LUCENE_CURRENT);
-	    } catch (IOException ex) {
-	      // default set should always be present as it is part of the
-	      // distribution (JAR)
-	      throw new RuntimeException("Unable to load default stopword set");
-	    }
-	  }
-	}
+  public static final CharArraySet getDefaultStopSet(){
+    return DefaultSetHolder.DEFAULT_SET;
+  }
+
+  private static class DefaultSetHolder {
+    private static final CharArraySet DEFAULT_SET;
+  
+    static {
+      try {
+        DEFAULT_SET = WordlistLoader.getWordSet(IOUtils.getDecodingReader(CzechAnalyzer.class, 
+            DEFAULT_STOPWORD_FILE, IOUtils.CHARSET_UTF_8), "#", Version.LUCENE_CURRENT);
+      } catch (IOException ex) {
+        // default set should always be present as it is part of the
+        // distribution (JAR)
+        throw new RuntimeException("Unable to load default stopword set");
+      }
+    }
+  }
 
  
   private final CharArraySet stemExclusionTable;
@@ -77,9 +77,9 @@ public final class CzechAnalyzer extends StopwordAnalyzerBase {
    *
    * @param matchVersion Lucene version to match
    */
-	public CzechAnalyzer(Version matchVersion) {
+  public CzechAnalyzer(Version matchVersion) {
     this(matchVersion, DefaultSetHolder.DEFAULT_SET);
-	}
+  }
 
   /**
    * Builds an analyzer with the given stop words.
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/CapitalizationFilterFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/CapitalizationFilterFactory.java
index 4481d7a..d0aec16 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/CapitalizationFilterFactory.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/CapitalizationFilterFactory.java
@@ -49,8 +49,8 @@ import java.util.StringTokenizer;
  *   &lt;analyzer&gt;
  *     &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
  *     &lt;filter class="solr.CapitalizationFilterFactory" onlyFirstWord="true"
- *     	     keep="java solr lucene" keepIgnoreCase="false"
- *     	     okPrefix="McK McD McA"/&gt;   
+ *           keep="java solr lucene" keepIgnoreCase="false"
+ *           okPrefix="McK McD McA"/&gt;   
  *   &lt;/analyzer&gt;
  * &lt;/fieldType&gt;</pre>
  *
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/HyphenatedWordsFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/HyphenatedWordsFilter.java
index ff59899..8f04ec0 100755
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/HyphenatedWordsFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/HyphenatedWordsFilter.java
@@ -31,8 +31,8 @@ import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
  * Example field definition in schema.xml:
  * <pre class="prettyprint">
  * &lt;fieldtype name="text" class="solr.TextField" positionIncrementGap="100"&gt;
- * 	&lt;analyzer type="index"&gt;
- * 		&lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
+ *  &lt;analyzer type="index"&gt;
+ *    &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
  *      &lt;filter class="solr.SynonymFilterFactory" synonyms="index_synonyms.txt" ignoreCase="true" expand="false"/&gt;
  *      &lt;filter class="solr.StopFilterFactory" ignoreCase="true"/&gt;
  *      &lt;filter class="solr.HyphenatedWordsFilterFactory"/&gt;
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/HyphenatedWordsFilterFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/HyphenatedWordsFilterFactory.java
index 7f24b28..225551c 100755
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/HyphenatedWordsFilterFactory.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/HyphenatedWordsFilterFactory.java
@@ -33,7 +33,7 @@ import org.apache.lucene.analysis.util.TokenFilterFactory;
  *
  */
 public class HyphenatedWordsFilterFactory extends TokenFilterFactory {
-	public HyphenatedWordsFilter create(TokenStream input) {
-		return new HyphenatedWordsFilter(input);
-	}
+  public HyphenatedWordsFilter create(TokenStream input) {
+    return new HyphenatedWordsFilter(input);
+  }
 }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/path/PathHierarchyTokenizerFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/path/PathHierarchyTokenizerFactory.java
index 14122d8..9cffac8 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/path/PathHierarchyTokenizerFactory.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/path/PathHierarchyTokenizerFactory.java
@@ -43,10 +43,10 @@ import org.apache.lucene.analysis.util.TokenizerFactory;
  * <pre class="prettyprint" >
  * &lt;fieldType name="descendent_path" class="solr.TextField"&gt;
  *   &lt;analyzer type="index"&gt;
- * 	   &lt;tokenizer class="solr.PathHierarchyTokenizerFactory" delimiter="/" /&gt;
+ *     &lt;tokenizer class="solr.PathHierarchyTokenizerFactory" delimiter="/" /&gt;
  *   &lt;/analyzer&gt;
  *   &lt;analyzer type="query"&gt;
- * 	   &lt;tokenizer class="solr.KeywordTokenizerFactory" /&gt;
+ *     &lt;tokenizer class="solr.KeywordTokenizerFactory" /&gt;
  *   &lt;/analyzer&gt;
  * &lt;/fieldType&gt;
  * </pre>
@@ -61,10 +61,10 @@ import org.apache.lucene.analysis.util.TokenizerFactory;
  * <pre class="prettyprint" >
  * &lt;fieldType name="descendent_path" class="solr.TextField"&gt;
  *   &lt;analyzer type="index"&gt;
- * 	   &lt;tokenizer class="solr.KeywordTokenizerFactory" /&gt;
+ *     &lt;tokenizer class="solr.KeywordTokenizerFactory" /&gt;
  *   &lt;/analyzer&gt;
  *   &lt;analyzer type="query"&gt;
- * 	   &lt;tokenizer class="solr.PathHierarchyTokenizerFactory" delimiter="/" /&gt;
+ *     &lt;tokenizer class="solr.PathHierarchyTokenizerFactory" delimiter="/" /&gt;
  *   &lt;/analyzer&gt;
  * &lt;/fieldType&gt;
  * </pre>
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer.java
index d4c7103..c56f817 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer.java
@@ -211,6 +211,6 @@ public final class QueryAutoStopWordAnalyzer extends AnalyzerWrapper {
       }
     }
     return allStopWords.toArray(new Term[allStopWords.size()]);
-	}
+  }
 
 }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/shingle/ShingleFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/shingle/ShingleFilter.java
index b14338a..80e99a3 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/shingle/ShingleFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/shingle/ShingleFilter.java
@@ -395,7 +395,7 @@ public final class ShingleFilter extends TokenFilter {
       exhausted = true;
     }
     return newTarget;
-	}
+  }
 
   /**
    * <p>Fills {@link #inputWindow} with input stream tokens, if available, 
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.java
index ecfa18a..5f6d5e7 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.java
@@ -1,4 +1,4 @@
-/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 8/6/12 11:57 AM */
+/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 9/17/12 9:28 AM */
 
 package org.apache.lucene.analysis.standard;
 
@@ -33,8 +33,8 @@ import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 /**
  * This class is a scanner generated by 
  * <a href="http://www.jflex.de/">JFlex</a> 1.5.0-SNAPSHOT
- * on 8/6/12 11:57 AM from the specification file
- * <tt>/home/rmuir/workspace/lucene-trunk/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.jflex</tt>
+ * on 9/17/12 9:28 AM from the specification file
+ * <tt>/Users/Erick/apache/trunk_4326/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.jflex</tt>
  */
 class ClassicTokenizerImpl implements StandardTokenizerInterface {
 
@@ -453,7 +453,7 @@ public final void getText(CharTermAttribute t) {
       }     
     }
 
-	// numRead < 0
+    // numRead < 0
     return true;
   }
 
@@ -674,44 +674,44 @@ public final void getText(CharTermAttribute t) {
       zzMarkedPos = zzMarkedPosL;
 
       switch (zzAction < 0 ? zzAction : ZZ_ACTION[zzAction]) {
-        case 10: 
-          { return EMAIL;
+        case 1: 
+          { /* Break so we don't hit fall-through warning: */ break;/* ignore */
           }
         case 11: break;
         case 2: 
           { return ALPHANUM;
           }
         case 12: break;
-        case 4: 
-          { return HOST;
+        case 3: 
+          { return CJ;
           }
         case 13: break;
-        case 8: 
-          { return ACRONYM_DEP;
+        case 4: 
+          { return HOST;
           }
         case 14: break;
         case 5: 
           { return NUM;
           }
         case 15: break;
-        case 1: 
-          { /* Break so we don't hit fall-through warning: */ break;/* ignore */
+        case 6: 
+          { return APOSTROPHE;
           }
         case 16: break;
-        case 9: 
-          { return ACRONYM;
-          }
-        case 17: break;
         case 7: 
           { return COMPANY;
           }
+        case 17: break;
+        case 8: 
+          { return ACRONYM_DEP;
+          }
         case 18: break;
-        case 6: 
-          { return APOSTROPHE;
+        case 9: 
+          { return ACRONYM;
           }
         case 19: break;
-        case 3: 
-          { return CJ;
+        case 10: 
+          { return EMAIL;
           }
         case 20: break;
         default: 
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.jflex b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.jflex
index da8a410..3d4a3a1 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.jflex
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.jflex
@@ -79,7 +79,7 @@ APOSTROPHE =  {ALPHA} ("'" {ALPHA})+
 // use a post-filter to remove dots
 ACRONYM    =  {LETTER} "." ({LETTER} ".")+
 
-ACRONYM_DEP	= {ALPHANUM} "." ({ALPHANUM} ".")+
+ACRONYM_DEP  = {ALPHANUM} "." ({ALPHANUM} ".")+
 
 // company names like AT&T and Excite@Home.
 COMPANY    =  {ALPHA} ("&"|"@") {ALPHA}
@@ -100,7 +100,7 @@ NUM        = ({ALPHANUM} {P} {HAS_DIGIT}
            | {HAS_DIGIT} {P} {ALPHANUM} ({P} {HAS_DIGIT} {P} {ALPHANUM})+)
 
 // punctuation
-P	         = ("_"|"-"|"/"|"."|",")
+P           = ("_"|"-"|"/"|"."|",")
 
 // at least one digit
 HAS_DIGIT  = ({LETTER}|[:digit:])* [:digit:] ({LETTER}|[:digit:])*
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/SUPPLEMENTARY.jflex-macro b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/SUPPLEMENTARY.jflex-macro
index 64c05aa..0a99d0d 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/SUPPLEMENTARY.jflex-macro
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/SUPPLEMENTARY.jflex-macro
@@ -14,7 +14,7 @@
  * limitations under the License.
  */
 
-// Generated using ICU4J 49.1.0.0 on Monday, August 6, 2012 3:57:23 PM UTC
+// Generated using ICU4J 49.1.0.0 on Monday, September 17, 2012 1:28:46 PM UTC
 // by org.apache.lucene.analysis.icu.GenerateJFlexSupplementaryMacros
 
 
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl.java
index f12e4b7..331eec0 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl.java
@@ -1,4 +1,4 @@
-/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 8/6/12 11:57 AM */
+/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 9/17/12 9:28 AM */
 
 package org.apache.lucene.analysis.standard;
 
@@ -936,7 +936,7 @@ public final class StandardTokenizerImpl implements StandardTokenizerInterface {
       }     
     }
 
-	// numRead < 0
+    // numRead < 0
     return true;
   }
 
@@ -1157,36 +1157,36 @@ public final class StandardTokenizerImpl implements StandardTokenizerInterface {
       zzMarkedPos = zzMarkedPosL;
 
       switch (zzAction < 0 ? zzAction : ZZ_ACTION[zzAction]) {
-        case 2: 
-          { return WORD_TYPE;
+        case 1: 
+          { /* Break so we don't hit fall-through warning: */ break; /* Not numeric, word, ideographic, hiragana, or SE Asian -- ignore it. */
           }
         case 9: break;
-        case 5: 
-          { return SOUTH_EAST_ASIAN_TYPE;
+        case 2: 
+          { return WORD_TYPE;
           }
         case 10: break;
-        case 4: 
-          { return KATAKANA_TYPE;
+        case 3: 
+          { return NUMERIC_TYPE;
           }
         case 11: break;
-        case 6: 
-          { return IDEOGRAPHIC_TYPE;
+        case 4: 
+          { return KATAKANA_TYPE;
           }
         case 12: break;
-        case 8: 
-          { return HANGUL_TYPE;
+        case 5: 
+          { return SOUTH_EAST_ASIAN_TYPE;
           }
         case 13: break;
-        case 3: 
-          { return NUMERIC_TYPE;
+        case 6: 
+          { return IDEOGRAPHIC_TYPE;
           }
         case 14: break;
         case 7: 
           { return HIRAGANA_TYPE;
           }
         case 15: break;
-        case 1: 
-          { /* Break so we don't hit fall-through warning: */ break; /* Not numeric, word, ideographic, hiragana, or SE Asian -- ignore it. */
+        case 8: 
+          { return HANGUL_TYPE;
           }
         case 16: break;
         default: 
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl.jflex b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl.jflex
index a519b4a..e40fca8 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl.jflex
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl.jflex
@@ -115,8 +115,8 @@ HiraganaEx = {Hiragana} ({Format} | {Extend})*
 
 %%
 
-// UAX#29 WB1. 	sot 	÷ 	
-//        WB2. 		÷ 	eot
+// UAX#29 WB1.   sot   ÷
+//        WB2.     ÷   eot
 //
 <<EOF>> { return StandardTokenizerInterface.YYEOF; }
 
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.java
index d044805..4822f4c 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.java
@@ -1,4 +1,4 @@
-/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 8/6/12 11:57 AM */
+/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 9/17/12 9:29 AM */
 
 package org.apache.lucene.analysis.standard;
 
@@ -4126,7 +4126,7 @@ public final class UAX29URLEmailTokenizerImpl implements StandardTokenizerInterf
       }     
     }
 
-	// numRead < 0
+    // numRead < 0
     return true;
   }
 
@@ -4347,50 +4347,50 @@ public final class UAX29URLEmailTokenizerImpl implements StandardTokenizerInterf
       zzMarkedPos = zzMarkedPosL;
 
       switch (zzAction < 0 ? zzAction : ZZ_ACTION[zzAction]) {
-        case 11: 
-          // lookahead expression with fixed base length
-          zzMarkedPos = zzStartRead + 6;
-          { return WORD_TYPE;
+        case 1: 
+          { /* Break so we don't hit fall-through warning: */ break;/* Not numeric, word, ideographic, hiragana, or SE Asian -- ignore it. */
           }
         case 12: break;
         case 2: 
           { return WORD_TYPE;
           }
         case 13: break;
-        case 5: 
-          { return SOUTH_EAST_ASIAN_TYPE;
+        case 3: 
+          { return NUMERIC_TYPE;
           }
         case 14: break;
-        case 1: 
-          { /* Break so we don't hit fall-through warning: */ break;/* Not numeric, word, ideographic, hiragana, or SE Asian -- ignore it. */
+        case 4: 
+          { return KATAKANA_TYPE;
           }
         case 15: break;
-        case 10: 
-          { return URL_TYPE;
+        case 5: 
+          { return SOUTH_EAST_ASIAN_TYPE;
           }
         case 16: break;
-        case 9: 
-          { return EMAIL_TYPE;
+        case 6: 
+          { return IDEOGRAPHIC_TYPE;
           }
         case 17: break;
-        case 4: 
-          { return KATAKANA_TYPE;
+        case 7: 
+          { return HIRAGANA_TYPE;
           }
         case 18: break;
-        case 6: 
-          { return IDEOGRAPHIC_TYPE;
-          }
-        case 19: break;
         case 8: 
           { return HANGUL_TYPE;
           }
+        case 19: break;
+        case 9: 
+          { return EMAIL_TYPE;
+          }
         case 20: break;
-        case 3: 
-          { return NUMERIC_TYPE;
+        case 10: 
+          { return URL_TYPE;
           }
         case 21: break;
-        case 7: 
-          { return HIRAGANA_TYPE;
+        case 11: 
+          // lookahead expression with fixed base length
+          zzMarkedPos = zzStartRead + 6;
+          { return WORD_TYPE;
           }
         case 22: break;
         default: 
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.jflex b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.jflex
index 4c41d0d..f3f1410 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.jflex
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.jflex
@@ -200,8 +200,8 @@ EMAIL = {EMAILlocalPart} "@" ({DomainNameStrict} | {EMAILbracketedHost})
 
 %%
 
-// UAX#29 WB1. 	sot 	÷ 	
-//        WB2. 		÷ 	eot
+// UAX#29 WB1.   sot   ÷
+//        WB2.     ÷   eot
 //
 <<EOF>> { return StandardTokenizerInterface.YYEOF; }
 
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.java
index db7ac88..0f946a6 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.java
@@ -1,4 +1,4 @@
-/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 8/6/12 11:57 AM */
+/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 9/17/12 9:29 AM */
 
 package org.apache.lucene.analysis.wikipedia;
 
@@ -25,8 +25,8 @@ import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 /**
  * This class is a scanner generated by 
  * <a href="http://www.jflex.de/">JFlex</a> 1.5.0-SNAPSHOT
- * on 8/6/12 11:57 AM from the specification file
- * <tt>/home/rmuir/workspace/lucene-trunk/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.jflex</tt>
+ * on 9/17/12 9:29 AM from the specification file
+ * <tt>/Users/Erick/apache/trunk_4326/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.jflex</tt>
  */
 class WikipediaTokenizerImpl {
 
@@ -37,16 +37,16 @@ class WikipediaTokenizerImpl {
   private static final int ZZ_BUFFERSIZE = 4096;
 
   /** lexical states */
-  public static final int THREE_SINGLE_QUOTES_STATE = 10;
+  public static final int YYINITIAL = 0;
+  public static final int CATEGORY_STATE = 2;
+  public static final int INTERNAL_LINK_STATE = 4;
   public static final int EXTERNAL_LINK_STATE = 6;
+  public static final int TWO_SINGLE_QUOTES_STATE = 8;
+  public static final int THREE_SINGLE_QUOTES_STATE = 10;
+  public static final int FIVE_SINGLE_QUOTES_STATE = 12;
   public static final int DOUBLE_EQUALS_STATE = 14;
-  public static final int INTERNAL_LINK_STATE = 4;
   public static final int DOUBLE_BRACE_STATE = 16;
-  public static final int CATEGORY_STATE = 2;
-  public static final int YYINITIAL = 0;
   public static final int STRING = 18;
-  public static final int FIVE_SINGLE_QUOTES_STATE = 12;
-  public static final int TWO_SINGLE_QUOTES_STATE = 8;
 
   /**
    * ZZ_LEXSTATE[l] is the state in the DFA for the lexical state l
@@ -589,7 +589,7 @@ final void reset() {
       }     
     }
 
-	// numRead < 0
+    // numRead < 0
     return true;
   }
 
@@ -810,188 +810,188 @@ final void reset() {
       zzMarkedPos = zzMarkedPosL;
 
       switch (zzAction < 0 ? zzAction : ZZ_ACTION[zzAction]) {
-        case 44: 
-          { numWikiTokensSeen = 0; positionInc = 1; currentTokType = CATEGORY; yybegin(CATEGORY_STATE);/* Break so we don't hit fall-through warning: */ break;
+        case 1: 
+          { numWikiTokensSeen = 0;  positionInc = 1; /* Break so we don't hit fall-through warning: */ break;
           }
         case 47: break;
-        case 37: 
-          { currentTokType = BOLD_ITALICS;  yybegin(FIVE_SINGLE_QUOTES_STATE); /* Break so we don't hit fall-through warning: */ break;
+        case 2: 
+          { positionInc = 1; return ALPHANUM;
           }
         case 48: break;
-        case 16: 
-          { currentTokType = HEADING; yybegin(DOUBLE_EQUALS_STATE); numWikiTokensSeen++; return currentTokType;
+        case 3: 
+          { positionInc = 1; return CJ;
           }
         case 49: break;
-        case 20: 
-          { numBalanced = 0; numWikiTokensSeen = 0; currentTokType = EXTERNAL_LINK;yybegin(EXTERNAL_LINK_STATE); /* Break so we don't hit fall-through warning: */ break;
+        case 4: 
+          { numWikiTokensSeen = 0; positionInc = 1; currentTokType = EXTERNAL_LINK_URL; yybegin(EXTERNAL_LINK_STATE);/* Break so we don't hit fall-through warning: */ break;
           }
         case 50: break;
-        case 40: 
-          { positionInc = 1; return ACRONYM;
-          }
-        case 51: break;
         case 5: 
           { positionInc = 1; /* Break so we don't hit fall-through warning: */ break;
           }
+        case 51: break;
+        case 6: 
+          { yybegin(CATEGORY_STATE); numWikiTokensSeen++; return currentTokType;
+          }
         case 52: break;
-        case 36: 
-          { positionInc = 1; return COMPANY;
+        case 7: 
+          { yybegin(INTERNAL_LINK_STATE); numWikiTokensSeen++; return currentTokType;
           }
         case 53: break;
-        case 10: 
-          { numLinkToks = 0; positionInc = 0; yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;
+        case 8: 
+          { /* Break so we don't hit fall-through warning: */ break;/* ignore */
           }
         case 54: break;
-        case 15: 
-          { currentTokType = SUB_HEADING; numWikiTokensSeen = 0; yybegin(STRING); /* Break so we don't hit fall-through warning: */ break;
+        case 9: 
+          { if (numLinkToks == 0){positionInc = 0;} else{positionInc = 1;} numWikiTokensSeen++; currentTokType = EXTERNAL_LINK; yybegin(EXTERNAL_LINK_STATE); numLinkToks++; return currentTokType;
           }
         case 55: break;
-        case 22: 
-          { numWikiTokensSeen = 0; positionInc = 1; if (numBalanced == 0){numBalanced++;yybegin(TWO_SINGLE_QUOTES_STATE);} else{numBalanced = 0;}/* Break so we don't hit fall-through warning: */ break;
+        case 10: 
+          { numLinkToks = 0; positionInc = 0; yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;
           }
         case 56: break;
-        case 35: 
-          { positionInc = 1; return NUM;
+        case 11: 
+          { currentTokType = BOLD;  yybegin(THREE_SINGLE_QUOTES_STATE); /* Break so we don't hit fall-through warning: */ break;
           }
         case 57: break;
-        case 33: 
-          { positionInc = 1; return APOSTROPHE;
+        case 12: 
+          { currentTokType = ITALICS; numWikiTokensSeen++;  yybegin(STRING); return currentTokType;/*italics*/
           }
         case 58: break;
-        case 21: 
-          { yybegin(STRING); return currentTokType;/*pipe*/
+        case 13: 
+          { currentTokType = EXTERNAL_LINK; numWikiTokensSeen = 0; yybegin(EXTERNAL_LINK_STATE); /* Break so we don't hit fall-through warning: */ break;
           }
         case 59: break;
-        case 18: 
-          { /* Break so we don't hit fall-through warning: */ break;/* ignore STRING */
+        case 14: 
+          { yybegin(STRING); numWikiTokensSeen++; return currentTokType;
           }
         case 60: break;
-        case 2: 
-          { positionInc = 1; return ALPHANUM;
+        case 15: 
+          { currentTokType = SUB_HEADING; numWikiTokensSeen = 0; yybegin(STRING); /* Break so we don't hit fall-through warning: */ break;
           }
         case 61: break;
-        case 1: 
-          { numWikiTokensSeen = 0;  positionInc = 1; /* Break so we don't hit fall-through warning: */ break;
+        case 16: 
+          { currentTokType = HEADING; yybegin(DOUBLE_EQUALS_STATE); numWikiTokensSeen++; return currentTokType;
           }
         case 62: break;
         case 17: 
           { yybegin(DOUBLE_BRACE_STATE); numWikiTokensSeen = 0; return currentTokType;
           }
         case 63: break;
-        case 39: 
-          { numBalanced = 0;currentTokType = ALPHANUM; yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;/*end sub header*/
+        case 18: 
+          { /* Break so we don't hit fall-through warning: */ break;/* ignore STRING */
           }
         case 64: break;
-        case 29: 
-          { currentTokType = INTERNAL_LINK; numWikiTokensSeen = 0;  yybegin(INTERNAL_LINK_STATE); /* Break so we don't hit fall-through warning: */ break;
+        case 19: 
+          { yybegin(STRING); numWikiTokensSeen++; return currentTokType;/* STRING ALPHANUM*/
           }
         case 65: break;
-        case 46: 
-          { numBalanced = 0; numWikiTokensSeen = 0; currentTokType = CATEGORY;yybegin(CATEGORY_STATE); /* Break so we don't hit fall-through warning: */ break;
+        case 20: 
+          { numBalanced = 0; numWikiTokensSeen = 0; currentTokType = EXTERNAL_LINK;yybegin(EXTERNAL_LINK_STATE); /* Break so we don't hit fall-through warning: */ break;
           }
         case 66: break;
-        case 27: 
-          { numLinkToks = 0; yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;
+        case 21: 
+          { yybegin(STRING); return currentTokType;/*pipe*/
           }
         case 67: break;
-        case 4: 
-          { numWikiTokensSeen = 0; positionInc = 1; currentTokType = EXTERNAL_LINK_URL; yybegin(EXTERNAL_LINK_STATE);/* Break so we don't hit fall-through warning: */ break;
+        case 22: 
+          { numWikiTokensSeen = 0; positionInc = 1; if (numBalanced == 0){numBalanced++;yybegin(TWO_SINGLE_QUOTES_STATE);} else{numBalanced = 0;}/* Break so we don't hit fall-through warning: */ break;
           }
         case 68: break;
-        case 38: 
-          { numBalanced = 0;currentTokType = ALPHANUM;yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;/*end bold*/
+        case 23: 
+          { numWikiTokensSeen = 0; positionInc = 1; yybegin(DOUBLE_EQUALS_STATE);/* Break so we don't hit fall-through warning: */ break;
           }
         case 69: break;
-        case 13: 
-          { currentTokType = EXTERNAL_LINK; numWikiTokensSeen = 0; yybegin(EXTERNAL_LINK_STATE); /* Break so we don't hit fall-through warning: */ break;
+        case 24: 
+          { numWikiTokensSeen = 0; positionInc = 1; currentTokType = INTERNAL_LINK; yybegin(INTERNAL_LINK_STATE);/* Break so we don't hit fall-through warning: */ break;
           }
         case 70: break;
-        case 3: 
-          { positionInc = 1; return CJ;
+        case 25: 
+          { numWikiTokensSeen = 0; positionInc = 1; currentTokType = CITATION; yybegin(DOUBLE_BRACE_STATE);/* Break so we don't hit fall-through warning: */ break;
           }
         case 71: break;
-        case 45: 
-          { currentTokType = CATEGORY; numWikiTokensSeen = 0; yybegin(CATEGORY_STATE); /* Break so we don't hit fall-through warning: */ break;
+        case 26: 
+          { yybegin(YYINITIAL);/* Break so we don't hit fall-through warning: */ break;
           }
         case 72: break;
-        case 6: 
-          { yybegin(CATEGORY_STATE); numWikiTokensSeen++; return currentTokType;
+        case 27: 
+          { numLinkToks = 0; yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;
           }
         case 73: break;
-        case 11: 
-          { currentTokType = BOLD;  yybegin(THREE_SINGLE_QUOTES_STATE); /* Break so we don't hit fall-through warning: */ break;
+        case 28: 
+          { currentTokType = INTERNAL_LINK; numWikiTokensSeen = 0; yybegin(INTERNAL_LINK_STATE); /* Break so we don't hit fall-through warning: */ break;
           }
         case 74: break;
-        case 25: 
-          { numWikiTokensSeen = 0; positionInc = 1; currentTokType = CITATION; yybegin(DOUBLE_BRACE_STATE);/* Break so we don't hit fall-through warning: */ break;
+        case 29: 
+          { currentTokType = INTERNAL_LINK; numWikiTokensSeen = 0;  yybegin(INTERNAL_LINK_STATE); /* Break so we don't hit fall-through warning: */ break;
           }
         case 75: break;
-        case 8: 
-          { /* Break so we don't hit fall-through warning: */ break;/* ignore */
+        case 30: 
+          { yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;
           }
         case 76: break;
-        case 19: 
-          { yybegin(STRING); numWikiTokensSeen++; return currentTokType;/* STRING ALPHANUM*/
+        case 31: 
+          { numBalanced = 0;currentTokType = ALPHANUM; yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;/*end italics*/
           }
         case 77: break;
-        case 43: 
-          { positionInc = 1; numWikiTokensSeen++; yybegin(EXTERNAL_LINK_STATE); return currentTokType;
+        case 32: 
+          { numBalanced = 0; numWikiTokensSeen = 0; currentTokType = INTERNAL_LINK;yybegin(INTERNAL_LINK_STATE); /* Break so we don't hit fall-through warning: */ break;
           }
         case 78: break;
-        case 42: 
-          { numBalanced = 0;currentTokType = ALPHANUM; yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;/*end bold italics*/
+        case 33: 
+          { positionInc = 1; return APOSTROPHE;
           }
         case 79: break;
-        case 30: 
-          { yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;
+        case 34: 
+          { positionInc = 1; return HOST;
           }
         case 80: break;
-        case 14: 
-          { yybegin(STRING); numWikiTokensSeen++; return currentTokType;
+        case 35: 
+          { positionInc = 1; return NUM;
           }
         case 81: break;
-        case 9: 
-          { if (numLinkToks == 0){positionInc = 0;} else{positionInc = 1;} numWikiTokensSeen++; currentTokType = EXTERNAL_LINK; yybegin(EXTERNAL_LINK_STATE); numLinkToks++; return currentTokType;
+        case 36: 
+          { positionInc = 1; return COMPANY;
           }
         case 82: break;
-        case 7: 
-          { yybegin(INTERNAL_LINK_STATE); numWikiTokensSeen++; return currentTokType;
+        case 37: 
+          { currentTokType = BOLD_ITALICS;  yybegin(FIVE_SINGLE_QUOTES_STATE); /* Break so we don't hit fall-through warning: */ break;
           }
         case 83: break;
-        case 41: 
-          { positionInc = 1; return EMAIL;
+        case 38: 
+          { numBalanced = 0;currentTokType = ALPHANUM;yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;/*end bold*/
           }
         case 84: break;
-        case 28: 
-          { currentTokType = INTERNAL_LINK; numWikiTokensSeen = 0; yybegin(INTERNAL_LINK_STATE); /* Break so we don't hit fall-through warning: */ break;
+        case 39: 
+          { numBalanced = 0;currentTokType = ALPHANUM; yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;/*end sub header*/
           }
         case 85: break;
-        case 23: 
-          { numWikiTokensSeen = 0; positionInc = 1; yybegin(DOUBLE_EQUALS_STATE);/* Break so we don't hit fall-through warning: */ break;
+        case 40: 
+          { positionInc = 1; return ACRONYM;
           }
         case 86: break;
-        case 34: 
-          { positionInc = 1; return HOST;
+        case 41: 
+          { positionInc = 1; return EMAIL;
           }
         case 87: break;
-        case 32: 
-          { numBalanced = 0; numWikiTokensSeen = 0; currentTokType = INTERNAL_LINK;yybegin(INTERNAL_LINK_STATE); /* Break so we don't hit fall-through warning: */ break;
+        case 42: 
+          { numBalanced = 0;currentTokType = ALPHANUM; yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;/*end bold italics*/
           }
         case 88: break;
-        case 12: 
-          { currentTokType = ITALICS; numWikiTokensSeen++;  yybegin(STRING); return currentTokType;/*italics*/
+        case 43: 
+          { positionInc = 1; numWikiTokensSeen++; yybegin(EXTERNAL_LINK_STATE); return currentTokType;
           }
         case 89: break;
-        case 24: 
-          { numWikiTokensSeen = 0; positionInc = 1; currentTokType = INTERNAL_LINK; yybegin(INTERNAL_LINK_STATE);/* Break so we don't hit fall-through warning: */ break;
+        case 44: 
+          { numWikiTokensSeen = 0; positionInc = 1; currentTokType = CATEGORY; yybegin(CATEGORY_STATE);/* Break so we don't hit fall-through warning: */ break;
           }
         case 90: break;
-        case 31: 
-          { numBalanced = 0;currentTokType = ALPHANUM; yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;/*end italics*/
+        case 45: 
+          { currentTokType = CATEGORY; numWikiTokensSeen = 0; yybegin(CATEGORY_STATE); /* Break so we don't hit fall-through warning: */ break;
           }
         case 91: break;
-        case 26: 
-          { yybegin(YYINITIAL);/* Break so we don't hit fall-through warning: */ break;
+        case 46: 
+          { numBalanced = 0; numWikiTokensSeen = 0; currentTokType = CATEGORY;yybegin(CATEGORY_STATE); /* Break so we don't hit fall-through warning: */ break;
           }
         case 92: break;
         default: 
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.jflex b/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.jflex
index dfbf660..4a55847 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.jflex
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.jflex
@@ -136,7 +136,7 @@ NUM        = ({ALPHANUM} {P} {HAS_DIGIT}
 TAGS = "<"\/?{ALPHANUM}({WHITESPACE}*{ALPHANUM}=\"{ALPHANUM}\")*">"
 
 // punctuation
-P	         = ("_"|"-"|"/"|"."|",")
+P           = ("_"|"-"|"/"|"."|",")
 
 // at least one digit
 HAS_DIGIT  =
diff --git a/lucene/analysis/common/src/java/org/tartarus/snowball/Among.java b/lucene/analysis/common/src/java/org/tartarus/snowball/Among.java
index 57aa159..1d4756e 100644
--- a/lucene/analysis/common/src/java/org/tartarus/snowball/Among.java
+++ b/lucene/analysis/common/src/java/org/tartarus/snowball/Among.java
@@ -43,25 +43,26 @@ import java.lang.reflect.Method;
  * reflection calls (Lovins, etc) use EMPTY_ARGS/EMPTY_PARAMS
  */
 public class Among {
-    private static final Class<?>[] EMPTY_PARAMS = new Class[0];
-    public Among (String s, int substring_i, int result,
-		  String methodname, SnowballProgram methodobject) {
-        this.s_size = s.length();
-        this.s = s.toCharArray();
-        this.substring_i = substring_i;
-	this.result = result;
-	this.methodobject = methodobject;
-	if (methodname.length() == 0) {
-	    this.method = null;
-	} else {
-	    try {
-		this.method = methodobject.getClass().
-		getDeclaredMethod(methodname, EMPTY_PARAMS);
-	    } catch (NoSuchMethodException e) {
-		throw new RuntimeException(e);
-	    }
-	}
+  private static final Class<?>[] EMPTY_PARAMS = new Class[0];
+
+  public Among(String s, int substring_i, int result,
+               String methodname, SnowballProgram methodobject) {
+    this.s_size = s.length();
+    this.s = s.toCharArray();
+    this.substring_i = substring_i;
+    this.result = result;
+    this.methodobject = methodobject;
+    if (methodname.length() == 0) {
+      this.method = null;
+    } else {
+      try {
+        this.method = methodobject.getClass().
+            getDeclaredMethod(methodname, EMPTY_PARAMS);
+      } catch (NoSuchMethodException e) {
+        throw new RuntimeException(e);
+      }
     }
+  }
 
     public final int s_size; /* search string */
     public final char[] s; /* search string */
diff --git a/lucene/analysis/common/src/java/org/tartarus/snowball/SnowballProgram.java b/lucene/analysis/common/src/java/org/tartarus/snowball/SnowballProgram.java
index fd8ba15..dbce73b 100644
--- a/lucene/analysis/common/src/java/org/tartarus/snowball/SnowballProgram.java
+++ b/lucene/analysis/common/src/java/org/tartarus/snowball/SnowballProgram.java
@@ -51,8 +51,8 @@ public abstract class SnowballProgram {
 
     protected SnowballProgram()
     {
-	current = new char[8];
-	setCurrent("");
+      current = new char[8];
+      setCurrent("");
     }
 
     public abstract boolean stem();
@@ -62,12 +62,12 @@ public abstract class SnowballProgram {
      */
     public void setCurrent(String value)
     {
-	current = value.toCharArray();
-	cursor = 0;
-	limit = value.length();
-	limit_backward = 0;
-	bra = cursor;
-	ket = limit;
+      current = value.toCharArray();
+      cursor = 0;
+      limit = value.length();
+      limit_backward = 0;
+      bra = cursor;
+      ket = limit;
     }
 
     /**
@@ -130,354 +130,350 @@ public abstract class SnowballProgram {
 
     protected void copy_from(SnowballProgram other)
     {
-	current          = other.current;
-	cursor           = other.cursor;
-	limit            = other.limit;
-	limit_backward   = other.limit_backward;
-	bra              = other.bra;
-	ket              = other.ket;
+      current          = other.current;
+      cursor           = other.cursor;
+      limit            = other.limit;
+      limit_backward   = other.limit_backward;
+      bra              = other.bra;
+      ket              = other.ket;
     }
 
     protected boolean in_grouping(char [] s, int min, int max)
     {
-	if (cursor >= limit) return false;
-	char ch = current[cursor];
-	if (ch > max || ch < min) return false;
-	ch -= min;
-	if ((s[ch >> 3] & (0X1 << (ch & 0X7))) == 0) return false;
-	cursor++;
-	return true;
+      if (cursor >= limit) return false;
+      char ch = current[cursor];
+      if (ch > max || ch < min) return false;
+      ch -= min;
+      if ((s[ch >> 3] & (0X1 << (ch & 0X7))) == 0) return false;
+      cursor++;
+      return true;
     }
 
     protected boolean in_grouping_b(char [] s, int min, int max)
     {
-	if (cursor <= limit_backward) return false;
-	char ch = current[cursor - 1];
-	if (ch > max || ch < min) return false;
-	ch -= min;
-	if ((s[ch >> 3] & (0X1 << (ch & 0X7))) == 0) return false;
-	cursor--;
-	return true;
+      if (cursor <= limit_backward) return false;
+      char ch = current[cursor - 1];
+      if (ch > max || ch < min) return false;
+      ch -= min;
+      if ((s[ch >> 3] & (0X1 << (ch & 0X7))) == 0) return false;
+      cursor--;
+      return true;
     }
 
     protected boolean out_grouping(char [] s, int min, int max)
     {
-	if (cursor >= limit) return false;
-	char ch = current[cursor];
-	if (ch > max || ch < min) {
-	    cursor++;
-	    return true;
-	}
-	ch -= min;
-	if ((s[ch >> 3] & (0X1 << (ch & 0X7))) == 0) {
-	    cursor ++;
-	    return true;
-	}
-	return false;
+      if (cursor >= limit) return false;
+      char ch = current[cursor];
+      if (ch > max || ch < min) {
+          cursor++;
+          return true;
+      }
+      ch -= min;
+      if ((s[ch >> 3] & (0X1 << (ch & 0X7))) == 0) {
+          cursor ++;
+          return true;
+      }
+      return false;
     }
 
     protected boolean out_grouping_b(char [] s, int min, int max)
     {
-	if (cursor <= limit_backward) return false;
-	char ch = current[cursor - 1];
-	if (ch > max || ch < min) {
-	    cursor--;
-	    return true;
-	}
-	ch -= min;
-	if ((s[ch >> 3] & (0X1 << (ch & 0X7))) == 0) {
-	    cursor--;
-	    return true;
-	}
-	return false;
+      if (cursor <= limit_backward) return false;
+      char ch = current[cursor - 1];
+      if (ch > max || ch < min) {
+          cursor--;
+          return true;
+      }
+      ch -= min;
+      if ((s[ch >> 3] & (0X1 << (ch & 0X7))) == 0) {
+          cursor--;
+          return true;
+      }
+      return false;
     }
 
     protected boolean in_range(int min, int max)
     {
-	if (cursor >= limit) return false;
-	char ch = current[cursor];
-	if (ch > max || ch < min) return false;
-	cursor++;
-	return true;
+      if (cursor >= limit) return false;
+      char ch = current[cursor];
+      if (ch > max || ch < min) return false;
+      cursor++;
+      return true;
     }
 
     protected boolean in_range_b(int min, int max)
     {
-	if (cursor <= limit_backward) return false;
-	char ch = current[cursor - 1];
-	if (ch > max || ch < min) return false;
-	cursor--;
-	return true;
+      if (cursor <= limit_backward) return false;
+      char ch = current[cursor - 1];
+      if (ch > max || ch < min) return false;
+      cursor--;
+      return true;
     }
 
     protected boolean out_range(int min, int max)
     {
-	if (cursor >= limit) return false;
-	char ch = current[cursor];
-	if (!(ch > max || ch < min)) return false;
-	cursor++;
-	return true;
+      if (cursor >= limit) return false;
+      char ch = current[cursor];
+      if (!(ch > max || ch < min)) return false;
+      cursor++;
+      return true;
     }
 
     protected boolean out_range_b(int min, int max)
     {
-	if (cursor <= limit_backward) return false;
-	char ch = current[cursor - 1];
-	if(!(ch > max || ch < min)) return false;
-	cursor--;
-	return true;
+      if (cursor <= limit_backward) return false;
+      char ch = current[cursor - 1];
+      if(!(ch > max || ch < min)) return false;
+      cursor--;
+      return true;
     }
 
     protected boolean eq_s(int s_size, CharSequence s)
     {
-	if (limit - cursor < s_size) return false;
-	int i;
-	for (i = 0; i != s_size; i++) {
-	    if (current[cursor + i] != s.charAt(i)) return false;
-	}
-	cursor += s_size;
-	return true;
+      if (limit - cursor < s_size) return false;
+      int i;
+      for (i = 0; i != s_size; i++) {
+          if (current[cursor + i] != s.charAt(i)) return false;
+      }
+      cursor += s_size;
+      return true;
     }
 
     protected boolean eq_s_b(int s_size, CharSequence s)
     {
-	if (cursor - limit_backward < s_size) return false;
-	int i;
-	for (i = 0; i != s_size; i++) {
-	    if (current[cursor - s_size + i] != s.charAt(i)) return false;
-	}
-	cursor -= s_size;
-	return true;
+      if (cursor - limit_backward < s_size) return false;
+      int i;
+      for (i = 0; i != s_size; i++) {
+          if (current[cursor - s_size + i] != s.charAt(i)) return false;
+      }
+      cursor -= s_size;
+      return true;
     }
 
     protected boolean eq_v(CharSequence s)
     {
-	return eq_s(s.length(), s);
+      return eq_s(s.length(), s);
     }
 
     protected boolean eq_v_b(CharSequence s)
-    {   return eq_s_b(s.length(), s);
+    {
+      return eq_s_b(s.length(), s);
     }
 
     protected int find_among(Among v[], int v_size)
     {
-	int i = 0;
-	int j = v_size;
-
-	int c = cursor;
-	int l = limit;
-
-	int common_i = 0;
-	int common_j = 0;
-
-	boolean first_key_inspected = false;
-
-	while(true) {
-	    int k = i + ((j - i) >> 1);
-	    int diff = 0;
-	    int common = common_i < common_j ? common_i : common_j; // smaller
-	    Among w = v[k];
-	    int i2;
-	    for (i2 = common; i2 < w.s_size; i2++) {
-		if (c + common == l) {
-		    diff = -1;
-		    break;
-		}
-		diff = current[c + common] - w.s[i2];
-		if (diff != 0) break;
-		common++;
-	    }
-	    if (diff < 0) {
-		j = k;
-		common_j = common;
-	    } else {
-		i = k;
-		common_i = common;
-	    }
-	    if (j - i <= 1) {
-		if (i > 0) break; // v->s has been inspected
-		if (j == i) break; // only one item in v
-
-		// - but now we need to go round once more to get
-		// v->s inspected. This looks messy, but is actually
-		// the optimal approach.
-
-		if (first_key_inspected) break;
-		first_key_inspected = true;
-	    }
-	}
-	while(true) {
-	    Among w = v[i];
-	    if (common_i >= w.s_size) {
-		cursor = c + w.s_size;
-		if (w.method == null) return w.result;
-		boolean res;
-		try {
-		    Object resobj = w.method.invoke(w.methodobject, EMPTY_ARGS);
-		    res = resobj.toString().equals("true");
-		} catch (InvocationTargetException e) {
-		    res = false;
-		    // FIXME - debug message
-		} catch (IllegalAccessException e) {
-		    res = false;
-		    // FIXME - debug message
-		}
-		cursor = c + w.s_size;
-		if (res) return w.result;
-	    }
-	    i = w.substring_i;
-	    if (i < 0) return 0;
-	}
+      int i = 0;
+      int j = v_size;
+
+      int c = cursor;
+      int l = limit;
+
+      int common_i = 0;
+      int common_j = 0;
+
+      boolean first_key_inspected = false;
+
+      while (true) {
+        int k = i + ((j - i) >> 1);
+        int diff = 0;
+        int common = common_i < common_j ? common_i : common_j; // smaller
+        Among w = v[k];
+        int i2;
+        for (i2 = common; i2 < w.s_size; i2++) {
+          if (c + common == l) {
+            diff = -1;
+            break;
+          }
+          diff = current[c + common] - w.s[i2];
+          if (diff != 0) break;
+          common++;
+        }
+        if (diff < 0) {
+          j = k;
+          common_j = common;
+        } else {
+          i = k;
+          common_i = common;
+        }
+        if (j - i <= 1) {
+          if (i > 0) break; // v->s has been inspected
+          if (j == i) break; // only one item in v
+
+          // - but now we need to go round once more to get
+          // v->s inspected. This looks messy, but is actually
+          // the optimal approach.
+
+          if (first_key_inspected) break;
+          first_key_inspected = true;
+        }
+      }
+      while (true) {
+        Among w = v[i];
+        if (common_i >= w.s_size) {
+          cursor = c + w.s_size;
+          if (w.method == null) return w.result;
+          boolean res;
+          try {
+            Object resobj = w.method.invoke(w.methodobject, EMPTY_ARGS);
+            res = resobj.toString().equals("true");
+          } catch (InvocationTargetException e) {
+            res = false;
+            // FIXME - debug message
+          } catch (IllegalAccessException e) {
+            res = false;
+            // FIXME - debug message
+          }
+          cursor = c + w.s_size;
+          if (res) return w.result;
+        }
+        i = w.substring_i;
+        if (i < 0) return 0;
+      }
     }
 
-    // find_among_b is for backwards processing. Same comments apply
+  // find_among_b is for backwards processing. Same comments apply
     protected int find_among_b(Among v[], int v_size)
     {
-	int i = 0;
-	int j = v_size;
-
-	int c = cursor;
-	int lb = limit_backward;
-
-	int common_i = 0;
-	int common_j = 0;
-
-	boolean first_key_inspected = false;
-
-	while(true) {
-	    int k = i + ((j - i) >> 1);
-	    int diff = 0;
-	    int common = common_i < common_j ? common_i : common_j;
-	    Among w = v[k];
-	    int i2;
-	    for (i2 = w.s_size - 1 - common; i2 >= 0; i2--) {
-		if (c - common == lb) {
-		    diff = -1;
-		    break;
-		}
-		diff = current[c - 1 - common] - w.s[i2];
-		if (diff != 0) break;
-		common++;
-	    }
-	    if (diff < 0) {
-		j = k;
-		common_j = common;
-	    } else {
-		i = k;
-		common_i = common;
-	    }
-	    if (j - i <= 1) {
-		if (i > 0) break;
-		if (j == i) break;
-		if (first_key_inspected) break;
-		first_key_inspected = true;
-	    }
-	}
-	while(true) {
-	    Among w = v[i];
-	    if (common_i >= w.s_size) {
-		cursor = c - w.s_size;
-		if (w.method == null) return w.result;
-
-		boolean res;
-		try {
-		    Object resobj = w.method.invoke(w.methodobject, EMPTY_ARGS);
-		    res = resobj.toString().equals("true");
-		} catch (InvocationTargetException e) {
-		    res = false;
-		    // FIXME - debug message
-		} catch (IllegalAccessException e) {
-		    res = false;
-		    // FIXME - debug message
-		}
-		cursor = c - w.s_size;
-		if (res) return w.result;
-	    }
-	    i = w.substring_i;
-	    if (i < 0) return 0;
-	}
+  int i = 0;
+  int j = v_size;
+
+  int c = cursor;
+  int lb = limit_backward;
+
+  int common_i = 0;
+  int common_j = 0;
+
+  boolean first_key_inspected = false;
+
+      while (true) {
+        int k = i + ((j - i) >> 1);
+        int diff = 0;
+        int common = common_i < common_j ? common_i : common_j;
+        Among w = v[k];
+        int i2;
+        for (i2 = w.s_size - 1 - common; i2 >= 0; i2--) {
+          if (c - common == lb) {
+            diff = -1;
+            break;
+          }
+          diff = current[c - 1 - common] - w.s[i2];
+          if (diff != 0) break;
+          common++;
+        }
+        if (diff < 0) {
+          j = k;
+          common_j = common;
+        } else {
+          i = k;
+          common_i = common;
+        }
+        if (j - i <= 1) {
+          if (i > 0) break;
+          if (j == i) break;
+          if (first_key_inspected) break;
+          first_key_inspected = true;
+        }
+      }
+      while (true) {
+        Among w = v[i];
+        if (common_i >= w.s_size) {
+          cursor = c - w.s_size;
+          if (w.method == null) return w.result;
+
+          boolean res;
+          try {
+            Object resobj = w.method.invoke(w.methodobject, EMPTY_ARGS);
+            res = resobj.toString().equals("true");
+          } catch (InvocationTargetException e) {
+            res = false;
+            // FIXME - debug message
+          } catch (IllegalAccessException e) {
+            res = false;
+            // FIXME - debug message
+          }
+          cursor = c - w.s_size;
+          if (res) return w.result;
+        }
+        i = w.substring_i;
+        if (i < 0) return 0;
+      }
     }
 
-    /* to replace chars between c_bra and c_ket in current by the
+  /* to replace chars between c_bra and c_ket in current by the
      * chars in s.
      */
-    protected int replace_s(int c_bra, int c_ket, CharSequence s)
-    {
-	final int adjustment = s.length() - (c_ket - c_bra);
-	final int newLength = limit + adjustment;
-	//resize if necessary
-	if (newLength > current.length) {
-	  char newBuffer[] = new char[ArrayUtil.oversize(newLength, RamUsageEstimator.NUM_BYTES_CHAR)];
-	  System.arraycopy(current, 0, newBuffer, 0, limit);
-	  current = newBuffer;
-	}
-	// if the substring being replaced is longer or shorter than the
-	// replacement, need to shift things around
-	if (adjustment != 0 && c_ket < limit) {
-	  System.arraycopy(current, c_ket, current, c_bra + s.length(), 
-	      limit - c_ket);
-	}
-	// insert the replacement text
-	// Note, faster is s.getChars(0, s.length(), current, c_bra);
-	// but would have to duplicate this method for both String and StringBuilder
-	for (int i = 0; i < s.length(); i++)
-	  current[c_bra + i] = s.charAt(i);
-	
-	limit += adjustment;
-	if (cursor >= c_ket) cursor += adjustment;
-	else if (cursor > c_bra) cursor = c_bra;
-	return adjustment;
-    }
-
-    protected void slice_check()
-    {
-	if (bra < 0 ||
-	    bra > ket ||
-	    ket > limit)
-	{
-	    throw new IllegalArgumentException("faulty slice operation: bra=" + bra + ",ket=" + ket + ",limit=" + limit);
-	// FIXME: report error somehow.
-	/*
-	    fprintf(stderr, "faulty slice operation:\n");
-	    debug(z, -1, 0);
-	    exit(1);
-	    */
-	}
+  protected int replace_s(int c_bra, int c_ket, CharSequence s) {
+    final int adjustment = s.length() - (c_ket - c_bra);
+    final int newLength = limit + adjustment;
+    //resize if necessary
+    if (newLength > current.length) {
+      char newBuffer[] = new char[ArrayUtil.oversize(newLength, RamUsageEstimator.NUM_BYTES_CHAR)];
+      System.arraycopy(current, 0, newBuffer, 0, limit);
+      current = newBuffer;
     }
-
-    protected void slice_from(CharSequence s)
-    {
-	slice_check();
-	replace_s(bra, ket, s);
+    // if the substring being replaced is longer or shorter than the
+    // replacement, need to shift things around
+    if (adjustment != 0 && c_ket < limit) {
+      System.arraycopy(current, c_ket, current, c_bra + s.length(),
+          limit - c_ket);
     }
- 
-    protected void slice_del()
-    {
-	slice_from((CharSequence)"");
+    // insert the replacement text
+    // Note, faster is s.getChars(0, s.length(), current, c_bra);
+    // but would have to duplicate this method for both String and StringBuilder
+    for (int i = 0; i < s.length(); i++)
+      current[c_bra + i] = s.charAt(i);
+
+    limit += adjustment;
+    if (cursor >= c_ket) cursor += adjustment;
+    else if (cursor > c_bra) cursor = c_bra;
+    return adjustment;
+  }
+
+  protected void slice_check() {
+    if (bra < 0 ||
+        bra > ket ||
+        ket > limit) {
+      throw new IllegalArgumentException("faulty slice operation: bra=" + bra + ",ket=" + ket + ",limit=" + limit);
+      // FIXME: report error somehow.
+      /*
+      fprintf(stderr, "faulty slice operation:\n");
+      debug(z, -1, 0);
+      exit(1);
+      */
     }
+  }
+
+  protected void slice_from(CharSequence s) {
+    slice_check();
+    replace_s(bra, ket, s);
+  }
+
+  protected void slice_del() {
+    slice_from((CharSequence) "");
+  }
 
-    protected void insert(int c_bra, int c_ket, CharSequence s)
+  protected void insert(int c_bra, int c_ket, CharSequence s)
     {
-	int adjustment = replace_s(c_bra, c_ket, s);
-	if (c_bra <= bra) bra += adjustment;
-	if (c_bra <= ket) ket += adjustment;
+      int adjustment = replace_s(c_bra, c_ket, s);
+      if (c_bra <= bra) bra += adjustment;
+      if (c_bra <= ket) ket += adjustment;
     }
 
     /* Copy the slice into the supplied StringBuffer */
     protected StringBuilder slice_to(StringBuilder s)
     {
-	slice_check();
-	int len = ket - bra;
-	s.setLength(0);
-	s.append(current, bra, len);
-	return s;
+      slice_check();
+      int len = ket - bra;
+      s.setLength(0);
+      s.append(current, bra, len);
+      return s;
     }
 
     protected StringBuilder assign_to(StringBuilder s)
     {
-	s.setLength(0);
-	s.append(current, 0, limit);
-	return s;
+      s.setLength(0);
+      s.append(current, 0, limit);
+      return s;
     }
 
 /*
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/br/TestBrazilianStemmer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/br/TestBrazilianStemmer.java
index 6b1a082..7088720 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/br/TestBrazilianStemmer.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/br/TestBrazilianStemmer.java
@@ -38,87 +38,87 @@ import org.apache.lucene.analysis.util.CharArraySet;
 public class TestBrazilianStemmer extends BaseTokenStreamTestCase {
   
   public void testWithSnowballExamples() throws Exception {
-	 check("boa", "boa");
-	 check("boainain", "boainain");
-	 check("boas", "boas");
-	 check("bôas", "boas"); // removes diacritic: different from snowball portugese
-	 check("boassu", "boassu");
-	 check("boataria", "boat");
-	 check("boate", "boat");
-	 check("boates", "boat");
-	 check("boatos", "boat");
-	 check("bob", "bob");
-	 check("boba", "bob");
-	 check("bobagem", "bobag");
-	 check("bobagens", "bobagens");
-	 check("bobalhões", "bobalho"); // removes diacritic: different from snowball portugese
-	 check("bobear", "bob");
-	 check("bobeira", "bobeir");
-	 check("bobinho", "bobinh");
-	 check("bobinhos", "bobinh");
-	 check("bobo", "bob");
-	 check("bobs", "bobs");
-	 check("boca", "boc");
-	 check("bocadas", "boc");
-	 check("bocadinho", "bocadinh");
-	 check("bocado", "boc");
-	 check("bocaiúva", "bocaiuv"); // removes diacritic: different from snowball portuguese
-	 check("boçal", "bocal"); // removes diacritic: different from snowball portuguese
-	 check("bocarra", "bocarr");
-	 check("bocas", "boc");
-	 check("bode", "bod");
-	 check("bodoque", "bodoqu");
-	 check("body", "body");
-	 check("boeing", "boeing");
-	 check("boem", "boem");
-	 check("boemia", "boem");
-	 check("boêmio", "boemi"); // removes diacritic: different from snowball portuguese
-	 check("bogotá", "bogot");
-	 check("boi", "boi");
-	 check("bóia", "boi"); // removes diacritic: different from snowball portuguese
-	 check("boiando", "boi");
-	 check("quiabo", "quiab");
-	 check("quicaram", "quic");
-	 check("quickly", "quickly");
-	 check("quieto", "quiet");
-	 check("quietos", "quiet");
-	 check("quilate", "quilat");
-	 check("quilates", "quilat");
-	 check("quilinhos", "quilinh");
-	 check("quilo", "quil");
-	 check("quilombo", "quilomb");
-	 check("quilométricas", "quilometr"); // removes diacritic: different from snowball portuguese
-	 check("quilométricos", "quilometr"); // removes diacritic: different from snowball portuguese
-	 check("quilômetro", "quilometr"); // removes diacritic: different from snowball portoguese
-	 check("quilômetros", "quilometr"); // removes diacritic: different from snowball portoguese
-	 check("quilos", "quil");
-	 check("quimica", "quimic");
-	 check("quilos", "quil");
-	 check("quimica", "quimic");
-	 check("quimicas", "quimic");
-	 check("quimico", "quimic");
-	 check("quimicos", "quimic");
-	 check("quimioterapia", "quimioterap");
-	 check("quimioterápicos", "quimioterap"); // removes diacritic: different from snowball portoguese
-	 check("quimono", "quimon");
-	 check("quincas", "quinc");
-	 check("quinhão", "quinha"); // removes diacritic: different from snowball portoguese
-	 check("quinhentos", "quinhent");
-	 check("quinn", "quinn");
-	 check("quino", "quin");
-	 check("quinta", "quint");
-	 check("quintal", "quintal");
-	 check("quintana", "quintan");
-	 check("quintanilha", "quintanilh");
-	 check("quintão", "quinta"); // removes diacritic: different from snowball portoguese
-	 check("quintessência", "quintessente"); // versus snowball portuguese 'quintessent'
-	 check("quintino", "quintin");
-	 check("quinto", "quint");
-	 check("quintos", "quint");
-	 check("quintuplicou", "quintuplic");
-	 check("quinze", "quinz");
-	 check("quinzena", "quinzen");
-	 check("quiosque", "quiosqu");
+   check("boa", "boa");
+   check("boainain", "boainain");
+   check("boas", "boas");
+   check("bôas", "boas"); // removes diacritic: different from snowball portugese
+   check("boassu", "boassu");
+   check("boataria", "boat");
+   check("boate", "boat");
+   check("boates", "boat");
+   check("boatos", "boat");
+   check("bob", "bob");
+   check("boba", "bob");
+   check("bobagem", "bobag");
+   check("bobagens", "bobagens");
+   check("bobalhões", "bobalho"); // removes diacritic: different from snowball portugese
+   check("bobear", "bob");
+   check("bobeira", "bobeir");
+   check("bobinho", "bobinh");
+   check("bobinhos", "bobinh");
+   check("bobo", "bob");
+   check("bobs", "bobs");
+   check("boca", "boc");
+   check("bocadas", "boc");
+   check("bocadinho", "bocadinh");
+   check("bocado", "boc");
+   check("bocaiúva", "bocaiuv"); // removes diacritic: different from snowball portuguese
+   check("boçal", "bocal"); // removes diacritic: different from snowball portuguese
+   check("bocarra", "bocarr");
+   check("bocas", "boc");
+   check("bode", "bod");
+   check("bodoque", "bodoqu");
+   check("body", "body");
+   check("boeing", "boeing");
+   check("boem", "boem");
+   check("boemia", "boem");
+   check("boêmio", "boemi"); // removes diacritic: different from snowball portuguese
+   check("bogotá", "bogot");
+   check("boi", "boi");
+   check("bóia", "boi"); // removes diacritic: different from snowball portuguese
+   check("boiando", "boi");
+   check("quiabo", "quiab");
+   check("quicaram", "quic");
+   check("quickly", "quickly");
+   check("quieto", "quiet");
+   check("quietos", "quiet");
+   check("quilate", "quilat");
+   check("quilates", "quilat");
+   check("quilinhos", "quilinh");
+   check("quilo", "quil");
+   check("quilombo", "quilomb");
+   check("quilométricas", "quilometr"); // removes diacritic: different from snowball portuguese
+   check("quilométricos", "quilometr"); // removes diacritic: different from snowball portuguese
+   check("quilômetro", "quilometr"); // removes diacritic: different from snowball portoguese
+   check("quilômetros", "quilometr"); // removes diacritic: different from snowball portoguese
+   check("quilos", "quil");
+   check("quimica", "quimic");
+   check("quilos", "quil");
+   check("quimica", "quimic");
+   check("quimicas", "quimic");
+   check("quimico", "quimic");
+   check("quimicos", "quimic");
+   check("quimioterapia", "quimioterap");
+   check("quimioterápicos", "quimioterap"); // removes diacritic: different from snowball portoguese
+   check("quimono", "quimon");
+   check("quincas", "quinc");
+   check("quinhão", "quinha"); // removes diacritic: different from snowball portoguese
+   check("quinhentos", "quinhent");
+   check("quinn", "quinn");
+   check("quino", "quin");
+   check("quinta", "quint");
+   check("quintal", "quintal");
+   check("quintana", "quintan");
+   check("quintanilha", "quintanilh");
+   check("quintão", "quinta"); // removes diacritic: different from snowball portoguese
+   check("quintessência", "quintessente"); // versus snowball portuguese 'quintessent'
+   check("quintino", "quintin");
+   check("quinto", "quint");
+   check("quintos", "quint");
+   check("quintuplicou", "quintuplic");
+   check("quinze", "quinz");
+   check("quinzena", "quinzen");
+   check("quiosque", "quiosqu");
   }
   
   public void testNormalization() throws Exception {
@@ -175,4 +175,4 @@ public class TestBrazilianStemmer extends BaseTokenStreamTestCase {
     };
     checkOneTermReuse(a, "", "");
   }
-}
\ No newline at end of file
+}
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/WordBreakTestUnicode_6_1_0.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/WordBreakTestUnicode_6_1_0.java
index 5608bf7..a9a79a3 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/WordBreakTestUnicode_6_1_0.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/WordBreakTestUnicode_6_1_0.java
@@ -44,3915 +44,3915 @@ import org.junit.Ignore;
 public class WordBreakTestUnicode_6_1_0 extends BaseTokenStreamTestCase {
 
   public void test(Analyzer analyzer) throws Exception {
-    // ÷ 0001 ÷ 0001 ÷	#  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 0001 ÷ 0001 ÷  #  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0001",
                      new String[] {  });
 
-    // ÷ 0001 ? 0308 ÷ 0001 ÷	#  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 0001 ? 0308 ÷ 0001 ÷  #  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u0001",
                      new String[] {  });
 
-    // ÷ 0001 ÷ 000D ÷	#  ÷ [0.2] <START OF HEADING> (Other) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 0001 ÷ 000D ÷  #  ÷ [0.2] <START OF HEADING> (Other) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\r",
                      new String[] {  });
 
-    // ÷ 0001 ? 0308 ÷ 000D ÷	#  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 0001 ? 0308 ÷ 000D ÷  #  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\r",
                      new String[] {  });
 
-    // ÷ 0001 ÷ 000A ÷	#  ÷ [0.2] <START OF HEADING> (Other) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 0001 ÷ 000A ÷  #  ÷ [0.2] <START OF HEADING> (Other) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\n",
                      new String[] {  });
 
-    // ÷ 0001 ? 0308 ÷ 000A ÷	#  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 0001 ? 0308 ÷ 000A ÷  #  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\n",
                      new String[] {  });
 
-    // ÷ 0001 ÷ 000B ÷	#  ÷ [0.2] <START OF HEADING> (Other) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 0001 ÷ 000B ÷  #  ÷ [0.2] <START OF HEADING> (Other) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u000B",
                      new String[] {  });
 
-    // ÷ 0001 ? 0308 ÷ 000B ÷	#  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 0001 ? 0308 ÷ 000B ÷  #  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u000B",
                      new String[] {  });
 
-    // ÷ 0001 ÷ 3031 ÷	#  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 0001 ÷ 3031 ÷  #  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u3031",
                      new String[] { "\u3031" });
 
-    // ÷ 0001 ? 0308 ÷ 3031 ÷	#  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 0001 ? 0308 ÷ 3031 ÷  #  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u3031",
                      new String[] { "\u3031" });
 
-    // ÷ 0001 ÷ 0041 ÷	#  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 0001 ÷ 0041 ÷  #  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0041",
                      new String[] { "\u0041" });
 
-    // ÷ 0001 ? 0308 ÷ 0041 ÷	#  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 0001 ? 0308 ÷ 0041 ÷  #  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u0041",
                      new String[] { "\u0041" });
 
-    // ÷ 0001 ÷ 003A ÷	#  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0001 ÷ 003A ÷  #  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u003A",
                      new String[] {  });
 
-    // ÷ 0001 ? 0308 ÷ 003A ÷	#  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0001 ? 0308 ÷ 003A ÷  #  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u003A",
                      new String[] {  });
 
-    // ÷ 0001 ÷ 002C ÷	#  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0001 ÷ 002C ÷  #  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u002C",
                      new String[] {  });
 
-    // ÷ 0001 ? 0308 ÷ 002C ÷	#  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0001 ? 0308 ÷ 002C ÷  #  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u002C",
                      new String[] {  });
 
-    // ÷ 0001 ÷ 0027 ÷	#  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0001 ÷ 0027 ÷  #  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0027",
                      new String[] {  });
 
-    // ÷ 0001 ? 0308 ÷ 0027 ÷	#  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0001 ? 0308 ÷ 0027 ÷  #  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u0027",
                      new String[] {  });
 
-    // ÷ 0001 ÷ 0030 ÷	#  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 0001 ÷ 0030 ÷  #  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0030",
                      new String[] { "\u0030" });
 
-    // ÷ 0001 ? 0308 ÷ 0030 ÷	#  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 0001 ? 0308 ÷ 0030 ÷  #  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u0030",
                      new String[] { "\u0030" });
 
-    // ÷ 0001 ÷ 005F ÷	#  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 0001 ÷ 005F ÷  #  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u005F",
                      new String[] {  });
 
-    // ÷ 0001 ? 0308 ÷ 005F ÷	#  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 0001 ? 0308 ÷ 005F ÷  #  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u005F",
                      new String[] {  });
 
-    // ÷ 0001 ? 00AD ÷	#  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 0001 ? 00AD ÷  #  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u00AD",
                      new String[] {  });
 
-    // ÷ 0001 ? 0308 ? 00AD ÷	#  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 0001 ? 0308 ? 00AD ÷  #  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u00AD",
                      new String[] {  });
 
-    // ÷ 0001 ? 0300 ÷	#  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 0001 ? 0300 ÷  #  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0300",
                      new String[] {  });
 
-    // ÷ 0001 ? 0308 ? 0300 ÷	#  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 0001 ? 0308 ? 0300 ÷  #  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u0300",
                      new String[] {  });
 
-    // ÷ 0001 ÷ 0061 ? 2060 ÷	#  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0001 ÷ 0061 ? 2060 ÷  #  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    // ÷ 0001 ? 0308 ÷ 0061 ? 2060 ÷	#  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0001 ? 0308 ÷ 0061 ? 2060 ÷  #  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    // ÷ 0001 ÷ 0061 ÷ 003A ÷	#  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0001 ÷ 0061 ÷ 003A ÷  #  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0061\u003A",
                      new String[] { "\u0061" });
 
-    // ÷ 0001 ? 0308 ÷ 0061 ÷ 003A ÷	#  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0001 ? 0308 ÷ 0061 ÷ 003A ÷  #  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u0061\u003A",
                      new String[] { "\u0061" });
 
-    // ÷ 0001 ÷ 0061 ÷ 0027 ÷	#  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0001 ÷ 0061 ÷ 0027 ÷  #  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0061\u0027",
                      new String[] { "\u0061" });
 
-    // ÷ 0001 ? 0308 ÷ 0061 ÷ 0027 ÷	#  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0001 ? 0308 ÷ 0061 ÷ 0027 ÷  #  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u0061\u0027",
                      new String[] { "\u0061" });
 
-    // ÷ 0001 ÷ 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0001 ÷ 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    // ÷ 0001 ? 0308 ÷ 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0001 ? 0308 ÷ 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    // ÷ 0001 ÷ 0061 ÷ 002C ÷	#  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0001 ÷ 0061 ÷ 002C ÷  #  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0061\u002C",
                      new String[] { "\u0061" });
 
-    // ÷ 0001 ? 0308 ÷ 0061 ÷ 002C ÷	#  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0001 ? 0308 ÷ 0061 ÷ 002C ÷  #  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u0061\u002C",
                      new String[] { "\u0061" });
 
-    // ÷ 0001 ÷ 0031 ÷ 003A ÷	#  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0001 ÷ 0031 ÷ 003A ÷  #  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0031\u003A",
                      new String[] { "\u0031" });
 
-    // ÷ 0001 ? 0308 ÷ 0031 ÷ 003A ÷	#  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0001 ? 0308 ÷ 0031 ÷ 003A ÷  #  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u0031\u003A",
                      new String[] { "\u0031" });
 
-    // ÷ 0001 ÷ 0031 ÷ 0027 ÷	#  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0001 ÷ 0031 ÷ 0027 ÷  #  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0031\u0027",
                      new String[] { "\u0031" });
 
-    // ÷ 0001 ? 0308 ÷ 0031 ÷ 0027 ÷	#  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0001 ? 0308 ÷ 0031 ÷ 0027 ÷  #  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u0031\u0027",
                      new String[] { "\u0031" });
 
-    // ÷ 0001 ÷ 0031 ÷ 002C ÷	#  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0001 ÷ 0031 ÷ 002C ÷  #  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0031\u002C",
                      new String[] { "\u0031" });
 
-    // ÷ 0001 ? 0308 ÷ 0031 ÷ 002C ÷	#  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0001 ? 0308 ÷ 0031 ÷ 002C ÷  #  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u0031\u002C",
                      new String[] { "\u0031" });
 
-    // ÷ 0001 ÷ 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0001 ÷ 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] <START OF HEADING> (Other) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    // ÷ 0001 ? 0308 ÷ 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0001 ? 0308 ÷ 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] <START OF HEADING> (Other) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    // ÷ 000D ÷ 0001 ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 000D ÷ 0001 ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0001",
                      new String[] {  });
 
-    // ÷ 000D ÷ 0308 ÷ 0001 ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 000D ÷ 0308 ÷ 0001 ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u0001",
                      new String[] {  });
 
-    // ÷ 000D ÷ 000D ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 000D ÷ 000D ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\r",
                      new String[] {  });
 
-    // ÷ 000D ÷ 0308 ÷ 000D ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 000D ÷ 0308 ÷ 000D ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\r",
                      new String[] {  });
 
-    // ÷ 000D ? 000A ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ? [3.0] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 000D ? 000A ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ? [3.0] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\n",
                      new String[] {  });
 
-    // ÷ 000D ÷ 0308 ÷ 000A ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 000D ÷ 0308 ÷ 000A ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\n",
                      new String[] {  });
 
-    // ÷ 000D ÷ 000B ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 000D ÷ 000B ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u000B",
                      new String[] {  });
 
-    // ÷ 000D ÷ 0308 ÷ 000B ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 000D ÷ 0308 ÷ 000B ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u000B",
                      new String[] {  });
 
-    // ÷ 000D ÷ 3031 ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 000D ÷ 3031 ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u3031",
                      new String[] { "\u3031" });
 
-    // ÷ 000D ÷ 0308 ÷ 3031 ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 000D ÷ 0308 ÷ 3031 ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u3031",
                      new String[] { "\u3031" });
 
-    // ÷ 000D ÷ 0041 ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 000D ÷ 0041 ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0041",
                      new String[] { "\u0041" });
 
-    // ÷ 000D ÷ 0308 ÷ 0041 ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 000D ÷ 0308 ÷ 0041 ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u0041",
                      new String[] { "\u0041" });
 
-    // ÷ 000D ÷ 003A ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COLON (MidLetter) ÷ [0.3]
+    // ÷ 000D ÷ 003A ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u003A",
                      new String[] {  });
 
-    // ÷ 000D ÷ 0308 ÷ 003A ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 000D ÷ 0308 ÷ 003A ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u003A",
                      new String[] {  });
 
-    // ÷ 000D ÷ 002C ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMMA (MidNum) ÷ [0.3]
+    // ÷ 000D ÷ 002C ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u002C",
                      new String[] {  });
 
-    // ÷ 000D ÷ 0308 ÷ 002C ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 000D ÷ 0308 ÷ 002C ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u002C",
                      new String[] {  });
 
-    // ÷ 000D ÷ 0027 ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 000D ÷ 0027 ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0027",
                      new String[] {  });
 
-    // ÷ 000D ÷ 0308 ÷ 0027 ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 000D ÷ 0308 ÷ 0027 ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u0027",
                      new String[] {  });
 
-    // ÷ 000D ÷ 0030 ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 000D ÷ 0030 ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0030",
                      new String[] { "\u0030" });
 
-    // ÷ 000D ÷ 0308 ÷ 0030 ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 000D ÷ 0308 ÷ 0030 ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u0030",
                      new String[] { "\u0030" });
 
-    // ÷ 000D ÷ 005F ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 000D ÷ 005F ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u005F",
                      new String[] {  });
 
-    // ÷ 000D ÷ 0308 ÷ 005F ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 000D ÷ 0308 ÷ 005F ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u005F",
                      new String[] {  });
 
-    // ÷ 000D ÷ 00AD ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 000D ÷ 00AD ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u00AD",
                      new String[] {  });
 
-    // ÷ 000D ÷ 0308 ? 00AD ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 000D ÷ 0308 ? 00AD ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u00AD",
                      new String[] {  });
 
-    // ÷ 000D ÷ 0300 ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 000D ÷ 0300 ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0300",
                      new String[] {  });
 
-    // ÷ 000D ÷ 0308 ? 0300 ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 000D ÷ 0308 ? 0300 ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u0300",
                      new String[] {  });
 
-    // ÷ 000D ÷ 0061 ? 2060 ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 000D ÷ 0061 ? 2060 ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    // ÷ 000D ÷ 0308 ÷ 0061 ? 2060 ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 000D ÷ 0308 ÷ 0061 ? 2060 ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    // ÷ 000D ÷ 0061 ÷ 003A ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 000D ÷ 0061 ÷ 003A ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0061\u003A",
                      new String[] { "\u0061" });
 
-    // ÷ 000D ÷ 0308 ÷ 0061 ÷ 003A ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 000D ÷ 0308 ÷ 0061 ÷ 003A ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u0061\u003A",
                      new String[] { "\u0061" });
 
-    // ÷ 000D ÷ 0061 ÷ 0027 ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 000D ÷ 0061 ÷ 0027 ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0061\u0027",
                      new String[] { "\u0061" });
 
-    // ÷ 000D ÷ 0308 ÷ 0061 ÷ 0027 ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 000D ÷ 0308 ÷ 0061 ÷ 0027 ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u0061\u0027",
                      new String[] { "\u0061" });
 
-    // ÷ 000D ÷ 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 000D ÷ 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    // ÷ 000D ÷ 0308 ÷ 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 000D ÷ 0308 ÷ 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    // ÷ 000D ÷ 0061 ÷ 002C ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 000D ÷ 0061 ÷ 002C ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0061\u002C",
                      new String[] { "\u0061" });
 
-    // ÷ 000D ÷ 0308 ÷ 0061 ÷ 002C ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 000D ÷ 0308 ÷ 0061 ÷ 002C ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u0061\u002C",
                      new String[] { "\u0061" });
 
-    // ÷ 000D ÷ 0031 ÷ 003A ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 000D ÷ 0031 ÷ 003A ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0031\u003A",
                      new String[] { "\u0031" });
 
-    // ÷ 000D ÷ 0308 ÷ 0031 ÷ 003A ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 000D ÷ 0308 ÷ 0031 ÷ 003A ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u0031\u003A",
                      new String[] { "\u0031" });
 
-    // ÷ 000D ÷ 0031 ÷ 0027 ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 000D ÷ 0031 ÷ 0027 ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0031\u0027",
                      new String[] { "\u0031" });
 
-    // ÷ 000D ÷ 0308 ÷ 0031 ÷ 0027 ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 000D ÷ 0308 ÷ 0031 ÷ 0027 ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u0031\u0027",
                      new String[] { "\u0031" });
 
-    // ÷ 000D ÷ 0031 ÷ 002C ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 000D ÷ 0031 ÷ 002C ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0031\u002C",
                      new String[] { "\u0031" });
 
-    // ÷ 000D ÷ 0308 ÷ 0031 ÷ 002C ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 000D ÷ 0308 ÷ 0031 ÷ 002C ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u0031\u002C",
                      new String[] { "\u0031" });
 
-    // ÷ 000D ÷ 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 000D ÷ 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    // ÷ 000D ÷ 0308 ÷ 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 000D ÷ 0308 ÷ 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] <CARRIAGE RETURN (CR)> (CR) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    // ÷ 000A ÷ 0001 ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 000A ÷ 0001 ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0001",
                      new String[] {  });
 
-    // ÷ 000A ÷ 0308 ÷ 0001 ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 000A ÷ 0308 ÷ 0001 ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u0001",
                      new String[] {  });
 
-    // ÷ 000A ÷ 000D ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 000A ÷ 000D ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\r",
                      new String[] {  });
 
-    // ÷ 000A ÷ 0308 ÷ 000D ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 000A ÷ 0308 ÷ 000D ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\r",
                      new String[] {  });
 
-    // ÷ 000A ÷ 000A ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 000A ÷ 000A ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\n",
                      new String[] {  });
 
-    // ÷ 000A ÷ 0308 ÷ 000A ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 000A ÷ 0308 ÷ 000A ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\n",
                      new String[] {  });
 
-    // ÷ 000A ÷ 000B ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 000A ÷ 000B ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u000B",
                      new String[] {  });
 
-    // ÷ 000A ÷ 0308 ÷ 000B ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 000A ÷ 0308 ÷ 000B ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u000B",
                      new String[] {  });
 
-    // ÷ 000A ÷ 3031 ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 000A ÷ 3031 ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u3031",
                      new String[] { "\u3031" });
 
-    // ÷ 000A ÷ 0308 ÷ 3031 ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 000A ÷ 0308 ÷ 3031 ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u3031",
                      new String[] { "\u3031" });
 
-    // ÷ 000A ÷ 0041 ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 000A ÷ 0041 ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0041",
                      new String[] { "\u0041" });
 
-    // ÷ 000A ÷ 0308 ÷ 0041 ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 000A ÷ 0308 ÷ 0041 ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u0041",
                      new String[] { "\u0041" });
 
-    // ÷ 000A ÷ 003A ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COLON (MidLetter) ÷ [0.3]
+    // ÷ 000A ÷ 003A ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u003A",
                      new String[] {  });
 
-    // ÷ 000A ÷ 0308 ÷ 003A ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 000A ÷ 0308 ÷ 003A ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u003A",
                      new String[] {  });
 
-    // ÷ 000A ÷ 002C ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMMA (MidNum) ÷ [0.3]
+    // ÷ 000A ÷ 002C ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u002C",
                      new String[] {  });
 
-    // ÷ 000A ÷ 0308 ÷ 002C ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 000A ÷ 0308 ÷ 002C ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u002C",
                      new String[] {  });
 
-    // ÷ 000A ÷ 0027 ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 000A ÷ 0027 ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0027",
                      new String[] {  });
 
-    // ÷ 000A ÷ 0308 ÷ 0027 ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 000A ÷ 0308 ÷ 0027 ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u0027",
                      new String[] {  });
 
-    // ÷ 000A ÷ 0030 ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 000A ÷ 0030 ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0030",
                      new String[] { "\u0030" });
 
-    // ÷ 000A ÷ 0308 ÷ 0030 ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 000A ÷ 0308 ÷ 0030 ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u0030",
                      new String[] { "\u0030" });
 
-    // ÷ 000A ÷ 005F ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 000A ÷ 005F ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u005F",
                      new String[] {  });
 
-    // ÷ 000A ÷ 0308 ÷ 005F ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 000A ÷ 0308 ÷ 005F ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u005F",
                      new String[] {  });
 
-    // ÷ 000A ÷ 00AD ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 000A ÷ 00AD ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u00AD",
                      new String[] {  });
 
-    // ÷ 000A ÷ 0308 ? 00AD ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 000A ÷ 0308 ? 00AD ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u00AD",
                      new String[] {  });
 
-    // ÷ 000A ÷ 0300 ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 000A ÷ 0300 ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0300",
                      new String[] {  });
 
-    // ÷ 000A ÷ 0308 ? 0300 ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 000A ÷ 0308 ? 0300 ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u0300",
                      new String[] {  });
 
-    // ÷ 000A ÷ 0061 ? 2060 ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 000A ÷ 0061 ? 2060 ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    // ÷ 000A ÷ 0308 ÷ 0061 ? 2060 ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 000A ÷ 0308 ÷ 0061 ? 2060 ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    // ÷ 000A ÷ 0061 ÷ 003A ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 000A ÷ 0061 ÷ 003A ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0061\u003A",
                      new String[] { "\u0061" });
 
-    // ÷ 000A ÷ 0308 ÷ 0061 ÷ 003A ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 000A ÷ 0308 ÷ 0061 ÷ 003A ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u0061\u003A",
                      new String[] { "\u0061" });
 
-    // ÷ 000A ÷ 0061 ÷ 0027 ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 000A ÷ 0061 ÷ 0027 ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0061\u0027",
                      new String[] { "\u0061" });
 
-    // ÷ 000A ÷ 0308 ÷ 0061 ÷ 0027 ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 000A ÷ 0308 ÷ 0061 ÷ 0027 ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u0061\u0027",
                      new String[] { "\u0061" });
 
-    // ÷ 000A ÷ 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 000A ÷ 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    // ÷ 000A ÷ 0308 ÷ 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 000A ÷ 0308 ÷ 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    // ÷ 000A ÷ 0061 ÷ 002C ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 000A ÷ 0061 ÷ 002C ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0061\u002C",
                      new String[] { "\u0061" });
 
-    // ÷ 000A ÷ 0308 ÷ 0061 ÷ 002C ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 000A ÷ 0308 ÷ 0061 ÷ 002C ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u0061\u002C",
                      new String[] { "\u0061" });
 
-    // ÷ 000A ÷ 0031 ÷ 003A ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 000A ÷ 0031 ÷ 003A ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0031\u003A",
                      new String[] { "\u0031" });
 
-    // ÷ 000A ÷ 0308 ÷ 0031 ÷ 003A ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 000A ÷ 0308 ÷ 0031 ÷ 003A ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u0031\u003A",
                      new String[] { "\u0031" });
 
-    // ÷ 000A ÷ 0031 ÷ 0027 ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 000A ÷ 0031 ÷ 0027 ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0031\u0027",
                      new String[] { "\u0031" });
 
-    // ÷ 000A ÷ 0308 ÷ 0031 ÷ 0027 ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 000A ÷ 0308 ÷ 0031 ÷ 0027 ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u0031\u0027",
                      new String[] { "\u0031" });
 
-    // ÷ 000A ÷ 0031 ÷ 002C ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 000A ÷ 0031 ÷ 002C ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0031\u002C",
                      new String[] { "\u0031" });
 
-    // ÷ 000A ÷ 0308 ÷ 0031 ÷ 002C ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 000A ÷ 0308 ÷ 0031 ÷ 002C ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u0031\u002C",
                      new String[] { "\u0031" });
 
-    // ÷ 000A ÷ 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 000A ÷ 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    // ÷ 000A ÷ 0308 ÷ 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 000A ÷ 0308 ÷ 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] <LINE FEED (LF)> (LF) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    // ÷ 000B ÷ 0001 ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 000B ÷ 0001 ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0001",
                      new String[] {  });
 
-    // ÷ 000B ÷ 0308 ÷ 0001 ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 000B ÷ 0308 ÷ 0001 ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u0001",
                      new String[] {  });
 
-    // ÷ 000B ÷ 000D ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 000B ÷ 000D ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\r",
                      new String[] {  });
 
-    // ÷ 000B ÷ 0308 ÷ 000D ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 000B ÷ 0308 ÷ 000D ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\r",
                      new String[] {  });
 
-    // ÷ 000B ÷ 000A ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 000B ÷ 000A ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\n",
                      new String[] {  });
 
-    // ÷ 000B ÷ 0308 ÷ 000A ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 000B ÷ 0308 ÷ 000A ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\n",
                      new String[] {  });
 
-    // ÷ 000B ÷ 000B ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 000B ÷ 000B ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u000B",
                      new String[] {  });
 
-    // ÷ 000B ÷ 0308 ÷ 000B ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 000B ÷ 0308 ÷ 000B ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u000B",
                      new String[] {  });
 
-    // ÷ 000B ÷ 3031 ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 000B ÷ 3031 ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u3031",
                      new String[] { "\u3031" });
 
-    // ÷ 000B ÷ 0308 ÷ 3031 ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 000B ÷ 0308 ÷ 3031 ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u3031",
                      new String[] { "\u3031" });
 
-    // ÷ 000B ÷ 0041 ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 000B ÷ 0041 ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0041",
                      new String[] { "\u0041" });
 
-    // ÷ 000B ÷ 0308 ÷ 0041 ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 000B ÷ 0308 ÷ 0041 ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u0041",
                      new String[] { "\u0041" });
 
-    // ÷ 000B ÷ 003A ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COLON (MidLetter) ÷ [0.3]
+    // ÷ 000B ÷ 003A ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u003A",
                      new String[] {  });
 
-    // ÷ 000B ÷ 0308 ÷ 003A ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 000B ÷ 0308 ÷ 003A ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u003A",
                      new String[] {  });
 
-    // ÷ 000B ÷ 002C ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMMA (MidNum) ÷ [0.3]
+    // ÷ 000B ÷ 002C ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u002C",
                      new String[] {  });
 
-    // ÷ 000B ÷ 0308 ÷ 002C ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 000B ÷ 0308 ÷ 002C ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u002C",
                      new String[] {  });
 
-    // ÷ 000B ÷ 0027 ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 000B ÷ 0027 ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0027",
                      new String[] {  });
 
-    // ÷ 000B ÷ 0308 ÷ 0027 ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 000B ÷ 0308 ÷ 0027 ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u0027",
                      new String[] {  });
 
-    // ÷ 000B ÷ 0030 ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 000B ÷ 0030 ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0030",
                      new String[] { "\u0030" });
 
-    // ÷ 000B ÷ 0308 ÷ 0030 ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 000B ÷ 0308 ÷ 0030 ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u0030",
                      new String[] { "\u0030" });
 
-    // ÷ 000B ÷ 005F ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 000B ÷ 005F ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u005F",
                      new String[] {  });
 
-    // ÷ 000B ÷ 0308 ÷ 005F ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 000B ÷ 0308 ÷ 005F ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u005F",
                      new String[] {  });
 
-    // ÷ 000B ÷ 00AD ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 000B ÷ 00AD ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u00AD",
                      new String[] {  });
 
-    // ÷ 000B ÷ 0308 ? 00AD ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 000B ÷ 0308 ? 00AD ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u00AD",
                      new String[] {  });
 
-    // ÷ 000B ÷ 0300 ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 000B ÷ 0300 ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0300",
                      new String[] {  });
 
-    // ÷ 000B ÷ 0308 ? 0300 ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 000B ÷ 0308 ? 0300 ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u0300",
                      new String[] {  });
 
-    // ÷ 000B ÷ 0061 ? 2060 ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 000B ÷ 0061 ? 2060 ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    // ÷ 000B ÷ 0308 ÷ 0061 ? 2060 ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 000B ÷ 0308 ÷ 0061 ? 2060 ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    // ÷ 000B ÷ 0061 ÷ 003A ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 000B ÷ 0061 ÷ 003A ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0061\u003A",
                      new String[] { "\u0061" });
 
-    // ÷ 000B ÷ 0308 ÷ 0061 ÷ 003A ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 000B ÷ 0308 ÷ 0061 ÷ 003A ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u0061\u003A",
                      new String[] { "\u0061" });
 
-    // ÷ 000B ÷ 0061 ÷ 0027 ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 000B ÷ 0061 ÷ 0027 ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0061\u0027",
                      new String[] { "\u0061" });
 
-    // ÷ 000B ÷ 0308 ÷ 0061 ÷ 0027 ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 000B ÷ 0308 ÷ 0061 ÷ 0027 ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u0061\u0027",
                      new String[] { "\u0061" });
 
-    // ÷ 000B ÷ 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 000B ÷ 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    // ÷ 000B ÷ 0308 ÷ 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 000B ÷ 0308 ÷ 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    // ÷ 000B ÷ 0061 ÷ 002C ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 000B ÷ 0061 ÷ 002C ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0061\u002C",
                      new String[] { "\u0061" });
 
-    // ÷ 000B ÷ 0308 ÷ 0061 ÷ 002C ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 000B ÷ 0308 ÷ 0061 ÷ 002C ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u0061\u002C",
                      new String[] { "\u0061" });
 
-    // ÷ 000B ÷ 0031 ÷ 003A ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 000B ÷ 0031 ÷ 003A ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0031\u003A",
                      new String[] { "\u0031" });
 
-    // ÷ 000B ÷ 0308 ÷ 0031 ÷ 003A ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 000B ÷ 0308 ÷ 0031 ÷ 003A ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u0031\u003A",
                      new String[] { "\u0031" });
 
-    // ÷ 000B ÷ 0031 ÷ 0027 ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 000B ÷ 0031 ÷ 0027 ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0031\u0027",
                      new String[] { "\u0031" });
 
-    // ÷ 000B ÷ 0308 ÷ 0031 ÷ 0027 ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 000B ÷ 0308 ÷ 0031 ÷ 0027 ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u0031\u0027",
                      new String[] { "\u0031" });
 
-    // ÷ 000B ÷ 0031 ÷ 002C ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 000B ÷ 0031 ÷ 002C ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0031\u002C",
                      new String[] { "\u0031" });
 
-    // ÷ 000B ÷ 0308 ÷ 0031 ÷ 002C ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 000B ÷ 0308 ÷ 0031 ÷ 002C ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u0031\u002C",
                      new String[] { "\u0031" });
 
-    // ÷ 000B ÷ 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 000B ÷ 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    // ÷ 000B ÷ 0308 ÷ 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 000B ÷ 0308 ÷ 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] <LINE TABULATION> (Newline) ÷ [3.1] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    // ÷ 3031 ÷ 0001 ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 3031 ÷ 0001 ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0001",
                      new String[] { "\u3031" });
 
-    // ÷ 3031 ? 0308 ÷ 0001 ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 3031 ? 0308 ÷ 0001 ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u0001",
                      new String[] { "\u3031\u0308" });
 
-    // ÷ 3031 ÷ 000D ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 3031 ÷ 000D ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\r",
                      new String[] { "\u3031" });
 
-    // ÷ 3031 ? 0308 ÷ 000D ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 3031 ? 0308 ÷ 000D ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\r",
                      new String[] { "\u3031\u0308" });
 
-    // ÷ 3031 ÷ 000A ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 3031 ÷ 000A ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\n",
                      new String[] { "\u3031" });
 
-    // ÷ 3031 ? 0308 ÷ 000A ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 3031 ? 0308 ÷ 000A ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\n",
                      new String[] { "\u3031\u0308" });
 
-    // ÷ 3031 ÷ 000B ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 3031 ÷ 000B ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u000B",
                      new String[] { "\u3031" });
 
-    // ÷ 3031 ? 0308 ÷ 000B ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 3031 ? 0308 ÷ 000B ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u000B",
                      new String[] { "\u3031\u0308" });
 
-    // ÷ 3031 ? 3031 ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [13.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 3031 ? 3031 ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [13.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u3031",
                      new String[] { "\u3031\u3031" });
 
-    // ÷ 3031 ? 0308 ? 3031 ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 3031 ? 0308 ? 3031 ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u3031",
                      new String[] { "\u3031\u0308\u3031" });
 
-    // ÷ 3031 ÷ 0041 ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 3031 ÷ 0041 ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0041",
                      new String[] { "\u3031", "\u0041" });
 
-    // ÷ 3031 ? 0308 ÷ 0041 ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 3031 ? 0308 ÷ 0041 ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u0041",
                      new String[] { "\u3031\u0308", "\u0041" });
 
-    // ÷ 3031 ÷ 003A ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 3031 ÷ 003A ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u003A",
                      new String[] { "\u3031" });
 
-    // ÷ 3031 ? 0308 ÷ 003A ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 3031 ? 0308 ÷ 003A ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u003A",
                      new String[] { "\u3031\u0308" });
 
-    // ÷ 3031 ÷ 002C ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 3031 ÷ 002C ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u002C",
                      new String[] { "\u3031" });
 
-    // ÷ 3031 ? 0308 ÷ 002C ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 3031 ? 0308 ÷ 002C ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u002C",
                      new String[] { "\u3031\u0308" });
 
-    // ÷ 3031 ÷ 0027 ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 3031 ÷ 0027 ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0027",
                      new String[] { "\u3031" });
 
-    // ÷ 3031 ? 0308 ÷ 0027 ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 3031 ? 0308 ÷ 0027 ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u0027",
                      new String[] { "\u3031\u0308" });
 
-    // ÷ 3031 ÷ 0030 ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 3031 ÷ 0030 ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0030",
                      new String[] { "\u3031", "\u0030" });
 
-    // ÷ 3031 ? 0308 ÷ 0030 ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 3031 ? 0308 ÷ 0030 ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u0030",
                      new String[] { "\u3031\u0308", "\u0030" });
 
-    // ÷ 3031 ? 005F ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [13.1] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 3031 ? 005F ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [13.1] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u005F",
                      new String[] { "\u3031\u005F" });
 
-    // ÷ 3031 ? 0308 ? 005F ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.1] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 3031 ? 0308 ? 005F ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.1] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u005F",
                      new String[] { "\u3031\u0308\u005F" });
 
-    // ÷ 3031 ? 00AD ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 3031 ? 00AD ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u00AD",
                      new String[] { "\u3031\u00AD" });
 
-    // ÷ 3031 ? 0308 ? 00AD ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 3031 ? 0308 ? 00AD ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u00AD",
                      new String[] { "\u3031\u0308\u00AD" });
 
-    // ÷ 3031 ? 0300 ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 3031 ? 0300 ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0300",
                      new String[] { "\u3031\u0300" });
 
-    // ÷ 3031 ? 0308 ? 0300 ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 3031 ? 0308 ? 0300 ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u0300",
                      new String[] { "\u3031\u0308\u0300" });
 
-    // ÷ 3031 ÷ 0061 ? 2060 ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 3031 ÷ 0061 ? 2060 ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0061\u2060",
                      new String[] { "\u3031", "\u0061\u2060" });
 
-    // ÷ 3031 ? 0308 ÷ 0061 ? 2060 ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 3031 ? 0308 ÷ 0061 ? 2060 ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u0061\u2060",
                      new String[] { "\u3031\u0308", "\u0061\u2060" });
 
-    // ÷ 3031 ÷ 0061 ÷ 003A ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 3031 ÷ 0061 ÷ 003A ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0061\u003A",
                      new String[] { "\u3031", "\u0061" });
 
-    // ÷ 3031 ? 0308 ÷ 0061 ÷ 003A ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 3031 ? 0308 ÷ 0061 ÷ 003A ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u0061\u003A",
                      new String[] { "\u3031\u0308", "\u0061" });
 
-    // ÷ 3031 ÷ 0061 ÷ 0027 ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 3031 ÷ 0061 ÷ 0027 ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0061\u0027",
                      new String[] { "\u3031", "\u0061" });
 
-    // ÷ 3031 ? 0308 ÷ 0061 ÷ 0027 ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 3031 ? 0308 ÷ 0061 ÷ 0027 ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u0061\u0027",
                      new String[] { "\u3031\u0308", "\u0061" });
 
-    // ÷ 3031 ÷ 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 3031 ÷ 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0061\u0027\u2060",
                      new String[] { "\u3031", "\u0061" });
 
-    // ÷ 3031 ? 0308 ÷ 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 3031 ? 0308 ÷ 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u0061\u0027\u2060",
                      new String[] { "\u3031\u0308", "\u0061" });
 
-    // ÷ 3031 ÷ 0061 ÷ 002C ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 3031 ÷ 0061 ÷ 002C ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0061\u002C",
                      new String[] { "\u3031", "\u0061" });
 
-    // ÷ 3031 ? 0308 ÷ 0061 ÷ 002C ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 3031 ? 0308 ÷ 0061 ÷ 002C ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u0061\u002C",
                      new String[] { "\u3031\u0308", "\u0061" });
 
-    // ÷ 3031 ÷ 0031 ÷ 003A ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 3031 ÷ 0031 ÷ 003A ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0031\u003A",
                      new String[] { "\u3031", "\u0031" });
 
-    // ÷ 3031 ? 0308 ÷ 0031 ÷ 003A ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 3031 ? 0308 ÷ 0031 ÷ 003A ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u0031\u003A",
                      new String[] { "\u3031\u0308", "\u0031" });
 
-    // ÷ 3031 ÷ 0031 ÷ 0027 ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 3031 ÷ 0031 ÷ 0027 ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0031\u0027",
                      new String[] { "\u3031", "\u0031" });
 
-    // ÷ 3031 ? 0308 ÷ 0031 ÷ 0027 ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 3031 ? 0308 ÷ 0031 ÷ 0027 ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u0031\u0027",
                      new String[] { "\u3031\u0308", "\u0031" });
 
-    // ÷ 3031 ÷ 0031 ÷ 002C ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 3031 ÷ 0031 ÷ 002C ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0031\u002C",
                      new String[] { "\u3031", "\u0031" });
 
-    // ÷ 3031 ? 0308 ÷ 0031 ÷ 002C ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 3031 ? 0308 ÷ 0031 ÷ 002C ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u0031\u002C",
                      new String[] { "\u3031\u0308", "\u0031" });
 
-    // ÷ 3031 ÷ 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 3031 ÷ 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0031\u002E\u2060",
                      new String[] { "\u3031", "\u0031" });
 
-    // ÷ 3031 ? 0308 ÷ 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 3031 ? 0308 ÷ 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] VERTICAL KANA REPEAT MARK (Katakana) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u0031\u002E\u2060",
                      new String[] { "\u3031\u0308", "\u0031" });
 
-    // ÷ 0041 ÷ 0001 ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 0041 ÷ 0001 ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0001",
                      new String[] { "\u0041" });
 
-    // ÷ 0041 ? 0308 ÷ 0001 ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 0041 ? 0308 ÷ 0001 ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u0001",
                      new String[] { "\u0041\u0308" });
 
-    // ÷ 0041 ÷ 000D ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 0041 ÷ 000D ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\r",
                      new String[] { "\u0041" });
 
-    // ÷ 0041 ? 0308 ÷ 000D ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 0041 ? 0308 ÷ 000D ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\r",
                      new String[] { "\u0041\u0308" });
 
-    // ÷ 0041 ÷ 000A ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 0041 ÷ 000A ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\n",
                      new String[] { "\u0041" });
 
-    // ÷ 0041 ? 0308 ÷ 000A ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 0041 ? 0308 ÷ 000A ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\n",
                      new String[] { "\u0041\u0308" });
 
-    // ÷ 0041 ÷ 000B ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 0041 ÷ 000B ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u000B",
                      new String[] { "\u0041" });
 
-    // ÷ 0041 ? 0308 ÷ 000B ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 0041 ? 0308 ÷ 000B ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u000B",
                      new String[] { "\u0041\u0308" });
 
-    // ÷ 0041 ÷ 3031 ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 0041 ÷ 3031 ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u3031",
                      new String[] { "\u0041", "\u3031" });
 
-    // ÷ 0041 ? 0308 ÷ 3031 ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 0041 ? 0308 ÷ 3031 ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u3031",
                      new String[] { "\u0041\u0308", "\u3031" });
 
-    // ÷ 0041 ? 0041 ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [5.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 0041 ? 0041 ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [5.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0041",
                      new String[] { "\u0041\u0041" });
 
-    // ÷ 0041 ? 0308 ? 0041 ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [5.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 0041 ? 0308 ? 0041 ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [5.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u0041",
                      new String[] { "\u0041\u0308\u0041" });
 
-    // ÷ 0041 ÷ 003A ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0041 ÷ 003A ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u003A",
                      new String[] { "\u0041" });
 
-    // ÷ 0041 ? 0308 ÷ 003A ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0041 ? 0308 ÷ 003A ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u003A",
                      new String[] { "\u0041\u0308" });
 
-    // ÷ 0041 ÷ 002C ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0041 ÷ 002C ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u002C",
                      new String[] { "\u0041" });
 
-    // ÷ 0041 ? 0308 ÷ 002C ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0041 ? 0308 ÷ 002C ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u002C",
                      new String[] { "\u0041\u0308" });
 
-    // ÷ 0041 ÷ 0027 ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0041 ÷ 0027 ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0027",
                      new String[] { "\u0041" });
 
-    // ÷ 0041 ? 0308 ÷ 0027 ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0041 ? 0308 ÷ 0027 ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u0027",
                      new String[] { "\u0041\u0308" });
 
-    // ÷ 0041 ? 0030 ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [9.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 0041 ? 0030 ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [9.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0030",
                      new String[] { "\u0041\u0030" });
 
-    // ÷ 0041 ? 0308 ? 0030 ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [9.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 0041 ? 0308 ? 0030 ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [9.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u0030",
                      new String[] { "\u0041\u0308\u0030" });
 
-    // ÷ 0041 ? 005F ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [13.1] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 0041 ? 005F ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [13.1] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u005F",
                      new String[] { "\u0041\u005F" });
 
-    // ÷ 0041 ? 0308 ? 005F ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.1] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 0041 ? 0308 ? 005F ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.1] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u005F",
                      new String[] { "\u0041\u0308\u005F" });
 
-    // ÷ 0041 ? 00AD ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 0041 ? 00AD ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u00AD",
                      new String[] { "\u0041\u00AD" });
 
-    // ÷ 0041 ? 0308 ? 00AD ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 0041 ? 0308 ? 00AD ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u00AD",
                      new String[] { "\u0041\u0308\u00AD" });
 
-    // ÷ 0041 ? 0300 ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 0041 ? 0300 ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0300",
                      new String[] { "\u0041\u0300" });
 
-    // ÷ 0041 ? 0308 ? 0300 ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 0041 ? 0308 ? 0300 ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u0300",
                      new String[] { "\u0041\u0308\u0300" });
 
-    // ÷ 0041 ? 0061 ? 2060 ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [5.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0041 ? 0061 ? 2060 ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [5.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0061\u2060",
                      new String[] { "\u0041\u0061\u2060" });
 
-    // ÷ 0041 ? 0308 ? 0061 ? 2060 ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0041 ? 0308 ? 0061 ? 2060 ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u0061\u2060",
                      new String[] { "\u0041\u0308\u0061\u2060" });
 
-    // ÷ 0041 ? 0061 ÷ 003A ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [5.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0041 ? 0061 ÷ 003A ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [5.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0061\u003A",
                      new String[] { "\u0041\u0061" });
 
-    // ÷ 0041 ? 0308 ? 0061 ÷ 003A ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0041 ? 0308 ? 0061 ÷ 003A ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u0061\u003A",
                      new String[] { "\u0041\u0308\u0061" });
 
-    // ÷ 0041 ? 0061 ÷ 0027 ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [5.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0041 ? 0061 ÷ 0027 ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [5.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0061\u0027",
                      new String[] { "\u0041\u0061" });
 
-    // ÷ 0041 ? 0308 ? 0061 ÷ 0027 ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0041 ? 0308 ? 0061 ÷ 0027 ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u0061\u0027",
                      new String[] { "\u0041\u0308\u0061" });
 
-    // ÷ 0041 ? 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [5.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0041 ? 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [5.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0061\u0027\u2060",
                      new String[] { "\u0041\u0061" });
 
-    // ÷ 0041 ? 0308 ? 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0041 ? 0308 ? 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u0061\u0027\u2060",
                      new String[] { "\u0041\u0308\u0061" });
 
-    // ÷ 0041 ? 0061 ÷ 002C ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [5.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0041 ? 0061 ÷ 002C ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [5.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0061\u002C",
                      new String[] { "\u0041\u0061" });
 
-    // ÷ 0041 ? 0308 ? 0061 ÷ 002C ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0041 ? 0308 ? 0061 ÷ 002C ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u0061\u002C",
                      new String[] { "\u0041\u0308\u0061" });
 
-    // ÷ 0041 ? 0031 ÷ 003A ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [9.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0041 ? 0031 ÷ 003A ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [9.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0031\u003A",
                      new String[] { "\u0041\u0031" });
 
-    // ÷ 0041 ? 0308 ? 0031 ÷ 003A ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [9.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0041 ? 0308 ? 0031 ÷ 003A ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [9.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u0031\u003A",
                      new String[] { "\u0041\u0308\u0031" });
 
-    // ÷ 0041 ? 0031 ÷ 0027 ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [9.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0041 ? 0031 ÷ 0027 ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [9.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0031\u0027",
                      new String[] { "\u0041\u0031" });
 
-    // ÷ 0041 ? 0308 ? 0031 ÷ 0027 ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [9.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0041 ? 0308 ? 0031 ÷ 0027 ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [9.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u0031\u0027",
                      new String[] { "\u0041\u0308\u0031" });
 
-    // ÷ 0041 ? 0031 ÷ 002C ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [9.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0041 ? 0031 ÷ 002C ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [9.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0031\u002C",
                      new String[] { "\u0041\u0031" });
 
-    // ÷ 0041 ? 0308 ? 0031 ÷ 002C ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [9.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0041 ? 0308 ? 0031 ÷ 002C ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [9.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u0031\u002C",
                      new String[] { "\u0041\u0308\u0031" });
 
-    // ÷ 0041 ? 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [9.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0041 ? 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [9.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0031\u002E\u2060",
                      new String[] { "\u0041\u0031" });
 
-    // ÷ 0041 ? 0308 ? 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [9.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0041 ? 0308 ? 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] LATIN CAPITAL LETTER A (ALetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [9.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u0031\u002E\u2060",
                      new String[] { "\u0041\u0308\u0031" });
 
-    // ÷ 003A ÷ 0001 ÷	#  ÷ [0.2] COLON (MidLetter) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 003A ÷ 0001 ÷  #  ÷ [0.2] COLON (MidLetter) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0001",
                      new String[] {  });
 
-    // ÷ 003A ? 0308 ÷ 0001 ÷	#  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 003A ? 0308 ÷ 0001 ÷  #  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u0001",
                      new String[] {  });
 
-    // ÷ 003A ÷ 000D ÷	#  ÷ [0.2] COLON (MidLetter) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 003A ÷ 000D ÷  #  ÷ [0.2] COLON (MidLetter) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\r",
                      new String[] {  });
 
-    // ÷ 003A ? 0308 ÷ 000D ÷	#  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 003A ? 0308 ÷ 000D ÷  #  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\r",
                      new String[] {  });
 
-    // ÷ 003A ÷ 000A ÷	#  ÷ [0.2] COLON (MidLetter) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 003A ÷ 000A ÷  #  ÷ [0.2] COLON (MidLetter) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\n",
                      new String[] {  });
 
-    // ÷ 003A ? 0308 ÷ 000A ÷	#  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 003A ? 0308 ÷ 000A ÷  #  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\n",
                      new String[] {  });
 
-    // ÷ 003A ÷ 000B ÷	#  ÷ [0.2] COLON (MidLetter) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 003A ÷ 000B ÷  #  ÷ [0.2] COLON (MidLetter) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u000B",
                      new String[] {  });
 
-    // ÷ 003A ? 0308 ÷ 000B ÷	#  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 003A ? 0308 ÷ 000B ÷  #  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u000B",
                      new String[] {  });
 
-    // ÷ 003A ÷ 3031 ÷	#  ÷ [0.2] COLON (MidLetter) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 003A ÷ 3031 ÷  #  ÷ [0.2] COLON (MidLetter) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u3031",
                      new String[] { "\u3031" });
 
-    // ÷ 003A ? 0308 ÷ 3031 ÷	#  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 003A ? 0308 ÷ 3031 ÷  #  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u3031",
                      new String[] { "\u3031" });
 
-    // ÷ 003A ÷ 0041 ÷	#  ÷ [0.2] COLON (MidLetter) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 003A ÷ 0041 ÷  #  ÷ [0.2] COLON (MidLetter) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0041",
                      new String[] { "\u0041" });
 
-    // ÷ 003A ? 0308 ÷ 0041 ÷	#  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 003A ? 0308 ÷ 0041 ÷  #  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u0041",
                      new String[] { "\u0041" });
 
-    // ÷ 003A ÷ 003A ÷	#  ÷ [0.2] COLON (MidLetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 003A ÷ 003A ÷  #  ÷ [0.2] COLON (MidLetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u003A",
                      new String[] {  });
 
-    // ÷ 003A ? 0308 ÷ 003A ÷	#  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 003A ? 0308 ÷ 003A ÷  #  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u003A",
                      new String[] {  });
 
-    // ÷ 003A ÷ 002C ÷	#  ÷ [0.2] COLON (MidLetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 003A ÷ 002C ÷  #  ÷ [0.2] COLON (MidLetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u002C",
                      new String[] {  });
 
-    // ÷ 003A ? 0308 ÷ 002C ÷	#  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 003A ? 0308 ÷ 002C ÷  #  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u002C",
                      new String[] {  });
 
-    // ÷ 003A ÷ 0027 ÷	#  ÷ [0.2] COLON (MidLetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 003A ÷ 0027 ÷  #  ÷ [0.2] COLON (MidLetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0027",
                      new String[] {  });
 
-    // ÷ 003A ? 0308 ÷ 0027 ÷	#  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 003A ? 0308 ÷ 0027 ÷  #  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u0027",
                      new String[] {  });
 
-    // ÷ 003A ÷ 0030 ÷	#  ÷ [0.2] COLON (MidLetter) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 003A ÷ 0030 ÷  #  ÷ [0.2] COLON (MidLetter) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0030",
                      new String[] { "\u0030" });
 
-    // ÷ 003A ? 0308 ÷ 0030 ÷	#  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 003A ? 0308 ÷ 0030 ÷  #  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u0030",
                      new String[] { "\u0030" });
 
-    // ÷ 003A ÷ 005F ÷	#  ÷ [0.2] COLON (MidLetter) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 003A ÷ 005F ÷  #  ÷ [0.2] COLON (MidLetter) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u005F",
                      new String[] {  });
 
-    // ÷ 003A ? 0308 ÷ 005F ÷	#  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 003A ? 0308 ÷ 005F ÷  #  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u005F",
                      new String[] {  });
 
-    // ÷ 003A ? 00AD ÷	#  ÷ [0.2] COLON (MidLetter) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 003A ? 00AD ÷  #  ÷ [0.2] COLON (MidLetter) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u00AD",
                      new String[] {  });
 
-    // ÷ 003A ? 0308 ? 00AD ÷	#  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 003A ? 0308 ? 00AD ÷  #  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u00AD",
                      new String[] {  });
 
-    // ÷ 003A ? 0300 ÷	#  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 003A ? 0300 ÷  #  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0300",
                      new String[] {  });
 
-    // ÷ 003A ? 0308 ? 0300 ÷	#  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 003A ? 0308 ? 0300 ÷  #  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u0300",
                      new String[] {  });
 
-    // ÷ 003A ÷ 0061 ? 2060 ÷	#  ÷ [0.2] COLON (MidLetter) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 003A ÷ 0061 ? 2060 ÷  #  ÷ [0.2] COLON (MidLetter) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    // ÷ 003A ? 0308 ÷ 0061 ? 2060 ÷	#  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 003A ? 0308 ÷ 0061 ? 2060 ÷  #  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    // ÷ 003A ÷ 0061 ÷ 003A ÷	#  ÷ [0.2] COLON (MidLetter) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 003A ÷ 0061 ÷ 003A ÷  #  ÷ [0.2] COLON (MidLetter) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0061\u003A",
                      new String[] { "\u0061" });
 
-    // ÷ 003A ? 0308 ÷ 0061 ÷ 003A ÷	#  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 003A ? 0308 ÷ 0061 ÷ 003A ÷  #  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u0061\u003A",
                      new String[] { "\u0061" });
 
-    // ÷ 003A ÷ 0061 ÷ 0027 ÷	#  ÷ [0.2] COLON (MidLetter) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 003A ÷ 0061 ÷ 0027 ÷  #  ÷ [0.2] COLON (MidLetter) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0061\u0027",
                      new String[] { "\u0061" });
 
-    // ÷ 003A ? 0308 ÷ 0061 ÷ 0027 ÷	#  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 003A ? 0308 ÷ 0061 ÷ 0027 ÷  #  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u0061\u0027",
                      new String[] { "\u0061" });
 
-    // ÷ 003A ÷ 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] COLON (MidLetter) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 003A ÷ 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] COLON (MidLetter) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    // ÷ 003A ? 0308 ÷ 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 003A ? 0308 ÷ 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    // ÷ 003A ÷ 0061 ÷ 002C ÷	#  ÷ [0.2] COLON (MidLetter) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 003A ÷ 0061 ÷ 002C ÷  #  ÷ [0.2] COLON (MidLetter) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0061\u002C",
                      new String[] { "\u0061" });
 
-    // ÷ 003A ? 0308 ÷ 0061 ÷ 002C ÷	#  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 003A ? 0308 ÷ 0061 ÷ 002C ÷  #  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u0061\u002C",
                      new String[] { "\u0061" });
 
-    // ÷ 003A ÷ 0031 ÷ 003A ÷	#  ÷ [0.2] COLON (MidLetter) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 003A ÷ 0031 ÷ 003A ÷  #  ÷ [0.2] COLON (MidLetter) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0031\u003A",
                      new String[] { "\u0031" });
 
-    // ÷ 003A ? 0308 ÷ 0031 ÷ 003A ÷	#  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 003A ? 0308 ÷ 0031 ÷ 003A ÷  #  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u0031\u003A",
                      new String[] { "\u0031" });
 
-    // ÷ 003A ÷ 0031 ÷ 0027 ÷	#  ÷ [0.2] COLON (MidLetter) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 003A ÷ 0031 ÷ 0027 ÷  #  ÷ [0.2] COLON (MidLetter) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0031\u0027",
                      new String[] { "\u0031" });
 
-    // ÷ 003A ? 0308 ÷ 0031 ÷ 0027 ÷	#  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 003A ? 0308 ÷ 0031 ÷ 0027 ÷  #  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u0031\u0027",
                      new String[] { "\u0031" });
 
-    // ÷ 003A ÷ 0031 ÷ 002C ÷	#  ÷ [0.2] COLON (MidLetter) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 003A ÷ 0031 ÷ 002C ÷  #  ÷ [0.2] COLON (MidLetter) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0031\u002C",
                      new String[] { "\u0031" });
 
-    // ÷ 003A ? 0308 ÷ 0031 ÷ 002C ÷	#  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 003A ? 0308 ÷ 0031 ÷ 002C ÷  #  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u0031\u002C",
                      new String[] { "\u0031" });
 
-    // ÷ 003A ÷ 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] COLON (MidLetter) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 003A ÷ 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] COLON (MidLetter) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    // ÷ 003A ? 0308 ÷ 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 003A ? 0308 ÷ 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    // ÷ 002C ÷ 0001 ÷	#  ÷ [0.2] COMMA (MidNum) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 002C ÷ 0001 ÷  #  ÷ [0.2] COMMA (MidNum) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0001",
                      new String[] {  });
 
-    // ÷ 002C ? 0308 ÷ 0001 ÷	#  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 002C ? 0308 ÷ 0001 ÷  #  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u0001",
                      new String[] {  });
 
-    // ÷ 002C ÷ 000D ÷	#  ÷ [0.2] COMMA (MidNum) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 002C ÷ 000D ÷  #  ÷ [0.2] COMMA (MidNum) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\r",
                      new String[] {  });
 
-    // ÷ 002C ? 0308 ÷ 000D ÷	#  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 002C ? 0308 ÷ 000D ÷  #  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\r",
                      new String[] {  });
 
-    // ÷ 002C ÷ 000A ÷	#  ÷ [0.2] COMMA (MidNum) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 002C ÷ 000A ÷  #  ÷ [0.2] COMMA (MidNum) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\n",
                      new String[] {  });
 
-    // ÷ 002C ? 0308 ÷ 000A ÷	#  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 002C ? 0308 ÷ 000A ÷  #  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\n",
                      new String[] {  });
 
-    // ÷ 002C ÷ 000B ÷	#  ÷ [0.2] COMMA (MidNum) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 002C ÷ 000B ÷  #  ÷ [0.2] COMMA (MidNum) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u000B",
                      new String[] {  });
 
-    // ÷ 002C ? 0308 ÷ 000B ÷	#  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 002C ? 0308 ÷ 000B ÷  #  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u000B",
                      new String[] {  });
 
-    // ÷ 002C ÷ 3031 ÷	#  ÷ [0.2] COMMA (MidNum) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 002C ÷ 3031 ÷  #  ÷ [0.2] COMMA (MidNum) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u3031",
                      new String[] { "\u3031" });
 
-    // ÷ 002C ? 0308 ÷ 3031 ÷	#  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 002C ? 0308 ÷ 3031 ÷  #  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u3031",
                      new String[] { "\u3031" });
 
-    // ÷ 002C ÷ 0041 ÷	#  ÷ [0.2] COMMA (MidNum) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 002C ÷ 0041 ÷  #  ÷ [0.2] COMMA (MidNum) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0041",
                      new String[] { "\u0041" });
 
-    // ÷ 002C ? 0308 ÷ 0041 ÷	#  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 002C ? 0308 ÷ 0041 ÷  #  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u0041",
                      new String[] { "\u0041" });
 
-    // ÷ 002C ÷ 003A ÷	#  ÷ [0.2] COMMA (MidNum) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 002C ÷ 003A ÷  #  ÷ [0.2] COMMA (MidNum) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u003A",
                      new String[] {  });
 
-    // ÷ 002C ? 0308 ÷ 003A ÷	#  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 002C ? 0308 ÷ 003A ÷  #  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u003A",
                      new String[] {  });
 
-    // ÷ 002C ÷ 002C ÷	#  ÷ [0.2] COMMA (MidNum) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 002C ÷ 002C ÷  #  ÷ [0.2] COMMA (MidNum) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u002C",
                      new String[] {  });
 
-    // ÷ 002C ? 0308 ÷ 002C ÷	#  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 002C ? 0308 ÷ 002C ÷  #  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u002C",
                      new String[] {  });
 
-    // ÷ 002C ÷ 0027 ÷	#  ÷ [0.2] COMMA (MidNum) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 002C ÷ 0027 ÷  #  ÷ [0.2] COMMA (MidNum) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0027",
                      new String[] {  });
 
-    // ÷ 002C ? 0308 ÷ 0027 ÷	#  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 002C ? 0308 ÷ 0027 ÷  #  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u0027",
                      new String[] {  });
 
-    // ÷ 002C ÷ 0030 ÷	#  ÷ [0.2] COMMA (MidNum) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 002C ÷ 0030 ÷  #  ÷ [0.2] COMMA (MidNum) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0030",
                      new String[] { "\u0030" });
 
-    // ÷ 002C ? 0308 ÷ 0030 ÷	#  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 002C ? 0308 ÷ 0030 ÷  #  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u0030",
                      new String[] { "\u0030" });
 
-    // ÷ 002C ÷ 005F ÷	#  ÷ [0.2] COMMA (MidNum) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 002C ÷ 005F ÷  #  ÷ [0.2] COMMA (MidNum) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u005F",
                      new String[] {  });
 
-    // ÷ 002C ? 0308 ÷ 005F ÷	#  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 002C ? 0308 ÷ 005F ÷  #  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u005F",
                      new String[] {  });
 
-    // ÷ 002C ? 00AD ÷	#  ÷ [0.2] COMMA (MidNum) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 002C ? 00AD ÷  #  ÷ [0.2] COMMA (MidNum) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u00AD",
                      new String[] {  });
 
-    // ÷ 002C ? 0308 ? 00AD ÷	#  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 002C ? 0308 ? 00AD ÷  #  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u00AD",
                      new String[] {  });
 
-    // ÷ 002C ? 0300 ÷	#  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 002C ? 0300 ÷  #  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0300",
                      new String[] {  });
 
-    // ÷ 002C ? 0308 ? 0300 ÷	#  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 002C ? 0308 ? 0300 ÷  #  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u0300",
                      new String[] {  });
 
-    // ÷ 002C ÷ 0061 ? 2060 ÷	#  ÷ [0.2] COMMA (MidNum) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 002C ÷ 0061 ? 2060 ÷  #  ÷ [0.2] COMMA (MidNum) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    // ÷ 002C ? 0308 ÷ 0061 ? 2060 ÷	#  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 002C ? 0308 ÷ 0061 ? 2060 ÷  #  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    // ÷ 002C ÷ 0061 ÷ 003A ÷	#  ÷ [0.2] COMMA (MidNum) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 002C ÷ 0061 ÷ 003A ÷  #  ÷ [0.2] COMMA (MidNum) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0061\u003A",
                      new String[] { "\u0061" });
 
-    // ÷ 002C ? 0308 ÷ 0061 ÷ 003A ÷	#  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 002C ? 0308 ÷ 0061 ÷ 003A ÷  #  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u0061\u003A",
                      new String[] { "\u0061" });
 
-    // ÷ 002C ÷ 0061 ÷ 0027 ÷	#  ÷ [0.2] COMMA (MidNum) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 002C ÷ 0061 ÷ 0027 ÷  #  ÷ [0.2] COMMA (MidNum) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0061\u0027",
                      new String[] { "\u0061" });
 
-    // ÷ 002C ? 0308 ÷ 0061 ÷ 0027 ÷	#  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 002C ? 0308 ÷ 0061 ÷ 0027 ÷  #  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u0061\u0027",
                      new String[] { "\u0061" });
 
-    // ÷ 002C ÷ 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] COMMA (MidNum) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 002C ÷ 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] COMMA (MidNum) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    // ÷ 002C ? 0308 ÷ 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 002C ? 0308 ÷ 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    // ÷ 002C ÷ 0061 ÷ 002C ÷	#  ÷ [0.2] COMMA (MidNum) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 002C ÷ 0061 ÷ 002C ÷  #  ÷ [0.2] COMMA (MidNum) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0061\u002C",
                      new String[] { "\u0061" });
 
-    // ÷ 002C ? 0308 ÷ 0061 ÷ 002C ÷	#  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 002C ? 0308 ÷ 0061 ÷ 002C ÷  #  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u0061\u002C",
                      new String[] { "\u0061" });
 
-    // ÷ 002C ÷ 0031 ÷ 003A ÷	#  ÷ [0.2] COMMA (MidNum) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 002C ÷ 0031 ÷ 003A ÷  #  ÷ [0.2] COMMA (MidNum) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0031\u003A",
                      new String[] { "\u0031" });
 
-    // ÷ 002C ? 0308 ÷ 0031 ÷ 003A ÷	#  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 002C ? 0308 ÷ 0031 ÷ 003A ÷  #  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u0031\u003A",
                      new String[] { "\u0031" });
 
-    // ÷ 002C ÷ 0031 ÷ 0027 ÷	#  ÷ [0.2] COMMA (MidNum) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 002C ÷ 0031 ÷ 0027 ÷  #  ÷ [0.2] COMMA (MidNum) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0031\u0027",
                      new String[] { "\u0031" });
 
-    // ÷ 002C ? 0308 ÷ 0031 ÷ 0027 ÷	#  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 002C ? 0308 ÷ 0031 ÷ 0027 ÷  #  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u0031\u0027",
                      new String[] { "\u0031" });
 
-    // ÷ 002C ÷ 0031 ÷ 002C ÷	#  ÷ [0.2] COMMA (MidNum) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 002C ÷ 0031 ÷ 002C ÷  #  ÷ [0.2] COMMA (MidNum) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0031\u002C",
                      new String[] { "\u0031" });
 
-    // ÷ 002C ? 0308 ÷ 0031 ÷ 002C ÷	#  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 002C ? 0308 ÷ 0031 ÷ 002C ÷  #  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u0031\u002C",
                      new String[] { "\u0031" });
 
-    // ÷ 002C ÷ 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] COMMA (MidNum) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 002C ÷ 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] COMMA (MidNum) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    // ÷ 002C ? 0308 ÷ 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 002C ? 0308 ÷ 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    // ÷ 0027 ÷ 0001 ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 0027 ÷ 0001 ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0001",
                      new String[] {  });
 
-    // ÷ 0027 ? 0308 ÷ 0001 ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 0027 ? 0308 ÷ 0001 ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u0001",
                      new String[] {  });
 
-    // ÷ 0027 ÷ 000D ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 0027 ÷ 000D ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\r",
                      new String[] {  });
 
-    // ÷ 0027 ? 0308 ÷ 000D ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 0027 ? 0308 ÷ 000D ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\r",
                      new String[] {  });
 
-    // ÷ 0027 ÷ 000A ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 0027 ÷ 000A ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\n",
                      new String[] {  });
 
-    // ÷ 0027 ? 0308 ÷ 000A ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 0027 ? 0308 ÷ 000A ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\n",
                      new String[] {  });
 
-    // ÷ 0027 ÷ 000B ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 0027 ÷ 000B ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u000B",
                      new String[] {  });
 
-    // ÷ 0027 ? 0308 ÷ 000B ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 0027 ? 0308 ÷ 000B ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u000B",
                      new String[] {  });
 
-    // ÷ 0027 ÷ 3031 ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 0027 ÷ 3031 ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u3031",
                      new String[] { "\u3031" });
 
-    // ÷ 0027 ? 0308 ÷ 3031 ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 0027 ? 0308 ÷ 3031 ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u3031",
                      new String[] { "\u3031" });
 
-    // ÷ 0027 ÷ 0041 ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 0027 ÷ 0041 ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0041",
                      new String[] { "\u0041" });
 
-    // ÷ 0027 ? 0308 ÷ 0041 ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 0027 ? 0308 ÷ 0041 ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u0041",
                      new String[] { "\u0041" });
 
-    // ÷ 0027 ÷ 003A ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0027 ÷ 003A ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u003A",
                      new String[] {  });
 
-    // ÷ 0027 ? 0308 ÷ 003A ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0027 ? 0308 ÷ 003A ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u003A",
                      new String[] {  });
 
-    // ÷ 0027 ÷ 002C ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0027 ÷ 002C ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u002C",
                      new String[] {  });
 
-    // ÷ 0027 ? 0308 ÷ 002C ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0027 ? 0308 ÷ 002C ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u002C",
                      new String[] {  });
 
-    // ÷ 0027 ÷ 0027 ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0027 ÷ 0027 ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0027",
                      new String[] {  });
 
-    // ÷ 0027 ? 0308 ÷ 0027 ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0027 ? 0308 ÷ 0027 ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u0027",
                      new String[] {  });
 
-    // ÷ 0027 ÷ 0030 ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 0027 ÷ 0030 ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0030",
                      new String[] { "\u0030" });
 
-    // ÷ 0027 ? 0308 ÷ 0030 ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 0027 ? 0308 ÷ 0030 ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u0030",
                      new String[] { "\u0030" });
 
-    // ÷ 0027 ÷ 005F ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 0027 ÷ 005F ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u005F",
                      new String[] {  });
 
-    // ÷ 0027 ? 0308 ÷ 005F ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 0027 ? 0308 ÷ 005F ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u005F",
                      new String[] {  });
 
-    // ÷ 0027 ? 00AD ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 0027 ? 00AD ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u00AD",
                      new String[] {  });
 
-    // ÷ 0027 ? 0308 ? 00AD ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 0027 ? 0308 ? 00AD ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u00AD",
                      new String[] {  });
 
-    // ÷ 0027 ? 0300 ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 0027 ? 0300 ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0300",
                      new String[] {  });
 
-    // ÷ 0027 ? 0308 ? 0300 ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 0027 ? 0308 ? 0300 ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u0300",
                      new String[] {  });
 
-    // ÷ 0027 ÷ 0061 ? 2060 ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0027 ÷ 0061 ? 2060 ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    // ÷ 0027 ? 0308 ÷ 0061 ? 2060 ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0027 ? 0308 ÷ 0061 ? 2060 ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    // ÷ 0027 ÷ 0061 ÷ 003A ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0027 ÷ 0061 ÷ 003A ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0061\u003A",
                      new String[] { "\u0061" });
 
-    // ÷ 0027 ? 0308 ÷ 0061 ÷ 003A ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0027 ? 0308 ÷ 0061 ÷ 003A ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u0061\u003A",
                      new String[] { "\u0061" });
 
-    // ÷ 0027 ÷ 0061 ÷ 0027 ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0027 ÷ 0061 ÷ 0027 ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0061\u0027",
                      new String[] { "\u0061" });
 
-    // ÷ 0027 ? 0308 ÷ 0061 ÷ 0027 ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0027 ? 0308 ÷ 0061 ÷ 0027 ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u0061\u0027",
                      new String[] { "\u0061" });
 
-    // ÷ 0027 ÷ 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0027 ÷ 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    // ÷ 0027 ? 0308 ÷ 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0027 ? 0308 ÷ 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    // ÷ 0027 ÷ 0061 ÷ 002C ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0027 ÷ 0061 ÷ 002C ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0061\u002C",
                      new String[] { "\u0061" });
 
-    // ÷ 0027 ? 0308 ÷ 0061 ÷ 002C ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0027 ? 0308 ÷ 0061 ÷ 002C ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u0061\u002C",
                      new String[] { "\u0061" });
 
-    // ÷ 0027 ÷ 0031 ÷ 003A ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0027 ÷ 0031 ÷ 003A ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0031\u003A",
                      new String[] { "\u0031" });
 
-    // ÷ 0027 ? 0308 ÷ 0031 ÷ 003A ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0027 ? 0308 ÷ 0031 ÷ 003A ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u0031\u003A",
                      new String[] { "\u0031" });
 
-    // ÷ 0027 ÷ 0031 ÷ 0027 ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0027 ÷ 0031 ÷ 0027 ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0031\u0027",
                      new String[] { "\u0031" });
 
-    // ÷ 0027 ? 0308 ÷ 0031 ÷ 0027 ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0027 ? 0308 ÷ 0031 ÷ 0027 ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u0031\u0027",
                      new String[] { "\u0031" });
 
-    // ÷ 0027 ÷ 0031 ÷ 002C ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0027 ÷ 0031 ÷ 002C ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0031\u002C",
                      new String[] { "\u0031" });
 
-    // ÷ 0027 ? 0308 ÷ 0031 ÷ 002C ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0027 ? 0308 ÷ 0031 ÷ 002C ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u0031\u002C",
                      new String[] { "\u0031" });
 
-    // ÷ 0027 ÷ 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0027 ÷ 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    // ÷ 0027 ? 0308 ÷ 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0027 ? 0308 ÷ 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    // ÷ 0030 ÷ 0001 ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 0030 ÷ 0001 ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0001",
                      new String[] { "\u0030" });
 
-    // ÷ 0030 ? 0308 ÷ 0001 ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 0030 ? 0308 ÷ 0001 ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u0001",
                      new String[] { "\u0030\u0308" });
 
-    // ÷ 0030 ÷ 000D ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 0030 ÷ 000D ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\r",
                      new String[] { "\u0030" });
 
-    // ÷ 0030 ? 0308 ÷ 000D ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 0030 ? 0308 ÷ 000D ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\r",
                      new String[] { "\u0030\u0308" });
 
-    // ÷ 0030 ÷ 000A ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 0030 ÷ 000A ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\n",
                      new String[] { "\u0030" });
 
-    // ÷ 0030 ? 0308 ÷ 000A ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 0030 ? 0308 ÷ 000A ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\n",
                      new String[] { "\u0030\u0308" });
 
-    // ÷ 0030 ÷ 000B ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 0030 ÷ 000B ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u000B",
                      new String[] { "\u0030" });
 
-    // ÷ 0030 ? 0308 ÷ 000B ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 0030 ? 0308 ÷ 000B ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u000B",
                      new String[] { "\u0030\u0308" });
 
-    // ÷ 0030 ÷ 3031 ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 0030 ÷ 3031 ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u3031",
                      new String[] { "\u0030", "\u3031" });
 
-    // ÷ 0030 ? 0308 ÷ 3031 ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 0030 ? 0308 ÷ 3031 ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u3031",
                      new String[] { "\u0030\u0308", "\u3031" });
 
-    // ÷ 0030 ? 0041 ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [10.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 0030 ? 0041 ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [10.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0041",
                      new String[] { "\u0030\u0041" });
 
-    // ÷ 0030 ? 0308 ? 0041 ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [10.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 0030 ? 0308 ? 0041 ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [10.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u0041",
                      new String[] { "\u0030\u0308\u0041" });
 
-    // ÷ 0030 ÷ 003A ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0030 ÷ 003A ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u003A",
                      new String[] { "\u0030" });
 
-    // ÷ 0030 ? 0308 ÷ 003A ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0030 ? 0308 ÷ 003A ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u003A",
                      new String[] { "\u0030\u0308" });
 
-    // ÷ 0030 ÷ 002C ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0030 ÷ 002C ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u002C",
                      new String[] { "\u0030" });
 
-    // ÷ 0030 ? 0308 ÷ 002C ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0030 ? 0308 ÷ 002C ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u002C",
                      new String[] { "\u0030\u0308" });
 
-    // ÷ 0030 ÷ 0027 ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0030 ÷ 0027 ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0027",
                      new String[] { "\u0030" });
 
-    // ÷ 0030 ? 0308 ÷ 0027 ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0030 ? 0308 ÷ 0027 ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u0027",
                      new String[] { "\u0030\u0308" });
 
-    // ÷ 0030 ? 0030 ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [8.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 0030 ? 0030 ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [8.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0030",
                      new String[] { "\u0030\u0030" });
 
-    // ÷ 0030 ? 0308 ? 0030 ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [8.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 0030 ? 0308 ? 0030 ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [8.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u0030",
                      new String[] { "\u0030\u0308\u0030" });
 
-    // ÷ 0030 ? 005F ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [13.1] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 0030 ? 005F ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [13.1] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u005F",
                      new String[] { "\u0030\u005F" });
 
-    // ÷ 0030 ? 0308 ? 005F ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.1] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 0030 ? 0308 ? 005F ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.1] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u005F",
                      new String[] { "\u0030\u0308\u005F" });
 
-    // ÷ 0030 ? 00AD ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 0030 ? 00AD ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u00AD",
                      new String[] { "\u0030\u00AD" });
 
-    // ÷ 0030 ? 0308 ? 00AD ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 0030 ? 0308 ? 00AD ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u00AD",
                      new String[] { "\u0030\u0308\u00AD" });
 
-    // ÷ 0030 ? 0300 ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 0030 ? 0300 ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0300",
                      new String[] { "\u0030\u0300" });
 
-    // ÷ 0030 ? 0308 ? 0300 ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 0030 ? 0308 ? 0300 ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u0300",
                      new String[] { "\u0030\u0308\u0300" });
 
-    // ÷ 0030 ? 0061 ? 2060 ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [10.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0030 ? 0061 ? 2060 ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [10.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0061\u2060",
                      new String[] { "\u0030\u0061\u2060" });
 
-    // ÷ 0030 ? 0308 ? 0061 ? 2060 ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [10.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0030 ? 0308 ? 0061 ? 2060 ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [10.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u0061\u2060",
                      new String[] { "\u0030\u0308\u0061\u2060" });
 
-    // ÷ 0030 ? 0061 ÷ 003A ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [10.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0030 ? 0061 ÷ 003A ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [10.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0061\u003A",
                      new String[] { "\u0030\u0061" });
 
-    // ÷ 0030 ? 0308 ? 0061 ÷ 003A ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [10.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0030 ? 0308 ? 0061 ÷ 003A ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [10.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u0061\u003A",
                      new String[] { "\u0030\u0308\u0061" });
 
-    // ÷ 0030 ? 0061 ÷ 0027 ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [10.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0030 ? 0061 ÷ 0027 ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [10.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0061\u0027",
                      new String[] { "\u0030\u0061" });
 
-    // ÷ 0030 ? 0308 ? 0061 ÷ 0027 ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [10.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0030 ? 0308 ? 0061 ÷ 0027 ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [10.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u0061\u0027",
                      new String[] { "\u0030\u0308\u0061" });
 
-    // ÷ 0030 ? 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [10.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0030 ? 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [10.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0061\u0027\u2060",
                      new String[] { "\u0030\u0061" });
 
-    // ÷ 0030 ? 0308 ? 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [10.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0030 ? 0308 ? 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [10.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u0061\u0027\u2060",
                      new String[] { "\u0030\u0308\u0061" });
 
-    // ÷ 0030 ? 0061 ÷ 002C ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [10.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0030 ? 0061 ÷ 002C ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [10.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0061\u002C",
                      new String[] { "\u0030\u0061" });
 
-    // ÷ 0030 ? 0308 ? 0061 ÷ 002C ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [10.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0030 ? 0308 ? 0061 ÷ 002C ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [10.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u0061\u002C",
                      new String[] { "\u0030\u0308\u0061" });
 
-    // ÷ 0030 ? 0031 ÷ 003A ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [8.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0030 ? 0031 ÷ 003A ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [8.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0031\u003A",
                      new String[] { "\u0030\u0031" });
 
-    // ÷ 0030 ? 0308 ? 0031 ÷ 003A ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [8.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0030 ? 0308 ? 0031 ÷ 003A ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [8.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u0031\u003A",
                      new String[] { "\u0030\u0308\u0031" });
 
-    // ÷ 0030 ? 0031 ÷ 0027 ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [8.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0030 ? 0031 ÷ 0027 ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [8.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0031\u0027",
                      new String[] { "\u0030\u0031" });
 
-    // ÷ 0030 ? 0308 ? 0031 ÷ 0027 ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [8.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0030 ? 0308 ? 0031 ÷ 0027 ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [8.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u0031\u0027",
                      new String[] { "\u0030\u0308\u0031" });
 
-    // ÷ 0030 ? 0031 ÷ 002C ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [8.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0030 ? 0031 ÷ 002C ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [8.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0031\u002C",
                      new String[] { "\u0030\u0031" });
 
-    // ÷ 0030 ? 0308 ? 0031 ÷ 002C ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [8.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0030 ? 0308 ? 0031 ÷ 002C ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [8.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u0031\u002C",
                      new String[] { "\u0030\u0308\u0031" });
 
-    // ÷ 0030 ? 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [8.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0030 ? 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [8.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0031\u002E\u2060",
                      new String[] { "\u0030\u0031" });
 
-    // ÷ 0030 ? 0308 ? 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [8.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0030 ? 0308 ? 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] DIGIT ZERO (Numeric) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [8.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u0031\u002E\u2060",
                      new String[] { "\u0030\u0308\u0031" });
 
-    // ÷ 005F ÷ 0001 ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 005F ÷ 0001 ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0001",
                      new String[] {  });
 
-    // ÷ 005F ? 0308 ÷ 0001 ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 005F ? 0308 ÷ 0001 ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u0001",
                      new String[] {  });
 
-    // ÷ 005F ÷ 000D ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 005F ÷ 000D ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\r",
                      new String[] {  });
 
-    // ÷ 005F ? 0308 ÷ 000D ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 005F ? 0308 ÷ 000D ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\r",
                      new String[] {  });
 
-    // ÷ 005F ÷ 000A ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 005F ÷ 000A ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\n",
                      new String[] {  });
 
-    // ÷ 005F ? 0308 ÷ 000A ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 005F ? 0308 ÷ 000A ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\n",
                      new String[] {  });
 
-    // ÷ 005F ÷ 000B ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 005F ÷ 000B ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u000B",
                      new String[] {  });
 
-    // ÷ 005F ? 0308 ÷ 000B ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 005F ? 0308 ÷ 000B ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u000B",
                      new String[] {  });
 
-    // ÷ 005F ? 3031 ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [13.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 005F ? 3031 ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [13.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u3031",
                      new String[] { "\u005F\u3031" });
 
-    // ÷ 005F ? 0308 ? 3031 ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 005F ? 0308 ? 3031 ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.2] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u3031",
                      new String[] { "\u005F\u0308\u3031" });
 
-    // ÷ 005F ? 0041 ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [13.2] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 005F ? 0041 ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [13.2] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0041",
                      new String[] { "\u005F\u0041" });
 
-    // ÷ 005F ? 0308 ? 0041 ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.2] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 005F ? 0308 ? 0041 ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.2] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u0041",
                      new String[] { "\u005F\u0308\u0041" });
 
-    // ÷ 005F ÷ 003A ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 005F ÷ 003A ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u003A",
                      new String[] {  });
 
-    // ÷ 005F ? 0308 ÷ 003A ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 005F ? 0308 ÷ 003A ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u003A",
                      new String[] {  });
 
-    // ÷ 005F ÷ 002C ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 005F ÷ 002C ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u002C",
                      new String[] {  });
 
-    // ÷ 005F ? 0308 ÷ 002C ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 005F ? 0308 ÷ 002C ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u002C",
                      new String[] {  });
 
-    // ÷ 005F ÷ 0027 ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 005F ÷ 0027 ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0027",
                      new String[] {  });
 
-    // ÷ 005F ? 0308 ÷ 0027 ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 005F ? 0308 ÷ 0027 ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u0027",
                      new String[] {  });
 
-    // ÷ 005F ? 0030 ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [13.2] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 005F ? 0030 ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [13.2] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0030",
                      new String[] { "\u005F\u0030" });
 
-    // ÷ 005F ? 0308 ? 0030 ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.2] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 005F ? 0308 ? 0030 ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.2] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u0030",
                      new String[] { "\u005F\u0308\u0030" });
 
-    // ÷ 005F ? 005F ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [13.1] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 005F ? 005F ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [13.1] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u005F",
                      new String[] {  });
 
-    // ÷ 005F ? 0308 ? 005F ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.1] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 005F ? 0308 ? 005F ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.1] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u005F",
                      new String[] {  });
 
-    // ÷ 005F ? 00AD ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 005F ? 00AD ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u00AD",
                      new String[] {  });
 
-    // ÷ 005F ? 0308 ? 00AD ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 005F ? 0308 ? 00AD ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u00AD",
                      new String[] {  });
 
-    // ÷ 005F ? 0300 ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 005F ? 0300 ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0300",
                      new String[] {  });
 
-    // ÷ 005F ? 0308 ? 0300 ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 005F ? 0308 ? 0300 ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u0300",
                      new String[] {  });
 
-    // ÷ 005F ? 0061 ? 2060 ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [13.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 005F ? 0061 ? 2060 ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [13.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0061\u2060",
                      new String[] { "\u005F\u0061\u2060" });
 
-    // ÷ 005F ? 0308 ? 0061 ? 2060 ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 005F ? 0308 ? 0061 ? 2060 ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u0061\u2060",
                      new String[] { "\u005F\u0308\u0061\u2060" });
 
-    // ÷ 005F ? 0061 ÷ 003A ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [13.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 005F ? 0061 ÷ 003A ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [13.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0061\u003A",
                      new String[] { "\u005F\u0061" });
 
-    // ÷ 005F ? 0308 ? 0061 ÷ 003A ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 005F ? 0308 ? 0061 ÷ 003A ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u0061\u003A",
                      new String[] { "\u005F\u0308\u0061" });
 
-    // ÷ 005F ? 0061 ÷ 0027 ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [13.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 005F ? 0061 ÷ 0027 ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [13.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0061\u0027",
                      new String[] { "\u005F\u0061" });
 
-    // ÷ 005F ? 0308 ? 0061 ÷ 0027 ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 005F ? 0308 ? 0061 ÷ 0027 ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u0061\u0027",
                      new String[] { "\u005F\u0308\u0061" });
 
-    // ÷ 005F ? 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [13.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 005F ? 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [13.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0061\u0027\u2060",
                      new String[] { "\u005F\u0061" });
 
-    // ÷ 005F ? 0308 ? 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 005F ? 0308 ? 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u0061\u0027\u2060",
                      new String[] { "\u005F\u0308\u0061" });
 
-    // ÷ 005F ? 0061 ÷ 002C ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [13.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 005F ? 0061 ÷ 002C ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [13.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0061\u002C",
                      new String[] { "\u005F\u0061" });
 
-    // ÷ 005F ? 0308 ? 0061 ÷ 002C ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 005F ? 0308 ? 0061 ÷ 002C ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u0061\u002C",
                      new String[] { "\u005F\u0308\u0061" });
 
-    // ÷ 005F ? 0031 ÷ 003A ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [13.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 005F ? 0031 ÷ 003A ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [13.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0031\u003A",
                      new String[] { "\u005F\u0031" });
 
-    // ÷ 005F ? 0308 ? 0031 ÷ 003A ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 005F ? 0308 ? 0031 ÷ 003A ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u0031\u003A",
                      new String[] { "\u005F\u0308\u0031" });
 
-    // ÷ 005F ? 0031 ÷ 0027 ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [13.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 005F ? 0031 ÷ 0027 ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [13.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0031\u0027",
                      new String[] { "\u005F\u0031" });
 
-    // ÷ 005F ? 0308 ? 0031 ÷ 0027 ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 005F ? 0308 ? 0031 ÷ 0027 ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u0031\u0027",
                      new String[] { "\u005F\u0308\u0031" });
 
-    // ÷ 005F ? 0031 ÷ 002C ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [13.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 005F ? 0031 ÷ 002C ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [13.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0031\u002C",
                      new String[] { "\u005F\u0031" });
 
-    // ÷ 005F ? 0308 ? 0031 ÷ 002C ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 005F ? 0308 ? 0031 ÷ 002C ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u0031\u002C",
                      new String[] { "\u005F\u0308\u0031" });
 
-    // ÷ 005F ? 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [13.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 005F ? 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [13.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0031\u002E\u2060",
                      new String[] { "\u005F\u0031" });
 
-    // ÷ 005F ? 0308 ? 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 005F ? 0308 ? 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] LOW LINE (ExtendNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u0031\u002E\u2060",
                      new String[] { "\u005F\u0308\u0031" });
 
-    // ÷ 00AD ÷ 0001 ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 00AD ÷ 0001 ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0001",
                      new String[] {  });
 
-    // ÷ 00AD ? 0308 ÷ 0001 ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 00AD ? 0308 ÷ 0001 ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u0001",
                      new String[] {  });
 
-    // ÷ 00AD ÷ 000D ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 00AD ÷ 000D ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\r",
                      new String[] {  });
 
-    // ÷ 00AD ? 0308 ÷ 000D ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 00AD ? 0308 ÷ 000D ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\r",
                      new String[] {  });
 
-    // ÷ 00AD ÷ 000A ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 00AD ÷ 000A ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\n",
                      new String[] {  });
 
-    // ÷ 00AD ? 0308 ÷ 000A ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 00AD ? 0308 ÷ 000A ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\n",
                      new String[] {  });
 
-    // ÷ 00AD ÷ 000B ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 00AD ÷ 000B ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u000B",
                      new String[] {  });
 
-    // ÷ 00AD ? 0308 ÷ 000B ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 00AD ? 0308 ÷ 000B ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u000B",
                      new String[] {  });
 
-    // ÷ 00AD ÷ 3031 ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 00AD ÷ 3031 ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u3031",
                      new String[] { "\u3031" });
 
-    // ÷ 00AD ? 0308 ÷ 3031 ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 00AD ? 0308 ÷ 3031 ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u3031",
                      new String[] { "\u3031" });
 
-    // ÷ 00AD ÷ 0041 ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 00AD ÷ 0041 ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0041",
                      new String[] { "\u0041" });
 
-    // ÷ 00AD ? 0308 ÷ 0041 ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 00AD ? 0308 ÷ 0041 ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u0041",
                      new String[] { "\u0041" });
 
-    // ÷ 00AD ÷ 003A ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 00AD ÷ 003A ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u003A",
                      new String[] {  });
 
-    // ÷ 00AD ? 0308 ÷ 003A ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 00AD ? 0308 ÷ 003A ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u003A",
                      new String[] {  });
 
-    // ÷ 00AD ÷ 002C ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 00AD ÷ 002C ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u002C",
                      new String[] {  });
 
-    // ÷ 00AD ? 0308 ÷ 002C ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 00AD ? 0308 ÷ 002C ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u002C",
                      new String[] {  });
 
-    // ÷ 00AD ÷ 0027 ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 00AD ÷ 0027 ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0027",
                      new String[] {  });
 
-    // ÷ 00AD ? 0308 ÷ 0027 ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 00AD ? 0308 ÷ 0027 ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u0027",
                      new String[] {  });
 
-    // ÷ 00AD ÷ 0030 ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 00AD ÷ 0030 ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0030",
                      new String[] { "\u0030" });
 
-    // ÷ 00AD ? 0308 ÷ 0030 ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 00AD ? 0308 ÷ 0030 ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u0030",
                      new String[] { "\u0030" });
 
-    // ÷ 00AD ÷ 005F ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 00AD ÷ 005F ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u005F",
                      new String[] {  });
 
-    // ÷ 00AD ? 0308 ÷ 005F ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 00AD ? 0308 ÷ 005F ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u005F",
                      new String[] {  });
 
-    // ÷ 00AD ? 00AD ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 00AD ? 00AD ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u00AD",
                      new String[] {  });
 
-    // ÷ 00AD ? 0308 ? 00AD ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 00AD ? 0308 ? 00AD ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u00AD",
                      new String[] {  });
 
-    // ÷ 00AD ? 0300 ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 00AD ? 0300 ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0300",
                      new String[] {  });
 
-    // ÷ 00AD ? 0308 ? 0300 ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 00AD ? 0308 ? 0300 ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u0300",
                      new String[] {  });
 
-    // ÷ 00AD ÷ 0061 ? 2060 ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 00AD ÷ 0061 ? 2060 ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    // ÷ 00AD ? 0308 ÷ 0061 ? 2060 ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 00AD ? 0308 ÷ 0061 ? 2060 ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    // ÷ 00AD ÷ 0061 ÷ 003A ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 00AD ÷ 0061 ÷ 003A ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0061\u003A",
                      new String[] { "\u0061" });
 
-    // ÷ 00AD ? 0308 ÷ 0061 ÷ 003A ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 00AD ? 0308 ÷ 0061 ÷ 003A ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u0061\u003A",
                      new String[] { "\u0061" });
 
-    // ÷ 00AD ÷ 0061 ÷ 0027 ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 00AD ÷ 0061 ÷ 0027 ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0061\u0027",
                      new String[] { "\u0061" });
 
-    // ÷ 00AD ? 0308 ÷ 0061 ÷ 0027 ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 00AD ? 0308 ÷ 0061 ÷ 0027 ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u0061\u0027",
                      new String[] { "\u0061" });
 
-    // ÷ 00AD ÷ 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 00AD ÷ 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    // ÷ 00AD ? 0308 ÷ 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 00AD ? 0308 ÷ 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    // ÷ 00AD ÷ 0061 ÷ 002C ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 00AD ÷ 0061 ÷ 002C ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0061\u002C",
                      new String[] { "\u0061" });
 
-    // ÷ 00AD ? 0308 ÷ 0061 ÷ 002C ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 00AD ? 0308 ÷ 0061 ÷ 002C ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u0061\u002C",
                      new String[] { "\u0061" });
 
-    // ÷ 00AD ÷ 0031 ÷ 003A ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 00AD ÷ 0031 ÷ 003A ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0031\u003A",
                      new String[] { "\u0031" });
 
-    // ÷ 00AD ? 0308 ÷ 0031 ÷ 003A ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 00AD ? 0308 ÷ 0031 ÷ 003A ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u0031\u003A",
                      new String[] { "\u0031" });
 
-    // ÷ 00AD ÷ 0031 ÷ 0027 ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 00AD ÷ 0031 ÷ 0027 ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0031\u0027",
                      new String[] { "\u0031" });
 
-    // ÷ 00AD ? 0308 ÷ 0031 ÷ 0027 ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 00AD ? 0308 ÷ 0031 ÷ 0027 ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u0031\u0027",
                      new String[] { "\u0031" });
 
-    // ÷ 00AD ÷ 0031 ÷ 002C ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 00AD ÷ 0031 ÷ 002C ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0031\u002C",
                      new String[] { "\u0031" });
 
-    // ÷ 00AD ? 0308 ÷ 0031 ÷ 002C ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 00AD ? 0308 ÷ 0031 ÷ 002C ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u0031\u002C",
                      new String[] { "\u0031" });
 
-    // ÷ 00AD ÷ 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 00AD ÷ 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    // ÷ 00AD ? 0308 ÷ 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 00AD ? 0308 ÷ 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] SOFT HYPHEN (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    // ÷ 0300 ÷ 0001 ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 0300 ÷ 0001 ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0001",
                      new String[] {  });
 
-    // ÷ 0300 ? 0308 ÷ 0001 ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 0300 ? 0308 ÷ 0001 ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u0001",
                      new String[] {  });
 
-    // ÷ 0300 ÷ 000D ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 0300 ÷ 000D ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\r",
                      new String[] {  });
 
-    // ÷ 0300 ? 0308 ÷ 000D ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 0300 ? 0308 ÷ 000D ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\r",
                      new String[] {  });
 
-    // ÷ 0300 ÷ 000A ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 0300 ÷ 000A ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\n",
                      new String[] {  });
 
-    // ÷ 0300 ? 0308 ÷ 000A ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 0300 ? 0308 ÷ 000A ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\n",
                      new String[] {  });
 
-    // ÷ 0300 ÷ 000B ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 0300 ÷ 000B ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u000B",
                      new String[] {  });
 
-    // ÷ 0300 ? 0308 ÷ 000B ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 0300 ? 0308 ÷ 000B ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u000B",
                      new String[] {  });
 
-    // ÷ 0300 ÷ 3031 ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 0300 ÷ 3031 ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u3031",
                      new String[] { "\u3031" });
 
-    // ÷ 0300 ? 0308 ÷ 3031 ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 0300 ? 0308 ÷ 3031 ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u3031",
                      new String[] { "\u3031" });
 
-    // ÷ 0300 ÷ 0041 ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 0300 ÷ 0041 ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0041",
                      new String[] { "\u0041" });
 
-    // ÷ 0300 ? 0308 ÷ 0041 ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 0300 ? 0308 ÷ 0041 ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u0041",
                      new String[] { "\u0041" });
 
-    // ÷ 0300 ÷ 003A ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0300 ÷ 003A ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u003A",
                      new String[] {  });
 
-    // ÷ 0300 ? 0308 ÷ 003A ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0300 ? 0308 ÷ 003A ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u003A",
                      new String[] {  });
 
-    // ÷ 0300 ÷ 002C ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0300 ÷ 002C ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u002C",
                      new String[] {  });
 
-    // ÷ 0300 ? 0308 ÷ 002C ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0300 ? 0308 ÷ 002C ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u002C",
                      new String[] {  });
 
-    // ÷ 0300 ÷ 0027 ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0300 ÷ 0027 ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0027",
                      new String[] {  });
 
-    // ÷ 0300 ? 0308 ÷ 0027 ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0300 ? 0308 ÷ 0027 ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u0027",
                      new String[] {  });
 
-    // ÷ 0300 ÷ 0030 ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 0300 ÷ 0030 ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0030",
                      new String[] { "\u0030" });
 
-    // ÷ 0300 ? 0308 ÷ 0030 ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 0300 ? 0308 ÷ 0030 ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u0030",
                      new String[] { "\u0030" });
 
-    // ÷ 0300 ÷ 005F ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 0300 ÷ 005F ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u005F",
                      new String[] {  });
 
-    // ÷ 0300 ? 0308 ÷ 005F ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 0300 ? 0308 ÷ 005F ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u005F",
                      new String[] {  });
 
-    // ÷ 0300 ? 00AD ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 0300 ? 00AD ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u00AD",
                      new String[] {  });
 
-    // ÷ 0300 ? 0308 ? 00AD ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 0300 ? 0308 ? 00AD ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u00AD",
                      new String[] {  });
 
-    // ÷ 0300 ? 0300 ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 0300 ? 0300 ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0300",
                      new String[] {  });
 
-    // ÷ 0300 ? 0308 ? 0300 ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 0300 ? 0308 ? 0300 ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u0300",
                      new String[] {  });
 
-    // ÷ 0300 ÷ 0061 ? 2060 ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0300 ÷ 0061 ? 2060 ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    // ÷ 0300 ? 0308 ÷ 0061 ? 2060 ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0300 ? 0308 ÷ 0061 ? 2060 ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    // ÷ 0300 ÷ 0061 ÷ 003A ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0300 ÷ 0061 ÷ 003A ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0061\u003A",
                      new String[] { "\u0061" });
 
-    // ÷ 0300 ? 0308 ÷ 0061 ÷ 003A ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0300 ? 0308 ÷ 0061 ÷ 003A ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u0061\u003A",
                      new String[] { "\u0061" });
 
-    // ÷ 0300 ÷ 0061 ÷ 0027 ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0300 ÷ 0061 ÷ 0027 ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0061\u0027",
                      new String[] { "\u0061" });
 
-    // ÷ 0300 ? 0308 ÷ 0061 ÷ 0027 ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0300 ? 0308 ÷ 0061 ÷ 0027 ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u0061\u0027",
                      new String[] { "\u0061" });
 
-    // ÷ 0300 ÷ 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0300 ÷ 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    // ÷ 0300 ? 0308 ÷ 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0300 ? 0308 ÷ 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    // ÷ 0300 ÷ 0061 ÷ 002C ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0300 ÷ 0061 ÷ 002C ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0061\u002C",
                      new String[] { "\u0061" });
 
-    // ÷ 0300 ? 0308 ÷ 0061 ÷ 002C ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0300 ? 0308 ÷ 0061 ÷ 002C ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u0061\u002C",
                      new String[] { "\u0061" });
 
-    // ÷ 0300 ÷ 0031 ÷ 003A ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0300 ÷ 0031 ÷ 003A ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0031\u003A",
                      new String[] { "\u0031" });
 
-    // ÷ 0300 ? 0308 ÷ 0031 ÷ 003A ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0300 ? 0308 ÷ 0031 ÷ 003A ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u0031\u003A",
                      new String[] { "\u0031" });
 
-    // ÷ 0300 ÷ 0031 ÷ 0027 ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0300 ÷ 0031 ÷ 0027 ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0031\u0027",
                      new String[] { "\u0031" });
 
-    // ÷ 0300 ? 0308 ÷ 0031 ÷ 0027 ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0300 ? 0308 ÷ 0031 ÷ 0027 ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u0031\u0027",
                      new String[] { "\u0031" });
 
-    // ÷ 0300 ÷ 0031 ÷ 002C ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0300 ÷ 0031 ÷ 002C ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0031\u002C",
                      new String[] { "\u0031" });
 
-    // ÷ 0300 ? 0308 ÷ 0031 ÷ 002C ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0300 ? 0308 ÷ 0031 ÷ 002C ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u0031\u002C",
                      new String[] { "\u0031" });
 
-    // ÷ 0300 ÷ 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0300 ÷ 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    // ÷ 0300 ? 0308 ÷ 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0300 ? 0308 ÷ 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] COMBINING GRAVE ACCENT (Extend_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    // ÷ 0061 ? 2060 ÷ 0001 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 0061 ? 2060 ÷ 0001 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0001",
                      new String[] { "\u0061\u2060" });
 
-    // ÷ 0061 ? 2060 ? 0308 ÷ 0001 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0308 ÷ 0001 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u0001",
                      new String[] { "\u0061\u2060\u0308" });
 
-    // ÷ 0061 ? 2060 ÷ 000D ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 0061 ? 2060 ÷ 000D ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\r",
                      new String[] { "\u0061\u2060" });
 
-    // ÷ 0061 ? 2060 ? 0308 ÷ 000D ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0308 ÷ 000D ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\r",
                      new String[] { "\u0061\u2060\u0308" });
 
-    // ÷ 0061 ? 2060 ÷ 000A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 0061 ? 2060 ÷ 000A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\n",
                      new String[] { "\u0061\u2060" });
 
-    // ÷ 0061 ? 2060 ? 0308 ÷ 000A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0308 ÷ 000A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\n",
                      new String[] { "\u0061\u2060\u0308" });
 
-    // ÷ 0061 ? 2060 ÷ 000B ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 0061 ? 2060 ÷ 000B ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u000B",
                      new String[] { "\u0061\u2060" });
 
-    // ÷ 0061 ? 2060 ? 0308 ÷ 000B ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0308 ÷ 000B ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u000B",
                      new String[] { "\u0061\u2060\u0308" });
 
-    // ÷ 0061 ? 2060 ÷ 3031 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 0061 ? 2060 ÷ 3031 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u3031",
                      new String[] { "\u0061\u2060", "\u3031" });
 
-    // ÷ 0061 ? 2060 ? 0308 ÷ 3031 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0308 ÷ 3031 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u3031",
                      new String[] { "\u0061\u2060\u0308", "\u3031" });
 
-    // ÷ 0061 ? 2060 ? 0041 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [5.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0041 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [5.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0041",
                      new String[] { "\u0061\u2060\u0041" });
 
-    // ÷ 0061 ? 2060 ? 0308 ? 0041 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [5.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0308 ? 0041 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [5.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u0041",
                      new String[] { "\u0061\u2060\u0308\u0041" });
 
-    // ÷ 0061 ? 2060 ÷ 003A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0061 ? 2060 ÷ 003A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u003A",
                      new String[] { "\u0061\u2060" });
 
-    // ÷ 0061 ? 2060 ? 0308 ÷ 003A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0308 ÷ 003A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u003A",
                      new String[] { "\u0061\u2060\u0308" });
 
-    // ÷ 0061 ? 2060 ÷ 002C ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0061 ? 2060 ÷ 002C ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u002C",
                      new String[] { "\u0061\u2060" });
 
-    // ÷ 0061 ? 2060 ? 0308 ÷ 002C ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0308 ÷ 002C ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u002C",
                      new String[] { "\u0061\u2060\u0308" });
 
-    // ÷ 0061 ? 2060 ÷ 0027 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0061 ? 2060 ÷ 0027 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0027",
                      new String[] { "\u0061\u2060" });
 
-    // ÷ 0061 ? 2060 ? 0308 ÷ 0027 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0308 ÷ 0027 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u0027",
                      new String[] { "\u0061\u2060\u0308" });
 
-    // ÷ 0061 ? 2060 ? 0030 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [9.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0030 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [9.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0030",
                      new String[] { "\u0061\u2060\u0030" });
 
-    // ÷ 0061 ? 2060 ? 0308 ? 0030 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [9.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0308 ? 0030 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [9.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u0030",
                      new String[] { "\u0061\u2060\u0308\u0030" });
 
-    // ÷ 0061 ? 2060 ? 005F ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [13.1] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 005F ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [13.1] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u005F",
                      new String[] { "\u0061\u2060\u005F" });
 
-    // ÷ 0061 ? 2060 ? 0308 ? 005F ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.1] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0308 ? 005F ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [13.1] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u005F",
                      new String[] { "\u0061\u2060\u0308\u005F" });
 
-    // ÷ 0061 ? 2060 ? 00AD ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 00AD ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u00AD",
                      new String[] { "\u0061\u2060\u00AD" });
 
-    // ÷ 0061 ? 2060 ? 0308 ? 00AD ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0308 ? 00AD ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u00AD",
                      new String[] { "\u0061\u2060\u0308\u00AD" });
 
-    // ÷ 0061 ? 2060 ? 0300 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0300 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0300",
                      new String[] { "\u0061\u2060\u0300" });
 
-    // ÷ 0061 ? 2060 ? 0308 ? 0300 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0308 ? 0300 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u0300",
                      new String[] { "\u0061\u2060\u0308\u0300" });
 
-    // ÷ 0061 ? 2060 ? 0061 ? 2060 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0061 ? 2060 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0061\u2060",
                      new String[] { "\u0061\u2060\u0061\u2060" });
 
-    // ÷ 0061 ? 2060 ? 0308 ? 0061 ? 2060 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0308 ? 0061 ? 2060 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u0061\u2060",
                      new String[] { "\u0061\u2060\u0308\u0061\u2060" });
 
-    // ÷ 0061 ? 2060 ? 0061 ÷ 003A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0061 ÷ 003A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0061\u003A",
                      new String[] { "\u0061\u2060\u0061" });
 
-    // ÷ 0061 ? 2060 ? 0308 ? 0061 ÷ 003A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0308 ? 0061 ÷ 003A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u0061\u003A",
                      new String[] { "\u0061\u2060\u0308\u0061" });
 
-    // ÷ 0061 ? 2060 ? 0061 ÷ 0027 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0061 ÷ 0027 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0061\u0027",
                      new String[] { "\u0061\u2060\u0061" });
 
-    // ÷ 0061 ? 2060 ? 0308 ? 0061 ÷ 0027 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0308 ? 0061 ÷ 0027 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u0061\u0027",
                      new String[] { "\u0061\u2060\u0308\u0061" });
 
-    // ÷ 0061 ? 2060 ? 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0061\u0027\u2060",
                      new String[] { "\u0061\u2060\u0061" });
 
-    // ÷ 0061 ? 2060 ? 0308 ? 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0308 ? 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u0061\u0027\u2060",
                      new String[] { "\u0061\u2060\u0308\u0061" });
 
-    // ÷ 0061 ? 2060 ? 0061 ÷ 002C ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0061 ÷ 002C ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0061\u002C",
                      new String[] { "\u0061\u2060\u0061" });
 
-    // ÷ 0061 ? 2060 ? 0308 ? 0061 ÷ 002C ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0308 ? 0061 ÷ 002C ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u0061\u002C",
                      new String[] { "\u0061\u2060\u0308\u0061" });
 
-    // ÷ 0061 ? 2060 ? 0031 ÷ 003A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [9.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0031 ÷ 003A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [9.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0031\u003A",
                      new String[] { "\u0061\u2060\u0031" });
 
-    // ÷ 0061 ? 2060 ? 0308 ? 0031 ÷ 003A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [9.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0308 ? 0031 ÷ 003A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [9.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u0031\u003A",
                      new String[] { "\u0061\u2060\u0308\u0031" });
 
-    // ÷ 0061 ? 2060 ? 0031 ÷ 0027 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [9.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0031 ÷ 0027 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [9.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0031\u0027",
                      new String[] { "\u0061\u2060\u0031" });
 
-    // ÷ 0061 ? 2060 ? 0308 ? 0031 ÷ 0027 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [9.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0308 ? 0031 ÷ 0027 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [9.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u0031\u0027",
                      new String[] { "\u0061\u2060\u0308\u0031" });
 
-    // ÷ 0061 ? 2060 ? 0031 ÷ 002C ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [9.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0031 ÷ 002C ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [9.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0031\u002C",
                      new String[] { "\u0061\u2060\u0031" });
 
-    // ÷ 0061 ? 2060 ? 0308 ? 0031 ÷ 002C ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [9.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0308 ? 0031 ÷ 002C ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [9.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u0031\u002C",
                      new String[] { "\u0061\u2060\u0308\u0031" });
 
-    // ÷ 0061 ? 2060 ? 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [9.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [9.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0031\u002E\u2060",
                      new String[] { "\u0061\u2060\u0031" });
 
-    // ÷ 0061 ? 2060 ? 0308 ? 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [9.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0061 ? 2060 ? 0308 ? 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [9.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u0031\u002E\u2060",
                      new String[] { "\u0061\u2060\u0308\u0031" });
 
-    // ÷ 0061 ÷ 003A ÷ 0001 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 0061 ÷ 003A ÷ 0001 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0001",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 003A ? 0308 ÷ 0001 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 0061 ÷ 003A ? 0308 ÷ 0001 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u0001",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 003A ÷ 000D ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 0061 ÷ 003A ÷ 000D ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\r",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 003A ? 0308 ÷ 000D ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 0061 ÷ 003A ? 0308 ÷ 000D ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\r",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 003A ÷ 000A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 0061 ÷ 003A ÷ 000A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\n",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 003A ? 0308 ÷ 000A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 0061 ÷ 003A ? 0308 ÷ 000A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\n",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 003A ÷ 000B ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 0061 ÷ 003A ÷ 000B ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u000B",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 003A ? 0308 ÷ 000B ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 0061 ÷ 003A ? 0308 ÷ 000B ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u000B",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 003A ÷ 3031 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 0061 ÷ 003A ÷ 3031 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u3031",
                      new String[] { "\u0061", "\u3031" });
 
-    // ÷ 0061 ÷ 003A ? 0308 ÷ 3031 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 0061 ÷ 003A ? 0308 ÷ 3031 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u3031",
                      new String[] { "\u0061", "\u3031" });
 
-    // ÷ 0061 ? 003A ? 0041 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] COLON (MidLetter) ? [7.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 0061 ? 003A ? 0041 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] COLON (MidLetter) ? [7.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0041",
                      new String[] { "\u0061\u003A\u0041" });
 
-    // ÷ 0061 ? 003A ? 0308 ? 0041 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 0061 ? 003A ? 0308 ? 0041 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u0041",
                      new String[] { "\u0061\u003A\u0308\u0041" });
 
-    // ÷ 0061 ÷ 003A ÷ 003A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0061 ÷ 003A ÷ 003A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u003A",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 003A ? 0308 ÷ 003A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0061 ÷ 003A ? 0308 ÷ 003A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u003A",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 003A ÷ 002C ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0061 ÷ 003A ÷ 002C ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u002C",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 003A ? 0308 ÷ 002C ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0061 ÷ 003A ? 0308 ÷ 002C ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u002C",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 003A ÷ 0027 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0061 ÷ 003A ÷ 0027 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0027",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 003A ? 0308 ÷ 0027 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0061 ÷ 003A ? 0308 ÷ 0027 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u0027",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 003A ÷ 0030 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 0061 ÷ 003A ÷ 0030 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0030",
                      new String[] { "\u0061", "\u0030" });
 
-    // ÷ 0061 ÷ 003A ? 0308 ÷ 0030 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 0061 ÷ 003A ? 0308 ÷ 0030 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u0030",
                      new String[] { "\u0061", "\u0030" });
 
-    // ÷ 0061 ÷ 003A ÷ 005F ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 0061 ÷ 003A ÷ 005F ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u005F",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 003A ? 0308 ÷ 005F ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 0061 ÷ 003A ? 0308 ÷ 005F ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u005F",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 003A ? 00AD ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 0061 ÷ 003A ? 00AD ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u00AD",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 003A ? 0308 ? 00AD ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 0061 ÷ 003A ? 0308 ? 00AD ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u00AD",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 003A ? 0300 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 0061 ÷ 003A ? 0300 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0300",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 003A ? 0308 ? 0300 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 0061 ÷ 003A ? 0308 ? 0300 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u0300",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ? 003A ? 0061 ? 2060 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] COLON (MidLetter) ? [7.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0061 ? 003A ? 0061 ? 2060 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] COLON (MidLetter) ? [7.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0061\u2060",
                      new String[] { "\u0061\u003A\u0061\u2060" });
 
-    // ÷ 0061 ? 003A ? 0308 ? 0061 ? 2060 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0061 ? 003A ? 0308 ? 0061 ? 2060 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u0061\u2060",
                      new String[] { "\u0061\u003A\u0308\u0061\u2060" });
 
-    // ÷ 0061 ? 003A ? 0061 ÷ 003A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] COLON (MidLetter) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0061 ? 003A ? 0061 ÷ 003A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] COLON (MidLetter) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0061\u003A",
                      new String[] { "\u0061\u003A\u0061" });
 
-    // ÷ 0061 ? 003A ? 0308 ? 0061 ÷ 003A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0061 ? 003A ? 0308 ? 0061 ÷ 003A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u0061\u003A",
                      new String[] { "\u0061\u003A\u0308\u0061" });
 
-    // ÷ 0061 ? 003A ? 0061 ÷ 0027 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] COLON (MidLetter) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0061 ? 003A ? 0061 ÷ 0027 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] COLON (MidLetter) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0061\u0027",
                      new String[] { "\u0061\u003A\u0061" });
 
-    // ÷ 0061 ? 003A ? 0308 ? 0061 ÷ 0027 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0061 ? 003A ? 0308 ? 0061 ÷ 0027 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u0061\u0027",
                      new String[] { "\u0061\u003A\u0308\u0061" });
 
-    // ÷ 0061 ? 003A ? 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] COLON (MidLetter) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0061 ? 003A ? 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] COLON (MidLetter) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0061\u0027\u2060",
                      new String[] { "\u0061\u003A\u0061" });
 
-    // ÷ 0061 ? 003A ? 0308 ? 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0061 ? 003A ? 0308 ? 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u0061\u0027\u2060",
                      new String[] { "\u0061\u003A\u0308\u0061" });
 
-    // ÷ 0061 ? 003A ? 0061 ÷ 002C ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] COLON (MidLetter) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0061 ? 003A ? 0061 ÷ 002C ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] COLON (MidLetter) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0061\u002C",
                      new String[] { "\u0061\u003A\u0061" });
 
-    // ÷ 0061 ? 003A ? 0308 ? 0061 ÷ 002C ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0061 ? 003A ? 0308 ? 0061 ÷ 002C ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u0061\u002C",
                      new String[] { "\u0061\u003A\u0308\u0061" });
 
-    // ÷ 0061 ÷ 003A ÷ 0031 ÷ 003A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0061 ÷ 003A ÷ 0031 ÷ 003A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0031\u003A",
                      new String[] { "\u0061", "\u0031" });
 
-    // ÷ 0061 ÷ 003A ? 0308 ÷ 0031 ÷ 003A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0061 ÷ 003A ? 0308 ÷ 0031 ÷ 003A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u0031\u003A",
                      new String[] { "\u0061", "\u0031" });
 
-    // ÷ 0061 ÷ 003A ÷ 0031 ÷ 0027 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0061 ÷ 003A ÷ 0031 ÷ 0027 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0031\u0027",
                      new String[] { "\u0061", "\u0031" });
 
-    // ÷ 0061 ÷ 003A ? 0308 ÷ 0031 ÷ 0027 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0061 ÷ 003A ? 0308 ÷ 0031 ÷ 0027 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u0031\u0027",
                      new String[] { "\u0061", "\u0031" });
 
-    // ÷ 0061 ÷ 003A ÷ 0031 ÷ 002C ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0061 ÷ 003A ÷ 0031 ÷ 002C ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0031\u002C",
                      new String[] { "\u0061", "\u0031" });
 
-    // ÷ 0061 ÷ 003A ? 0308 ÷ 0031 ÷ 002C ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0061 ÷ 003A ? 0308 ÷ 0031 ÷ 002C ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u0031\u002C",
                      new String[] { "\u0061", "\u0031" });
 
-    // ÷ 0061 ÷ 003A ÷ 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0061 ÷ 003A ÷ 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0031\u002E\u2060",
                      new String[] { "\u0061", "\u0031" });
 
-    // ÷ 0061 ÷ 003A ? 0308 ÷ 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0061 ÷ 003A ? 0308 ÷ 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u0031\u002E\u2060",
                      new String[] { "\u0061", "\u0031" });
 
-    // ÷ 0061 ÷ 0027 ÷ 0001 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ÷ 0001 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0001",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ? 0308 ÷ 0001 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 0308 ÷ 0001 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u0001",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ÷ 000D ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ÷ 000D ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\r",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ? 0308 ÷ 000D ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 0308 ÷ 000D ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\r",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ÷ 000A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ÷ 000A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\n",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ? 0308 ÷ 000A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 0308 ÷ 000A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\n",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ÷ 000B ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ÷ 000B ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u000B",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ? 0308 ÷ 000B ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 0308 ÷ 000B ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u000B",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ÷ 3031 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ÷ 3031 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u3031",
                      new String[] { "\u0061", "\u3031" });
 
-    // ÷ 0061 ÷ 0027 ? 0308 ÷ 3031 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 0308 ÷ 3031 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u3031",
                      new String[] { "\u0061", "\u3031" });
 
-    // ÷ 0061 ? 0027 ? 0041 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [7.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 0061 ? 0027 ? 0041 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [7.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0041",
                      new String[] { "\u0061\u0027\u0041" });
 
-    // ÷ 0061 ? 0027 ? 0308 ? 0041 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 0061 ? 0027 ? 0308 ? 0041 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u0041",
                      new String[] { "\u0061\u0027\u0308\u0041" });
 
-    // ÷ 0061 ÷ 0027 ÷ 003A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ÷ 003A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u003A",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ? 0308 ÷ 003A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 0308 ÷ 003A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u003A",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ÷ 002C ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ÷ 002C ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u002C",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ? 0308 ÷ 002C ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 0308 ÷ 002C ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u002C",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ÷ 0027 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ÷ 0027 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0027",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ? 0308 ÷ 0027 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 0308 ÷ 0027 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u0027",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ÷ 0030 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ÷ 0030 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0030",
                      new String[] { "\u0061", "\u0030" });
 
-    // ÷ 0061 ÷ 0027 ? 0308 ÷ 0030 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 0308 ÷ 0030 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u0030",
                      new String[] { "\u0061", "\u0030" });
 
-    // ÷ 0061 ÷ 0027 ÷ 005F ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ÷ 005F ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u005F",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ? 0308 ÷ 005F ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 0308 ÷ 005F ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u005F",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ? 00AD ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 00AD ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u00AD",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ? 0308 ? 00AD ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 0308 ? 00AD ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u00AD",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ? 0300 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 0300 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0300",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ? 0308 ? 0300 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 0308 ? 0300 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u0300",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ? 0027 ? 0061 ? 2060 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [7.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0061 ? 0027 ? 0061 ? 2060 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [7.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0061\u2060",
                      new String[] { "\u0061\u0027\u0061\u2060" });
 
-    // ÷ 0061 ? 0027 ? 0308 ? 0061 ? 2060 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0061 ? 0027 ? 0308 ? 0061 ? 2060 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u0061\u2060",
                      new String[] { "\u0061\u0027\u0308\u0061\u2060" });
 
-    // ÷ 0061 ? 0027 ? 0061 ÷ 003A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0061 ? 0027 ? 0061 ÷ 003A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0061\u003A",
                      new String[] { "\u0061\u0027\u0061" });
 
-    // ÷ 0061 ? 0027 ? 0308 ? 0061 ÷ 003A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0061 ? 0027 ? 0308 ? 0061 ÷ 003A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u0061\u003A",
                      new String[] { "\u0061\u0027\u0308\u0061" });
 
-    // ÷ 0061 ? 0027 ? 0061 ÷ 0027 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0061 ? 0027 ? 0061 ÷ 0027 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0061\u0027",
                      new String[] { "\u0061\u0027\u0061" });
 
-    // ÷ 0061 ? 0027 ? 0308 ? 0061 ÷ 0027 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0061 ? 0027 ? 0308 ? 0061 ÷ 0027 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u0061\u0027",
                      new String[] { "\u0061\u0027\u0308\u0061" });
 
-    // ÷ 0061 ? 0027 ? 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0061 ? 0027 ? 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0061\u0027\u2060",
                      new String[] { "\u0061\u0027\u0061" });
 
-    // ÷ 0061 ? 0027 ? 0308 ? 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0061 ? 0027 ? 0308 ? 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u0061\u0027\u2060",
                      new String[] { "\u0061\u0027\u0308\u0061" });
 
-    // ÷ 0061 ? 0027 ? 0061 ÷ 002C ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0061 ? 0027 ? 0061 ÷ 002C ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0061\u002C",
                      new String[] { "\u0061\u0027\u0061" });
 
-    // ÷ 0061 ? 0027 ? 0308 ? 0061 ÷ 002C ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0061 ? 0027 ? 0308 ? 0061 ÷ 002C ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u0061\u002C",
                      new String[] { "\u0061\u0027\u0308\u0061" });
 
-    // ÷ 0061 ÷ 0027 ÷ 0031 ÷ 003A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ÷ 0031 ÷ 003A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0031\u003A",
                      new String[] { "\u0061", "\u0031" });
 
-    // ÷ 0061 ÷ 0027 ? 0308 ÷ 0031 ÷ 003A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 0308 ÷ 0031 ÷ 003A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u0031\u003A",
                      new String[] { "\u0061", "\u0031" });
 
-    // ÷ 0061 ÷ 0027 ÷ 0031 ÷ 0027 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ÷ 0031 ÷ 0027 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0031\u0027",
                      new String[] { "\u0061", "\u0031" });
 
-    // ÷ 0061 ÷ 0027 ? 0308 ÷ 0031 ÷ 0027 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 0308 ÷ 0031 ÷ 0027 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u0031\u0027",
                      new String[] { "\u0061", "\u0031" });
 
-    // ÷ 0061 ÷ 0027 ÷ 0031 ÷ 002C ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ÷ 0031 ÷ 002C ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0031\u002C",
                      new String[] { "\u0061", "\u0031" });
 
-    // ÷ 0061 ÷ 0027 ? 0308 ÷ 0031 ÷ 002C ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 0308 ÷ 0031 ÷ 002C ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u0031\u002C",
                      new String[] { "\u0061", "\u0031" });
 
-    // ÷ 0061 ÷ 0027 ÷ 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ÷ 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0031\u002E\u2060",
                      new String[] { "\u0061", "\u0031" });
 
-    // ÷ 0061 ÷ 0027 ? 0308 ÷ 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 0308 ÷ 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u0031\u002E\u2060",
                      new String[] { "\u0061", "\u0031" });
 
-    // ÷ 0061 ÷ 0027 ? 2060 ÷ 0001 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 2060 ÷ 0001 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0001",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ? 2060 ? 0308 ÷ 0001 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 2060 ? 0308 ÷ 0001 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u0001",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ? 2060 ÷ 000D ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 2060 ÷ 000D ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\r",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ? 2060 ? 0308 ÷ 000D ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 2060 ? 0308 ÷ 000D ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\r",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ? 2060 ÷ 000A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 2060 ÷ 000A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\n",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ? 2060 ? 0308 ÷ 000A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 2060 ? 0308 ÷ 000A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\n",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ? 2060 ÷ 000B ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 2060 ÷ 000B ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u000B",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ? 2060 ? 0308 ÷ 000B ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 2060 ? 0308 ÷ 000B ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u000B",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ? 2060 ÷ 3031 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 2060 ÷ 3031 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u3031",
                      new String[] { "\u0061", "\u3031" });
 
-    // ÷ 0061 ÷ 0027 ? 2060 ? 0308 ÷ 3031 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 2060 ? 0308 ÷ 3031 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u3031",
                      new String[] { "\u0061", "\u3031" });
 
-    // ÷ 0061 ? 0027 ? 2060 ? 0041 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [7.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 0061 ? 0027 ? 2060 ? 0041 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [7.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0041",
                      new String[] { "\u0061\u0027\u2060\u0041" });
 
-    // ÷ 0061 ? 0027 ? 2060 ? 0308 ? 0041 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 0061 ? 0027 ? 2060 ? 0308 ? 0041 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u0041",
                      new String[] { "\u0061\u0027\u2060\u0308\u0041" });
 
-    // ÷ 0061 ÷ 0027 ? 2060 ÷ 003A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 2060 ÷ 003A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u003A",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ? 2060 ? 0308 ÷ 003A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 2060 ? 0308 ÷ 003A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u003A",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ? 2060 ÷ 002C ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 2060 ÷ 002C ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u002C",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ? 2060 ? 0308 ÷ 002C ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 2060 ? 0308 ÷ 002C ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u002C",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ? 2060 ÷ 0027 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 2060 ÷ 0027 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0027",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ? 2060 ? 0308 ÷ 0027 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 2060 ? 0308 ÷ 0027 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u0027",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ? 2060 ÷ 0030 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 2060 ÷ 0030 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0030",
                      new String[] { "\u0061", "\u0030" });
 
-    // ÷ 0061 ÷ 0027 ? 2060 ? 0308 ÷ 0030 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 2060 ? 0308 ÷ 0030 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u0030",
                      new String[] { "\u0061", "\u0030" });
 
-    // ÷ 0061 ÷ 0027 ? 2060 ÷ 005F ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 2060 ÷ 005F ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u005F",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ? 2060 ? 0308 ÷ 005F ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 2060 ? 0308 ÷ 005F ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u005F",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ? 2060 ? 00AD ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 2060 ? 00AD ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u00AD",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ? 2060 ? 0308 ? 00AD ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 2060 ? 0308 ? 00AD ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u00AD",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ? 2060 ? 0300 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 2060 ? 0300 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0300",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 0027 ? 2060 ? 0308 ? 0300 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 2060 ? 0308 ? 0300 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u0300",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ? 0027 ? 2060 ? 0061 ? 2060 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0061 ? 0027 ? 2060 ? 0061 ? 2060 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0061\u2060",
                      new String[] { "\u0061\u0027\u2060\u0061\u2060" });
 
-    // ÷ 0061 ? 0027 ? 2060 ? 0308 ? 0061 ? 2060 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0061 ? 0027 ? 2060 ? 0308 ? 0061 ? 2060 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u0061\u2060",
                      new String[] { "\u0061\u0027\u2060\u0308\u0061\u2060" });
 
-    // ÷ 0061 ? 0027 ? 2060 ? 0061 ÷ 003A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0061 ? 0027 ? 2060 ? 0061 ÷ 003A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0061\u003A",
                      new String[] { "\u0061\u0027\u2060\u0061" });
 
-    // ÷ 0061 ? 0027 ? 2060 ? 0308 ? 0061 ÷ 003A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0061 ? 0027 ? 2060 ? 0308 ? 0061 ÷ 003A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u0061\u003A",
                      new String[] { "\u0061\u0027\u2060\u0308\u0061" });
 
-    // ÷ 0061 ? 0027 ? 2060 ? 0061 ÷ 0027 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0061 ? 0027 ? 2060 ? 0061 ÷ 0027 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0061\u0027",
                      new String[] { "\u0061\u0027\u2060\u0061" });
 
-    // ÷ 0061 ? 0027 ? 2060 ? 0308 ? 0061 ÷ 0027 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0061 ? 0027 ? 2060 ? 0308 ? 0061 ÷ 0027 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u0061\u0027",
                      new String[] { "\u0061\u0027\u2060\u0308\u0061" });
 
-    // ÷ 0061 ? 0027 ? 2060 ? 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0061 ? 0027 ? 2060 ? 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0061\u0027\u2060",
                      new String[] { "\u0061\u0027\u2060\u0061" });
 
-    // ÷ 0061 ? 0027 ? 2060 ? 0308 ? 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0061 ? 0027 ? 2060 ? 0308 ? 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u0061\u0027\u2060",
                      new String[] { "\u0061\u0027\u2060\u0308\u0061" });
 
-    // ÷ 0061 ? 0027 ? 2060 ? 0061 ÷ 002C ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0061 ? 0027 ? 2060 ? 0061 ÷ 002C ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0061\u002C",
                      new String[] { "\u0061\u0027\u2060\u0061" });
 
-    // ÷ 0061 ? 0027 ? 2060 ? 0308 ? 0061 ÷ 002C ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0061 ? 0027 ? 2060 ? 0308 ? 0061 ÷ 002C ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [7.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u0061\u002C",
                      new String[] { "\u0061\u0027\u2060\u0308\u0061" });
 
-    // ÷ 0061 ÷ 0027 ? 2060 ÷ 0031 ÷ 003A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 2060 ÷ 0031 ÷ 003A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0031\u003A",
                      new String[] { "\u0061", "\u0031" });
 
-    // ÷ 0061 ÷ 0027 ? 2060 ? 0308 ÷ 0031 ÷ 003A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 2060 ? 0308 ÷ 0031 ÷ 003A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u0031\u003A",
                      new String[] { "\u0061", "\u0031" });
 
-    // ÷ 0061 ÷ 0027 ? 2060 ÷ 0031 ÷ 0027 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 2060 ÷ 0031 ÷ 0027 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0031\u0027",
                      new String[] { "\u0061", "\u0031" });
 
-    // ÷ 0061 ÷ 0027 ? 2060 ? 0308 ÷ 0031 ÷ 0027 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 2060 ? 0308 ÷ 0031 ÷ 0027 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u0031\u0027",
                      new String[] { "\u0061", "\u0031" });
 
-    // ÷ 0061 ÷ 0027 ? 2060 ÷ 0031 ÷ 002C ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 2060 ÷ 0031 ÷ 002C ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0031\u002C",
                      new String[] { "\u0061", "\u0031" });
 
-    // ÷ 0061 ÷ 0027 ? 2060 ? 0308 ÷ 0031 ÷ 002C ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 2060 ? 0308 ÷ 0031 ÷ 002C ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u0031\u002C",
                      new String[] { "\u0061", "\u0031" });
 
-    // ÷ 0061 ÷ 0027 ? 2060 ÷ 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 2060 ÷ 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0031\u002E\u2060",
                      new String[] { "\u0061", "\u0031" });
 
-    // ÷ 0061 ÷ 0027 ? 2060 ? 0308 ÷ 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0061 ÷ 0027 ? 2060 ? 0308 ÷ 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u0031\u002E\u2060",
                      new String[] { "\u0061", "\u0031" });
 
-    // ÷ 0061 ÷ 002C ÷ 0001 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ÷ 0001 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0001",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 002C ? 0308 ÷ 0001 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ? 0308 ÷ 0001 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u0001",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 002C ÷ 000D ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ÷ 000D ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\r",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 002C ? 0308 ÷ 000D ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ? 0308 ÷ 000D ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\r",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 002C ÷ 000A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ÷ 000A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\n",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 002C ? 0308 ÷ 000A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ? 0308 ÷ 000A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\n",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 002C ÷ 000B ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ÷ 000B ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u000B",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 002C ? 0308 ÷ 000B ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ? 0308 ÷ 000B ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u000B",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 002C ÷ 3031 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ÷ 3031 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u3031",
                      new String[] { "\u0061", "\u3031" });
 
-    // ÷ 0061 ÷ 002C ? 0308 ÷ 3031 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ? 0308 ÷ 3031 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u3031",
                      new String[] { "\u0061", "\u3031" });
 
-    // ÷ 0061 ÷ 002C ÷ 0041 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ÷ 0041 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0041",
                      new String[] { "\u0061", "\u0041" });
 
-    // ÷ 0061 ÷ 002C ? 0308 ÷ 0041 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ? 0308 ÷ 0041 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u0041",
                      new String[] { "\u0061", "\u0041" });
 
-    // ÷ 0061 ÷ 002C ÷ 003A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ÷ 003A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u003A",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 002C ? 0308 ÷ 003A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ? 0308 ÷ 003A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u003A",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 002C ÷ 002C ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ÷ 002C ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u002C",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 002C ? 0308 ÷ 002C ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ? 0308 ÷ 002C ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u002C",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 002C ÷ 0027 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ÷ 0027 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0027",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 002C ? 0308 ÷ 0027 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ? 0308 ÷ 0027 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u0027",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 002C ÷ 0030 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ÷ 0030 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0030",
                      new String[] { "\u0061", "\u0030" });
 
-    // ÷ 0061 ÷ 002C ? 0308 ÷ 0030 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ? 0308 ÷ 0030 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u0030",
                      new String[] { "\u0061", "\u0030" });
 
-    // ÷ 0061 ÷ 002C ÷ 005F ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ÷ 005F ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u005F",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 002C ? 0308 ÷ 005F ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ? 0308 ÷ 005F ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u005F",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 002C ? 00AD ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ? 00AD ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u00AD",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 002C ? 0308 ? 00AD ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ? 0308 ? 00AD ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u00AD",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 002C ? 0300 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ? 0300 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0300",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 002C ? 0308 ? 0300 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ? 0308 ? 0300 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u0300",
                      new String[] { "\u0061" });
 
-    // ÷ 0061 ÷ 002C ÷ 0061 ? 2060 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ÷ 0061 ? 2060 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0061\u2060",
                      new String[] { "\u0061", "\u0061\u2060" });
 
-    // ÷ 0061 ÷ 002C ? 0308 ÷ 0061 ? 2060 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ? 0308 ÷ 0061 ? 2060 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u0061\u2060",
                      new String[] { "\u0061", "\u0061\u2060" });
 
-    // ÷ 0061 ÷ 002C ÷ 0061 ÷ 003A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ÷ 0061 ÷ 003A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0061\u003A",
                      new String[] { "\u0061", "\u0061" });
 
-    // ÷ 0061 ÷ 002C ? 0308 ÷ 0061 ÷ 003A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ? 0308 ÷ 0061 ÷ 003A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u0061\u003A",
                      new String[] { "\u0061", "\u0061" });
 
-    // ÷ 0061 ÷ 002C ÷ 0061 ÷ 0027 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ÷ 0061 ÷ 0027 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0061\u0027",
                      new String[] { "\u0061", "\u0061" });
 
-    // ÷ 0061 ÷ 002C ? 0308 ÷ 0061 ÷ 0027 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ? 0308 ÷ 0061 ÷ 0027 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u0061\u0027",
                      new String[] { "\u0061", "\u0061" });
 
-    // ÷ 0061 ÷ 002C ÷ 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ÷ 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0061\u0027\u2060",
                      new String[] { "\u0061", "\u0061" });
 
-    // ÷ 0061 ÷ 002C ? 0308 ÷ 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ? 0308 ÷ 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u0061\u0027\u2060",
                      new String[] { "\u0061", "\u0061" });
 
-    // ÷ 0061 ÷ 002C ÷ 0061 ÷ 002C ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ÷ 0061 ÷ 002C ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0061\u002C",
                      new String[] { "\u0061", "\u0061" });
 
-    // ÷ 0061 ÷ 002C ? 0308 ÷ 0061 ÷ 002C ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ? 0308 ÷ 0061 ÷ 002C ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u0061\u002C",
                      new String[] { "\u0061", "\u0061" });
 
-    // ÷ 0061 ÷ 002C ÷ 0031 ÷ 003A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ÷ 0031 ÷ 003A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0031\u003A",
                      new String[] { "\u0061", "\u0031" });
 
-    // ÷ 0061 ÷ 002C ? 0308 ÷ 0031 ÷ 003A ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ? 0308 ÷ 0031 ÷ 003A ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u0031\u003A",
                      new String[] { "\u0061", "\u0031" });
 
-    // ÷ 0061 ÷ 002C ÷ 0031 ÷ 0027 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ÷ 0031 ÷ 0027 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0031\u0027",
                      new String[] { "\u0061", "\u0031" });
 
-    // ÷ 0061 ÷ 002C ? 0308 ÷ 0031 ÷ 0027 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ? 0308 ÷ 0031 ÷ 0027 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u0031\u0027",
                      new String[] { "\u0061", "\u0031" });
 
-    // ÷ 0061 ÷ 002C ÷ 0031 ÷ 002C ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ÷ 0031 ÷ 002C ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0031\u002C",
                      new String[] { "\u0061", "\u0031" });
 
-    // ÷ 0061 ÷ 002C ? 0308 ÷ 0031 ÷ 002C ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ? 0308 ÷ 0031 ÷ 002C ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u0031\u002C",
                      new String[] { "\u0061", "\u0031" });
 
-    // ÷ 0061 ÷ 002C ÷ 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ÷ 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0031\u002E\u2060",
                      new String[] { "\u0061", "\u0031" });
 
-    // ÷ 0061 ÷ 002C ? 0308 ÷ 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0061 ÷ 002C ? 0308 ÷ 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u0031\u002E\u2060",
                      new String[] { "\u0061", "\u0031" });
 
-    // ÷ 0031 ÷ 003A ÷ 0001 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ÷ 0001 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0001",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 003A ? 0308 ÷ 0001 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ? 0308 ÷ 0001 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u0001",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 003A ÷ 000D ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ÷ 000D ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\r",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 003A ? 0308 ÷ 000D ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ? 0308 ÷ 000D ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\r",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 003A ÷ 000A ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ÷ 000A ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\n",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 003A ? 0308 ÷ 000A ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ? 0308 ÷ 000A ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\n",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 003A ÷ 000B ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ÷ 000B ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u000B",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 003A ? 0308 ÷ 000B ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ? 0308 ÷ 000B ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u000B",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 003A ÷ 3031 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ÷ 3031 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u3031",
                      new String[] { "\u0031", "\u3031" });
 
-    // ÷ 0031 ÷ 003A ? 0308 ÷ 3031 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ? 0308 ÷ 3031 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u3031",
                      new String[] { "\u0031", "\u3031" });
 
-    // ÷ 0031 ÷ 003A ÷ 0041 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ÷ 0041 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0041",
                      new String[] { "\u0031", "\u0041" });
 
-    // ÷ 0031 ÷ 003A ? 0308 ÷ 0041 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ? 0308 ÷ 0041 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u0041",
                      new String[] { "\u0031", "\u0041" });
 
-    // ÷ 0031 ÷ 003A ÷ 003A ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ÷ 003A ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u003A",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 003A ? 0308 ÷ 003A ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ? 0308 ÷ 003A ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u003A",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 003A ÷ 002C ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ÷ 002C ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u002C",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 003A ? 0308 ÷ 002C ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ? 0308 ÷ 002C ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u002C",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 003A ÷ 0027 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ÷ 0027 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0027",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 003A ? 0308 ÷ 0027 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ? 0308 ÷ 0027 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u0027",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 003A ÷ 0030 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ÷ 0030 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0030",
                      new String[] { "\u0031", "\u0030" });
 
-    // ÷ 0031 ÷ 003A ? 0308 ÷ 0030 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ? 0308 ÷ 0030 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u0030",
                      new String[] { "\u0031", "\u0030" });
 
-    // ÷ 0031 ÷ 003A ÷ 005F ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ÷ 005F ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u005F",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 003A ? 0308 ÷ 005F ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ? 0308 ÷ 005F ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u005F",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 003A ? 00AD ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ? 00AD ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u00AD",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 003A ? 0308 ? 00AD ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ? 0308 ? 00AD ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u00AD",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 003A ? 0300 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ? 0300 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0300",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 003A ? 0308 ? 0300 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ? 0308 ? 0300 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u0300",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 003A ÷ 0061 ? 2060 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ÷ 0061 ? 2060 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0061\u2060",
                      new String[] { "\u0031", "\u0061\u2060" });
 
-    // ÷ 0031 ÷ 003A ? 0308 ÷ 0061 ? 2060 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ? 0308 ÷ 0061 ? 2060 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u0061\u2060",
                      new String[] { "\u0031", "\u0061\u2060" });
 
-    // ÷ 0031 ÷ 003A ÷ 0061 ÷ 003A ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ÷ 0061 ÷ 003A ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0061\u003A",
                      new String[] { "\u0031", "\u0061" });
 
-    // ÷ 0031 ÷ 003A ? 0308 ÷ 0061 ÷ 003A ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ? 0308 ÷ 0061 ÷ 003A ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u0061\u003A",
                      new String[] { "\u0031", "\u0061" });
 
-    // ÷ 0031 ÷ 003A ÷ 0061 ÷ 0027 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ÷ 0061 ÷ 0027 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0061\u0027",
                      new String[] { "\u0031", "\u0061" });
 
-    // ÷ 0031 ÷ 003A ? 0308 ÷ 0061 ÷ 0027 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ? 0308 ÷ 0061 ÷ 0027 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u0061\u0027",
                      new String[] { "\u0031", "\u0061" });
 
-    // ÷ 0031 ÷ 003A ÷ 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ÷ 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0061\u0027\u2060",
                      new String[] { "\u0031", "\u0061" });
 
-    // ÷ 0031 ÷ 003A ? 0308 ÷ 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ? 0308 ÷ 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u0061\u0027\u2060",
                      new String[] { "\u0031", "\u0061" });
 
-    // ÷ 0031 ÷ 003A ÷ 0061 ÷ 002C ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ÷ 0061 ÷ 002C ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0061\u002C",
                      new String[] { "\u0031", "\u0061" });
 
-    // ÷ 0031 ÷ 003A ? 0308 ÷ 0061 ÷ 002C ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ? 0308 ÷ 0061 ÷ 002C ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u0061\u002C",
                      new String[] { "\u0031", "\u0061" });
 
-    // ÷ 0031 ÷ 003A ÷ 0031 ÷ 003A ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ÷ 0031 ÷ 003A ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0031\u003A",
                      new String[] { "\u0031", "\u0031" });
 
-    // ÷ 0031 ÷ 003A ? 0308 ÷ 0031 ÷ 003A ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ? 0308 ÷ 0031 ÷ 003A ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u0031\u003A",
                      new String[] { "\u0031", "\u0031" });
 
-    // ÷ 0031 ÷ 003A ÷ 0031 ÷ 0027 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ÷ 0031 ÷ 0027 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0031\u0027",
                      new String[] { "\u0031", "\u0031" });
 
-    // ÷ 0031 ÷ 003A ? 0308 ÷ 0031 ÷ 0027 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ? 0308 ÷ 0031 ÷ 0027 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u0031\u0027",
                      new String[] { "\u0031", "\u0031" });
 
-    // ÷ 0031 ÷ 003A ÷ 0031 ÷ 002C ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ÷ 0031 ÷ 002C ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0031\u002C",
                      new String[] { "\u0031", "\u0031" });
 
-    // ÷ 0031 ÷ 003A ? 0308 ÷ 0031 ÷ 002C ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ? 0308 ÷ 0031 ÷ 002C ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u0031\u002C",
                      new String[] { "\u0031", "\u0031" });
 
-    // ÷ 0031 ÷ 003A ÷ 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ÷ 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0031\u002E\u2060",
                      new String[] { "\u0031", "\u0031" });
 
-    // ÷ 0031 ÷ 003A ? 0308 ÷ 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 003A ? 0308 ÷ 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u0031\u002E\u2060",
                      new String[] { "\u0031", "\u0031" });
 
-    // ÷ 0031 ÷ 0027 ÷ 0001 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ÷ 0001 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0001",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 0027 ? 0308 ÷ 0001 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ? 0308 ÷ 0001 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u0001",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 0027 ÷ 000D ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ÷ 000D ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\r",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 0027 ? 0308 ÷ 000D ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ? 0308 ÷ 000D ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\r",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 0027 ÷ 000A ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ÷ 000A ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\n",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 0027 ? 0308 ÷ 000A ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ? 0308 ÷ 000A ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\n",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 0027 ÷ 000B ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ÷ 000B ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u000B",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 0027 ? 0308 ÷ 000B ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ? 0308 ÷ 000B ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u000B",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 0027 ÷ 3031 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ÷ 3031 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u3031",
                      new String[] { "\u0031", "\u3031" });
 
-    // ÷ 0031 ÷ 0027 ? 0308 ÷ 3031 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ? 0308 ÷ 3031 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u3031",
                      new String[] { "\u0031", "\u3031" });
 
-    // ÷ 0031 ÷ 0027 ÷ 0041 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ÷ 0041 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0041",
                      new String[] { "\u0031", "\u0041" });
 
-    // ÷ 0031 ÷ 0027 ? 0308 ÷ 0041 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ? 0308 ÷ 0041 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u0041",
                      new String[] { "\u0031", "\u0041" });
 
-    // ÷ 0031 ÷ 0027 ÷ 003A ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ÷ 003A ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u003A",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 0027 ? 0308 ÷ 003A ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ? 0308 ÷ 003A ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u003A",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 0027 ÷ 002C ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ÷ 002C ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u002C",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 0027 ? 0308 ÷ 002C ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ? 0308 ÷ 002C ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u002C",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 0027 ÷ 0027 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ÷ 0027 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0027",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 0027 ? 0308 ÷ 0027 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ? 0308 ÷ 0027 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u0027",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ? 0027 ? 0030 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] APOSTROPHE (MidNumLet) ? [11.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 0031 ? 0027 ? 0030 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] APOSTROPHE (MidNumLet) ? [11.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0030",
                      new String[] { "\u0031\u0027\u0030" });
 
-    // ÷ 0031 ? 0027 ? 0308 ? 0030 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [11.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 0031 ? 0027 ? 0308 ? 0030 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [11.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u0030",
                      new String[] { "\u0031\u0027\u0308\u0030" });
 
-    // ÷ 0031 ÷ 0027 ÷ 005F ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ÷ 005F ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u005F",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 0027 ? 0308 ÷ 005F ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ? 0308 ÷ 005F ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u005F",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 0027 ? 00AD ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ? 00AD ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u00AD",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 0027 ? 0308 ? 00AD ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ? 0308 ? 00AD ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u00AD",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 0027 ? 0300 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ? 0300 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0300",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 0027 ? 0308 ? 0300 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ? 0308 ? 0300 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u0300",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 0027 ÷ 0061 ? 2060 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ÷ 0061 ? 2060 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0061\u2060",
                      new String[] { "\u0031", "\u0061\u2060" });
 
-    // ÷ 0031 ÷ 0027 ? 0308 ÷ 0061 ? 2060 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ? 0308 ÷ 0061 ? 2060 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u0061\u2060",
                      new String[] { "\u0031", "\u0061\u2060" });
 
-    // ÷ 0031 ÷ 0027 ÷ 0061 ÷ 003A ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ÷ 0061 ÷ 003A ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0061\u003A",
                      new String[] { "\u0031", "\u0061" });
 
-    // ÷ 0031 ÷ 0027 ? 0308 ÷ 0061 ÷ 003A ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ? 0308 ÷ 0061 ÷ 003A ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u0061\u003A",
                      new String[] { "\u0031", "\u0061" });
 
-    // ÷ 0031 ÷ 0027 ÷ 0061 ÷ 0027 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ÷ 0061 ÷ 0027 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0061\u0027",
                      new String[] { "\u0031", "\u0061" });
 
-    // ÷ 0031 ÷ 0027 ? 0308 ÷ 0061 ÷ 0027 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ? 0308 ÷ 0061 ÷ 0027 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u0061\u0027",
                      new String[] { "\u0031", "\u0061" });
 
-    // ÷ 0031 ÷ 0027 ÷ 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ÷ 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0061\u0027\u2060",
                      new String[] { "\u0031", "\u0061" });
 
-    // ÷ 0031 ÷ 0027 ? 0308 ÷ 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ? 0308 ÷ 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u0061\u0027\u2060",
                      new String[] { "\u0031", "\u0061" });
 
-    // ÷ 0031 ÷ 0027 ÷ 0061 ÷ 002C ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ÷ 0061 ÷ 002C ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0061\u002C",
                      new String[] { "\u0031", "\u0061" });
 
-    // ÷ 0031 ÷ 0027 ? 0308 ÷ 0061 ÷ 002C ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0031 ÷ 0027 ? 0308 ÷ 0061 ÷ 002C ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u0061\u002C",
                      new String[] { "\u0031", "\u0061" });
 
-    // ÷ 0031 ? 0027 ? 0031 ÷ 003A ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] APOSTROPHE (MidNumLet) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0031 ? 0027 ? 0031 ÷ 003A ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] APOSTROPHE (MidNumLet) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0031\u003A",
                      new String[] { "\u0031\u0027\u0031" });
 
-    // ÷ 0031 ? 0027 ? 0308 ? 0031 ÷ 003A ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0031 ? 0027 ? 0308 ? 0031 ÷ 003A ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u0031\u003A",
                      new String[] { "\u0031\u0027\u0308\u0031" });
 
-    // ÷ 0031 ? 0027 ? 0031 ÷ 0027 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] APOSTROPHE (MidNumLet) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0031 ? 0027 ? 0031 ÷ 0027 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] APOSTROPHE (MidNumLet) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0031\u0027",
                      new String[] { "\u0031\u0027\u0031" });
 
-    // ÷ 0031 ? 0027 ? 0308 ? 0031 ÷ 0027 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0031 ? 0027 ? 0308 ? 0031 ÷ 0027 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u0031\u0027",
                      new String[] { "\u0031\u0027\u0308\u0031" });
 
-    // ÷ 0031 ? 0027 ? 0031 ÷ 002C ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] APOSTROPHE (MidNumLet) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0031 ? 0027 ? 0031 ÷ 002C ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] APOSTROPHE (MidNumLet) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0031\u002C",
                      new String[] { "\u0031\u0027\u0031" });
 
-    // ÷ 0031 ? 0027 ? 0308 ? 0031 ÷ 002C ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0031 ? 0027 ? 0308 ? 0031 ÷ 002C ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u0031\u002C",
                      new String[] { "\u0031\u0027\u0308\u0031" });
 
-    // ÷ 0031 ? 0027 ? 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] APOSTROPHE (MidNumLet) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0031 ? 0027 ? 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] APOSTROPHE (MidNumLet) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0031\u002E\u2060",
                      new String[] { "\u0031\u0027\u0031" });
 
-    // ÷ 0031 ? 0027 ? 0308 ? 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0031 ? 0027 ? 0308 ? 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] APOSTROPHE (MidNumLet) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u0031\u002E\u2060",
                      new String[] { "\u0031\u0027\u0308\u0031" });
 
-    // ÷ 0031 ÷ 002C ÷ 0001 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ÷ 0001 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0001",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002C ? 0308 ÷ 0001 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ? 0308 ÷ 0001 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u0001",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002C ÷ 000D ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ÷ 000D ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\r",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002C ? 0308 ÷ 000D ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ? 0308 ÷ 000D ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\r",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002C ÷ 000A ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ÷ 000A ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\n",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002C ? 0308 ÷ 000A ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ? 0308 ÷ 000A ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\n",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002C ÷ 000B ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ÷ 000B ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u000B",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002C ? 0308 ÷ 000B ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ? 0308 ÷ 000B ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u000B",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002C ÷ 3031 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ÷ 3031 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u3031",
                      new String[] { "\u0031", "\u3031" });
 
-    // ÷ 0031 ÷ 002C ? 0308 ÷ 3031 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ? 0308 ÷ 3031 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u3031",
                      new String[] { "\u0031", "\u3031" });
 
-    // ÷ 0031 ÷ 002C ÷ 0041 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ÷ 0041 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0041",
                      new String[] { "\u0031", "\u0041" });
 
-    // ÷ 0031 ÷ 002C ? 0308 ÷ 0041 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ? 0308 ÷ 0041 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u0041",
                      new String[] { "\u0031", "\u0041" });
 
-    // ÷ 0031 ÷ 002C ÷ 003A ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ÷ 003A ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u003A",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002C ? 0308 ÷ 003A ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ? 0308 ÷ 003A ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u003A",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002C ÷ 002C ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ÷ 002C ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u002C",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002C ? 0308 ÷ 002C ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ? 0308 ÷ 002C ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u002C",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002C ÷ 0027 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ÷ 0027 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0027",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002C ? 0308 ÷ 0027 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ? 0308 ÷ 0027 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u0027",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ? 002C ? 0030 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] COMMA (MidNum) ? [11.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 0031 ? 002C ? 0030 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] COMMA (MidNum) ? [11.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0030",
                      new String[] { "\u0031\u002C\u0030" });
 
-    // ÷ 0031 ? 002C ? 0308 ? 0030 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [11.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 0031 ? 002C ? 0308 ? 0030 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [11.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u0030",
                      new String[] { "\u0031\u002C\u0308\u0030" });
 
-    // ÷ 0031 ÷ 002C ÷ 005F ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ÷ 005F ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u005F",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002C ? 0308 ÷ 005F ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ? 0308 ÷ 005F ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u005F",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002C ? 00AD ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ? 00AD ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u00AD",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002C ? 0308 ? 00AD ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ? 0308 ? 00AD ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u00AD",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002C ? 0300 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ? 0300 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0300",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002C ? 0308 ? 0300 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ? 0308 ? 0300 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u0300",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002C ÷ 0061 ? 2060 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ÷ 0061 ? 2060 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0061\u2060",
                      new String[] { "\u0031", "\u0061\u2060" });
 
-    // ÷ 0031 ÷ 002C ? 0308 ÷ 0061 ? 2060 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ? 0308 ÷ 0061 ? 2060 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u0061\u2060",
                      new String[] { "\u0031", "\u0061\u2060" });
 
-    // ÷ 0031 ÷ 002C ÷ 0061 ÷ 003A ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ÷ 0061 ÷ 003A ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0061\u003A",
                      new String[] { "\u0031", "\u0061" });
 
-    // ÷ 0031 ÷ 002C ? 0308 ÷ 0061 ÷ 003A ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ? 0308 ÷ 0061 ÷ 003A ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u0061\u003A",
                      new String[] { "\u0031", "\u0061" });
 
-    // ÷ 0031 ÷ 002C ÷ 0061 ÷ 0027 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ÷ 0061 ÷ 0027 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0061\u0027",
                      new String[] { "\u0031", "\u0061" });
 
-    // ÷ 0031 ÷ 002C ? 0308 ÷ 0061 ÷ 0027 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ? 0308 ÷ 0061 ÷ 0027 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u0061\u0027",
                      new String[] { "\u0031", "\u0061" });
 
-    // ÷ 0031 ÷ 002C ÷ 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ÷ 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0061\u0027\u2060",
                      new String[] { "\u0031", "\u0061" });
 
-    // ÷ 0031 ÷ 002C ? 0308 ÷ 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ? 0308 ÷ 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u0061\u0027\u2060",
                      new String[] { "\u0031", "\u0061" });
 
-    // ÷ 0031 ÷ 002C ÷ 0061 ÷ 002C ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ÷ 0061 ÷ 002C ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0061\u002C",
                      new String[] { "\u0031", "\u0061" });
 
-    // ÷ 0031 ÷ 002C ? 0308 ÷ 0061 ÷ 002C ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0031 ÷ 002C ? 0308 ÷ 0061 ÷ 002C ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u0061\u002C",
                      new String[] { "\u0031", "\u0061" });
 
-    // ÷ 0031 ? 002C ? 0031 ÷ 003A ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] COMMA (MidNum) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0031 ? 002C ? 0031 ÷ 003A ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] COMMA (MidNum) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0031\u003A",
                      new String[] { "\u0031\u002C\u0031" });
 
-    // ÷ 0031 ? 002C ? 0308 ? 0031 ÷ 003A ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0031 ? 002C ? 0308 ? 0031 ÷ 003A ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u0031\u003A",
                      new String[] { "\u0031\u002C\u0308\u0031" });
 
-    // ÷ 0031 ? 002C ? 0031 ÷ 0027 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] COMMA (MidNum) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0031 ? 002C ? 0031 ÷ 0027 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] COMMA (MidNum) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0031\u0027",
                      new String[] { "\u0031\u002C\u0031" });
 
-    // ÷ 0031 ? 002C ? 0308 ? 0031 ÷ 0027 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0031 ? 002C ? 0308 ? 0031 ÷ 0027 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u0031\u0027",
                      new String[] { "\u0031\u002C\u0308\u0031" });
 
-    // ÷ 0031 ? 002C ? 0031 ÷ 002C ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] COMMA (MidNum) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0031 ? 002C ? 0031 ÷ 002C ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] COMMA (MidNum) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0031\u002C",
                      new String[] { "\u0031\u002C\u0031" });
 
-    // ÷ 0031 ? 002C ? 0308 ? 0031 ÷ 002C ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0031 ? 002C ? 0308 ? 0031 ÷ 002C ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u0031\u002C",
                      new String[] { "\u0031\u002C\u0308\u0031" });
 
-    // ÷ 0031 ? 002C ? 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] COMMA (MidNum) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0031 ? 002C ? 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] COMMA (MidNum) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0031\u002E\u2060",
                      new String[] { "\u0031\u002C\u0031" });
 
-    // ÷ 0031 ? 002C ? 0308 ? 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0031 ? 002C ? 0308 ? 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] COMMA (MidNum) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u0031\u002E\u2060",
                      new String[] { "\u0031\u002C\u0308\u0031" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ÷ 0001 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ÷ 0001 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0001",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ÷ 0001 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ÷ 0001 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] <START OF HEADING> (Other) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u0001",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ÷ 000D ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ÷ 000D ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\r",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ÷ 000D ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ÷ 000D ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <CARRIAGE RETURN (CR)> (CR) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\r",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ÷ 000A ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ÷ 000A ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\n",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ÷ 000A ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ÷ 000A ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE FEED (LF)> (LF) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\n",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ÷ 000B ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ÷ 000B ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u000B",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ÷ 000B ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ÷ 000B ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [3.2] <LINE TABULATION> (Newline) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u000B",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ÷ 3031 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ÷ 3031 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u3031",
                      new String[] { "\u0031", "\u3031" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ÷ 3031 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ÷ 3031 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] VERTICAL KANA REPEAT MARK (Katakana) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u3031",
                      new String[] { "\u0031", "\u3031" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ÷ 0041 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ÷ 0041 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0041",
                      new String[] { "\u0031", "\u0041" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ÷ 0041 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ÷ 0041 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN CAPITAL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u0041",
                      new String[] { "\u0031", "\u0041" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ÷ 003A ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ÷ 003A ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u003A",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ÷ 003A ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ÷ 003A ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u003A",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ÷ 002C ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ÷ 002C ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u002C",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ÷ 002C ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ÷ 002C ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u002C",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ÷ 0027 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ÷ 0027 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0027",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ÷ 0027 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ÷ 0027 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u0027",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ? 002E ? 2060 ? 0030 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [11.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 0031 ? 002E ? 2060 ? 0030 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [11.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0030",
                      new String[] { "\u0031\u002E\u2060\u0030" });
 
-    // ÷ 0031 ? 002E ? 2060 ? 0308 ? 0030 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [11.0] DIGIT ZERO (Numeric) ÷ [0.3]
+    // ÷ 0031 ? 002E ? 2060 ? 0308 ? 0030 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [11.0] DIGIT ZERO (Numeric) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u0030",
                      new String[] { "\u0031\u002E\u2060\u0308\u0030" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ÷ 005F ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ÷ 005F ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u005F",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ÷ 005F ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ÷ 005F ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LOW LINE (ExtendNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u005F",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ? 00AD ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ? 00AD ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u00AD",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ? 00AD ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ? 00AD ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] SOFT HYPHEN (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u00AD",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ? 0300 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ? 0300 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0300",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ? 0300 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ? 0300 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [4.0] COMBINING GRAVE ACCENT (Extend_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u0300",
                      new String[] { "\u0031" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ÷ 0061 ? 2060 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ÷ 0061 ? 2060 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0061\u2060",
                      new String[] { "\u0031", "\u0061\u2060" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ÷ 0061 ? 2060 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ÷ 0061 ? 2060 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u0061\u2060",
                      new String[] { "\u0031", "\u0061\u2060" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ÷ 0061 ÷ 003A ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ÷ 0061 ÷ 003A ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0061\u003A",
                      new String[] { "\u0031", "\u0061" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ÷ 0061 ÷ 003A ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ÷ 0061 ÷ 003A ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u0061\u003A",
                      new String[] { "\u0031", "\u0061" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ÷ 0061 ÷ 0027 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ÷ 0061 ÷ 0027 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0061\u0027",
                      new String[] { "\u0031", "\u0061" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ÷ 0061 ÷ 0027 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ÷ 0061 ÷ 0027 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u0061\u0027",
                      new String[] { "\u0031", "\u0061" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ÷ 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ÷ 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0061\u0027\u2060",
                      new String[] { "\u0031", "\u0061" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ÷ 0061 ÷ 0027 ? 2060 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ÷ 0061 ÷ 0027 ? 2060 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u0061\u0027\u2060",
                      new String[] { "\u0031", "\u0061" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ÷ 0061 ÷ 002C ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ÷ 0061 ÷ 002C ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0061\u002C",
                      new String[] { "\u0031", "\u0061" });
 
-    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ÷ 0061 ÷ 002C ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0031 ÷ 002E ? 2060 ? 0308 ÷ 0061 ÷ 002C ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u0061\u002C",
                      new String[] { "\u0031", "\u0061" });
 
-    // ÷ 0031 ? 002E ? 2060 ? 0031 ÷ 003A ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0031 ? 002E ? 2060 ? 0031 ÷ 003A ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0031\u003A",
                      new String[] { "\u0031\u002E\u2060\u0031" });
 
-    // ÷ 0031 ? 002E ? 2060 ? 0308 ? 0031 ÷ 003A ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
+    // ÷ 0031 ? 002E ? 2060 ? 0308 ? 0031 ÷ 003A ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] COLON (MidLetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u0031\u003A",
                      new String[] { "\u0031\u002E\u2060\u0308\u0031" });
 
-    // ÷ 0031 ? 002E ? 2060 ? 0031 ÷ 0027 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0031 ? 002E ? 2060 ? 0031 ÷ 0027 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0031\u0027",
                      new String[] { "\u0031\u002E\u2060\u0031" });
 
-    // ÷ 0031 ? 002E ? 2060 ? 0308 ? 0031 ÷ 0027 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
+    // ÷ 0031 ? 002E ? 2060 ? 0308 ? 0031 ÷ 0027 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] APOSTROPHE (MidNumLet) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u0031\u0027",
                      new String[] { "\u0031\u002E\u2060\u0308\u0031" });
 
-    // ÷ 0031 ? 002E ? 2060 ? 0031 ÷ 002C ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0031 ? 002E ? 2060 ? 0031 ÷ 002C ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0031\u002C",
                      new String[] { "\u0031\u002E\u2060\u0031" });
 
-    // ÷ 0031 ? 002E ? 2060 ? 0308 ? 0031 ÷ 002C ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
+    // ÷ 0031 ? 002E ? 2060 ? 0308 ? 0031 ÷ 002C ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] COMMA (MidNum) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u0031\u002C",
                      new String[] { "\u0031\u002E\u2060\u0308\u0031" });
 
-    // ÷ 0031 ? 002E ? 2060 ? 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0031 ? 002E ? 2060 ? 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0031\u002E\u2060",
                      new String[] { "\u0031\u002E\u2060\u0031" });
 
-    // ÷ 0031 ? 002E ? 2060 ? 0308 ? 0031 ÷ 002E ? 2060 ÷	#  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 0031 ? 002E ? 2060 ? 0308 ? 0031 ÷ 002E ? 2060 ÷  #  ÷ [0.2] DIGIT ONE (Numeric) ? [12.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [4.0] COMBINING DIAERESIS (Extend_FE) ? [11.0] DIGIT ONE (Numeric) ÷ [999.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u0031\u002E\u2060",
                      new String[] { "\u0031\u002E\u2060\u0308\u0031" });
 
-    // ÷ 0063 ? 0061 ? 006E ? 0027 ? 0074 ÷	#  ÷ [0.2] LATIN SMALL LETTER C (ALetter) ? [5.0] LATIN SMALL LETTER A (ALetter) ? [5.0] LATIN SMALL LETTER N (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [7.0] LATIN SMALL LETTER T (ALetter) ÷ [0.3]
+    // ÷ 0063 ? 0061 ? 006E ? 0027 ? 0074 ÷  #  ÷ [0.2] LATIN SMALL LETTER C (ALetter) ? [5.0] LATIN SMALL LETTER A (ALetter) ? [5.0] LATIN SMALL LETTER N (ALetter) ? [6.0] APOSTROPHE (MidNumLet) ? [7.0] LATIN SMALL LETTER T (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0063\u0061\u006E\u0027\u0074",
                      new String[] { "\u0063\u0061\u006E\u0027\u0074" });
 
-    // ÷ 0063 ? 0061 ? 006E ? 2019 ? 0074 ÷	#  ÷ [0.2] LATIN SMALL LETTER C (ALetter) ? [5.0] LATIN SMALL LETTER A (ALetter) ? [5.0] LATIN SMALL LETTER N (ALetter) ? [6.0] RIGHT SINGLE QUOTATION MARK (MidNumLet) ? [7.0] LATIN SMALL LETTER T (ALetter) ÷ [0.3]
+    // ÷ 0063 ? 0061 ? 006E ? 2019 ? 0074 ÷  #  ÷ [0.2] LATIN SMALL LETTER C (ALetter) ? [5.0] LATIN SMALL LETTER A (ALetter) ? [5.0] LATIN SMALL LETTER N (ALetter) ? [6.0] RIGHT SINGLE QUOTATION MARK (MidNumLet) ? [7.0] LATIN SMALL LETTER T (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0063\u0061\u006E\u2019\u0074",
                      new String[] { "\u0063\u0061\u006E\u2019\u0074" });
 
-    // ÷ 0061 ? 0062 ? 00AD ? 0062 ? 0079 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [5.0] LATIN SMALL LETTER B (ALetter) ? [4.0] SOFT HYPHEN (Format_FE) ? [5.0] LATIN SMALL LETTER B (ALetter) ? [5.0] LATIN SMALL LETTER Y (ALetter) ÷ [0.3]
+    // ÷ 0061 ? 0062 ? 00AD ? 0062 ? 0079 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ? [5.0] LATIN SMALL LETTER B (ALetter) ? [4.0] SOFT HYPHEN (Format_FE) ? [5.0] LATIN SMALL LETTER B (ALetter) ? [5.0] LATIN SMALL LETTER Y (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0062\u00AD\u0062\u0079",
                      new String[] { "\u0061\u0062\u00AD\u0062\u0079" });
 
-    // ÷ 0061 ÷ 0024 ÷ 002D ÷ 0033 ? 0034 ? 002C ? 0035 ? 0036 ? 0037 ? 002E ? 0031 ? 0034 ÷ 0025 ÷ 0062 ÷	#  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] DOLLAR SIGN (Other) ÷ [999.0] HYPHEN-MINUS (Other) ÷ [999.0] DIGIT THREE (Numeric) ? [8.0] DIGIT FOUR (Numeric) ? [12.0] COMMA (MidNum) ? [11.0] DIGIT FIVE (Numeric) ? [8.0] DIGIT SIX (Numeric) ? [8.0] DIGIT SEVEN (Numeric) ? [12.0] FULL STOP (MidNumLet) ? [11.0] DIGIT ONE (Numeric) ? [8.0] DIGIT FOUR (Numeric) ÷ [999.0] PERCENT SIGN (Other) ÷ [999.0] LATIN SMALL LETTER B (ALetter) ÷ [0.3]
+    // ÷ 0061 ÷ 0024 ÷ 002D ÷ 0033 ? 0034 ? 002C ? 0035 ? 0036 ? 0037 ? 002E ? 0031 ? 0034 ÷ 0025 ÷ 0062 ÷  #  ÷ [0.2] LATIN SMALL LETTER A (ALetter) ÷ [999.0] DOLLAR SIGN (Other) ÷ [999.0] HYPHEN-MINUS (Other) ÷ [999.0] DIGIT THREE (Numeric) ? [8.0] DIGIT FOUR (Numeric) ? [12.0] COMMA (MidNum) ? [11.0] DIGIT FIVE (Numeric) ? [8.0] DIGIT SIX (Numeric) ? [8.0] DIGIT SEVEN (Numeric) ? [12.0] FULL STOP (MidNumLet) ? [11.0] DIGIT ONE (Numeric) ? [8.0] DIGIT FOUR (Numeric) ÷ [999.0] PERCENT SIGN (Other) ÷ [999.0] LATIN SMALL LETTER B (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0024\u002D\u0033\u0034\u002C\u0035\u0036\u0037\u002E\u0031\u0034\u0025\u0062",
                      new String[] { "\u0061", "\u0033\u0034\u002C\u0035\u0036\u0037\u002E\u0031\u0034", "\u0062" });
 
-    // ÷ 0033 ? 0061 ÷	#  ÷ [0.2] DIGIT THREE (Numeric) ? [10.0] LATIN SMALL LETTER A (ALetter) ÷ [0.3]
+    // ÷ 0033 ? 0061 ÷  #  ÷ [0.2] DIGIT THREE (Numeric) ? [10.0] LATIN SMALL LETTER A (ALetter) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u0033\u0061",
                      new String[] { "\u0033\u0061" });
 
-    // ÷ 2060 ÷ 0063 ? 2060 ? 0061 ? 2060 ? 006E ? 2060 ? 0027 ? 2060 ? 0074 ? 2060 ? 2060 ÷	#  ÷ [0.2] WORD JOINER (Format_FE) ÷ [999.0] LATIN SMALL LETTER C (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [5.0] LATIN SMALL LETTER N (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [7.0] LATIN SMALL LETTER T (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 2060 ÷ 0063 ? 2060 ? 0061 ? 2060 ? 006E ? 2060 ? 0027 ? 2060 ? 0074 ? 2060 ? 2060 ÷  #  ÷ [0.2] WORD JOINER (Format_FE) ÷ [999.0] LATIN SMALL LETTER C (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [5.0] LATIN SMALL LETTER N (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [6.0] APOSTROPHE (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [7.0] LATIN SMALL LETTER T (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u2060\u0063\u2060\u0061\u2060\u006E\u2060\u0027\u2060\u0074\u2060\u2060",
                      new String[] { "\u0063\u2060\u0061\u2060\u006E\u2060\u0027\u2060\u0074\u2060\u2060" });
 
-    // ÷ 2060 ÷ 0063 ? 2060 ? 0061 ? 2060 ? 006E ? 2060 ? 2019 ? 2060 ? 0074 ? 2060 ? 2060 ÷	#  ÷ [0.2] WORD JOINER (Format_FE) ÷ [999.0] LATIN SMALL LETTER C (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [5.0] LATIN SMALL LETTER N (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [6.0] RIGHT SINGLE QUOTATION MARK (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [7.0] LATIN SMALL LETTER T (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 2060 ÷ 0063 ? 2060 ? 0061 ? 2060 ? 006E ? 2060 ? 2019 ? 2060 ? 0074 ? 2060 ? 2060 ÷  #  ÷ [0.2] WORD JOINER (Format_FE) ÷ [999.0] LATIN SMALL LETTER C (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [5.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [5.0] LATIN SMALL LETTER N (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [6.0] RIGHT SINGLE QUOTATION MARK (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [7.0] LATIN SMALL LETTER T (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u2060\u0063\u2060\u0061\u2060\u006E\u2060\u2019\u2060\u0074\u2060\u2060",
                      new String[] { "\u0063\u2060\u0061\u2060\u006E\u2060\u2019\u2060\u0074\u2060\u2060" });
 
-    // ÷ 2060 ÷ 0061 ? 2060 ? 0062 ? 2060 ? 00AD ? 2060 ? 0062 ? 2060 ? 0079 ? 2060 ? 2060 ÷	#  ÷ [0.2] WORD JOINER (Format_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [5.0] LATIN SMALL LETTER B (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] SOFT HYPHEN (Format_FE) ? [4.0] WORD JOINER (Format_FE) ? [5.0] LATIN SMALL LETTER B (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [5.0] LATIN SMALL LETTER Y (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 2060 ÷ 0061 ? 2060 ? 0062 ? 2060 ? 00AD ? 2060 ? 0062 ? 2060 ? 0079 ? 2060 ? 2060 ÷  #  ÷ [0.2] WORD JOINER (Format_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [5.0] LATIN SMALL LETTER B (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] SOFT HYPHEN (Format_FE) ? [4.0] WORD JOINER (Format_FE) ? [5.0] LATIN SMALL LETTER B (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [5.0] LATIN SMALL LETTER Y (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u2060\u0061\u2060\u0062\u2060\u00AD\u2060\u0062\u2060\u0079\u2060\u2060",
                      new String[] { "\u0061\u2060\u0062\u2060\u00AD\u2060\u0062\u2060\u0079\u2060\u2060" });
 
-    // ÷ 2060 ÷ 0061 ? 2060 ÷ 0024 ? 2060 ÷ 002D ? 2060 ÷ 0033 ? 2060 ? 0034 ? 2060 ? 002C ? 2060 ? 0035 ? 2060 ? 0036 ? 2060 ? 0037 ? 2060 ? 002E ? 2060 ? 0031 ? 2060 ? 0034 ? 2060 ÷ 0025 ? 2060 ÷ 0062 ? 2060 ? 2060 ÷	#  ÷ [0.2] WORD JOINER (Format_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] DOLLAR SIGN (Other) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] HYPHEN-MINUS (Other) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] DIGIT THREE (Numeric) ? [4.0] WORD JOINER (Format_FE) ? [8.0] DIGIT FOUR (Numeric) ? [4.0] WORD JOINER (Format_FE) ? [12.0] COMMA (MidNum) ? [4.0] WORD JOINER (Format_FE) ? [11.0] DIGIT FIVE (Numeric) ? [4.0] WORD JOINER (Format_FE) ? [8.0] DIGIT SIX (Numeric) ? [4.0] WORD JOINER (Format_FE) ? [8.0] DIGIT SEVEN (Numeric) ? [4.0] WORD JOINER (Format_FE) ? [12.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [11.0] DIGIT ONE (Numeric) ? [4.0] WORD JOINER (Format_FE) ? [8.0] DIGIT FOUR (Numeric) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] PERCENT SIGN (Other) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] LATIN SMALL LETTER B (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 2060 ÷ 0061 ? 2060 ÷ 0024 ? 2060 ÷ 002D ? 2060 ÷ 0033 ? 2060 ? 0034 ? 2060 ? 002C ? 2060 ? 0035 ? 2060 ? 0036 ? 2060 ? 0037 ? 2060 ? 002E ? 2060 ? 0031 ? 2060 ? 0034 ? 2060 ÷ 0025 ? 2060 ÷ 0062 ? 2060 ? 2060 ÷  #  ÷ [0.2] WORD JOINER (Format_FE) ÷ [999.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] DOLLAR SIGN (Other) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] HYPHEN-MINUS (Other) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] DIGIT THREE (Numeric) ? [4.0] WORD JOINER (Format_FE) ? [8.0] DIGIT FOUR (Numeric) ? [4.0] WORD JOINER (Format_FE) ? [12.0] COMMA (MidNum) ? [4.0] WORD JOINER (Format_FE) ? [11.0] DIGIT FIVE (Numeric) ? [4.0] WORD JOINER (Format_FE) ? [8.0] DIGIT SIX (Numeric) ? [4.0] WORD JOINER (Format_FE) ? [8.0] DIGIT SEVEN (Numeric) ? [4.0] WORD JOINER (Format_FE) ? [12.0] FULL STOP (MidNumLet) ? [4.0] WORD JOINER (Format_FE) ? [11.0] DIGIT ONE (Numeric) ? [4.0] WORD JOINER (Format_FE) ? [8.0] DIGIT FOUR (Numeric) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] PERCENT SIGN (Other) ? [4.0] WORD JOINER (Format_FE) ÷ [999.0] LATIN SMALL LETTER B (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u2060\u0061\u2060\u0024\u2060\u002D\u2060\u0033\u2060\u0034\u2060\u002C\u2060\u0035\u2060\u0036\u2060\u0037\u2060\u002E\u2060\u0031\u2060\u0034\u2060\u0025\u2060\u0062\u2060\u2060",
                      new String[] { "\u0061\u2060", "\u0033\u2060\u0034\u2060\u002C\u2060\u0035\u2060\u0036\u2060\u0037\u2060\u002E\u2060\u0031\u2060\u0034\u2060", "\u0062\u2060\u2060" });
 
-    // ÷ 2060 ÷ 0033 ? 2060 ? 0061 ? 2060 ? 2060 ÷	#  ÷ [0.2] WORD JOINER (Format_FE) ÷ [999.0] DIGIT THREE (Numeric) ? [4.0] WORD JOINER (Format_FE) ? [10.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
+    // ÷ 2060 ÷ 0033 ? 2060 ? 0061 ? 2060 ? 2060 ÷  #  ÷ [0.2] WORD JOINER (Format_FE) ÷ [999.0] DIGIT THREE (Numeric) ? [4.0] WORD JOINER (Format_FE) ? [10.0] LATIN SMALL LETTER A (ALetter) ? [4.0] WORD JOINER (Format_FE) ? [4.0] WORD JOINER (Format_FE) ÷ [0.3]
     assertAnalyzesTo(analyzer, "\u2060\u0033\u2060\u0061\u2060\u2060",
                      new String[] { "\u0033\u2060\u0061\u2060\u2060" });
 
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java
index 27fccac..e64e4a0 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java
@@ -46,7 +46,7 @@ public class GreekAnalyzerTest extends BaseTokenStreamTestCase {
     assertAnalyzesTo(a, "?Ρ?Ϋ????Σ??Σ  ??ογο?, ο με???? και οι άλλοι",
         new String[] { "??ο??οθε?", "α?ογ", "με??", "αλλ" });
   }
-	
+
   public void testReusableTokenStream() throws Exception {
     Analyzer a = new GreekAnalyzer(TEST_VERSION_CURRENT);
     // Verify the correct analysis of capitals and small accented letters, and
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestFrenchAnalyzer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestFrenchAnalyzer.java
index 7353c4d..e1953bf 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestFrenchAnalyzer.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestFrenchAnalyzer.java
@@ -31,93 +31,92 @@ import org.apache.lucene.util.Version;
 
 public class TestFrenchAnalyzer extends BaseTokenStreamTestCase {
 
-	public void testAnalyzer() throws Exception {
-		FrenchAnalyzer fa = new FrenchAnalyzer(TEST_VERSION_CURRENT);
-	
-		assertAnalyzesTo(fa, "", new String[] {
-		});
-
-		assertAnalyzesTo(
-			fa,
-			"chien chat cheval",
-			new String[] { "chien", "chat", "cheval" });
-
-		assertAnalyzesTo(
-			fa,
-			"chien CHAT CHEVAL",
-			new String[] { "chien", "chat", "cheval" });
-
-		assertAnalyzesTo(
-			fa,
-			"  chien  ,? + = -  CHAT /: > CHEVAL",
-			new String[] { "chien", "chat", "cheval" });
-
-		assertAnalyzesTo(fa, "chien++", new String[] { "chien" });
-
-		assertAnalyzesTo(
-			fa,
-			"mot \"entreguillemet\"",
-			new String[] { "mot", "entreguilemet" });
-
-		// let's do some french specific tests now	
-
-		/* 1. couldn't resist
-		 I would expect this to stay one term as in French the minus 
-		sign is often used for composing words */
-		assertAnalyzesTo(
-			fa,
-			"Jean-François",
-			new String[] { "jean", "francoi" });
-
-		// 2. stopwords
-		assertAnalyzesTo(
-			fa,
-			"le la chien les aux chat du des ? cheval",
-			new String[] { "chien", "chat", "cheval" });
-
-		// some nouns and adjectives
-		assertAnalyzesTo(
-			fa,
-			"lances chismes habitable chiste éléments captifs",
-			new String[] {
-				"lanc",
-				"chism",
-				"habitabl",
-				"chist",
-				"element",
-				"captif" });
-
-		// some verbs
-		assertAnalyzesTo(
-			fa,
-			"finissions souffrirent rugissante",
-			new String[] { "finision", "soufrirent", "rugisant" });
-
-		// some everything else
-		// aujourd'hui stays one term which is OK
-		assertAnalyzesTo(
-			fa,
-			"C3PO aujourd'hui oeuf ïâöû?ä anticonstitutionnellement Java++ ",
-			new String[] {
-				"c3po",
-				"aujourd'hui",
-				"oeuf",
-				"ïaöuaä",
-				"anticonstitutionel",
-				"java" });
-
-		// some more everything else
-		// here 1940-1945 stays as one term, 1940:1945 not ?
-		assertAnalyzesTo(
-			fa,
-			"33Bis 1940-1945 1940:1945 (---i+++)*",
-			new String[] { "33bi", "1940", "1945", "1940", "1945", "i" });
-
-	}
-	
-	public void testReusableTokenStream() throws Exception {
-	  FrenchAnalyzer fa = new FrenchAnalyzer(TEST_VERSION_CURRENT);
-	  // stopwords
+  public void testAnalyzer() throws Exception {
+    FrenchAnalyzer fa = new FrenchAnalyzer(TEST_VERSION_CURRENT);
+  
+    assertAnalyzesTo(fa, "", new String[] {
+    });
+
+    assertAnalyzesTo(
+      fa,
+      "chien chat cheval",
+      new String[] { "chien", "chat", "cheval" });
+
+    assertAnalyzesTo(
+      fa,
+      "chien CHAT CHEVAL",
+      new String[] { "chien", "chat", "cheval" });
+
+    assertAnalyzesTo(
+      fa,
+      "  chien  ,? + = -  CHAT /: > CHEVAL",
+      new String[] { "chien", "chat", "cheval" });
+
+    assertAnalyzesTo(fa, "chien++", new String[] { "chien" });
+
+    assertAnalyzesTo(
+      fa,
+      "mot \"entreguillemet\"",
+      new String[] { "mot", "entreguilemet" });
+
+     // let's do some french specific tests now   
+          /* 1. couldn't resist
+      I would expect this to stay one term as in French the minus 
+    sign is often used for composing words */
+    assertAnalyzesTo(
+      fa,
+      "Jean-François",
+      new String[] { "jean", "francoi" });
+
+    // 2. stopwords
+    assertAnalyzesTo(
+      fa,
+      "le la chien les aux chat du des ? cheval",
+      new String[] { "chien", "chat", "cheval" });
+
+    // some nouns and adjectives
+    assertAnalyzesTo(
+      fa,
+      "lances chismes habitable chiste éléments captifs",
+      new String[] {
+        "lanc",
+        "chism",
+        "habitabl",
+        "chist",
+        "element",
+        "captif" });
+
+    // some verbs
+    assertAnalyzesTo(
+      fa,
+      "finissions souffrirent rugissante",
+      new String[] { "finision", "soufrirent", "rugisant" });
+
+    // some everything else
+    // aujourd'hui stays one term which is OK
+    assertAnalyzesTo(
+      fa,
+      "C3PO aujourd'hui oeuf ïâöû?ä anticonstitutionnellement Java++ ",
+      new String[] {
+        "c3po",
+        "aujourd'hui",
+        "oeuf",
+        "ïaöuaä",
+        "anticonstitutionel",
+        "java" });
+
+    // some more everything else
+    // here 1940-1945 stays as one term, 1940:1945 not ?
+    assertAnalyzesTo(
+      fa,
+      "33Bis 1940-1945 1940:1945 (---i+++)*",
+      new String[] { "33bi", "1940", "1945", "1940", "1945", "i" });
+
+  }
+  
+  public void testReusableTokenStream() throws Exception {
+    FrenchAnalyzer fa = new FrenchAnalyzer(TEST_VERSION_CURRENT);
+    // stopwords
       assertAnalyzesToReuse(
           fa,
           "le la chien les aux chat du des ? cheval",
@@ -134,7 +133,7 @@ public class TestFrenchAnalyzer extends BaseTokenStreamTestCase {
               "chist",
               "element",
               "captif" });
-	}
+  }
 
   public void testExclusionTableViaCtor() throws Exception {
     CharArraySet set = new CharArraySet(TEST_VERSION_CURRENT, 1, true);
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestHyphenatedWordsFilter.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestHyphenatedWordsFilter.java
index e2f6c97..1a8bfc7 100755
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestHyphenatedWordsFilter.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestHyphenatedWordsFilter.java
@@ -32,37 +32,37 @@ import org.apache.lucene.analysis.core.KeywordTokenizer;
  * HyphenatedWordsFilter test
  */
 public class TestHyphenatedWordsFilter extends BaseTokenStreamTestCase {
-	public void testHyphenatedWords() throws Exception {
-		String input = "ecologi-\r\ncal devel-\r\n\r\nop compre-\u0009hensive-hands-on and ecologi-\ncal";
-		// first test
-		TokenStream ts = new MockTokenizer(new StringReader(input), MockTokenizer.WHITESPACE, false);
-		ts = new HyphenatedWordsFilter(ts);
-		assertTokenStreamContents(ts, 
-		    new String[] { "ecological", "develop", "comprehensive-hands-on", "and", "ecological" });
-	}
-	
-	/**
-	 * Test that HyphenatedWordsFilter behaves correctly with a final hyphen
-	 */
-	public void testHyphenAtEnd() throws Exception {
-	    String input = "ecologi-\r\ncal devel-\r\n\r\nop compre-\u0009hensive-hands-on and ecology-";
-	    // first test
-	    TokenStream ts = new MockTokenizer(new StringReader(input), MockTokenizer.WHITESPACE, false);
-	    ts = new HyphenatedWordsFilter(ts);
-	    assertTokenStreamContents(ts, 
-	        new String[] { "ecological", "develop", "comprehensive-hands-on", "and", "ecology-" });
-	}
-	
-	public void testOffsets() throws Exception {
-	  String input = "abc- def geh 1234- 5678-";
+  public void testHyphenatedWords() throws Exception {
+    String input = "ecologi-\r\ncal devel-\r\n\r\nop compre-\u0009hensive-hands-on and ecologi-\ncal";
+    // first test
+    TokenStream ts = new MockTokenizer(new StringReader(input), MockTokenizer.WHITESPACE, false);
+    ts = new HyphenatedWordsFilter(ts);
+    assertTokenStreamContents(ts,
+        new String[] { "ecological", "develop", "comprehensive-hands-on", "and", "ecological" });
+  }
+
+  /**
+   * Test that HyphenatedWordsFilter behaves correctly with a final hyphen
+   */
+  public void testHyphenAtEnd() throws Exception {
+      String input = "ecologi-\r\ncal devel-\r\n\r\nop compre-\u0009hensive-hands-on and ecology-";
+      // first test
+      TokenStream ts = new MockTokenizer(new StringReader(input), MockTokenizer.WHITESPACE, false);
+      ts = new HyphenatedWordsFilter(ts);
+      assertTokenStreamContents(ts,
+          new String[] { "ecological", "develop", "comprehensive-hands-on", "and", "ecology-" });
+  }
+
+  public void testOffsets() throws Exception {
+    String input = "abc- def geh 1234- 5678-";
     TokenStream ts = new MockTokenizer(new StringReader(input), MockTokenizer.WHITESPACE, false);
     ts = new HyphenatedWordsFilter(ts);
     assertTokenStreamContents(ts, 
         new String[] { "abcdef", "geh", "12345678-" },
         new int[] { 0, 9, 13 },
         new int[] { 8, 12, 24 });
-	}
-	
+  }
+
   /** blast some random strings through the analyzer */
   public void testRandomString() throws Exception {
     Analyzer a = new Analyzer() {
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/nl/TestDutchStemmer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/nl/TestDutchStemmer.java
index d734850..5ce448f 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/nl/TestDutchStemmer.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/nl/TestDutchStemmer.java
@@ -34,83 +34,83 @@ import org.apache.lucene.util.Version;
 public class TestDutchStemmer extends BaseTokenStreamTestCase {
   
   public void testWithSnowballExamples() throws Exception {
-	 check("lichaamsziek", "lichaamsziek");
-	 check("lichamelijk", "licham");
-	 check("lichamelijke", "licham");
-	 check("lichamelijkheden", "licham");
-	 check("lichamen", "licham");
-	 check("lichere", "licher");
-	 check("licht", "licht");
-	 check("lichtbeeld", "lichtbeeld");
-	 check("lichtbruin", "lichtbruin");
-	 check("lichtdoorlatende", "lichtdoorlat");
-	 check("lichte", "licht");
-	 check("lichten", "licht");
-	 check("lichtende", "lichtend");
-	 check("lichtenvoorde", "lichtenvoord");
-	 check("lichter", "lichter");
-	 check("lichtere", "lichter");
-	 check("lichters", "lichter");
-	 check("lichtgevoeligheid", "lichtgevoel");
-	 check("lichtgewicht", "lichtgewicht");
-	 check("lichtgrijs", "lichtgrijs");
-	 check("lichthoeveelheid", "lichthoevel");
-	 check("lichtintensiteit", "lichtintensiteit");
-	 check("lichtje", "lichtj");
-	 check("lichtjes", "lichtjes");
-	 check("lichtkranten", "lichtkrant");
-	 check("lichtkring", "lichtkring");
-	 check("lichtkringen", "lichtkring");
-	 check("lichtregelsystemen", "lichtregelsystem");
-	 check("lichtste", "lichtst");
-	 check("lichtstromende", "lichtstrom");
-	 check("lichtte", "licht");
-	 check("lichtten", "licht");
-	 check("lichttoetreding", "lichttoetred");
-	 check("lichtverontreinigde", "lichtverontreinigd");
-	 check("lichtzinnige", "lichtzinn");
-	 check("lid", "lid");
-	 check("lidia", "lidia");
-	 check("lidmaatschap", "lidmaatschap");
-	 check("lidstaten", "lidstat");
-	 check("lidvereniging", "lidveren");
-	 check("opgingen", "opging");
-	 check("opglanzing", "opglanz");
-	 check("opglanzingen", "opglanz");
-	 check("opglimlachten", "opglimlacht");
-	 check("opglimpen", "opglimp");
-	 check("opglimpende", "opglimp");
-	 check("opglimping", "opglimp");
-	 check("opglimpingen", "opglimp");
-	 check("opgraven", "opgrav");
-	 check("opgrijnzen", "opgrijnz");
-	 check("opgrijzende", "opgrijz");
-	 check("opgroeien", "opgroei");
-	 check("opgroeiende", "opgroei");
-	 check("opgroeiplaats", "opgroeiplat");
-	 check("ophaal", "ophal");
-	 check("ophaaldienst", "ophaaldienst");
-	 check("ophaalkosten", "ophaalkost");
-	 check("ophaalsystemen", "ophaalsystem");
-	 check("ophaalt", "ophaalt");
-	 check("ophaaltruck", "ophaaltruck");
-	 check("ophalen", "ophal");
-	 check("ophalend", "ophal");
-	 check("ophalers", "ophaler");
-	 check("ophef", "ophef");
-	 check("opheldering", "ophelder");
-	 check("ophemelde", "ophemeld");
-	 check("ophemelen", "ophemel");
-	 check("opheusden", "opheusd");
-	 check("ophief", "ophief");
-	 check("ophield", "ophield");
-	 check("ophieven", "ophiev");
-	 check("ophoepelt", "ophoepelt");
-	 check("ophoog", "ophog");
-	 check("ophoogzand", "ophoogzand");
-	 check("ophopen", "ophop");
-	 check("ophoping", "ophop");
-	 check("ophouden", "ophoud");
+   check("lichaamsziek", "lichaamsziek");
+   check("lichamelijk", "licham");
+   check("lichamelijke", "licham");
+   check("lichamelijkheden", "licham");
+   check("lichamen", "licham");
+   check("lichere", "licher");
+   check("licht", "licht");
+   check("lichtbeeld", "lichtbeeld");
+   check("lichtbruin", "lichtbruin");
+   check("lichtdoorlatende", "lichtdoorlat");
+   check("lichte", "licht");
+   check("lichten", "licht");
+   check("lichtende", "lichtend");
+   check("lichtenvoorde", "lichtenvoord");
+   check("lichter", "lichter");
+   check("lichtere", "lichter");
+   check("lichters", "lichter");
+   check("lichtgevoeligheid", "lichtgevoel");
+   check("lichtgewicht", "lichtgewicht");
+   check("lichtgrijs", "lichtgrijs");
+   check("lichthoeveelheid", "lichthoevel");
+   check("lichtintensiteit", "lichtintensiteit");
+   check("lichtje", "lichtj");
+   check("lichtjes", "lichtjes");
+   check("lichtkranten", "lichtkrant");
+   check("lichtkring", "lichtkring");
+   check("lichtkringen", "lichtkring");
+   check("lichtregelsystemen", "lichtregelsystem");
+   check("lichtste", "lichtst");
+   check("lichtstromende", "lichtstrom");
+   check("lichtte", "licht");
+   check("lichtten", "licht");
+   check("lichttoetreding", "lichttoetred");
+   check("lichtverontreinigde", "lichtverontreinigd");
+   check("lichtzinnige", "lichtzinn");
+   check("lid", "lid");
+   check("lidia", "lidia");
+   check("lidmaatschap", "lidmaatschap");
+   check("lidstaten", "lidstat");
+   check("lidvereniging", "lidveren");
+   check("opgingen", "opging");
+   check("opglanzing", "opglanz");
+   check("opglanzingen", "opglanz");
+   check("opglimlachten", "opglimlacht");
+   check("opglimpen", "opglimp");
+   check("opglimpende", "opglimp");
+   check("opglimping", "opglimp");
+   check("opglimpingen", "opglimp");
+   check("opgraven", "opgrav");
+   check("opgrijnzen", "opgrijnz");
+   check("opgrijzende", "opgrijz");
+   check("opgroeien", "opgroei");
+   check("opgroeiende", "opgroei");
+   check("opgroeiplaats", "opgroeiplat");
+   check("ophaal", "ophal");
+   check("ophaaldienst", "ophaaldienst");
+   check("ophaalkosten", "ophaalkost");
+   check("ophaalsystemen", "ophaalsystem");
+   check("ophaalt", "ophaalt");
+   check("ophaaltruck", "ophaaltruck");
+   check("ophalen", "ophal");
+   check("ophalend", "ophal");
+   check("ophalers", "ophaler");
+   check("ophef", "ophef");
+   check("opheldering", "ophelder");
+   check("ophemelde", "ophemeld");
+   check("ophemelen", "ophemel");
+   check("opheusden", "opheusd");
+   check("ophief", "ophief");
+   check("ophield", "ophield");
+   check("ophieven", "ophiev");
+   check("ophoepelt", "ophoepelt");
+   check("ophoog", "ophog");
+   check("ophoogzand", "ophoogzand");
+   check("ophopen", "ophop");
+   check("ophoping", "ophop");
+   check("ophouden", "ophoud");
   }
   
   public void testSnowballCorrectness() throws Exception {
@@ -171,4 +171,4 @@ public class TestDutchStemmer extends BaseTokenStreamTestCase {
     checkRandomData(random(), new DutchAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
   
-}
\ No newline at end of file
+}
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternTokenizer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternTokenizer.java
index 5e000be..c7028e7 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternTokenizer.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternTokenizer.java
@@ -37,7 +37,7 @@ import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 
 public class TestPatternTokenizer extends BaseTokenStreamTestCase 
 {
-	public void testSplitting() throws Exception 
+  public void testSplitting() throws Exception 
   {
     String qpattern = "\\'([^\\']+)\\'"; // get stuff between "'"
     String[][] tests = {
@@ -71,8 +71,8 @@ public class TestPatternTokenizer extends BaseTokenStreamTestCase
         }
       }*/
     } 
-	}
-	
+  }
+
   public void testOffsetCorrection() throws Exception {
     final String INPUT = "G&uuml;nther G&uuml;nther is here";
 
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/th/TestThaiAnalyzer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/th/TestThaiAnalyzer.java
index 901e544..ab4a4d9 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/th/TestThaiAnalyzer.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/th/TestThaiAnalyzer.java
@@ -38,31 +38,31 @@ import org.apache.lucene.util.Version;
  */
 
 public class TestThaiAnalyzer extends BaseTokenStreamTestCase {
-	
+  
   @Override
   public void setUp() throws Exception {
     super.setUp();
     assumeTrue("JRE does not support Thai dictionary-based BreakIterator", ThaiWordFilter.DBBI_AVAILABLE);
   }
-	/* 
-	 * testcase for offsets
-	 */
-	public void testOffsets() throws Exception {
-		assertAnalyzesTo(new ThaiAnalyzer(TEST_VERSION_CURRENT, CharArraySet.EMPTY_SET), "?ร?ี????????????????", 
-		    new String[] { "??", "??", "???", "????", "???", "ว??", "??", "?" },
-				new int[] { 0, 3, 6, 9, 13, 17, 20, 23 },
-				new int[] { 3, 6, 9, 13, 17, 20, 23, 25 });
-	}
-	
-	public void testStopWords() throws Exception {
-	  assertAnalyzesTo(new ThaiAnalyzer(TEST_VERSION_CURRENT), "?ร?ี????????????????", 
-	      new String[] { "???", "??", "?" },
-	      new int[] { 13, 20, 23 },
-	      new int[] { 17, 23, 25 },
-	      new int[] { 5, 2, 1 });
-	}
-	
-	public void testTokenType() throws Exception {
+  /* 
+   * testcase for offsets
+   */
+  public void testOffsets() throws Exception {
+    assertAnalyzesTo(new ThaiAnalyzer(TEST_VERSION_CURRENT, CharArraySet.EMPTY_SET), "?ร?ี????????????????", 
+        new String[] { "??", "??", "???", "????", "???", "ว??", "??", "?" },
+        new int[] { 0, 3, 6, 9, 13, 17, 20, 23 },
+        new int[] { 3, 6, 9, 13, 17, 20, 23, 25 });
+  }
+  
+  public void testStopWords() throws Exception {
+    assertAnalyzesTo(new ThaiAnalyzer(TEST_VERSION_CURRENT), "?ร?ี????????????????", 
+        new String[] { "???", "??", "?" },
+        new int[] { 13, 20, 23 },
+        new int[] { 17, 23, 25 },
+        new int[] { 5, 2, 1 });
+  }
+  
+  public void testTokenType() throws Exception {
       assertAnalyzesTo(new ThaiAnalyzer(TEST_VERSION_CURRENT, CharArraySet.EMPTY_SET), "?ร?ี???????????????? ???", 
                        new String[] { "??", "??", "???", "????", "???", "ว??", "??", "?", "???" },
                        new String[] { "<SOUTHEAST_ASIAN>", "<SOUTHEAST_ASIAN>", 
@@ -70,31 +70,31 @@ public class TestThaiAnalyzer extends BaseTokenStreamTestCase {
                                       "<SOUTHEAST_ASIAN>", "<SOUTHEAST_ASIAN>",
                                       "<SOUTHEAST_ASIAN>", "<SOUTHEAST_ASIAN>",
                                       "<NUM>" });
-	}
+  }
 
-	/*
-	 * Test that position increments are adjusted correctly for stopwords.
-	 */
-	// note this test uses stopfilter's stopset
-	public void testPositionIncrements() throws Exception {
-	  final ThaiAnalyzer analyzer = new ThaiAnalyzer(TEST_VERSION_CURRENT, StopAnalyzer.ENGLISH_STOP_WORDS_SET);
+  /*
+   * Test that position increments are adjusted correctly for stopwords.
+   */
+  // note this test uses stopfilter's stopset
+  public void testPositionIncrements() throws Exception {
+    final ThaiAnalyzer analyzer = new ThaiAnalyzer(TEST_VERSION_CURRENT, StopAnalyzer.ENGLISH_STOP_WORDS_SET);
     assertAnalyzesTo(analyzer, "?ร?ี??????? the ???ว?า?า??", 
         new String[] { "??", "??", "???", "????", "???", "ว??", "??", "?" },
         new int[] { 0, 3, 6, 9, 18, 22, 25, 28 },
         new int[] { 3, 6, 9, 13, 22, 25, 28, 30 },
         new int[] { 1, 1, 1, 1, 2, 1, 1, 1 });
-	 
-	  // case that a stopword is adjacent to thai text, with no whitespace
+   
+    // case that a stopword is adjacent to thai text, with no whitespace
     assertAnalyzesTo(analyzer, "?ร?ี???????he ???ว?า?า??", 
         new String[] { "??", "??", "???", "????", "???", "ว??", "??", "?" },
         new int[] { 0, 3, 6, 9, 17, 21, 24, 27 },
         new int[] { 3, 6, 9, 13, 21, 24, 27, 29 },
         new int[] { 1, 1, 1, 1, 2, 1, 1, 1 });
-	}
-	
-	public void testReusableTokenStream() throws Exception {
-	  ThaiAnalyzer analyzer = new ThaiAnalyzer(TEST_VERSION_CURRENT, CharArraySet.EMPTY_SET);
-	  assertAnalyzesToReuse(analyzer, "", new String[] {});
+  }
+  
+  public void testReusableTokenStream() throws Exception {
+    ThaiAnalyzer analyzer = new ThaiAnalyzer(TEST_VERSION_CURRENT, CharArraySet.EMPTY_SET);
+    assertAnalyzesToReuse(analyzer, "", new String[] {});
 
       assertAnalyzesToReuse(
           analyzer,
@@ -105,8 +105,8 @@ public class TestThaiAnalyzer extends BaseTokenStreamTestCase {
           analyzer,
           "?ิษั??? XY&Z - ?ย?ั? xyz@demo.com",
           new String[] { "?ิษั?", "??", "xy", "z", "??", "??", "xyz", "demo.com" });
-	}
-	
+  }
+  
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
     checkRandomData(random(), new ThaiAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestWordlistLoader.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestWordlistLoader.java
index 421faa7..ac33b30 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestWordlistLoader.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestWordlistLoader.java
@@ -46,8 +46,8 @@ public class TestWordlistLoader extends LuceneTestCase {
 
   private void checkSet(CharArraySet wordset) {
     assertEquals(3, wordset.size());
-    assertTrue(wordset.contains("ONE"));		// case is not modified
-    assertTrue(wordset.contains("two"));		// surrounding whitespace is removed
+    assertTrue(wordset.contains("ONE"));  // case is not modified
+    assertTrue(wordset.contains("two"));  // surrounding whitespace is removed
     assertTrue(wordset.contains("three"));
     assertFalse(wordset.contains("four"));
   }
diff --git a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseTokenizer.java b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseTokenizer.java
index 04d5921..a3f074e 100644
--- a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseTokenizer.java
+++ b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseTokenizer.java
@@ -292,12 +292,12 @@ public final class JapaneseTokenizer extends Tokenizer {
         if (!characterDefinition.isKanji((char) buffer.get(pos2))) {
           allKanji = false;
           break;
-        }				
+        }
       }
-      if (allKanji) {	// Process only Kanji keywords
+      if (allKanji) {  // Process only Kanji keywords
         return (length - SEARCH_MODE_KANJI_LENGTH) * SEARCH_MODE_KANJI_PENALTY;
       } else if (length > SEARCH_MODE_OTHER_LENGTH) {
-        return (length - SEARCH_MODE_OTHER_LENGTH) * SEARCH_MODE_OTHER_PENALTY;								
+        return (length - SEARCH_MODE_OTHER_LENGTH) * SEARCH_MODE_OTHER_PENALTY;
       }
     }
     return 0;
@@ -807,7 +807,7 @@ public final class JapaneseTokenizer extends Tokenizer {
             }
             if (characterId == characterDefinition.getCharacterClass((char) ch) &&
                 isPunctuation((char) ch) == isPunct) {
-              unknownWordLength++;    			
+              unknownWordLength++;
             } else {
               break;
             }
diff --git a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/BinaryDictionary.java b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/BinaryDictionary.java
index edad0fa..251875c 100644
--- a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/BinaryDictionary.java
+++ b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/BinaryDictionary.java
@@ -150,7 +150,7 @@ public abstract class BinaryDictionary implements Dictionary {
     ref.length = targetMapOffsets[sourceId + 1] - ref.offset;
   }
   
-  @Override	
+  @Override
   public int getLeftId(int wordId) {
     return buffer.getShort(wordId) >>> 3;
   }
@@ -162,7 +162,7 @@ public abstract class BinaryDictionary implements Dictionary {
   
   @Override
   public int getWordCost(int wordId) {
-    return buffer.getShort(wordId + 2);	// Skip id
+    return buffer.getShort(wordId + 2);  // Skip id
   }
 
   @Override
diff --git a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/Dictionary.java b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/Dictionary.java
index 9971db2..11277a7 100644
--- a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/Dictionary.java
+++ b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/Dictionary.java
@@ -28,21 +28,21 @@ public interface Dictionary {
   /**
    * Get left id of specified word
    * @param wordId
-   * @return	left id
+   * @return left id
    */
   public int getLeftId(int wordId);
   
   /**
    * Get right id of specified word
    * @param wordId
-   * @return	left id
+   * @return left id
    */
   public int getRightId(int wordId);
   
   /**
    * Get word cost of specified word
    * @param wordId
-   * @return	left id
+   * @return left id
    */
   public int getWordCost(int wordId);
   
diff --git a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UnknownDictionary.java b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UnknownDictionary.java
index fd88529..185f0ab 100644
--- a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UnknownDictionary.java
+++ b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UnknownDictionary.java
@@ -40,7 +40,7 @@ public final class UnknownDictionary extends BinaryDictionary {
     int length = 1;
     for (int i = 1; i < len; i++) {
       if (characterIdOfFirstCharacter == characterDefinition.getCharacterClass(text[offset+i])){
-        length++;    			
+        length++;
       } else {
         break;
       }
diff --git a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary.java b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary.java
index 58568be..a415555 100644
--- a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary.java
+++ b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary.java
@@ -246,7 +246,7 @@ public final class UserDictionary implements Dictionary {
       return null;
     }
     
-    return allFeatures.split(INTERNAL_SEPARATOR);		
+    return allFeatures.split(INTERNAL_SEPARATOR);
   }
   
   
@@ -261,7 +261,7 @@ public final class UserDictionary implements Dictionary {
         sb.append(CSVUtil.quoteEscape(feature)).append(",");
       }
     } else if (fields.length == 1) { // One feature doesn't need to escape value
-      sb.append(allFeatures[fields[0]]).append(",");			
+      sb.append(allFeatures[fields[0]]).append(",");
     } else {
       for (int field : fields){
         sb.append(CSVUtil.quoteEscape(allFeatures[field])).append(",");
diff --git a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/util/CSVUtil.java b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/util/CSVUtil.java
index 2cf9061..493cbf3 100644
--- a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/util/CSVUtil.java
+++ b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/util/CSVUtil.java
@@ -42,7 +42,7 @@ public final class CSVUtil {
    */
   public static String[] parse(String line) {
     boolean insideQuote = false;
-    ArrayList<String> result = new ArrayList<String>();		
+    ArrayList<String> result = new ArrayList<String>();
     int quoteCount = 0;
     StringBuilder sb = new StringBuilder();
     for(int i = 0; i < line.length(); i++) {
diff --git a/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizer.java b/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizer.java
index 9ef38b2..5dce63e 100644
--- a/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizer.java
+++ b/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizer.java
@@ -326,12 +326,12 @@ public class TestJapaneseTokenizer extends BaseTokenStreamTestCase {
   
   public void testSegmentation() throws Exception {
     // Skip tests for Michelle Kwan -- UniDic segments Kwan as ?? ???
-    //		String input = "????????????????????????????????????????????????????";
-    //		String[] surfaceForms = {
-    //				"?????", "??", "?????", "??", "???", "??", "??", "??", "??",
-    //				"????", "??????", "??", "??", "??", "??",
-    //				"?????????", "??"
-    //		};
+    //   String input = "????????????????????????????????????????????????????";
+    //   String[] surfaceForms = {
+        //        "?????", "??", "?????", "??", "???", "??", "??", "??", "??",
+        //        "????", "??????", "??", "??", "??", "??",
+        //        "?????????", "??"
+    //   };
     String input = "???????????????????????????????";
     String[] surfaceForms = {
         "????", "??????", "??", "??", "??", "??",
diff --git a/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/dict/UserDictionaryTest.java b/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/dict/UserDictionaryTest.java
index dff5f56..75177f9 100644
--- a/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/dict/UserDictionaryTest.java
+++ b/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/dict/UserDictionaryTest.java
@@ -75,6 +75,6 @@ public class UserDictionaryTest extends LuceneTestCase {
   @Test
   public void testRead() throws IOException {
     UserDictionary dictionary = TestJapaneseTokenizer.readDict();
-    assertNotNull(dictionary);		
+    assertNotNull(dictionary);
   }
 }
diff --git a/lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder.java b/lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder.java
index b39044f..2465c6a 100644
--- a/lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder.java
+++ b/lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder.java
@@ -174,26 +174,26 @@ public class TokenInfoDictionaryBuilder {
   /*
    * IPADIC features
    * 
-   * 0	- surface
-   * 1	- left cost
-   * 2	- right cost
-   * 3	- word cost
-   * 4-9	- pos
-   * 10	- base form
-   * 11	- reading
-   * 12	- pronounciation
+   * 0   - surface
+   * 1   - left cost
+   * 2   - right cost
+   * 3   - word cost
+   * 4-9 - pos
+   * 10  - base form
+   * 11  - reading
+   * 12  - pronounciation
    *
    * UniDic features
    * 
-   * 0	- surface
-   * 1	- left cost
-   * 2	- right cost
-   * 3	- word cost
-   * 4-9	- pos
-   * 10	- base form reading
-   * 11	- base form
-   * 12	- surface form
-   * 13	- surface reading
+   * 0   - surface
+   * 1   - left cost
+   * 2   - right cost
+   * 3   - word cost
+   * 4-9 - pos
+   * 10  - base form reading
+   * 11  - base form
+   * 12  - surface form
+   * 13  - surface reading
    */
   
   public String[] formatEntry(String[] features) {
@@ -221,7 +221,7 @@ public class TokenInfoDictionaryBuilder {
       } else {
         features2[11] = features[13];
         features2[12] = features[13];
-      }			
+      }
       return features2;
     }
   }
diff --git a/lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/UnknownDictionaryBuilder.java b/lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/UnknownDictionaryBuilder.java
index eb7e500..5676799 100644
--- a/lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/UnknownDictionaryBuilder.java
+++ b/lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/UnknownDictionaryBuilder.java
@@ -107,22 +107,22 @@ public class UnknownDictionaryBuilder {
         continue;
       }
       
-      if(line.startsWith("0x")) {	// Category mapping
-        String[] values = line.split(" ", 2);	// Split only first space
+      if(line.startsWith("0x")) {  // Category mapping
+        String[] values = line.split(" ", 2);  // Split only first space
         
         if(!values[0].contains("..")) {
           int cp = Integer.decode(values[0]).intValue();
-          dictionary.putCharacterCategory(cp, values[1]);					
+          dictionary.putCharacterCategory(cp, values[1]);
         } else {
           String[] codePoints = values[0].split("\\.\\.");
           int cpFrom = Integer.decode(codePoints[0]).intValue();
           int cpTo = Integer.decode(codePoints[1]).intValue();
           
           for(int i = cpFrom; i <= cpTo; i++){
-            dictionary.putCharacterCategory(i, values[1]);					
+            dictionary.putCharacterCategory(i, values[1]);
           }
         }
-      } else {	// Invoke definition
+      } else {  // Invoke definition
         String[] values = line.split(" "); // Consecutive space is merged above
         String characterClassName = values[0];
         int invoke = Integer.parseInt(values[1]);
diff --git a/lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestPhoneticFilterFactory.java b/lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestPhoneticFilterFactory.java
index 8bdd6e0..2ec8dea 100644
--- a/lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestPhoneticFilterFactory.java
+++ b/lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestPhoneticFilterFactory.java
@@ -175,23 +175,23 @@ public class TestPhoneticFilterFactory extends BaseTokenStreamTestCase {
   }
   
   public void testSpeed() throws Exception {
-	  checkSpeedEncoding("Metaphone", "easgasg", "ESKS");
-	  checkSpeedEncoding("DoubleMetaphone", "easgasg", "ASKS");
-	  checkSpeedEncoding("Soundex", "easgasg", "E220");
-	  checkSpeedEncoding("RefinedSoundex", "easgasg", "E034034");
-	  checkSpeedEncoding("Caverphone", "Carlene", "KLN1111111");
-	  checkSpeedEncoding("ColognePhonetic", "Schmitt", "862");
+    checkSpeedEncoding("Metaphone", "easgasg", "ESKS");
+    checkSpeedEncoding("DoubleMetaphone", "easgasg", "ASKS");
+    checkSpeedEncoding("Soundex", "easgasg", "E220");
+    checkSpeedEncoding("RefinedSoundex", "easgasg", "E034034");
+    checkSpeedEncoding("Caverphone", "Carlene", "KLN1111111");
+    checkSpeedEncoding("ColognePhonetic", "Schmitt", "862");
   }
   
   private void checkSpeedEncoding(String encoder, String toBeEncoded, String estimated) throws Exception {
-	  long start = System.currentTimeMillis();
-	  for ( int i=0; i<REPEATS; i++) {
-		    assertAlgorithm(encoder, "false", toBeEncoded,
-		            new String[] { estimated });
-	  }
-	  long duration = System.currentTimeMillis()-start;
-	  if (VERBOSE)
-	    System.out.println(encoder + " encodings per msec: "+(REPEATS/duration));
+    long start = System.currentTimeMillis();
+    for ( int i=0; i<REPEATS; i++) {
+        assertAlgorithm(encoder, "false", toBeEncoded,
+                new String[] { estimated });
+    }
+    long duration = System.currentTimeMillis()-start;
+    if (VERBOSE)
+      System.out.println(encoder + " encodings per msec: "+(REPEATS/duration));
   }
   
 }
diff --git a/lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/AbstractDictionary.java b/lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/AbstractDictionary.java
index f9a61e1..7d3622a 100644
--- a/lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/AbstractDictionary.java
+++ b/lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/AbstractDictionary.java
@@ -115,7 +115,7 @@ abstract class AbstractDictionary {
       }
       int b0 = (buffer[0] & 0x0FF) - 161; // Code starts from A1, therefore subtract 0xA1=161
       int b1 = (buffer[1] & 0x0FF) - 161; // There is no Chinese char for the first and last symbol. 
-      											// Therefore, each code page only has 16*6-2=94 characters.
+                                          // Therefore, each code page only has 16*6-2=94 characters.
       return (short) (b0 * 94 + b1);
     } catch (UnsupportedEncodingException e) {
       throw new RuntimeException(e);
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/StreamUtils.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/StreamUtils.java
index a2c25ba..9f20d48 100644
--- a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/StreamUtils.java
+++ b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/StreamUtils.java
@@ -37,43 +37,43 @@ import org.apache.commons.compress.compressors.CompressorStreamFactory;
  */
 public class StreamUtils {
 
-	/** Buffer size used across the benchmark package */
-	public static final int BUFFER_SIZE = 1 << 16; // 64K
-	
-	/** File format type */
-	public enum Type {
-		/** BZIP2 is automatically used for <b>.bz2</b> and <b>.bzip2</b> extensions. */
-		BZIP2(CompressorStreamFactory.BZIP2),
-		/** GZIP is automatically used for <b>.gz</b> and <b>.gzip</b> extensions. */
-		GZIP(CompressorStreamFactory.GZIP),
-		/** Plain text is used for anything which is not GZIP or BZIP. */
-		PLAIN(null);
-		private final String csfType;
-		Type(String csfType) {
-			this.csfType = csfType;
-		}
-		private InputStream inputStream(InputStream in) throws IOException {
-			try {
-				return csfType==null ? in : new CompressorStreamFactory().createCompressorInputStream(csfType, in);
-			} catch (CompressorException e) {
-    		IOException ioe = new IOException(e.getMessage());
-    		ioe.initCause(e);
-    		throw ioe;			}  
-		}
-		private OutputStream outputStream(OutputStream os) throws IOException {
-			try {
-				return csfType==null ? os : new CompressorStreamFactory().createCompressorOutputStream(csfType, os);
-			} catch (CompressorException e) {
-				IOException ioe = new IOException(e.getMessage());
-				ioe.initCause(e);
-				throw ioe;  
-			}  
-		}
-	}
-	
+  /** Buffer size used across the benchmark package */
+  public static final int BUFFER_SIZE = 1 << 16; // 64K
+
+  /** File format type */
+  public enum Type {
+    /** BZIP2 is automatically used for <b>.bz2</b> and <b>.bzip2</b> extensions. */
+    BZIP2(CompressorStreamFactory.BZIP2),
+    /** GZIP is automatically used for <b>.gz</b> and <b>.gzip</b> extensions. */
+    GZIP(CompressorStreamFactory.GZIP),
+    /** Plain text is used for anything which is not GZIP or BZIP. */
+    PLAIN(null);
+    private final String csfType;
+    Type(String csfType) {
+      this.csfType = csfType;
+    }
+    private InputStream inputStream(InputStream in) throws IOException {
+      try {
+        return csfType==null ? in : new CompressorStreamFactory().createCompressorInputStream(csfType, in);
+      } catch (CompressorException e) {
+        IOException ioe = new IOException(e.getMessage());
+        ioe.initCause(e);
+        throw ioe;      }
+    }
+    private OutputStream outputStream(OutputStream os) throws IOException {
+      try {
+        return csfType==null ? os : new CompressorStreamFactory().createCompressorOutputStream(csfType, os);
+      } catch (CompressorException e) {
+        IOException ioe = new IOException(e.getMessage());
+        ioe.initCause(e);
+        throw ioe;
+      }
+    }
+  }
+
   private static final Map<String,Type> extensionToType = new HashMap<String,Type>();
   static {
-  	// these in are lower case, we will lower case at the test as well
+    // these in are lower case, we will lower case at the test as well
     extensionToType.put(".bz2", Type.BZIP2);
     extensionToType.put(".bzip", Type.BZIP2);
     extensionToType.put(".gz", Type.GZIP);
@@ -95,14 +95,14 @@ public class StreamUtils {
 
   /** Return the type of the file, or null if unknown */
   private static Type fileType(File file) {
-  	Type type = null;
+    Type type = null;
     String fileName = file.getName();
     int idx = fileName.lastIndexOf('.');
     if (idx != -1) {
       type = extensionToType.get(fileName.substring(idx).toLowerCase(Locale.ROOT));
     }
     return type==null ? Type.PLAIN : type;
-	}
+  }
   
   /**
    * Returns an {@link OutputStream} over the requested file, identifying
diff --git a/lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/WriteLineDocTaskTest.java b/lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/WriteLineDocTaskTest.java
index 65cb73e..eb1b1b8 100644
--- a/lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/WriteLineDocTaskTest.java
+++ b/lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/WriteLineDocTaskTest.java
@@ -157,16 +157,16 @@ public class WriteLineDocTaskTest extends BenchmarkTestCase {
                           String expDate, String expBody) throws Exception {
     InputStream in = new FileInputStream(file);
     switch(fileType) {
-    	case BZIP2:
-    		in = csFactory.createCompressorInputStream(CompressorStreamFactory.BZIP2, in);
-    		break;
-    	case GZIP:
-    		in = csFactory.createCompressorInputStream(CompressorStreamFactory.GZIP, in);
-                break;
-    	case PLAIN:
-    		break; // nothing to do
-    	default:
-    		assertFalse("Unknown file type!",true); //fail, should not happen
+      case BZIP2:
+        in = csFactory.createCompressorInputStream(CompressorStreamFactory.BZIP2, in);
+        break;
+      case GZIP:
+        in = csFactory.createCompressorInputStream(CompressorStreamFactory.GZIP, in);
+        break;
+      case PLAIN:
+        break; // nothing to do
+      default:
+        assertFalse("Unknown file type!",true); //fail, should not happen
     }
     BufferedReader br = new BufferedReader(new InputStreamReader(in, "utf-8"));
     try {
diff --git a/lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/utils/StreamUtilsTest.java b/lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/utils/StreamUtilsTest.java
index cb15be2..6bd7ee2 100644
--- a/lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/utils/StreamUtilsTest.java
+++ b/lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/utils/StreamUtilsTest.java
@@ -57,38 +57,38 @@ public class StreamUtilsTest extends BenchmarkTestCase {
 
   @Test
   public void testGetInputStreamBzip2() throws Exception {
-  	assertReadText(rawBzip2File("bz2"));
-  	assertReadText(rawBzip2File("bzip"));
-  	assertReadText(rawBzip2File("BZ2"));
-  	assertReadText(rawBzip2File("BZIP"));
+    assertReadText(rawBzip2File("bz2"));
+    assertReadText(rawBzip2File("bzip"));
+    assertReadText(rawBzip2File("BZ2"));
+    assertReadText(rawBzip2File("BZIP"));
   }
 
   @Test
   public void testGetOutputStreamBzip2() throws Exception {
-  	assertReadText(autoOutFile("bz2"));
-  	assertReadText(autoOutFile("bzip"));
-  	assertReadText(autoOutFile("BZ2"));
-  	assertReadText(autoOutFile("BZIP"));
+    assertReadText(autoOutFile("bz2"));
+    assertReadText(autoOutFile("bzip"));
+    assertReadText(autoOutFile("BZ2"));
+    assertReadText(autoOutFile("BZIP"));
   }
   
   @Test
   public void testGetOutputStreamGzip() throws Exception {
-  	assertReadText(autoOutFile("gz"));
-  	assertReadText(autoOutFile("gzip"));
-  	assertReadText(autoOutFile("GZ"));
-  	assertReadText(autoOutFile("GZIP"));
+    assertReadText(autoOutFile("gz"));
+    assertReadText(autoOutFile("gzip"));
+    assertReadText(autoOutFile("GZ"));
+    assertReadText(autoOutFile("GZIP"));
   }
 
   @Test
   public void testGetOutputStreamPlain() throws Exception {
-  	assertReadText(autoOutFile("txt"));
-  	assertReadText(autoOutFile("text"));
-  	assertReadText(autoOutFile("TXT"));
-  	assertReadText(autoOutFile("TEXT"));
+    assertReadText(autoOutFile("txt"));
+    assertReadText(autoOutFile("text"));
+    assertReadText(autoOutFile("TXT"));
+    assertReadText(autoOutFile("TEXT"));
   }
   
   private File rawTextFile(String ext) throws Exception {
-    File f = new File(testDir,"testfile." +	ext);
+    File f = new File(testDir,"testfile." +  ext);
     BufferedWriter w = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(f), IOUtils.CHARSET_UTF_8));
     w.write(TEXT);
     w.newLine();
@@ -97,32 +97,32 @@ public class StreamUtilsTest extends BenchmarkTestCase {
   }
   
   private File rawGzipFile(String ext) throws Exception {
-    File f = new File(testDir,"testfile." +	ext);
+    File f = new File(testDir,"testfile." +  ext);
     OutputStream os = new CompressorStreamFactory().createCompressorOutputStream(CompressorStreamFactory.GZIP, new FileOutputStream(f));
     writeText(os);
     return f;
   }
 
   private File rawBzip2File(String ext) throws Exception {
-  	File f = new File(testDir,"testfile." +	ext);
-  	OutputStream os = new CompressorStreamFactory().createCompressorOutputStream(CompressorStreamFactory.BZIP2, new FileOutputStream(f));
-  	writeText(os);
-  	return f;
+    File f = new File(testDir,"testfile." +  ext);
+    OutputStream os = new CompressorStreamFactory().createCompressorOutputStream(CompressorStreamFactory.BZIP2, new FileOutputStream(f));
+    writeText(os);
+    return f;
   }
 
   private File autoOutFile(String ext) throws Exception {
-  	File f = new File(testDir,"testfile." +	ext);
-  	OutputStream os = StreamUtils.outputStream(f);
-  	writeText(os);
-  	return f;
+    File f = new File(testDir,"testfile." +  ext);
+    OutputStream os = StreamUtils.outputStream(f);
+    writeText(os);
+    return f;
   }
 
-	private void writeText(OutputStream os) throws IOException {
-		BufferedWriter w = new BufferedWriter(new OutputStreamWriter(os, IOUtils.CHARSET_UTF_8));
-  	w.write(TEXT);
-  	w.newLine();
-  	w.close();
-	}
+  private void writeText(OutputStream os) throws IOException {
+    BufferedWriter w = new BufferedWriter(new OutputStreamWriter(os, IOUtils.CHARSET_UTF_8));
+    w.write(TEXT);
+    w.newLine();
+    w.close();
+  }
 
   private void assertReadText(File f) throws Exception {
     InputStream ir = StreamUtils.inputStream(f);
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexReader.java
index e3a3832..7396436 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexReader.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexReader.java
@@ -170,7 +170,7 @@ public class FixedGapTermsIndexReader extends TermsIndexReaderBase {
 
     @Override
     public long seek(BytesRef target) {
-      int lo = 0;				  // binary search
+      int lo = 0;          // binary search
       int hi = fieldIndex.numIndexTerms - 1;
       assert totalIndexInterval > 0 : "totalIndexInterval=" + totalIndexInterval;
 
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/BitVector.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/BitVector.java
index f45df60..3ed18a1 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/BitVector.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/BitVector.java
@@ -163,7 +163,7 @@ final class BitVector implements Cloneable, MutableBits {
       int c = 0;
       int end = bits.length;
       for (int i = 0; i < end; i++) {
-        c += BYTE_COUNTS[bits[i] & 0xFF];	  // sum bits per byte
+        c += BYTE_COUNTS[bits[i] & 0xFF];  // sum bits per byte
       }
       count = c;
     }
@@ -176,12 +176,12 @@ final class BitVector implements Cloneable, MutableBits {
     int c = 0;
     int end = bits.length;
     for (int i = 0; i < end; i++) {
-      c += BYTE_COUNTS[bits[i] & 0xFF];	  // sum bits per byte
+      c += BYTE_COUNTS[bits[i] & 0xFF];  // sum bits per byte
     }
     return c;
   }
 
-  private static final byte[] BYTE_COUNTS = {	  // table of bits/byte
+  private static final byte[] BYTE_COUNTS = {  // table of bits/byte
     0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4,
     1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5,
     1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5,
diff --git a/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java b/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
index f964aa5d..c1656cf 100644
--- a/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
+++ b/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
@@ -1672,7 +1672,7 @@ public class CheckIndex {
                          "              times, to check more than one segment, eg '-segment _2 -segment _a'.\n" +
                          "              You can't use this with the -fix option\n" +
                          "  -dir-impl X: use a specific " + FSDirectory.class.getSimpleName() + " implementation. " +
-                         		"If no package is specified the " + FSDirectory.class.getPackage().getName() + " package will be used.\n" +
+                         "If no package is specified the " + FSDirectory.class.getPackage().getName() + " package will be used.\n" +
                          "\n" +
                          "**WARNING**: -fix should only be used on an emergency basis as it will cause\n" +
                          "documents (perhaps many) to be permanently removed from the index.  Always make\n" +
diff --git a/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushControl.java b/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushControl.java
index 4d26414..dde4fcf 100644
--- a/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushControl.java
+++ b/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushControl.java
@@ -396,11 +396,11 @@ final class DocumentsWriterFlushControl  {
     return flushingWriters.size();
   }
   
-  public boolean doApplyAllDeletes() {	
+  public boolean doApplyAllDeletes() {
     return flushDeletes.getAndSet(false);
   }
 
-  public void setApplyAllDeletes() {	
+  public void setApplyAllDeletes() {
     flushDeletes.set(true);
   }
   
diff --git a/lucene/core/src/java/org/apache/lucene/index/IndexFileDeleter.java b/lucene/core/src/java/org/apache/lucene/index/IndexFileDeleter.java
index 2c0856e..175a295 100644
--- a/lucene/core/src/java/org/apache/lucene/index/IndexFileDeleter.java
+++ b/lucene/core/src/java/org/apache/lucene/index/IndexFileDeleter.java
@@ -571,7 +571,7 @@ final class IndexFileDeleter {
         infoStream.message("IFD", "delete \"" + fileName + "\"");
       }
       directory.deleteFile(fileName);
-    } catch (IOException e) {			  // if delete fails
+    } catch (IOException e) {  // if delete fails
       if (directory.fileExists(fileName)) {
 
         // Some operating systems (e.g. Windows) don't
diff --git a/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java b/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
index 4290c05..a80b855 100644
--- a/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
@@ -2847,7 +2847,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit {
       final boolean anySegmentFlushed;
       
       synchronized (fullFlushLock) {
-    	boolean flushSuccess = false;
+      boolean flushSuccess = false;
         try {
           anySegmentFlushed = docWriter.flushAllThreads();
           flushSuccess = true;
diff --git a/lucene/core/src/java/org/apache/lucene/index/SegmentInfo.java b/lucene/core/src/java/org/apache/lucene/index/SegmentInfo.java
index 6e7807a..955ca32 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SegmentInfo.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SegmentInfo.java
@@ -42,9 +42,9 @@ public final class SegmentInfo {
   public static final int NO = -1;          // e.g. no norms; no deletes;
   public static final int YES = 1;          // e.g. have norms; have deletes;
 
-  public final String name;				  // unique name in dir
-  private int docCount;				  // number of docs in seg
-  public final Directory dir;				  // where segment resides
+  public final String name;     // unique name in dir
+  private int docCount;         // number of docs in seg
+  public final Directory dir;   // where segment resides
 
   private boolean isCompoundFile;
 
diff --git a/lucene/core/src/java/org/apache/lucene/search/BooleanQuery.java b/lucene/core/src/java/org/apache/lucene/search/BooleanQuery.java
index 1a08435..d0b9b5c 100644
--- a/lucene/core/src/java/org/apache/lucene/search/BooleanQuery.java
+++ b/lucene/core/src/java/org/apache/lucene/search/BooleanQuery.java
@@ -404,7 +404,7 @@ public class BooleanQuery extends Query implements Iterable<BooleanClause> {
   public Query rewrite(IndexReader reader) throws IOException {
     if (minNrShouldMatch == 0 && clauses.size() == 1) {                    // optimize 1-clause queries
       BooleanClause c = clauses.get(0);
-      if (!c.isProhibited()) {			  // just return clause
+      if (!c.isProhibited()) {  // just return clause
 
         Query query = c.getQuery().rewrite(reader);    // rewrite first
 
@@ -475,7 +475,7 @@ public class BooleanQuery extends Query implements Iterable<BooleanClause> {
 
       Query subQuery = c.getQuery();
       if (subQuery != null) {
-        if (subQuery instanceof BooleanQuery) {	  // wrap sub-bools in parens
+        if (subQuery instanceof BooleanQuery) {  // wrap sub-bools in parens
           buffer.append("(");
           buffer.append(subQuery.toString(field));
           buffer.append(")");
diff --git a/lucene/core/src/java/org/apache/lucene/search/FieldCache.java b/lucene/core/src/java/org/apache/lucene/search/FieldCache.java
index ff59940..a1978e3 100644
--- a/lucene/core/src/java/org/apache/lucene/search/FieldCache.java
+++ b/lucene/core/src/java/org/apache/lucene/search/FieldCache.java
@@ -508,7 +508,7 @@ public interface FieldCache {
       // this special case is the reason that Arrays.binarySearch() isn't useful.
       if (key == null)
         return 0;
-	  
+  
       int low = 1;
       int high = numOrd()-1;
 
diff --git a/lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.java b/lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.java
index 18f93f3..0c91623 100644
--- a/lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.java
+++ b/lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.java
@@ -52,7 +52,7 @@ import org.apache.lucene.util.packed.PackedInts;
  * @since   lucene 1.4
  */
 class FieldCacheImpl implements FieldCache {
-	
+
   private Map<Class<?>,Cache> caches;
   FieldCacheImpl() {
     init();
@@ -173,7 +173,7 @@ class FieldCacheImpl implements FieldCache {
         ((AtomicReader)key).addReaderClosedListener(purgeReader); 
       } else {
         // last chance
-        reader.addReaderClosedListener(purgeReader); 				
+        reader.addReaderClosedListener(purgeReader);
       }
     }
   }
diff --git a/lucene/core/src/java/org/apache/lucene/search/FilteredDocIdSetIterator.java b/lucene/core/src/java/org/apache/lucene/search/FilteredDocIdSetIterator.java
index f16da33..35c50bb 100644
--- a/lucene/core/src/java/org/apache/lucene/search/FilteredDocIdSetIterator.java
+++ b/lucene/core/src/java/org/apache/lucene/search/FilteredDocIdSetIterator.java
@@ -28,7 +28,7 @@ import java.io.IOException;
 public abstract class FilteredDocIdSetIterator extends DocIdSetIterator {
   protected DocIdSetIterator _innerIter;
   private int doc;
-	
+
   /**
    * Constructor.
    * @param innerIter Underlying DocIdSetIterator.
@@ -40,7 +40,7 @@ public abstract class FilteredDocIdSetIterator extends DocIdSetIterator {
     _innerIter = innerIter;
     doc = -1;
   }
-	
+
   /**
    * Validation method to determine whether a docid should be in the result set.
    * @param doc docid to be tested
@@ -48,7 +48,7 @@ public abstract class FilteredDocIdSetIterator extends DocIdSetIterator {
    * @see #FilteredDocIdSetIterator(DocIdSetIterator)
    */
   protected abstract boolean match(int doc);
-	
+
   @Override
   public int docID() {
     return doc;
diff --git a/lucene/core/src/java/org/apache/lucene/search/MultiPhraseQuery.java b/lucene/core/src/java/org/apache/lucene/search/MultiPhraseQuery.java
index 88ce67e..992162a 100644
--- a/lucene/core/src/java/org/apache/lucene/search/MultiPhraseQuery.java
+++ b/lucene/core/src/java/org/apache/lucene/search/MultiPhraseQuery.java
@@ -110,7 +110,7 @@ public class MultiPhraseQuery extends Query {
    * Do not modify the List or its contents.
    */
   public List<Term[]> getTermArrays() {
-	  return Collections.unmodifiableList(termArrays);
+    return Collections.unmodifiableList(termArrays);
   }
 
   /**
diff --git a/lucene/core/src/java/org/apache/lucene/search/PhrasePositions.java b/lucene/core/src/java/org/apache/lucene/search/PhrasePositions.java
index 15a32bb..c975b01 100644
--- a/lucene/core/src/java/org/apache/lucene/search/PhrasePositions.java
+++ b/lucene/core/src/java/org/apache/lucene/search/PhrasePositions.java
@@ -24,13 +24,13 @@ import org.apache.lucene.index.*;
  * Position of a term in a document that takes into account the term offset within the phrase. 
  */
 final class PhrasePositions {
-  int doc;					  // current doc
-  int position;					  // position in doc
-  int count;					  // remaining pos in this doc
-  int offset;					  // position in phrase
+  int doc;              // current doc
+  int position;         // position in doc
+  int count;            // remaining pos in this doc
+  int offset;           // position in phrase
   final int ord;                                  // unique across all PhrasePositions instances
-  final DocsAndPositionsEnum postings;  	  // stream of docs & positions
-  PhrasePositions next;	                          // used to make lists
+  final DocsAndPositionsEnum postings;            // stream of docs & positions
+  PhrasePositions next;                           // used to make lists
   int rptGroup = -1; // >=0 indicates that this is a repeating PP
   int rptInd; // index in the rptGroup
   final Term[] terms; // for repetitions initialization 
@@ -42,7 +42,7 @@ final class PhrasePositions {
     this.terms = terms;
   }
 
-  final boolean next() throws IOException {	  // increments to next doc
+  final boolean next() throws IOException {  // increments to next doc
     doc = postings.nextDoc();
     if (doc == DocIdSetIterator.NO_MORE_DOCS) {
       return false;
@@ -59,7 +59,7 @@ final class PhrasePositions {
   }
 
   final void firstPosition() throws IOException {
-    count = postings.freq();				  // read first pos
+    count = postings.freq();  // read first pos
     nextPosition();
   }
 
@@ -70,7 +70,7 @@ final class PhrasePositions {
    * have exactly the same <code>position</code>.
    */
   final boolean nextPosition() throws IOException {
-    if (count-- > 0) {				  // read subsequent pos's
+    if (count-- > 0) {  // read subsequent pos's
       position = postings.nextPosition() - offset;
       return true;
     } else
diff --git a/lucene/core/src/java/org/apache/lucene/search/PhraseQuery.java b/lucene/core/src/java/org/apache/lucene/search/PhraseQuery.java
index d8e2835..724ed2c 100644
--- a/lucene/core/src/java/org/apache/lucene/search/PhraseQuery.java
+++ b/lucene/core/src/java/org/apache/lucene/search/PhraseQuery.java
@@ -281,7 +281,7 @@ public class PhraseQuery extends Query {
         ArrayUtil.mergeSort(postingsFreqs);
       }
 
-      if (slop == 0) {				  // optimize exact case
+      if (slop == 0) {  // optimize exact case
         ExactPhraseScorer s = new ExactPhraseScorer(this, postingsFreqs, similarity.exactSimScorer(stats, context));
         if (s.noDocs) {
           return null;
diff --git a/lucene/core/src/java/org/apache/lucene/search/payloads/MinPayloadFunction.java b/lucene/core/src/java/org/apache/lucene/search/payloads/MinPayloadFunction.java
index 789ceaa..6e962e7 100644
--- a/lucene/core/src/java/org/apache/lucene/search/payloads/MinPayloadFunction.java
+++ b/lucene/core/src/java/org/apache/lucene/search/payloads/MinPayloadFunction.java
@@ -24,12 +24,12 @@ package org.apache.lucene.search.payloads;
 public class MinPayloadFunction extends PayloadFunction {
 
   @Override
-	public float currentScore(int docId, String field, int start, int end, int numPayloadsSeen, float currentScore, float currentPayloadScore) {
+  public float currentScore(int docId, String field, int start, int end, int numPayloadsSeen, float currentScore, float currentPayloadScore) {
     if (numPayloadsSeen == 0) {
       return currentPayloadScore;
     } else {
-		return Math.min(currentPayloadScore, currentScore);
-	}
+      return Math.min(currentPayloadScore, currentScore);
+    }
   }
 
   @Override
diff --git a/lucene/core/src/java/org/apache/lucene/search/payloads/PayloadFunction.java b/lucene/core/src/java/org/apache/lucene/search/payloads/PayloadFunction.java
index 976ed09..585aa15 100644
--- a/lucene/core/src/java/org/apache/lucene/search/payloads/PayloadFunction.java
+++ b/lucene/core/src/java/org/apache/lucene/search/payloads/PayloadFunction.java
@@ -56,10 +56,10 @@ public abstract class PayloadFunction {
   public abstract float docScore(int docId, String field, int numPayloadsSeen, float payloadScore);
   
   public Explanation explain(int docId, String field, int numPayloadsSeen, float payloadScore){
-	  Explanation result = new Explanation();
-	  result.setDescription(getClass().getSimpleName() + ".docScore()");
-	  result.setValue(docScore(docId, field, numPayloadsSeen, payloadScore));
-	  return result;
+    Explanation result = new Explanation();
+    result.setDescription(getClass().getSimpleName() + ".docScore()");
+    result.setValue(docScore(docId, field, numPayloadsSeen, payloadScore));
+    return result;
   };
   
   @Override
diff --git a/lucene/core/src/java/org/apache/lucene/search/payloads/PayloadNearQuery.java b/lucene/core/src/java/org/apache/lucene/search/payloads/PayloadNearQuery.java
index 5a94f08..ef2c6e5 100644
--- a/lucene/core/src/java/org/apache/lucene/search/payloads/PayloadNearQuery.java
+++ b/lucene/core/src/java/org/apache/lucene/search/payloads/PayloadNearQuery.java
@@ -257,7 +257,7 @@ public class PayloadNearQuery extends SpanNearQuery {
             getPayloads(spansArr);            
             more = spans.next();
           } while (more && (doc == spans.doc()));
-          return true;    	
+          return true;
     }
 
     @Override
diff --git a/lucene/core/src/java/org/apache/lucene/search/spans/NearSpansOrdered.java b/lucene/core/src/java/org/apache/lucene/search/spans/NearSpansOrdered.java
index a27c0cc..a5bc329 100644
--- a/lucene/core/src/java/org/apache/lucene/search/spans/NearSpansOrdered.java
+++ b/lucene/core/src/java/org/apache/lucene/search/spans/NearSpansOrdered.java
@@ -117,7 +117,7 @@ public class NearSpansOrdered extends Spans {
   public int end() { return matchEnd; }
   
   public Spans[] getSubSpans() {
-	  return subSpans;
+    return subSpans;
   }  
 
   // TODO: Remove warning after API has been finalized
diff --git a/lucene/core/src/java/org/apache/lucene/search/spans/NearSpansUnordered.java b/lucene/core/src/java/org/apache/lucene/search/spans/NearSpansUnordered.java
index db1cf88..7279b04 100644
--- a/lucene/core/src/java/org/apache/lucene/search/spans/NearSpansUnordered.java
+++ b/lucene/core/src/java/org/apache/lucene/search/spans/NearSpansUnordered.java
@@ -151,7 +151,7 @@ public class NearSpansUnordered extends Spans {
     }
   }
   public Spans[] getSubSpans() {
-	  return subSpans;
+    return subSpans;
   }
   @Override
   public boolean next() throws IOException {
@@ -286,7 +286,7 @@ public class NearSpansUnordered extends Spans {
   }
 
   private void addToList(SpansCell cell) {
-    if (last != null) {			  // add next to end of list
+    if (last != null) {  // add next to end of list
       last.next = cell;
     } else
       first = cell;
@@ -295,7 +295,7 @@ public class NearSpansUnordered extends Spans {
   }
 
   private void firstToLast() {
-    last.next = first;			  // move first to end of list
+    last.next = first;  // move first to end of list
     last = first;
     first = first.next;
     last.next = null;
diff --git a/lucene/core/src/java/org/apache/lucene/search/spans/SpanNearQuery.java b/lucene/core/src/java/org/apache/lucene/search/spans/SpanNearQuery.java
index 68aff02..7c7781c 100644
--- a/lucene/core/src/java/org/apache/lucene/search/spans/SpanNearQuery.java
+++ b/lucene/core/src/java/org/apache/lucene/search/spans/SpanNearQuery.java
@@ -92,9 +92,9 @@ public class SpanNearQuery extends SpanQuery implements Cloneable {
   
   @Override
   public void extractTerms(Set<Term> terms) {
-	    for (final SpanQuery clause : clauses) {
-	      clause.extractTerms(terms);
-	    }
+    for (final SpanQuery clause : clauses) {
+      clause.extractTerms(terms);
+    }
   }  
   
 
diff --git a/lucene/core/src/java/org/apache/lucene/search/spans/SpanPositionCheckQuery.java b/lucene/core/src/java/org/apache/lucene/search/spans/SpanPositionCheckQuery.java
index 65e8f53..5c12fc0 100644
--- a/lucene/core/src/java/org/apache/lucene/search/spans/SpanPositionCheckQuery.java
+++ b/lucene/core/src/java/org/apache/lucene/search/spans/SpanPositionCheckQuery.java
@@ -57,7 +57,7 @@ public abstract class SpanPositionCheckQuery extends SpanQuery implements Clonea
 
   @Override
   public void extractTerms(Set<Term> terms) {
-	    match.extractTerms(terms);
+    match.extractTerms(terms);
   }
 
   /** 
@@ -186,4 +186,4 @@ public abstract class SpanPositionCheckQuery extends SpanQuery implements Clonea
       }
 
   }
-}
\ No newline at end of file
+}
diff --git a/lucene/core/src/java/org/apache/lucene/search/spans/Spans.java b/lucene/core/src/java/org/apache/lucene/search/spans/Spans.java
index 9c094d7..2e3c0ce 100644
--- a/lucene/core/src/java/org/apache/lucene/search/spans/Spans.java
+++ b/lucene/core/src/java/org/apache/lucene/search/spans/Spans.java
@@ -34,7 +34,7 @@ public abstract class Spans {
    *   boolean skipTo(int target) {
    *     do {
    *       if (!next())
-   * 	     return false;
+   *         return false;
    *     } while (target > doc());
    *     return true;
    *   }
diff --git a/lucene/core/src/java/org/apache/lucene/store/BufferedIndexInput.java b/lucene/core/src/java/org/apache/lucene/store/BufferedIndexInput.java
index a59fb67..ceb9131 100644
--- a/lucene/core/src/java/org/apache/lucene/store/BufferedIndexInput.java
+++ b/lucene/core/src/java/org/apache/lucene/store/BufferedIndexInput.java
@@ -41,9 +41,9 @@ public abstract class BufferedIndexInput extends IndexInput {
   
   protected byte[] buffer;
   
-  private long bufferStart = 0;			  // position in file of buffer
-  private int bufferLength = 0;			  // end of valid bytes
-  private int bufferPosition = 0;		  // next byte to read
+  private long bufferStart = 0;       // position in file of buffer
+  private int bufferLength = 0;       // end of valid bytes
+  private int bufferPosition = 0;     // next byte to read
 
   @Override
   public final byte readByte() throws IOException {
@@ -259,7 +259,7 @@ public abstract class BufferedIndexInput extends IndexInput {
   private void refill() throws IOException {
     long start = bufferStart + bufferPosition;
     long end = start + bufferSize;
-    if (end > length())				  // don't read past EOF
+    if (end > length())  // don't read past EOF
       end = length();
     int newLength = (int)(end - start);
     if (newLength <= 0)
@@ -294,7 +294,7 @@ public abstract class BufferedIndexInput extends IndexInput {
     else {
       bufferStart = pos;
       bufferPosition = 0;
-      bufferLength = 0;				  // trigger refill() on read()
+      bufferLength = 0;  // trigger refill() on read()
       seekInternal(pos);
     }
   }
diff --git a/lucene/core/src/java/org/apache/lucene/store/Lock.java b/lucene/core/src/java/org/apache/lucene/store/Lock.java
index 1d35f5d..9eb5c8a 100644
--- a/lucene/core/src/java/org/apache/lucene/store/Lock.java
+++ b/lucene/core/src/java/org/apache/lucene/store/Lock.java
@@ -135,7 +135,7 @@ public abstract class Lock {
          return doBody();
       } finally {
         if (locked)
-	      lock.release();
+          lock.release();
       }
     }
   }
diff --git a/lucene/core/src/java/org/apache/lucene/util/Constants.java b/lucene/core/src/java/org/apache/lucene/util/Constants.java
index 3855173..086ba2b 100644
--- a/lucene/core/src/java/org/apache/lucene/util/Constants.java
+++ b/lucene/core/src/java/org/apache/lucene/util/Constants.java
@@ -26,7 +26,7 @@ import org.apache.lucene.LucenePackage;
  **/
 
 public final class Constants {
-  private Constants() {}			  // can't construct
+  private Constants() {}  // can't construct
 
   /** JVM vendor info. */
   public static final String JVM_VENDOR = System.getProperty("java.vm.vendor");
diff --git a/lucene/core/src/java/org/apache/lucene/util/PriorityQueue.java b/lucene/core/src/java/org/apache/lucene/util/PriorityQueue.java
index a06c843..ef658bc 100644
--- a/lucene/core/src/java/org/apache/lucene/util/PriorityQueue.java
+++ b/lucene/core/src/java/org/apache/lucene/util/PriorityQueue.java
@@ -177,11 +177,11 @@ public abstract class PriorityQueue<T> {
     time. */
   public final T pop() {
     if (size > 0) {
-      T result = heap[1];			  // save first value
-      heap[1] = heap[size];			  // move last to first
-      heap[size] = null;			  // permit GC of objects
+      T result = heap[1];       // save first value
+      heap[1] = heap[size];     // move last to first
+      heap[size] = null;        // permit GC of objects
       size--;
-      downHeap();				  // adjust heap
+      downHeap();               // adjust heap
       return result;
     } else
       return null;
@@ -226,26 +226,26 @@ public abstract class PriorityQueue<T> {
 
   private final void upHeap() {
     int i = size;
-    T node = heap[i];			  // save bottom node
+    T node = heap[i];          // save bottom node
     int j = i >>> 1;
     while (j > 0 && lessThan(node, heap[j])) {
-      heap[i] = heap[j];			  // shift parents down
+      heap[i] = heap[j];       // shift parents down
       i = j;
       j = j >>> 1;
     }
-    heap[i] = node;				  // install saved node
+    heap[i] = node;            // install saved node
   }
 
   private final void downHeap() {
     int i = 1;
-    T node = heap[i];			  // save top node
-    int j = i << 1;				  // find smaller child
+    T node = heap[i];          // save top node
+    int j = i << 1;            // find smaller child
     int k = j + 1;
     if (k <= size && lessThan(heap[k], heap[j])) {
       j = k;
     }
     while (j <= size && lessThan(heap[j], node)) {
-      heap[i] = heap[j];			  // shift up child
+      heap[i] = heap[j];       // shift up child
       i = j;
       j = i << 1;
       k = j + 1;
@@ -253,7 +253,7 @@ public abstract class PriorityQueue<T> {
         j = k;
       }
     }
-    heap[i] = node;				  // install saved node
+    heap[i] = node;            // install saved node
   }
   
   /** This method returns the internal heap array as Object[].
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestLongPostings.java b/lucene/core/src/test/org/apache/lucene/index/TestLongPostings.java
index e162097..8683f4b 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestLongPostings.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestLongPostings.java
@@ -111,26 +111,26 @@ public class TestLongPostings extends LuceneTestCase {
     }
 
     final IndexReader r;
-	  final IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))
-	    .setOpenMode(IndexWriterConfig.OpenMode.CREATE)
-	    .setMergePolicy(newLogMergePolicy());
-	  iwc.setRAMBufferSizeMB(16.0 + 16.0 * random().nextDouble());
-	  iwc.setMaxBufferedDocs(-1);
-	  final RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);
-	
-	  for(int idx=0;idx<NUM_DOCS;idx++) {
-	    final Document doc = new Document();
-	    String s = isS1.get(idx) ? s1 : s2;
-	    final Field f = newTextField("field", s, Field.Store.NO);
-	    final int count = _TestUtil.nextInt(random(), 1, 4);
-	    for(int ct=0;ct<count;ct++) {
-	      doc.add(f);
-	    }
-	    riw.addDocument(doc);
-	  }
-	
-	  r = riw.getReader();
-	  riw.close();
+    final IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))
+      .setOpenMode(IndexWriterConfig.OpenMode.CREATE)
+      .setMergePolicy(newLogMergePolicy());
+    iwc.setRAMBufferSizeMB(16.0 + 16.0 * random().nextDouble());
+    iwc.setMaxBufferedDocs(-1);
+    final RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);
+
+    for(int idx=0;idx<NUM_DOCS;idx++) {
+      final Document doc = new Document();
+      String s = isS1.get(idx) ? s1 : s2;
+      final Field f = newTextField("field", s, Field.Store.NO);
+      final int count = _TestUtil.nextInt(random(), 1, 4);
+      for(int ct=0;ct<count;ct++) {
+        doc.add(f);
+      }
+      riw.addDocument(doc);
+    }
+
+    r = riw.getReader();
+    riw.close();
 
     /*
     if (VERBOSE) {
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestParallelReaderEmptyIndex.java b/lucene/core/src/test/org/apache/lucene/index/TestParallelReaderEmptyIndex.java
index aa8f6aa..5641916 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestParallelReaderEmptyIndex.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestParallelReaderEmptyIndex.java
@@ -152,7 +152,7 @@ public class TestParallelReaderEmptyIndex extends LuceneTestCase {
 
     rd1.close();
     rd2.close();
-		
+
     iwOut.forceMerge(1);
     iwOut.close();
     
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestSameTokenSamePosition.java b/lucene/core/src/test/org/apache/lucene/index/TestSameTokenSamePosition.java
index 5d627f1..52b8ce8 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestSameTokenSamePosition.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestSameTokenSamePosition.java
@@ -78,7 +78,7 @@ final class BugReproTokenStream extends TokenStream {
       offsetAtt.setOffset(starts[nextTokenIndex], ends[nextTokenIndex]);
       posIncAtt.setPositionIncrement(incs[nextTokenIndex]);
       nextTokenIndex++;
-      return true;			
+      return true;
     } else {
       return false;
     }
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestTransactionRollback.java b/lucene/core/src/test/org/apache/lucene/index/TestTransactionRollback.java
index f28fcfc..abc155c 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestTransactionRollback.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestTransactionRollback.java
@@ -41,13 +41,13 @@ import org.apache.lucene.util.Bits;
  */
 
 public class TestTransactionRollback extends LuceneTestCase {
-	
+
   private static final String FIELD_RECORD_ID = "record_id";
   private Directory dir;
-	
+
   //Rolls back index to a chosen ID
   private void rollBackLast(int id) throws Exception {
-		
+
     // System.out.println("Attempting to rollback to "+id);
     String ids="-"+id;
     IndexCommit last=null;
@@ -62,7 +62,7 @@ public class TestTransactionRollback extends LuceneTestCase {
 
     if (last==null)
       throw new RuntimeException("Couldn't find commit point "+id);
-		
+
     IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(
         TEST_VERSION_CURRENT, new MockAnalyzer(random())).setIndexDeletionPolicy(
         new RollbackDeletionPolicy(id)).setIndexCommit(last));
@@ -72,22 +72,22 @@ public class TestTransactionRollback extends LuceneTestCase {
     w.close();
   }
 
-  public void testRepeatedRollBacks() throws Exception {		
+  public void testRepeatedRollBacks() throws Exception {
 
     int expectedLastRecordId=100;
     while (expectedLastRecordId>10) {
-      expectedLastRecordId -=10;			
+      expectedLastRecordId -=10;
       rollBackLast(expectedLastRecordId);
       
       BitSet expecteds = new BitSet(100);
       expecteds.set(1,(expectedLastRecordId+1),true);
-      checkExpecteds(expecteds);			
+      checkExpecteds(expecteds);
     }
   }
-	
+
   private void checkExpecteds(BitSet expecteds) throws Exception {
     IndexReader r = DirectoryReader.open(dir);
-		
+
     //Perhaps not the most efficient approach but meets our
     //needs here.
     final Bits liveDocs = MultiFields.getLiveDocs(r);
@@ -114,7 +114,7 @@ public class TestTransactionRollback extends LuceneTestCase {
       Collection files = comm.getFileNames();
       for (Iterator iterator2 = files.iterator(); iterator2.hasNext();) {
         String filename = (String) iterator2.next();
-        System.out.print(filename+", ");				
+        System.out.print(filename+", ");
       }
       System.out.println();
     }
@@ -133,7 +133,7 @@ public class TestTransactionRollback extends LuceneTestCase {
       Document doc=new Document();
       doc.add(newTextField(FIELD_RECORD_ID, ""+currentRecordId, Field.Store.YES));
       w.addDocument(doc);
-			
+
       if (currentRecordId%10 == 0) {
         Map<String,String> data = new HashMap<String,String>();
         data.put("index", "records 1-"+currentRecordId);
@@ -177,16 +177,16 @@ public class TestTransactionRollback extends LuceneTestCase {
                              " UserData="+commit.getUserData() +")  ("+(commits.size()-1)+" commit points left) files=");
             Collection files = commit.getFileNames();
             for (Iterator iterator2 = files.iterator(); iterator2.hasNext();) {
-              System.out.print(" "+iterator2.next());				
+              System.out.print(" "+iterator2.next());
             }
             System.out.println();
             */
-						
-            commit.delete();									
+
+            commit.delete();
           }
         }
       }
-    }		
+    }
   }
 
   class DeleteLastCommitPolicy implements IndexDeletionPolicy {
@@ -198,7 +198,7 @@ public class TestTransactionRollback extends LuceneTestCase {
     }
   }
 
-  public void testRollbackDeletionPolicy() throws Exception {		
+  public void testRollbackDeletionPolicy() throws Exception {
     for(int i=0;i<2;i++) {
       // Unless you specify a prior commit point, rollback
       // should not work:
@@ -209,7 +209,7 @@ public class TestTransactionRollback extends LuceneTestCase {
       r.close();
     }
   }
-	
+
   // Keeps all commit points (used to build index)
   class KeepAllDeletionPolicy implements IndexDeletionPolicy {
     public void onCommit(List<? extends IndexCommit> commits) throws IOException {}
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestTransactions.java b/lucene/core/src/test/org/apache/lucene/index/TestTransactions.java
index 981b1b0..b46bbf1 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestTransactions.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestTransactions.java
@@ -129,7 +129,7 @@ public class TestTransactions extends LuceneTestCase {
           }
           try {
             writer2.prepareCommit();
-          } catch (Throwable t) { 	
+          } catch (Throwable t) {
             writer1.rollback();
             writer2.rollback();
             return;
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestCachingCollector.java b/lucene/core/src/test/org/apache/lucene/search/TestCachingCollector.java
index 298698b..b68ce41 100755
--- a/lucene/core/src/test/org/apache/lucene/search/TestCachingCollector.java
+++ b/lucene/core/src/test/org/apache/lucene/search/TestCachingCollector.java
@@ -145,7 +145,7 @@ public class TestCachingCollector extends LuceneTestCase {
     try {
       cc.replay(new NoOpCollector(false)); // this call should fail
       fail("should have failed if an in-order Collector was given to replay(), " +
-      		"while CachingCollector was initialized with out-of-order collection");
+           "while CachingCollector was initialized with out-of-order collection");
     } catch (IllegalArgumentException e) {
       // ok
     }
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestDocIdSet.java b/lucene/core/src/test/org/apache/lucene/search/TestDocIdSet.java
index f9a9735..98fe6fb 100644
--- a/lucene/core/src/test/org/apache/lucene/search/TestDocIdSet.java
+++ b/lucene/core/src/test/org/apache/lucene/search/TestDocIdSet.java
@@ -62,15 +62,15 @@ public class TestDocIdSet extends LuceneTestCase {
           };
         } 
       };
-	  
-		
+
+
     DocIdSet filteredSet = new FilteredDocIdSet(innerSet){
         @Override
         protected boolean match(int docid) {
           return docid%2 == 0;  //validate only even docids
-        }	
+        }
       };
-	  
+
     DocIdSetIterator iter = filteredSet.iterator();
     ArrayList<Integer> list = new ArrayList<Integer>();
     int doc = iter.advance(3);
@@ -80,7 +80,7 @@ public class TestDocIdSet extends LuceneTestCase {
         list.add(Integer.valueOf(doc));
       }
     }
-	  
+
     int[] docs = new int[list.size()];
     int c=0;
     Iterator<Integer> intIter = list.iterator();
@@ -151,7 +151,7 @@ public class TestDocIdSet extends LuceneTestCase {
           @Override
           protected boolean match(int docid) {
             return true;
-          }	
+          }
         };
       }
     };
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestFuzzyQuery.java b/lucene/core/src/test/org/apache/lucene/search/TestFuzzyQuery.java
index 7f4da03..a006d51 100644
--- a/lucene/core/src/test/org/apache/lucene/search/TestFuzzyQuery.java
+++ b/lucene/core/src/test/org/apache/lucene/search/TestFuzzyQuery.java
@@ -101,7 +101,7 @@ public class TestFuzzyQuery extends LuceneTestCase {
     }
 
     // not similar enough:
-    query = new FuzzyQuery(new Term("field", "xxxxx"), FuzzyQuery.defaultMaxEdits, 0);  	
+    query = new FuzzyQuery(new Term("field", "xxxxx"), FuzzyQuery.defaultMaxEdits, 0);
     hits = searcher.search(query, null, 1000).scoreDocs;
     assertEquals(0, hits.length);
     query = new FuzzyQuery(new Term("field", "aaccc"), FuzzyQuery.defaultMaxEdits, 0);   // edit distance to "aaaaa" = 3
diff --git a/lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadNearQuery.java b/lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadNearQuery.java
index 1452c9c..f6cd702 100644
--- a/lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadNearQuery.java
+++ b/lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadNearQuery.java
@@ -140,7 +140,7 @@ public class TestPayloadNearQuery extends LuceneTestCase {
 
     query = newPhraseQuery("field", "twenty two", true, new AveragePayloadFunction());
     QueryUtils.check(query);
-		
+
     // all 10 hits should have score = 3 because adjacent terms have payloads of 2,4
     // and all the similarity factors are set to 1
     hits = searcher.search(query, null, 100);
@@ -162,8 +162,8 @@ public class TestPayloadNearQuery extends LuceneTestCase {
       assertEquals("should be 100 hits", 100, hits.totalHits);
       for (int j = 0; j < hits.scoreDocs.length; j++) {
         ScoreDoc doc = hits.scoreDocs[j];
-        //				System.out.println("Doc: " + doc.toString());
-        //				System.out.println("Explain: " + searcher.explain(query, doc.doc));
+        //        System.out.println("Doc: " + doc.toString());
+        //        System.out.println("Explain: " + searcher.explain(query, doc.doc));
         assertTrue(doc.score + " does not equal: " + 3, doc.score == 3);
       }
     }
@@ -192,71 +192,71 @@ public class TestPayloadNearQuery extends LuceneTestCase {
   }
   
   public void testAverageFunction() throws IOException {
-	  PayloadNearQuery query;
-	  TopDocs hits;
+    PayloadNearQuery query;
+    TopDocs hits;
 
-	  query = newPhraseQuery("field", "twenty two", true, new AveragePayloadFunction());
-	  QueryUtils.check(query);
-	  // all 10 hits should have score = 3 because adjacent terms have payloads of 2,4
-	  // and all the similarity factors are set to 1
-	  hits = searcher.search(query, null, 100);
-	  assertTrue("hits is null and it shouldn't be", hits != null);
-	  assertTrue("should be 10 hits", hits.totalHits == 10);
-	  for (int j = 0; j < hits.scoreDocs.length; j++) {
-		  ScoreDoc doc = hits.scoreDocs[j];
-		  assertTrue(doc.score + " does not equal: " + 3, doc.score == 3);
-		  Explanation explain = searcher.explain(query, hits.scoreDocs[j].doc);
-		  String exp = explain.toString();
-		  assertTrue(exp, exp.indexOf("AveragePayloadFunction") > -1);
-		  assertTrue(hits.scoreDocs[j].score + " explain value does not equal: " + 3, explain.getValue() == 3f);
-	  }
+    query = newPhraseQuery("field", "twenty two", true, new AveragePayloadFunction());
+    QueryUtils.check(query);
+    // all 10 hits should have score = 3 because adjacent terms have payloads of 2,4
+    // and all the similarity factors are set to 1
+    hits = searcher.search(query, null, 100);
+    assertTrue("hits is null and it shouldn't be", hits != null);
+    assertTrue("should be 10 hits", hits.totalHits == 10);
+    for (int j = 0; j < hits.scoreDocs.length; j++) {
+      ScoreDoc doc = hits.scoreDocs[j];
+      assertTrue(doc.score + " does not equal: " + 3, doc.score == 3);
+      Explanation explain = searcher.explain(query, hits.scoreDocs[j].doc);
+      String exp = explain.toString();
+      assertTrue(exp, exp.indexOf("AveragePayloadFunction") > -1);
+      assertTrue(hits.scoreDocs[j].score + " explain value does not equal: " + 3, explain.getValue() == 3f);
+    }
   }
   public void testMaxFunction() throws IOException {
-	  PayloadNearQuery query;
-	  TopDocs hits;
+    PayloadNearQuery query;
+    TopDocs hits;
 
-	  query = newPhraseQuery("field", "twenty two", true, new MaxPayloadFunction());
-	  QueryUtils.check(query);
-	  // all 10 hits should have score = 4 (max payload value)
-	  hits = searcher.search(query, null, 100);
-	  assertTrue("hits is null and it shouldn't be", hits != null);
-	  assertTrue("should be 10 hits", hits.totalHits == 10);
-	  for (int j = 0; j < hits.scoreDocs.length; j++) {
-		  ScoreDoc doc = hits.scoreDocs[j];
-		  assertTrue(doc.score + " does not equal: " + 4, doc.score == 4);
-		  Explanation explain = searcher.explain(query, hits.scoreDocs[j].doc);
-		  String exp = explain.toString();
-		  assertTrue(exp, exp.indexOf("MaxPayloadFunction") > -1);
-		  assertTrue(hits.scoreDocs[j].score + " explain value does not equal: " + 4, explain.getValue() == 4f);
-	  }
+    query = newPhraseQuery("field", "twenty two", true, new MaxPayloadFunction());
+    QueryUtils.check(query);
+    // all 10 hits should have score = 4 (max payload value)
+    hits = searcher.search(query, null, 100);
+    assertTrue("hits is null and it shouldn't be", hits != null);
+    assertTrue("should be 10 hits", hits.totalHits == 10);
+    for (int j = 0; j < hits.scoreDocs.length; j++) {
+      ScoreDoc doc = hits.scoreDocs[j];
+      assertTrue(doc.score + " does not equal: " + 4, doc.score == 4);
+      Explanation explain = searcher.explain(query, hits.scoreDocs[j].doc);
+      String exp = explain.toString();
+      assertTrue(exp, exp.indexOf("MaxPayloadFunction") > -1);
+      assertTrue(hits.scoreDocs[j].score + " explain value does not equal: " + 4, explain.getValue() == 4f);
+    }
   }  
   public void testMinFunction() throws IOException {
-	  PayloadNearQuery query;
-	  TopDocs hits;
+    PayloadNearQuery query;
+    TopDocs hits;
 
-	  query = newPhraseQuery("field", "twenty two", true, new MinPayloadFunction());
-	  QueryUtils.check(query);
-	  // all 10 hits should have score = 2 (min payload value)
-	  hits = searcher.search(query, null, 100);
-	  assertTrue("hits is null and it shouldn't be", hits != null);
-	  assertTrue("should be 10 hits", hits.totalHits == 10);
-	  for (int j = 0; j < hits.scoreDocs.length; j++) {
-		  ScoreDoc doc = hits.scoreDocs[j];
-		  assertTrue(doc.score + " does not equal: " + 2, doc.score == 2);
-		  Explanation explain = searcher.explain(query, hits.scoreDocs[j].doc);
-		  String exp = explain.toString();
-		  assertTrue(exp, exp.indexOf("MinPayloadFunction") > -1);
-		  assertTrue(hits.scoreDocs[j].score + " explain value does not equal: " + 2, explain.getValue() == 2f);
-	  }
+    query = newPhraseQuery("field", "twenty two", true, new MinPayloadFunction());
+    QueryUtils.check(query);
+    // all 10 hits should have score = 2 (min payload value)
+    hits = searcher.search(query, null, 100);
+    assertTrue("hits is null and it shouldn't be", hits != null);
+    assertTrue("should be 10 hits", hits.totalHits == 10);
+    for (int j = 0; j < hits.scoreDocs.length; j++) {
+      ScoreDoc doc = hits.scoreDocs[j];
+      assertTrue(doc.score + " does not equal: " + 2, doc.score == 2);
+      Explanation explain = searcher.explain(query, hits.scoreDocs[j].doc);
+      String exp = explain.toString();
+      assertTrue(exp, exp.indexOf("MinPayloadFunction") > -1);
+      assertTrue(hits.scoreDocs[j].score + " explain value does not equal: " + 2, explain.getValue() == 2f);
+    }
   }  
   private SpanQuery[] getClauses() {
-	    SpanNearQuery q1, q2;
-	    q1 = spanNearQuery("field2", "twenty two");
-	    q2 = spanNearQuery("field2", "twenty three");
-	    SpanQuery[] clauses = new SpanQuery[2];
-	    clauses[0] = q1;
-	    clauses[1] = q2;
-	    return clauses;
+      SpanNearQuery q1, q2;
+      q1 = spanNearQuery("field2", "twenty two");
+      q2 = spanNearQuery("field2", "twenty three");
+      SpanQuery[] clauses = new SpanQuery[2];
+      clauses[0] = q1;
+      clauses[1] = q2;
+      return clauses;
   }
   private SpanNearQuery spanNearQuery(String fieldName, String words) {
     String[] wordList = words.split("[\\s]+");
@@ -274,8 +274,8 @@ public class TestPayloadNearQuery extends LuceneTestCase {
     hits = searcher.search(query, null, 100);
     assertTrue("hits is null and it shouldn't be", hits != null);
     ScoreDoc doc = hits.scoreDocs[0];
-    //		System.out.println("Doc: " + doc.toString());
-    //		System.out.println("Explain: " + searcher.explain(query, doc.doc));
+    //    System.out.println("Doc: " + doc.toString());
+    //    System.out.println("Explain: " + searcher.explain(query, doc.doc));
     assertTrue("there should only be one hit", hits.totalHits == 1);
     // should have score = 3 because adjacent terms have payloads of 2,4
     assertTrue(doc.score + " does not equal: " + 3, doc.score == 3); 
@@ -299,8 +299,8 @@ public class TestPayloadNearQuery extends LuceneTestCase {
     assertTrue("should only be one hit", hits.scoreDocs.length == 1);
     // the score should be 3 - the average of all the underlying payloads
     ScoreDoc doc = hits.scoreDocs[0];
-    //		System.out.println("Doc: " + doc.toString());
-    //		System.out.println("Explain: " + searcher.explain(query, doc.doc));
+    //    System.out.println("Doc: " + doc.toString());
+    //    System.out.println("Explain: " + searcher.explain(query, doc.doc));
     assertTrue(doc.score + " does not equal: " + 3, doc.score == 3);  
   }
 
diff --git a/lucene/core/src/test/org/apache/lucene/search/spans/TestBasics.java b/lucene/core/src/test/org/apache/lucene/search/spans/TestBasics.java
index 1de3ca3..5ea9370 100644
--- a/lucene/core/src/test/org/apache/lucene/search/spans/TestBasics.java
+++ b/lucene/core/src/test/org/apache/lucene/search/spans/TestBasics.java
@@ -582,21 +582,21 @@ public class TestBasics extends LuceneTestCase {
   
   @Test
   public void testSpansSkipTo() throws Exception {
-	  SpanTermQuery t1 = new SpanTermQuery(new Term("field", "seventy"));
-	  SpanTermQuery t2 = new SpanTermQuery(new Term("field", "seventy"));
-	  Spans s1 = MultiSpansWrapper.wrap(searcher.getTopReaderContext(), t1);
-	  Spans s2 = MultiSpansWrapper.wrap(searcher.getTopReaderContext(), t2);
-	  
-	  assertTrue(s1.next());
-	  assertTrue(s2.next());
-	  
-	  boolean hasMore = true;
-	  
-	  do {
-		  hasMore = skipToAccoringToJavaDocs(s1, s1.doc());
-		  assertEquals(hasMore, s2.skipTo(s2.doc()));
-		  assertEquals(s1.doc(), s2.doc());
-	  } while (hasMore);
+    SpanTermQuery t1 = new SpanTermQuery(new Term("field", "seventy"));
+    SpanTermQuery t2 = new SpanTermQuery(new Term("field", "seventy"));
+    Spans s1 = MultiSpansWrapper.wrap(searcher.getTopReaderContext(), t1);
+    Spans s2 = MultiSpansWrapper.wrap(searcher.getTopReaderContext(), t2);
+
+    assertTrue(s1.next());
+    assertTrue(s2.next());
+
+    boolean hasMore = true;
+
+    do {
+      hasMore = skipToAccoringToJavaDocs(s1, s1.doc());
+      assertEquals(hasMore, s2.skipTo(s2.doc()));
+      assertEquals(s1.doc(), s2.doc());
+    } while (hasMore);
   }
 
   /** Skips to the first match beyond the current, whose document number is
diff --git a/lucene/core/src/test/org/apache/lucene/store/TestWindowsMMap.java b/lucene/core/src/test/org/apache/lucene/store/TestWindowsMMap.java
index 394a9a2..953bfac 100644
--- a/lucene/core/src/test/org/apache/lucene/store/TestWindowsMMap.java
+++ b/lucene/core/src/test/org/apache/lucene/store/TestWindowsMMap.java
@@ -84,7 +84,7 @@ public class TestWindowsMMap extends LuceneTestCase {
     for(int dx = 0; dx < num; dx ++) {
       String f = randomField();
       Document doc = new Document();
-      doc.add(newTextField("data", f, Field.Store.YES));	
+      doc.add(newTextField("data", f, Field.Store.YES));  
       writer.addDocument(doc);
     }
     
diff --git a/lucene/core/src/test/org/apache/lucene/util/TestBitUtil.java b/lucene/core/src/test/org/apache/lucene/util/TestBitUtil.java
index 41f579b..b4c610d 100644
--- a/lucene/core/src/test/org/apache/lucene/util/TestBitUtil.java
+++ b/lucene/core/src/test/org/apache/lucene/util/TestBitUtil.java
@@ -71,12 +71,12 @@ public class TestBitUtil extends LuceneTestCase {
     long sumRes = 0;
     while (iters-- >= 0) {
       for (int i = 1; i <= 63; i++) {
-      	long a = testArg(i);
-	sumRes += BitUtil.nlz(a);
-	sumRes += BitUtil.nlz(a+1);
-	sumRes += BitUtil.nlz(a-1);
-	sumRes += BitUtil.nlz(a+10);
-	sumRes += BitUtil.nlz(a-10);
+        long a = testArg(i);
+        sumRes += BitUtil.nlz(a);
+        sumRes += BitUtil.nlz(a + 1);
+        sumRes += BitUtil.nlz(a - 1);
+        sumRes += BitUtil.nlz(a + 10);
+        sumRes += BitUtil.nlz(a - 10);
       }
     }
     return sumRes;
@@ -86,12 +86,12 @@ public class TestBitUtil extends LuceneTestCase {
     long sumRes = 0;
     while (iters-- >= 0) {
       for (int i = 1; i <= 63; i++) {
-      	long a = testArg(i);
-	sumRes += Long.numberOfLeadingZeros(a);
-	sumRes += Long.numberOfLeadingZeros(a+1);
-	sumRes += Long.numberOfLeadingZeros(a-1);
-	sumRes += Long.numberOfLeadingZeros(a+10);
-	sumRes += Long.numberOfLeadingZeros(a-10);
+        long a = testArg(i);
+        sumRes += Long.numberOfLeadingZeros(a);
+        sumRes += Long.numberOfLeadingZeros(a + 1);
+        sumRes += Long.numberOfLeadingZeros(a - 1);
+        sumRes += Long.numberOfLeadingZeros(a + 10);
+        sumRes += Long.numberOfLeadingZeros(a - 10);
       }
     }
     return sumRes;
diff --git a/lucene/core/src/test/org/apache/lucene/util/TestFixedBitSet.java b/lucene/core/src/test/org/apache/lucene/util/TestFixedBitSet.java
index dc41cbf..f4350cf 100644
--- a/lucene/core/src/test/org/apache/lucene/util/TestFixedBitSet.java
+++ b/lucene/core/src/test/org/apache/lucene/util/TestFixedBitSet.java
@@ -49,7 +49,7 @@ public class TestFixedBitSet extends LuceneTestCase {
       // aa = a.prevSetBit(aa-1);
       aa--;
       while ((aa >= 0) && (! a.get(aa))) {
-      	aa--;
+        aa--;
       }
       if (b.length() == 0) {
         bb = -1;
diff --git a/lucene/core/src/test/org/apache/lucene/util/TestOpenBitSet.java b/lucene/core/src/test/org/apache/lucene/util/TestOpenBitSet.java
index 010c857..459bb61 100644
--- a/lucene/core/src/test/org/apache/lucene/util/TestOpenBitSet.java
+++ b/lucene/core/src/test/org/apache/lucene/util/TestOpenBitSet.java
@@ -71,7 +71,7 @@ public class TestOpenBitSet extends LuceneTestCase {
       // aa = a.prevSetBit(aa-1);
       aa--;
       while ((aa >= 0) && (! a.get(aa))) {
-      	aa--;
+        aa--;
       }
       bb = b.prevSetBit(bb-1);
       assertEquals(aa,bb);
@@ -85,7 +85,7 @@ public class TestOpenBitSet extends LuceneTestCase {
       // aa = a.prevSetBit(aa-1);
       aa--;
       while ((aa >= 0) && (! a.get(aa))) {
-      	aa--;
+        aa--;
       }
       bb = (int) b.prevSetBit((long) (bb-1));
       assertEquals(aa,bb);
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader.java
index 2dbf787..dacff89 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader.java
@@ -356,7 +356,7 @@ public class DirectoryTaxonomyReader implements TaxonomyReader {
     // only possible writer, and it is "synchronized" to avoid this case).
     DirectoryReader r2 = DirectoryReader.openIfChanged(indexReader);
     if (r2 == null) {
-    	return false; // no changes, nothing to do
+      return false; // no changes, nothing to do
     } 
     
     // validate that a refresh is valid at this point, i.e. that the taxonomy 
@@ -364,13 +364,13 @@ public class DirectoryTaxonomyReader implements TaxonomyReader {
     String t1 = indexReader.getIndexCommit().getUserData().get(DirectoryTaxonomyWriter.INDEX_CREATE_TIME);
     String t2 = r2.getIndexCommit().getUserData().get(DirectoryTaxonomyWriter.INDEX_CREATE_TIME);
     if (t1==null) {
-    	if (t2!=null) {
-    		r2.close();
-    		throw new InconsistentTaxonomyException("Taxonomy was recreated at: "+t2);
-    	}
+      if (t2!=null) {
+        r2.close();
+        throw new InconsistentTaxonomyException("Taxonomy was recreated at: "+t2);
+      }
     } else if (!t1.equals(t2)) {
-    	r2.close();
-    	throw new InconsistentTaxonomyException("Taxonomy was recreated at: "+t2+"  !=  "+t1);
+      r2.close();
+      throw new InconsistentTaxonomyException("Taxonomy was recreated at: "+t2+"  !=  "+t1);
     }
     
       IndexReader oldreader = indexReader;
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/DefaultEncoder.java b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/DefaultEncoder.java
index 4a4572b..0b10125 100644
--- a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/DefaultEncoder.java
+++ b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/DefaultEncoder.java
@@ -21,12 +21,12 @@ package org.apache.lucene.search.highlight;
  */
 public class DefaultEncoder implements Encoder
 {
-	public DefaultEncoder()
-	{
-	}
+  public DefaultEncoder()
+  {
+  }
 
-	public String encodeText(String originalText)
-	{
-		return originalText;
-	}
+  public String encodeText(String originalText)
+  {
+    return originalText;
+  }
 }
\ No newline at end of file
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/Encoder.java b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/Encoder.java
index 35b4231..be56f6c 100644
--- a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/Encoder.java
+++ b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/Encoder.java
@@ -22,8 +22,8 @@ package org.apache.lucene.search.highlight;
  */
 public interface Encoder
 {
-	/**
-	 * @param originalText The section of text being output
-	 */
-	String encodeText(String originalText);
+  /**
+   * @param originalText The section of text being output
+   */
+  String encodeText(String originalText);
 }
\ No newline at end of file
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/Formatter.java b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/Formatter.java
index 1aceed9..d75663bb 100644
--- a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/Formatter.java
+++ b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/Formatter.java
@@ -24,10 +24,10 @@ package org.apache.lucene.search.highlight;
  */
 public interface Formatter
 {
-	/**
-	 * @param originalText The section of text being considered for markup
-	 * @param tokenGroup contains one or several overlapping Tokens along with
-	 * their scores and positions.
-	 */
-	String highlightTerm(String originalText, TokenGroup tokenGroup);
+  /**
+   * @param originalText The section of text being considered for markup
+   * @param tokenGroup contains one or several overlapping Tokens along with
+   * their scores and positions.
+   */
+  String highlightTerm(String originalText, TokenGroup tokenGroup);
 }
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/GradientFormatter.java b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/GradientFormatter.java
index 58f022a..6c79aae 100644
--- a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/GradientFormatter.java
+++ b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/GradientFormatter.java
@@ -42,7 +42,7 @@ public class GradientFormatter implements Formatter
      * 
      * @param maxScore
      *            The score (and above) displayed as maxColor (See QueryScorer.getMaxWeight 
-     * 			  which can be used to calibrate scoring scale)
+     *         which can be used to calibrate scoring scale)
      * @param minForegroundColor
      *            The hex color used for representing IDF scores of zero eg
      *            #FFFFFF (white) or null if no foreground color required
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter.java b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter.java
index 07cebe6..6e75224 100644
--- a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter.java
+++ b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter.java
@@ -38,445 +38,445 @@ public class Highlighter
   public static final int DEFAULT_MAX_CHARS_TO_ANALYZE = 50*1024;
 
   private int maxDocCharsToAnalyze = DEFAULT_MAX_CHARS_TO_ANALYZE;
-	private Formatter formatter;
-	private Encoder encoder;
-	private Fragmenter textFragmenter=new SimpleFragmenter();
-	private Scorer fragmentScorer=null;
-
-	public Highlighter(Scorer fragmentScorer)
-	{
-		this(new SimpleHTMLFormatter(),fragmentScorer);
-	}
-
-
- 	public Highlighter(Formatter formatter, Scorer fragmentScorer)
- 	{
-		this(formatter,new DefaultEncoder(),fragmentScorer);
-	}
-
-
-	public Highlighter(Formatter formatter, Encoder encoder, Scorer fragmentScorer)
-	{
- 		this.formatter = formatter;
-		this.encoder = encoder;
- 		this.fragmentScorer = fragmentScorer;
- 	}
-
-	/**
-	 * Highlights chosen terms in a text, extracting the most relevant section.
-	 * This is a convenience method that calls
-	 * {@link #getBestFragment(TokenStream, String)}
-	 *
-	 * @param analyzer   the analyzer that will be used to split <code>text</code>
-	 * into chunks
-	 * @param text text to highlight terms in
-	 * @param fieldName Name of field used to influence analyzer's tokenization policy
-	 *
-	 * @return highlighted text fragment or null if no terms found
-	 * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length
-	 */
-	public final String getBestFragment(Analyzer analyzer, String fieldName,String text)
-		throws IOException, InvalidTokenOffsetsException
-	{
-		TokenStream tokenStream = analyzer.tokenStream(fieldName, new StringReader(text));
-		return getBestFragment(tokenStream, text);
-	}
-
-	/**
-	 * Highlights chosen terms in a text, extracting the most relevant section.
-	 * The document text is analysed in chunks to record hit statistics
-	 * across the document. After accumulating stats, the fragment with the highest score
-	 * is returned
-	 *
-	 * @param tokenStream   a stream of tokens identified in the text parameter, including offset information.
-	 * This is typically produced by an analyzer re-parsing a document's
-	 * text. Some work may be done on retrieving TokenStreams more efficiently
-	 * by adding support for storing original text position data in the Lucene
-	 * index but this support is not currently available (as of Lucene 1.4 rc2).
-	 * @param text text to highlight terms in
-	 *
-	 * @return highlighted text fragment or null if no terms found
-	 * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length
-	 */
-	public final String getBestFragment(TokenStream tokenStream, String text)
-		throws IOException, InvalidTokenOffsetsException
-	{
-		String[] results = getBestFragments(tokenStream,text, 1);
-		if (results.length > 0)
-		{
-			return results[0];
-		}
-		return null;
-	}
-
-	/**
-	 * Highlights chosen terms in a text, extracting the most relevant sections.
-	 * This is a convenience method that calls
-	 * {@link #getBestFragments(TokenStream, String, int)}
-	 *
-	 * @param analyzer   the analyzer that will be used to split <code>text</code>
-	 * into chunks
-	 * @param fieldName     the name of the field being highlighted (used by analyzer)
-	 * @param text        	text to highlight terms in
-	 * @param maxNumFragments  the maximum number of fragments.
-	 *
-	 * @return highlighted text fragments (between 0 and maxNumFragments number of fragments)
-	 * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length
-	 */
-	public final String[] getBestFragments(
-		Analyzer analyzer,
-		String fieldName,
-		String text,
-		int maxNumFragments)
-		throws IOException, InvalidTokenOffsetsException
-	{
-		TokenStream tokenStream = analyzer.tokenStream(fieldName, new StringReader(text));
-		return getBestFragments(tokenStream, text, maxNumFragments);
-	}
-
-	/**
-	 * Highlights chosen terms in a text, extracting the most relevant sections.
-	 * The document text is analysed in chunks to record hit statistics
-	 * across the document. After accumulating stats, the fragments with the highest scores
-	 * are returned as an array of strings in order of score (contiguous fragments are merged into
-	 * one in their original order to improve readability)
-	 *
-	 * @param text        	text to highlight terms in
-	 * @param maxNumFragments  the maximum number of fragments.
-	 *
-	 * @return highlighted text fragments (between 0 and maxNumFragments number of fragments)
-	 * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length
-	 */
-	public final String[] getBestFragments(
-		TokenStream tokenStream,
-		String text,
-		int maxNumFragments)
-		throws IOException, InvalidTokenOffsetsException
-	{
-		maxNumFragments = Math.max(1, maxNumFragments); //sanity check
-
-		TextFragment[] frag =getBestTextFragments(tokenStream,text, true,maxNumFragments);
-
-		//Get text
-		ArrayList<String> fragTexts = new ArrayList<String>();
-		for (int i = 0; i < frag.length; i++)
-		{
-			if ((frag[i] != null) && (frag[i].getScore() > 0))
-			{
-				fragTexts.add(frag[i].toString());
-			}
-		}
-		return fragTexts.toArray(new String[0]);
-	}
-
-
-	/**
-	 * Low level api to get the most relevant (formatted) sections of the document.
-	 * This method has been made public to allow visibility of score information held in TextFragment objects.
-	 * Thanks to Jason Calabrese for help in redefining the interface.
-	 * @param tokenStream
-	 * @param text
-	 * @param maxNumFragments
-	 * @param mergeContiguousFragments
-	 * @throws IOException
-	 * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length
-	 */
-	public final TextFragment[] getBestTextFragments(
-		TokenStream tokenStream,
-		String text,
-		boolean mergeContiguousFragments,
-		int maxNumFragments)
-		throws IOException, InvalidTokenOffsetsException
-	{
-		ArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();
-		StringBuilder newText=new StringBuilder();
-		
-	    CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);
-	    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);
-	    tokenStream.addAttribute(PositionIncrementAttribute.class);
-	    tokenStream.reset();
-	    
-		TextFragment currentFrag =	new TextFragment(newText,newText.length(), docFrags.size());
-		
+  private Formatter formatter;
+  private Encoder encoder;
+  private Fragmenter textFragmenter=new SimpleFragmenter();
+  private Scorer fragmentScorer=null;
+
+  public Highlighter(Scorer fragmentScorer)
+  {
+    this(new SimpleHTMLFormatter(),fragmentScorer);
+  }
+
+
+   public Highlighter(Formatter formatter, Scorer fragmentScorer)
+   {
+    this(formatter,new DefaultEncoder(),fragmentScorer);
+  }
+
+
+  public Highlighter(Formatter formatter, Encoder encoder, Scorer fragmentScorer)
+  {
+     this.formatter = formatter;
+    this.encoder = encoder;
+     this.fragmentScorer = fragmentScorer;
+   }
+
+  /**
+   * Highlights chosen terms in a text, extracting the most relevant section.
+   * This is a convenience method that calls
+   * {@link #getBestFragment(TokenStream, String)}
+   *
+   * @param analyzer   the analyzer that will be used to split <code>text</code>
+   * into chunks
+   * @param text text to highlight terms in
+   * @param fieldName Name of field used to influence analyzer's tokenization policy
+   *
+   * @return highlighted text fragment or null if no terms found
+   * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length
+   */
+  public final String getBestFragment(Analyzer analyzer, String fieldName,String text)
+    throws IOException, InvalidTokenOffsetsException
+  {
+    TokenStream tokenStream = analyzer.tokenStream(fieldName, new StringReader(text));
+    return getBestFragment(tokenStream, text);
+  }
+
+  /**
+   * Highlights chosen terms in a text, extracting the most relevant section.
+   * The document text is analysed in chunks to record hit statistics
+   * across the document. After accumulating stats, the fragment with the highest score
+   * is returned
+   *
+   * @param tokenStream   a stream of tokens identified in the text parameter, including offset information.
+   * This is typically produced by an analyzer re-parsing a document's
+   * text. Some work may be done on retrieving TokenStreams more efficiently
+   * by adding support for storing original text position data in the Lucene
+   * index but this support is not currently available (as of Lucene 1.4 rc2).
+   * @param text text to highlight terms in
+   *
+   * @return highlighted text fragment or null if no terms found
+   * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length
+   */
+  public final String getBestFragment(TokenStream tokenStream, String text)
+    throws IOException, InvalidTokenOffsetsException
+  {
+    String[] results = getBestFragments(tokenStream,text, 1);
+    if (results.length > 0)
+    {
+      return results[0];
+    }
+    return null;
+  }
+
+  /**
+   * Highlights chosen terms in a text, extracting the most relevant sections.
+   * This is a convenience method that calls
+   * {@link #getBestFragments(TokenStream, String, int)}
+   *
+   * @param analyzer   the analyzer that will be used to split <code>text</code>
+   * into chunks
+   * @param fieldName     the name of the field being highlighted (used by analyzer)
+   * @param text          text to highlight terms in
+   * @param maxNumFragments  the maximum number of fragments.
+   *
+   * @return highlighted text fragments (between 0 and maxNumFragments number of fragments)
+   * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length
+   */
+  public final String[] getBestFragments(
+    Analyzer analyzer,
+    String fieldName,
+    String text,
+    int maxNumFragments)
+    throws IOException, InvalidTokenOffsetsException
+  {
+    TokenStream tokenStream = analyzer.tokenStream(fieldName, new StringReader(text));
+    return getBestFragments(tokenStream, text, maxNumFragments);
+  }
+
+  /**
+   * Highlights chosen terms in a text, extracting the most relevant sections.
+   * The document text is analysed in chunks to record hit statistics
+   * across the document. After accumulating stats, the fragments with the highest scores
+   * are returned as an array of strings in order of score (contiguous fragments are merged into
+   * one in their original order to improve readability)
+   *
+   * @param text          text to highlight terms in
+   * @param maxNumFragments  the maximum number of fragments.
+   *
+   * @return highlighted text fragments (between 0 and maxNumFragments number of fragments)
+   * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length
+   */
+  public final String[] getBestFragments(
+    TokenStream tokenStream,
+    String text,
+    int maxNumFragments)
+    throws IOException, InvalidTokenOffsetsException
+  {
+    maxNumFragments = Math.max(1, maxNumFragments); //sanity check
+
+    TextFragment[] frag =getBestTextFragments(tokenStream,text, true,maxNumFragments);
+
+    //Get text
+    ArrayList<String> fragTexts = new ArrayList<String>();
+    for (int i = 0; i < frag.length; i++)
+    {
+      if ((frag[i] != null) && (frag[i].getScore() > 0))
+      {
+        fragTexts.add(frag[i].toString());
+      }
+    }
+    return fragTexts.toArray(new String[0]);
+  }
+
+
+  /**
+   * Low level api to get the most relevant (formatted) sections of the document.
+   * This method has been made public to allow visibility of score information held in TextFragment objects.
+   * Thanks to Jason Calabrese for help in redefining the interface.
+   * @param tokenStream
+   * @param text
+   * @param maxNumFragments
+   * @param mergeContiguousFragments
+   * @throws IOException
+   * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length
+   */
+  public final TextFragment[] getBestTextFragments(
+    TokenStream tokenStream,
+    String text,
+    boolean mergeContiguousFragments,
+    int maxNumFragments)
+    throws IOException, InvalidTokenOffsetsException
+  {
+    ArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();
+    StringBuilder newText=new StringBuilder();
+
+      CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);
+      OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);
+      tokenStream.addAttribute(PositionIncrementAttribute.class);
+      tokenStream.reset();
+
+    TextFragment currentFrag =  new TextFragment(newText,newText.length(), docFrags.size());
+
     if (fragmentScorer instanceof QueryScorer) {
       ((QueryScorer) fragmentScorer).setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);
     }
     
-		TokenStream newStream = fragmentScorer.init(tokenStream);
-		if(newStream != null) {
-		  tokenStream = newStream;
-		}
-		fragmentScorer.startFragment(currentFrag);
-		docFrags.add(currentFrag);
-
-		FragmentQueue fragQueue = new FragmentQueue(maxNumFragments);
-
-		try
-		{
-
-			String tokenText;
-			int startOffset;
-			int endOffset;
-			int lastEndOffset = 0;
-			textFragmenter.start(text, tokenStream);
-
-			TokenGroup tokenGroup=new TokenGroup(tokenStream);
-
-			for (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);
-			      next = tokenStream.incrementToken())
-			{
-				if(	(offsetAtt.endOffset()>text.length())
-					||
-					(offsetAtt.startOffset()>text.length())
-					)						
-				{
-					throw new InvalidTokenOffsetsException("Token "+ termAtt.toString()
-							+" exceeds length of provided text sized "+text.length());
-				}
-				if((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))
-				{
-					//the current token is distinct from previous tokens -
-					// markup the cached token group info
-					startOffset = tokenGroup.matchStartOffset;
-					endOffset = tokenGroup.matchEndOffset;
-					tokenText = text.substring(startOffset, endOffset);
-					String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);
-					//store any whitespace etc from between this and last group
-					if (startOffset > lastEndOffset)
-						newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));
-					newText.append(markedUpText);
-					lastEndOffset=Math.max(endOffset, lastEndOffset);
-					tokenGroup.clear();
-
-					//check if current token marks the start of a new fragment
-					if(textFragmenter.isNewFragment())
-					{
-						currentFrag.setScore(fragmentScorer.getFragmentScore());
-						//record stats for a new fragment
-						currentFrag.textEndPos = newText.length();
-						currentFrag =new TextFragment(newText, newText.length(), docFrags.size());
-						fragmentScorer.startFragment(currentFrag);
-						docFrags.add(currentFrag);
-					}
-				}
-
-				tokenGroup.addToken(fragmentScorer.getTokenScore());
-
-//				if(lastEndOffset>maxDocBytesToAnalyze)
-//				{
-//					break;
-//				}
-			}
-			currentFrag.setScore(fragmentScorer.getFragmentScore());
-
-			if(tokenGroup.numTokens>0)
-			{
-				//flush the accumulated text (same code as in above loop)
-				startOffset = tokenGroup.matchStartOffset;
-				endOffset = tokenGroup.matchEndOffset;
-				tokenText = text.substring(startOffset, endOffset);
-				String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);
-				//store any whitespace etc from between this and last group
-				if (startOffset > lastEndOffset)
-					newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));
-				newText.append(markedUpText);
-				lastEndOffset=Math.max(lastEndOffset,endOffset);
-			}
-
-			//Test what remains of the original text beyond the point where we stopped analyzing 
-			if (
-//					if there is text beyond the last token considered..
-					(lastEndOffset < text.length()) 
-					&&
-//					and that text is not too large...
-					(text.length()<= maxDocCharsToAnalyze)
-				)				
-			{
-				//append it to the last fragment
-				newText.append(encoder.encodeText(text.substring(lastEndOffset)));
-			}
-
-			currentFrag.textEndPos = newText.length();
-
-			//sort the most relevant sections of the text
-			for (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)
-			{
-				currentFrag = i.next();
-
-				//If you are running with a version of Lucene before 11th Sept 03
-				// you do not have PriorityQueue.insert() - so uncomment the code below
-				/*
-									if (currentFrag.getScore() >= minScore)
-									{
-										fragQueue.put(currentFrag);
-										if (fragQueue.size() > maxNumFragments)
-										{ // if hit queue overfull
-											fragQueue.pop(); // remove lowest in hit queue
-											minScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore
-										}
-
-
-									}
-				*/
-				//The above code caused a problem as a result of Christoph Goller's 11th Sept 03
-				//fix to PriorityQueue. The correct method to use here is the new "insert" method
-				// USE ABOVE CODE IF THIS DOES NOT COMPILE!
-				fragQueue.insertWithOverflow(currentFrag);
-			}
-
-			//return the most relevant fragments
-			TextFragment frag[] = new TextFragment[fragQueue.size()];
-			for (int i = frag.length - 1; i >= 0; i--)
-			{
-				frag[i] = fragQueue.pop();
-			}
-
-			//merge any contiguous fragments to improve readability
-			if(mergeContiguousFragments)
-			{
-				mergeContiguousFragments(frag);
-				ArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();
-				for (int i = 0; i < frag.length; i++)
-				{
-					if ((frag[i] != null) && (frag[i].getScore() > 0))
-					{
-						fragTexts.add(frag[i]);
-					}
-				}
-				frag= fragTexts.toArray(new TextFragment[0]);
-			}
-
-			return frag;
-
-		}
-		finally
-		{
-			if (tokenStream != null)
-			{
-				try
-				{
-				  tokenStream.end();
-					tokenStream.close();
-				}
-				catch (Exception e)
-				{
-				}
-			}
-		}
-	}
-
-
-	/** Improves readability of a score-sorted list of TextFragments by merging any fragments
-	 * that were contiguous in the original text into one larger fragment with the correct order.
-	 * This will leave a "null" in the array entry for the lesser scored fragment. 
-	 * 
-	 * @param frag An array of document fragments in descending score
-	 */
-	private void mergeContiguousFragments(TextFragment[] frag)
-	{
-		boolean mergingStillBeingDone;
-		if (frag.length > 1)
-			do
-			{
-				mergingStillBeingDone = false; //initialise loop control flag
-				//for each fragment, scan other frags looking for contiguous blocks
-				for (int i = 0; i < frag.length; i++)
-				{
-					if (frag[i] == null)
-					{
-						continue;
-					}
-					//merge any contiguous blocks 
-					for (int x = 0; x < frag.length; x++)
-					{
-						if (frag[x] == null)
-						{
-							continue;
-						}
-						if (frag[i] == null)
-						{
-							break;
-						}
-						TextFragment frag1 = null;
-						TextFragment frag2 = null;
-						int frag1Num = 0;
-						int frag2Num = 0;
-						int bestScoringFragNum;
-						int worstScoringFragNum;
-						//if blocks are contiguous....
-						if (frag[i].follows(frag[x]))
-						{
-							frag1 = frag[x];
-							frag1Num = x;
-							frag2 = frag[i];
-							frag2Num = i;
-						}
-						else
-							if (frag[x].follows(frag[i]))
-							{
-								frag1 = frag[i];
-								frag1Num = i;
-								frag2 = frag[x];
-								frag2Num = x;
-							}
-						//merging required..
-						if (frag1 != null)
-						{
-							if (frag1.getScore() > frag2.getScore())
-							{
-								bestScoringFragNum = frag1Num;
-								worstScoringFragNum = frag2Num;
-							}
-							else
-							{
-								bestScoringFragNum = frag2Num;
-								worstScoringFragNum = frag1Num;
-							}
-							frag1.merge(frag2);
-							frag[worstScoringFragNum] = null;
-							mergingStillBeingDone = true;
-							frag[bestScoringFragNum] = frag1;
-						}
-					}
-				}
-			}
-			while (mergingStillBeingDone);
-	}
-	
-	
-	/**
-	 * Highlights terms in the  text , extracting the most relevant sections
-	 * and concatenating the chosen fragments with a separator (typically "...").
-	 * The document text is analysed in chunks to record hit statistics
-	 * across the document. After accumulating stats, the fragments with the highest scores
-	 * are returned in order as "separator" delimited strings.
-	 *
-	 * @param text        text to highlight terms in
-	 * @param maxNumFragments  the maximum number of fragments.
-	 * @param separator  the separator used to intersperse the document fragments (typically "...")
-	 *
-	 * @return highlighted text
-	 * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length
-	 */
-	public final String getBestFragments(
-		TokenStream tokenStream,	
-		String text,
-		int maxNumFragments,
-		String separator)
-		throws IOException, InvalidTokenOffsetsException
-	{
-		String sections[] =	getBestFragments(tokenStream,text, maxNumFragments);
-		StringBuilder result = new StringBuilder();
-		for (int i = 0; i < sections.length; i++)
-		{
-			if (i > 0)
-			{
-				result.append(separator);
-			}
-			result.append(sections[i]);
-		}
-		return result.toString();
-	}
+    TokenStream newStream = fragmentScorer.init(tokenStream);
+    if(newStream != null) {
+      tokenStream = newStream;
+    }
+    fragmentScorer.startFragment(currentFrag);
+    docFrags.add(currentFrag);
+
+    FragmentQueue fragQueue = new FragmentQueue(maxNumFragments);
+
+    try
+    {
+
+      String tokenText;
+      int startOffset;
+      int endOffset;
+      int lastEndOffset = 0;
+      textFragmenter.start(text, tokenStream);
+
+      TokenGroup tokenGroup=new TokenGroup(tokenStream);
+
+      for (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);
+            next = tokenStream.incrementToken())
+      {
+        if(  (offsetAtt.endOffset()>text.length())
+          ||
+          (offsetAtt.startOffset()>text.length())
+          )
+        {
+          throw new InvalidTokenOffsetsException("Token "+ termAtt.toString()
+              +" exceeds length of provided text sized "+text.length());
+        }
+        if((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))
+        {
+          //the current token is distinct from previous tokens -
+          // markup the cached token group info
+          startOffset = tokenGroup.matchStartOffset;
+          endOffset = tokenGroup.matchEndOffset;
+          tokenText = text.substring(startOffset, endOffset);
+          String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);
+          //store any whitespace etc from between this and last group
+          if (startOffset > lastEndOffset)
+            newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));
+          newText.append(markedUpText);
+          lastEndOffset=Math.max(endOffset, lastEndOffset);
+          tokenGroup.clear();
+
+          //check if current token marks the start of a new fragment
+          if(textFragmenter.isNewFragment())
+          {
+            currentFrag.setScore(fragmentScorer.getFragmentScore());
+            //record stats for a new fragment
+            currentFrag.textEndPos = newText.length();
+            currentFrag =new TextFragment(newText, newText.length(), docFrags.size());
+            fragmentScorer.startFragment(currentFrag);
+            docFrags.add(currentFrag);
+          }
+        }
+
+        tokenGroup.addToken(fragmentScorer.getTokenScore());
+
+//        if(lastEndOffset>maxDocBytesToAnalyze)
+//        {
+//          break;
+//        }
+      }
+      currentFrag.setScore(fragmentScorer.getFragmentScore());
+
+      if(tokenGroup.numTokens>0)
+      {
+        //flush the accumulated text (same code as in above loop)
+        startOffset = tokenGroup.matchStartOffset;
+        endOffset = tokenGroup.matchEndOffset;
+        tokenText = text.substring(startOffset, endOffset);
+        String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);
+        //store any whitespace etc from between this and last group
+        if (startOffset > lastEndOffset)
+          newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));
+        newText.append(markedUpText);
+        lastEndOffset=Math.max(lastEndOffset,endOffset);
+      }
+
+      //Test what remains of the original text beyond the point where we stopped analyzing
+      if (
+//          if there is text beyond the last token considered..
+          (lastEndOffset < text.length())
+          &&
+//          and that text is not too large...
+          (text.length()<= maxDocCharsToAnalyze)
+        )
+      {
+        //append it to the last fragment
+        newText.append(encoder.encodeText(text.substring(lastEndOffset)));
+      }
+
+      currentFrag.textEndPos = newText.length();
+
+      //sort the most relevant sections of the text
+      for (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)
+      {
+        currentFrag = i.next();
+
+        //If you are running with a version of Lucene before 11th Sept 03
+        // you do not have PriorityQueue.insert() - so uncomment the code below
+        /*
+                  if (currentFrag.getScore() >= minScore)
+                  {
+                    fragQueue.put(currentFrag);
+                    if (fragQueue.size() > maxNumFragments)
+                    { // if hit queue overfull
+                      fragQueue.pop(); // remove lowest in hit queue
+                      minScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore
+                    }
+
+
+                  }
+        */
+        //The above code caused a problem as a result of Christoph Goller's 11th Sept 03
+        //fix to PriorityQueue. The correct method to use here is the new "insert" method
+        // USE ABOVE CODE IF THIS DOES NOT COMPILE!
+        fragQueue.insertWithOverflow(currentFrag);
+      }
+
+      //return the most relevant fragments
+      TextFragment frag[] = new TextFragment[fragQueue.size()];
+      for (int i = frag.length - 1; i >= 0; i--)
+      {
+        frag[i] = fragQueue.pop();
+      }
+
+      //merge any contiguous fragments to improve readability
+      if(mergeContiguousFragments)
+      {
+        mergeContiguousFragments(frag);
+        ArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();
+        for (int i = 0; i < frag.length; i++)
+        {
+          if ((frag[i] != null) && (frag[i].getScore() > 0))
+          {
+            fragTexts.add(frag[i]);
+          }
+        }
+        frag= fragTexts.toArray(new TextFragment[0]);
+      }
+
+      return frag;
+
+    }
+    finally
+    {
+      if (tokenStream != null)
+      {
+        try
+        {
+          tokenStream.end();
+          tokenStream.close();
+        }
+        catch (Exception e)
+        {
+        }
+      }
+    }
+  }
+
+
+  /** Improves readability of a score-sorted list of TextFragments by merging any fragments
+   * that were contiguous in the original text into one larger fragment with the correct order.
+   * This will leave a "null" in the array entry for the lesser scored fragment.
+   *
+   * @param frag An array of document fragments in descending score
+   */
+  private void mergeContiguousFragments(TextFragment[] frag)
+  {
+    boolean mergingStillBeingDone;
+    if (frag.length > 1)
+      do
+      {
+        mergingStillBeingDone = false; //initialise loop control flag
+        //for each fragment, scan other frags looking for contiguous blocks
+        for (int i = 0; i < frag.length; i++)
+        {
+          if (frag[i] == null)
+          {
+            continue;
+          }
+          //merge any contiguous blocks
+          for (int x = 0; x < frag.length; x++)
+          {
+            if (frag[x] == null)
+            {
+              continue;
+            }
+            if (frag[i] == null)
+            {
+              break;
+            }
+            TextFragment frag1 = null;
+            TextFragment frag2 = null;
+            int frag1Num = 0;
+            int frag2Num = 0;
+            int bestScoringFragNum;
+            int worstScoringFragNum;
+            //if blocks are contiguous....
+            if (frag[i].follows(frag[x]))
+            {
+              frag1 = frag[x];
+              frag1Num = x;
+              frag2 = frag[i];
+              frag2Num = i;
+            }
+            else
+              if (frag[x].follows(frag[i]))
+              {
+                frag1 = frag[i];
+                frag1Num = i;
+                frag2 = frag[x];
+                frag2Num = x;
+              }
+            //merging required..
+            if (frag1 != null)
+            {
+              if (frag1.getScore() > frag2.getScore())
+              {
+                bestScoringFragNum = frag1Num;
+                worstScoringFragNum = frag2Num;
+              }
+              else
+              {
+                bestScoringFragNum = frag2Num;
+                worstScoringFragNum = frag1Num;
+              }
+              frag1.merge(frag2);
+              frag[worstScoringFragNum] = null;
+              mergingStillBeingDone = true;
+              frag[bestScoringFragNum] = frag1;
+            }
+          }
+        }
+      }
+      while (mergingStillBeingDone);
+  }
+
+
+  /**
+   * Highlights terms in the  text , extracting the most relevant sections
+   * and concatenating the chosen fragments with a separator (typically "...").
+   * The document text is analysed in chunks to record hit statistics
+   * across the document. After accumulating stats, the fragments with the highest scores
+   * are returned in order as "separator" delimited strings.
+   *
+   * @param text        text to highlight terms in
+   * @param maxNumFragments  the maximum number of fragments.
+   * @param separator  the separator used to intersperse the document fragments (typically "...")
+   *
+   * @return highlighted text
+   * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length
+   */
+  public final String getBestFragments(
+    TokenStream tokenStream,
+    String text,
+    int maxNumFragments,
+    String separator)
+    throws IOException, InvalidTokenOffsetsException
+  {
+    String sections[] =  getBestFragments(tokenStream,text, maxNumFragments);
+    StringBuilder result = new StringBuilder();
+    for (int i = 0; i < sections.length; i++)
+    {
+      if (i > 0)
+      {
+        result.append(separator);
+      }
+      result.append(sections[i]);
+    }
+    return result.toString();
+  }
 
   public int getMaxDocCharsToAnalyze() {
     return maxDocCharsToAnalyze;
@@ -487,35 +487,35 @@ public class Highlighter
   }
 
   
-	public Fragmenter getTextFragmenter()
-	{
-		return textFragmenter;
-	}
-
-	/**
-	 * @param fragmenter
-	 */
-	public void setTextFragmenter(Fragmenter fragmenter)
-	{
-		textFragmenter = fragmenter;
-	}
-
-	/**
-	 * @return Object used to score each text fragment 
-	 */
-	public Scorer getFragmentScorer()
-	{
-		return fragmentScorer;
-	}
-
-
-	/**
-	 * @param scorer
-	 */
-	public void setFragmentScorer(Scorer scorer)
-	{
-		fragmentScorer = scorer;
-	}
+  public Fragmenter getTextFragmenter()
+  {
+    return textFragmenter;
+  }
+
+  /**
+   * @param fragmenter
+   */
+  public void setTextFragmenter(Fragmenter fragmenter)
+  {
+    textFragmenter = fragmenter;
+  }
+
+  /**
+   * @return Object used to score each text fragment
+   */
+  public Scorer getFragmentScorer()
+  {
+    return fragmentScorer;
+  }
+
+
+  /**
+   * @param scorer
+   */
+  public void setFragmentScorer(Scorer scorer)
+  {
+    fragmentScorer = scorer;
+  }
 
     public Encoder getEncoder()
     {
@@ -528,17 +528,17 @@ public class Highlighter
 }
 class FragmentQueue extends PriorityQueue<TextFragment>
 {
-	public FragmentQueue(int size)
-	{
-		super(size);
-	}
-
-	@Override
-	public final boolean lessThan(TextFragment fragA, TextFragment fragB)
-	{
-		if (fragA.getScore() == fragB.getScore())
-			return fragA.fragNum > fragB.fragNum;
-		else
-			return fragA.getScore() < fragB.getScore();
-	}
+  public FragmentQueue(int size)
+  {
+    super(size);
+  }
+
+  @Override
+  public final boolean lessThan(TextFragment fragA, TextFragment fragB)
+  {
+    if (fragA.getScore() == fragB.getScore())
+      return fragA.fragNum > fragB.fragNum;
+    else
+      return fragA.getScore() < fragB.getScore();
+  }
 }
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/InvalidTokenOffsetsException.java b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/InvalidTokenOffsetsException.java
index c159ccc..633b4f4 100644
--- a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/InvalidTokenOffsetsException.java
+++ b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/InvalidTokenOffsetsException.java
@@ -23,9 +23,9 @@ package org.apache.lucene.search.highlight;
 public class InvalidTokenOffsetsException extends Exception
 {
 
-	public InvalidTokenOffsetsException(String message)
-	{
-		super(message);
-	}
+  public InvalidTokenOffsetsException(String message)
+  {
+    super(message);
+  }
 
 }
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/QueryTermExtractor.java b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/QueryTermExtractor.java
index 147209f..2fe6aa5 100644
--- a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/QueryTermExtractor.java
+++ b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/QueryTermExtractor.java
@@ -37,126 +37,118 @@ import org.apache.lucene.search.Query;
 public final class QueryTermExtractor
 {
 
-	/**
-	 * Extracts all terms texts of a given Query into an array of WeightedTerms
-	 *
-	 * @param query      Query to extract term texts from
-	 * @return an array of the terms used in a query, plus their weights.
-	 */
-	public static final WeightedTerm[] getTerms(Query query) 
-	{
-		return getTerms(query,false);
-	}
+  /**
+   * Extracts all terms texts of a given Query into an array of WeightedTerms
+   *
+   * @param query      Query to extract term texts from
+   * @return an array of the terms used in a query, plus their weights.
+   */
+  public static final WeightedTerm[] getTerms(Query query)
+  {
+    return getTerms(query,false);
+  }
 
-	/**
-	 * Extracts all terms texts of a given Query into an array of WeightedTerms
-	 *
-	 * @param query      Query to extract term texts from
-	 * @param reader used to compute IDF which can be used to a) score selected fragments better 
-	 * b) use graded highlights eg changing intensity of font color
-	 * @param fieldName the field on which Inverse Document Frequency (IDF) calculations are based
-	 * @return an array of the terms used in a query, plus their weights.
-	 */
-	public static final WeightedTerm[] getIdfWeightedTerms(Query query, IndexReader reader, String fieldName) 
-	{
-	    WeightedTerm[] terms=getTerms(query,false, fieldName);
-	    int totalNumDocs=reader.maxDoc();
-	    for (int i = 0; i < terms.length; i++)
+  /**
+   * Extracts all terms texts of a given Query into an array of WeightedTerms
+   *
+   * @param query      Query to extract term texts from
+   * @param reader used to compute IDF which can be used to a) score selected fragments better
+   * b) use graded highlights eg changing intensity of font color
+   * @param fieldName the field on which Inverse Document Frequency (IDF) calculations are based
+   * @return an array of the terms used in a query, plus their weights.
+   */
+  public static final WeightedTerm[] getIdfWeightedTerms(Query query, IndexReader reader, String fieldName)
+  {
+      WeightedTerm[] terms=getTerms(query,false, fieldName);
+      int totalNumDocs=reader.maxDoc();
+      for (int i = 0; i < terms.length; i++)
         {
-	        try
+          try
             {
                 int docFreq=reader.docFreq(new Term(fieldName,terms[i].term));
                 //IDF algorithm taken from DefaultSimilarity class
                 float idf=(float)(Math.log(totalNumDocs/(double)(docFreq+1)) + 1.0);
                 terms[i].weight*=idf;
             } 
-	        catch (IOException e)
+          catch (IOException e)
             {
-	            //ignore 
+              //ignore
             }
         }
-		return terms;
-	}
+    return terms;
+  }
 
-	/**
-	 * Extracts all terms texts of a given Query into an array of WeightedTerms
-	 *
-	 * @param query      Query to extract term texts from
-	 * @param prohibited <code>true</code> to extract "prohibited" terms, too
-	 * @param fieldName  The fieldName used to filter query terms
+  /**
+   * Extracts all terms texts of a given Query into an array of WeightedTerms
+   *
+   * @param query      Query to extract term texts from
+   * @param prohibited <code>true</code> to extract "prohibited" terms, too
+   * @param fieldName  The fieldName used to filter query terms
    * @return an array of the terms used in a query, plus their weights.
    */
-	public static final WeightedTerm[] getTerms(Query query, boolean prohibited, String fieldName) 
-	{
-		HashSet<WeightedTerm> terms=new HashSet<WeightedTerm>();
-		getTerms(query,terms,prohibited,fieldName);
-		return terms.toArray(new WeightedTerm[0]);
-	}
-	
-	/**
-	 * Extracts all terms texts of a given Query into an array of WeightedTerms
-	 *
-	 * @param query      Query to extract term texts from
-	 * @param prohibited <code>true</code> to extract "prohibited" terms, too
+  public static final WeightedTerm[] getTerms(Query query, boolean prohibited, String fieldName)
+  {
+    HashSet<WeightedTerm> terms=new HashSet<WeightedTerm>();
+    getTerms(query,terms,prohibited,fieldName);
+    return terms.toArray(new WeightedTerm[0]);
+  }
+
+  /**
+   * Extracts all terms texts of a given Query into an array of WeightedTerms
+   *
+   * @param query      Query to extract term texts from
+   * @param prohibited <code>true</code> to extract "prohibited" terms, too
    * @return an array of the terms used in a query, plus their weights.
    */
-	public static final WeightedTerm[] getTerms(Query query, boolean prohibited) 
-	{
-	    return getTerms(query,prohibited,null);
-	}	
+  public static final WeightedTerm[] getTerms(Query query, boolean prohibited)
+  {
+      return getTerms(query,prohibited,null);
+  }
+
+  private static final void getTerms(Query query, HashSet<WeightedTerm> terms, boolean prohibited, String fieldName) {
+    try {
+      if (query instanceof BooleanQuery)
+        getTermsFromBooleanQuery((BooleanQuery) query, terms, prohibited, fieldName);
+      else if (query instanceof FilteredQuery)
+        getTermsFromFilteredQuery((FilteredQuery) query, terms, prohibited, fieldName);
+      else {
+        HashSet<Term> nonWeightedTerms = new HashSet<Term>();
+        query.extractTerms(nonWeightedTerms);
+        for (Iterator<Term> iter = nonWeightedTerms.iterator(); iter.hasNext(); ) {
+          Term term = iter.next();
+          if ((fieldName == null) || (term.field().equals(fieldName))) {
+            terms.add(new WeightedTerm(query.getBoost(), term.text()));
+          }
+        }
+      }
+    } catch (UnsupportedOperationException ignore) {
+      //this is non-fatal for our purposes
+    }
+  }
 
-	private static final void getTerms(Query query, HashSet<WeightedTerm> terms,boolean prohibited, String fieldName) 
-	{
-       	try
-       	{
-    		if (query instanceof BooleanQuery)
-    			getTermsFromBooleanQuery((BooleanQuery) query, terms, prohibited, fieldName);
-    		else
-    			if(query instanceof FilteredQuery)
-    				getTermsFromFilteredQuery((FilteredQuery)query, terms,prohibited, fieldName);
-    			else
-    		{
-	       		HashSet<Term> nonWeightedTerms=new HashSet<Term>();
-	       		query.extractTerms(nonWeightedTerms);
-	       		for (Iterator<Term> iter = nonWeightedTerms.iterator(); iter.hasNext();)
-				{
-					Term term = iter.next();
-                                        if((fieldName==null)||(term.field().equals(fieldName)))
-					{
-						terms.add(new WeightedTerm(query.getBoost(),term.text()));
-					}
-				}
-    		}
-	      }
-	      catch(UnsupportedOperationException ignore)
-	      {
-	    	  //this is non-fatal for our purposes
-       	  }		        			        	
-	}
+  /**
+   * extractTerms is currently the only query-independent means of introspecting queries but it only reveals
+   * a list of terms for that query - not the boosts each individual term in that query may or may not have.
+   * "Container" queries such as BooleanQuery should be unwrapped to get at the boost info held
+   * in each child element.
+   * Some discussion around this topic here:
+   * http://www.gossamer-threads.com/lists/lucene/java-dev/34208?search_string=introspection;#34208
+   * Unfortunately there seemed to be limited interest in requiring all Query objects to implement
+   * something common which would allow access to child queries so what follows here are query-specific
+   * implementations for accessing embedded query elements.
+   */
+  private static final void getTermsFromBooleanQuery(BooleanQuery query, HashSet<WeightedTerm> terms, boolean prohibited, String fieldName)
+  {
+    BooleanClause[] queryClauses = query.getClauses();
+    for (int i = 0; i < queryClauses.length; i++)
+    {
+      if (prohibited || queryClauses[i].getOccur()!=BooleanClause.Occur.MUST_NOT)
+        getTerms(queryClauses[i].getQuery(), terms, prohibited, fieldName);
+    }
+  }
+  private static void getTermsFromFilteredQuery(FilteredQuery query, HashSet<WeightedTerm> terms, boolean prohibited, String fieldName)
+  {
+    getTerms(query.getQuery(),terms,prohibited,fieldName);
+  }
 
-	/**
-	 * extractTerms is currently the only query-independent means of introspecting queries but it only reveals
-	 * a list of terms for that query - not the boosts each individual term in that query may or may not have.
-	 * "Container" queries such as BooleanQuery should be unwrapped to get at the boost info held
-	 * in each child element. 
-	 * Some discussion around this topic here:
-	 * http://www.gossamer-threads.com/lists/lucene/java-dev/34208?search_string=introspection;#34208
-	 * Unfortunately there seemed to be limited interest in requiring all Query objects to implement
-	 * something common which would allow access to child queries so what follows here are query-specific
-	 * implementations for accessing embedded query elements. 
-	 */
-	private static final void getTermsFromBooleanQuery(BooleanQuery query, HashSet<WeightedTerm> terms, boolean prohibited, String fieldName)
-	{
-		BooleanClause[] queryClauses = query.getClauses();
-		for (int i = 0; i < queryClauses.length; i++)
-		{
-			if (prohibited || queryClauses[i].getOccur()!=BooleanClause.Occur.MUST_NOT)
-				getTerms(queryClauses[i].getQuery(), terms, prohibited, fieldName);
-		}
-	}	
-	private static void getTermsFromFilteredQuery(FilteredQuery query, HashSet<WeightedTerm> terms, boolean prohibited, String fieldName)
-	{
-		getTerms(query.getQuery(),terms,prohibited,fieldName);		
-	}
-	
 }
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/SimpleHTMLEncoder.java b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/SimpleHTMLEncoder.java
index 30bf38b..8e95c86 100644
--- a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/SimpleHTMLEncoder.java
+++ b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/SimpleHTMLEncoder.java
@@ -21,61 +21,61 @@ package org.apache.lucene.search.highlight;
  */
 public class SimpleHTMLEncoder implements Encoder
 {
-	public SimpleHTMLEncoder()
-	{
-	}
+  public SimpleHTMLEncoder()
+  {
+  }
 
-	public String encodeText(String originalText)
-	{
-		return htmlEncode(originalText);
-	}
-	
-	/**
-	 * Encode string into HTML
-	 */
-	public final static String htmlEncode(String plainText) 
-	{
-		if (plainText == null || plainText.length() == 0)
-		{
-			return "";
-		}
+  public String encodeText(String originalText)
+  {
+    return htmlEncode(originalText);
+  }
 
-		StringBuilder result = new StringBuilder(plainText.length());
+  /**
+   * Encode string into HTML
+   */
+  public final static String htmlEncode(String plainText)
+  {
+    if (plainText == null || plainText.length() == 0)
+    {
+      return "";
+    }
 
-		for (int index=0; index<plainText.length(); index++) 
-		{
-			char ch = plainText.charAt(index);
+    StringBuilder result = new StringBuilder(plainText.length());
 
-			switch (ch) 
-			{
-			case '"':
-				result.append("&quot;");
-				break;
+    for (int index=0; index<plainText.length(); index++)
+    {
+      char ch = plainText.charAt(index);
 
-			case '&':
-				result.append("&amp;");
-				break;
+      switch (ch)
+      {
+      case '"':
+        result.append("&quot;");
+        break;
 
-			case '<':
-				result.append("&lt;");
-				break;
+      case '&':
+        result.append("&amp;");
+        break;
 
-			case '>':
-				result.append("&gt;");
-				break;
+      case '<':
+        result.append("&lt;");
+        break;
 
-			default:
-				   if (ch < 128) 
-				   {
-			           result.append(ch);
-			       } 
-				   else 
-			       {
-			           result.append("&#").append((int)ch).append(";");
-			       }
-			}
-		}
+      case '>':
+        result.append("&gt;");
+        break;
 
-		return result.toString();
-	}
+      default:
+           if (ch < 128)
+           {
+                 result.append(ch);
+             }
+           else
+             {
+                 result.append("&#").append((int)ch).append(";");
+             }
+      }
+    }
+
+    return result.toString();
+  }
 }
\ No newline at end of file
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/SimpleHTMLFormatter.java b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/SimpleHTMLFormatter.java
index 2afebe8..b741de3 100644
--- a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/SimpleHTMLFormatter.java
+++ b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/SimpleHTMLFormatter.java
@@ -26,34 +26,34 @@ public class SimpleHTMLFormatter implements Formatter {
   private static final String DEFAULT_PRE_TAG = "<B>";
   private static final String DEFAULT_POST_TAG = "</B>";
   
-	private String preTag;
-	private String postTag;
-	
-	public SimpleHTMLFormatter(String preTag, String postTag) {
-		this.preTag = preTag;
-		this.postTag = postTag;
-	}
+  private String preTag;
+  private String postTag;
 
-	/** Default constructor uses HTML: &lt;B&gt; tags to markup terms. */
-	public SimpleHTMLFormatter() {
-	  this(DEFAULT_PRE_TAG, DEFAULT_POST_TAG);
-	}
+  public SimpleHTMLFormatter(String preTag, String postTag) {
+    this.preTag = preTag;
+    this.postTag = postTag;
+  }
 
-	/* (non-Javadoc)
-	 * @see org.apache.lucene.search.highlight.Formatter#highlightTerm(java.lang.String, org.apache.lucene.search.highlight.TokenGroup)
-	 */
-	public String highlightTerm(String originalText, TokenGroup tokenGroup) {
-	  if (tokenGroup.getTotalScore() <= 0) {
-	    return originalText;
-	  }
-	  
-	  // Allocate StringBuilder with the right number of characters from the
+  /** Default constructor uses HTML: &lt;B&gt; tags to markup terms. */
+  public SimpleHTMLFormatter() {
+    this(DEFAULT_PRE_TAG, DEFAULT_POST_TAG);
+  }
+
+  /* (non-Javadoc)
+   * @see org.apache.lucene.search.highlight.Formatter#highlightTerm(java.lang.String, org.apache.lucene.search.highlight.TokenGroup)
+   */
+  public String highlightTerm(String originalText, TokenGroup tokenGroup) {
+    if (tokenGroup.getTotalScore() <= 0) {
+      return originalText;
+    }
+
+    // Allocate StringBuilder with the right number of characters from the
     // beginning, to avoid char[] allocations in the middle of appends.
-	  StringBuilder returnBuffer = new StringBuilder(preTag.length() + originalText.length() + postTag.length());
-	  returnBuffer.append(preTag);
-	  returnBuffer.append(originalText);
-	  returnBuffer.append(postTag);
-	  return returnBuffer.toString();
-	}
-	
+    StringBuilder returnBuffer = new StringBuilder(preTag.length() + originalText.length() + postTag.length());
+    returnBuffer.append(preTag);
+    returnBuffer.append(originalText);
+    returnBuffer.append(postTag);
+    return returnBuffer.toString();
+  }
+
 }
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/SpanGradientFormatter.java b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/SpanGradientFormatter.java
index 49a92a3..5e6df1e 100755
--- a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/SpanGradientFormatter.java
+++ b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/SpanGradientFormatter.java
@@ -22,57 +22,49 @@ package org.apache.lucene.search.highlight;
  * doesn't work in Mozilla, thus this class.
  *
  * @see GradientFormatter
- *
  */
 
 public class SpanGradientFormatter
-	extends GradientFormatter
-{
-	public SpanGradientFormatter(float maxScore, String minForegroundColor,
-            String maxForegroundColor, String minBackgroundColor,
-            String maxBackgroundColor)
-    {
-		super( maxScore, minForegroundColor,
-			   maxForegroundColor, minBackgroundColor,
-			   maxBackgroundColor);
-	}
-	
+    extends GradientFormatter {
+  public SpanGradientFormatter(float maxScore, String minForegroundColor,
+                               String maxForegroundColor, String minBackgroundColor,
+                               String maxBackgroundColor) {
+    super(maxScore, minForegroundColor,
+        maxForegroundColor, minBackgroundColor,
+        maxBackgroundColor);
+  }
+
 
-	
-	@Override
-	public String highlightTerm(String originalText, TokenGroup tokenGroup)
-    {
-        if (tokenGroup.getTotalScore() == 0)
-            return originalText;
-        float score = tokenGroup.getTotalScore();
-        if (score == 0)
-        {
-            return originalText;
-        }
+  @Override
+  public String highlightTerm(String originalText, TokenGroup tokenGroup) {
+    if (tokenGroup.getTotalScore() == 0)
+      return originalText;
+    float score = tokenGroup.getTotalScore();
+    if (score == 0) {
+      return originalText;
+    }
 
-		// try to size sb correctly
-        StringBuilder sb = new StringBuilder( originalText.length() + EXTRA);
-		
-		sb.append("<span style=\""); 
-		if (highlightForeground) 
-		{
-			sb.append("color: "); 
-			sb.append(getForegroundColorString(score)); 
-			sb.append("; "); 
-		}
-		if (highlightBackground)
-		{
-			sb.append("background: ");
-			sb.append(getBackgroundColorString(score));
-			sb.append("; ");
-		}
-		sb.append("\">");
-		sb.append(originalText);
-		sb.append("</span>");
-        return sb.toString();
+    // try to size sb correctly
+    StringBuilder sb = new StringBuilder(originalText.length() + EXTRA);
+
+    sb.append("<span style=\"");
+    if (highlightForeground) {
+      sb.append("color: ");
+      sb.append(getForegroundColorString(score));
+      sb.append("; ");
+    }
+    if (highlightBackground) {
+      sb.append("background: ");
+      sb.append(getBackgroundColorString(score));
+      sb.append("; ");
     }
+    sb.append("\">");
+    sb.append(originalText);
+    sb.append("</span>");
+    return sb.toString();
+  }
 
-	// guess how much extra text we'll add to the text we're highlighting to try to avoid a  StringBuilder resize
-	private static final String TEMPLATE = "<span style=\"background: #EEEEEE; color: #000000;\">...</span>";
-	private static final int EXTRA = TEMPLATE.length();	
+  // guess how much extra text we'll add to the text we're highlighting to try to avoid a  StringBuilder resize
+  private static final String TEMPLATE = "<span style=\"background: #EEEEEE; color: #000000;\">...</span>";
+  private static final int EXTRA = TEMPLATE.length();
 }
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/TextFragment.java b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/TextFragment.java
index 2b31a42..dd56f21 100644
--- a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/TextFragment.java
+++ b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/TextFragment.java
@@ -25,57 +25,57 @@ package org.apache.lucene.search.highlight;
  */
 public class TextFragment
 {
-	CharSequence markedUpText;
-	int fragNum;
-	int textStartPos;
-	int textEndPos;
-	float score;
+  CharSequence markedUpText;
+  int fragNum;
+  int textStartPos;
+  int textEndPos;
+  float score;
 
-	public TextFragment(CharSequence markedUpText,int textStartPos, int fragNum)
-	{
-		this.markedUpText=markedUpText;
-		this.textStartPos = textStartPos;
-		this.fragNum = fragNum;
-	}
+  public TextFragment(CharSequence markedUpText,int textStartPos, int fragNum)
+  {
+    this.markedUpText=markedUpText;
+    this.textStartPos = textStartPos;
+    this.fragNum = fragNum;
+  }
 
-	void setScore(float score)
-	{
-		this.score=score;
-	}
-	public float getScore()
-	{
-		return score;
-	}
-	/**
-	 * @param frag2 Fragment to be merged into this one
-	 */
+  void setScore(float score)
+  {
+    this.score=score;
+  }
+  public float getScore()
+  {
+    return score;
+  }
+  /**
+   * @param frag2 Fragment to be merged into this one
+   */
   public void merge(TextFragment frag2)
   {
     textEndPos = frag2.textEndPos;
     score=Math.max(score,frag2.score);
   }
   /**
-	 * @param fragment 
-	 * @return true if this fragment follows the one passed
-	 */
-	public boolean follows(TextFragment fragment)
-	{
-		return textStartPos == fragment.textEndPos;
-	}
+   * @param fragment
+   * @return true if this fragment follows the one passed
+   */
+  public boolean follows(TextFragment fragment)
+  {
+    return textStartPos == fragment.textEndPos;
+  }
 
-	/**
-	 * @return the fragment sequence number
-	 */
-	public int getFragNum()
-	{
-		return fragNum;
-	}
+  /**
+   * @return the fragment sequence number
+   */
+  public int getFragNum()
+  {
+    return fragNum;
+  }
 
-	/* Returns the marked-up text for this text fragment 
-	 */
-	@Override
-	public String toString() {
-		return markedUpText.subSequence(textStartPos, textEndPos).toString();
-	}
+  /* Returns the marked-up text for this text fragment
+   */
+  @Override
+  public String toString() {
+    return markedUpText.subSequence(textStartPos, textEndPos).toString();
+  }
 
 }
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/WeightedTerm.java b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/WeightedTerm.java
index 9d25aa7..68a7a2b 100644
--- a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/WeightedTerm.java
+++ b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/WeightedTerm.java
@@ -20,45 +20,45 @@ package org.apache.lucene.search.highlight;
  */
 public class WeightedTerm
 {
-	float weight; // multiplier
-	String term; //stemmed form
-	public WeightedTerm (float weight,String term)
-	{
-		this.weight=weight;
-		this.term=term;
-	}
-	
-	
-	/**
-	 * @return the term value (stemmed)
-	 */
-	public String getTerm()
-	{
-		return term;
-	}
+  float weight; // multiplier
+  String term; //stemmed form
+  public WeightedTerm (float weight,String term)
+  {
+    this.weight=weight;
+    this.term=term;
+  }
 
-	/**
-	 * @return the weight associated with this term
-	 */
-	public float getWeight()
-	{
-		return weight;
-	}
 
-	/**
-	 * @param term the term value (stemmed)
-	 */
-	public void setTerm(String term)
-	{
-		this.term = term;
-	}
+  /**
+   * @return the term value (stemmed)
+   */
+  public String getTerm()
+  {
+    return term;
+  }
 
-	/**
-	 * @param weight the weight associated with this term
-	 */
-	public void setWeight(float weight)
-	{
-		this.weight = weight;
-	}
+  /**
+   * @return the weight associated with this term
+   */
+  public float getWeight()
+  {
+    return weight;
+  }
+
+  /**
+   * @param term the term value (stemmed)
+   */
+  public void setTerm(String term)
+  {
+    this.term = term;
+  }
+
+  /**
+   * @param weight the weight associated with this term
+   */
+  public void setWeight(float weight)
+  {
+    this.weight = weight;
+  }
 
 }
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DualFloatFunction.java b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DualFloatFunction.java
index cace63c..b384918 100755
--- a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DualFloatFunction.java
+++ b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DualFloatFunction.java
@@ -54,11 +54,12 @@ public abstract class DualFloatFunction extends ValueSource {
     return new FloatDocValues(this) {
       @Override
       public float floatVal(int doc) {
-	return func(doc, aVals, bVals);
+        return func(doc, aVals, bVals);
       }
+
       @Override
       public String toString(int doc) {
-	return name() + '(' + aVals.toString(doc) + ',' + bVals.toString(doc) + ')';
+        return name() + '(' + aVals.toString(doc) + ',' + bVals.toString(doc) + ')';
       }
     };
   }
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiBoolFunction.java b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiBoolFunction.java
index 2c89302..28b6cd2 100644
--- a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiBoolFunction.java
+++ b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiBoolFunction.java
@@ -50,7 +50,7 @@ public abstract class MultiBoolFunction extends BoolFunction {
     return new BoolDocValues(this) {
       @Override
       public boolean boolVal(int doc) {
-	      return func(doc, vals);
+        return func(doc, vals);
       }
 
       @Override
@@ -105,4 +105,4 @@ public abstract class MultiBoolFunction extends BoolFunction {
       source.createWeight(context, searcher);
     }
   }
-}
\ No newline at end of file
+}
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ScaleFloatFunction.java b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ScaleFloatFunction.java
index 3b247a3..388f3a2 100755
--- a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ScaleFloatFunction.java
+++ b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ScaleFloatFunction.java
@@ -115,11 +115,11 @@ public class ScaleFloatFunction extends ValueSource {
     return new FloatDocValues(this) {
       @Override
       public float floatVal(int doc) {
-	return (vals.floatVal(doc) - minSource) * scale + min;
+        return (vals.floatVal(doc) - minSource) * scale + min;
       }
       @Override
       public String toString(int doc) {
-	return "scale(" + vals.toString(doc) + ",toMin=" + min + ",toMax=" + max
+        return "scale(" + vals.toString(doc) + ",toMin=" + min + ",toMax=" + max
                 + ",fromMin=" + minSource
                 + ",fromMax=" + maxSource
                 + ")";
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleBoolFunction.java b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleBoolFunction.java
index f4a7f74..fe4757a 100644
--- a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleBoolFunction.java
+++ b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleBoolFunction.java
@@ -44,11 +44,11 @@ public abstract class SimpleBoolFunction extends BoolFunction {
     return new BoolDocValues(this) {
       @Override
       public boolean boolVal(int doc) {
-	      return func(doc, vals);
+        return func(doc, vals);
       }
       @Override
       public String toString(int doc) {
-	      return name() + '(' + vals.toString(doc) + ')';
+        return name() + '(' + vals.toString(doc) + ')';
       }
     };
   }
@@ -74,4 +74,4 @@ public abstract class SimpleBoolFunction extends BoolFunction {
   public void createWeight(Map context, IndexSearcher searcher) throws IOException {
     source.createWeight(context, searcher);
   }
-}
\ No newline at end of file
+}
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleFloatFunction.java b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleFloatFunction.java
index 824ac55..54dd606 100755
--- a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleFloatFunction.java
+++ b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleFloatFunction.java
@@ -40,11 +40,11 @@ import java.util.Map;
     return new FloatDocValues(this) {
       @Override
       public float floatVal(int doc) {
-	return func(doc, vals);
+        return func(doc, vals);
       }
       @Override
       public String toString(int doc) {
-	return name() + '(' + vals.toString(doc) + ')';
+        return name() + '(' + vals.toString(doc) + ')';
       }
     };
   }
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/FastCharStream.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/FastCharStream.java
index 6eafaf2..a7c4db1 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/FastCharStream.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/FastCharStream.java
@@ -29,13 +29,13 @@ import java.io.*;
 public final class FastCharStream implements CharStream {
   char[] buffer = null;
 
-  int bufferLength = 0;				  // end of valid chars
-  int bufferPosition = 0;			  // next char to read
+  int bufferLength = 0;          // end of valid chars
+  int bufferPosition = 0;        // next char to read
 
-  int tokenStart = 0;				  // offset in buffer
-  int bufferStart = 0;				  // position in file of buffer
+  int tokenStart = 0;          // offset in buffer
+  int bufferStart = 0;          // position in file of buffer
 
-  Reader input;					  // source of chars
+  Reader input;            // source of chars
 
   /** Constructs from a Reader. */
   public FastCharStream(Reader r) {
@@ -51,24 +51,24 @@ public final class FastCharStream implements CharStream {
   private final void refill() throws IOException {
     int newPosition = bufferLength - tokenStart;
 
-    if (tokenStart == 0) {			  // token won't fit in buffer
-      if (buffer == null) {			  // first time: alloc buffer
-	buffer = new char[2048];
+    if (tokenStart == 0) {        // token won't fit in buffer
+      if (buffer == null) {        // first time: alloc buffer
+  buffer = new char[2048];
       } else if (bufferLength == buffer.length) { // grow buffer
-	char[] newBuffer = new char[buffer.length*2];
-	System.arraycopy(buffer, 0, newBuffer, 0, bufferLength);
-	buffer = newBuffer;
+  char[] newBuffer = new char[buffer.length*2];
+  System.arraycopy(buffer, 0, newBuffer, 0, bufferLength);
+  buffer = newBuffer;
       }
-    } else {					  // shift token to front
+    } else {            // shift token to front
       System.arraycopy(buffer, tokenStart, buffer, 0, newPosition);
     }
 
-    bufferLength = newPosition;			  // update state
+    bufferLength = newPosition;        // update state
     bufferPosition = newPosition;
     bufferStart += tokenStart;
     tokenStart = 0;
 
-    int charsRead =				  // fill space in buffer
+    int charsRead =          // fill space in buffer
       input.read(buffer, newPosition, buffer.length-newPosition);
     if (charsRead == -1)
       throw new IOException("read past eof");
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParser.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParser.java
index 26bcfc5..273ed8c 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParser.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParser.java
@@ -166,10 +166,10 @@ public class QueryParser extends QueryParserBase implements QueryParserConstants
 
 // This makes sure that there is no garbage after the query string
   final public Query TopLevelQuery(String field) throws ParseException {
-        Query q;
+  Query q;
     q = Query(field);
     jj_consume_token(0);
-                {if (true) return q;}
+    {if (true) return q;}
     throw new Error("Missing return statement in function");
   }
 
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParser.jj b/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParser.jj
index 593262b..6942a76 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParser.jj
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParser.jj
@@ -211,13 +211,13 @@ int Modifiers() : {
 // This makes sure that there is no garbage after the query string
 Query TopLevelQuery(String field) : 
 {
-	Query q;
+  Query q;
 }
 {
-	q=Query(field) <EOF>
-	{
-		return q;
-	}
+  q=Query(field) <EOF>
+  {
+    return q;
+  }
 }
 
 Query Query(String field) :
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/nodes/FieldQueryNode.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/nodes/FieldQueryNode.java
index 0a8b53b..3c37a04 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/nodes/FieldQueryNode.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/nodes/FieldQueryNode.java
@@ -179,12 +179,12 @@ public class FieldQueryNode extends QueryNodeImpl implements FieldValuePairQuery
 
   }
 
-	public CharSequence getValue() {
-		return getText();
-	}
+  public CharSequence getValue() {
+    return getText();
+  }
 
-	public void setValue(CharSequence value) {
-		setText(value);
-	}
+  public void setValue(CharSequence value) {
+    setText(value);
+  }
 
 }
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/FastCharStream.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/FastCharStream.java
index 3fc9e8a..95f5f3f 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/FastCharStream.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/FastCharStream.java
@@ -29,13 +29,13 @@ import java.io.*;
 public final class FastCharStream implements CharStream {
   char[] buffer = null;
 
-  int bufferLength = 0;				  // end of valid chars
-  int bufferPosition = 0;			  // next char to read
+  int bufferLength = 0;          // end of valid chars
+  int bufferPosition = 0;        // next char to read
 
-  int tokenStart = 0;				  // offset in buffer
-  int bufferStart = 0;				  // position in file of buffer
+  int tokenStart = 0;          // offset in buffer
+  int bufferStart = 0;          // position in file of buffer
 
-  Reader input;					  // source of chars
+  Reader input;            // source of chars
 
   /** Constructs from a Reader. */
   public FastCharStream(Reader r) {
@@ -51,24 +51,24 @@ public final class FastCharStream implements CharStream {
   private final void refill() throws IOException {
     int newPosition = bufferLength - tokenStart;
 
-    if (tokenStart == 0) {			  // token won't fit in buffer
-      if (buffer == null) {			  // first time: alloc buffer
-	buffer = new char[2048];
+    if (tokenStart == 0) {        // token won't fit in buffer
+      if (buffer == null) {        // first time: alloc buffer
+        buffer = new char[2048];
       } else if (bufferLength == buffer.length) { // grow buffer
-	char[] newBuffer = new char[buffer.length*2];
-	System.arraycopy(buffer, 0, newBuffer, 0, bufferLength);
-	buffer = newBuffer;
+        char[] newBuffer = new char[buffer.length * 2];
+        System.arraycopy(buffer, 0, newBuffer, 0, bufferLength);
+        buffer = newBuffer;
       }
-    } else {					  // shift token to front
+    } else {            // shift token to front
       System.arraycopy(buffer, tokenStart, buffer, 0, newPosition);
     }
 
-    bufferLength = newPosition;			  // update state
+    bufferLength = newPosition;        // update state
     bufferPosition = newPosition;
     bufferStart += tokenStart;
     tokenStart = 0;
 
-    int charsRead =				  // fill space in buffer
+    int charsRead =          // fill space in buffer
       input.read(buffer, newPosition, buffer.length-newPosition);
     if (charsRead == -1)
       throw new IOException("read past eof");
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/StandardSyntaxParser.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/StandardSyntaxParser.java
index 676d8a0..4844ec8 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/StandardSyntaxParser.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/StandardSyntaxParser.java
@@ -45,14 +45,14 @@ import org.apache.lucene.queryparser.flexible.standard.nodes.TermRangeQueryNode;
  */
 public class StandardSyntaxParser implements SyntaxParser, StandardSyntaxParserConstants {
 
-        private static final int CONJ_NONE =0;
-        private static final int CONJ_AND =2;
-        private static final int CONJ_OR =2;
+  private static final int CONJ_NONE =0;
+  private static final int CONJ_AND =2;
+  private static final int CONJ_OR =2;
 
 
    // syntax parser constructor
    public StandardSyntaxParser() {
-        this(new FastCharStream(new StringReader("")));
+     this(new FastCharStream(new StringReader("")));
   }
      /** Parses a query string, returning a {@link org.apache.lucene.queryparser.flexible.core.nodes.QueryNode}.
      *  @param query  the query string to be parsed.
@@ -143,10 +143,10 @@ public class StandardSyntaxParser implements SyntaxParser, StandardSyntaxParserC
 
 // This makes sure that there is no garbage after the query string
   final public QueryNode TopLevelQuery(CharSequence field) throws ParseException {
-        QueryNode q;
+  QueryNode q;
     q = Query(field);
     jj_consume_token(0);
-                {if (true) return q;}
+     {if (true) return q;}
     throw new Error("Missing return statement in function");
   }
 
@@ -184,23 +184,23 @@ public class StandardSyntaxParser implements SyntaxParser, StandardSyntaxParserC
         break label_1;
       }
       c = DisjQuery(field);
-             if (clauses == null) {
-                 clauses = new Vector<QueryNode>();
-                 clauses.addElement(first);
-             }
-         clauses.addElement(c);
+       if (clauses == null) {
+           clauses = new Vector<QueryNode>();
+           clauses.addElement(first);
+        }
+        clauses.addElement(c);
     }
         if (clauses != null) {
-                {if (true) return new BooleanQueryNode(clauses);}
-        } else {
-                {if (true) return first;}
-        }
+        {if (true) return new BooleanQueryNode(clauses);}
+      } else {
+          {if (true) return first;}
+      }
     throw new Error("Missing return statement in function");
   }
 
   final public QueryNode DisjQuery(CharSequence field) throws ParseException {
-        QueryNode first, c;
-        Vector<QueryNode> clauses = null;
+  QueryNode first, c;
+  Vector<QueryNode> clauses = null;
     first = ConjQuery(field);
     label_2:
     while (true) {
@@ -221,7 +221,7 @@ public class StandardSyntaxParser implements SyntaxParser, StandardSyntaxParserC
      clauses.addElement(c);
     }
     if (clauses != null) {
-            {if (true) return new OrQueryNode(clauses);}
+      {if (true) return new OrQueryNode(clauses);}
     } else {
         {if (true) return first;}
     }
@@ -229,8 +229,8 @@ public class StandardSyntaxParser implements SyntaxParser, StandardSyntaxParserC
   }
 
   final public QueryNode ConjQuery(CharSequence field) throws ParseException {
-        QueryNode first, c;
-        Vector<QueryNode> clauses = null;
+  QueryNode first, c;
+  Vector<QueryNode> clauses = null;
     first = ModClause(field);
     label_3:
     while (true) {
@@ -251,7 +251,7 @@ public class StandardSyntaxParser implements SyntaxParser, StandardSyntaxParserC
      clauses.addElement(c);
     }
     if (clauses != null) {
-            {if (true) return new AndQueryNode(clauses);}
+      {if (true) return new AndQueryNode(clauses);}
     } else {
         {if (true) return first;}
     }
@@ -272,27 +272,27 @@ public class StandardSyntaxParser implements SyntaxParser, StandardSyntaxParserC
 //     if (mods == ModifierQueryNode.Modifier.MOD_NONE) firstQuery=q;
 //     
 //     // do not create modifier nodes with MOD_NONE
-//    	if (mods != ModifierQueryNode.Modifier.MOD_NONE) {
-//    		q = new ModifierQueryNode(q, mods);
-//    	}
-//    	clauses.add(q);
+//      if (mods != ModifierQueryNode.Modifier.MOD_NONE) {
+//          q = new ModifierQueryNode(q, mods);
+//         }
+//      clauses.add(q);
 //   }
 //   (
 //     conj=Conjunction() mods=Modifiers() q=Clause(field)
 //     { 
-// 	    // do not create modifier nodes with MOD_NONE
-// 	   	if (mods != ModifierQueryNode.Modifier.MOD_NONE) {
-// 	   		q = new ModifierQueryNode(q, mods);
-// 	   	}
-// 	   	clauses.add(q);
-// 	   	//TODO: figure out what to do with AND and ORs
+//       // do not create modifier nodes with MOD_NONE
+//         if (mods != ModifierQueryNode.Modifier.MOD_NONE) {
+//          q = new ModifierQueryNode(q, mods);
+//         }
+//          clauses.add(q);
+//        //TODO: figure out what to do with AND and ORs
 //   }
 //   )*
 //     {
 //      if (clauses.size() == 1 && firstQuery != null)
 //         return firstQuery;
 //       else {
-//   		return new BooleanQueryNode(clauses);
+//       return new BooleanQueryNode(clauses);
 //       }
 //     }
 // }
@@ -301,10 +301,10 @@ public class StandardSyntaxParser implements SyntaxParser, StandardSyntaxParserC
   ModifierQueryNode.Modifier mods;
     mods = Modifiers();
     q = Clause(field);
-                if (mods != ModifierQueryNode.Modifier.MOD_NONE) {
-                        q = new ModifierQueryNode(q, mods);
-                }
-                {if (true) return q;}
+        if (mods != ModifierQueryNode.Modifier.MOD_NONE) {
+           q = new ModifierQueryNode(q, mods);
+        }
+        {if (true) return q;}
     throw new Error("Missing return statement in function");
   }
 
@@ -378,18 +378,18 @@ public class StandardSyntaxParser implements SyntaxParser, StandardSyntaxParserC
         }
         switch (operator.kind) {
             case OP_LESSTHAN:
-                lowerInclusive = true;
-                upperInclusive = false;
+              lowerInclusive = true;
+              upperInclusive = false;
 
-                qLower = new FieldQueryNode(field,
+               qLower = new FieldQueryNode(field,
                                          "*", term.beginColumn, term.endColumn);
-                        qUpper = new FieldQueryNode(field,
+            qUpper = new FieldQueryNode(field,
                                  EscapeQuerySyntaxImpl.discardEscapeChar(term.image), term.beginColumn, term.endColumn);
 
                 break;
             case OP_LESSTHANEQ:
-                lowerInclusive = true;
-                upperInclusive = true;
+              lowerInclusive = true;
+              upperInclusive = true;
 
                 qLower = new FieldQueryNode(field,
                                          "*", term.beginColumn, term.endColumn);
@@ -397,8 +397,8 @@ public class StandardSyntaxParser implements SyntaxParser, StandardSyntaxParserC
                                          EscapeQuerySyntaxImpl.discardEscapeChar(term.image), term.beginColumn, term.endColumn);
                 break;
             case OP_MORETHAN:
-                lowerInclusive = false;
-                upperInclusive = true;
+              lowerInclusive = false;
+              upperInclusive = true;
 
                 qLower = new FieldQueryNode(field,
                                          EscapeQuerySyntaxImpl.discardEscapeChar(term.image), term.beginColumn, term.endColumn);
@@ -406,8 +406,8 @@ public class StandardSyntaxParser implements SyntaxParser, StandardSyntaxParserC
                                          "*", term.beginColumn, term.endColumn);
                 break;
             case OP_MORETHANEQ:
-                lowerInclusive = true;
-                upperInclusive = true;
+              lowerInclusive = true;
+              upperInclusive = true;
 
                 qLower = new FieldQueryNode(field,
                                          EscapeQuerySyntaxImpl.discardEscapeChar(term.image), term.beginColumn, term.endColumn);
@@ -488,18 +488,18 @@ public class StandardSyntaxParser implements SyntaxParser, StandardSyntaxParserC
       }
     }
       if (boost != null) {
-                  float f = (float)1.0;
-                  try {
-                    f = Float.valueOf(boost.image).floatValue();
-                    // avoid boosting null queries, such as those caused by stop words
-                if (q != null) {
-                        q = new BoostQueryNode(q, f);
-                }
-                  } catch (Exception ignored) {
-                        /* Should this be handled somehow? (defaults to "no boost", if
+      float f = (float)1.0;
+      try {
+        f = Float.valueOf(boost.image).floatValue();
+        // avoid boosting null queries, such as those caused by stop words
+          if (q != null) {
+            q = new BoostQueryNode(q, f);
+          }
+      } catch (Exception ignored) {
+        /* Should this be handled somehow? (defaults to "no boost", if
              * boost number is invalid)
              */
-                  }
+      }
       }
       if (group) { q = new GroupQueryNode(q);}
       {if (true) return q;}
@@ -522,7 +522,7 @@ public class StandardSyntaxParser implements SyntaxParser, StandardSyntaxParserC
       switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
       case TERM:
         term = jj_consume_token(TERM);
-                         q = new FieldQueryNode(field, EscapeQuerySyntaxImpl.discardEscapeChar(term.image), term.beginColumn, term.endColumn);
+                    q = new FieldQueryNode(field, EscapeQuerySyntaxImpl.discardEscapeChar(term.image), term.beginColumn, term.endColumn);
         break;
       case REGEXPTERM:
         term = jj_consume_token(REGEXPTERM);
@@ -564,16 +564,16 @@ public class StandardSyntaxParser implements SyntaxParser, StandardSyntaxParserC
         ;
       }
        if (fuzzy) {
-          float fms = defaultMinSimilarity;
-          try {
+           float fms = defaultMinSimilarity;
+           try {
             fms = Float.valueOf(fuzzySlop.image.substring(1)).floatValue();
-          } catch (Exception ignored) { }
-         if(fms < 0.0f){
-           {if (true) throw new ParseException(new MessageImpl(QueryParserMessages.INVALID_SYNTAX_FUZZY_LIMITS));}
-         } else if (fms >= 1.0f && fms != (int) fms) {
-           {if (true) throw new ParseException(new MessageImpl(QueryParserMessages.INVALID_SYNTAX_FUZZY_EDITS));}
-         }
-         q = new FuzzyQueryNode(field, EscapeQuerySyntaxImpl.discardEscapeChar(term.image), fms, term.beginColumn, term.endColumn);
+           } catch (Exception ignored) { }
+           if(fms < 0.0f){
+                {if (true) throw new ParseException(new MessageImpl(QueryParserMessages.INVALID_SYNTAX_FUZZY_LIMITS));}
+          } else if (fms >= 1.0f && fms != (int) fms) {
+            {if (true) throw new ParseException(new MessageImpl(QueryParserMessages.INVALID_SYNTAX_FUZZY_EDITS));}
+          }
+          q = new FuzzyQueryNode(field, EscapeQuerySyntaxImpl.discardEscapeChar(term.image), fms, term.beginColumn, term.endColumn);
        } else if (regexp) {
          String re = term.image.substring(1, term.image.length()-1);
          q = new RegexpQueryNode(field, re, 0, re.length());
@@ -656,9 +656,9 @@ public class StandardSyntaxParser implements SyntaxParser, StandardSyntaxParserC
           }
 
           qLower = new FieldQueryNode(field,
-                                               EscapeQuerySyntaxImpl.discardEscapeChar(goop1.image), goop1.beginColumn, goop1.endColumn);
-                  qUpper = new FieldQueryNode(field,
-                                               EscapeQuerySyntaxImpl.discardEscapeChar(goop2.image), goop2.beginColumn, goop2.endColumn);
+                                   EscapeQuerySyntaxImpl.discardEscapeChar(goop1.image), goop1.beginColumn, goop1.endColumn);
+      qUpper = new FieldQueryNode(field,
+                                   EscapeQuerySyntaxImpl.discardEscapeChar(goop2.image), goop2.beginColumn, goop2.endColumn);
           q = new TermRangeQueryNode(qLower, qUpper, startInc ? true : false, endInc ? true : false);
       break;
     case QUOTED:
@@ -690,8 +690,8 @@ public class StandardSyntaxParser implements SyntaxParser, StandardSyntaxParserC
            }
            catch (Exception ignored) {
             /* Should this be handled somehow? (defaults to "no PhraseSlop", if
-	         * slop number is invalid)
-	         */
+           * slop number is invalid)
+           */
            }
          }
       break;
@@ -700,20 +700,20 @@ public class StandardSyntaxParser implements SyntaxParser, StandardSyntaxParserC
       jj_consume_token(-1);
       throw new ParseException();
     }
-          if (boost != null) {
-                  float f = (float)1.0;
-                  try {
-                    f = Float.valueOf(boost.image).floatValue();
-                    // avoid boosting null queries, such as those caused by stop words
-                if (q != null) {
-                        q = new BoostQueryNode(q, f);
-                }
-                  } catch (Exception ignored) {
-                        /* Should this be handled somehow? (defaults to "no boost", if
-	         * boost number is invalid)
-	         */
-                  }
+    if (boost != null) {
+      float f = (float)1.0;
+      try {
+        f = Float.valueOf(boost.image).floatValue();
+        // avoid boosting null queries, such as those caused by stop words
+          if (q != null) {
+            q = new BoostQueryNode(q, f);
           }
+      } catch (Exception ignored) {
+        /* Should this be handled somehow? (defaults to "no boost", if
+           * boost number is invalid)
+           */
+      }
+    }
       {if (true) return q;}
     throw new Error("Missing return statement in function");
   }
@@ -748,11 +748,6 @@ public class StandardSyntaxParser implements SyntaxParser, StandardSyntaxParserC
     return false;
   }
 
-  private boolean jj_3R_10() {
-    if (jj_scan_token(TERM)) return true;
-    return false;
-  }
-
   private boolean jj_3R_11() {
     if (jj_scan_token(REGEXPTERM)) return true;
     return false;
@@ -779,6 +774,11 @@ public class StandardSyntaxParser implements SyntaxParser, StandardSyntaxParserC
     return false;
   }
 
+  private boolean jj_3R_10() {
+    if (jj_scan_token(TERM)) return true;
+    return false;
+  }
+
   private boolean jj_3R_7() {
     Token xsp;
     xsp = jj_scanpos;
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/StandardSyntaxParser.jj b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/StandardSyntaxParser.jj
index bdd5d21..652879a 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/StandardSyntaxParser.jj
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/StandardSyntaxParser.jj
@@ -57,14 +57,14 @@ import org.apache.lucene.queryparser.flexible.standard.nodes.TermRangeQueryNode;
  */
 public class StandardSyntaxParser implements SyntaxParser {
 
-	private static final int CONJ_NONE =0;
-	private static final int CONJ_AND =2;
-	private static final int CONJ_OR =2;
+  private static final int CONJ_NONE =0;
+  private static final int CONJ_AND =2;
+  private static final int CONJ_OR =2;
 
  
    // syntax parser constructor
    public StandardSyntaxParser() {
-   	this(new FastCharStream(new StringReader("")));
+     this(new FastCharStream(new StringReader("")));
   }
      /** Parses a query string, returning a {@link org.apache.lucene.queryparser.flexible.core.nodes.QueryNode}.
      *  @param query  the query string to be parsed.
@@ -178,13 +178,13 @@ ModifierQueryNode.Modifier Modifiers() : {
 // This makes sure that there is no garbage after the query string
 QueryNode TopLevelQuery(CharSequence field) : 
 {
-	QueryNode q;
+  QueryNode q;
 }
 {
-	q=Query(field) <EOF>
-	{
-		return q;
-	}
+  q=Query(field) <EOF>
+  {
+     return q;
+  }
 }
 
 // These changes were made to introduce operator precedence:
@@ -209,25 +209,25 @@ QueryNode Query(CharSequence field) :
   (
     c=DisjQuery(field)
     { 
-	     if (clauses == null) {
-	         clauses = new Vector<QueryNode>();
-	         clauses.addElement(first); 
-	     } 
-    	 clauses.addElement(c);
+       if (clauses == null) {
+           clauses = new Vector<QueryNode>();
+           clauses.addElement(first);
+        }
+        clauses.addElement(c);
     }
     )*
     {
         if (clauses != null) { 
-	    	return new BooleanQueryNode(clauses);
-    	} else {
-        	return first;
-    	}
+        return new BooleanQueryNode(clauses);
+      } else {
+          return first;
+      }
     }
 }
 
 QueryNode DisjQuery(CharSequence field) : {
-	QueryNode first, c;
-	Vector<QueryNode> clauses = null;
+  QueryNode first, c;
+  Vector<QueryNode> clauses = null;
 }
 {
   first = ConjQuery(field)
@@ -243,7 +243,7 @@ QueryNode DisjQuery(CharSequence field) : {
   )*
   {
     if (clauses != null) { 
-	    return new OrQueryNode(clauses);
+      return new OrQueryNode(clauses);
     } else {
         return first;
     }
@@ -251,8 +251,8 @@ QueryNode DisjQuery(CharSequence field) : {
 }
 
 QueryNode ConjQuery(CharSequence field) : {
-	QueryNode first, c;
-	Vector<QueryNode> clauses = null;
+  QueryNode first, c;
+  Vector<QueryNode> clauses = null;
 }
 {
   first = ModClause(field)
@@ -268,7 +268,7 @@ QueryNode ConjQuery(CharSequence field) : {
   )*
   {
     if (clauses != null) {     
-	    return new AndQueryNode(clauses);
+      return new AndQueryNode(clauses);
     } else {
         return first;
     }
@@ -289,27 +289,27 @@ QueryNode ConjQuery(CharSequence field) : {
 //     if (mods == ModifierQueryNode.Modifier.MOD_NONE) firstQuery=q;
 //     
 //     // do not create modifier nodes with MOD_NONE
-//    	if (mods != ModifierQueryNode.Modifier.MOD_NONE) {
-//    		q = new ModifierQueryNode(q, mods);
-//    	}
-//    	clauses.add(q);
+//      if (mods != ModifierQueryNode.Modifier.MOD_NONE) {
+//          q = new ModifierQueryNode(q, mods);
+//         }
+//      clauses.add(q);
 //   }
 //   (
 //     conj=Conjunction() mods=Modifiers() q=Clause(field)
 //     { 
-// 	    // do not create modifier nodes with MOD_NONE
-// 	   	if (mods != ModifierQueryNode.Modifier.MOD_NONE) {
-// 	   		q = new ModifierQueryNode(q, mods);
-// 	   	}
-// 	   	clauses.add(q);
-// 	   	//TODO: figure out what to do with AND and ORs
+//       // do not create modifier nodes with MOD_NONE
+//         if (mods != ModifierQueryNode.Modifier.MOD_NONE) {
+//          q = new ModifierQueryNode(q, mods);
+//         }
+//          clauses.add(q);
+//        //TODO: figure out what to do with AND and ORs
 //   }
 //   )*
 //     {
 //      if (clauses.size() == 1 && firstQuery != null)
 //         return firstQuery;
 //       else {
-//   		return new BooleanQueryNode(clauses);
+//       return new BooleanQueryNode(clauses);
 //       }
 //     }
 // }
@@ -320,10 +320,10 @@ QueryNode ModClause(CharSequence field) : {
 }
 {
    mods=Modifiers() q= Clause(field) {
- 	   	if (mods != ModifierQueryNode.Modifier.MOD_NONE) {
- 	   		q = new ModifierQueryNode(q, mods);
- 	   	}
- 	   	return q;
+        if (mods != ModifierQueryNode.Modifier.MOD_NONE) {
+           q = new ModifierQueryNode(q, mods);
+        }
+        return q;
    }
 }
 
@@ -347,18 +347,18 @@ QueryNode Clause(CharSequence field) : {
         }
         switch (operator.kind) {
             case OP_LESSTHAN:
-            	lowerInclusive = true;
-            	upperInclusive = false;
-            	
-            	qLower = new FieldQueryNode(field,
+              lowerInclusive = true;
+              upperInclusive = false;
+
+               qLower = new FieldQueryNode(field,
                                          "*", term.beginColumn, term.endColumn);
-        		qUpper = new FieldQueryNode(field, 
+            qUpper = new FieldQueryNode(field,
                                  EscapeQuerySyntaxImpl.discardEscapeChar(term.image), term.beginColumn, term.endColumn);
                 
                 break;
             case OP_LESSTHANEQ:
-            	lowerInclusive = true;
-            	upperInclusive = true;
+              lowerInclusive = true;
+              upperInclusive = true;
             
                 qLower = new FieldQueryNode(field, 
                                          "*", term.beginColumn, term.endColumn);
@@ -366,8 +366,8 @@ QueryNode Clause(CharSequence field) : {
                                          EscapeQuerySyntaxImpl.discardEscapeChar(term.image), term.beginColumn, term.endColumn);
                 break;
             case OP_MORETHAN:
-            	lowerInclusive = false;
-            	upperInclusive = true;
+              lowerInclusive = false;
+              upperInclusive = true;
             
                 qLower = new FieldQueryNode(field, 
                                          EscapeQuerySyntaxImpl.discardEscapeChar(term.image), term.beginColumn, term.endColumn);
@@ -375,9 +375,9 @@ QueryNode Clause(CharSequence field) : {
                                          "*", term.beginColumn, term.endColumn);
                 break;
             case OP_MORETHANEQ:
-            	lowerInclusive = true;
-            	upperInclusive = true;
-            	
+              lowerInclusive = true;
+              upperInclusive = true;
+
                 qLower = new FieldQueryNode(field, 
                                          EscapeQuerySyntaxImpl.discardEscapeChar(term.image), term.beginColumn, term.endColumn);
                 qUpper = new FieldQueryNode(field, 
@@ -401,18 +401,18 @@ QueryNode Clause(CharSequence field) : {
 )
     {
       if (boost != null) {
-		  float f = (float)1.0;
-		  try {
-		    f = Float.valueOf(boost.image).floatValue();
-		    // avoid boosting null queries, such as those caused by stop words
-	      	if (q != null) {
-	        	q = new BoostQueryNode(q, f);
-	      	}
-		  } catch (Exception ignored) {
-		  	/* Should this be handled somehow? (defaults to "no boost", if
+      float f = (float)1.0;
+      try {
+        f = Float.valueOf(boost.image).floatValue();
+        // avoid boosting null queries, such as those caused by stop words
+          if (q != null) {
+            q = new BoostQueryNode(q, f);
+          }
+      } catch (Exception ignored) {
+        /* Should this be handled somehow? (defaults to "no boost", if
              * boost number is invalid)
-             */		  
-		  }
+             */
+      }
       }
       if (group) { q = new GroupQueryNode(q);}
       return q;
@@ -433,7 +433,7 @@ QueryNode Term(CharSequence field) : {
 {
   (
      (
- 	   term=<TERM> { q = new FieldQueryNode(field, EscapeQuerySyntaxImpl.discardEscapeChar(term.image), term.beginColumn, term.endColumn); }
+      term=<TERM> { q = new FieldQueryNode(field, EscapeQuerySyntaxImpl.discardEscapeChar(term.image), term.beginColumn, term.endColumn); }
        | term=<REGEXPTERM> { regexp=true; }
        | term=<NUMBER>
      )
@@ -441,16 +441,16 @@ QueryNode Term(CharSequence field) : {
      [ <CARAT> boost=<NUMBER> [ fuzzySlop=<FUZZY_SLOP> { fuzzy=true; } ] ]
      {
        if (fuzzy) {
-       	  float fms = defaultMinSimilarity;
-       	  try {
+           float fms = defaultMinSimilarity;
+           try {
             fms = Float.valueOf(fuzzySlop.image.substring(1)).floatValue();
-       	  } catch (Exception ignored) { }
-       	 if(fms < 0.0f){
-       	   throw new ParseException(new MessageImpl(QueryParserMessages.INVALID_SYNTAX_FUZZY_LIMITS));
-       	 } else if (fms >= 1.0f && fms != (int) fms) {
-       	   throw new ParseException(new MessageImpl(QueryParserMessages.INVALID_SYNTAX_FUZZY_EDITS));
-       	 }
-       	 q = new FuzzyQueryNode(field, EscapeQuerySyntaxImpl.discardEscapeChar(term.image), fms, term.beginColumn, term.endColumn);
+           } catch (Exception ignored) { }
+           if(fms < 0.0f){
+                throw new ParseException(new MessageImpl(QueryParserMessages.INVALID_SYNTAX_FUZZY_LIMITS));
+          } else if (fms >= 1.0f && fms != (int) fms) {
+            throw new ParseException(new MessageImpl(QueryParserMessages.INVALID_SYNTAX_FUZZY_EDITS));
+          }
+          q = new FuzzyQueryNode(field, EscapeQuerySyntaxImpl.discardEscapeChar(term.image), fms, term.beginColumn, term.endColumn);
        } else if (regexp) {
          String re = term.image.substring(1, term.image.length()-1);
          q = new RegexpQueryNode(field, re, 0, re.length());
@@ -471,9 +471,9 @@ QueryNode Term(CharSequence field) : {
           }
           
           qLower = new FieldQueryNode(field, 
-		                               EscapeQuerySyntaxImpl.discardEscapeChar(goop1.image), goop1.beginColumn, goop1.endColumn);
-		  qUpper = new FieldQueryNode(field,  
-		                               EscapeQuerySyntaxImpl.discardEscapeChar(goop2.image), goop2.beginColumn, goop2.endColumn);
+                                   EscapeQuerySyntaxImpl.discardEscapeChar(goop1.image), goop1.beginColumn, goop1.endColumn);
+      qUpper = new FieldQueryNode(field,
+                                   EscapeQuerySyntaxImpl.discardEscapeChar(goop2.image), goop2.beginColumn, goop2.endColumn);
           q = new TermRangeQueryNode(qLower, qUpper, startInc ? true : false, endInc ? true : false);
         }
      | term=<QUOTED> {q = new QuotedFieldQueryNode(field, EscapeQuerySyntaxImpl.discardEscapeChar(term.image.substring(1, term.image.length()-1)), term.beginColumn + 1, term.endColumn - 1);}
@@ -489,28 +489,28 @@ QueryNode Term(CharSequence field) : {
            }
            catch (Exception ignored) {
             /* Should this be handled somehow? (defaults to "no PhraseSlop", if
-	         * slop number is invalid)
-	         */		
+           * slop number is invalid)
+           */
            }
          }
               
        }
   )
   {
-	  if (boost != null) {
-		  float f = (float)1.0;
-		  try {
-		    f = Float.valueOf(boost.image).floatValue();
-		    // avoid boosting null queries, such as those caused by stop words
-	      	if (q != null) {
-	        	q = new BoostQueryNode(q, f);
-	      	}
-		  } catch (Exception ignored) {
-		  	/* Should this be handled somehow? (defaults to "no boost", if
-	         * boost number is invalid)
-	         */		  
-		  }
-	  }
+    if (boost != null) {
+      float f = (float)1.0;
+      try {
+        f = Float.valueOf(boost.image).floatValue();
+        // avoid boosting null queries, such as those caused by stop words
+          if (q != null) {
+            q = new BoostQueryNode(q, f);
+          }
+      } catch (Exception ignored) {
+        /* Should this be handled somehow? (defaults to "no boost", if
+           * boost number is invalid)
+           */
+      }
+    }
       return q;
   }
 }
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/parser/FastCharStream.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/parser/FastCharStream.java
index e221541..361a037 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/parser/FastCharStream.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/parser/FastCharStream.java
@@ -26,13 +26,13 @@ import java.io.*;
 public final class FastCharStream implements CharStream {
   char[] buffer = null;
 
-  int bufferLength = 0;				  // end of valid chars
-  int bufferPosition = 0;			  // next char to read
+  int bufferLength = 0;          // end of valid chars
+  int bufferPosition = 0;        // next char to read
 
-  int tokenStart = 0;				  // offset in buffer
-  int bufferStart = 0;				  // position in file of buffer
+  int tokenStart = 0;          // offset in buffer
+  int bufferStart = 0;          // position in file of buffer
 
-  Reader input;					  // source of chars
+  Reader input;            // source of chars
 
   /** Constructs from a Reader. */
   public FastCharStream(Reader r) {
@@ -48,24 +48,24 @@ public final class FastCharStream implements CharStream {
   private final void refill() throws IOException {
     int newPosition = bufferLength - tokenStart;
 
-    if (tokenStart == 0) {			  // token won't fit in buffer
-      if (buffer == null) {			  // first time: alloc buffer
-	buffer = new char[2048];
+    if (tokenStart == 0) {        // token won't fit in buffer
+      if (buffer == null) {        // first time: alloc buffer
+        buffer = new char[2048];
       } else if (bufferLength == buffer.length) { // grow buffer
-	char[] newBuffer = new char[buffer.length*2];
-	System.arraycopy(buffer, 0, newBuffer, 0, bufferLength);
-	buffer = newBuffer;
+        char[] newBuffer = new char[buffer.length * 2];
+        System.arraycopy(buffer, 0, newBuffer, 0, bufferLength);
+        buffer = newBuffer;
       }
-    } else {					  // shift token to front
+    } else {            // shift token to front
       System.arraycopy(buffer, tokenStart, buffer, 0, newPosition);
     }
 
-    bufferLength = newPosition;			  // update state
+    bufferLength = newPosition;        // update state
     bufferPosition = newPosition;
     bufferStart += tokenStart;
     tokenStart = 0;
 
-    int charsRead =				  // fill space in buffer
+    int charsRead =          // fill space in buffer
       input.read(buffer, newPosition, buffer.length-newPosition);
     if (charsRead == -1)
       throw new IOException("read past eof");
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/BasicQueryFactory.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/BasicQueryFactory.java
index c328b20..c5c6e39 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/BasicQueryFactory.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/BasicQueryFactory.java
@@ -47,9 +47,9 @@ public class BasicQueryFactory {
   
   public String toString() {
     return getClass().getName()
-	  + "(maxBasicQueries: " + maxBasicQueries
-	  + ", queriesMade: " + queriesMade
-	  + ")";
+    + "(maxBasicQueries: " + maxBasicQueries
+    + ", queriesMade: " + queriesMade
+    + ")";
   }
 
   private boolean atMax() {
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/RewriteQuery.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/RewriteQuery.java
index 73a4909..1275b78 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/RewriteQuery.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/RewriteQuery.java
@@ -45,19 +45,19 @@ abstract class RewriteQuery<SQ extends SrndQuery> extends Query {
   @Override
   public String toString(String field) {
     return getClass().getName()
-	  + (field == null ? "" : "(unused: " + field + ")")
-	  + "(" + fieldName
-	  + ", " + srndQuery.toString()
-	  + ", " + qf.toString()
-	  + ")";
+    + (field == null ? "" : "(unused: " + field + ")")
+    + "(" + fieldName
+    + ", " + srndQuery.toString()
+    + ", " + qf.toString()
+    + ")";
   }
 
   @Override
   public int hashCode() {
     return getClass().hashCode()
-	  ^ fieldName.hashCode()
-	  ^ qf.hashCode()
-	  ^ srndQuery.hashCode();
+    ^ fieldName.hashCode()
+    ^ qf.hashCode()
+    ^ srndQuery.hashCode();
   }
 
   @Override
@@ -68,8 +68,8 @@ abstract class RewriteQuery<SQ extends SrndQuery> extends Query {
       return false;
     RewriteQuery other = (RewriteQuery)obj;
     return fieldName.equals(other.fieldName)
-	&& qf.equals(other.qf)
-	&& srndQuery.equals(other.srndQuery);
+  && qf.equals(other.qf)
+  && srndQuery.equals(other.srndQuery);
   }
 
   /** @throws UnsupportedOperationException */
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SimpleTermRewriteQuery.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SimpleTermRewriteQuery.java
index e7dedd2..9c8496c 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SimpleTermRewriteQuery.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SimpleTermRewriteQuery.java
@@ -37,16 +37,16 @@ class SimpleTermRewriteQuery extends RewriteQuery<SimpleTerm> {
   public Query rewrite(IndexReader reader) throws IOException {
     final List<Query> luceneSubQueries = new ArrayList<Query>();
     srndQuery.visitMatchingTerms(reader, fieldName,
-	  new SimpleTerm.MatchingTermVisitor() {
-	    public void visitMatchingTerm(Term term) throws IOException {
-	      luceneSubQueries.add(qf.newTermQuery(term));
-	    }
-	  });
+    new SimpleTerm.MatchingTermVisitor() {
+      public void visitMatchingTerm(Term term) throws IOException {
+        luceneSubQueries.add(qf.newTermQuery(term));
+      }
+    });
     return  (luceneSubQueries.size() == 0) ? SrndQuery.theEmptyLcnQuery
-	  : (luceneSubQueries.size() == 1) ? luceneSubQueries.get(0)
-	  : SrndBooleanQuery.makeBooleanQuery(
-	    /* luceneSubQueries all have default weight */
-	    luceneSubQueries, BooleanClause.Occur.SHOULD); /* OR the subquery terms */
+    : (luceneSubQueries.size() == 1) ? luceneSubQueries.get(0)
+    : SrndBooleanQuery.makeBooleanQuery(
+      /* luceneSubQueries all have default weight */
+      luceneSubQueries, BooleanClause.Occur.SHOULD); /* OR the subquery terms */
   }
 }
 
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/CoreParser.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/CoreParser.java
index ae200c5..c1929f9 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/CoreParser.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/CoreParser.java
@@ -153,5 +153,5 @@ public class CoreParser implements QueryBuilder {
 
   public Query getQuery(Element e) throws ParserException {
     return queryFactory.getQuery(e);
-	}
+  }
 }
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/CorePlusExtensionsParser.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/CorePlusExtensionsParser.java
index e7f844c..cc90d3c 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/CorePlusExtensionsParser.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/CorePlusExtensionsParser.java
@@ -58,6 +58,6 @@ public class CorePlusExtensionsParser extends CoreParser {
     queryFactory.addBuilder("LikeThisQuery", new LikeThisQueryBuilder(analyzer, fields));
     queryFactory.addBuilder("BoostingQuery", new BoostingQueryBuilder(queryFactory));
     queryFactory.addBuilder("FuzzyLikeThisQuery", new FuzzyLikeThisQueryBuilder(analyzer));
-		
-	}
+
+  }
 }
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/DOMUtils.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/DOMUtils.java
index 4493892..27e2683 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/DOMUtils.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/DOMUtils.java
@@ -212,7 +212,7 @@ public class DOMUtils {
     }
 
     return doc;
-	}	
+  }
 }
 
 
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/FilterBuilder.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/FilterBuilder.java
index af54944..0e732f6 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/FilterBuilder.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/FilterBuilder.java
@@ -27,5 +27,5 @@ import org.w3c.dom.Element;
  */
 public interface FilterBuilder {
 
-	 public Filter getFilter(Element e) throws ParserException;
+   public Filter getFilter(Element e) throws ParserException;
 }
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/ParserException.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/ParserException.java
index 3974d36..d902bda 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/ParserException.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/ParserException.java
@@ -25,32 +25,32 @@ package org.apache.lucene.queryparser.xml;
  */
 public class ParserException extends Exception {
 
-	/**
-	 * 
-	 */
-	public ParserException() {
-		super();
-	}
+  /**
+   *
+   */
+  public ParserException() {
+    super();
+  }
 
-	/**
-	 * @param message
-	 */
-	public ParserException(String message) {
-		super(message);
-	}
+  /**
+   * @param message
+   */
+  public ParserException(String message) {
+    super(message);
+  }
 
-	/**
-	 * @param message
-	 * @param cause
-	 */
-	public ParserException(String message, Throwable cause) {
-		super(message, cause);
-	}
+  /**
+   * @param message
+   * @param cause
+   */
+  public ParserException(String message, Throwable cause) {
+    super(message, cause);
+  }
 
-	/**
-	 * @param cause
-	 */
-	public ParserException(Throwable cause) {
-		super(cause);
-	}
+  /**
+   * @param cause
+   */
+  public ParserException(Throwable cause) {
+    super(cause);
+  }
 }
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/QueryBuilder.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/QueryBuilder.java
index 8e676d7..eff70d3 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/QueryBuilder.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/QueryBuilder.java
@@ -23,6 +23,6 @@ import org.w3c.dom.Element;
  * expected to be thread-safe so that they can be used to simultaneously parse multiple XML documents.
  */
 public interface QueryBuilder {
-	
-	public Query getQuery(Element e) throws ParserException;
+
+  public Query getQuery(Element e) throws ParserException;
 }
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/QueryTemplateManager.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/QueryTemplateManager.java
index 6813fd0..4abc2fd 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/QueryTemplateManager.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/QueryTemplateManager.java
@@ -199,5 +199,5 @@ public class QueryTemplateManager {
     org.w3c.dom.Document xslDoc = builder.parse(xslIs);
     DOMSource ds = new DOMSource(xslDoc);
     return tFactory.newTemplates(ds);
-	}
+  }
 }
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/CachedFilterBuilder.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/CachedFilterBuilder.java
index 06f7fe5..9547649 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/CachedFilterBuilder.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/CachedFilterBuilder.java
@@ -32,10 +32,10 @@ import java.util.Map;
  * Filters are cached in an LRU Cache keyed on the contained query or filter object. Using this will
  * speed up overall performance for repeated uses of the same expensive query/filter. The sorts of
  * queries/filters likely to benefit from caching need not necessarily be complex - e.g. simple
- * TermQuerys with a large DF (document frequency) can be expensive	on large indexes.
- * A good example of this might be a term query on a field with only 2 possible	values -
+ * TermQuerys with a large DF (document frequency) can be expensive  on large indexes.
+ * A good example of this might be a term query on a field with only 2 possible  values -
  * "true" or "false". In a large index, querying or filtering on this field requires reading
- * millions	of document ids from disk which can more usefully be cached as a filter bitset.
+ * millions  of document ids from disk which can more usefully be cached as a filter bitset.
  * <p/>
  * For Queries/Filters to be cached and reused the object must implement hashcode and
  * equals methods correctly so that duplicate queries/filters can be detected in the cache.
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/NumericRangeFilterBuilder.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/NumericRangeFilterBuilder.java
index c7b46c0..da00a3e 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/NumericRangeFilterBuilder.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/NumericRangeFilterBuilder.java
@@ -158,7 +158,7 @@ public class NumericRangeFilterBuilder implements FilterBuilder {
     @Override
     public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
       return null;
-		}
+    }
 
-	}
+  }
 }
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/SpanQueryBuilder.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/SpanQueryBuilder.java
index 3ca6c64..8bbaa1c 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/SpanQueryBuilder.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/SpanQueryBuilder.java
@@ -24,6 +24,6 @@ import org.w3c.dom.Element;
  * Interface for retrieving a {@link SpanQuery}.
  */
 public interface SpanQueryBuilder extends QueryBuilder {
-	
-	public SpanQuery getSpanQuery(Element e) throws ParserException;
+
+  public SpanQuery getSpanQuery(Element e) throws ParserException;
 }
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/UserInputQueryBuilder.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/UserInputQueryBuilder.java
index 58c3d1b..cf75a4e 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/UserInputQueryBuilder.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/UserInputQueryBuilder.java
@@ -88,6 +88,6 @@ public class UserInputQueryBuilder implements QueryBuilder {
    */
   protected QueryParser createQueryParser(String fieldName, Analyzer analyzer) {
     return new QueryParser(Version.LUCENE_CURRENT, fieldName, analyzer);
-	}
+  }
 
 }
diff --git a/lucene/queryparser/src/test/org/apache/lucene/queryparser/xml/TestQueryTemplateManager.java b/lucene/queryparser/src/test/org/apache/lucene/queryparser/xml/TestQueryTemplateManager.java
index 3429c4f..7f81ddb 100644
--- a/lucene/queryparser/src/test/org/apache/lucene/queryparser/xml/TestQueryTemplateManager.java
+++ b/lucene/queryparser/src/test/org/apache/lucene/queryparser/xml/TestQueryTemplateManager.java
@@ -89,7 +89,7 @@ public class TestQueryTemplateManager extends LuceneTestCase {
       Properties queryFormProperties = getPropsFromString(queryForm);
 
       //Get the required query XSL template for this test
-//			Templates template=getTemplate(queryFormProperties.getProperty("template"));
+//      Templates template=getTemplate(queryFormProperties.getProperty("template"));
 
       //Transform the queryFormProperties into a Lucene XML query
       Document doc = qtm.getQueryAsDOM(queryFormProperties, queryFormProperties.getProperty("template"));
diff --git a/lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery.java b/lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery.java
index f40b134..07e9d48 100644
--- a/lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery.java
+++ b/lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery.java
@@ -121,17 +121,17 @@ public class FuzzyLikeThisQuery extends Query
 
     class FieldVals
     {
-    	String queryString;
-    	String fieldName;
-    	float minSimilarity;
-    	int prefixLength;
-		public FieldVals(String name, float similarity, int length, String queryString)
-		{
-			fieldName = name;
-			minSimilarity = similarity;
-			prefixLength = length;
-			this.queryString = queryString;
-		}
+      String queryString;
+      String fieldName;
+      float minSimilarity;
+      int prefixLength;
+    public FieldVals(String name, float similarity, int length, String queryString)
+    {
+      fieldName = name;
+      minSimilarity = similarity;
+      prefixLength = length;
+      this.queryString = queryString;
+    }
 
     @Override
     public int hashCode() {
@@ -174,7 +174,7 @@ public class FuzzyLikeThisQuery extends Query
     }
     
 
-    	
+
     }
     
     /**
@@ -186,77 +186,72 @@ public class FuzzyLikeThisQuery extends Query
      */
     public void addTerms(String queryString, String fieldName,float minSimilarity, int prefixLength) 
     {
-    	fieldVals.add(new FieldVals(fieldName,minSimilarity,prefixLength,queryString));
+      fieldVals.add(new FieldVals(fieldName,minSimilarity,prefixLength,queryString));
     }
-    
-    
-    private void addTerms(IndexReader reader,FieldVals f) throws IOException
-    {
-        if(f.queryString==null) return;
-        TokenStream ts=analyzer.tokenStream(f.fieldName, new StringReader(f.queryString));
-        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);
-        
-        int corpusNumDocs=reader.numDocs();
-        HashSet<String> processedTerms=new HashSet<String>();
-        ts.reset();
-        while (ts.incrementToken()) 
-        {
-                String term = termAtt.toString();
-        	if(!processedTerms.contains(term))
-        	{
-                  processedTerms.add(term);
-                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term
-                  float minScore=0;
-                  Term startTerm=new Term(f.fieldName, term);
-                  AttributeSource atts = new AttributeSource();
-                  MaxNonCompetitiveBoostAttribute maxBoostAtt =
-                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);
-                  SlowFuzzyTermsEnum fe = new SlowFuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()), atts, startTerm, f.minSimilarity, f.prefixLength);
-                  //store the df so all variants use same idf
-                  int df = reader.docFreq(startTerm);
-                  int numVariants=0;
-                  int totalVariantDocFreqs=0;
-                  BytesRef possibleMatch;
-                  BoostAttribute boostAtt =
-                    fe.attributes().addAttribute(BoostAttribute.class);
-                  while ((possibleMatch = fe.next()) != null) {
-                      numVariants++;
-                      totalVariantDocFreqs+=fe.docFreq();
-                      float score=boostAtt.getBoost();
-                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){
-                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)),score,startTerm);                    
-                        variantsQ.insertWithOverflow(st);
-                        minScore = variantsQ.top().score; // maintain minScore
-                      }
-                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);
-                    }
 
-                  if(numVariants>0)
-                    {
-                      int avgDf=totalVariantDocFreqs/numVariants;
-                      if(df==0)//no direct match we can use as df for all variants 
-	                {
-	                    df=avgDf; //use avg df of all variants
-	                }
-	                
-                    // take the top variants (scored by edit distance) and reset the score
-                    // to include an IDF factor then add to the global queue for ranking 
-                    // overall top query terms
-                    int size = variantsQ.size();
-                    for(int i = 0; i < size; i++)
-	                {
-	                  ScoreTerm st = variantsQ.pop();
-	                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);
-	                  q.insertWithOverflow(st);
-	                }                            
-                }
-        	}
+
+  private void addTerms(IndexReader reader, FieldVals f) throws IOException {
+    if (f.queryString == null) return;
+    TokenStream ts = analyzer.tokenStream(f.fieldName, new StringReader(f.queryString));
+    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);
+
+    int corpusNumDocs = reader.numDocs();
+    HashSet<String> processedTerms = new HashSet<String>();
+    ts.reset();
+    while (ts.incrementToken()) {
+      String term = termAtt.toString();
+      if (!processedTerms.contains(term)) {
+        processedTerms.add(term);
+        ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term
+        float minScore = 0;
+        Term startTerm = new Term(f.fieldName, term);
+        AttributeSource atts = new AttributeSource();
+        MaxNonCompetitiveBoostAttribute maxBoostAtt =
+            atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);
+        SlowFuzzyTermsEnum fe = new SlowFuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()), atts, startTerm, f.minSimilarity, f.prefixLength);
+        //store the df so all variants use same idf
+        int df = reader.docFreq(startTerm);
+        int numVariants = 0;
+        int totalVariantDocFreqs = 0;
+        BytesRef possibleMatch;
+        BoostAttribute boostAtt =
+            fe.attributes().addAttribute(BoostAttribute.class);
+        while ((possibleMatch = fe.next()) != null) {
+          numVariants++;
+          totalVariantDocFreqs += fe.docFreq();
+          float score = boostAtt.getBoost();
+          if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {
+            ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);
+            variantsQ.insertWithOverflow(st);
+            minScore = variantsQ.top().score; // maintain minScore
+          }
+          maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);
         }
-        ts.end();
-        ts.close();
+
+        if (numVariants > 0) {
+          int avgDf = totalVariantDocFreqs / numVariants;
+          if (df == 0)//no direct match we can use as df for all variants
+          {
+            df = avgDf; //use avg df of all variants
+          }
+
+          // take the top variants (scored by edit distance) and reset the score
+          // to include an IDF factor then add to the global queue for ranking
+          // overall top query terms
+          int size = variantsQ.size();
+          for (int i = 0; i < size; i++) {
+            ScoreTerm st = variantsQ.pop();
+            st.score = (st.score * st.score) * sim.idf(df, corpusNumDocs);
+            q.insertWithOverflow(st);
+          }
+        }
+      }
     }
-            
-    @Override
+    ts.end();
+    ts.close();
+  }
+
+  @Override
     public Query rewrite(IndexReader reader) throws IOException
     {
         if(rewrittenQuery!=null)
@@ -264,12 +259,11 @@ public class FuzzyLikeThisQuery extends Query
             return rewrittenQuery;
         }
         //load up the list of possible terms
-        for (Iterator<FieldVals> iter = fieldVals.iterator(); iter.hasNext();)
-		{
-			FieldVals f = iter.next();
-			addTerms(reader,f);			
-		}
-        //clear the list of fields
+        for (Iterator<FieldVals> iter = fieldVals.iterator(); iter.hasNext(); ) {
+          FieldVals f = iter.next();
+          addTerms(reader, f);
+        }
+      //clear the list of fields
         fieldVals.clear();
         
         BooleanQuery bq=new BooleanQuery();
@@ -368,15 +362,15 @@ public class FuzzyLikeThisQuery extends Query
     }
 
 
-	public boolean isIgnoreTF()
-	{
-		return ignoreTF;
-	}
+  public boolean isIgnoreTF()
+  {
+    return ignoreTF;
+  }
 
 
-	public void setIgnoreTF(boolean ignoreTF)
-	{
-		this.ignoreTF = ignoreTF;
-	}   
+  public void setIgnoreTF(boolean ignoreTF)
+  {
+    this.ignoreTF = ignoreTF;
+  }
     
 }
diff --git a/lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/TestSlowFuzzyQuery.java b/lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/TestSlowFuzzyQuery.java
index a4226f2..a4a125d 100644
--- a/lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/TestSlowFuzzyQuery.java
+++ b/lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/TestSlowFuzzyQuery.java
@@ -105,7 +105,7 @@ public class TestSlowFuzzyQuery extends LuceneTestCase {
     }
 
     // not similar enough:
-    query = new SlowFuzzyQuery(new Term("field", "xxxxx"), SlowFuzzyQuery.defaultMinSimilarity, 0);  	
+    query = new SlowFuzzyQuery(new Term("field", "xxxxx"), SlowFuzzyQuery.defaultMinSimilarity, 0);
     hits = searcher.search(query, null, 1000).scoreDocs;
     assertEquals(0, hits.length);
     query = new SlowFuzzyQuery(new Term("field", "aaccc"), SlowFuzzyQuery.defaultMinSimilarity, 0);   // edit distance to "aaaaa" = 3
diff --git a/lucene/suggest/src/java/org/apache/lucene/search/spell/DirectSpellChecker.java b/lucene/suggest/src/java/org/apache/lucene/search/spell/DirectSpellChecker.java
index 5597690..0495a4d 100644
--- a/lucene/suggest/src/java/org/apache/lucene/search/spell/DirectSpellChecker.java
+++ b/lucene/suggest/src/java/org/apache/lucene/search/spell/DirectSpellChecker.java
@@ -294,7 +294,7 @@ public class DirectSpellChecker {
    */
   public SuggestWord[] suggestSimilar(Term term, int numSug, IndexReader ir, 
       SuggestMode suggestMode) throws IOException {
-  	return suggestSimilar(term, numSug, ir, suggestMode, this.accuracy);
+    return suggestSimilar(term, numSug, ir, suggestMode, this.accuracy);
   }
   
   /**
diff --git a/lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java b/lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java
index 476b160..acebb99 100755
--- a/lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java
+++ b/lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java
@@ -283,7 +283,7 @@ public class SpellChecker implements java.io.Closeable {
    */
   public String[] suggestSimilar(String word, int numSug, IndexReader ir,
       String field, SuggestMode suggestMode) throws IOException {
-  	return suggestSimilar(word, numSug, ir, field, suggestMode, this.accuracy);
+    return suggestSimilar(word, numSug, ir, field, suggestMode, this.accuracy);
   }
   
   /**
diff --git a/lucene/suggest/src/java/org/apache/lucene/search/suggest/tst/TernaryTreeNode.java b/lucene/suggest/src/java/org/apache/lucene/search/suggest/tst/TernaryTreeNode.java
index 0ebe010..fa64ad6 100644
--- a/lucene/suggest/src/java/org/apache/lucene/search/suggest/tst/TernaryTreeNode.java
+++ b/lucene/suggest/src/java/org/apache/lucene/search/suggest/tst/TernaryTreeNode.java
@@ -23,20 +23,20 @@ package org.apache.lucene.search.suggest.tst;
 
 public class TernaryTreeNode {
   /** the character stored by a node. */
-	char splitchar;
-	/** a reference object to the node containing character smaller than this node's character. */
-	TernaryTreeNode loKid;
-	/** 
-	 *  a reference object to the node containing character next to this node's character as 
-	 *  occurring in the inserted token.
-	 */
-	TernaryTreeNode eqKid;
-	/** a reference object to the node containing character higher than this node's character. */
-	TernaryTreeNode hiKid;
-	/** 
-	 * used by leaf nodes to store the complete tokens to be added to suggest list while 
-	 * auto-completing the prefix.
-	 */
-	String token;
-	Object val;
+  char splitchar;
+  /** a reference object to the node containing character smaller than this node's character. */
+  TernaryTreeNode loKid;
+  /** 
+   *  a reference object to the node containing character next to this node's character as 
+   *  occurring in the inserted token.
+   */
+  TernaryTreeNode eqKid;
+  /** a reference object to the node containing character higher than this node's character. */
+  TernaryTreeNode hiKid;
+  /** 
+   * used by leaf nodes to store the complete tokens to be added to suggest list while 
+   * auto-completing the prefix.
+   */
+  String token;
+  Object val;
 }
diff --git a/lucene/test-framework/src/java/org/apache/lucene/analysis/MockTokenizer.java b/lucene/test-framework/src/java/org/apache/lucene/analysis/MockTokenizer.java
index d244f17..0a0628b 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/analysis/MockTokenizer.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/analysis/MockTokenizer.java
@@ -160,7 +160,7 @@ public class MockTokenizer extends Tokenizer {
           return Character.toCodePoint((char) ch, (char) ch2);
         } else {
           assert false : "stream ends with unpaired high surrogate: " + Integer.toHexString(ch);
-	}
+        }
       }
       return ch;
     }
diff --git a/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java b/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
index a477ea1..97f719c 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
@@ -358,7 +358,7 @@ public abstract class LuceneTestCase extends Assert {
       } else {
         Logger.getLogger(LuceneTestCase.class.getSimpleName()).warning(
             "Property '" + SYSPROP_MAXFAILURES + "'=" + maxFailures + ", 'failfast' is" +
-            		" ignored.");
+            " ignored.");
       }
     }
 
diff --git a/lucene/test-framework/src/java/org/apache/lucene/util/RunListenerPrintReproduceInfo.java b/lucene/test-framework/src/java/org/apache/lucene/util/RunListenerPrintReproduceInfo.java
index cb4ebd2..1422992 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/util/RunListenerPrintReproduceInfo.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/util/RunListenerPrintReproduceInfo.java
@@ -129,7 +129,7 @@ public final class RunListenerPrintReproduceInfo extends RunListener {
   private void reportAdditionalFailureInfo(final String testName) {
     if (TEST_LINE_DOCS_FILE.endsWith(JENKINS_LARGE_LINE_DOCS_FILE)) {
       System.err.println("NOTE: download the large Jenkins line-docs file by running " +
-      		"'ant get-jenkins-line-docs' in the lucene directory.");
+        "'ant get-jenkins-line-docs' in the lucene directory.");
     }
 
     final StringBuilder b = new StringBuilder();
diff --git a/lucene/test-framework/src/java/org/apache/lucene/util/TestRuleAssertionsRequired.java b/lucene/test-framework/src/java/org/apache/lucene/util/TestRuleAssertionsRequired.java
index 42dd64b..8608328 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/util/TestRuleAssertionsRequired.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/util/TestRuleAssertionsRequired.java
@@ -33,7 +33,7 @@ public class TestRuleAssertionsRequired implements TestRule {
         try {
           assert false;
           String msg = "Test class requires enabled assertions, enable globally (-ea)" +
-          		" or for Solr/Lucene subpackages only: " + description.getClassName();
+              " or for Solr/Lucene subpackages only: " + description.getClassName();
           System.err.println(msg);
           throw new Exception(msg);
         } catch (AssertionError e) {
diff --git a/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DIHLogLevels.java b/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DIHLogLevels.java
index 833d750..e38c658 100644
--- a/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DIHLogLevels.java
+++ b/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DIHLogLevels.java
@@ -18,5 +18,5 @@ package org.apache.solr.handler.dataimport;
  */
 
 public enum DIHLogLevels {
-	START_ENTITY, END_ENTITY, TRANSFORMED_ROW, ENTITY_META, PRE_TRANSFORMER_ROW, START_DOC, END_DOC, ENTITY_OUT, ROW_END, TRANSFORMER_EXCEPTION, ENTITY_EXCEPTION, DISABLE_LOGGING, ENABLE_LOGGING, NONE
+  START_ENTITY, END_ENTITY, TRANSFORMED_ROW, ENTITY_META, PRE_TRANSFORMER_ROW, START_DOC, END_DOC, ENTITY_OUT, ROW_END, TRANSFORMER_EXCEPTION, ENTITY_EXCEPTION, DISABLE_LOGGING, ENABLE_LOGGING, NONE
 }
diff --git a/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DIHWriter.java b/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DIHWriter.java
index 774b350..2b4947c 100644
--- a/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DIHWriter.java
+++ b/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DIHWriter.java
@@ -25,81 +25,81 @@ import org.apache.solr.common.SolrInputDocument;
  *
  */
 public interface DIHWriter {
-	
-	/**
-	 * <p>
-	 *  If this writer supports transactions or commit points, then commit any changes, 
-	 *  optionally optimizing the data for read/write performance
-	 * </p>
-	 * @param optimize
-	 */
-	public void commit(boolean optimize);
-	
-	/**
-	 * <p>
-	 *  Release resources used by this writer.  After calling close, reads & updates will throw exceptions.
-	 * </p>
-	 */
-	public void close();
-
-	/**
-	 * <p>
-	 *  If this writer supports transactions or commit points, then roll back any uncommitted changes.
-	 * </p>
-	 */
-	public void rollback();
-
-	/**
-	 * <p>
-	 *  Delete from the writer's underlying data store based the passed-in writer-specific query. (Optional Operation)
-	 * </p>
-	 * @param q
-	 */
-	public void deleteByQuery(String q);
-
-	/**
-	 * <p>
-	 *  Delete everything from the writer's underlying data store
-	 * </p>
-	 */
-	public void doDeleteAll();
-
-	/**
-	 * <p>
-	 *  Delete from the writer's underlying data store based on the passed-in Primary Key
-	 * </p>
-	 * @param key
-	 */
-	public void deleteDoc(Object key);
-	
-
-
-	/**
-	 * <p>
-	 *  Add a document to this writer's underlying data store.
-	 * </p>
-	 * @param doc
-	 * @return true on success, false on failure
-	 */
-	public boolean upload(SolrInputDocument doc);
-
-
-	
-	/**
-	 * <p>
-	 *  Provide context information for this writer.  init() should be called before using the writer.
-	 * </p>
-	 * @param context
-	 */
-	public void init(Context context) ;
-
-	
-	/**
-	 * <p>
-	 *  Specify the keys to be modified by a delta update (required by writers that can store duplicate keys)
-	 * </p>
-	 * @param deltaKeys
-	 */
-	public void setDeltaKeys(Set<Map<String, Object>> deltaKeys) ;
+
+  /**
+   * <p>
+   *  If this writer supports transactions or commit points, then commit any changes,
+   *  optionally optimizing the data for read/write performance
+   * </p>
+   * @param optimize
+   */
+  public void commit(boolean optimize);
+
+  /**
+   * <p>
+   *  Release resources used by this writer.  After calling close, reads & updates will throw exceptions.
+   * </p>
+   */
+  public void close();
+
+  /**
+   * <p>
+   *  If this writer supports transactions or commit points, then roll back any uncommitted changes.
+   * </p>
+   */
+  public void rollback();
+
+  /**
+   * <p>
+   *  Delete from the writer's underlying data store based the passed-in writer-specific query. (Optional Operation)
+   * </p>
+   * @param q
+   */
+  public void deleteByQuery(String q);
+
+  /**
+   * <p>
+   *  Delete everything from the writer's underlying data store
+   * </p>
+   */
+  public void doDeleteAll();
+
+  /**
+   * <p>
+   *  Delete from the writer's underlying data store based on the passed-in Primary Key
+   * </p>
+   * @param key
+   */
+  public void deleteDoc(Object key);
+
+
+
+  /**
+   * <p>
+   *  Add a document to this writer's underlying data store.
+   * </p>
+   * @param doc
+   * @return true on success, false on failure
+   */
+  public boolean upload(SolrInputDocument doc);
+
+
+
+  /**
+   * <p>
+   *  Provide context information for this writer.  init() should be called before using the writer.
+   * </p>
+   * @param context
+   */
+  public void init(Context context) ;
+
+
+  /**
+   * <p>
+   *  Specify the keys to be modified by a delta update (required by writers that can store duplicate keys)
+   * </p>
+   * @param deltaKeys
+   */
+  public void setDeltaKeys(Set<Map<String, Object>> deltaKeys) ;
 
 }
diff --git a/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImportHandler.java b/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImportHandler.java
index bf2fb99..a2cc5d0 100644
--- a/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImportHandler.java
+++ b/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImportHandler.java
@@ -178,7 +178,7 @@ public class DataImportHandler extends RequestHandlerBase implements
             rsp.add("mode", "debug");
             rsp.add("documents", requestParams.getDebugInfo().debugDocuments);
             if (requestParams.getDebugInfo().debugVerboseOutput != null) {
-            	rsp.add("verbose-output", requestParams.getDebugInfo().debugVerboseOutput);
+              rsp.add("verbose-output", requestParams.getDebugInfo().debugVerboseOutput);
             }
           } else {
             message = DataImporter.MSG.DEBUG_NOT_ENABLED;
diff --git a/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImporter.java b/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImporter.java
index 901bb9e..baaa5dd 100644
--- a/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImporter.java
+++ b/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImporter.java
@@ -393,7 +393,7 @@ public class DataImporter {
   }
 
   private void checkWritablePersistFile(SolrWriter writer) {
-//  	File persistFile = propWriter.getPersistFile();
+//    File persistFile = propWriter.getPersistFile();
 //    boolean isWritable = persistFile.exists() ? persistFile.canWrite() : persistFile.getParentFile().canWrite();
     if (isDeltaImportSupported && !propWriter.isWritable()) {
       throw new DataImportHandlerException(SEVERE,
diff --git a/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DebugLogger.java b/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DebugLogger.java
index d3bad1f..a86556b 100644
--- a/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DebugLogger.java
+++ b/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DebugLogger.java
@@ -143,7 +143,7 @@ class DebugLogger {
 
   private void popAllTransformers() {
     while (true) {
-    	DIHLogLevels type = debugStack.peek().type;
+      DIHLogLevels type = debugStack.peek().type;
       if (type == DIHLogLevels.START_DOC || type == DIHLogLevels.START_ENTITY)
         break;
       debugStack.pop();
diff --git a/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DocBuilder.java b/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DocBuilder.java
index 7d904e3..9cfe89d 100644
--- a/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DocBuilder.java
+++ b/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DocBuilder.java
@@ -89,17 +89,17 @@ public class DocBuilder {
     
     String writerClassStr = null;
     if(reqParams!=null && reqParams.getRawParams() != null) {
-    	writerClassStr = (String) reqParams.getRawParams().get(PARAM_WRITER_IMPL);
+      writerClassStr = (String) reqParams.getRawParams().get(PARAM_WRITER_IMPL);
     }
     if(writerClassStr != null && !writerClassStr.equals(DEFAULT_WRITER_NAME) && !writerClassStr.equals(DocBuilder.class.getPackage().getName() + "." + DEFAULT_WRITER_NAME)) {
-    	try {
-    		Class<DIHWriter> writerClass = loadClass(writerClassStr, dataImporter.getCore());
-    		this.writer = writerClass.newInstance();
-    	} catch (Exception e) {
-    		throw new DataImportHandlerException(DataImportHandlerException.SEVERE, "Unable to load Writer implementation:" + writerClassStr, e);
-    	}
-   	} else {
-    	writer = solrWriter;
+      try {
+        Class<DIHWriter> writerClass = loadClass(writerClassStr, dataImporter.getCore());
+        this.writer = writerClass.newInstance();
+      } catch (Exception e) {
+        throw new DataImportHandlerException(DataImportHandlerException.SEVERE, "Unable to load Writer implementation:" + writerClassStr, e);
+      }
+     } else {
+      writer = solrWriter;
     }
     ContextImpl ctx = new ContextImpl(null, null, null, null, reqParams.getRawParams(), null, this);
     writer.init(ctx);
@@ -178,111 +178,111 @@ public class DocBuilder {
   @SuppressWarnings("unchecked")
   public void execute() {
     List<EntityProcessorWrapper> epwList = null;
-  	try {
-	    dataImporter.store(DataImporter.STATUS_MSGS, statusMessages);
-	    config = dataImporter.getConfig();
-	    final AtomicLong startTime = new AtomicLong(System.currentTimeMillis());
-	    statusMessages.put(TIME_ELAPSED, new Object() {
-	      @Override
-	      public String toString() {
-	        return getTimeElapsedSince(startTime.get());
-	      }
-	    });
-	
-	    statusMessages.put(DataImporter.MSG.TOTAL_QUERIES_EXECUTED,
-	            importStatistics.queryCount);
-	    statusMessages.put(DataImporter.MSG.TOTAL_ROWS_EXECUTED,
-	            importStatistics.rowsCount);
-	    statusMessages.put(DataImporter.MSG.TOTAL_DOC_PROCESSED,
-	            importStatistics.docCount);
-	    statusMessages.put(DataImporter.MSG.TOTAL_DOCS_SKIPPED,
-	            importStatistics.skipDocCount);
-	
-	    List<String> entities = reqParams.getEntitiesToRun();
-	
-	    // Trigger onImportStart
-	    if (config.getOnImportStart() != null) {
-	      invokeEventListener(config.getOnImportStart());
-	    }
-	    AtomicBoolean fullCleanDone = new AtomicBoolean(false);
-	    //we must not do a delete of *:* multiple times if there are multiple root entities to be run
-	    Properties lastIndexTimeProps = new Properties();
-	    lastIndexTimeProps.setProperty(LAST_INDEX_KEY,
-	            DataImporter.DATE_TIME_FORMAT.get().format(dataImporter.getIndexStartTime()));
-	    
-	    epwList = new ArrayList<EntityProcessorWrapper>(config.getEntities().size());
-	    for (Entity e : config.getEntities()) {
-	      epwList.add(getEntityProcessorWrapper(e));
-	    }	    
-	    for (EntityProcessorWrapper epw : epwList) {
-	      if (entities != null && !entities.contains(epw.getEntity().getName()))
-	        continue;
-	      lastIndexTimeProps.setProperty(epw.getEntity().getName() + "." + LAST_INDEX_KEY,
-	              DataImporter.DATE_TIME_FORMAT.get().format(new Date()));
-	      currentEntityProcessorWrapper = epw;
-	      String delQuery = epw.getEntity().getAllAttributes().get("preImportDeleteQuery");
-	      if (dataImporter.getStatus() == DataImporter.Status.RUNNING_DELTA_DUMP) {
-	        cleanByQuery(delQuery, fullCleanDone);
-	        doDelta();
-	        delQuery = epw.getEntity().getAllAttributes().get("postImportDeleteQuery");
-	        if (delQuery != null) {
-	          fullCleanDone.set(false);
-	          cleanByQuery(delQuery, fullCleanDone);
-	        }
-	      } else {
-	        cleanByQuery(delQuery, fullCleanDone);
-	        doFullDump();
-	        delQuery = epw.getEntity().getAllAttributes().get("postImportDeleteQuery");
-	        if (delQuery != null) {
-	          fullCleanDone.set(false);
-	          cleanByQuery(delQuery, fullCleanDone);
-	        }
-	      }
-	      statusMessages.remove(DataImporter.MSG.TOTAL_DOC_PROCESSED);
-	    }
-	
-	    if (stop.get()) {
-	      // Dont commit if aborted using command=abort
-	      statusMessages.put("Aborted", DataImporter.DATE_TIME_FORMAT.get().format(new Date()));
-	      rollback();
-	    } else {
-	      // Do not commit unnecessarily if this is a delta-import and no documents were created or deleted
-	      if (!reqParams.isClean()) {
-	        if (importStatistics.docCount.get() > 0 || importStatistics.deletedDocCount.get() > 0) {
-	          finish(lastIndexTimeProps);
-	        }
-	      } else {
-	        // Finished operation normally, commit now
-	        finish(lastIndexTimeProps);
-	      } 
-	      
-	      if (config.getOnImportEnd() != null) {
-	        invokeEventListener(config.getOnImportEnd());
-	      }
-	    }
-	
-	    statusMessages.remove(TIME_ELAPSED);
-	    statusMessages.put(DataImporter.MSG.TOTAL_DOC_PROCESSED, ""+ importStatistics.docCount.get());
-	    if(importStatistics.failedDocCount.get() > 0)
-	      statusMessages.put(DataImporter.MSG.TOTAL_FAILED_DOCS, ""+ importStatistics.failedDocCount.get());
-	
-	    statusMessages.put("Time taken", getTimeElapsedSince(startTime.get()));
-	    LOG.info("Time taken = " + getTimeElapsedSince(startTime.get()));
-	  } catch(Exception e)
-		{
-			throw new RuntimeException(e);
-		} finally
-		{
-			if (writer != null) {
-	      writer.close();
-	    }
-			if (epwList != null) {
-			  closeEntityProcessorWrappers(epwList);
-			}
-			if(reqParams.isDebug()) {
-				reqParams.getDebugInfo().debugVerboseOutput = getDebugLogger().output;	
-			}
-		}
+    try {
+      dataImporter.store(DataImporter.STATUS_MSGS, statusMessages);
+      config = dataImporter.getConfig();
+      final AtomicLong startTime = new AtomicLong(System.currentTimeMillis());
+      statusMessages.put(TIME_ELAPSED, new Object() {
+        @Override
+        public String toString() {
+          return getTimeElapsedSince(startTime.get());
+        }
+      });
+
+      statusMessages.put(DataImporter.MSG.TOTAL_QUERIES_EXECUTED,
+              importStatistics.queryCount);
+      statusMessages.put(DataImporter.MSG.TOTAL_ROWS_EXECUTED,
+              importStatistics.rowsCount);
+      statusMessages.put(DataImporter.MSG.TOTAL_DOC_PROCESSED,
+              importStatistics.docCount);
+      statusMessages.put(DataImporter.MSG.TOTAL_DOCS_SKIPPED,
+              importStatistics.skipDocCount);
+
+      List<String> entities = reqParams.getEntitiesToRun();
+
+      // Trigger onImportStart
+      if (config.getOnImportStart() != null) {
+        invokeEventListener(config.getOnImportStart());
+      }
+      AtomicBoolean fullCleanDone = new AtomicBoolean(false);
+      //we must not do a delete of *:* multiple times if there are multiple root entities to be run
+      Properties lastIndexTimeProps = new Properties();
+      lastIndexTimeProps.setProperty(LAST_INDEX_KEY,
+              DataImporter.DATE_TIME_FORMAT.get().format(dataImporter.getIndexStartTime()));
+
+      epwList = new ArrayList<EntityProcessorWrapper>(config.getEntities().size());
+      for (Entity e : config.getEntities()) {
+        epwList.add(getEntityProcessorWrapper(e));
+      }
+      for (EntityProcessorWrapper epw : epwList) {
+        if (entities != null && !entities.contains(epw.getEntity().getName()))
+          continue;
+        lastIndexTimeProps.setProperty(epw.getEntity().getName() + "." + LAST_INDEX_KEY,
+                DataImporter.DATE_TIME_FORMAT.get().format(new Date()));
+        currentEntityProcessorWrapper = epw;
+        String delQuery = epw.getEntity().getAllAttributes().get("preImportDeleteQuery");
+        if (dataImporter.getStatus() == DataImporter.Status.RUNNING_DELTA_DUMP) {
+          cleanByQuery(delQuery, fullCleanDone);
+          doDelta();
+          delQuery = epw.getEntity().getAllAttributes().get("postImportDeleteQuery");
+          if (delQuery != null) {
+            fullCleanDone.set(false);
+            cleanByQuery(delQuery, fullCleanDone);
+          }
+        } else {
+          cleanByQuery(delQuery, fullCleanDone);
+          doFullDump();
+          delQuery = epw.getEntity().getAllAttributes().get("postImportDeleteQuery");
+          if (delQuery != null) {
+            fullCleanDone.set(false);
+            cleanByQuery(delQuery, fullCleanDone);
+          }
+        }
+        statusMessages.remove(DataImporter.MSG.TOTAL_DOC_PROCESSED);
+      }
+
+      if (stop.get()) {
+        // Dont commit if aborted using command=abort
+        statusMessages.put("Aborted", DataImporter.DATE_TIME_FORMAT.get().format(new Date()));
+        rollback();
+      } else {
+        // Do not commit unnecessarily if this is a delta-import and no documents were created or deleted
+        if (!reqParams.isClean()) {
+          if (importStatistics.docCount.get() > 0 || importStatistics.deletedDocCount.get() > 0) {
+            finish(lastIndexTimeProps);
+          }
+        } else {
+          // Finished operation normally, commit now
+          finish(lastIndexTimeProps);
+        }
+
+        if (config.getOnImportEnd() != null) {
+          invokeEventListener(config.getOnImportEnd());
+        }
+      }
+
+      statusMessages.remove(TIME_ELAPSED);
+      statusMessages.put(DataImporter.MSG.TOTAL_DOC_PROCESSED, ""+ importStatistics.docCount.get());
+      if(importStatistics.failedDocCount.get() > 0)
+        statusMessages.put(DataImporter.MSG.TOTAL_FAILED_DOCS, ""+ importStatistics.failedDocCount.get());
+
+      statusMessages.put("Time taken", getTimeElapsedSince(startTime.get()));
+      LOG.info("Time taken = " + getTimeElapsedSince(startTime.get()));
+    } catch(Exception e)
+    {
+      throw new RuntimeException(e);
+    } finally
+    {
+      if (writer != null) {
+        writer.close();
+      }
+      if (epwList != null) {
+        closeEntityProcessorWrappers(epwList);
+      }
+      if(reqParams.isDebug()) {
+        reqParams.getDebugInfo().debugVerboseOutput = getDebugLogger().output;
+      }
+    }
   }
   private void closeEntityProcessorWrappers(List<EntityProcessorWrapper> epwList) {
     for(EntityProcessorWrapper epw : epwList) {
@@ -506,7 +506,7 @@ public class DocBuilder {
             if (!doc.isEmpty()) {
               boolean result = writer.upload(doc);
               if(reqParams.isDebug()) {
-              	reqParams.getDebugInfo().debugDocuments.add(doc);
+                reqParams.getDebugInfo().debugDocuments.add(doc);
               }
               doc = null;
               if (result){
diff --git a/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/EntityProcessorBase.java b/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/EntityProcessorBase.java
index 330c58f..1aa882e 100644
--- a/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/EntityProcessorBase.java
+++ b/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/EntityProcessorBase.java
@@ -133,11 +133,11 @@ public class EntityProcessorBase extends EntityProcessor {
 
   @Override
   public void destroy() {
-  	query = null;
-  	if(cacheSupport!=null){
-  	  cacheSupport.destroyAll();
-  	}
-  	cacheSupport = null;
+    query = null;
+    if(cacheSupport!=null){
+      cacheSupport.destroyAll();
+    }
+    cacheSupport = null;
   }
 
   
diff --git a/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/SimplePropertiesWriter.java b/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/SimplePropertiesWriter.java
index decbabc..dcdb4b5 100644
--- a/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/SimplePropertiesWriter.java
+++ b/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/SimplePropertiesWriter.java
@@ -30,15 +30,15 @@ import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 public class SimplePropertiesWriter implements DIHPropertiesWriter {
-	private static final Logger log = LoggerFactory.getLogger(SimplePropertiesWriter.class);
+  private static final Logger log = LoggerFactory.getLogger(SimplePropertiesWriter.class);
 
-	static final String IMPORTER_PROPERTIES = "dataimport.properties";
+  static final String IMPORTER_PROPERTIES = "dataimport.properties";
 
-	static final String LAST_INDEX_KEY = "last_index_time";
+  static final String LAST_INDEX_KEY = "last_index_time";
 
-	private String persistFilename = IMPORTER_PROPERTIES;
+  private String persistFilename = IMPORTER_PROPERTIES;
 
-	private String configDir = null;
+  private String configDir = null;
 
 
 
@@ -48,15 +48,15 @@ public class SimplePropertiesWriter implements DIHPropertiesWriter {
       String persistFileName = dataImporter.getHandlerName();
 
       this.configDir = configDir;
-	  if(persistFileName != null){
+    if(persistFileName != null){
         persistFilename = persistFileName + ".properties";
       }
     }
 
 
 
-	
-	private File getPersistFile() {
+
+  private File getPersistFile() {
     String filePath = configDir;
     if (configDir != null && !configDir.endsWith(File.separator))
       filePath += File.separator;
@@ -71,53 +71,53 @@ public class SimplePropertiesWriter implements DIHPropertiesWriter {
     }
 
     @Override
-	public void persist(Properties p) {
-		OutputStream propOutput = null;
-
-		Properties props = readIndexerProperties();
-
-		try {
-			props.putAll(p);
-			String filePath = configDir;
-			if (configDir != null && !configDir.endsWith(File.separator))
-				filePath += File.separator;
-			filePath += persistFilename;
-			propOutput = new FileOutputStream(filePath);
-			props.store(propOutput, null);
-			log.info("Wrote last indexed time to " + persistFilename);
-		} catch (Exception e) {
-			throw new DataImportHandlerException(DataImportHandlerException.SEVERE, "Unable to persist Index Start Time", e);
-		} finally {
-			try {
-				if (propOutput != null)
-					propOutput.close();
-			} catch (IOException e) {
-				propOutput = null;
-			}
-		}
-	}
-
-	@Override
-	public Properties readIndexerProperties() {
-		Properties props = new Properties();
-		InputStream propInput = null;
-
-		try {
-			propInput = new FileInputStream(configDir + persistFilename);
-			props.load(propInput);
-			log.info("Read " + persistFilename);
-		} catch (Exception e) {
-			log.warn("Unable to read: " + persistFilename);
-		} finally {
-			try {
-				if (propInput != null)
-					propInput.close();
-			} catch (IOException e) {
-				propInput = null;
-			}
-		}
-
-		return props;
-	}
+  public void persist(Properties p) {
+    OutputStream propOutput = null;
+
+    Properties props = readIndexerProperties();
+
+    try {
+      props.putAll(p);
+      String filePath = configDir;
+      if (configDir != null && !configDir.endsWith(File.separator))
+        filePath += File.separator;
+      filePath += persistFilename;
+      propOutput = new FileOutputStream(filePath);
+      props.store(propOutput, null);
+      log.info("Wrote last indexed time to " + persistFilename);
+    } catch (Exception e) {
+      throw new DataImportHandlerException(DataImportHandlerException.SEVERE, "Unable to persist Index Start Time", e);
+    } finally {
+      try {
+        if (propOutput != null)
+          propOutput.close();
+      } catch (IOException e) {
+        propOutput = null;
+      }
+    }
+  }
+
+  @Override
+  public Properties readIndexerProperties() {
+    Properties props = new Properties();
+    InputStream propInput = null;
+
+    try {
+      propInput = new FileInputStream(configDir + persistFilename);
+      props.load(propInput);
+      log.info("Read " + persistFilename);
+    } catch (Exception e) {
+      log.warn("Unable to read: " + persistFilename);
+    } finally {
+      try {
+        if (propInput != null)
+          propInput.close();
+      } catch (IOException e) {
+        propInput = null;
+      }
+    }
+
+    return props;
+  }
 
 }
diff --git a/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/SolrWriter.java b/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/SolrWriter.java
index 7df6e26..e9c7b2a 100644
--- a/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/SolrWriter.java
+++ b/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/SolrWriter.java
@@ -54,12 +54,12 @@ public class SolrWriter extends DIHWriterBase implements DIHWriter {
   
   @Override
   public void close() {
-  	try {
-  		processor.finish();
-  	} catch (IOException e) {
-  		throw new DataImportHandlerException(DataImportHandlerException.SEVERE,
-  				"Unable to call finish() on UpdateRequestProcessor", e);
-  	}
+    try {
+      processor.finish();
+    } catch (IOException e) {
+      throw new DataImportHandlerException(DataImportHandlerException.SEVERE,
+          "Unable to call finish() on UpdateRequestProcessor", e);
+    }
   }
   @Override
   public boolean upload(SolrInputDocument d) {
@@ -87,8 +87,8 @@ public class SolrWriter extends DIHWriterBase implements DIHWriter {
       log.error("Exception while deleteing: " + id, e);
     }
   }
-  	
-	@Override
+
+  @Override
   public void deleteByQuery(String query) {
     try {
       log.info("Deleting documents from Solr with query: " + query);
@@ -100,7 +100,7 @@ public class SolrWriter extends DIHWriterBase implements DIHWriter {
     }
   }
 
-	@Override
+  @Override
   public void commit(boolean optimize) {
     try {
       CommitUpdateCommand commit = new CommitUpdateCommand(req,optimize);
@@ -110,7 +110,7 @@ public class SolrWriter extends DIHWriterBase implements DIHWriter {
     }
   }
 
-	@Override
+  @Override
   public void rollback() {
     try {
       RollbackUpdateCommand rollback = new RollbackUpdateCommand(req);
@@ -120,7 +120,7 @@ public class SolrWriter extends DIHWriterBase implements DIHWriter {
     }
   }
 
-	@Override
+  @Override
   public void doDeleteAll() {
     try {
       DeleteUpdateCommand deleteCommand = new DeleteUpdateCommand(req);
@@ -158,8 +158,8 @@ public class SolrWriter extends DIHWriterBase implements DIHWriter {
       return null;
     }
   }
-	@Override
-	public void init(Context context) {
-		/* NO-OP */		
-	}	
+  @Override
+  public void init(Context context) {
+    /* NO-OP */
+  }
 }
diff --git a/solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/AbstractDIHCacheTestCase.java b/solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/AbstractDIHCacheTestCase.java
index 23682d8..2d6648c 100644
--- a/solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/AbstractDIHCacheTestCase.java
+++ b/solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/AbstractDIHCacheTestCase.java
@@ -38,197 +38,197 @@ import org.junit.After;
 import org.junit.Assert;
 import org.junit.Before;
 
-public class AbstractDIHCacheTestCase {	
-	protected static final Date Feb21_2011 = new Date(1298268000000l);
-	protected final String[] fieldTypes = { "INTEGER", "BIGDECIMAL", "STRING", "STRING",   "FLOAT",   "DATE",   "CLOB" };
-	protected final String[] fieldNames = { "a_id",    "PI",         "letter", "examples", "a_float", "a_date", "DESCRIPTION" };
-	protected List<ControlData> data = new ArrayList<ControlData>();
-	protected Clob APPLE = null;
-	
-	@Before
-	public void setup() {
-		try {
-			APPLE = new SerialClob(new String("Apples grow on trees and they are good to eat.").toCharArray());
-		} catch (SQLException sqe) {
-			Assert.fail("Could not Set up Test");
-		}
-
-		// The first row needs to have all non-null fields,
-		// otherwise we would have to always send the fieldTypes & fieldNames as CacheProperties when building.
-		data = new ArrayList<ControlData>();
-		data.add(new ControlData(new Object[] { new Integer(1), new BigDecimal(Math.PI), "A", "Apple", new Float(1.11), Feb21_2011, APPLE }));
-		data.add(new ControlData(new Object[] { new Integer(2), new BigDecimal(Math.PI), "B", "Ball", new Float(2.22), Feb21_2011, null }));
-		data.add(new ControlData(new Object[] { new Integer(4), new BigDecimal(Math.PI), "D", "Dog", new Float(4.44), Feb21_2011, null }));
-		data.add(new ControlData(new Object[] { new Integer(3), new BigDecimal(Math.PI), "C", "Cookie", new Float(3.33), Feb21_2011, null }));
-		data.add(new ControlData(new Object[] { new Integer(4), new BigDecimal(Math.PI), "D", "Daisy", new Float(4.44), Feb21_2011, null }));
-		data.add(new ControlData(new Object[] { new Integer(4), new BigDecimal(Math.PI), "D", "Drawing", new Float(4.44), Feb21_2011, null }));
-		data.add(new ControlData(new Object[] { new Integer(5), new BigDecimal(Math.PI), "E",
-				Arrays.asList("Eggplant", "Ear", "Elephant", "Engine"), new Float(5.55), Feb21_2011, null }));
-	}
-
-	@After
-	public void teardown() {
-		APPLE = null;
-		data = null;
-	}
-	
-	//A limitation of this test class is that the primary key needs to be the first one in the list.
-	//DIHCaches, however, can handle any field being the primary key.
-	class ControlData implements Comparable<ControlData>, Iterable<Object> {
-		Object[] data;
-
-		ControlData(Object[] data) {
-			this.data = data;
-		}
-
-		@SuppressWarnings("unchecked")
-		public int compareTo(ControlData cd) {
-			Comparable c1 = (Comparable) data[0];
-			Comparable c2 = (Comparable) cd.data[0];
-			return c1.compareTo(c2);
-		}
-
-		public Iterator<Object> iterator() {
-			return Arrays.asList(data).iterator();
-		}
-	}
-	
-	protected void loadData(DIHCache cache, List<ControlData> theData, String[] theFieldNames, boolean keepOrdered) {
-		for (ControlData cd : theData) {
-			cache.add(controlDataToMap(cd, theFieldNames, keepOrdered));
-		}
-	}
-
-	protected List<ControlData> extractDataInKeyOrder(DIHCache cache, String[] theFieldNames) {
-		List<Object[]> data = new ArrayList<Object[]>();
-		Iterator<Map<String, Object>> cacheIter = cache.iterator();
-		while (cacheIter.hasNext()) {
-			data.add(mapToObjectArray(cacheIter.next(), theFieldNames));
-		}
-		return listToControlData(data);
-	}
-
-	//This method assumes that the Primary Keys are integers and that the first id=1.  
-	//It will look for id's sequentially until one is skipped, then will stop.
-	protected List<ControlData> extractDataByKeyLookup(DIHCache cache, String[] theFieldNames) {
-		int recId = 1;
-		List<Object[]> data = new ArrayList<Object[]>();
-		while (true) {
-			Iterator<Map<String, Object>> listORecs = cache.iterator(recId);
-			if (listORecs == null) {
-				break;
-			}
-
-			while(listORecs.hasNext()) {
-				data.add(mapToObjectArray(listORecs.next(), theFieldNames));
-			}
-			recId++;
-		}
-		return listToControlData(data);
-	}
-
-	protected List<ControlData> listToControlData(List<Object[]> data) {
-		List<ControlData> returnData = new ArrayList<ControlData>(data.size());
-		for (int i = 0; i < data.size(); i++) {
-			returnData.add(new ControlData(data.get(i)));
-		}
-		return returnData;
-	}
-
-	protected Object[] mapToObjectArray(Map<String, Object> rec, String[] theFieldNames) {
-		Object[] oos = new Object[theFieldNames.length];
-		for (int i = 0; i < theFieldNames.length; i++) {
-			oos[i] = rec.get(theFieldNames[i]);
-		}
-		return oos;
-	}
-
-	protected void compareData(List<ControlData> theControl, List<ControlData> test) {
-		// The test data should come back primarily in Key order and secondarily in insertion order.
-		List<ControlData> control = new ArrayList<ControlData>(theControl);
-		Collections.sort(control);
-
-		StringBuilder errors = new StringBuilder();
-		if (test.size() != control.size()) {
-			errors.append("-Returned data has " + test.size() + " records.  expected: " + control.size() + "\n");
-		}
-		for (int i = 0; i < control.size() && i < test.size(); i++) {
-			Object[] controlRec = control.get(i).data;
-			Object[] testRec = test.get(i).data;
-			if (testRec.length != controlRec.length) {
-				errors.append("-Record indexAt=" + i + " has " + testRec.length + " data elements.  extpected: " + controlRec.length + "\n");
-			}
-			for (int j = 0; j < controlRec.length && j < testRec.length; j++) {
-				Object controlObj = controlRec[j];
-				Object testObj = testRec[j];
-				if (controlObj == null && testObj != null) {
-					errors.append("-Record indexAt=" + i + ", Data Element indexAt=" + j + " is not NULL as expected.\n");
-				} else if (controlObj != null && testObj == null) {
-					errors.append("-Record indexAt=" + i + ", Data Element indexAt=" + j + " is NULL.  Expected: " + controlObj + " (class="
-							+ controlObj.getClass().getName() + ")\n");
-				} else if (controlObj != null && testObj != null && controlObj instanceof Clob) {
-					String controlString = clobToString((Clob) controlObj);
-					String testString = clobToString((Clob) testObj);
-					if (!controlString.equals(testString)) {
-						errors.append("-Record indexAt=" + i + ", Data Element indexAt=" + j + " has: " + testString + " (class=Clob) ... Expected: " + controlString
-								+ " (class=Clob)\n");
-					}
-				} else if (controlObj != null && !controlObj.equals(testObj)) {
-					errors.append("-Record indexAt=" + i + ", Data Element indexAt=" + j + " has: " + testObj + " (class=" + testObj.getClass().getName()
-							+ ") ... Expected: " + controlObj + " (class=" + controlObj.getClass().getName() + ")\n");
-				}
-			}
-		}
-		if (errors.length() > 0) {
-			Assert.fail(errors.toString());
-		}
-	}
-
-	protected Map<String, Object> controlDataToMap(ControlData cd, String[] theFieldNames, boolean keepOrdered) {
-		Map<String, Object> rec = null;
-		if (keepOrdered) {
-			rec = new LinkedHashMap<String, Object>();
-		} else {
-			rec = new HashMap<String, Object>();
-		}
-		for (int i = 0; i < cd.data.length; i++) {
-			String fieldName = theFieldNames[i];
-			Object data = cd.data[i];
-			rec.put(fieldName, data);
-		}
-		return rec;
-	}
-
-	protected String stringArrayToCommaDelimitedList(String[] strs) {
-		StringBuilder sb = new StringBuilder();
-		for (String a : strs) {
-			if (sb.length() > 0) {
-				sb.append(",");
-			}
-			sb.append(a);
-		}
-		return sb.toString();
-	}
-
-	protected String clobToString(Clob cl) {
-		StringBuilder sb = new StringBuilder();
-		try {
-			Reader in = cl.getCharacterStream();
-			char[] cbuf = new char[1024];
-			int numGot = -1;
-			while ((numGot = in.read(cbuf)) != -1) {
-				sb.append(String.valueOf(cbuf, 0, numGot));
-			}
-		} catch (Exception e) {
-			Assert.fail(e.toString());
-		}
-		return sb.toString();
-	}
-	
-	public static Context getContext(final Map<String, String> entityAttrs) {
-		VariableResolverImpl resolver = new VariableResolverImpl();
+public class AbstractDIHCacheTestCase {
+  protected static final Date Feb21_2011 = new Date(1298268000000l);
+  protected final String[] fieldTypes = { "INTEGER", "BIGDECIMAL", "STRING", "STRING",   "FLOAT",   "DATE",   "CLOB" };
+  protected final String[] fieldNames = { "a_id",    "PI",         "letter", "examples", "a_float", "a_date", "DESCRIPTION" };
+  protected List<ControlData> data = new ArrayList<ControlData>();
+  protected Clob APPLE = null;
+
+  @Before
+  public void setup() {
+    try {
+      APPLE = new SerialClob(new String("Apples grow on trees and they are good to eat.").toCharArray());
+    } catch (SQLException sqe) {
+      Assert.fail("Could not Set up Test");
+    }
+
+    // The first row needs to have all non-null fields,
+    // otherwise we would have to always send the fieldTypes & fieldNames as CacheProperties when building.
+    data = new ArrayList<ControlData>();
+    data.add(new ControlData(new Object[] { new Integer(1), new BigDecimal(Math.PI), "A", "Apple", new Float(1.11), Feb21_2011, APPLE }));
+    data.add(new ControlData(new Object[] { new Integer(2), new BigDecimal(Math.PI), "B", "Ball", new Float(2.22), Feb21_2011, null }));
+    data.add(new ControlData(new Object[] { new Integer(4), new BigDecimal(Math.PI), "D", "Dog", new Float(4.44), Feb21_2011, null }));
+    data.add(new ControlData(new Object[] { new Integer(3), new BigDecimal(Math.PI), "C", "Cookie", new Float(3.33), Feb21_2011, null }));
+    data.add(new ControlData(new Object[] { new Integer(4), new BigDecimal(Math.PI), "D", "Daisy", new Float(4.44), Feb21_2011, null }));
+    data.add(new ControlData(new Object[] { new Integer(4), new BigDecimal(Math.PI), "D", "Drawing", new Float(4.44), Feb21_2011, null }));
+    data.add(new ControlData(new Object[] { new Integer(5), new BigDecimal(Math.PI), "E",
+        Arrays.asList("Eggplant", "Ear", "Elephant", "Engine"), new Float(5.55), Feb21_2011, null }));
+  }
+
+  @After
+  public void teardown() {
+    APPLE = null;
+    data = null;
+  }
+
+  //A limitation of this test class is that the primary key needs to be the first one in the list.
+  //DIHCaches, however, can handle any field being the primary key.
+  class ControlData implements Comparable<ControlData>, Iterable<Object> {
+    Object[] data;
+
+    ControlData(Object[] data) {
+      this.data = data;
+    }
+
+    @SuppressWarnings("unchecked")
+    public int compareTo(ControlData cd) {
+      Comparable c1 = (Comparable) data[0];
+      Comparable c2 = (Comparable) cd.data[0];
+      return c1.compareTo(c2);
+    }
+
+    public Iterator<Object> iterator() {
+      return Arrays.asList(data).iterator();
+    }
+  }
+
+  protected void loadData(DIHCache cache, List<ControlData> theData, String[] theFieldNames, boolean keepOrdered) {
+    for (ControlData cd : theData) {
+      cache.add(controlDataToMap(cd, theFieldNames, keepOrdered));
+    }
+  }
+
+  protected List<ControlData> extractDataInKeyOrder(DIHCache cache, String[] theFieldNames) {
+    List<Object[]> data = new ArrayList<Object[]>();
+    Iterator<Map<String, Object>> cacheIter = cache.iterator();
+    while (cacheIter.hasNext()) {
+      data.add(mapToObjectArray(cacheIter.next(), theFieldNames));
+    }
+    return listToControlData(data);
+  }
+
+  //This method assumes that the Primary Keys are integers and that the first id=1.
+  //It will look for id's sequentially until one is skipped, then will stop.
+  protected List<ControlData> extractDataByKeyLookup(DIHCache cache, String[] theFieldNames) {
+    int recId = 1;
+    List<Object[]> data = new ArrayList<Object[]>();
+    while (true) {
+      Iterator<Map<String, Object>> listORecs = cache.iterator(recId);
+      if (listORecs == null) {
+        break;
+      }
+
+      while(listORecs.hasNext()) {
+        data.add(mapToObjectArray(listORecs.next(), theFieldNames));
+      }
+      recId++;
+    }
+    return listToControlData(data);
+  }
+
+  protected List<ControlData> listToControlData(List<Object[]> data) {
+    List<ControlData> returnData = new ArrayList<ControlData>(data.size());
+    for (int i = 0; i < data.size(); i++) {
+      returnData.add(new ControlData(data.get(i)));
+    }
+    return returnData;
+  }
+
+  protected Object[] mapToObjectArray(Map<String, Object> rec, String[] theFieldNames) {
+    Object[] oos = new Object[theFieldNames.length];
+    for (int i = 0; i < theFieldNames.length; i++) {
+      oos[i] = rec.get(theFieldNames[i]);
+    }
+    return oos;
+  }
+
+  protected void compareData(List<ControlData> theControl, List<ControlData> test) {
+    // The test data should come back primarily in Key order and secondarily in insertion order.
+    List<ControlData> control = new ArrayList<ControlData>(theControl);
+    Collections.sort(control);
+
+    StringBuilder errors = new StringBuilder();
+    if (test.size() != control.size()) {
+      errors.append("-Returned data has " + test.size() + " records.  expected: " + control.size() + "\n");
+    }
+    for (int i = 0; i < control.size() && i < test.size(); i++) {
+      Object[] controlRec = control.get(i).data;
+      Object[] testRec = test.get(i).data;
+      if (testRec.length != controlRec.length) {
+        errors.append("-Record indexAt=" + i + " has " + testRec.length + " data elements.  extpected: " + controlRec.length + "\n");
+      }
+      for (int j = 0; j < controlRec.length && j < testRec.length; j++) {
+        Object controlObj = controlRec[j];
+        Object testObj = testRec[j];
+        if (controlObj == null && testObj != null) {
+          errors.append("-Record indexAt=" + i + ", Data Element indexAt=" + j + " is not NULL as expected.\n");
+        } else if (controlObj != null && testObj == null) {
+          errors.append("-Record indexAt=" + i + ", Data Element indexAt=" + j + " is NULL.  Expected: " + controlObj + " (class="
+              + controlObj.getClass().getName() + ")\n");
+        } else if (controlObj != null && testObj != null && controlObj instanceof Clob) {
+          String controlString = clobToString((Clob) controlObj);
+          String testString = clobToString((Clob) testObj);
+          if (!controlString.equals(testString)) {
+            errors.append("-Record indexAt=" + i + ", Data Element indexAt=" + j + " has: " + testString + " (class=Clob) ... Expected: " + controlString
+                + " (class=Clob)\n");
+          }
+        } else if (controlObj != null && !controlObj.equals(testObj)) {
+          errors.append("-Record indexAt=" + i + ", Data Element indexAt=" + j + " has: " + testObj + " (class=" + testObj.getClass().getName()
+              + ") ... Expected: " + controlObj + " (class=" + controlObj.getClass().getName() + ")\n");
+        }
+      }
+    }
+    if (errors.length() > 0) {
+      Assert.fail(errors.toString());
+    }
+  }
+
+  protected Map<String, Object> controlDataToMap(ControlData cd, String[] theFieldNames, boolean keepOrdered) {
+    Map<String, Object> rec = null;
+    if (keepOrdered) {
+      rec = new LinkedHashMap<String, Object>();
+    } else {
+      rec = new HashMap<String, Object>();
+    }
+    for (int i = 0; i < cd.data.length; i++) {
+      String fieldName = theFieldNames[i];
+      Object data = cd.data[i];
+      rec.put(fieldName, data);
+    }
+    return rec;
+  }
+
+  protected String stringArrayToCommaDelimitedList(String[] strs) {
+    StringBuilder sb = new StringBuilder();
+    for (String a : strs) {
+      if (sb.length() > 0) {
+        sb.append(",");
+      }
+      sb.append(a);
+    }
+    return sb.toString();
+  }
+
+  protected String clobToString(Clob cl) {
+    StringBuilder sb = new StringBuilder();
+    try {
+      Reader in = cl.getCharacterStream();
+      char[] cbuf = new char[1024];
+      int numGot = -1;
+      while ((numGot = in.read(cbuf)) != -1) {
+        sb.append(String.valueOf(cbuf, 0, numGot));
+      }
+    } catch (Exception e) {
+      Assert.fail(e.toString());
+    }
+    return sb.toString();
+  }
+
+  public static Context getContext(final Map<String, String> entityAttrs) {
+    VariableResolverImpl resolver = new VariableResolverImpl();
     final Context delegate = new ContextImpl(null, resolver, null, null, new HashMap<String, Object>(), null, null);
     return new TestContext(entityAttrs, delegate, null, true);
   }
-	
+
 }
diff --git a/solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestSortedMapBackedCache.java b/solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestSortedMapBackedCache.java
index 2cf83f4..023831a 100644
--- a/solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestSortedMapBackedCache.java
+++ b/solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestSortedMapBackedCache.java
@@ -30,56 +30,56 @@ import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 public class TestSortedMapBackedCache extends AbstractDIHCacheTestCase {
-	
-	public static Logger log = LoggerFactory.getLogger(TestSortedMapBackedCache.class);
-	
-	@Test
-	public void testCacheWithKeyLookup() {
-		DIHCache cache = null;
-		try {
-			cache = new SortedMapBackedCache();
-			cache.open(getContext(new HashMap<String,String>()));
-			loadData(cache, data, fieldNames, true);
-			List<ControlData> testData = extractDataByKeyLookup(cache, fieldNames);
-			compareData(data, testData);
-		} catch (Exception e) {
-			log.warn("Exception thrown: " + e.toString());
-			Assert.fail();
-		} finally {
-			try {
-				cache.destroy();
-			} catch (Exception ex) {
-			}
-		}
-	}
-
-	@Test
-	public void testCacheWithOrderedLookup() {
-		DIHCache cache = null;
-		try {
-			cache = new SortedMapBackedCache();
-			cache.open(getContext(new HashMap<String,String>()));
-			loadData(cache, data, fieldNames, true);
-			List<ControlData> testData = extractDataInKeyOrder(cache, fieldNames);
-			compareData(data, testData);
-		} catch (Exception e) {
-			log.warn("Exception thrown: " + e.toString());
-			Assert.fail();
-		} finally {
-			try {
-				cache.destroy();
-			} catch (Exception ex) {
-			}
-		}
-	}
-	
-	@Test
-	public void testNullKeys() throws Exception {
-	  //A null key should just be ignored, but not throw an exception
-	  DIHCache cache = null;
-	  try {
-	    cache = new SortedMapBackedCache();
-	    Map<String, String> cacheProps = new HashMap<String, String>();
+  
+  public static Logger log = LoggerFactory.getLogger(TestSortedMapBackedCache.class);
+  
+  @Test
+  public void testCacheWithKeyLookup() {
+    DIHCache cache = null;
+    try {
+      cache = new SortedMapBackedCache();
+      cache.open(getContext(new HashMap<String,String>()));
+      loadData(cache, data, fieldNames, true);
+      List<ControlData> testData = extractDataByKeyLookup(cache, fieldNames);
+      compareData(data, testData);
+    } catch (Exception e) {
+      log.warn("Exception thrown: " + e.toString());
+      Assert.fail();
+    } finally {
+      try {
+        cache.destroy();
+      } catch (Exception ex) {
+      }
+    }
+  }
+
+  @Test
+  public void testCacheWithOrderedLookup() {
+    DIHCache cache = null;
+    try {
+      cache = new SortedMapBackedCache();
+      cache.open(getContext(new HashMap<String,String>()));
+      loadData(cache, data, fieldNames, true);
+      List<ControlData> testData = extractDataInKeyOrder(cache, fieldNames);
+      compareData(data, testData);
+    } catch (Exception e) {
+      log.warn("Exception thrown: " + e.toString());
+      Assert.fail();
+    } finally {
+      try {
+        cache.destroy();
+      } catch (Exception ex) {
+      }
+    }
+  }
+  
+  @Test
+  public void testNullKeys() throws Exception {
+    //A null key should just be ignored, but not throw an exception
+    DIHCache cache = null;
+    try {
+      cache = new SortedMapBackedCache();
+      Map<String, String> cacheProps = new HashMap<String, String>();
       cacheProps.put(DIHCacheSupport.CACHE_PRIMARY_KEY, "a_id");
       cache.open(getContext(cacheProps));
       
@@ -94,98 +94,98 @@ public class TestSortedMapBackedCache extends AbstractDIHCacheTestCase {
       }
       Assert.assertNull(cache.iterator(null));
       cache.delete(null);      
-	  } catch (Exception e) {
-	    throw e;
+    } catch (Exception e) {
+      throw e;
+    } finally {
+      try {
+        cache.destroy();
+      } catch (Exception ex) {
+      }
+    }    
+  }
+
+  @Test
+  public void testCacheReopensWithUpdate() {
+    DIHCache cache = null;
+    try {      
+      Map<String, String> cacheProps = new HashMap<String, String>();
+      cacheProps.put(DIHCacheSupport.CACHE_PRIMARY_KEY, "a_id");
+      
+      cache = new SortedMapBackedCache();
+      cache.open(getContext(cacheProps));
+      // We can let the data hit the cache with the fields out of order because
+      // we've identified the pk up-front.
+      loadData(cache, data, fieldNames, false);
+
+      // Close the cache.
+      cache.close();
+
+      List<ControlData> newControlData = new ArrayList<ControlData>();
+      Object[] newIdEqualsThree = null;
+      int j = 0;
+      for (int i = 0; i < data.size(); i++) {
+        // We'll be deleting a_id=1 so remove it from the control data.
+        if (data.get(i).data[0].equals(new Integer(1))) {
+          continue;
+        }
+
+        // We'll be changing "Cookie" to "Carrot" in a_id=3 so change it in the control data.
+        if (data.get(i).data[0].equals(new Integer(3))) {
+          newIdEqualsThree = new Object[data.get(i).data.length];
+          System.arraycopy(data.get(i).data, 0, newIdEqualsThree, 0, newIdEqualsThree.length);
+          newIdEqualsThree[3] = "Carrot";
+          newControlData.add(new ControlData(newIdEqualsThree));
+        }
+        // Everything else can just be copied over.
+        else {
+          newControlData.add(data.get(i));
+        }
+
+        j++;
+      }
+
+      // These new rows of data will get added to the cache, so add them to the control data too.
+      Object[] newDataRow1 = new Object[] { new Integer(99), new BigDecimal(Math.PI), "Z", "Zebra", new Float(99.99), Feb21_2011, null };
+      Object[] newDataRow2 = new Object[] { new Integer(2), new BigDecimal(Math.PI), "B", "Ballerina", new Float(2.22), Feb21_2011, null };
+
+      newControlData.add(new ControlData(newDataRow1));
+      newControlData.add(new ControlData(newDataRow2));
+
+      // Re-open the cache
+      cache.open(getContext(new HashMap<String,String>()));
+
+      // Delete a_id=1 from the cache.
+      cache.delete(new Integer(1));
+
+      // Because the cache allows duplicates, the only way to update is to
+      // delete first then add.
+      cache.delete(new Integer(3));
+      cache.add(controlDataToMap(new ControlData(newIdEqualsThree), fieldNames, false));
+
+      // Add this row with a new Primary key.
+      cache.add(controlDataToMap(new ControlData(newDataRow1), fieldNames, false));
+
+      // Add this row, creating two records in the cache with a_id=2.
+      cache.add(controlDataToMap(new ControlData(newDataRow2), fieldNames, false));
+
+      // Read the cache back and compare to the newControlData
+      List<ControlData> testData = extractDataInKeyOrder(cache, fieldNames);
+      compareData(newControlData, testData);
+
+      // Now try reading the cache read-only.
+      cache.close();
+      cache.open(getContext(new HashMap<String,String>()));
+      testData = extractDataInKeyOrder(cache, fieldNames);
+      compareData(newControlData, testData);
+
+    } catch (Exception e) {
+      log.warn("Exception thrown: " + e.toString());
+      Assert.fail();
     } finally {
       try {
         cache.destroy();
       } catch (Exception ex) {
       }
-    }	  
-	}
-
-	@Test
-	public void testCacheReopensWithUpdate() {
-		DIHCache cache = null;
-		try {			
-			Map<String, String> cacheProps = new HashMap<String, String>();
-			cacheProps.put(DIHCacheSupport.CACHE_PRIMARY_KEY, "a_id");
-			
-			cache = new SortedMapBackedCache();
-			cache.open(getContext(cacheProps));
-			// We can let the data hit the cache with the fields out of order because
-			// we've identified the pk up-front.
-			loadData(cache, data, fieldNames, false);
-
-			// Close the cache.
-			cache.close();
-
-			List<ControlData> newControlData = new ArrayList<ControlData>();
-			Object[] newIdEqualsThree = null;
-			int j = 0;
-			for (int i = 0; i < data.size(); i++) {
-				// We'll be deleting a_id=1 so remove it from the control data.
-				if (data.get(i).data[0].equals(new Integer(1))) {
-					continue;
-				}
-
-				// We'll be changing "Cookie" to "Carrot" in a_id=3 so change it in the control data.
-				if (data.get(i).data[0].equals(new Integer(3))) {
-					newIdEqualsThree = new Object[data.get(i).data.length];
-					System.arraycopy(data.get(i).data, 0, newIdEqualsThree, 0, newIdEqualsThree.length);
-					newIdEqualsThree[3] = "Carrot";
-					newControlData.add(new ControlData(newIdEqualsThree));
-				}
-				// Everything else can just be copied over.
-				else {
-					newControlData.add(data.get(i));
-				}
-
-				j++;
-			}
-
-			// These new rows of data will get added to the cache, so add them to the control data too.
-			Object[] newDataRow1 = new Object[] { new Integer(99), new BigDecimal(Math.PI), "Z", "Zebra", new Float(99.99), Feb21_2011, null };
-			Object[] newDataRow2 = new Object[] { new Integer(2), new BigDecimal(Math.PI), "B", "Ballerina", new Float(2.22), Feb21_2011, null };
-
-			newControlData.add(new ControlData(newDataRow1));
-			newControlData.add(new ControlData(newDataRow2));
-
-			// Re-open the cache
-			cache.open(getContext(new HashMap<String,String>()));
-
-			// Delete a_id=1 from the cache.
-			cache.delete(new Integer(1));
-
-			// Because the cache allows duplicates, the only way to update is to
-			// delete first then add.
-			cache.delete(new Integer(3));
-			cache.add(controlDataToMap(new ControlData(newIdEqualsThree), fieldNames, false));
-
-			// Add this row with a new Primary key.
-			cache.add(controlDataToMap(new ControlData(newDataRow1), fieldNames, false));
-
-			// Add this row, creating two records in the cache with a_id=2.
-			cache.add(controlDataToMap(new ControlData(newDataRow2), fieldNames, false));
-
-			// Read the cache back and compare to the newControlData
-			List<ControlData> testData = extractDataInKeyOrder(cache, fieldNames);
-			compareData(newControlData, testData);
-
-			// Now try reading the cache read-only.
-			cache.close();
-			cache.open(getContext(new HashMap<String,String>()));
-			testData = extractDataInKeyOrder(cache, fieldNames);
-			compareData(newControlData, testData);
-
-		} catch (Exception e) {
-			log.warn("Exception thrown: " + e.toString());
-			Assert.fail();
-		} finally {
-			try {
-				cache.destroy();
-			} catch (Exception ex) {
-			}
-		}
-	}
+    }
+  }
 }
diff --git a/solr/contrib/uima/src/test/org/apache/solr/uima/ts/EntityAnnotation_Type.java b/solr/contrib/uima/src/test/org/apache/solr/uima/ts/EntityAnnotation_Type.java
index 5be6a1a..3b44208 100644
--- a/solr/contrib/uima/src/test/org/apache/solr/uima/ts/EntityAnnotation_Type.java
+++ b/solr/contrib/uima/src/test/org/apache/solr/uima/ts/EntityAnnotation_Type.java
@@ -1,4 +1,3 @@
-
 /* First created by JCasGen Sat May 07 22:33:38 JST 2011 */
 package org.apache.solr.uima.ts;
 
@@ -23,17 +22,17 @@ public class EntityAnnotation_Type extends Annotation_Type {
   private final FSGenerator fsGenerator = 
     new FSGenerator() {
       public FeatureStructure createFS(int addr, CASImpl cas) {
-  			 if (EntityAnnotation_Type.this.useExistingInstance) {
-  			   // Return eq fs instance if already created
-  		     FeatureStructure fs = EntityAnnotation_Type.this.jcas.getJfsFromCaddr(addr);
-  		     if (null == fs) {
-  		       fs = new EntityAnnotation(addr, EntityAnnotation_Type.this);
-  			   EntityAnnotation_Type.this.jcas.putJfsFromCaddr(addr, fs);
-  			   return fs;
-  		     }
-  		     return fs;
+         if (EntityAnnotation_Type.this.useExistingInstance) {
+           // Return eq fs instance if already created
+           FeatureStructure fs = EntityAnnotation_Type.this.jcas.getJfsFromCaddr(addr);
+           if (null == fs) {
+             fs = new EntityAnnotation(addr, EntityAnnotation_Type.this);
+             EntityAnnotation_Type.this.jcas.putJfsFromCaddr(addr, fs);
+             return fs;
+           }
+           return fs;
         } else return new EntityAnnotation(addr, EntityAnnotation_Type.this);
-  	  }
+      }
     };
   /** @generated */
   public final static int typeIndexID = EntityAnnotation.typeIndexID;
@@ -80,7 +79,7 @@ public class EntityAnnotation_Type extends Annotation_Type {
 
 
   /** initialize variables to correspond with Cas Type and Features
-	* @generated */
+  * @generated */
   public EntityAnnotation_Type(JCas jcas, Type casType) {
     super(jcas, casType);
     casImpl.getFSClassRegistry().addGeneratorForType((TypeImpl)this.casType, getFSGenerator());
@@ -98,4 +97,4 @@ public class EntityAnnotation_Type extends Annotation_Type {
 
 
 
-    
\ No newline at end of file
+    
diff --git a/solr/contrib/uima/src/test/org/apache/solr/uima/ts/SentimentAnnotation_Type.java b/solr/contrib/uima/src/test/org/apache/solr/uima/ts/SentimentAnnotation_Type.java
index 88945c4..1ecc508 100644
--- a/solr/contrib/uima/src/test/org/apache/solr/uima/ts/SentimentAnnotation_Type.java
+++ b/solr/contrib/uima/src/test/org/apache/solr/uima/ts/SentimentAnnotation_Type.java
@@ -1,4 +1,3 @@
-
 /* First created by JCasGen Fri Mar 04 13:08:40 CET 2011 */
 package org.apache.solr.uima.ts;
 
@@ -23,17 +22,17 @@ public class SentimentAnnotation_Type extends Annotation_Type {
   private final FSGenerator fsGenerator = 
     new FSGenerator() {
       public FeatureStructure createFS(int addr, CASImpl cas) {
-  			 if (SentimentAnnotation_Type.this.useExistingInstance) {
-  			   // Return eq fs instance if already created
-  		     FeatureStructure fs = SentimentAnnotation_Type.this.jcas.getJfsFromCaddr(addr);
-  		     if (null == fs) {
-  		       fs = new SentimentAnnotation(addr, SentimentAnnotation_Type.this);
-  			   SentimentAnnotation_Type.this.jcas.putJfsFromCaddr(addr, fs);
-  			   return fs;
-  		     }
-  		     return fs;
+        if (SentimentAnnotation_Type.this.useExistingInstance) {
+          // Return eq fs instance if already created
+          FeatureStructure fs = SentimentAnnotation_Type.this.jcas.getJfsFromCaddr(addr);
+          if (null == fs) {
+            fs = new SentimentAnnotation(addr, SentimentAnnotation_Type.this);
+            SentimentAnnotation_Type.this.jcas.putJfsFromCaddr(addr, fs);
+            return fs;
+          }
+          return fs;
         } else return new SentimentAnnotation(addr, SentimentAnnotation_Type.this);
-  	  }
+      }
     };
   /** @generated */
   public final static int typeIndexID = SentimentAnnotation.typeIndexID;
@@ -62,7 +61,7 @@ public class SentimentAnnotation_Type extends Annotation_Type {
 
 
   /** initialize variables to correspond with Cas Type and Features
-	* @generated */
+  * @generated */
   public SentimentAnnotation_Type(JCas jcas, Type casType) {
     super(jcas, casType);
     casImpl.getFSClassRegistry().addGeneratorForType((TypeImpl)this.casType, getFSGenerator());
@@ -76,4 +75,4 @@ public class SentimentAnnotation_Type extends Annotation_Type {
 
 
 
-    
\ No newline at end of file
+    
diff --git a/solr/contrib/velocity/src/java/org/apache/solr/response/PageTool.java b/solr/contrib/velocity/src/java/org/apache/solr/response/PageTool.java
index 48eff97..b2f4400 100644
--- a/solr/contrib/velocity/src/java/org/apache/solr/response/PageTool.java
+++ b/solr/contrib/velocity/src/java/org/apache/solr/response/PageTool.java
@@ -58,7 +58,7 @@ public class PageTool {
         results_found = doc_list.getNumFound();
         start = doc_list.getStart();
       } else {
-	  throw new SolrException(SolrException.ErrorCode.UNKNOWN, "Unknown response type "+docs+". Expected one of DocSlice, ResultContext or SolrDocumentList");
+        throw new SolrException(SolrException.ErrorCode.UNKNOWN, "Unknown response type "+docs+". Expected one of DocSlice, ResultContext or SolrDocumentList");
       }
     }
 
diff --git a/solr/core/src/java/org/apache/solr/cloud/CloudDescriptor.java b/solr/core/src/java/org/apache/solr/cloud/CloudDescriptor.java
index c2921f6..1b8c742 100644
--- a/solr/core/src/java/org/apache/solr/cloud/CloudDescriptor.java
+++ b/solr/core/src/java/org/apache/solr/cloud/CloudDescriptor.java
@@ -54,11 +54,11 @@ public class CloudDescriptor {
   }
 
   public String getRoles(){
-	  return roles;
+    return roles;
   }
   
   public void setRoles(String roles){
-	  this.roles = roles;
+    this.roles = roles;
   }
   
   /** Optional parameters that can change how a core is created. */
diff --git a/solr/core/src/java/org/apache/solr/core/Config.java b/solr/core/src/java/org/apache/solr/core/Config.java
index 4ecb3ba..a85f644 100644
--- a/solr/core/src/java/org/apache/solr/core/Config.java
+++ b/solr/core/src/java/org/apache/solr/core/Config.java
@@ -141,8 +141,8 @@ public class Config {
       SolrException.log(log, "Exception during parsing file: " + name, e);
       throw e;
     } catch( SolrException e ){
-    	SolrException.log(log,"Error in "+name,e);
-    	throw e;
+      SolrException.log(log,"Error in "+name,e);
+      throw e;
     }
   }
   
diff --git a/solr/core/src/java/org/apache/solr/core/CoreContainer.java b/solr/core/src/java/org/apache/solr/core/CoreContainer.java
index 93a1fa1..c8f4e8a 100644
--- a/solr/core/src/java/org/apache/solr/core/CoreContainer.java
+++ b/solr/core/src/java/org/apache/solr/core/CoreContainer.java
@@ -529,7 +529,7 @@ public class CoreContainer
           }
           opt = DOMUtil.getAttr(node, CORE_ROLES, null);
           if(opt != null){
-        	  p.getCloudDescriptor().setRoles(opt);
+            p.getCloudDescriptor().setRoles(opt);
           }
         }
         opt = DOMUtil.getAttr(node, CORE_PROPERTIES, null);
diff --git a/solr/core/src/java/org/apache/solr/handler/component/ResponseBuilder.java b/solr/core/src/java/org/apache/solr/handler/component/ResponseBuilder.java
index 59c576a..8f00c42 100644
--- a/solr/core/src/java/org/apache/solr/handler/component/ResponseBuilder.java
+++ b/solr/core/src/java/org/apache/solr/handler/component/ResponseBuilder.java
@@ -409,11 +409,11 @@ public class ResponseBuilder
 
   public ScoreDoc getScoreDoc()
   {
-	  return scoreDoc;
+    return scoreDoc;
   }
   
   public void setScoreDoc(ScoreDoc scoreDoc)
   {
-	  this.scoreDoc = scoreDoc;
+    this.scoreDoc = scoreDoc;
   }
 }
diff --git a/solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java b/solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java
index fd1e4e5..9f9c6f5 100644
--- a/solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java
+++ b/solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java
@@ -191,41 +191,41 @@ public class SpellCheckComponent extends SearchComponent implements SolrCoreAwar
   }
   
   @SuppressWarnings("unchecked")
-	protected void addCollationsToResponse(SolrParams params, SpellingResult spellingResult, ResponseBuilder rb, String q,
-	    NamedList response, boolean suggestionsMayOverlap) {
-		int maxCollations = params.getInt(SPELLCHECK_MAX_COLLATIONS, 1);
-		int maxCollationTries = params.getInt(SPELLCHECK_MAX_COLLATION_TRIES, 0);
-		int maxCollationEvaluations = params.getInt(SPELLCHECK_MAX_COLLATION_EVALUATIONS, 10000);
-		boolean collationExtendedResults = params.getBool(SPELLCHECK_COLLATE_EXTENDED_RESULTS, false);
-		boolean shard = params.getBool(ShardParams.IS_SHARD, false);
-
-		SpellCheckCollator collator = new SpellCheckCollator();
-		List<SpellCheckCollation> collations = collator.collate(spellingResult, q, rb, maxCollations, maxCollationTries, maxCollationEvaluations, suggestionsMayOverlap);
+  protected void addCollationsToResponse(SolrParams params, SpellingResult spellingResult, ResponseBuilder rb, String q,
+      NamedList response, boolean suggestionsMayOverlap) {
+    int maxCollations = params.getInt(SPELLCHECK_MAX_COLLATIONS, 1);
+    int maxCollationTries = params.getInt(SPELLCHECK_MAX_COLLATION_TRIES, 0);
+    int maxCollationEvaluations = params.getInt(SPELLCHECK_MAX_COLLATION_EVALUATIONS, 10000);
+    boolean collationExtendedResults = params.getBool(SPELLCHECK_COLLATE_EXTENDED_RESULTS, false);
+    boolean shard = params.getBool(ShardParams.IS_SHARD, false);
+
+    SpellCheckCollator collator = new SpellCheckCollator();
+    List<SpellCheckCollation> collations = collator.collate(spellingResult, q, rb, maxCollations, maxCollationTries, maxCollationEvaluations, suggestionsMayOverlap);
     //by sorting here we guarantee a non-distributed request returns all 
-		//results in the same order as a distributed request would, 
-		//even in cases when the internal rank is the same.
-		Collections.sort(collations);
-		
-		for (SpellCheckCollation collation : collations) {
-			if (collationExtendedResults) {
-				NamedList extendedResult = new NamedList();
-				extendedResult.add("collationQuery", collation.getCollationQuery());
-				extendedResult.add("hits", collation.getHits());
-				extendedResult.add("misspellingsAndCorrections", collation.getMisspellingsAndCorrections());
-				if(maxCollationTries>0 && shard)
-				{
-					extendedResult.add("collationInternalRank", collation.getInternalRank());
-				}
-				response.add("collation", extendedResult);
-			} else {
-				response.add("collation", collation.getCollationQuery());
-				if(maxCollationTries>0 && shard)
-				{
-					response.add("collationInternalRank", collation.getInternalRank());
-				}
-			}
-		}
-	}
+    //results in the same order as a distributed request would,
+    //even in cases when the internal rank is the same.
+    Collections.sort(collations);
+
+    for (SpellCheckCollation collation : collations) {
+      if (collationExtendedResults) {
+        NamedList extendedResult = new NamedList();
+        extendedResult.add("collationQuery", collation.getCollationQuery());
+        extendedResult.add("hits", collation.getHits());
+        extendedResult.add("misspellingsAndCorrections", collation.getMisspellingsAndCorrections());
+        if(maxCollationTries>0 && shard)
+        {
+          extendedResult.add("collationInternalRank", collation.getInternalRank());
+        }
+        response.add("collation", extendedResult);
+      } else {
+        response.add("collation", collation.getCollationQuery());
+        if(maxCollationTries>0 && shard)
+        {
+          response.add("collationInternalRank", collation.getInternalRank());
+        }
+      }
+    }
+  }
 
   /**
    * For every param that is of the form "spellcheck.[dictionary name].XXXX=YYYY, add
@@ -297,8 +297,8 @@ public class SpellCheckComponent extends SearchComponent implements SolrCoreAwar
           NamedList nl = (NamedList) srsp.getSolrResponse().getResponse().get("spellcheck");
           LOG.info(srsp.getShard() + " " + nl);
           if (nl != null) {
-          	mergeData.totalNumberShardResponses++;
-          	collectShardSuggestions(nl, mergeData);          
+            mergeData.totalNumberShardResponses++;
+            collectShardSuggestions(nl, mergeData);
             collectShardCollations(mergeData, nl, maxCollationTries);
           }
         }
@@ -317,22 +317,22 @@ public class SpellCheckComponent extends SearchComponent implements SolrCoreAwar
       SpellCheckCollation[] sortedCollations = mergeData.collations.values()
           .toArray(new SpellCheckCollation[mergeData.collations.size()]);
       Arrays.sort(sortedCollations);
-			int i = 0;
-			while (i < maxCollations && i < sortedCollations.length) {
-				SpellCheckCollation collation = sortedCollations[i];
-				i++;
-				if (collationExtendedResults) {
-					NamedList extendedResult = new NamedList();
-					extendedResult.add("collationQuery", collation.getCollationQuery());
-					extendedResult.add("hits", collation.getHits());
-					extendedResult.add("misspellingsAndCorrections", collation
-							.getMisspellingsAndCorrections());
-					suggestions.add("collation", extendedResult);
-				} else {
-					suggestions.add("collation", collation.getCollationQuery());
-				}
-			}
-		}
+      int i = 0;
+      while (i < maxCollations && i < sortedCollations.length) {
+        SpellCheckCollation collation = sortedCollations[i];
+        i++;
+        if (collationExtendedResults) {
+          NamedList extendedResult = new NamedList();
+          extendedResult.add("collationQuery", collation.getCollationQuery());
+          extendedResult.add("hits", collation.getHits());
+          extendedResult.add("misspellingsAndCorrections", collation
+              .getMisspellingsAndCorrections());
+          suggestions.add("collation", extendedResult);
+        } else {
+          suggestions.add("collation", collation.getCollationQuery());
+        }
+      }
+    }
     
     response.add("suggestions", suggestions);
     rb.rsp.add("spellcheck", response);
diff --git a/solr/core/src/java/org/apache/solr/handler/loader/CSVLoaderBase.java b/solr/core/src/java/org/apache/solr/handler/loader/CSVLoaderBase.java
index 4cc2d66..db8e82a 100644
--- a/solr/core/src/java/org/apache/solr/handler/loader/CSVLoaderBase.java
+++ b/solr/core/src/java/org/apache/solr/handler/loader/CSVLoaderBase.java
@@ -388,12 +388,12 @@ abstract class CSVLoaderBase extends ContentStreamLoader {
 
     // add any literals
     for (SchemaField sf : literals.keySet()) {
-    	String fn = sf.getName();
-    	String val = literals.get(sf);
-    	doc.addField(fn, val);
+      String fn = sf.getName();
+      String val = literals.get(sf);
+      doc.addField(fn, val);
     }
    
     template.solrDoc = doc;
     processor.processAdd(template);
   }
-}
\ No newline at end of file
+}
diff --git a/solr/core/src/java/org/apache/solr/response/PHPSerializedResponseWriter.java b/solr/core/src/java/org/apache/solr/response/PHPSerializedResponseWriter.java
index 72cad7c..b2a46b1 100755
--- a/solr/core/src/java/org/apache/solr/response/PHPSerializedResponseWriter.java
+++ b/solr/core/src/java/org/apache/solr/response/PHPSerializedResponseWriter.java
@@ -173,10 +173,10 @@ class PHPSerializedWriter extends JSONWriter {
   
   @Override
   public void writeMapOpener(int size) throws IOException, IllegalArgumentException {
-  	// negative size value indicates that something has gone wrong
-  	if (size < 0) {
-  		throw new IllegalArgumentException("Map size must not be negative");
-  	}
+    // negative size value indicates that something has gone wrong
+    if (size < 0) {
+      throw new IllegalArgumentException("Map size must not be negative");
+    }
     writer.write("a:"+size+":{");
   }
   
@@ -192,10 +192,10 @@ class PHPSerializedWriter extends JSONWriter {
 
   @Override
   public void writeArrayOpener(int size) throws IOException, IllegalArgumentException {
-  	// negative size value indicates that something has gone wrong
-  	if (size < 0) {
-  		throw new IllegalArgumentException("Array size must not be negative");
-  	}
+    // negative size value indicates that something has gone wrong
+    if (size < 0) {
+      throw new IllegalArgumentException("Array size must not be negative");
+    }
     writer.write("a:"+size+":{");
   }
 
diff --git a/solr/core/src/java/org/apache/solr/response/RawResponseWriter.java b/solr/core/src/java/org/apache/solr/response/RawResponseWriter.java
index 0107ca1..7d29f67 100644
--- a/solr/core/src/java/org/apache/solr/response/RawResponseWriter.java
+++ b/solr/core/src/java/org/apache/solr/response/RawResponseWriter.java
@@ -96,7 +96,7 @@ public class RawResponseWriter implements BinaryQueryResponseWriter
   }
 
 public void write(OutputStream out, SolrQueryRequest request,
-		SolrQueryResponse response) throws IOException {
+    SolrQueryResponse response) throws IOException {
     Object obj = response.getValues().get( CONTENT );
     if( obj != null && (obj instanceof ContentStream ) ) {
       // copy the contents to the writer...
@@ -110,8 +110,8 @@ public void write(OutputStream out, SolrQueryRequest request,
     }
     else {
       //getBaseWriter( request ).write( writer, request, response );
-    	throw new IOException("did not find a CONTENT object");
+      throw new IOException("did not find a CONTENT object");
     }
-	
-}
+
+  }
 }
diff --git a/solr/core/src/java/org/apache/solr/response/SolrQueryResponse.java b/solr/core/src/java/org/apache/solr/response/SolrQueryResponse.java
index 6918bdf..c3c75e8 100644
--- a/solr/core/src/java/org/apache/solr/response/SolrQueryResponse.java
+++ b/solr/core/src/java/org/apache/solr/response/SolrQueryResponse.java
@@ -194,8 +194,8 @@ public class SolrQueryResponse {
   /** Repsonse header to be logged */ 
   public NamedList<Object> getResponseHeader() {
     @SuppressWarnings("unchecked")
-	  SimpleOrderedMap<Object> header = (SimpleOrderedMap<Object>) values.get("responseHeader");
-	  return header;
+    SimpleOrderedMap<Object> header = (SimpleOrderedMap<Object>) values.get("responseHeader");
+    return header;
   }
   
   /** Add a value to be logged.
@@ -204,7 +204,7 @@ public class SolrQueryResponse {
    * @param val value of the thing to log
    */
   public void addToLog(String name, Object val) {
-	  toLog.add(name, val);
+    toLog.add(name, val);
   }
   
   /** Get loggable items.
@@ -212,7 +212,7 @@ public class SolrQueryResponse {
    * @return things to log
    */
   public NamedList<Object> getToLog() {
-	  return toLog;
+    return toLog;
   }
   
   /**
diff --git a/solr/core/src/java/org/apache/solr/schema/SchemaField.java b/solr/core/src/java/org/apache/solr/schema/SchemaField.java
index 6626fa6..bb63659 100644
--- a/solr/core/src/java/org/apache/solr/schema/SchemaField.java
+++ b/solr/core/src/java/org/apache/solr/schema/SchemaField.java
@@ -185,7 +185,7 @@ public final class SchemaField extends FieldProperties {
 
     String defaultValue = null;
     if( props.containsKey( "default" ) ) {
-    	defaultValue = props.get( "default" );
+      defaultValue = props.get( "default" );
     }
     return new SchemaField(name, ft, calcProps(name, ft, props), defaultValue );
   }
diff --git a/solr/core/src/java/org/apache/solr/search/QParser.java b/solr/core/src/java/org/apache/solr/search/QParser.java
index b5cf1ec..2eb3fe7 100755
--- a/solr/core/src/java/org/apache/solr/search/QParser.java
+++ b/solr/core/src/java/org/apache/solr/search/QParser.java
@@ -224,18 +224,18 @@ public abstract class QParser {
     String pageScoreS = null;
     String pageDocS = null;
 
-	  pageScoreS = params.get(CommonParams.PAGESCORE);
-	  pageDocS = params.get(CommonParams.PAGEDOC);
-		  
-	  if (pageScoreS == null || pageDocS == null)
-		  return null;
-	  
-	  int pageDoc = pageDocS != null ? Integer.parseInt(pageDocS) : -1;
-	  float pageScore = pageScoreS != null ? new Float(pageScoreS) : -1;
-	  if(pageDoc != -1 && pageScore != -1){
+    pageScoreS = params.get(CommonParams.PAGESCORE);
+    pageDocS = params.get(CommonParams.PAGEDOC);
+
+    if (pageScoreS == null || pageDocS == null)
+      return null;
+
+    int pageDoc = pageDocS != null ? Integer.parseInt(pageDocS) : -1;
+    float pageScore = pageScoreS != null ? new Float(pageScoreS) : -1;
+    if(pageDoc != -1 && pageScore != -1){
       return new ScoreDoc(pageDoc, pageScore);
     }
-	  else {
+    else {
       return null;
     }
 
diff --git a/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java b/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java
index 5f8294c..c2d1445 100644
--- a/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java
+++ b/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java
@@ -1369,7 +1369,7 @@ public class SolrIndexSearcher extends IndexSearcher implements Closeable,SolrIn
       TopDocsCollector topCollector;
       if (cmd.getSort() == null) {
         if(cmd.getScoreDoc() != null) {
-        	topCollector = TopScoreDocCollector.create(len, cmd.getScoreDoc(), true); //create the Collector with InOrderPagingCollector
+          topCollector = TopScoreDocCollector.create(len, cmd.getScoreDoc(), true); //create the Collector with InOrderPagingCollector
         } else {
           topCollector = TopScoreDocCollector.create(len, true);
         }
@@ -2029,11 +2029,11 @@ public class SolrIndexSearcher extends IndexSearcher implements Closeable,SolrIn
     
     public ScoreDoc getScoreDoc()
     {
-    	return scoreDoc;
+      return scoreDoc;
     }
     public void setScoreDoc(ScoreDoc scoreDoc)
     {
-    	this.scoreDoc = scoreDoc;
+      this.scoreDoc = scoreDoc;
     }
     //Issue 1726 end
 
diff --git a/solr/core/src/java/org/apache/solr/spelling/AbstractLuceneSpellChecker.java b/solr/core/src/java/org/apache/solr/spelling/AbstractLuceneSpellChecker.java
index 91322d9..eac1af9 100644
--- a/solr/core/src/java/org/apache/solr/spelling/AbstractLuceneSpellChecker.java
+++ b/solr/core/src/java/org/apache/solr/spelling/AbstractLuceneSpellChecker.java
@@ -142,7 +142,7 @@ public abstract class AbstractLuceneSpellChecker extends SolrSpellChecker {
   
   @Override
   public SpellingResult getSuggestions(SpellingOptions options) throws IOException {
-  	SpellingResult result = new SpellingResult(options.tokens);
+    SpellingResult result = new SpellingResult(options.tokens);
     IndexReader reader = determineReader(options.reader);
     Term term = field != null ? new Term(field, "") : null;
     float theAccuracy = (options.accuracy == Float.MIN_VALUE) ? spellChecker.getAccuracy() : options.accuracy;
@@ -187,13 +187,13 @@ public abstract class AbstractLuceneSpellChecker extends SolrSpellChecker {
         int countLimit = Math.min(options.count, suggestions.length);
         if(countLimit>0)
         {
-	        for (int i = 0; i < countLimit; i++) {
-	          term = new Term(field, suggestions[i]);
-	          result.add(token, suggestions[i], reader.docFreq(term));
-	        }
+          for (int i = 0; i < countLimit; i++) {
+            term = new Term(field, suggestions[i]);
+            result.add(token, suggestions[i], reader.docFreq(term));
+          }
         } else {
-        	List<String> suggList = Collections.emptyList();
-        	result.add(token, suggList);
+          List<String> suggList = Collections.emptyList();
+          result.add(token, suggList);
         }
       } else {
         if (suggestions.length > 0) {
@@ -203,8 +203,8 @@ public abstract class AbstractLuceneSpellChecker extends SolrSpellChecker {
           }
           result.add(token, suggList);
         } else {
-        	List<String> suggList = Collections.emptyList();
-        	result.add(token, suggList);
+          List<String> suggList = Collections.emptyList();
+          result.add(token, suggList);
         }
       }
     }
diff --git a/solr/core/src/java/org/apache/solr/spelling/ConjunctionSolrSpellChecker.java b/solr/core/src/java/org/apache/solr/spelling/ConjunctionSolrSpellChecker.java
index b1485d3..d6efcfb 100644
--- a/solr/core/src/java/org/apache/solr/spelling/ConjunctionSolrSpellChecker.java
+++ b/solr/core/src/java/org/apache/solr/spelling/ConjunctionSolrSpellChecker.java
@@ -139,7 +139,7 @@ public class ConjunctionSolrSpellChecker extends SolrSpellChecker {
     Map<Token, Integer> combinedTokenFrequency = new HashMap<Token, Integer>();
     Map<Token, List<LinkedHashMap<String, Integer>>> allSuggestions = new LinkedHashMap<Token, List<LinkedHashMap<String, Integer>>>();
     for(SpellingResult result : results) {
-    	if(result.getTokenFrequency()!=null) {
+      if(result.getTokenFrequency()!=null) {
         combinedTokenFrequency.putAll(result.getTokenFrequency());
       }
       for(Map.Entry<Token, LinkedHashMap<String, Integer>> entry : result.getSuggestions().entrySet()) {
diff --git a/solr/core/src/java/org/apache/solr/spelling/DirectSolrSpellChecker.java b/solr/core/src/java/org/apache/solr/spelling/DirectSolrSpellChecker.java
index 22850e4..1884da7 100644
--- a/solr/core/src/java/org/apache/solr/spelling/DirectSolrSpellChecker.java
+++ b/solr/core/src/java/org/apache/solr/spelling/DirectSolrSpellChecker.java
@@ -214,7 +214,7 @@ public class DirectSolrSpellChecker extends SolrSpellChecker {
         result.add(token, empty);
       } else {        
         for (SuggestWord suggestion : suggestions) {
-          result.add(token, suggestion.string, suggestion.freq);      	
+          result.add(token, suggestion.string, suggestion.freq);
         }
       }
     }
diff --git a/solr/core/src/java/org/apache/solr/update/UpdateLog.java b/solr/core/src/java/org/apache/solr/update/UpdateLog.java
index c066bd7..ef1f305 100644
--- a/solr/core/src/java/org/apache/solr/update/UpdateLog.java
+++ b/solr/core/src/java/org/apache/solr/update/UpdateLog.java
@@ -17,7 +17,6 @@
 
 package org.apache.solr.update;
 
-import org.apache.lucene.search.Query;
 import org.apache.lucene.util.BytesRef;
 import org.apache.solr.common.SolrException;
 import org.apache.solr.common.SolrException.ErrorCode;
@@ -63,17 +62,17 @@ public class UpdateLog implements PluginInfoInitialized {
 
 
   public enum SyncLevel { NONE, FLUSH, FSYNC;
-	  public static SyncLevel getSyncLevel(String level){
-	    if (level == null) {
-	      return SyncLevel.FLUSH;
-	    }
-		  try{
-			  return SyncLevel.valueOf(level.toUpperCase(Locale.ROOT));
-		  } catch(Exception ex){
-		    log.warn("There was an error reading the SyncLevel - default to " + SyncLevel.FLUSH, ex);
-			  return SyncLevel.FLUSH;
-		  }
-	  }
+    public static SyncLevel getSyncLevel(String level){
+      if (level == null) {
+        return SyncLevel.FLUSH;
+      }
+      try{
+        return SyncLevel.valueOf(level.toUpperCase(Locale.ROOT));
+      } catch(Exception ex){
+        log.warn("There was an error reading the SyncLevel - default to " + SyncLevel.FLUSH, ex);
+        return SyncLevel.FLUSH;
+      }
+    }
   }
   public enum State { REPLAYING, BUFFERING, APPLYING_BUFFERED, ACTIVE }
 
diff --git a/solr/core/src/java/org/apache/solr/util/LongPriorityQueue.java b/solr/core/src/java/org/apache/solr/util/LongPriorityQueue.java
index 5833249..2b0dcf7 100755
--- a/solr/core/src/java/org/apache/solr/util/LongPriorityQueue.java
+++ b/solr/core/src/java/org/apache/solr/util/LongPriorityQueue.java
@@ -152,10 +152,10 @@ public class LongPriorityQueue {
     time.  Only valid if size() > 0.
    */
   public long pop() {
-    long result = heap[1];	          // save first value
-    heap[1] = heap[size];	          // move last to first
+    long result = heap[1];            // save first value
+    heap[1] = heap[size];            // move last to first
     size--;
-    downHeap();				  // adjust heap
+    downHeap();          // adjust heap
     return result;
   }
   
@@ -187,11 +187,11 @@ public class LongPriorityQueue {
    */
   public long[] sort(int n) {
     while (--n >= 0) {
-      long result = heap[1];	          // save first value
-      heap[1] = heap[size];	          // move last to first
+      long result = heap[1];            // save first value
+      heap[1] = heap[size];            // move last to first
       heap[size] = result;                  // place it last
       size--;
-      downHeap();				  // adjust heap
+      downHeap();          // adjust heap
     }
     return heap;
   }
@@ -203,26 +203,26 @@ public class LongPriorityQueue {
 
   private void upHeap() {
     int i = size;
-    long node = heap[i];			  // save bottom node
+    long node = heap[i];        // save bottom node
     int j = i >>> 1;
     while (j > 0 && node < heap[j]) {
-      heap[i] = heap[j];			  // shift parents down
+      heap[i] = heap[j];        // shift parents down
       i = j;
       j = j >>> 1;
     }
-    heap[i] = node;				  // install saved node
+    heap[i] = node;          // install saved node
   }
 
   private void downHeap() {
     int i = 1;
-    long node = heap[i];			  // save top node
-    int j = i << 1;				  // find smaller child
+    long node = heap[i];        // save top node
+    int j = i << 1;          // find smaller child
     int k = j + 1;
     if (k <= size && heap[k] < heap[j]) {
       j = k;
     }
     while (j <= size && heap[j] < node) {
-      heap[i] = heap[j];			  // shift up child
+      heap[i] = heap[j];        // shift up child
       i = j;
       j = i << 1;
       k = j + 1;
@@ -230,6 +230,6 @@ public class LongPriorityQueue {
         j = k;
       }
     }
-    heap[i] = node;				  // install saved node
+    heap[i] = node;          // install saved node
   }
 }
diff --git a/solr/core/src/test/org/apache/solr/TestDistributedGrouping.java b/solr/core/src/test/org/apache/solr/TestDistributedGrouping.java
index a681b19..01b912f 100755
--- a/solr/core/src/test/org/apache/solr/TestDistributedGrouping.java
+++ b/solr/core/src/test/org/apache/solr/TestDistributedGrouping.java
@@ -140,7 +140,7 @@ public class TestDistributedGrouping extends BaseDistributedSearchTestCase {
 
     commit();
 
-	  // test grouping
+    // test grouping
     // The second sort = id asc . The sorting behaviour is different in dist mode. See TopDocs#merge
     // The shard the result came from matters in the order if both document sortvalues are equal
     query("q", "*:*", "rows", 100, "fl", "id," + i1, "group", "true", "group.field", i1, "group.limit", 10, "sort", i1 + " asc, id asc");
diff --git a/solr/core/src/test/org/apache/solr/handler/XmlUpdateRequestHandlerTest.java b/solr/core/src/test/org/apache/solr/handler/XmlUpdateRequestHandlerTest.java
index 76af46d..4b59336 100644
--- a/solr/core/src/test/org/apache/solr/handler/XmlUpdateRequestHandlerTest.java
+++ b/solr/core/src/test/org/apache/solr/handler/XmlUpdateRequestHandlerTest.java
@@ -112,65 +112,65 @@ public class XmlUpdateRequestHandlerTest extends SolrTestCaseJ4 {
   
   @Test
   public void testReadDelete() throws Exception {
-	    String xml =
-	      "<update>" +
-	      " <delete>" +
-	      "   <query>id:150</query>" +
-	      "   <id>150</id>" +
-	      "   <id>200</id>" +
-	      "   <query>id:200</query>" +
-	      " </delete>" +
-	      " <delete commitWithin=\"500\">" +
-	      "   <query>id:150</query>" +
-	      " </delete>" +
-	      " <delete>" +
-	      "   <id>150</id>" +
-	      " </delete>" +
-	      "</update>";
-	    
-	    MockUpdateRequestProcessor p = new MockUpdateRequestProcessor(null);
-	    p.expectDelete(null, "id:150", -1);
-	    p.expectDelete("150", null, -1);
-	    p.expectDelete("200", null, -1);
-	    p.expectDelete(null, "id:200", -1);
-	    p.expectDelete(null, "id:150", 500);
-	    p.expectDelete("150", null, -1);
-
-	    XMLLoader loader = new XMLLoader().init(null);
-	    loader.load(req(), new SolrQueryResponse(), new ContentStreamBase.StringStream(xml), p);
-	    
-	    p.assertNoCommandsPending();
-	  }
-	  
-	  private class MockUpdateRequestProcessor extends UpdateRequestProcessor {
-	    
-	    private Queue<DeleteUpdateCommand> deleteCommands = new LinkedList<DeleteUpdateCommand>();
-	    
-	    public MockUpdateRequestProcessor(UpdateRequestProcessor next) {
-	      super(next);
-	    }
-	    
-	    public void expectDelete(String id, String query, int commitWithin) {
-	      DeleteUpdateCommand cmd = new DeleteUpdateCommand(null);
-	      cmd.id = id;
-	      cmd.query = query;
-	      cmd.commitWithin = commitWithin;
-	      deleteCommands.add(cmd);
-	    }
-	    
-	    public void assertNoCommandsPending() {
-	      assertTrue(deleteCommands.isEmpty());
-	    }
-	    
-	    @Override
-	    public void processDelete(DeleteUpdateCommand cmd) throws IOException {
-	      DeleteUpdateCommand expected = deleteCommands.poll();
-	      assertNotNull("Unexpected delete command: [" + cmd + "]", expected);
-	      assertTrue("Expected [" + expected + "] but found [" + cmd + "]",
-	          ObjectUtils.equals(expected.id, cmd.id) &&
-	          ObjectUtils.equals(expected.query, cmd.query) &&
-	          expected.commitWithin==cmd.commitWithin);
-	    }
-	  }
+      String xml =
+        "<update>" +
+        " <delete>" +
+        "   <query>id:150</query>" +
+        "   <id>150</id>" +
+        "   <id>200</id>" +
+        "   <query>id:200</query>" +
+        " </delete>" +
+        " <delete commitWithin=\"500\">" +
+        "   <query>id:150</query>" +
+        " </delete>" +
+        " <delete>" +
+        "   <id>150</id>" +
+        " </delete>" +
+        "</update>";
+
+      MockUpdateRequestProcessor p = new MockUpdateRequestProcessor(null);
+      p.expectDelete(null, "id:150", -1);
+      p.expectDelete("150", null, -1);
+      p.expectDelete("200", null, -1);
+      p.expectDelete(null, "id:200", -1);
+      p.expectDelete(null, "id:150", 500);
+      p.expectDelete("150", null, -1);
+
+      XMLLoader loader = new XMLLoader().init(null);
+      loader.load(req(), new SolrQueryResponse(), new ContentStreamBase.StringStream(xml), p);
+
+      p.assertNoCommandsPending();
+    }
+
+    private class MockUpdateRequestProcessor extends UpdateRequestProcessor {
+
+      private Queue<DeleteUpdateCommand> deleteCommands = new LinkedList<DeleteUpdateCommand>();
+
+      public MockUpdateRequestProcessor(UpdateRequestProcessor next) {
+        super(next);
+      }
+
+      public void expectDelete(String id, String query, int commitWithin) {
+        DeleteUpdateCommand cmd = new DeleteUpdateCommand(null);
+        cmd.id = id;
+        cmd.query = query;
+        cmd.commitWithin = commitWithin;
+        deleteCommands.add(cmd);
+      }
+
+      public void assertNoCommandsPending() {
+        assertTrue(deleteCommands.isEmpty());
+      }
+
+      @Override
+      public void processDelete(DeleteUpdateCommand cmd) throws IOException {
+        DeleteUpdateCommand expected = deleteCommands.poll();
+        assertNotNull("Unexpected delete command: [" + cmd + "]", expected);
+        assertTrue("Expected [" + expected + "] but found [" + cmd + "]",
+            ObjectUtils.equals(expected.id, cmd.id) &&
+            ObjectUtils.equals(expected.query, cmd.query) &&
+            expected.commitWithin==cmd.commitWithin);
+      }
+    }
 
 }
diff --git a/solr/core/src/test/org/apache/solr/handler/XsltUpdateRequestHandlerTest.java b/solr/core/src/test/org/apache/solr/handler/XsltUpdateRequestHandlerTest.java
index 677fb50..f0974a1 100644
--- a/solr/core/src/test/org/apache/solr/handler/XsltUpdateRequestHandlerTest.java
+++ b/solr/core/src/test/org/apache/solr/handler/XsltUpdateRequestHandlerTest.java
@@ -64,29 +64,29 @@ public class XsltUpdateRequestHandlerTest extends SolrTestCaseJ4 {
       " </document>" +
       "</random>";
 
-  	Map<String,String> args = new HashMap<String, String>();
-  	args.put(CommonParams.TR, "xsl-update-handler-test.xsl");
+    Map<String,String> args = new HashMap<String, String>();
+    args.put(CommonParams.TR, "xsl-update-handler-test.xsl");
       
-  	SolrCore core = h.getCore();
-  	LocalSolrQueryRequest req = new LocalSolrQueryRequest( core, new MapSolrParams( args) );
-  	ArrayList<ContentStream> streams = new ArrayList<ContentStream>();
-  	streams.add(new ContentStreamBase.StringStream(xml));
-  	req.setContentStreams(streams);
-  	SolrQueryResponse rsp = new SolrQueryResponse();
-  	UpdateRequestHandler handler = new UpdateRequestHandler();
-  	handler.init(new NamedList<String>());
-  	handler.handleRequestBody(req, rsp);
-  	StringWriter sw = new StringWriter(32000);
-  	QueryResponseWriter responseWriter = core.getQueryResponseWriter(req);
-  	responseWriter.write(sw,req,rsp);
-  	req.close();
-  	String response = sw.toString();
-  	assertU(response);
+    SolrCore core = h.getCore();
+    LocalSolrQueryRequest req = new LocalSolrQueryRequest( core, new MapSolrParams( args) );
+    ArrayList<ContentStream> streams = new ArrayList<ContentStream>();
+    streams.add(new ContentStreamBase.StringStream(xml));
+    req.setContentStreams(streams);
+    SolrQueryResponse rsp = new SolrQueryResponse();
+    UpdateRequestHandler handler = new UpdateRequestHandler();
+    handler.init(new NamedList<String>());
+    handler.handleRequestBody(req, rsp);
+    StringWriter sw = new StringWriter(32000);
+    QueryResponseWriter responseWriter = core.getQueryResponseWriter(req);
+    responseWriter.write(sw,req,rsp);
+    req.close();
+    String response = sw.toString();
+    assertU(response);
     assertU(commit());
 
     assertQ("test document was correctly committed", req("q","*:*")
             , "//result[@numFound='1']"
             , "//int[@name='id'][.='12345']"
-    		);  
+        );
   }
 }
diff --git a/solr/core/src/test/org/apache/solr/handler/component/DistributedSpellCheckComponentTest.java b/solr/core/src/test/org/apache/solr/handler/component/DistributedSpellCheckComponentTest.java
index 1ec5692..52c804d 100644
--- a/solr/core/src/test/org/apache/solr/handler/component/DistributedSpellCheckComponentTest.java
+++ b/solr/core/src/test/org/apache/solr/handler/component/DistributedSpellCheckComponentTest.java
@@ -42,12 +42,12 @@ public class DistributedSpellCheckComponentTest extends BaseDistributedSearchTes
   private String requestHandlerName;
   private String reqHandlerWithWordbreak;
   
-	public DistributedSpellCheckComponentTest()
-	{
-		//fixShardCount=true;
-		//shardCount=2;
-		//stress=0;
-	}
+  public DistributedSpellCheckComponentTest()
+  {
+    //fixShardCount=true;
+    //shardCount=2;
+    //stress=0;
+  }
 
   @BeforeClass
   public static void beforeClass() throws Exception {
@@ -100,7 +100,7 @@ public class DistributedSpellCheckComponentTest extends BaseDistributedSearchTes
   
   @Override
   public void doTest() throws Exception {
-  	del("*:*");
+    del("*:*");
     index(id, "1", "lowerfilt", "toyota");
     index(id, "2", "lowerfilt", "chevrolet");
     index(id, "3", "lowerfilt", "suzuki");
diff --git a/solr/core/src/test/org/apache/solr/handler/component/SpellCheckComponentTest.java b/solr/core/src/test/org/apache/solr/handler/component/SpellCheckComponentTest.java
index ea25270..8128060 100644
--- a/solr/core/src/test/org/apache/solr/handler/component/SpellCheckComponentTest.java
+++ b/solr/core/src/test/org/apache/solr/handler/component/SpellCheckComponentTest.java
@@ -206,55 +206,55 @@ public class SpellCheckComponentTest extends SolrTestCaseJ4 {
     
     @Test
     public void testThresholdTokenFrequency() throws Exception {
-    	
-  	  	//"document" is in 2 documents but "another" is only in 1.  
-  	  	//So with a threshold of 29%, "another" is absent from the dictionary 
-  	  	//while "document" is present.
-    	
-  	  	assertJQ(req("qt",rh, SpellCheckComponent.COMPONENT_NAME, "true", "q","documenq", SpellingParams.SPELLCHECK_DICT, "threshold", SpellingParams.SPELLCHECK_COUNT,"5", SpellingParams.SPELLCHECK_EXTENDED_RESULTS,"true")
-  	        ,"/spellcheck/suggestions/[1]/suggestion==[{'word':'document','freq':2}]"
-  	    );
-  	  	
-  	  	assertJQ(req("qt",rh, SpellCheckComponent.COMPONENT_NAME, "true", "q","documenq", SpellingParams.SPELLCHECK_DICT, "threshold_direct", SpellingParams.SPELLCHECK_COUNT,"5", SpellingParams.SPELLCHECK_EXTENDED_RESULTS,"true")
-  	        ,"/spellcheck/suggestions/[1]/suggestion==[{'word':'document','freq':2}]"
-  	    );
-  	  	
-  	  	//TODO:  how do we make this into a 1-liner using "assertQ()" ???
-  	  	SolrCore core = h.getCore();
-  	  	SearchComponent speller = core.getSearchComponent("spellcheck");
-  	  	assertTrue("speller is null and it shouldn't be", speller != null);
-  	  	
-  	  	ModifiableSolrParams params = new ModifiableSolrParams();		
-  			params.add(SpellCheckComponent.COMPONENT_NAME, "true");
-  			params.add(SpellingParams.SPELLCHECK_COUNT, "10");	
-  			params.add(SpellingParams.SPELLCHECK_DICT, "threshold");
-  			params.add(SpellingParams.SPELLCHECK_EXTENDED_RESULTS,"true");
-  			params.add(CommonParams.Q, "anotheq");
-  			
-  			SolrRequestHandler handler = core.getRequestHandler("spellCheckCompRH");
-  			SolrQueryResponse rsp = new SolrQueryResponse();
-  			rsp.add("responseHeader", new SimpleOrderedMap());
-  			SolrQueryRequest req = new LocalSolrQueryRequest(core, params);
-  			handler.handleRequest(req, rsp);
-  			req.close();
-  			NamedList values = rsp.getValues();
-  			NamedList spellCheck = (NamedList) values.get("spellcheck");
-  			NamedList suggestions = (NamedList) spellCheck.get("suggestions");
-  			assertTrue(suggestions.get("suggestion")==null);
-  			assertTrue((Boolean) suggestions.get("correctlySpelled")==false);
-  			
-  			params.remove(SpellingParams.SPELLCHECK_DICT);
-  			params.add(SpellingParams.SPELLCHECK_DICT, "threshold_direct");
-  			rsp = new SolrQueryResponse();
-  			rsp.add("responseHeader", new SimpleOrderedMap());
-  			req = new LocalSolrQueryRequest(core, params);
-  			handler.handleRequest(req, rsp);
-  			req.close();
-  			values = rsp.getValues();
-  			spellCheck = (NamedList) values.get("spellcheck");
-  			suggestions = (NamedList) spellCheck.get("suggestions");
-  			assertTrue(suggestions.get("suggestion")==null);
-  			
-  			assertTrue((Boolean) suggestions.get("correctlySpelled")==false);
+
+        //"document" is in 2 documents but "another" is only in 1.
+        //So with a threshold of 29%, "another" is absent from the dictionary
+        //while "document" is present.
+
+        assertJQ(req("qt",rh, SpellCheckComponent.COMPONENT_NAME, "true", "q","documenq", SpellingParams.SPELLCHECK_DICT, "threshold", SpellingParams.SPELLCHECK_COUNT,"5", SpellingParams.SPELLCHECK_EXTENDED_RESULTS,"true")
+            ,"/spellcheck/suggestions/[1]/suggestion==[{'word':'document','freq':2}]"
+        );
+
+        assertJQ(req("qt",rh, SpellCheckComponent.COMPONENT_NAME, "true", "q","documenq", SpellingParams.SPELLCHECK_DICT, "threshold_direct", SpellingParams.SPELLCHECK_COUNT,"5", SpellingParams.SPELLCHECK_EXTENDED_RESULTS,"true")
+            ,"/spellcheck/suggestions/[1]/suggestion==[{'word':'document','freq':2}]"
+        );
+
+        //TODO:  how do we make this into a 1-liner using "assertQ()" ???
+        SolrCore core = h.getCore();
+        SearchComponent speller = core.getSearchComponent("spellcheck");
+        assertTrue("speller is null and it shouldn't be", speller != null);
+
+        ModifiableSolrParams params = new ModifiableSolrParams();
+        params.add(SpellCheckComponent.COMPONENT_NAME, "true");
+        params.add(SpellingParams.SPELLCHECK_COUNT, "10");
+        params.add(SpellingParams.SPELLCHECK_DICT, "threshold");
+        params.add(SpellingParams.SPELLCHECK_EXTENDED_RESULTS,"true");
+        params.add(CommonParams.Q, "anotheq");
+
+        SolrRequestHandler handler = core.getRequestHandler("spellCheckCompRH");
+        SolrQueryResponse rsp = new SolrQueryResponse();
+        rsp.add("responseHeader", new SimpleOrderedMap());
+        SolrQueryRequest req = new LocalSolrQueryRequest(core, params);
+        handler.handleRequest(req, rsp);
+        req.close();
+        NamedList values = rsp.getValues();
+        NamedList spellCheck = (NamedList) values.get("spellcheck");
+        NamedList suggestions = (NamedList) spellCheck.get("suggestions");
+        assertTrue(suggestions.get("suggestion")==null);
+        assertTrue((Boolean) suggestions.get("correctlySpelled")==false);
+
+        params.remove(SpellingParams.SPELLCHECK_DICT);
+        params.add(SpellingParams.SPELLCHECK_DICT, "threshold_direct");
+        rsp = new SolrQueryResponse();
+        rsp.add("responseHeader", new SimpleOrderedMap());
+        req = new LocalSolrQueryRequest(core, params);
+        handler.handleRequest(req, rsp);
+        req.close();
+        values = rsp.getValues();
+        spellCheck = (NamedList) values.get("spellcheck");
+        suggestions = (NamedList) spellCheck.get("suggestions");
+        assertTrue(suggestions.get("suggestion")==null);
+
+        assertTrue((Boolean) suggestions.get("correctlySpelled")==false);
     }
 }
diff --git a/solr/core/src/test/org/apache/solr/handler/component/StatsComponentTest.java b/solr/core/src/test/org/apache/solr/handler/component/StatsComponentTest.java
index 96a2317..43320df 100644
--- a/solr/core/src/test/org/apache/solr/handler/component/StatsComponentTest.java
+++ b/solr/core/src/test/org/apache/solr/handler/component/StatsComponentTest.java
@@ -261,34 +261,34 @@ public class StatsComponentTest extends AbstractSolrTestCase {
   }
   
   public void doTestFacetStatisticsMissingResult(String f) throws Exception {
-	    assertU(adoc("id", "1", f, "10", "active_s", "true"));
-	    assertU(adoc("id", "2", f, "20", "active_s", "true"));
-	    assertU(adoc("id", "3", "active_s", "false"));
-	    assertU(adoc("id", "4", f, "40", "active_s", "false"));
-	    assertU(commit());
-
-	    assertQ("test value for active_s=true", req("q","*:*", "stats","true", "stats.field",f, "stats.facet","active_s")
-	            , "//lst[@name='true']/double[@name='min'][.='10.0']"
-	            , "//lst[@name='true']/double[@name='max'][.='20.0']"
-	            , "//lst[@name='true']/double[@name='sum'][.='30.0']"
-	            , "//lst[@name='true']/long[@name='count'][.='2']"
-	            , "//lst[@name='true']/long[@name='missing'][.='0']"
-	            , "//lst[@name='true']/double[@name='sumOfSquares'][.='500.0']"
-	            , "//lst[@name='true']/double[@name='mean'][.='15.0']"
-	            , "//lst[@name='true']/double[@name='stddev'][.='7.0710678118654755']"
-	    );
-
-	    assertQ("test value for active_s=false", req("q","*:*", "stats","true", "stats.field",f, "stats.facet","active_s")
-	            , "//lst[@name='false']/double[@name='min'][.='40.0']"
-	            , "//lst[@name='false']/double[@name='max'][.='40.0']"
-	            , "//lst[@name='false']/double[@name='sum'][.='40.0']"
-	            , "//lst[@name='false']/long[@name='count'][.='1']"
-	            , "//lst[@name='false']/long[@name='missing'][.='1']"
-	            , "//lst[@name='false']/double[@name='sumOfSquares'][.='1600.0']"
-	            , "//lst[@name='false']/double[@name='mean'][.='40.0']"
-	            , "//lst[@name='false']/double[@name='stddev'][.='0.0']"
-	    );
-	  }
+      assertU(adoc("id", "1", f, "10", "active_s", "true"));
+      assertU(adoc("id", "2", f, "20", "active_s", "true"));
+      assertU(adoc("id", "3", "active_s", "false"));
+      assertU(adoc("id", "4", f, "40", "active_s", "false"));
+      assertU(commit());
+
+      assertQ("test value for active_s=true", req("q","*:*", "stats","true", "stats.field",f, "stats.facet","active_s")
+              , "//lst[@name='true']/double[@name='min'][.='10.0']"
+              , "//lst[@name='true']/double[@name='max'][.='20.0']"
+              , "//lst[@name='true']/double[@name='sum'][.='30.0']"
+              , "//lst[@name='true']/long[@name='count'][.='2']"
+              , "//lst[@name='true']/long[@name='missing'][.='0']"
+              , "//lst[@name='true']/double[@name='sumOfSquares'][.='500.0']"
+              , "//lst[@name='true']/double[@name='mean'][.='15.0']"
+              , "//lst[@name='true']/double[@name='stddev'][.='7.0710678118654755']"
+      );
+
+      assertQ("test value for active_s=false", req("q","*:*", "stats","true", "stats.field",f, "stats.facet","active_s")
+              , "//lst[@name='false']/double[@name='min'][.='40.0']"
+              , "//lst[@name='false']/double[@name='max'][.='40.0']"
+              , "//lst[@name='false']/double[@name='sum'][.='40.0']"
+              , "//lst[@name='false']/long[@name='count'][.='1']"
+              , "//lst[@name='false']/long[@name='missing'][.='1']"
+              , "//lst[@name='false']/double[@name='sumOfSquares'][.='1600.0']"
+              , "//lst[@name='false']/double[@name='mean'][.='40.0']"
+              , "//lst[@name='false']/double[@name='stddev'][.='0.0']"
+      );
+    }
 
   public void testFieldStatisticsResultsNumericFieldAlwaysMissing() throws Exception {
     SolrCore core = h.getCore();
diff --git a/solr/core/src/test/org/apache/solr/highlight/DummyHighlighter.java b/solr/core/src/test/org/apache/solr/highlight/DummyHighlighter.java
index 580ee11..0d660b4 100644
--- a/solr/core/src/test/org/apache/solr/highlight/DummyHighlighter.java
+++ b/solr/core/src/test/org/apache/solr/highlight/DummyHighlighter.java
@@ -27,17 +27,17 @@ import org.apache.solr.search.DocList;
 
 public class DummyHighlighter extends SolrHighlighter {
 
-	@Override
-	public NamedList<Object> doHighlighting(DocList docs, Query query,
-			SolrQueryRequest req, String[] defaultFields) throws IOException {
-		NamedList fragments = new SimpleOrderedMap();
-		fragments.add("dummy", "thing1");
-		return fragments;
-	}
+  @Override
+  public NamedList<Object> doHighlighting(DocList docs, Query query,
+      SolrQueryRequest req, String[] defaultFields) throws IOException {
+    NamedList fragments = new SimpleOrderedMap();
+    fragments.add("dummy", "thing1");
+    return fragments;
+  }
 
-	@Override
-	public void initalize(SolrConfig config) {
-		// do nothing
-	}
+  @Override
+  public void initalize(SolrConfig config) {
+    // do nothing
+  }
 
 }
diff --git a/solr/core/src/test/org/apache/solr/highlight/HighlighterConfigTest.java b/solr/core/src/test/org/apache/solr/highlight/HighlighterConfigTest.java
index 69f2305..823579d 100644
--- a/solr/core/src/test/org/apache/solr/highlight/HighlighterConfigTest.java
+++ b/solr/core/src/test/org/apache/solr/highlight/HighlighterConfigTest.java
@@ -23,47 +23,47 @@ import org.apache.solr.util.TestHarness;
 import java.util.HashMap;
 
 public class HighlighterConfigTest extends AbstractSolrTestCase {
-	  @Override public String getSchemaFile() { return "schema.xml"; }
-	  // the default case (i.e. <highlight> without a class attribute) is tested every time sorlconfig.xml is used
-	  @Override public String getSolrConfigFile() { return "solrconfig-highlight.xml"; }
+    @Override public String getSchemaFile() { return "schema.xml"; }
+    // the default case (i.e. <highlight> without a class attribute) is tested every time sorlconfig.xml is used
+    @Override public String getSolrConfigFile() { return "solrconfig-highlight.xml"; }
 
-	  @Override 
-	  public void setUp() throws Exception {
-	    // if you override setUp or tearDown, you better call
-	    // the super classes version
-	    super.setUp();
-	  }
-	  
-	  @Override 
-	  public void tearDown() throws Exception {
-	    // if you override setUp or tearDown, you better call
-	    // the super classes version
-	    super.tearDown();
-	  }
-	  
-	  public void testConfig()
-	  {
+    @Override
+    public void setUp() throws Exception {
+      // if you override setUp or tearDown, you better call
+      // the super classes version
+      super.setUp();
+    }
+
+    @Override
+    public void tearDown() throws Exception {
+      // if you override setUp or tearDown, you better call
+      // the super classes version
+      super.tearDown();
+    }
+
+    public void testConfig()
+    {
             SolrHighlighter highlighter = HighlightComponent.getHighlighter(h.getCore());
-	    log.info( "highlighter" );
+      log.info( "highlighter" );
+
+      assertTrue( highlighter instanceof DummyHighlighter );
+
+      // check to see that doHighlight is called from the DummyHighlighter
+      HashMap<String,String> args = new HashMap<String,String>();
+      args.put("hl", "true");
+      args.put("df", "t_text");
+      args.put("hl.fl", "");
+      TestHarness.LocalRequestFactory sumLRF = h.getRequestFactory(
+        "standard", 0, 200, args);
 
-	    assertTrue( highlighter instanceof DummyHighlighter );
-	    
-	    // check to see that doHighlight is called from the DummyHighlighter
-	    HashMap<String,String> args = new HashMap<String,String>();
-	    args.put("hl", "true");
-	    args.put("df", "t_text");
-	    args.put("hl.fl", "");
-	    TestHarness.LocalRequestFactory sumLRF = h.getRequestFactory(
-	      "standard", 0, 200, args);
-	    
-	    assertU(adoc("t_text", "a long day's night", "id", "1"));
-	    assertU(commit());
-	    assertU(optimize());
-	    assertQ("Basic summarization",
-	            sumLRF.makeRequest("long"),
-	            "//lst[@name='highlighting']/str[@name='dummy']"
-	            );
-	  }
+      assertU(adoc("t_text", "a long day's night", "id", "1"));
+      assertU(commit());
+      assertU(optimize());
+      assertQ("Basic summarization",
+              sumLRF.makeRequest("long"),
+              "//lst[@name='highlighting']/str[@name='dummy']"
+              );
+    }
 }
 
 
diff --git a/solr/core/src/test/org/apache/solr/search/SpatialFilterTest.java b/solr/core/src/test/org/apache/solr/search/SpatialFilterTest.java
index 550108b..79e832e 100644
--- a/solr/core/src/test/org/apache/solr/search/SpatialFilterTest.java
+++ b/solr/core/src/test/org/apache/solr/search/SpatialFilterTest.java
@@ -119,19 +119,19 @@ public class SpatialFilterTest extends SolrTestCaseJ4 {
     checkHits(fieldName, false, "43.517030,-96.789603", 110, 1, 17);
     
     
-	// Tests SOLR-2829
-	String fieldNameHome = "home_ll";
-	String fieldNameWork = "work_ll";
-
-	clearIndex();
-	assertU(adoc("id", "1", fieldNameHome, "52.67,7.30", fieldNameWork,"48.60,11.61"));
-	assertU(commit());
-
-	checkHits(fieldNameHome, "52.67,7.30", 1, 1);
-	checkHits(fieldNameWork, "48.60,11.61", 1, 1);
-	checkHits(fieldNameWork, "52.67,7.30", 1, 0);
-	checkHits(fieldNameHome, "48.60,11.61", 1, 0); 
-	  
+    // Tests SOLR-2829
+    String fieldNameHome = "home_ll";
+    String fieldNameWork = "work_ll";
+
+    clearIndex();
+    assertU(adoc("id", "1", fieldNameHome, "52.67,7.30", fieldNameWork,"48.60,11.61"));
+    assertU(commit());
+
+    checkHits(fieldNameHome, "52.67,7.30", 1, 1);
+    checkHits(fieldNameWork, "48.60,11.61", 1, 1);
+    checkHits(fieldNameWork, "52.67,7.30", 1, 0);
+    checkHits(fieldNameHome, "48.60,11.61", 1, 0);
+
   }
 
   private void checkHits(String fieldName, String pt, double distance, int count, int ... docIds) {
@@ -206,4 +206,4 @@ public class SpatialFilterTest extends SolrTestCaseJ4 {
             + NumericRangeQuery.class,
             query instanceof NumericRangeQuery);
     req.close();
-  }*/
\ No newline at end of file
+  }*/
diff --git a/solr/core/src/test/org/apache/solr/search/function/NvlValueSourceParser.java b/solr/core/src/test/org/apache/solr/search/function/NvlValueSourceParser.java
index d12ed97..49a240a 100755
--- a/solr/core/src/test/org/apache/solr/search/function/NvlValueSourceParser.java
+++ b/solr/core/src/test/org/apache/solr/search/function/NvlValueSourceParser.java
@@ -47,33 +47,33 @@ public class NvlValueSourceParser extends ValueSourceParser {
 
     @Override
     public ValueSource parse(FunctionQParser fp) throws ParseException {
-	ValueSource source = fp.parseValueSource();
-	final float nvl = fp.parseFloat();
+      ValueSource source = fp.parseValueSource();
+      final float nvl = fp.parseFloat();
 
-	return new SimpleFloatFunction(source) {
-	    @Override
+      return new SimpleFloatFunction(source) {
+        @Override
       protected String name() {
-		return "nvl";
-	    }
+          return "nvl";
+        }
 
-	    @Override
-      protected float func(int doc, FunctionValues vals) {
-		float v = vals.floatVal(doc);
-		if (v == nvlFloatValue) {
-		    return nvl;
-		} else {
-		    return v;
-		}
-	    }
-	};
+        @Override
+        protected float func(int doc, FunctionValues vals) {
+          float v = vals.floatVal(doc);
+          if (v == nvlFloatValue) {
+            return nvl;
+          } else {
+            return v;
+          }
+        }
+      };
     }
 
-    @Override
-    public void init(NamedList args) {
-	/* initialize the value to consider as null */
-	Float nvlFloatValueArg = (Float) args.get("nvlFloatValue");
-	if (nvlFloatValueArg != null) {
-	    this.nvlFloatValue = nvlFloatValueArg;
-	}
+  @Override
+  public void init(NamedList args) {
+    /* initialize the value to consider as null */
+    Float nvlFloatValueArg = (Float) args.get("nvlFloatValue");
+    if (nvlFloatValueArg != null) {
+      this.nvlFloatValue = nvlFloatValueArg;
     }
+  }
 }
\ No newline at end of file
diff --git a/solr/core/src/test/org/apache/solr/spelling/DirectSolrSpellCheckerTest.java b/solr/core/src/test/org/apache/solr/spelling/DirectSolrSpellCheckerTest.java
index 2ea50f3..044778c 100644
--- a/solr/core/src/test/org/apache/solr/spelling/DirectSolrSpellCheckerTest.java
+++ b/solr/core/src/test/org/apache/solr/spelling/DirectSolrSpellCheckerTest.java
@@ -86,12 +86,12 @@ public class DirectSolrSpellCheckerTest extends SolrTestCaseJ4 {
   
   @Test
   public void testOnlyMorePopularWithExtendedResults() throws Exception {
-  	assertQ(req("q", "teststop:fox", "qt", "spellCheckCompRH", SpellCheckComponent.COMPONENT_NAME, "true", SpellingParams.SPELLCHECK_DICT, "direct", SpellingParams.SPELLCHECK_EXTENDED_RESULTS, "true", SpellingParams.SPELLCHECK_ONLY_MORE_POPULAR, "true"),
+    assertQ(req("q", "teststop:fox", "qt", "spellCheckCompRH", SpellCheckComponent.COMPONENT_NAME, "true", SpellingParams.SPELLCHECK_DICT, "direct", SpellingParams.SPELLCHECK_EXTENDED_RESULTS, "true", SpellingParams.SPELLCHECK_ONLY_MORE_POPULAR, "true"),
         "//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='fox']/int[@name='origFreq']=1",
         "//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='fox']/arr[@name='suggestion']/lst/str[@name='word']='foo'",
         "//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='fox']/arr[@name='suggestion']/lst/int[@name='freq']=2",
         "//lst[@name='spellcheck']/lst[@name='suggestions']/bool[@name='correctlySpelled']='true'"
-  	);
+    );
   }  
   
 }
diff --git a/solr/core/src/test/org/apache/solr/spelling/SpellCheckCollatorTest.java b/solr/core/src/test/org/apache/solr/spelling/SpellCheckCollatorTest.java
index ccca7ac..ac7b19e 100644
--- a/solr/core/src/test/org/apache/solr/spelling/SpellCheckCollatorTest.java
+++ b/solr/core/src/test/org/apache/solr/spelling/SpellCheckCollatorTest.java
@@ -40,30 +40,30 @@ import org.junit.Test;
 
 @Slow
 public class SpellCheckCollatorTest extends SolrTestCaseJ4 {
-	@BeforeClass
-	public static void beforeClass() throws Exception {
- 		initCore("solrconfig-spellcheckcomponent.xml", "schema.xml");
-		assertNull(h.validateUpdate(adoc("id", "0", "lowerfilt", "faith hope and love")));
-		assertNull(h.validateUpdate(adoc("id", "1", "lowerfilt", "faith hope and loaves")));
-		assertNull(h.validateUpdate(adoc("id", "2", "lowerfilt", "fat hops and loaves")));
-		assertNull(h.validateUpdate(adoc("id", "3", "lowerfilt", "faith of homer")));
-		assertNull(h.validateUpdate(adoc("id", "4", "lowerfilt", "fat of homer")));
-		assertNull(h.validateUpdate(adoc("id", "5", "lowerfilt1", "peace")));
-		assertNull(h.validateUpdate(adoc("id", "6", "lowerfilt", "hyphenated word")));
- 		assertNull(h.validateUpdate(adoc("id", "7", "teststop", "Jane filled out a form at Charles De Gaulle")));
- 		assertNull(h.validateUpdate(adoc("id", "8", "teststop", "Dick flew from Heathrow")));
- 		assertNull(h.validateUpdate(adoc("id", "9", "teststop", "Jane is stuck in customs because Spot chewed up the form")));
- 		assertNull(h.validateUpdate(adoc("id", "10", "teststop", "Once in Paris Dick built a fire on the hearth")));
- 		assertNull(h.validateUpdate(adoc("id", "11", "teststop", "Dick waited for Jane as he watched the sparks flow upward")));
- 		assertNull(h.validateUpdate(adoc("id", "12", "teststop", "This June parisian rendez-vous is ruined because of a customs snafu")));
- 		assertNull(h.validateUpdate(adoc("id", "13", "teststop", "partisan political machine")));
-		assertNull(h.validateUpdate(commit()));
-	}
+  @BeforeClass
+  public static void beforeClass() throws Exception {
+     initCore("solrconfig-spellcheckcomponent.xml", "schema.xml");
+    assertNull(h.validateUpdate(adoc("id", "0", "lowerfilt", "faith hope and love")));
+    assertNull(h.validateUpdate(adoc("id", "1", "lowerfilt", "faith hope and loaves")));
+    assertNull(h.validateUpdate(adoc("id", "2", "lowerfilt", "fat hops and loaves")));
+    assertNull(h.validateUpdate(adoc("id", "3", "lowerfilt", "faith of homer")));
+    assertNull(h.validateUpdate(adoc("id", "4", "lowerfilt", "fat of homer")));
+    assertNull(h.validateUpdate(adoc("id", "5", "lowerfilt1", "peace")));
+    assertNull(h.validateUpdate(adoc("id", "6", "lowerfilt", "hyphenated word")));
+     assertNull(h.validateUpdate(adoc("id", "7", "teststop", "Jane filled out a form at Charles De Gaulle")));
+     assertNull(h.validateUpdate(adoc("id", "8", "teststop", "Dick flew from Heathrow")));
+     assertNull(h.validateUpdate(adoc("id", "9", "teststop", "Jane is stuck in customs because Spot chewed up the form")));
+     assertNull(h.validateUpdate(adoc("id", "10", "teststop", "Once in Paris Dick built a fire on the hearth")));
+     assertNull(h.validateUpdate(adoc("id", "11", "teststop", "Dick waited for Jane as he watched the sparks flow upward")));
+     assertNull(h.validateUpdate(adoc("id", "12", "teststop", "This June parisian rendez-vous is ruined because of a customs snafu")));
+     assertNull(h.validateUpdate(adoc("id", "13", "teststop", "partisan political machine")));
+    assertNull(h.validateUpdate(commit()));
+  }
 
-	@Test
-	public void testCollationWithHypens() throws Exception
-	{
-	  SolrCore core = h.getCore();
+  @Test
+  public void testCollationWithHypens() throws Exception
+  {
+    SolrCore core = h.getCore();
     SearchComponent speller = core.getSearchComponent("spellcheck");
     assertTrue("speller is null and it shouldn't be", speller != null);
     
@@ -111,12 +111,12 @@ public class SpellCheckCollatorTest extends SolrTestCaseJ4 {
     }
 
   }
-	
-	public void testCollateWithOverride() throws Exception
-	{
-	  assertQ(
+
+  public void testCollateWithOverride() throws Exception
+  {
+    assertQ(
       req(
-    	  SpellCheckComponent.COMPONENT_NAME, "true",
+        SpellCheckComponent.COMPONENT_NAME, "true",
         SpellCheckComponent.SPELLCHECK_DICT, "direct",
         SpellingParams.SPELLCHECK_COUNT, "10",   
         SpellingParams.SPELLCHECK_COLLATE, "true",
@@ -130,303 +130,303 @@ public class SpellCheckCollatorTest extends SolrTestCaseJ4 {
       ),
       "//lst[@name='spellcheck']/lst[@name='suggestions']/str[@name='collation']='parisian political machine'"
     );
-	  assertQ(
-	      req(
-	        SpellCheckComponent.COMPONENT_NAME, "true",
-	        SpellCheckComponent.SPELLCHECK_DICT, "direct",
-	        SpellingParams.SPELLCHECK_COUNT, "10",   
-	        SpellingParams.SPELLCHECK_COLLATE, "true",
-	        SpellingParams.SPELLCHECK_MAX_COLLATION_TRIES, "10",
-	        SpellingParams.SPELLCHECK_MAX_COLLATIONS, "10",
-	        "qt", "spellCheckCompRH",
-	        "defType", "edismax",
-	        "qf", "teststop",
-	        "mm", "1",
-	        SpellingParams.SPELLCHECK_COLLATE_PARAM_OVERRIDE + "mm", "100%",
-	        CommonParams.Q, "partisian politcal mashine"
-	      ),
-	     "//lst[@name='spellcheck']/lst[@name='suggestions']/str[@name='collation']='partisan political machine'"
-	   );
+    assertQ(
+        req(
+          SpellCheckComponent.COMPONENT_NAME, "true",
+          SpellCheckComponent.SPELLCHECK_DICT, "direct",
+          SpellingParams.SPELLCHECK_COUNT, "10",
+          SpellingParams.SPELLCHECK_COLLATE, "true",
+          SpellingParams.SPELLCHECK_MAX_COLLATION_TRIES, "10",
+          SpellingParams.SPELLCHECK_MAX_COLLATIONS, "10",
+          "qt", "spellCheckCompRH",
+          "defType", "edismax",
+          "qf", "teststop",
+          "mm", "1",
+          SpellingParams.SPELLCHECK_COLLATE_PARAM_OVERRIDE + "mm", "100%",
+          CommonParams.Q, "partisian politcal mashine"
+        ),
+       "//lst[@name='spellcheck']/lst[@name='suggestions']/str[@name='collation']='partisan political machine'"
+     );
     
-	}
+  }
+
+  @Test
+  public void testCollateWithFilter() throws Exception
+  {
+    SolrCore core = h.getCore();
+    SearchComponent speller = core.getSearchComponent("spellcheck");
+    assertTrue("speller is null and it shouldn't be", speller != null);
+
+    ModifiableSolrParams params = new ModifiableSolrParams();
+    params.add(SpellCheckComponent.COMPONENT_NAME, "true");
+    params.add(SpellingParams.SPELLCHECK_BUILD, "true");
+    params.add(SpellingParams.SPELLCHECK_COUNT, "10");
+    params.add(SpellingParams.SPELLCHECK_COLLATE, "true");
+    params.add(SpellingParams.SPELLCHECK_MAX_COLLATION_TRIES, "10");
+    params.add(SpellingParams.SPELLCHECK_MAX_COLLATIONS, "10");
+    params.add(CommonParams.Q, "lowerfilt:(+fauth +home +loane)");
+    params.add(CommonParams.FQ, "NOT(id:1)");
+
+    //Because a FilterQuery is applied which removes doc id#1 from possible hits, we would
+    //not want the collations to return us "lowerfilt:(+faith +hope +loaves)" as this only matches doc id#1.
+    SolrRequestHandler handler = core.getRequestHandler("spellCheckCompRH");
+    SolrQueryResponse rsp = new SolrQueryResponse();
+    rsp.add("responseHeader", new SimpleOrderedMap());
+    SolrQueryRequest req = new LocalSolrQueryRequest(core, params);
+    handler.handleRequest(req, rsp);
+    req.close();
+    NamedList values = rsp.getValues();
+    NamedList spellCheck = (NamedList) values.get("spellcheck");
+    NamedList suggestions = (NamedList) spellCheck.get("suggestions");
+    List<String> collations = suggestions.getAll("collation");
+    assertTrue(collations.size() > 0);
+    for(String collation : collations) {
+      assertTrue(!collation.equals("lowerfilt:(+faith +hope +loaves)"));
+    }
+  }
+
+  @Test
+  public void testCollateWithMultipleRequestHandlers() throws Exception
+  {
+    SolrCore core = h.getCore();
+    SearchComponent speller = core.getSearchComponent("spellcheck");
+    assertTrue("speller is null and it shouldn't be", speller != null);
+
+    ModifiableSolrParams params = new ModifiableSolrParams();
+    params.add(SpellCheckComponent.COMPONENT_NAME, "true");
+    params.add(SpellingParams.SPELLCHECK_DICT, "multipleFields");
+    params.add(SpellingParams.SPELLCHECK_BUILD, "true");
+    params.add(SpellingParams.SPELLCHECK_COUNT, "10");
+    params.add(SpellingParams.SPELLCHECK_COLLATE, "true");
+    params.add(SpellingParams.SPELLCHECK_MAX_COLLATION_TRIES, "1");
+    params.add(SpellingParams.SPELLCHECK_MAX_COLLATIONS, "1");
+    params.add(CommonParams.Q, "peac");
+
+    //SpellCheckCompRH has no "qf" defined.  It will not find "peace" from "peac" despite it being in the dictionary
+    //because requrying against this Request Handler results in 0 hits.
+    SolrRequestHandler handler = core.getRequestHandler("spellCheckCompRH");
+    SolrQueryResponse rsp = new SolrQueryResponse();
+    rsp.add("responseHeader", new SimpleOrderedMap());
+    SolrQueryRequest req = new LocalSolrQueryRequest(core, params);
+    handler.handleRequest(req, rsp);
+    req.close();
+    NamedList values = rsp.getValues();
+    NamedList spellCheck = (NamedList) values.get("spellcheck");
+    NamedList suggestions = (NamedList) spellCheck.get("suggestions");
+    String singleCollation = (String) suggestions.get("collation");
+    assertNull(singleCollation);
+
+    //SpellCheckCompRH1 has "lowerfilt1" defined in the "qf" param.  It will find "peace" from "peac" because
+    //requrying field "lowerfilt1" returns the hit.
+    params.remove(SpellingParams.SPELLCHECK_BUILD);
+    handler = core.getRequestHandler("spellCheckCompRH1");
+    rsp = new SolrQueryResponse();
+    rsp.add("responseHeader", new SimpleOrderedMap());
+    req = new LocalSolrQueryRequest(core, params);
+    handler.handleRequest(req, rsp);
+    req.close();
+    values = rsp.getValues();
+    spellCheck = (NamedList) values.get("spellcheck");
+    suggestions = (NamedList) spellCheck.get("suggestions");
+    singleCollation = (String) suggestions.get("collation");
+    assertEquals(singleCollation, "peace");
+  }
 
-	@Test
-	public void testCollateWithFilter() throws Exception
-	{
-		SolrCore core = h.getCore();
-		SearchComponent speller = core.getSearchComponent("spellcheck");
-		assertTrue("speller is null and it shouldn't be", speller != null);
-		
-		ModifiableSolrParams params = new ModifiableSolrParams();		
-		params.add(SpellCheckComponent.COMPONENT_NAME, "true");
-		params.add(SpellingParams.SPELLCHECK_BUILD, "true");
-		params.add(SpellingParams.SPELLCHECK_COUNT, "10");		
-		params.add(SpellingParams.SPELLCHECK_COLLATE, "true");
-		params.add(SpellingParams.SPELLCHECK_MAX_COLLATION_TRIES, "10");
-		params.add(SpellingParams.SPELLCHECK_MAX_COLLATIONS, "10");
-		params.add(CommonParams.Q, "lowerfilt:(+fauth +home +loane)");
-		params.add(CommonParams.FQ, "NOT(id:1)");
-		
-		//Because a FilterQuery is applied which removes doc id#1 from possible hits, we would
-		//not want the collations to return us "lowerfilt:(+faith +hope +loaves)" as this only matches doc id#1.
-		SolrRequestHandler handler = core.getRequestHandler("spellCheckCompRH");
-		SolrQueryResponse rsp = new SolrQueryResponse();
-		rsp.add("responseHeader", new SimpleOrderedMap());
-		SolrQueryRequest req = new LocalSolrQueryRequest(core, params);
-		handler.handleRequest(req, rsp);
-		req.close();
-		NamedList values = rsp.getValues();
-		NamedList spellCheck = (NamedList) values.get("spellcheck");
-		NamedList suggestions = (NamedList) spellCheck.get("suggestions");
-		List<String> collations = suggestions.getAll("collation");
-		assertTrue(collations.size() > 0);
-		for(String collation : collations) {
-			assertTrue(!collation.equals("lowerfilt:(+faith +hope +loaves)"));	
-		}
-	}
-	
-	@Test
-	public void testCollateWithMultipleRequestHandlers() throws Exception
-	{
-		SolrCore core = h.getCore();
-		SearchComponent speller = core.getSearchComponent("spellcheck");
-		assertTrue("speller is null and it shouldn't be", speller != null);
-		
-		ModifiableSolrParams params = new ModifiableSolrParams();		
-		params.add(SpellCheckComponent.COMPONENT_NAME, "true");
-		params.add(SpellingParams.SPELLCHECK_DICT, "multipleFields");
-		params.add(SpellingParams.SPELLCHECK_BUILD, "true");
-		params.add(SpellingParams.SPELLCHECK_COUNT, "10");		
-		params.add(SpellingParams.SPELLCHECK_COLLATE, "true");
-		params.add(SpellingParams.SPELLCHECK_MAX_COLLATION_TRIES, "1");
-		params.add(SpellingParams.SPELLCHECK_MAX_COLLATIONS, "1");
-		params.add(CommonParams.Q, "peac");	
-		
-		//SpellCheckCompRH has no "qf" defined.  It will not find "peace" from "peac" despite it being in the dictionary
-		//because requrying against this Request Handler results in 0 hits.	
-		SolrRequestHandler handler = core.getRequestHandler("spellCheckCompRH");
-		SolrQueryResponse rsp = new SolrQueryResponse();
-		rsp.add("responseHeader", new SimpleOrderedMap());
-		SolrQueryRequest req = new LocalSolrQueryRequest(core, params);
-		handler.handleRequest(req, rsp);
-		req.close();
-		NamedList values = rsp.getValues();
-		NamedList spellCheck = (NamedList) values.get("spellcheck");
-		NamedList suggestions = (NamedList) spellCheck.get("suggestions");
-		String singleCollation = (String) suggestions.get("collation");
-		assertNull(singleCollation);
-		
-		//SpellCheckCompRH1 has "lowerfilt1" defined in the "qf" param.  It will find "peace" from "peac" because
-		//requrying field "lowerfilt1" returns the hit.
-		params.remove(SpellingParams.SPELLCHECK_BUILD);
-		handler = core.getRequestHandler("spellCheckCompRH1");
-		rsp = new SolrQueryResponse();
-		rsp.add("responseHeader", new SimpleOrderedMap());
-		req = new LocalSolrQueryRequest(core, params);
-		handler.handleRequest(req, rsp);
-		req.close();
-		values = rsp.getValues();
-		spellCheck = (NamedList) values.get("spellcheck");
-		suggestions = (NamedList) spellCheck.get("suggestions");
-		singleCollation = (String) suggestions.get("collation");
-		assertEquals(singleCollation, "peace");		
-	}
+  @Test
+  public void testExtendedCollate() throws Exception {
+    SolrCore core = h.getCore();
+    SearchComponent speller = core.getSearchComponent("spellcheck");
+    assertTrue("speller is null and it shouldn't be", speller != null);
 
-	@Test
-	public void testExtendedCollate() throws Exception {
-		SolrCore core = h.getCore();
-		SearchComponent speller = core.getSearchComponent("spellcheck");
-		assertTrue("speller is null and it shouldn't be", speller != null);
+    ModifiableSolrParams params = new ModifiableSolrParams();
+    params.add(CommonParams.QT, "spellCheckCompRH");
+    params.add(CommonParams.Q, "lowerfilt:(+fauth +home +loane)");
+    params.add(SpellingParams.SPELLCHECK_EXTENDED_RESULTS, "true");
+    params.add(SpellCheckComponent.COMPONENT_NAME, "true");
+    params.add(SpellingParams.SPELLCHECK_BUILD, "true");
+    params.add(SpellingParams.SPELLCHECK_COUNT, "10");
+    params.add(SpellingParams.SPELLCHECK_COLLATE, "true");
 
-		ModifiableSolrParams params = new ModifiableSolrParams();
-		params.add(CommonParams.QT, "spellCheckCompRH");
-		params.add(CommonParams.Q, "lowerfilt:(+fauth +home +loane)");
-		params.add(SpellingParams.SPELLCHECK_EXTENDED_RESULTS, "true");
-		params.add(SpellCheckComponent.COMPONENT_NAME, "true");
-		params.add(SpellingParams.SPELLCHECK_BUILD, "true");
-		params.add(SpellingParams.SPELLCHECK_COUNT, "10");
-		params.add(SpellingParams.SPELLCHECK_COLLATE, "true");
+    // Testing backwards-compatible behavior.
+    // Returns 1 collation as a single string.
+    // All words are "correct" per the dictionary, but this collation would
+    // return no results if tried.
+    SolrRequestHandler handler = core.getRequestHandler("spellCheckCompRH");
+    SolrQueryResponse rsp = new SolrQueryResponse();
+    rsp.add("responseHeader", new SimpleOrderedMap());
+    SolrQueryRequest req = new LocalSolrQueryRequest(core, params);
+    handler.handleRequest(req, rsp);
+    req.close();
+    NamedList values = rsp.getValues();
+    NamedList spellCheck = (NamedList) values.get("spellcheck");
+    NamedList suggestions = (NamedList) spellCheck.get("suggestions");
+    String singleCollation = (String) suggestions.get("collation");
+    assertEquals("lowerfilt:(+faith +homer +loaves)", singleCollation);
 
-		// Testing backwards-compatible behavior.
-		// Returns 1 collation as a single string.
-		// All words are "correct" per the dictionary, but this collation would
-		// return no results if tried.
-		SolrRequestHandler handler = core.getRequestHandler("spellCheckCompRH");
-		SolrQueryResponse rsp = new SolrQueryResponse();
-		rsp.add("responseHeader", new SimpleOrderedMap());
-		SolrQueryRequest req = new LocalSolrQueryRequest(core, params);
-		handler.handleRequest(req, rsp);
-		req.close();
-		NamedList values = rsp.getValues();
-		NamedList spellCheck = (NamedList) values.get("spellcheck");
-		NamedList suggestions = (NamedList) spellCheck.get("suggestions");
-		String singleCollation = (String) suggestions.get("collation");
-		assertEquals("lowerfilt:(+faith +homer +loaves)", singleCollation);
+    // Testing backwards-compatible response format but will only return a
+    // collation that would return results.
+    params.remove(SpellingParams.SPELLCHECK_BUILD);
+    params.add(SpellingParams.SPELLCHECK_MAX_COLLATION_TRIES, "5");
+    params.add(SpellingParams.SPELLCHECK_MAX_COLLATIONS, "1");
+    handler = core.getRequestHandler("spellCheckCompRH");
+    rsp = new SolrQueryResponse();
+    rsp.add("responseHeader", new SimpleOrderedMap());
+    req = new LocalSolrQueryRequest(core, params);
+    handler.handleRequest(req, rsp);
+    req.close();
+    values = rsp.getValues();
+    spellCheck = (NamedList) values.get("spellcheck");
+    suggestions = (NamedList) spellCheck.get("suggestions");
+    singleCollation = (String) suggestions.get("collation");
+    assertEquals("lowerfilt:(+faith +hope +loaves)", singleCollation);
 
-		// Testing backwards-compatible response format but will only return a
-		// collation that would return results.
-		params.remove(SpellingParams.SPELLCHECK_BUILD);
-		params.add(SpellingParams.SPELLCHECK_MAX_COLLATION_TRIES, "5");
-		params.add(SpellingParams.SPELLCHECK_MAX_COLLATIONS, "1");
-		handler = core.getRequestHandler("spellCheckCompRH");
-		rsp = new SolrQueryResponse();
-		rsp.add("responseHeader", new SimpleOrderedMap());
-		req = new LocalSolrQueryRequest(core, params);
-		handler.handleRequest(req, rsp);
+    // Testing returning multiple collations if more than one valid
+    // combination exists.
+    params.remove(SpellingParams.SPELLCHECK_MAX_COLLATION_TRIES);
+    params.remove(SpellingParams.SPELLCHECK_MAX_COLLATIONS);
+    params.add(SpellingParams.SPELLCHECK_MAX_COLLATION_TRIES, "10");
+    params.add(SpellingParams.SPELLCHECK_MAX_COLLATIONS, "2");
+    handler = core.getRequestHandler("spellCheckCompRH");
+    rsp = new SolrQueryResponse();
+    rsp.add("responseHeader", new SimpleOrderedMap());
+    req = new LocalSolrQueryRequest(core, params);
+    handler.handleRequest(req, rsp);
     req.close();
-		values = rsp.getValues();
-		spellCheck = (NamedList) values.get("spellcheck");
-		suggestions = (NamedList) spellCheck.get("suggestions");
-		singleCollation = (String) suggestions.get("collation");
-		assertEquals("lowerfilt:(+faith +hope +loaves)", singleCollation);
+    values = rsp.getValues();
+    spellCheck = (NamedList) values.get("spellcheck");
+    suggestions = (NamedList) spellCheck.get("suggestions");
+    List<String> collations = suggestions.getAll("collation");
+    assertTrue(collations.size() == 2);
+    for (String multipleCollation : collations) {
+      assertTrue(multipleCollation.equals("lowerfilt:(+faith +hope +love)")
+          || multipleCollation.equals("lowerfilt:(+faith +hope +loaves)"));
+    }
 
-		// Testing returning multiple collations if more than one valid
-		// combination exists.
-		params.remove(SpellingParams.SPELLCHECK_MAX_COLLATION_TRIES);
-		params.remove(SpellingParams.SPELLCHECK_MAX_COLLATIONS);
-		params.add(SpellingParams.SPELLCHECK_MAX_COLLATION_TRIES, "10");
-		params.add(SpellingParams.SPELLCHECK_MAX_COLLATIONS, "2");
-		handler = core.getRequestHandler("spellCheckCompRH");
-		rsp = new SolrQueryResponse();
-		rsp.add("responseHeader", new SimpleOrderedMap());
-		req = new LocalSolrQueryRequest(core, params);
-		handler.handleRequest(req, rsp);
-		req.close();
-		values = rsp.getValues();
-		spellCheck = (NamedList) values.get("spellcheck");
-		suggestions = (NamedList) spellCheck.get("suggestions");
-		List<String> collations = suggestions.getAll("collation");
-		assertTrue(collations.size() == 2);
-		for (String multipleCollation : collations) {
-			assertTrue(multipleCollation.equals("lowerfilt:(+faith +hope +love)")
-					|| multipleCollation.equals("lowerfilt:(+faith +hope +loaves)"));
-		}
+    // Testing return multiple collations with expanded collation response
+    // format.
+    params.add(SpellingParams.SPELLCHECK_COLLATE_EXTENDED_RESULTS, "true");
+    handler = core.getRequestHandler("spellCheckCompRH");
+    rsp = new SolrQueryResponse();
+    rsp.add("responseHeader", new SimpleOrderedMap());
+    req = new LocalSolrQueryRequest(core, params);
+    handler.handleRequest(req, rsp);
+    req.close();
+    values = rsp.getValues();
+    spellCheck = (NamedList) values.get("spellcheck");
+    suggestions = (NamedList) spellCheck.get("suggestions");
+    List<NamedList> expandedCollationList = suggestions.getAll("collation");
+    Set<String> usedcollations = new HashSet<String>();
+    assertTrue(expandedCollationList.size() == 2);
+    for (NamedList expandedCollation : expandedCollationList) {
+      String multipleCollation = (String) expandedCollation.get("collationQuery");
+      assertTrue(multipleCollation.equals("lowerfilt:(+faith +hope +love)")
+          || multipleCollation.equals("lowerfilt:(+faith +hope +loaves)"));
+      assertTrue(!usedcollations.contains(multipleCollation));
+      usedcollations.add(multipleCollation);
 
-		// Testing return multiple collations with expanded collation response
-		// format.
-		params.add(SpellingParams.SPELLCHECK_COLLATE_EXTENDED_RESULTS, "true");
-		handler = core.getRequestHandler("spellCheckCompRH");
-		rsp = new SolrQueryResponse();
-		rsp.add("responseHeader", new SimpleOrderedMap());
-		req = new LocalSolrQueryRequest(core, params);
-		handler.handleRequest(req, rsp);
-		req.close();
-		values = rsp.getValues();
-		spellCheck = (NamedList) values.get("spellcheck");
-		suggestions = (NamedList) spellCheck.get("suggestions");
-		List<NamedList> expandedCollationList = suggestions.getAll("collation");
-		Set<String> usedcollations = new HashSet<String>();
-		assertTrue(expandedCollationList.size() == 2);
-		for (NamedList expandedCollation : expandedCollationList) {
-			String multipleCollation = (String) expandedCollation.get("collationQuery");
-			assertTrue(multipleCollation.equals("lowerfilt:(+faith +hope +love)")
-					|| multipleCollation.equals("lowerfilt:(+faith +hope +loaves)"));
-			assertTrue(!usedcollations.contains(multipleCollation));
-			usedcollations.add(multipleCollation);
+      int hits = (Integer) expandedCollation.get("hits");
+      assertTrue(hits == 1);
 
-			int hits = (Integer) expandedCollation.get("hits");
-			assertTrue(hits == 1);
+      NamedList misspellingsAndCorrections = (NamedList) expandedCollation.get("misspellingsAndCorrections");
+      assertTrue(misspellingsAndCorrections.size() == 3);
 
-			NamedList misspellingsAndCorrections = (NamedList) expandedCollation.get("misspellingsAndCorrections");
-			assertTrue(misspellingsAndCorrections.size() == 3);
+      String correctionForFauth = (String) misspellingsAndCorrections.get("fauth");
+      String correctionForHome = (String) misspellingsAndCorrections.get("home");
+      String correctionForLoane = (String) misspellingsAndCorrections.get("loane");
+      assertTrue(correctionForFauth.equals("faith"));
+      assertTrue(correctionForHome.equals("hope"));
+      assertTrue(correctionForLoane.equals("love") || correctionForLoane.equals("loaves"));
+    }
+  }
 
-			String correctionForFauth = (String) misspellingsAndCorrections.get("fauth");
-			String correctionForHome = (String) misspellingsAndCorrections.get("home");
-			String correctionForLoane = (String) misspellingsAndCorrections.get("loane");
-			assertTrue(correctionForFauth.equals("faith"));
-			assertTrue(correctionForHome.equals("hope"));
-			assertTrue(correctionForLoane.equals("love") || correctionForLoane.equals("loaves"));
-		}
-	}
-	
-	@Test
-	public void testCollateWithGrouping() throws Exception
-	{
-		SolrCore core = h.getCore();
-		SearchComponent speller = core.getSearchComponent("spellcheck");
-		assertTrue("speller is null and it shouldn't be", speller != null);
-		
-		ModifiableSolrParams params = new ModifiableSolrParams();		
-		params.add(SpellCheckComponent.COMPONENT_NAME, "true");
-		params.add(SpellingParams.SPELLCHECK_BUILD, "true");
-		params.add(SpellingParams.SPELLCHECK_COUNT, "10");		
-		params.add(SpellingParams.SPELLCHECK_COLLATE, "true");
-		params.add(SpellingParams.SPELLCHECK_MAX_COLLATION_TRIES, "5");
-		params.add(SpellingParams.SPELLCHECK_MAX_COLLATIONS, "1");
-		params.add(CommonParams.Q, "lowerfilt:(+fauth)");
-		params.add(GroupParams.GROUP, "true");
-		params.add(GroupParams.GROUP_FIELD, "id");
-		
-		//Because a FilterQuery is applied which removes doc id#1 from possible hits, we would
-		//not want the collations to return us "lowerfilt:(+faith +hope +loaves)" as this only matches doc id#1.
-		SolrRequestHandler handler = core.getRequestHandler("spellCheckCompRH");
-		SolrQueryResponse rsp = new SolrQueryResponse();
-		rsp.add("responseHeader", new SimpleOrderedMap());
-		SolrQueryRequest req = new LocalSolrQueryRequest(core, params);
-		handler.handleRequest(req, rsp);
-		req.close();
-		NamedList values = rsp.getValues();
-		NamedList spellCheck = (NamedList) values.get("spellcheck");
-		NamedList suggestions = (NamedList) spellCheck.get("suggestions");
-		List<String> collations = suggestions.getAll("collation");
-		assertTrue(collations.size() == 1);
-	}
-	
-	@Test
-	public void testContextSensitiveCollate() throws Exception {
-		//                     DirectSolrSpellChecker   IndexBasedSpellChecker
-		String[] dictionary = {"direct",                "default_teststop" };
-		for(int i=0 ; i<1 ; i++) {		
-			assertQ(
-				req(
-					"q", "teststop:(flew AND form AND heathrow)", 
-					"qt", "spellCheckCompRH",
-					"indent", "true",
-					SpellCheckComponent.COMPONENT_NAME, "true",
-					SpellCheckComponent.SPELLCHECK_DICT, dictionary[i],
-					SpellCheckComponent.SPELLCHECK_EXTENDED_RESULTS, "true", 
-					SpellCheckComponent.SPELLCHECK_COUNT, "10",
-					SpellCheckComponent.SPELLCHECK_ALTERNATIVE_TERM_COUNT, "5",
-					SpellCheckComponent.SPELLCHECK_MAX_RESULTS_FOR_SUGGEST, "0",
-					SpellCheckComponent.SPELLCHECK_COLLATE, "true",
-					SpellCheckComponent.SPELLCHECK_MAX_COLLATION_TRIES, "10",
-					SpellCheckComponent.SPELLCHECK_MAX_COLLATIONS, "1",
-					SpellCheckComponent.SPELLCHECK_COLLATE_EXTENDED_RESULTS, "true"
-				),
-				"//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='flew']/arr[@name='suggestion']/lst/str[@name='word']='flow'",
-				"//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='form']/arr[@name='suggestion']/lst/str[@name='word']='from'",
+  @Test
+  public void testCollateWithGrouping() throws Exception
+  {
+    SolrCore core = h.getCore();
+    SearchComponent speller = core.getSearchComponent("spellcheck");
+    assertTrue("speller is null and it shouldn't be", speller != null);
+
+    ModifiableSolrParams params = new ModifiableSolrParams();
+    params.add(SpellCheckComponent.COMPONENT_NAME, "true");
+    params.add(SpellingParams.SPELLCHECK_BUILD, "true");
+    params.add(SpellingParams.SPELLCHECK_COUNT, "10");
+    params.add(SpellingParams.SPELLCHECK_COLLATE, "true");
+    params.add(SpellingParams.SPELLCHECK_MAX_COLLATION_TRIES, "5");
+    params.add(SpellingParams.SPELLCHECK_MAX_COLLATIONS, "1");
+    params.add(CommonParams.Q, "lowerfilt:(+fauth)");
+    params.add(GroupParams.GROUP, "true");
+    params.add(GroupParams.GROUP_FIELD, "id");
+
+    //Because a FilterQuery is applied which removes doc id#1 from possible hits, we would
+    //not want the collations to return us "lowerfilt:(+faith +hope +loaves)" as this only matches doc id#1.
+    SolrRequestHandler handler = core.getRequestHandler("spellCheckCompRH");
+    SolrQueryResponse rsp = new SolrQueryResponse();
+    rsp.add("responseHeader", new SimpleOrderedMap());
+    SolrQueryRequest req = new LocalSolrQueryRequest(core, params);
+    handler.handleRequest(req, rsp);
+    req.close();
+    NamedList values = rsp.getValues();
+    NamedList spellCheck = (NamedList) values.get("spellcheck");
+    NamedList suggestions = (NamedList) spellCheck.get("suggestions");
+    List<String> collations = suggestions.getAll("collation");
+    assertTrue(collations.size() == 1);
+  }
+
+  @Test
+  public void testContextSensitiveCollate() throws Exception {
+    //                     DirectSolrSpellChecker   IndexBasedSpellChecker
+    String[] dictionary = {"direct",                "default_teststop" };
+    for(int i=0 ; i<1 ; i++) {
+      assertQ(
+        req(
+          "q", "teststop:(flew AND form AND heathrow)",
+          "qt", "spellCheckCompRH",
+          "indent", "true",
+          SpellCheckComponent.COMPONENT_NAME, "true",
+          SpellCheckComponent.SPELLCHECK_DICT, dictionary[i],
+          SpellCheckComponent.SPELLCHECK_EXTENDED_RESULTS, "true",
+          SpellCheckComponent.SPELLCHECK_COUNT, "10",
+          SpellCheckComponent.SPELLCHECK_ALTERNATIVE_TERM_COUNT, "5",
+          SpellCheckComponent.SPELLCHECK_MAX_RESULTS_FOR_SUGGEST, "0",
+          SpellCheckComponent.SPELLCHECK_COLLATE, "true",
+          SpellCheckComponent.SPELLCHECK_MAX_COLLATION_TRIES, "10",
+          SpellCheckComponent.SPELLCHECK_MAX_COLLATIONS, "1",
+          SpellCheckComponent.SPELLCHECK_COLLATE_EXTENDED_RESULTS, "true"
+        ),
+        "//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='flew']/arr[@name='suggestion']/lst/str[@name='word']='flow'",
+        "//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='form']/arr[@name='suggestion']/lst/str[@name='word']='from'",
 /* DirectSolrSpellChecker won't suggest if the edit distance > 2, so we can't test for this one...
-				"//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='heathrow']/arr[@name='suggestion']/lst/str[@name='word']='hearth'",
+        "//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='heathrow']/arr[@name='suggestion']/lst/str[@name='word']='hearth'",
 */
-				"//lst[@name='spellcheck']/lst[@name='suggestions']/bool[@name='correctlySpelled']='false'",
-				"//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='collation']/str[@name='collationQuery']='teststop:(flew AND from AND heathrow)'",
-				"//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='collation']/int[@name='hits']=1",
-				"//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='collation']/lst[@name='misspellingsAndCorrections']/str[@name='form']='from'"		
-			);
-			
-			assertQ(
-				req(
-					"q", "teststop:(june AND customs)", 
-					"qt", "spellCheckCompRH",
-					"indent", "true",
-					SpellCheckComponent.COMPONENT_NAME, "true",
-					SpellCheckComponent.SPELLCHECK_DICT, dictionary[i],
-					SpellCheckComponent.SPELLCHECK_EXTENDED_RESULTS, "true", 
-					SpellCheckComponent.SPELLCHECK_COUNT, "10",
-					SpellCheckComponent.SPELLCHECK_ALTERNATIVE_TERM_COUNT, "5",
-					SpellCheckComponent.SPELLCHECK_MAX_RESULTS_FOR_SUGGEST, "1",
-					SpellCheckComponent.SPELLCHECK_COLLATE, "true",
-					SpellCheckComponent.SPELLCHECK_MAX_COLLATION_TRIES, "10",
-					SpellCheckComponent.SPELLCHECK_MAX_COLLATIONS, "1",
-					SpellCheckComponent.SPELLCHECK_COLLATE_EXTENDED_RESULTS, "true"
-				),
-				"//result[@numFound=1]",
-				"//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='june']/arr[@name='suggestion']/lst/str[@name='word']='jane'",
-				"//lst[@name='spellcheck']/lst[@name='suggestions']/bool[@name='correctlySpelled']='false'",
-				"//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='collation']/str[@name='collationQuery']='teststop:(jane AND customs)'",
-				"//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='collation']/int[@name='hits']=1",
-				"//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='collation']/lst[@name='misspellingsAndCorrections']/str[@name='june']='jane'"
-			);
-		}				
-	}
+        "//lst[@name='spellcheck']/lst[@name='suggestions']/bool[@name='correctlySpelled']='false'",
+        "//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='collation']/str[@name='collationQuery']='teststop:(flew AND from AND heathrow)'",
+        "//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='collation']/int[@name='hits']=1",
+        "//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='collation']/lst[@name='misspellingsAndCorrections']/str[@name='form']='from'"
+      );
+
+      assertQ(
+        req(
+          "q", "teststop:(june AND customs)",
+          "qt", "spellCheckCompRH",
+          "indent", "true",
+          SpellCheckComponent.COMPONENT_NAME, "true",
+          SpellCheckComponent.SPELLCHECK_DICT, dictionary[i],
+          SpellCheckComponent.SPELLCHECK_EXTENDED_RESULTS, "true",
+          SpellCheckComponent.SPELLCHECK_COUNT, "10",
+          SpellCheckComponent.SPELLCHECK_ALTERNATIVE_TERM_COUNT, "5",
+          SpellCheckComponent.SPELLCHECK_MAX_RESULTS_FOR_SUGGEST, "1",
+          SpellCheckComponent.SPELLCHECK_COLLATE, "true",
+          SpellCheckComponent.SPELLCHECK_MAX_COLLATION_TRIES, "10",
+          SpellCheckComponent.SPELLCHECK_MAX_COLLATIONS, "1",
+          SpellCheckComponent.SPELLCHECK_COLLATE_EXTENDED_RESULTS, "true"
+        ),
+        "//result[@numFound=1]",
+        "//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='june']/arr[@name='suggestion']/lst/str[@name='word']='jane'",
+        "//lst[@name='spellcheck']/lst[@name='suggestions']/bool[@name='correctlySpelled']='false'",
+        "//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='collation']/str[@name='collationQuery']='teststop:(jane AND customs)'",
+        "//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='collation']/int[@name='hits']=1",
+        "//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='collation']/lst[@name='misspellingsAndCorrections']/str[@name='june']='jane'"
+      );
+    }
+  }
 }
diff --git a/solr/core/src/test/org/apache/solr/spelling/SpellPossibilityIteratorTest.java b/solr/core/src/test/org/apache/solr/spelling/SpellPossibilityIteratorTest.java
index e233ebc..cca7b76 100644
--- a/solr/core/src/test/org/apache/solr/spelling/SpellPossibilityIteratorTest.java
+++ b/solr/core/src/test/org/apache/solr/spelling/SpellPossibilityIteratorTest.java
@@ -28,71 +28,71 @@ import org.junit.Before;
 import org.junit.Test;
 
 public class SpellPossibilityIteratorTest extends SolrTestCaseJ4 {
-	private static final Token TOKEN_AYE = new Token("AYE", 0, 3);
-	private static final Token TOKEN_BEE = new Token("BEE", 4, 7);
-	private static final Token TOKEN_AYE_BEE = new Token("AYE BEE", 0, 7);
-	private static final Token TOKEN_CEE = new Token("CEE", 8, 11);
-	
-	private LinkedHashMap<String, Integer> AYE;
-	private LinkedHashMap<String, Integer> BEE;
-	private LinkedHashMap<String, Integer> AYE_BEE;
-	private LinkedHashMap<String, Integer> CEE;
-	
-	@Override
+  private static final Token TOKEN_AYE = new Token("AYE", 0, 3);
+  private static final Token TOKEN_BEE = new Token("BEE", 4, 7);
+  private static final Token TOKEN_AYE_BEE = new Token("AYE BEE", 0, 7);
+  private static final Token TOKEN_CEE = new Token("CEE", 8, 11);
+
+  private LinkedHashMap<String, Integer> AYE;
+  private LinkedHashMap<String, Integer> BEE;
+  private LinkedHashMap<String, Integer> AYE_BEE;
+  private LinkedHashMap<String, Integer> CEE;
+
+  @Override
   @Before
-	public void setUp() throws Exception {
-	  super.setUp();
-
-		AYE = new LinkedHashMap<String, Integer>();
-		AYE.put("I", 0);
-		AYE.put("II", 0);
-		AYE.put("III", 0);
-		AYE.put("IV", 0);
-		AYE.put("V", 0);
-		AYE.put("VI", 0);
-		AYE.put("VII", 0);
-		AYE.put("VIII", 0);
-		
-		BEE = new LinkedHashMap<String, Integer>();
-		BEE.put("alpha", 0);
-		BEE.put("beta", 0);
-		BEE.put("gamma", 0);
-		BEE.put("delta", 0);
-		BEE.put("epsilon", 0);
-		BEE.put("zeta", 0);
-		BEE.put("eta", 0);
-		BEE.put("theta", 0);
-		BEE.put("iota", 0);
-		
-		AYE_BEE = new LinkedHashMap<String, Integer>();
-		AYE_BEE.put("one-alpha", 0);
-		AYE_BEE.put("two-beta", 0);
-		AYE_BEE.put("three-gamma", 0);
-		AYE_BEE.put("four-delta", 0);
-		AYE_BEE.put("five-epsilon", 0);
-		AYE_BEE.put("six-zeta", 0);
-		AYE_BEE.put("seven-eta", 0);
-		AYE_BEE.put("eight-theta", 0);
-		AYE_BEE.put("nine-iota", 0);
-		
-
-		CEE = new LinkedHashMap<String, Integer>();
-		CEE.put("one", 0);
-		CEE.put("two", 0);
-		CEE.put("three", 0);
-		CEE.put("four", 0);
-		CEE.put("five", 0);
-		CEE.put("six", 0);
-		CEE.put("seven", 0);
-		CEE.put("eight", 0);
-		CEE.put("nine", 0);
-		CEE.put("ten", 0);
-	}
-	
-	@Test
-	public void testScalability() throws Exception {
-	  Map<Token, LinkedHashMap<String, Integer>> lotsaSuggestions = new LinkedHashMap<Token, LinkedHashMap<String, Integer>>();
-	  lotsaSuggestions.put(TOKEN_AYE , AYE);
+  public void setUp() throws Exception {
+    super.setUp();
+
+    AYE = new LinkedHashMap<String, Integer>();
+    AYE.put("I", 0);
+    AYE.put("II", 0);
+    AYE.put("III", 0);
+    AYE.put("IV", 0);
+    AYE.put("V", 0);
+    AYE.put("VI", 0);
+    AYE.put("VII", 0);
+    AYE.put("VIII", 0);
+
+    BEE = new LinkedHashMap<String, Integer>();
+    BEE.put("alpha", 0);
+    BEE.put("beta", 0);
+    BEE.put("gamma", 0);
+    BEE.put("delta", 0);
+    BEE.put("epsilon", 0);
+    BEE.put("zeta", 0);
+    BEE.put("eta", 0);
+    BEE.put("theta", 0);
+    BEE.put("iota", 0);
+
+    AYE_BEE = new LinkedHashMap<String, Integer>();
+    AYE_BEE.put("one-alpha", 0);
+    AYE_BEE.put("two-beta", 0);
+    AYE_BEE.put("three-gamma", 0);
+    AYE_BEE.put("four-delta", 0);
+    AYE_BEE.put("five-epsilon", 0);
+    AYE_BEE.put("six-zeta", 0);
+    AYE_BEE.put("seven-eta", 0);
+    AYE_BEE.put("eight-theta", 0);
+    AYE_BEE.put("nine-iota", 0);
+
+
+    CEE = new LinkedHashMap<String, Integer>();
+    CEE.put("one", 0);
+    CEE.put("two", 0);
+    CEE.put("three", 0);
+    CEE.put("four", 0);
+    CEE.put("five", 0);
+    CEE.put("six", 0);
+    CEE.put("seven", 0);
+    CEE.put("eight", 0);
+    CEE.put("nine", 0);
+    CEE.put("ten", 0);
+  }
+
+  @Test
+  public void testScalability() throws Exception {
+    Map<Token, LinkedHashMap<String, Integer>> lotsaSuggestions = new LinkedHashMap<Token, LinkedHashMap<String, Integer>>();
+    lotsaSuggestions.put(TOKEN_AYE , AYE);
     lotsaSuggestions.put(TOKEN_BEE , BEE);
     lotsaSuggestions.put(TOKEN_CEE , CEE);
     
@@ -112,15 +112,15 @@ public class SpellPossibilityIteratorTest extends SolrTestCaseJ4 {
     lotsaSuggestions.put(new Token("BEE4", 4, 7),  BEE);
     lotsaSuggestions.put(new Token("CEE4", 8, 11), CEE);
     
-		PossibilityIterator iter = new PossibilityIterator(lotsaSuggestions, 1000, 10000, false);
-		int count = 0;
-		while (iter.hasNext()) {			
-			PossibilityIterator.RankedSpellPossibility rsp = iter.next();
-			count++;
-		}
-		assertTrue(count==1000);
-		
-		lotsaSuggestions.put(new Token("AYE_BEE1", 0, 7), AYE_BEE);
+    PossibilityIterator iter = new PossibilityIterator(lotsaSuggestions, 1000, 10000, false);
+    int count = 0;
+    while (iter.hasNext()) {
+      PossibilityIterator.RankedSpellPossibility rsp = iter.next();
+      count++;
+    }
+    assertTrue(count==1000);
+
+    lotsaSuggestions.put(new Token("AYE_BEE1", 0, 7), AYE_BEE);
     lotsaSuggestions.put(new Token("AYE_BEE2", 0, 7), AYE_BEE);
     lotsaSuggestions.put(new Token("AYE_BEE3", 0, 7), AYE_BEE);
     lotsaSuggestions.put(new Token("AYE_BEE4", 0, 7), AYE_BEE);
@@ -131,62 +131,62 @@ public class SpellPossibilityIteratorTest extends SolrTestCaseJ4 {
       count++;
     }
     assertTrue(count<100);
-	}
-	
-	@Test
-	public void testSpellPossibilityIterator() throws Exception {
-	  Map<Token, LinkedHashMap<String, Integer>> suggestions = new LinkedHashMap<Token, LinkedHashMap<String, Integer>>();
-	  suggestions.put(TOKEN_AYE , AYE);
+  }
+
+  @Test
+  public void testSpellPossibilityIterator() throws Exception {
+    Map<Token, LinkedHashMap<String, Integer>> suggestions = new LinkedHashMap<Token, LinkedHashMap<String, Integer>>();
+    suggestions.put(TOKEN_AYE , AYE);
     suggestions.put(TOKEN_BEE , BEE);
     suggestions.put(TOKEN_CEE , CEE);
     
-		PossibilityIterator iter = new PossibilityIterator(suggestions, 1000, 10000, false);
-		int count = 0;
-		while (iter.hasNext()) {
-			
-		  PossibilityIterator.RankedSpellPossibility rsp = iter.next();
-			if(count==0) {
-				assertTrue("I".equals(rsp.corrections.get(0).getCorrection()));
-				assertTrue("alpha".equals(rsp.corrections.get(1).getCorrection()));
-				assertTrue("one".equals(rsp.corrections.get(2).getCorrection()));
-			}
-			count++;
-		}
-		assertTrue(("Three maps (8*9*10) should return 720 iterations but instead returned " + count), count == 720);
-
-		suggestions.remove(TOKEN_CEE);
-		iter = new PossibilityIterator(suggestions, 100, 10000, false);
-		count = 0;
-		while (iter.hasNext()) {
-			iter.next();
-			count++;
-		}
-		assertTrue(("Two maps (8*9) should return 72 iterations but instead returned " + count), count == 72);
-
-		suggestions.remove(TOKEN_BEE);
-		iter = new PossibilityIterator(suggestions, 5, 10000, false);
-		count = 0;
-		while (iter.hasNext()) {
-			iter.next();
-			count++;
-		}
-		assertTrue(("We requested 5 suggestions but got " + count), count == 5);
-
-		suggestions.remove(TOKEN_AYE);
-		iter = new PossibilityIterator(suggestions, Integer.MAX_VALUE, 10000, false);
-		count = 0;
-		while (iter.hasNext()) {
-			iter.next();
-			count++;
-		}
-		assertTrue(("No maps should return 0 iterations but instead returned " + count), count == 0);
-
-	}
-	
-	@Test
+    PossibilityIterator iter = new PossibilityIterator(suggestions, 1000, 10000, false);
+    int count = 0;
+    while (iter.hasNext()) {
+
+      PossibilityIterator.RankedSpellPossibility rsp = iter.next();
+      if(count==0) {
+        assertTrue("I".equals(rsp.corrections.get(0).getCorrection()));
+        assertTrue("alpha".equals(rsp.corrections.get(1).getCorrection()));
+        assertTrue("one".equals(rsp.corrections.get(2).getCorrection()));
+      }
+      count++;
+    }
+    assertTrue(("Three maps (8*9*10) should return 720 iterations but instead returned " + count), count == 720);
+
+    suggestions.remove(TOKEN_CEE);
+    iter = new PossibilityIterator(suggestions, 100, 10000, false);
+    count = 0;
+    while (iter.hasNext()) {
+      iter.next();
+      count++;
+    }
+    assertTrue(("Two maps (8*9) should return 72 iterations but instead returned " + count), count == 72);
+
+    suggestions.remove(TOKEN_BEE);
+    iter = new PossibilityIterator(suggestions, 5, 10000, false);
+    count = 0;
+    while (iter.hasNext()) {
+      iter.next();
+      count++;
+    }
+    assertTrue(("We requested 5 suggestions but got " + count), count == 5);
+
+    suggestions.remove(TOKEN_AYE);
+    iter = new PossibilityIterator(suggestions, Integer.MAX_VALUE, 10000, false);
+    count = 0;
+    while (iter.hasNext()) {
+      iter.next();
+      count++;
+    }
+    assertTrue(("No maps should return 0 iterations but instead returned " + count), count == 0);
+
+  }
+
+  @Test
   public void testOverlappingTokens() throws Exception {
-	  Map<Token, LinkedHashMap<String, Integer>> overlappingSuggestions = new LinkedHashMap<Token, LinkedHashMap<String, Integer>>();
-	  overlappingSuggestions.put(TOKEN_AYE, AYE);
+    Map<Token, LinkedHashMap<String, Integer>> overlappingSuggestions = new LinkedHashMap<Token, LinkedHashMap<String, Integer>>();
+    overlappingSuggestions.put(TOKEN_AYE, AYE);
     overlappingSuggestions.put(TOKEN_BEE, BEE);
     overlappingSuggestions.put(TOKEN_AYE_BEE, AYE_BEE);
     overlappingSuggestions.put(TOKEN_CEE, CEE);
diff --git a/solr/core/src/test/org/apache/solr/update/UpdateParamsTest.java b/solr/core/src/test/org/apache/solr/update/UpdateParamsTest.java
index dc38c84..7c6c214 100644
--- a/solr/core/src/test/org/apache/solr/update/UpdateParamsTest.java
+++ b/solr/core/src/test/org/apache/solr/update/UpdateParamsTest.java
@@ -54,8 +54,8 @@ public class UpdateParamsTest extends AbstractSolrTestCase {
     
     // First check that the old param behaves as it should
     try {
-    	handler.handleRequestBody(req, rsp);
-    	assertTrue("Old param update.processor should not have any effect anymore", true);
+      handler.handleRequestBody(req, rsp);
+      assertTrue("Old param update.processor should not have any effect anymore", true);
     } catch (Exception e) {
       assertFalse("Got wrong exception while testing update.chain", e.getMessage().equals("unknown UpdateRequestProcessorChain: nonexistant"));
     }
@@ -65,10 +65,10 @@ public class UpdateParamsTest extends AbstractSolrTestCase {
     params.getMap().put(UpdateParams.UPDATE_CHAIN, "nonexistant");    
     req.setParams(params);
     try {
-    	handler.handleRequestBody(req, rsp);
-    	assertFalse("Faulty update.chain parameter not causing an error - i.e. it is not detected", true);
+      handler.handleRequestBody(req, rsp);
+      assertFalse("Faulty update.chain parameter not causing an error - i.e. it is not detected", true);
     } catch (Exception e) {
-    	assertEquals("Got wrong exception while testing update.chain", e.getMessage(), "unknown UpdateRequestProcessorChain: nonexistant");
+      assertEquals("Got wrong exception while testing update.chain", e.getMessage(), "unknown UpdateRequestProcessorChain: nonexistant");
     }
     
   }
diff --git a/solr/solrj/src/java/org/apache/solr/client/solrj/beans/DocumentObjectBinder.java b/solr/solrj/src/java/org/apache/solr/client/solrj/beans/DocumentObjectBinder.java
index bdd168c..b772ea0 100644
--- a/solr/solrj/src/java/org/apache/solr/client/solrj/beans/DocumentObjectBinder.java
+++ b/solr/solrj/src/java/org/apache/solr/client/solrj/beans/DocumentObjectBinder.java
@@ -88,7 +88,7 @@ public class DocumentObjectBinder {
       } else {
         doc.setField(field.name, field.get(obj), 1.0f);
       }
-	}
+    }
     return doc;
   }
   
diff --git a/solr/solrj/src/java/org/apache/solr/client/solrj/request/LukeRequest.java b/solr/solrj/src/java/org/apache/solr/client/solrj/request/LukeRequest.java
index 206075a..bf74865 100644
--- a/solr/solrj/src/java/org/apache/solr/client/solrj/request/LukeRequest.java
+++ b/solr/solrj/src/java/org/apache/solr/client/solrj/request/LukeRequest.java
@@ -108,7 +108,7 @@ public class LukeRequest extends SolrRequest
       params.add( "numTerms", numTerms+"" );
     }
     if (showSchema) {
-    	params.add("show", "schema");
+      params.add("show", "schema");
     }
     return params;
   }
diff --git a/solr/solrj/src/java/org/apache/solr/client/solrj/response/SpellCheckResponse.java b/solr/solrj/src/java/org/apache/solr/client/solrj/response/SpellCheckResponse.java
index c4c63aa..a38fdb6 100644
--- a/solr/solrj/src/java/org/apache/solr/client/solrj/response/SpellCheckResponse.java
+++ b/solr/solrj/src/java/org/apache/solr/client/solrj/response/SpellCheckResponse.java
@@ -45,41 +45,41 @@ public class SpellCheckResponse {
       String n = sugg.getName(i);
       if ("correctlySpelled".equals(n)) {
         correctlySpelled = (Boolean) sugg.getVal(i);
-			} else if ("collationInternalRank".equals(n)){
-				//continue;
-			} else if ("collation".equals(n)) {
-				List<Object> collationInfo = sugg.getAll(n);
-				collations = new ArrayList<Collation>(collationInfo.size());
-				for (Object o : collationInfo) {
-					if (o instanceof String) {
-						collations.add(new Collation()
-								.setCollationQueryString((String) o));
-					} else if (o instanceof NamedList) {
+      } else if ("collationInternalRank".equals(n)){
+        //continue;
+      } else if ("collation".equals(n)) {
+        List<Object> collationInfo = sugg.getAll(n);
+        collations = new ArrayList<Collation>(collationInfo.size());
+        for (Object o : collationInfo) {
+          if (o instanceof String) {
+            collations.add(new Collation()
+                .setCollationQueryString((String) o));
+          } else if (o instanceof NamedList) {
             @SuppressWarnings("unchecked")
-						NamedList<Object> expandedCollation = (NamedList<Object>) o;
-						String collationQuery 
+            NamedList<Object> expandedCollation = (NamedList<Object>) o;
+            String collationQuery
               = (String) expandedCollation.get("collationQuery");
-						int hits = (Integer) expandedCollation.get("hits");
+            int hits = (Integer) expandedCollation.get("hits");
             @SuppressWarnings("unchecked")
-						NamedList<String> misspellingsAndCorrections 
+            NamedList<String> misspellingsAndCorrections
               = (NamedList<String>) expandedCollation.get("misspellingsAndCorrections");
 
-						Collation collation = new Collation();
-						collation.setCollationQueryString(collationQuery);
-						collation.setNumberOfHits(hits);
-
-						for (int ii = 0; ii < misspellingsAndCorrections.size(); ii++) {
-							String misspelling = misspellingsAndCorrections.getName(ii);
-							String correction = misspellingsAndCorrections.getVal(ii);
-							collation.addMisspellingsAndCorrection(new Correction(
-									misspelling, correction));
-						}
-						collations.add(collation);
-					} else {
-						throw new AssertionError(
-								"Should get Lists of Strings or List of NamedLists here.");
-					}
-				} 	
+            Collation collation = new Collation();
+            collation.setCollationQueryString(collationQuery);
+            collation.setNumberOfHits(hits);
+
+            for (int ii = 0; ii < misspellingsAndCorrections.size(); ii++) {
+              String misspelling = misspellingsAndCorrections.getName(ii);
+              String correction = misspellingsAndCorrections.getVal(ii);
+              collation.addMisspellingsAndCorrection(new Correction(
+                  misspelling, correction));
+            }
+            collations.add(collation);
+          } else {
+            throw new AssertionError(
+                "Should get Lists of Strings or List of NamedLists here.");
+          }
+        }
       } else {
         @SuppressWarnings("unchecked")
         Suggestion s = new Suggestion(n, (NamedList<Object>) sugg.getVal(i));
@@ -129,7 +129,7 @@ public class SpellCheckResponse {
    * @return all collations
    */
   public List<Collation> getCollatedResults() {
-  	return collations;
+    return collations;
   }
 
   public static class Suggestion {
@@ -219,62 +219,62 @@ public class SpellCheckResponse {
 
   }
 
-	public class Collation {
-		private String collationQueryString;
-		private List<Correction> misspellingsAndCorrections = new ArrayList<Correction>();
-		private long numberOfHits;
+  public class Collation {
+    private String collationQueryString;
+    private List<Correction> misspellingsAndCorrections = new ArrayList<Correction>();
+    private long numberOfHits;
 
-		public long getNumberOfHits() {
-			return numberOfHits;
-		}
+    public long getNumberOfHits() {
+      return numberOfHits;
+    }
 
-		public void setNumberOfHits(long numberOfHits) {
-			this.numberOfHits = numberOfHits;
-		}
+    public void setNumberOfHits(long numberOfHits) {
+      this.numberOfHits = numberOfHits;
+    }
 
-		public String getCollationQueryString() {
-			return collationQueryString;
-		}
+    public String getCollationQueryString() {
+      return collationQueryString;
+    }
 
-		public Collation setCollationQueryString(String collationQueryString) {
-			this.collationQueryString = collationQueryString;
-			return this;
-		}
+    public Collation setCollationQueryString(String collationQueryString) {
+      this.collationQueryString = collationQueryString;
+      return this;
+    }
 
-		public List<Correction> getMisspellingsAndCorrections() {
-			return misspellingsAndCorrections;
-		}
+    public List<Correction> getMisspellingsAndCorrections() {
+      return misspellingsAndCorrections;
+    }
 
-		public Collation addMisspellingsAndCorrection(Correction correction) {
-			this.misspellingsAndCorrections.add(correction);
-			return this;
-		}
+    public Collation addMisspellingsAndCorrection(Correction correction) {
+      this.misspellingsAndCorrections.add(correction);
+      return this;
+    }
 
-	}
+  }
 
-	public class Correction {
-		private String original;
-		private String correction;
+  public class Correction {
+    private String original;
+    private String correction;
 
-		public Correction(String original, String correction) {
-			this.original = original;
-			this.correction = correction;
-		}
+    public Correction(String original, String correction) {
+      this.original = original;
+      this.correction = correction;
+    }
 
-		public String getOriginal() {
-			return original;
-		}
+    public String getOriginal() {
+      return original;
+    }
 
-		public void setOriginal(String original) {
-			this.original = original;
-		}
+    public void setOriginal(String original) {
+      this.original = original;
+    }
 
-		public String getCorrection() {
-			return correction;
-		}
+    public String getCorrection() {
+      return correction;
+    }
 
-		public void setCorrection(String correction) {
-			this.correction = correction;
-		}
-	}
+    public void setCorrection(String correction) {
+      this.correction = correction;
+    }
+  }
 }
diff --git a/solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.java b/solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.java
index 8f3e7fb..7024b49 100644
--- a/solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.java
+++ b/solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.java
@@ -141,7 +141,7 @@ public class ZkStateReader {
 
           public void command() {
             try {
-            	ZkStateReader.this.createClusterStateWatchersAndUpdate();
+              ZkStateReader.this.createClusterStateWatchersAndUpdate();
             } catch (KeeperException e) {
               log.error("", e);
               throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR,
@@ -378,12 +378,12 @@ public class ZkStateReader {
   }
   
   abstract class RunnableWatcher implements Runnable {
-		Watcher watcher;
-		public RunnableWatcher(Watcher watcher){
-			this.watcher = watcher;
-		}
+    Watcher watcher;
+    public RunnableWatcher(Watcher watcher){
+      this.watcher = watcher;
+    }
 
-	}
+  }
   
   public String getLeaderUrl(String collection, String shard, int timeout)
       throws InterruptedException, KeeperException {
diff --git a/solr/solrj/src/java/org/apache/solr/common/util/NamedList.java b/solr/solrj/src/java/org/apache/solr/common/util/NamedList.java
index 9392d1e..bfcfa22 100644
--- a/solr/solrj/src/java/org/apache/solr/common/util/NamedList.java
+++ b/solr/solrj/src/java/org/apache/solr/common/util/NamedList.java
@@ -367,12 +367,12 @@ public class NamedList<T> implements Cloneable, Serializable, Iterable<Map.Entry
           @Override
           public String toString()
           {
-        	  return getKey()+"="+getValue();
+            return getKey()+"="+getValue();
           }
 
-    		  public T setValue(T value) {
+          public T setValue(T value) {
             return list.setVal(index, value);
-    		  }
+          }
         };
         return nv;
       }
diff --git a/solr/solrj/src/test/org/apache/solr/client/solrj/response/TestSpellCheckResponse.java b/solr/solrj/src/test/org/apache/solr/client/solrj/response/TestSpellCheckResponse.java
index 6fbd529..c5150c0 100644
--- a/solr/solrj/src/test/org/apache/solr/client/solrj/response/TestSpellCheckResponse.java
+++ b/solr/solrj/src/test/org/apache/solr/client/solrj/response/TestSpellCheckResponse.java
@@ -108,7 +108,7 @@ public class TestSpellCheckResponse extends SolrJettyTestBase {
   
   @Test
   public void testSpellCheckCollationResponse() throws Exception {
-  	getSolrServer();
+    getSolrServer();
     server.deleteByQuery("*:*");
     server.commit(true, true);
     SolrInputDocument doc = new SolrInputDocument();
@@ -156,27 +156,27 @@ public class TestSpellCheckResponse extends SolrJettyTestBase {
     assertEquals(2, collations.size());
     for(Collation collation : collations)
     {
-    	assertTrue("name:(+faith +hope +love)".equals(collation.getCollationQueryString()) || "name:(+faith +hope +loaves)".equals(collation.getCollationQueryString()));
+      assertTrue("name:(+faith +hope +love)".equals(collation.getCollationQueryString()) || "name:(+faith +hope +loaves)".equals(collation.getCollationQueryString()));
       assertTrue(collation.getNumberOfHits()==1);
-    	
-    	List<Correction> misspellingsAndCorrections = collation.getMisspellingsAndCorrections();
-    	assertTrue(misspellingsAndCorrections.size()==3);
-    	for(Correction correction : misspellingsAndCorrections)
-    	{    	
-    		if("fauth".equals(correction.getOriginal()))
-    		{
-    			assertTrue("faith".equals(correction.getCorrection()));
-    		} else if("home".equals(correction.getOriginal()))
-    		{
-    			assertTrue("hope".equals(correction.getCorrection()));
-    		} else if("loane".equals(correction.getOriginal()))
-    		{
-    			assertTrue("love".equals(correction.getCorrection()) || "loaves".equals(correction.getCorrection()));
-    		} else
-    		{
-    			fail("Original Word Should have been either fauth, home or loane.");
-    		}	    	
-    	}
+
+      List<Correction> misspellingsAndCorrections = collation.getMisspellingsAndCorrections();
+      assertTrue(misspellingsAndCorrections.size()==3);
+      for(Correction correction : misspellingsAndCorrections)
+      {
+        if("fauth".equals(correction.getOriginal()))
+        {
+          assertTrue("faith".equals(correction.getCorrection()));
+        } else if("home".equals(correction.getOriginal()))
+        {
+          assertTrue("hope".equals(correction.getCorrection()));
+        } else if("loane".equals(correction.getOriginal()))
+        {
+          assertTrue("love".equals(correction.getCorrection()) || "loaves".equals(correction.getCorrection()));
+        } else
+        {
+          fail("Original Word Should have been either fauth, home or loane.");
+        }
+      }
     }
     
     query.set(SpellingParams.SPELLCHECK_COLLATE_EXTENDED_RESULTS, false);

