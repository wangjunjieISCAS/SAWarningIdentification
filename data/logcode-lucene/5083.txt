GitDiffStart: dbe6c7ff052927be10a0a6953686771d788ebda5 | Fri Oct 24 04:48:13 2014 +0000
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/DictionaryCompoundWordTokenFilterFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/DictionaryCompoundWordTokenFilterFactory.java
index d84a530..cc461f4 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/DictionaryCompoundWordTokenFilterFactory.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/DictionaryCompoundWordTokenFilterFactory.java
@@ -22,13 +22,12 @@ import org.apache.lucene.analysis.util.CharArraySet;
 import org.apache.lucene.analysis.util.ResourceLoader;
 import org.apache.lucene.analysis.util.ResourceLoaderAware;
 import org.apache.lucene.analysis.util.TokenFilterFactory;
-import org.apache.lucene.util.Version;
 
 import java.util.Map;
 import java.io.IOException;
 
 /** 
- * Factory for {@link Lucene43DictionaryCompoundWordTokenFilter}.
+ * Factory for {@link DictionaryCompoundWordTokenFilter}.
  * <pre class="prettyprint">
  * &lt;fieldType name="text_dictcomp" class="solr.TextField" positionIncrementGap="100"&gt;
  *   &lt;analyzer&gt;
@@ -51,9 +50,9 @@ public class DictionaryCompoundWordTokenFilterFactory extends TokenFilterFactory
     super(args);
     assureMatchVersion();
     dictFile = require(args, "dictionary");
-    minWordSize = getInt(args, "minWordSize", Lucene43CompoundWordTokenFilterBase.DEFAULT_MIN_WORD_SIZE);
-    minSubwordSize = getInt(args, "minSubwordSize", Lucene43CompoundWordTokenFilterBase.DEFAULT_MIN_SUBWORD_SIZE);
-    maxSubwordSize = getInt(args, "maxSubwordSize", Lucene43CompoundWordTokenFilterBase.DEFAULT_MAX_SUBWORD_SIZE);
+    minWordSize = getInt(args, "minWordSize", CompoundWordTokenFilterBase.DEFAULT_MIN_WORD_SIZE);
+    minSubwordSize = getInt(args, "minSubwordSize", CompoundWordTokenFilterBase.DEFAULT_MIN_SUBWORD_SIZE);
+    maxSubwordSize = getInt(args, "maxSubwordSize", CompoundWordTokenFilterBase.DEFAULT_MAX_SUBWORD_SIZE);
     onlyLongestMatch = getBoolean(args, "onlyLongestMatch", true);
     if (!args.isEmpty()) {
       throw new IllegalArgumentException("Unknown parameters: " + args);
@@ -71,10 +70,7 @@ public class DictionaryCompoundWordTokenFilterFactory extends TokenFilterFactory
     if (dictionary == null) {
       return input;
     }
-    if (luceneMatchVersion.onOrAfter(Version.LUCENE_4_4_0)) {
-      return new DictionaryCompoundWordTokenFilter(input, dictionary, minWordSize, minSubwordSize, maxSubwordSize, onlyLongestMatch);
-    }
-    return new Lucene43DictionaryCompoundWordTokenFilter(input, dictionary, minWordSize, minSubwordSize, maxSubwordSize, onlyLongestMatch);
+    return new DictionaryCompoundWordTokenFilter(input, dictionary, minWordSize, minSubwordSize, maxSubwordSize, onlyLongestMatch);
   }
 }
 
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/HyphenationCompoundWordTokenFilterFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/HyphenationCompoundWordTokenFilterFactory.java
index 0f587b6..d4b04f4 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/HyphenationCompoundWordTokenFilterFactory.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/HyphenationCompoundWordTokenFilterFactory.java
@@ -34,7 +34,7 @@ import org.apache.lucene.util.Version;
 import org.xml.sax.InputSource;
 
 /**
- * Factory for {@link Lucene43HyphenationCompoundWordTokenFilter}.
+ * Factory for {@link HyphenationCompoundWordTokenFilter}.
  * <p>
  * This factory accepts the following parameters:
  * <ul>
@@ -58,7 +58,7 @@ import org.xml.sax.InputSource;
  *   &lt;/analyzer&gt;
  * &lt;/fieldType&gt;</pre>
  *
- * @see Lucene43HyphenationCompoundWordTokenFilter
+ * @see HyphenationCompoundWordTokenFilter
  */
 public class HyphenationCompoundWordTokenFilterFactory extends TokenFilterFactory implements ResourceLoaderAware {
   private CharArraySet dictionary;
@@ -78,9 +78,9 @@ public class HyphenationCompoundWordTokenFilterFactory extends TokenFilterFactor
     dictFile = get(args, "dictionary");
     encoding = get(args, "encoding");
     hypFile = require(args, "hyphenator");
-    minWordSize = getInt(args, "minWordSize", Lucene43CompoundWordTokenFilterBase.DEFAULT_MIN_WORD_SIZE);
-    minSubwordSize = getInt(args, "minSubwordSize", Lucene43CompoundWordTokenFilterBase.DEFAULT_MIN_SUBWORD_SIZE);
-    maxSubwordSize = getInt(args, "maxSubwordSize", Lucene43CompoundWordTokenFilterBase.DEFAULT_MAX_SUBWORD_SIZE);
+    minWordSize = getInt(args, "minWordSize", CompoundWordTokenFilterBase.DEFAULT_MIN_WORD_SIZE);
+    minSubwordSize = getInt(args, "minSubwordSize", CompoundWordTokenFilterBase.DEFAULT_MIN_SUBWORD_SIZE);
+    maxSubwordSize = getInt(args, "maxSubwordSize", CompoundWordTokenFilterBase.DEFAULT_MAX_SUBWORD_SIZE);
     onlyLongestMatch = getBoolean(args, "onlyLongestMatch", false);
     if (!args.isEmpty()) {
       throw new IllegalArgumentException("Unknown parameters: " + args);
@@ -99,11 +99,7 @@ public class HyphenationCompoundWordTokenFilterFactory extends TokenFilterFactor
       final InputSource is = new InputSource(stream);
       is.setEncoding(encoding); // if it's null let xml parser decide
       is.setSystemId(hypFile);
-      if (luceneMatchVersion.onOrAfter(Version.LUCENE_4_4_0)) {
-        hyphenator = HyphenationCompoundWordTokenFilter.getHyphenationTree(is);
-      } else {
-        hyphenator = Lucene43HyphenationCompoundWordTokenFilter.getHyphenationTree(is);
-      }
+      hyphenator = HyphenationCompoundWordTokenFilter.getHyphenationTree(is);
     } finally {
       IOUtils.closeWhileHandlingException(stream);
     }
@@ -111,9 +107,6 @@ public class HyphenationCompoundWordTokenFilterFactory extends TokenFilterFactor
   
   @Override
   public TokenFilter create(TokenStream input) {
-    if (luceneMatchVersion.onOrAfter(Version.LUCENE_4_4_0)) {
-      return new HyphenationCompoundWordTokenFilter(input, hyphenator, dictionary, minWordSize, minSubwordSize, maxSubwordSize, onlyLongestMatch);
-    }
-    return new Lucene43HyphenationCompoundWordTokenFilter(input, hyphenator, dictionary, minWordSize, minSubwordSize, maxSubwordSize, onlyLongestMatch);
+    return new HyphenationCompoundWordTokenFilter(input, hyphenator, dictionary, minWordSize, minSubwordSize, maxSubwordSize, onlyLongestMatch);
   }
 }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/Lucene43CompoundWordTokenFilterBase.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/Lucene43CompoundWordTokenFilterBase.java
deleted file mode 100644
index e5b4070..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/Lucene43CompoundWordTokenFilterBase.java
+++ /dev/null
@@ -1,162 +0,0 @@
-package org.apache.lucene.analysis.compound;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.LinkedList;
-
-import org.apache.lucene.analysis.TokenFilter;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.analysis.util.CharArraySet;
-import org.apache.lucene.util.AttributeSource;
-
-/**
- * Base class for decomposition token filters using pre-4.4 behavior.
- * <p>
- * @deprecated Use {@link CompoundWordTokenFilterBase}
- */
-@Deprecated
-public abstract class Lucene43CompoundWordTokenFilterBase extends TokenFilter {
-  /**
-   * The default for minimal word length that gets decomposed
-   */
-  public static final int DEFAULT_MIN_WORD_SIZE = 5;
-
-  /**
-   * The default for minimal length of subwords that get propagated to the output of this filter
-   */
-  public static final int DEFAULT_MIN_SUBWORD_SIZE = 2;
-
-  /**
-   * The default for maximal length of subwords that get propagated to the output of this filter
-   */
-  public static final int DEFAULT_MAX_SUBWORD_SIZE = 15;
-
-  protected final CharArraySet dictionary;
-  protected final LinkedList<CompoundToken> tokens;
-  protected final int minWordSize;
-  protected final int minSubwordSize;
-  protected final int maxSubwordSize;
-  protected final boolean onlyLongestMatch;
-  
-  protected final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-  protected final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
-  private final PositionIncrementAttribute posIncAtt = addAttribute(PositionIncrementAttribute.class);
-  
-  private AttributeSource.State current;
-
-  protected Lucene43CompoundWordTokenFilterBase(TokenStream input, CharArraySet dictionary, boolean onlyLongestMatch) {
-    this(input,dictionary,DEFAULT_MIN_WORD_SIZE,DEFAULT_MIN_SUBWORD_SIZE,DEFAULT_MAX_SUBWORD_SIZE, onlyLongestMatch);
-  }
-
-  protected Lucene43CompoundWordTokenFilterBase(TokenStream input, CharArraySet dictionary) {
-    this(input,dictionary,DEFAULT_MIN_WORD_SIZE,DEFAULT_MIN_SUBWORD_SIZE,DEFAULT_MAX_SUBWORD_SIZE, false);
-  }
-
-  protected Lucene43CompoundWordTokenFilterBase(TokenStream input, CharArraySet dictionary, int minWordSize, int minSubwordSize, int maxSubwordSize, boolean onlyLongestMatch) {
-    super(input);
-    this.tokens=new LinkedList<>();
-    if (minWordSize < 0) {
-      throw new IllegalArgumentException("minWordSize cannot be negative");
-    }
-    this.minWordSize=minWordSize;
-    if (minSubwordSize < 0) {
-      throw new IllegalArgumentException("minSubwordSize cannot be negative");
-    }
-    this.minSubwordSize=minSubwordSize;
-    if (maxSubwordSize < 0) {
-      throw new IllegalArgumentException("maxSubwordSize cannot be negative");
-    }
-    this.maxSubwordSize=maxSubwordSize;
-    this.onlyLongestMatch=onlyLongestMatch;
-    this.dictionary = dictionary;
-  }
-  
-  @Override
-  public final boolean incrementToken() throws IOException {
-    if (!tokens.isEmpty()) {
-      assert current != null;
-      CompoundToken token = tokens.removeFirst();
-      restoreState(current); // keep all other attributes untouched
-      termAtt.setEmpty().append(token.txt);
-      offsetAtt.setOffset(token.startOffset, token.endOffset);
-      posIncAtt.setPositionIncrement(0);
-      return true;
-    }
-
-    current = null; // not really needed, but for safety
-    if (input.incrementToken()) {
-      // Only words longer than minWordSize get processed
-      if (termAtt.length() >= this.minWordSize) {
-        decompose();
-        // only capture the state if we really need it for producing new tokens
-        if (!tokens.isEmpty()) {
-          current = captureState();
-        }
-      }
-      // return original token:
-      return true;
-    } else {
-      return false;
-    }
-  }
-
-  /** Decomposes the current {@link #termAtt} and places {@link CompoundToken} instances in the {@link #tokens} list.
-   * The original token may not be placed in the list, as it is automatically passed through this filter.
-   */
-  protected abstract void decompose();
-
-  @Override
-  public void reset() throws IOException {
-    super.reset();
-    tokens.clear();
-    current = null;
-  }
-  
-  /**
-   * Helper class to hold decompounded token information
-   */
-  protected class CompoundToken {
-    public final CharSequence txt;
-    public final int startOffset, endOffset;
-
-    /** Construct the compound token based on a slice of the current {@link Lucene43CompoundWordTokenFilterBase#termAtt}. */
-    public CompoundToken(int offset, int length) {
-      this.txt = Lucene43CompoundWordTokenFilterBase.this.termAtt.subSequence(offset, offset + length);
-      
-      // offsets of the original word
-      int startOff = Lucene43CompoundWordTokenFilterBase.this.offsetAtt.startOffset();
-      int endOff = Lucene43CompoundWordTokenFilterBase.this.offsetAtt.endOffset();
-      
-      if (endOff - startOff != Lucene43CompoundWordTokenFilterBase.this.termAtt.length()) {
-        // if length by start + end offsets doesn't match the term text then assume
-        // this is a synonym and don't adjust the offsets.
-        this.startOffset = startOff;
-        this.endOffset = endOff;
-      } else {
-        final int newStart = startOff + offset;
-        this.startOffset = newStart;
-        this.endOffset = newStart + length;
-      }
-    }
-
-  }  
-}
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/Lucene43DictionaryCompoundWordTokenFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/Lucene43DictionaryCompoundWordTokenFilter.java
deleted file mode 100644
index ec856e1..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/Lucene43DictionaryCompoundWordTokenFilter.java
+++ /dev/null
@@ -1,100 +0,0 @@
-package org.apache.lucene.analysis.compound;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-import org.apache.lucene.analysis.TokenFilter;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.util.CharArraySet;
-
-/**
- * A {@link TokenFilter} that decomposes compound words found in many Germanic languages, using
- * pre-4.4 behavior.
- * @deprecated Use {@link DictionaryCompoundWordTokenFilter}.
- */
-@Deprecated
-public class Lucene43DictionaryCompoundWordTokenFilter extends Lucene43CompoundWordTokenFilterBase {
-  
-  /**
-   * Creates a new {@link Lucene43DictionaryCompoundWordTokenFilter}
-   *
-   * @param input
-   *          the {@link TokenStream} to process
-   * @param dictionary
-   *          the word dictionary to match against.
-   */
-  public Lucene43DictionaryCompoundWordTokenFilter(TokenStream input, CharArraySet dictionary) {
-    super(input, dictionary);
-    if (dictionary == null) {
-      throw new IllegalArgumentException("dictionary cannot be null");
-    }
-  }
-  
-  /**
-   * Creates a new {@link Lucene43DictionaryCompoundWordTokenFilter}
-   *
-   * @param input
-   *          the {@link TokenStream} to process
-   * @param dictionary
-   *          the word dictionary to match against.
-   * @param minWordSize
-   *          only words longer than this get processed
-   * @param minSubwordSize
-   *          only subwords longer than this get to the output stream
-   * @param maxSubwordSize
-   *          only subwords shorter than this get to the output stream
-   * @param onlyLongestMatch
-   *          Add only the longest matching subword to the stream
-   */
-  public Lucene43DictionaryCompoundWordTokenFilter(TokenStream input, CharArraySet dictionary,
-                                                   int minWordSize, int minSubwordSize, int maxSubwordSize, boolean onlyLongestMatch) {
-    super(input, dictionary, minWordSize, minSubwordSize, maxSubwordSize, onlyLongestMatch);
-    if (dictionary == null) {
-      throw new IllegalArgumentException("dictionary cannot be null");
-    }
-  }
-
-  @Override
-  protected void decompose() {
-    final int len = termAtt.length();
-    for (int i=0;i<=len-this.minSubwordSize;++i) {
-        CompoundToken longestMatchToken=null;
-        for (int j=this.minSubwordSize;j<=this.maxSubwordSize;++j) {
-            if(i+j>len) {
-                break;
-            }
-            if(dictionary.contains(termAtt.buffer(), i, j)) {
-                if (this.onlyLongestMatch) {
-                   if (longestMatchToken!=null) {
-                     if (longestMatchToken.txt.length()<j) {
-                       longestMatchToken=new CompoundToken(i,j);
-                     }
-                   } else {
-                     longestMatchToken=new CompoundToken(i,j);
-                   }
-                } else {
-                   tokens.add(new CompoundToken(i,j));
-                }
-            } 
-        }
-        if (this.onlyLongestMatch && longestMatchToken!=null) {
-          tokens.add(longestMatchToken);
-        }
-    }
-  }
-}
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/Lucene43HyphenationCompoundWordTokenFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/Lucene43HyphenationCompoundWordTokenFilter.java
deleted file mode 100644
index 050dcf8..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/Lucene43HyphenationCompoundWordTokenFilter.java
+++ /dev/null
@@ -1,204 +0,0 @@
-package org.apache.lucene.analysis.compound;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.analysis.TokenFilter;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.compound.hyphenation.Hyphenation;
-import org.apache.lucene.analysis.compound.hyphenation.HyphenationTree;
-import org.apache.lucene.analysis.util.CharArraySet;
-import org.xml.sax.InputSource;
-
-/**
- * A {@link TokenFilter} that decomposes compound words found in many Germanic languages,
- * using pre-4.4 behavior.
- *
- * @deprecated Use {@link HyphenationCompoundWordTokenFilter}.
- */
-@Deprecated
-public class Lucene43HyphenationCompoundWordTokenFilter extends
-    Lucene43CompoundWordTokenFilterBase {
-  private HyphenationTree hyphenator;
-
-  /**
-   * Creates a new {@link Lucene43HyphenationCompoundWordTokenFilter} instance.
-   *
-   * @param input
-   *          the {@link TokenStream} to process
-   * @param hyphenator
-   *          the hyphenation pattern tree to use for hyphenation
-   * @param dictionary
-   *          the word dictionary to match against.
-   */
-  public Lucene43HyphenationCompoundWordTokenFilter(TokenStream input,
-                                                    HyphenationTree hyphenator, CharArraySet dictionary) {
-    this(input, hyphenator, dictionary, DEFAULT_MIN_WORD_SIZE,
-        DEFAULT_MIN_SUBWORD_SIZE, DEFAULT_MAX_SUBWORD_SIZE, false);
-  }
-
-  /**
-   * Creates a new {@link Lucene43HyphenationCompoundWordTokenFilter} instance.
-   *
-   * @param input
-   *          the {@link TokenStream} to process
-   * @param hyphenator
-   *          the hyphenation pattern tree to use for hyphenation
-   * @param dictionary
-   *          the word dictionary to match against.
-   * @param minWordSize
-   *          only words longer than this get processed
-   * @param minSubwordSize
-   *          only subwords longer than this get to the output stream
-   * @param maxSubwordSize
-   *          only subwords shorter than this get to the output stream
-   * @param onlyLongestMatch
-   *          Add only the longest matching subword to the stream
-   */
-  public Lucene43HyphenationCompoundWordTokenFilter(TokenStream input,
-                                                    HyphenationTree hyphenator, CharArraySet dictionary, int minWordSize,
-                                                    int minSubwordSize, int maxSubwordSize, boolean onlyLongestMatch) {
-    super(input, dictionary, minWordSize, minSubwordSize, maxSubwordSize,
-        onlyLongestMatch);
-
-    this.hyphenator = hyphenator;
-  }
-
-  /**
-   * Create a HyphenationCompoundWordTokenFilter with no dictionary.
-   * <p>
-   * Calls {@link #Lucene43HyphenationCompoundWordTokenFilter(TokenStream, HyphenationTree, CharArraySet, int, int, int, boolean)
-   * HyphenationCompoundWordTokenFilter(matchVersion, input, hyphenator,
-   * null, minWordSize, minSubwordSize, maxSubwordSize }
-   */
-  public Lucene43HyphenationCompoundWordTokenFilter(TokenStream input,
-                                                    HyphenationTree hyphenator, int minWordSize, int minSubwordSize,
-                                                    int maxSubwordSize) {
-    this(input, hyphenator, null, minWordSize, minSubwordSize,
-        maxSubwordSize, false);
-  }
-  
-  /**
-   * Create a HyphenationCompoundWordTokenFilter with no dictionary.
-   * <p>
-   * Calls {@link #Lucene43HyphenationCompoundWordTokenFilter(TokenStream, HyphenationTree, int, int, int)
-   * HyphenationCompoundWordTokenFilter(matchVersion, input, hyphenator, 
-   * DEFAULT_MIN_WORD_SIZE, DEFAULT_MIN_SUBWORD_SIZE, DEFAULT_MAX_SUBWORD_SIZE }
-   */
-  public Lucene43HyphenationCompoundWordTokenFilter(TokenStream input,
-                                                    HyphenationTree hyphenator) {
-    this(input, hyphenator, DEFAULT_MIN_WORD_SIZE, DEFAULT_MIN_SUBWORD_SIZE,
-        DEFAULT_MAX_SUBWORD_SIZE);
-  }
-
-  /**
-   * Create a hyphenator tree
-   * 
-   * @param hyphenationFilename the filename of the XML grammar to load
-   * @return An object representing the hyphenation patterns
-   * @throws IOException If there is a low-level I/O error.
-   */
-  public static HyphenationTree getHyphenationTree(String hyphenationFilename)
-      throws IOException {
-    return getHyphenationTree(new InputSource(hyphenationFilename));
-  }
-
-  /**
-   * Create a hyphenator tree
-   * 
-   * @param hyphenationSource the InputSource pointing to the XML grammar
-   * @return An object representing the hyphenation patterns
-   * @throws IOException If there is a low-level I/O error.
-   */
-  public static HyphenationTree getHyphenationTree(InputSource hyphenationSource)
-      throws IOException {
-    HyphenationTree tree = new HyphenationTree();
-    tree.loadPatterns(hyphenationSource);
-    return tree;
-  }
-
-  @Override
-  protected void decompose() {
-    // get the hyphenation points
-    Hyphenation hyphens = hyphenator.hyphenate(termAtt.buffer(), 0, termAtt.length(), 1, 1);
-    // No hyphen points found -> exit
-    if (hyphens == null) {
-      return;
-    }
-
-    final int[] hyp = hyphens.getHyphenationPoints();
-
-    for (int i = 0; i < hyp.length; ++i) {
-      int remaining = hyp.length - i;
-      int start = hyp[i];
-      CompoundToken longestMatchToken = null;
-      for (int j = 1; j < remaining; j++) {
-        int partLength = hyp[i + j] - start;
-
-        // if the part is longer than maxSubwordSize we
-        // are done with this round
-        if (partLength > this.maxSubwordSize) {
-          break;
-        }
-
-        // we only put subwords to the token stream
-        // that are longer than minPartSize
-        if (partLength < this.minSubwordSize) {
-          // BOGUS/BROKEN/FUNKY/WACKO: somehow we have negative 'parts' according to the 
-          // calculation above, and we rely upon minSubwordSize being >=0 to filter them out...
-          continue;
-        }
-
-        // check the dictionary
-        if (dictionary == null || dictionary.contains(termAtt.buffer(), start, partLength)) {
-          if (this.onlyLongestMatch) {
-            if (longestMatchToken != null) {
-              if (longestMatchToken.txt.length() < partLength) {
-                longestMatchToken = new CompoundToken(start, partLength);
-              }
-            } else {
-              longestMatchToken = new CompoundToken(start, partLength);
-            }
-          } else {
-            tokens.add(new CompoundToken(start, partLength));
-          }
-        } else if (dictionary.contains(termAtt.buffer(), start, partLength - 1)) {
-          // check the dictionary again with a word that is one character
-          // shorter
-          // to avoid problems with genitive 's characters and other binding
-          // characters
-          if (this.onlyLongestMatch) {
-            if (longestMatchToken != null) {
-              if (longestMatchToken.txt.length() < partLength - 1) {
-                longestMatchToken = new CompoundToken(start, partLength - 1);
-              }
-            } else {
-              longestMatchToken = new CompoundToken(start, partLength - 1);
-            }
-          } else {
-            tokens.add(new CompoundToken(start, partLength - 1));
-          }
-        }
-      }
-      if (this.onlyLongestMatch && longestMatchToken!=null) {
-        tokens.add(longestMatchToken);
-      }
-    }
-  }
-}
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/Lucene47WordDelimiterFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/Lucene47WordDelimiterFilter.java
deleted file mode 100644
index 6cf312f..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/Lucene47WordDelimiterFilter.java
+++ /dev/null
@@ -1,556 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
- 
-package org.apache.lucene.analysis.miscellaneous;
-
-import org.apache.lucene.analysis.TokenFilter;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.core.WhitespaceTokenizer;
-import org.apache.lucene.analysis.standard.StandardTokenizer;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
-import org.apache.lucene.analysis.util.CharArraySet;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.RamUsageEstimator;
-
-import java.io.IOException;
-
-/**
- * Old Broken version of {@link WordDelimiterFilter}
- */
-@Deprecated
-public final class Lucene47WordDelimiterFilter extends TokenFilter {
-  
-  public static final int LOWER = 0x01;
-  public static final int UPPER = 0x02;
-  public static final int DIGIT = 0x04;
-  public static final int SUBWORD_DELIM = 0x08;
-
-  // combinations: for testing, not for setting bits
-  public static final int ALPHA = 0x03;
-  public static final int ALPHANUM = 0x07;
-
-  /**
-   * Causes parts of words to be generated:
-   * <p/>
-   * "PowerShot" => "Power" "Shot"
-   */
-  public static final int GENERATE_WORD_PARTS = 1;
-
-  /**
-   * Causes number subwords to be generated:
-   * <p/>
-   * "500-42" => "500" "42"
-   */
-  public static final int GENERATE_NUMBER_PARTS = 2;
-
-  /**
-   * Causes maximum runs of word parts to be catenated:
-   * <p/>
-   * "wi-fi" => "wifi"
-   */
-  public static final int CATENATE_WORDS = 4;
-
-  /**
-   * Causes maximum runs of word parts to be catenated:
-   * <p/>
-   * "wi-fi" => "wifi"
-   */
-  public static final int CATENATE_NUMBERS = 8;
-
-  /**
-   * Causes all subword parts to be catenated:
-   * <p/>
-   * "wi-fi-4000" => "wifi4000"
-   */
-  public static final int CATENATE_ALL = 16;
-
-  /**
-   * Causes original words are preserved and added to the subword list (Defaults to false)
-   * <p/>
-   * "500-42" => "500" "42" "500-42"
-   */
-  public static final int PRESERVE_ORIGINAL = 32;
-
-  /**
-   * If not set, causes case changes to be ignored (subwords will only be generated
-   * given SUBWORD_DELIM tokens)
-   */
-  public static final int SPLIT_ON_CASE_CHANGE = 64;
-
-  /**
-   * If not set, causes numeric changes to be ignored (subwords will only be generated
-   * given SUBWORD_DELIM tokens).
-   */
-  public static final int SPLIT_ON_NUMERICS = 128;
-
-  /**
-   * Causes trailing "'s" to be removed for each subword
-   * <p/>
-   * "O'Neil's" => "O", "Neil"
-   */
-  public static final int STEM_ENGLISH_POSSESSIVE = 256;
-  
-  /**
-   * If not null is the set of tokens to protect from being delimited
-   *
-   */
-  final CharArraySet protWords;
-
-  private final int flags;
-    
-  private final CharTermAttribute termAttribute = addAttribute(CharTermAttribute.class);
-  private final OffsetAttribute offsetAttribute = addAttribute(OffsetAttribute.class);
-  private final PositionIncrementAttribute posIncAttribute = addAttribute(PositionIncrementAttribute.class);
-  private final TypeAttribute typeAttribute = addAttribute(TypeAttribute.class);
-
-  // used for iterating word delimiter breaks
-  private final WordDelimiterIterator iterator;
-
-  // used for concatenating runs of similar typed subwords (word,number)
-  private final WordDelimiterConcatenation concat = new WordDelimiterConcatenation();
-  // number of subwords last output by concat.
-  private int lastConcatCount = 0;
-
-  // used for catenate all
-  private final WordDelimiterConcatenation concatAll = new WordDelimiterConcatenation();
-
-  // used for accumulating position increment gaps
-  private int accumPosInc = 0;
-
-  private char savedBuffer[] = new char[1024];
-  private int savedStartOffset;
-  private int savedEndOffset;
-  private String savedType;
-  private boolean hasSavedState = false;
-  // if length by start + end offsets doesn't match the term text then assume
-  // this is a synonym and don't adjust the offsets.
-  private boolean hasIllegalOffsets = false;
-
-  // for a run of the same subword type within a word, have we output anything?
-  private boolean hasOutputToken = false;
-  // when preserve original is on, have we output any token following it?
-  // this token must have posInc=0!
-  private boolean hasOutputFollowingOriginal = false;
-
-  /**
-   * Creates a new WordDelimiterFilter
-   *
-   * @param in TokenStream to be filtered
-   * @param charTypeTable table containing character types
-   * @param configurationFlags Flags configuring the filter
-   * @param protWords If not null is the set of tokens to protect from being delimited
-   */
-  public Lucene47WordDelimiterFilter(TokenStream in, byte[] charTypeTable, int configurationFlags, CharArraySet protWords) {
-    super(in);
-    this.flags = configurationFlags;
-    this.protWords = protWords;
-    this.iterator = new WordDelimiterIterator(
-        charTypeTable, has(SPLIT_ON_CASE_CHANGE), has(SPLIT_ON_NUMERICS), has(STEM_ENGLISH_POSSESSIVE));
-  }
-
-  /**
-   * Creates a new WordDelimiterFilter using {@link WordDelimiterIterator#DEFAULT_WORD_DELIM_TABLE}
-   * as its charTypeTable
-   *
-   * @param in TokenStream to be filtered
-   * @param configurationFlags Flags configuring the filter
-   * @param protWords If not null is the set of tokens to protect from being delimited
-   */
-  public Lucene47WordDelimiterFilter(TokenStream in, int configurationFlags, CharArraySet protWords) {
-    this(in, WordDelimiterIterator.DEFAULT_WORD_DELIM_TABLE, configurationFlags, protWords);
-  }
-
-  @Override
-  public boolean incrementToken() throws IOException {
-    while (true) {
-      if (!hasSavedState) {
-        // process a new input word
-        if (!input.incrementToken()) {
-          return false;
-        }
-
-        int termLength = termAttribute.length();
-        char[] termBuffer = termAttribute.buffer();
-        
-        accumPosInc += posIncAttribute.getPositionIncrement();
-
-        iterator.setText(termBuffer, termLength);
-        iterator.next();
-
-        // word of no delimiters, or protected word: just return it
-        if ((iterator.current == 0 && iterator.end == termLength) ||
-            (protWords != null && protWords.contains(termBuffer, 0, termLength))) {
-          posIncAttribute.setPositionIncrement(accumPosInc);
-          accumPosInc = 0;
-          return true;
-        }
-        
-        // word of simply delimiters
-        if (iterator.end == WordDelimiterIterator.DONE && !has(PRESERVE_ORIGINAL)) {
-          // if the posInc is 1, simply ignore it in the accumulation
-          if (posIncAttribute.getPositionIncrement() == 1) {
-            accumPosInc--;
-          }
-          continue;
-        }
-
-        saveState();
-
-        hasOutputToken = false;
-        hasOutputFollowingOriginal = !has(PRESERVE_ORIGINAL);
-        lastConcatCount = 0;
-        
-        if (has(PRESERVE_ORIGINAL)) {
-          posIncAttribute.setPositionIncrement(accumPosInc);
-          accumPosInc = 0;
-          return true;
-        }
-      }
-      
-      // at the end of the string, output any concatenations
-      if (iterator.end == WordDelimiterIterator.DONE) {
-        if (!concat.isEmpty()) {
-          if (flushConcatenation(concat)) {
-            return true;
-          }
-        }
-        
-        if (!concatAll.isEmpty()) {
-          // only if we haven't output this same combo above!
-          if (concatAll.subwordCount > lastConcatCount) {
-            concatAll.writeAndClear();
-            return true;
-          }
-          concatAll.clear();
-        }
-        
-        // no saved concatenations, on to the next input word
-        hasSavedState = false;
-        continue;
-      }
-      
-      // word surrounded by delimiters: always output
-      if (iterator.isSingleWord()) {
-        generatePart(true);
-        iterator.next();
-        return true;
-      }
-      
-      int wordType = iterator.type();
-      
-      // do we already have queued up incompatible concatenations?
-      if (!concat.isEmpty() && (concat.type & wordType) == 0) {
-        if (flushConcatenation(concat)) {
-          hasOutputToken = false;
-          return true;
-        }
-        hasOutputToken = false;
-      }
-      
-      // add subwords depending upon options
-      if (shouldConcatenate(wordType)) {
-        if (concat.isEmpty()) {
-          concat.type = wordType;
-        }
-        concatenate(concat);
-      }
-      
-      // add all subwords (catenateAll)
-      if (has(CATENATE_ALL)) {
-        concatenate(concatAll);
-      }
-      
-      // if we should output the word or number part
-      if (shouldGenerateParts(wordType)) {
-        generatePart(false);
-        iterator.next();
-        return true;
-      }
-        
-      iterator.next();
-    }
-  }
-
-  /**
-   * {@inheritDoc}
-   */
-  @Override
-  public void reset() throws IOException {
-    super.reset();
-    hasSavedState = false;
-    concat.clear();
-    concatAll.clear();
-    accumPosInc = 0;
-  }
-
-  // ================================================= Helper Methods ================================================
-
-  /**
-   * Saves the existing attribute states
-   */
-  private void saveState() {
-    // otherwise, we have delimiters, save state
-    savedStartOffset = offsetAttribute.startOffset();
-    savedEndOffset = offsetAttribute.endOffset();
-    // if length by start + end offsets doesn't match the term text then assume this is a synonym and don't adjust the offsets.
-    hasIllegalOffsets = (savedEndOffset - savedStartOffset != termAttribute.length());
-    savedType = typeAttribute.type();
-
-    if (savedBuffer.length < termAttribute.length()) {
-      savedBuffer = new char[ArrayUtil.oversize(termAttribute.length(), RamUsageEstimator.NUM_BYTES_CHAR)];
-    }
-
-    System.arraycopy(termAttribute.buffer(), 0, savedBuffer, 0, termAttribute.length());
-    iterator.text = savedBuffer;
-
-    hasSavedState = true;
-  }
-
-  /**
-   * Flushes the given WordDelimiterConcatenation by either writing its concat and then clearing, or just clearing.
-   *
-   * @param concatenation WordDelimiterConcatenation that will be flushed
-   * @return {@code true} if the concatenation was written before it was cleared, {@code false} otherwise
-   */
-  private boolean flushConcatenation(WordDelimiterConcatenation concatenation) {
-    lastConcatCount = concatenation.subwordCount;
-    if (concatenation.subwordCount != 1 || !shouldGenerateParts(concatenation.type)) {
-      concatenation.writeAndClear();
-      return true;
-    }
-    concatenation.clear();
-    return false;
-  }
-
-  /**
-   * Determines whether to concatenate a word or number if the current word is the given type
-   *
-   * @param wordType Type of the current word used to determine if it should be concatenated
-   * @return {@code true} if concatenation should occur, {@code false} otherwise
-   */
-  private boolean shouldConcatenate(int wordType) {
-    return (has(CATENATE_WORDS) && isAlpha(wordType)) || (has(CATENATE_NUMBERS) && isDigit(wordType));
-  }
-
-  /**
-   * Determines whether a word/number part should be generated for a word of the given type
-   *
-   * @param wordType Type of the word used to determine if a word/number part should be generated
-   * @return {@code true} if a word/number part should be generated, {@code false} otherwise
-   */
-  private boolean shouldGenerateParts(int wordType) {
-    return (has(GENERATE_WORD_PARTS) && isAlpha(wordType)) || (has(GENERATE_NUMBER_PARTS) && isDigit(wordType));
-  }
-
-  /**
-   * Concatenates the saved buffer to the given WordDelimiterConcatenation
-   *
-   * @param concatenation WordDelimiterConcatenation to concatenate the buffer to
-   */
-  private void concatenate(WordDelimiterConcatenation concatenation) {
-    if (concatenation.isEmpty()) {
-      concatenation.startOffset = savedStartOffset + iterator.current;
-    }
-    concatenation.append(savedBuffer, iterator.current, iterator.end - iterator.current);
-    concatenation.endOffset = savedStartOffset + iterator.end;
-  }
-
-  /**
-   * Generates a word/number part, updating the appropriate attributes
-   *
-   * @param isSingleWord {@code true} if the generation is occurring from a single word, {@code false} otherwise
-   */
-  private void generatePart(boolean isSingleWord) {
-    clearAttributes();
-    termAttribute.copyBuffer(savedBuffer, iterator.current, iterator.end - iterator.current);
-
-    int startOffset = savedStartOffset + iterator.current;
-    int endOffset = savedStartOffset + iterator.end;
-    
-    if (hasIllegalOffsets) {
-      // historically this filter did this regardless for 'isSingleWord', 
-      // but we must do a sanity check:
-      if (isSingleWord && startOffset <= savedEndOffset) {
-        offsetAttribute.setOffset(startOffset, savedEndOffset);
-      } else {
-        offsetAttribute.setOffset(savedStartOffset, savedEndOffset);
-      }
-    } else {
-      offsetAttribute.setOffset(startOffset, endOffset);
-    }
-    posIncAttribute.setPositionIncrement(position(false));
-    typeAttribute.setType(savedType);
-  }
-
-  /**
-   * Get the position increment gap for a subword or concatenation
-   *
-   * @param inject true if this token wants to be injected
-   * @return position increment gap
-   */
-  private int position(boolean inject) {
-    int posInc = accumPosInc;
-
-    if (hasOutputToken) {
-      accumPosInc = 0;
-      return inject ? 0 : Math.max(1, posInc);
-    }
-
-    hasOutputToken = true;
-    
-    if (!hasOutputFollowingOriginal) {
-      // the first token following the original is 0 regardless
-      hasOutputFollowingOriginal = true;
-      return 0;
-    }
-    // clear the accumulated position increment
-    accumPosInc = 0;
-    return Math.max(1, posInc);
-  }
-
-  /**
-   * Checks if the given word type includes {@link #ALPHA}
-   *
-   * @param type Word type to check
-   * @return {@code true} if the type contains ALPHA, {@code false} otherwise
-   */
-  static boolean isAlpha(int type) {
-    return (type & ALPHA) != 0;
-  }
-
-  /**
-   * Checks if the given word type includes {@link #DIGIT}
-   *
-   * @param type Word type to check
-   * @return {@code true} if the type contains DIGIT, {@code false} otherwise
-   */
-  static boolean isDigit(int type) {
-    return (type & DIGIT) != 0;
-  }
-
-  /**
-   * Checks if the given word type includes {@link #SUBWORD_DELIM}
-   *
-   * @param type Word type to check
-   * @return {@code true} if the type contains SUBWORD_DELIM, {@code false} otherwise
-   */
-  static boolean isSubwordDelim(int type) {
-    return (type & SUBWORD_DELIM) != 0;
-  }
-
-  /**
-   * Checks if the given word type includes {@link #UPPER}
-   *
-   * @param type Word type to check
-   * @return {@code true} if the type contains UPPER, {@code false} otherwise
-   */
-  static boolean isUpper(int type) {
-    return (type & UPPER) != 0;
-  }
-
-  /**
-   * Determines whether the given flag is set
-   *
-   * @param flag Flag to see if set
-   * @return {@code true} if flag is set
-   */
-  private boolean has(int flag) {
-    return (flags & flag) != 0;
-  }
-
-  // ================================================= Inner Classes =================================================
-
-  /**
-   * A WDF concatenated 'run'
-   */
-  final class WordDelimiterConcatenation {
-    final StringBuilder buffer = new StringBuilder();
-    int startOffset;
-    int endOffset;
-    int type;
-    int subwordCount;
-
-    /**
-     * Appends the given text of the given length, to the concetenation at the given offset
-     *
-     * @param text Text to append
-     * @param offset Offset in the concetenation to add the text
-     * @param length Length of the text to append
-     */
-    void append(char text[], int offset, int length) {
-      buffer.append(text, offset, length);
-      subwordCount++;
-    }
-
-    /**
-     * Writes the concatenation to the attributes
-     */
-    void write() {
-      clearAttributes();
-      if (termAttribute.length() < buffer.length()) {
-        termAttribute.resizeBuffer(buffer.length());
-      }
-      char termbuffer[] = termAttribute.buffer();
-      
-      buffer.getChars(0, buffer.length(), termbuffer, 0);
-      termAttribute.setLength(buffer.length());
-        
-      if (hasIllegalOffsets) {
-        offsetAttribute.setOffset(savedStartOffset, savedEndOffset);
-      }
-      else {
-        offsetAttribute.setOffset(startOffset, endOffset);
-      }
-      posIncAttribute.setPositionIncrement(position(true));
-      typeAttribute.setType(savedType);
-      accumPosInc = 0;
-    }
-
-    /**
-     * Determines if the concatenation is empty
-     *
-     * @return {@code true} if the concatenation is empty, {@code false} otherwise
-     */
-    boolean isEmpty() {
-      return buffer.length() == 0;
-    }
-
-    /**
-     * Clears the concatenation and resets its state
-     */
-    void clear() {
-      buffer.setLength(0);
-      startOffset = endOffset = type = subwordCount = 0;
-    }
-
-    /**
-     * Convenience method for the common scenario of having to write the concetenation and then clearing its state
-     */
-    void writeAndClear() {
-      write();
-      clear();
-    }
-  }
-  // questions:
-  // negative numbers?  -42 indexed as just 42?
-  // dollar sign?  $42
-  // percent sign?  33%
-  // downsides:  if source text is "powershot" then a query of "PowerShot" won't match!
-}
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/WordDelimiterFilterFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/WordDelimiterFilterFactory.java
index 123ed9e..9ab83c8 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/WordDelimiterFilterFactory.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/WordDelimiterFilterFactory.java
@@ -118,13 +118,8 @@ public class WordDelimiterFilterFactory extends TokenFilterFactory implements Re
 
   @Override
   public TokenFilter create(TokenStream input) {
-    if (luceneMatchVersion.onOrAfter(Version.LUCENE_4_8_0)) {
-      return new WordDelimiterFilter(input, typeTable == null ? WordDelimiterIterator.DEFAULT_WORD_DELIM_TABLE : typeTable,
+    return new WordDelimiterFilter(input, typeTable == null ? WordDelimiterIterator.DEFAULT_WORD_DELIM_TABLE : typeTable,
                                    flags, protectedWords);
-    } else {
-      return new Lucene47WordDelimiterFilter(input, typeTable == null ? WordDelimiterIterator.DEFAULT_WORD_DELIM_TABLE : typeTable,
-                                  flags, protectedWords);
-    }
   }
   
   // source => type
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramFilterFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramFilterFactory.java
index 131dc57..fddf7cd 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramFilterFactory.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramFilterFactory.java
@@ -22,7 +22,6 @@ import java.util.Map;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.util.TokenFilterFactory;
-import org.apache.lucene.util.Version;
 
 /**
  * Creates new instances of {@link EdgeNGramTokenFilter}.
@@ -50,9 +49,6 @@ public class EdgeNGramFilterFactory extends TokenFilterFactory {
 
   @Override
   public TokenFilter create(TokenStream input) {
-    if (luceneMatchVersion.onOrAfter(Version.LUCENE_4_4_0)) {
-      return new EdgeNGramTokenFilter(input, minGramSize, maxGramSize);
-    }
-    return new Lucene43EdgeNGramTokenFilter(input, minGramSize, maxGramSize);
+    return new EdgeNGramTokenFilter(input, minGramSize, maxGramSize);
   }
 }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenizerFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenizerFactory.java
index 6931026..c751fcd 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenizerFactory.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenizerFactory.java
@@ -20,7 +20,6 @@ package org.apache.lucene.analysis.ngram;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.util.TokenizerFactory;
 import org.apache.lucene.util.AttributeFactory;
-import org.apache.lucene.util.Version;
 
 import java.util.Map;
 
@@ -49,9 +48,6 @@ public class EdgeNGramTokenizerFactory extends TokenizerFactory {
   
   @Override
   public Tokenizer create(AttributeFactory factory) {
-    if (luceneMatchVersion.onOrAfter(Version.LUCENE_4_4_0)) {
-      return new EdgeNGramTokenizer(factory, minGramSize, maxGramSize);
-    }
-    return new Lucene43NGramTokenizer(factory, minGramSize, maxGramSize);
+    return new EdgeNGramTokenizer(factory, minGramSize, maxGramSize);
   }
 }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/Lucene43EdgeNGramTokenFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/Lucene43EdgeNGramTokenFilter.java
deleted file mode 100644
index d465ce9..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/Lucene43EdgeNGramTokenFilter.java
+++ /dev/null
@@ -1,126 +0,0 @@
-package org.apache.lucene.analysis.ngram;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.TokenFilter;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionLengthAttribute;
-import org.apache.lucene.analysis.util.CharacterUtils;
-
-import java.io.IOException;
-
-/**
- * Tokenizes the given token into n-grams of given size(s), using pre-4.4 behavior.
- *
- * @deprecated Use {@link org.apache.lucene.analysis.ngram.EdgeNGramTokenFilter}.
- */
-@Deprecated
-public final class Lucene43EdgeNGramTokenFilter extends TokenFilter {
-  public static final int DEFAULT_MAX_GRAM_SIZE = 1;
-  public static final int DEFAULT_MIN_GRAM_SIZE = 1;
-
-  private final CharacterUtils charUtils;
-  private final int minGram;
-  private final int maxGram;
-  private char[] curTermBuffer;
-  private int curTermLength;
-  private int curCodePointCount;
-  private int curGramSize;
-  private int tokStart;
-  private int tokEnd; // only used if the length changed before this filter
-  private int savePosIncr;
-  private int savePosLen;
-
-  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-  private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
-  private final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);
-  private final PositionLengthAttribute posLenAtt = addAttribute(PositionLengthAttribute.class);
-
-  /**
-   * Creates EdgeNGramTokenFilter that can generate n-grams in the sizes of the given range
-   *
-   * @param input {@link org.apache.lucene.analysis.TokenStream} holding the input to be tokenized
-   * @param minGram the smallest n-gram to generate
-   * @param maxGram the largest n-gram to generate
-   */
-  public Lucene43EdgeNGramTokenFilter(TokenStream input, int minGram, int maxGram) {
-    super(input);
-
-    if (minGram < 1) {
-      throw new IllegalArgumentException("minGram must be greater than zero");
-    }
-
-    if (minGram > maxGram) {
-      throw new IllegalArgumentException("minGram must not be greater than maxGram");
-    }
-
-    this.charUtils = CharacterUtils.getJava4Instance();
-    this.minGram = minGram;
-    this.maxGram = maxGram;
-  }
-
-  @Override
-  public final boolean incrementToken() throws IOException {
-    while (true) {
-      if (curTermBuffer == null) {
-        if (!input.incrementToken()) {
-          return false;
-        } else {
-          curTermBuffer = termAtt.buffer().clone();
-          curTermLength = termAtt.length();
-          curCodePointCount = charUtils.codePointCount(termAtt);
-          curGramSize = minGram;
-          tokStart = offsetAtt.startOffset();
-          tokEnd = offsetAtt.endOffset();
-          savePosIncr += posIncrAtt.getPositionIncrement();
-          savePosLen = posLenAtt.getPositionLength();
-        }
-      }
-      if (curGramSize <= maxGram) {         // if we have hit the end of our n-gram size range, quit
-        if (curGramSize <= curCodePointCount) { // if the remaining input is too short, we can't generate any n-grams
-          // grab gramSize chars from front or back
-          clearAttributes();
-          offsetAtt.setOffset(tokStart, tokEnd);
-          // first ngram gets increment, others don't
-          if (curGramSize == minGram) {
-            posIncrAtt.setPositionIncrement(savePosIncr);
-            savePosIncr = 0;
-          } else {
-            posIncrAtt.setPositionIncrement(0);
-          }
-          posLenAtt.setPositionLength(savePosLen);
-          final int charLength = charUtils.offsetByCodePoints(curTermBuffer, 0, curTermLength, 0, curGramSize);
-          termAtt.copyBuffer(curTermBuffer, 0, charLength);
-          curGramSize++;
-          return true;
-        }
-      }
-      curTermBuffer = null;
-    }
-  }
-
-  @Override
-  public void reset() throws IOException {
-    super.reset();
-    curTermBuffer = null;
-    savePosIncr = 0;
-  }
-}
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/Lucene43EdgeNGramTokenizer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/Lucene43EdgeNGramTokenizer.java
deleted file mode 100644
index 9789560..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/Lucene43EdgeNGramTokenizer.java
+++ /dev/null
@@ -1,255 +0,0 @@
-package org.apache.lucene.analysis.ngram;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.AttributeFactory;
-import org.apache.lucene.util.Version;
-
-/**
- * Old version of {@link EdgeNGramTokenizer} which doesn't handle correctly
- * supplementary characters.
- */
-@Deprecated
-public final class Lucene43EdgeNGramTokenizer extends Tokenizer {
-  public static final Side DEFAULT_SIDE = Side.FRONT;
-  public static final int DEFAULT_MAX_GRAM_SIZE = 1;
-  public static final int DEFAULT_MIN_GRAM_SIZE = 1;
-
-  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-  private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
-  private final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);
-
-  /** Specifies which side of the input the n-gram should be generated from */
-  public static enum Side {
-
-    /** Get the n-gram from the front of the input */
-    FRONT {
-      @Override
-      public String getLabel() { return "front"; }
-    },
-
-    /** Get the n-gram from the end of the input */
-    BACK  {
-      @Override
-      public String getLabel() { return "back"; }
-    };
-
-    public abstract String getLabel();
-
-    // Get the appropriate Side from a string
-    public static Side getSide(String sideName) {
-      if (FRONT.getLabel().equals(sideName)) {
-        return FRONT;
-      }
-      if (BACK.getLabel().equals(sideName)) {
-        return BACK;
-      }
-      return null;
-    }
-  }
-
-  private int minGram;
-  private int maxGram;
-  private int gramSize;
-  private Side side;
-  private boolean started;
-  private int inLen; // length of the input AFTER trim()
-  private int charsRead; // length of the input
-  private String inStr;
-
-
-  /**
-   * Creates EdgeNGramTokenizer that can generate n-grams in the sizes of the given range
-   *
-   * @param side the {@link Side} from which to chop off an n-gram
-   * @param minGram the smallest n-gram to generate
-   * @param maxGram the largest n-gram to generate
-   */
-  public Lucene43EdgeNGramTokenizer(Side side, int minGram, int maxGram) {
-    init(side, minGram, maxGram);
-  }
-
-  /**
-   * Creates EdgeNGramTokenizer that can generate n-grams in the sizes of the given range
-   *
-   * @param factory {@link org.apache.lucene.util.AttributeFactory} to use
-   * @param side the {@link Side} from which to chop off an n-gram
-   * @param minGram the smallest n-gram to generate
-   * @param maxGram the largest n-gram to generate
-   */
-  public Lucene43EdgeNGramTokenizer(AttributeFactory factory, Side side, int minGram, int maxGram) {
-    super(factory);
-    init(side, minGram, maxGram);
-  }
-
-  /**
-   * Creates EdgeNGramTokenizer that can generate n-grams in the sizes of the given range
-   *
-   * @param factory {@link org.apache.lucene.util.AttributeFactory} to use
-   * @param sideLabel the {@link Side} from which to chop off an n-gram
-   * @param minGram the smallest n-gram to generate
-   * @param maxGram the largest n-gram to generate
-   */
-  public Lucene43EdgeNGramTokenizer(AttributeFactory factory, String sideLabel, int minGram, int maxGram) {
-    this(factory, Side.getSide(sideLabel), minGram, maxGram);
-  }
-
-  /**
-   * Creates EdgeNGramTokenizer that can generate n-grams in the sizes of the given range
-   *
-   * @param minGram the smallest n-gram to generate
-   * @param maxGram the largest n-gram to generate
-   */
-  public Lucene43EdgeNGramTokenizer(int minGram, int maxGram) {
-    this(Side.FRONT, minGram, maxGram);
-  }
-
-  /**
-   * Creates EdgeNGramTokenizer that can generate n-grams in the sizes of the given range
-   *
-   * @param sideLabel the name of the {@link Side} from which to chop off an n-gram
-   * @param minGram the smallest n-gram to generate
-   * @param maxGram the largest n-gram to generate
-   */
-  @Deprecated
-  public Lucene43EdgeNGramTokenizer(String sideLabel, int minGram, int maxGram) {
-    this(Side.getSide(sideLabel), minGram, maxGram);
-  }
-
-  /**
-   * Creates EdgeNGramTokenizer that can generate n-grams in the sizes of the given range
-   *
-   * @param factory {@link org.apache.lucene.util.AttributeFactory} to use
-   * @param minGram the smallest n-gram to generate
-   * @param maxGram the largest n-gram to generate
-   */
-  public Lucene43EdgeNGramTokenizer(AttributeFactory factory, int minGram, int maxGram) {
-    this(factory, Side.FRONT, minGram, maxGram);
-  }
-
-  private void init(Side side, int minGram, int maxGram) {
-
-    if (side == null) {
-      throw new IllegalArgumentException("sideLabel must be either front or back");
-    }
-
-    if (minGram < 1) {
-      throw new IllegalArgumentException("minGram must be greater than zero");
-    }
-
-    if (minGram > maxGram) {
-      throw new IllegalArgumentException("minGram must not be greater than maxGram");
-    }
-
-    maxGram = Math.min(maxGram, 1024);
-
-    this.minGram = minGram;
-    this.maxGram = maxGram;
-    this.side = side;
-  }
-
-  /** Returns the next token in the stream, or null at EOS. */
-  @Override
-  public boolean incrementToken() throws IOException {
-    clearAttributes();
-    // if we are just starting, read the whole input
-    if (!started) {
-      started = true;
-      gramSize = minGram;
-      final int limit = side == Side.FRONT ? maxGram : 1024;
-      char[] chars = new char[Math.min(1024, limit)];
-      charsRead = 0;
-      // TODO: refactor to a shared readFully somewhere:
-      boolean exhausted = false;
-      while (charsRead < limit) {
-        final int inc = input.read(chars, charsRead, chars.length-charsRead);
-        if (inc == -1) {
-          exhausted = true;
-          break;
-        }
-        charsRead += inc;
-        if (charsRead == chars.length && charsRead < limit) {
-          chars = ArrayUtil.grow(chars);
-        }
-      }
-
-      inStr = new String(chars, 0, charsRead);
-      inStr = inStr.trim();
-
-      if (!exhausted) {
-        // Read extra throwaway chars so that on end() we
-        // report the correct offset:
-        char[] throwaway = new char[1024];
-        while(true) {
-          final int inc = input.read(throwaway, 0, throwaway.length);
-          if (inc == -1) {
-            break;
-          }
-          charsRead += inc;
-        }
-      }
-
-      inLen = inStr.length();
-      if (inLen == 0) {
-        return false;
-      }
-      posIncrAtt.setPositionIncrement(1);
-    } else {
-      posIncrAtt.setPositionIncrement(0);
-    }
-
-    // if the remaining input is too short, we can't generate any n-grams
-    if (gramSize > inLen) {
-      return false;
-    }
-
-    // if we have hit the end of our n-gram size range, quit
-    if (gramSize > maxGram || gramSize > inLen) {
-      return false;
-    }
-
-    // grab gramSize chars from front or back
-    int start = side == Side.FRONT ? 0 : inLen - gramSize;
-    int end = start + gramSize;
-    termAtt.setEmpty().append(inStr, start, end);
-    offsetAtt.setOffset(correctOffset(start), correctOffset(end));
-    gramSize++;
-    return true;
-  }
-  
-  @Override
-  public void end() throws IOException {
-    super.end();
-    // set final offset
-    final int finalOffset = correctOffset(charsRead);
-    this.offsetAtt.setOffset(finalOffset, finalOffset);
-  }    
-
-  @Override
-  public void reset() throws IOException {
-    super.reset();
-    started = false;
-  }
-}
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/Lucene43NGramTokenFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/Lucene43NGramTokenFilter.java
deleted file mode 100644
index 1205fb3..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/Lucene43NGramTokenFilter.java
+++ /dev/null
@@ -1,150 +0,0 @@
-package org.apache.lucene.analysis.ngram;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.TokenFilter;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.miscellaneous.CodepointCountFilter;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionLengthAttribute;
-import org.apache.lucene.analysis.util.CharacterUtils;
-
-import java.io.IOException;
-
-/**
- * Tokenizes the input into n-grams of the given size(s), matching Lucene 4.3 and before behavior.
- *
- * @deprecated Use {@link org.apache.lucene.analysis.ngram.NGramTokenFilter} instead.
- */
-@Deprecated
-public final class Lucene43NGramTokenFilter extends TokenFilter {
-  public static final int DEFAULT_MIN_NGRAM_SIZE = 1;
-  public static final int DEFAULT_MAX_NGRAM_SIZE = 2;
-
-  private final int minGram, maxGram;
-
-  private char[] curTermBuffer;
-  private int curTermLength;
-  private int curCodePointCount;
-  private int curGramSize;
-  private int curPos;
-  private int curPosInc, curPosLen;
-  private int tokStart;
-  private int tokEnd;
-  private boolean hasIllegalOffsets; // only if the length changed before this filter
-
-  private final CharacterUtils charUtils;
-  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-  private final PositionIncrementAttribute posIncAtt;
-  private final PositionLengthAttribute posLenAtt;
-  private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
-
-  /**
-   * Creates Lucene43NGramTokenFilter with given min and max n-grams.
-   * @param input {@link org.apache.lucene.analysis.TokenStream} holding the input to be tokenized
-   * @param minGram the smallest n-gram to generate
-   * @param maxGram the largest n-gram to generate
-   */
-  public Lucene43NGramTokenFilter(TokenStream input, int minGram, int maxGram) {
-    super(new CodepointCountFilter(input, minGram, Integer.MAX_VALUE));
-    this.charUtils = CharacterUtils.getJava4Instance();
-    if (minGram < 1) {
-      throw new IllegalArgumentException("minGram must be greater than zero");
-    }
-    if (minGram > maxGram) {
-      throw new IllegalArgumentException("minGram must not be greater than maxGram");
-    }
-    this.minGram = minGram;
-    this.maxGram = maxGram;
-
-    posIncAtt = new PositionIncrementAttribute() {
-      @Override
-      public void setPositionIncrement(int positionIncrement) {}
-      @Override
-      public int getPositionIncrement() {
-          return 0;
-        }
-    };
-    posLenAtt = new PositionLengthAttribute() {
-      @Override
-      public void setPositionLength(int positionLength) {}
-      @Override
-      public int getPositionLength() {
-          return 0;
-        }
-    };
-  }
-
-  /**
-   * Creates NGramTokenFilter with default min and max n-grams.
-   * @param input {@link org.apache.lucene.analysis.TokenStream} holding the input to be tokenized
-   */
-  public Lucene43NGramTokenFilter(TokenStream input) {
-    this(input, DEFAULT_MIN_NGRAM_SIZE, DEFAULT_MAX_NGRAM_SIZE);
-  }
-
-  /** Returns the next token in the stream, or null at EOS. */
-  @Override
-  public final boolean incrementToken() throws IOException {
-    while (true) {
-      if (curTermBuffer == null) {
-        if (!input.incrementToken()) {
-          return false;
-        } else {
-          curTermBuffer = termAtt.buffer().clone();
-          curTermLength = termAtt.length();
-          curCodePointCount = charUtils.codePointCount(termAtt);
-          curGramSize = minGram;
-          curPos = 0;
-          curPosInc = posIncAtt.getPositionIncrement();
-          curPosLen = posLenAtt.getPositionLength();
-          tokStart = offsetAtt.startOffset();
-          tokEnd = offsetAtt.endOffset();
-          // if length by start + end offsets doesn't match the term text then assume
-          // this is a synonym and don't adjust the offsets.
-          hasIllegalOffsets = (tokStart + curTermLength) != tokEnd;
-        }
-      }
-
-      while (curGramSize <= maxGram) {
-        while (curPos+curGramSize <= curTermLength) {     // while there is input
-          clearAttributes();
-          termAtt.copyBuffer(curTermBuffer, curPos, curGramSize);
-          if (hasIllegalOffsets) {
-            offsetAtt.setOffset(tokStart, tokEnd);
-          } else {
-            offsetAtt.setOffset(tokStart + curPos, tokStart + curPos + curGramSize);
-          }
-          curPos++;
-          return true;
-        }
-        curGramSize++;                         // increase n-gram size
-        curPos = 0;
-      }
-      curTermBuffer = null;
-    }
-  }
-
-  @Override
-  public void reset() throws IOException {
-    super.reset();
-    curTermBuffer = null;
-  }
-}
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/Lucene43NGramTokenizer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/Lucene43NGramTokenizer.java
deleted file mode 100644
index b1d007a..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/Lucene43NGramTokenizer.java
+++ /dev/null
@@ -1,153 +0,0 @@
-package org.apache.lucene.analysis.ngram;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-
-import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.util.AttributeFactory;
-
-/**
- * Old broken version of {@link NGramTokenizer}.
- */
-@Deprecated
-public final class Lucene43NGramTokenizer extends Tokenizer {
-  public static final int DEFAULT_MIN_NGRAM_SIZE = 1;
-  public static final int DEFAULT_MAX_NGRAM_SIZE = 2;
-
-  private int minGram, maxGram;
-  private int gramSize;
-  private int pos;
-  private int inLen; // length of the input AFTER trim()
-  private int charsRead; // length of the input
-  private String inStr;
-  private boolean started;
-  
-  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-  private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
-
-  /**
-   * Creates NGramTokenizer with given min and max n-grams.
-   * @param minGram the smallest n-gram to generate
-   * @param maxGram the largest n-gram to generate
-   */
-  public Lucene43NGramTokenizer(int minGram, int maxGram) {
-    init(minGram, maxGram);
-  }
-
-  /**
-   * Creates NGramTokenizer with given min and max n-grams.
-   * @param factory {@link org.apache.lucene.util.AttributeFactory} to use
-   * @param minGram the smallest n-gram to generate
-   * @param maxGram the largest n-gram to generate
-   */
-  public Lucene43NGramTokenizer(AttributeFactory factory, int minGram, int maxGram) {
-    super(factory);
-    init(minGram, maxGram);
-  }
-
-  /**
-   * Creates NGramTokenizer with default min and max n-grams.
-   */
-  public Lucene43NGramTokenizer() {
-    this(DEFAULT_MIN_NGRAM_SIZE, DEFAULT_MAX_NGRAM_SIZE);
-  }
-  
-  private void init(int minGram, int maxGram) {
-    if (minGram < 1) {
-      throw new IllegalArgumentException("minGram must be greater than zero");
-    }
-    if (minGram > maxGram) {
-      throw new IllegalArgumentException("minGram must not be greater than maxGram");
-    }
-    this.minGram = minGram;
-    this.maxGram = maxGram;
-  }
-
-  /** Returns the next token in the stream, or null at EOS. */
-  @Override
-  public boolean incrementToken() throws IOException {
-    clearAttributes();
-    if (!started) {
-      started = true;
-      gramSize = minGram;
-      char[] chars = new char[1024];
-      charsRead = 0;
-      // TODO: refactor to a shared readFully somewhere:
-      while (charsRead < chars.length) {
-        int inc = input.read(chars, charsRead, chars.length-charsRead);
-        if (inc == -1) {
-          break;
-        }
-        charsRead += inc;
-      }
-      inStr = new String(chars, 0, charsRead).trim();  // remove any trailing empty strings 
-
-      if (charsRead == chars.length) {
-        // Read extra throwaway chars so that on end() we
-        // report the correct offset:
-        char[] throwaway = new char[1024];
-        while(true) {
-          final int inc = input.read(throwaway, 0, throwaway.length);
-          if (inc == -1) {
-            break;
-          }
-          charsRead += inc;
-        }
-      }
-
-      inLen = inStr.length();
-      if (inLen == 0) {
-        return false;
-      }
-    }
-
-    if (pos+gramSize > inLen) {            // if we hit the end of the string
-      pos = 0;                           // reset to beginning of string
-      gramSize++;                        // increase n-gram size
-      if (gramSize > maxGram)            // we are done
-        return false;
-      if (pos+gramSize > inLen)
-        return false;
-    }
-
-    int oldPos = pos;
-    pos++;
-    termAtt.setEmpty().append(inStr, oldPos, oldPos+gramSize);
-    offsetAtt.setOffset(correctOffset(oldPos), correctOffset(oldPos+gramSize));
-    return true;
-  }
-  
-  @Override
-  public void end() throws IOException {
-    super.end();
-    // set final offset
-    final int finalOffset = correctOffset(charsRead);
-    this.offsetAtt.setOffset(finalOffset, finalOffset);
-  }    
-  
-  @Override
-  public void reset() throws IOException {
-    super.reset();
-    started = false;
-    pos = 0;
-  }
-}
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramFilterFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramFilterFactory.java
index 99d2a11..9626da4 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramFilterFactory.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramFilterFactory.java
@@ -22,7 +22,6 @@ import java.util.Map;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.util.TokenFilterFactory;
-import org.apache.lucene.util.Version;
 
 /**
  * Factory for {@link NGramTokenFilter}.
@@ -50,9 +49,6 @@ public class NGramFilterFactory extends TokenFilterFactory {
 
   @Override
   public TokenFilter create(TokenStream input) {
-    if (luceneMatchVersion.onOrAfter(Version.LUCENE_4_4_0)) {
-      return new NGramTokenFilter(input, minGramSize, maxGramSize);
-    }
-    return new Lucene43NGramTokenFilter(input, minGramSize, maxGramSize);
+    return new NGramTokenFilter(input, minGramSize, maxGramSize);
   }
 }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenFilter.java
index 83b19e6..e673499 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenFilter.java
@@ -37,10 +37,6 @@ import org.apache.lucene.analysis.util.CharacterUtils;
  * <li>sorts n-grams by their offset in the original token first, then
  * increasing length (meaning that "abc" will give "a", "ab", "abc", "b", "bc",
  * "c").</li></ul>
- * <p>You can make this filter use the old behavior by using
- * {@link org.apache.lucene.analysis.ngram.Lucene43NGramTokenFilter} but this is not recommended as
- * it will lead to broken {@link TokenStream}s that will cause highlighting
- * bugs.
  * <p>If you were using this {@link TokenFilter} to perform partial highlighting,
  * this won't work anymore since this filter doesn't update offsets. You should
  * modify your analysis chain to use {@link NGramTokenizer}, and potentially
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer.java
index 177e467..2e24147 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer.java
@@ -51,8 +51,6 @@ import org.apache.lucene.util.AttributeFactory;
  * tokens in a different order, tokens are now emitted by increasing start
  * offsets while they used to be emitted by increasing lengths (which prevented
  * from supporting large input streams).
- * <p>Although <b style="color:red">highly</b> discouraged, it is still possible
- * to use the old behavior through {@link Lucene43NGramTokenizer}.
  */
 // non-final to allow for overriding isTokenChar, but all other methods should be final
 public class NGramTokenizer extends Tokenizer {
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizerFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizerFactory.java
index f096a4b..b48a54b 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizerFactory.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizerFactory.java
@@ -21,7 +21,6 @@ import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.util.TokenizerFactory;
 import org.apache.lucene.util.AttributeFactory;
-import org.apache.lucene.util.Version;
 
 import java.io.Reader;
 import java.util.Map;
@@ -52,10 +51,6 @@ public class NGramTokenizerFactory extends TokenizerFactory {
   /** Creates the {@link TokenStream} of n-grams from the given {@link Reader} and {@link AttributeFactory}. */
   @Override
   public Tokenizer create(AttributeFactory factory) {
-    if (luceneMatchVersion.onOrAfter(Version.LUCENE_4_4_0)) {
-      return new NGramTokenizer(factory, minGramSize, maxGramSize);
-    } else {
-      return new Lucene43NGramTokenizer(factory, minGramSize, maxGramSize);
-    }
+    return new NGramTokenizer(factory, minGramSize, maxGramSize);
   }
 }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiAnalyzer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiAnalyzer.java
index 2ed72f4..4f6edba 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiAnalyzer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiAnalyzer.java
@@ -24,11 +24,8 @@ import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.core.LowerCaseFilter;
 import org.apache.lucene.analysis.core.StopFilter;
-import org.apache.lucene.analysis.standard.StandardFilter;
-import org.apache.lucene.analysis.standard.StandardTokenizer;
 import org.apache.lucene.analysis.util.CharArraySet;
 import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
-import org.apache.lucene.util.Version;
 
 /**
  * {@link Analyzer} for Thai language. It uses {@link java.text.BreakIterator} to break words.
@@ -92,23 +89,14 @@ public final class ThaiAnalyzer extends StopwordAnalyzerBase {
    * used to tokenize all the text in the provided {@link Reader}.
    * 
    * @return {@link org.apache.lucene.analysis.Analyzer.TokenStreamComponents}
-   *         built from a {@link StandardTokenizer} filtered with
-   *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link ThaiWordFilter}, and
-   *         {@link StopFilter}
+   *         built from a {@link ThaiTokenizer} filtered with
+   *         {@link LowerCaseFilter} and {@link StopFilter}
    */
   @Override
   protected TokenStreamComponents createComponents(String fieldName) {
-    if (getVersion().onOrAfter(Version.LUCENE_4_8_0)) {
-      final Tokenizer source = new ThaiTokenizer();
-      TokenStream result = new LowerCaseFilter(source);
-      result = new StopFilter(result, stopwords);
-      return new TokenStreamComponents(source, result);
-    } else {
-      final Tokenizer source = new StandardTokenizer();
-      TokenStream result = new StandardFilter(source);
-      result = new LowerCaseFilter(result);
-      result = new ThaiWordFilter(result);
-      return new TokenStreamComponents(source, new StopFilter(result, stopwords));
-    }
+    final Tokenizer source = new ThaiTokenizer();
+    TokenStream result = new LowerCaseFilter(source);
+    result = new StopFilter(result, stopwords);
+    return new TokenStreamComponents(source, result);
   }
 }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiWordFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiWordFilter.java
deleted file mode 100644
index 7eb1eda..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiWordFilter.java
+++ /dev/null
@@ -1,136 +0,0 @@
-package org.apache.lucene.analysis.th;
-
-/**
- * Copyright 2006 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.lang.Character.UnicodeBlock;
-import java.text.BreakIterator;
-import java.util.Locale;
-
-import org.apache.lucene.analysis.TokenFilter;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.analysis.util.CharArrayIterator;
-import org.apache.lucene.util.AttributeSource;
-
-/**
- * {@link TokenFilter} that use {@link java.text.BreakIterator} to break each 
- * Token that is Thai into separate Token(s) for each Thai word.
- * <p>WARNING: this filter may not be supported by all JREs.
- *    It is known to work with Sun/Oracle and Harmony JREs.
- *    If your application needs to be fully portable, consider using ICUTokenizer instead,
- *    which uses an ICU Thai BreakIterator that will always be available.
- * @deprecated Use {@link ThaiTokenizer} instead.
- */
-@Deprecated
-public final class ThaiWordFilter extends TokenFilter {
-  /** 
-   * True if the JRE supports a working dictionary-based breakiterator for Thai.
-   * If this is false, this filter will not work at all!
-   */
-  public static final boolean DBBI_AVAILABLE = ThaiTokenizer.DBBI_AVAILABLE;
-  private static final BreakIterator proto = BreakIterator.getWordInstance(new Locale("th"));
-  private final BreakIterator breaker = (BreakIterator) proto.clone();
-  private final CharArrayIterator charIterator = CharArrayIterator.newWordInstance();
-  
-  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-  private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
-  private final PositionIncrementAttribute posAtt = addAttribute(PositionIncrementAttribute.class);
-  
-  private AttributeSource clonedToken = null;
-  private CharTermAttribute clonedTermAtt = null;
-  private OffsetAttribute clonedOffsetAtt = null;
-  private boolean hasMoreTokensInClone = false;
-  private boolean hasIllegalOffsets = false; // only if the length changed before this filter
-
-  /** Creates a new ThaiWordFilter with the specified match version. */
-  public ThaiWordFilter(TokenStream input) {
-    super(input);
-    if (!DBBI_AVAILABLE)
-      throw new UnsupportedOperationException("This JRE does not have support for Thai segmentation");
-  }
-  
-  @Override
-  public boolean incrementToken() throws IOException {
-    if (hasMoreTokensInClone) {
-      int start = breaker.current();
-      int end = breaker.next();
-      if (end != BreakIterator.DONE) {
-        clonedToken.copyTo(this);
-        termAtt.copyBuffer(clonedTermAtt.buffer(), start, end - start);
-        if (hasIllegalOffsets) {
-          offsetAtt.setOffset(clonedOffsetAtt.startOffset(), clonedOffsetAtt.endOffset());
-        } else {
-          offsetAtt.setOffset(clonedOffsetAtt.startOffset() + start, clonedOffsetAtt.startOffset() + end);
-        }
-        posAtt.setPositionIncrement(1);
-        return true;
-      }
-      hasMoreTokensInClone = false;
-    }
-
-    if (!input.incrementToken()) {
-      return false;
-    }
-    
-    if (termAtt.length() == 0 || UnicodeBlock.of(termAtt.charAt(0)) != UnicodeBlock.THAI) {
-      return true;
-    }
-    
-    hasMoreTokensInClone = true;
-    
-    // if length by start + end offsets doesn't match the term text then assume
-    // this is a synonym and don't adjust the offsets.
-    hasIllegalOffsets = offsetAtt.endOffset() - offsetAtt.startOffset() != termAtt.length();
-
-    // we lazy init the cloned token, as in ctor not all attributes may be added
-    if (clonedToken == null) {
-      clonedToken = cloneAttributes();
-      clonedTermAtt = clonedToken.getAttribute(CharTermAttribute.class);
-      clonedOffsetAtt = clonedToken.getAttribute(OffsetAttribute.class);
-    } else {
-      this.copyTo(clonedToken);
-    }
-    
-    // reinit CharacterIterator
-    charIterator.setText(clonedTermAtt.buffer(), 0, clonedTermAtt.length());
-    breaker.setText(charIterator);
-    int end = breaker.next();
-    if (end != BreakIterator.DONE) {
-      termAtt.setLength(end);
-      if (hasIllegalOffsets) {
-        offsetAtt.setOffset(clonedOffsetAtt.startOffset(), clonedOffsetAtt.endOffset());
-      } else {
-        offsetAtt.setOffset(clonedOffsetAtt.startOffset(), clonedOffsetAtt.startOffset() + end);
-      }
-      // position increment keeps as it is for first token
-      return true;
-    }
-    return false;
-  }
-  
-  @Override
-  public void reset() throws IOException {
-    super.reset();
-    hasMoreTokensInClone = false;
-    clonedToken = null;
-    clonedTermAtt = null;
-    clonedOffsetAtt = null;
-  }
-}
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiWordFilterFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiWordFilterFactory.java
deleted file mode 100644
index 154187e..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiWordFilterFactory.java
+++ /dev/null
@@ -1,54 +0,0 @@
-package org.apache.lucene.analysis.th;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Map;
-
-import org.apache.lucene.analysis.th.ThaiWordFilter;
-
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.util.TokenFilterFactory;
-
-/** 
- * Factory for {@link ThaiWordFilter}.
- * <pre class="prettyprint">
- * &lt;fieldType name="text_thai" class="solr.TextField" positionIncrementGap="100"&gt;
- *   &lt;analyzer&gt;
- *     &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
- *     &lt;filter class="solr.ThaiWordFilterFactory"/&gt;
- *   &lt;/analyzer&gt;
- * &lt;/fieldType&gt;</pre>
- * @deprecated Use {@link ThaiTokenizerFactory} instead
- */
-@Deprecated
-public class ThaiWordFilterFactory extends TokenFilterFactory {
-  
-  /** Creates a new ThaiWordFilterFactory */
-  public ThaiWordFilterFactory(Map<String,String> args) {
-    super(args);
-    if (!args.isEmpty()) {
-      throw new IllegalArgumentException("Unknown parameters: " + args);
-    }
-  }
-  
-  @Override
-  public ThaiWordFilter create(TokenStream input) {
-    return new ThaiWordFilter(input);
-  }
-}
-
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/tr/TurkishAnalyzer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/tr/TurkishAnalyzer.java
index d74d3c9..2cc5575 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/tr/TurkishAnalyzer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/tr/TurkishAnalyzer.java
@@ -119,9 +119,7 @@ public final class TurkishAnalyzer extends StopwordAnalyzerBase {
   protected TokenStreamComponents createComponents(String fieldName) {
     final Tokenizer source = new StandardTokenizer();
     TokenStream result = new StandardFilter(source);
-    if (getVersion().onOrAfter(Version.LUCENE_4_8_0)) {
-      result = new ApostropheFilter(result);
-    }
+    result = new ApostropheFilter(result);
     result = new TurkishLowerCaseFilter(result);
     result = new StopFilter(result, stopwords);
     if (!stemExclusionSet.isEmpty()) {
diff --git a/lucene/analysis/common/src/resources/META-INF/services/org.apache.lucene.analysis.util.TokenFilterFactory b/lucene/analysis/common/src/resources/META-INF/services/org.apache.lucene.analysis.util.TokenFilterFactory
index c0a4719..abc6c12 100644
--- a/lucene/analysis/common/src/resources/META-INF/services/org.apache.lucene.analysis.util.TokenFilterFactory
+++ b/lucene/analysis/common/src/resources/META-INF/services/org.apache.lucene.analysis.util.TokenFilterFactory
@@ -95,6 +95,5 @@ org.apache.lucene.analysis.standard.ClassicFilterFactory
 org.apache.lucene.analysis.standard.StandardFilterFactory
 org.apache.lucene.analysis.sv.SwedishLightStemFilterFactory
 org.apache.lucene.analysis.synonym.SynonymFilterFactory
-org.apache.lucene.analysis.th.ThaiWordFilterFactory
 org.apache.lucene.analysis.tr.TurkishLowerCaseFilterFactory
 org.apache.lucene.analysis.util.ElisionFilterFactory
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestRandomChains.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestRandomChains.java
index 2216212..adc4f18 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestRandomChains.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestRandomChains.java
@@ -63,7 +63,7 @@ import org.apache.lucene.analysis.charfilter.NormalizeCharMap;
 import org.apache.lucene.analysis.cjk.CJKBigramFilter;
 import org.apache.lucene.analysis.commongrams.CommonGramsFilter;
 import org.apache.lucene.analysis.commongrams.CommonGramsQueryFilter;
-import org.apache.lucene.analysis.compound.Lucene43HyphenationCompoundWordTokenFilter;
+import org.apache.lucene.analysis.compound.HyphenationCompoundWordTokenFilter;
 import org.apache.lucene.analysis.compound.TestCompoundWordTokenFilter;
 import org.apache.lucene.analysis.compound.hyphenation.HyphenationTree;
 import org.apache.lucene.analysis.hunspell.Dictionary;
@@ -434,7 +434,7 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
         // TODO: make nastier
         try {
           InputSource is = new InputSource(TestCompoundWordTokenFilter.class.getResource("da_UTF8.xml").toExternalForm());
-          HyphenationTree hyphenator = Lucene43HyphenationCompoundWordTokenFilter.getHyphenationTree(is);
+          HyphenationTree hyphenator = HyphenationCompoundWordTokenFilter.getHyphenationTree(is);
           return hyphenator;
         } catch (Exception ex) {
           Rethrow.rethrow(ex);
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLucene47WordDelimiterFilter.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLucene47WordDelimiterFilter.java
deleted file mode 100644
index 7a20192..0000000
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLucene47WordDelimiterFilter.java
+++ /dev/null
@@ -1,374 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.lucene.analysis.miscellaneous;
-
-import org.apache.lucene.analysis.*;
-import org.apache.lucene.analysis.core.KeywordTokenizer;
-import org.apache.lucene.analysis.core.StopFilter;
-import org.apache.lucene.analysis.standard.StandardAnalyzer;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.util.CharArraySet;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.*;
-
-import static org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter.*;
-import static org.apache.lucene.analysis.miscellaneous.WordDelimiterIterator.DEFAULT_WORD_DELIM_TABLE;
-
-/**
- * New WordDelimiterFilter tests... most of the tests are in ConvertedLegacyTest
- * TODO: should explicitly test things like protWords and not rely on
- * the factory tests in Solr.
- */
-@Deprecated
-public class TestLucene47WordDelimiterFilter extends BaseTokenStreamTestCase {
-
-  /***
-  public void testPerformance() throws IOException {
-    String s = "now is the time-for all good men to come to-the aid of their country.";
-    Token tok = new Token();
-    long start = System.currentTimeMillis();
-    int ret=0;
-    for (int i=0; i<1000000; i++) {
-      StringReader r = new StringReader(s);
-      TokenStream ts = new WhitespaceTokenizer(r);
-      ts = new WordDelimiterFilter(ts, 1,1,1,1,0);
-
-      while (ts.next(tok) != null) ret++;
-    }
-
-    System.out.println("ret="+ret+" time="+(System.currentTimeMillis()-start));
-  }
-  ***/
-
-  @Test
-  public void testOffsets() throws IOException {
-    int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;
-    // test that subwords and catenated subwords have
-    // the correct offsets.
-    TokenFilter wdf = new Lucene47WordDelimiterFilter(new SingleTokenTokenStream(new Token("foo-bar", 5, 12)), DEFAULT_WORD_DELIM_TABLE, flags, null);
-
-    assertTokenStreamContents(wdf, 
-        new String[] { "foo", "bar", "foobar" },
-        new int[] { 5, 9, 5 }, 
-        new int[] { 8, 12, 12 },
-        null, null, null, null, false);
-
-    wdf = new Lucene47WordDelimiterFilter(new SingleTokenTokenStream(new Token("foo-bar", 5, 6)), DEFAULT_WORD_DELIM_TABLE, flags, null);
-    
-    assertTokenStreamContents(wdf,
-        new String[] { "foo", "bar", "foobar" },
-        new int[] { 5, 5, 5 },
-        new int[] { 6, 6, 6 },
-        null, null, null, null, false);
-  }
-  
-  @Test
-  public void testOffsetChange() throws Exception {
-    int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;
-    TokenFilter wdf = new Lucene47WordDelimiterFilter(new SingleTokenTokenStream(new Token("belkeit)", 7, 16)), DEFAULT_WORD_DELIM_TABLE, flags, null);
-    
-    assertTokenStreamContents(wdf,
-        new String[] { "belkeit" },
-        new int[] { 7 },
-        new int[] { 15 });
-  }
-  
-  @Test
-  public void testOffsetChange2() throws Exception {
-    int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;
-    TokenFilter wdf = new Lucene47WordDelimiterFilter(new SingleTokenTokenStream(new Token("(belkeit", 7, 17)), DEFAULT_WORD_DELIM_TABLE, flags, null);
-    
-    assertTokenStreamContents(wdf,
-        new String[] { "belkeit" },
-        new int[] { 8 },
-        new int[] { 17 });
-  }
-  
-  @Test
-  public void testOffsetChange3() throws Exception {
-    int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;
-    TokenFilter wdf = new Lucene47WordDelimiterFilter(new SingleTokenTokenStream(new Token("(belkeit", 7, 16)), DEFAULT_WORD_DELIM_TABLE, flags, null);
-    
-    assertTokenStreamContents(wdf,
-        new String[] { "belkeit" },
-        new int[] { 8 },
-        new int[] { 16 });
-  }
-  
-  @Test
-  public void testOffsetChange4() throws Exception {
-    int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;
-    TokenFilter wdf = new Lucene47WordDelimiterFilter(new SingleTokenTokenStream(new Token("(foo,bar)", 7, 16)), DEFAULT_WORD_DELIM_TABLE, flags, null);
-    
-    assertTokenStreamContents(wdf,
-        new String[] { "foo", "bar", "foobar"},
-        new int[] { 8, 12, 8 },
-        new int[] { 11, 15, 15 },
-        null, null, null, null, false);
-  }
-
-  public void doSplit(final String input, String... output) throws Exception {
-    int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;
-    TokenFilter wdf = new Lucene47WordDelimiterFilter(keywordMockTokenizer(input),
-        WordDelimiterIterator.DEFAULT_WORD_DELIM_TABLE, flags, null);
-    
-    assertTokenStreamContents(wdf, output);
-  }
-
-  @Test
-  public void testSplits() throws Exception {
-    doSplit("basic-split","basic","split");
-    doSplit("camelCase","camel","Case");
-
-    // non-space marking symbol shouldn't cause split
-    // this is an example in Thai    
-    doSplit("\u0e1a\u0e49\u0e32\u0e19","\u0e1a\u0e49\u0e32\u0e19");
-    // possessive followed by delimiter
-    doSplit("test's'", "test");
-
-    // some russian upper and lowercase
-    doSplit("???", "???");
-    // now cause a split (russian camelCase)
-    doSplit("????", "?", "???");
-
-    // a composed titlecase character, don't split
-    doSplit("a?ungla", "a?ungla");
-    
-    // a modifier letter, don't split
-    doSplit("???????????????????", "???????????????????");
-    
-    // enclosing mark, don't split
-    doSplit("test??", "test??");
-    
-    // combining spacing mark (the virama), don't split
-    doSplit("???", "???");
-    
-    // don't split non-ascii digits
-    doSplit("", "");
-    
-    // don't split supplementaries into unpaired surrogates
-    doSplit("??????", "??????");
-  }
-  
-  public void doSplitPossessive(int stemPossessive, final String input, final String... output) throws Exception {
-    int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS;
-    flags |= (stemPossessive == 1) ? STEM_ENGLISH_POSSESSIVE : 0;
-    TokenFilter wdf = new Lucene47WordDelimiterFilter(keywordMockTokenizer(input), flags, null);
-
-    assertTokenStreamContents(wdf, output);
-  }
-  
-  /*
-   * Test option that allows disabling the special "'s" stemming, instead treating the single quote like other delimiters. 
-   */
-  @Test
-  public void testPossessives() throws Exception {
-    doSplitPossessive(1, "ra's", "ra");
-    doSplitPossessive(0, "ra's", "ra", "s");
-  }
-  
-  /*
-   * Set a large position increment gap of 10 if the token is "largegap" or "/"
-   */
-  private final class LargePosIncTokenFilter extends TokenFilter {
-    private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-    private PositionIncrementAttribute posIncAtt = addAttribute(PositionIncrementAttribute.class);
-    
-    protected LargePosIncTokenFilter(TokenStream input) {
-      super(input);
-    }
-
-    @Override
-    public boolean incrementToken() throws IOException {
-      if (input.incrementToken()) {
-        if (termAtt.toString().equals("largegap") || termAtt.toString().equals("/"))
-          posIncAtt.setPositionIncrement(10);
-        return true;
-      } else {
-        return false;
-      }
-    }  
-  }
-  
-  @Test
-  public void testPositionIncrements() throws Exception {
-    final int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;
-    final CharArraySet protWords = new CharArraySet(new HashSet<>(Arrays.asList("NUTCH")), false);
-    
-    /* analyzer that uses whitespace + wdf */
-    Analyzer a = new Analyzer() {
-      @Override
-      public TokenStreamComponents createComponents(String field) {
-        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
-        return new TokenStreamComponents(tokenizer, new Lucene47WordDelimiterFilter(
-            tokenizer,
-            flags, protWords));
-      }
-    };
-
-    /* in this case, works as expected. */
-    assertAnalyzesTo(a, "LUCENE / SOLR", new String[] { "LUCENE", "SOLR" },
-        new int[] { 0, 9 },
-        new int[] { 6, 13 },
-        null,
-        new int[] { 1, 1 },
-        null,
-        false);
-    
-    /* only in this case, posInc of 2 ?! */
-    assertAnalyzesTo(a, "LUCENE / solR", new String[] { "LUCENE", "sol", "R", "solR" },
-        new int[] { 0, 9, 12, 9 },
-        new int[] { 6, 12, 13, 13 },
-        null,
-        new int[] { 1, 1, 1, 0 },
-        null,
-        false);
-    
-    assertAnalyzesTo(a, "LUCENE / NUTCH SOLR", new String[] { "LUCENE", "NUTCH", "SOLR" },
-        new int[] { 0, 9, 15 },
-        new int[] { 6, 14, 19 },
-        null,
-        new int[] { 1, 1, 1 },
-        null,
-        false);
-    
-    /* analyzer that will consume tokens with large position increments */
-    Analyzer a2 = new Analyzer() {
-      @Override
-      public TokenStreamComponents createComponents(String field) {
-        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
-        return new TokenStreamComponents(tokenizer, new Lucene47WordDelimiterFilter(
-            new LargePosIncTokenFilter(tokenizer),
-            flags, protWords));
-      }
-    };
-    
-    /* increment of "largegap" is preserved */
-    assertAnalyzesTo(a2, "LUCENE largegap SOLR", new String[] { "LUCENE", "largegap", "SOLR" },
-        new int[] { 0, 7, 16 },
-        new int[] { 6, 15, 20 },
-        null,
-        new int[] { 1, 10, 1 },
-        null,
-        false);
-    
-    /* the "/" had a position increment of 10, where did it go?!?!! */
-    assertAnalyzesTo(a2, "LUCENE / SOLR", new String[] { "LUCENE", "SOLR" },
-        new int[] { 0, 9 },
-        new int[] { 6, 13 },
-        null,
-        new int[] { 1, 11 },
-        null,
-        false);
-    
-    /* in this case, the increment of 10 from the "/" is carried over */
-    assertAnalyzesTo(a2, "LUCENE / solR", new String[] { "LUCENE", "sol", "R", "solR" },
-        new int[] { 0, 9, 12, 9 },
-        new int[] { 6, 12, 13, 13 },
-        null,
-        new int[] { 1, 11, 1, 0 },
-        null,
-        false);
-    
-    assertAnalyzesTo(a2, "LUCENE / NUTCH SOLR", new String[] { "LUCENE", "NUTCH", "SOLR" },
-        new int[] { 0, 9, 15 },
-        new int[] { 6, 14, 19 },
-        null,
-        new int[] { 1, 11, 1 },
-        null,
-        false);
-
-    Analyzer a3 = new Analyzer() {
-      @Override
-      public TokenStreamComponents createComponents(String field) {
-        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
-        StopFilter filter = new StopFilter(tokenizer, StandardAnalyzer.STOP_WORDS_SET);
-        return new TokenStreamComponents(tokenizer, new Lucene47WordDelimiterFilter(filter, flags, protWords));
-      }
-    };
-
-    assertAnalyzesTo(a3, "lucene.solr", 
-        new String[] { "lucene", "solr", "lucenesolr" },
-        new int[] { 0, 7, 0 },
-        new int[] { 6, 11, 11 },
-        null,
-        new int[] { 1, 1, 0 },
-        null,
-        false);
-
-    /* the stopword should add a gap here */
-    assertAnalyzesTo(a3, "the lucene.solr", 
-        new String[] { "lucene", "solr", "lucenesolr" }, 
-        new int[] { 4, 11, 4 }, 
-        new int[] { 10, 15, 15 },
-        null,
-        new int[] { 2, 1, 0 },
-        null,
-        false);
-  }
-  
-  /** blast some random strings through the analyzer */
-  public void testRandomStrings() throws Exception {
-    int numIterations = atLeast(5);
-    for (int i = 0; i < numIterations; i++) {
-      final int flags = random().nextInt(512);
-      final CharArraySet protectedWords;
-      if (random().nextBoolean()) {
-        protectedWords = new CharArraySet(new HashSet<>(Arrays.asList("a", "b", "cd")), false);
-      } else {
-        protectedWords = null;
-      }
-      
-      Analyzer a = new Analyzer() {
-        
-        @Override
-        protected TokenStreamComponents createComponents(String fieldName) {
-          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
-          return new TokenStreamComponents(tokenizer, new Lucene47WordDelimiterFilter(tokenizer, flags, protectedWords));
-        }
-      };
-      checkRandomData(random(), a, 200, 20, false, false);
-    }
-  }
-  
-  public void testEmptyTerm() throws IOException {
-    Random random = random();
-    for (int i = 0; i < 512; i++) {
-      final int flags = i;
-      final CharArraySet protectedWords;
-      if (random.nextBoolean()) {
-        protectedWords = new CharArraySet(new HashSet<>(Arrays.asList("a", "b", "cd")), false);
-      } else {
-        protectedWords = null;
-      }
-    
-      Analyzer a = new Analyzer() { 
-        @Override
-        protected TokenStreamComponents createComponents(String fieldName) {
-          Tokenizer tokenizer = new KeywordTokenizer();
-          return new TokenStreamComponents(tokenizer, new Lucene47WordDelimiterFilter(tokenizer, flags, protectedWords));
-        }
-      };
-      // depending upon options, this thing may or may not preserve the empty term
-      checkAnalysisConsistency(random, a, random.nextBoolean(), "");
-    }
-  }
-}
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java
index 2aa71b7..860d17e 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java
@@ -234,9 +234,4 @@ public class EdgeNGramTokenFilterTest extends BaseTokenStreamTestCase {
     }
     assertFalse(tk.incrementToken());
   }
-
-  public void test43Tokenizer() {
-    new Lucene43EdgeNGramTokenizer(1, 1);
-  }
-
 }
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java
index 3aa3271..85a7598 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java
@@ -172,18 +172,6 @@ public class NGramTokenFilterTest extends BaseTokenStreamTestCase {
     checkAnalysisConsistency(random, a, random.nextBoolean(), "");
   }
 
-  public void testLucene43() throws IOException {
-    TokenFilter filter = new Lucene43NGramTokenFilter(input, 2, 3);
-    assertTokenStreamContents(filter,
-        new String[]{"ab","bc","cd","de","abc","bcd","cde"},
-        new int[]{0,1,2,3,0,1,2},
-        new int[]{2,3,4,5,3,4,5},
-        null,
-        new int[]{1,1,1,1,1,1,1},
-        null, null, false
-        );
-  }
-
   public void testSupplementaryCharacters() throws IOException {
     final String s = TestUtil.randomUnicodeString(random(), 10);
     final int codePointCount = s.codePointCount(0, s.length());
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest.java
index f5da1b1..8102e75 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest.java
@@ -245,10 +245,4 @@ public class NGramTokenizerTest extends BaseTokenStreamTestCase {
     testNGrams(minGram, maxGram, s, "");
     testNGrams(minGram, maxGram, s, "abcdef");
   }
-
-  public void test43Tokenizer() {
-    // TODO: do more than instantiate (ie check the old broken behavior)
-    new Lucene43NGramTokenizer(1, 1);
-  }
-
 }
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/th/TestThaiWordFilterFactory.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/th/TestThaiWordFilterFactory.java
deleted file mode 100644
index 1d0d87e..0000000
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/th/TestThaiWordFilterFactory.java
+++ /dev/null
@@ -1,55 +0,0 @@
-package org.apache.lucene.analysis.th;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Reader;
-import java.io.StringReader;
-
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.th.ThaiWordFilter;
-import org.apache.lucene.analysis.util.BaseTokenStreamFactoryTestCase;
-
-/**
- * Simple tests to ensure the Thai word filter factory is working.
- */
-@Deprecated
-public class TestThaiWordFilterFactory extends BaseTokenStreamFactoryTestCase {
-  /**
-   * Ensure the filter actually decomposes text.
-   */
-  public void testWordBreak() throws Exception {
-    assumeTrue("JRE does not support Thai dictionary-based BreakIterator", ThaiWordFilter.DBBI_AVAILABLE);
-    Reader reader = new StringReader("??????????????????");
-    TokenStream stream = whitespaceMockTokenizer(reader);
-    stream = tokenFilterFactory("ThaiWord").create(stream);
-    assertTokenStreamContents(stream, new String[] {"??", "??", "???",
-        "????", "???", "??", "??", "?"});
-  }
-  
-  /** Test that bogus arguments result in exception */
-  public void testBogusArguments() throws Exception {
-    assumeTrue("JRE does not support Thai dictionary-based BreakIterator", ThaiWordFilter.DBBI_AVAILABLE);
-    try {
-      tokenFilterFactory("ThaiWord", "bogusArg", "bogusValue");
-      fail();
-    } catch (IllegalArgumentException expected) {
-      assertTrue(expected.getMessage().contains("Unknown parameters"));
-    }
-  }
-}
diff --git a/lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SentenceTokenizer.java b/lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SentenceTokenizer.java
deleted file mode 100644
index 7a85895..0000000
--- a/lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SentenceTokenizer.java
+++ /dev/null
@@ -1,124 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.lucene.analysis.cn.smart;
-
-import java.io.IOException;
-import java.io.Reader;
-
-import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
-import org.apache.lucene.util.AttributeFactory;
-import org.apache.lucene.util.AttributeSource;
-
-/**
- * Tokenizes input text into sentences.
- * <p>
- * The output tokens can then be broken into words with {@link WordTokenFilter}
- * </p>
- * @lucene.experimental
- * @deprecated Use {@link HMMChineseTokenizer} instead
- */
-@Deprecated
-public final class SentenceTokenizer extends Tokenizer {
-
-  /**
-   * End of sentence punctuation: ??????,!?;
-   */
-  private final static String PUNCTION = "??????,!?;";
-
-  private final StringBuilder buffer = new StringBuilder();
-
-  private int tokenStart = 0, tokenEnd = 0;
-  
-  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-  private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
-  private final TypeAttribute typeAtt = addAttribute(TypeAttribute.class);
-
-  public SentenceTokenizer() {
-  }
-
-  public SentenceTokenizer(AttributeFactory factory) {
-    super(factory);
-  }
-  
-  @Override
-  public boolean incrementToken() throws IOException {
-    clearAttributes();
-    buffer.setLength(0);
-    int ci;
-    char ch, pch;
-    boolean atBegin = true;
-    tokenStart = tokenEnd;
-    ci = input.read();
-    ch = (char) ci;
-
-    while (true) {
-      if (ci == -1) {
-        break;
-      } else if (PUNCTION.indexOf(ch) != -1) {
-        // End of a sentence
-        buffer.append(ch);
-        tokenEnd++;
-        break;
-      } else if (atBegin && Utility.SPACES.indexOf(ch) != -1) {
-        tokenStart++;
-        tokenEnd++;
-        ci = input.read();
-        ch = (char) ci;
-      } else {
-        buffer.append(ch);
-        atBegin = false;
-        tokenEnd++;
-        pch = ch;
-        ci = input.read();
-        ch = (char) ci;
-        // Two spaces, such as CR, LF
-        if (Utility.SPACES.indexOf(ch) != -1
-            && Utility.SPACES.indexOf(pch) != -1) {
-          // buffer.append(ch);
-          tokenEnd++;
-          break;
-        }
-      }
-    }
-    if (buffer.length() == 0)
-      return false;
-    else {
-      termAtt.setEmpty().append(buffer);
-      offsetAtt.setOffset(correctOffset(tokenStart), correctOffset(tokenEnd));
-      typeAtt.setType("sentence");
-      return true;
-    }
-  }
-
-  @Override
-  public void reset() throws IOException {
-    super.reset();
-    tokenStart = tokenEnd = 0;
-  }
-
-  @Override
-  public void end() throws IOException {
-    super.end();
-    // set final offset
-    final int finalOffset = correctOffset(tokenEnd);
-    offsetAtt.setOffset(finalOffset, finalOffset);
-  }
-}
diff --git a/lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SmartChineseAnalyzer.java b/lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SmartChineseAnalyzer.java
index a268cc1..8113443 100644
--- a/lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SmartChineseAnalyzer.java
+++ b/lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SmartChineseAnalyzer.java
@@ -29,7 +29,6 @@ import org.apache.lucene.analysis.en.PorterStemFilter;
 import org.apache.lucene.analysis.util.CharArraySet;
 import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.Version;
 
 /**
  * <p>
@@ -130,15 +129,8 @@ public final class SmartChineseAnalyzer extends Analyzer {
 
   @Override
   public TokenStreamComponents createComponents(String fieldName) {
-    final Tokenizer tokenizer;
-    TokenStream result;
-    if (getVersion().onOrAfter(Version.LUCENE_4_8_0)) {
-      tokenizer = new HMMChineseTokenizer();
-      result = tokenizer;
-    } else {
-      tokenizer = new SentenceTokenizer();
-      result = new WordTokenFilter(tokenizer);
-    }
+    final Tokenizer tokenizer = new HMMChineseTokenizer();
+    TokenStream result = tokenizer;
     // result = new LowerCaseFilter(result);
     // LowerCaseFilter is not needed, as SegTokenFilter lowercases Basic Latin text.
     // The porter stemming is too strict, this is not a bug, this is a feature:)
diff --git a/lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SmartChineseSentenceTokenizerFactory.java b/lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SmartChineseSentenceTokenizerFactory.java
deleted file mode 100644
index da844d3..0000000
--- a/lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SmartChineseSentenceTokenizerFactory.java
+++ /dev/null
@@ -1,46 +0,0 @@
-package org.apache.lucene.analysis.cn.smart;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Reader;
-import java.util.Map;
-
-import org.apache.lucene.analysis.util.TokenizerFactory;
-import org.apache.lucene.util.AttributeFactory;
-
-/**
- * Factory for the SmartChineseAnalyzer {@link SentenceTokenizer}
- * @lucene.experimental
- * @deprecated Use {@link HMMChineseTokenizerFactory} instead
- */
-@Deprecated
-public class SmartChineseSentenceTokenizerFactory extends TokenizerFactory {
-  
-  /** Creates a new SmartChineseSentenceTokenizerFactory */
-  public SmartChineseSentenceTokenizerFactory(Map<String,String> args) {
-    super(args);
-    if (!args.isEmpty()) {
-      throw new IllegalArgumentException("Unknown parameters: " + args);
-    }
-  }
-  
-  @Override
-  public SentenceTokenizer create(AttributeFactory factory) {
-    return new SentenceTokenizer(factory);
-  }
-}
diff --git a/lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SmartChineseWordTokenFilterFactory.java b/lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SmartChineseWordTokenFilterFactory.java
deleted file mode 100644
index eb1a758..0000000
--- a/lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SmartChineseWordTokenFilterFactory.java
+++ /dev/null
@@ -1,52 +0,0 @@
-package org.apache.lucene.analysis.cn.smart;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Map;
-
-import org.apache.lucene.analysis.TokenFilter;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.cn.smart.WordTokenFilter;
-import org.apache.lucene.analysis.util.TokenFilterFactory;
-
-/**
- * Factory for the SmartChineseAnalyzer {@link WordTokenFilter}
- * <p>
- * Note: this class will currently emit tokens for punctuation. So you should either add
- * a WordDelimiterFilter after to remove these (with concatenate off), or use the 
- * SmartChinese stoplist with a StopFilterFactory via:
- * <code>words="org/apache/lucene/analysis/cn/smart/stopwords.txt"</code>
- * @lucene.experimental
- * @deprecated Use {@link HMMChineseTokenizerFactory} instead
- */
-@Deprecated
-public class SmartChineseWordTokenFilterFactory extends TokenFilterFactory {
-  
-  /** Creates a new SmartChineseWordTokenFilterFactory */
-  public SmartChineseWordTokenFilterFactory(Map<String,String> args) {
-    super(args);
-    if (!args.isEmpty()) {
-      throw new IllegalArgumentException("Unknown parameters: " + args);
-    }
-  }
-  
-  @Override
-  public TokenFilter create(TokenStream input) {
-      return new WordTokenFilter(input);
-  }
-}
diff --git a/lucene/analysis/smartcn/src/resources/META-INF/services/org.apache.lucene.analysis.util.TokenFilterFactory b/lucene/analysis/smartcn/src/resources/META-INF/services/org.apache.lucene.analysis.util.TokenFilterFactory
deleted file mode 100644
index adf7f77..0000000
--- a/lucene/analysis/smartcn/src/resources/META-INF/services/org.apache.lucene.analysis.util.TokenFilterFactory
+++ /dev/null
@@ -1,16 +0,0 @@
-#  Licensed to the Apache Software Foundation (ASF) under one or more
-#  contributor license agreements.  See the NOTICE file distributed with
-#  this work for additional information regarding copyright ownership.
-#  The ASF licenses this file to You under the Apache License, Version 2.0
-#  (the "License"); you may not use this file except in compliance with
-#  the License.  You may obtain a copy of the License at
-#
-#       http://www.apache.org/licenses/LICENSE-2.0
-#
-#  Unless required by applicable law or agreed to in writing, software
-#  distributed under the License is distributed on an "AS IS" BASIS,
-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#  See the License for the specific language governing permissions and
-#  limitations under the License.
-
-org.apache.lucene.analysis.cn.smart.SmartChineseWordTokenFilterFactory
diff --git a/lucene/analysis/smartcn/src/resources/META-INF/services/org.apache.lucene.analysis.util.TokenizerFactory b/lucene/analysis/smartcn/src/resources/META-INF/services/org.apache.lucene.analysis.util.TokenizerFactory
index fc183f9..b885d88 100644
--- a/lucene/analysis/smartcn/src/resources/META-INF/services/org.apache.lucene.analysis.util.TokenizerFactory
+++ b/lucene/analysis/smartcn/src/resources/META-INF/services/org.apache.lucene.analysis.util.TokenizerFactory
@@ -14,4 +14,3 @@
 #  limitations under the License.
 
 org.apache.lucene.analysis.cn.smart.HMMChineseTokenizerFactory
-org.apache.lucene.analysis.cn.smart.SmartChineseSentenceTokenizerFactory
diff --git a/lucene/analysis/smartcn/src/test/org/apache/lucene/analysis/cn/smart/TestSmartChineseFactories.java b/lucene/analysis/smartcn/src/test/org/apache/lucene/analysis/cn/smart/TestSmartChineseFactories.java
deleted file mode 100644
index a5a61fc..0000000
--- a/lucene/analysis/smartcn/src/test/org/apache/lucene/analysis/cn/smart/TestSmartChineseFactories.java
+++ /dev/null
@@ -1,83 +0,0 @@
-package org.apache.lucene.analysis.cn.smart;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Reader;
-import java.io.StringReader;
-import java.util.HashMap;
-
-import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.Tokenizer;
-
-/** 
- * Tests for {@link SmartChineseSentenceTokenizerFactory} and 
- * {@link SmartChineseWordTokenFilterFactory}
- */
-@Deprecated
-public class TestSmartChineseFactories extends BaseTokenStreamTestCase {
-  /** Test showing the behavior with whitespace */
-  public void testSimple() throws Exception {
-    Reader reader = new StringReader("????????????");
-    TokenStream stream = whitespaceMockTokenizer(reader);
-    SmartChineseWordTokenFilterFactory factory = new SmartChineseWordTokenFilterFactory(new HashMap<String,String>());
-    stream = factory.create(stream);
-    // TODO: fix smart chinese to not emit punctuation tokens
-    // at the moment: you have to clean up with WDF, or use the stoplist, etc
-    assertTokenStreamContents(stream, 
-       new String[] { "??", "?", "?", "???", "??", "???", "," });
-  }
-  
-  /** Test showing the behavior with whitespace */
-  public void testTokenizer() throws Exception {
-    Reader reader = new StringReader("???????????????????????");
-    SmartChineseSentenceTokenizerFactory tokenizerFactory = new SmartChineseSentenceTokenizerFactory(new HashMap<String,String>());
-    TokenStream stream = tokenizerFactory.create();
-    ((Tokenizer)stream).setReader(reader);
-    SmartChineseWordTokenFilterFactory factory = new SmartChineseWordTokenFilterFactory(new HashMap<String,String>());
-    stream = factory.create(stream);
-    // TODO: fix smart chinese to not emit punctuation tokens
-    // at the moment: you have to clean up with WDF, or use the stoplist, etc
-    assertTokenStreamContents(stream, 
-       new String[] { "??", "?", "?", "???", "??", "???", ",", 
-        "??", "?", "?", "???", "??", "???", ","
-        });
-  }
-  
-  /** Test that bogus arguments result in exception */
-  public void testBogusArguments() throws Exception {
-    try {
-      new SmartChineseSentenceTokenizerFactory(new HashMap<String,String>() {{
-        put("bogusArg", "bogusValue");
-      }});
-      fail();
-    } catch (IllegalArgumentException expected) {
-      assertTrue(expected.getMessage().contains("Unknown parameters"));
-    }
-    
-    try {
-      new SmartChineseWordTokenFilterFactory(new HashMap<String,String>() {{
-        put("bogusArg", "bogusValue");
-      }});
-      fail();
-    } catch (IllegalArgumentException expected) {
-      assertTrue(expected.getMessage().contains("Unknown parameters"));
-    }
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/Placeholder.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/Placeholder.java
new file mode 100644
index 0000000..86b2351
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/Placeholder.java
@@ -0,0 +1,23 @@
+package org.apache.lucene.codecs;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** Remove this file when adding back compat codecs */
+public class Placeholder {
+  
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/UndeadNormsProducer.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/UndeadNormsProducer.java
deleted file mode 100644
index fa5b4f7..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/UndeadNormsProducer.java
+++ /dev/null
@@ -1,113 +0,0 @@
-package org.apache.lucene.codecs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.Map;
-
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.NumericDocValues;
-import org.apache.lucene.util.Accountable;
-
-/** 
- * Used only for backwards compatibility corner case, to provide
- * re-animated norms when all fields are undead.
- * 
- * @lucene.internal */
-public class UndeadNormsProducer extends NormsProducer {
-
-  /** Used to bring undead norms back to life. */
-  public final static String LEGACY_UNDEAD_NORMS_KEY = UndeadNormsProducer.class.getSimpleName() + ".undeadnorms";
-
-  /** Use this instance */
-  public final static NormsProducer INSTANCE = new UndeadNormsProducer();
-
-  private UndeadNormsProducer() {
-  }
-
-  /* Returns true if all indexed fields have undead norms. */
-  public static boolean isUndeadArmy(FieldInfos fieldInfos) {
-
-    boolean everythingIsUndead = true;
-    for(FieldInfo fieldInfo : fieldInfos) {
-      if (fieldInfo.hasNorms()) {
-        String isUndead = fieldInfo.getAttribute(LEGACY_UNDEAD_NORMS_KEY);
-        if (isUndead != null) {
-          assert "true".equals(isUndead);
-        } else {
-          everythingIsUndead = false;
-        }
-      }
-    }
-
-    return everythingIsUndead;
-  }
-
-  /** Returns true if this field has undead norms. */
-  public static boolean isUndead(FieldInfo fieldInfo) {
-    String isUndead = fieldInfo.getAttribute(LEGACY_UNDEAD_NORMS_KEY);
-    if (isUndead != null) {
-      // Bring undead norms back to life; this is set in Lucene40FieldInfosFormat, to emulate pre-5.0 undead norms
-      assert "true".equals(isUndead);
-      return true;
-    } else {
-      return false;
-    }
-  }
-
-  /** Call this to note that the field with these attributes has undead norms. */
-  public static void setUndead(Map<String,String> attributes) {
-    attributes.put(LEGACY_UNDEAD_NORMS_KEY, "true");
-  }
-
-  @Override
-  public NumericDocValues getNorms(FieldInfo field) throws IOException {
-    return DocValues.emptyNumeric();
-  }
-  
-  @Override
-  public void close() {
-  }
-
-  @Override
-  public long ramBytesUsed() {
-    return 0;
-  }
-  
-  @Override
-  public Iterable<? extends Accountable> getChildResources() {
-    return Collections.emptyList();
-  }
-
-  @Override
-  public void checkIntegrity() throws IOException {
-  }
-  
-  @Override
-  public NormsProducer getMergeInstance() throws IOException {
-    return this;
-  }
-
-  @Override
-  public String toString() {
-    return getClass().getSimpleName();
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/Lucene40BlockTreeTermsReader.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/Lucene40BlockTreeTermsReader.java
deleted file mode 100644
index 562c8e3..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/Lucene40BlockTreeTermsReader.java
+++ /dev/null
@@ -1,358 +0,0 @@
-package org.apache.lucene.codecs.blocktree;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.Iterator;
-import java.util.List;
-import java.util.TreeMap;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.FieldsProducer;
-import org.apache.lucene.codecs.PostingsReaderBase;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.Accountable;
-import org.apache.lucene.util.Accountables;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.fst.ByteSequenceOutputs;
-import org.apache.lucene.util.fst.Outputs;
-
-/** A block-based terms index and dictionary that assigns
- *  terms to variable length blocks according to how they
- *  share prefixes.  The terms index is a prefix trie
- *  whose leaves are term blocks.  The advantage of this
- *  approach is that seekExact is often able to
- *  determine a term cannot exist without doing any IO, and
- *  intersection with Automata is very fast.  Note that this
- *  terms dictionary has it's own fixed terms index (ie, it
- *  does not support a pluggable terms index
- *  implementation).
- *
- *  <p><b>NOTE</b>: this terms dictionary supports
- *  min/maxItemsPerBlock during indexing to control how
- *  much memory the terms index uses.</p>
- *
- *  <p>The data structure used by this implementation is very
- *  similar to a burst trie
- *  (http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.18.3499),
- *  but with added logic to break up too-large blocks of all
- *  terms sharing a given prefix into smaller ones.</p>
- *
- *  <p>Use {@link org.apache.lucene.index.CheckIndex} with the <code>-verbose</code>
- *  option to see summary statistics on the blocks in the
- *  dictionary.
- *
- * @lucene.experimental
- * @deprecated Only for 4.x backcompat
- */
-@Deprecated
-public final class Lucene40BlockTreeTermsReader extends FieldsProducer {
-
-  /** Extension of terms file */
-  static final String TERMS_EXTENSION = "tim";
-  final static String TERMS_CODEC_NAME = "BLOCK_TREE_TERMS_DICT";
-
-  /** Initial terms format. */
-  public static final int VERSION_START = 0;
-
-  /** Append-only */
-  public static final int VERSION_APPEND_ONLY = 1;
-
-  /** Meta data as array */
-  public static final int VERSION_META_ARRAY = 2;
-
-  /** checksums */
-  public static final int VERSION_CHECKSUM = 3;
-
-  /** min/max term */
-  public static final int VERSION_MIN_MAX_TERMS = 4;
-
-  /** Current terms format. */
-  public static final int VERSION_CURRENT = VERSION_MIN_MAX_TERMS;
-
-  /** Extension of terms index file */
-  static final String TERMS_INDEX_EXTENSION = "tip";
-  final static String TERMS_INDEX_CODEC_NAME = "BLOCK_TREE_TERMS_INDEX";
-  static final int OUTPUT_FLAGS_NUM_BITS = 2;
-  static final int OUTPUT_FLAGS_MASK = 0x3;
-  static final int OUTPUT_FLAG_IS_FLOOR = 0x1;
-  static final int OUTPUT_FLAG_HAS_TERMS = 0x2;
-  static final Outputs<BytesRef> FST_OUTPUTS = ByteSequenceOutputs.getSingleton();
-  static final BytesRef NO_OUTPUT = FST_OUTPUTS.getNoOutput();
-
-  // Open input to the main terms dict file (_X.tib)
-  final IndexInput in;
-
-  //private static final boolean DEBUG = BlockTreeTermsWriter.DEBUG;
-
-  // Reads the terms dict entries, to gather state to
-  // produce DocsEnum on demand
-  final PostingsReaderBase postingsReader;
-
-  private final TreeMap<String,Lucene40FieldReader> fields = new TreeMap<>();
-
-  /** File offset where the directory starts in the terms file. */
-  private long dirOffset;
-
-  /** File offset where the directory starts in the index file. */
-  private long indexDirOffset;
-
-  final String segment;
-  
-  private final int version;
-
-  /** Sole constructor. */
-  public Lucene40BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state)
-    throws IOException {
-    
-    this.postingsReader = postingsReader;
-
-    this.segment = state.segmentInfo.name;
-    String termsFileName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);
-    in = state.directory.openInput(termsFileName, state.context);
-
-    boolean success = false;
-    IndexInput indexIn = null;
-
-    try {
-      version = readHeader(in);
-      String indexFileName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);
-      indexIn = state.directory.openInput(indexFileName, state.context);
-      int indexVersion = readIndexHeader(indexIn);
-      if (indexVersion != version) {
-        throw new CorruptIndexException("mixmatched version files: " + in + "=" + version + "," + indexIn + "=" + indexVersion, indexIn);
-      }
-      
-      // verify
-      if (version >= VERSION_CHECKSUM) {
-        CodecUtil.checksumEntireFile(indexIn);
-      }
-
-      // Have PostingsReader init itself
-      postingsReader.init(in, state);
-      
-      
-      // NOTE: data file is too costly to verify checksum against all the bytes on open,
-      // but for now we at least verify proper structure of the checksum footer: which looks
-      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
-      // such as file truncation.
-      if (version >= VERSION_CHECKSUM) {
-        CodecUtil.retrieveChecksum(in);
-      }
-
-      // Read per-field details
-      seekDir(in, dirOffset);
-      seekDir(indexIn, indexDirOffset);
-
-      final int numFields = in.readVInt();
-      if (numFields < 0) {
-        throw new CorruptIndexException("invalid numFields: " + numFields, in);
-      }
-
-      for(int i=0;i<numFields;i++) {
-        final int field = in.readVInt();
-        final long numTerms = in.readVLong();
-        if (numTerms <= 0) {
-          throw new CorruptIndexException("Illegal numTerms for field number: " + field, in);
-        }
-        final int numBytes = in.readVInt();
-        if (numBytes < 0) {
-          throw new CorruptIndexException("invalid rootCode for field number: " + field + ", numBytes=" + numBytes, in);
-        }
-        final BytesRef rootCode = new BytesRef(new byte[numBytes]);
-        in.readBytes(rootCode.bytes, 0, numBytes);
-        rootCode.length = numBytes;
-        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);
-        if (fieldInfo == null) {
-          throw new CorruptIndexException("invalid field number: " + field, in);
-        }
-        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();
-        final long sumDocFreq = in.readVLong();
-        final int docCount = in.readVInt();
-        final int longsSize = version >= VERSION_META_ARRAY ? in.readVInt() : 0;
-        if (longsSize < 0) {
-          throw new CorruptIndexException("invalid longsSize for field: " + fieldInfo.name + ", longsSize=" + longsSize, in);
-        }
-        BytesRef minTerm, maxTerm;
-        if (version >= VERSION_MIN_MAX_TERMS) {
-          minTerm = readBytesRef(in);
-          maxTerm = readBytesRef(in);
-        } else {
-          minTerm = maxTerm = null;
-        }
-        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs
-          throw new CorruptIndexException("invalid docCount: " + docCount + " maxDoc: " + state.segmentInfo.getDocCount(), in);
-        }
-        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field
-          throw new CorruptIndexException("invalid sumDocFreq: " + sumDocFreq + " docCount: " + docCount, in);
-        }
-        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings
-          throw new CorruptIndexException("invalid sumTotalTermFreq: " + sumTotalTermFreq + " sumDocFreq: " + sumDocFreq, in);
-        }
-        final long indexStartFP = indexIn.readVLong();
-        Lucene40FieldReader previous = fields.put(fieldInfo.name,       
-                                          new Lucene40FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,
-                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));
-        if (previous != null) {
-          throw new CorruptIndexException("duplicate field: " + fieldInfo.name, in);
-        }
-      }
-      indexIn.close();
-
-      success = true;
-    } finally {
-      if (!success) {
-        // this.close() will close in:
-        IOUtils.closeWhileHandlingException(indexIn, this);
-      }
-    }
-  }
-
-  private static BytesRef readBytesRef(IndexInput in) throws IOException {
-    BytesRef bytes = new BytesRef();
-    bytes.length = in.readVInt();
-    bytes.bytes = new byte[bytes.length];
-    in.readBytes(bytes.bytes, 0, bytes.length);
-    return bytes;
-  }
-
-  /** Reads terms file header. */
-  private int readHeader(IndexInput input) throws IOException {
-    int version = CodecUtil.checkHeader(input, TERMS_CODEC_NAME,
-                          VERSION_START,
-                          VERSION_CURRENT);
-    if (version < VERSION_APPEND_ONLY) {
-      dirOffset = input.readLong();
-    }
-    return version;
-  }
-
-  /** Reads index file header. */
-  private int readIndexHeader(IndexInput input) throws IOException {
-    int version = CodecUtil.checkHeader(input, TERMS_INDEX_CODEC_NAME,
-                          VERSION_START,
-                          VERSION_CURRENT);
-    if (version < VERSION_APPEND_ONLY) {
-      indexDirOffset = input.readLong(); 
-    }
-    return version;
-  }
-
-  /** Seek {@code input} to the directory offset. */
-  private void seekDir(IndexInput input, long dirOffset)
-      throws IOException {
-    if (version >= VERSION_CHECKSUM) {
-      input.seek(input.length() - CodecUtil.footerLength() - 8);
-      dirOffset = input.readLong();
-    } else if (version >= VERSION_APPEND_ONLY) {
-      input.seek(input.length() - 8);
-      dirOffset = input.readLong();
-    }
-    input.seek(dirOffset);
-  }
-
-  // for debugging
-  // private static String toHex(int v) {
-  //   return "0x" + Integer.toHexString(v);
-  // }
-
-  @Override
-  public void close() throws IOException {
-    try {
-      IOUtils.close(in, postingsReader);
-    } finally { 
-      // Clear so refs to terms index is GCable even if
-      // app hangs onto us:
-      fields.clear();
-    }
-  }
-
-  @Override
-  public Iterator<String> iterator() {
-    return Collections.unmodifiableSet(fields.keySet()).iterator();
-  }
-
-  @Override
-  public Terms terms(String field) throws IOException {
-    assert field != null;
-    return fields.get(field);
-  }
-
-  @Override
-  public int size() {
-    return fields.size();
-  }
-
-  // for debugging
-  String brToString(BytesRef b) {
-    if (b == null) {
-      return "null";
-    } else {
-      try {
-        return b.utf8ToString() + " " + b;
-      } catch (Throwable t) {
-        // If BytesRef isn't actually UTF8, or it's eg a
-        // prefix of UTF8 that ends mid-unicode-char, we
-        // fallback to hex:
-        return b.toString();
-      }
-    }
-  }
-
-  @Override
-  public long ramBytesUsed() {
-    long sizeInBytes = postingsReader.ramBytesUsed();
-    for(Lucene40FieldReader reader : fields.values()) {
-      sizeInBytes += reader.ramBytesUsed();
-    }
-    return sizeInBytes;
-  }
-
-  @Override
-  public Iterable<? extends Accountable> getChildResources() {
-    List<Accountable> resources = new ArrayList<>();
-    resources.addAll(Accountables.namedAccountables("field", fields));
-    resources.add(Accountables.namedAccountable("delegate", postingsReader));
-    return Collections.unmodifiableList(resources);
-  }
-
-  @Override
-  public void checkIntegrity() throws IOException {
-    if (version >= VERSION_CHECKSUM) {      
-      // term dictionary
-      CodecUtil.checksumEntireFile(in);
-      
-      // postings
-      postingsReader.checkIntegrity();
-    }
-  }
-
-  @Override
-  public String toString() {
-    return getClass().getSimpleName() + "(fields=" + fields.size() + ",delegate=" + postingsReader + ")";
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/Lucene40FieldReader.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/Lucene40FieldReader.java
deleted file mode 100644
index bed4d8c..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/Lucene40FieldReader.java
+++ /dev/null
@@ -1,202 +0,0 @@
-package org.apache.lucene.codecs.blocktree;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collections;
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.store.ByteArrayDataInput;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.Accountable;
-import org.apache.lucene.util.Accountables;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.RamUsageEstimator;
-import org.apache.lucene.util.automaton.CompiledAutomaton;
-import org.apache.lucene.util.fst.ByteSequenceOutputs;
-import org.apache.lucene.util.fst.FST;
-
-/**
- * BlockTree's implementation of {@link Terms}.
- * @deprecated Only for 4.x backcompat
- */
-@Deprecated
-final class Lucene40FieldReader extends Terms implements Accountable {
-
-  private static final long BASE_RAM_BYTES_USED =
-      RamUsageEstimator.shallowSizeOfInstance(Lucene40FieldReader.class)
-      + 3 * RamUsageEstimator.shallowSizeOfInstance(BytesRef.class);
-
-  final long numTerms;
-  final FieldInfo fieldInfo;
-  final long sumTotalTermFreq;
-  final long sumDocFreq;
-  final int docCount;
-  final long indexStartFP;
-  final long rootBlockFP;
-  final BytesRef rootCode;
-  final BytesRef minTerm;
-  final BytesRef maxTerm;
-  final int longsSize;
-  final Lucene40BlockTreeTermsReader parent;
-
-  final FST<BytesRef> index;
-  //private boolean DEBUG;
-
-  Lucene40FieldReader(Lucene40BlockTreeTermsReader parent, FieldInfo fieldInfo, long numTerms, BytesRef rootCode, long sumTotalTermFreq, long sumDocFreq, int docCount,
-                      long indexStartFP, int longsSize, IndexInput indexIn, BytesRef minTerm, BytesRef maxTerm) throws IOException {
-    assert numTerms > 0;
-    this.fieldInfo = fieldInfo;
-    //DEBUG = BlockTreeTermsReader.DEBUG && fieldInfo.name.equals("id");
-    this.parent = parent;
-    this.numTerms = numTerms;
-    this.sumTotalTermFreq = sumTotalTermFreq; 
-    this.sumDocFreq = sumDocFreq; 
-    this.docCount = docCount;
-    this.indexStartFP = indexStartFP;
-    this.rootCode = rootCode;
-    this.longsSize = longsSize;
-    this.minTerm = minTerm;
-    this.maxTerm = maxTerm;
-    // if (DEBUG) {
-    //   System.out.println("BTTR: seg=" + segment + " field=" + fieldInfo.name + " rootBlockCode=" + rootCode + " divisor=" + indexDivisor);
-    // }
-
-    rootBlockFP = (new ByteArrayDataInput(rootCode.bytes, rootCode.offset, rootCode.length)).readVLong() >>> Lucene40BlockTreeTermsReader.OUTPUT_FLAGS_NUM_BITS;
-
-    if (indexIn != null) {
-      final IndexInput clone = indexIn.clone();
-      //System.out.println("start=" + indexStartFP + " field=" + fieldInfo.name);
-      clone.seek(indexStartFP);
-      index = new FST<>(clone, ByteSequenceOutputs.getSingleton());
-        
-      /*
-        if (false) {
-        final String dotFileName = segment + "_" + fieldInfo.name + ".dot";
-        Writer w = new OutputStreamWriter(new FileOutputStream(dotFileName));
-        Util.toDot(index, w, false, false);
-        System.out.println("FST INDEX: SAVED to " + dotFileName);
-        w.close();
-        }
-      */
-    } else {
-      index = null;
-    }
-  }
-
-  @Override
-  public BytesRef getMin() throws IOException {
-    if (minTerm == null) {
-      // Older index that didn't store min/maxTerm
-      return super.getMin();
-    } else {
-      return minTerm;
-    }
-  }
-
-  @Override
-  public BytesRef getMax() throws IOException {
-    if (maxTerm == null) {
-      // Older index that didn't store min/maxTerm
-      return super.getMax();
-    } else {
-      return maxTerm;
-    }
-  }
-
-  /** For debugging -- used by CheckIndex too*/
-  @Override
-  public Lucene40Stats getStats() throws IOException {
-    return new Lucene40SegmentTermsEnum(this).computeBlockStats();
-  }
-
-  @Override
-  public boolean hasFreqs() {
-    return fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;
-  }
-
-  @Override
-  public boolean hasOffsets() {
-    return fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
-  }
-
-  @Override
-  public boolean hasPositions() {
-    return fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
-  }
-    
-  @Override
-  public boolean hasPayloads() {
-    return fieldInfo.hasPayloads();
-  }
-
-  @Override
-  public TermsEnum iterator(TermsEnum reuse) throws IOException {
-    return new Lucene40SegmentTermsEnum(this);
-  }
-
-  @Override
-  public long size() {
-    return numTerms;
-  }
-
-  @Override
-  public long getSumTotalTermFreq() {
-    return sumTotalTermFreq;
-  }
-
-  @Override
-  public long getSumDocFreq() {
-    return sumDocFreq;
-  }
-
-  @Override
-  public int getDocCount() {
-    return docCount;
-  }
-
-  @Override
-  public TermsEnum intersect(CompiledAutomaton compiled, BytesRef startTerm) throws IOException {
-    if (compiled.type != CompiledAutomaton.AUTOMATON_TYPE.NORMAL) {
-      throw new IllegalArgumentException("please use CompiledAutomaton.getTermsEnum instead");
-    }
-    return new Lucene40IntersectTermsEnum(this, compiled, startTerm);
-  }
-    
-  @Override
-  public long ramBytesUsed() {
-    return BASE_RAM_BYTES_USED + ((index!=null)? index.ramBytesUsed() : 0);
-  }
-
-  @Override
-  public Iterable<? extends Accountable> getChildResources() {
-    if (index == null) {
-      return Collections.emptyList();
-    } else {
-      return Collections.singleton(Accountables.namedAccountable("term index", index));
-    }
-  }
-
-  @Override
-  public String toString() {
-    return "BlockTreeTerms(terms=" + numTerms + ",postings=" + sumDocFreq + ",positions=" + sumTotalTermFreq + ",docs=" + docCount + ")";
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/Lucene40IntersectTermsEnum.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/Lucene40IntersectTermsEnum.java
deleted file mode 100644
index 292b57a..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/Lucene40IntersectTermsEnum.java
+++ /dev/null
@@ -1,490 +0,0 @@
-package org.apache.lucene.codecs.blocktree;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.TermState;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.RamUsageEstimator;
-import org.apache.lucene.util.StringHelper;
-import org.apache.lucene.util.automaton.CompiledAutomaton;
-import org.apache.lucene.util.automaton.RunAutomaton;
-import org.apache.lucene.util.fst.ByteSequenceOutputs;
-import org.apache.lucene.util.fst.FST;
-import org.apache.lucene.util.fst.Outputs;
-
-// NOTE: cannot seek!
-
-/**
- * @deprecated Only for 4.x backcompat
- */
-@Deprecated
-final class Lucene40IntersectTermsEnum extends TermsEnum {
-  final IndexInput in;
-  final static Outputs<BytesRef> fstOutputs = ByteSequenceOutputs.getSingleton();
-
-  private Lucene40IntersectTermsEnumFrame[] stack;
-      
-  @SuppressWarnings({"rawtypes","unchecked"}) private FST.Arc<BytesRef>[] arcs = new FST.Arc[5];
-
-  final RunAutomaton runAutomaton;
-  final CompiledAutomaton compiledAutomaton;
-
-  private Lucene40IntersectTermsEnumFrame currentFrame;
-
-  private final BytesRef term = new BytesRef();
-
-  private final FST.BytesReader fstReader;
-
-  final Lucene40FieldReader fr;
-
-  private BytesRef savedStartTerm;
-      
-  // TODO: in some cases we can filter by length?  eg
-  // regexp foo*bar must be at least length 6 bytes
-  public Lucene40IntersectTermsEnum(Lucene40FieldReader fr, CompiledAutomaton compiled, BytesRef startTerm) throws IOException {
-    // if (DEBUG) {
-    //   System.out.println("\nintEnum.init seg=" + segment + " commonSuffix=" + brToString(compiled.commonSuffixRef));
-    // }
-    this.fr = fr;
-    runAutomaton = compiled.runAutomaton;
-    compiledAutomaton = compiled;
-    in = fr.parent.in.clone();
-    stack = new Lucene40IntersectTermsEnumFrame[5];
-    for(int idx=0;idx<stack.length;idx++) {
-      stack[idx] = new Lucene40IntersectTermsEnumFrame(this, idx);
-    }
-    for(int arcIdx=0;arcIdx<arcs.length;arcIdx++) {
-      arcs[arcIdx] = new FST.Arc<>();
-    }
-
-    if (fr.index == null) {
-      fstReader = null;
-    } else {
-      fstReader = fr.index.getBytesReader();
-    }
-
-    // TODO: if the automaton is "smallish" we really
-    // should use the terms index to seek at least to
-    // the initial term and likely to subsequent terms
-    // (or, maybe just fallback to ATE for such cases).
-    // Else the seek cost of loading the frames will be
-    // too costly.
-
-    final FST.Arc<BytesRef> arc = fr.index.getFirstArc(arcs[0]);
-    // Empty string prefix must have an output in the index!
-    assert arc.isFinal();
-
-    // Special pushFrame since it's the first one:
-    final Lucene40IntersectTermsEnumFrame f = stack[0];
-    f.fp = f.fpOrig = fr.rootBlockFP;
-    f.prefix = 0;
-    f.setState(runAutomaton.getInitialState());
-    f.arc = arc;
-    f.outputPrefix = arc.output;
-    f.load(fr.rootCode);
-
-    // for assert:
-    assert setSavedStartTerm(startTerm);
-
-    currentFrame = f;
-    if (startTerm != null) {
-      seekToStartTerm(startTerm);
-    }
-  }
-
-  // only for assert:
-  private boolean setSavedStartTerm(BytesRef startTerm) {
-    savedStartTerm = startTerm == null ? null : BytesRef.deepCopyOf(startTerm);
-    return true;
-  }
-
-  @Override
-  public TermState termState() throws IOException {
-    currentFrame.decodeMetaData();
-    return currentFrame.termState.clone();
-  }
-
-  private Lucene40IntersectTermsEnumFrame getFrame(int ord) throws IOException {
-    if (ord >= stack.length) {
-      final Lucene40IntersectTermsEnumFrame[] next = new Lucene40IntersectTermsEnumFrame[ArrayUtil.oversize(1+ord, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
-      System.arraycopy(stack, 0, next, 0, stack.length);
-      for(int stackOrd=stack.length;stackOrd<next.length;stackOrd++) {
-        next[stackOrd] = new Lucene40IntersectTermsEnumFrame(this, stackOrd);
-      }
-      stack = next;
-    }
-    assert stack[ord].ord == ord;
-    return stack[ord];
-  }
-
-  private FST.Arc<BytesRef> getArc(int ord) {
-    if (ord >= arcs.length) {
-      @SuppressWarnings({"rawtypes","unchecked"}) final FST.Arc<BytesRef>[] next =
-      new FST.Arc[ArrayUtil.oversize(1+ord, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
-      System.arraycopy(arcs, 0, next, 0, arcs.length);
-      for(int arcOrd=arcs.length;arcOrd<next.length;arcOrd++) {
-        next[arcOrd] = new FST.Arc<>();
-      }
-      arcs = next;
-    }
-    return arcs[ord];
-  }
-
-  private Lucene40IntersectTermsEnumFrame pushFrame(int state) throws IOException {
-    final Lucene40IntersectTermsEnumFrame f = getFrame(currentFrame == null ? 0 : 1+currentFrame.ord);
-        
-    f.fp = f.fpOrig = currentFrame.lastSubFP;
-    f.prefix = currentFrame.prefix + currentFrame.suffix;
-    // if (DEBUG) System.out.println("    pushFrame state=" + state + " prefix=" + f.prefix);
-    f.setState(state);
-
-    // Walk the arc through the index -- we only
-    // "bother" with this so we can get the floor data
-    // from the index and skip floor blocks when
-    // possible:
-    FST.Arc<BytesRef> arc = currentFrame.arc;
-    int idx = currentFrame.prefix;
-    assert currentFrame.suffix > 0;
-    BytesRef output = currentFrame.outputPrefix;
-    while (idx < f.prefix) {
-      final int target = term.bytes[idx] & 0xff;
-      // TODO: we could be more efficient for the next()
-      // case by using current arc as starting point,
-      // passed to findTargetArc
-      arc = fr.index.findTargetArc(target, arc, getArc(1+idx), fstReader);
-      assert arc != null;
-      output = fstOutputs.add(output, arc.output);
-      idx++;
-    }
-
-    f.arc = arc;
-    f.outputPrefix = output;
-    assert arc.isFinal();
-    f.load(fstOutputs.add(output, arc.nextFinalOutput));
-    return f;
-  }
-
-  @Override
-  public BytesRef term() {
-    return term;
-  }
-
-  @Override
-  public int docFreq() throws IOException {
-    //if (DEBUG) System.out.println("BTIR.docFreq");
-    currentFrame.decodeMetaData();
-    //if (DEBUG) System.out.println("  return " + currentFrame.termState.docFreq);
-    return currentFrame.termState.docFreq;
-  }
-
-  @Override
-  public long totalTermFreq() throws IOException {
-    currentFrame.decodeMetaData();
-    return currentFrame.termState.totalTermFreq;
-  }
-
-  @Override
-  public DocsEnum docs(Bits skipDocs, DocsEnum reuse, int flags) throws IOException {
-    currentFrame.decodeMetaData();
-    return fr.parent.postingsReader.docs(fr.fieldInfo, currentFrame.termState, skipDocs, reuse, flags);
-  }
-
-  @Override
-  public DocsAndPositionsEnum docsAndPositions(Bits skipDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
-    if (fr.fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) < 0) {
-      // Positions were not indexed:
-      return null;
-    }
-
-    currentFrame.decodeMetaData();
-    return fr.parent.postingsReader.docsAndPositions(fr.fieldInfo, currentFrame.termState, skipDocs, reuse, flags);
-  }
-
-  private int getState() {
-    int state = currentFrame.state;
-    for(int idx=0;idx<currentFrame.suffix;idx++) {
-      state = runAutomaton.step(state,  currentFrame.suffixBytes[currentFrame.startBytePos+idx] & 0xff);
-      assert state != -1;
-    }
-    return state;
-  }
-
-  // NOTE: specialized to only doing the first-time
-  // seek, but we could generalize it to allow
-  // arbitrary seekExact/Ceil.  Note that this is a
-  // seekFloor!
-  private void seekToStartTerm(BytesRef target) throws IOException {
-    //if (DEBUG) System.out.println("seek to startTerm=" + target.utf8ToString());
-    assert currentFrame.ord == 0;
-    if (term.length < target.length) {
-      term.bytes = ArrayUtil.grow(term.bytes, target.length);
-    }
-    FST.Arc<BytesRef> arc = arcs[0];
-    assert arc == currentFrame.arc;
-
-    for(int idx=0;idx<=target.length;idx++) {
-
-      while (true) {
-        final int savePos = currentFrame.suffixesReader.getPosition();
-        final int saveStartBytePos = currentFrame.startBytePos;
-        final int saveSuffix = currentFrame.suffix;
-        final long saveLastSubFP = currentFrame.lastSubFP;
-        final int saveTermBlockOrd = currentFrame.termState.termBlockOrd;
-
-        final boolean isSubBlock = currentFrame.next();
-
-        //if (DEBUG) System.out.println("    cycle ent=" + currentFrame.nextEnt + " (of " + currentFrame.entCount + ") prefix=" + currentFrame.prefix + " suffix=" + currentFrame.suffix + " isBlock=" + isSubBlock + " firstLabel=" + (currentFrame.suffix == 0 ? "" : (currentFrame.suffixBytes[currentFrame.startBytePos])&0xff));
-        term.length = currentFrame.prefix + currentFrame.suffix;
-        if (term.bytes.length < term.length) {
-          term.bytes = ArrayUtil.grow(term.bytes, term.length);
-        }
-        System.arraycopy(currentFrame.suffixBytes, currentFrame.startBytePos, term.bytes, currentFrame.prefix, currentFrame.suffix);
-
-        if (isSubBlock && StringHelper.startsWith(target, term)) {
-          // Recurse
-          //if (DEBUG) System.out.println("      recurse!");
-          currentFrame = pushFrame(getState());
-          break;
-        } else {
-          final int cmp = term.compareTo(target);
-          if (cmp < 0) {
-            if (currentFrame.nextEnt == currentFrame.entCount) {
-              if (!currentFrame.isLastInFloor) {
-                //if (DEBUG) System.out.println("  load floorBlock");
-                currentFrame.loadNextFloorBlock();
-                continue;
-              } else {
-                //if (DEBUG) System.out.println("  return term=" + brToString(term));
-                return;
-              }
-            }
-            continue;
-          } else if (cmp == 0) {
-            //if (DEBUG) System.out.println("  return term=" + brToString(term));
-            return;
-          } else {
-            // Fallback to prior entry: the semantics of
-            // this method is that the first call to
-            // next() will return the term after the
-            // requested term
-            currentFrame.nextEnt--;
-            currentFrame.lastSubFP = saveLastSubFP;
-            currentFrame.startBytePos = saveStartBytePos;
-            currentFrame.suffix = saveSuffix;
-            currentFrame.suffixesReader.setPosition(savePos);
-            currentFrame.termState.termBlockOrd = saveTermBlockOrd;
-            System.arraycopy(currentFrame.suffixBytes, currentFrame.startBytePos, term.bytes, currentFrame.prefix, currentFrame.suffix);
-            term.length = currentFrame.prefix + currentFrame.suffix;
-            // If the last entry was a block we don't
-            // need to bother recursing and pushing to
-            // the last term under it because the first
-            // next() will simply skip the frame anyway
-            return;
-          }
-        }
-      }
-    }
-
-    assert false;
-  }
-
-  @Override
-  public BytesRef next() throws IOException {
-
-    // if (DEBUG) {
-    //   System.out.println("\nintEnum.next seg=" + segment);
-    //   System.out.println("  frame ord=" + currentFrame.ord + " prefix=" + brToString(new BytesRef(term.bytes, term.offset, currentFrame.prefix)) + " state=" + currentFrame.state + " lastInFloor?=" + currentFrame.isLastInFloor + " fp=" + currentFrame.fp + " trans=" + (currentFrame.transitions.length == 0 ? "n/a" : currentFrame.transitions[currentFrame.transitionIndex]) + " outputPrefix=" + currentFrame.outputPrefix);
-    // }
-
-    nextTerm:
-    while(true) {
-      // Pop finished frames
-      while (currentFrame.nextEnt == currentFrame.entCount) {
-        if (!currentFrame.isLastInFloor) {
-          //if (DEBUG) System.out.println("    next-floor-block");
-          currentFrame.loadNextFloorBlock();
-          //if (DEBUG) System.out.println("\n  frame ord=" + currentFrame.ord + " prefix=" + brToString(new BytesRef(term.bytes, term.offset, currentFrame.prefix)) + " state=" + currentFrame.state + " lastInFloor?=" + currentFrame.isLastInFloor + " fp=" + currentFrame.fp + " trans=" + (currentFrame.transitions.length == 0 ? "n/a" : currentFrame.transitions[currentFrame.transitionIndex]) + " outputPrefix=" + currentFrame.outputPrefix);
-        } else {
-          //if (DEBUG) System.out.println("  pop frame");
-          if (currentFrame.ord == 0) {
-            return null;
-          }
-          final long lastFP = currentFrame.fpOrig;
-          currentFrame = stack[currentFrame.ord-1];
-          assert currentFrame.lastSubFP == lastFP;
-          //if (DEBUG) System.out.println("\n  frame ord=" + currentFrame.ord + " prefix=" + brToString(new BytesRef(term.bytes, term.offset, currentFrame.prefix)) + " state=" + currentFrame.state + " lastInFloor?=" + currentFrame.isLastInFloor + " fp=" + currentFrame.fp + " trans=" + (currentFrame.transitions.length == 0 ? "n/a" : currentFrame.transitions[currentFrame.transitionIndex]) + " outputPrefix=" + currentFrame.outputPrefix);
-        }
-      }
-
-      final boolean isSubBlock = currentFrame.next();
-      // if (DEBUG) {
-      //   final BytesRef suffixRef = new BytesRef();
-      //   suffixRef.bytes = currentFrame.suffixBytes;
-      //   suffixRef.offset = currentFrame.startBytePos;
-      //   suffixRef.length = currentFrame.suffix;
-      //   System.out.println("    " + (isSubBlock ? "sub-block" : "term") + " " + currentFrame.nextEnt + " (of " + currentFrame.entCount + ") suffix=" + brToString(suffixRef));
-      // }
-
-      if (currentFrame.suffix != 0) {
-        final int label = currentFrame.suffixBytes[currentFrame.startBytePos] & 0xff;
-        while (label > currentFrame.curTransitionMax) {
-          if (currentFrame.transitionIndex >= currentFrame.transitionCount-1) {
-            // Stop processing this frame -- no further
-            // matches are possible because we've moved
-            // beyond what the max transition will allow
-            //if (DEBUG) System.out.println("      break: trans=" + (currentFrame.transitions.length == 0 ? "n/a" : currentFrame.transitions[currentFrame.transitionIndex]));
-
-            // sneaky!  forces a pop above
-            currentFrame.isLastInFloor = true;
-            currentFrame.nextEnt = currentFrame.entCount;
-            continue nextTerm;
-          }
-          currentFrame.transitionIndex++;
-          compiledAutomaton.automaton.getNextTransition(currentFrame.transition);
-          currentFrame.curTransitionMax = currentFrame.transition.max;
-          //if (DEBUG) System.out.println("      next trans=" + currentFrame.transitions[currentFrame.transitionIndex]);
-        }
-      }
-
-      // First test the common suffix, if set:
-      if (compiledAutomaton.commonSuffixRef != null && !isSubBlock) {
-        final int termLen = currentFrame.prefix + currentFrame.suffix;
-        if (termLen < compiledAutomaton.commonSuffixRef.length) {
-          // No match
-          // if (DEBUG) {
-          //   System.out.println("      skip: common suffix length");
-          // }
-          continue nextTerm;
-        }
-
-        final byte[] suffixBytes = currentFrame.suffixBytes;
-        final byte[] commonSuffixBytes = compiledAutomaton.commonSuffixRef.bytes;
-
-        final int lenInPrefix = compiledAutomaton.commonSuffixRef.length - currentFrame.suffix;
-        assert compiledAutomaton.commonSuffixRef.offset == 0;
-        int suffixBytesPos;
-        int commonSuffixBytesPos = 0;
-
-        if (lenInPrefix > 0) {
-          // A prefix of the common suffix overlaps with
-          // the suffix of the block prefix so we first
-          // test whether the prefix part matches:
-          final byte[] termBytes = term.bytes;
-          int termBytesPos = currentFrame.prefix - lenInPrefix;
-          assert termBytesPos >= 0;
-          final int termBytesPosEnd = currentFrame.prefix;
-          while (termBytesPos < termBytesPosEnd) {
-            if (termBytes[termBytesPos++] != commonSuffixBytes[commonSuffixBytesPos++]) {
-              // if (DEBUG) {
-              //   System.out.println("      skip: common suffix mismatch (in prefix)");
-              // }
-              continue nextTerm;
-            }
-          }
-          suffixBytesPos = currentFrame.startBytePos;
-        } else {
-          suffixBytesPos = currentFrame.startBytePos + currentFrame.suffix - compiledAutomaton.commonSuffixRef.length;
-        }
-
-        // Test overlapping suffix part:
-        final int commonSuffixBytesPosEnd = compiledAutomaton.commonSuffixRef.length;
-        while (commonSuffixBytesPos < commonSuffixBytesPosEnd) {
-          if (suffixBytes[suffixBytesPos++] != commonSuffixBytes[commonSuffixBytesPos++]) {
-            // if (DEBUG) {
-            //   System.out.println("      skip: common suffix mismatch");
-            // }
-            continue nextTerm;
-          }
-        }
-      }
-
-      // TODO: maybe we should do the same linear test
-      // that AutomatonTermsEnum does, so that if we
-      // reach a part of the automaton where .* is
-      // "temporarily" accepted, we just blindly .next()
-      // until the limit
-
-      // See if the term prefix matches the automaton:
-      int state = currentFrame.state;
-      for (int idx=0;idx<currentFrame.suffix;idx++) {
-        state = runAutomaton.step(state,  currentFrame.suffixBytes[currentFrame.startBytePos+idx] & 0xff);
-        if (state == -1) {
-          // No match
-          //System.out.println("    no s=" + state);
-          continue nextTerm;
-        } else {
-          //System.out.println("    c s=" + state);
-        }
-      }
-
-      if (isSubBlock) {
-        // Match!  Recurse:
-        //if (DEBUG) System.out.println("      sub-block match to state=" + state + "; recurse fp=" + currentFrame.lastSubFP);
-        copyTerm();
-        currentFrame = pushFrame(state);
-        //if (DEBUG) System.out.println("\n  frame ord=" + currentFrame.ord + " prefix=" + brToString(new BytesRef(term.bytes, term.offset, currentFrame.prefix)) + " state=" + currentFrame.state + " lastInFloor?=" + currentFrame.isLastInFloor + " fp=" + currentFrame.fp + " trans=" + (currentFrame.transitions.length == 0 ? "n/a" : currentFrame.transitions[currentFrame.transitionIndex]) + " outputPrefix=" + currentFrame.outputPrefix);
-      } else if (runAutomaton.isAccept(state)) {
-        copyTerm();
-        //if (DEBUG) System.out.println("      term match to state=" + state + "; return term=" + brToString(term));
-        assert savedStartTerm == null || term.compareTo(savedStartTerm) > 0: "saveStartTerm=" + savedStartTerm.utf8ToString() + " term=" + term.utf8ToString();
-        return term;
-      } else {
-        //System.out.println("    no s=" + state);
-      }
-    }
-  }
-
-  private void copyTerm() {
-    //System.out.println("      copyTerm cur.prefix=" + currentFrame.prefix + " cur.suffix=" + currentFrame.suffix + " first=" + (char) currentFrame.suffixBytes[currentFrame.startBytePos]);
-    final int len = currentFrame.prefix + currentFrame.suffix;
-    if (term.bytes.length < len) {
-      term.bytes = ArrayUtil.grow(term.bytes, len);
-    }
-    System.arraycopy(currentFrame.suffixBytes, currentFrame.startBytePos, term.bytes, currentFrame.prefix, currentFrame.suffix);
-    term.length = len;
-  }
-
-  @Override
-  public boolean seekExact(BytesRef text) {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  public void seekExact(long ord) {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  public long ord() {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  public SeekStatus seekCeil(BytesRef text) {
-    throw new UnsupportedOperationException();
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/Lucene40IntersectTermsEnumFrame.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/Lucene40IntersectTermsEnumFrame.java
deleted file mode 100644
index a6f2200..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/Lucene40IntersectTermsEnumFrame.java
+++ /dev/null
@@ -1,302 +0,0 @@
-package org.apache.lucene.codecs.blocktree;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.BlockTermState;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.store.ByteArrayDataInput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.automaton.Transition;
-import org.apache.lucene.util.fst.FST;
-
-
-
-/**
- * @deprecated Only for 4.x backcompat
- */
-// TODO: can we share this with the frame in STE?
-@Deprecated
-final class Lucene40IntersectTermsEnumFrame {
-  final int ord;
-  long fp;
-  long fpOrig;
-  long fpEnd;
-  long lastSubFP;
-
-  // State in automaton
-  int state;
-
-  int metaDataUpto;
-
-  byte[] suffixBytes = new byte[128];
-  final ByteArrayDataInput suffixesReader = new ByteArrayDataInput();
-
-  byte[] statBytes = new byte[64];
-  final ByteArrayDataInput statsReader = new ByteArrayDataInput();
-
-  byte[] floorData = new byte[32];
-  final ByteArrayDataInput floorDataReader = new ByteArrayDataInput();
-
-  // Length of prefix shared by all terms in this block
-  int prefix;
-
-  // Number of entries (term or sub-block) in this block
-  int entCount;
-
-  // Which term we will next read
-  int nextEnt;
-
-  // True if this block is either not a floor block,
-  // or, it's the last sub-block of a floor block
-  boolean isLastInFloor;
-
-  // True if all entries are terms
-  boolean isLeafBlock;
-
-  int numFollowFloorBlocks;
-  int nextFloorLabel;
-        
-  Transition transition = new Transition();
-  int curTransitionMax;
-  int transitionIndex;
-  int transitionCount;
-
-  FST.Arc<BytesRef> arc;
-
-  final BlockTermState termState;
-  
-  // metadata buffer, holding monotonic values
-  public long[] longs;
-  // metadata buffer, holding general values
-  public byte[] bytes;
-  ByteArrayDataInput bytesReader;
-
-  // Cumulative output so far
-  BytesRef outputPrefix;
-
-  int startBytePos;
-  int suffix;
-
-  private final Lucene40IntersectTermsEnum ite;
-
-  public Lucene40IntersectTermsEnumFrame(Lucene40IntersectTermsEnum ite, int ord) throws IOException {
-    this.ite = ite;
-    this.ord = ord;
-    this.termState = ite.fr.parent.postingsReader.newTermState();
-    this.termState.totalTermFreq = -1;
-    this.longs = new long[ite.fr.longsSize];
-  }
-
-  void loadNextFloorBlock() throws IOException {
-    assert numFollowFloorBlocks > 0;
-    //if (DEBUG) System.out.println("    loadNextFoorBlock trans=" + transitions[transitionIndex]);
-
-    do {
-      fp = fpOrig + (floorDataReader.readVLong() >>> 1);
-      numFollowFloorBlocks--;
-      // if (DEBUG) System.out.println("    skip floor block2!  nextFloorLabel=" + (char) nextFloorLabel + " vs target=" + (char) transitions[transitionIndex].getMin() + " newFP=" + fp + " numFollowFloorBlocks=" + numFollowFloorBlocks);
-      if (numFollowFloorBlocks != 0) {
-        nextFloorLabel = floorDataReader.readByte() & 0xff;
-      } else {
-        nextFloorLabel = 256;
-      }
-      // if (DEBUG) System.out.println("    nextFloorLabel=" + (char) nextFloorLabel);
-    } while (numFollowFloorBlocks != 0 && nextFloorLabel <= transition.min);
-
-    load(null);
-  }
-
-  public void setState(int state) {
-    this.state = state;
-    transitionIndex = 0;
-    transitionCount = ite.compiledAutomaton.automaton.getNumTransitions(state);
-    if (transitionCount != 0) {
-      ite.compiledAutomaton.automaton.initTransition(state, transition);
-      ite.compiledAutomaton.automaton.getNextTransition(transition);
-      curTransitionMax = transition.max;
-    } else {
-      curTransitionMax = -1;
-    }
-  }
-
-  void load(BytesRef frameIndexData) throws IOException {
-
-    // if (DEBUG) System.out.println("    load fp=" + fp + " fpOrig=" + fpOrig + " frameIndexData=" + frameIndexData + " trans=" + (transitions.length != 0 ? transitions[0] : "n/a" + " state=" + state));
-
-    if (frameIndexData != null && transitionCount != 0) {
-      // Floor frame
-      if (floorData.length < frameIndexData.length) {
-        this.floorData = new byte[ArrayUtil.oversize(frameIndexData.length, 1)];
-      }
-      System.arraycopy(frameIndexData.bytes, frameIndexData.offset, floorData, 0, frameIndexData.length);
-      floorDataReader.reset(floorData, 0, frameIndexData.length);
-      // Skip first long -- has redundant fp, hasTerms
-      // flag, isFloor flag
-      final long code = floorDataReader.readVLong();
-      if ((code & Lucene40BlockTreeTermsReader.OUTPUT_FLAG_IS_FLOOR) != 0) {
-        numFollowFloorBlocks = floorDataReader.readVInt();
-        nextFloorLabel = floorDataReader.readByte() & 0xff;
-        // if (DEBUG) System.out.println("    numFollowFloorBlocks=" + numFollowFloorBlocks + " nextFloorLabel=" + nextFloorLabel);
-
-        // If current state is accept, we must process
-        // first block in case it has empty suffix:
-        if (!ite.runAutomaton.isAccept(state)) {
-          // Maybe skip floor blocks:
-          assert transitionIndex == 0: "transitionIndex=" + transitionIndex;
-          while (numFollowFloorBlocks != 0 && nextFloorLabel <= transition.min) {
-            fp = fpOrig + (floorDataReader.readVLong() >>> 1);
-            numFollowFloorBlocks--;
-            // if (DEBUG) System.out.println("    skip floor block!  nextFloorLabel=" + (char) nextFloorLabel + " vs target=" + (char) transitions[0].getMin() + " newFP=" + fp + " numFollowFloorBlocks=" + numFollowFloorBlocks);
-            if (numFollowFloorBlocks != 0) {
-              nextFloorLabel = floorDataReader.readByte() & 0xff;
-            } else {
-              nextFloorLabel = 256;
-            }
-          }
-        }
-      }
-    }
-
-    ite.in.seek(fp);
-    int code = ite.in.readVInt();
-    entCount = code >>> 1;
-    assert entCount > 0;
-    isLastInFloor = (code & 1) != 0;
-
-    // term suffixes:
-    code = ite.in.readVInt();
-    isLeafBlock = (code & 1) != 0;
-    int numBytes = code >>> 1;
-    // if (DEBUG) System.out.println("      entCount=" + entCount + " lastInFloor?=" + isLastInFloor + " leafBlock?=" + isLeafBlock + " numSuffixBytes=" + numBytes);
-    if (suffixBytes.length < numBytes) {
-      suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];
-    }
-    ite.in.readBytes(suffixBytes, 0, numBytes);
-    suffixesReader.reset(suffixBytes, 0, numBytes);
-
-    // stats
-    numBytes = ite.in.readVInt();
-    if (statBytes.length < numBytes) {
-      statBytes = new byte[ArrayUtil.oversize(numBytes, 1)];
-    }
-    ite.in.readBytes(statBytes, 0, numBytes);
-    statsReader.reset(statBytes, 0, numBytes);
-    metaDataUpto = 0;
-
-    termState.termBlockOrd = 0;
-    nextEnt = 0;
-         
-    // metadata
-    numBytes = ite.in.readVInt();
-    if (bytes == null) {
-      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];
-      bytesReader = new ByteArrayDataInput();
-    } else if (bytes.length < numBytes) {
-      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];
-    }
-    ite.in.readBytes(bytes, 0, numBytes);
-    bytesReader.reset(bytes, 0, numBytes);
-
-    if (!isLastInFloor) {
-      // Sub-blocks of a single floor block are always
-      // written one after another -- tail recurse:
-      fpEnd = ite.in.getFilePointer();
-    }
-  }
-
-  // TODO: maybe add scanToLabel; should give perf boost
-
-  public boolean next() {
-    return isLeafBlock ? nextLeaf() : nextNonLeaf();
-  }
-
-  // Decodes next entry; returns true if it's a sub-block
-  public boolean nextLeaf() {
-    //if (DEBUG) System.out.println("  frame.next ord=" + ord + " nextEnt=" + nextEnt + " entCount=" + entCount);
-    assert nextEnt != -1 && nextEnt < entCount: "nextEnt=" + nextEnt + " entCount=" + entCount + " fp=" + fp;
-    nextEnt++;
-    suffix = suffixesReader.readVInt();
-    startBytePos = suffixesReader.getPosition();
-    suffixesReader.skipBytes(suffix);
-    return false;
-  }
-
-  public boolean nextNonLeaf() {
-    //if (DEBUG) System.out.println("  frame.next ord=" + ord + " nextEnt=" + nextEnt + " entCount=" + entCount);
-    assert nextEnt != -1 && nextEnt < entCount: "nextEnt=" + nextEnt + " entCount=" + entCount + " fp=" + fp;
-    nextEnt++;
-    final int code = suffixesReader.readVInt();
-    suffix = code >>> 1;
-    startBytePos = suffixesReader.getPosition();
-    suffixesReader.skipBytes(suffix);
-    if ((code & 1) == 0) {
-      // A normal term
-      termState.termBlockOrd++;
-      return false;
-    } else {
-      // A sub-block; make sub-FP absolute:
-      lastSubFP = fp - suffixesReader.readVLong();
-      return true;
-    }
-  }
-
-  public int getTermBlockOrd() {
-    return isLeafBlock ? nextEnt : termState.termBlockOrd;
-  }
-
-  public void decodeMetaData() throws IOException {
-
-    // lazily catch up on metadata decode:
-    final int limit = getTermBlockOrd();
-    boolean absolute = metaDataUpto == 0;
-    assert limit > 0;
-
-    // TODO: better API would be "jump straight to term=N"???
-    while (metaDataUpto < limit) {
-
-      // TODO: we could make "tiers" of metadata, ie,
-      // decode docFreq/totalTF but don't decode postings
-      // metadata; this way caller could get
-      // docFreq/totalTF w/o paying decode cost for
-      // postings
-
-      // TODO: if docFreq were bulk decoded we could
-      // just skipN here:
-
-      // stats
-      termState.docFreq = statsReader.readVInt();
-      //if (DEBUG) System.out.println("    dF=" + state.docFreq);
-      if (ite.fr.fieldInfo.getIndexOptions() != IndexOptions.DOCS_ONLY) {
-        termState.totalTermFreq = termState.docFreq + statsReader.readVLong();
-        //if (DEBUG) System.out.println("    totTF=" + state.totalTermFreq);
-      }
-      // metadata 
-      for (int i = 0; i < ite.fr.longsSize; i++) {
-        longs[i] = bytesReader.readVLong();
-      }
-      ite.fr.parent.postingsReader.decodeTerm(longs, bytesReader, ite.fr.fieldInfo, termState, absolute);
-
-      metaDataUpto++;
-      absolute = false;
-    }
-    termState.termBlockOrd = metaDataUpto;
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/Lucene40SegmentTermsEnum.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/Lucene40SegmentTermsEnum.java
deleted file mode 100644
index fb8f099..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/Lucene40SegmentTermsEnum.java
+++ /dev/null
@@ -1,1051 +0,0 @@
-package org.apache.lucene.codecs.blocktree;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.PrintStream;
-
-import org.apache.lucene.codecs.BlockTermState;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.TermState;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.store.ByteArrayDataInput;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.BytesRefBuilder;
-import org.apache.lucene.util.RamUsageEstimator;
-import org.apache.lucene.util.fst.FST;
-import org.apache.lucene.util.fst.Util;
-
-/**
- * Iterates through terms in this field
- * @deprecated Only for 4.x backcompat
- */
-@Deprecated
-final class Lucene40SegmentTermsEnum extends TermsEnum {
-
-  // Lazy init:
-  IndexInput in;
-
-  private Lucene40SegmentTermsEnumFrame[] stack;
-  private final Lucene40SegmentTermsEnumFrame staticFrame;
-  Lucene40SegmentTermsEnumFrame currentFrame;
-  boolean termExists;
-  final Lucene40FieldReader fr;
-
-  private int targetBeforeCurrentLength;
-
-  // static boolean DEBUG = false;
-
-  private final ByteArrayDataInput scratchReader = new ByteArrayDataInput();
-
-  // What prefix of the current term was present in the index; when we only next() through the index, this stays at 0.  It's only set when
-  // we seekCeil/Exact:
-  private int validIndexPrefix;
-
-  // assert only:
-  private boolean eof;
-
-  final BytesRefBuilder term = new BytesRefBuilder();
-  private final FST.BytesReader fstReader;
-
-  @SuppressWarnings({"rawtypes","unchecked"}) private FST.Arc<BytesRef>[] arcs = new FST.Arc[1];
-
-  public Lucene40SegmentTermsEnum(Lucene40FieldReader fr) throws IOException {
-    this.fr = fr;
-
-    // if (DEBUG) {
-    //   System.out.println("BTTR.init seg=" + fr.parent.segment);
-    // }
-    stack = new Lucene40SegmentTermsEnumFrame[0];
-        
-    // Used to hold seek by TermState, or cached seek
-    staticFrame = new Lucene40SegmentTermsEnumFrame(this, -1);
-
-    if (fr.index == null) {
-      fstReader = null;
-    } else {
-      fstReader = fr.index.getBytesReader();
-    }
-
-    // Init w/ root block; don't use index since it may
-    // not (and need not) have been loaded
-    for(int arcIdx=0;arcIdx<arcs.length;arcIdx++) {
-      arcs[arcIdx] = new FST.Arc<>();
-    }
-
-    currentFrame = staticFrame;
-    final FST.Arc<BytesRef> arc;
-    if (fr.index != null) {
-      arc = fr.index.getFirstArc(arcs[0]);
-      // Empty string prefix must have an output in the index!
-      assert arc.isFinal();
-    } else {
-      arc = null;
-    }
-    //currentFrame = pushFrame(arc, rootCode, 0);
-    //currentFrame.loadBlock();
-    validIndexPrefix = 0;
-    // if (DEBUG) {
-    //   System.out.println("init frame state " + currentFrame.ord);
-    //   printSeekState();
-    // }
-
-    //System.out.println();
-    // computeBlockStats().print(System.out);
-  }
-      
-  // Not private to avoid synthetic access$NNN methods
-  void initIndexInput() {
-    if (this.in == null) {
-      this.in = fr.parent.in.clone();
-    }
-  }
-
-  /** Runs next() through the entire terms dict,
-   *  computing aggregate statistics. */
-  public Lucene40Stats computeBlockStats() throws IOException {
-
-    Lucene40Stats stats = new Lucene40Stats(fr.parent.segment, fr.fieldInfo.name);
-    if (fr.index != null) {
-      stats.indexNodeCount = fr.index.getNodeCount();
-      stats.indexArcCount = fr.index.getArcCount();
-      stats.indexNumBytes = fr.index.ramBytesUsed();
-    }
-        
-    currentFrame = staticFrame;
-    FST.Arc<BytesRef> arc;
-    if (fr.index != null) {
-      arc = fr.index.getFirstArc(arcs[0]);
-      // Empty string prefix must have an output in the index!
-      assert arc.isFinal();
-    } else {
-      arc = null;
-    }
-
-    // Empty string prefix must have an output in the
-    // index!
-    currentFrame = pushFrame(arc, fr.rootCode, 0);
-    currentFrame.fpOrig = currentFrame.fp;
-    currentFrame.loadBlock();
-    validIndexPrefix = 0;
-
-    stats.startBlock(currentFrame, !currentFrame.isLastInFloor);
-
-    allTerms:
-    while (true) {
-
-      // Pop finished blocks
-      while (currentFrame.nextEnt == currentFrame.entCount) {
-        stats.endBlock(currentFrame);
-        if (!currentFrame.isLastInFloor) {
-          currentFrame.loadNextFloorBlock();
-          stats.startBlock(currentFrame, true);
-        } else {
-          if (currentFrame.ord == 0) {
-            break allTerms;
-          }
-          final long lastFP = currentFrame.fpOrig;
-          currentFrame = stack[currentFrame.ord-1];
-          assert lastFP == currentFrame.lastSubFP;
-          // if (DEBUG) {
-          //   System.out.println("  reset validIndexPrefix=" + validIndexPrefix);
-          // }
-        }
-      }
-
-      while(true) {
-        if (currentFrame.next()) {
-          // Push to new block:
-          currentFrame = pushFrame(null, currentFrame.lastSubFP, term.length());
-          currentFrame.fpOrig = currentFrame.fp;
-          // This is a "next" frame -- even if it's
-          // floor'd we must pretend it isn't so we don't
-          // try to scan to the right floor frame:
-          currentFrame.isFloor = false;
-          //currentFrame.hasTerms = true;
-          currentFrame.loadBlock();
-          stats.startBlock(currentFrame, !currentFrame.isLastInFloor);
-        } else {
-          stats.term(term.get());
-          break;
-        }
-      }
-    }
-
-    stats.finish();
-
-    // Put root frame back:
-    currentFrame = staticFrame;
-    if (fr.index != null) {
-      arc = fr.index.getFirstArc(arcs[0]);
-      // Empty string prefix must have an output in the index!
-      assert arc.isFinal();
-    } else {
-      arc = null;
-    }
-    currentFrame = pushFrame(arc, fr.rootCode, 0);
-    currentFrame.rewind();
-    currentFrame.loadBlock();
-    validIndexPrefix = 0;
-    term.clear();
-
-    return stats;
-  }
-
-  private Lucene40SegmentTermsEnumFrame getFrame(int ord) throws IOException {
-    if (ord >= stack.length) {
-      final Lucene40SegmentTermsEnumFrame[] next = new Lucene40SegmentTermsEnumFrame[ArrayUtil.oversize(1+ord, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
-      System.arraycopy(stack, 0, next, 0, stack.length);
-      for(int stackOrd=stack.length;stackOrd<next.length;stackOrd++) {
-        next[stackOrd] = new Lucene40SegmentTermsEnumFrame(this, stackOrd);
-      }
-      stack = next;
-    }
-    assert stack[ord].ord == ord;
-    return stack[ord];
-  }
-
-  private FST.Arc<BytesRef> getArc(int ord) {
-    if (ord >= arcs.length) {
-      @SuppressWarnings({"rawtypes","unchecked"}) final FST.Arc<BytesRef>[] next =
-      new FST.Arc[ArrayUtil.oversize(1+ord, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
-      System.arraycopy(arcs, 0, next, 0, arcs.length);
-      for(int arcOrd=arcs.length;arcOrd<next.length;arcOrd++) {
-        next[arcOrd] = new FST.Arc<>();
-      }
-      arcs = next;
-    }
-    return arcs[ord];
-  }
-
-  // Pushes a frame we seek'd to
-  Lucene40SegmentTermsEnumFrame pushFrame(FST.Arc<BytesRef> arc, BytesRef frameData, int length) throws IOException {
-    scratchReader.reset(frameData.bytes, frameData.offset, frameData.length);
-    final long code = scratchReader.readVLong();
-    final long fpSeek = code >>> Lucene40BlockTreeTermsReader.OUTPUT_FLAGS_NUM_BITS;
-    final Lucene40SegmentTermsEnumFrame f = getFrame(1+currentFrame.ord);
-    f.hasTerms = (code & Lucene40BlockTreeTermsReader.OUTPUT_FLAG_HAS_TERMS) != 0;
-    f.hasTermsOrig = f.hasTerms;
-    f.isFloor = (code & Lucene40BlockTreeTermsReader.OUTPUT_FLAG_IS_FLOOR) != 0;
-    if (f.isFloor) {
-      f.setFloorData(scratchReader, frameData);
-    }
-    pushFrame(arc, fpSeek, length);
-
-    return f;
-  }
-
-  // Pushes next'd frame or seek'd frame; we later
-  // lazy-load the frame only when needed
-  Lucene40SegmentTermsEnumFrame pushFrame(FST.Arc<BytesRef> arc, long fp, int length) throws IOException {
-    final Lucene40SegmentTermsEnumFrame f = getFrame(1+currentFrame.ord);
-    f.arc = arc;
-    if (f.fpOrig == fp && f.nextEnt != -1) {
-      //if (DEBUG) System.out.println("      push reused frame ord=" + f.ord + " fp=" + f.fp + " isFloor?=" + f.isFloor + " hasTerms=" + f.hasTerms + " pref=" + term + " nextEnt=" + f.nextEnt + " targetBeforeCurrentLength=" + targetBeforeCurrentLength + " term.length=" + term.length + " vs prefix=" + f.prefix);
-      //if (f.prefix > targetBeforeCurrentLength) {
-      if (f.ord > targetBeforeCurrentLength) {
-        f.rewind();
-      } else {
-        // if (DEBUG) {
-        //   System.out.println("        skip rewind!");
-        // }
-      }
-      assert length == f.prefix;
-    } else {
-      f.nextEnt = -1;
-      f.prefix = length;
-      f.state.termBlockOrd = 0;
-      f.fpOrig = f.fp = fp;
-      f.lastSubFP = -1;
-      // if (DEBUG) {
-      //   final int sav = term.length;
-      //   term.length = length;
-      //   System.out.println("      push new frame ord=" + f.ord + " fp=" + f.fp + " hasTerms=" + f.hasTerms + " isFloor=" + f.isFloor + " pref=" + brToString(term));
-      //   term.length = sav;
-      // }
-    }
-
-    return f;
-  }
-
-  // asserts only
-  private boolean clearEOF() {
-    eof = false;
-    return true;
-  }
-
-  // asserts only
-  private boolean setEOF() {
-    eof = true;
-    return true;
-  }
-
-  // for debugging
-  @SuppressWarnings("unused")
-  static String brToString(BytesRef b) {
-    try {
-      return b.utf8ToString() + " " + b;
-    } catch (Throwable t) {
-      // If BytesRef isn't actually UTF8, or it's eg a
-      // prefix of UTF8 that ends mid-unicode-char, we
-      // fallback to hex:
-      return b.toString();
-    }
-  }
-
-  @Override
-  public boolean seekExact(final BytesRef target) throws IOException {
-
-    if (fr.index == null) {
-      throw new IllegalStateException("terms index was not loaded");
-    }
-
-    term.grow(1 + target.length);
-
-    assert clearEOF();
-
-    // if (DEBUG) {
-    //   System.out.println("\nBTTR.seekExact seg=" + fr.parent.segment + " target=" + fr.fieldInfo.name + ":" + brToString(target) + " current=" + brToString(term) + " (exists?=" + termExists + ") validIndexPrefix=" + validIndexPrefix);
-    //   printSeekState(System.out);
-    // }
-
-    FST.Arc<BytesRef> arc;
-    int targetUpto;
-    BytesRef output;
-
-    targetBeforeCurrentLength = currentFrame.ord;
-
-    if (currentFrame != staticFrame) {
-
-      // We are already seek'd; find the common
-      // prefix of new seek term vs current term and
-      // re-use the corresponding seek state.  For
-      // example, if app first seeks to foobar, then
-      // seeks to foobaz, we can re-use the seek state
-      // for the first 5 bytes.
-
-      // if (DEBUG) {
-      //   System.out.println("  re-use current seek state validIndexPrefix=" + validIndexPrefix);
-      // }
-
-      arc = arcs[0];
-      assert arc.isFinal();
-      output = arc.output;
-      targetUpto = 0;
-          
-      Lucene40SegmentTermsEnumFrame lastFrame = stack[0];
-      assert validIndexPrefix <= term.length();
-
-      final int targetLimit = Math.min(target.length, validIndexPrefix);
-
-      int cmp = 0;
-
-      // TODO: reverse vLong byte order for better FST
-      // prefix output sharing
-
-      // First compare up to valid seek frames:
-      while (targetUpto < targetLimit) {
-        cmp = (term.byteAt(targetUpto)&0xFF) - (target.bytes[target.offset + targetUpto]&0xFF);
-        // if (DEBUG) {
-        //    System.out.println("    cycle targetUpto=" + targetUpto + " (vs limit=" + targetLimit + ") cmp=" + cmp + " (targetLabel=" + (char) (target.bytes[target.offset + targetUpto]) + " vs termLabel=" + (char) (term.bytes[targetUpto]) + ")"   + " arc.output=" + arc.output + " output=" + output);
-        // }
-        if (cmp != 0) {
-          break;
-        }
-        arc = arcs[1+targetUpto];
-        assert arc.label == (target.bytes[target.offset + targetUpto] & 0xFF): "arc.label=" + (char) arc.label + " targetLabel=" + (char) (target.bytes[target.offset + targetUpto] & 0xFF);
-        if (arc.output != Lucene40BlockTreeTermsReader.NO_OUTPUT) {
-          output = Lucene40BlockTreeTermsReader.FST_OUTPUTS.add(output, arc.output);
-        }
-        if (arc.isFinal()) {
-          lastFrame = stack[1+lastFrame.ord];
-        }
-        targetUpto++;
-      }
-
-      if (cmp == 0) {
-        final int targetUptoMid = targetUpto;
-
-        // Second compare the rest of the term, but
-        // don't save arc/output/frame; we only do this
-        // to find out if the target term is before,
-        // equal or after the current term
-        final int targetLimit2 = Math.min(target.length, term.length());
-        while (targetUpto < targetLimit2) {
-          cmp = (term.byteAt(targetUpto)&0xFF) - (target.bytes[target.offset + targetUpto]&0xFF);
-          // if (DEBUG) {
-          //    System.out.println("    cycle2 targetUpto=" + targetUpto + " (vs limit=" + targetLimit + ") cmp=" + cmp + " (targetLabel=" + (char) (target.bytes[target.offset + targetUpto]) + " vs termLabel=" + (char) (term.bytes[targetUpto]) + ")");
-          // }
-          if (cmp != 0) {
-            break;
-          }
-          targetUpto++;
-        }
-
-        if (cmp == 0) {
-          cmp = term.length() - target.length;
-        }
-        targetUpto = targetUptoMid;
-      }
-
-      if (cmp < 0) {
-        // Common case: target term is after current
-        // term, ie, app is seeking multiple terms
-        // in sorted order
-        // if (DEBUG) {
-        //   System.out.println("  target is after current (shares prefixLen=" + targetUpto + "); frame.ord=" + lastFrame.ord);
-        // }
-        currentFrame = lastFrame;
-
-      } else if (cmp > 0) {
-        // Uncommon case: target term
-        // is before current term; this means we can
-        // keep the currentFrame but we must rewind it
-        // (so we scan from the start)
-        targetBeforeCurrentLength = lastFrame.ord;
-        // if (DEBUG) {
-        //   System.out.println("  target is before current (shares prefixLen=" + targetUpto + "); rewind frame ord=" + lastFrame.ord);
-        // }
-        currentFrame = lastFrame;
-        currentFrame.rewind();
-      } else {
-        // Target is exactly the same as current term
-        assert term.length() == target.length;
-        if (termExists) {
-          // if (DEBUG) {
-          //   System.out.println("  target is same as current; return true");
-          // }
-          return true;
-        } else {
-          // if (DEBUG) {
-          //   System.out.println("  target is same as current but term doesn't exist");
-          // }
-        }
-        //validIndexPrefix = currentFrame.depth;
-        //term.length = target.length;
-        //return termExists;
-      }
-
-    } else {
-
-      targetBeforeCurrentLength = -1;
-      arc = fr.index.getFirstArc(arcs[0]);
-
-      // Empty string prefix must have an output (block) in the index!
-      assert arc.isFinal();
-      assert arc.output != null;
-
-      // if (DEBUG) {
-      //   System.out.println("    no seek state; push root frame");
-      // }
-
-      output = arc.output;
-
-      currentFrame = staticFrame;
-
-      //term.length = 0;
-      targetUpto = 0;
-      currentFrame = pushFrame(arc, Lucene40BlockTreeTermsReader.FST_OUTPUTS.add(output, arc.nextFinalOutput), 0);
-    }
-
-    // if (DEBUG) {
-    //   System.out.println("  start index loop targetUpto=" + targetUpto + " output=" + output + " currentFrame.ord=" + currentFrame.ord + " targetBeforeCurrentLength=" + targetBeforeCurrentLength);
-    // }
-
-    // We are done sharing the common prefix with the incoming target and where we are currently seek'd; now continue walking the index:
-    while (targetUpto < target.length) {
-
-      final int targetLabel = target.bytes[target.offset + targetUpto] & 0xFF;
-
-      final FST.Arc<BytesRef> nextArc = fr.index.findTargetArc(targetLabel, arc, getArc(1+targetUpto), fstReader);
-
-      if (nextArc == null) {
-
-        // Index is exhausted
-        // if (DEBUG) {
-        //   System.out.println("    index: index exhausted label=" + ((char) targetLabel) + " " + toHex(targetLabel));
-        // }
-            
-        validIndexPrefix = currentFrame.prefix;
-        //validIndexPrefix = targetUpto;
-
-        currentFrame.scanToFloorFrame(target);
-
-        if (!currentFrame.hasTerms) {
-          termExists = false;
-          term.setByteAt(targetUpto, (byte) targetLabel);
-          term.setLength(1+targetUpto);
-          // if (DEBUG) {
-          //   System.out.println("  FAST NOT_FOUND term=" + brToString(term));
-          // }
-          return false;
-        }
-
-        currentFrame.loadBlock();
-
-        final SeekStatus result = currentFrame.scanToTerm(target, true);            
-        if (result == SeekStatus.FOUND) {
-          // if (DEBUG) {
-          //   System.out.println("  return FOUND term=" + term.utf8ToString() + " " + term);
-          // }
-          return true;
-        } else {
-          // if (DEBUG) {
-          //   System.out.println("  got " + result + "; return NOT_FOUND term=" + brToString(term));
-          // }
-          return false;
-        }
-      } else {
-        // Follow this arc
-        arc = nextArc;
-        term.setByteAt(targetUpto, (byte) targetLabel);
-        // Aggregate output as we go:
-        assert arc.output != null;
-        if (arc.output != Lucene40BlockTreeTermsReader.NO_OUTPUT) {
-          output = Lucene40BlockTreeTermsReader.FST_OUTPUTS.add(output, arc.output);
-        }
-
-        // if (DEBUG) {
-        //   System.out.println("    index: follow label=" + toHex(target.bytes[target.offset + targetUpto]&0xff) + " arc.output=" + arc.output + " arc.nfo=" + arc.nextFinalOutput);
-        // }
-        targetUpto++;
-
-        if (arc.isFinal()) {
-          //if (DEBUG) System.out.println("    arc is final!");
-          currentFrame = pushFrame(arc, Lucene40BlockTreeTermsReader.FST_OUTPUTS.add(output, arc.nextFinalOutput), targetUpto);
-          //if (DEBUG) System.out.println("    curFrame.ord=" + currentFrame.ord + " hasTerms=" + currentFrame.hasTerms);
-        }
-      }
-    }
-
-    //validIndexPrefix = targetUpto;
-    validIndexPrefix = currentFrame.prefix;
-
-    currentFrame.scanToFloorFrame(target);
-
-    // Target term is entirely contained in the index:
-    if (!currentFrame.hasTerms) {
-      termExists = false;
-      term.setLength(targetUpto);
-      // if (DEBUG) {
-      //   System.out.println("  FAST NOT_FOUND term=" + brToString(term));
-      // }
-      return false;
-    }
-
-    currentFrame.loadBlock();
-
-    final SeekStatus result = currentFrame.scanToTerm(target, true);            
-    if (result == SeekStatus.FOUND) {
-      // if (DEBUG) {
-      //   System.out.println("  return FOUND term=" + term.utf8ToString() + " " + term);
-      // }
-      return true;
-    } else {
-      // if (DEBUG) {
-      //   System.out.println("  got result " + result + "; return NOT_FOUND term=" + term.utf8ToString());
-      // }
-
-      return false;
-    }
-  }
-
-  @Override
-  public SeekStatus seekCeil(final BytesRef target) throws IOException {
-    if (fr.index == null) {
-      throw new IllegalStateException("terms index was not loaded");
-    }
-
-    term.grow(1 + target.length);
-
-    assert clearEOF();
-
-    // if (DEBUG) {
-    //   System.out.println("\nBTTR.seekCeil seg=" + fr.parent.segment + " target=" + fr.fieldInfo.name + ":" + target.utf8ToString() + " " + target + " current=" + brToString(term) + " (exists?=" + termExists + ") validIndexPrefix=  " + validIndexPrefix);
-    //   printSeekState(System.out);
-    // }
-
-    FST.Arc<BytesRef> arc;
-    int targetUpto;
-    BytesRef output;
-
-    targetBeforeCurrentLength = currentFrame.ord;
-
-    if (currentFrame != staticFrame) {
-
-      // We are already seek'd; find the common
-      // prefix of new seek term vs current term and
-      // re-use the corresponding seek state.  For
-      // example, if app first seeks to foobar, then
-      // seeks to foobaz, we can re-use the seek state
-      // for the first 5 bytes.
-
-      //if (DEBUG) {
-      //System.out.println("  re-use current seek state validIndexPrefix=" + validIndexPrefix);
-      //}
-
-      arc = arcs[0];
-      assert arc.isFinal();
-      output = arc.output;
-      targetUpto = 0;
-          
-      Lucene40SegmentTermsEnumFrame lastFrame = stack[0];
-      assert validIndexPrefix <= term.length();
-
-      final int targetLimit = Math.min(target.length, validIndexPrefix);
-
-      int cmp = 0;
-
-      // TODO: we should write our vLong backwards (MSB
-      // first) to get better sharing from the FST
-
-      // First compare up to valid seek frames:
-      while (targetUpto < targetLimit) {
-        cmp = (term.byteAt(targetUpto)&0xFF) - (target.bytes[target.offset + targetUpto]&0xFF);
-        //if (DEBUG) {
-        //System.out.println("    cycle targetUpto=" + targetUpto + " (vs limit=" + targetLimit + ") cmp=" + cmp + " (targetLabel=" + (char) (target.bytes[target.offset + targetUpto]) + " vs termLabel=" + (char) (term.bytes[targetUpto]) + ")"   + " arc.output=" + arc.output + " output=" + output);
-        //}
-        if (cmp != 0) {
-          break;
-        }
-        arc = arcs[1+targetUpto];
-        assert arc.label == (target.bytes[target.offset + targetUpto] & 0xFF): "arc.label=" + (char) arc.label + " targetLabel=" + (char) (target.bytes[target.offset + targetUpto] & 0xFF);
-        // TODO: we could save the outputs in local
-        // byte[][] instead of making new objs ever
-        // seek; but, often the FST doesn't have any
-        // shared bytes (but this could change if we
-        // reverse vLong byte order)
-        if (arc.output != Lucene40BlockTreeTermsReader.NO_OUTPUT) {
-          output = Lucene40BlockTreeTermsReader.FST_OUTPUTS.add(output, arc.output);
-        }
-        if (arc.isFinal()) {
-          lastFrame = stack[1+lastFrame.ord];
-        }
-        targetUpto++;
-      }
-
-
-      if (cmp == 0) {
-        final int targetUptoMid = targetUpto;
-        // Second compare the rest of the term, but
-        // don't save arc/output/frame:
-        final int targetLimit2 = Math.min(target.length, term.length());
-        while (targetUpto < targetLimit2) {
-          cmp = (term.byteAt(targetUpto)&0xFF) - (target.bytes[target.offset + targetUpto]&0xFF);
-          //if (DEBUG) {
-          //System.out.println("    cycle2 targetUpto=" + targetUpto + " (vs limit=" + targetLimit + ") cmp=" + cmp + " (targetLabel=" + (char) (target.bytes[target.offset + targetUpto]) + " vs termLabel=" + (char) (term.bytes[targetUpto]) + ")");
-          //}
-          if (cmp != 0) {
-            break;
-          }
-          targetUpto++;
-        }
-
-        if (cmp == 0) {
-          cmp = term.length() - target.length;
-        }
-        targetUpto = targetUptoMid;
-      }
-
-      if (cmp < 0) {
-        // Common case: target term is after current
-        // term, ie, app is seeking multiple terms
-        // in sorted order
-        //if (DEBUG) {
-        //System.out.println("  target is after current (shares prefixLen=" + targetUpto + "); clear frame.scanned ord=" + lastFrame.ord);
-        //}
-        currentFrame = lastFrame;
-
-      } else if (cmp > 0) {
-        // Uncommon case: target term
-        // is before current term; this means we can
-        // keep the currentFrame but we must rewind it
-        // (so we scan from the start)
-        targetBeforeCurrentLength = 0;
-        //if (DEBUG) {
-        //System.out.println("  target is before current (shares prefixLen=" + targetUpto + "); rewind frame ord=" + lastFrame.ord);
-        //}
-        currentFrame = lastFrame;
-        currentFrame.rewind();
-      } else {
-        // Target is exactly the same as current term
-        assert term.length() == target.length;
-        if (termExists) {
-          //if (DEBUG) {
-          //System.out.println("  target is same as current; return FOUND");
-          //}
-          return SeekStatus.FOUND;
-        } else {
-          //if (DEBUG) {
-          //System.out.println("  target is same as current but term doesn't exist");
-          //}
-        }
-      }
-
-    } else {
-
-      targetBeforeCurrentLength = -1;
-      arc = fr.index.getFirstArc(arcs[0]);
-
-      // Empty string prefix must have an output (block) in the index!
-      assert arc.isFinal();
-      assert arc.output != null;
-
-      //if (DEBUG) {
-      //System.out.println("    no seek state; push root frame");
-      //}
-
-      output = arc.output;
-
-      currentFrame = staticFrame;
-
-      //term.length = 0;
-      targetUpto = 0;
-      currentFrame = pushFrame(arc, Lucene40BlockTreeTermsReader.FST_OUTPUTS.add(output, arc.nextFinalOutput), 0);
-    }
-
-    //if (DEBUG) {
-    //System.out.println("  start index loop targetUpto=" + targetUpto + " output=" + output + " currentFrame.ord+1=" + currentFrame.ord + " targetBeforeCurrentLength=" + targetBeforeCurrentLength);
-    //}
-
-    // We are done sharing the common prefix with the incoming target and where we are currently seek'd; now continue walking the index:
-    while (targetUpto < target.length) {
-
-      final int targetLabel = target.bytes[target.offset + targetUpto] & 0xFF;
-
-      final FST.Arc<BytesRef> nextArc = fr.index.findTargetArc(targetLabel, arc, getArc(1+targetUpto), fstReader);
-
-      if (nextArc == null) {
-
-        // Index is exhausted
-        // if (DEBUG) {
-        //   System.out.println("    index: index exhausted label=" + ((char) targetLabel) + " " + toHex(targetLabel));
-        // }
-            
-        validIndexPrefix = currentFrame.prefix;
-        //validIndexPrefix = targetUpto;
-
-        currentFrame.scanToFloorFrame(target);
-
-        currentFrame.loadBlock();
-
-        final SeekStatus result = currentFrame.scanToTerm(target, false);
-        if (result == SeekStatus.END) {
-          term.copyBytes(target);
-          termExists = false;
-
-          if (next() != null) {
-            //if (DEBUG) {
-            //System.out.println("  return NOT_FOUND term=" + brToString(term) + " " + term);
-            //}
-            return SeekStatus.NOT_FOUND;
-          } else {
-            //if (DEBUG) {
-            //System.out.println("  return END");
-            //}
-            return SeekStatus.END;
-          }
-        } else {
-          //if (DEBUG) {
-          //System.out.println("  return " + result + " term=" + brToString(term) + " " + term);
-          //}
-          return result;
-        }
-      } else {
-        // Follow this arc
-        term.setByteAt(targetUpto, (byte) targetLabel);
-        arc = nextArc;
-        // Aggregate output as we go:
-        assert arc.output != null;
-        if (arc.output != Lucene40BlockTreeTermsReader.NO_OUTPUT) {
-          output = Lucene40BlockTreeTermsReader.FST_OUTPUTS.add(output, arc.output);
-        }
-
-        //if (DEBUG) {
-        //System.out.println("    index: follow label=" + toHex(target.bytes[target.offset + targetUpto]&0xff) + " arc.output=" + arc.output + " arc.nfo=" + arc.nextFinalOutput);
-        //}
-        targetUpto++;
-
-        if (arc.isFinal()) {
-          //if (DEBUG) System.out.println("    arc is final!");
-          currentFrame = pushFrame(arc, Lucene40BlockTreeTermsReader.FST_OUTPUTS.add(output, arc.nextFinalOutput), targetUpto);
-          //if (DEBUG) System.out.println("    curFrame.ord=" + currentFrame.ord + " hasTerms=" + currentFrame.hasTerms);
-        }
-      }
-    }
-
-    //validIndexPrefix = targetUpto;
-    validIndexPrefix = currentFrame.prefix;
-
-    currentFrame.scanToFloorFrame(target);
-
-    currentFrame.loadBlock();
-
-    final SeekStatus result = currentFrame.scanToTerm(target, false);
-
-    if (result == SeekStatus.END) {
-      term.copyBytes(target);
-      termExists = false;
-      if (next() != null) {
-        //if (DEBUG) {
-        //System.out.println("  return NOT_FOUND term=" + term.utf8ToString() + " " + term);
-        //}
-        return SeekStatus.NOT_FOUND;
-      } else {
-        //if (DEBUG) {
-        //System.out.println("  return END");
-        //}
-        return SeekStatus.END;
-      }
-    } else {
-      return result;
-    }
-  }
-
-  @SuppressWarnings("unused")
-  private void printSeekState(PrintStream out) throws IOException {
-    if (currentFrame == staticFrame) {
-      out.println("  no prior seek");
-    } else {
-      out.println("  prior seek state:");
-      int ord = 0;
-      boolean isSeekFrame = true;
-      while(true) {
-        Lucene40SegmentTermsEnumFrame f = getFrame(ord);
-        assert f != null;
-        final BytesRef prefix = new BytesRef(term.get().bytes, 0, f.prefix);
-        if (f.nextEnt == -1) {
-          out.println("    frame " + (isSeekFrame ? "(seek)" : "(next)") + " ord=" + ord + " fp=" + f.fp + (f.isFloor ? (" (fpOrig=" + f.fpOrig + ")") : "") + " prefixLen=" + f.prefix + " prefix=" + prefix + (f.nextEnt == -1 ? "" : (" (of " + f.entCount + ")")) + " hasTerms=" + f.hasTerms + " isFloor=" + f.isFloor + " code=" + ((f.fp<< Lucene40BlockTreeTermsReader.OUTPUT_FLAGS_NUM_BITS) + (f.hasTerms ? Lucene40BlockTreeTermsReader.OUTPUT_FLAG_HAS_TERMS:0) + (f.isFloor ? Lucene40BlockTreeTermsReader.OUTPUT_FLAG_IS_FLOOR:0)) + " isLastInFloor=" + f.isLastInFloor + " mdUpto=" + f.metaDataUpto + " tbOrd=" + f.getTermBlockOrd());
-        } else {
-          out.println("    frame " + (isSeekFrame ? "(seek, loaded)" : "(next, loaded)") + " ord=" + ord + " fp=" + f.fp + (f.isFloor ? (" (fpOrig=" + f.fpOrig + ")") : "") + " prefixLen=" + f.prefix + " prefix=" + prefix + " nextEnt=" + f.nextEnt + (f.nextEnt == -1 ? "" : (" (of " + f.entCount + ")")) + " hasTerms=" + f.hasTerms + " isFloor=" + f.isFloor + " code=" + ((f.fp<< Lucene40BlockTreeTermsReader.OUTPUT_FLAGS_NUM_BITS) + (f.hasTerms ? Lucene40BlockTreeTermsReader.OUTPUT_FLAG_HAS_TERMS:0) + (f.isFloor ? Lucene40BlockTreeTermsReader.OUTPUT_FLAG_IS_FLOOR:0)) + " lastSubFP=" + f.lastSubFP + " isLastInFloor=" + f.isLastInFloor + " mdUpto=" + f.metaDataUpto + " tbOrd=" + f.getTermBlockOrd());
-        }
-        if (fr.index != null) {
-          assert !isSeekFrame || f.arc != null: "isSeekFrame=" + isSeekFrame + " f.arc=" + f.arc;
-          if (f.prefix > 0 && isSeekFrame && f.arc.label != (term.byteAt(f.prefix-1)&0xFF)) {
-            out.println("      broken seek state: arc.label=" + (char) f.arc.label + " vs term byte=" + (char) (term.byteAt(f.prefix-1)&0xFF));
-            throw new RuntimeException("seek state is broken");
-          }
-          BytesRef output = Util.get(fr.index, prefix);
-          if (output == null) {
-            out.println("      broken seek state: prefix is not final in index");
-            throw new RuntimeException("seek state is broken");
-          } else if (isSeekFrame && !f.isFloor) {
-            final ByteArrayDataInput reader = new ByteArrayDataInput(output.bytes, output.offset, output.length);
-            final long codeOrig = reader.readVLong();
-            final long code = (f.fp << Lucene40BlockTreeTermsReader.OUTPUT_FLAGS_NUM_BITS) | (f.hasTerms ? Lucene40BlockTreeTermsReader.OUTPUT_FLAG_HAS_TERMS:0) | (f.isFloor ? Lucene40BlockTreeTermsReader.OUTPUT_FLAG_IS_FLOOR:0);
-            if (codeOrig != code) {
-              out.println("      broken seek state: output code=" + codeOrig + " doesn't match frame code=" + code);
-              throw new RuntimeException("seek state is broken");
-            }
-          }
-        }
-        if (f == currentFrame) {
-          break;
-        }
-        if (f.prefix == validIndexPrefix) {
-          isSeekFrame = false;
-        }
-        ord++;
-      }
-    }
-  }
-
-  /* Decodes only the term bytes of the next term.  If caller then asks for
-     metadata, ie docFreq, totalTermFreq or pulls a D/&PEnum, we then (lazily)
-     decode all metadata up to the current term. */
-  @Override
-  public BytesRef next() throws IOException {
-    if (in == null) {
-      // Fresh TermsEnum; seek to first term:
-      final FST.Arc<BytesRef> arc;
-      if (fr.index != null) {
-        arc = fr.index.getFirstArc(arcs[0]);
-        // Empty string prefix must have an output in the index!
-        assert arc.isFinal();
-      } else {
-        arc = null;
-      }
-      currentFrame = pushFrame(arc, fr.rootCode, 0);
-      currentFrame.loadBlock();
-    }
-
-    targetBeforeCurrentLength = currentFrame.ord;
-
-    assert !eof;
-    // if (DEBUG) {
-    //   System.out.println("\nBTTR.next seg=" + fr.parent.segment + " term=" + brToString(term) + " termExists?=" + termExists + " field=" + fr.fieldInfo.name + " termBlockOrd=" + currentFrame.state.termBlockOrd + " validIndexPrefix=" + validIndexPrefix);
-    //   printSeekState(System.out);
-    // }
-
-    if (currentFrame == staticFrame) {
-      // If seek was previously called and the term was
-      // cached, or seek(TermState) was called, usually
-      // caller is just going to pull a D/&PEnum or get
-      // docFreq, etc.  But, if they then call next(),
-      // this method catches up all internal state so next()
-      // works properly:
-      //if (DEBUG) System.out.println("  re-seek to pending term=" + term.utf8ToString() + " " + term);
-      final boolean result = seekExact(term.get());
-      assert result;
-    }
-
-    // Pop finished blocks
-    while (currentFrame.nextEnt == currentFrame.entCount) {
-      if (!currentFrame.isLastInFloor) {
-        currentFrame.loadNextFloorBlock();
-      } else {
-        //if (DEBUG) System.out.println("  pop frame");
-        if (currentFrame.ord == 0) {
-          //if (DEBUG) System.out.println("  return null");
-          assert setEOF();
-          term.clear();
-          validIndexPrefix = 0;
-          currentFrame.rewind();
-          termExists = false;
-          return null;
-        }
-        final long lastFP = currentFrame.fpOrig;
-        currentFrame = stack[currentFrame.ord-1];
-
-        if (currentFrame.nextEnt == -1 || currentFrame.lastSubFP != lastFP) {
-          // We popped into a frame that's not loaded
-          // yet or not scan'd to the right entry
-          currentFrame.scanToFloorFrame(term.get());
-          currentFrame.loadBlock();
-          currentFrame.scanToSubBlock(lastFP);
-        }
-
-        // Note that the seek state (last seek) has been
-        // invalidated beyond this depth
-        validIndexPrefix = Math.min(validIndexPrefix, currentFrame.prefix);
-        //if (DEBUG) {
-        //System.out.println("  reset validIndexPrefix=" + validIndexPrefix);
-        //}
-      }
-    }
-
-    while(true) {
-      if (currentFrame.next()) {
-        // Push to new block:
-        //if (DEBUG) System.out.println("  push frame");
-        currentFrame = pushFrame(null, currentFrame.lastSubFP, term.length());
-        // This is a "next" frame -- even if it's
-        // floor'd we must pretend it isn't so we don't
-        // try to scan to the right floor frame:
-        currentFrame.isFloor = false;
-        //currentFrame.hasTerms = true;
-        currentFrame.loadBlock();
-      } else {
-        //if (DEBUG) System.out.println("  return term=" + term.utf8ToString() + " " + term + " currentFrame.ord=" + currentFrame.ord);
-        return term.get();
-      }
-    }
-  }
-
-  @Override
-  public BytesRef term() {
-    assert !eof;
-    return term.get();
-  }
-
-  @Override
-  public int docFreq() throws IOException {
-    assert !eof;
-    //if (DEBUG) System.out.println("BTR.docFreq");
-    currentFrame.decodeMetaData();
-    //if (DEBUG) System.out.println("  return " + currentFrame.state.docFreq);
-    return currentFrame.state.docFreq;
-  }
-
-  @Override
-  public long totalTermFreq() throws IOException {
-    assert !eof;
-    currentFrame.decodeMetaData();
-    return currentFrame.state.totalTermFreq;
-  }
-
-  @Override
-  public DocsEnum docs(Bits skipDocs, DocsEnum reuse, int flags) throws IOException {
-    assert !eof;
-    //if (DEBUG) {
-    //System.out.println("BTTR.docs seg=" + segment);
-    //}
-    currentFrame.decodeMetaData();
-    //if (DEBUG) {
-    //System.out.println("  state=" + currentFrame.state);
-    //}
-    return fr.parent.postingsReader.docs(fr.fieldInfo, currentFrame.state, skipDocs, reuse, flags);
-  }
-
-  @Override
-  public DocsAndPositionsEnum docsAndPositions(Bits skipDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
-    if (fr.fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) < 0) {
-      // Positions were not indexed:
-      return null;
-    }
-
-    assert !eof;
-    currentFrame.decodeMetaData();
-    return fr.parent.postingsReader.docsAndPositions(fr.fieldInfo, currentFrame.state, skipDocs, reuse, flags);
-  }
-
-  @Override
-  public void seekExact(BytesRef target, TermState otherState) {
-    // if (DEBUG) {
-    //   System.out.println("BTTR.seekExact termState seg=" + segment + " target=" + target.utf8ToString() + " " + target + " state=" + otherState);
-    // }
-    assert clearEOF();
-    if (target.compareTo(term.get()) != 0 || !termExists) {
-      assert otherState != null && otherState instanceof BlockTermState;
-      currentFrame = staticFrame;
-      currentFrame.state.copyFrom(otherState);
-      term.copyBytes(target);
-      currentFrame.metaDataUpto = currentFrame.getTermBlockOrd();
-      assert currentFrame.metaDataUpto > 0;
-      validIndexPrefix = 0;
-    } else {
-      // if (DEBUG) {
-      //   System.out.println("  skip seek: already on target state=" + currentFrame.state);
-      // }
-    }
-  }
-      
-  @Override
-  public TermState termState() throws IOException {
-    assert !eof;
-    currentFrame.decodeMetaData();
-    TermState ts = currentFrame.state.clone();
-    //if (DEBUG) System.out.println("BTTR.termState seg=" + segment + " state=" + ts);
-    return ts;
-  }
-
-  @Override
-  public void seekExact(long ord) {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  public long ord() {
-    throw new UnsupportedOperationException();
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/Lucene40SegmentTermsEnumFrame.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/Lucene40SegmentTermsEnumFrame.java
deleted file mode 100644
index 1cb3527..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/Lucene40SegmentTermsEnumFrame.java
+++ /dev/null
@@ -1,732 +0,0 @@
-package org.apache.lucene.codecs.blocktree;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.BlockTermState;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.TermsEnum.SeekStatus;
-import org.apache.lucene.store.ByteArrayDataInput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.fst.FST;
-
-/**
- * @deprecated Only for 4.x backcompat
- */
-@Deprecated
-final class Lucene40SegmentTermsEnumFrame {
-  // Our index in stack[]:
-  final int ord;
-
-  boolean hasTerms;
-  boolean hasTermsOrig;
-  boolean isFloor;
-
-  FST.Arc<BytesRef> arc;
-
-  // File pointer where this block was loaded from
-  long fp;
-  long fpOrig;
-  long fpEnd;
-
-  byte[] suffixBytes = new byte[128];
-  final ByteArrayDataInput suffixesReader = new ByteArrayDataInput();
-
-  byte[] statBytes = new byte[64];
-  final ByteArrayDataInput statsReader = new ByteArrayDataInput();
-
-  byte[] floorData = new byte[32];
-  final ByteArrayDataInput floorDataReader = new ByteArrayDataInput();
-
-  // Length of prefix shared by all terms in this block
-  int prefix;
-
-  // Number of entries (term or sub-block) in this block
-  int entCount;
-
-  // Which term we will next read, or -1 if the block
-  // isn't loaded yet
-  int nextEnt;
-
-  // True if this block is either not a floor block,
-  // or, it's the last sub-block of a floor block
-  boolean isLastInFloor;
-
-  // True if all entries are terms
-  boolean isLeafBlock;
-
-  long lastSubFP;
-
-  int nextFloorLabel;
-  int numFollowFloorBlocks;
-
-  // Next term to decode metaData; we decode metaData
-  // lazily so that scanning to find the matching term is
-  // fast and only if you find a match and app wants the
-  // stats or docs/positions enums, will we decode the
-  // metaData
-  int metaDataUpto;
-
-  final BlockTermState state;
-
-  // metadata buffer, holding monotonic values
-  public long[] longs;
-  // metadata buffer, holding general values
-  public byte[] bytes;
-  ByteArrayDataInput bytesReader;
-
-  private final Lucene40SegmentTermsEnum ste;
-
-  public Lucene40SegmentTermsEnumFrame(Lucene40SegmentTermsEnum ste, int ord) throws IOException {
-    this.ste = ste;
-    this.ord = ord;
-    this.state = ste.fr.parent.postingsReader.newTermState();
-    this.state.totalTermFreq = -1;
-    this.longs = new long[ste.fr.longsSize];
-  }
-
-  public void setFloorData(ByteArrayDataInput in, BytesRef source) {
-    final int numBytes = source.length - (in.getPosition() - source.offset);
-    if (numBytes > floorData.length) {
-      floorData = new byte[ArrayUtil.oversize(numBytes, 1)];
-    }
-    System.arraycopy(source.bytes, source.offset+in.getPosition(), floorData, 0, numBytes);
-    floorDataReader.reset(floorData, 0, numBytes);
-    numFollowFloorBlocks = floorDataReader.readVInt();
-    nextFloorLabel = floorDataReader.readByte() & 0xff;
-    //if (DEBUG) {
-    //System.out.println("    setFloorData fpOrig=" + fpOrig + " bytes=" + new BytesRef(source.bytes, source.offset + in.getPosition(), numBytes) + " numFollowFloorBlocks=" + numFollowFloorBlocks + " nextFloorLabel=" + toHex(nextFloorLabel));
-    //}
-  }
-
-  public int getTermBlockOrd() {
-    return isLeafBlock ? nextEnt : state.termBlockOrd;
-  }
-
-  void loadNextFloorBlock() throws IOException {
-    //if (DEBUG) {
-    //System.out.println("    loadNextFloorBlock fp=" + fp + " fpEnd=" + fpEnd);
-    //}
-    assert arc == null || isFloor: "arc=" + arc + " isFloor=" + isFloor;
-    fp = fpEnd;
-    nextEnt = -1;
-    loadBlock();
-  }
-
-  /* Does initial decode of next block of terms; this
-     doesn't actually decode the docFreq, totalTermFreq,
-     postings details (frq/prx offset, etc.) metadata;
-     it just loads them as byte[] blobs which are then      
-     decoded on-demand if the metadata is ever requested
-     for any term in this block.  This enables terms-only
-     intensive consumes (eg certain MTQs, respelling) to
-     not pay the price of decoding metadata they won't
-     use. */
-  void loadBlock() throws IOException {
-
-    // Clone the IndexInput lazily, so that consumers
-    // that just pull a TermsEnum to
-    // seekExact(TermState) don't pay this cost:
-    ste.initIndexInput();
-
-    if (nextEnt != -1) {
-      // Already loaded
-      return;
-    }
-    //System.out.println("blc=" + blockLoadCount);
-
-    ste.in.seek(fp);
-    int code = ste.in.readVInt();
-    entCount = code >>> 1;
-    assert entCount > 0;
-    isLastInFloor = (code & 1) != 0;
-
-    assert arc == null || (isLastInFloor || isFloor): "fp=" + fp + " arc=" + arc + " isFloor=" + isFloor + " isLastInFloor=" + isLastInFloor;
-
-    // TODO: if suffixes were stored in random-access
-    // array structure, then we could do binary search
-    // instead of linear scan to find target term; eg
-    // we could have simple array of offsets
-
-    // term suffixes:
-    code = ste.in.readVInt();
-    isLeafBlock = (code & 1) != 0;
-    int numBytes = code >>> 1;
-    if (suffixBytes.length < numBytes) {
-      suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];
-    }
-    ste.in.readBytes(suffixBytes, 0, numBytes);
-    suffixesReader.reset(suffixBytes, 0, numBytes);
-
-    /*if (DEBUG) {
-      if (arc == null) {
-      System.out.println("    loadBlock (next) fp=" + fp + " entCount=" + entCount + " prefixLen=" + prefix + " isLastInFloor=" + isLastInFloor + " leaf?=" + isLeafBlock);
-      } else {
-      System.out.println("    loadBlock (seek) fp=" + fp + " entCount=" + entCount + " prefixLen=" + prefix + " hasTerms?=" + hasTerms + " isFloor?=" + isFloor + " isLastInFloor=" + isLastInFloor + " leaf?=" + isLeafBlock);
-      }
-      }*/
-
-    // stats
-    numBytes = ste.in.readVInt();
-    if (statBytes.length < numBytes) {
-      statBytes = new byte[ArrayUtil.oversize(numBytes, 1)];
-    }
-    ste.in.readBytes(statBytes, 0, numBytes);
-    statsReader.reset(statBytes, 0, numBytes);
-    metaDataUpto = 0;
-
-    state.termBlockOrd = 0;
-    nextEnt = 0;
-    lastSubFP = -1;
-
-    // TODO: we could skip this if !hasTerms; but
-    // that's rare so won't help much
-    // metadata
-    numBytes = ste.in.readVInt();
-    if (bytes == null) {
-      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];
-      bytesReader = new ByteArrayDataInput();
-    } else if (bytes.length < numBytes) {
-      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];
-    }
-    ste.in.readBytes(bytes, 0, numBytes);
-    bytesReader.reset(bytes, 0, numBytes);
-
-
-    // Sub-blocks of a single floor block are always
-    // written one after another -- tail recurse:
-    fpEnd = ste.in.getFilePointer();
-    // if (DEBUG) {
-    //   System.out.println("      fpEnd=" + fpEnd);
-    // }
-  }
-
-  void rewind() {
-
-    // Force reload:
-    fp = fpOrig;
-    nextEnt = -1;
-    hasTerms = hasTermsOrig;
-    if (isFloor) {
-      floorDataReader.rewind();
-      numFollowFloorBlocks = floorDataReader.readVInt();
-      assert numFollowFloorBlocks > 0;
-      nextFloorLabel = floorDataReader.readByte() & 0xff;
-    }
-
-    /*
-    //System.out.println("rewind");
-    // Keeps the block loaded, but rewinds its state:
-    if (nextEnt > 0 || fp != fpOrig) {
-    if (DEBUG) {
-    System.out.println("      rewind frame ord=" + ord + " fpOrig=" + fpOrig + " fp=" + fp + " hasTerms?=" + hasTerms + " isFloor?=" + isFloor + " nextEnt=" + nextEnt + " prefixLen=" + prefix);
-    }
-    if (fp != fpOrig) {
-    fp = fpOrig;
-    nextEnt = -1;
-    } else {
-    nextEnt = 0;
-    }
-    hasTerms = hasTermsOrig;
-    if (isFloor) {
-    floorDataReader.rewind();
-    numFollowFloorBlocks = floorDataReader.readVInt();
-    nextFloorLabel = floorDataReader.readByte() & 0xff;
-    }
-    assert suffixBytes != null;
-    suffixesReader.rewind();
-    assert statBytes != null;
-    statsReader.rewind();
-    metaDataUpto = 0;
-    state.termBlockOrd = 0;
-    // TODO: skip this if !hasTerms?  Then postings
-    // impl wouldn't have to write useless 0 byte
-    postingsReader.resetTermsBlock(fieldInfo, state);
-    lastSubFP = -1;
-    } else if (DEBUG) {
-    System.out.println("      skip rewind fp=" + fp + " fpOrig=" + fpOrig + " nextEnt=" + nextEnt + " ord=" + ord);
-    }
-    */
-  }
-
-  public boolean next() {
-    return isLeafBlock ? nextLeaf() : nextNonLeaf();
-  }
-
-  // Decodes next entry; returns true if it's a sub-block
-  public boolean nextLeaf() {
-    //if (DEBUG) System.out.println("  frame.next ord=" + ord + " nextEnt=" + nextEnt + " entCount=" + entCount);
-    assert nextEnt != -1 && nextEnt < entCount: "nextEnt=" + nextEnt + " entCount=" + entCount + " fp=" + fp;
-    nextEnt++;
-    suffix = suffixesReader.readVInt();
-    startBytePos = suffixesReader.getPosition();
-    ste.term.setLength(prefix + suffix);
-    ste.term.grow(ste.term.length());
-    suffixesReader.readBytes(ste.term.bytes(), prefix, suffix);
-    // A normal term
-    ste.termExists = true;
-    return false;
-  }
-
-  public boolean nextNonLeaf() {
-    //if (DEBUG) System.out.println("  frame.next ord=" + ord + " nextEnt=" + nextEnt + " entCount=" + entCount);
-    assert nextEnt != -1 && nextEnt < entCount: "nextEnt=" + nextEnt + " entCount=" + entCount + " fp=" + fp;
-    nextEnt++;
-    final int code = suffixesReader.readVInt();
-    suffix = code >>> 1;
-    startBytePos = suffixesReader.getPosition();
-    ste.term.setLength(prefix + suffix);
-    ste.term.grow(ste.term.length());
-    suffixesReader.readBytes(ste.term.bytes(), prefix, suffix);
-    if ((code & 1) == 0) {
-      // A normal term
-      ste.termExists = true;
-      subCode = 0;
-      state.termBlockOrd++;
-      return false;
-    } else {
-      // A sub-block; make sub-FP absolute:
-      ste.termExists = false;
-      subCode = suffixesReader.readVLong();
-      lastSubFP = fp - subCode;
-      //if (DEBUG) {
-      //System.out.println("    lastSubFP=" + lastSubFP);
-      //}
-      return true;
-    }
-  }
-        
-  // TODO: make this array'd so we can do bin search?
-  // likely not worth it?  need to measure how many
-  // floor blocks we "typically" get
-  public void scanToFloorFrame(BytesRef target) {
-
-    if (!isFloor || target.length <= prefix) {
-      // if (DEBUG) {
-      //   System.out.println("    scanToFloorFrame skip: isFloor=" + isFloor + " target.length=" + target.length + " vs prefix=" + prefix);
-      // }
-      return;
-    }
-
-    final int targetLabel = target.bytes[target.offset + prefix] & 0xFF;
-
-    // if (DEBUG) {
-    //   System.out.println("    scanToFloorFrame fpOrig=" + fpOrig + " targetLabel=" + toHex(targetLabel) + " vs nextFloorLabel=" + toHex(nextFloorLabel) + " numFollowFloorBlocks=" + numFollowFloorBlocks);
-    // }
-
-    if (targetLabel < nextFloorLabel) {
-      // if (DEBUG) {
-      //   System.out.println("      already on correct block");
-      // }
-      return;
-    }
-
-    assert numFollowFloorBlocks != 0;
-
-    long newFP = fpOrig;
-    while (true) {
-      final long code = floorDataReader.readVLong();
-      newFP = fpOrig + (code >>> 1);
-      hasTerms = (code & 1) != 0;
-      // if (DEBUG) {
-      //   System.out.println("      label=" + toHex(nextFloorLabel) + " fp=" + newFP + " hasTerms?=" + hasTerms + " numFollowFloor=" + numFollowFloorBlocks);
-      // }
-            
-      isLastInFloor = numFollowFloorBlocks == 1;
-      numFollowFloorBlocks--;
-
-      if (isLastInFloor) {
-        nextFloorLabel = 256;
-        // if (DEBUG) {
-        //   System.out.println("        stop!  last block nextFloorLabel=" + toHex(nextFloorLabel));
-        // }
-        break;
-      } else {
-        nextFloorLabel = floorDataReader.readByte() & 0xff;
-        if (targetLabel < nextFloorLabel) {
-          // if (DEBUG) {
-          //   System.out.println("        stop!  nextFloorLabel=" + toHex(nextFloorLabel));
-          // }
-          break;
-        }
-      }
-    }
-
-    if (newFP != fp) {
-      // Force re-load of the block:
-      // if (DEBUG) {
-      //   System.out.println("      force switch to fp=" + newFP + " oldFP=" + fp);
-      // }
-      nextEnt = -1;
-      fp = newFP;
-    } else {
-      // if (DEBUG) {
-      //   System.out.println("      stay on same fp=" + newFP);
-      // }
-    }
-  }
-    
-  public void decodeMetaData() throws IOException {
-
-    //if (DEBUG) System.out.println("\nBTTR.decodeMetadata seg=" + segment + " mdUpto=" + metaDataUpto + " vs termBlockOrd=" + state.termBlockOrd);
-
-    // lazily catch up on metadata decode:
-    final int limit = getTermBlockOrd();
-    boolean absolute = metaDataUpto == 0;
-    assert limit > 0;
-
-    // TODO: better API would be "jump straight to term=N"???
-    while (metaDataUpto < limit) {
-
-      // TODO: we could make "tiers" of metadata, ie,
-      // decode docFreq/totalTF but don't decode postings
-      // metadata; this way caller could get
-      // docFreq/totalTF w/o paying decode cost for
-      // postings
-
-      // TODO: if docFreq were bulk decoded we could
-      // just skipN here:
-
-      // stats
-      state.docFreq = statsReader.readVInt();
-      //if (DEBUG) System.out.println("    dF=" + state.docFreq);
-      if (ste.fr.fieldInfo.getIndexOptions() != IndexOptions.DOCS_ONLY) {
-        state.totalTermFreq = state.docFreq + statsReader.readVLong();
-        //if (DEBUG) System.out.println("    totTF=" + state.totalTermFreq);
-      }
-      // metadata 
-      for (int i = 0; i < ste.fr.longsSize; i++) {
-        longs[i] = bytesReader.readVLong();
-      }
-      ste.fr.parent.postingsReader.decodeTerm(longs, bytesReader, ste.fr.fieldInfo, state, absolute);
-
-      metaDataUpto++;
-      absolute = false;
-    }
-    state.termBlockOrd = metaDataUpto;
-  }
-
-  // Used only by assert
-  private boolean prefixMatches(BytesRef target) {
-    for(int bytePos=0;bytePos<prefix;bytePos++) {
-      if (target.bytes[target.offset + bytePos] != ste.term.byteAt(bytePos)) {
-        return false;
-      }
-    }
-
-    return true;
-  }
-
-  // Scans to sub-block that has this target fp; only
-  // called by next(); NOTE: does not set
-  // startBytePos/suffix as a side effect
-  public void scanToSubBlock(long subFP) {
-    assert !isLeafBlock;
-    //if (DEBUG) System.out.println("  scanToSubBlock fp=" + fp + " subFP=" + subFP + " entCount=" + entCount + " lastSubFP=" + lastSubFP);
-    //assert nextEnt == 0;
-    if (lastSubFP == subFP) {
-      //if (DEBUG) System.out.println("    already positioned");
-      return;
-    }
-    assert subFP < fp : "fp=" + fp + " subFP=" + subFP;
-    final long targetSubCode = fp - subFP;
-    //if (DEBUG) System.out.println("    targetSubCode=" + targetSubCode);
-    while(true) {
-      assert nextEnt < entCount;
-      nextEnt++;
-      final int code = suffixesReader.readVInt();
-      suffixesReader.skipBytes(isLeafBlock ? code : code >>> 1);
-      //if (DEBUG) System.out.println("    " + nextEnt + " (of " + entCount + ") ent isSubBlock=" + ((code&1)==1));
-      if ((code & 1) != 0) {
-        final long subCode = suffixesReader.readVLong();
-        //if (DEBUG) System.out.println("      subCode=" + subCode);
-        if (targetSubCode == subCode) {
-          //if (DEBUG) System.out.println("        match!");
-          lastSubFP = subFP;
-          return;
-        }
-      } else {
-        state.termBlockOrd++;
-      }
-    }
-  }
-
-  // NOTE: sets startBytePos/suffix as a side effect
-  public SeekStatus scanToTerm(BytesRef target, boolean exactOnly) throws IOException {
-    return isLeafBlock ? scanToTermLeaf(target, exactOnly) : scanToTermNonLeaf(target, exactOnly);
-  }
-
-  private int startBytePos;
-  private int suffix;
-  private long subCode;
-
-  // Target's prefix matches this block's prefix; we
-  // scan the entries check if the suffix matches.
-  public SeekStatus scanToTermLeaf(BytesRef target, boolean exactOnly) throws IOException {
-
-    // if (DEBUG) System.out.println("    scanToTermLeaf: block fp=" + fp + " prefix=" + prefix + " nextEnt=" + nextEnt + " (of " + entCount + ") target=" + brToString(target) + " term=" + brToString(term));
-
-    assert nextEnt != -1;
-
-    ste.termExists = true;
-    subCode = 0;
-
-    if (nextEnt == entCount) {
-      if (exactOnly) {
-        fillTerm();
-      }
-      return SeekStatus.END;
-    }
-
-    assert prefixMatches(target);
-
-    // Loop over each entry (term or sub-block) in this block:
-    //nextTerm: while(nextEnt < entCount) {
-    nextTerm: while (true) {
-      nextEnt++;
-
-      suffix = suffixesReader.readVInt();
-
-      // if (DEBUG) {
-      //   BytesRef suffixBytesRef = new BytesRef();
-      //   suffixBytesRef.bytes = suffixBytes;
-      //   suffixBytesRef.offset = suffixesReader.getPosition();
-      //   suffixBytesRef.length = suffix;
-      //   System.out.println("      cycle: term " + (nextEnt-1) + " (of " + entCount + ") suffix=" + brToString(suffixBytesRef));
-      // }
-
-      final int termLen = prefix + suffix;
-      startBytePos = suffixesReader.getPosition();
-      suffixesReader.skipBytes(suffix);
-
-      final int targetLimit = target.offset + (target.length < termLen ? target.length : termLen);
-      int targetPos = target.offset + prefix;
-
-      // Loop over bytes in the suffix, comparing to
-      // the target
-      int bytePos = startBytePos;
-      while(true) {
-        final int cmp;
-        final boolean stop;
-        if (targetPos < targetLimit) {
-          cmp = (suffixBytes[bytePos++]&0xFF) - (target.bytes[targetPos++]&0xFF);
-          stop = false;
-        } else {
-          assert targetPos == targetLimit;
-          cmp = termLen - target.length;
-          stop = true;
-        }
-
-        if (cmp < 0) {
-          // Current entry is still before the target;
-          // keep scanning
-
-          if (nextEnt == entCount) {
-            if (exactOnly) {
-              fillTerm();
-            }
-            // We are done scanning this block
-            break nextTerm;
-          } else {
-            continue nextTerm;
-          }
-        } else if (cmp > 0) {
-
-          // Done!  Current entry is after target --
-          // return NOT_FOUND:
-          fillTerm();
-
-          //if (DEBUG) System.out.println("        not found");
-          return SeekStatus.NOT_FOUND;
-        } else if (stop) {
-          // Exact match!
-
-          // This cannot be a sub-block because we
-          // would have followed the index to this
-          // sub-block from the start:
-
-          assert ste.termExists;
-          fillTerm();
-          //if (DEBUG) System.out.println("        found!");
-          return SeekStatus.FOUND;
-        }
-      }
-    }
-
-    // It is possible (and OK) that terms index pointed us
-    // at this block, but, we scanned the entire block and
-    // did not find the term to position to.  This happens
-    // when the target is after the last term in the block
-    // (but, before the next term in the index).  EG
-    // target could be foozzz, and terms index pointed us
-    // to the foo* block, but the last term in this block
-    // was fooz (and, eg, first term in the next block will
-    // bee fop).
-    //if (DEBUG) System.out.println("      block end");
-    if (exactOnly) {
-      fillTerm();
-    }
-
-    // TODO: not consistent that in the
-    // not-exact case we don't next() into the next
-    // frame here
-    return SeekStatus.END;
-  }
-
-  // Target's prefix matches this block's prefix; we
-  // scan the entries check if the suffix matches.
-  public SeekStatus scanToTermNonLeaf(BytesRef target, boolean exactOnly) throws IOException {
-
-    //if (DEBUG) System.out.println("    scanToTermNonLeaf: block fp=" + fp + " prefix=" + prefix + " nextEnt=" + nextEnt + " (of " + entCount + ") target=" + brToString(target) + " term=" + brToString(term));
-
-    assert nextEnt != -1;
-
-    if (nextEnt == entCount) {
-      if (exactOnly) {
-        fillTerm();
-        ste.termExists = subCode == 0;
-      }
-      return SeekStatus.END;
-    }
-
-    assert prefixMatches(target);
-
-    // Loop over each entry (term or sub-block) in this block:
-    //nextTerm: while(nextEnt < entCount) {
-    nextTerm: while (true) {
-      nextEnt++;
-
-      final int code = suffixesReader.readVInt();
-      suffix = code >>> 1;
-      // if (DEBUG) {
-      //   BytesRef suffixBytesRef = new BytesRef();
-      //   suffixBytesRef.bytes = suffixBytes;
-      //   suffixBytesRef.offset = suffixesReader.getPosition();
-      //   suffixBytesRef.length = suffix;
-      //   System.out.println("      cycle: " + ((code&1)==1 ? "sub-block" : "term") + " " + (nextEnt-1) + " (of " + entCount + ") suffix=" + brToString(suffixBytesRef));
-      // }
-
-      ste.termExists = (code & 1) == 0;
-      final int termLen = prefix + suffix;
-      startBytePos = suffixesReader.getPosition();
-      suffixesReader.skipBytes(suffix);
-      if (ste.termExists) {
-        state.termBlockOrd++;
-        subCode = 0;
-      } else {
-        subCode = suffixesReader.readVLong();
-        lastSubFP = fp - subCode;
-      }
-
-      final int targetLimit = target.offset + (target.length < termLen ? target.length : termLen);
-      int targetPos = target.offset + prefix;
-
-      // Loop over bytes in the suffix, comparing to
-      // the target
-      int bytePos = startBytePos;
-      while(true) {
-        final int cmp;
-        final boolean stop;
-        if (targetPos < targetLimit) {
-          cmp = (suffixBytes[bytePos++]&0xFF) - (target.bytes[targetPos++]&0xFF);
-          stop = false;
-        } else {
-          assert targetPos == targetLimit;
-          cmp = termLen - target.length;
-          stop = true;
-        }
-
-        if (cmp < 0) {
-          // Current entry is still before the target;
-          // keep scanning
-
-          if (nextEnt == entCount) {
-            if (exactOnly) {
-              fillTerm();
-              //termExists = true;
-            }
-            // We are done scanning this block
-            break nextTerm;
-          } else {
-            continue nextTerm;
-          }
-        } else if (cmp > 0) {
-
-          // Done!  Current entry is after target --
-          // return NOT_FOUND:
-          fillTerm();
-
-          if (!exactOnly && !ste.termExists) {
-            // We are on a sub-block, and caller wants
-            // us to position to the next term after
-            // the target, so we must recurse into the
-            // sub-frame(s):
-            ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, termLen);
-            ste.currentFrame.loadBlock();
-            while (ste.currentFrame.next()) {
-              ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, ste.term.length());
-              ste.currentFrame.loadBlock();
-            }
-          }
-                
-          //if (DEBUG) System.out.println("        not found");
-          return SeekStatus.NOT_FOUND;
-        } else if (stop) {
-          // Exact match!
-
-          // This cannot be a sub-block because we
-          // would have followed the index to this
-          // sub-block from the start:
-
-          assert ste.termExists;
-          fillTerm();
-          //if (DEBUG) System.out.println("        found!");
-          return SeekStatus.FOUND;
-        }
-      }
-    }
-
-    // It is possible (and OK) that terms index pointed us
-    // at this block, but, we scanned the entire block and
-    // did not find the term to position to.  This happens
-    // when the target is after the last term in the block
-    // (but, before the next term in the index).  EG
-    // target could be foozzz, and terms index pointed us
-    // to the foo* block, but the last term in this block
-    // was fooz (and, eg, first term in the next block will
-    // bee fop).
-    //if (DEBUG) System.out.println("      block end");
-    if (exactOnly) {
-      fillTerm();
-    }
-
-    // TODO: not consistent that in the
-    // not-exact case we don't next() into the next
-    // frame here
-    return SeekStatus.END;
-  }
-
-  private void fillTerm() {
-    final int termLength = prefix + suffix;
-    ste.term.setLength(termLength);
-    ste.term.grow(termLength);
-    System.arraycopy(suffixBytes, startBytePos, ste.term.bytes(), prefix, suffix);
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/Lucene40Stats.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/Lucene40Stats.java
deleted file mode 100644
index 28e7163..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/Lucene40Stats.java
+++ /dev/null
@@ -1,201 +0,0 @@
-package org.apache.lucene.codecs.blocktree;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.ByteArrayOutputStream;
-import java.io.PrintStream;
-import java.io.UnsupportedEncodingException;
-import java.util.Locale;
-
-import org.apache.lucene.codecs.PostingsReaderBase;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * BlockTree statistics for a single field 
- * returned by {@link Lucene40FieldReader#getStats()}.
- * @deprecated Only for 4.x backcompat
- */
-@Deprecated
-final class Lucene40Stats {
-  /** How many nodes in the index FST. */
-  public long indexNodeCount;
-
-  /** How many arcs in the index FST. */
-  public long indexArcCount;
-
-  /** Byte size of the index. */
-  public long indexNumBytes;
-
-  /** Total number of terms in the field. */
-  public long totalTermCount;
-
-  /** Total number of bytes (sum of term lengths) across all terms in the field. */
-  public long totalTermBytes;
-
-  /** The number of normal (non-floor) blocks in the terms file. */
-  public int nonFloorBlockCount;
-
-  /** The number of floor blocks (meta-blocks larger than the
-   *  allowed {@code maxItemsPerBlock}) in the terms file. */
-  public int floorBlockCount;
-    
-  /** The number of sub-blocks within the floor blocks. */
-  public int floorSubBlockCount;
-
-  /** The number of "internal" blocks (that have both
-   *  terms and sub-blocks). */
-  public int mixedBlockCount;
-
-  /** The number of "leaf" blocks (blocks that have only
-   *  terms). */
-  public int termsOnlyBlockCount;
-
-  /** The number of "internal" blocks that do not contain
-   *  terms (have only sub-blocks). */
-  public int subBlocksOnlyBlockCount;
-
-  /** Total number of blocks. */
-  public int totalBlockCount;
-
-  /** Number of blocks at each prefix depth. */
-  public int[] blockCountByPrefixLen = new int[10];
-  private int startBlockCount;
-  private int endBlockCount;
-
-  /** Total number of bytes used to store term suffixes. */
-  public long totalBlockSuffixBytes;
-
-  /** Total number of bytes used to store term stats (not
-   *  including what the {@link PostingsReaderBase}
-   *  stores. */
-  public long totalBlockStatsBytes;
-
-  /** Total bytes stored by the {@link PostingsReaderBase},
-   *  plus the other few vInts stored in the frame. */
-  public long totalBlockOtherBytes;
-
-  /** Segment name. */
-  public final String segment;
-
-  /** Field name. */
-  public final String field;
-
-  Lucene40Stats(String segment, String field) {
-    this.segment = segment;
-    this.field = field;
-  }
-
-  void startBlock(Lucene40SegmentTermsEnumFrame frame, boolean isFloor) {
-    totalBlockCount++;
-    if (isFloor) {
-      if (frame.fp == frame.fpOrig) {
-        floorBlockCount++;
-      }
-      floorSubBlockCount++;
-    } else {
-      nonFloorBlockCount++;
-    }
-
-    if (blockCountByPrefixLen.length <= frame.prefix) {
-      blockCountByPrefixLen = ArrayUtil.grow(blockCountByPrefixLen, 1+frame.prefix);
-    }
-    blockCountByPrefixLen[frame.prefix]++;
-    startBlockCount++;
-    totalBlockSuffixBytes += frame.suffixesReader.length();
-    totalBlockStatsBytes += frame.statsReader.length();
-  }
-
-  void endBlock(Lucene40SegmentTermsEnumFrame frame) {
-    final int termCount = frame.isLeafBlock ? frame.entCount : frame.state.termBlockOrd;
-    final int subBlockCount = frame.entCount - termCount;
-    totalTermCount += termCount;
-    if (termCount != 0 && subBlockCount != 0) {
-      mixedBlockCount++;
-    } else if (termCount != 0) {
-      termsOnlyBlockCount++;
-    } else if (subBlockCount != 0) {
-      subBlocksOnlyBlockCount++;
-    } else {
-      throw new IllegalStateException();
-    }
-    endBlockCount++;
-    final long otherBytes = frame.fpEnd - frame.fp - frame.suffixesReader.length() - frame.statsReader.length();
-    assert otherBytes > 0 : "otherBytes=" + otherBytes + " frame.fp=" + frame.fp + " frame.fpEnd=" + frame.fpEnd;
-    totalBlockOtherBytes += otherBytes;
-  }
-
-  void term(BytesRef term) {
-    totalTermBytes += term.length;
-  }
-
-  void finish() {
-    assert startBlockCount == endBlockCount: "startBlockCount=" + startBlockCount + " endBlockCount=" + endBlockCount;
-    assert totalBlockCount == floorSubBlockCount + nonFloorBlockCount: "floorSubBlockCount=" + floorSubBlockCount + " nonFloorBlockCount=" + nonFloorBlockCount + " totalBlockCount=" + totalBlockCount;
-    assert totalBlockCount == mixedBlockCount + termsOnlyBlockCount + subBlocksOnlyBlockCount: "totalBlockCount=" + totalBlockCount + " mixedBlockCount=" + mixedBlockCount + " subBlocksOnlyBlockCount=" + subBlocksOnlyBlockCount + " termsOnlyBlockCount=" + termsOnlyBlockCount;
-  }
-
-  @Override
-  public String toString() {
-    final ByteArrayOutputStream bos = new ByteArrayOutputStream(1024);
-    PrintStream out;
-    try {
-      out = new PrintStream(bos, false, IOUtils.UTF_8);
-    } catch (UnsupportedEncodingException bogus) {
-      throw new RuntimeException(bogus);
-    }
-      
-    out.println("  index FST:");
-    out.println("    " + indexNodeCount + " nodes");
-    out.println("    " + indexArcCount + " arcs");
-    out.println("    " + indexNumBytes + " bytes");
-    out.println("  terms:");
-    out.println("    " + totalTermCount + " terms");
-    out.println("    " + totalTermBytes + " bytes" + (totalTermCount != 0 ? " (" + String.format(Locale.ROOT, "%.1f", ((double) totalTermBytes)/totalTermCount) + " bytes/term)" : ""));
-    out.println("  blocks:");
-    out.println("    " + totalBlockCount + " blocks");
-    out.println("    " + termsOnlyBlockCount + " terms-only blocks");
-    out.println("    " + subBlocksOnlyBlockCount + " sub-block-only blocks");
-    out.println("    " + mixedBlockCount + " mixed blocks");
-    out.println("    " + floorBlockCount + " floor blocks");
-    out.println("    " + (totalBlockCount-floorSubBlockCount) + " non-floor blocks");
-    out.println("    " + floorSubBlockCount + " floor sub-blocks");
-    out.println("    " + totalBlockSuffixBytes + " term suffix bytes" + (totalBlockCount != 0 ? " (" + String.format(Locale.ROOT, "%.1f", ((double) totalBlockSuffixBytes)/totalBlockCount) + " suffix-bytes/block)" : ""));
-    out.println("    " + totalBlockStatsBytes + " term stats bytes" + (totalBlockCount != 0 ? " (" + String.format(Locale.ROOT, "%.1f", ((double) totalBlockStatsBytes)/totalBlockCount) + " stats-bytes/block)" : ""));
-    out.println("    " + totalBlockOtherBytes + " other bytes" + (totalBlockCount != 0 ? " (" + String.format(Locale.ROOT, "%.1f", ((double) totalBlockOtherBytes)/totalBlockCount) + " other-bytes/block)" : ""));
-    if (totalBlockCount != 0) {
-      out.println("    by prefix length:");
-      int total = 0;
-      for(int prefix=0;prefix<blockCountByPrefixLen.length;prefix++) {
-        final int blockCount = blockCountByPrefixLen[prefix];
-        total += blockCount;
-        if (blockCount != 0) {
-          out.println("      " + String.format(Locale.ROOT, "%2d", prefix) + ": " + blockCount);
-        }
-      }
-      assert totalBlockCount == total;
-    }
-
-    try {
-      return bos.toString(IOUtils.UTF_8);
-    } catch (UnsupportedEncodingException bogus) {
-      throw new RuntimeException(bogus);
-    }
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/package.html b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/package.html
deleted file mode 100644
index 6886299..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/package.html
+++ /dev/null
@@ -1,26 +0,0 @@
-<!--
-  Licensed to the Apache Software Foundation (ASF) under one or more
-  contributor license agreements.  See the NOTICE file distributed with
-  this work for additional information regarding copyright ownership.
-  The ASF licenses this file to You under the Apache License, Version 2.0
-  (the "License"); you may not use this file except in compliance with
-  the License.  You may obtain a copy of the License at
-  
-      http://www.apache.org/licenses/LICENSE-2.0
-  
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License.
-  -->
-
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-BlockTree terms dictionary from Lucene 4.0-4.10
-</body>
-</html>
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/BitVector.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/BitVector.java
deleted file mode 100644
index 6ff4a1d..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/BitVector.java
+++ /dev/null
@@ -1,354 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.index.IndexFormatTooOldException;
-import org.apache.lucene.store.ChecksumIndexInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BitUtil;
-import org.apache.lucene.util.MutableBits;
-
-/** 
- * Bitset for support of 4.x live documents
- * @deprecated only for old 4.x segments
- */
-@Deprecated
-final class BitVector implements Cloneable, MutableBits {
-
-  private byte[] bits;
-  private int size;
-  private int count;
-  private int version;
-
-  /** Constructs a vector capable of holding <code>n</code> bits. */
-  BitVector(int n) {
-    size = n;
-    bits = new byte[getNumBytes(size)];
-    count = 0;
-  }
-
-  BitVector(byte[] bits, int size) {
-    this.bits = bits;
-    this.size = size;
-    count = -1;
-  }
-  
-  private int getNumBytes(int size) {
-    int bytesLength = size >>> 3;
-    if ((size & 7) != 0) {
-      bytesLength++;
-    }
-    return bytesLength;
-  }
-  
-  @Override
-  public BitVector clone() {
-    byte[] copyBits = new byte[bits.length];
-    System.arraycopy(bits, 0, copyBits, 0, bits.length);
-    BitVector clone = new BitVector(copyBits, size);
-    clone.count = count;
-    return clone;
-  }
-  
-  /** Sets the value of <code>bit</code> to one. */
-  public final void set(int bit) {
-    if (bit >= size) {
-      throw new ArrayIndexOutOfBoundsException("bit=" + bit + " size=" + size);
-    }
-    bits[bit >> 3] |= 1 << (bit & 7);
-    count = -1;
-  }
-
-  /** Sets the value of <code>bit</code> to zero. */
-  @Override
-  public final void clear(int bit) {
-    if (bit >= size) {
-      throw new ArrayIndexOutOfBoundsException(bit);
-    }
-    bits[bit >> 3] &= ~(1 << (bit & 7));
-    count = -1;
-  }
-
-  /** Returns <code>true</code> if <code>bit</code> is one and
-    <code>false</code> if it is zero. */
-  @Override
-  public final boolean get(int bit) {
-    assert bit >= 0 && bit < size: "bit " + bit + " is out of bounds 0.." + (size-1);
-    return (bits[bit >> 3] & (1 << (bit & 7))) != 0;
-  }
-
-  /** Returns the number of bits in this vector.  This is also one greater than
-    the number of the largest valid bit number. */
-  final int size() {
-    return size;
-  }
-
-  @Override
-  public int length() {
-    return size;
-  }
-
-  /** Returns the total number of one bits in this vector.  This is efficiently
-    computed and cached, so that, if the vector is not changed, no
-    recomputation is done for repeated calls. */
-  final int count() {
-    // if the vector has been modified
-    if (count == -1) {
-      int c = 0;
-      int end = bits.length;
-      for (int i = 0; i < end; i++) {
-        c += BitUtil.bitCount(bits[i]);  // sum bits per byte
-      }
-      count = c;
-    }
-    assert count <= size: "count=" + count + " size=" + size;
-    return count;
-  }
-
-  /** For testing */
-  final int getRecomputedCount() {
-    int c = 0;
-    int end = bits.length;
-    for (int i = 0; i < end; i++) {
-      c += BitUtil.bitCount(bits[i]);  // sum bits per byte
-    }
-    return c;
-  }
-
-
-
-  private static String CODEC = "BitVector";
-
-  // Version before version tracking was added:
-  final static int VERSION_PRE = -1;
-
-  // First version:
-  final static int VERSION_START = 0;
-
-  // Changed DGaps to encode gaps between cleared bits, not
-  // set:
-  final static int VERSION_DGAPS_CLEARED = 1;
-  
-  // added checksum
-  final static int VERSION_CHECKSUM = 2;
-
-  // Increment version to change it:
-  final static int VERSION_CURRENT = VERSION_CHECKSUM;
-
-  int getVersion() {
-    return version;
-  }
-
-  /** Writes this vector to the file <code>name</code> in Directory
-    <code>d</code>, in a format that can be read by the constructor {@link
-    #BitVector(Directory, String, IOContext)}.  */
-  final void write(Directory d, String name, IOContext context) throws IOException {
-    assert !(d instanceof Lucene40CompoundReader);
-    try (IndexOutput output = d.createOutput(name, context)) {
-      output.writeInt(-2);
-      CodecUtil.writeHeader(output, CODEC, VERSION_CURRENT);
-      if (isSparse()) { 
-        // sparse bit-set more efficiently saved as d-gaps.
-        writeClearedDgaps(output);
-      } else {
-        writeBits(output);
-      }
-      CodecUtil.writeFooter(output);
-      assert verifyCount();
-    }
-  }
-
-  /** Invert all bits */
-  void invertAll() {
-    if (count != -1) {
-      count = size - count;
-    }
-    if (bits.length > 0) {
-      for(int idx=0;idx<bits.length;idx++) {
-        bits[idx] = (byte) (~bits[idx]);
-      }
-      clearUnusedBits();
-    }
-  }
-
-  private void clearUnusedBits() {
-    // Take care not to invert the "unused" bits in the
-    // last byte:
-    if (bits.length > 0) {
-      final int lastNBits = size & 7;
-      if (lastNBits != 0) {
-        final int mask = (1 << lastNBits)-1;
-        bits[bits.length-1] &= mask;
-      }
-    }
-  }
-
-  /** Write as a bit set */
-  private void writeBits(IndexOutput output) throws IOException {
-    output.writeInt(size());        // write size
-    output.writeInt(count());       // write count
-    output.writeBytes(bits, bits.length);
-  }
-  
-  /** Write as a d-gaps list */
-  private void writeClearedDgaps(IndexOutput output) throws IOException {
-    output.writeInt(-1);            // mark using d-gaps                         
-    output.writeInt(size());        // write size
-    output.writeInt(count());       // write count
-    int last=0;
-    int numCleared = size()-count();
-    for (int i=0; i<bits.length && numCleared>0; i++) {
-      if (bits[i] != (byte) 0xff) {
-        output.writeVInt(i-last);
-        output.writeByte(bits[i]);
-        last = i;
-        numCleared -= (8-BitUtil.bitCount(bits[i]));
-        assert numCleared >= 0 || (i == (bits.length-1) && numCleared == -(8-(size&7)));
-      }
-    }
-  }
-
-  /** Indicates if the bit vector is sparse and should be saved as a d-gaps list, or dense, and should be saved as a bit set. */
-  private boolean isSparse() {
-
-    final int clearedCount = size() - count();
-    if (clearedCount == 0) {
-      return true;
-    }
-
-    final int avgGapLength = bits.length / clearedCount;
-
-    // expected number of bytes for vInt encoding of each gap
-    final int expectedDGapBytes;
-    if (avgGapLength <= (1<< 7)) {
-      expectedDGapBytes = 1;
-    } else if (avgGapLength <= (1<<14)) {
-      expectedDGapBytes = 2;
-    } else if (avgGapLength <= (1<<21)) {
-      expectedDGapBytes = 3;
-    } else if (avgGapLength <= (1<<28)) {
-      expectedDGapBytes = 4;
-    } else {
-      expectedDGapBytes = 5;
-    }
-
-    // +1 because we write the byte itself that contains the
-    // set bit
-    final int bytesPerSetBit = expectedDGapBytes + 1;
-    
-    // note: adding 32 because we start with ((int) -1) to indicate d-gaps format.
-    final long expectedBits = 32 + 8 * bytesPerSetBit * clearedCount;
-
-    // note: factor is for read/write of byte-arrays being faster than vints.  
-    final long factor = 10;  
-    return factor * expectedBits < size();
-  }
-
-  /** Constructs a bit vector from the file <code>name</code> in Directory
-    <code>d</code>, as written by the {@link #write} method.
-    */
-  BitVector(Directory d, String name, IOContext context) throws IOException {
-    try (ChecksumIndexInput input = d.openChecksumInput(name, context)) {
-      final int firstInt = input.readInt();
-
-      if (firstInt == -2) {
-        // New format, with full header & version:
-        version = CodecUtil.checkHeader(input, CODEC, VERSION_START, VERSION_CURRENT);
-        size = input.readInt();
-      } else {
-        // we started writing full header well before 4.0
-        throw new IndexFormatTooOldException(input.toString(), Integer.toString(firstInt));
-      }
-      if (size == -1) {
-        if (version >= VERSION_DGAPS_CLEARED) {
-          readClearedDgaps(input);
-        } else {
-          readSetDgaps(input);
-        }
-      } else {
-        readBits(input);
-      }
-
-      if (version < VERSION_DGAPS_CLEARED) {
-        invertAll();
-      }
-
-      if (version >= VERSION_CHECKSUM) {
-        CodecUtil.checkFooter(input);
-      } else {
-        CodecUtil.checkEOF(input);
-      }
-      assert verifyCount();
-    }
-  }
-
-  // asserts only
-  private boolean verifyCount() {
-    assert count != -1;
-    final int countSav = count;
-    count = -1;
-    assert countSav == count(): "saved count was " + countSav + " but recomputed count is " + count;
-    return true;
-  }
-
-  /** Read as a bit set */
-  private void readBits(IndexInput input) throws IOException {
-    count = input.readInt();        // read count
-    bits = new byte[getNumBytes(size)];     // allocate bits
-    input.readBytes(bits, 0, bits.length);
-  }
-
-  /** read as a d-gaps list */ 
-  private void readSetDgaps(IndexInput input) throws IOException {
-    size = input.readInt();       // (re)read size
-    count = input.readInt();        // read count
-    bits = new byte[getNumBytes(size)];     // allocate bits
-    int last=0;
-    int n = count();
-    while (n>0) {
-      last += input.readVInt();
-      bits[last] = input.readByte();
-      n -= BitUtil.bitCount(bits[last]);
-      assert n >= 0;
-    }          
-  }
-
-  /** read as a d-gaps cleared bits list */ 
-  private void readClearedDgaps(IndexInput input) throws IOException {
-    size = input.readInt();       // (re)read size
-    count = input.readInt();        // read count
-    bits = new byte[getNumBytes(size)];     // allocate bits
-    Arrays.fill(bits, (byte) 0xff);
-    clearUnusedBits();
-    int last=0;
-    int numCleared = size()-count();
-    while (numCleared>0) {
-      last += input.readVInt();
-      bits[last] = input.readByte();
-      numCleared -= 8-BitUtil.bitCount(bits[last]);
-      assert numCleared >= 0 || (last == (bits.length-1) && numCleared == -(8-(size&7)));
-    }
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java
deleted file mode 100644
index d05662f..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java
+++ /dev/null
@@ -1,116 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.CompoundFormat;
-import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.LiveDocsFormat;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.SegmentInfoFormat;
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.StoredFieldsFormat;
-import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
-
-/**
- * Reader for the 4.0 file format
- * @deprecated Only for reading old 4.0 segments
- */
-@Deprecated
-public class Lucene40Codec extends Codec {
-  private final StoredFieldsFormat fieldsFormat = new Lucene40StoredFieldsFormat();
-  private final TermVectorsFormat vectorsFormat = new Lucene40TermVectorsFormat();
-  private final FieldInfosFormat fieldInfosFormat = new Lucene40FieldInfosFormat();
-  private final SegmentInfoFormat infosFormat = new Lucene40SegmentInfoFormat();
-  private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat();
-  private final CompoundFormat compoundFormat = new Lucene40CompoundFormat();
-  
-  private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
-    @Override
-    public PostingsFormat getPostingsFormatForField(String field) {
-      return Lucene40Codec.this.getPostingsFormatForField(field);
-    }
-  };
-
-  /** Sole constructor. */
-  public Lucene40Codec() {
-    super("Lucene40");
-  }
-  
-  @Override
-  public StoredFieldsFormat storedFieldsFormat() {
-    return fieldsFormat;
-  }
-  
-  @Override
-  public TermVectorsFormat termVectorsFormat() {
-    return vectorsFormat;
-  }
-
-  @Override
-  public final PostingsFormat postingsFormat() {
-    return postingsFormat;
-  }
-  
-  @Override
-  public FieldInfosFormat fieldInfosFormat() {
-    return fieldInfosFormat;
-  }
-  
-  @Override
-  public SegmentInfoFormat segmentInfoFormat() {
-    return infosFormat;
-  }
-  
-  @Override
-  public CompoundFormat compoundFormat() {
-    return compoundFormat;
-  }
-
-  private final DocValuesFormat defaultDVFormat = new Lucene40DocValuesFormat();
-
-  @Override
-  public DocValuesFormat docValuesFormat() {
-    return defaultDVFormat;
-  }
-
-  private final NormsFormat normsFormat = new Lucene40NormsFormat();
-
-  @Override
-  public NormsFormat normsFormat() {
-    return normsFormat;
-  }
-
-  @Override
-  public final LiveDocsFormat liveDocsFormat() {
-    return liveDocsFormat;
-  }
-
-  /** Returns the postings format that should be used for writing 
-   *  new segments of <code>field</code>.
-   *  
-   *  The default implementation always returns "Lucene40"
-   */
-  public PostingsFormat getPostingsFormatForField(String field) {
-    return defaultFormat;
-  }
-  
-  private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene40");
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40CompoundFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40CompoundFormat.java
deleted file mode 100644
index dfae90f..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40CompoundFormat.java
+++ /dev/null
@@ -1,70 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collection;
-
-import org.apache.lucene.codecs.CompoundFormat;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.MergeState.CheckAbort;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-/**
- * Lucene 4.0 compound file format
- * @deprecated only for reading old 4.x segments
- */
-@Deprecated
-public final class Lucene40CompoundFormat extends CompoundFormat {
-  
-  /** Sole constructor. */
-  public Lucene40CompoundFormat() {
-  }
-
-  @Override
-  public Directory getCompoundReader(Directory dir, SegmentInfo si, IOContext context) throws IOException {
-    String fileName = IndexFileNames.segmentFileName(si.name, "", COMPOUND_FILE_EXTENSION);
-    return new Lucene40CompoundReader(dir, fileName, context, false);
-  }
-
-  @Override
-  public void write(Directory dir, SegmentInfo si, Collection<String> files, CheckAbort checkAbort, IOContext context) throws IOException {
-    String fileName = IndexFileNames.segmentFileName(si.name, "", COMPOUND_FILE_EXTENSION);
-    try (Directory cfs = new Lucene40CompoundReader(dir, fileName, context, true)) {
-      for (String file : files) {
-        dir.copy(cfs, file, file, context);
-        checkAbort.work(dir.fileLength(file));
-      }
-    }
-  }
-  
-  @Override
-  public String[] files(SegmentInfo si) {
-    return new String[] {
-      IndexFileNames.segmentFileName(si.name, "", COMPOUND_FILE_EXTENSION),
-      IndexFileNames.segmentFileName(si.name, "", COMPOUND_FILE_ENTRIES_EXTENSION)
-    };
-  }
-  
-  /** Extension of compound file */
-  static final String COMPOUND_FILE_EXTENSION = "cfs";
-  /** Extension of compound file entries */
-  static final String COMPOUND_FILE_ENTRIES_EXTENSION = "cfe";
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40CompoundReader.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40CompoundReader.java
deleted file mode 100644
index de108db..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40CompoundReader.java
+++ /dev/null
@@ -1,251 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.store.BaseDirectory;
-import org.apache.lucene.store.BufferedIndexInput;
-import org.apache.lucene.store.ChecksumIndexInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.Lock;
-import org.apache.lucene.util.IOUtils;
-
-import java.util.Collection;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-import java.io.FileNotFoundException;
-import java.io.IOException;
-
-/**
- * Lucene 4.x compound file format
- * @deprecated only for reading 4.x segments
- */
-@Deprecated
-final class Lucene40CompoundReader extends BaseDirectory {
-  
-  // TODO: would be great to move this read-write stuff out of here into test.
-
-  /** Offset/Length for a slice inside of a compound file */
-  public static final class FileEntry {
-    long offset;
-    long length;
-  }
-  
-  private final Directory directory;
-  private final String fileName;
-  protected final int readBufferSize;  
-  private final Map<String,FileEntry> entries;
-  private final boolean openForWrite;
-  private static final Map<String,FileEntry> SENTINEL = Collections.emptyMap();
-  private final Lucene40CompoundWriter writer;
-  private final IndexInput handle;
-  private int version;
-  
-  /**
-   * Create a new CompoundFileDirectory.
-   */
-  public Lucene40CompoundReader(Directory directory, String fileName, IOContext context, boolean openForWrite) throws IOException {
-    this.directory = directory;
-    this.fileName = fileName;
-    this.readBufferSize = BufferedIndexInput.bufferSize(context);
-    this.isOpen = false;
-    this.openForWrite = openForWrite;
-    if (!openForWrite) {
-      boolean success = false;
-      handle = directory.openInput(fileName, context);
-      try {
-        this.entries = readEntries(directory, fileName);
-        if (version >= Lucene40CompoundWriter.VERSION_CHECKSUM) {
-          CodecUtil.checkHeader(handle, Lucene40CompoundWriter.DATA_CODEC, version, version);
-          // NOTE: data file is too costly to verify checksum against all the bytes on open,
-          // but for now we at least verify proper structure of the checksum footer: which looks
-          // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
-          // such as file truncation.
-          CodecUtil.retrieveChecksum(handle);
-        }
-        success = true;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(handle);
-        }
-      }
-      this.isOpen = true;
-      writer = null;
-    } else {
-      assert !(directory instanceof Lucene40CompoundReader) : "compound file inside of compound file: " + fileName;
-      this.entries = SENTINEL;
-      this.isOpen = true;
-      writer = new Lucene40CompoundWriter(directory, fileName, context);
-      handle = null;
-    }
-  }
-
-  /** Helper method that reads CFS entries from an input stream */
-  private final Map<String, FileEntry> readEntries(Directory dir, String name) throws IOException {
-    ChecksumIndexInput entriesStream = null;
-    Map<String,FileEntry> mapping = null;
-    boolean success = false;
-    try {
-      final String entriesFileName = IndexFileNames.segmentFileName(
-                                            IndexFileNames.stripExtension(name), "",
-                                             Lucene40CompoundFormat.COMPOUND_FILE_ENTRIES_EXTENSION);
-      entriesStream = dir.openChecksumInput(entriesFileName, IOContext.READONCE);
-      version = CodecUtil.checkHeader(entriesStream, Lucene40CompoundWriter.ENTRY_CODEC, Lucene40CompoundWriter.VERSION_START, Lucene40CompoundWriter.VERSION_CURRENT);
-      final int numEntries = entriesStream.readVInt();
-      mapping = new HashMap<>(numEntries);
-      for (int i = 0; i < numEntries; i++) {
-        final FileEntry fileEntry = new FileEntry();
-        final String id = entriesStream.readString();
-        FileEntry previous = mapping.put(id, fileEntry);
-        if (previous != null) {
-          throw new CorruptIndexException("Duplicate cfs entry id=" + id + " in CFS ", entriesStream);
-        }
-        fileEntry.offset = entriesStream.readLong();
-        fileEntry.length = entriesStream.readLong();
-      }
-      if (version >= Lucene40CompoundWriter.VERSION_CHECKSUM) {
-        CodecUtil.checkFooter(entriesStream);
-      } else {
-        CodecUtil.checkEOF(entriesStream);
-      }
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(entriesStream);
-      } else {
-        IOUtils.closeWhileHandlingException(entriesStream);
-      }
-    }
-    return mapping;
-  }
-  
-  public Directory getDirectory() {
-    return directory;
-  }
-  
-  public String getName() {
-    return fileName;
-  }
-  
-  @Override
-  public synchronized void close() throws IOException {
-    if (!isOpen) {
-      // allow double close - usually to be consistent with other closeables
-      return; // already closed
-     }
-    isOpen = false;
-    if (writer != null) {
-      assert openForWrite;
-      writer.close();
-    } else {
-      IOUtils.close(handle);
-    }
-  }
-  
-  @Override
-  public synchronized IndexInput openInput(String name, IOContext context) throws IOException {
-    ensureOpen();
-    assert !openForWrite;
-    final String id = IndexFileNames.stripSegmentName(name);
-    final FileEntry entry = entries.get(id);
-    if (entry == null) {
-      throw new FileNotFoundException("No sub-file with id " + id + " found (fileName=" + name + " files: " + entries.keySet() + ")");
-    }
-    return handle.slice(name, entry.offset, entry.length);
-  }
-  
-  /** Returns an array of strings, one for each file in the directory. */
-  @Override
-  public String[] listAll() {
-    ensureOpen();
-    String[] res;
-    if (writer != null) {
-      res = writer.listAll(); 
-    } else {
-      res = entries.keySet().toArray(new String[entries.size()]);
-      // Add the segment name
-      String seg = IndexFileNames.parseSegmentName(fileName);
-      for (int i = 0; i < res.length; i++) {
-        res[i] = seg + res[i];
-      }
-    }
-    return res;
-  }
-  
-  /** Not implemented
-   * @throws UnsupportedOperationException always: not supported by CFS */
-  @Override
-  public void deleteFile(String name) {
-    throw new UnsupportedOperationException();
-  }
-  
-  /** Not implemented
-   * @throws UnsupportedOperationException always: not supported by CFS */
-  public void renameFile(String from, String to) {
-    throw new UnsupportedOperationException();
-  }
-  
-  /** Returns the length of a file in the directory.
-   * @throws IOException if the file does not exist */
-  @Override
-  public long fileLength(String name) throws IOException {
-    ensureOpen();
-    if (this.writer != null) {
-      return writer.fileLength(name);
-    }
-    FileEntry e = entries.get(IndexFileNames.stripSegmentName(name));
-    if (e == null)
-      throw new FileNotFoundException(name);
-    return e.length;
-  }
-  
-  @Override
-  public IndexOutput createOutput(String name, IOContext context) throws IOException {
-    ensureOpen();
-    if (!openForWrite) {
-      throw new UnsupportedOperationException();
-    }
-    return writer.createOutput(name, context);
-  }
-  
-  @Override
-  public void sync(Collection<String> names) {
-    throw new UnsupportedOperationException();
-  }
-  
-  @Override
-  public Lock makeLock(String name) {
-    throw new UnsupportedOperationException();
-  }
-  
-  @Override
-  public void clearLock(String name) throws IOException {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  public String toString() {
-    return "CompoundFileDirectory(file=\"" + fileName + "\" in dir=" + directory + ")";
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40CompoundWriter.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40CompoundWriter.java
deleted file mode 100644
index 49d82c1..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40CompoundWriter.java
+++ /dev/null
@@ -1,360 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable;
-import java.io.FileNotFoundException;
-import java.io.IOException;
-import java.util.Collection;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.LinkedList;
-import java.util.Map;
-import java.util.Queue;
-import java.util.Set;
-import java.util.concurrent.atomic.AtomicBoolean;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.store.AlreadyClosedException;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.FlushInfo;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Combines multiple files into a single compound file.
- * @deprecated only for testing
- */
-@Deprecated
-final class Lucene40CompoundWriter implements Closeable{
-
-  private static final class FileEntry {
-    /** source file */
-    String file;
-    long length;
-    /** temporary holder for the start of this file's data section */
-    long offset;
-    /** the directory which contains the file. */
-    Directory dir;
-  }
-
-  // versioning for the .cfs file
-  static final String DATA_CODEC = "CompoundFileWriterData";
-  static final int VERSION_START = 0;
-  static final int VERSION_CHECKSUM = 1;
-  static final int VERSION_CURRENT = VERSION_CHECKSUM;
-
-  // versioning for the .cfe file
-  static final String ENTRY_CODEC = "CompoundFileWriterEntries";
-
-  private final Directory directory;
-  private final Map<String, FileEntry> entries = new HashMap<>();
-  private final Set<String> seenIDs = new HashSet<>();
-  // all entries that are written to a sep. file but not yet moved into CFS
-  private final Queue<FileEntry> pendingEntries = new LinkedList<>();
-  private boolean closed = false;
-  private IndexOutput dataOut;
-  private final AtomicBoolean outputTaken = new AtomicBoolean(false);
-  final String entryTableName;
-  final String dataFileName;
-  
-  // preserve the IOContext we were originally passed
-  // previously this was not also passed to the .CFE
-  private final IOContext context;
-
-  /**
-   * Create the compound stream in the specified file. The file name is the
-   * entire name (no extensions are added).
-   * 
-   * @throws NullPointerException
-   *           if <code>dir</code> or <code>name</code> is null
-   */
-  Lucene40CompoundWriter(Directory dir, String name, IOContext context) {
-    if (dir == null)
-      throw new NullPointerException("directory cannot be null");
-    if (name == null)
-      throw new NullPointerException("name cannot be null");
-    directory = dir;
-    entryTableName = IndexFileNames.segmentFileName(
-        IndexFileNames.stripExtension(name), "",
-        Lucene40CompoundFormat.COMPOUND_FILE_ENTRIES_EXTENSION);
-    dataFileName = name;
-    this.context = context;
-  }
-  
-  private synchronized IndexOutput getOutput(IOContext context) throws IOException {
-    if (dataOut == null) {
-      boolean success = false;
-      try {
-        dataOut = directory.createOutput(dataFileName, this.context);
-        CodecUtil.writeHeader(dataOut, DATA_CODEC, VERSION_CURRENT);
-        success = true;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(dataOut);
-        }
-      }
-    } 
-    return dataOut;
-  }
-
-  /** Returns the directory of the compound file. */
-  Directory getDirectory() {
-    return directory;
-  }
-
-  /** Returns the name of the compound file. */
-  String getName() {
-    return dataFileName;
-  }
-
-  /**
-   * Closes all resources and writes the entry table
-   * 
-   * @throws IllegalStateException
-   *           if close() had been called before or if no file has been added to
-   *           this object
-   */
-  @Override
-  public void close() throws IOException {
-    if (closed) {
-      return;
-    }
-    IndexOutput entryTableOut = null;
-    // TODO this code should clean up after itself
-    // (remove partial .cfs/.cfe)
-    boolean success = false;
-    try {
-      if (!pendingEntries.isEmpty() || outputTaken.get()) {
-        throw new IllegalStateException("CFS has pending open files");
-      }
-      closed = true;
-      getOutput(this.context);
-      assert dataOut != null;
-      CodecUtil.writeFooter(dataOut);
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(dataOut);
-      } else {
-        IOUtils.closeWhileHandlingException(dataOut);
-      }
-    }
-    success = false;
-    try {
-      entryTableOut = directory.createOutput(entryTableName, this.context);
-      writeEntryTable(entries.values(), entryTableOut);
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(entryTableOut);
-      } else {
-        IOUtils.closeWhileHandlingException(entryTableOut);
-      }
-    }
-  }
-
-  private final void ensureOpen() {
-    if (closed) {
-      throw new AlreadyClosedException("CFS Directory is already closed");
-    }
-  }
-
-  /**
-   * Copy the contents of the file with specified extension into the provided
-   * output stream.
-   */
-  private final long copyFileEntry(IndexOutput dataOut, FileEntry fileEntry)
-      throws IOException {
-    final IndexInput is = fileEntry.dir.openInput(fileEntry.file, IOContext.READONCE);
-    boolean success = false;
-    try {
-      final long startPtr = dataOut.getFilePointer();
-      final long length = fileEntry.length;
-      dataOut.copyBytes(is, length);
-      // Verify that the output length diff is equal to original file
-      long endPtr = dataOut.getFilePointer();
-      long diff = endPtr - startPtr;
-      if (diff != length)
-        throw new IOException("Difference in the output file offsets " + diff
-            + " does not match the original file length " + length);
-      fileEntry.offset = startPtr;
-      success = true;
-      return length;
-    } finally {
-      if (success) {
-        IOUtils.close(is);
-        // copy successful - delete file
-        // if we can't we rely on IFD to pick up and retry
-        IOUtils.deleteFilesIgnoringExceptions(fileEntry.dir, fileEntry.file);
-      } else {
-        IOUtils.closeWhileHandlingException(is);
-      }
-    }
-  }
-
-  protected void writeEntryTable(Collection<FileEntry> entries,
-      IndexOutput entryOut) throws IOException {
-    CodecUtil.writeHeader(entryOut, ENTRY_CODEC, VERSION_CURRENT);
-    entryOut.writeVInt(entries.size());
-    for (FileEntry fe : entries) {
-      entryOut.writeString(IndexFileNames.stripSegmentName(fe.file));
-      entryOut.writeLong(fe.offset);
-      entryOut.writeLong(fe.length);
-    }
-    CodecUtil.writeFooter(entryOut);
-  }
-
-  IndexOutput createOutput(String name, IOContext context) throws IOException {
-    ensureOpen();
-    boolean success = false;
-    boolean outputLocked = false;
-    try {
-      assert name != null : "name must not be null";
-      if (entries.containsKey(name)) {
-        throw new IllegalArgumentException("File " + name + " already exists");
-      }
-      final FileEntry entry = new FileEntry();
-      entry.file = name;
-      entries.put(name, entry);
-      final String id = IndexFileNames.stripSegmentName(name);
-      assert !seenIDs.contains(id): "file=\"" + name + "\" maps to id=\"" + id + "\", which was already written";
-      seenIDs.add(id);
-      final DirectCFSIndexOutput out;
-
-      if ((outputLocked = outputTaken.compareAndSet(false, true))) {
-        out = new DirectCFSIndexOutput(getOutput(this.context), entry, false);
-      } else {
-        entry.dir = this.directory;
-        out = new DirectCFSIndexOutput(directory.createOutput(name, this.context), entry,
-            true);
-      }
-      success = true;
-      return out;
-    } finally {
-      if (!success) {
-        entries.remove(name);
-        if (outputLocked) { // release the output lock if not successful
-          assert outputTaken.get();
-          releaseOutputLock();
-        }
-      }
-    }
-  }
-
-  final void releaseOutputLock() {
-    outputTaken.compareAndSet(true, false);
-  }
-
-  private final void prunePendingEntries() throws IOException {
-    // claim the output and copy all pending files in
-    if (outputTaken.compareAndSet(false, true)) {
-      try {
-        while (!pendingEntries.isEmpty()) {
-          FileEntry entry = pendingEntries.poll();
-          copyFileEntry(getOutput(this.context), entry);
-          entries.put(entry.file, entry);
-        }
-      } finally {
-        final boolean compareAndSet = outputTaken.compareAndSet(true, false);
-        assert compareAndSet;
-      }
-    }
-  }
-
-  long fileLength(String name) throws IOException {
-    FileEntry fileEntry = entries.get(name);
-    if (fileEntry == null) {
-      throw new FileNotFoundException(name + " does not exist");
-    }
-    return fileEntry.length;
-  }
-
-  boolean fileExists(String name) {
-    return entries.containsKey(name);
-  }
-
-  String[] listAll() {
-    return entries.keySet().toArray(new String[0]);
-  }
-
-  private final class DirectCFSIndexOutput extends IndexOutput {
-    private final IndexOutput delegate;
-    private final long offset;
-    private boolean closed;
-    private FileEntry entry;
-    private long writtenBytes;
-    private final boolean isSeparate;
-
-    DirectCFSIndexOutput(IndexOutput delegate, FileEntry entry,
-        boolean isSeparate) {
-      super();
-      this.delegate = delegate;
-      this.entry = entry;
-      entry.offset = offset = delegate.getFilePointer();
-      this.isSeparate = isSeparate;
-
-    }
-
-    @Override
-    public void close() throws IOException {
-      if (!closed) {
-        closed = true;
-        entry.length = writtenBytes;
-        if (isSeparate) {
-          delegate.close();
-          // we are a separate file - push into the pending entries
-          pendingEntries.add(entry);
-        } else {
-          // we have been written into the CFS directly - release the lock
-          releaseOutputLock();
-        }
-        // now prune all pending entries and push them into the CFS
-        prunePendingEntries();
-      }
-    }
-
-    @Override
-    public long getFilePointer() {
-      return delegate.getFilePointer() - offset;
-    }
-
-    @Override
-    public void writeByte(byte b) throws IOException {
-      assert !closed;
-      writtenBytes++;
-      delegate.writeByte(b);
-    }
-
-    @Override
-    public void writeBytes(byte[] b, int offset, int length) throws IOException {
-      assert !closed;
-      writtenBytes += length;
-      delegate.writeBytes(b, offset, length);
-    }
-
-    @Override
-    public long getChecksum() throws IOException {
-      return delegate.getChecksum();
-    }
-  }
-
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesFormat.java
deleted file mode 100644
index 3af4395..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesFormat.java
+++ /dev/null
@@ -1,109 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.DocValuesProducer;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-
-/**
- * Lucene 4.0 DocValues format.
- * @deprecated Only for reading old 4.0 and 4.1 segments
- */
-@Deprecated
-public class Lucene40DocValuesFormat extends DocValuesFormat {
-  
-  /** Maximum length for each binary doc values field. */
-  static final int MAX_BINARY_FIELD_LENGTH = (1 << 15) - 2;
-  
-  /** Sole constructor. */
-  public Lucene40DocValuesFormat() {
-    super("Lucene40");
-  }
-  
-  @Override
-  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    throw new UnsupportedOperationException("this codec can only be used for reading");
-  }
-  
-  @Override
-  public final DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
-    String filename = IndexFileNames.segmentFileName(state.segmentInfo.name, 
-                                                     "dv", 
-                                                     Lucene40CompoundFormat.COMPOUND_FILE_EXTENSION);
-    return new Lucene40DocValuesReader(state, filename, Lucene40FieldInfosFormat.LEGACY_DV_TYPE_KEY);
-  }
-  
-  // constants for VAR_INTS
-  static final String VAR_INTS_CODEC_NAME = "PackedInts";
-  static final int VAR_INTS_VERSION_START = 0;
-  static final int VAR_INTS_VERSION_CURRENT = VAR_INTS_VERSION_START;
-  static final byte VAR_INTS_PACKED = 0x00;
-  static final byte VAR_INTS_FIXED_64 = 0x01;
-  
-  // constants for FIXED_INTS_8, FIXED_INTS_16, FIXED_INTS_32, FIXED_INTS_64
-  static final String INTS_CODEC_NAME = "Ints";
-  static final int INTS_VERSION_START = 0;
-  static final int INTS_VERSION_CURRENT = INTS_VERSION_START;
-  
-  // constants for FLOAT_32, FLOAT_64
-  static final String FLOATS_CODEC_NAME = "Floats";
-  static final int FLOATS_VERSION_START = 0;
-  static final int FLOATS_VERSION_CURRENT = FLOATS_VERSION_START;
-  
-  // constants for BYTES_FIXED_STRAIGHT
-  static final String BYTES_FIXED_STRAIGHT_CODEC_NAME = "FixedStraightBytes";
-  static final int BYTES_FIXED_STRAIGHT_VERSION_START = 0;
-  static final int BYTES_FIXED_STRAIGHT_VERSION_CURRENT = BYTES_FIXED_STRAIGHT_VERSION_START;
-  
-  // constants for BYTES_VAR_STRAIGHT
-  static final String BYTES_VAR_STRAIGHT_CODEC_NAME_IDX = "VarStraightBytesIdx";
-  static final String BYTES_VAR_STRAIGHT_CODEC_NAME_DAT = "VarStraightBytesDat";
-  static final int BYTES_VAR_STRAIGHT_VERSION_START = 0;
-  static final int BYTES_VAR_STRAIGHT_VERSION_CURRENT = BYTES_VAR_STRAIGHT_VERSION_START;
-  
-  // constants for BYTES_FIXED_DEREF
-  static final String BYTES_FIXED_DEREF_CODEC_NAME_IDX = "FixedDerefBytesIdx";
-  static final String BYTES_FIXED_DEREF_CODEC_NAME_DAT = "FixedDerefBytesDat";
-  static final int BYTES_FIXED_DEREF_VERSION_START = 0;
-  static final int BYTES_FIXED_DEREF_VERSION_CURRENT = BYTES_FIXED_DEREF_VERSION_START;
-  
-  // constants for BYTES_VAR_DEREF
-  static final String BYTES_VAR_DEREF_CODEC_NAME_IDX = "VarDerefBytesIdx";
-  static final String BYTES_VAR_DEREF_CODEC_NAME_DAT = "VarDerefBytesDat";
-  static final int BYTES_VAR_DEREF_VERSION_START = 0;
-  static final int BYTES_VAR_DEREF_VERSION_CURRENT = BYTES_VAR_DEREF_VERSION_START;
-  
-  // constants for BYTES_FIXED_SORTED
-  static final String BYTES_FIXED_SORTED_CODEC_NAME_IDX = "FixedSortedBytesIdx";
-  static final String BYTES_FIXED_SORTED_CODEC_NAME_DAT = "FixedSortedBytesDat";
-  static final int BYTES_FIXED_SORTED_VERSION_START = 0;
-  static final int BYTES_FIXED_SORTED_VERSION_CURRENT = BYTES_FIXED_SORTED_VERSION_START;
-  
-  // constants for BYTES_VAR_SORTED
-  // NOTE THIS IS NOT A BUG! 4.0 actually screwed this up (VAR_SORTED and VAR_DEREF have same codec header)
-  static final String BYTES_VAR_SORTED_CODEC_NAME_IDX = "VarDerefBytesIdx";
-  static final String BYTES_VAR_SORTED_CODEC_NAME_DAT = "VarDerefBytesDat";
-  static final int BYTES_VAR_SORTED_VERSION_START = 0;
-  static final int BYTES_VAR_SORTED_VERSION_CURRENT = BYTES_VAR_SORTED_VERSION_START;
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesReader.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesReader.java
deleted file mode 100644
index 2584157..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesReader.java
+++ /dev/null
@@ -1,752 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-import java.util.concurrent.atomic.AtomicLong;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.DocValuesProducer;
-import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosFormat.LegacyDocValuesType;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.NumericDocValues;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SortedDocValues;
-import org.apache.lucene.index.SortedNumericDocValues;
-import org.apache.lucene.index.SortedSetDocValues;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.Accountable;
-import org.apache.lucene.util.Accountables;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.PagedBytes;
-import org.apache.lucene.util.RamUsageEstimator;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * Reads the 4.0 format of norms/docvalues
- * @deprecated Only for reading old 4.0 and 4.1 segments
- */
-@Deprecated
-final class Lucene40DocValuesReader extends DocValuesProducer {
-  private final Directory dir;
-  private final SegmentReadState state;
-  private final String legacyKey;
-  private static final String segmentSuffix = "dv";
-
-  // ram instances we have already loaded
-  private final Map<String,NumericDocValues> numericInstances = new HashMap<>();
-  private final Map<String,BinaryDocValues> binaryInstances = new HashMap<>();
-  private final Map<String,SortedDocValues> sortedInstances = new HashMap<>();
-  
-  private final Map<String,Accountable> instanceInfo = new HashMap<>();
-
-  private final AtomicLong ramBytesUsed;
-  
-  private final boolean merging;
-  
-  // clone for merge: when merging we don't do any instances.put()s
-  Lucene40DocValuesReader(Lucene40DocValuesReader original) throws IOException {
-    assert Thread.holdsLock(original);
-    dir = original.dir;
-    state = original.state;
-    legacyKey = original.legacyKey;
-    numericInstances.putAll(original.numericInstances);
-    binaryInstances.putAll(original.binaryInstances);
-    sortedInstances.putAll(original.sortedInstances);
-    instanceInfo.putAll(original.instanceInfo);
-    ramBytesUsed = new AtomicLong(original.ramBytesUsed.get());
-    merging = true;
-  }
-
-  Lucene40DocValuesReader(SegmentReadState state, String filename, String legacyKey) throws IOException {
-    this.state = state;
-    this.legacyKey = legacyKey;
-    this.dir = new Lucene40CompoundReader(state.directory, filename, state.context, false);
-    ramBytesUsed = new AtomicLong(RamUsageEstimator.shallowSizeOf(getClass()));
-    merging = false;
-  }
-
-  @Override
-  public synchronized NumericDocValues getNumeric(FieldInfo field) throws IOException {
-    NumericDocValues instance = numericInstances.get(field.name);
-    if (instance == null) {
-      String fileName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
-      IndexInput input = dir.openInput(fileName, state.context);
-      boolean success = false;
-      try {
-        switch(LegacyDocValuesType.valueOf(field.getAttribute(legacyKey))) {
-          case VAR_INTS:
-            instance = loadVarIntsField(field, input);
-            break;
-          case FIXED_INTS_8:
-            instance = loadByteField(field, input);
-            break;
-          case FIXED_INTS_16:
-            instance = loadShortField(field, input);
-            break;
-          case FIXED_INTS_32:
-            instance = loadIntField(field, input);
-            break;
-          case FIXED_INTS_64:
-            instance = loadLongField(field, input);
-            break;
-          case FLOAT_32:
-            instance = loadFloatField(field, input);
-            break;
-          case FLOAT_64:
-            instance = loadDoubleField(field, input);
-            break;
-          default:
-            throw new AssertionError();
-        }
-        CodecUtil.checkEOF(input);
-        success = true;
-      } finally {
-        if (success) {
-          IOUtils.close(input);
-        } else {
-          IOUtils.closeWhileHandlingException(input);
-        }
-      }
-      if (!merging) {
-        numericInstances.put(field.name, instance);
-      }
-    }
-    return instance;
-  }
-
-  private NumericDocValues loadVarIntsField(FieldInfo field, IndexInput input) throws IOException {
-    CodecUtil.checkHeader(input, Lucene40DocValuesFormat.VAR_INTS_CODEC_NAME,
-                                 Lucene40DocValuesFormat.VAR_INTS_VERSION_START,
-                                 Lucene40DocValuesFormat.VAR_INTS_VERSION_CURRENT);
-    byte header = input.readByte();
-    if (header == Lucene40DocValuesFormat.VAR_INTS_FIXED_64) {
-      int maxDoc = state.segmentInfo.getDocCount();
-      final long values[] = new long[maxDoc];
-      for (int i = 0; i < values.length; i++) {
-        values[i] = input.readLong();
-      }
-      long bytesUsed = RamUsageEstimator.sizeOf(values);
-      if (!merging) {
-        instanceInfo.put(field.name, Accountables.namedAccountable("long array", bytesUsed));
-        ramBytesUsed.addAndGet(bytesUsed);
-      }
-      return new NumericDocValues() {
-        @Override
-        public long get(int docID) {
-          return values[docID];
-        }
-      };
-    } else if (header == Lucene40DocValuesFormat.VAR_INTS_PACKED) {
-      final long minValue = input.readLong();
-      final long defaultValue = input.readLong();
-      final PackedInts.Reader reader = PackedInts.getReader(input);
-      if (!merging) {
-        instanceInfo.put(field.name, reader);
-        ramBytesUsed.addAndGet(reader.ramBytesUsed());
-      }
-      return new NumericDocValues() {
-        @Override
-        public long get(int docID) {
-          final long value = reader.get(docID);
-          if (value == defaultValue) {
-            return 0;
-          } else {
-            return minValue + value;
-          }
-        }
-      };
-    } else {
-      throw new CorruptIndexException("invalid VAR_INTS header byte: " + header, input);
-    }
-  }
-
-  private NumericDocValues loadByteField(FieldInfo field, IndexInput input) throws IOException {
-    CodecUtil.checkHeader(input, Lucene40DocValuesFormat.INTS_CODEC_NAME,
-                                 Lucene40DocValuesFormat.INTS_VERSION_START,
-                                 Lucene40DocValuesFormat.INTS_VERSION_CURRENT);
-    int valueSize = input.readInt();
-    if (valueSize != 1) {
-      throw new CorruptIndexException("invalid valueSize: " + valueSize, input);
-    }
-    int maxDoc = state.segmentInfo.getDocCount();
-    final byte values[] = new byte[maxDoc];
-    input.readBytes(values, 0, values.length);
-    long bytesUsed = RamUsageEstimator.sizeOf(values);
-    if (!merging) {
-      instanceInfo.put(field.name, Accountables.namedAccountable("byte array", bytesUsed));
-      ramBytesUsed.addAndGet(bytesUsed);
-    }
-    return new NumericDocValues() {
-      @Override
-      public long get(int docID) {
-        return values[docID];
-      }
-    };
-  }
-
-  private NumericDocValues loadShortField(FieldInfo field, IndexInput input) throws IOException {
-    CodecUtil.checkHeader(input, Lucene40DocValuesFormat.INTS_CODEC_NAME,
-                                 Lucene40DocValuesFormat.INTS_VERSION_START,
-                                 Lucene40DocValuesFormat.INTS_VERSION_CURRENT);
-    int valueSize = input.readInt();
-    if (valueSize != 2) {
-      throw new CorruptIndexException("invalid valueSize: " + valueSize, input);
-    }
-    int maxDoc = state.segmentInfo.getDocCount();
-    final short values[] = new short[maxDoc];
-    for (int i = 0; i < values.length; i++) {
-      values[i] = input.readShort();
-    }
-    long bytesUsed = RamUsageEstimator.sizeOf(values);
-    if (!merging) {
-      instanceInfo.put(field.name, Accountables.namedAccountable("short array", bytesUsed));
-      ramBytesUsed.addAndGet(bytesUsed);
-    }
-    return new NumericDocValues() {
-      @Override
-      public long get(int docID) {
-        return values[docID];
-      }
-    };
-  }
-
-  private NumericDocValues loadIntField(FieldInfo field, IndexInput input) throws IOException {
-    CodecUtil.checkHeader(input, Lucene40DocValuesFormat.INTS_CODEC_NAME,
-                                 Lucene40DocValuesFormat.INTS_VERSION_START,
-                                 Lucene40DocValuesFormat.INTS_VERSION_CURRENT);
-    int valueSize = input.readInt();
-    if (valueSize != 4) {
-      throw new CorruptIndexException("invalid valueSize: " + valueSize, input);
-    }
-    int maxDoc = state.segmentInfo.getDocCount();
-    final int values[] = new int[maxDoc];
-    for (int i = 0; i < values.length; i++) {
-      values[i] = input.readInt();
-    }
-    long bytesUsed = RamUsageEstimator.sizeOf(values);
-    if (!merging) {
-      instanceInfo.put(field.name, Accountables.namedAccountable("int array", bytesUsed));
-      ramBytesUsed.addAndGet(bytesUsed);
-    }
-    return new NumericDocValues() {
-      @Override
-      public long get(int docID) {
-        return values[docID];
-      }
-    };
-  }
-
-  private NumericDocValues loadLongField(FieldInfo field, IndexInput input) throws IOException {
-    CodecUtil.checkHeader(input, Lucene40DocValuesFormat.INTS_CODEC_NAME,
-                                 Lucene40DocValuesFormat.INTS_VERSION_START,
-                                 Lucene40DocValuesFormat.INTS_VERSION_CURRENT);
-    int valueSize = input.readInt();
-    if (valueSize != 8) {
-      throw new CorruptIndexException("invalid valueSize: " + valueSize, input);
-    }
-    int maxDoc = state.segmentInfo.getDocCount();
-    final long values[] = new long[maxDoc];
-    for (int i = 0; i < values.length; i++) {
-      values[i] = input.readLong();
-    }
-    long bytesUsed = RamUsageEstimator.sizeOf(values);
-    if (!merging) {
-      instanceInfo.put(field.name, Accountables.namedAccountable("long array", bytesUsed));
-      ramBytesUsed.addAndGet(bytesUsed);
-    }
-    return new NumericDocValues() {
-      @Override
-      public long get(int docID) {
-        return values[docID];
-      }
-    };
-  }
-
-  private NumericDocValues loadFloatField(FieldInfo field, IndexInput input) throws IOException {
-    CodecUtil.checkHeader(input, Lucene40DocValuesFormat.FLOATS_CODEC_NAME,
-                                 Lucene40DocValuesFormat.FLOATS_VERSION_START,
-                                 Lucene40DocValuesFormat.FLOATS_VERSION_CURRENT);
-    int valueSize = input.readInt();
-    if (valueSize != 4) {
-      throw new CorruptIndexException("invalid valueSize: " + valueSize, input);
-    }
-    int maxDoc = state.segmentInfo.getDocCount();
-    final int values[] = new int[maxDoc];
-    for (int i = 0; i < values.length; i++) {
-      values[i] = input.readInt();
-    }
-    long bytesUsed = RamUsageEstimator.sizeOf(values);
-    if (!merging) {
-      instanceInfo.put(field.name, Accountables.namedAccountable("float array", bytesUsed));
-      ramBytesUsed.addAndGet(bytesUsed);
-    }
-    return new NumericDocValues() {
-      @Override
-      public long get(int docID) {
-        return values[docID];
-      }
-    };
-  }
-
-  private NumericDocValues loadDoubleField(FieldInfo field, IndexInput input) throws IOException {
-    CodecUtil.checkHeader(input, Lucene40DocValuesFormat.FLOATS_CODEC_NAME,
-                                 Lucene40DocValuesFormat.FLOATS_VERSION_START,
-                                 Lucene40DocValuesFormat.FLOATS_VERSION_CURRENT);
-    int valueSize = input.readInt();
-    if (valueSize != 8) {
-      throw new CorruptIndexException("invalid valueSize: " + valueSize, input);
-    }
-    int maxDoc = state.segmentInfo.getDocCount();
-    final long values[] = new long[maxDoc];
-    for (int i = 0; i < values.length; i++) {
-      values[i] = input.readLong();
-    }
-    long bytesUsed = RamUsageEstimator.sizeOf(values);
-    if (!merging) {
-      instanceInfo.put(field.name, Accountables.namedAccountable("double array", bytesUsed));
-      ramBytesUsed.addAndGet(bytesUsed);
-    }
-    return new NumericDocValues() {
-      @Override
-      public long get(int docID) {
-        return values[docID];
-      }
-    };
-  }
-
-  @Override
-  public synchronized BinaryDocValues getBinary(FieldInfo field) throws IOException {
-    BinaryDocValues instance = binaryInstances.get(field.name);
-    if (instance == null) {
-      switch(LegacyDocValuesType.valueOf(field.getAttribute(legacyKey))) {
-        case BYTES_FIXED_STRAIGHT:
-          instance = loadBytesFixedStraight(field);
-          break;
-        case BYTES_VAR_STRAIGHT:
-          instance = loadBytesVarStraight(field);
-          break;
-        case BYTES_FIXED_DEREF:
-          instance = loadBytesFixedDeref(field);
-          break;
-        case BYTES_VAR_DEREF:
-          instance = loadBytesVarDeref(field);
-          break;
-        default:
-          throw new AssertionError();
-      }
-      if (!merging) {
-        binaryInstances.put(field.name, instance);
-      }
-    }
-    return instance;
-  }
-
-  private BinaryDocValues loadBytesFixedStraight(FieldInfo field) throws IOException {
-    String fileName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
-    IndexInput input = dir.openInput(fileName, state.context);
-    boolean success = false;
-    try {
-      CodecUtil.checkHeader(input, Lucene40DocValuesFormat.BYTES_FIXED_STRAIGHT_CODEC_NAME,
-                                   Lucene40DocValuesFormat.BYTES_FIXED_STRAIGHT_VERSION_START,
-                                   Lucene40DocValuesFormat.BYTES_FIXED_STRAIGHT_VERSION_CURRENT);
-      final int fixedLength = input.readInt();
-      PagedBytes bytes = new PagedBytes(16);
-      bytes.copy(input, fixedLength * (long)state.segmentInfo.getDocCount());
-      final PagedBytes.Reader bytesReader = bytes.freeze(true);
-      CodecUtil.checkEOF(input);
-      success = true;
-      if (!merging) {
-        ramBytesUsed.addAndGet(bytesReader.ramBytesUsed());
-        instanceInfo.put(field.name, bytesReader);
-      }
-      return new BinaryDocValues() {
-
-        @Override
-        public BytesRef get(int docID) {
-          final BytesRef term = new BytesRef();
-          bytesReader.fillSlice(term, fixedLength * (long)docID, fixedLength);
-          return term;
-        }
-      };
-    } finally {
-      if (success) {
-        IOUtils.close(input);
-      } else {
-        IOUtils.closeWhileHandlingException(input);
-      }
-    }
-  }
-
-  private BinaryDocValues loadBytesVarStraight(FieldInfo field) throws IOException {
-    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
-    String indexName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "idx");
-    IndexInput data = null;
-    IndexInput index = null;
-    boolean success = false;
-    try {
-      data = dir.openInput(dataName, state.context);
-      CodecUtil.checkHeader(data, Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_CODEC_NAME_DAT,
-                                  Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_VERSION_START,
-                                  Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_VERSION_CURRENT);
-      index = dir.openInput(indexName, state.context);
-      CodecUtil.checkHeader(index, Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_CODEC_NAME_IDX,
-                                   Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_VERSION_START,
-                                   Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_VERSION_CURRENT);
-      long totalBytes = index.readVLong();
-      PagedBytes bytes = new PagedBytes(16);
-      bytes.copy(data, totalBytes);
-      final PagedBytes.Reader bytesReader = bytes.freeze(true);
-      final PackedInts.Reader reader = PackedInts.getReader(index);
-      CodecUtil.checkEOF(data);
-      CodecUtil.checkEOF(index);
-      success = true;
-      long bytesUsed = bytesReader.ramBytesUsed() + reader.ramBytesUsed();
-      if (!merging) {
-        ramBytesUsed.addAndGet(bytesUsed);
-        instanceInfo.put(field.name, Accountables.namedAccountable("variable straight", bytesUsed));
-      }
-      return new BinaryDocValues() {
-        @Override
-        public BytesRef get(int docID) {
-          final BytesRef term = new BytesRef();
-          long startAddress = reader.get(docID);
-          long endAddress = reader.get(docID+1);
-          bytesReader.fillSlice(term, startAddress, (int)(endAddress - startAddress));
-          return term;
-        }
-      };
-    } finally {
-      if (success) {
-        IOUtils.close(data, index);
-      } else {
-        IOUtils.closeWhileHandlingException(data, index);
-      }
-    }
-  }
-
-  private BinaryDocValues loadBytesFixedDeref(FieldInfo field) throws IOException {
-    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
-    String indexName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "idx");
-    IndexInput data = null;
-    IndexInput index = null;
-    boolean success = false;
-    try {
-      data = dir.openInput(dataName, state.context);
-      CodecUtil.checkHeader(data, Lucene40DocValuesFormat.BYTES_FIXED_DEREF_CODEC_NAME_DAT,
-                                  Lucene40DocValuesFormat.BYTES_FIXED_DEREF_VERSION_START,
-                                  Lucene40DocValuesFormat.BYTES_FIXED_DEREF_VERSION_CURRENT);
-      index = dir.openInput(indexName, state.context);
-      CodecUtil.checkHeader(index, Lucene40DocValuesFormat.BYTES_FIXED_DEREF_CODEC_NAME_IDX,
-                                   Lucene40DocValuesFormat.BYTES_FIXED_DEREF_VERSION_START,
-                                   Lucene40DocValuesFormat.BYTES_FIXED_DEREF_VERSION_CURRENT);
-
-      final int fixedLength = data.readInt();
-      final int valueCount = index.readInt();
-      PagedBytes bytes = new PagedBytes(16);
-      bytes.copy(data, fixedLength * (long) valueCount);
-      final PagedBytes.Reader bytesReader = bytes.freeze(true);
-      final PackedInts.Reader reader = PackedInts.getReader(index);
-      CodecUtil.checkEOF(data);
-      CodecUtil.checkEOF(index);
-      long bytesUsed = bytesReader.ramBytesUsed() + reader.ramBytesUsed();
-      if (!merging) {
-        ramBytesUsed.addAndGet(bytesUsed);
-        instanceInfo.put(field.name, Accountables.namedAccountable("fixed deref", bytesUsed));
-      }
-      success = true;
-      return new BinaryDocValues() {
-        @Override
-        public BytesRef get(int docID) {
-          final BytesRef term = new BytesRef();
-          final long offset = fixedLength * reader.get(docID);
-          bytesReader.fillSlice(term, offset, fixedLength);
-          return term;
-        }
-      };
-    } finally {
-      if (success) {
-        IOUtils.close(data, index);
-      } else {
-        IOUtils.closeWhileHandlingException(data, index);
-      }
-    }
-  }
-
-  private BinaryDocValues loadBytesVarDeref(FieldInfo field) throws IOException {
-    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
-    String indexName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "idx");
-    IndexInput data = null;
-    IndexInput index = null;
-    boolean success = false;
-    try {
-      data = dir.openInput(dataName, state.context);
-      CodecUtil.checkHeader(data, Lucene40DocValuesFormat.BYTES_VAR_DEREF_CODEC_NAME_DAT,
-                                  Lucene40DocValuesFormat.BYTES_VAR_DEREF_VERSION_START,
-                                  Lucene40DocValuesFormat.BYTES_VAR_DEREF_VERSION_CURRENT);
-      index = dir.openInput(indexName, state.context);
-      CodecUtil.checkHeader(index, Lucene40DocValuesFormat.BYTES_VAR_DEREF_CODEC_NAME_IDX,
-                                   Lucene40DocValuesFormat.BYTES_VAR_DEREF_VERSION_START,
-                                   Lucene40DocValuesFormat.BYTES_VAR_DEREF_VERSION_CURRENT);
-
-      final long totalBytes = index.readLong();
-      final PagedBytes bytes = new PagedBytes(16);
-      bytes.copy(data, totalBytes);
-      final PagedBytes.Reader bytesReader = bytes.freeze(true);
-      final PackedInts.Reader reader = PackedInts.getReader(index);
-      CodecUtil.checkEOF(data);
-      CodecUtil.checkEOF(index);
-      long bytesUsed = bytesReader.ramBytesUsed() + reader.ramBytesUsed();
-      if (!merging) {
-        ramBytesUsed.addAndGet(bytesUsed);
-        instanceInfo.put(field.name, Accountables.namedAccountable("variable deref", bytesUsed));
-      }
-      success = true;
-      return new BinaryDocValues() {
-        
-        @Override
-        public BytesRef get(int docID) {
-          final BytesRef term = new BytesRef();
-          long startAddress = reader.get(docID);
-          BytesRef lengthBytes = new BytesRef();
-          bytesReader.fillSlice(lengthBytes, startAddress, 1);
-          byte code = lengthBytes.bytes[lengthBytes.offset];
-          if ((code & 128) == 0) {
-            // length is 1 byte
-            bytesReader.fillSlice(term, startAddress + 1, (int) code);
-          } else {
-            bytesReader.fillSlice(lengthBytes, startAddress + 1, 1);
-            int length = ((code & 0x7f) << 8) | (lengthBytes.bytes[lengthBytes.offset] & 0xff);
-            bytesReader.fillSlice(term, startAddress + 2, length);
-          }
-          return term;
-        }
-      };
-    } finally {
-      if (success) {
-        IOUtils.close(data, index);
-      } else {
-        IOUtils.closeWhileHandlingException(data, index);
-      }
-    }
-  }
-
-  @Override
-  public synchronized SortedDocValues getSorted(FieldInfo field) throws IOException {
-    SortedDocValues instance = sortedInstances.get(field.name);
-    if (instance == null) {
-      String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
-      String indexName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "idx");
-      IndexInput data = null;
-      IndexInput index = null;
-      boolean success = false;
-      try {
-        data = dir.openInput(dataName, state.context);
-        index = dir.openInput(indexName, state.context);
-        switch(LegacyDocValuesType.valueOf(field.getAttribute(legacyKey))) {
-          case BYTES_FIXED_SORTED:
-            instance = loadBytesFixedSorted(field, data, index);
-            break;
-          case BYTES_VAR_SORTED:
-            instance = loadBytesVarSorted(field, data, index);
-            break;
-          default:
-            throw new AssertionError();
-        }
-        CodecUtil.checkEOF(data);
-        CodecUtil.checkEOF(index);
-        success = true;
-      } finally {
-        if (success) {
-          IOUtils.close(data, index);
-        } else {
-          IOUtils.closeWhileHandlingException(data, index);
-        }
-      }
-      if (!merging) {
-        sortedInstances.put(field.name, instance);
-      }
-    }
-    return instance;
-  }
-
-  private SortedDocValues loadBytesFixedSorted(FieldInfo field, IndexInput data, IndexInput index) throws IOException {
-    CodecUtil.checkHeader(data, Lucene40DocValuesFormat.BYTES_FIXED_SORTED_CODEC_NAME_DAT,
-                                Lucene40DocValuesFormat.BYTES_FIXED_SORTED_VERSION_START,
-                                Lucene40DocValuesFormat.BYTES_FIXED_SORTED_VERSION_CURRENT);
-    CodecUtil.checkHeader(index, Lucene40DocValuesFormat.BYTES_FIXED_SORTED_CODEC_NAME_IDX,
-                                 Lucene40DocValuesFormat.BYTES_FIXED_SORTED_VERSION_START,
-                                 Lucene40DocValuesFormat.BYTES_FIXED_SORTED_VERSION_CURRENT);
-
-    final int fixedLength = data.readInt();
-    final int valueCount = index.readInt();
-
-    PagedBytes bytes = new PagedBytes(16);
-    bytes.copy(data, fixedLength * (long) valueCount);
-    final PagedBytes.Reader bytesReader = bytes.freeze(true);
-    final PackedInts.Reader reader = PackedInts.getReader(index);
-    long bytesUsed = bytesReader.ramBytesUsed() + reader.ramBytesUsed();
-    if (!merging) {
-      ramBytesUsed.addAndGet(bytesUsed);
-      instanceInfo.put(field.name, Accountables.namedAccountable("fixed sorted", bytesUsed));
-    }
-
-    return correctBuggyOrds(new SortedDocValues() {
-      @Override
-      public int getOrd(int docID) {
-        return (int) reader.get(docID);
-      }
-
-      @Override
-      public BytesRef lookupOrd(int ord) {
-        final BytesRef term = new BytesRef();
-        bytesReader.fillSlice(term, fixedLength * (long) ord, fixedLength);
-        return term;
-      }
-
-      @Override
-      public int getValueCount() {
-        return valueCount;
-      }
-    });
-  }
-
-  private SortedDocValues loadBytesVarSorted(FieldInfo field, IndexInput data, IndexInput index) throws IOException {
-    CodecUtil.checkHeader(data, Lucene40DocValuesFormat.BYTES_VAR_SORTED_CODEC_NAME_DAT,
-                                Lucene40DocValuesFormat.BYTES_VAR_SORTED_VERSION_START,
-                                Lucene40DocValuesFormat.BYTES_VAR_SORTED_VERSION_CURRENT);
-    CodecUtil.checkHeader(index, Lucene40DocValuesFormat.BYTES_VAR_SORTED_CODEC_NAME_IDX,
-                                 Lucene40DocValuesFormat.BYTES_VAR_SORTED_VERSION_START,
-                                 Lucene40DocValuesFormat.BYTES_VAR_SORTED_VERSION_CURRENT);
-
-    long maxAddress = index.readLong();
-    PagedBytes bytes = new PagedBytes(16);
-    bytes.copy(data, maxAddress);
-    final PagedBytes.Reader bytesReader = bytes.freeze(true);
-    final PackedInts.Reader addressReader = PackedInts.getReader(index);
-    final PackedInts.Reader ordsReader = PackedInts.getReader(index);
-
-    final int valueCount = addressReader.size() - 1;
-    long bytesUsed = bytesReader.ramBytesUsed() + addressReader.ramBytesUsed() + ordsReader.ramBytesUsed();
-    if (!merging) {
-      ramBytesUsed.addAndGet(bytesUsed);
-      instanceInfo.put(field.name, Accountables.namedAccountable("var sorted", bytesUsed));
-    }
-
-    return correctBuggyOrds(new SortedDocValues() {
-      @Override
-      public int getOrd(int docID) {
-        return (int)ordsReader.get(docID);
-      }
-
-      @Override
-      public BytesRef lookupOrd(int ord) {
-        final BytesRef term = new BytesRef();
-        long startAddress = addressReader.get(ord);
-        long endAddress = addressReader.get(ord+1);
-        bytesReader.fillSlice(term, startAddress, (int)(endAddress - startAddress));
-        return term;
-      }
-
-      @Override
-      public int getValueCount() {
-        return valueCount;
-      }
-    });
-  }
-
-  // detects and corrects LUCENE-4717 in old indexes
-  private SortedDocValues correctBuggyOrds(final SortedDocValues in) {
-    final int maxDoc = state.segmentInfo.getDocCount();
-    for (int i = 0; i < maxDoc; i++) {
-      if (in.getOrd(i) == 0) {
-        return in; // ok
-      }
-    }
-
-    // we had ord holes, return an ord-shifting-impl that corrects the bug
-    return new SortedDocValues() {
-      @Override
-      public int getOrd(int docID) {
-        return in.getOrd(docID) - 1;
-      }
-
-      @Override
-      public BytesRef lookupOrd(int ord) {
-        return in.lookupOrd(ord+1);
-      }
-
-      @Override
-      public int getValueCount() {
-        return in.getValueCount() - 1;
-      }
-    };
-  }
-  
-  @Override
-  public SortedNumericDocValues getSortedNumeric(FieldInfo field) throws IOException {
-    throw new IllegalStateException("Lucene 4.0 does not support SortedNumeric: how did you pull this off?");
-  }
-
-  @Override
-  public SortedSetDocValues getSortedSet(FieldInfo field) throws IOException {
-    throw new IllegalStateException("Lucene 4.0 does not support SortedSet: how did you pull this off?");
-  }
-
-  @Override
-  public Bits getDocsWithField(FieldInfo field) throws IOException {
-    return new Bits.MatchAllBits(state.segmentInfo.getDocCount());
-  }
-
-  @Override
-  public void close() throws IOException {
-    dir.close();
-  }
-
-  @Override
-  public long ramBytesUsed() {
-    return ramBytesUsed.get();
-  }
-  
-  @Override
-  public synchronized Iterable<? extends Accountable> getChildResources() {
-    return Accountables.namedAccountables("field", instanceInfo);
-  }
-
-  @Override
-  public void checkIntegrity() throws IOException {
-  }
-
-  @Override
-  public synchronized DocValuesProducer getMergeInstance() throws IOException {
-    return new Lucene40DocValuesReader(this);
-  }
-
-  @Override
-  public String toString() {
-    return getClass().getSimpleName();
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosFormat.java
deleted file mode 100644
index 26a8e04..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosFormat.java
+++ /dev/null
@@ -1,176 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.Map;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.UndeadNormsProducer;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfo.DocValuesType;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Lucene 4.0 Field Infos format.
- * @deprecated Only for reading old 4.0 and 4.1 segments
- */
-@Deprecated
-public class Lucene40FieldInfosFormat extends FieldInfosFormat {
-  
-  /** Sole constructor. */
-  public Lucene40FieldInfosFormat() {
-  }
-
-  @Override
-  public final FieldInfos read(Directory directory, SegmentInfo segmentInfo, String segmentSuffix, IOContext iocontext) throws IOException {
-    final String fileName = IndexFileNames.segmentFileName(segmentInfo.name, "", Lucene40FieldInfosFormat.FIELD_INFOS_EXTENSION);
-    IndexInput input = directory.openInput(fileName, iocontext);
-    
-    boolean success = false;
-    try {
-      CodecUtil.checkHeader(input, Lucene40FieldInfosFormat.CODEC_NAME, 
-                                   Lucene40FieldInfosFormat.FORMAT_START, 
-                                   Lucene40FieldInfosFormat.FORMAT_CURRENT);
-
-      final int size = input.readVInt(); //read in the size
-      FieldInfo infos[] = new FieldInfo[size];
-
-      for (int i = 0; i < size; i++) {
-        String name = input.readString();
-        final int fieldNumber = input.readVInt();
-        byte bits = input.readByte();
-        boolean isIndexed = (bits & Lucene40FieldInfosFormat.IS_INDEXED) != 0;
-        boolean storeTermVector = (bits & Lucene40FieldInfosFormat.STORE_TERMVECTOR) != 0;
-        boolean omitNorms = (bits & Lucene40FieldInfosFormat.OMIT_NORMS) != 0;
-        boolean storePayloads = (bits & Lucene40FieldInfosFormat.STORE_PAYLOADS) != 0;
-        final IndexOptions indexOptions;
-        if (!isIndexed) {
-          indexOptions = null;
-        } else if ((bits & Lucene40FieldInfosFormat.OMIT_TERM_FREQ_AND_POSITIONS) != 0) {
-          indexOptions = IndexOptions.DOCS_ONLY;
-        } else if ((bits & Lucene40FieldInfosFormat.OMIT_POSITIONS) != 0) {
-          indexOptions = IndexOptions.DOCS_AND_FREQS;
-        } else if ((bits & Lucene40FieldInfosFormat.STORE_OFFSETS_IN_POSTINGS) != 0) {
-          indexOptions = IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;
-        } else {
-          indexOptions = IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
-        }
-
-        // LUCENE-3027: past indices were able to write
-        // storePayloads=true when omitTFAP is also true,
-        // which is invalid.  We correct that, here:
-        if (isIndexed && indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) < 0) {
-          storePayloads = false;
-        }
-        // DV Types are packed in one byte
-        byte val = input.readByte();
-        final LegacyDocValuesType oldValuesType = getDocValuesType((byte) (val & 0x0F));
-        final LegacyDocValuesType oldNormsType = getDocValuesType((byte) ((val >>> 4) & 0x0F));
-        final Map<String,String> attributes = input.readStringStringMap();
-        if (oldValuesType.mapping != null) {
-          attributes.put(LEGACY_DV_TYPE_KEY, oldValuesType.name());
-        }
-        if (oldNormsType.mapping != null) {
-          if (oldNormsType.mapping != DocValuesType.NUMERIC) {
-            throw new CorruptIndexException("invalid norm type: " + oldNormsType, input);
-          }
-          attributes.put(LEGACY_NORM_TYPE_KEY, oldNormsType.name());
-        }
-        if (isIndexed && omitNorms == false && oldNormsType.mapping == null) {
-          // Undead norms!  Lucene40NormsReader will check this and bring norms back from the dead:
-          UndeadNormsProducer.setUndead(attributes);
-        }
-        infos[i] = new FieldInfo(name, fieldNumber, storeTermVector, 
-          omitNorms, storePayloads, indexOptions, oldValuesType.mapping, -1, Collections.unmodifiableMap(attributes));
-      }
-
-      CodecUtil.checkEOF(input);
-      FieldInfos fieldInfos = new FieldInfos(infos);
-      success = true;
-      return fieldInfos;
-    } finally {
-      if (success) {
-        input.close();
-      } else {
-        IOUtils.closeWhileHandlingException(input);
-      }
-    }
-  }
-  
-  static final String LEGACY_DV_TYPE_KEY = Lucene40FieldInfosFormat.class.getSimpleName() + ".dvtype";
-  static final String LEGACY_NORM_TYPE_KEY = Lucene40FieldInfosFormat.class.getSimpleName() + ".normtype";
-  
-  // mapping of 4.0 types -> 4.2 types
-  static enum LegacyDocValuesType {
-    NONE(null),
-    VAR_INTS(DocValuesType.NUMERIC),
-    FLOAT_32(DocValuesType.NUMERIC),
-    FLOAT_64(DocValuesType.NUMERIC),
-    BYTES_FIXED_STRAIGHT(DocValuesType.BINARY),
-    BYTES_FIXED_DEREF(DocValuesType.BINARY),
-    BYTES_VAR_STRAIGHT(DocValuesType.BINARY),
-    BYTES_VAR_DEREF(DocValuesType.BINARY),
-    FIXED_INTS_16(DocValuesType.NUMERIC),
-    FIXED_INTS_32(DocValuesType.NUMERIC),
-    FIXED_INTS_64(DocValuesType.NUMERIC),
-    FIXED_INTS_8(DocValuesType.NUMERIC),
-    BYTES_FIXED_SORTED(DocValuesType.SORTED),
-    BYTES_VAR_SORTED(DocValuesType.SORTED);
-    
-    final DocValuesType mapping;
-    LegacyDocValuesType(DocValuesType mapping) {
-      this.mapping = mapping;
-    }
-  }
-  
-  // decodes a 4.0 type
-  private static LegacyDocValuesType getDocValuesType(byte b) {
-    return LegacyDocValuesType.values()[b];
-  }
-  
-  @Override
-  public void write(Directory directory, SegmentInfo segmentInfo, String segmentSuffix, FieldInfos infos, IOContext context) throws IOException {
-    throw new UnsupportedOperationException("this codec can only be used for reading");
-  }
-
-  /** Extension of field infos */
-  static final String FIELD_INFOS_EXTENSION = "fnm";
-  
-  static final String CODEC_NAME = "Lucene40FieldInfos";
-  static final int FORMAT_START = 0;
-  static final int FORMAT_CURRENT = FORMAT_START;
-  
-  static final byte IS_INDEXED = 0x1;
-  static final byte STORE_TERMVECTOR = 0x2;
-  static final byte STORE_OFFSETS_IN_POSTINGS = 0x4;
-  static final byte OMIT_NORMS = 0x10;
-  static final byte STORE_PAYLOADS = 0x20;
-  static final byte OMIT_TERM_FREQ_AND_POSITIONS = 0x40;
-  static final byte OMIT_POSITIONS = -128;
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40LiveDocsFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40LiveDocsFormat.java
deleted file mode 100644
index e30cb8d..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40LiveDocsFormat.java
+++ /dev/null
@@ -1,91 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collection;
-
-import org.apache.lucene.codecs.LiveDocsFormat;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentCommitInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.MutableBits;
-
-/**
- * Lucene 4.0 Live Documents Format.
- * @deprecated Only for reading old 4.x segments
- */
-@Deprecated
-public final class Lucene40LiveDocsFormat extends LiveDocsFormat {
-
-  /** Extension of deletes */
-  static final String DELETES_EXTENSION = "del";
-
-  /** Sole constructor. */
-  public Lucene40LiveDocsFormat() {
-  }
-  
-  @Override
-  public MutableBits newLiveDocs(int size) throws IOException {
-    BitVector bitVector = new BitVector(size);
-    bitVector.invertAll();
-    return bitVector;
-  }
-
-  @Override
-  public MutableBits newLiveDocs(Bits existing) throws IOException {
-    final BitVector liveDocs = (BitVector) existing;
-    return liveDocs.clone();
-  }
-
-  @Override
-  public Bits readLiveDocs(Directory dir, SegmentCommitInfo info, IOContext context) throws IOException {
-    String filename = IndexFileNames.fileNameFromGeneration(info.info.name, DELETES_EXTENSION, info.getDelGen());
-    final BitVector liveDocs = new BitVector(dir, filename, context);
-    if (liveDocs.length() != info.info.getDocCount()) {
-      throw new CorruptIndexException("liveDocs.length()=" + liveDocs.length() + "info.docCount=" + info.info.getDocCount(), filename);
-    }
-    if (liveDocs.count() != info.info.getDocCount() - info.getDelCount()) {
-      throw new CorruptIndexException("liveDocs.count()=" + liveDocs.count() + " info.docCount=" + info.info.getDocCount() + " info.getDelCount()=" + info.getDelCount(), filename);
-    }
-    return liveDocs;
-  }
-
-  @Override
-  public void writeLiveDocs(MutableBits bits, Directory dir, SegmentCommitInfo info, int newDelCount, IOContext context) throws IOException {
-    String filename = IndexFileNames.fileNameFromGeneration(info.info.name, DELETES_EXTENSION, info.getNextDelGen());
-    final BitVector liveDocs = (BitVector) bits;
-    if (liveDocs.length() != info.info.getDocCount()) {
-      throw new CorruptIndexException("liveDocs.length()=" + liveDocs.length() + "info.docCount=" + info.info.getDocCount(), filename);
-    }
-    if (liveDocs.count() != info.info.getDocCount() - info.getDelCount() - newDelCount) {
-      throw new CorruptIndexException("liveDocs.count()=" + liveDocs.count() + " info.docCount=" + info.info.getDocCount() + " info.getDelCount()=" + info.getDelCount() + " newDelCount=" + newDelCount, filename);
-    }
-    liveDocs.write(dir, filename, context);
-  }
-
-  @Override
-  public void files(SegmentCommitInfo info, Collection<String> files) throws IOException {
-    if (info.hasDeletions()) {
-      files.add(IndexFileNames.fileNameFromGeneration(info.info.name, DELETES_EXTENSION, info.getDelGen()));
-    }
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsFormat.java
deleted file mode 100644
index 7a84fbb..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsFormat.java
+++ /dev/null
@@ -1,56 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.NormsConsumer;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.NormsProducer;
-import org.apache.lucene.codecs.UndeadNormsProducer;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-
-/**
- * Lucene 4.0 Norms Format.
- * @deprecated Only for reading old 4.0 and 4.1 segments
- */
-@Deprecated
-public class Lucene40NormsFormat extends NormsFormat {
-
-  /** Sole constructor. */
-  public Lucene40NormsFormat() {}
-  
-  @Override
-  public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
-    throw new UnsupportedOperationException("this codec can only be used for reading");
-  }
-
-  @Override
-  public NormsProducer normsProducer(SegmentReadState state) throws IOException {
-    String filename = IndexFileNames.segmentFileName(state.segmentInfo.name, 
-                                                     "nrm", 
-                                                     Lucene40CompoundFormat.COMPOUND_FILE_EXTENSION);
-    if (UndeadNormsProducer.isUndeadArmy(state.fieldInfos)) {
-      return UndeadNormsProducer.INSTANCE;
-    } else {
-      return new Lucene40NormsReader(state, filename);
-    }
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsReader.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsReader.java
deleted file mode 100644
index 490c43a..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsReader.java
+++ /dev/null
@@ -1,86 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DocValuesProducer;
-import org.apache.lucene.codecs.NormsProducer;
-import org.apache.lucene.codecs.UndeadNormsProducer;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.NumericDocValues;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.util.Accountable;
-
-/**
- * Reads 4.0/4.1 norms.
- * @deprecated Only for reading old 4.0 and 4.1 segments
- */
-@Deprecated
-final class Lucene40NormsReader extends NormsProducer {
-  private final DocValuesProducer impl;
-  
-  // clone for merge
-  Lucene40NormsReader(DocValuesProducer impl) throws IOException {
-    this.impl = impl.getMergeInstance();
-  }
-  
-  Lucene40NormsReader(SegmentReadState state, String filename) throws IOException {
-    impl = new Lucene40DocValuesReader(state, filename, Lucene40FieldInfosFormat.LEGACY_NORM_TYPE_KEY);
-  }
-  
-  @Override
-  public NumericDocValues getNorms(FieldInfo field) throws IOException {
-    if (UndeadNormsProducer.isUndead(field)) {
-      // Bring undead norms back to life; this is set in Lucene40FieldInfosFormat, to emulate pre-5.0 undead norms
-      return DocValues.emptyNumeric();
-    }
-    return impl.getNumeric(field);
-  }
-  
-  @Override
-  public void close() throws IOException {
-    impl.close();
-  }
-
-  @Override
-  public long ramBytesUsed() {
-    return impl.ramBytesUsed();
-  }
-  
-  @Override
-  public Iterable<? extends Accountable> getChildResources() {
-    return impl.getChildResources();
-  }
-
-  @Override
-  public void checkIntegrity() throws IOException {
-    impl.checkIntegrity();
-  }
-  
-  @Override
-  public NormsProducer getMergeInstance() throws IOException {
-    return new Lucene40NormsReader(impl);
-  }
-
-  @Override
-  public String toString() {
-    return getClass().getSimpleName() + "(" + impl + ")";
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.java
deleted file mode 100644
index eeb7ca0..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.java
+++ /dev/null
@@ -1,74 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.FieldsConsumer;
-import org.apache.lucene.codecs.FieldsProducer;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.PostingsReaderBase;
-import org.apache.lucene.codecs.blocktree.Lucene40BlockTreeTermsReader;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-
-/** 
- * Lucene 4.0 Postings format.
- * @deprecated Only for reading old 4.0 segments 
- */
-@Deprecated
-public class Lucene40PostingsFormat extends PostingsFormat {
-
-  /** Creates {@code Lucene40PostingsFormat} with default
-   *  settings. */
-  public Lucene40PostingsFormat() {
-    super("Lucene40");
-  }
-
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    throw new UnsupportedOperationException("this codec can only be used for reading");
-  }
-
-  @Override
-  public final FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    PostingsReaderBase postings = new Lucene40PostingsReader(state.directory, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
-
-    boolean success = false;
-    try {
-      FieldsProducer ret = new Lucene40BlockTreeTermsReader(postings, state);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        postings.close();
-      }
-    }
-  }
-
-  /** Extension of freq postings file */
-  static final String FREQ_EXTENSION = "frq";
-
-  /** Extension of prox postings file */
-  static final String PROX_EXTENSION = "prx";
-
-  @Override
-  public String toString() {
-    return getName();
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsReader.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsReader.java
deleted file mode 100644
index d60b15b..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsReader.java
+++ /dev/null
@@ -1,1179 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.Collections;
-
-import org.apache.lucene.codecs.BlockTermState;
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.PostingsReaderBase;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.TermState;
-import org.apache.lucene.store.DataInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.Accountable;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.BytesRefBuilder;
-import org.apache.lucene.util.IOUtils;
-
-/** 
- * Reader for 4.0 postings format
- * @deprecated Only for reading old 4.0 segments */
-@Deprecated
-final class Lucene40PostingsReader extends PostingsReaderBase {
-
-  final static String TERMS_CODEC = "Lucene40PostingsWriterTerms";
-  final static String FRQ_CODEC = "Lucene40PostingsWriterFrq";
-  final static String PRX_CODEC = "Lucene40PostingsWriterPrx";
-
-  //private static boolean DEBUG = BlockTreeTermsWriter.DEBUG;
-  
-  // Increment version to change it:
-  final static int VERSION_START = 0;
-  final static int VERSION_LONG_SKIP = 1;
-  final static int VERSION_CURRENT = VERSION_LONG_SKIP;
-
-  private final IndexInput freqIn;
-  private final IndexInput proxIn;
-  // public static boolean DEBUG = BlockTreeTermsWriter.DEBUG;
-
-  int skipInterval;
-  int maxSkipLevels;
-  int skipMinimum;
-
-  // private String segment;
-
-  /** Sole constructor. */
-  public Lucene40PostingsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo segmentInfo, IOContext ioContext, String segmentSuffix) throws IOException {
-    boolean success = false;
-    IndexInput freqIn = null;
-    IndexInput proxIn = null;
-    try {
-      freqIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, Lucene40PostingsFormat.FREQ_EXTENSION),
-                           ioContext);
-      CodecUtil.checkHeader(freqIn, FRQ_CODEC, VERSION_START, VERSION_CURRENT);
-      // TODO: hasProx should (somehow!) become codec private,
-      // but it's tricky because 1) FIS.hasProx is global (it
-      // could be all fields that have prox are written by a
-      // different codec), 2) the field may have had prox in
-      // the past but all docs w/ that field were deleted.
-      // Really we'd need to init prxOut lazily on write, and
-      // then somewhere record that we actually wrote it so we
-      // know whether to open on read:
-      if (fieldInfos.hasProx()) {
-        proxIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, Lucene40PostingsFormat.PROX_EXTENSION),
-                             ioContext);
-        CodecUtil.checkHeader(proxIn, PRX_CODEC, VERSION_START, VERSION_CURRENT);
-      } else {
-        proxIn = null;
-      }
-      this.freqIn = freqIn;
-      this.proxIn = proxIn;
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(freqIn, proxIn);
-      }
-    }
-  }
-
-  @Override
-  public void init(IndexInput termsIn, SegmentReadState state) throws IOException {
-
-    // Make sure we are talking to the matching past writer
-    CodecUtil.checkHeader(termsIn, TERMS_CODEC, VERSION_START, VERSION_CURRENT);
-
-    skipInterval = termsIn.readInt();
-    maxSkipLevels = termsIn.readInt();
-    skipMinimum = termsIn.readInt();
-  }
-
-  // Must keep final because we do non-standard clone
-  private final static class StandardTermState extends BlockTermState {
-    long freqOffset;
-    long proxOffset;
-    long skipOffset;
-
-    @Override
-    public StandardTermState clone() {
-      StandardTermState other = new StandardTermState();
-      other.copyFrom(this);
-      return other;
-    }
-
-    @Override
-    public void copyFrom(TermState _other) {
-      super.copyFrom(_other);
-      StandardTermState other = (StandardTermState) _other;
-      freqOffset = other.freqOffset;
-      proxOffset = other.proxOffset;
-      skipOffset = other.skipOffset;
-    }
-
-    @Override
-    public String toString() {
-      return super.toString() + " freqFP=" + freqOffset + " proxFP=" + proxOffset + " skipOffset=" + skipOffset;
-    }
-  }
-
-  @Override
-  public BlockTermState newTermState() {
-    return new StandardTermState();
-  }
-
-  @Override
-  public void close() throws IOException {
-    try {
-      if (freqIn != null) {
-        freqIn.close();
-      }
-    } finally {
-      if (proxIn != null) {
-        proxIn.close();
-      }
-    }
-  }
-
-  @Override
-  public void decodeTerm(long[] longs, DataInput in, FieldInfo fieldInfo, BlockTermState _termState, boolean absolute)
-    throws IOException {
-    final StandardTermState termState = (StandardTermState) _termState;
-    // if (DEBUG) System.out.println("SPR: nextTerm seg=" + segment + " tbOrd=" + termState.termBlockOrd + " bytesReader.fp=" + termState.bytesReader.getPosition());
-    final boolean isFirstTerm = termState.termBlockOrd == 0;
-    if (absolute) {
-      termState.freqOffset = 0;
-      termState.proxOffset = 0;
-    }
-
-    termState.freqOffset += in.readVLong();
-    /*
-    if (DEBUG) {
-      System.out.println("  dF=" + termState.docFreq);
-      System.out.println("  freqFP=" + termState.freqOffset);
-    }
-    */
-    assert termState.freqOffset < freqIn.length();
-
-    if (termState.docFreq >= skipMinimum) {
-      termState.skipOffset = in.readVLong();
-      // if (DEBUG) System.out.println("  skipOffset=" + termState.skipOffset + " vs freqIn.length=" + freqIn.length());
-      assert termState.freqOffset + termState.skipOffset < freqIn.length();
-    } else {
-      // undefined
-    }
-
-    if (fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) {
-      termState.proxOffset += in.readVLong();
-      // if (DEBUG) System.out.println("  proxFP=" + termState.proxOffset);
-    }
-  }
-    
-  @Override
-  public DocsEnum docs(FieldInfo fieldInfo, BlockTermState termState, Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
-    if (canReuse(reuse, liveDocs)) {
-      // if (DEBUG) System.out.println("SPR.docs ts=" + termState);
-      return ((SegmentDocsEnumBase) reuse).reset(fieldInfo, (StandardTermState)termState);
-    }
-    return newDocsEnum(liveDocs, fieldInfo, (StandardTermState)termState);
-  }
-  
-  private boolean canReuse(DocsEnum reuse, Bits liveDocs) {
-    if (reuse != null && (reuse instanceof SegmentDocsEnumBase)) {
-      SegmentDocsEnumBase docsEnum = (SegmentDocsEnumBase) reuse;
-      // If you are using ParellelReader, and pass in a
-      // reused DocsEnum, it could have come from another
-      // reader also using standard codec
-      if (docsEnum.startFreqIn == freqIn) {
-        // we only reuse if the the actual the incoming enum has the same liveDocs as the given liveDocs
-        return liveDocs == docsEnum.liveDocs;
-      }
-    }
-    return false;
-  }
-  
-  private DocsEnum newDocsEnum(Bits liveDocs, FieldInfo fieldInfo, StandardTermState termState) throws IOException {
-    if (liveDocs == null) {
-      return new AllDocsSegmentDocsEnum(freqIn).reset(fieldInfo, termState);
-    } else {
-      return new LiveDocsSegmentDocsEnum(freqIn, liveDocs).reset(fieldInfo, termState);
-    }
-  }
-
-  @Override
-  public DocsAndPositionsEnum docsAndPositions(FieldInfo fieldInfo, BlockTermState termState, Bits liveDocs,
-                                               DocsAndPositionsEnum reuse, int flags)
-    throws IOException {
-
-    boolean hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
-
-    // TODO: can we optimize if FLAG_PAYLOADS / FLAG_OFFSETS
-    // isn't passed?
-
-    // TODO: refactor
-    if (fieldInfo.hasPayloads() || hasOffsets) {
-      SegmentFullPositionsEnum docsEnum;
-      if (reuse == null || !(reuse instanceof SegmentFullPositionsEnum)) {
-        docsEnum = new SegmentFullPositionsEnum(freqIn, proxIn);
-      } else {
-        docsEnum = (SegmentFullPositionsEnum) reuse;
-        if (docsEnum.startFreqIn != freqIn) {
-          // If you are using ParellelReader, and pass in a
-          // reused DocsEnum, it could have come from another
-          // reader also using standard codec
-          docsEnum = new SegmentFullPositionsEnum(freqIn, proxIn);
-        }
-      }
-      return docsEnum.reset(fieldInfo, (StandardTermState) termState, liveDocs);
-    } else {
-      SegmentDocsAndPositionsEnum docsEnum;
-      if (reuse == null || !(reuse instanceof SegmentDocsAndPositionsEnum)) {
-        docsEnum = new SegmentDocsAndPositionsEnum(freqIn, proxIn);
-      } else {
-        docsEnum = (SegmentDocsAndPositionsEnum) reuse;
-        if (docsEnum.startFreqIn != freqIn) {
-          // If you are using ParellelReader, and pass in a
-          // reused DocsEnum, it could have come from another
-          // reader also using standard codec
-          docsEnum = new SegmentDocsAndPositionsEnum(freqIn, proxIn);
-        }
-      }
-      return docsEnum.reset(fieldInfo, (StandardTermState) termState, liveDocs);
-    }
-  }
-
-  static final int BUFFERSIZE = 64;
-  
-  private abstract class SegmentDocsEnumBase extends DocsEnum {
-    
-    protected final int[] docs = new int[BUFFERSIZE];
-    protected final int[] freqs = new int[BUFFERSIZE];
-    
-    final IndexInput freqIn; // reuse
-    final IndexInput startFreqIn; // reuse
-    Lucene40SkipListReader skipper; // reuse - lazy loaded
-    
-    protected boolean indexOmitsTF;                               // does current field omit term freq?
-    protected boolean storePayloads;                        // does current field store payloads?
-    protected boolean storeOffsets;                         // does current field store offsets?
-
-    protected int limit;                                    // number of docs in this posting
-    protected int ord;                                      // how many docs we've read
-    protected int doc;                                 // doc we last read
-    protected int accum;                                    // accumulator for doc deltas
-    protected int freq;                                     // freq we last read
-    protected int maxBufferedDocId;
-    
-    protected int start;
-    protected int count;
-
-
-    protected long freqOffset;
-    protected long skipOffset;
-
-    protected boolean skipped;
-    protected final Bits liveDocs;
-    
-    SegmentDocsEnumBase(IndexInput startFreqIn, Bits liveDocs) {
-      this.startFreqIn = startFreqIn;
-      this.freqIn = startFreqIn.clone();
-      this.liveDocs = liveDocs;
-      
-    }
-    
-    
-    DocsEnum reset(FieldInfo fieldInfo, StandardTermState termState) throws IOException {
-      indexOmitsTF = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY;
-      storePayloads = fieldInfo.hasPayloads();
-      storeOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
-      freqOffset = termState.freqOffset;
-      skipOffset = termState.skipOffset;
-
-      // TODO: for full enum case (eg segment merging) this
-      // seek is unnecessary; maybe we can avoid in such
-      // cases
-      freqIn.seek(termState.freqOffset);
-      limit = termState.docFreq;
-      assert limit > 0;
-      ord = 0;
-      doc = -1;
-      accum = 0;
-      // if (DEBUG) System.out.println("  sde limit=" + limit + " freqFP=" + freqOffset);
-      skipped = false;
-
-      start = -1;
-      count = 0;
-      freq = 1;
-      if (indexOmitsTF) {
-        Arrays.fill(freqs, 1);
-      }
-      maxBufferedDocId = -1;
-      return this;
-    }
-    
-    @Override
-    public final int freq() {
-      return freq;
-    }
-
-    @Override
-    public final int docID() {
-      return doc;
-    }
-    
-    @Override
-    public final int advance(int target) throws IOException {
-      // last doc in our buffer is >= target, binary search + next()
-      if (++start < count && maxBufferedDocId >= target) {
-        if ((count-start) > 32) { // 32 seemed to be a sweetspot here so use binsearch if the pending results are a lot
-          start = binarySearch(count - 1, start, target, docs);
-          return nextDoc();
-        } else {
-          return linearScan(target);
-        }
-      }
-      
-      start = count; // buffer is consumed
-      
-      return doc = skipTo(target);
-    }
-    
-    private final int binarySearch(int hi, int low, int target, int[] docs) {
-      while (low <= hi) {
-        int mid = (hi + low) >>> 1;
-        int doc = docs[mid];
-        if (doc < target) {
-          low = mid + 1;
-        } else if (doc > target) {
-          hi = mid - 1;
-        } else {
-          low = mid;
-          break;
-        }
-      }
-      return low-1;
-    }
-    
-    final int readFreq(final IndexInput freqIn, final int code)
-        throws IOException {
-      if ((code & 1) != 0) { // if low bit is set
-        return 1; // freq is one
-      } else {
-        return freqIn.readVInt(); // else read freq
-      }
-    }
-    
-    protected abstract int linearScan(int scanTo) throws IOException;
-    
-    protected abstract int scanTo(int target) throws IOException;
-
-    protected final int refill() throws IOException {
-      final int doc = nextUnreadDoc();
-      count = 0;
-      start = -1;
-      if (doc == NO_MORE_DOCS) {
-        return NO_MORE_DOCS;
-      }
-      final int numDocs = Math.min(docs.length, limit - ord);
-      ord += numDocs;
-      if (indexOmitsTF) {
-        count = fillDocs(numDocs);
-      } else {
-        count = fillDocsAndFreqs(numDocs);
-      }
-      maxBufferedDocId = count > 0 ? docs[count-1] : NO_MORE_DOCS;
-      return doc;
-    }
-    
-
-    protected abstract int nextUnreadDoc() throws IOException;
-
-
-    private final int fillDocs(int size) throws IOException {
-      final IndexInput freqIn = this.freqIn;
-      final int docs[] = this.docs;
-      int docAc = accum;
-      for (int i = 0; i < size; i++) {
-        docAc += freqIn.readVInt();
-        docs[i] = docAc;
-      }
-      accum = docAc;
-      return size;
-    }
-    
-    private final int fillDocsAndFreqs(int size) throws IOException {
-      final IndexInput freqIn = this.freqIn;
-      final int docs[] = this.docs;
-      final int freqs[] = this.freqs;
-      int docAc = accum;
-      for (int i = 0; i < size; i++) {
-        final int code = freqIn.readVInt();
-        docAc += code >>> 1; // shift off low bit
-        freqs[i] = readFreq(freqIn, code);
-        docs[i] = docAc;
-      }
-      accum = docAc;
-      return size;
-     
-    }
-
-    private final int skipTo(int target) throws IOException {
-      if ((target - skipInterval) >= accum && limit >= skipMinimum) {
-
-        // There are enough docs in the posting to have
-        // skip data, and it isn't too close.
-
-        if (skipper == null) {
-          // This is the first time this enum has ever been used for skipping -- do lazy init
-          skipper = new Lucene40SkipListReader(freqIn.clone(), maxSkipLevels, skipInterval);
-        }
-
-        if (!skipped) {
-
-          // This is the first time this posting has
-          // skipped since reset() was called, so now we
-          // load the skip data for this posting
-
-          skipper.init(freqOffset + skipOffset,
-                       freqOffset, 0,
-                       limit, storePayloads, storeOffsets);
-
-          skipped = true;
-        }
-
-        final int newOrd = skipper.skipTo(target); 
-
-        if (newOrd > ord) {
-          // Skipper moved
-
-          ord = newOrd;
-          accum = skipper.getDoc();
-          freqIn.seek(skipper.getFreqPointer());
-        }
-      }
-      return scanTo(target);
-    }
-    
-    @Override
-    public long cost() {
-      return limit;
-    }
-  }
-  
-  private final class AllDocsSegmentDocsEnum extends SegmentDocsEnumBase {
-
-    AllDocsSegmentDocsEnum(IndexInput startFreqIn) {
-      super(startFreqIn, null);
-      assert liveDocs == null;
-    }
-    
-    @Override
-    public final int nextDoc() throws IOException {
-      if (++start < count) {
-        freq = freqs[start];
-        return doc = docs[start];
-      }
-      return doc = refill();
-    }
-    
-
-    @Override
-    protected final int linearScan(int scanTo) throws IOException {
-      final int[] docs = this.docs;
-      final int upTo = count;
-      for (int i = start; i < upTo; i++) {
-        final int d = docs[i];
-        if (scanTo <= d) {
-          start = i;
-          freq = freqs[i];
-          return doc = docs[i];
-        }
-      }
-      return doc = refill();
-    }
-
-    @Override
-    protected int scanTo(int target) throws IOException { 
-      int docAcc = accum;
-      int frq = 1;
-      final IndexInput freqIn = this.freqIn;
-      final boolean omitTF = indexOmitsTF;
-      final int loopLimit = limit;
-      for (int i = ord; i < loopLimit; i++) {
-        int code = freqIn.readVInt();
-        if (omitTF) {
-          docAcc += code;
-        } else {
-          docAcc += code >>> 1; // shift off low bit
-          frq = readFreq(freqIn, code);
-        }
-        if (docAcc >= target) {
-          freq = frq;
-          ord = i + 1;
-          return accum = docAcc;
-        }
-      }
-      ord = limit;
-      freq = frq;
-      accum = docAcc;
-      return NO_MORE_DOCS;
-    }
-
-    @Override
-    protected final int nextUnreadDoc() throws IOException {
-      if (ord++ < limit) {
-        int code = freqIn.readVInt();
-        if (indexOmitsTF) {
-          accum += code;
-        } else {
-          accum += code >>> 1; // shift off low bit
-          freq = readFreq(freqIn, code);
-        }
-        return accum;
-      } else {
-        return NO_MORE_DOCS;
-      }
-    }
-    
-  }
-  
-  private final class LiveDocsSegmentDocsEnum extends SegmentDocsEnumBase {
-
-    LiveDocsSegmentDocsEnum(IndexInput startFreqIn, Bits liveDocs) {
-      super(startFreqIn, liveDocs);
-      assert liveDocs != null;
-    }
-    
-    @Override
-    public final int nextDoc() throws IOException {
-      final Bits liveDocs = this.liveDocs;
-      for (int i = start+1; i < count; i++) {
-        int d = docs[i];
-        if (liveDocs.get(d)) {
-          start = i;
-          freq = freqs[i];
-          return doc = d;
-        }
-      }
-      start = count;
-      return doc = refill();
-    }
-
-    @Override
-    protected final int linearScan(int scanTo) throws IOException {
-      final int[] docs = this.docs;
-      final int upTo = count;
-      final Bits liveDocs = this.liveDocs;
-      for (int i = start; i < upTo; i++) {
-        int d = docs[i];
-        if (scanTo <= d && liveDocs.get(d)) {
-          start = i;
-          freq = freqs[i];
-          return doc = docs[i];
-        }
-      }
-      return doc = refill();
-    }
-    
-    @Override
-    protected int scanTo(int target) throws IOException { 
-      int docAcc = accum;
-      int frq = 1;
-      final IndexInput freqIn = this.freqIn;
-      final boolean omitTF = indexOmitsTF;
-      final int loopLimit = limit;
-      final Bits liveDocs = this.liveDocs;
-      for (int i = ord; i < loopLimit; i++) {
-        int code = freqIn.readVInt();
-        if (omitTF) {
-          docAcc += code;
-        } else {
-          docAcc += code >>> 1; // shift off low bit
-          frq = readFreq(freqIn, code);
-        }
-        if (docAcc >= target && liveDocs.get(docAcc)) {
-          freq = frq;
-          ord = i + 1;
-          return accum = docAcc;
-        }
-      }
-      ord = limit;
-      freq = frq;
-      accum = docAcc;
-      return NO_MORE_DOCS;
-    }
-
-    @Override
-    protected final int nextUnreadDoc() throws IOException {
-      int docAcc = accum;
-      int frq = 1;
-      final IndexInput freqIn = this.freqIn;
-      final boolean omitTF = indexOmitsTF;
-      final int loopLimit = limit;
-      final Bits liveDocs = this.liveDocs;
-      for (int i = ord; i < loopLimit; i++) {
-        int code = freqIn.readVInt();
-        if (omitTF) {
-          docAcc += code;
-        } else {
-          docAcc += code >>> 1; // shift off low bit
-          frq = readFreq(freqIn, code);
-        }
-        if (liveDocs.get(docAcc)) {
-          freq = frq;
-          ord = i + 1;
-          return accum = docAcc;
-        }
-      }
-      ord = limit;
-      freq = frq;
-      accum = docAcc;
-      return NO_MORE_DOCS;
-      
-    }
-  }
-  
-  // TODO specialize DocsAndPosEnum too
-  
-  // Decodes docs & positions. payloads nor offsets are present.
-  private final class SegmentDocsAndPositionsEnum extends DocsAndPositionsEnum {
-    final IndexInput startFreqIn;
-    private final IndexInput freqIn;
-    private final IndexInput proxIn;
-    int limit;                                    // number of docs in this posting
-    int ord;                                      // how many docs we've read
-    int doc = -1;                                 // doc we last read
-    int accum;                                    // accumulator for doc deltas
-    int freq;                                     // freq we last read
-    int position;
-
-    Bits liveDocs;
-
-    long freqOffset;
-    long skipOffset;
-    long proxOffset;
-
-    int posPendingCount;
-
-    boolean skipped;
-    Lucene40SkipListReader skipper;
-    private long lazyProxPointer;
-
-    public SegmentDocsAndPositionsEnum(IndexInput freqIn, IndexInput proxIn) {
-      startFreqIn = freqIn;
-      this.freqIn = freqIn.clone();
-      this.proxIn = proxIn.clone();
-    }
-
-    public SegmentDocsAndPositionsEnum reset(FieldInfo fieldInfo, StandardTermState termState, Bits liveDocs) throws IOException {
-      assert fieldInfo.getIndexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
-      assert !fieldInfo.hasPayloads();
-
-      this.liveDocs = liveDocs;
-
-      // TODO: for full enum case (eg segment merging) this
-      // seek is unnecessary; maybe we can avoid in such
-      // cases
-      freqIn.seek(termState.freqOffset);
-      lazyProxPointer = termState.proxOffset;
-
-      limit = termState.docFreq;
-      assert limit > 0;
-
-      ord = 0;
-      doc = -1;
-      accum = 0;
-      position = 0;
-
-      skipped = false;
-      posPendingCount = 0;
-
-      freqOffset = termState.freqOffset;
-      proxOffset = termState.proxOffset;
-      skipOffset = termState.skipOffset;
-      // if (DEBUG) System.out.println("StandardR.D&PE reset seg=" + segment + " limit=" + limit + " freqFP=" + freqOffset + " proxFP=" + proxOffset);
-
-      return this;
-    }
-
-    @Override
-    public int nextDoc() throws IOException {
-      // if (DEBUG) System.out.println("SPR.nextDoc seg=" + segment + " freqIn.fp=" + freqIn.getFilePointer());
-      while(true) {
-        if (ord == limit) {
-          // if (DEBUG) System.out.println("  return END");
-          return doc = NO_MORE_DOCS;
-        }
-
-        ord++;
-
-        // Decode next doc/freq pair
-        final int code = freqIn.readVInt();
-
-        accum += code >>> 1;              // shift off low bit
-        if ((code & 1) != 0) {          // if low bit is set
-          freq = 1;                     // freq is one
-        } else {
-          freq = freqIn.readVInt();     // else read freq
-        }
-        posPendingCount += freq;
-
-        if (liveDocs == null || liveDocs.get(accum)) {
-          break;
-        }
-      }
-
-      position = 0;
-
-      // if (DEBUG) System.out.println("  return doc=" + doc);
-      return (doc = accum);
-    }
-
-    @Override
-    public int docID() {
-      return doc;
-    }
-
-    @Override
-    public int freq() {
-      return freq;
-    }
-
-    @Override
-    public int advance(int target) throws IOException {
-
-      //System.out.println("StandardR.D&PE advance target=" + target);
-
-      if ((target - skipInterval) >= doc && limit >= skipMinimum) {
-
-        // There are enough docs in the posting to have
-        // skip data, and it isn't too close
-
-        if (skipper == null) {
-          // This is the first time this enum has ever been used for skipping -- do lazy init
-          skipper = new Lucene40SkipListReader(freqIn.clone(), maxSkipLevels, skipInterval);
-        }
-
-        if (!skipped) {
-
-          // This is the first time this posting has
-          // skipped, since reset() was called, so now we
-          // load the skip data for this posting
-
-          skipper.init(freqOffset+skipOffset,
-                       freqOffset, proxOffset,
-                       limit, false, false);
-
-          skipped = true;
-        }
-
-        final int newOrd = skipper.skipTo(target); 
-
-        if (newOrd > ord) {
-          // Skipper moved
-          ord = newOrd;
-          doc = accum = skipper.getDoc();
-          freqIn.seek(skipper.getFreqPointer());
-          lazyProxPointer = skipper.getProxPointer();
-          posPendingCount = 0;
-          position = 0;
-        }
-      }
-        
-      // Now, linear scan for the rest:
-      do {
-        nextDoc();
-      } while (target > doc);
-
-      return doc;
-    }
-
-    @Override
-    public int nextPosition() throws IOException {
-
-      if (lazyProxPointer != -1) {
-        proxIn.seek(lazyProxPointer);
-        lazyProxPointer = -1;
-      }
-
-      // scan over any docs that were iterated without their positions
-      if (posPendingCount > freq) {
-        position = 0;
-        while(posPendingCount != freq) {
-          if ((proxIn.readByte() & 0x80) == 0) {
-            posPendingCount--;
-          }
-        }
-      }
-
-      position += proxIn.readVInt();
-
-      posPendingCount--;
-
-      assert posPendingCount >= 0: "nextPosition() was called too many times (more than freq() times) posPendingCount=" + posPendingCount;
-
-      return position;
-    }
-
-    @Override
-    public int startOffset() {
-      return -1;
-    }
-
-    @Override
-    public int endOffset() {
-      return -1;
-    }
-
-    /** Returns the payload at this position, or null if no
-     *  payload was indexed. */
-    @Override
-    public BytesRef getPayload() throws IOException {
-      return null;
-    }
-    
-    @Override
-    public long cost() {
-      return limit;
-    }
-  }
-  
-  // Decodes docs & positions & (payloads and/or offsets)
-  private class SegmentFullPositionsEnum extends DocsAndPositionsEnum {
-    final IndexInput startFreqIn;
-    private final IndexInput freqIn;
-    private final IndexInput proxIn;
-
-    int limit;                                    // number of docs in this posting
-    int ord;                                      // how many docs we've read
-    int doc = -1;                                 // doc we last read
-    int accum;                                    // accumulator for doc deltas
-    int freq;                                     // freq we last read
-    int position;
-
-    Bits liveDocs;
-
-    long freqOffset;
-    long skipOffset;
-    long proxOffset;
-
-    int posPendingCount;
-    int payloadLength;
-    boolean payloadPending;
-
-    boolean skipped;
-    Lucene40SkipListReader skipper;
-    private BytesRefBuilder payload;
-    private long lazyProxPointer;
-    
-    boolean storePayloads;
-    boolean storeOffsets;
-    
-    int offsetLength;
-    int startOffset;
-
-    public SegmentFullPositionsEnum(IndexInput freqIn, IndexInput proxIn) {
-      startFreqIn = freqIn;
-      this.freqIn = freqIn.clone();
-      this.proxIn = proxIn.clone();
-    }
-
-    public SegmentFullPositionsEnum reset(FieldInfo fieldInfo, StandardTermState termState, Bits liveDocs) throws IOException {
-      storeOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
-      storePayloads = fieldInfo.hasPayloads();
-      assert fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
-      assert storePayloads || storeOffsets;
-      if (payload == null) {
-        payload = new BytesRefBuilder();
-      }
-
-      this.liveDocs = liveDocs;
-
-      // TODO: for full enum case (eg segment merging) this
-      // seek is unnecessary; maybe we can avoid in such
-      // cases
-      freqIn.seek(termState.freqOffset);
-      lazyProxPointer = termState.proxOffset;
-
-      limit = termState.docFreq;
-      ord = 0;
-      doc = -1;
-      accum = 0;
-      position = 0;
-      startOffset = 0;
-
-      skipped = false;
-      posPendingCount = 0;
-      payloadPending = false;
-
-      freqOffset = termState.freqOffset;
-      proxOffset = termState.proxOffset;
-      skipOffset = termState.skipOffset;
-      //System.out.println("StandardR.D&PE reset seg=" + segment + " limit=" + limit + " freqFP=" + freqOffset + " proxFP=" + proxOffset + " this=" + this);
-
-      return this;
-    }
-
-    @Override
-    public int nextDoc() throws IOException {
-      while(true) {
-        if (ord == limit) {
-          //System.out.println("StandardR.D&PE seg=" + segment + " nextDoc return doc=END");
-          return doc = NO_MORE_DOCS;
-        }
-
-        ord++;
-
-        // Decode next doc/freq pair
-        final int code = freqIn.readVInt();
-
-        accum += code >>> 1; // shift off low bit
-        if ((code & 1) != 0) { // if low bit is set
-          freq = 1; // freq is one
-        } else {
-          freq = freqIn.readVInt(); // else read freq
-        }
-        posPendingCount += freq;
-
-        if (liveDocs == null || liveDocs.get(accum)) {
-          break;
-        }
-      }
-
-      position = 0;
-      startOffset = 0;
-
-      //System.out.println("StandardR.D&PE nextDoc seg=" + segment + " return doc=" + doc);
-      return (doc = accum);
-    }
-
-    @Override
-    public int docID() {
-      return doc;
-    }
-
-    @Override
-    public int freq() throws IOException {
-      return freq;
-    }
-
-    @Override
-    public int advance(int target) throws IOException {
-
-      //System.out.println("StandardR.D&PE advance seg=" + segment + " target=" + target + " this=" + this);
-
-      if ((target - skipInterval) >= doc && limit >= skipMinimum) {
-
-        // There are enough docs in the posting to have
-        // skip data, and it isn't too close
-
-        if (skipper == null) {
-          // This is the first time this enum has ever been used for skipping -- do lazy init
-          skipper = new Lucene40SkipListReader(freqIn.clone(), maxSkipLevels, skipInterval);
-        }
-
-        if (!skipped) {
-
-          // This is the first time this posting has
-          // skipped, since reset() was called, so now we
-          // load the skip data for this posting
-          //System.out.println("  init skipper freqOffset=" + freqOffset + " skipOffset=" + skipOffset + " vs len=" + freqIn.length());
-          skipper.init(freqOffset+skipOffset,
-                       freqOffset, proxOffset,
-                       limit, storePayloads, storeOffsets);
-
-          skipped = true;
-        }
-
-        final int newOrd = skipper.skipTo(target); 
-
-        if (newOrd > ord) {
-          // Skipper moved
-          ord = newOrd;
-          doc = accum = skipper.getDoc();
-          freqIn.seek(skipper.getFreqPointer());
-          lazyProxPointer = skipper.getProxPointer();
-          posPendingCount = 0;
-          position = 0;
-          startOffset = 0;
-          payloadPending = false;
-          payloadLength = skipper.getPayloadLength();
-          offsetLength = skipper.getOffsetLength();
-        }
-      }
-        
-      // Now, linear scan for the rest:
-      do {
-        nextDoc();
-      } while (target > doc);
-
-      return doc;
-    }
-
-    @Override
-    public int nextPosition() throws IOException {
-
-      if (lazyProxPointer != -1) {
-        proxIn.seek(lazyProxPointer);
-        lazyProxPointer = -1;
-      }
-      
-      if (payloadPending && payloadLength > 0) {
-        // payload of last position was never retrieved -- skip it
-        proxIn.seek(proxIn.getFilePointer() + payloadLength);
-        payloadPending = false;
-      }
-
-      // scan over any docs that were iterated without their positions
-      while(posPendingCount > freq) {
-        final int code = proxIn.readVInt();
-
-        if (storePayloads) {
-          if ((code & 1) != 0) {
-            // new payload length
-            payloadLength = proxIn.readVInt();
-            assert payloadLength >= 0;
-          }
-          assert payloadLength != -1;
-        }
-        
-        if (storeOffsets) {
-          if ((proxIn.readVInt() & 1) != 0) {
-            // new offset length
-            offsetLength = proxIn.readVInt();
-          }
-        }
-        
-        if (storePayloads) {
-          proxIn.seek(proxIn.getFilePointer() + payloadLength);
-        }
-
-        posPendingCount--;
-        position = 0;
-        startOffset = 0;
-        payloadPending = false;
-        //System.out.println("StandardR.D&PE skipPos");
-      }
-
-      // read next position
-      if (payloadPending && payloadLength > 0) {
-        // payload wasn't retrieved for last position
-        proxIn.seek(proxIn.getFilePointer()+payloadLength);
-      }
-
-      int code = proxIn.readVInt();
-      if (storePayloads) {
-        if ((code & 1) != 0) {
-          // new payload length
-          payloadLength = proxIn.readVInt();
-          assert payloadLength >= 0;
-        }
-        assert payloadLength != -1;
-          
-        payloadPending = true;
-        code >>>= 1;
-      }
-      position += code;
-      
-      if (storeOffsets) {
-        int offsetCode = proxIn.readVInt();
-        if ((offsetCode & 1) != 0) {
-          // new offset length
-          offsetLength = proxIn.readVInt();
-        }
-        startOffset += offsetCode >>> 1;
-      }
-
-      posPendingCount--;
-
-      assert posPendingCount >= 0: "nextPosition() was called too many times (more than freq() times) posPendingCount=" + posPendingCount;
-
-      //System.out.println("StandardR.D&PE nextPos   return pos=" + position);
-      return position;
-    }
-
-    @Override
-    public int startOffset() throws IOException {
-      return storeOffsets ? startOffset : -1;
-    }
-
-    @Override
-    public int endOffset() throws IOException {
-      return storeOffsets ? startOffset + offsetLength : -1;
-    }
-
-    /** Returns the payload at this position, or null if no
-     *  payload was indexed. */
-    @Override
-    public BytesRef getPayload() throws IOException {
-      if (storePayloads) {
-        if (payloadLength <= 0) {
-          return null;
-        }
-        assert lazyProxPointer == -1;
-        assert posPendingCount < freq;
-        
-        if (payloadPending) {
-          payload.grow(payloadLength);
-
-          proxIn.readBytes(payload.bytes(), 0, payloadLength);
-          payload.setLength(payloadLength);
-          payloadPending = false;
-        }
-
-        return payload.get();
-      } else {
-        return null;
-      }
-    }
-    
-    @Override
-    public long cost() {
-      return limit;
-    }
-  }
-
-  @Override
-  public long ramBytesUsed() {
-    return 0;
-  }
-  
-  @Override
-  public Iterable<? extends Accountable> getChildResources() {
-    return Collections.emptyList();
-  }
-
-  @Override
-  public void checkIntegrity() throws IOException {}
-
-  @Override
-  public String toString() {
-    return getClass().getSimpleName() + "(positions=" + (proxIn != null) + ")";
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoFormat.java
deleted file mode 100644
index 6ec5021..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoFormat.java
+++ /dev/null
@@ -1,99 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.text.ParseException;
-import java.util.Map;
-import java.util.Set;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.SegmentInfoFormat;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.Version;
-
-/**
- * Lucene 4.0 Segment info format.
- * @deprecated Only for reading old 4.0-4.5 segments
- */
-@Deprecated
-public class Lucene40SegmentInfoFormat extends SegmentInfoFormat {
-
-  /** Sole constructor. */
-  public Lucene40SegmentInfoFormat() {
-  }
-  
-  @Override
-  public final SegmentInfo read(Directory dir, String segment, byte segmentID[], IOContext context) throws IOException {
-    final String fileName = IndexFileNames.segmentFileName(segment, "", Lucene40SegmentInfoFormat.SI_EXTENSION);
-    final IndexInput input = dir.openInput(fileName, context);
-    boolean success = false;
-    try {
-      CodecUtil.checkHeader(input, Lucene40SegmentInfoFormat.CODEC_NAME,
-                                   Lucene40SegmentInfoFormat.VERSION_START,
-                                   Lucene40SegmentInfoFormat.VERSION_CURRENT);
-      final Version version;
-      try {
-        version = Version.parse(input.readString());
-      } catch (ParseException pe) {
-        throw new CorruptIndexException("unable to parse version string: " + pe.getMessage(), input, pe);
-      }
-      final int docCount = input.readInt();
-      if (docCount < 0) {
-        throw new CorruptIndexException("invalid docCount: " + docCount, input);
-      }
-      final boolean isCompoundFile = input.readByte() == SegmentInfo.YES;
-      final Map<String,String> diagnostics = input.readStringStringMap();
-      input.readStringStringMap(); // read deprecated attributes
-      final Set<String> files = input.readStringSet();
-      
-      CodecUtil.checkEOF(input);
-
-      final SegmentInfo si = new SegmentInfo(dir, version, segment, docCount, isCompoundFile, null, diagnostics, null);
-      si.setFiles(files);
-
-      success = true;
-
-      return si;
-
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(input);
-      } else {
-        input.close();
-      }
-    }
-  }
-
-  @Override
-  public void write(Directory dir, SegmentInfo info, IOContext ioContext) throws IOException {
-    throw new UnsupportedOperationException("this codec can only be used for reading");
-  }
-
-  /** File extension used to store {@link SegmentInfo}. */
-  static final String SI_EXTENSION = "si";
-  static final String CODEC_NAME = "Lucene40SegmentInfo";
-  static final int VERSION_START = 0;
-  static final int VERSION_CURRENT = VERSION_START;
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40SkipListReader.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40SkipListReader.java
deleted file mode 100644
index 3275fa6..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40SkipListReader.java
+++ /dev/null
@@ -1,140 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.codecs.MultiLevelSkipListReader;
-import org.apache.lucene.store.IndexInput;
-
-/**
- * Lucene 4.0 skiplist reader
- * @deprecated Only for reading old 4.0 segments
- */
-@Deprecated
-final class Lucene40SkipListReader extends MultiLevelSkipListReader {
-  private boolean currentFieldStoresPayloads;
-  private boolean currentFieldStoresOffsets;
-  private long freqPointer[];
-  private long proxPointer[];
-  private int payloadLength[];
-  private int offsetLength[];
-  
-  private long lastFreqPointer;
-  private long lastProxPointer;
-  private int lastPayloadLength;
-  private int lastOffsetLength;
-                           
-  /** Sole constructor. */
-  public Lucene40SkipListReader(IndexInput skipStream, int maxSkipLevels, int skipInterval) {
-    super(skipStream, maxSkipLevels, skipInterval);
-    freqPointer = new long[maxSkipLevels];
-    proxPointer = new long[maxSkipLevels];
-    payloadLength = new int[maxSkipLevels];
-    offsetLength = new int[maxSkipLevels];
-  }
-
-  /** Per-term initialization. */
-  public void init(long skipPointer, long freqBasePointer, long proxBasePointer, int df, boolean storesPayloads, boolean storesOffsets) {
-    super.init(skipPointer, df);
-    this.currentFieldStoresPayloads = storesPayloads;
-    this.currentFieldStoresOffsets = storesOffsets;
-    lastFreqPointer = freqBasePointer;
-    lastProxPointer = proxBasePointer;
-
-    Arrays.fill(freqPointer, freqBasePointer);
-    Arrays.fill(proxPointer, proxBasePointer);
-    Arrays.fill(payloadLength, 0);
-    Arrays.fill(offsetLength, 0);
-  }
-
-  /** Returns the freq pointer of the doc to which the last call of 
-   * {@link MultiLevelSkipListReader#skipTo(int)} has skipped.  */
-  public long getFreqPointer() {
-    return lastFreqPointer;
-  }
-
-  /** Returns the prox pointer of the doc to which the last call of 
-   * {@link MultiLevelSkipListReader#skipTo(int)} has skipped.  */
-  public long getProxPointer() {
-    return lastProxPointer;
-  }
-  
-  /** Returns the payload length of the payload stored just before 
-   * the doc to which the last call of {@link MultiLevelSkipListReader#skipTo(int)} 
-   * has skipped.  */
-  public int getPayloadLength() {
-    return lastPayloadLength;
-  }
-  
-  /** Returns the offset length (endOffset-startOffset) of the position stored just before 
-   * the doc to which the last call of {@link MultiLevelSkipListReader#skipTo(int)} 
-   * has skipped.  */
-  public int getOffsetLength() {
-    return lastOffsetLength;
-  }
-  
-  @Override
-  protected void seekChild(int level) throws IOException {
-    super.seekChild(level);
-    freqPointer[level] = lastFreqPointer;
-    proxPointer[level] = lastProxPointer;
-    payloadLength[level] = lastPayloadLength;
-    offsetLength[level] = lastOffsetLength;
-  }
-  
-  @Override
-  protected void setLastSkipData(int level) {
-    super.setLastSkipData(level);
-    lastFreqPointer = freqPointer[level];
-    lastProxPointer = proxPointer[level];
-    lastPayloadLength = payloadLength[level];
-    lastOffsetLength = offsetLength[level];
-  }
-
-
-  @Override
-  protected int readSkipData(int level, IndexInput skipStream) throws IOException {
-    int delta;
-    if (currentFieldStoresPayloads || currentFieldStoresOffsets) {
-      // the current field stores payloads and/or offsets.
-      // if the doc delta is odd then we have
-      // to read the current payload/offset lengths
-      // because it differs from the lengths of the
-      // previous payload/offset
-      delta = skipStream.readVInt();
-      if ((delta & 1) != 0) {
-        if (currentFieldStoresPayloads) {
-          payloadLength[level] = skipStream.readVInt();
-        }
-        if (currentFieldStoresOffsets) {
-          offsetLength[level] = skipStream.readVInt();
-        }
-      }
-      delta >>>= 1;
-    } else {
-      delta = skipStream.readVInt();
-    }
-
-    freqPointer[level] += skipStream.readVInt();
-    proxPointer[level] += skipStream.readVInt();
-    
-    return delta;
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsFormat.java
deleted file mode 100644
index 35dee65..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsFormat.java
+++ /dev/null
@@ -1,51 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.StoredFieldsFormat;
-import org.apache.lucene.codecs.StoredFieldsReader;
-import org.apache.lucene.codecs.StoredFieldsWriter;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-/** 
- * Lucene 4.0 Stored Fields Format.
- * @deprecated only for reading 4.0 segments */
-@Deprecated
-public class Lucene40StoredFieldsFormat extends StoredFieldsFormat {
-
-  /** Sole constructor. */
-  public Lucene40StoredFieldsFormat() {
-  }
-
-  @Override
-  public final StoredFieldsReader fieldsReader(Directory directory, SegmentInfo si,
-      FieldInfos fn, IOContext context) throws IOException {
-    return new Lucene40StoredFieldsReader(directory, si, fn, context);
-  }
-
-  @Override
-  public StoredFieldsWriter fieldsWriter(Directory directory, SegmentInfo si,
-      IOContext context) throws IOException {
-    throw new UnsupportedOperationException("this codec can only be used for reading");
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsReader.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsReader.java
deleted file mode 100644
index d8e3fcd..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsReader.java
+++ /dev/null
@@ -1,273 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.StoredFieldsReader;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.StoredFieldVisitor;
-import org.apache.lucene.store.AlreadyClosedException;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.Accountable;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.RamUsageEstimator;
-
-import java.io.Closeable;
-import java.nio.charset.StandardCharsets;
-import java.util.Collections;
-
-/**
- * Reader for 4.0 stored fields
- * @deprecated only for reading 4.0 segments
- */
-@Deprecated
-final class Lucene40StoredFieldsReader extends StoredFieldsReader implements Cloneable, Closeable {
-
-  // NOTE: bit 0 is free here!  You can steal it!
-  static final int FIELD_IS_BINARY = 1 << 1;
-
-  // the old bit 1 << 2 was compressed, is now left out
-
-  private static final int _NUMERIC_BIT_SHIFT = 3;
-  static final int FIELD_IS_NUMERIC_MASK = 0x07 << _NUMERIC_BIT_SHIFT;
-
-  static final int FIELD_IS_NUMERIC_INT = 1 << _NUMERIC_BIT_SHIFT;
-  static final int FIELD_IS_NUMERIC_LONG = 2 << _NUMERIC_BIT_SHIFT;
-  static final int FIELD_IS_NUMERIC_FLOAT = 3 << _NUMERIC_BIT_SHIFT;
-  static final int FIELD_IS_NUMERIC_DOUBLE = 4 << _NUMERIC_BIT_SHIFT;
-
-  // the next possible bits are: 1 << 6; 1 << 7
-  // currently unused: static final int FIELD_IS_NUMERIC_SHORT = 5 << _NUMERIC_BIT_SHIFT;
-  // currently unused: static final int FIELD_IS_NUMERIC_BYTE = 6 << _NUMERIC_BIT_SHIFT;
-
-  static final String CODEC_NAME_IDX = "Lucene40StoredFieldsIndex";
-  static final String CODEC_NAME_DAT = "Lucene40StoredFieldsData";
-  static final int VERSION_START = 0;
-  static final int VERSION_CURRENT = VERSION_START;
-  static final long HEADER_LENGTH_IDX = CodecUtil.headerLength(CODEC_NAME_IDX);
-  static final long HEADER_LENGTH_DAT = CodecUtil.headerLength(CODEC_NAME_DAT);
-
-
-
-  /** Extension of stored fields file */
-  static final String FIELDS_EXTENSION = "fdt";
-  
-  /** Extension of stored fields index file */
-  static final String FIELDS_INDEX_EXTENSION = "fdx";
-  
-  private static final long RAM_BYTES_USED = RamUsageEstimator.shallowSizeOfInstance(Lucene40StoredFieldsReader.class);
-
-  private final FieldInfos fieldInfos;
-  private final IndexInput fieldsStream;
-  private final IndexInput indexStream;
-  private int numTotalDocs;
-  private int size;
-  private boolean closed;
-
-  /** Returns a cloned FieldsReader that shares open
-   *  IndexInputs with the original one.  It is the caller's
-   *  job not to close the original FieldsReader until all
-   *  clones are called (eg, currently SegmentReader manages
-   *  this logic). */
-  @Override
-  public Lucene40StoredFieldsReader clone() {
-    ensureOpen();
-    return new Lucene40StoredFieldsReader(fieldInfos, numTotalDocs, size, fieldsStream.clone(), indexStream.clone());
-  }
-  
-  /** Used only by clone. */
-  private Lucene40StoredFieldsReader(FieldInfos fieldInfos, int numTotalDocs, int size, IndexInput fieldsStream, IndexInput indexStream) {
-    this.fieldInfos = fieldInfos;
-    this.numTotalDocs = numTotalDocs;
-    this.size = size;
-    this.fieldsStream = fieldsStream;
-    this.indexStream = indexStream;
-  }
-
-  /** Sole constructor. */
-  public Lucene40StoredFieldsReader(Directory d, SegmentInfo si, FieldInfos fn, IOContext context) throws IOException {
-    final String segment = si.name;
-    boolean success = false;
-    fieldInfos = fn;
-    try {
-      fieldsStream = d.openInput(IndexFileNames.segmentFileName(segment, "", FIELDS_EXTENSION), context);
-      final String indexStreamFN = IndexFileNames.segmentFileName(segment, "", FIELDS_INDEX_EXTENSION);
-      indexStream = d.openInput(indexStreamFN, context);
-      
-      CodecUtil.checkHeader(indexStream, CODEC_NAME_IDX, VERSION_START, VERSION_CURRENT);
-      CodecUtil.checkHeader(fieldsStream, CODEC_NAME_DAT, VERSION_START, VERSION_CURRENT);
-      assert HEADER_LENGTH_DAT == fieldsStream.getFilePointer();
-      assert HEADER_LENGTH_IDX == indexStream.getFilePointer();
-      final long indexSize = indexStream.length() - HEADER_LENGTH_IDX;
-      this.size = (int) (indexSize >> 3);
-      // Verify two sources of "maxDoc" agree:
-      if (this.size != si.getDocCount()) {
-        throw new CorruptIndexException("doc counts differ for segment " + segment + ": fieldsReader shows " + this.size + " but segmentInfo shows " + si.getDocCount(), indexStream);
-      }
-      numTotalDocs = (int) (indexSize >> 3);
-      success = true;
-    } finally {
-      // With lock-less commits, it's entirely possible (and
-      // fine) to hit a FileNotFound exception above. In
-      // this case, we want to explicitly close any subset
-      // of things that were opened so that we don't have to
-      // wait for a GC to do so.
-      if (!success) {
-        try {
-          close();
-        } catch (Throwable t) {} // ensure we throw our original exception
-      }
-    }
-  }
-
-  /**
-   * @throws AlreadyClosedException if this FieldsReader is closed
-   */
-  private void ensureOpen() throws AlreadyClosedException {
-    if (closed) {
-      throw new AlreadyClosedException("this FieldsReader is closed");
-    }
-  }
-
-  /**
-   * Closes the underlying {@link org.apache.lucene.store.IndexInput} streams.
-   * This means that the Fields values will not be accessible.
-   *
-   * @throws IOException If an I/O error occurs
-   */
-  @Override
-  public final void close() throws IOException {
-    if (!closed) {
-      IOUtils.close(fieldsStream, indexStream);
-      closed = true;
-    }
-  }
-
-  /** Returns number of documents. */
-  public final int size() {
-    return size;
-  }
-
-  private void seekIndex(int docID) throws IOException {
-    indexStream.seek(HEADER_LENGTH_IDX + docID * 8L);
-  }
-
-  @Override
-  public final void visitDocument(int n, StoredFieldVisitor visitor) throws IOException {
-    seekIndex(n);
-    fieldsStream.seek(indexStream.readLong());
-
-    final int numFields = fieldsStream.readVInt();
-    for (int fieldIDX = 0; fieldIDX < numFields; fieldIDX++) {
-      int fieldNumber = fieldsStream.readVInt();
-      FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);
-      
-      int bits = fieldsStream.readByte() & 0xFF;
-      assert bits <= (FIELD_IS_NUMERIC_MASK | FIELD_IS_BINARY): "bits=" + Integer.toHexString(bits);
-
-      switch(visitor.needsField(fieldInfo)) {
-        case YES:
-          readField(visitor, fieldInfo, bits);
-          break;
-        case NO: 
-          skipField(bits);
-          break;
-        case STOP: 
-          return;
-      }
-    }
-  }
-
-  private void readField(StoredFieldVisitor visitor, FieldInfo info, int bits) throws IOException {
-    final int numeric = bits & FIELD_IS_NUMERIC_MASK;
-    if (numeric != 0) {
-      switch(numeric) {
-        case FIELD_IS_NUMERIC_INT:
-          visitor.intField(info, fieldsStream.readInt());
-          return;
-        case FIELD_IS_NUMERIC_LONG:
-          visitor.longField(info, fieldsStream.readLong());
-          return;
-        case FIELD_IS_NUMERIC_FLOAT:
-          visitor.floatField(info, Float.intBitsToFloat(fieldsStream.readInt()));
-          return;
-        case FIELD_IS_NUMERIC_DOUBLE:
-          visitor.doubleField(info, Double.longBitsToDouble(fieldsStream.readLong()));
-          return;
-        default:
-          throw new CorruptIndexException("Invalid numeric type: " + Integer.toHexString(numeric), fieldsStream);
-      }
-    } else { 
-      final int length = fieldsStream.readVInt();
-      byte bytes[] = new byte[length];
-      fieldsStream.readBytes(bytes, 0, length);
-      if ((bits & FIELD_IS_BINARY) != 0) {
-        visitor.binaryField(info, bytes);
-      } else {
-        visitor.stringField(info, new String(bytes, 0, bytes.length, StandardCharsets.UTF_8));
-      }
-    }
-  }
-  
-  private void skipField(int bits) throws IOException {
-    final int numeric = bits & FIELD_IS_NUMERIC_MASK;
-    if (numeric != 0) {
-      switch(numeric) {
-        case FIELD_IS_NUMERIC_INT:
-        case FIELD_IS_NUMERIC_FLOAT:
-          fieldsStream.readInt();
-          return;
-        case FIELD_IS_NUMERIC_LONG:
-        case FIELD_IS_NUMERIC_DOUBLE:
-          fieldsStream.readLong();
-          return;
-        default: 
-          throw new CorruptIndexException("Invalid numeric type: " + Integer.toHexString(numeric), fieldsStream);
-      }
-    } else {
-      final int length = fieldsStream.readVInt();
-      fieldsStream.seek(fieldsStream.getFilePointer() + length);
-    }
-  }
-
-  @Override
-  public long ramBytesUsed() {
-    return RAM_BYTES_USED;
-  }
-  
-  @Override
-  public Iterable<? extends Accountable> getChildResources() {
-    return Collections.emptyList();
-  }
-
-  @Override
-  public void checkIntegrity() throws IOException {}
-
-  @Override
-  public String toString() {
-    return getClass().getSimpleName();
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsFormat.java
deleted file mode 100644
index 93d2f8b..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsFormat.java
+++ /dev/null
@@ -1,50 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.codecs.TermVectorsReader;
-import org.apache.lucene.codecs.TermVectorsWriter;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-/**
- * Lucene 4.0 Term Vectors format.
- * @deprecated only for reading 4.0 and 4.1 segments
- */
-@Deprecated
-public class Lucene40TermVectorsFormat extends TermVectorsFormat {
-
-  /** Sole constructor. */
-  public Lucene40TermVectorsFormat() {
-  }
-  
-  @Override
-  public final TermVectorsReader vectorsReader(Directory directory, SegmentInfo segmentInfo, FieldInfos fieldInfos, IOContext context) throws IOException {
-    return new Lucene40TermVectorsReader(directory, segmentInfo, fieldInfos, context);
-  }
-
-  @Override
-  public TermVectorsWriter vectorsWriter(Directory directory, SegmentInfo segmentInfo, IOContext context) throws IOException {
-    throw new UnsupportedOperationException("this codec can only be used for reading");
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsReader.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsReader.java
deleted file mode 100644
index 2287c2a..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsReader.java
+++ /dev/null
@@ -1,727 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable;
-import java.io.IOException;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.NoSuchElementException;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.TermVectorsReader;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.Accountable;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.BytesRefBuilder;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Lucene 4.0 Term Vectors reader.
- * @deprecated only for reading 4.0 and 4.1 segments
- */
-@Deprecated
-final class Lucene40TermVectorsReader extends TermVectorsReader implements Closeable {
-
-  static final byte STORE_POSITIONS_WITH_TERMVECTOR = 0x1;
-
-  static final byte STORE_OFFSET_WITH_TERMVECTOR = 0x2;
-  
-  static final byte STORE_PAYLOAD_WITH_TERMVECTOR = 0x4;
-  
-  /** Extension of vectors fields file */
-  static final String VECTORS_FIELDS_EXTENSION = "tvf";
-
-  /** Extension of vectors documents file */
-  static final String VECTORS_DOCUMENTS_EXTENSION = "tvd";
-
-  /** Extension of vectors index file */
-  static final String VECTORS_INDEX_EXTENSION = "tvx";
-  
-  static final String CODEC_NAME_FIELDS = "Lucene40TermVectorsFields";
-  static final String CODEC_NAME_DOCS = "Lucene40TermVectorsDocs";
-  static final String CODEC_NAME_INDEX = "Lucene40TermVectorsIndex";
-
-  static final int VERSION_NO_PAYLOADS = 0;
-  static final int VERSION_PAYLOADS = 1;
-  static final int VERSION_START = VERSION_NO_PAYLOADS;
-  static final int VERSION_CURRENT = VERSION_PAYLOADS;
-  
-  static final long HEADER_LENGTH_FIELDS = CodecUtil.headerLength(CODEC_NAME_FIELDS);
-  static final long HEADER_LENGTH_DOCS = CodecUtil.headerLength(CODEC_NAME_DOCS);
-  static final long HEADER_LENGTH_INDEX = CodecUtil.headerLength(CODEC_NAME_INDEX);
-
-  private FieldInfos fieldInfos;
-
-  private IndexInput tvx;
-  private IndexInput tvd;
-  private IndexInput tvf;
-  private int size;
-  private int numTotalDocs;
-  
-
-  /** Used by clone. */
-  Lucene40TermVectorsReader(FieldInfos fieldInfos, IndexInput tvx, IndexInput tvd, IndexInput tvf, int size, int numTotalDocs) {
-    this.fieldInfos = fieldInfos;
-    this.tvx = tvx;
-    this.tvd = tvd;
-    this.tvf = tvf;
-    this.size = size;
-    this.numTotalDocs = numTotalDocs;
-  }
-    
-  /** Sole constructor. */
-  public Lucene40TermVectorsReader(Directory d, SegmentInfo si, FieldInfos fieldInfos, IOContext context)
-    throws IOException {
-    final String segment = si.name;
-    final int size = si.getDocCount();
-    
-    boolean success = false;
-
-    try {
-      String idxName = IndexFileNames.segmentFileName(segment, "", VECTORS_INDEX_EXTENSION);
-      tvx = d.openInput(idxName, context);
-      final int tvxVersion = CodecUtil.checkHeader(tvx, CODEC_NAME_INDEX, VERSION_START, VERSION_CURRENT);
-      
-      String fn = IndexFileNames.segmentFileName(segment, "", VECTORS_DOCUMENTS_EXTENSION);
-      tvd = d.openInput(fn, context);
-      final int tvdVersion = CodecUtil.checkHeader(tvd, CODEC_NAME_DOCS, VERSION_START, VERSION_CURRENT);
-      fn = IndexFileNames.segmentFileName(segment, "", VECTORS_FIELDS_EXTENSION);
-      tvf = d.openInput(fn, context);
-      final int tvfVersion = CodecUtil.checkHeader(tvf, CODEC_NAME_FIELDS, VERSION_START, VERSION_CURRENT);
-      assert HEADER_LENGTH_INDEX == tvx.getFilePointer();
-      assert HEADER_LENGTH_DOCS == tvd.getFilePointer();
-      assert HEADER_LENGTH_FIELDS == tvf.getFilePointer();
-      if (tvxVersion != tvdVersion) {
-        throw new CorruptIndexException("version mismatch: tvx=" + tvxVersion + " != tvd=" + tvdVersion, tvd);
-      }
-      if (tvxVersion != tvfVersion) {
-        throw new CorruptIndexException("version mismatch: tvx=" + tvxVersion + " != tvf=" + tvfVersion, tvf);
-      }
-
-      numTotalDocs = (int) (tvx.length()-HEADER_LENGTH_INDEX >> 4);
-
-      this.size = numTotalDocs;
-      assert size == 0 || numTotalDocs == size;
-
-      this.fieldInfos = fieldInfos;
-      success = true;
-    } finally {
-      // With lock-less commits, it's entirely possible (and
-      // fine) to hit a FileNotFound exception above. In
-      // this case, we want to explicitly close any subset
-      // of things that were opened so that we don't have to
-      // wait for a GC to do so.
-      if (!success) {
-        try {
-          close();
-        } catch (Throwable t) {} // ensure we throw our original exception
-      }
-    }
-  }
-
-  // Not private to avoid synthetic access$NNN methods
-  void seekTvx(final int docNum) throws IOException {
-    tvx.seek(docNum * 16L + HEADER_LENGTH_INDEX);
-  }
-
-  @Override
-  public void close() throws IOException {
-    IOUtils.close(tvx, tvd, tvf);
-  }
-
-  /**
-   * 
-   * @return The number of documents in the reader
-   */
-  int size() {
-    return size;
-  }
-
-  private class TVFields extends Fields {
-    private final int[] fieldNumbers;
-    private final long[] fieldFPs;
-    private final Map<Integer,Integer> fieldNumberToIndex = new HashMap<>();
-
-    public TVFields(int docID) throws IOException {
-      seekTvx(docID);
-      tvd.seek(tvx.readLong());
-      
-      final int fieldCount = tvd.readVInt();
-      assert fieldCount >= 0;
-      if (fieldCount != 0) {
-        fieldNumbers = new int[fieldCount];
-        fieldFPs = new long[fieldCount];
-        for(int fieldUpto=0;fieldUpto<fieldCount;fieldUpto++) {
-          final int fieldNumber = tvd.readVInt();
-          fieldNumbers[fieldUpto] = fieldNumber;
-          fieldNumberToIndex.put(fieldNumber, fieldUpto);
-        }
-
-        long position = tvx.readLong();
-        fieldFPs[0] = position;
-        for(int fieldUpto=1;fieldUpto<fieldCount;fieldUpto++) {
-          position += tvd.readVLong();
-          fieldFPs[fieldUpto] = position;
-        }
-      } else {
-        // TODO: we can improve writer here, eg write 0 into
-        // tvx file, so we know on first read from tvx that
-        // this doc has no TVs
-        fieldNumbers = null;
-        fieldFPs = null;
-      }
-    }
-    
-    @Override
-    public Iterator<String> iterator() {
-      return new Iterator<String>() {
-        private int fieldUpto;
-
-        @Override
-        public String next() {
-          if (fieldNumbers != null && fieldUpto < fieldNumbers.length) {
-            return fieldInfos.fieldInfo(fieldNumbers[fieldUpto++]).name;
-          } else {
-            throw new NoSuchElementException();
-          }
-        }
-
-        @Override
-        public boolean hasNext() {
-          return fieldNumbers != null && fieldUpto < fieldNumbers.length;
-        }
-
-        @Override
-        public void remove() {
-          throw new UnsupportedOperationException();
-        }
-      };
-    }
-
-    @Override
-    public Terms terms(String field) throws IOException {
-      final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
-      if (fieldInfo == null) {
-        // No such field
-        return null;
-      }
-
-      final Integer fieldIndex = fieldNumberToIndex.get(fieldInfo.number);
-      if (fieldIndex == null) {
-        // Term vectors were not indexed for this field
-        return null;
-      }
-
-      return new TVTerms(fieldFPs[fieldIndex]);
-    }
-
-    @Override
-    public int size() {
-      if (fieldNumbers == null) {
-        return 0;
-      } else {
-        return fieldNumbers.length;
-      }
-    }
-  }
-
-  private class TVTerms extends Terms {
-    private final int numTerms;
-    private final long tvfFPStart;
-    private final boolean storePositions;
-    private final boolean storeOffsets;
-    private final boolean storePayloads;
-
-    public TVTerms(long tvfFP) throws IOException {
-      tvf.seek(tvfFP);
-      numTerms = tvf.readVInt();
-      final byte bits = tvf.readByte();
-      storePositions = (bits & STORE_POSITIONS_WITH_TERMVECTOR) != 0;
-      storeOffsets = (bits & STORE_OFFSET_WITH_TERMVECTOR) != 0;
-      storePayloads = (bits & STORE_PAYLOAD_WITH_TERMVECTOR) != 0;
-      tvfFPStart = tvf.getFilePointer();
-    }
-
-    @Override
-    public TermsEnum iterator(TermsEnum reuse) throws IOException {
-      TVTermsEnum termsEnum;
-      if (reuse instanceof TVTermsEnum) {
-        termsEnum = (TVTermsEnum) reuse;
-        if (!termsEnum.canReuse(tvf)) {
-          termsEnum = new TVTermsEnum();
-        }
-      } else {
-        termsEnum = new TVTermsEnum();
-      }
-      termsEnum.reset(numTerms, tvfFPStart, storePositions, storeOffsets, storePayloads);
-      return termsEnum;
-    }
-
-    @Override
-    public long size() {
-      return numTerms;
-    }
-
-    @Override
-    public long getSumTotalTermFreq() {
-      return -1;
-    }
-
-    @Override
-    public long getSumDocFreq() {
-      // Every term occurs in just one doc:
-      return numTerms;
-    }
-
-    @Override
-    public int getDocCount() {
-      return 1;
-    }
-
-    @Override
-    public boolean hasFreqs() {
-      return true;
-    }
-
-    @Override
-    public boolean hasOffsets() {
-      return storeOffsets;
-    }
-
-    @Override
-    public boolean hasPositions() {
-      return storePositions;
-    }
-    
-    @Override
-    public boolean hasPayloads() {
-      return storePayloads;
-    }
-  }
-
-  private class TVTermsEnum extends TermsEnum {
-    private final IndexInput origTVF;
-    private final IndexInput tvf;
-    private int numTerms;
-    private int nextTerm;
-    private int freq;
-    private BytesRefBuilder lastTerm = new BytesRefBuilder();
-    private BytesRefBuilder term = new BytesRefBuilder();
-    private boolean storePositions;
-    private boolean storeOffsets;
-    private boolean storePayloads;
-    private long tvfFP;
-
-    private int[] positions;
-    private int[] startOffsets;
-    private int[] endOffsets;
-    
-    // one shared byte[] for any term's payloads
-    private int[] payloadOffsets;
-    private int lastPayloadLength;
-    private byte[] payloadData;
-
-    // NOTE: tvf is pre-positioned by caller
-    public TVTermsEnum() {
-      this.origTVF = Lucene40TermVectorsReader.this.tvf;
-      tvf = origTVF.clone();
-    }
-
-    public boolean canReuse(IndexInput tvf) {
-      return tvf == origTVF;
-    }
-
-    public void reset(int numTerms, long tvfFPStart, boolean storePositions, boolean storeOffsets, boolean storePayloads) throws IOException {
-      this.numTerms = numTerms;
-      this.storePositions = storePositions;
-      this.storeOffsets = storeOffsets;
-      this.storePayloads = storePayloads;
-      nextTerm = 0;
-      tvf.seek(tvfFPStart);
-      tvfFP = tvfFPStart;
-      positions = null;
-      startOffsets = null;
-      endOffsets = null;
-      payloadOffsets = null;
-      payloadData = null;
-      lastPayloadLength = -1;
-    }
-
-    // NOTE: slow!  (linear scan)
-    @Override
-    public SeekStatus seekCeil(BytesRef text)
-      throws IOException {
-      if (nextTerm != 0) {
-        final int cmp = text.compareTo(term.get());
-        if (cmp < 0) {
-          nextTerm = 0;
-          tvf.seek(tvfFP);
-        } else if (cmp == 0) {
-          return SeekStatus.FOUND;
-        }
-      }
-
-      while (next() != null) {
-        final int cmp = text.compareTo(term.get());
-        if (cmp < 0) {
-          return SeekStatus.NOT_FOUND;
-        } else if (cmp == 0) {
-          return SeekStatus.FOUND;
-        }
-      }
-
-      return SeekStatus.END;
-    }
-
-    @Override
-    public void seekExact(long ord) {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public BytesRef next() throws IOException {
-      if (nextTerm >= numTerms) {
-        return null;
-      }
-      term.copyBytes(lastTerm.get());
-      final int start = tvf.readVInt();
-      final int deltaLen = tvf.readVInt();
-      term.setLength(start + deltaLen);
-      term.grow(term.length());
-      tvf.readBytes(term.bytes(), start, deltaLen);
-      freq = tvf.readVInt();
-
-      if (storePayloads) {
-        positions = new int[freq];
-        payloadOffsets = new int[freq];
-        int totalPayloadLength = 0;
-        int pos = 0;
-        for(int posUpto=0;posUpto<freq;posUpto++) {
-          int code = tvf.readVInt();
-          pos += code >>> 1;
-          positions[posUpto] = pos;
-          if ((code & 1) != 0) {
-            // length change
-            lastPayloadLength = tvf.readVInt();
-          }
-          payloadOffsets[posUpto] = totalPayloadLength;
-          totalPayloadLength += lastPayloadLength;
-          assert totalPayloadLength >= 0;
-        }
-        payloadData = new byte[totalPayloadLength];
-        tvf.readBytes(payloadData, 0, payloadData.length);
-      } else if (storePositions /* no payloads */) {
-        // TODO: we could maybe reuse last array, if we can
-        // somehow be careful about consumer never using two
-        // D&PEnums at once...
-        positions = new int[freq];
-        int pos = 0;
-        for(int posUpto=0;posUpto<freq;posUpto++) {
-          pos += tvf.readVInt();
-          positions[posUpto] = pos;
-        }
-      }
-
-      if (storeOffsets) {
-        startOffsets = new int[freq];
-        endOffsets = new int[freq];
-        int offset = 0;
-        for(int posUpto=0;posUpto<freq;posUpto++) {
-          startOffsets[posUpto] = offset + tvf.readVInt();
-          offset = endOffsets[posUpto] = startOffsets[posUpto] + tvf.readVInt();
-        }
-      }
-
-      lastTerm.copyBytes(term.get());
-      nextTerm++;
-      return term.get();
-    }
-
-    @Override
-    public BytesRef term() {
-      return term.get();
-    }
-
-    @Override
-    public long ord() {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public int docFreq() {
-      return 1;
-    }
-
-    @Override
-    public long totalTermFreq() {
-      return freq;
-    }
-
-    @Override
-    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags /* ignored */) throws IOException {
-      TVDocsEnum docsEnum;
-      if (reuse != null && reuse instanceof TVDocsEnum) {
-        docsEnum = (TVDocsEnum) reuse;
-      } else {
-        docsEnum = new TVDocsEnum();
-      }
-      docsEnum.reset(liveDocs, freq);
-      return docsEnum;
-    }
-
-    @Override
-    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
-
-      if (!storePositions && !storeOffsets) {
-        return null;
-      }
-      
-      TVDocsAndPositionsEnum docsAndPositionsEnum;
-      if (reuse != null && reuse instanceof TVDocsAndPositionsEnum) {
-        docsAndPositionsEnum = (TVDocsAndPositionsEnum) reuse;
-      } else {
-        docsAndPositionsEnum = new TVDocsAndPositionsEnum();
-      }
-      docsAndPositionsEnum.reset(liveDocs, positions, startOffsets, endOffsets, payloadOffsets, payloadData);
-      return docsAndPositionsEnum;
-    }
-  }
-
-  // NOTE: sort of a silly class, since you can get the
-  // freq() already by TermsEnum.totalTermFreq
-  private static class TVDocsEnum extends DocsEnum {
-    private boolean didNext;
-    private int doc = -1;
-    private int freq;
-    private Bits liveDocs;
-
-    @Override
-    public int freq() throws IOException {
-      return freq;
-    }
-
-    @Override
-    public int docID() {
-      return doc;
-    }
-
-    @Override
-    public int nextDoc() {
-      if (!didNext && (liveDocs == null || liveDocs.get(0))) {
-        didNext = true;
-        return (doc = 0);
-      } else {
-        return (doc = NO_MORE_DOCS);
-      }
-    }
-
-    @Override
-    public int advance(int target) throws IOException {
-      return slowAdvance(target);
-    }
-
-    public void reset(Bits liveDocs, int freq) {
-      this.liveDocs = liveDocs;
-      this.freq = freq;
-      this.doc = -1;
-      didNext = false;
-    }
-    
-    @Override
-    public long cost() {
-      return 1;
-    }
-  }
-
-  private static class TVDocsAndPositionsEnum extends DocsAndPositionsEnum {
-    private boolean didNext;
-    private int doc = -1;
-    private int nextPos;
-    private Bits liveDocs;
-    private int[] positions;
-    private int[] startOffsets;
-    private int[] endOffsets;
-    private int[] payloadOffsets;
-    private BytesRef payload = new BytesRef();
-    private byte[] payloadBytes;
-
-    @Override
-    public int freq() throws IOException {
-      if (positions != null) {
-        return positions.length;
-      } else {
-        assert startOffsets != null;
-        return startOffsets.length;
-      }
-    }
-
-    @Override
-    public int docID() {
-      return doc;
-    }
-
-    @Override
-    public int nextDoc() {
-      if (!didNext && (liveDocs == null || liveDocs.get(0))) {
-        didNext = true;
-        return (doc = 0);
-      } else {
-        return (doc = NO_MORE_DOCS);
-      }
-    }
-
-    @Override
-    public int advance(int target) throws IOException {
-      return slowAdvance(target);
-    }
-
-    public void reset(Bits liveDocs, int[] positions, int[] startOffsets, int[] endOffsets, int[] payloadLengths, byte[] payloadBytes) {
-      this.liveDocs = liveDocs;
-      this.positions = positions;
-      this.startOffsets = startOffsets;
-      this.endOffsets = endOffsets;
-      this.payloadOffsets = payloadLengths;
-      this.payloadBytes = payloadBytes;
-      this.doc = -1;
-      didNext = false;
-      nextPos = 0;
-    }
-
-    @Override
-    public BytesRef getPayload() {
-      if (payloadOffsets == null) {
-        return null;
-      } else {
-        int off = payloadOffsets[nextPos-1];
-        int end = nextPos == payloadOffsets.length ? payloadBytes.length : payloadOffsets[nextPos];
-        if (end - off == 0) {
-          return null;
-        }
-        payload.bytes = payloadBytes;
-        payload.offset = off;
-        payload.length = end - off;
-        return payload;
-      }
-    }
-
-    @Override
-    public int nextPosition() {
-      assert (positions != null && nextPos < positions.length) ||
-        startOffsets != null && nextPos < startOffsets.length;
-
-      if (positions != null) {
-        return positions[nextPos++];
-      } else {
-        nextPos++;
-        return -1;
-      }
-    }
-
-    @Override
-    public int startOffset() {
-      if (startOffsets == null) {
-        return -1;
-      } else {
-        return startOffsets[nextPos-1];
-      }
-    }
-
-    @Override
-    public int endOffset() {
-      if (endOffsets == null) {
-        return -1;
-      } else {
-        return endOffsets[nextPos-1];
-      }
-    }
-    
-    @Override
-    public long cost() {
-      return 1;
-    }
-  }
-
-  @Override
-  public Fields get(int docID) throws IOException {
-    if (tvx != null) {
-      Fields fields = new TVFields(docID);
-      if (fields.size() == 0) {
-        // TODO: we can improve writer here, eg write 0 into
-        // tvx file, so we know on first read from tvx that
-        // this doc has no TVs
-        return null;
-      } else {
-        return fields;
-      }
-    } else {
-      return null;
-    }
-  }
-
-  @Override
-  public TermVectorsReader clone() {
-    IndexInput cloneTvx = null;
-    IndexInput cloneTvd = null;
-    IndexInput cloneTvf = null;
-
-    // These are null when a TermVectorsReader was created
-    // on a segment that did not have term vectors saved
-    if (tvx != null && tvd != null && tvf != null) {
-      cloneTvx = tvx.clone();
-      cloneTvd = tvd.clone();
-      cloneTvf = tvf.clone();
-    }
-    
-    return new Lucene40TermVectorsReader(fieldInfos, cloneTvx, cloneTvd, cloneTvf, size, numTotalDocs);
-  }
-
-  @Override
-  public long ramBytesUsed() {
-    return 0;
-  }
-  
-  @Override
-  public Iterable<? extends Accountable> getChildResources() {
-    return Collections.emptyList();
-  }
-
-  @Override
-  public void checkIntegrity() throws IOException {}
-  
-  @Override
-  public String toString() {
-    return getClass().getSimpleName();
-  }
-}
-
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/package.html b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/package.html
deleted file mode 100644
index 7959cc0..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Lucene 4.0 file format.
-</body>
-</html>
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/ForUtil.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/ForUtil.java
deleted file mode 100644
index cf76197..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/ForUtil.java
+++ /dev/null
@@ -1,247 +0,0 @@
-package org.apache.lucene.codecs.lucene41;
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.store.DataInput;
-import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.packed.PackedInts.Decoder;
-import org.apache.lucene.util.packed.PackedInts.FormatAndBits;
-import org.apache.lucene.util.packed.PackedInts;
-
-import static org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat.BLOCK_SIZE;
-
-/**
- * Lucene 4.1 postings format.
- * @deprecated only for reading old 4.x segments
- */
-@Deprecated
-final class ForUtil {
-
-  /**
-   * Special number of bits per value used whenever all values to encode are equal.
-   */
-  private static final int ALL_VALUES_EQUAL = 0;
-
-  /**
-   * Upper limit of the number of bytes that might be required to stored
-   * <code>BLOCK_SIZE</code> encoded values.
-   */
-  static final int MAX_ENCODED_SIZE = BLOCK_SIZE * 4;
-
-  /**
-   * Upper limit of the number of values that might be decoded in a single call to
-   * {@link #readBlock(IndexInput, byte[], int[])}. Although values after
-   * <code>BLOCK_SIZE</code> are garbage, it is necessary to allocate value buffers
-   * whose size is >= MAX_DATA_SIZE to avoid {@link ArrayIndexOutOfBoundsException}s.
-   */
-  static final int MAX_DATA_SIZE;
-  static {
-    int maxDataSize = 0;
-    for(int version=PackedInts.VERSION_START;version<=PackedInts.VERSION_CURRENT;version++) {
-      for (PackedInts.Format format : PackedInts.Format.values()) {
-        for (int bpv = 1; bpv <= 32; ++bpv) {
-          if (!format.isSupported(bpv)) {
-            continue;
-          }
-          final PackedInts.Decoder decoder = PackedInts.getDecoder(format, version, bpv);
-          final int iterations = computeIterations(decoder);
-          maxDataSize = Math.max(maxDataSize, iterations * decoder.byteValueCount());
-        }
-      }
-    }
-    MAX_DATA_SIZE = maxDataSize;
-  }
-
-  /**
-   * Compute the number of iterations required to decode <code>BLOCK_SIZE</code>
-   * values with the provided {@link Decoder}.
-   */
-  private static int computeIterations(PackedInts.Decoder decoder) {
-    return (int) Math.ceil((float) BLOCK_SIZE / decoder.byteValueCount());
-  }
-
-  /**
-   * Compute the number of bytes required to encode a block of values that require
-   * <code>bitsPerValue</code> bits per value with format <code>format</code>.
-   */
-  private static int encodedSize(PackedInts.Format format, int packedIntsVersion, int bitsPerValue) {
-    final long byteCount = format.byteCount(packedIntsVersion, BLOCK_SIZE, bitsPerValue);
-    assert byteCount >= 0 && byteCount <= Integer.MAX_VALUE : byteCount;
-    return (int) byteCount;
-  }
-
-  private final int[] encodedSizes;
-  private final PackedInts.Encoder[] encoders;
-  private final PackedInts.Decoder[] decoders;
-  private final int[] iterations;
-
-  /**
-   * Create a new {@link ForUtil} instance and save state into <code>out</code>.
-   */
-  ForUtil(float acceptableOverheadRatio, DataOutput out) throws IOException {
-    out.writeVInt(PackedInts.VERSION_CURRENT);
-    encodedSizes = new int[33];
-    encoders = new PackedInts.Encoder[33];
-    decoders = new PackedInts.Decoder[33];
-    iterations = new int[33];
-
-    for (int bpv = 1; bpv <= 32; ++bpv) {
-      final FormatAndBits formatAndBits = PackedInts.fastestFormatAndBits(
-          BLOCK_SIZE, bpv, acceptableOverheadRatio);
-      assert formatAndBits.format.isSupported(formatAndBits.bitsPerValue);
-      assert formatAndBits.bitsPerValue <= 32;
-      encodedSizes[bpv] = encodedSize(formatAndBits.format, PackedInts.VERSION_CURRENT, formatAndBits.bitsPerValue);
-      encoders[bpv] = PackedInts.getEncoder(
-          formatAndBits.format, PackedInts.VERSION_CURRENT, formatAndBits.bitsPerValue);
-      decoders[bpv] = PackedInts.getDecoder(
-          formatAndBits.format, PackedInts.VERSION_CURRENT, formatAndBits.bitsPerValue);
-      iterations[bpv] = computeIterations(decoders[bpv]);
-
-      out.writeVInt(formatAndBits.format.getId() << 5 | (formatAndBits.bitsPerValue - 1));
-    }
-  }
-
-  /**
-   * Restore a {@link ForUtil} from a {@link DataInput}.
-   */
-  ForUtil(DataInput in) throws IOException {
-    int packedIntsVersion = in.readVInt();
-    PackedInts.checkVersion(packedIntsVersion);
-    encodedSizes = new int[33];
-    encoders = new PackedInts.Encoder[33];
-    decoders = new PackedInts.Decoder[33];
-    iterations = new int[33];
-
-    for (int bpv = 1; bpv <= 32; ++bpv) {
-      final int code = in.readVInt();
-      final int formatId = code >>> 5;
-      final int bitsPerValue = (code & 31) + 1;
-
-      final PackedInts.Format format = PackedInts.Format.byId(formatId);
-      assert format.isSupported(bitsPerValue);
-      encodedSizes[bpv] = encodedSize(format, packedIntsVersion, bitsPerValue);
-      encoders[bpv] = PackedInts.getEncoder(
-          format, packedIntsVersion, bitsPerValue);
-      decoders[bpv] = PackedInts.getDecoder(
-          format, packedIntsVersion, bitsPerValue);
-      iterations[bpv] = computeIterations(decoders[bpv]);
-    }
-  }
-
-  /**
-   * Write a block of data (<code>For</code> format).
-   *
-   * @param data     the data to write
-   * @param encoded  a buffer to use to encode data
-   * @param out      the destination output
-   * @throws IOException If there is a low-level I/O error
-   */
-  void writeBlock(int[] data, byte[] encoded, IndexOutput out) throws IOException {
-    if (isAllEqual(data)) {
-      out.writeByte((byte) ALL_VALUES_EQUAL);
-      out.writeVInt(data[0]);
-      return;
-    }
-
-    final int numBits = bitsRequired(data);
-    assert numBits > 0 && numBits <= 32 : numBits;
-    final PackedInts.Encoder encoder = encoders[numBits];
-    final int iters = iterations[numBits];
-    assert iters * encoder.byteValueCount() >= BLOCK_SIZE;
-    final int encodedSize = encodedSizes[numBits];
-    assert iters * encoder.byteBlockCount() >= encodedSize;
-
-    out.writeByte((byte) numBits);
-
-    encoder.encode(data, 0, encoded, 0, iters);
-    out.writeBytes(encoded, encodedSize);
-  }
-
-  /**
-   * Read the next block of data (<code>For</code> format).
-   *
-   * @param in        the input to use to read data
-   * @param encoded   a buffer that can be used to store encoded data
-   * @param decoded   where to write decoded data
-   * @throws IOException If there is a low-level I/O error
-   */
-  void readBlock(IndexInput in, byte[] encoded, int[] decoded) throws IOException {
-    final int numBits = in.readByte();
-    assert numBits <= 32 : numBits;
-
-    if (numBits == ALL_VALUES_EQUAL) {
-      final int value = in.readVInt();
-      Arrays.fill(decoded, 0, BLOCK_SIZE, value);
-      return;
-    }
-
-    final int encodedSize = encodedSizes[numBits];
-    in.readBytes(encoded, 0, encodedSize);
-
-    final PackedInts.Decoder decoder = decoders[numBits];
-    final int iters = iterations[numBits];
-    assert iters * decoder.byteValueCount() >= BLOCK_SIZE;
-
-    decoder.decode(encoded, 0, decoded, 0, iters);
-  }
-
-  /**
-   * Skip the next block of data.
-   *
-   * @param in      the input where to read data
-   * @throws IOException If there is a low-level I/O error
-   */
-  void skipBlock(IndexInput in) throws IOException {
-    final int numBits = in.readByte();
-    if (numBits == ALL_VALUES_EQUAL) {
-      in.readVInt();
-      return;
-    }
-    assert numBits > 0 && numBits <= 32 : numBits;
-    final int encodedSize = encodedSizes[numBits];
-    in.seek(in.getFilePointer() + encodedSize);
-  }
-
-  private static boolean isAllEqual(final int[] data) {
-    final int v = data[0];
-    for (int i = 1; i < BLOCK_SIZE; ++i) {
-      if (data[i] != v) {
-        return false;
-      }
-    }
-    return true;
-  }
-
-  /**
-   * Compute the number of bits required to serialize any of the longs in
-   * <code>data</code>.
-   */
-  private static int bitsRequired(final int[] data) {
-    long or = 0;
-    for (int i = 0; i < BLOCK_SIZE; ++i) {
-      assert data[i] >= 0;
-      or |= data[i];
-    }
-    return PackedInts.bitsRequired(or);
-  }
-
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/IntBlockTermState.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/IntBlockTermState.java
deleted file mode 100644
index b5add1e..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/IntBlockTermState.java
+++ /dev/null
@@ -1,62 +0,0 @@
-package org.apache.lucene.codecs.lucene41;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.BlockTermState;
-import org.apache.lucene.index.TermState;
-
-/**
- * term state for Lucene 4.1 postings format
- * @deprecated only for reading old 4.x segments
- */
-@Deprecated
-final class IntBlockTermState extends BlockTermState {
-  long docStartFP = 0;
-  long posStartFP = 0;
-  long payStartFP = 0;
-  long skipOffset = -1;
-  long lastPosBlockOffset = -1;
-  // docid when there is a single pulsed posting, otherwise -1
-  // freq is always implicitly totalTermFreq in this case.
-  int singletonDocID = -1;
-
-  @Override
-  public IntBlockTermState clone() {
-    IntBlockTermState other = new IntBlockTermState();
-    other.copyFrom(this);
-    return other;
-  }
-
-  @Override
-  public void copyFrom(TermState _other) {
-    super.copyFrom(_other);
-    IntBlockTermState other = (IntBlockTermState) _other;
-    docStartFP = other.docStartFP;
-    posStartFP = other.posStartFP;
-    payStartFP = other.payStartFP;
-    lastPosBlockOffset = other.lastPosBlockOffset;
-    skipOffset = other.skipOffset;
-    singletonDocID = other.singletonDocID;
-  }
-
-
-  @Override
-  public String toString() {
-    return super.toString() + " docStartFP=" + docStartFP + " posStartFP=" + posStartFP + " payStartFP=" + payStartFP + " lastPosBlockOffset=" + lastPosBlockOffset + " singletonDocID=" + singletonDocID;
-  }
-}
\ No newline at end of file
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41Codec.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41Codec.java
deleted file mode 100644
index 5ea3a25..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41Codec.java
+++ /dev/null
@@ -1,121 +0,0 @@
-package org.apache.lucene.codecs.lucene41;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.CompoundFormat;
-import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.LiveDocsFormat;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.SegmentInfoFormat;
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.StoredFieldsFormat;
-import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40CompoundFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40DocValuesFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40NormsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40SegmentInfoFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40TermVectorsFormat;
-import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
-
-/**
- * Implements the Lucene 4.1 index format
- * @deprecated Only for reading old 4.1 segments
- */
-@Deprecated
-public class Lucene41Codec extends Codec {
-  private final StoredFieldsFormat fieldsFormat = new Lucene41StoredFieldsFormat();
-  private final TermVectorsFormat vectorsFormat = new Lucene40TermVectorsFormat();
-  private final FieldInfosFormat fieldInfosFormat = new Lucene40FieldInfosFormat();
-  private final SegmentInfoFormat infosFormat = new Lucene40SegmentInfoFormat();
-  private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat();
-  private final CompoundFormat compoundFormat = new Lucene40CompoundFormat();
-  
-  private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
-    @Override
-    public PostingsFormat getPostingsFormatForField(String field) {
-      return Lucene41Codec.this.getPostingsFormatForField(field);
-    }
-  };
-
-  /** Sole constructor. */
-  public Lucene41Codec() {
-    super("Lucene41");
-  }
-  
-  @Override
-  public StoredFieldsFormat storedFieldsFormat() {
-    return fieldsFormat;
-  }
-  
-  @Override
-  public TermVectorsFormat termVectorsFormat() {
-    return vectorsFormat;
-  }
-
-  @Override
-  public final PostingsFormat postingsFormat() {
-    return postingsFormat;
-  }
-  
-  @Override
-  public FieldInfosFormat fieldInfosFormat() {
-    return fieldInfosFormat;
-  }
-  
-  @Override
-  public SegmentInfoFormat segmentInfoFormat() {
-    return infosFormat;
-  }
-  
-  @Override
-  public final LiveDocsFormat liveDocsFormat() {
-    return liveDocsFormat;
-  }
-  
-  @Override
-  public CompoundFormat compoundFormat() {
-    return compoundFormat;
-  }
-
-  /** Returns the postings format that should be used for writing 
-   *  new segments of <code>field</code>.
-   *  
-   *  The default implementation always returns "Lucene41"
-   */
-  public PostingsFormat getPostingsFormatForField(String field) {
-    return defaultFormat;
-  }
-  
-  @Override
-  public DocValuesFormat docValuesFormat() {
-    return dvFormat;
-  }
-
-  private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene41");
-  private final DocValuesFormat dvFormat = new Lucene40DocValuesFormat();
-  private final NormsFormat normsFormat = new Lucene40NormsFormat();
-
-  @Override
-  public NormsFormat normsFormat() {
-    return normsFormat;
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41PostingsFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41PostingsFormat.java
deleted file mode 100644
index 24217dd..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41PostingsFormat.java
+++ /dev/null
@@ -1,113 +0,0 @@
-package org.apache.lucene.codecs.lucene41;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.FieldsConsumer;
-import org.apache.lucene.codecs.FieldsProducer;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.PostingsReaderBase;
-import org.apache.lucene.codecs.blocktree.Lucene40BlockTreeTermsReader;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Lucene 4.1 postings format.
- * @deprecated only for reading old 4.x segments
- */
-@Deprecated
-public class Lucene41PostingsFormat extends PostingsFormat {
-  /**
-   * Filename extension for document number, frequencies, and skip data.
-   * See chapter: <a href="#Frequencies">Frequencies and Skip Data</a>
-   */
-  public static final String DOC_EXTENSION = "doc";
-
-  /**
-   * Filename extension for positions. 
-   * See chapter: <a href="#Positions">Positions</a>
-   */
-  public static final String POS_EXTENSION = "pos";
-
-  /**
-   * Filename extension for payloads and offsets.
-   * See chapter: <a href="#Payloads">Payloads and Offsets</a>
-   */
-  public static final String PAY_EXTENSION = "pay";
-  
-  /** 
-   * Expert: The maximum number of skip levels. Smaller values result in 
-   * slightly smaller indexes, but slower skipping in big posting lists.
-   */
-  static final int maxSkipLevels = 10;
-
-  final static String TERMS_CODEC = "Lucene41PostingsWriterTerms";
-  final static String DOC_CODEC = "Lucene41PostingsWriterDoc";
-  final static String POS_CODEC = "Lucene41PostingsWriterPos";
-  final static String PAY_CODEC = "Lucene41PostingsWriterPay";
-
-  // Increment version to change it
-  final static int VERSION_START = 0;
-  final static int VERSION_META_ARRAY = 1;
-  final static int VERSION_CHECKSUM = 2;
-  final static int VERSION_CURRENT = VERSION_CHECKSUM;
-
-  /**
-   * Fixed packed block size, number of integers encoded in 
-   * a single packed block.
-   */
-  // NOTE: must be multiple of 64 because of PackedInts long-aligned encoding/decoding
-  public final static int BLOCK_SIZE = 128;
-
-  /** Creates {@code Lucene41PostingsFormat} with default
-   *  settings. */
-  public Lucene41PostingsFormat() {
-    super("Lucene41");
-  }
-
-  @Override
-  public String toString() {
-    return getName() + "(blocksize=" + BLOCK_SIZE + ")";
-  }
-
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    throw new UnsupportedOperationException("this codec can only be used for reading");
-  }
-
-  @Override
-  public final FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    PostingsReaderBase postingsReader = new Lucene41PostingsReader(state.directory,
-                                                                state.fieldInfos,
-                                                                state.segmentInfo,
-                                                                state.context,
-                                                                state.segmentSuffix);
-    boolean success = false;
-    try {
-      FieldsProducer ret = new Lucene40BlockTreeTermsReader(postingsReader, state);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(postingsReader);
-      }
-    }
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41PostingsReader.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41PostingsReader.java
deleted file mode 100644
index 2b7eb57..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41PostingsReader.java
+++ /dev/null
@@ -1,1608 +0,0 @@
-package org.apache.lucene.codecs.lucene41;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import static org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat.BLOCK_SIZE;
-import static org.apache.lucene.codecs.lucene41.ForUtil.MAX_DATA_SIZE;
-import static org.apache.lucene.codecs.lucene41.ForUtil.MAX_ENCODED_SIZE;
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.Collections;
-
-import org.apache.lucene.codecs.BlockTermState;
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.PostingsReaderBase;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.DataInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.Accountable;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.RamUsageEstimator;
-
-/**
- * Lucene 4.1 postings format.
- * @deprecated only for reading old 4.x segments
- */
-@Deprecated
-public final class Lucene41PostingsReader extends PostingsReaderBase {
-
-  private static final long BASE_RAM_BYTES_USED = RamUsageEstimator.shallowSizeOfInstance(Lucene41PostingsReader.class);
-
-  private final IndexInput docIn;
-  private final IndexInput posIn;
-  private final IndexInput payIn;
-
-  private final ForUtil forUtil;
-  private int version;
-
-  // public static boolean DEBUG = false;
-
-  /** Sole constructor. */
-  public Lucene41PostingsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo segmentInfo, IOContext ioContext, String segmentSuffix) throws IOException {
-    boolean success = false;
-    IndexInput docIn = null;
-    IndexInput posIn = null;
-    IndexInput payIn = null;
-    try {
-      docIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, Lucene41PostingsFormat.DOC_EXTENSION),
-                            ioContext);
-      version = CodecUtil.checkHeader(docIn,
-                            Lucene41PostingsFormat.DOC_CODEC,
-                            Lucene41PostingsFormat.VERSION_START,
-                            Lucene41PostingsFormat.VERSION_CURRENT);
-      forUtil = new ForUtil(docIn);
-      
-      if (version >= Lucene41PostingsFormat.VERSION_CHECKSUM) {
-        // NOTE: data file is too costly to verify checksum against all the bytes on open,
-        // but for now we at least verify proper structure of the checksum footer: which looks
-        // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
-        // such as file truncation.
-        CodecUtil.retrieveChecksum(docIn);
-      }
-
-      if (fieldInfos.hasProx()) {
-        posIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, Lucene41PostingsFormat.POS_EXTENSION),
-                              ioContext);
-        CodecUtil.checkHeader(posIn, Lucene41PostingsFormat.POS_CODEC, version, version);
-        
-        if (version >= Lucene41PostingsFormat.VERSION_CHECKSUM) {
-          // NOTE: data file is too costly to verify checksum against all the bytes on open,
-          // but for now we at least verify proper structure of the checksum footer: which looks
-          // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
-          // such as file truncation.
-          CodecUtil.retrieveChecksum(posIn);
-        }
-
-        if (fieldInfos.hasPayloads() || fieldInfos.hasOffsets()) {
-          payIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, Lucene41PostingsFormat.PAY_EXTENSION),
-                                ioContext);
-          CodecUtil.checkHeader(payIn, Lucene41PostingsFormat.PAY_CODEC, version, version);
-          
-          if (version >= Lucene41PostingsFormat.VERSION_CHECKSUM) {
-            // NOTE: data file is too costly to verify checksum against all the bytes on open,
-            // but for now we at least verify proper structure of the checksum footer: which looks
-            // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
-            // such as file truncation.
-            CodecUtil.retrieveChecksum(payIn);
-          }
-        }
-      }
-
-      this.docIn = docIn;
-      this.posIn = posIn;
-      this.payIn = payIn;
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(docIn, posIn, payIn);
-      }
-    }
-  }
-
-  @Override
-  public void init(IndexInput termsIn, SegmentReadState state) throws IOException {
-    // Make sure we are talking to the matching postings writer
-    CodecUtil.checkHeader(termsIn,
-                          Lucene41PostingsFormat.TERMS_CODEC,
-                          Lucene41PostingsFormat.VERSION_START,
-                          Lucene41PostingsFormat.VERSION_CURRENT);
-    final int indexBlockSize = termsIn.readVInt();
-    if (indexBlockSize != BLOCK_SIZE) {
-      throw new IllegalStateException("index-time BLOCK_SIZE (" + indexBlockSize + ") != read-time BLOCK_SIZE (" + BLOCK_SIZE + ")");
-    }
-  }
-
-  /**
-   * Read values that have been written using variable-length encoding instead of bit-packing.
-   */
-  static void readVIntBlock(IndexInput docIn, int[] docBuffer,
-      int[] freqBuffer, int num, boolean indexHasFreq) throws IOException {
-    if (indexHasFreq) {
-      for(int i=0;i<num;i++) {
-        final int code = docIn.readVInt();
-        docBuffer[i] = code >>> 1;
-        if ((code & 1) != 0) {
-          freqBuffer[i] = 1;
-        } else {
-          freqBuffer[i] = docIn.readVInt();
-        }
-      }
-    } else {
-      for(int i=0;i<num;i++) {
-        docBuffer[i] = docIn.readVInt();
-      }
-    }
-  }
-
-  @Override
-  public BlockTermState newTermState() {
-    return new IntBlockTermState();
-  }
-
-  @Override
-  public void close() throws IOException {
-    IOUtils.close(docIn, posIn, payIn);
-  }
-
-  @Override
-  public void decodeTerm(long[] longs, DataInput in, FieldInfo fieldInfo, BlockTermState _termState, boolean absolute)
-    throws IOException {
-    final IntBlockTermState termState = (IntBlockTermState) _termState;
-    final boolean fieldHasPositions = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
-    final boolean fieldHasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
-    final boolean fieldHasPayloads = fieldInfo.hasPayloads();
-
-    if (absolute) {
-      termState.docStartFP = 0;
-      termState.posStartFP = 0;
-      termState.payStartFP = 0;
-    }
-    if (version < Lucene41PostingsFormat.VERSION_META_ARRAY) {  // backward compatibility
-      _decodeTerm(in, fieldInfo, termState);
-      return;
-    }
-    termState.docStartFP += longs[0];
-    if (fieldHasPositions) {
-      termState.posStartFP += longs[1];
-      if (fieldHasOffsets || fieldHasPayloads) {
-        termState.payStartFP += longs[2];
-      }
-    }
-    if (termState.docFreq == 1) {
-      termState.singletonDocID = in.readVInt();
-    } else {
-      termState.singletonDocID = -1;
-    }
-    if (fieldHasPositions) {
-      if (termState.totalTermFreq > BLOCK_SIZE) {
-        termState.lastPosBlockOffset = in.readVLong();
-      } else {
-        termState.lastPosBlockOffset = -1;
-      }
-    }
-    if (termState.docFreq > BLOCK_SIZE) {
-      termState.skipOffset = in.readVLong();
-    } else {
-      termState.skipOffset = -1;
-    }
-  }
-  private void _decodeTerm(DataInput in, FieldInfo fieldInfo, IntBlockTermState termState) throws IOException {
-    final boolean fieldHasPositions = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
-    final boolean fieldHasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
-    final boolean fieldHasPayloads = fieldInfo.hasPayloads();
-    if (termState.docFreq == 1) {
-      termState.singletonDocID = in.readVInt();
-    } else {
-      termState.singletonDocID = -1;
-      termState.docStartFP += in.readVLong();
-    }
-    if (fieldHasPositions) {
-      termState.posStartFP += in.readVLong();
-      if (termState.totalTermFreq > BLOCK_SIZE) {
-        termState.lastPosBlockOffset = in.readVLong();
-      } else {
-        termState.lastPosBlockOffset = -1;
-      }
-      if ((fieldHasPayloads || fieldHasOffsets) && termState.totalTermFreq >= BLOCK_SIZE) {
-        termState.payStartFP += in.readVLong();
-      }
-    }
-    if (termState.docFreq > BLOCK_SIZE) {
-      termState.skipOffset = in.readVLong();
-    } else {
-      termState.skipOffset = -1;
-    }
-  }
-    
-  @Override
-  public DocsEnum docs(FieldInfo fieldInfo, BlockTermState termState, Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
-    BlockDocsEnum docsEnum;
-    if (reuse instanceof BlockDocsEnum) {
-      docsEnum = (BlockDocsEnum) reuse;
-      if (!docsEnum.canReuse(docIn, fieldInfo)) {
-        docsEnum = new BlockDocsEnum(fieldInfo);
-      }
-    } else {
-      docsEnum = new BlockDocsEnum(fieldInfo);
-    }
-    return docsEnum.reset(liveDocs, (IntBlockTermState) termState, flags);
-  }
-
-  // TODO: specialize to liveDocs vs not
-  
-  @Override
-  public DocsAndPositionsEnum docsAndPositions(FieldInfo fieldInfo, BlockTermState termState, Bits liveDocs,
-                                               DocsAndPositionsEnum reuse, int flags)
-    throws IOException {
-
-    boolean indexHasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
-    boolean indexHasPayloads = fieldInfo.hasPayloads();
-
-    if ((!indexHasOffsets || (flags & DocsAndPositionsEnum.FLAG_OFFSETS) == 0) &&
-        (!indexHasPayloads || (flags & DocsAndPositionsEnum.FLAG_PAYLOADS) == 0)) {
-      BlockDocsAndPositionsEnum docsAndPositionsEnum;
-      if (reuse instanceof BlockDocsAndPositionsEnum) {
-        docsAndPositionsEnum = (BlockDocsAndPositionsEnum) reuse;
-        if (!docsAndPositionsEnum.canReuse(docIn, fieldInfo)) {
-          docsAndPositionsEnum = new BlockDocsAndPositionsEnum(fieldInfo);
-        }
-      } else {
-        docsAndPositionsEnum = new BlockDocsAndPositionsEnum(fieldInfo);
-      }
-      return docsAndPositionsEnum.reset(liveDocs, (IntBlockTermState) termState);
-    } else {
-      EverythingEnum everythingEnum;
-      if (reuse instanceof EverythingEnum) {
-        everythingEnum = (EverythingEnum) reuse;
-        if (!everythingEnum.canReuse(docIn, fieldInfo)) {
-          everythingEnum = new EverythingEnum(fieldInfo);
-        }
-      } else {
-        everythingEnum = new EverythingEnum(fieldInfo);
-      }
-      return everythingEnum.reset(liveDocs, (IntBlockTermState) termState, flags);
-    }
-  }
-
-  final class BlockDocsEnum extends DocsEnum {
-    private final byte[] encoded;
-    
-    private final int[] docDeltaBuffer = new int[MAX_DATA_SIZE];
-    private final int[] freqBuffer = new int[MAX_DATA_SIZE];
-
-    private int docBufferUpto;
-
-    private Lucene41SkipReader skipper;
-    private boolean skipped;
-
-    final IndexInput startDocIn;
-
-    IndexInput docIn;
-    final boolean indexHasFreq;
-    final boolean indexHasPos;
-    final boolean indexHasOffsets;
-    final boolean indexHasPayloads;
-
-    private int docFreq;                              // number of docs in this posting list
-    private long totalTermFreq;                       // sum of freqs in this posting list (or docFreq when omitted)
-    private int docUpto;                              // how many docs we've read
-    private int doc;                                  // doc we last read
-    private int accum;                                // accumulator for doc deltas
-    private int freq;                                 // freq we last read
-
-    // Where this term's postings start in the .doc file:
-    private long docTermStartFP;
-
-    // Where this term's skip data starts (after
-    // docTermStartFP) in the .doc file (or -1 if there is
-    // no skip data for this term):
-    private long skipOffset;
-
-    // docID for next skip point, we won't use skipper if 
-    // target docID is not larger than this
-    private int nextSkipDoc;
-
-    private Bits liveDocs;
-    
-    private boolean needsFreq; // true if the caller actually needs frequencies
-    private int singletonDocID; // docid when there is a single pulsed posting, otherwise -1
-
-    public BlockDocsEnum(FieldInfo fieldInfo) throws IOException {
-      this.startDocIn = Lucene41PostingsReader.this.docIn;
-      this.docIn = null;
-      indexHasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;
-      indexHasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
-      indexHasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
-      indexHasPayloads = fieldInfo.hasPayloads();
-      encoded = new byte[MAX_ENCODED_SIZE];    
-    }
-
-    public boolean canReuse(IndexInput docIn, FieldInfo fieldInfo) {
-      return docIn == startDocIn &&
-        indexHasFreq == (fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0) &&
-        indexHasPos == (fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) &&
-        indexHasPayloads == fieldInfo.hasPayloads();
-    }
-    
-    public DocsEnum reset(Bits liveDocs, IntBlockTermState termState, int flags) throws IOException {
-      this.liveDocs = liveDocs;
-      // if (DEBUG) {
-      //   System.out.println("  FPR.reset: termState=" + termState);
-      // }
-      docFreq = termState.docFreq;
-      totalTermFreq = indexHasFreq ? termState.totalTermFreq : docFreq;
-      docTermStartFP = termState.docStartFP;
-      skipOffset = termState.skipOffset;
-      singletonDocID = termState.singletonDocID;
-      if (docFreq > 1) {
-        if (docIn == null) {
-          // lazy init
-          docIn = startDocIn.clone();
-        }
-        docIn.seek(docTermStartFP);
-      }
-
-      doc = -1;
-      this.needsFreq = (flags & DocsEnum.FLAG_FREQS) != 0;
-      if (!indexHasFreq) {
-        Arrays.fill(freqBuffer, 1);
-      }
-      accum = 0;
-      docUpto = 0;
-      nextSkipDoc = BLOCK_SIZE - 1; // we won't skip if target is found in first block
-      docBufferUpto = BLOCK_SIZE;
-      skipped = false;
-      return this;
-    }
-    
-    @Override
-    public int freq() throws IOException {
-      return freq;
-    }
-
-    @Override
-    public int docID() {
-      return doc;
-    }
-    
-    private void refillDocs() throws IOException {
-      final int left = docFreq - docUpto;
-      assert left > 0;
-
-      if (left >= BLOCK_SIZE) {
-        // if (DEBUG) {
-        //   System.out.println("    fill doc block from fp=" + docIn.getFilePointer());
-        // }
-        forUtil.readBlock(docIn, encoded, docDeltaBuffer);
-
-        if (indexHasFreq) {
-          // if (DEBUG) {
-          //   System.out.println("    fill freq block from fp=" + docIn.getFilePointer());
-          // }
-          if (needsFreq) {
-            forUtil.readBlock(docIn, encoded, freqBuffer);
-          } else {
-            forUtil.skipBlock(docIn); // skip over freqs
-          }
-        }
-      } else if (docFreq == 1) {
-        docDeltaBuffer[0] = singletonDocID;
-        freqBuffer[0] = (int) totalTermFreq;
-      } else {
-        // Read vInts:
-        // if (DEBUG) {
-        //   System.out.println("    fill last vInt block from fp=" + docIn.getFilePointer());
-        // }
-        readVIntBlock(docIn, docDeltaBuffer, freqBuffer, left, indexHasFreq);
-      }
-      docBufferUpto = 0;
-    }
-
-    @Override
-    public int nextDoc() throws IOException {
-      // if (DEBUG) {
-      //   System.out.println("\nFPR.nextDoc");
-      // }
-      while (true) {
-        // if (DEBUG) {
-        //   System.out.println("  docUpto=" + docUpto + " (of df=" + docFreq + ") docBufferUpto=" + docBufferUpto);
-        // }
-
-        if (docUpto == docFreq) {
-          // if (DEBUG) {
-          //   System.out.println("  return doc=END");
-          // }
-          return doc = NO_MORE_DOCS;
-        }
-        if (docBufferUpto == BLOCK_SIZE) {
-          refillDocs();
-        }
-
-        // if (DEBUG) {
-        //   System.out.println("    accum=" + accum + " docDeltaBuffer[" + docBufferUpto + "]=" + docDeltaBuffer[docBufferUpto]);
-        // }
-        accum += docDeltaBuffer[docBufferUpto];
-        docUpto++;
-
-        if (liveDocs == null || liveDocs.get(accum)) {
-          doc = accum;
-          freq = freqBuffer[docBufferUpto];
-          docBufferUpto++;
-          // if (DEBUG) {
-          //   System.out.println("  return doc=" + doc + " freq=" + freq);
-          // }
-          return doc;
-        }
-        // if (DEBUG) {
-        //   System.out.println("  doc=" + accum + " is deleted; try next doc");
-        // }
-        docBufferUpto++;
-      }
-    }
-
-    @Override
-    public int advance(int target) throws IOException {
-      // TODO: make frq block load lazy/skippable
-      // if (DEBUG) {
-      //   System.out.println("  FPR.advance target=" + target);
-      // }
-
-      // current skip docID < docIDs generated from current buffer <= next skip docID
-      // we don't need to skip if target is buffered already
-      if (docFreq > BLOCK_SIZE && target > nextSkipDoc) {
-
-        // if (DEBUG) {
-        //   System.out.println("load skipper");
-        // }
-
-        if (skipper == null) {
-          // Lazy init: first time this enum has ever been used for skipping
-          skipper = new Lucene41SkipReader(docIn.clone(),
-                                        Lucene41PostingsFormat.maxSkipLevels,
-                                        BLOCK_SIZE,
-                                        indexHasPos,
-                                        indexHasOffsets,
-                                        indexHasPayloads);
-        }
-
-        if (!skipped) {
-          assert skipOffset != -1;
-          // This is the first time this enum has skipped
-          // since reset() was called; load the skip data:
-          skipper.init(docTermStartFP+skipOffset, docTermStartFP, 0, 0, docFreq);
-          skipped = true;
-        }
-
-        // always plus one to fix the result, since skip position in Lucene41SkipReader 
-        // is a little different from MultiLevelSkipListReader
-        final int newDocUpto = skipper.skipTo(target) + 1; 
-
-        if (newDocUpto > docUpto) {
-          // Skipper moved
-          // if (DEBUG) {
-          //   System.out.println("skipper moved to docUpto=" + newDocUpto + " vs current=" + docUpto + "; docID=" + skipper.getDoc() + " fp=" + skipper.getDocPointer());
-          // }
-          assert newDocUpto % BLOCK_SIZE == 0 : "got " + newDocUpto;
-          docUpto = newDocUpto;
-
-          // Force to read next block
-          docBufferUpto = BLOCK_SIZE;
-          accum = skipper.getDoc();               // actually, this is just lastSkipEntry
-          docIn.seek(skipper.getDocPointer());    // now point to the block we want to search
-        }
-        // next time we call advance, this is used to 
-        // foresee whether skipper is necessary.
-        nextSkipDoc = skipper.getNextSkipDoc();
-      }
-      if (docUpto == docFreq) {
-        return doc = NO_MORE_DOCS;
-      }
-      if (docBufferUpto == BLOCK_SIZE) {
-        refillDocs();
-      }
-
-      // Now scan... this is an inlined/pared down version
-      // of nextDoc():
-      while (true) {
-        // if (DEBUG) {
-        //   System.out.println("  scan doc=" + accum + " docBufferUpto=" + docBufferUpto);
-        // }
-        accum += docDeltaBuffer[docBufferUpto];
-        docUpto++;
-
-        if (accum >= target) {
-          break;
-        }
-        docBufferUpto++;
-        if (docUpto == docFreq) {
-          return doc = NO_MORE_DOCS;
-        }
-      }
-
-      if (liveDocs == null || liveDocs.get(accum)) {
-        // if (DEBUG) {
-        //   System.out.println("  return doc=" + accum);
-        // }
-        freq = freqBuffer[docBufferUpto];
-        docBufferUpto++;
-        return doc = accum;
-      } else {
-        // if (DEBUG) {
-        //   System.out.println("  now do nextDoc()");
-        // }
-        docBufferUpto++;
-        return nextDoc();
-      }
-    }
-    
-    @Override
-    public long cost() {
-      return docFreq;
-    }
-  }
-
-
-  final class BlockDocsAndPositionsEnum extends DocsAndPositionsEnum {
-    
-    private final byte[] encoded;
-
-    private final int[] docDeltaBuffer = new int[MAX_DATA_SIZE];
-    private final int[] freqBuffer = new int[MAX_DATA_SIZE];
-    private final int[] posDeltaBuffer = new int[MAX_DATA_SIZE];
-
-    private int docBufferUpto;
-    private int posBufferUpto;
-
-    private Lucene41SkipReader skipper;
-    private boolean skipped;
-
-    final IndexInput startDocIn;
-
-    IndexInput docIn;
-    final IndexInput posIn;
-
-    final boolean indexHasOffsets;
-    final boolean indexHasPayloads;
-
-    private int docFreq;                              // number of docs in this posting list
-    private long totalTermFreq;                       // number of positions in this posting list
-    private int docUpto;                              // how many docs we've read
-    private int doc;                                  // doc we last read
-    private int accum;                                // accumulator for doc deltas
-    private int freq;                                 // freq we last read
-    private int position;                             // current position
-
-    // how many positions "behind" we are; nextPosition must
-    // skip these to "catch up":
-    private int posPendingCount;
-
-    // Lazy pos seek: if != -1 then we must seek to this FP
-    // before reading positions:
-    private long posPendingFP;
-
-    // Where this term's postings start in the .doc file:
-    private long docTermStartFP;
-
-    // Where this term's postings start in the .pos file:
-    private long posTermStartFP;
-
-    // Where this term's payloads/offsets start in the .pay
-    // file:
-    private long payTermStartFP;
-
-    // File pointer where the last (vInt encoded) pos delta
-    // block is.  We need this to know whether to bulk
-    // decode vs vInt decode the block:
-    private long lastPosBlockFP;
-
-    // Where this term's skip data starts (after
-    // docTermStartFP) in the .doc file (or -1 if there is
-    // no skip data for this term):
-    private long skipOffset;
-
-    private int nextSkipDoc;
-
-    private Bits liveDocs;
-    private int singletonDocID; // docid when there is a single pulsed posting, otherwise -1
-    
-    public BlockDocsAndPositionsEnum(FieldInfo fieldInfo) throws IOException {
-      this.startDocIn = Lucene41PostingsReader.this.docIn;
-      this.docIn = null;
-      this.posIn = Lucene41PostingsReader.this.posIn.clone();
-      encoded = new byte[MAX_ENCODED_SIZE];
-      indexHasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
-      indexHasPayloads = fieldInfo.hasPayloads();
-    }
-
-    public boolean canReuse(IndexInput docIn, FieldInfo fieldInfo) {
-      return docIn == startDocIn &&
-        indexHasOffsets == (fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0) &&
-        indexHasPayloads == fieldInfo.hasPayloads();
-    }
-    
-    public DocsAndPositionsEnum reset(Bits liveDocs, IntBlockTermState termState) throws IOException {
-      this.liveDocs = liveDocs;
-      // if (DEBUG) {
-      //   System.out.println("  FPR.reset: termState=" + termState);
-      // }
-      docFreq = termState.docFreq;
-      docTermStartFP = termState.docStartFP;
-      posTermStartFP = termState.posStartFP;
-      payTermStartFP = termState.payStartFP;
-      skipOffset = termState.skipOffset;
-      totalTermFreq = termState.totalTermFreq;
-      singletonDocID = termState.singletonDocID;
-      if (docFreq > 1) {
-        if (docIn == null) {
-          // lazy init
-          docIn = startDocIn.clone();
-        }
-        docIn.seek(docTermStartFP);
-      }
-      posPendingFP = posTermStartFP;
-      posPendingCount = 0;
-      if (termState.totalTermFreq < BLOCK_SIZE) {
-        lastPosBlockFP = posTermStartFP;
-      } else if (termState.totalTermFreq == BLOCK_SIZE) {
-        lastPosBlockFP = -1;
-      } else {
-        lastPosBlockFP = posTermStartFP + termState.lastPosBlockOffset;
-      }
-
-      doc = -1;
-      accum = 0;
-      docUpto = 0;
-      if (docFreq > BLOCK_SIZE) {
-        nextSkipDoc = BLOCK_SIZE - 1; // we won't skip if target is found in first block
-      } else {
-        nextSkipDoc = NO_MORE_DOCS; // not enough docs for skipping
-      }
-      docBufferUpto = BLOCK_SIZE;
-      skipped = false;
-      return this;
-    }
-    
-    @Override
-    public int freq() throws IOException {
-      return freq;
-    }
-
-    @Override
-    public int docID() {
-      return doc;
-    }
-
-    private void refillDocs() throws IOException {
-      final int left = docFreq - docUpto;
-      assert left > 0;
-
-      if (left >= BLOCK_SIZE) {
-        // if (DEBUG) {
-        //   System.out.println("    fill doc block from fp=" + docIn.getFilePointer());
-        // }
-        forUtil.readBlock(docIn, encoded, docDeltaBuffer);
-        // if (DEBUG) {
-        //   System.out.println("    fill freq block from fp=" + docIn.getFilePointer());
-        // }
-        forUtil.readBlock(docIn, encoded, freqBuffer);
-      } else if (docFreq == 1) {
-        docDeltaBuffer[0] = singletonDocID;
-        freqBuffer[0] = (int) totalTermFreq;
-      } else {
-        // Read vInts:
-        // if (DEBUG) {
-        //   System.out.println("    fill last vInt doc block from fp=" + docIn.getFilePointer());
-        // }
-        readVIntBlock(docIn, docDeltaBuffer, freqBuffer, left, true);
-      }
-      docBufferUpto = 0;
-    }
-    
-    private void refillPositions() throws IOException {
-      // if (DEBUG) {
-      //   System.out.println("      refillPositions");
-      // }
-      if (posIn.getFilePointer() == lastPosBlockFP) {
-        // if (DEBUG) {
-        //   System.out.println("        vInt pos block @ fp=" + posIn.getFilePointer() + " hasPayloads=" + indexHasPayloads + " hasOffsets=" + indexHasOffsets);
-        // }
-        final int count = (int) (totalTermFreq % BLOCK_SIZE);
-        int payloadLength = 0;
-        for(int i=0;i<count;i++) {
-          int code = posIn.readVInt();
-          if (indexHasPayloads) {
-            if ((code & 1) != 0) {
-              payloadLength = posIn.readVInt();
-            }
-            posDeltaBuffer[i] = code >>> 1;
-            if (payloadLength != 0) {
-              posIn.seek(posIn.getFilePointer() + payloadLength);
-            }
-          } else {
-            posDeltaBuffer[i] = code;
-          }
-          if (indexHasOffsets) {
-            if ((posIn.readVInt() & 1) != 0) {
-              // offset length changed
-              posIn.readVInt();
-            }
-          }
-        }
-      } else {
-        // if (DEBUG) {
-        //   System.out.println("        bulk pos block @ fp=" + posIn.getFilePointer());
-        // }
-        forUtil.readBlock(posIn, encoded, posDeltaBuffer);
-      }
-    }
-
-    @Override
-    public int nextDoc() throws IOException {
-      // if (DEBUG) {
-      //   System.out.println("  FPR.nextDoc");
-      // }
-      while (true) {
-        // if (DEBUG) {
-        //   System.out.println("    docUpto=" + docUpto + " (of df=" + docFreq + ") docBufferUpto=" + docBufferUpto);
-        // }
-        if (docUpto == docFreq) {
-          return doc = NO_MORE_DOCS;
-        }
-        if (docBufferUpto == BLOCK_SIZE) {
-          refillDocs();
-        }
-        // if (DEBUG) {
-        //   System.out.println("    accum=" + accum + " docDeltaBuffer[" + docBufferUpto + "]=" + docDeltaBuffer[docBufferUpto]);
-        // }
-        accum += docDeltaBuffer[docBufferUpto];
-        freq = freqBuffer[docBufferUpto];
-        posPendingCount += freq;
-        docBufferUpto++;
-        docUpto++;
-
-        if (liveDocs == null || liveDocs.get(accum)) {
-          doc = accum;
-          position = 0;
-          // if (DEBUG) {
-          //   System.out.println("    return doc=" + doc + " freq=" + freq + " posPendingCount=" + posPendingCount);
-          // }
-          return doc;
-        }
-        // if (DEBUG) {
-        //   System.out.println("    doc=" + accum + " is deleted; try next doc");
-        // }
-      }
-    }
-    
-    @Override
-    public int advance(int target) throws IOException {
-      // TODO: make frq block load lazy/skippable
-      // if (DEBUG) {
-      //   System.out.println("  FPR.advance target=" + target);
-      // }
-
-      if (target > nextSkipDoc) {
-        // if (DEBUG) {
-        //   System.out.println("    try skipper");
-        // }
-        if (skipper == null) {
-          // Lazy init: first time this enum has ever been used for skipping
-          // if (DEBUG) {
-          //   System.out.println("    create skipper");
-          // }
-          skipper = new Lucene41SkipReader(docIn.clone(),
-                                        Lucene41PostingsFormat.maxSkipLevels,
-                                        BLOCK_SIZE,
-                                        true,
-                                        indexHasOffsets,
-                                        indexHasPayloads);
-        }
-
-        if (!skipped) {
-          assert skipOffset != -1;
-          // This is the first time this enum has skipped
-          // since reset() was called; load the skip data:
-          // if (DEBUG) {
-          //   System.out.println("    init skipper");
-          // }
-          skipper.init(docTermStartFP+skipOffset, docTermStartFP, posTermStartFP, payTermStartFP, docFreq);
-          skipped = true;
-        }
-
-        final int newDocUpto = skipper.skipTo(target) + 1; 
-
-        if (newDocUpto > docUpto) {
-          // Skipper moved
-          // if (DEBUG) {
-          //   System.out.println("    skipper moved to docUpto=" + newDocUpto + " vs current=" + docUpto + "; docID=" + skipper.getDoc() + " fp=" + skipper.getDocPointer() + " pos.fp=" + skipper.getPosPointer() + " pos.bufferUpto=" + skipper.getPosBufferUpto());
-          // }
-
-          assert newDocUpto % BLOCK_SIZE == 0 : "got " + newDocUpto;
-          docUpto = newDocUpto;
-
-          // Force to read next block
-          docBufferUpto = BLOCK_SIZE;
-          accum = skipper.getDoc();
-          docIn.seek(skipper.getDocPointer());
-          posPendingFP = skipper.getPosPointer();
-          posPendingCount = skipper.getPosBufferUpto();
-        }
-        nextSkipDoc = skipper.getNextSkipDoc();
-      }
-      if (docUpto == docFreq) {
-        return doc = NO_MORE_DOCS;
-      }
-      if (docBufferUpto == BLOCK_SIZE) {
-        refillDocs();
-      }
-
-      // Now scan... this is an inlined/pared down version
-      // of nextDoc():
-      while (true) {
-        // if (DEBUG) {
-        //   System.out.println("  scan doc=" + accum + " docBufferUpto=" + docBufferUpto);
-        // }
-        accum += docDeltaBuffer[docBufferUpto];
-        freq = freqBuffer[docBufferUpto];
-        posPendingCount += freq;
-        docBufferUpto++;
-        docUpto++;
-
-        if (accum >= target) {
-          break;
-        }
-        if (docUpto == docFreq) {
-          return doc = NO_MORE_DOCS;
-        }
-      }
-
-      if (liveDocs == null || liveDocs.get(accum)) {
-        // if (DEBUG) {
-        //   System.out.println("  return doc=" + accum);
-        // }
-        position = 0;
-        return doc = accum;
-      } else {
-        // if (DEBUG) {
-        //   System.out.println("  now do nextDoc()");
-        // }
-        return nextDoc();
-      }
-    }
-
-    // TODO: in theory we could avoid loading frq block
-    // when not needed, ie, use skip data to load how far to
-    // seek the pos pointer ... instead of having to load frq
-    // blocks only to sum up how many positions to skip
-    private void skipPositions() throws IOException {
-      // Skip positions now:
-      int toSkip = posPendingCount - freq;
-      // if (DEBUG) {
-      //   System.out.println("      FPR.skipPositions: toSkip=" + toSkip);
-      // }
-
-      final int leftInBlock = BLOCK_SIZE - posBufferUpto;
-      if (toSkip < leftInBlock) {
-        posBufferUpto += toSkip;
-        // if (DEBUG) {
-        //   System.out.println("        skip w/in block to posBufferUpto=" + posBufferUpto);
-        // }
-      } else {
-        toSkip -= leftInBlock;
-        while(toSkip >= BLOCK_SIZE) {
-          // if (DEBUG) {
-          //   System.out.println("        skip whole block @ fp=" + posIn.getFilePointer());
-          // }
-          assert posIn.getFilePointer() != lastPosBlockFP;
-          forUtil.skipBlock(posIn);
-          toSkip -= BLOCK_SIZE;
-        }
-        refillPositions();
-        posBufferUpto = toSkip;
-        // if (DEBUG) {
-        //   System.out.println("        skip w/in block to posBufferUpto=" + posBufferUpto);
-        // }
-      }
-
-      position = 0;
-    }
-
-    @Override
-    public int nextPosition() throws IOException {
-      // if (DEBUG) {
-      //   System.out.println("    FPR.nextPosition posPendingCount=" + posPendingCount + " posBufferUpto=" + posBufferUpto);
-      // }
-      if (posPendingFP != -1) {
-        // if (DEBUG) {
-        //   System.out.println("      seek to pendingFP=" + posPendingFP);
-        // }
-        posIn.seek(posPendingFP);
-        posPendingFP = -1;
-
-        // Force buffer refill:
-        posBufferUpto = BLOCK_SIZE;
-      }
-
-      if (posPendingCount > freq) {
-        skipPositions();
-        posPendingCount = freq;
-      }
-
-      if (posBufferUpto == BLOCK_SIZE) {
-        refillPositions();
-        posBufferUpto = 0;
-      }
-      position += posDeltaBuffer[posBufferUpto++];
-      posPendingCount--;
-      // if (DEBUG) {
-      //   System.out.println("      return pos=" + position);
-      // }
-      return position;
-    }
-
-    @Override
-    public int startOffset() {
-      return -1;
-    }
-  
-    @Override
-    public int endOffset() {
-      return -1;
-    }
-  
-    @Override
-    public BytesRef getPayload() {
-      return null;
-    }
-    
-    @Override
-    public long cost() {
-      return docFreq;
-    }
-  }
-
-  // Also handles payloads + offsets
-  final class EverythingEnum extends DocsAndPositionsEnum {
-    
-    private final byte[] encoded;
-
-    private final int[] docDeltaBuffer = new int[MAX_DATA_SIZE];
-    private final int[] freqBuffer = new int[MAX_DATA_SIZE];
-    private final int[] posDeltaBuffer = new int[MAX_DATA_SIZE];
-
-    private final int[] payloadLengthBuffer;
-    private final int[] offsetStartDeltaBuffer;
-    private final int[] offsetLengthBuffer;
-
-    private byte[] payloadBytes;
-    private int payloadByteUpto;
-    private int payloadLength;
-
-    private int lastStartOffset;
-    private int startOffset;
-    private int endOffset;
-
-    private int docBufferUpto;
-    private int posBufferUpto;
-
-    private Lucene41SkipReader skipper;
-    private boolean skipped;
-
-    final IndexInput startDocIn;
-
-    IndexInput docIn;
-    final IndexInput posIn;
-    final IndexInput payIn;
-    final BytesRef payload;
-
-    final boolean indexHasOffsets;
-    final boolean indexHasPayloads;
-
-    private int docFreq;                              // number of docs in this posting list
-    private long totalTermFreq;                       // number of positions in this posting list
-    private int docUpto;                              // how many docs we've read
-    private int doc;                                  // doc we last read
-    private int accum;                                // accumulator for doc deltas
-    private int freq;                                 // freq we last read
-    private int position;                             // current position
-
-    // how many positions "behind" we are; nextPosition must
-    // skip these to "catch up":
-    private int posPendingCount;
-
-    // Lazy pos seek: if != -1 then we must seek to this FP
-    // before reading positions:
-    private long posPendingFP;
-
-    // Lazy pay seek: if != -1 then we must seek to this FP
-    // before reading payloads/offsets:
-    private long payPendingFP;
-
-    // Where this term's postings start in the .doc file:
-    private long docTermStartFP;
-
-    // Where this term's postings start in the .pos file:
-    private long posTermStartFP;
-
-    // Where this term's payloads/offsets start in the .pay
-    // file:
-    private long payTermStartFP;
-
-    // File pointer where the last (vInt encoded) pos delta
-    // block is.  We need this to know whether to bulk
-    // decode vs vInt decode the block:
-    private long lastPosBlockFP;
-
-    // Where this term's skip data starts (after
-    // docTermStartFP) in the .doc file (or -1 if there is
-    // no skip data for this term):
-    private long skipOffset;
-
-    private int nextSkipDoc;
-
-    private Bits liveDocs;
-    
-    private boolean needsOffsets; // true if we actually need offsets
-    private boolean needsPayloads; // true if we actually need payloads
-    private int singletonDocID; // docid when there is a single pulsed posting, otherwise -1
-    
-    public EverythingEnum(FieldInfo fieldInfo) throws IOException {
-      this.startDocIn = Lucene41PostingsReader.this.docIn;
-      this.docIn = null;
-      this.posIn = Lucene41PostingsReader.this.posIn.clone();
-      this.payIn = Lucene41PostingsReader.this.payIn.clone();
-      encoded = new byte[MAX_ENCODED_SIZE];
-      indexHasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
-      if (indexHasOffsets) {
-        offsetStartDeltaBuffer = new int[MAX_DATA_SIZE];
-        offsetLengthBuffer = new int[MAX_DATA_SIZE];
-      } else {
-        offsetStartDeltaBuffer = null;
-        offsetLengthBuffer = null;
-        startOffset = -1;
-        endOffset = -1;
-      }
-
-      indexHasPayloads = fieldInfo.hasPayloads();
-      if (indexHasPayloads) {
-        payloadLengthBuffer = new int[MAX_DATA_SIZE];
-        payloadBytes = new byte[128];
-        payload = new BytesRef();
-      } else {
-        payloadLengthBuffer = null;
-        payloadBytes = null;
-        payload = null;
-      }
-    }
-
-    public boolean canReuse(IndexInput docIn, FieldInfo fieldInfo) {
-      return docIn == startDocIn &&
-        indexHasOffsets == (fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0) &&
-        indexHasPayloads == fieldInfo.hasPayloads();
-    }
-    
-    public EverythingEnum reset(Bits liveDocs, IntBlockTermState termState, int flags) throws IOException {
-      this.liveDocs = liveDocs;
-      // if (DEBUG) {
-      //   System.out.println("  FPR.reset: termState=" + termState);
-      // }
-      docFreq = termState.docFreq;
-      docTermStartFP = termState.docStartFP;
-      posTermStartFP = termState.posStartFP;
-      payTermStartFP = termState.payStartFP;
-      skipOffset = termState.skipOffset;
-      totalTermFreq = termState.totalTermFreq;
-      singletonDocID = termState.singletonDocID;
-      if (docFreq > 1) {
-        if (docIn == null) {
-          // lazy init
-          docIn = startDocIn.clone();
-        }
-        docIn.seek(docTermStartFP);
-      }
-      posPendingFP = posTermStartFP;
-      payPendingFP = payTermStartFP;
-      posPendingCount = 0;
-      if (termState.totalTermFreq < BLOCK_SIZE) {
-        lastPosBlockFP = posTermStartFP;
-      } else if (termState.totalTermFreq == BLOCK_SIZE) {
-        lastPosBlockFP = -1;
-      } else {
-        lastPosBlockFP = posTermStartFP + termState.lastPosBlockOffset;
-      }
-
-      this.needsOffsets = (flags & DocsAndPositionsEnum.FLAG_OFFSETS) != 0;
-      this.needsPayloads = (flags & DocsAndPositionsEnum.FLAG_PAYLOADS) != 0;
-
-      doc = -1;
-      accum = 0;
-      docUpto = 0;
-      if (docFreq > BLOCK_SIZE) {
-        nextSkipDoc = BLOCK_SIZE - 1; // we won't skip if target is found in first block
-      } else {
-        nextSkipDoc = NO_MORE_DOCS; // not enough docs for skipping
-      }
-      docBufferUpto = BLOCK_SIZE;
-      skipped = false;
-      return this;
-    }
-    
-    @Override
-    public int freq() throws IOException {
-      return freq;
-    }
-
-    @Override
-    public int docID() {
-      return doc;
-    }
-
-    private void refillDocs() throws IOException {
-      final int left = docFreq - docUpto;
-      assert left > 0;
-
-      if (left >= BLOCK_SIZE) {
-        // if (DEBUG) {
-        //   System.out.println("    fill doc block from fp=" + docIn.getFilePointer());
-        // }
-        forUtil.readBlock(docIn, encoded, docDeltaBuffer);
-        // if (DEBUG) {
-        //   System.out.println("    fill freq block from fp=" + docIn.getFilePointer());
-        // }
-        forUtil.readBlock(docIn, encoded, freqBuffer);
-      } else if (docFreq == 1) {
-        docDeltaBuffer[0] = singletonDocID;
-        freqBuffer[0] = (int) totalTermFreq;
-      } else {
-        // if (DEBUG) {
-        //   System.out.println("    fill last vInt doc block from fp=" + docIn.getFilePointer());
-        // }
-        readVIntBlock(docIn, docDeltaBuffer, freqBuffer, left, true);
-      }
-      docBufferUpto = 0;
-    }
-    
-    private void refillPositions() throws IOException {
-      // if (DEBUG) {
-      //   System.out.println("      refillPositions");
-      // }
-      if (posIn.getFilePointer() == lastPosBlockFP) {
-        // if (DEBUG) {
-        //   System.out.println("        vInt pos block @ fp=" + posIn.getFilePointer() + " hasPayloads=" + indexHasPayloads + " hasOffsets=" + indexHasOffsets);
-        // }
-        final int count = (int) (totalTermFreq % BLOCK_SIZE);
-        int payloadLength = 0;
-        int offsetLength = 0;
-        payloadByteUpto = 0;
-        for(int i=0;i<count;i++) {
-          int code = posIn.readVInt();
-          if (indexHasPayloads) {
-            if ((code & 1) != 0) {
-              payloadLength = posIn.readVInt();
-            }
-            // if (DEBUG) {
-            //   System.out.println("        i=" + i + " payloadLen=" + payloadLength);
-            // }
-            payloadLengthBuffer[i] = payloadLength;
-            posDeltaBuffer[i] = code >>> 1;
-            if (payloadLength != 0) {
-              if (payloadByteUpto + payloadLength > payloadBytes.length) {
-                payloadBytes = ArrayUtil.grow(payloadBytes, payloadByteUpto + payloadLength);
-              }
-              //System.out.println("          read payload @ pos.fp=" + posIn.getFilePointer());
-              posIn.readBytes(payloadBytes, payloadByteUpto, payloadLength);
-              payloadByteUpto += payloadLength;
-            }
-          } else {
-            posDeltaBuffer[i] = code;
-          }
-
-          if (indexHasOffsets) {
-            // if (DEBUG) {
-            //   System.out.println("        i=" + i + " read offsets from posIn.fp=" + posIn.getFilePointer());
-            // }
-            int deltaCode = posIn.readVInt();
-            if ((deltaCode & 1) != 0) {
-              offsetLength = posIn.readVInt();
-            }
-            offsetStartDeltaBuffer[i] = deltaCode >>> 1;
-            offsetLengthBuffer[i] = offsetLength;
-            // if (DEBUG) {
-            //   System.out.println("          startOffDelta=" + offsetStartDeltaBuffer[i] + " offsetLen=" + offsetLengthBuffer[i]);
-            // }
-          }
-        }
-        payloadByteUpto = 0;
-      } else {
-        // if (DEBUG) {
-        //   System.out.println("        bulk pos block @ fp=" + posIn.getFilePointer());
-        // }
-        forUtil.readBlock(posIn, encoded, posDeltaBuffer);
-
-        if (indexHasPayloads) {
-          // if (DEBUG) {
-          //   System.out.println("        bulk payload block @ pay.fp=" + payIn.getFilePointer());
-          // }
-          if (needsPayloads) {
-            forUtil.readBlock(payIn, encoded, payloadLengthBuffer);
-            int numBytes = payIn.readVInt();
-            // if (DEBUG) {
-            //   System.out.println("        " + numBytes + " payload bytes @ pay.fp=" + payIn.getFilePointer());
-            // }
-            if (numBytes > payloadBytes.length) {
-              payloadBytes = ArrayUtil.grow(payloadBytes, numBytes);
-            }
-            payIn.readBytes(payloadBytes, 0, numBytes);
-          } else {
-            // this works, because when writing a vint block we always force the first length to be written
-            forUtil.skipBlock(payIn); // skip over lengths
-            int numBytes = payIn.readVInt(); // read length of payloadBytes
-            payIn.seek(payIn.getFilePointer() + numBytes); // skip over payloadBytes
-          }
-          payloadByteUpto = 0;
-        }
-
-        if (indexHasOffsets) {
-          // if (DEBUG) {
-          //   System.out.println("        bulk offset block @ pay.fp=" + payIn.getFilePointer());
-          // }
-          if (needsOffsets) {
-            forUtil.readBlock(payIn, encoded, offsetStartDeltaBuffer);
-            forUtil.readBlock(payIn, encoded, offsetLengthBuffer);
-          } else {
-            // this works, because when writing a vint block we always force the first length to be written
-            forUtil.skipBlock(payIn); // skip over starts
-            forUtil.skipBlock(payIn); // skip over lengths
-          }
-        }
-      }
-    }
-
-    @Override
-    public int nextDoc() throws IOException {
-      // if (DEBUG) {
-      //   System.out.println("  FPR.nextDoc");
-      // }
-      while (true) {
-        // if (DEBUG) {
-        //   System.out.println("    docUpto=" + docUpto + " (of df=" + docFreq + ") docBufferUpto=" + docBufferUpto);
-        // }
-        if (docUpto == docFreq) {
-          return doc = NO_MORE_DOCS;
-        }
-        if (docBufferUpto == BLOCK_SIZE) {
-          refillDocs();
-        }
-        // if (DEBUG) {
-        //   System.out.println("    accum=" + accum + " docDeltaBuffer[" + docBufferUpto + "]=" + docDeltaBuffer[docBufferUpto]);
-        // }
-        accum += docDeltaBuffer[docBufferUpto];
-        freq = freqBuffer[docBufferUpto];
-        posPendingCount += freq;
-        docBufferUpto++;
-        docUpto++;
-
-        if (liveDocs == null || liveDocs.get(accum)) {
-          doc = accum;
-          // if (DEBUG) {
-          //   System.out.println("    return doc=" + doc + " freq=" + freq + " posPendingCount=" + posPendingCount);
-          // }
-          position = 0;
-          lastStartOffset = 0;
-          return doc;
-        }
-
-        // if (DEBUG) {
-        //   System.out.println("    doc=" + accum + " is deleted; try next doc");
-        // }
-      }
-    }
-    
-    @Override
-    public int advance(int target) throws IOException {
-      // TODO: make frq block load lazy/skippable
-      // if (DEBUG) {
-      //   System.out.println("  FPR.advance target=" + target);
-      // }
-
-      if (target > nextSkipDoc) {
-
-        // if (DEBUG) {
-        //   System.out.println("    try skipper");
-        // }
-
-        if (skipper == null) {
-          // Lazy init: first time this enum has ever been used for skipping
-          // if (DEBUG) {
-          //   System.out.println("    create skipper");
-          // }
-          skipper = new Lucene41SkipReader(docIn.clone(),
-                                        Lucene41PostingsFormat.maxSkipLevels,
-                                        BLOCK_SIZE,
-                                        true,
-                                        indexHasOffsets,
-                                        indexHasPayloads);
-        }
-
-        if (!skipped) {
-          assert skipOffset != -1;
-          // This is the first time this enum has skipped
-          // since reset() was called; load the skip data:
-          // if (DEBUG) {
-          //   System.out.println("    init skipper");
-          // }
-          skipper.init(docTermStartFP+skipOffset, docTermStartFP, posTermStartFP, payTermStartFP, docFreq);
-          skipped = true;
-        }
-
-        final int newDocUpto = skipper.skipTo(target) + 1; 
-
-        if (newDocUpto > docUpto) {
-          // Skipper moved
-          // if (DEBUG) {
-          //   System.out.println("    skipper moved to docUpto=" + newDocUpto + " vs current=" + docUpto + "; docID=" + skipper.getDoc() + " fp=" + skipper.getDocPointer() + " pos.fp=" + skipper.getPosPointer() + " pos.bufferUpto=" + skipper.getPosBufferUpto() + " pay.fp=" + skipper.getPayPointer() + " lastStartOffset=" + lastStartOffset);
-          // }
-          assert newDocUpto % BLOCK_SIZE == 0 : "got " + newDocUpto;
-          docUpto = newDocUpto;
-
-          // Force to read next block
-          docBufferUpto = BLOCK_SIZE;
-          accum = skipper.getDoc();
-          docIn.seek(skipper.getDocPointer());
-          posPendingFP = skipper.getPosPointer();
-          payPendingFP = skipper.getPayPointer();
-          posPendingCount = skipper.getPosBufferUpto();
-          lastStartOffset = 0; // new document
-          payloadByteUpto = skipper.getPayloadByteUpto();
-        }
-        nextSkipDoc = skipper.getNextSkipDoc();
-      }
-      if (docUpto == docFreq) {
-        return doc = NO_MORE_DOCS;
-      }
-      if (docBufferUpto == BLOCK_SIZE) {
-        refillDocs();
-      }
-
-      // Now scan:
-      while (true) {
-        // if (DEBUG) {
-        //   System.out.println("  scan doc=" + accum + " docBufferUpto=" + docBufferUpto);
-        // }
-        accum += docDeltaBuffer[docBufferUpto];
-        freq = freqBuffer[docBufferUpto];
-        posPendingCount += freq;
-        docBufferUpto++;
-        docUpto++;
-
-        if (accum >= target) {
-          break;
-        }
-        if (docUpto == docFreq) {
-          return doc = NO_MORE_DOCS;
-        }
-      }
-
-      if (liveDocs == null || liveDocs.get(accum)) {
-        // if (DEBUG) {
-        //   System.out.println("  return doc=" + accum);
-        // }
-        position = 0;
-        lastStartOffset = 0;
-        return doc = accum;
-      } else {
-        // if (DEBUG) {
-        //   System.out.println("  now do nextDoc()");
-        // }
-        return nextDoc();
-      }
-    }
-
-    // TODO: in theory we could avoid loading frq block
-    // when not needed, ie, use skip data to load how far to
-    // seek the pos pointer ... instead of having to load frq
-    // blocks only to sum up how many positions to skip
-    private void skipPositions() throws IOException {
-      // Skip positions now:
-      int toSkip = posPendingCount - freq;
-      // if (DEBUG) {
-      //   System.out.println("      FPR.skipPositions: toSkip=" + toSkip);
-      // }
-
-      final int leftInBlock = BLOCK_SIZE - posBufferUpto;
-      if (toSkip < leftInBlock) {
-        int end = posBufferUpto + toSkip;
-        while(posBufferUpto < end) {
-          if (indexHasPayloads) {
-            payloadByteUpto += payloadLengthBuffer[posBufferUpto];
-          }
-          posBufferUpto++;
-        }
-        // if (DEBUG) {
-        //   System.out.println("        skip w/in block to posBufferUpto=" + posBufferUpto);
-        // }
-      } else {
-        toSkip -= leftInBlock;
-        while(toSkip >= BLOCK_SIZE) {
-          // if (DEBUG) {
-          //   System.out.println("        skip whole block @ fp=" + posIn.getFilePointer());
-          // }
-          assert posIn.getFilePointer() != lastPosBlockFP;
-          forUtil.skipBlock(posIn);
-
-          if (indexHasPayloads) {
-            // Skip payloadLength block:
-            forUtil.skipBlock(payIn);
-
-            // Skip payloadBytes block:
-            int numBytes = payIn.readVInt();
-            payIn.seek(payIn.getFilePointer() + numBytes);
-          }
-
-          if (indexHasOffsets) {
-            forUtil.skipBlock(payIn);
-            forUtil.skipBlock(payIn);
-          }
-          toSkip -= BLOCK_SIZE;
-        }
-        refillPositions();
-        payloadByteUpto = 0;
-        posBufferUpto = 0;
-        while(posBufferUpto < toSkip) {
-          if (indexHasPayloads) {
-            payloadByteUpto += payloadLengthBuffer[posBufferUpto];
-          }
-          posBufferUpto++;
-        }
-        // if (DEBUG) {
-        //   System.out.println("        skip w/in block to posBufferUpto=" + posBufferUpto);
-        // }
-      }
-
-      position = 0;
-      lastStartOffset = 0;
-    }
-
-    @Override
-    public int nextPosition() throws IOException {
-      // if (DEBUG) {
-      //   System.out.println("    FPR.nextPosition posPendingCount=" + posPendingCount + " posBufferUpto=" + posBufferUpto + " payloadByteUpto=" + payloadByteUpto)// ;
-      // }
-      if (posPendingFP != -1) {
-        // if (DEBUG) {
-        //   System.out.println("      seek pos to pendingFP=" + posPendingFP);
-        // }
-        posIn.seek(posPendingFP);
-        posPendingFP = -1;
-
-        if (payPendingFP != -1) {
-          // if (DEBUG) {
-          //   System.out.println("      seek pay to pendingFP=" + payPendingFP);
-          // }
-          payIn.seek(payPendingFP);
-          payPendingFP = -1;
-        }
-
-        // Force buffer refill:
-        posBufferUpto = BLOCK_SIZE;
-      }
-
-      if (posPendingCount > freq) {
-        skipPositions();
-        posPendingCount = freq;
-      }
-
-      if (posBufferUpto == BLOCK_SIZE) {
-        refillPositions();
-        posBufferUpto = 0;
-      }
-      position += posDeltaBuffer[posBufferUpto];
-
-      if (indexHasPayloads) {
-        payloadLength = payloadLengthBuffer[posBufferUpto];
-        payload.bytes = payloadBytes;
-        payload.offset = payloadByteUpto;
-        payload.length = payloadLength;
-        payloadByteUpto += payloadLength;
-      }
-
-      if (indexHasOffsets) {
-        startOffset = lastStartOffset + offsetStartDeltaBuffer[posBufferUpto];
-        endOffset = startOffset + offsetLengthBuffer[posBufferUpto];
-        lastStartOffset = startOffset;
-      }
-
-      posBufferUpto++;
-      posPendingCount--;
-      // if (DEBUG) {
-      //   System.out.println("      return pos=" + position);
-      // }
-      return position;
-    }
-
-    @Override
-    public int startOffset() {
-      return startOffset;
-    }
-  
-    @Override
-    public int endOffset() {
-      return endOffset;
-    }
-  
-    @Override
-    public BytesRef getPayload() {
-      // if (DEBUG) {
-      //   System.out.println("    FPR.getPayload payloadLength=" + payloadLength + " payloadByteUpto=" + payloadByteUpto);
-      // }
-      if (payloadLength == 0) {
-        return null;
-      } else {
-        return payload;
-      }
-    }
-    
-    @Override
-    public long cost() {
-      return docFreq;
-    }
-  }
-
-  @Override
-  public long ramBytesUsed() {
-    return BASE_RAM_BYTES_USED;
-  }
-  
-  @Override
-  public Iterable<? extends Accountable> getChildResources() {
-    return Collections.emptyList();
-  }
-
-  @Override
-  public void checkIntegrity() throws IOException {
-    if (version >= Lucene41PostingsFormat.VERSION_CHECKSUM) {
-      if (docIn != null) {
-        CodecUtil.checksumEntireFile(docIn);
-      }
-      if (posIn != null) {
-        CodecUtil.checksumEntireFile(posIn);
-      }
-      if (payIn != null) {
-        CodecUtil.checksumEntireFile(payIn);
-      }
-    }
-  }
-
-  @Override
-  public String toString() {
-    return getClass().getSimpleName() + "(positions=" + (posIn != null) + ",payloads=" + (payIn != null) +")";
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41SkipReader.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41SkipReader.java
deleted file mode 100644
index 4f57430..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41SkipReader.java
+++ /dev/null
@@ -1,201 +0,0 @@
-package org.apache.lucene.codecs.lucene41;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.codecs.MultiLevelSkipListReader;
-import org.apache.lucene.store.IndexInput;
-
-/**
- * Lucene 4.1 skiplist format.
- * @deprecated only for reading old 4.x segments
- */
-@Deprecated
-final class Lucene41SkipReader extends MultiLevelSkipListReader {
-  // private boolean DEBUG = Lucene41PostingsReader.DEBUG;
-  private final int blockSize;
-
-  private long docPointer[];
-  private long posPointer[];
-  private long payPointer[];
-  private int posBufferUpto[];
-  private int payloadByteUpto[];
-
-  private long lastPosPointer;
-  private long lastPayPointer;
-  private int lastPayloadByteUpto;
-  private long lastDocPointer;
-  private int lastPosBufferUpto;
-
-  public Lucene41SkipReader(IndexInput skipStream, int maxSkipLevels, int blockSize, boolean hasPos, boolean hasOffsets, boolean hasPayloads) {
-    super(skipStream, maxSkipLevels, blockSize, 8);
-    this.blockSize = blockSize;
-    docPointer = new long[maxSkipLevels];
-    if (hasPos) {
-      posPointer = new long[maxSkipLevels];
-      posBufferUpto = new int[maxSkipLevels];
-      if (hasPayloads) {
-        payloadByteUpto = new int[maxSkipLevels];
-      } else {
-        payloadByteUpto = null;
-      }
-      if (hasOffsets || hasPayloads) {
-        payPointer = new long[maxSkipLevels];
-      } else {
-        payPointer = null;
-      }
-    } else {
-      posPointer = null;
-    }
-  }
-
-  /**
-   * Trim original docFreq to tell skipReader read proper number of skip points.
-   *
-   * Since our definition in Lucene41Skip* is a little different from MultiLevelSkip*
-   * This trimmed docFreq will prevent skipReader from:
-   * 1. silly reading a non-existed skip point after the last block boundary
-   * 2. moving into the vInt block
-   *
-   */
-  protected int trim(int df) {
-    return df % blockSize == 0? df - 1: df;
-  }
-
-  public void init(long skipPointer, long docBasePointer, long posBasePointer, long payBasePointer, int df) {
-    super.init(skipPointer, trim(df));
-    lastDocPointer = docBasePointer;
-    lastPosPointer = posBasePointer;
-    lastPayPointer = payBasePointer;
-
-    Arrays.fill(docPointer, docBasePointer);
-    if (posPointer != null) {
-      Arrays.fill(posPointer, posBasePointer);
-      if (payPointer != null) {
-        Arrays.fill(payPointer, payBasePointer);
-      }
-    } else {
-      assert posBasePointer == 0;
-    }
-  }
-
-  /** Returns the doc pointer of the doc to which the last call of 
-   * {@link MultiLevelSkipListReader#skipTo(int)} has skipped.  */
-  public long getDocPointer() {
-    return lastDocPointer;
-  }
-
-  public long getPosPointer() {
-    return lastPosPointer;
-  }
-
-  public int getPosBufferUpto() {
-    return lastPosBufferUpto;
-  }
-
-  public long getPayPointer() {
-    return lastPayPointer;
-  }
-
-  public int getPayloadByteUpto() {
-    return lastPayloadByteUpto;
-  }
-
-  public int getNextSkipDoc() {
-    return skipDoc[0];
-  }
-
-  @Override
-  protected void seekChild(int level) throws IOException {
-    super.seekChild(level);
-    // if (DEBUG) {
-    //   System.out.println("seekChild level=" + level);
-    // }
-    docPointer[level] = lastDocPointer;
-    if (posPointer != null) {
-      posPointer[level] = lastPosPointer;
-      posBufferUpto[level] = lastPosBufferUpto;
-      if (payloadByteUpto != null) {
-        payloadByteUpto[level] = lastPayloadByteUpto;
-      }
-      if (payPointer != null) {
-        payPointer[level] = lastPayPointer;
-      }
-    }
-  }
-  
-  @Override
-  protected void setLastSkipData(int level) {
-    super.setLastSkipData(level);
-    lastDocPointer = docPointer[level];
-    // if (DEBUG) {
-    //   System.out.println("setLastSkipData level=" + level);
-    //   System.out.println("  lastDocPointer=" + lastDocPointer);
-    // }
-    if (posPointer != null) {
-      lastPosPointer = posPointer[level];
-      lastPosBufferUpto = posBufferUpto[level];
-      // if (DEBUG) {
-      //   System.out.println("  lastPosPointer=" + lastPosPointer + " lastPosBUfferUpto=" + lastPosBufferUpto);
-      // }
-      if (payPointer != null) {
-        lastPayPointer = payPointer[level];
-      }
-      if (payloadByteUpto != null) {
-        lastPayloadByteUpto = payloadByteUpto[level];
-      }
-    }
-  }
-
-  @Override
-  protected int readSkipData(int level, IndexInput skipStream) throws IOException {
-    // if (DEBUG) {
-    //   System.out.println("readSkipData level=" + level);
-    // }
-    int delta = skipStream.readVInt();
-    // if (DEBUG) {
-    //   System.out.println("  delta=" + delta);
-    // }
-    docPointer[level] += skipStream.readVInt();
-    // if (DEBUG) {
-    //   System.out.println("  docFP=" + docPointer[level]);
-    // }
-
-    if (posPointer != null) {
-      posPointer[level] += skipStream.readVInt();
-      // if (DEBUG) {
-      //   System.out.println("  posFP=" + posPointer[level]);
-      // }
-      posBufferUpto[level] = skipStream.readVInt();
-      // if (DEBUG) {
-      //   System.out.println("  posBufferUpto=" + posBufferUpto[level]);
-      // }
-
-      if (payloadByteUpto != null) {
-        payloadByteUpto[level] = skipStream.readVInt();
-      }
-
-      if (payPointer != null) {
-        payPointer[level] += skipStream.readVInt();
-      }
-    }
-    return delta;
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsFormat.java
deleted file mode 100644
index 6001e71..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsFormat.java
+++ /dev/null
@@ -1,56 +0,0 @@
-package org.apache.lucene.codecs.lucene41;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.StoredFieldsFormat;
-import org.apache.lucene.codecs.StoredFieldsReader;
-import org.apache.lucene.codecs.StoredFieldsWriter;
-import org.apache.lucene.codecs.compressing.CompressionMode;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-/**
- * Lucene 4.1 stored fields format.
- * @deprecated only for reading old 4.x segments
- */
-@Deprecated
-public class Lucene41StoredFieldsFormat extends StoredFieldsFormat {
-  static final String FORMAT_NAME = "Lucene41StoredFields";
-  static final String SEGMENT_SUFFIX = "";
-  static final CompressionMode COMPRESSION_MODE = CompressionMode.FAST;
-  static final int CHUNK_SIZE = 1 << 14;
-
-  @Override
-  public final StoredFieldsReader fieldsReader(Directory directory, SegmentInfo si, FieldInfos fn, IOContext context) throws IOException {
-    return new Lucene41StoredFieldsReader(directory, si, SEGMENT_SUFFIX, fn, context, FORMAT_NAME, COMPRESSION_MODE);
-  }
-
-  @Override
-  public StoredFieldsWriter fieldsWriter(Directory directory, SegmentInfo si, IOContext context) throws IOException {
-    throw new UnsupportedOperationException("this codec can only be used for reading");
-  }
-
-  @Override
-  public String toString() {
-    return getClass().getSimpleName() + "(compressionMode=" + COMPRESSION_MODE + ", chunkSize=" + CHUNK_SIZE + ")";
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsIndexReader.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsIndexReader.java
deleted file mode 100644
index b0b7ac8..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsIndexReader.java
+++ /dev/null
@@ -1,214 +0,0 @@
-package org.apache.lucene.codecs.lucene41;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import static org.apache.lucene.util.BitUtil.zigZagDecode;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.Accountable;
-import org.apache.lucene.util.Accountables;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.RamUsageEstimator;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * Reader for 4.x stored fields/term vectors index
- * @deprecated only for reading old segments
- */
-@Deprecated
-public final class Lucene41StoredFieldsIndexReader implements Cloneable, Accountable {
-
-  private static final long BASE_RAM_BYTES_USED = RamUsageEstimator.shallowSizeOfInstance(Lucene41StoredFieldsIndexReader.class);
-
-  final int maxDoc;
-  final int[] docBases;
-  final long[] startPointers;
-  final int[] avgChunkDocs;
-  final long[] avgChunkSizes;
-  final PackedInts.Reader[] docBasesDeltas; // delta from the avg
-  final PackedInts.Reader[] startPointersDeltas; // delta from the avg
-
-  // It is the responsibility of the caller to close fieldsIndexIn after this constructor
-  // has been called
-  public Lucene41StoredFieldsIndexReader(IndexInput fieldsIndexIn, SegmentInfo si) throws IOException {
-    maxDoc = si.getDocCount();
-    int[] docBases = new int[16];
-    long[] startPointers = new long[16];
-    int[] avgChunkDocs = new int[16];
-    long[] avgChunkSizes = new long[16];
-    PackedInts.Reader[] docBasesDeltas = new PackedInts.Reader[16];
-    PackedInts.Reader[] startPointersDeltas = new PackedInts.Reader[16];
-
-    final int packedIntsVersion = fieldsIndexIn.readVInt();
-
-    int blockCount = 0;
-
-    for (;;) {
-      final int numChunks = fieldsIndexIn.readVInt();
-      if (numChunks == 0) {
-        break;
-      }
-      if (blockCount == docBases.length) {
-        final int newSize = ArrayUtil.oversize(blockCount + 1, 8);
-        docBases = Arrays.copyOf(docBases, newSize);
-        startPointers = Arrays.copyOf(startPointers, newSize);
-        avgChunkDocs = Arrays.copyOf(avgChunkDocs, newSize);
-        avgChunkSizes = Arrays.copyOf(avgChunkSizes, newSize);
-        docBasesDeltas = Arrays.copyOf(docBasesDeltas, newSize);
-        startPointersDeltas = Arrays.copyOf(startPointersDeltas, newSize);
-      }
-
-      // doc bases
-      docBases[blockCount] = fieldsIndexIn.readVInt();
-      avgChunkDocs[blockCount] = fieldsIndexIn.readVInt();
-      final int bitsPerDocBase = fieldsIndexIn.readVInt();
-      if (bitsPerDocBase > 32) {
-        throw new CorruptIndexException("Corrupted bitsPerDocBase: " + bitsPerDocBase, fieldsIndexIn);
-      }
-      docBasesDeltas[blockCount] = PackedInts.getReaderNoHeader(fieldsIndexIn, PackedInts.Format.PACKED, packedIntsVersion, numChunks, bitsPerDocBase);
-
-      // start pointers
-      startPointers[blockCount] = fieldsIndexIn.readVLong();
-      avgChunkSizes[blockCount] = fieldsIndexIn.readVLong();
-      final int bitsPerStartPointer = fieldsIndexIn.readVInt();
-      if (bitsPerStartPointer > 64) {
-        throw new CorruptIndexException("Corrupted bitsPerStartPointer: " + bitsPerStartPointer, fieldsIndexIn);
-      }
-      startPointersDeltas[blockCount] = PackedInts.getReaderNoHeader(fieldsIndexIn, PackedInts.Format.PACKED, packedIntsVersion, numChunks, bitsPerStartPointer);
-
-      ++blockCount;
-    }
-
-    this.docBases = Arrays.copyOf(docBases, blockCount);
-    this.startPointers = Arrays.copyOf(startPointers, blockCount);
-    this.avgChunkDocs = Arrays.copyOf(avgChunkDocs, blockCount);
-    this.avgChunkSizes = Arrays.copyOf(avgChunkSizes, blockCount);
-    this.docBasesDeltas = Arrays.copyOf(docBasesDeltas, blockCount);
-    this.startPointersDeltas = Arrays.copyOf(startPointersDeltas, blockCount);
-  }
-
-  private int block(int docID) {
-    int lo = 0, hi = docBases.length - 1;
-    while (lo <= hi) {
-      final int mid = (lo + hi) >>> 1;
-      final int midValue = docBases[mid];
-      if (midValue == docID) {
-        return mid;
-      } else if (midValue < docID) {
-        lo = mid + 1;
-      } else {
-        hi = mid - 1;
-      }
-    }
-    return hi;
-  }
-
-  private int relativeDocBase(int block, int relativeChunk) {
-    final int expected = avgChunkDocs[block] * relativeChunk;
-    final long delta = zigZagDecode(docBasesDeltas[block].get(relativeChunk));
-    return expected + (int) delta;
-  }
-
-  private long relativeStartPointer(int block, int relativeChunk) {
-    final long expected = avgChunkSizes[block] * relativeChunk;
-    final long delta = zigZagDecode(startPointersDeltas[block].get(relativeChunk));
-    return expected + delta;
-  }
-
-  private int relativeChunk(int block, int relativeDoc) {
-    int lo = 0, hi = docBasesDeltas[block].size() - 1;
-    while (lo <= hi) {
-      final int mid = (lo + hi) >>> 1;
-      final int midValue = relativeDocBase(block, mid);
-      if (midValue == relativeDoc) {
-        return mid;
-      } else if (midValue < relativeDoc) {
-        lo = mid + 1;
-      } else {
-        hi = mid - 1;
-      }
-    }
-    return hi;
-  }
-
-  public long getStartPointer(int docID) {
-    if (docID < 0 || docID >= maxDoc) {
-      throw new IllegalArgumentException("docID out of range [0-" + maxDoc + "]: " + docID);
-    }
-    final int block = block(docID);
-    final int relativeChunk = relativeChunk(block, docID - docBases[block]);
-    return startPointers[block] + relativeStartPointer(block, relativeChunk);
-  }
-
-  @Override
-  public Lucene41StoredFieldsIndexReader clone() {
-    return this;
-  }
-
-  @Override
-  public long ramBytesUsed() {
-    long res = BASE_RAM_BYTES_USED;
-
-    res += RamUsageEstimator.shallowSizeOf(docBasesDeltas);
-    for (PackedInts.Reader r : docBasesDeltas) {
-      res += r.ramBytesUsed();
-    }
-    res += RamUsageEstimator.shallowSizeOf(startPointersDeltas);
-    for (PackedInts.Reader r : startPointersDeltas) {
-      res += r.ramBytesUsed();
-    }
-
-    res += RamUsageEstimator.sizeOf(docBases);
-    res += RamUsageEstimator.sizeOf(startPointers);
-    res += RamUsageEstimator.sizeOf(avgChunkDocs); 
-    res += RamUsageEstimator.sizeOf(avgChunkSizes);
-
-    return res;
-  }
-
-  @Override
-  public Iterable<? extends Accountable> getChildResources() {
-    List<Accountable> resources = new ArrayList<>();
-    
-    long docBaseDeltaBytes = RamUsageEstimator.shallowSizeOf(docBasesDeltas);
-    for (PackedInts.Reader r : docBasesDeltas) {
-      docBaseDeltaBytes += r.ramBytesUsed();
-    }
-    resources.add(Accountables.namedAccountable("doc base deltas", docBaseDeltaBytes));
-    
-    long startPointerDeltaBytes = RamUsageEstimator.shallowSizeOf(startPointersDeltas);
-    for (PackedInts.Reader r : startPointersDeltas) {
-      startPointerDeltaBytes += r.ramBytesUsed();
-    }
-    resources.add(Accountables.namedAccountable("start pointer deltas", startPointerDeltaBytes));
-    
-    return resources;
-  }
-
-  @Override
-  public String toString() {
-    return getClass().getSimpleName() + "(blocks=" + docBases.length + ")";
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsReader.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsReader.java
deleted file mode 100644
index 43d1e7e..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsReader.java
+++ /dev/null
@@ -1,417 +0,0 @@
-package org.apache.lucene.codecs.lucene41;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.EOFException;
-import java.io.IOException;
-import java.nio.charset.StandardCharsets;
-import java.util.Collections;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.StoredFieldsReader;
-import org.apache.lucene.codecs.compressing.CompressionMode;
-import org.apache.lucene.codecs.compressing.Decompressor;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.StoredFieldVisitor;
-import org.apache.lucene.store.AlreadyClosedException;
-import org.apache.lucene.store.ByteArrayDataInput;
-import org.apache.lucene.store.ChecksumIndexInput;
-import org.apache.lucene.store.DataInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.Accountable;
-import org.apache.lucene.util.Accountables;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * {@link StoredFieldsReader} impl for {@code Lucene41StoredFieldsFormat}.
- * @deprecated only for reading old segments
- */
-@Deprecated
-final class Lucene41StoredFieldsReader extends StoredFieldsReader {
-
-  // Do not reuse the decompression buffer when there is more than 32kb to decompress
-  private static final int BUFFER_REUSE_THRESHOLD = 1 << 15;
-  
-  static final int         STRING = 0x00;
-  static final int       BYTE_ARR = 0x01;
-  static final int    NUMERIC_INT = 0x02;
-  static final int  NUMERIC_FLOAT = 0x03;
-  static final int   NUMERIC_LONG = 0x04;
-  static final int NUMERIC_DOUBLE = 0x05;
-  
-  static final String CODEC_SFX_IDX = "Index";
-  static final String CODEC_SFX_DAT = "Data";
-  
-  static final int TYPE_BITS = PackedInts.bitsRequired(NUMERIC_DOUBLE);
-  static final int TYPE_MASK = (int) PackedInts.maxValue(TYPE_BITS);
-  
-  static final int VERSION_START = 0;
-  static final int VERSION_BIG_CHUNKS = 1;
-  static final int VERSION_CHECKSUM = 2;
-  static final int VERSION_CURRENT = VERSION_CHECKSUM;
-  
-  /** Extension of stored fields file */
-  public static final String FIELDS_EXTENSION = "fdt";
-  
-  /** Extension of stored fields index file */
-  public static final String FIELDS_INDEX_EXTENSION = "fdx";
-
-  private final int version;
-  private final FieldInfos fieldInfos;
-  private final Lucene41StoredFieldsIndexReader indexReader;
-  private final long maxPointer;
-  private final IndexInput fieldsStream;
-  private final int chunkSize;
-  private final int packedIntsVersion;
-  private final CompressionMode compressionMode;
-  private final Decompressor decompressor;
-  private final BytesRef bytes;
-  private final int numDocs;
-  private boolean closed;
-
-  // used by clone
-  private Lucene41StoredFieldsReader(Lucene41StoredFieldsReader reader) {
-    this.version = reader.version;
-    this.fieldInfos = reader.fieldInfos;
-    this.fieldsStream = reader.fieldsStream.clone();
-    this.indexReader = reader.indexReader.clone();
-    this.maxPointer = reader.maxPointer;
-    this.chunkSize = reader.chunkSize;
-    this.packedIntsVersion = reader.packedIntsVersion;
-    this.compressionMode = reader.compressionMode;
-    this.decompressor = reader.decompressor.clone();
-    this.numDocs = reader.numDocs;
-    this.bytes = new BytesRef(reader.bytes.bytes.length);
-    this.closed = false;
-  }
-
-  /** Sole constructor. */
-  public Lucene41StoredFieldsReader(Directory d, SegmentInfo si, String segmentSuffix, FieldInfos fn,
-      IOContext context, String formatName, CompressionMode compressionMode) throws IOException {
-    this.compressionMode = compressionMode;
-    final String segment = si.name;
-    boolean success = false;
-    fieldInfos = fn;
-    numDocs = si.getDocCount();
-    ChecksumIndexInput indexStream = null;
-    try {
-      final String indexStreamFN = IndexFileNames.segmentFileName(segment, segmentSuffix, FIELDS_INDEX_EXTENSION);
-      final String fieldsStreamFN = IndexFileNames.segmentFileName(segment, segmentSuffix, FIELDS_EXTENSION);
-      // Load the index into memory
-      indexStream = d.openChecksumInput(indexStreamFN, context);
-      final String codecNameIdx = formatName + CODEC_SFX_IDX;
-      version = CodecUtil.checkHeader(indexStream, codecNameIdx, VERSION_START, VERSION_CURRENT);
-      assert CodecUtil.headerLength(codecNameIdx) == indexStream.getFilePointer();
-      indexReader = new Lucene41StoredFieldsIndexReader(indexStream, si);
-
-      long maxPointer = -1;
-      
-      if (version >= VERSION_CHECKSUM) {
-        maxPointer = indexStream.readVLong();
-        CodecUtil.checkFooter(indexStream);
-      } else {
-        CodecUtil.checkEOF(indexStream);
-      }
-      indexStream.close();
-      indexStream = null;
-
-      // Open the data file and read metadata
-      fieldsStream = d.openInput(fieldsStreamFN, context);
-      if (version >= VERSION_CHECKSUM) {
-        if (maxPointer + CodecUtil.footerLength() != fieldsStream.length()) {
-          throw new CorruptIndexException("Invalid fieldsStream maxPointer (file truncated?): maxPointer=" + maxPointer + ", length=" + fieldsStream.length(), fieldsStream);
-        }
-      } else {
-        maxPointer = fieldsStream.length();
-      }
-      this.maxPointer = maxPointer;
-      final String codecNameDat = formatName + CODEC_SFX_DAT;
-      final int fieldsVersion = CodecUtil.checkHeader(fieldsStream, codecNameDat, VERSION_START, VERSION_CURRENT);
-      if (version != fieldsVersion) {
-        throw new CorruptIndexException("Version mismatch between stored fields index and data: " + version + " != " + fieldsVersion, fieldsStream);
-      }
-      assert CodecUtil.headerLength(codecNameDat) == fieldsStream.getFilePointer();
-
-      if (version >= VERSION_BIG_CHUNKS) {
-        chunkSize = fieldsStream.readVInt();
-      } else {
-        chunkSize = -1;
-      }
-      packedIntsVersion = fieldsStream.readVInt();
-      decompressor = compressionMode.newDecompressor();
-      this.bytes = new BytesRef();
-      
-      if (version >= VERSION_CHECKSUM) {
-        // NOTE: data file is too costly to verify checksum against all the bytes on open,
-        // but for now we at least verify proper structure of the checksum footer: which looks
-        // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
-        // such as file truncation.
-        CodecUtil.retrieveChecksum(fieldsStream);
-      }
-
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(this, indexStream);
-      }
-    }
-  }
-
-  /**
-   * @throws AlreadyClosedException if this FieldsReader is closed
-   */
-  private void ensureOpen() throws AlreadyClosedException {
-    if (closed) {
-      throw new AlreadyClosedException("this FieldsReader is closed");
-    }
-  }
-
-  /** 
-   * Close the underlying {@link IndexInput}s.
-   */
-  @Override
-  public void close() throws IOException {
-    if (!closed) {
-      IOUtils.close(fieldsStream);
-      closed = true;
-    }
-  }
-
-  private static void readField(DataInput in, StoredFieldVisitor visitor, FieldInfo info, int bits) throws IOException {
-    switch (bits & TYPE_MASK) {
-      case BYTE_ARR:
-        int length = in.readVInt();
-        byte[] data = new byte[length];
-        in.readBytes(data, 0, length);
-        visitor.binaryField(info, data);
-        break;
-      case STRING:
-        length = in.readVInt();
-        data = new byte[length];
-        in.readBytes(data, 0, length);
-        visitor.stringField(info, new String(data, StandardCharsets.UTF_8));
-        break;
-      case NUMERIC_INT:
-        visitor.intField(info, in.readInt());
-        break;
-      case NUMERIC_FLOAT:
-        visitor.floatField(info, Float.intBitsToFloat(in.readInt()));
-        break;
-      case NUMERIC_LONG:
-        visitor.longField(info, in.readLong());
-        break;
-      case NUMERIC_DOUBLE:
-        visitor.doubleField(info, Double.longBitsToDouble(in.readLong()));
-        break;
-      default:
-        throw new AssertionError("Unknown type flag: " + Integer.toHexString(bits));
-    }
-  }
-
-  private static void skipField(DataInput in, int bits) throws IOException {
-    switch (bits & TYPE_MASK) {
-      case BYTE_ARR:
-      case STRING:
-        final int length = in.readVInt();
-        in.skipBytes(length);
-        break;
-      case NUMERIC_INT:
-      case NUMERIC_FLOAT:
-        in.readInt();
-        break;
-      case NUMERIC_LONG:
-      case NUMERIC_DOUBLE:
-        in.readLong();
-        break;
-      default:
-        throw new AssertionError("Unknown type flag: " + Integer.toHexString(bits));
-    }
-  }
-
-  @Override
-  public void visitDocument(int docID, StoredFieldVisitor visitor)
-      throws IOException {
-    fieldsStream.seek(indexReader.getStartPointer(docID));
-
-    final int docBase = fieldsStream.readVInt();
-    final int chunkDocs = fieldsStream.readVInt();
-    if (docID < docBase
-        || docID >= docBase + chunkDocs
-        || docBase + chunkDocs > numDocs) {
-      throw new CorruptIndexException("Corrupted: docID=" + docID
-          + ", docBase=" + docBase + ", chunkDocs=" + chunkDocs
-          + ", numDocs=" + numDocs, fieldsStream);
-    }
-
-    final int numStoredFields, offset, length, totalLength;
-    if (chunkDocs == 1) {
-      numStoredFields = fieldsStream.readVInt();
-      offset = 0;
-      length = fieldsStream.readVInt();
-      totalLength = length;
-    } else {
-      final int bitsPerStoredFields = fieldsStream.readVInt();
-      if (bitsPerStoredFields == 0) {
-        numStoredFields = fieldsStream.readVInt();
-      } else if (bitsPerStoredFields > 31) {
-        throw new CorruptIndexException("bitsPerStoredFields=" + bitsPerStoredFields, fieldsStream);
-      } else {
-        final long filePointer = fieldsStream.getFilePointer();
-        final PackedInts.Reader reader = PackedInts.getDirectReaderNoHeader(fieldsStream, PackedInts.Format.PACKED, packedIntsVersion, chunkDocs, bitsPerStoredFields);
-        numStoredFields = (int) (reader.get(docID - docBase));
-        fieldsStream.seek(filePointer + PackedInts.Format.PACKED.byteCount(packedIntsVersion, chunkDocs, bitsPerStoredFields));
-      }
-
-      final int bitsPerLength = fieldsStream.readVInt();
-      if (bitsPerLength == 0) {
-        length = fieldsStream.readVInt();
-        offset = (docID - docBase) * length;
-        totalLength = chunkDocs * length;
-      } else if (bitsPerStoredFields > 31) {
-        throw new CorruptIndexException("bitsPerLength=" + bitsPerLength, fieldsStream);
-      } else {
-        final PackedInts.ReaderIterator it = PackedInts.getReaderIteratorNoHeader(fieldsStream, PackedInts.Format.PACKED, packedIntsVersion, chunkDocs, bitsPerLength, 1);
-        int off = 0;
-        for (int i = 0; i < docID - docBase; ++i) {
-          off += it.next();
-        }
-        offset = off;
-        length = (int) it.next();
-        off += length;
-        for (int i = docID - docBase + 1; i < chunkDocs; ++i) {
-          off += it.next();
-        }
-        totalLength = off;
-      }
-    }
-
-    if ((length == 0) != (numStoredFields == 0)) {
-      throw new CorruptIndexException("length=" + length + ", numStoredFields=" + numStoredFields, fieldsStream);
-    }
-    if (numStoredFields == 0) {
-      // nothing to do
-      return;
-    }
-
-    final DataInput documentInput;
-    if (version >= VERSION_BIG_CHUNKS && totalLength >= 2 * chunkSize) {
-      assert chunkSize > 0;
-      assert offset < chunkSize;
-
-      decompressor.decompress(fieldsStream, chunkSize, offset, Math.min(length, chunkSize - offset), bytes);
-      documentInput = new DataInput() {
-
-        int decompressed = bytes.length;
-
-        void fillBuffer() throws IOException {
-          assert decompressed <= length;
-          if (decompressed == length) {
-            throw new EOFException();
-          }
-          final int toDecompress = Math.min(length - decompressed, chunkSize);
-          decompressor.decompress(fieldsStream, toDecompress, 0, toDecompress, bytes);
-          decompressed += toDecompress;
-        }
-
-        @Override
-        public byte readByte() throws IOException {
-          if (bytes.length == 0) {
-            fillBuffer();
-          }
-          --bytes.length;
-          return bytes.bytes[bytes.offset++];
-        }
-
-        @Override
-        public void readBytes(byte[] b, int offset, int len) throws IOException {
-          while (len > bytes.length) {
-            System.arraycopy(bytes.bytes, bytes.offset, b, offset, bytes.length);
-            len -= bytes.length;
-            offset += bytes.length;
-            fillBuffer();
-          }
-          System.arraycopy(bytes.bytes, bytes.offset, b, offset, len);
-          bytes.offset += len;
-          bytes.length -= len;
-        }
-
-      };
-    } else {
-      final BytesRef bytes = totalLength <= BUFFER_REUSE_THRESHOLD ? this.bytes : new BytesRef();
-      decompressor.decompress(fieldsStream, totalLength, offset, length, bytes);
-      assert bytes.length == length;
-      documentInput = new ByteArrayDataInput(bytes.bytes, bytes.offset, bytes.length);
-    }
-
-    for (int fieldIDX = 0; fieldIDX < numStoredFields; fieldIDX++) {
-      final long infoAndBits = documentInput.readVLong();
-      final int fieldNumber = (int) (infoAndBits >>> TYPE_BITS);
-      final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);
-
-      final int bits = (int) (infoAndBits & TYPE_MASK);
-      assert bits <= NUMERIC_DOUBLE: "bits=" + Integer.toHexString(bits);
-
-      switch(visitor.needsField(fieldInfo)) {
-        case YES:
-          readField(documentInput, visitor, fieldInfo, bits);
-          break;
-        case NO:
-          skipField(documentInput, bits);
-          break;
-        case STOP:
-          return;
-      }
-    }
-  }
-
-  @Override
-  public StoredFieldsReader clone() {
-    ensureOpen();
-    return new Lucene41StoredFieldsReader(this);
-  }
-
-  @Override
-  public long ramBytesUsed() {
-    return indexReader.ramBytesUsed();
-  }
-  
-  @Override
-  public Iterable<? extends Accountable> getChildResources() {
-    return Collections.singleton(Accountables.namedAccountable("stored field index", indexReader));
-  }
-
-  @Override
-  public void checkIntegrity() throws IOException {
-    if (version >= VERSION_CHECKSUM) {
-      CodecUtil.checksumEntireFile(fieldsStream);
-    }
-  }
-
-  @Override
-  public String toString() {
-    return getClass().getSimpleName() + "(mode=" + compressionMode + ",chunksize=" + chunkSize + ")";
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/package.html b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/package.html
deleted file mode 100644
index abea0c2..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Lucene 4.1 file format.
-</body>
-</html>
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene410/Lucene410Codec.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene410/Lucene410Codec.java
deleted file mode 100644
index 6c40db9..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene410/Lucene410Codec.java
+++ /dev/null
@@ -1,148 +0,0 @@
-package org.apache.lucene.codecs.lucene410;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.CompoundFormat;
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.LiveDocsFormat;
-import org.apache.lucene.codecs.NormsConsumer;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.SegmentInfoFormat;
-import org.apache.lucene.codecs.StoredFieldsFormat;
-import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40CompoundFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
-import org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat;
-import org.apache.lucene.codecs.lucene42.Lucene42TermVectorsFormat;
-import org.apache.lucene.codecs.lucene46.Lucene46FieldInfosFormat;
-import org.apache.lucene.codecs.lucene46.Lucene46SegmentInfoFormat;
-import org.apache.lucene.codecs.lucene49.Lucene49NormsFormat;
-import org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat;
-import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
-import org.apache.lucene.index.SegmentWriteState;
-
-/**
- * Implements the Lucene 4.10 codec
- * @deprecated only for reading old 4.10 segments
- */
-@Deprecated
-public class Lucene410Codec extends Codec {
-  private final StoredFieldsFormat fieldsFormat = new Lucene41StoredFieldsFormat();
-  private final TermVectorsFormat vectorsFormat = new Lucene42TermVectorsFormat();
-  private final FieldInfosFormat fieldInfosFormat = new Lucene46FieldInfosFormat();
-  private final SegmentInfoFormat segmentInfosFormat = new Lucene46SegmentInfoFormat();
-  private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat();
-  private final CompoundFormat compoundFormat = new Lucene40CompoundFormat();
-  
-  private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
-    @Override
-    public PostingsFormat getPostingsFormatForField(String field) {
-      return Lucene410Codec.this.getPostingsFormatForField(field);
-    }
-  };
-  
-  private final DocValuesFormat docValuesFormat = new PerFieldDocValuesFormat() {
-    @Override
-    public DocValuesFormat getDocValuesFormatForField(String field) {
-      return Lucene410Codec.this.getDocValuesFormatForField(field);
-    }
-  };
-
-  /** Sole constructor. */
-  public Lucene410Codec() {
-    super("Lucene410");
-  }
-  
-  @Override
-  public StoredFieldsFormat storedFieldsFormat() {
-    return fieldsFormat;
-  }
-  
-  @Override
-  public TermVectorsFormat termVectorsFormat() {
-    return vectorsFormat;
-  }
-
-  @Override
-  public final PostingsFormat postingsFormat() {
-    return postingsFormat;
-  }
-  
-  @Override
-  public final FieldInfosFormat fieldInfosFormat() {
-    return fieldInfosFormat;
-  }
-  
-  @Override
-  public SegmentInfoFormat segmentInfoFormat() {
-    return segmentInfosFormat;
-  }
-  
-  @Override
-  public final LiveDocsFormat liveDocsFormat() {
-    return liveDocsFormat;
-  }
-  
-  @Override
-  public CompoundFormat compoundFormat() {
-    return compoundFormat;
-  }
-
-  /** Returns the postings format that should be used for writing 
-   *  new segments of <code>field</code>.
-   *  
-   *  The default implementation always returns "Lucene41"
-   */
-  public PostingsFormat getPostingsFormatForField(String field) {
-    return defaultFormat;
-  }
-  
-  /** Returns the docvalues format that should be used for writing 
-   *  new segments of <code>field</code>.
-   *  
-   *  The default implementation always returns "Lucene410"
-   */
-  public DocValuesFormat getDocValuesFormatForField(String field) {
-    return defaultDVFormat;
-  }
-  
-  @Override
-  public DocValuesFormat docValuesFormat() {
-    return docValuesFormat;
-  }
-
-  private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene41");
-  private final DocValuesFormat defaultDVFormat = DocValuesFormat.forName("Lucene410");
-
-  private final NormsFormat normsFormat = new Lucene49NormsFormat() {
-    @Override
-    public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
-      throw new UnsupportedOperationException("this codec can only be used for reading");
-    }
-  };
-
-  @Override
-  public NormsFormat normsFormat() {
-    return normsFormat;
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene410/Lucene410DocValuesConsumer.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene410/Lucene410DocValuesConsumer.java
deleted file mode 100644
index c230d33..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene410/Lucene410DocValuesConsumer.java
+++ /dev/null
@@ -1,579 +0,0 @@
-package org.apache.lucene.codecs.lucene410;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable; // javadocs
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.HashSet;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.FieldInfo.DocValuesType;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.RAMOutputStream;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.BytesRefBuilder;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.MathUtil;
-import org.apache.lucene.util.PagedBytes;
-import org.apache.lucene.util.PagedBytes.PagedBytesDataInput;
-import org.apache.lucene.util.StringHelper;
-import org.apache.lucene.util.packed.DirectWriter;
-import org.apache.lucene.util.packed.MonotonicBlockPackedWriter;
-import org.apache.lucene.util.packed.PackedInts;
-
-/** 
- * writer for 4.10 docvalues format
- * @deprecated only for old 4.x segments
- */
-@Deprecated
-class Lucene410DocValuesConsumer extends DocValuesConsumer implements Closeable {
-
-  static final int BLOCK_SIZE = 16384;
-  
-  // address terms in blocks of 16 terms
-  static final int INTERVAL_SHIFT = 4;
-  static final int INTERVAL_COUNT = 1 << INTERVAL_SHIFT;
-  static final int INTERVAL_MASK = INTERVAL_COUNT - 1;
-  
-  // build reverse index from every 1024th term
-  static final int REVERSE_INTERVAL_SHIFT = 10;
-  static final int REVERSE_INTERVAL_COUNT = 1 << REVERSE_INTERVAL_SHIFT;
-  static final int REVERSE_INTERVAL_MASK = REVERSE_INTERVAL_COUNT - 1;
-  
-  // for conversion from reverse index to block
-  static final int BLOCK_INTERVAL_SHIFT = REVERSE_INTERVAL_SHIFT - INTERVAL_SHIFT;
-  static final int BLOCK_INTERVAL_COUNT = 1 << BLOCK_INTERVAL_SHIFT;
-  static final int BLOCK_INTERVAL_MASK = BLOCK_INTERVAL_COUNT - 1;
-
-  /** Compressed using packed blocks of ints. */
-  public static final int DELTA_COMPRESSED = 0;
-  /** Compressed by computing the GCD. */
-  public static final int GCD_COMPRESSED = 1;
-  /** Compressed by giving IDs to unique values. */
-  public static final int TABLE_COMPRESSED = 2;
-  /** Compressed with monotonically increasing values */
-  public static final int MONOTONIC_COMPRESSED = 3;
-  
-  /** Uncompressed binary, written directly (fixed length). */
-  public static final int BINARY_FIXED_UNCOMPRESSED = 0;
-  /** Uncompressed binary, written directly (variable length). */
-  public static final int BINARY_VARIABLE_UNCOMPRESSED = 1;
-  /** Compressed binary with shared prefixes */
-  public static final int BINARY_PREFIX_COMPRESSED = 2;
-
-  /** Standard storage for sorted set values with 1 level of indirection:
-   *  docId -> address -> ord. */
-  public static final int SORTED_WITH_ADDRESSES = 0;
-  /** Single-valued sorted set values, encoded as sorted values, so no level
-   *  of indirection: docId -> ord. */
-  public static final int SORTED_SINGLE_VALUED = 1;
-
-  IndexOutput data, meta;
-  final int maxDoc;
-  
-  /** expert: Creates a new writer */
-  public Lucene410DocValuesConsumer(SegmentWriteState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
-    boolean success = false;
-    try {
-      String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
-      data = state.directory.createOutput(dataName, state.context);
-      CodecUtil.writeHeader(data, dataCodec, Lucene410DocValuesFormat.VERSION_CURRENT);
-      String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
-      meta = state.directory.createOutput(metaName, state.context);
-      CodecUtil.writeHeader(meta, metaCodec, Lucene410DocValuesFormat.VERSION_CURRENT);
-      maxDoc = state.segmentInfo.getDocCount();
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(this);
-      }
-    }
-  }
-  
-  @Override
-  public void addNumericField(FieldInfo field, Iterable<Number> values) throws IOException {
-    checkCanWrite(field);
-    addNumericField(field, values, true);
-  }
-
-  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {
-    long count = 0;
-    long minValue = Long.MAX_VALUE;
-    long maxValue = Long.MIN_VALUE;
-    long gcd = 0;
-    boolean missing = false;
-    // TODO: more efficient?
-    HashSet<Long> uniqueValues = null;
-    if (optimizeStorage) {
-      uniqueValues = new HashSet<>();
-
-      for (Number nv : values) {
-        final long v;
-        if (nv == null) {
-          v = 0;
-          missing = true;
-        } else {
-          v = nv.longValue();
-        }
-
-        if (gcd != 1) {
-          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {
-            // in that case v - minValue might overflow and make the GCD computation return
-            // wrong results. Since these extreme values are unlikely, we just discard
-            // GCD computation for them
-            gcd = 1;
-          } else if (count != 0) { // minValue needs to be set first
-            gcd = MathUtil.gcd(gcd, v - minValue);
-          }
-        }
-
-        minValue = Math.min(minValue, v);
-        maxValue = Math.max(maxValue, v);
-
-        if (uniqueValues != null) {
-          if (uniqueValues.add(v)) {
-            if (uniqueValues.size() > 256) {
-              uniqueValues = null;
-            }
-          }
-        }
-
-        ++count;
-      }
-    } else {
-      for (Number nv : values) {
-        long v = nv.longValue();
-        minValue = Math.min(minValue, v);
-        maxValue = Math.max(maxValue, v);
-        ++count;
-      }
-    }
-    
-    final long delta = maxValue - minValue;
-    final int deltaBitsRequired = DirectWriter.unsignedBitsRequired(delta);
-    final int tableBitsRequired = uniqueValues == null
-        ? Integer.MAX_VALUE
-        : DirectWriter.bitsRequired(uniqueValues.size() - 1);
-
-    final int format;
-    if (uniqueValues != null && tableBitsRequired < deltaBitsRequired) {
-      format = TABLE_COMPRESSED;
-    } else if (gcd != 0 && gcd != 1) {
-      final long gcdDelta = (maxValue - minValue) / gcd;
-      final long gcdBitsRequired = DirectWriter.unsignedBitsRequired(gcdDelta);
-      format = gcdBitsRequired < deltaBitsRequired ? GCD_COMPRESSED : DELTA_COMPRESSED;
-    } else {
-      format = DELTA_COMPRESSED;
-    }
-    meta.writeVInt(field.number);
-    meta.writeByte(Lucene410DocValuesFormat.NUMERIC);
-    meta.writeVInt(format);
-    if (missing) {
-      meta.writeLong(data.getFilePointer());
-      writeMissingBitset(values);
-    } else {
-      meta.writeLong(-1L);
-    }
-    meta.writeLong(data.getFilePointer());
-    meta.writeVLong(count);
-
-    switch (format) {
-      case GCD_COMPRESSED:
-        meta.writeLong(minValue);
-        meta.writeLong(gcd);
-        final long maxDelta = (maxValue - minValue) / gcd;
-        final int bits = DirectWriter.unsignedBitsRequired(maxDelta);
-        meta.writeVInt(bits);
-        final DirectWriter quotientWriter = DirectWriter.getInstance(data, count, bits);
-        for (Number nv : values) {
-          long value = nv == null ? 0 : nv.longValue();
-          quotientWriter.add((value - minValue) / gcd);
-        }
-        quotientWriter.finish();
-        break;
-      case DELTA_COMPRESSED:
-        final long minDelta = delta < 0 ? 0 : minValue;
-        meta.writeLong(minDelta);
-        meta.writeVInt(deltaBitsRequired);
-        final DirectWriter writer = DirectWriter.getInstance(data, count, deltaBitsRequired);
-        for (Number nv : values) {
-          long v = nv == null ? 0 : nv.longValue();
-          writer.add(v - minDelta);
-        }
-        writer.finish();
-        break;
-      case TABLE_COMPRESSED:
-        final Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);
-        Arrays.sort(decode);
-        final HashMap<Long,Integer> encode = new HashMap<>();
-        meta.writeVInt(decode.length);
-        for (int i = 0; i < decode.length; i++) {
-          meta.writeLong(decode[i]);
-          encode.put(decode[i], i);
-        }
-        meta.writeVInt(tableBitsRequired);
-        final DirectWriter ordsWriter = DirectWriter.getInstance(data, count, tableBitsRequired);
-        for (Number nv : values) {
-          ordsWriter.add(encode.get(nv == null ? 0 : nv.longValue()));
-        }
-        ordsWriter.finish();
-        break;
-      default:
-        throw new AssertionError();
-    }
-    meta.writeLong(data.getFilePointer());
-  }
-  
-  // TODO: in some cases representing missing with minValue-1 wouldn't take up additional space and so on,
-  // but this is very simple, and algorithms only check this for values of 0 anyway (doesnt slow down normal decode)
-  void writeMissingBitset(Iterable<?> values) throws IOException {
-    byte bits = 0;
-    int count = 0;
-    for (Object v : values) {
-      if (count == 8) {
-        data.writeByte(bits);
-        count = 0;
-        bits = 0;
-      }
-      if (v != null) {
-        bits |= 1 << (count & 7);
-      }
-      count++;
-    }
-    if (count > 0) {
-      data.writeByte(bits);
-    }
-  }
-
-  @Override
-  public void addBinaryField(FieldInfo field, Iterable<BytesRef> values) throws IOException {
-    checkCanWrite(field);
-    // write the byte[] data
-    meta.writeVInt(field.number);
-    meta.writeByte(Lucene410DocValuesFormat.BINARY);
-    int minLength = Integer.MAX_VALUE;
-    int maxLength = Integer.MIN_VALUE;
-    final long startFP = data.getFilePointer();
-    long count = 0;
-    boolean missing = false;
-    for(BytesRef v : values) {
-      final int length;
-      if (v == null) {
-        length = 0;
-        missing = true;
-      } else {
-        length = v.length;
-      }
-      minLength = Math.min(minLength, length);
-      maxLength = Math.max(maxLength, length);
-      if (v != null) {
-        data.writeBytes(v.bytes, v.offset, v.length);
-      }
-      count++;
-    }
-    meta.writeVInt(minLength == maxLength ? BINARY_FIXED_UNCOMPRESSED : BINARY_VARIABLE_UNCOMPRESSED);
-    if (missing) {
-      meta.writeLong(data.getFilePointer());
-      writeMissingBitset(values);
-    } else {
-      meta.writeLong(-1L);
-    }
-    meta.writeVInt(minLength);
-    meta.writeVInt(maxLength);
-    meta.writeVLong(count);
-    meta.writeLong(startFP);
-    
-    // if minLength == maxLength, its a fixed-length byte[], we are done (the addresses are implicit)
-    // otherwise, we need to record the length fields...
-    if (minLength != maxLength) {
-      meta.writeLong(data.getFilePointer());
-      meta.writeVInt(PackedInts.VERSION_CURRENT);
-      meta.writeVInt(BLOCK_SIZE);
-
-      final MonotonicBlockPackedWriter writer = new MonotonicBlockPackedWriter(data, BLOCK_SIZE);
-      long addr = 0;
-      writer.add(addr);
-      for (BytesRef v : values) {
-        if (v != null) {
-          addr += v.length;
-        }
-        writer.add(addr);
-      }
-      writer.finish();
-    }
-  }
-  
-  /** expert: writes a value dictionary for a sorted/sortedset field */
-  private void addTermsDict(FieldInfo field, final Iterable<BytesRef> values) throws IOException {
-    // first check if its a "fixed-length" terms dict
-    int minLength = Integer.MAX_VALUE;
-    int maxLength = Integer.MIN_VALUE;
-    long numValues = 0;
-    for (BytesRef v : values) {
-      minLength = Math.min(minLength, v.length);
-      maxLength = Math.max(maxLength, v.length);
-      numValues++;
-    }
-    if (minLength == maxLength) {
-      // no index needed: direct addressing by mult
-      addBinaryField(field, values);
-    } else if (numValues < REVERSE_INTERVAL_COUNT) {
-      // low cardinality: waste a few KB of ram, but can't really use fancy index etc
-      addBinaryField(field, values);
-    } else {
-      assert numValues > 0; // we don't have to handle the empty case
-      // header
-      meta.writeVInt(field.number);
-      meta.writeByte(Lucene410DocValuesFormat.BINARY);
-      meta.writeVInt(BINARY_PREFIX_COMPRESSED);
-      meta.writeLong(-1L);
-      // now write the bytes: sharing prefixes within a block
-      final long startFP = data.getFilePointer();
-      // currently, we have to store the delta from expected for every 1/nth term
-      // we could avoid this, but its not much and less overall RAM than the previous approach!
-      RAMOutputStream addressBuffer = new RAMOutputStream();
-      MonotonicBlockPackedWriter termAddresses = new MonotonicBlockPackedWriter(addressBuffer, BLOCK_SIZE);
-      // buffers up 16 terms
-      RAMOutputStream bytesBuffer = new RAMOutputStream();
-      // buffers up block header
-      RAMOutputStream headerBuffer = new RAMOutputStream();
-      BytesRefBuilder lastTerm = new BytesRefBuilder();
-      lastTerm.grow(maxLength);
-      long count = 0;
-      int suffixDeltas[] = new int[INTERVAL_COUNT];
-      for (BytesRef v : values) {
-        int termPosition = (int) (count & INTERVAL_MASK);
-        if (termPosition == 0) {
-          termAddresses.add(data.getFilePointer() - startFP);
-          // abs-encode first term
-          headerBuffer.writeVInt(v.length);
-          headerBuffer.writeBytes(v.bytes, v.offset, v.length);
-          lastTerm.copyBytes(v);
-        } else {
-          // prefix-code: we only share at most 255 characters, to encode the length as a single
-          // byte and have random access. Larger terms just get less compression.
-          int sharedPrefix = Math.min(255, StringHelper.bytesDifference(lastTerm.get(), v));
-          bytesBuffer.writeByte((byte) sharedPrefix);
-          bytesBuffer.writeBytes(v.bytes, v.offset + sharedPrefix, v.length - sharedPrefix);
-          // we can encode one smaller, because terms are unique.
-          suffixDeltas[termPosition] = v.length - sharedPrefix - 1;
-        }
-        
-        count++;
-        // flush block
-        if ((count & INTERVAL_MASK) == 0) {
-          flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);
-        }
-      }
-      // flush trailing crap
-      int leftover = (int) (count & INTERVAL_MASK);
-      if (leftover > 0) {
-        Arrays.fill(suffixDeltas, leftover, suffixDeltas.length, 0);
-        flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);
-      }
-      final long indexStartFP = data.getFilePointer();
-      // write addresses of indexed terms
-      termAddresses.finish();
-      addressBuffer.writeTo(data);
-      addressBuffer = null;
-      termAddresses = null;
-      meta.writeVInt(minLength);
-      meta.writeVInt(maxLength);
-      meta.writeVLong(count);
-      meta.writeLong(startFP);
-      meta.writeLong(indexStartFP);
-      meta.writeVInt(PackedInts.VERSION_CURRENT);
-      meta.writeVInt(BLOCK_SIZE);
-      addReverseTermIndex(field, values, maxLength);
-    }
-  }
-  
-  // writes term dictionary "block"
-  // first term is absolute encoded as vint length + bytes.
-  // lengths of subsequent N terms are encoded as either N bytes or N shorts.
-  // in the double-byte case, the first byte is indicated with -1.
-  // subsequent terms are encoded as byte suffixLength + bytes.
-  private void flushTermsDictBlock(RAMOutputStream headerBuffer, RAMOutputStream bytesBuffer, int suffixDeltas[]) throws IOException {
-    boolean twoByte = false;
-    for (int i = 1; i < suffixDeltas.length; i++) {
-      if (suffixDeltas[i] > 254) {
-        twoByte = true;
-      }
-    }
-    if (twoByte) {
-      headerBuffer.writeByte((byte)255);
-      for (int i = 1; i < suffixDeltas.length; i++) {
-        headerBuffer.writeShort((short) suffixDeltas[i]);
-      }
-    } else {
-      for (int i = 1; i < suffixDeltas.length; i++) {
-        headerBuffer.writeByte((byte) suffixDeltas[i]);
-      }
-    }
-    headerBuffer.writeTo(data);
-    headerBuffer.reset();
-    bytesBuffer.writeTo(data);
-    bytesBuffer.reset();
-  }
-  
-  // writes reverse term index: used for binary searching a term into a range of 64 blocks
-  // for every 64 blocks (1024 terms) we store a term, trimming any suffix unnecessary for comparison
-  // terms are written as a contiguous byte[], but never spanning 2^15 byte boundaries.
-  private void addReverseTermIndex(FieldInfo field, final Iterable<BytesRef> values, int maxLength) throws IOException {
-    long count = 0;
-    BytesRefBuilder priorTerm = new BytesRefBuilder();
-    priorTerm.grow(maxLength);
-    BytesRef indexTerm = new BytesRef();
-    long startFP = data.getFilePointer();
-    PagedBytes pagedBytes = new PagedBytes(15);
-    MonotonicBlockPackedWriter addresses = new MonotonicBlockPackedWriter(data, BLOCK_SIZE);
-    
-    for (BytesRef b : values) {
-      int termPosition = (int) (count & REVERSE_INTERVAL_MASK);
-      if (termPosition == 0) {
-        int len = StringHelper.sortKeyLength(priorTerm.get(), b);
-        indexTerm.bytes = b.bytes;
-        indexTerm.offset = b.offset;
-        indexTerm.length = len;
-        addresses.add(pagedBytes.copyUsingLengthPrefix(indexTerm));
-      } else if (termPosition == REVERSE_INTERVAL_MASK) {
-        priorTerm.copyBytes(b);
-      }
-      count++;
-    }
-    addresses.finish();
-    long numBytes = pagedBytes.getPointer();
-    pagedBytes.freeze(true);
-    PagedBytesDataInput in = pagedBytes.getDataInput();
-    meta.writeLong(startFP);
-    data.writeVLong(numBytes);
-    data.copyBytes(in, numBytes);
-  }
-
-  @Override
-  public void addSortedField(FieldInfo field, Iterable<BytesRef> values, Iterable<Number> docToOrd) throws IOException {
-    checkCanWrite(field);
-    meta.writeVInt(field.number);
-    meta.writeByte(Lucene410DocValuesFormat.SORTED);
-    addTermsDict(field, values);
-    addNumericField(field, docToOrd, false);
-  }
-
-  @Override
-  public void addSortedNumericField(FieldInfo field, final Iterable<Number> docToValueCount, final Iterable<Number> values) throws IOException {
-    checkCanWrite(field);
-    meta.writeVInt(field.number);
-    meta.writeByte(Lucene410DocValuesFormat.SORTED_NUMERIC);
-    if (isSingleValued(docToValueCount)) {
-      meta.writeVInt(SORTED_SINGLE_VALUED);
-      // The field is single-valued, we can encode it as NUMERIC
-      addNumericField(field, singletonView(docToValueCount, values, null));
-    } else {
-      meta.writeVInt(SORTED_WITH_ADDRESSES);
-      // write the stream of values as a numeric field
-      addNumericField(field, values, true);
-      // write the doc -> ord count as a absolute index to the stream
-      addAddresses(field, docToValueCount);
-    }
-  }
-
-  @Override
-  public void addSortedSetField(FieldInfo field, Iterable<BytesRef> values, final Iterable<Number> docToOrdCount, final Iterable<Number> ords) throws IOException {
-    checkCanWrite(field);
-    meta.writeVInt(field.number);
-    meta.writeByte(Lucene410DocValuesFormat.SORTED_SET);
-
-    if (isSingleValued(docToOrdCount)) {
-      meta.writeVInt(SORTED_SINGLE_VALUED);
-      // The field is single-valued, we can encode it as SORTED
-      addSortedField(field, values, singletonView(docToOrdCount, ords, -1L));
-    } else {
-      meta.writeVInt(SORTED_WITH_ADDRESSES);
-
-      // write the ord -> byte[] as a binary field
-      addTermsDict(field, values);
-
-      // write the stream of ords as a numeric field
-      // NOTE: we could return an iterator that delta-encodes these within a doc
-      addNumericField(field, ords, false);
-
-      // write the doc -> ord count as a absolute index to the stream
-      addAddresses(field, docToOrdCount);
-    }
-  }
-  
-  // writes addressing information as MONOTONIC_COMPRESSED integer
-  private void addAddresses(FieldInfo field, Iterable<Number> values) throws IOException {
-    meta.writeVInt(field.number);
-    meta.writeByte(Lucene410DocValuesFormat.NUMERIC);
-    meta.writeVInt(MONOTONIC_COMPRESSED);
-    meta.writeLong(-1L);
-    meta.writeLong(data.getFilePointer());
-    meta.writeVLong(maxDoc);
-    meta.writeVInt(PackedInts.VERSION_CURRENT);
-    meta.writeVInt(BLOCK_SIZE);
-
-    final MonotonicBlockPackedWriter writer = new MonotonicBlockPackedWriter(data, BLOCK_SIZE);
-    long addr = 0;
-    writer.add(addr);
-    for (Number v : values) {
-      addr += v.longValue();
-      writer.add(addr);
-    }
-    writer.finish();
-    meta.writeLong(data.getFilePointer());
-  }
-
-  @Override
-  public void close() throws IOException {
-    boolean success = false;
-    try {
-      if (meta != null) {
-        meta.writeVInt(-1); // write EOF marker
-        CodecUtil.writeFooter(meta); // write checksum
-      }
-      if (data != null) {
-        CodecUtil.writeFooter(data); // write checksum
-      }
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(data, meta);
-      } else {
-        IOUtils.closeWhileHandlingException(data, meta);
-      }
-      meta = data = null;
-    }
-  }
-  
-  void checkCanWrite(FieldInfo field) {
-    if ((field.getDocValuesType() == DocValuesType.NUMERIC || 
-        field.getDocValuesType() == DocValuesType.BINARY) && 
-        field.getDocValuesGen() != -1) {
-      // ok
-    } else {
-      throw new UnsupportedOperationException("this codec can only be used for reading");
-    }
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene410/Lucene410DocValuesFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene410/Lucene410DocValuesFormat.java
deleted file mode 100644
index 472ff9a..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene410/Lucene410DocValuesFormat.java
+++ /dev/null
@@ -1,61 +0,0 @@
-package org.apache.lucene.codecs.lucene410;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.codecs.DocValuesProducer;
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-
-/** 
- * 4.10 docvalues format
- * @deprecated only for old 4.x segments
- */
-@Deprecated
-public class Lucene410DocValuesFormat extends DocValuesFormat {
-
-  /** Sole Constructor */
-  public Lucene410DocValuesFormat() {
-    super("Lucene410");
-  }
-
-  @Override
-  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    return new Lucene410DocValuesConsumer(state, DATA_CODEC, DATA_EXTENSION, META_CODEC, META_EXTENSION);
-  }
-
-  @Override
-  public final DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
-    return new Lucene410DocValuesProducer(state, DATA_CODEC, DATA_EXTENSION, META_CODEC, META_EXTENSION);
-  }
-  
-  static final String DATA_CODEC = "Lucene410DocValuesData";
-  static final String DATA_EXTENSION = "dvd";
-  static final String META_CODEC = "Lucene410ValuesMetadata";
-  static final String META_EXTENSION = "dvm";
-  static final int VERSION_START = 0;
-  static final int VERSION_CURRENT = VERSION_START;
-  static final byte NUMERIC = 0;
-  static final byte BINARY = 1;
-  static final byte SORTED = 2;
-  static final byte SORTED_SET = 3;
-  static final byte SORTED_NUMERIC = 4;
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene410/Lucene410DocValuesProducer.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene410/Lucene410DocValuesProducer.java
deleted file mode 100644
index fafea3a..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene410/Lucene410DocValuesProducer.java
+++ /dev/null
@@ -1,1126 +0,0 @@
-package org.apache.lucene.codecs.lucene410;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import static org.apache.lucene.codecs.lucene410.Lucene410DocValuesConsumer.BINARY_FIXED_UNCOMPRESSED;
-import static org.apache.lucene.codecs.lucene410.Lucene410DocValuesConsumer.BINARY_PREFIX_COMPRESSED;
-import static org.apache.lucene.codecs.lucene410.Lucene410DocValuesConsumer.BINARY_VARIABLE_UNCOMPRESSED;
-import static org.apache.lucene.codecs.lucene410.Lucene410DocValuesConsumer.DELTA_COMPRESSED;
-import static org.apache.lucene.codecs.lucene410.Lucene410DocValuesConsumer.GCD_COMPRESSED;
-import static org.apache.lucene.codecs.lucene410.Lucene410DocValuesConsumer.MONOTONIC_COMPRESSED;
-import static org.apache.lucene.codecs.lucene410.Lucene410DocValuesConsumer.SORTED_SINGLE_VALUED;
-import static org.apache.lucene.codecs.lucene410.Lucene410DocValuesConsumer.SORTED_WITH_ADDRESSES;
-import static org.apache.lucene.codecs.lucene410.Lucene410DocValuesConsumer.TABLE_COMPRESSED;
-import static org.apache.lucene.codecs.lucene410.Lucene410DocValuesConsumer.INTERVAL_SHIFT;
-import static org.apache.lucene.codecs.lucene410.Lucene410DocValuesConsumer.INTERVAL_COUNT;
-import static org.apache.lucene.codecs.lucene410.Lucene410DocValuesConsumer.INTERVAL_MASK;
-import static org.apache.lucene.codecs.lucene410.Lucene410DocValuesConsumer.REVERSE_INTERVAL_SHIFT;
-import static org.apache.lucene.codecs.lucene410.Lucene410DocValuesConsumer.REVERSE_INTERVAL_MASK;
-import static org.apache.lucene.codecs.lucene410.Lucene410DocValuesConsumer.BLOCK_INTERVAL_SHIFT;
-import static org.apache.lucene.codecs.lucene410.Lucene410DocValuesConsumer.BLOCK_INTERVAL_MASK;
-
-import java.io.Closeable; // javadocs
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.concurrent.atomic.AtomicLong;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.DocValuesProducer;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.NumericDocValues;
-import org.apache.lucene.index.RandomAccessOrds;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SortedDocValues;
-import org.apache.lucene.index.SortedNumericDocValues;
-import org.apache.lucene.index.SortedSetDocValues;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.store.ChecksumIndexInput;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.RandomAccessInput;
-import org.apache.lucene.util.Accountable;
-import org.apache.lucene.util.Accountables;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.LongValues;
-import org.apache.lucene.util.PagedBytes;
-import org.apache.lucene.util.RamUsageEstimator;
-import org.apache.lucene.util.packed.DirectReader;
-import org.apache.lucene.util.packed.MonotonicBlockPackedReader;
-
-/** 
- * reader for 4.10 docvalues format
- * @deprecated only for old 4.x segments
- */
-@Deprecated
-class Lucene410DocValuesProducer extends DocValuesProducer implements Closeable {
-  private final Map<String,NumericEntry> numerics = new HashMap<>();
-  private final Map<String,BinaryEntry> binaries = new HashMap<>();
-  private final Map<String,SortedSetEntry> sortedSets = new HashMap<>();
-  private final Map<String,SortedSetEntry> sortedNumerics = new HashMap<>();
-  private final Map<String,NumericEntry> ords = new HashMap<>();
-  private final Map<String,NumericEntry> ordIndexes = new HashMap<>();
-  private final int numFields;
-  private final AtomicLong ramBytesUsed;
-  private final IndexInput data;
-  private final int maxDoc;
-
-  // memory-resident structures
-  private final Map<String,MonotonicBlockPackedReader> addressInstances = new HashMap<>();
-  private final Map<String,MonotonicBlockPackedReader> ordIndexInstances = new HashMap<>();
-  private final Map<String,ReverseTermsIndex> reverseIndexInstances = new HashMap<>();
-  
-  private final boolean merging;
-  
-  // clone for merge: when merging we don't do any instances.put()s
-  Lucene410DocValuesProducer(Lucene410DocValuesProducer original) throws IOException {
-    assert Thread.holdsLock(original);
-    numerics.putAll(original.numerics);
-    binaries.putAll(original.binaries);
-    sortedSets.putAll(original.sortedSets);
-    sortedNumerics.putAll(original.sortedNumerics);
-    ords.putAll(original.ords);
-    ordIndexes.putAll(original.ordIndexes);
-    numFields = original.numFields;
-    ramBytesUsed = new AtomicLong(original.ramBytesUsed.get());
-    data = original.data.clone();
-    maxDoc = original.maxDoc;
-    
-    addressInstances.putAll(original.addressInstances);
-    ordIndexInstances.putAll(original.ordIndexInstances);
-    reverseIndexInstances.putAll(original.reverseIndexInstances);
-    merging = true;
-  }
-  
-  /** expert: instantiates a new reader */
-  Lucene410DocValuesProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
-    String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
-    this.maxDoc = state.segmentInfo.getDocCount();
-    merging = false;
-    
-    int version = -1;
-    int numFields = -1;
-    
-    // read in the entries from the metadata file.
-    try (ChecksumIndexInput in = state.directory.openChecksumInput(metaName, state.context)) {
-      Throwable priorE = null;
-      try {
-        version = CodecUtil.checkHeader(in, metaCodec, 
-                                        Lucene410DocValuesFormat.VERSION_START,
-                                        Lucene410DocValuesFormat.VERSION_CURRENT);
-        numFields = readFields(in, state.fieldInfos);
-      } catch (Throwable exception) {
-        priorE = exception;
-      } finally {
-        CodecUtil.checkFooter(in, priorE);
-      }
-    }
-    
-    this.numFields = numFields;
-    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
-    this.data = state.directory.openInput(dataName, state.context);
-    boolean success = false;
-    try {
-      final int version2 = CodecUtil.checkHeader(data, dataCodec, 
-                                                 Lucene410DocValuesFormat.VERSION_START,
-                                                 Lucene410DocValuesFormat.VERSION_CURRENT);
-      if (version != version2) {
-        throw new CorruptIndexException("Format versions mismatch: meta=" + version + ", data=" + version2, data);
-      }
-      
-      // NOTE: data file is too costly to verify checksum against all the bytes on open,
-      // but for now we at least verify proper structure of the checksum footer: which looks
-      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
-      // such as file truncation.
-      CodecUtil.retrieveChecksum(data);
-
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(this.data);
-      }
-    }
-    
-    ramBytesUsed = new AtomicLong(RamUsageEstimator.shallowSizeOfInstance(getClass()));
-  }
-
-  private void readSortedField(FieldInfo info, IndexInput meta) throws IOException {
-    // sorted = binary + numeric
-    if (meta.readVInt() != info.number) {
-      throw new CorruptIndexException("sorted entry for field: " + info.name + " is corrupt", meta);
-    }
-    if (meta.readByte() != Lucene410DocValuesFormat.BINARY) {
-      throw new CorruptIndexException("sorted entry for field: " + info.name + " is corrupt", meta);
-    }
-    BinaryEntry b = readBinaryEntry(meta);
-    binaries.put(info.name, b);
-    
-    if (meta.readVInt() != info.number) {
-      throw new CorruptIndexException("sorted entry for field: " + info.name + " is corrupt", meta);
-    }
-    if (meta.readByte() != Lucene410DocValuesFormat.NUMERIC) {
-      throw new CorruptIndexException("sorted entry for field: " + info.name + " is corrupt", meta);
-    }
-    NumericEntry n = readNumericEntry(meta);
-    ords.put(info.name, n);
-  }
-
-  private void readSortedSetFieldWithAddresses(FieldInfo info, IndexInput meta) throws IOException {
-    // sortedset = binary + numeric (addresses) + ordIndex
-    if (meta.readVInt() != info.number) {
-      throw new CorruptIndexException("sortedset entry for field: " + info.name + " is corrupt", meta);
-    }
-    if (meta.readByte() != Lucene410DocValuesFormat.BINARY) {
-      throw new CorruptIndexException("sortedset entry for field: " + info.name + " is corrupt", meta);
-    }
-    BinaryEntry b = readBinaryEntry(meta);
-    binaries.put(info.name, b);
-
-    if (meta.readVInt() != info.number) {
-      throw new CorruptIndexException("sortedset entry for field: " + info.name + " is corrupt", meta);
-    }
-    if (meta.readByte() != Lucene410DocValuesFormat.NUMERIC) {
-      throw new CorruptIndexException("sortedset entry for field: " + info.name + " is corrupt", meta);
-    }
-    NumericEntry n1 = readNumericEntry(meta);
-    ords.put(info.name, n1);
-
-    if (meta.readVInt() != info.number) {
-      throw new CorruptIndexException("sortedset entry for field: " + info.name + " is corrupt", meta);
-    }
-    if (meta.readByte() != Lucene410DocValuesFormat.NUMERIC) {
-      throw new CorruptIndexException("sortedset entry for field: " + info.name + " is corrupt", meta);
-    }
-    NumericEntry n2 = readNumericEntry(meta);
-    ordIndexes.put(info.name, n2);
-  }
-
-  private int readFields(IndexInput meta, FieldInfos infos) throws IOException {
-    int numFields = 0;
-    int fieldNumber = meta.readVInt();
-    while (fieldNumber != -1) {
-      numFields++;
-      FieldInfo info = infos.fieldInfo(fieldNumber);
-      if (info == null) {
-        // trickier to validate more: because we use multiple entries for "composite" types like sortedset, etc.
-        throw new CorruptIndexException("Invalid field number: " + fieldNumber, meta);
-      }
-      byte type = meta.readByte();
-      if (type == Lucene410DocValuesFormat.NUMERIC) {
-        numerics.put(info.name, readNumericEntry(meta));
-      } else if (type == Lucene410DocValuesFormat.BINARY) {
-        BinaryEntry b = readBinaryEntry(meta);
-        binaries.put(info.name, b);
-      } else if (type == Lucene410DocValuesFormat.SORTED) {
-        readSortedField(info, meta);
-      } else if (type == Lucene410DocValuesFormat.SORTED_SET) {
-        SortedSetEntry ss = readSortedSetEntry(meta);
-        sortedSets.put(info.name, ss);
-        if (ss.format == SORTED_WITH_ADDRESSES) {
-          readSortedSetFieldWithAddresses(info, meta);
-        } else if (ss.format == SORTED_SINGLE_VALUED) {
-          if (meta.readVInt() != fieldNumber) {
-            throw new CorruptIndexException("sortedset entry for field: " + info.name + " is corrupt", meta);
-          }
-          if (meta.readByte() != Lucene410DocValuesFormat.SORTED) {
-            throw new CorruptIndexException("sortedset entry for field: " + info.name + " is corrupt", meta);
-          }
-          readSortedField(info, meta);
-        } else {
-          throw new AssertionError();
-        }
-      } else if (type == Lucene410DocValuesFormat.SORTED_NUMERIC) {
-        SortedSetEntry ss = readSortedSetEntry(meta);
-        sortedNumerics.put(info.name, ss);
-        if (meta.readVInt() != fieldNumber) {
-          throw new CorruptIndexException("sortednumeric entry for field: " + info.name + " is corrupt", meta);
-        }
-        if (meta.readByte() != Lucene410DocValuesFormat.NUMERIC) {
-          throw new CorruptIndexException("sortednumeric entry for field: " + info.name + " is corrupt", meta);
-        }
-        numerics.put(info.name, readNumericEntry(meta));
-        if (ss.format == SORTED_WITH_ADDRESSES) {
-          if (meta.readVInt() != fieldNumber) {
-            throw new CorruptIndexException("sortednumeric entry for field: " + info.name + " is corrupt", meta);
-          }
-          if (meta.readByte() != Lucene410DocValuesFormat.NUMERIC) {
-            throw new CorruptIndexException("sortednumeric entry for field: " + info.name + " is corrupt", meta);
-          }
-          NumericEntry ordIndex = readNumericEntry(meta);
-          ordIndexes.put(info.name, ordIndex);
-        } else if (ss.format != SORTED_SINGLE_VALUED) {
-          throw new AssertionError();
-        }
-      } else {
-        throw new CorruptIndexException("invalid type: " + type, meta);
-      }
-      fieldNumber = meta.readVInt();
-    }
-    return numFields;
-  }
-  
-  static NumericEntry readNumericEntry(IndexInput meta) throws IOException {
-    NumericEntry entry = new NumericEntry();
-    entry.format = meta.readVInt();
-    entry.missingOffset = meta.readLong();
-    entry.offset = meta.readLong();
-    entry.count = meta.readVLong();
-    switch(entry.format) {
-      case GCD_COMPRESSED:
-        entry.minValue = meta.readLong();
-        entry.gcd = meta.readLong();
-        entry.bitsPerValue = meta.readVInt();
-        break;
-      case TABLE_COMPRESSED:
-        final int uniqueValues = meta.readVInt();
-        if (uniqueValues > 256) {
-          throw new CorruptIndexException("TABLE_COMPRESSED cannot have more than 256 distinct values, got=" + uniqueValues, meta);
-        }
-        entry.table = new long[uniqueValues];
-        for (int i = 0; i < uniqueValues; ++i) {
-          entry.table[i] = meta.readLong();
-        }
-        entry.bitsPerValue = meta.readVInt();
-        break;
-      case DELTA_COMPRESSED:
-        entry.minValue = meta.readLong();
-        entry.bitsPerValue = meta.readVInt();
-        break;
-      case MONOTONIC_COMPRESSED:
-        entry.packedIntsVersion = meta.readVInt();
-        entry.blockSize = meta.readVInt();
-        break;
-      default:
-        throw new CorruptIndexException("Unknown format: " + entry.format + ", input=", meta);
-    }
-    entry.endOffset = meta.readLong();
-    return entry;
-  }
-  
-  static BinaryEntry readBinaryEntry(IndexInput meta) throws IOException {
-    BinaryEntry entry = new BinaryEntry();
-    entry.format = meta.readVInt();
-    entry.missingOffset = meta.readLong();
-    entry.minLength = meta.readVInt();
-    entry.maxLength = meta.readVInt();
-    entry.count = meta.readVLong();
-    entry.offset = meta.readLong();
-    switch(entry.format) {
-      case BINARY_FIXED_UNCOMPRESSED:
-        break;
-      case BINARY_PREFIX_COMPRESSED:
-        entry.addressesOffset = meta.readLong();
-        entry.packedIntsVersion = meta.readVInt();
-        entry.blockSize = meta.readVInt();
-        entry.reverseIndexOffset = meta.readLong();
-        break;
-      case BINARY_VARIABLE_UNCOMPRESSED:
-        entry.addressesOffset = meta.readLong();
-        entry.packedIntsVersion = meta.readVInt();
-        entry.blockSize = meta.readVInt();
-        break;
-      default:
-        throw new CorruptIndexException("Unknown format: " + entry.format, meta);
-    }
-    return entry;
-  }
-
-  SortedSetEntry readSortedSetEntry(IndexInput meta) throws IOException {
-    SortedSetEntry entry = new SortedSetEntry();
-    entry.format = meta.readVInt();
-    if (entry.format != SORTED_SINGLE_VALUED && entry.format != SORTED_WITH_ADDRESSES) {
-      throw new CorruptIndexException("Unknown format: " + entry.format, meta);
-    }
-    return entry;
-  }
-
-  @Override
-  public NumericDocValues getNumeric(FieldInfo field) throws IOException {
-    NumericEntry entry = numerics.get(field.name);
-    return getNumeric(entry);
-  }
-  
-  @Override
-  public long ramBytesUsed() {
-    return ramBytesUsed.get();
-  }
-  
-  @Override
-  public synchronized Iterable<? extends Accountable> getChildResources() {
-    List<Accountable> resources = new ArrayList<>();
-    resources.addAll(Accountables.namedAccountables("addresses field", addressInstances));
-    resources.addAll(Accountables.namedAccountables("ord index field", ordIndexInstances));
-    resources.addAll(Accountables.namedAccountables("reverse index field", reverseIndexInstances));
-    return Collections.unmodifiableList(resources);
-  }
-  
-  @Override
-  public void checkIntegrity() throws IOException {
-    CodecUtil.checksumEntireFile(data);
-  }
-
-  @Override
-  public String toString() {
-    return getClass().getSimpleName() + "(fields=" + numFields + ")";
-  }
-
-  LongValues getNumeric(NumericEntry entry) throws IOException {
-    RandomAccessInput slice = this.data.randomAccessSlice(entry.offset, entry.endOffset - entry.offset);
-    switch (entry.format) {
-      case DELTA_COMPRESSED:
-        final long delta = entry.minValue;
-        final LongValues values = DirectReader.getInstance(slice, entry.bitsPerValue);
-        return new LongValues() {
-          @Override
-          public long get(long id) {
-            return delta + values.get(id);
-          }
-        };
-      case GCD_COMPRESSED:
-        final long min = entry.minValue;
-        final long mult = entry.gcd;
-        final LongValues quotientReader = DirectReader.getInstance(slice, entry.bitsPerValue);
-        return new LongValues() {
-          @Override
-          public long get(long id) {
-            return min + mult * quotientReader.get(id);
-          }
-        };
-      case TABLE_COMPRESSED:
-        final long table[] = entry.table;
-        final LongValues ords = DirectReader.getInstance(slice, entry.bitsPerValue);
-        return new LongValues() {
-          @Override
-          public long get(long id) {
-            return table[(int) ords.get(id)];
-          }
-        };
-      default:
-        throw new AssertionError();
-    }
-  }
-
-  @Override
-  public BinaryDocValues getBinary(FieldInfo field) throws IOException {
-    BinaryEntry bytes = binaries.get(field.name);
-    switch(bytes.format) {
-      case BINARY_FIXED_UNCOMPRESSED:
-        return getFixedBinary(field, bytes);
-      case BINARY_VARIABLE_UNCOMPRESSED:
-        return getVariableBinary(field, bytes);
-      case BINARY_PREFIX_COMPRESSED:
-        return getCompressedBinary(field, bytes);
-      default:
-        throw new AssertionError();
-    }
-  }
-  
-  private BinaryDocValues getFixedBinary(FieldInfo field, final BinaryEntry bytes) throws IOException {
-    final IndexInput data = this.data.slice("fixed-binary", bytes.offset, bytes.count * bytes.maxLength);
-
-    final BytesRef term = new BytesRef(bytes.maxLength);
-    final byte[] buffer = term.bytes;
-    final int length = term.length = bytes.maxLength;
-    
-    return new LongBinaryDocValues() {
-      @Override
-      public BytesRef get(long id) {
-        try {
-          data.seek(id * length);
-          data.readBytes(buffer, 0, buffer.length);
-          return term;
-        } catch (IOException e) {
-          throw new RuntimeException(e);
-        }
-      }
-    };
-  }
-  
-  /** returns an address instance for variable-length binary values. */
-  private synchronized MonotonicBlockPackedReader getAddressInstance(FieldInfo field, BinaryEntry bytes) throws IOException {
-    MonotonicBlockPackedReader addresses = addressInstances.get(field.name);
-    if (addresses == null) {
-      data.seek(bytes.addressesOffset);
-      addresses = MonotonicBlockPackedReader.of(data, bytes.packedIntsVersion, bytes.blockSize, bytes.count+1, false);
-      if (!merging) {
-        addressInstances.put(field.name, addresses);
-        ramBytesUsed.addAndGet(addresses.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
-      }
-    }
-    return addresses;
-  }
-  
-  private BinaryDocValues getVariableBinary(FieldInfo field, final BinaryEntry bytes) throws IOException {
-    final MonotonicBlockPackedReader addresses = getAddressInstance(field, bytes);
-
-    final IndexInput data = this.data.slice("var-binary", bytes.offset, bytes.addressesOffset - bytes.offset);
-    final BytesRef term = new BytesRef(Math.max(0, bytes.maxLength));
-    final byte buffer[] = term.bytes;
-    
-    return new LongBinaryDocValues() {      
-      @Override
-      public BytesRef get(long id) {
-        long startAddress = addresses.get(id);
-        long endAddress = addresses.get(id+1);
-        int length = (int) (endAddress - startAddress);
-        try {
-          data.seek(startAddress);
-          data.readBytes(buffer, 0, length);
-          term.length = length;
-          return term;
-        } catch (IOException e) {
-          throw new RuntimeException(e);
-        }
-      }
-    };
-  }
-  
-  /** returns an address instance for prefix-compressed binary values. */
-  private synchronized MonotonicBlockPackedReader getIntervalInstance(FieldInfo field, BinaryEntry bytes) throws IOException {
-    MonotonicBlockPackedReader addresses = addressInstances.get(field.name);
-    if (addresses == null) {
-      data.seek(bytes.addressesOffset);
-      final long size = (bytes.count + INTERVAL_MASK) >>> INTERVAL_SHIFT;
-      addresses = MonotonicBlockPackedReader.of(data, bytes.packedIntsVersion, bytes.blockSize, size, false);
-      if (!merging) {
-        addressInstances.put(field.name, addresses);
-        ramBytesUsed.addAndGet(addresses.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
-      }
-    }
-    return addresses;
-  }
-  
-  /** returns a reverse lookup instance for prefix-compressed binary values. */
-  private synchronized ReverseTermsIndex getReverseIndexInstance(FieldInfo field, BinaryEntry bytes) throws IOException {
-    ReverseTermsIndex index = reverseIndexInstances.get(field.name);
-    if (index == null) {
-      index = new ReverseTermsIndex();
-      data.seek(bytes.reverseIndexOffset);
-      long size = (bytes.count + REVERSE_INTERVAL_MASK) >>> REVERSE_INTERVAL_SHIFT;
-      index.termAddresses = MonotonicBlockPackedReader.of(data, bytes.packedIntsVersion, bytes.blockSize, size, false);
-      long dataSize = data.readVLong();
-      PagedBytes pagedBytes = new PagedBytes(15);
-      pagedBytes.copy(data, dataSize);
-      index.terms = pagedBytes.freeze(true);
-      if (!merging) {
-        reverseIndexInstances.put(field.name, index);
-        ramBytesUsed.addAndGet(index.ramBytesUsed());
-      }
-    }
-    return index;
-  }
-
-  private BinaryDocValues getCompressedBinary(FieldInfo field, final BinaryEntry bytes) throws IOException {
-    final MonotonicBlockPackedReader addresses = getIntervalInstance(field, bytes);
-    final ReverseTermsIndex index = getReverseIndexInstance(field, bytes);
-    assert addresses.size() > 0; // we don't have to handle empty case
-    IndexInput slice = data.slice("terms", bytes.offset, bytes.addressesOffset - bytes.offset);
-    return new CompressedBinaryDocValues(bytes, addresses, index, slice);
-  }
-
-  @Override
-  public SortedDocValues getSorted(FieldInfo field) throws IOException {
-    final int valueCount = (int) binaries.get(field.name).count;
-    final BinaryDocValues binary = getBinary(field);
-    NumericEntry entry = ords.get(field.name);
-    final LongValues ordinals = getNumeric(entry);
-    return new SortedDocValues() {
-
-      @Override
-      public int getOrd(int docID) {
-        return (int) ordinals.get(docID);
-      }
-
-      @Override
-      public BytesRef lookupOrd(int ord) {
-        return binary.get(ord);
-      }
-
-      @Override
-      public int getValueCount() {
-        return valueCount;
-      }
-
-      @Override
-      public int lookupTerm(BytesRef key) {
-        if (binary instanceof CompressedBinaryDocValues) {
-          return (int) ((CompressedBinaryDocValues)binary).lookupTerm(key);
-        } else {
-          return super.lookupTerm(key);
-        }
-      }
-
-      @Override
-      public TermsEnum termsEnum() {
-        if (binary instanceof CompressedBinaryDocValues) {
-          return ((CompressedBinaryDocValues)binary).getTermsEnum();
-        } else {
-          return super.termsEnum();
-        }
-      }
-    };
-  }
-  
-  /** returns an address instance for sortedset ordinal lists */
-  private synchronized MonotonicBlockPackedReader getOrdIndexInstance(FieldInfo field, NumericEntry entry) throws IOException {
-    MonotonicBlockPackedReader instance = ordIndexInstances.get(field.name);
-    if (instance == null) {
-      data.seek(entry.offset);
-      instance = MonotonicBlockPackedReader.of(data, entry.packedIntsVersion, entry.blockSize, entry.count+1, false);
-      if (!merging) {
-        ordIndexInstances.put(field.name, instance);
-        ramBytesUsed.addAndGet(instance.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
-      }
-    }
-    return instance;
-  }
-  
-  @Override
-  public SortedNumericDocValues getSortedNumeric(FieldInfo field) throws IOException {
-    SortedSetEntry ss = sortedNumerics.get(field.name);
-    NumericEntry numericEntry = numerics.get(field.name);
-    final LongValues values = getNumeric(numericEntry);
-    if (ss.format == SORTED_SINGLE_VALUED) {
-      final Bits docsWithField = getMissingBits(numericEntry.missingOffset);
-      return DocValues.singleton(values, docsWithField);
-    } else if (ss.format == SORTED_WITH_ADDRESSES) {
-      final MonotonicBlockPackedReader ordIndex = getOrdIndexInstance(field, ordIndexes.get(field.name));
-      
-      return new SortedNumericDocValues() {
-        long startOffset;
-        long endOffset;
-        
-        @Override
-        public void setDocument(int doc) {
-          startOffset = ordIndex.get(doc);
-          endOffset = ordIndex.get(doc+1L);
-        }
-
-        @Override
-        public long valueAt(int index) {
-          return values.get(startOffset + index);
-        }
-
-        @Override
-        public int count() {
-          return (int) (endOffset - startOffset);
-        }
-      };
-    } else {
-      throw new AssertionError();
-    }
-  }
-
-  @Override
-  public SortedSetDocValues getSortedSet(FieldInfo field) throws IOException {
-    SortedSetEntry ss = sortedSets.get(field.name);
-    if (ss.format == SORTED_SINGLE_VALUED) {
-      final SortedDocValues values = getSorted(field);
-      return DocValues.singleton(values);
-    } else if (ss.format != SORTED_WITH_ADDRESSES) {
-      throw new AssertionError();
-    }
-
-    final long valueCount = binaries.get(field.name).count;
-    // we keep the byte[]s and list of ords on disk, these could be large
-    final LongBinaryDocValues binary = (LongBinaryDocValues) getBinary(field);
-    final LongValues ordinals = getNumeric(ords.get(field.name));
-    // but the addresses to the ord stream are in RAM
-    final MonotonicBlockPackedReader ordIndex = getOrdIndexInstance(field, ordIndexes.get(field.name));
-    
-    return new RandomAccessOrds() {
-      long startOffset;
-      long offset;
-      long endOffset;
-      
-      @Override
-      public long nextOrd() {
-        if (offset == endOffset) {
-          return NO_MORE_ORDS;
-        } else {
-          long ord = ordinals.get(offset);
-          offset++;
-          return ord;
-        }
-      }
-
-      @Override
-      public void setDocument(int docID) {
-        startOffset = offset = ordIndex.get(docID);
-        endOffset = ordIndex.get(docID+1L);
-      }
-
-      @Override
-      public BytesRef lookupOrd(long ord) {
-        return binary.get(ord);
-      }
-
-      @Override
-      public long getValueCount() {
-        return valueCount;
-      }
-      
-      @Override
-      public long lookupTerm(BytesRef key) {
-        if (binary instanceof CompressedBinaryDocValues) {
-          return ((CompressedBinaryDocValues)binary).lookupTerm(key);
-        } else {
-          return super.lookupTerm(key);
-        }
-      }
-
-      @Override
-      public TermsEnum termsEnum() {
-        if (binary instanceof CompressedBinaryDocValues) {
-          return ((CompressedBinaryDocValues)binary).getTermsEnum();
-        } else {
-          return super.termsEnum();
-        }
-      }
-
-      @Override
-      public long ordAt(int index) {
-        return ordinals.get(startOffset + index);
-      }
-
-      @Override
-      public int cardinality() {
-        return (int) (endOffset - startOffset);
-      }
-    };
-  }
-  
-  private Bits getMissingBits(final long offset) throws IOException {
-    if (offset == -1) {
-      return new Bits.MatchAllBits(maxDoc);
-    } else {
-      int length = (int) ((maxDoc + 7L) >>> 3);
-      final RandomAccessInput in = data.randomAccessSlice(offset, length);
-      return new Bits() {
-        @Override
-        public boolean get(int index) {
-          try {
-            return (in.readByte(index >> 3) & (1 << (index & 7))) != 0;
-          } catch (IOException e) {
-            throw new RuntimeException(e);
-          }
-        }
-
-        @Override
-        public int length() {
-          return maxDoc;
-        }
-      };
-    }
-  }
-
-  @Override
-  public Bits getDocsWithField(FieldInfo field) throws IOException {
-    switch(field.getDocValuesType()) {
-      case SORTED_SET:
-        return DocValues.docsWithValue(getSortedSet(field), maxDoc);
-      case SORTED_NUMERIC:
-        return DocValues.docsWithValue(getSortedNumeric(field), maxDoc);
-      case SORTED:
-        return DocValues.docsWithValue(getSorted(field), maxDoc);
-      case BINARY:
-        BinaryEntry be = binaries.get(field.name);
-        return getMissingBits(be.missingOffset);
-      case NUMERIC:
-        NumericEntry ne = numerics.get(field.name);
-        return getMissingBits(ne.missingOffset);
-      default:
-        throw new AssertionError();
-    }
-  }
-
-  @Override
-  public synchronized DocValuesProducer getMergeInstance() throws IOException {
-    return new Lucene410DocValuesProducer(this);
-  }
-
-  @Override
-  public void close() throws IOException {
-    data.close();
-  }
-  
-  /** metadata entry for a numeric docvalues field */
-  static class NumericEntry {
-    private NumericEntry() {}
-    /** offset to the bitset representing docsWithField, or -1 if no documents have missing values */
-    long missingOffset;
-    /** offset to the actual numeric values */
-    public long offset;
-    /** end offset to the actual numeric values */
-    public long endOffset;
-    /** bits per value used to pack the numeric values */
-    public int bitsPerValue;
-
-    int format;
-    /** packed ints version used to encode these numerics */
-    public int packedIntsVersion;
-    /** count of values written */
-    public long count;
-    /** packed ints blocksize */
-    public int blockSize;
-    
-    long minValue;
-    long gcd;
-    long table[];
-  }
-  
-  /** metadata entry for a binary docvalues field */
-  static class BinaryEntry {
-    private BinaryEntry() {}
-    /** offset to the bitset representing docsWithField, or -1 if no documents have missing values */
-    long missingOffset;
-    /** offset to the actual binary values */
-    long offset;
-
-    int format;
-    /** count of values written */
-    public long count;
-    int minLength;
-    int maxLength;
-    /** offset to the addressing data that maps a value to its slice of the byte[] */
-    public long addressesOffset;
-    /** offset to the reverse index */
-    public long reverseIndexOffset;
-    /** packed ints version used to encode addressing information */
-    public int packedIntsVersion;
-    /** packed ints blocksize */
-    public int blockSize;
-  }
-
-  /** metadata entry for a sorted-set docvalues field */
-  static class SortedSetEntry {
-    private SortedSetEntry() {}
-    int format;
-  }
-
-  // internally we compose complex dv (sorted/sortedset) from other ones
-  static abstract class LongBinaryDocValues extends BinaryDocValues {
-    @Override
-    public final BytesRef get(int docID) {
-      return get((long)docID);
-    }
-    
-    abstract BytesRef get(long id);
-  }
-  
-  // used for reverse lookup to a small range of blocks
-  static class ReverseTermsIndex implements Accountable {
-    public MonotonicBlockPackedReader termAddresses;
-    public PagedBytes.Reader terms;
-    
-    @Override
-    public long ramBytesUsed() {
-      return termAddresses.ramBytesUsed() + terms.ramBytesUsed();
-    }
-    
-    @Override
-    public Iterable<? extends Accountable> getChildResources() {
-      List<Accountable> resources = new ArrayList<>();
-      resources.add(Accountables.namedAccountable("term bytes", terms));
-      resources.add(Accountables.namedAccountable("term addresses", termAddresses));
-      return Collections.unmodifiableList(resources);
-    }
-
-    @Override
-    public String toString() {
-      return getClass().getSimpleName() + "(size=" + termAddresses.size() + ")";
-    }
-  }
-  
-  //in the compressed case, we add a few additional operations for
-  //more efficient reverse lookup and enumeration
-  static final class CompressedBinaryDocValues extends LongBinaryDocValues {    
-    final long numValues;
-    final long numIndexValues;
-    final int maxTermLength;
-    final MonotonicBlockPackedReader addresses;
-    final IndexInput data;
-    final CompressedBinaryTermsEnum termsEnum;
-    final PagedBytes.Reader reverseTerms;
-    final MonotonicBlockPackedReader reverseAddresses;
-    final long numReverseIndexValues;
-    
-    public CompressedBinaryDocValues(BinaryEntry bytes, MonotonicBlockPackedReader addresses, ReverseTermsIndex index, IndexInput data) throws IOException {
-      this.maxTermLength = bytes.maxLength;
-      this.numValues = bytes.count;
-      this.addresses = addresses;
-      this.numIndexValues = addresses.size();
-      this.data = data;
-      this.reverseTerms = index.terms;
-      this.reverseAddresses = index.termAddresses;
-      this.numReverseIndexValues = reverseAddresses.size();
-      this.termsEnum = getTermsEnum(data);
-    }
-    
-    @Override
-    public BytesRef get(long id) {
-      try {
-        termsEnum.seekExact(id);
-        return termsEnum.term();
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      }
-    }
-    
-    long lookupTerm(BytesRef key) {
-      try {
-        switch (termsEnum.seekCeil(key)) {
-          case FOUND: return termsEnum.ord();
-          case NOT_FOUND: return -termsEnum.ord()-1;
-          default: return -numValues-1;
-        }
-      } catch (IOException bogus) {
-        throw new RuntimeException(bogus);
-      }
-    }
-    
-    TermsEnum getTermsEnum() {
-      try {
-        return getTermsEnum(data.clone());
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      }
-    }
-    
-    private CompressedBinaryTermsEnum getTermsEnum(IndexInput input) throws IOException {
-      return new CompressedBinaryTermsEnum(input);
-    }
-    
-    class CompressedBinaryTermsEnum extends TermsEnum {
-      private long currentOrd = -1;
-      // offset to the start of the current block 
-      private long currentBlockStart;
-      private final IndexInput input;
-      // delta from currentBlockStart to start of each term
-      private final int offsets[] = new int[INTERVAL_COUNT];
-      private final byte buffer[] = new byte[2*INTERVAL_COUNT-1];
-      
-      private final BytesRef term = new BytesRef(maxTermLength);
-      private final BytesRef firstTerm = new BytesRef(maxTermLength);
-      private final BytesRef scratch = new BytesRef();
-      
-      CompressedBinaryTermsEnum(IndexInput input) throws IOException {
-        this.input = input;
-        input.seek(0);
-      }
-      
-      private void readHeader() throws IOException {
-        firstTerm.length = input.readVInt();
-        input.readBytes(firstTerm.bytes, 0, firstTerm.length);
-        input.readBytes(buffer, 0, INTERVAL_COUNT-1);
-        if (buffer[0] == -1) {
-          readShortAddresses();
-        } else {
-          readByteAddresses();
-        }
-        currentBlockStart = input.getFilePointer();
-      }
-      
-      // read single byte addresses: each is delta - 2
-      // (shared prefix byte and length > 0 are both implicit)
-      private void readByteAddresses() throws IOException {
-        int addr = 0;
-        for (int i = 1; i < offsets.length; i++) {
-          addr += 2 + (buffer[i-1] & 0xFF);
-          offsets[i] = addr;
-        }
-      }
-      
-      // read double byte addresses: each is delta - 2
-      // (shared prefix byte and length > 0 are both implicit)
-      private void readShortAddresses() throws IOException {
-        input.readBytes(buffer, INTERVAL_COUNT-1, INTERVAL_COUNT);
-        int addr = 0;
-        for (int i = 1; i < offsets.length; i++) {
-          int x = i<<1;
-          addr += 2 + ((buffer[x-1] << 8) | (buffer[x] & 0xFF));
-          offsets[i] = addr;
-        }
-      }
-      
-      // set term to the first term
-      private void readFirstTerm() throws IOException {
-        term.length = firstTerm.length;
-        System.arraycopy(firstTerm.bytes, firstTerm.offset, term.bytes, 0, term.length);
-      }
-      
-      // read term at offset, delta encoded from first term
-      private void readTerm(int offset) throws IOException {
-        int start = input.readByte() & 0xFF;
-        System.arraycopy(firstTerm.bytes, firstTerm.offset, term.bytes, 0, start);
-        int suffix = offsets[offset] - offsets[offset-1] - 1;
-        input.readBytes(term.bytes, start, suffix);
-        term.length = start + suffix;
-      }
-      
-      @Override
-      public BytesRef next() throws IOException {
-        currentOrd++;
-        if (currentOrd >= numValues) {
-          return null;
-        } else { 
-          int offset = (int) (currentOrd & INTERVAL_MASK);
-          if (offset == 0) {
-            // switch to next block
-            readHeader();
-            readFirstTerm();
-          } else {
-            readTerm(offset);
-          }
-          return term;
-        }
-      }
-      
-      // binary search reverse index to find smaller 
-      // range of blocks to search
-      long binarySearchIndex(BytesRef text) throws IOException {
-        long low = 0;
-        long high = numReverseIndexValues - 1;
-        while (low <= high) {
-          long mid = (low + high) >>> 1;
-          reverseTerms.fill(scratch, reverseAddresses.get(mid));
-          int cmp = scratch.compareTo(text);
-          
-          if (cmp < 0) {
-            low = mid + 1;
-          } else if (cmp > 0) {
-            high = mid - 1;
-          } else {
-            return mid;
-          }
-        }
-        return high;
-      }
-      
-      // binary search against first term in block range 
-      // to find term's block
-      long binarySearchBlock(BytesRef text, long low, long high) throws IOException {       
-        while (low <= high) {
-          long mid = (low + high) >>> 1;
-          input.seek(addresses.get(mid));
-          term.length = input.readVInt();
-          input.readBytes(term.bytes, 0, term.length);
-          int cmp = term.compareTo(text);
-          
-          if (cmp < 0) {
-            low = mid + 1;
-          } else if (cmp > 0) {
-            high = mid - 1;
-          } else {
-            return mid;
-          }
-        }
-        return high;
-      }
-      
-      @Override
-      public SeekStatus seekCeil(BytesRef text) throws IOException {
-        // locate block: narrow to block range with index, then search blocks
-        final long block;
-        long indexPos = binarySearchIndex(text);
-        if (indexPos < 0) {
-          block = 0;
-        } else {
-          long low = indexPos << BLOCK_INTERVAL_SHIFT;
-          long high = Math.min(numIndexValues - 1, low + BLOCK_INTERVAL_MASK);
-          block = Math.max(low, binarySearchBlock(text, low, high));
-        }
-        
-        // position before block, then scan to term.
-        input.seek(addresses.get(block));
-        currentOrd = (block << INTERVAL_SHIFT) - 1;
-        
-        while (next() != null) {
-          int cmp = term.compareTo(text);
-          if (cmp == 0) {
-            return SeekStatus.FOUND;
-          } else if (cmp > 0) {
-            return SeekStatus.NOT_FOUND;
-          }
-        }
-        return SeekStatus.END;
-      }
-      
-      @Override
-      public void seekExact(long ord) throws IOException {
-        long block = ord >>> INTERVAL_SHIFT;
-        if (block != currentOrd >>> INTERVAL_SHIFT) {
-          // switch to different block
-          input.seek(addresses.get(block));
-          readHeader();
-        }
-        
-        currentOrd = ord;
-        
-        int offset = (int) (ord & INTERVAL_MASK);
-        if (offset == 0) {
-          readFirstTerm();
-        } else {
-          input.seek(currentBlockStart + offsets[offset-1]);
-          readTerm(offset);
-        }
-      }
-      
-      @Override
-      public BytesRef term() throws IOException {
-        return term;
-      }
-      
-      @Override
-      public long ord() throws IOException {
-        return currentOrd;
-      }
-      
-      @Override
-      public int docFreq() throws IOException {
-        throw new UnsupportedOperationException();
-      }
-      
-      @Override
-      public long totalTermFreq() throws IOException {
-        return -1;
-      }
-      
-      @Override
-      public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
-        throw new UnsupportedOperationException();
-      }
-      
-      @Override
-      public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
-        throw new UnsupportedOperationException();
-      }
-    }
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene410/package.html b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene410/package.html
deleted file mode 100644
index 0aad87f..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene410/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Lucene 4.10 file format.
-</body>
-</html>
\ No newline at end of file
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42Codec.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42Codec.java
deleted file mode 100644
index 6a224da..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42Codec.java
+++ /dev/null
@@ -1,146 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.CompoundFormat;
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.LiveDocsFormat;
-import org.apache.lucene.codecs.NormsConsumer;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.SegmentInfoFormat;
-import org.apache.lucene.codecs.StoredFieldsFormat;
-import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40CompoundFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40SegmentInfoFormat;
-import org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat;
-import org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat;
-import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
-import org.apache.lucene.index.SegmentWriteState;
-
-/**
- * Implements the Lucene 4.2 index format
- * @deprecated Only for reading old 4.2 segments
- */
-@Deprecated
-public class Lucene42Codec extends Codec {
-  private final StoredFieldsFormat fieldsFormat = new Lucene41StoredFieldsFormat();
-  private final TermVectorsFormat vectorsFormat = new Lucene42TermVectorsFormat();
-  private final FieldInfosFormat fieldInfosFormat = new Lucene42FieldInfosFormat();
-  private final SegmentInfoFormat infosFormat = new Lucene40SegmentInfoFormat();
-  private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat();
-  private final CompoundFormat compoundFormat = new Lucene40CompoundFormat();
-  
-  private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
-    @Override
-    public PostingsFormat getPostingsFormatForField(String field) {
-      return Lucene42Codec.this.getPostingsFormatForField(field);
-    }
-  };
-  
-  
-  private final DocValuesFormat docValuesFormat = new PerFieldDocValuesFormat() {
-    @Override
-    public DocValuesFormat getDocValuesFormatForField(String field) {
-      return Lucene42Codec.this.getDocValuesFormatForField(field);
-    }
-  };
-
-  /** Sole constructor. */
-  public Lucene42Codec() {
-    super("Lucene42");
-  }
-  
-  @Override
-  public StoredFieldsFormat storedFieldsFormat() {
-    return fieldsFormat;
-  }
-  
-  @Override
-  public TermVectorsFormat termVectorsFormat() {
-    return vectorsFormat;
-  }
-
-  @Override
-  public final PostingsFormat postingsFormat() {
-    return postingsFormat;
-  }
-  
-  @Override
-  public FieldInfosFormat fieldInfosFormat() {
-    return fieldInfosFormat;
-  }
-  
-  @Override
-  public SegmentInfoFormat segmentInfoFormat() {
-    return infosFormat;
-  }
-  
-  @Override
-  public final LiveDocsFormat liveDocsFormat() {
-    return liveDocsFormat;
-  }
-  
-  @Override
-  public CompoundFormat compoundFormat() {
-    return compoundFormat;
-  }
-
-  /** Returns the postings format that should be used for writing 
-   *  new segments of <code>field</code>.
-   *  
-   *  The default implementation always returns "Lucene41"
-   */
-  public PostingsFormat getPostingsFormatForField(String field) {
-    return defaultFormat;
-  }
-  
-  /** Returns the docvalues format that should be used for writing 
-   *  new segments of <code>field</code>.
-   *  
-   *  The default implementation always returns "Lucene42"
-   */
-  public DocValuesFormat getDocValuesFormatForField(String field) {
-    return defaultDVFormat;
-  }
-  
-  @Override
-  public final DocValuesFormat docValuesFormat() {
-    return docValuesFormat;
-  }
-
-  private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene41");
-  private final DocValuesFormat defaultDVFormat = DocValuesFormat.forName("Lucene42");
-
-  private final NormsFormat normsFormat = new Lucene42NormsFormat() {
-    @Override
-    public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
-      throw new UnsupportedOperationException("this codec can only be used for reading");
-    }
-  };
-
-  @Override
-  public NormsFormat normsFormat() {
-    return normsFormat;
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesFormat.java
deleted file mode 100644
index fa2d5ee..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesFormat.java
+++ /dev/null
@@ -1,76 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.codecs.DocValuesProducer;
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * Lucene 4.2 DocValues format.
- * @deprecated Only for reading old 4.2 segments
- */
-@Deprecated
-public class Lucene42DocValuesFormat extends DocValuesFormat {
-
-  /** Maximum length for each binary doc values field. */
-  static final int MAX_BINARY_FIELD_LENGTH = (1 << 15) - 2;
-  
-  final float acceptableOverheadRatio;
-  
-  /** 
-   * Calls {@link #Lucene42DocValuesFormat(float) 
-   * Lucene42DocValuesFormat(PackedInts.DEFAULT)} 
-   */
-  public Lucene42DocValuesFormat() {
-    this(PackedInts.DEFAULT);
-  }
-  
-  /**
-   * Creates a new Lucene42DocValuesFormat with the specified
-   * <code>acceptableOverheadRatio</code> for NumericDocValues.
-   * @param acceptableOverheadRatio compression parameter for numerics. 
-   *        Currently this is only used when the number of unique values is small.
-   *        
-   * @lucene.experimental
-   */
-  public Lucene42DocValuesFormat(float acceptableOverheadRatio) {
-    super("Lucene42");
-    this.acceptableOverheadRatio = acceptableOverheadRatio;
-  }
-
-  @Override
-  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    throw new UnsupportedOperationException("this codec can only be used for reading");
-  }
-  
-  @Override
-  public final DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
-    return new Lucene42DocValuesProducer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION);
-  }
-  
-  static final String DATA_CODEC = "Lucene42DocValuesData";
-  static final String DATA_EXTENSION = "dvd";
-  static final String METADATA_CODEC = "Lucene42DocValuesMetadata";
-  static final String METADATA_EXTENSION = "dvm";
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesProducer.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesProducer.java
deleted file mode 100644
index c8b929a..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesProducer.java
+++ /dev/null
@@ -1,713 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.concurrent.atomic.AtomicLong;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.DocValuesProducer;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.NumericDocValues;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SortedDocValues;
-import org.apache.lucene.index.SortedNumericDocValues;
-import org.apache.lucene.index.SortedSetDocValues;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.store.ByteArrayDataInput;
-import org.apache.lucene.store.ChecksumIndexInput;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.Accountable;
-import org.apache.lucene.util.Accountables;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.BytesRefBuilder;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.IntsRef;
-import org.apache.lucene.util.IntsRefBuilder;
-import org.apache.lucene.util.PagedBytes;
-import org.apache.lucene.util.RamUsageEstimator;
-import org.apache.lucene.util.fst.BytesRefFSTEnum;
-import org.apache.lucene.util.fst.BytesRefFSTEnum.InputOutput;
-import org.apache.lucene.util.fst.FST;
-import org.apache.lucene.util.fst.FST.Arc;
-import org.apache.lucene.util.fst.FST.BytesReader;
-import org.apache.lucene.util.fst.PositiveIntOutputs;
-import org.apache.lucene.util.fst.Util;
-import org.apache.lucene.util.packed.BlockPackedReader;
-import org.apache.lucene.util.packed.MonotonicBlockPackedReader;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * Reader for 4.2 docvalues
- * @deprecated only for reading old 4.x segments
- */
-@Deprecated
-final class Lucene42DocValuesProducer extends DocValuesProducer {
-  // metadata maps (just file pointers and minimal stuff)
-  private final Map<String,NumericEntry> numerics;
-  private final Map<String,BinaryEntry> binaries;
-  private final Map<String,FSTEntry> fsts;
-  private final IndexInput data;
-  private final int version;
-  private final int numEntries;
-  
-  // ram instances we have already loaded
-  private final Map<String,NumericDocValues> numericInstances = new HashMap<>();
-  private final Map<String,BinaryDocValues> binaryInstances = new HashMap<>();
-  private final Map<String,FST<Long>> fstInstances = new HashMap<>();
-  
-  private final Map<String,Accountable> numericInfo = new HashMap<>();
-  private final Map<String,Accountable> binaryInfo = new HashMap<>();
-  private final Map<String,Accountable> addressInfo = new HashMap<>();
-  
-  private final int maxDoc;
-  private final AtomicLong ramBytesUsed;
-  
-  static final byte NUMBER = 0;
-  static final byte BYTES = 1;
-  static final byte FST = 2;
-
-  static final int BLOCK_SIZE = 4096;
-  
-  static final byte DELTA_COMPRESSED = 0;
-  static final byte TABLE_COMPRESSED = 1;
-  static final byte UNCOMPRESSED = 2;
-  static final byte GCD_COMPRESSED = 3;
-  
-  static final int VERSION_START = 0;
-  static final int VERSION_GCD_COMPRESSION = 1;
-  static final int VERSION_CHECKSUM = 2;
-  static final int VERSION_CURRENT = VERSION_CHECKSUM;
-  
-  private final boolean merging;
-
-  // clone for merge: when merging we don't do any instances.put()s
-  Lucene42DocValuesProducer(Lucene42DocValuesProducer original) throws IOException {
-    assert Thread.holdsLock(original);
-    numerics = original.numerics;
-    binaries = original.binaries;
-    fsts = original.fsts;
-    data = original.data.clone();
-    version = original.version;
-    numEntries = original.numEntries;
-    
-    numericInstances.putAll(original.numericInstances);
-    binaryInstances.putAll(original.binaryInstances);
-    fstInstances.putAll(original.fstInstances);
-    numericInfo.putAll(original.numericInfo);
-    binaryInfo.putAll(original.binaryInfo);
-    addressInfo.putAll(original.addressInfo);
-    
-    maxDoc = original.maxDoc;
-    ramBytesUsed = new AtomicLong(original.ramBytesUsed.get());
-    merging = true;
-  }
-    
-  Lucene42DocValuesProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
-    maxDoc = state.segmentInfo.getDocCount();
-    merging = false;
-    String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
-    // read in the entries from the metadata file.
-    ChecksumIndexInput in = state.directory.openChecksumInput(metaName, state.context);
-    boolean success = false;
-    ramBytesUsed = new AtomicLong(RamUsageEstimator.shallowSizeOfInstance(getClass()));
-    try {
-      version = CodecUtil.checkHeader(in, metaCodec, 
-                                      VERSION_START,
-                                      VERSION_CURRENT);
-      numerics = new HashMap<>();
-      binaries = new HashMap<>();
-      fsts = new HashMap<>();
-      numEntries = readFields(in, state.fieldInfos);
-
-      if (version >= VERSION_CHECKSUM) {
-        CodecUtil.checkFooter(in);
-      } else {
-        CodecUtil.checkEOF(in);
-      }
-      
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(in);
-      } else {
-        IOUtils.closeWhileHandlingException(in);
-      }
-    }
-
-    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
-    this.data = state.directory.openInput(dataName, state.context);
-    success = false;
-    try {
-      final int version2 = CodecUtil.checkHeader(data, dataCodec, 
-                                                 VERSION_START,
-                                                 VERSION_CURRENT);
-      if (version != version2) {
-        throw new CorruptIndexException("Format versions mismatch: meta=" + version + ", data=" + version2, data);
-      }
-      
-      if (version >= VERSION_CHECKSUM) {
-        // NOTE: data file is too costly to verify checksum against all the bytes on open,
-        // but for now we at least verify proper structure of the checksum footer: which looks
-        // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
-        // such as file truncation.
-        CodecUtil.retrieveChecksum(data);
-      }
-
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(this.data);
-      }
-    }
-  }
-  
-  private int readFields(IndexInput meta, FieldInfos infos) throws IOException {
-    int numEntries = 0;
-    int fieldNumber = meta.readVInt();
-    while (fieldNumber != -1) {
-      numEntries++;
-      FieldInfo info = infos.fieldInfo(fieldNumber);
-      if (info == null) {
-        // trickier to validate more: because we re-use for norms, because we use multiple entries
-        // for "composite" types like sortedset, etc.
-        throw new CorruptIndexException("Invalid field number: " + fieldNumber, meta);
-      }
-      int fieldType = meta.readByte();
-      if (fieldType == NUMBER) {
-        NumericEntry entry = new NumericEntry();
-        entry.offset = meta.readLong();
-        entry.format = meta.readByte();
-        switch(entry.format) {
-          case DELTA_COMPRESSED:
-          case TABLE_COMPRESSED:
-          case GCD_COMPRESSED:
-          case UNCOMPRESSED:
-               break;
-          default:
-               throw new CorruptIndexException("Unknown format: " + entry.format, meta);
-        }
-        if (entry.format != UNCOMPRESSED) {
-          entry.packedIntsVersion = meta.readVInt();
-        }
-        numerics.put(info.name, entry);
-      } else if (fieldType == BYTES) {
-        BinaryEntry entry = new BinaryEntry();
-        entry.offset = meta.readLong();
-        entry.numBytes = meta.readLong();
-        entry.minLength = meta.readVInt();
-        entry.maxLength = meta.readVInt();
-        if (entry.minLength != entry.maxLength) {
-          entry.packedIntsVersion = meta.readVInt();
-          entry.blockSize = meta.readVInt();
-        }
-        binaries.put(info.name, entry);
-      } else if (fieldType == FST) {
-        FSTEntry entry = new FSTEntry();
-        entry.offset = meta.readLong();
-        entry.numOrds = meta.readVLong();
-        fsts.put(info.name, entry);
-      } else {
-        throw new CorruptIndexException("invalid entry type: " + fieldType, meta);
-      }
-      fieldNumber = meta.readVInt();
-    }
-    return numEntries;
-  }
-
-  @Override
-  public synchronized NumericDocValues getNumeric(FieldInfo field) throws IOException {
-    NumericDocValues instance = numericInstances.get(field.name);
-    if (instance == null) {
-      instance = loadNumeric(field);
-      if (!merging) {
-        numericInstances.put(field.name, instance);
-      }
-    }
-    return instance;
-  }
-  
-  @Override
-  public long ramBytesUsed() {
-    return ramBytesUsed.get();
-  }
-  
-  @Override
-  public synchronized Iterable<? extends Accountable> getChildResources() {
-    List<Accountable> resources = new ArrayList<>();
-    resources.addAll(Accountables.namedAccountables("numeric field", numericInfo));
-    resources.addAll(Accountables.namedAccountables("binary field", binaryInfo));
-    resources.addAll(Accountables.namedAccountables("addresses field", addressInfo));
-    resources.addAll(Accountables.namedAccountables("terms dict field", fstInstances));
-    return Collections.unmodifiableList(resources);
-  }
-  
-  @Override
-  public void checkIntegrity() throws IOException {
-    if (version >= VERSION_CHECKSUM) {
-      CodecUtil.checksumEntireFile(data);
-    }
-  }
-
-  @Override
-  public String toString() {
-    return getClass().getSimpleName() + "(entries=" + numEntries + ")";
-  }
-
-  private NumericDocValues loadNumeric(FieldInfo field) throws IOException {
-    NumericEntry entry = numerics.get(field.name);
-    data.seek(entry.offset);
-    switch (entry.format) {
-      case TABLE_COMPRESSED:
-        int size = data.readVInt();
-        if (size > 256) {
-          throw new CorruptIndexException("TABLE_COMPRESSED cannot have more than 256 distinct values, got=" + size, data);
-        }
-        final long decode[] = new long[size];
-        for (int i = 0; i < decode.length; i++) {
-          decode[i] = data.readLong();
-        }
-        final int formatID = data.readVInt();
-        final int bitsPerValue = data.readVInt();
-        final PackedInts.Reader ordsReader = PackedInts.getReaderNoHeader(data, PackedInts.Format.byId(formatID), entry.packedIntsVersion, maxDoc, bitsPerValue);
-        if (!merging) {
-          ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(decode) + ordsReader.ramBytesUsed());
-          numericInfo.put(field.name, ordsReader);
-        }
-        return new NumericDocValues() {
-          @Override
-          public long get(int docID) {
-            return decode[(int)ordsReader.get(docID)];
-          }
-        };
-      case DELTA_COMPRESSED:
-        final int blockSize = data.readVInt();
-        final BlockPackedReader reader = new BlockPackedReader(data, entry.packedIntsVersion, blockSize, maxDoc, false);
-        if (!merging) {
-          ramBytesUsed.addAndGet(reader.ramBytesUsed());
-          numericInfo.put(field.name, reader);
-        }
-        return reader;
-      case UNCOMPRESSED:
-        final byte bytes[] = new byte[maxDoc];
-        data.readBytes(bytes, 0, bytes.length);
-        if (!merging) {
-          ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(bytes));
-          numericInfo.put(field.name, Accountables.namedAccountable("byte array", maxDoc));
-        }
-        return new NumericDocValues() {
-          @Override
-          public long get(int docID) {
-            return bytes[docID];
-          }
-        };
-      case GCD_COMPRESSED:
-        final long min = data.readLong();
-        final long mult = data.readLong();
-        final int quotientBlockSize = data.readVInt();
-        final BlockPackedReader quotientReader = new BlockPackedReader(data, entry.packedIntsVersion, quotientBlockSize, maxDoc, false);
-        if (!merging) {
-          ramBytesUsed.addAndGet(quotientReader.ramBytesUsed());
-          numericInfo.put(field.name, quotientReader);
-        }
-        return new NumericDocValues() {
-          @Override
-          public long get(int docID) {
-            return min + mult * quotientReader.get(docID);
-          }
-        };
-      default:
-        throw new AssertionError();
-    }
-  }
-
-  @Override
-  public synchronized BinaryDocValues getBinary(FieldInfo field) throws IOException {
-    BinaryDocValues instance = binaryInstances.get(field.name);
-    if (instance == null) {
-      instance = loadBinary(field);
-      if (!merging) {
-        binaryInstances.put(field.name, instance);
-      }
-    }
-    return instance;
-  }
-  
-  private BinaryDocValues loadBinary(FieldInfo field) throws IOException {
-    BinaryEntry entry = binaries.get(field.name);
-    data.seek(entry.offset);
-    PagedBytes bytes = new PagedBytes(16);
-    bytes.copy(data, entry.numBytes);
-    final PagedBytes.Reader bytesReader = bytes.freeze(true);
-    if (!merging) {
-      binaryInfo.put(field.name, bytesReader);
-    }
-    if (entry.minLength == entry.maxLength) {
-      final int fixedLength = entry.minLength;
-      if (!merging) {
-        ramBytesUsed.addAndGet(bytesReader.ramBytesUsed());
-      }
-      return new BinaryDocValues() {
-        @Override
-        public BytesRef get(int docID) {
-          final BytesRef term = new BytesRef();
-          bytesReader.fillSlice(term, fixedLength * (long)docID, fixedLength);
-          return term;
-        }
-      };
-    } else {
-      final MonotonicBlockPackedReader addresses = MonotonicBlockPackedReader.of(data, entry.packedIntsVersion, entry.blockSize, maxDoc, false);
-      if (!merging) {
-        addressInfo.put(field.name, addresses);
-        ramBytesUsed.addAndGet(bytesReader.ramBytesUsed() + addresses.ramBytesUsed());
-      }
-      return new BinaryDocValues() {
-
-        @Override
-        public BytesRef get(int docID) {
-          long startAddress = docID == 0 ? 0 : addresses.get(docID-1);
-          long endAddress = addresses.get(docID); 
-          final BytesRef term = new BytesRef();
-          bytesReader.fillSlice(term, startAddress, (int) (endAddress - startAddress));
-          return term;
-        }
-      };
-    }
-  }
-  
-  @Override
-  public SortedDocValues getSorted(FieldInfo field) throws IOException {
-    final FSTEntry entry = fsts.get(field.name);
-    FST<Long> instance;
-    synchronized(this) {
-      instance = fstInstances.get(field.name);
-      if (instance == null) {
-        data.seek(entry.offset);
-        instance = new FST<>(data, PositiveIntOutputs.getSingleton());
-        if (!merging) {
-          ramBytesUsed.addAndGet(instance.ramBytesUsed());
-          fstInstances.put(field.name, instance);
-        }
-      }
-    }
-    final NumericDocValues docToOrd = getNumeric(field);
-    final FST<Long> fst = instance;
-    
-    // per-thread resources
-    final BytesReader in = fst.getBytesReader();
-    final Arc<Long> firstArc = new Arc<>();
-    final Arc<Long> scratchArc = new Arc<>();
-    final IntsRefBuilder scratchInts = new IntsRefBuilder();
-    final BytesRefFSTEnum<Long> fstEnum = new BytesRefFSTEnum<>(fst);
-    
-    return new SortedDocValues() {
-
-      final BytesRefBuilder term = new BytesRefBuilder();
-
-      @Override
-      public int getOrd(int docID) {
-        return (int) docToOrd.get(docID);
-      }
-
-      @Override
-      public BytesRef lookupOrd(int ord) {
-        try {
-          in.setPosition(0);
-          fst.getFirstArc(firstArc);
-          IntsRef output = Util.getByOutput(fst, ord, in, firstArc, scratchArc, scratchInts);
-          term.grow(output.length);
-          term.clear();
-          return Util.toBytesRef(output, term);
-        } catch (IOException bogus) {
-          throw new RuntimeException(bogus);
-        }
-      }
-
-      @Override
-      public int lookupTerm(BytesRef key) {
-        try {
-          InputOutput<Long> o = fstEnum.seekCeil(key);
-          if (o == null) {
-            return -getValueCount()-1;
-          } else if (o.input.equals(key)) {
-            return o.output.intValue();
-          } else {
-            return (int) -o.output-1;
-          }
-        } catch (IOException bogus) {
-          throw new RuntimeException(bogus);
-        }
-      }
-
-      @Override
-      public int getValueCount() {
-        return (int)entry.numOrds;
-      }
-
-      @Override
-      public TermsEnum termsEnum() {
-        return new FSTTermsEnum(fst);
-      }
-    };
-  }
-  
-  @Override
-  public SortedSetDocValues getSortedSet(FieldInfo field) throws IOException {
-    final FSTEntry entry = fsts.get(field.name);
-    if (entry.numOrds == 0) {
-      return DocValues.emptySortedSet(); // empty FST!
-    }
-    FST<Long> instance;
-    synchronized(this) {
-      instance = fstInstances.get(field.name);
-      if (instance == null) {
-        data.seek(entry.offset);
-        instance = new FST<>(data, PositiveIntOutputs.getSingleton());
-        if (!merging) {
-          ramBytesUsed.addAndGet(instance.ramBytesUsed());
-          fstInstances.put(field.name, instance);
-        }
-      }
-    }
-    final BinaryDocValues docToOrds = getBinary(field);
-    final FST<Long> fst = instance;
-    
-    // per-thread resources
-    final BytesReader in = fst.getBytesReader();
-    final Arc<Long> firstArc = new Arc<>();
-    final Arc<Long> scratchArc = new Arc<>();
-    final IntsRefBuilder scratchInts = new IntsRefBuilder();
-    final BytesRefFSTEnum<Long> fstEnum = new BytesRefFSTEnum<>(fst);
-    final ByteArrayDataInput input = new ByteArrayDataInput();
-    return new SortedSetDocValues() {
-      final BytesRefBuilder term = new BytesRefBuilder();
-      BytesRef ordsRef;
-      long currentOrd;
-
-      @Override
-      public long nextOrd() {
-        if (input.eof()) {
-          return NO_MORE_ORDS;
-        } else {
-          currentOrd += input.readVLong();
-          return currentOrd;
-        }
-      }
-      
-      @Override
-      public void setDocument(int docID) {
-        ordsRef = docToOrds.get(docID);
-        input.reset(ordsRef.bytes, ordsRef.offset, ordsRef.length);
-        currentOrd = 0;
-      }
-
-      @Override
-      public BytesRef lookupOrd(long ord) {
-        try {
-          in.setPosition(0);
-          fst.getFirstArc(firstArc);
-          IntsRef output = Util.getByOutput(fst, ord, in, firstArc, scratchArc, scratchInts);
-          term.grow(output.length);
-          term.clear();
-          return Util.toBytesRef(output, term);
-        } catch (IOException bogus) {
-          throw new RuntimeException(bogus);
-        }
-      }
-
-      @Override
-      public long lookupTerm(BytesRef key) {
-        try {
-          InputOutput<Long> o = fstEnum.seekCeil(key);
-          if (o == null) {
-            return -getValueCount()-1;
-          } else if (o.input.equals(key)) {
-            return o.output.intValue();
-          } else {
-            return -o.output-1;
-          }
-        } catch (IOException bogus) {
-          throw new RuntimeException(bogus);
-        }
-      }
-
-      @Override
-      public long getValueCount() {
-        return entry.numOrds;
-      }
-
-      @Override
-      public TermsEnum termsEnum() {
-        return new FSTTermsEnum(fst);
-      }
-    };
-  }
-  
-  @Override
-  public Bits getDocsWithField(FieldInfo field) throws IOException {
-    if (field.getDocValuesType() == FieldInfo.DocValuesType.SORTED_SET) {
-      return DocValues.docsWithValue(getSortedSet(field), maxDoc);
-    } else {
-      return new Bits.MatchAllBits(maxDoc);
-    }
-  }
-  
-  @Override
-  public SortedNumericDocValues getSortedNumeric(FieldInfo field) throws IOException {
-    throw new IllegalStateException("Lucene 4.2 does not support SortedNumeric: how did you pull this off?");
-  }
-
-  @Override
-  public synchronized DocValuesProducer getMergeInstance() throws IOException {
-    return new Lucene42DocValuesProducer(this);
-  }
-
-  @Override
-  public void close() throws IOException {
-    data.close();
-  }
-  
-  static class NumericEntry {
-    long offset;
-    byte format;
-    int packedIntsVersion;
-  }
-  
-  static class BinaryEntry {
-    long offset;
-    long numBytes;
-    int minLength;
-    int maxLength;
-    int packedIntsVersion;
-    int blockSize;
-  }
-  
-  static class FSTEntry {
-    long offset;
-    long numOrds;
-  }
-  
-  // exposes FSTEnum directly as a TermsEnum: avoids binary-search next()
-  static class FSTTermsEnum extends TermsEnum {
-    final BytesRefFSTEnum<Long> in;
-    
-    // this is all for the complicated seek(ord)...
-    // maybe we should add a FSTEnum that supports this operation?
-    final FST<Long> fst;
-    final FST.BytesReader bytesReader;
-    final Arc<Long> firstArc = new Arc<>();
-    final Arc<Long> scratchArc = new Arc<>();
-    final IntsRefBuilder scratchInts = new IntsRefBuilder();
-    final BytesRefBuilder scratchBytes = new BytesRefBuilder();
-    
-    FSTTermsEnum(FST<Long> fst) {
-      this.fst = fst;
-      in = new BytesRefFSTEnum<>(fst);
-      bytesReader = fst.getBytesReader();
-    }
-
-    @Override
-    public BytesRef next() throws IOException {
-      InputOutput<Long> io = in.next();
-      if (io == null) {
-        return null;
-      } else {
-        return io.input;
-      }
-    }
-
-    @Override
-    public SeekStatus seekCeil(BytesRef text) throws IOException {
-      if (in.seekCeil(text) == null) {
-        return SeekStatus.END;
-      } else if (term().equals(text)) {
-        // TODO: add SeekStatus to FSTEnum like in https://issues.apache.org/jira/browse/LUCENE-3729
-        // to remove this comparision?
-        return SeekStatus.FOUND;
-      } else {
-        return SeekStatus.NOT_FOUND;
-      }
-    }
-
-    @Override
-    public boolean seekExact(BytesRef text) throws IOException {
-      if (in.seekExact(text) == null) {
-        return false;
-      } else {
-        return true;
-      }
-    }
-
-    @Override
-    public void seekExact(long ord) throws IOException {
-      // TODO: would be better to make this simpler and faster.
-      // but we dont want to introduce a bug that corrupts our enum state!
-      bytesReader.setPosition(0);
-      fst.getFirstArc(firstArc);
-      IntsRef output = Util.getByOutput(fst, ord, bytesReader, firstArc, scratchArc, scratchInts);
-      BytesRefBuilder scratchBytes = new BytesRefBuilder();
-      scratchBytes.clear();
-      Util.toBytesRef(output, scratchBytes);
-      // TODO: we could do this lazily, better to try to push into FSTEnum though?
-      in.seekExact(scratchBytes.get());
-    }
-
-    @Override
-    public BytesRef term() throws IOException {
-      return in.current().input;
-    }
-
-    @Override
-    public long ord() throws IOException {
-      return in.current().output;
-    }
-
-    @Override
-    public int docFreq() throws IOException {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public long totalTermFreq() throws IOException {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
-      throw new UnsupportedOperationException();
-    }
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosFormat.java
deleted file mode 100644
index 4dd66de..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosFormat.java
+++ /dev/null
@@ -1,150 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.Map;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.UndeadNormsProducer;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfo.DocValuesType;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Lucene 4.2 Field Infos format.
- * @deprecated Only for reading old 4.2-4.5 segments
- */
-@Deprecated
-public class Lucene42FieldInfosFormat extends FieldInfosFormat {
-
-  /** Sole constructor. */
-  public Lucene42FieldInfosFormat() {
-  }
-
-  @Override
-  public FieldInfos read(Directory directory, SegmentInfo segmentInfo, String segmentSuffix, IOContext iocontext) throws IOException {
-    final String fileName = IndexFileNames.segmentFileName(segmentInfo.name, "", Lucene42FieldInfosFormat.EXTENSION);
-    IndexInput input = directory.openInput(fileName, iocontext);
-    
-    boolean success = false;
-    try {
-      CodecUtil.checkHeader(input, Lucene42FieldInfosFormat.CODEC_NAME, 
-                                   Lucene42FieldInfosFormat.FORMAT_START, 
-                                   Lucene42FieldInfosFormat.FORMAT_CURRENT);
-
-      final int size = input.readVInt(); //read in the size
-      FieldInfo infos[] = new FieldInfo[size];
-
-      for (int i = 0; i < size; i++) {
-        String name = input.readString();
-        final int fieldNumber = input.readVInt();
-        byte bits = input.readByte();
-        boolean isIndexed = (bits & Lucene42FieldInfosFormat.IS_INDEXED) != 0;
-        boolean storeTermVector = (bits & Lucene42FieldInfosFormat.STORE_TERMVECTOR) != 0;
-        boolean omitNorms = (bits & Lucene42FieldInfosFormat.OMIT_NORMS) != 0;
-        boolean storePayloads = (bits & Lucene42FieldInfosFormat.STORE_PAYLOADS) != 0;
-        final IndexOptions indexOptions;
-        if (!isIndexed) {
-          indexOptions = null;
-        } else if ((bits & Lucene42FieldInfosFormat.OMIT_TERM_FREQ_AND_POSITIONS) != 0) {
-          indexOptions = IndexOptions.DOCS_ONLY;
-        } else if ((bits & Lucene42FieldInfosFormat.OMIT_POSITIONS) != 0) {
-          indexOptions = IndexOptions.DOCS_AND_FREQS;
-        } else if ((bits & Lucene42FieldInfosFormat.STORE_OFFSETS_IN_POSTINGS) != 0) {
-          indexOptions = IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;
-        } else {
-          indexOptions = IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
-        }
-
-        // DV Types are packed in one byte
-        byte val = input.readByte();
-        final DocValuesType docValuesType = getDocValuesType(input, (byte) (val & 0x0F));
-        final DocValuesType normsType = getDocValuesType(input, (byte) ((val >>> 4) & 0x0F));
-        final Map<String,String> attributes = input.readStringStringMap();
-
-        if (isIndexed && omitNorms == false && normsType == null) {
-          // Undead norms!  Lucene42NormsProducer will check this and bring norms back from the dead:
-          UndeadNormsProducer.setUndead(attributes);
-        }
-
-        infos[i] = new FieldInfo(name, fieldNumber, storeTermVector, 
-          omitNorms, storePayloads, indexOptions, docValuesType, -1, Collections.unmodifiableMap(attributes));
-      }
-
-      CodecUtil.checkEOF(input);
-      FieldInfos fieldInfos = new FieldInfos(infos);
-      success = true;
-      return fieldInfos;
-    } finally {
-      if (success) {
-        input.close();
-      } else {
-        IOUtils.closeWhileHandlingException(input);
-      }
-    }
-  }
-  
-  private static DocValuesType getDocValuesType(IndexInput input, byte b) throws IOException {
-    if (b == 0) {
-      return null;
-    } else if (b == 1) {
-      return DocValuesType.NUMERIC;
-    } else if (b == 2) {
-      return DocValuesType.BINARY;
-    } else if (b == 3) {
-      return DocValuesType.SORTED;
-    } else if (b == 4) {
-      return DocValuesType.SORTED_SET;
-    } else {
-      throw new CorruptIndexException("invalid docvalues byte: " + b, input);
-    }
-  }
-  
-  @Override
-  public void write(Directory directory, SegmentInfo segmentInfo, String segmentSuffix, FieldInfos infos, IOContext context) throws IOException {
-    throw new UnsupportedOperationException("this codec can only be used for reading");
-  }
-
-  /** Extension of field infos */
-  static final String EXTENSION = "fnm";
-  
-  // Codec header
-  static final String CODEC_NAME = "Lucene42FieldInfos";
-  static final int FORMAT_START = 0;
-  static final int FORMAT_CURRENT = FORMAT_START;
-  
-  // Field flags
-  static final byte IS_INDEXED = 0x1;
-  static final byte STORE_TERMVECTOR = 0x2;
-  static final byte STORE_OFFSETS_IN_POSTINGS = 0x4;
-  static final byte OMIT_NORMS = 0x10;
-  static final byte STORE_PAYLOADS = 0x20;
-  static final byte OMIT_TERM_FREQ_AND_POSITIONS = 0x40;
-  static final byte OMIT_POSITIONS = -128;
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsFormat.java
deleted file mode 100644
index c2123d1..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsFormat.java
+++ /dev/null
@@ -1,77 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.NormsConsumer;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.NormsProducer;
-import org.apache.lucene.codecs.UndeadNormsProducer;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * Lucene 4.2 score normalization format.
- * @deprecated only for reading old 4.x segments
- */
-@Deprecated
-public class Lucene42NormsFormat extends NormsFormat {
-  final float acceptableOverheadRatio;
-
-  /** 
-   * Calls {@link #Lucene42NormsFormat(float) 
-   * Lucene42DocValuesFormat(PackedInts.FASTEST)} 
-   */
-  public Lucene42NormsFormat() {
-    // note: we choose FASTEST here (otherwise our norms are half as big but 15% slower than previous lucene)
-    this(PackedInts.FASTEST);
-  }
-  
-  /**
-   * Creates a new Lucene42DocValuesFormat with the specified
-   * <code>acceptableOverheadRatio</code> for NumericDocValues.
-   * @param acceptableOverheadRatio compression parameter for numerics. 
-   *        Currently this is only used when the number of unique values is small.
-   *        
-   * @lucene.experimental
-   */
-  public Lucene42NormsFormat(float acceptableOverheadRatio) {
-    this.acceptableOverheadRatio = acceptableOverheadRatio;
-  }
-  
-  @Override
-  public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
-    throw new UnsupportedOperationException("this codec can only be used for reading");
-  }
-  
-  @Override
-  public final NormsProducer normsProducer(SegmentReadState state) throws IOException {
-    if (UndeadNormsProducer.isUndeadArmy(state.fieldInfos)) {
-      return UndeadNormsProducer.INSTANCE;
-    } else {
-      return new Lucene42NormsProducer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION);
-    }
-  }
-  
-  static final String DATA_CODEC = "Lucene41NormsData";
-  static final String DATA_EXTENSION = "nvd";
-  static final String METADATA_CODEC = "Lucene41NormsMetadata";
-  static final String METADATA_EXTENSION = "nvm";
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsProducer.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsProducer.java
deleted file mode 100644
index 7048b1d..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsProducer.java
+++ /dev/null
@@ -1,86 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DocValuesProducer;
-import org.apache.lucene.codecs.NormsProducer;
-import org.apache.lucene.codecs.UndeadNormsProducer;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.NumericDocValues;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.util.Accountable;
-
-/**
- * Reads 4.2-4.8 norms.
- * @deprecated Only for reading old segments
- */
-@Deprecated
-final class Lucene42NormsProducer extends NormsProducer {
-  private final DocValuesProducer impl;
-  
-  // clone for merge
-  Lucene42NormsProducer(DocValuesProducer impl) throws IOException {
-    this.impl = impl.getMergeInstance();
-  }
-  
-  Lucene42NormsProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
-    impl = new Lucene42DocValuesProducer(state, dataCodec, dataExtension, metaCodec, metaExtension);
-  }
-
-  @Override
-  public NumericDocValues getNorms(FieldInfo field) throws IOException {
-    if (UndeadNormsProducer.isUndead(field)) {
-      // Bring undead norms back to life; this is set in Lucene42FieldInfosFormat, to emulate pre-5.0 undead norms
-      return DocValues.emptyNumeric();
-    }
-    return impl.getNumeric(field);
-  }
-
-  @Override
-  public void checkIntegrity() throws IOException {
-    impl.checkIntegrity();
-  }
-
-  @Override
-  public long ramBytesUsed() {
-    return impl.ramBytesUsed();
-  }
-  
-  @Override
-  public Iterable<? extends Accountable> getChildResources() {
-    return impl.getChildResources();
-  }
-
-  @Override
-  public void close() throws IOException {
-    impl.close();
-  }
-
-  @Override
-  public NormsProducer getMergeInstance() throws IOException {
-    return new Lucene42NormsProducer(impl);
-  }
-
-  @Override
-  public String toString() {
-    return getClass().getSimpleName() + "(" + impl + ")";
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42TermVectorsFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42TermVectorsFormat.java
deleted file mode 100644
index 3f732e5..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42TermVectorsFormat.java
+++ /dev/null
@@ -1,57 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.codecs.TermVectorsReader;
-import org.apache.lucene.codecs.TermVectorsWriter;
-import org.apache.lucene.codecs.compressing.CompressionMode;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-/**
- * Lucene 4.2 {@link TermVectorsFormat term vectors format}.
- * @deprecated only for reading old segments
- */
-@Deprecated
-public class Lucene42TermVectorsFormat extends TermVectorsFormat {
-  // this is actually what 4.2 TVF wrote!
-  static final String FORMAT_NAME = "Lucene41StoredFields";
-  static final String SEGMENT_SUFFIX = "";
-  static final CompressionMode COMPRESSION_MODE = CompressionMode.FAST;
-  static final int CHUNK_SIZE = 1 << 12;
-
-  @Override
-  public final TermVectorsReader vectorsReader(Directory directory, SegmentInfo segmentInfo, FieldInfos fieldInfos, IOContext context) throws IOException {
-    return new Lucene42TermVectorsReader(directory, segmentInfo, SEGMENT_SUFFIX, fieldInfos, context, FORMAT_NAME, COMPRESSION_MODE);
-  }
-
-  @Override
-  public TermVectorsWriter vectorsWriter(Directory directory, SegmentInfo segmentInfo, IOContext context) throws IOException {
-    throw new UnsupportedOperationException("this codec can only be used for reading");
-  }
-
-  @Override
-  public String toString() {
-    return getClass().getSimpleName() + "(compressionMode=" + COMPRESSION_MODE + ", chunkSize=" + CHUNK_SIZE + ")";
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42TermVectorsReader.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42TermVectorsReader.java
deleted file mode 100644
index 5c16ff4..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42TermVectorsReader.java
+++ /dev/null
@@ -1,1073 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable;
-import java.io.IOException;
-import java.util.Collections;
-import java.util.Iterator;
-import java.util.NoSuchElementException;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.TermVectorsReader;
-import org.apache.lucene.codecs.compressing.CompressionMode;
-import org.apache.lucene.codecs.compressing.Decompressor;
-import org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsIndexReader;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.store.AlreadyClosedException;
-import org.apache.lucene.store.ByteArrayDataInput;
-import org.apache.lucene.store.ChecksumIndexInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.Accountable;
-import org.apache.lucene.util.Accountables;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.LongsRef;
-import org.apache.lucene.util.packed.BlockPackedReaderIterator;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * 4.2 term vectors reader
- * @deprecated only for reading old segments
- */
-@Deprecated
-final class Lucene42TermVectorsReader extends TermVectorsReader implements Closeable {
-
-  private final FieldInfos fieldInfos;
-  final Lucene41StoredFieldsIndexReader indexReader;
-  final IndexInput vectorsStream;
-  private final int version;
-  private final int packedIntsVersion;
-  private final CompressionMode compressionMode;
-  private final Decompressor decompressor;
-  private final int chunkSize;
-  private final int numDocs;
-  private boolean closed;
-  private final BlockPackedReaderIterator reader;
-  
-  static final String VECTORS_EXTENSION = "tvd";
-  static final String VECTORS_INDEX_EXTENSION = "tvx";
-
-  static final String CODEC_SFX_IDX = "Index";
-  static final String CODEC_SFX_DAT = "Data";
-
-  static final int VERSION_START = 0;
-  static final int VERSION_CHECKSUM = 1;
-  static final int VERSION_CURRENT = VERSION_CHECKSUM;
-  
-  static final int BLOCK_SIZE = 64;
-
-  static final int POSITIONS = 0x01;
-  static final int   OFFSETS = 0x02;
-  static final int  PAYLOADS = 0x04;
-  static final int FLAGS_BITS = PackedInts.bitsRequired(POSITIONS | OFFSETS | PAYLOADS);
-
-  // used by clone
-  private Lucene42TermVectorsReader(Lucene42TermVectorsReader reader) {
-    this.fieldInfos = reader.fieldInfos;
-    this.vectorsStream = reader.vectorsStream.clone();
-    this.indexReader = reader.indexReader.clone();
-    this.packedIntsVersion = reader.packedIntsVersion;
-    this.compressionMode = reader.compressionMode;
-    this.decompressor = reader.decompressor.clone();
-    this.chunkSize = reader.chunkSize;
-    this.numDocs = reader.numDocs;
-    this.reader = new BlockPackedReaderIterator(vectorsStream, packedIntsVersion, BLOCK_SIZE, 0);
-    this.version = reader.version;
-    this.closed = false;
-  }
-
-  /** Sole constructor. */
-  public Lucene42TermVectorsReader(Directory d, SegmentInfo si, String segmentSuffix, FieldInfos fn,
-      IOContext context, String formatName, CompressionMode compressionMode) throws IOException {
-    this.compressionMode = compressionMode;
-    final String segment = si.name;
-    boolean success = false;
-    fieldInfos = fn;
-    numDocs = si.getDocCount();
-    ChecksumIndexInput indexStream = null;
-    try {
-      // Load the index into memory
-      final String indexStreamFN = IndexFileNames.segmentFileName(segment, segmentSuffix, VECTORS_INDEX_EXTENSION);
-      indexStream = d.openChecksumInput(indexStreamFN, context);
-      final String codecNameIdx = formatName + CODEC_SFX_IDX;
-      version = CodecUtil.checkHeader(indexStream, codecNameIdx, VERSION_START, VERSION_CURRENT);
-      assert CodecUtil.headerLength(codecNameIdx) == indexStream.getFilePointer();
-      indexReader = new Lucene41StoredFieldsIndexReader(indexStream, si);
-      
-      if (version >= VERSION_CHECKSUM) {
-        indexStream.readVLong(); // the end of the data file
-        CodecUtil.checkFooter(indexStream);
-      } else {
-        CodecUtil.checkEOF(indexStream);
-      }
-      indexStream.close();
-      indexStream = null;
-
-      // Open the data file and read metadata
-      final String vectorsStreamFN = IndexFileNames.segmentFileName(segment, segmentSuffix, VECTORS_EXTENSION);
-      vectorsStream = d.openInput(vectorsStreamFN, context);
-      final String codecNameDat = formatName + CODEC_SFX_DAT;
-      int version2 = CodecUtil.checkHeader(vectorsStream, codecNameDat, VERSION_START, VERSION_CURRENT);
-      if (version != version2) {
-        throw new CorruptIndexException("Version mismatch between stored fields index and data: " + version + " != " + version2, vectorsStream);
-      }
-      assert CodecUtil.headerLength(codecNameDat) == vectorsStream.getFilePointer();
-      
-      long pos = vectorsStream.getFilePointer();
-      if (version >= VERSION_CHECKSUM) {
-        // NOTE: data file is too costly to verify checksum against all the bytes on open,
-        // but for now we at least verify proper structure of the checksum footer: which looks
-        // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
-        // such as file truncation.
-        CodecUtil.retrieveChecksum(vectorsStream);
-        vectorsStream.seek(pos);
-      }
-
-      packedIntsVersion = vectorsStream.readVInt();
-      chunkSize = vectorsStream.readVInt();
-      decompressor = compressionMode.newDecompressor();
-      this.reader = new BlockPackedReaderIterator(vectorsStream, packedIntsVersion, BLOCK_SIZE, 0);
-
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(this, indexStream);
-      }
-    }
-  }
-
-  /**
-   * @throws AlreadyClosedException if this TermVectorsReader is closed
-   */
-  private void ensureOpen() throws AlreadyClosedException {
-    if (closed) {
-      throw new AlreadyClosedException("this FieldsReader is closed");
-    }
-  }
-
-  @Override
-  public void close() throws IOException {
-    if (!closed) {
-      IOUtils.close(vectorsStream);
-      closed = true;
-    }
-  }
-
-  @Override
-  public TermVectorsReader clone() {
-    return new Lucene42TermVectorsReader(this);
-  }
-
-  @Override
-  public Fields get(int doc) throws IOException {
-    ensureOpen();
-
-    // seek to the right place
-    {
-      final long startPointer = indexReader.getStartPointer(doc);
-      vectorsStream.seek(startPointer);
-    }
-
-    // decode
-    // - docBase: first doc ID of the chunk
-    // - chunkDocs: number of docs of the chunk
-    final int docBase = vectorsStream.readVInt();
-    final int chunkDocs = vectorsStream.readVInt();
-    if (doc < docBase || doc >= docBase + chunkDocs || docBase + chunkDocs > numDocs) {
-      throw new CorruptIndexException("docBase=" + docBase + ",chunkDocs=" + chunkDocs + ",doc=" + doc, vectorsStream);
-    }
-
-    final int skip; // number of fields to skip
-    final int numFields; // number of fields of the document we're looking for
-    final int totalFields; // total number of fields of the chunk (sum for all docs)
-    if (chunkDocs == 1) {
-      skip = 0;
-      numFields = totalFields = vectorsStream.readVInt();
-    } else {
-      reader.reset(vectorsStream, chunkDocs);
-      int sum = 0;
-      for (int i = docBase; i < doc; ++i) {
-        sum += reader.next();
-      }
-      skip = sum;
-      numFields = (int) reader.next();
-      sum += numFields;
-      for (int i = doc + 1; i < docBase + chunkDocs; ++i) {
-        sum += reader.next();
-      }
-      totalFields = sum;
-    }
-
-    if (numFields == 0) {
-      // no vectors
-      return null;
-    }
-
-    // read field numbers that have term vectors
-    final int[] fieldNums;
-    {
-      final int token = vectorsStream.readByte() & 0xFF;
-      assert token != 0; // means no term vectors, cannot happen since we checked for numFields == 0
-      final int bitsPerFieldNum = token & 0x1F;
-      int totalDistinctFields = token >>> 5;
-      if (totalDistinctFields == 0x07) {
-        totalDistinctFields += vectorsStream.readVInt();
-      }
-      ++totalDistinctFields;
-      final PackedInts.ReaderIterator it = PackedInts.getReaderIteratorNoHeader(vectorsStream, PackedInts.Format.PACKED, packedIntsVersion, totalDistinctFields, bitsPerFieldNum, 1);
-      fieldNums = new int[totalDistinctFields];
-      for (int i = 0; i < totalDistinctFields; ++i) {
-        fieldNums[i] = (int) it.next();
-      }
-    }
-
-    // read field numbers and flags
-    final int[] fieldNumOffs = new int[numFields];
-    final PackedInts.Reader flags;
-    {
-      final int bitsPerOff = PackedInts.bitsRequired(fieldNums.length - 1);
-      final PackedInts.Reader allFieldNumOffs = PackedInts.getReaderNoHeader(vectorsStream, PackedInts.Format.PACKED, packedIntsVersion, totalFields, bitsPerOff);
-      switch (vectorsStream.readVInt()) {
-        case 0:
-          final PackedInts.Reader fieldFlags = PackedInts.getReaderNoHeader(vectorsStream, PackedInts.Format.PACKED, packedIntsVersion, fieldNums.length, FLAGS_BITS);
-          PackedInts.Mutable f = PackedInts.getMutable(totalFields, FLAGS_BITS, PackedInts.COMPACT);
-          for (int i = 0; i < totalFields; ++i) {
-            final int fieldNumOff = (int) allFieldNumOffs.get(i);
-            assert fieldNumOff >= 0 && fieldNumOff < fieldNums.length;
-            final int fgs = (int) fieldFlags.get(fieldNumOff);
-            f.set(i, fgs);
-          }
-          flags = f;
-          break;
-        case 1:
-          flags = PackedInts.getReaderNoHeader(vectorsStream, PackedInts.Format.PACKED, packedIntsVersion, totalFields, FLAGS_BITS);
-          break;
-        default:
-          throw new AssertionError();
-      }
-      for (int i = 0; i < numFields; ++i) {
-        fieldNumOffs[i] = (int) allFieldNumOffs.get(skip + i);
-      }
-    }
-
-    // number of terms per field for all fields
-    final PackedInts.Reader numTerms;
-    final int totalTerms;
-    {
-      final int bitsRequired = vectorsStream.readVInt();
-      numTerms = PackedInts.getReaderNoHeader(vectorsStream, PackedInts.Format.PACKED, packedIntsVersion, totalFields, bitsRequired);
-      int sum = 0;
-      for (int i = 0; i < totalFields; ++i) {
-        sum += numTerms.get(i);
-      }
-      totalTerms = sum;
-    }
-
-    // term lengths
-    int docOff = 0, docLen = 0, totalLen;
-    final int[] fieldLengths = new int[numFields];
-    final int[][] prefixLengths = new int[numFields][];
-    final int[][] suffixLengths = new int[numFields][];
-    {
-      reader.reset(vectorsStream, totalTerms);
-      // skip
-      int toSkip = 0;
-      for (int i = 0; i < skip; ++i) {
-        toSkip += numTerms.get(i);
-      }
-      reader.skip(toSkip);
-      // read prefix lengths
-      for (int i = 0; i < numFields; ++i) {
-        final int termCount = (int) numTerms.get(skip + i);
-        final int[] fieldPrefixLengths = new int[termCount];
-        prefixLengths[i] = fieldPrefixLengths;
-        for (int j = 0; j < termCount; ) {
-          final LongsRef next = reader.next(termCount - j);
-          for (int k = 0; k < next.length; ++k) {
-            fieldPrefixLengths[j++] = (int) next.longs[next.offset + k];
-          }
-        }
-      }
-      reader.skip(totalTerms - reader.ord());
-
-      reader.reset(vectorsStream, totalTerms);
-      // skip
-      toSkip = 0;
-      for (int i = 0; i < skip; ++i) {
-        for (int j = 0; j < numTerms.get(i); ++j) {
-          docOff += reader.next();
-        }
-      }
-      for (int i = 0; i < numFields; ++i) {
-        final int termCount = (int) numTerms.get(skip + i);
-        final int[] fieldSuffixLengths = new int[termCount];
-        suffixLengths[i] = fieldSuffixLengths;
-        for (int j = 0; j < termCount; ) {
-          final LongsRef next = reader.next(termCount - j);
-          for (int k = 0; k < next.length; ++k) {
-            fieldSuffixLengths[j++] = (int) next.longs[next.offset + k];
-          }
-        }
-        fieldLengths[i] = sum(suffixLengths[i]);
-        docLen += fieldLengths[i];
-      }
-      totalLen = docOff + docLen;
-      for (int i = skip + numFields; i < totalFields; ++i) {
-        for (int j = 0; j < numTerms.get(i); ++j) {
-          totalLen += reader.next();
-        }
-      }
-    }
-
-    // term freqs
-    final int[] termFreqs = new int[totalTerms];
-    {
-      reader.reset(vectorsStream, totalTerms);
-      for (int i = 0; i < totalTerms; ) {
-        final LongsRef next = reader.next(totalTerms - i);
-        for (int k = 0; k < next.length; ++k) {
-          termFreqs[i++] = 1 + (int) next.longs[next.offset + k];
-        }
-      }
-    }
-
-    // total number of positions, offsets and payloads
-    int totalPositions = 0, totalOffsets = 0, totalPayloads = 0;
-    for (int i = 0, termIndex = 0; i < totalFields; ++i) {
-      final int f = (int) flags.get(i);
-      final int termCount = (int) numTerms.get(i);
-      for (int j = 0; j < termCount; ++j) {
-        final int freq = termFreqs[termIndex++];
-        if ((f & POSITIONS) != 0) {
-          totalPositions += freq;
-        }
-        if ((f & OFFSETS) != 0) {
-          totalOffsets += freq;
-        }
-        if ((f & PAYLOADS) != 0) {
-          totalPayloads += freq;
-        }
-      }
-      assert i != totalFields - 1 || termIndex == totalTerms : termIndex + " " + totalTerms;
-    }
-
-    final int[][] positionIndex = positionIndex(skip, numFields, numTerms, termFreqs);
-    final int[][] positions, startOffsets, lengths;
-    if (totalPositions > 0) {
-      positions = readPositions(skip, numFields, flags, numTerms, termFreqs, POSITIONS, totalPositions, positionIndex);
-    } else {
-      positions = new int[numFields][];
-    }
-
-    if (totalOffsets > 0) {
-      // average number of chars per term
-      final float[] charsPerTerm = new float[fieldNums.length];
-      for (int i = 0; i < charsPerTerm.length; ++i) {
-        charsPerTerm[i] = Float.intBitsToFloat(vectorsStream.readInt());
-      }
-      startOffsets = readPositions(skip, numFields, flags, numTerms, termFreqs, OFFSETS, totalOffsets, positionIndex);
-      lengths = readPositions(skip, numFields, flags, numTerms, termFreqs, OFFSETS, totalOffsets, positionIndex);
-
-      for (int i = 0; i < numFields; ++i) {
-        final int[] fStartOffsets = startOffsets[i];
-        final int[] fPositions = positions[i];
-        // patch offsets from positions
-        if (fStartOffsets != null && fPositions != null) {
-          final float fieldCharsPerTerm = charsPerTerm[fieldNumOffs[i]];
-          for (int j = 0; j < startOffsets[i].length; ++j) {
-            fStartOffsets[j] += (int) (fieldCharsPerTerm * fPositions[j]);
-          }
-        }
-        if (fStartOffsets != null) {
-          final int[] fPrefixLengths = prefixLengths[i];
-          final int[] fSuffixLengths = suffixLengths[i];
-          final int[] fLengths = lengths[i];
-          for (int j = 0, end = (int) numTerms.get(skip + i); j < end; ++j) {
-            // delta-decode start offsets and  patch lengths using term lengths
-            final int termLength = fPrefixLengths[j] + fSuffixLengths[j];
-            lengths[i][positionIndex[i][j]] += termLength;
-            for (int k = positionIndex[i][j] + 1; k < positionIndex[i][j + 1]; ++k) {
-              fStartOffsets[k] += fStartOffsets[k - 1];
-              fLengths[k] += termLength;
-            }
-          }
-        }
-      }
-    } else {
-      startOffsets = lengths = new int[numFields][];
-    }
-    if (totalPositions > 0) {
-      // delta-decode positions
-      for (int i = 0; i < numFields; ++i) {
-        final int[] fPositions = positions[i];
-        final int[] fpositionIndex = positionIndex[i];
-        if (fPositions != null) {
-          for (int j = 0, end = (int) numTerms.get(skip + i); j < end; ++j) {
-            // delta-decode start offsets
-            for (int k = fpositionIndex[j] + 1; k < fpositionIndex[j + 1]; ++k) {
-              fPositions[k] += fPositions[k - 1];
-            }
-          }
-        }
-      }
-    }
-
-    // payload lengths
-    final int[][] payloadIndex = new int[numFields][];
-    int totalPayloadLength = 0;
-    int payloadOff = 0;
-    int payloadLen = 0;
-    if (totalPayloads > 0) {
-      reader.reset(vectorsStream, totalPayloads);
-      // skip
-      int termIndex = 0;
-      for (int i = 0; i < skip; ++i) {
-        final int f = (int) flags.get(i);
-        final int termCount = (int) numTerms.get(i);
-        if ((f & PAYLOADS) != 0) {
-          for (int j = 0; j < termCount; ++j) {
-            final int freq = termFreqs[termIndex + j];
-            for (int k = 0; k < freq; ++k) {
-              final int l = (int) reader.next();
-              payloadOff += l;
-            }
-          }
-        }
-        termIndex += termCount;
-      }
-      totalPayloadLength = payloadOff;
-      // read doc payload lengths
-      for (int i = 0; i < numFields; ++i) {
-        final int f = (int) flags.get(skip + i);
-        final int termCount = (int) numTerms.get(skip + i);
-        if ((f & PAYLOADS) != 0) {
-          final int totalFreq = positionIndex[i][termCount];
-          payloadIndex[i] = new int[totalFreq + 1];
-          int posIdx = 0;
-          payloadIndex[i][posIdx] = payloadLen;
-          for (int j = 0; j < termCount; ++j) {
-            final int freq = termFreqs[termIndex + j];
-            for (int k = 0; k < freq; ++k) {
-              final int payloadLength = (int) reader.next();
-              payloadLen += payloadLength;
-              payloadIndex[i][posIdx+1] = payloadLen;
-              ++posIdx;
-            }
-          }
-          assert posIdx == totalFreq;
-        }
-        termIndex += termCount;
-      }
-      totalPayloadLength += payloadLen;
-      for (int i = skip + numFields; i < totalFields; ++i) {
-        final int f = (int) flags.get(i);
-        final int termCount = (int) numTerms.get(i);
-        if ((f & PAYLOADS) != 0) {
-          for (int j = 0; j < termCount; ++j) {
-            final int freq = termFreqs[termIndex + j];
-            for (int k = 0; k < freq; ++k) {
-              totalPayloadLength += reader.next();
-            }
-          }
-        }
-        termIndex += termCount;
-      }
-      assert termIndex == totalTerms : termIndex + " " + totalTerms;
-    }
-
-    // decompress data
-    final BytesRef suffixBytes = new BytesRef();
-    decompressor.decompress(vectorsStream, totalLen + totalPayloadLength, docOff + payloadOff, docLen + payloadLen, suffixBytes);
-    suffixBytes.length = docLen;
-    final BytesRef payloadBytes = new BytesRef(suffixBytes.bytes, suffixBytes.offset + docLen, payloadLen);
-
-    final int[] fieldFlags = new int[numFields];
-    for (int i = 0; i < numFields; ++i) {
-      fieldFlags[i] = (int) flags.get(skip + i);
-    }
-
-    final int[] fieldNumTerms = new int[numFields];
-    for (int i = 0; i < numFields; ++i) {
-      fieldNumTerms[i] = (int) numTerms.get(skip + i);
-    }
-
-    final int[][] fieldTermFreqs = new int[numFields][];
-    {
-      int termIdx = 0;
-      for (int i = 0; i < skip; ++i) {
-        termIdx += numTerms.get(i);
-      }
-      for (int i = 0; i < numFields; ++i) {
-        final int termCount = (int) numTerms.get(skip + i);
-        fieldTermFreqs[i] = new int[termCount];
-        for (int j = 0; j < termCount; ++j) {
-          fieldTermFreqs[i][j] = termFreqs[termIdx++];
-        }
-      }
-    }
-
-    assert sum(fieldLengths) == docLen : sum(fieldLengths) + " != " + docLen;
-
-    return new TVFields(fieldNums, fieldFlags, fieldNumOffs, fieldNumTerms, fieldLengths,
-        prefixLengths, suffixLengths, fieldTermFreqs,
-        positionIndex, positions, startOffsets, lengths,
-        payloadBytes, payloadIndex,
-        suffixBytes);
-  }
-
-  // field -> term index -> position index
-  private int[][] positionIndex(int skip, int numFields, PackedInts.Reader numTerms, int[] termFreqs) {
-    final int[][] positionIndex = new int[numFields][];
-    int termIndex = 0;
-    for (int i = 0; i < skip; ++i) {
-      final int termCount = (int) numTerms.get(i);
-      termIndex += termCount;
-    }
-    for (int i = 0; i < numFields; ++i) {
-      final int termCount = (int) numTerms.get(skip + i);
-      positionIndex[i] = new int[termCount + 1];
-      for (int j = 0; j < termCount; ++j) {
-        final int freq = termFreqs[termIndex+j];
-        positionIndex[i][j + 1] = positionIndex[i][j] + freq;
-      }
-      termIndex += termCount;
-    }
-    return positionIndex;
-  }
-
-  private int[][] readPositions(int skip, int numFields, PackedInts.Reader flags, PackedInts.Reader numTerms, int[] termFreqs, int flag, final int totalPositions, int[][] positionIndex) throws IOException {
-    final int[][] positions = new int[numFields][];
-    reader.reset(vectorsStream, totalPositions);
-    // skip
-    int toSkip = 0;
-    int termIndex = 0;
-    for (int i = 0; i < skip; ++i) {
-      final int f = (int) flags.get(i);
-      final int termCount = (int) numTerms.get(i);
-      if ((f & flag) != 0) {
-        for (int j = 0; j < termCount; ++j) {
-          final int freq = termFreqs[termIndex+j];
-          toSkip += freq;
-        }
-      }
-      termIndex += termCount;
-    }
-    reader.skip(toSkip);
-    // read doc positions
-    for (int i = 0; i < numFields; ++i) {
-      final int f = (int) flags.get(skip + i);
-      final int termCount = (int) numTerms.get(skip + i);
-      if ((f & flag) != 0) {
-        final int totalFreq = positionIndex[i][termCount];
-        final int[] fieldPositions = new int[totalFreq];
-        positions[i] = fieldPositions;
-        for (int j = 0; j < totalFreq; ) {
-          final LongsRef nextPositions = reader.next(totalFreq - j);
-          for (int k = 0; k < nextPositions.length; ++k) {
-            fieldPositions[j++] = (int) nextPositions.longs[nextPositions.offset + k];
-          }
-        }
-      }
-      termIndex += termCount;
-    }
-    reader.skip(totalPositions - reader.ord());
-    return positions;
-  }
-
-  private class TVFields extends Fields {
-
-    private final int[] fieldNums, fieldFlags, fieldNumOffs, numTerms, fieldLengths;
-    private final int[][] prefixLengths, suffixLengths, termFreqs, positionIndex, positions, startOffsets, lengths, payloadIndex;
-    private final BytesRef suffixBytes, payloadBytes;
-
-    public TVFields(int[] fieldNums, int[] fieldFlags, int[] fieldNumOffs, int[] numTerms, int[] fieldLengths,
-        int[][] prefixLengths, int[][] suffixLengths, int[][] termFreqs,
-        int[][] positionIndex, int[][] positions, int[][] startOffsets, int[][] lengths,
-        BytesRef payloadBytes, int[][] payloadIndex,
-        BytesRef suffixBytes) {
-      this.fieldNums = fieldNums;
-      this.fieldFlags = fieldFlags;
-      this.fieldNumOffs = fieldNumOffs;
-      this.numTerms = numTerms;
-      this.fieldLengths = fieldLengths;
-      this.prefixLengths = prefixLengths;
-      this.suffixLengths = suffixLengths;
-      this.termFreqs = termFreqs;
-      this.positionIndex = positionIndex;
-      this.positions = positions;
-      this.startOffsets = startOffsets;
-      this.lengths = lengths;
-      this.payloadBytes = payloadBytes;
-      this.payloadIndex = payloadIndex;
-      this.suffixBytes = suffixBytes;
-    }
-
-    @Override
-    public Iterator<String> iterator() {
-      return new Iterator<String>() {
-        int i = 0;
-        @Override
-        public boolean hasNext() {
-          return i < fieldNumOffs.length;
-        }
-        @Override
-        public String next() {
-          if (!hasNext()) {
-            throw new NoSuchElementException();
-          }
-          final int fieldNum = fieldNums[fieldNumOffs[i++]];
-          return fieldInfos.fieldInfo(fieldNum).name;
-        }
-        @Override
-        public void remove() {
-          throw new UnsupportedOperationException();
-        }
-      };
-    }
-
-    @Override
-    public Terms terms(String field) throws IOException {
-      final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
-      if (fieldInfo == null) {
-        return null;
-      }
-      int idx = -1;
-      for (int i = 0; i < fieldNumOffs.length; ++i) {
-        if (fieldNums[fieldNumOffs[i]] == fieldInfo.number) {
-          idx = i;
-          break;
-        }
-      }
-
-      if (idx == -1 || numTerms[idx] == 0) {
-        // no term
-        return null;
-      }
-      int fieldOff = 0, fieldLen = -1;
-      for (int i = 0; i < fieldNumOffs.length; ++i) {
-        if (i < idx) {
-          fieldOff += fieldLengths[i];
-        } else {
-          fieldLen = fieldLengths[i];
-          break;
-        }
-      }
-      assert fieldLen >= 0;
-      return new TVTerms(numTerms[idx], fieldFlags[idx],
-          prefixLengths[idx], suffixLengths[idx], termFreqs[idx],
-          positionIndex[idx], positions[idx], startOffsets[idx], lengths[idx],
-          payloadIndex[idx], payloadBytes,
-          new BytesRef(suffixBytes.bytes, suffixBytes.offset + fieldOff, fieldLen));
-    }
-
-    @Override
-    public int size() {
-      return fieldNumOffs.length;
-    }
-
-  }
-
-  private class TVTerms extends Terms {
-
-    private final int numTerms, flags;
-    private final int[] prefixLengths, suffixLengths, termFreqs, positionIndex, positions, startOffsets, lengths, payloadIndex;
-    private final BytesRef termBytes, payloadBytes;
-
-    TVTerms(int numTerms, int flags, int[] prefixLengths, int[] suffixLengths, int[] termFreqs,
-        int[] positionIndex, int[] positions, int[] startOffsets, int[] lengths,
-        int[] payloadIndex, BytesRef payloadBytes,
-        BytesRef termBytes) {
-      this.numTerms = numTerms;
-      this.flags = flags;
-      this.prefixLengths = prefixLengths;
-      this.suffixLengths = suffixLengths;
-      this.termFreqs = termFreqs;
-      this.positionIndex = positionIndex;
-      this.positions = positions;
-      this.startOffsets = startOffsets;
-      this.lengths = lengths;
-      this.payloadIndex = payloadIndex;
-      this.payloadBytes = payloadBytes;
-      this.termBytes = termBytes;
-    }
-
-    @Override
-    public TermsEnum iterator(TermsEnum reuse) throws IOException {
-      final TVTermsEnum termsEnum;
-      if (reuse != null && reuse instanceof TVTermsEnum) {
-        termsEnum = (TVTermsEnum) reuse;
-      } else {
-        termsEnum = new TVTermsEnum();
-      }
-      termsEnum.reset(numTerms, flags, prefixLengths, suffixLengths, termFreqs, positionIndex, positions, startOffsets, lengths,
-          payloadIndex, payloadBytes,
-          new ByteArrayDataInput(termBytes.bytes, termBytes.offset, termBytes.length));
-      return termsEnum;
-    }
-
-    @Override
-    public long size() throws IOException {
-      return numTerms;
-    }
-
-    @Override
-    public long getSumTotalTermFreq() throws IOException {
-      return -1L;
-    }
-
-    @Override
-    public long getSumDocFreq() throws IOException {
-      return numTerms;
-    }
-
-    @Override
-    public int getDocCount() throws IOException {
-      return 1;
-    }
-
-    @Override
-    public boolean hasFreqs() {
-      return true;
-    }
-
-    @Override
-    public boolean hasOffsets() {
-      return (flags & OFFSETS) != 0;
-    }
-
-    @Override
-    public boolean hasPositions() {
-      return (flags & POSITIONS) != 0;
-    }
-
-    @Override
-    public boolean hasPayloads() {
-      return (flags & PAYLOADS) != 0;
-    }
-
-  }
-
-  private static class TVTermsEnum extends TermsEnum {
-
-    private int numTerms, startPos, ord;
-    private int[] prefixLengths, suffixLengths, termFreqs, positionIndex, positions, startOffsets, lengths, payloadIndex;
-    private ByteArrayDataInput in;
-    private BytesRef payloads;
-    private final BytesRef term;
-
-    private TVTermsEnum() {
-      term = new BytesRef(16);
-    }
-
-    void reset(int numTerms, int flags, int[] prefixLengths, int[] suffixLengths, int[] termFreqs, int[] positionIndex, int[] positions, int[] startOffsets, int[] lengths,
-        int[] payloadIndex, BytesRef payloads, ByteArrayDataInput in) {
-      this.numTerms = numTerms;
-      this.prefixLengths = prefixLengths;
-      this.suffixLengths = suffixLengths;
-      this.termFreqs = termFreqs;
-      this.positionIndex = positionIndex;
-      this.positions = positions;
-      this.startOffsets = startOffsets;
-      this.lengths = lengths;
-      this.payloadIndex = payloadIndex;
-      this.payloads = payloads;
-      this.in = in;
-      startPos = in.getPosition();
-      reset();
-    }
-
-    void reset() {
-      term.length = 0;
-      in.setPosition(startPos);
-      ord = -1;
-    }
-
-    @Override
-    public BytesRef next() throws IOException {
-      if (ord == numTerms - 1) {
-        return null;
-      } else {
-        assert ord < numTerms;
-        ++ord;
-      }
-
-      // read term
-      term.offset = 0;
-      term.length = prefixLengths[ord] + suffixLengths[ord];
-      if (term.length > term.bytes.length) {
-        term.bytes = ArrayUtil.grow(term.bytes, term.length);
-      }
-      in.readBytes(term.bytes, prefixLengths[ord], suffixLengths[ord]);
-
-      return term;
-    }
-
-    @Override
-    public SeekStatus seekCeil(BytesRef text)
-        throws IOException {
-      if (ord < numTerms && ord >= 0) {
-        final int cmp = term().compareTo(text);
-        if (cmp == 0) {
-          return SeekStatus.FOUND;
-        } else if (cmp > 0) {
-          reset();
-        }
-      }
-      // linear scan
-      while (true) {
-        final BytesRef term = next();
-        if (term == null) {
-          return SeekStatus.END;
-        }
-        final int cmp = term.compareTo(text);
-        if (cmp > 0) {
-          return SeekStatus.NOT_FOUND;
-        } else if (cmp == 0) {
-          return SeekStatus.FOUND;
-        }
-      }
-    }
-
-    @Override
-    public void seekExact(long ord) throws IOException {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public BytesRef term() throws IOException {
-      return term;
-    }
-
-    @Override
-    public long ord() throws IOException {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public int docFreq() throws IOException {
-      return 1;
-    }
-
-    @Override
-    public long totalTermFreq() throws IOException {
-      return termFreqs[ord];
-    }
-
-    @Override
-    public final DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
-      final TVDocsEnum docsEnum;
-      if (reuse != null && reuse instanceof TVDocsEnum) {
-        docsEnum = (TVDocsEnum) reuse;
-      } else {
-        docsEnum = new TVDocsEnum();
-      }
-
-      docsEnum.reset(liveDocs, termFreqs[ord], positionIndex[ord], positions, startOffsets, lengths, payloads, payloadIndex);
-      return docsEnum;
-    }
-
-    @Override
-    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
-      if (positions == null && startOffsets == null) {
-        return null;
-      }
-      // TODO: slightly sheisty
-      return (DocsAndPositionsEnum) docs(liveDocs, reuse, flags);
-    }
-
-  }
-
-  private static class TVDocsEnum extends DocsAndPositionsEnum {
-
-    private Bits liveDocs;
-    private int doc = -1;
-    private int termFreq;
-    private int positionIndex;
-    private int[] positions;
-    private int[] startOffsets;
-    private int[] lengths;
-    private final BytesRef payload;
-    private int[] payloadIndex;
-    private int basePayloadOffset;
-    private int i;
-
-    TVDocsEnum() {
-      payload = new BytesRef();
-    }
-
-    public void reset(Bits liveDocs, int freq, int positionIndex, int[] positions,
-        int[] startOffsets, int[] lengths, BytesRef payloads,
-        int[] payloadIndex) {
-      this.liveDocs = liveDocs;
-      this.termFreq = freq;
-      this.positionIndex = positionIndex;
-      this.positions = positions;
-      this.startOffsets = startOffsets;
-      this.lengths = lengths;
-      this.basePayloadOffset = payloads.offset;
-      this.payload.bytes = payloads.bytes;
-      payload.offset = payload.length = 0;
-      this.payloadIndex = payloadIndex;
-
-      doc = i = -1;
-    }
-
-    private void checkDoc() {
-      if (doc == NO_MORE_DOCS) {
-        throw new IllegalStateException("DocsEnum exhausted");
-      } else if (doc == -1) {
-        throw new IllegalStateException("DocsEnum not started");
-      }
-    }
-
-    private void checkPosition() {
-      checkDoc();
-      if (i < 0) {
-        throw new IllegalStateException("Position enum not started");
-      } else if (i >= termFreq) {
-        throw new IllegalStateException("Read past last position");
-      }
-    }
-
-    @Override
-    public int nextPosition() throws IOException {
-      if (doc != 0) {
-        throw new IllegalStateException();
-      } else if (i >= termFreq - 1) {
-        throw new IllegalStateException("Read past last position");
-      }
-
-      ++i;
-
-      if (payloadIndex != null) {
-        payload.offset = basePayloadOffset + payloadIndex[positionIndex + i];
-        payload.length = payloadIndex[positionIndex + i + 1] - payloadIndex[positionIndex + i];
-      }
-
-      if (positions == null) {
-        return -1;
-      } else {
-        return positions[positionIndex + i];
-      }
-    }
-
-    @Override
-    public int startOffset() throws IOException {
-      checkPosition();
-      if (startOffsets == null) {
-        return -1;
-      } else {
-        return startOffsets[positionIndex + i];
-      }
-    }
-
-    @Override
-    public int endOffset() throws IOException {
-      checkPosition();
-      if (startOffsets == null) {
-        return -1;
-      } else {
-        return startOffsets[positionIndex + i] + lengths[positionIndex + i];
-      }
-    }
-
-    @Override
-    public BytesRef getPayload() throws IOException {
-      checkPosition();
-      if (payloadIndex == null || payload.length == 0) {
-        return null;
-      } else {
-        return payload;
-      }
-    }
-
-    @Override
-    public int freq() throws IOException {
-      checkDoc();
-      return termFreq;
-    }
-
-    @Override
-    public int docID() {
-      return doc;
-    }
-
-    @Override
-    public int nextDoc() throws IOException {
-      if (doc == -1 && (liveDocs == null || liveDocs.get(0))) {
-        return (doc = 0);
-      } else {
-        return (doc = NO_MORE_DOCS);
-      }
-    }
-
-    @Override
-    public int advance(int target) throws IOException {
-      return slowAdvance(target);
-    }
-
-    @Override
-    public long cost() {
-      return 1;
-    }
-  }
-
-  private static int sum(int[] arr) {
-    int sum = 0;
-    for (int el : arr) {
-      sum += el;
-    }
-    return sum;
-  }
-
-  @Override
-  public long ramBytesUsed() {
-    return indexReader.ramBytesUsed();
-  }
-  
-  @Override
-  public Iterable<? extends Accountable> getChildResources() {
-    return Collections.singleton(Accountables.namedAccountable("term vector index", indexReader));
-  }
-  
-  @Override
-  public void checkIntegrity() throws IOException {
-    if (version >= VERSION_CHECKSUM) {
-      CodecUtil.checksumEntireFile(vectorsStream);
-    }
-  }
-
-  @Override
-  public String toString() {
-    return getClass().getSimpleName() + "(mode=" + compressionMode + ",chunksize=" + chunkSize + ")";
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/package.html b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/package.html
deleted file mode 100644
index 48043b3..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Lucene 4.2 file format.
-</body>
-</html>
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45Codec.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45Codec.java
deleted file mode 100644
index a7f66cf..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45Codec.java
+++ /dev/null
@@ -1,149 +0,0 @@
-package org.apache.lucene.codecs.lucene45;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.CompoundFormat;
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.LiveDocsFormat;
-import org.apache.lucene.codecs.NormsConsumer;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.SegmentInfoFormat;
-import org.apache.lucene.codecs.StoredFieldsFormat;
-import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40CompoundFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40SegmentInfoFormat;
-import org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat;
-import org.apache.lucene.codecs.lucene42.Lucene42FieldInfosFormat;
-import org.apache.lucene.codecs.lucene42.Lucene42NormsFormat;
-import org.apache.lucene.codecs.lucene42.Lucene42TermVectorsFormat;
-import org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat;
-import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
-import org.apache.lucene.index.SegmentWriteState;
-
-/**
- * Implements the Lucene 4.5 index format
- * @deprecated Only for reading old 4.3-4.5 segments
- */
-@Deprecated
-public class Lucene45Codec extends Codec {
-  private final StoredFieldsFormat fieldsFormat = new Lucene41StoredFieldsFormat();
-  private final TermVectorsFormat vectorsFormat = new Lucene42TermVectorsFormat();
-  private final FieldInfosFormat fieldInfosFormat = new Lucene42FieldInfosFormat();
-  private final SegmentInfoFormat infosFormat = new Lucene40SegmentInfoFormat();
-  private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat();
-  private final CompoundFormat compoundFormat = new Lucene40CompoundFormat();
-  
-  private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
-    @Override
-    public PostingsFormat getPostingsFormatForField(String field) {
-      return Lucene45Codec.this.getPostingsFormatForField(field);
-    }
-  };
-  
-  
-  private final DocValuesFormat docValuesFormat = new PerFieldDocValuesFormat() {
-    @Override
-    public DocValuesFormat getDocValuesFormatForField(String field) {
-      return Lucene45Codec.this.getDocValuesFormatForField(field);
-    }
-  };
-
-  /** Sole constructor. */
-  public Lucene45Codec() {
-    super("Lucene45");
-  }
-  
-  @Override
-  public StoredFieldsFormat storedFieldsFormat() {
-    return fieldsFormat;
-  }
-  
-  @Override
-  public TermVectorsFormat termVectorsFormat() {
-    return vectorsFormat;
-  }
-
-  @Override
-  public final PostingsFormat postingsFormat() {
-    return postingsFormat;
-  }
-  
-  @Override
-  public FieldInfosFormat fieldInfosFormat() {
-    return fieldInfosFormat;
-  }
-  
-  @Override
-  public SegmentInfoFormat segmentInfoFormat() {
-    return infosFormat;
-  }
-  
-  @Override
-  public final LiveDocsFormat liveDocsFormat() {
-    return liveDocsFormat;
-  }
-  
-  @Override
-  public CompoundFormat compoundFormat() {
-    return compoundFormat;
-  }
-
-  /** Returns the postings format that should be used for writing 
-   *  new segments of <code>field</code>.
-   *  
-   *  The default implementation always returns "Lucene41"
-   */
-  public PostingsFormat getPostingsFormatForField(String field) {
-    return defaultFormat;
-  }
-  
-  /** Returns the docvalues format that should be used for writing 
-   *  new segments of <code>field</code>.
-   *  
-   *  The default implementation always returns "Lucene45"
-   */
-  public DocValuesFormat getDocValuesFormatForField(String field) {
-    return defaultDVFormat;
-  }
-  
-  @Override
-  public final DocValuesFormat docValuesFormat() {
-    return docValuesFormat;
-  }
-
-  private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene41");
-  private final DocValuesFormat defaultDVFormat = DocValuesFormat.forName("Lucene45");
-
-  private final NormsFormat normsFormat = new Lucene42NormsFormat() {
-    @Override
-    public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
-      throw new UnsupportedOperationException("this codec can only be used for reading");
-    }
-  };
-
-  @Override
-  public NormsFormat normsFormat() {
-    return normsFormat;
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesConsumer.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesConsumer.java
deleted file mode 100644
index 3a7ace1..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesConsumer.java
+++ /dev/null
@@ -1,437 +0,0 @@
-package org.apache.lucene.codecs.lucene45;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable; // javadocs
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.HashSet;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.FieldInfo.DocValuesType;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.RAMOutputStream;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.BytesRefBuilder;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.MathUtil;
-import org.apache.lucene.util.StringHelper;
-import org.apache.lucene.util.packed.BlockPackedWriter;
-import org.apache.lucene.util.packed.MonotonicBlockPackedWriter;
-import org.apache.lucene.util.packed.PackedInts;
-
-/** 
- * writer for 4.5 docvalues format
- * @deprecated only for old 4.x segments
- */
-@Deprecated
-class Lucene45DocValuesConsumer extends DocValuesConsumer implements Closeable {
-
-  static final int BLOCK_SIZE = 16384;
-  static final int ADDRESS_INTERVAL = 16;
-
-  /** Compressed using packed blocks of ints. */
-  public static final int DELTA_COMPRESSED = 0;
-  /** Compressed by computing the GCD. */
-  public static final int GCD_COMPRESSED = 1;
-  /** Compressed by giving IDs to unique values. */
-  public static final int TABLE_COMPRESSED = 2;
-  
-  /** Uncompressed binary, written directly (fixed length). */
-  public static final int BINARY_FIXED_UNCOMPRESSED = 0;
-  /** Uncompressed binary, written directly (variable length). */
-  public static final int BINARY_VARIABLE_UNCOMPRESSED = 1;
-  /** Compressed binary with shared prefixes */
-  public static final int BINARY_PREFIX_COMPRESSED = 2;
-
-  /** Standard storage for sorted set values with 1 level of indirection:
-   *  docId -> address -> ord. */
-  public static final int SORTED_SET_WITH_ADDRESSES = 0;
-  /** Single-valued sorted set values, encoded as sorted values, so no level
-   *  of indirection: docId -> ord. */
-  public static final int SORTED_SET_SINGLE_VALUED_SORTED = 1;
-
-  IndexOutput data, meta;
-  final int maxDoc;
-  
-  /** expert: Creates a new writer */
-  public Lucene45DocValuesConsumer(SegmentWriteState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
-    boolean success = false;
-    try {
-      String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
-      data = state.directory.createOutput(dataName, state.context);
-      CodecUtil.writeHeader(data, dataCodec, Lucene45DocValuesFormat.VERSION_CURRENT);
-      String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
-      meta = state.directory.createOutput(metaName, state.context);
-      CodecUtil.writeHeader(meta, metaCodec, Lucene45DocValuesFormat.VERSION_CURRENT);
-      maxDoc = state.segmentInfo.getDocCount();
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(this);
-      }
-    }
-  }
-  
-  @Override
-  public void addNumericField(FieldInfo field, Iterable<Number> values) throws IOException {
-    checkCanWrite(field);
-    addNumericField(field, values, true);
-  }
-
-  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {
-    long count = 0;
-    long minValue = Long.MAX_VALUE;
-    long maxValue = Long.MIN_VALUE;
-    long gcd = 0;
-    boolean missing = false;
-    // TODO: more efficient?
-    HashSet<Long> uniqueValues = null;
-    if (optimizeStorage) {
-      uniqueValues = new HashSet<>();
-
-      for (Number nv : values) {
-        final long v;
-        if (nv == null) {
-          v = 0;
-          missing = true;
-        } else {
-          v = nv.longValue();
-        }
-
-        if (gcd != 1) {
-          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {
-            // in that case v - minValue might overflow and make the GCD computation return
-            // wrong results. Since these extreme values are unlikely, we just discard
-            // GCD computation for them
-            gcd = 1;
-          } else if (count != 0) { // minValue needs to be set first
-            gcd = MathUtil.gcd(gcd, v - minValue);
-          }
-        }
-
-        minValue = Math.min(minValue, v);
-        maxValue = Math.max(maxValue, v);
-
-        if (uniqueValues != null) {
-          if (uniqueValues.add(v)) {
-            if (uniqueValues.size() > 256) {
-              uniqueValues = null;
-            }
-          }
-        }
-
-        ++count;
-      }
-    } else {
-      for (@SuppressWarnings("unused") Number nv : values) {
-        ++count;
-      }
-    }
-    
-    final long delta = maxValue - minValue;
-
-    final int format;
-    if (uniqueValues != null
-        && (PackedInts.bitsRequired(uniqueValues.size() - 1) < PackedInts.unsignedBitsRequired(delta))
-        && count <= Integer.MAX_VALUE) {
-      format = TABLE_COMPRESSED;
-    } else if (gcd != 0 && gcd != 1) {
-      format = GCD_COMPRESSED;
-    } else {
-      format = DELTA_COMPRESSED;
-    }
-    meta.writeVInt(field.number);
-    meta.writeByte(Lucene45DocValuesFormat.NUMERIC);
-    meta.writeVInt(format);
-    if (missing) {
-      meta.writeLong(data.getFilePointer());
-      writeMissingBitset(values);
-    } else {
-      meta.writeLong(-1L);
-    }
-    meta.writeVInt(PackedInts.VERSION_CURRENT);
-    meta.writeLong(data.getFilePointer());
-    meta.writeVLong(count);
-    meta.writeVInt(BLOCK_SIZE);
-
-    switch (format) {
-      case GCD_COMPRESSED:
-        meta.writeLong(minValue);
-        meta.writeLong(gcd);
-        final BlockPackedWriter quotientWriter = new BlockPackedWriter(data, BLOCK_SIZE);
-        for (Number nv : values) {
-          long value = nv == null ? 0 : nv.longValue();
-          quotientWriter.add((value - minValue) / gcd);
-        }
-        quotientWriter.finish();
-        break;
-      case DELTA_COMPRESSED:
-        final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);
-        for (Number nv : values) {
-          writer.add(nv == null ? 0 : nv.longValue());
-        }
-        writer.finish();
-        break;
-      case TABLE_COMPRESSED:
-        final Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);
-        final HashMap<Long,Integer> encode = new HashMap<>();
-        meta.writeVInt(decode.length);
-        for (int i = 0; i < decode.length; i++) {
-          meta.writeLong(decode[i]);
-          encode.put(decode[i], i);
-        }
-        final int bitsRequired = PackedInts.bitsRequired(uniqueValues.size() - 1);
-        final PackedInts.Writer ordsWriter = PackedInts.getWriterNoHeader(data, PackedInts.Format.PACKED, (int) count, bitsRequired, PackedInts.DEFAULT_BUFFER_SIZE);
-        for (Number nv : values) {
-          ordsWriter.add(encode.get(nv == null ? 0 : nv.longValue()));
-        }
-        ordsWriter.finish();
-        break;
-      default:
-        throw new AssertionError();
-    }
-  }
-  
-  // TODO: in some cases representing missing with minValue-1 wouldn't take up additional space and so on,
-  // but this is very simple, and algorithms only check this for values of 0 anyway (doesnt slow down normal decode)
-  void writeMissingBitset(Iterable<?> values) throws IOException {
-    byte bits = 0;
-    int count = 0;
-    for (Object v : values) {
-      if (count == 8) {
-        data.writeByte(bits);
-        count = 0;
-        bits = 0;
-      }
-      if (v != null) {
-        bits |= 1 << (count & 7);
-      }
-      count++;
-    }
-    if (count > 0) {
-      data.writeByte(bits);
-    }
-  }
-
-  @Override
-  public void addBinaryField(FieldInfo field, Iterable<BytesRef> values) throws IOException {
-    checkCanWrite(field);
-    // write the byte[] data
-    meta.writeVInt(field.number);
-    meta.writeByte(Lucene45DocValuesFormat.BINARY);
-    int minLength = Integer.MAX_VALUE;
-    int maxLength = Integer.MIN_VALUE;
-    final long startFP = data.getFilePointer();
-    long count = 0;
-    boolean missing = false;
-    for(BytesRef v : values) {
-      final int length;
-      if (v == null) {
-        length = 0;
-        missing = true;
-      } else {
-        length = v.length;
-      }
-      minLength = Math.min(minLength, length);
-      maxLength = Math.max(maxLength, length);
-      if (v != null) {
-        data.writeBytes(v.bytes, v.offset, v.length);
-      }
-      count++;
-    }
-    meta.writeVInt(minLength == maxLength ? BINARY_FIXED_UNCOMPRESSED : BINARY_VARIABLE_UNCOMPRESSED);
-    if (missing) {
-      meta.writeLong(data.getFilePointer());
-      writeMissingBitset(values);
-    } else {
-      meta.writeLong(-1L);
-    }
-    meta.writeVInt(minLength);
-    meta.writeVInt(maxLength);
-    meta.writeVLong(count);
-    meta.writeLong(startFP);
-    
-    // if minLength == maxLength, its a fixed-length byte[], we are done (the addresses are implicit)
-    // otherwise, we need to record the length fields...
-    if (minLength != maxLength) {
-      meta.writeLong(data.getFilePointer());
-      meta.writeVInt(PackedInts.VERSION_CURRENT);
-      meta.writeVInt(BLOCK_SIZE);
-
-      final MonotonicBlockPackedWriter writer = new MonotonicBlockPackedWriter(data, BLOCK_SIZE);
-      long addr = 0;
-      for (BytesRef v : values) {
-        if (v != null) {
-          addr += v.length;
-        }
-        writer.add(addr);
-      }
-      writer.finish();
-    }
-  }
-  
-  /** expert: writes a value dictionary for a sorted/sortedset field */
-  protected void addTermsDict(FieldInfo field, final Iterable<BytesRef> values) throws IOException {
-    // first check if its a "fixed-length" terms dict
-    int minLength = Integer.MAX_VALUE;
-    int maxLength = Integer.MIN_VALUE;
-    for (BytesRef v : values) {
-      minLength = Math.min(minLength, v.length);
-      maxLength = Math.max(maxLength, v.length);
-    }
-    if (minLength == maxLength) {
-      // no index needed: direct addressing by mult
-      addBinaryField(field, values);
-    } else {
-      // header
-      meta.writeVInt(field.number);
-      meta.writeByte(Lucene45DocValuesFormat.BINARY);
-      meta.writeVInt(BINARY_PREFIX_COMPRESSED);
-      meta.writeLong(-1L);
-      // now write the bytes: sharing prefixes within a block
-      final long startFP = data.getFilePointer();
-      // currently, we have to store the delta from expected for every 1/nth term
-      // we could avoid this, but its not much and less overall RAM than the previous approach!
-      RAMOutputStream addressBuffer = new RAMOutputStream();
-      MonotonicBlockPackedWriter termAddresses = new MonotonicBlockPackedWriter(addressBuffer, BLOCK_SIZE);
-      BytesRefBuilder lastTerm = new BytesRefBuilder();
-      long count = 0;
-      for (BytesRef v : values) {
-        if (count % ADDRESS_INTERVAL == 0) {
-          termAddresses.add(data.getFilePointer() - startFP);
-          // force the first term in a block to be abs-encoded
-          lastTerm.clear();
-        }
-        
-        // prefix-code
-        int sharedPrefix = StringHelper.bytesDifference(lastTerm.get(), v);
-        data.writeVInt(sharedPrefix);
-        data.writeVInt(v.length - sharedPrefix);
-        data.writeBytes(v.bytes, v.offset + sharedPrefix, v.length - sharedPrefix);
-        lastTerm.copyBytes(v);
-        count++;
-      }
-      final long indexStartFP = data.getFilePointer();
-      // write addresses of indexed terms
-      termAddresses.finish();
-      addressBuffer.writeTo(data);
-      addressBuffer = null;
-      termAddresses = null;
-      meta.writeVInt(minLength);
-      meta.writeVInt(maxLength);
-      meta.writeVLong(count);
-      meta.writeLong(startFP);
-      meta.writeVInt(ADDRESS_INTERVAL);
-      meta.writeLong(indexStartFP);
-      meta.writeVInt(PackedInts.VERSION_CURRENT);
-      meta.writeVInt(BLOCK_SIZE);
-    }
-  }
-
-  @Override
-  public void addSortedField(FieldInfo field, Iterable<BytesRef> values, Iterable<Number> docToOrd) throws IOException {
-    checkCanWrite(field);
-    meta.writeVInt(field.number);
-    meta.writeByte(Lucene45DocValuesFormat.SORTED);
-    addTermsDict(field, values);
-    addNumericField(field, docToOrd, false);
-  }
-  
-  @Override
-  public void addSortedNumericField(FieldInfo field, Iterable<Number> docToValueCount, Iterable<Number> values) throws IOException {
-    throw new UnsupportedOperationException("Lucene 4.5 does not support SORTED_NUMERIC");
-  }
-
-  @Override
-  public void addSortedSetField(FieldInfo field, Iterable<BytesRef> values, final Iterable<Number> docToOrdCount, final Iterable<Number> ords) throws IOException {
-    checkCanWrite(field);
-    meta.writeVInt(field.number);
-    meta.writeByte(Lucene45DocValuesFormat.SORTED_SET);
-
-    if (isSingleValued(docToOrdCount)) {
-      meta.writeVInt(SORTED_SET_SINGLE_VALUED_SORTED);
-      // The field is single-valued, we can encode it as SORTED
-      addSortedField(field, values, singletonView(docToOrdCount, ords, -1L));
-      return;
-    }
-
-    meta.writeVInt(SORTED_SET_WITH_ADDRESSES);
-
-    // write the ord -> byte[] as a binary field
-    addTermsDict(field, values);
-
-    // write the stream of ords as a numeric field
-    // NOTE: we could return an iterator that delta-encodes these within a doc
-    addNumericField(field, ords, false);
-
-    // write the doc -> ord count as a absolute index to the stream
-    meta.writeVInt(field.number);
-    meta.writeByte(Lucene45DocValuesFormat.NUMERIC);
-    meta.writeVInt(DELTA_COMPRESSED);
-    meta.writeLong(-1L);
-    meta.writeVInt(PackedInts.VERSION_CURRENT);
-    meta.writeLong(data.getFilePointer());
-    meta.writeVLong(maxDoc);
-    meta.writeVInt(BLOCK_SIZE);
-
-    final MonotonicBlockPackedWriter writer = new MonotonicBlockPackedWriter(data, BLOCK_SIZE);
-    long addr = 0;
-    for (Number v : docToOrdCount) {
-      addr += v.longValue();
-      writer.add(addr);
-    }
-    writer.finish();
-  }
-
-  @Override
-  public void close() throws IOException {
-    boolean success = false;
-    try {
-      if (meta != null) {
-        meta.writeVInt(-1); // write EOF marker
-        CodecUtil.writeFooter(meta); // write checksum
-      }
-      if (data != null) {
-        CodecUtil.writeFooter(data); // write checksum
-      }
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(data, meta);
-      } else {
-        IOUtils.closeWhileHandlingException(data, meta);
-      }
-      meta = data = null;
-    }
-  }
-  
-  void checkCanWrite(FieldInfo field) {
-    if ((field.getDocValuesType() == DocValuesType.NUMERIC || 
-        field.getDocValuesType() == DocValuesType.BINARY) && 
-        field.getDocValuesGen() != -1) {
-      // ok
-    } else {
-      throw new UnsupportedOperationException("this codec can only be used for reading");
-    }
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesFormat.java
deleted file mode 100644
index e717d44..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesFormat.java
+++ /dev/null
@@ -1,63 +0,0 @@
-package org.apache.lucene.codecs.lucene45;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.codecs.DocValuesProducer;
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-
-/**
- * Lucene 4.5 DocValues format.
- * @deprecated Only for reading old 4.3-4.5 segments
- */
-@Deprecated
-public class Lucene45DocValuesFormat extends DocValuesFormat {
-
-  /** Sole Constructor */
-  public Lucene45DocValuesFormat() {
-    super("Lucene45");
-  }
-
-  @Override
-  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    // really we should be read-only, but to allow posting of dv updates against old segments...
-    return new Lucene45DocValuesConsumer(state, DATA_CODEC, DATA_EXTENSION, META_CODEC, META_EXTENSION);
-  }
-
-  @Override
-  public final DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
-    return new Lucene45DocValuesProducer(state, DATA_CODEC, DATA_EXTENSION, META_CODEC, META_EXTENSION);
-  }
-  
-  static final String DATA_CODEC = "Lucene45DocValuesData";
-  static final String DATA_EXTENSION = "dvd";
-  static final String META_CODEC = "Lucene45ValuesMetadata";
-  static final String META_EXTENSION = "dvm";
-  static final int VERSION_START = 0;
-  static final int VERSION_SORTED_SET_SINGLE_VALUE_OPTIMIZED = 1;
-  static final int VERSION_CHECKSUM = 2;
-  static final int VERSION_CURRENT = VERSION_CHECKSUM;
-  static final byte NUMERIC = 0;
-  static final byte BINARY = 1;
-  static final byte SORTED = 2;
-  static final byte SORTED_SET = 3;
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesProducer.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesProducer.java
deleted file mode 100644
index 44e731f..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesProducer.java
+++ /dev/null
@@ -1,961 +0,0 @@
-package org.apache.lucene.codecs.lucene45;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import static org.apache.lucene.codecs.lucene45.Lucene45DocValuesConsumer.BINARY_FIXED_UNCOMPRESSED;
-import static org.apache.lucene.codecs.lucene45.Lucene45DocValuesConsumer.BINARY_PREFIX_COMPRESSED;
-import static org.apache.lucene.codecs.lucene45.Lucene45DocValuesConsumer.BINARY_VARIABLE_UNCOMPRESSED;
-import static org.apache.lucene.codecs.lucene45.Lucene45DocValuesConsumer.DELTA_COMPRESSED;
-import static org.apache.lucene.codecs.lucene45.Lucene45DocValuesConsumer.GCD_COMPRESSED;
-import static org.apache.lucene.codecs.lucene45.Lucene45DocValuesConsumer.SORTED_SET_SINGLE_VALUED_SORTED;
-import static org.apache.lucene.codecs.lucene45.Lucene45DocValuesConsumer.SORTED_SET_WITH_ADDRESSES;
-import static org.apache.lucene.codecs.lucene45.Lucene45DocValuesConsumer.TABLE_COMPRESSED;
-import static org.apache.lucene.codecs.lucene45.Lucene45DocValuesFormat.VERSION_SORTED_SET_SINGLE_VALUE_OPTIMIZED;
-
-import java.io.Closeable; // javadocs
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.concurrent.atomic.AtomicLong;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.DocValuesProducer;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.NumericDocValues;
-import org.apache.lucene.index.RandomAccessOrds;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SortedDocValues;
-import org.apache.lucene.index.SortedNumericDocValues;
-import org.apache.lucene.index.SortedSetDocValues;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.TermsEnum.SeekStatus;
-import org.apache.lucene.store.ChecksumIndexInput;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.Accountable;
-import org.apache.lucene.util.Accountables;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.LongValues;
-import org.apache.lucene.util.RamUsageEstimator;
-import org.apache.lucene.util.Version;
-import org.apache.lucene.util.packed.BlockPackedReader;
-import org.apache.lucene.util.packed.MonotonicBlockPackedReader;
-import org.apache.lucene.util.packed.PackedInts;
-
-/** 
- * reader for 4.5 docvalues format
- * @deprecated only for reading old 4.x segments
- */
-@Deprecated
-class Lucene45DocValuesProducer extends DocValuesProducer implements Closeable {
-  private final Map<Integer,NumericEntry> numerics;
-  private final Map<Integer,BinaryEntry> binaries;
-  private final Map<Integer,SortedSetEntry> sortedSets;
-  private final Map<Integer,NumericEntry> ords;
-  private final Map<Integer,NumericEntry> ordIndexes;
-  private final AtomicLong ramBytesUsed;
-  private final IndexInput data;
-  private final int maxDoc;
-  private final int version;
-  private final int numFields;
-  
-  // We need this for pre-4.9 indexes which recorded multiple fields' DocValues
-  // updates under the same generation, and therefore the passed FieldInfos may
-  // not include all the fields that are encoded in this generation. In that
-  // case, we are more lenient about the fields we read and the passed-in
-  // FieldInfos.
-  @Deprecated
-  private final boolean lenientFieldInfoCheck;
-
-  // memory-resident structures
-  private final Map<Integer,MonotonicBlockPackedReader> addressInstances = new HashMap<>();
-  private final Map<Integer,MonotonicBlockPackedReader> ordIndexInstances = new HashMap<>();
-  
-  private final boolean merging;
-  
-  // clone for merge: when merging we don't do any instances.put()s
-  Lucene45DocValuesProducer(Lucene45DocValuesProducer original) throws IOException {
-    assert Thread.holdsLock(original);
-    numerics = original.numerics;
-    binaries = original.binaries;
-    sortedSets = original.sortedSets;
-    ords = original.ords;
-    ordIndexes = original.ordIndexes;
-    ramBytesUsed = new AtomicLong(original.ramBytesUsed.get());
-    data = original.data.clone();
-    maxDoc = original.maxDoc;
-    version = original.version;
-    numFields = original.numFields;
-    lenientFieldInfoCheck = original.lenientFieldInfoCheck;
-    addressInstances.putAll(original.addressInstances);
-    ordIndexInstances.putAll(original.ordIndexInstances);
-    merging = true;
-  }
-  
-  /** expert: instantiates a new reader */
-  @SuppressWarnings("deprecation")
-  protected Lucene45DocValuesProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
-    Version ver = state.segmentInfo.getVersion();
-    lenientFieldInfoCheck = Version.LUCENE_4_9_0.onOrAfter(ver);
-    String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
-    // read in the entries from the metadata file.
-    ChecksumIndexInput in = state.directory.openChecksumInput(metaName, state.context);
-    this.maxDoc = state.segmentInfo.getDocCount();
-    merging = false;
-    boolean success = false;
-    try {
-      version = CodecUtil.checkHeader(in, metaCodec, 
-                                      Lucene45DocValuesFormat.VERSION_START,
-                                      Lucene45DocValuesFormat.VERSION_CURRENT);
-      numerics = new HashMap<>();
-      ords = new HashMap<>();
-      ordIndexes = new HashMap<>();
-      binaries = new HashMap<>();
-      sortedSets = new HashMap<>();
-      numFields = readFields(in, state.fieldInfos);
-
-      if (version >= Lucene45DocValuesFormat.VERSION_CHECKSUM) {
-        CodecUtil.checkFooter(in);
-      } else {
-        CodecUtil.checkEOF(in);
-      }
-
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(in);
-      } else {
-        IOUtils.closeWhileHandlingException(in);
-      }
-    }
-
-    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
-    this.data = state.directory.openInput(dataName, state.context);
-    success = false;
-    try {
-      final int version2 = CodecUtil.checkHeader(data, dataCodec, 
-                                                 Lucene45DocValuesFormat.VERSION_START,
-                                                 Lucene45DocValuesFormat.VERSION_CURRENT);
-      if (version != version2) {
-        throw new CorruptIndexException("Format versions mismatch: meta=" + version + ", data=" + version2, data);
-      }
-      
-      if (version >= Lucene45DocValuesFormat.VERSION_CHECKSUM) {
-        // NOTE: data file is too costly to verify checksum against all the bytes on open,
-        // but for now we at least verify proper structure of the checksum footer: which looks
-        // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
-        // such as file truncation.
-        CodecUtil.retrieveChecksum(data);
-      }
-
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(this.data);
-      }
-    }
-    
-    ramBytesUsed = new AtomicLong(RamUsageEstimator.shallowSizeOfInstance(getClass()));
-  }
-
-  private void readSortedField(int fieldNumber, IndexInput meta, FieldInfos infos) throws IOException {
-    // sorted = binary + numeric
-    if (meta.readVInt() != fieldNumber) {
-      throw new CorruptIndexException("sorted entry for field: " + fieldNumber + " is corrupt", meta);
-    }
-    if (meta.readByte() != Lucene45DocValuesFormat.BINARY) {
-      throw new CorruptIndexException("sorted entry for field: " + fieldNumber + " is corrupt", meta);
-    }
-    BinaryEntry b = readBinaryEntry(meta);
-    binaries.put(fieldNumber, b);
-    
-    if (meta.readVInt() != fieldNumber) {
-      throw new CorruptIndexException("sorted entry for field: " + fieldNumber + " is corrupt", meta);
-    }
-    if (meta.readByte() != Lucene45DocValuesFormat.NUMERIC) {
-      throw new CorruptIndexException("sorted entry for field: " + fieldNumber + " is corrupt", meta);
-    }
-    NumericEntry n = readNumericEntry(meta);
-    ords.put(fieldNumber, n);
-  }
-
-  private void readSortedSetFieldWithAddresses(int fieldNumber, IndexInput meta, FieldInfos infos) throws IOException {
-    // sortedset = binary + numeric (addresses) + ordIndex
-    if (meta.readVInt() != fieldNumber) {
-      throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt", meta);
-    }
-    if (meta.readByte() != Lucene45DocValuesFormat.BINARY) {
-      throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt", meta);
-    }
-    BinaryEntry b = readBinaryEntry(meta);
-    binaries.put(fieldNumber, b);
-
-    if (meta.readVInt() != fieldNumber) {
-      throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt", meta);
-    }
-    if (meta.readByte() != Lucene45DocValuesFormat.NUMERIC) {
-      throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt", meta);
-    }
-    NumericEntry n1 = readNumericEntry(meta);
-    ords.put(fieldNumber, n1);
-
-    if (meta.readVInt() != fieldNumber) {
-      throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt", meta);
-    }
-    if (meta.readByte() != Lucene45DocValuesFormat.NUMERIC) {
-      throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt", meta);
-    }
-    NumericEntry n2 = readNumericEntry(meta);
-    ordIndexes.put(fieldNumber, n2);
-  }
-
-  private int readFields(IndexInput meta, FieldInfos infos) throws IOException {
-    int numFields = 0;
-    int fieldNumber = meta.readVInt();
-    while (fieldNumber != -1) {
-      numFields++;
-      if ((lenientFieldInfoCheck && fieldNumber < 0) || (!lenientFieldInfoCheck && infos.fieldInfo(fieldNumber) == null)) {
-        // trickier to validate more: because we re-use for norms, because we use multiple entries
-        // for "composite" types like sortedset, etc.
-        throw new CorruptIndexException("Invalid field number: " + fieldNumber, meta);
-      }
-      byte type = meta.readByte();
-      if (type == Lucene45DocValuesFormat.NUMERIC) {
-        numerics.put(fieldNumber, readNumericEntry(meta));
-      } else if (type == Lucene45DocValuesFormat.BINARY) {
-        BinaryEntry b = readBinaryEntry(meta);
-        binaries.put(fieldNumber, b);
-      } else if (type == Lucene45DocValuesFormat.SORTED) {
-        readSortedField(fieldNumber, meta, infos);
-      } else if (type == Lucene45DocValuesFormat.SORTED_SET) {
-        SortedSetEntry ss = readSortedSetEntry(meta);
-        sortedSets.put(fieldNumber, ss);
-        if (ss.format == SORTED_SET_WITH_ADDRESSES) {
-          readSortedSetFieldWithAddresses(fieldNumber, meta, infos);
-        } else if (ss.format == SORTED_SET_SINGLE_VALUED_SORTED) {
-          if (meta.readVInt() != fieldNumber) {
-            throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt", meta);
-          }
-          if (meta.readByte() != Lucene45DocValuesFormat.SORTED) {
-            throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt", meta);
-          }
-          readSortedField(fieldNumber, meta, infos);
-        } else {
-          throw new AssertionError();
-        }
-      } else {
-        throw new CorruptIndexException("invalid type: " + type, meta);
-      }
-      fieldNumber = meta.readVInt();
-    }
-    return numFields;
-  }
-  
-  static NumericEntry readNumericEntry(IndexInput meta) throws IOException {
-    NumericEntry entry = new NumericEntry();
-    entry.format = meta.readVInt();
-    entry.missingOffset = meta.readLong();
-    entry.packedIntsVersion = meta.readVInt();
-    entry.offset = meta.readLong();
-    entry.count = meta.readVLong();
-    entry.blockSize = meta.readVInt();
-    switch(entry.format) {
-      case GCD_COMPRESSED:
-        entry.minValue = meta.readLong();
-        entry.gcd = meta.readLong();
-        break;
-      case TABLE_COMPRESSED:
-        if (entry.count > Integer.MAX_VALUE) {
-          throw new CorruptIndexException("Cannot use TABLE_COMPRESSED with more than MAX_VALUE values, got=" + entry.count, meta);
-        }
-        final int uniqueValues = meta.readVInt();
-        if (uniqueValues > 256) {
-          throw new CorruptIndexException("TABLE_COMPRESSED cannot have more than 256 distinct values, got=" + uniqueValues, meta);
-        }
-        entry.table = new long[uniqueValues];
-        for (int i = 0; i < uniqueValues; ++i) {
-          entry.table[i] = meta.readLong();
-        }
-        break;
-      case DELTA_COMPRESSED:
-        break;
-      default:
-        throw new CorruptIndexException("Unknown format: " + entry.format, meta);
-    }
-    return entry;
-  }
-  
-  static BinaryEntry readBinaryEntry(IndexInput meta) throws IOException {
-    BinaryEntry entry = new BinaryEntry();
-    entry.format = meta.readVInt();
-    entry.missingOffset = meta.readLong();
-    entry.minLength = meta.readVInt();
-    entry.maxLength = meta.readVInt();
-    entry.count = meta.readVLong();
-    entry.offset = meta.readLong();
-    switch(entry.format) {
-      case BINARY_FIXED_UNCOMPRESSED:
-        break;
-      case BINARY_PREFIX_COMPRESSED:
-        entry.addressInterval = meta.readVInt();
-        entry.addressesOffset = meta.readLong();
-        entry.packedIntsVersion = meta.readVInt();
-        entry.blockSize = meta.readVInt();
-        break;
-      case BINARY_VARIABLE_UNCOMPRESSED:
-        entry.addressesOffset = meta.readLong();
-        entry.packedIntsVersion = meta.readVInt();
-        entry.blockSize = meta.readVInt();
-        break;
-      default:
-        throw new CorruptIndexException("Unknown format: " + entry.format, meta);
-    }
-    return entry;
-  }
-
-  SortedSetEntry readSortedSetEntry(IndexInput meta) throws IOException {
-    SortedSetEntry entry = new SortedSetEntry();
-    if (version >= VERSION_SORTED_SET_SINGLE_VALUE_OPTIMIZED) {
-      entry.format = meta.readVInt();
-    } else {
-      entry.format = SORTED_SET_WITH_ADDRESSES;
-    }
-    if (entry.format != SORTED_SET_SINGLE_VALUED_SORTED && entry.format != SORTED_SET_WITH_ADDRESSES) {
-      throw new CorruptIndexException("Unknown format: " + entry.format, meta);
-    }
-    return entry;
-  }
-
-  @Override
-  public NumericDocValues getNumeric(FieldInfo field) throws IOException {
-    NumericEntry entry = numerics.get(field.number);
-    return getNumeric(entry);
-  }
-  
-  @Override
-  public long ramBytesUsed() {
-    return ramBytesUsed.get();
-  }
-  
-  @Override
-  public synchronized Iterable<? extends Accountable> getChildResources() {
-    List<Accountable> resources = new ArrayList<>();
-    resources.addAll(Accountables.namedAccountables("addresses field number", addressInstances));
-    resources.addAll(Accountables.namedAccountables("ord index field number", ordIndexInstances));
-    return Collections.unmodifiableList(resources);
-  }
-  
-  @Override
-  public void checkIntegrity() throws IOException {
-    if (version >= Lucene45DocValuesFormat.VERSION_CHECKSUM) {
-      CodecUtil.checksumEntireFile(data);
-    }
-  }
-  
-  @Override
-  public String toString() {
-    return getClass().getSimpleName() + "(fields=" + numFields + ")";
-  }
-
-  LongValues getNumeric(NumericEntry entry) throws IOException {
-    final IndexInput data = this.data.clone();
-    data.seek(entry.offset);
-
-    switch (entry.format) {
-      case DELTA_COMPRESSED:
-        final BlockPackedReader reader = new BlockPackedReader(data, entry.packedIntsVersion, entry.blockSize, entry.count, true);
-        return reader;
-      case GCD_COMPRESSED:
-        final long min = entry.minValue;
-        final long mult = entry.gcd;
-        final BlockPackedReader quotientReader = new BlockPackedReader(data, entry.packedIntsVersion, entry.blockSize, entry.count, true);
-        return new LongValues() {
-          @Override
-          public long get(long id) {
-            return min + mult * quotientReader.get(id);
-          }
-        };
-      case TABLE_COMPRESSED:
-        final long table[] = entry.table;
-        final int bitsRequired = PackedInts.bitsRequired(table.length - 1);
-        final PackedInts.Reader ords = PackedInts.getDirectReaderNoHeader(data, PackedInts.Format.PACKED, entry.packedIntsVersion, (int) entry.count, bitsRequired);
-        return new LongValues() {
-          @Override
-          public long get(long id) {
-            return table[(int) ords.get((int) id)];
-          }
-        };
-      default:
-        throw new AssertionError();
-    }
-  }
-
-  @Override
-  public BinaryDocValues getBinary(FieldInfo field) throws IOException {
-    BinaryEntry bytes = binaries.get(field.number);
-    switch(bytes.format) {
-      case BINARY_FIXED_UNCOMPRESSED:
-        return getFixedBinary(field, bytes);
-      case BINARY_VARIABLE_UNCOMPRESSED:
-        return getVariableBinary(field, bytes);
-      case BINARY_PREFIX_COMPRESSED:
-        return getCompressedBinary(field, bytes);
-      default:
-        throw new AssertionError();
-    }
-  }
-  
-  private BinaryDocValues getFixedBinary(FieldInfo field, final BinaryEntry bytes) {
-    final IndexInput data = this.data.clone();
-
-    return new LongBinaryDocValues() {
-      final BytesRef term;
-      {
-        term = new BytesRef(bytes.maxLength);
-        term.offset = 0;
-        term.length = bytes.maxLength;
-      }
-
-      @Override
-      public BytesRef get(long id) {
-        long address = bytes.offset + id * bytes.maxLength;
-        try {
-          data.seek(address);
-          data.readBytes(term.bytes, 0, term.length);
-          return term;
-        } catch (IOException e) {
-          throw new RuntimeException(e);
-        }
-      }
-    };
-  }
-  
-  /** returns an address instance for variable-length binary values.
-   *  @lucene.internal */
-  protected synchronized MonotonicBlockPackedReader getAddressInstance(IndexInput data, FieldInfo field, BinaryEntry bytes) throws IOException {
-    final MonotonicBlockPackedReader addresses;
-    MonotonicBlockPackedReader addrInstance = addressInstances.get(field.number);
-    if (addrInstance == null) {
-      data.seek(bytes.addressesOffset);
-      addrInstance = MonotonicBlockPackedReader.of(data, bytes.packedIntsVersion, bytes.blockSize, bytes.count, false);
-      if (!merging) {
-        addressInstances.put(field.number, addrInstance);
-        ramBytesUsed.addAndGet(addrInstance.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
-      }
-    }
-    addresses = addrInstance;
-    return addresses;
-  }
-  
-  private BinaryDocValues getVariableBinary(FieldInfo field, final BinaryEntry bytes) throws IOException {
-    final IndexInput data = this.data.clone();
-    
-    final MonotonicBlockPackedReader addresses = getAddressInstance(data, field, bytes);
-
-    return new LongBinaryDocValues() {
-      final BytesRef term = new BytesRef(Math.max(0, bytes.maxLength));
-
-      @Override
-      public BytesRef get(long id) {
-        long startAddress = bytes.offset + (id == 0 ? 0 : addresses.get(id-1));
-        long endAddress = bytes.offset + addresses.get(id);
-        int length = (int) (endAddress - startAddress);
-        try {
-          data.seek(startAddress);
-          data.readBytes(term.bytes, 0, length);
-          term.length = length;
-          return term;
-        } catch (IOException e) {
-          throw new RuntimeException(e);
-        }
-      }
-    };
-  }
-  
-  /** returns an address instance for prefix-compressed binary values. 
-   * @lucene.internal */
-  protected synchronized MonotonicBlockPackedReader getIntervalInstance(IndexInput data, FieldInfo field, BinaryEntry bytes) throws IOException {
-    final MonotonicBlockPackedReader addresses;
-    final long interval = bytes.addressInterval;
-    MonotonicBlockPackedReader addrInstance = addressInstances.get(field.number);
-    if (addrInstance == null) {
-      data.seek(bytes.addressesOffset);
-      final long size;
-      if (bytes.count % interval == 0) {
-        size = bytes.count / interval;
-      } else {
-        size = 1L + bytes.count / interval;
-      }
-      addrInstance = MonotonicBlockPackedReader.of(data, bytes.packedIntsVersion, bytes.blockSize, size, false);
-      if (!merging) {
-        addressInstances.put(field.number, addrInstance);
-        ramBytesUsed.addAndGet(addrInstance.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
-      }
-    }
-    addresses = addrInstance;
-    return addresses;
-  }
-
-
-  private BinaryDocValues getCompressedBinary(FieldInfo field, final BinaryEntry bytes) throws IOException {
-    final IndexInput data = this.data.clone();
-
-    final MonotonicBlockPackedReader addresses = getIntervalInstance(data, field, bytes);
-    
-    return new CompressedBinaryDocValues(bytes, addresses, data);
-  }
-
-  @Override
-  public SortedDocValues getSorted(FieldInfo field) throws IOException {
-    final int valueCount = (int) binaries.get(field.number).count;
-    final BinaryDocValues binary = getBinary(field);
-    NumericEntry entry = ords.get(field.number);
-    IndexInput data = this.data.clone();
-    data.seek(entry.offset);
-    final BlockPackedReader ordinals = new BlockPackedReader(data, entry.packedIntsVersion, entry.blockSize, entry.count, true);
-    
-    return new SortedDocValues() {
-
-      @Override
-      public int getOrd(int docID) {
-        return (int) ordinals.get(docID);
-      }
-
-      @Override
-      public BytesRef lookupOrd(int ord) {
-        return binary.get(ord);
-      }
-
-      @Override
-      public int getValueCount() {
-        return valueCount;
-      }
-
-      @Override
-      public int lookupTerm(BytesRef key) {
-        if (binary instanceof CompressedBinaryDocValues) {
-          return (int) ((CompressedBinaryDocValues)binary).lookupTerm(key);
-        } else {
-        return super.lookupTerm(key);
-        }
-      }
-
-      @Override
-      public TermsEnum termsEnum() {
-        if (binary instanceof CompressedBinaryDocValues) {
-          return ((CompressedBinaryDocValues)binary).getTermsEnum();
-        } else {
-          return super.termsEnum();
-        }
-      }
-    };
-  }
-  
-  /** returns an address instance for sortedset ordinal lists
-   * @lucene.internal */
-  protected synchronized MonotonicBlockPackedReader getOrdIndexInstance(IndexInput data, FieldInfo field, NumericEntry entry) throws IOException {
-    final MonotonicBlockPackedReader ordIndex;
-    MonotonicBlockPackedReader ordIndexInstance = ordIndexInstances.get(field.number);
-    if (ordIndexInstance == null) {
-      data.seek(entry.offset);
-      ordIndexInstance = MonotonicBlockPackedReader.of(data, entry.packedIntsVersion, entry.blockSize, entry.count, false);
-      if (!merging) {
-        ordIndexInstances.put(field.number, ordIndexInstance);
-        ramBytesUsed.addAndGet(ordIndexInstance.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
-      }
-    }
-    ordIndex = ordIndexInstance;
-    return ordIndex;
-  }
-  
-  @Override
-  public SortedNumericDocValues getSortedNumeric(FieldInfo field) throws IOException {
-    throw new IllegalStateException("Lucene 4.5 does not support SortedNumeric: how did you pull this off?");
-  }
-
-  @Override
-  public SortedSetDocValues getSortedSet(FieldInfo field) throws IOException {
-    SortedSetEntry ss = sortedSets.get(field.number);
-    if (ss.format == SORTED_SET_SINGLE_VALUED_SORTED) {
-      final SortedDocValues values = getSorted(field);
-      return DocValues.singleton(values);
-    } else if (ss.format != SORTED_SET_WITH_ADDRESSES) {
-      throw new AssertionError();
-    }
-
-    final IndexInput data = this.data.clone();
-    final long valueCount = binaries.get(field.number).count;
-    // we keep the byte[]s and list of ords on disk, these could be large
-    final LongBinaryDocValues binary = (LongBinaryDocValues) getBinary(field);
-    final LongValues ordinals = getNumeric(ords.get(field.number));
-    // but the addresses to the ord stream are in RAM
-    final MonotonicBlockPackedReader ordIndex = getOrdIndexInstance(data, field, ordIndexes.get(field.number));
-    
-    return new RandomAccessOrds() {
-      long startOffset;
-      long offset;
-      long endOffset;
-      
-      @Override
-      public long nextOrd() {
-        if (offset == endOffset) {
-          return NO_MORE_ORDS;
-        } else {
-          long ord = ordinals.get(offset);
-          offset++;
-          return ord;
-        }
-      }
-
-      @Override
-      public void setDocument(int docID) {
-        startOffset = offset = (docID == 0 ? 0 : ordIndex.get(docID-1));
-        endOffset = ordIndex.get(docID);
-      }
-
-      @Override
-      public BytesRef lookupOrd(long ord) {
-        return binary.get(ord);
-      }
-
-      @Override
-      public long getValueCount() {
-        return valueCount;
-      }
-      
-      @Override
-      public long lookupTerm(BytesRef key) {
-        if (binary instanceof CompressedBinaryDocValues) {
-          return ((CompressedBinaryDocValues)binary).lookupTerm(key);
-        } else {
-          return super.lookupTerm(key);
-        }
-      }
-
-      @Override
-      public TermsEnum termsEnum() {
-        if (binary instanceof CompressedBinaryDocValues) {
-          return ((CompressedBinaryDocValues)binary).getTermsEnum();
-        } else {
-          return super.termsEnum();
-        }
-      }
-
-      @Override
-      public long ordAt(int index) {
-        return ordinals.get(startOffset + index);
-      }
-
-      @Override
-      public int cardinality() {
-        return (int) (endOffset - startOffset);
-      }
-    };
-  }
-  
-  private Bits getMissingBits(final long offset) throws IOException {
-    if (offset == -1) {
-      return new Bits.MatchAllBits(maxDoc);
-    } else {
-      final IndexInput in = data.clone();
-      return new Bits() {
-
-        @Override
-        public boolean get(int index) {
-          try {
-            in.seek(offset + (index >> 3));
-            return (in.readByte() & (1 << (index & 7))) != 0;
-          } catch (IOException e) {
-            throw new RuntimeException(e);
-          }
-        }
-
-        @Override
-        public int length() {
-          return maxDoc;
-        }
-      };
-    }
-  }
-
-  @Override
-  public Bits getDocsWithField(FieldInfo field) throws IOException {
-    switch(field.getDocValuesType()) {
-      case SORTED_SET:
-        return DocValues.docsWithValue(getSortedSet(field), maxDoc);
-      case SORTED:
-        return DocValues.docsWithValue(getSorted(field), maxDoc);
-      case BINARY:
-        BinaryEntry be = binaries.get(field.number);
-        return getMissingBits(be.missingOffset);
-      case NUMERIC:
-        NumericEntry ne = numerics.get(field.number);
-        return getMissingBits(ne.missingOffset);
-      default:
-        throw new AssertionError();
-    }
-  }
-
-  @Override
-  public synchronized DocValuesProducer getMergeInstance() throws IOException {
-    return new Lucene45DocValuesProducer(this);
-  }
-
-  @Override
-  public void close() throws IOException {
-    data.close();
-  }
-  
-  /** metadata entry for a numeric docvalues field */
-  protected static class NumericEntry {
-    private NumericEntry() {}
-    /** offset to the bitset representing docsWithField, or -1 if no documents have missing values */
-    long missingOffset;
-    /** offset to the actual numeric values */
-    public long offset;
-
-    int format;
-    /** packed ints version used to encode these numerics */
-    public int packedIntsVersion;
-    /** count of values written */
-    public long count;
-    /** packed ints blocksize */
-    public int blockSize;
-    
-    long minValue;
-    long gcd;
-    long table[];
-  }
-  
-  /** metadata entry for a binary docvalues field */
-  protected static class BinaryEntry {
-    private BinaryEntry() {}
-    /** offset to the bitset representing docsWithField, or -1 if no documents have missing values */
-    long missingOffset;
-    /** offset to the actual binary values */
-    long offset;
-
-    int format;
-    /** count of values written */
-    public long count;
-    int minLength;
-    int maxLength;
-    /** offset to the addressing data that maps a value to its slice of the byte[] */
-    public long addressesOffset;
-    /** interval of shared prefix chunks (when using prefix-compressed binary) */
-    public long addressInterval;
-    /** packed ints version used to encode addressing information */
-    public int packedIntsVersion;
-    /** packed ints blocksize */
-    public int blockSize;
-  }
-
-  /** metadata entry for a sorted-set docvalues field */
-  protected static class SortedSetEntry {
-    private SortedSetEntry() {}
-    int format;
-  }
-
-  // internally we compose complex dv (sorted/sortedset) from other ones
-  static abstract class LongBinaryDocValues extends BinaryDocValues {
-    @Override
-    public final BytesRef get(int docID) {
-      return get((long) docID);
-    }
-    
-    abstract BytesRef get(long id);
-  }
-  
-  // in the compressed case, we add a few additional operations for
-  // more efficient reverse lookup and enumeration
-  static class CompressedBinaryDocValues extends LongBinaryDocValues {
-    final BinaryEntry bytes;
-    final long interval;
-    final long numValues;
-    final long numIndexValues;
-    final MonotonicBlockPackedReader addresses;
-    final IndexInput data;
-    final TermsEnum termsEnum;
-    
-    public CompressedBinaryDocValues(BinaryEntry bytes, MonotonicBlockPackedReader addresses, IndexInput data) throws IOException {
-      this.bytes = bytes;
-      this.interval = bytes.addressInterval;
-      this.addresses = addresses;
-      this.data = data;
-      this.numValues = bytes.count;
-      this.numIndexValues = addresses.size();
-      this.termsEnum = getTermsEnum(data);
-    }
-    
-    @Override
-    public BytesRef get(long id) {
-      try {
-        termsEnum.seekExact(id);
-        return termsEnum.term();
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      }
-    }
-    
-    long lookupTerm(BytesRef key) {
-      try {
-        SeekStatus status = termsEnum.seekCeil(key);
-        if (status == SeekStatus.END) {
-          return -numValues-1;
-        } else if (status == SeekStatus.FOUND) {
-          return termsEnum.ord();
-        } else {
-          return -termsEnum.ord()-1;
-        }
-      } catch (IOException bogus) {
-        throw new RuntimeException(bogus);
-      }
-    }
-    
-    TermsEnum getTermsEnum() {
-      try {
-        return getTermsEnum(data.clone());
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      }
-    }
-    
-    private TermsEnum getTermsEnum(final IndexInput input) throws IOException {
-      input.seek(bytes.offset);
-      
-      return new TermsEnum() {
-        private long currentOrd = -1;
-        // TODO: maxLength is negative when all terms are merged away...
-        private final BytesRef term = new BytesRef(bytes.maxLength < 0 ? 0 : bytes.maxLength);
-
-        @Override
-        public BytesRef next() throws IOException {
-          if (++currentOrd >= numValues) {
-            return null;
-          } else {
-            int start = input.readVInt();
-            int suffix = input.readVInt();
-            input.readBytes(term.bytes, start, suffix);
-            term.length = start + suffix;
-            return term;
-          }
-        }
-
-        @Override
-        public SeekStatus seekCeil(BytesRef text) throws IOException {
-          // binary-search just the index values to find the block,
-          // then scan within the block
-          long low = 0;
-          long high = numIndexValues-1;
-
-          while (low <= high) {
-            long mid = (low + high) >>> 1;
-            seekExact(mid * interval);
-            int cmp = term.compareTo(text);
-
-            if (cmp < 0) {
-              low = mid + 1;
-            } else if (cmp > 0) {
-              high = mid - 1;
-            } else {
-              // we got lucky, found an indexed term
-              return SeekStatus.FOUND;
-            }
-          }
-          
-          if (numIndexValues == 0) {
-            return SeekStatus.END;
-          }
-          
-          // block before insertion point
-          long block = low-1;
-          seekExact(block < 0 ? -1 : block * interval);
-          
-          while (next() != null) {
-            int cmp = term.compareTo(text);
-            if (cmp == 0) {
-              return SeekStatus.FOUND;
-            } else if (cmp > 0) {
-              return SeekStatus.NOT_FOUND;
-            }
-          }
-          
-          return SeekStatus.END;
-        }
-
-        @Override
-        public void seekExact(long ord) throws IOException {
-          long block = ord / interval;
-
-          if (ord >= currentOrd && block == currentOrd / interval) {
-            // seek within current block
-          } else {
-            // position before start of block
-            currentOrd = ord - ord % interval - 1;
-            input.seek(bytes.offset + addresses.get(block));
-          }
-          
-          while (currentOrd < ord) {
-            next();
-          }
-        }
-
-        @Override
-        public BytesRef term() throws IOException {
-          return term;
-        }
-
-        @Override
-        public long ord() throws IOException {
-          return currentOrd;
-        }
-
-        @Override
-        public int docFreq() throws IOException {
-          throw new UnsupportedOperationException();
-        }
-
-        @Override
-        public long totalTermFreq() throws IOException {
-          return -1;
-        }
-
-        @Override
-        public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
-          throw new UnsupportedOperationException();
-        }
-
-        @Override
-        public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
-          throw new UnsupportedOperationException();
-        }
-      };
-    }
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/package.html b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/package.html
deleted file mode 100644
index 62c1807..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Lucene 4.5 file format.
-</body>
-</html>
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene46/Lucene46Codec.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene46/Lucene46Codec.java
deleted file mode 100755
index 67c89cc..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene46/Lucene46Codec.java
+++ /dev/null
@@ -1,146 +0,0 @@
-package org.apache.lucene.codecs.lucene46;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.CompoundFormat;
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.LiveDocsFormat;
-import org.apache.lucene.codecs.NormsConsumer;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.SegmentInfoFormat;
-import org.apache.lucene.codecs.StoredFieldsFormat;
-import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40CompoundFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
-import org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat;
-import org.apache.lucene.codecs.lucene42.Lucene42NormsFormat;
-import org.apache.lucene.codecs.lucene42.Lucene42TermVectorsFormat;
-import org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat;
-import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
-import org.apache.lucene.index.SegmentWriteState;
-
-/**
- * Implements the Lucene 4.6 index format
- * @deprecated Only for reading old 4.6-4.8 segments
- */
-@Deprecated
-public class Lucene46Codec extends Codec {
-  private final StoredFieldsFormat fieldsFormat = new Lucene41StoredFieldsFormat();
-  private final TermVectorsFormat vectorsFormat = new Lucene42TermVectorsFormat();
-  private final FieldInfosFormat fieldInfosFormat = new Lucene46FieldInfosFormat();
-  private final SegmentInfoFormat segmentInfosFormat = new Lucene46SegmentInfoFormat();
-  private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat();
-  private final CompoundFormat compoundFormat = new Lucene40CompoundFormat();
-  
-  private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
-    @Override
-    public PostingsFormat getPostingsFormatForField(String field) {
-      return Lucene46Codec.this.getPostingsFormatForField(field);
-    }
-  };
-  
-  private final DocValuesFormat docValuesFormat = new PerFieldDocValuesFormat() {
-    @Override
-    public DocValuesFormat getDocValuesFormatForField(String field) {
-      return Lucene46Codec.this.getDocValuesFormatForField(field);
-    }
-  };
-
-  /** Sole constructor. */
-  public Lucene46Codec() {
-    super("Lucene46");
-  }
-  
-  @Override
-  public StoredFieldsFormat storedFieldsFormat() {
-    return fieldsFormat;
-  }
-  
-  @Override
-  public TermVectorsFormat termVectorsFormat() {
-    return vectorsFormat;
-  }
-
-  @Override
-  public final PostingsFormat postingsFormat() {
-    return postingsFormat;
-  }
-  
-  @Override
-  public final FieldInfosFormat fieldInfosFormat() {
-    return fieldInfosFormat;
-  }
-  
-  @Override
-  public SegmentInfoFormat segmentInfoFormat() {
-    return segmentInfosFormat;
-  }
-  
-  @Override
-  public final LiveDocsFormat liveDocsFormat() {
-    return liveDocsFormat;
-  }
-  
-  @Override
-  public CompoundFormat compoundFormat() {
-    return compoundFormat;
-  }
-
-  /** Returns the postings format that should be used for writing 
-   *  new segments of <code>field</code>.
-   *  
-   *  The default implementation always returns "Lucene41"
-   */
-  public PostingsFormat getPostingsFormatForField(String field) {
-    return defaultFormat;
-  }
-  
-  /** Returns the docvalues format that should be used for writing 
-   *  new segments of <code>field</code>.
-   *  
-   *  The default implementation always returns "Lucene45"
-   */
-  public DocValuesFormat getDocValuesFormatForField(String field) {
-    return defaultDVFormat;
-  }
-  
-  @Override
-  public final DocValuesFormat docValuesFormat() {
-    return docValuesFormat;
-  }
-
-  private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene41");
-  private final DocValuesFormat defaultDVFormat = DocValuesFormat.forName("Lucene45");
-
-  private final NormsFormat normsFormat = new Lucene42NormsFormat() {
-    @Override
-    public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
-      throw new UnsupportedOperationException("this codec can only be used for reading");
-    }
-  };
-
-  @Override
-  public NormsFormat normsFormat() {
-    return normsFormat;
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene46/Lucene46FieldInfosFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene46/Lucene46FieldInfosFormat.java
deleted file mode 100755
index d5a9310..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene46/Lucene46FieldInfosFormat.java
+++ /dev/null
@@ -1,205 +0,0 @@
-package org.apache.lucene.codecs.lucene46;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.Map;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.UndeadNormsProducer;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfo.DocValuesType;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.ChecksumIndexInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-
-/**
- * Lucene 4.6 Field Infos format.
- * @deprecated only for old 4.x segments
- */
-@Deprecated
-public final class Lucene46FieldInfosFormat extends FieldInfosFormat {
-
-  /** Sole constructor. */
-  public Lucene46FieldInfosFormat() {
-  }
-
-  @Override
-  public FieldInfos read(Directory directory, SegmentInfo segmentInfo, String segmentSuffix, IOContext context) throws IOException {
-    final String fileName = IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, Lucene46FieldInfosFormat.EXTENSION);
-    try (ChecksumIndexInput input = directory.openChecksumInput(fileName, context)) {
-      int codecVersion = CodecUtil.checkHeader(input, Lucene46FieldInfosFormat.CODEC_NAME, 
-                                                      Lucene46FieldInfosFormat.FORMAT_START, 
-                                                      Lucene46FieldInfosFormat.FORMAT_CURRENT);
-
-      final int size = input.readVInt(); //read in the size
-      FieldInfo infos[] = new FieldInfo[size];
-
-      for (int i = 0; i < size; i++) {
-        String name = input.readString();
-        final int fieldNumber = input.readVInt();
-        if (fieldNumber < 0) {
-          throw new CorruptIndexException("invalid field number for field: " + name + ", fieldNumber=" + fieldNumber, input);
-        }
-        byte bits = input.readByte();
-        boolean isIndexed = (bits & Lucene46FieldInfosFormat.IS_INDEXED) != 0;
-        boolean storeTermVector = (bits & Lucene46FieldInfosFormat.STORE_TERMVECTOR) != 0;
-        boolean omitNorms = (bits & Lucene46FieldInfosFormat.OMIT_NORMS) != 0;
-        boolean storePayloads = (bits & Lucene46FieldInfosFormat.STORE_PAYLOADS) != 0;
-        final IndexOptions indexOptions;
-        if (!isIndexed) {
-          indexOptions = null;
-        } else if ((bits & Lucene46FieldInfosFormat.OMIT_TERM_FREQ_AND_POSITIONS) != 0) {
-          indexOptions = IndexOptions.DOCS_ONLY;
-        } else if ((bits & Lucene46FieldInfosFormat.OMIT_POSITIONS) != 0) {
-          indexOptions = IndexOptions.DOCS_AND_FREQS;
-        } else if ((bits & Lucene46FieldInfosFormat.STORE_OFFSETS_IN_POSTINGS) != 0) {
-          indexOptions = IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;
-        } else {
-          indexOptions = IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
-        }
-
-        // DV Types are packed in one byte
-        byte val = input.readByte();
-        final DocValuesType docValuesType = getDocValuesType(input, (byte) (val & 0x0F));
-        final DocValuesType normsType = getDocValuesType(input, (byte) ((val >>> 4) & 0x0F));
-        final long dvGen = input.readLong();
-        final Map<String,String> attributes = input.readStringStringMap();
-
-        if (isIndexed && omitNorms == false && normsType == null) {
-          // Undead norms!  Lucene42NormsProducer will check this and bring norms back from the dead:
-          UndeadNormsProducer.setUndead(attributes);
-        }
-
-        infos[i] = new FieldInfo(name, fieldNumber, storeTermVector, 
-          omitNorms, storePayloads, indexOptions, docValuesType, dvGen, Collections.unmodifiableMap(attributes));
-      }
-      
-      if (codecVersion >= Lucene46FieldInfosFormat.FORMAT_CHECKSUM) {
-        CodecUtil.checkFooter(input);
-      } else {
-        CodecUtil.checkEOF(input);
-      }
-      return new FieldInfos(infos);
-    }
-  }
-  
-  private static DocValuesType getDocValuesType(IndexInput input, byte b) throws IOException {
-    if (b == 0) {
-      return null;
-    } else if (b == 1) {
-      return DocValuesType.NUMERIC;
-    } else if (b == 2) {
-      return DocValuesType.BINARY;
-    } else if (b == 3) {
-      return DocValuesType.SORTED;
-    } else if (b == 4) {
-      return DocValuesType.SORTED_SET;
-    } else if (b == 5) {
-      return DocValuesType.SORTED_NUMERIC;
-    } else {
-      throw new CorruptIndexException("invalid docvalues byte: " + b, input);
-    }
-  }
-
-  @Override
-  public void write(Directory directory, SegmentInfo segmentInfo, String segmentSuffix, FieldInfos infos, IOContext context) throws IOException {
-    final String fileName = IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, Lucene46FieldInfosFormat.EXTENSION);
-    try (IndexOutput output = directory.createOutput(fileName, context)) {
-      CodecUtil.writeHeader(output, Lucene46FieldInfosFormat.CODEC_NAME, Lucene46FieldInfosFormat.FORMAT_CURRENT);
-      output.writeVInt(infos.size());
-      for (FieldInfo fi : infos) {
-        IndexOptions indexOptions = fi.getIndexOptions();
-        byte bits = 0x0;
-        if (fi.hasVectors()) bits |= Lucene46FieldInfosFormat.STORE_TERMVECTOR;
-        if (fi.omitsNorms()) bits |= Lucene46FieldInfosFormat.OMIT_NORMS;
-        if (fi.hasPayloads()) bits |= Lucene46FieldInfosFormat.STORE_PAYLOADS;
-        if (fi.isIndexed()) {
-          bits |= Lucene46FieldInfosFormat.IS_INDEXED;
-          assert indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 || !fi.hasPayloads();
-          if (indexOptions == IndexOptions.DOCS_ONLY) {
-            bits |= Lucene46FieldInfosFormat.OMIT_TERM_FREQ_AND_POSITIONS;
-          } else if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) {
-            bits |= Lucene46FieldInfosFormat.STORE_OFFSETS_IN_POSTINGS;
-          } else if (indexOptions == IndexOptions.DOCS_AND_FREQS) {
-            bits |= Lucene46FieldInfosFormat.OMIT_POSITIONS;
-          }
-        }
-        output.writeString(fi.name);
-        output.writeVInt(fi.number);
-        output.writeByte(bits);
-
-        // pack the DV types in one byte
-        final byte dv = docValuesByte(fi.getDocValuesType());
-        final byte nrm = docValuesByte(fi.hasNorms() ? DocValuesType.NUMERIC : null);
-        assert (dv & (~0xF)) == 0 && (nrm & (~0x0F)) == 0;
-        byte val = (byte) (0xff & ((nrm << 4) | dv));
-        output.writeByte(val);
-        output.writeLong(fi.getDocValuesGen());
-        output.writeStringStringMap(fi.attributes());
-      }
-      CodecUtil.writeFooter(output);
-    }
-  }
-  
-  private static byte docValuesByte(DocValuesType type) {
-    if (type == null) {
-      return 0;
-    } else if (type == DocValuesType.NUMERIC) {
-      return 1;
-    } else if (type == DocValuesType.BINARY) {
-      return 2;
-    } else if (type == DocValuesType.SORTED) {
-      return 3;
-    } else if (type == DocValuesType.SORTED_SET) {
-      return 4;
-    } else if (type == DocValuesType.SORTED_NUMERIC) {
-      return 5;
-    } else {
-      throw new AssertionError();
-    }
-  }
-  
-  /** Extension of field infos */
-  static final String EXTENSION = "fnm";
-  
-  // Codec header
-  static final String CODEC_NAME = "Lucene46FieldInfos";
-  static final int FORMAT_START = 0;
-  static final int FORMAT_CHECKSUM = 1;
-  static final int FORMAT_SORTED_NUMERIC = 2;
-  static final int FORMAT_CURRENT = FORMAT_SORTED_NUMERIC;
-  
-  // Field flags
-  static final byte IS_INDEXED = 0x1;
-  static final byte STORE_TERMVECTOR = 0x2;
-  static final byte STORE_OFFSETS_IN_POSTINGS = 0x4;
-  static final byte OMIT_NORMS = 0x10;
-  static final byte STORE_PAYLOADS = 0x20;
-  static final byte OMIT_TERM_FREQ_AND_POSITIONS = 0x40;
-  static final byte OMIT_POSITIONS = -128;
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene46/Lucene46SegmentInfoFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene46/Lucene46SegmentInfoFormat.java
deleted file mode 100755
index e6baa97..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene46/Lucene46SegmentInfoFormat.java
+++ /dev/null
@@ -1,92 +0,0 @@
-package org.apache.lucene.codecs.lucene46;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.text.ParseException;
-import java.util.Map;
-import java.util.Set;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.SegmentInfoFormat;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.ChecksumIndexInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.util.Version;
-
-/**
- * Lucene 4.6 Segment info format.
- * @deprecated only for old 4.x segments
- */
-@Deprecated
-public class Lucene46SegmentInfoFormat extends SegmentInfoFormat {
-
-  /** Sole constructor. */
-  public Lucene46SegmentInfoFormat() {
-  }
-  
-  @Override
-  public SegmentInfo read(Directory dir, String segment, byte segmentID[], IOContext context) throws IOException {
-    final String fileName = IndexFileNames.segmentFileName(segment, "", Lucene46SegmentInfoFormat.SI_EXTENSION);
-    try (ChecksumIndexInput input = dir.openChecksumInput(fileName, context)) {
-      int codecVersion = CodecUtil.checkHeader(input, Lucene46SegmentInfoFormat.CODEC_NAME,
-                                                      Lucene46SegmentInfoFormat.VERSION_START,
-                                                      Lucene46SegmentInfoFormat.VERSION_CURRENT);
-      final Version version;
-      try {
-        version = Version.parse(input.readString());
-      } catch (ParseException pe) {
-        throw new CorruptIndexException("unable to parse version string: " + pe.getMessage(), input, pe);
-      }
-
-      final int docCount = input.readInt();
-      if (docCount < 0) {
-        throw new CorruptIndexException("invalid docCount: " + docCount, input);
-      }
-      final boolean isCompoundFile = input.readByte() == SegmentInfo.YES;
-      final Map<String,String> diagnostics = input.readStringStringMap();
-      final Set<String> files = input.readStringSet();
-
-      if (codecVersion >= Lucene46SegmentInfoFormat.VERSION_CHECKSUM) {
-        CodecUtil.checkFooter(input);
-      } else {
-        CodecUtil.checkEOF(input);
-      }
-
-      final SegmentInfo si = new SegmentInfo(dir, version, segment, docCount, isCompoundFile, null, diagnostics, null);
-      si.setFiles(files);
-
-      return si;
-    }
-  }
-
-  @Override
-  public void write(Directory dir, SegmentInfo info, IOContext ioContext) throws IOException {
-    throw new UnsupportedOperationException("this codec can only be used for reading");
-  }
-
-  /** File extension used to store {@link SegmentInfo}. */
-  final static String SI_EXTENSION = "si";
-  static final String CODEC_NAME = "Lucene46SegmentInfo";
-  static final int VERSION_START = 0;
-  static final int VERSION_CHECKSUM = 1;
-  static final int VERSION_CURRENT = VERSION_CHECKSUM;
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene46/package.html b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene46/package.html
deleted file mode 100644
index 8acd7aa..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene46/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Lucene 4.6 file format.
-</body>
-</html>
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49Codec.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49Codec.java
deleted file mode 100644
index 5fa74a6..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49Codec.java
+++ /dev/null
@@ -1,147 +0,0 @@
-package org.apache.lucene.codecs.lucene49;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.CompoundFormat;
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.LiveDocsFormat;
-import org.apache.lucene.codecs.NormsConsumer;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.SegmentInfoFormat;
-import org.apache.lucene.codecs.StoredFieldsFormat;
-import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40CompoundFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
-import org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat;
-import org.apache.lucene.codecs.lucene42.Lucene42TermVectorsFormat;
-import org.apache.lucene.codecs.lucene46.Lucene46FieldInfosFormat;
-import org.apache.lucene.codecs.lucene46.Lucene46SegmentInfoFormat;
-import org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat;
-import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
-import org.apache.lucene.index.SegmentWriteState;
-
-/**
- * Implements the Lucene 4.9 index format
- * @deprecated only for old 4.x segments
- */
-@Deprecated
-public class Lucene49Codec extends Codec {
-  private final StoredFieldsFormat fieldsFormat = new Lucene41StoredFieldsFormat();
-  private final TermVectorsFormat vectorsFormat = new Lucene42TermVectorsFormat();
-  private final FieldInfosFormat fieldInfosFormat = new Lucene46FieldInfosFormat();
-  private final SegmentInfoFormat segmentInfosFormat = new Lucene46SegmentInfoFormat();
-  private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat();
-  private final CompoundFormat compoundFormat = new Lucene40CompoundFormat();
-  
-  private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
-    @Override
-    public PostingsFormat getPostingsFormatForField(String field) {
-      return Lucene49Codec.this.getPostingsFormatForField(field);
-    }
-  };
-  
-  private final DocValuesFormat docValuesFormat = new PerFieldDocValuesFormat() {
-    @Override
-    public DocValuesFormat getDocValuesFormatForField(String field) {
-      return Lucene49Codec.this.getDocValuesFormatForField(field);
-    }
-  };
-
-  /** Sole constructor. */
-  public Lucene49Codec() {
-    super("Lucene49");
-  }
-  
-  @Override
-  public StoredFieldsFormat storedFieldsFormat() {
-    return fieldsFormat;
-  }
-  
-  @Override
-  public TermVectorsFormat termVectorsFormat() {
-    return vectorsFormat;
-  }
-
-  @Override
-  public final PostingsFormat postingsFormat() {
-    return postingsFormat;
-  }
-  
-  @Override
-  public final FieldInfosFormat fieldInfosFormat() {
-    return fieldInfosFormat;
-  }
-  
-  @Override
-  public SegmentInfoFormat segmentInfoFormat() {
-    return segmentInfosFormat;
-  }
-  
-  @Override
-  public final LiveDocsFormat liveDocsFormat() {
-    return liveDocsFormat;
-  }
-  
-  @Override
-  public CompoundFormat compoundFormat() {
-    return compoundFormat;
-  }
-
-  /** Returns the postings format that should be used for writing 
-   *  new segments of <code>field</code>.
-   *  
-   *  The default implementation always returns "Lucene41"
-   */
-  public PostingsFormat getPostingsFormatForField(String field) {
-    return defaultFormat;
-  }
-  
-  /** Returns the docvalues format that should be used for writing 
-   *  new segments of <code>field</code>.
-   *  
-   *  The default implementation always returns "Lucene49"
-   */
-  public DocValuesFormat getDocValuesFormatForField(String field) {
-    return defaultDVFormat;
-  }
-  
-  @Override
-  public final DocValuesFormat docValuesFormat() {
-    return docValuesFormat;
-  }
-
-  private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene41");
-  private final DocValuesFormat defaultDVFormat = DocValuesFormat.forName("Lucene49");
-
-  private final NormsFormat normsFormat = new Lucene49NormsFormat() {
-    @Override
-    public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
-      throw new UnsupportedOperationException("this codec can only be used for reading");
-    }
-  };
-
-  @Override
-  public NormsFormat normsFormat() {
-   return normsFormat;
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesConsumer.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesConsumer.java
deleted file mode 100644
index 9fbc719..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesConsumer.java
+++ /dev/null
@@ -1,475 +0,0 @@
-package org.apache.lucene.codecs.lucene49;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable; // javadocs
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.HashSet;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.FieldInfo.DocValuesType;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.RAMOutputStream;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.BytesRefBuilder;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.MathUtil;
-import org.apache.lucene.util.StringHelper;
-import org.apache.lucene.util.packed.DirectWriter;
-import org.apache.lucene.util.packed.MonotonicBlockPackedWriter;
-import org.apache.lucene.util.packed.PackedInts;
-
-/** 
- * writer for 4.9 docvalues format
- * @deprecated only for old 4.x segments
- */
-@Deprecated
-class Lucene49DocValuesConsumer extends DocValuesConsumer implements Closeable {
-
-  static final int BLOCK_SIZE = 16384;
-  static final int ADDRESS_INTERVAL = 16;
-
-  /** Compressed using packed blocks of ints. */
-  public static final int DELTA_COMPRESSED = 0;
-  /** Compressed by computing the GCD. */
-  public static final int GCD_COMPRESSED = 1;
-  /** Compressed by giving IDs to unique values. */
-  public static final int TABLE_COMPRESSED = 2;
-  /** Compressed with monotonically increasing values */
-  public static final int MONOTONIC_COMPRESSED = 3;
-  
-  /** Uncompressed binary, written directly (fixed length). */
-  public static final int BINARY_FIXED_UNCOMPRESSED = 0;
-  /** Uncompressed binary, written directly (variable length). */
-  public static final int BINARY_VARIABLE_UNCOMPRESSED = 1;
-  /** Compressed binary with shared prefixes */
-  public static final int BINARY_PREFIX_COMPRESSED = 2;
-
-  /** Standard storage for sorted set values with 1 level of indirection:
-   *  docId -> address -> ord. */
-  public static final int SORTED_WITH_ADDRESSES = 0;
-  /** Single-valued sorted set values, encoded as sorted values, so no level
-   *  of indirection: docId -> ord. */
-  public static final int SORTED_SINGLE_VALUED = 1;
-
-  IndexOutput data, meta;
-  final int maxDoc;
-  
-  /** expert: Creates a new writer */
-  public Lucene49DocValuesConsumer(SegmentWriteState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
-    boolean success = false;
-    try {
-      String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
-      data = state.directory.createOutput(dataName, state.context);
-      CodecUtil.writeHeader(data, dataCodec, Lucene49DocValuesFormat.VERSION_CURRENT);
-      String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
-      meta = state.directory.createOutput(metaName, state.context);
-      CodecUtil.writeHeader(meta, metaCodec, Lucene49DocValuesFormat.VERSION_CURRENT);
-      maxDoc = state.segmentInfo.getDocCount();
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(this);
-      }
-    }
-  }
-  
-  @Override
-  public void addNumericField(FieldInfo field, Iterable<Number> values) throws IOException {
-    checkCanWrite(field);
-    addNumericField(field, values, true);
-  }
-
-  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {
-    long count = 0;
-    long minValue = Long.MAX_VALUE;
-    long maxValue = Long.MIN_VALUE;
-    long gcd = 0;
-    boolean missing = false;
-    // TODO: more efficient?
-    HashSet<Long> uniqueValues = null;
-    if (optimizeStorage) {
-      uniqueValues = new HashSet<>();
-
-      for (Number nv : values) {
-        final long v;
-        if (nv == null) {
-          v = 0;
-          missing = true;
-        } else {
-          v = nv.longValue();
-        }
-
-        if (gcd != 1) {
-          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {
-            // in that case v - minValue might overflow and make the GCD computation return
-            // wrong results. Since these extreme values are unlikely, we just discard
-            // GCD computation for them
-            gcd = 1;
-          } else if (count != 0) { // minValue needs to be set first
-            gcd = MathUtil.gcd(gcd, v - minValue);
-          }
-        }
-
-        minValue = Math.min(minValue, v);
-        maxValue = Math.max(maxValue, v);
-
-        if (uniqueValues != null) {
-          if (uniqueValues.add(v)) {
-            if (uniqueValues.size() > 256) {
-              uniqueValues = null;
-            }
-          }
-        }
-
-        ++count;
-      }
-    } else {
-      for (Number nv : values) {
-        long v = nv.longValue();
-        minValue = Math.min(minValue, v);
-        maxValue = Math.max(maxValue, v);
-        ++count;
-      }
-    }
-    
-    final long delta = maxValue - minValue;
-    final int deltaBitsRequired = DirectWriter.unsignedBitsRequired(delta);
-    final int tableBitsRequired = uniqueValues == null
-        ? Integer.MAX_VALUE
-        : DirectWriter.bitsRequired(uniqueValues.size() - 1);
-
-    final int format;
-    if (uniqueValues != null && tableBitsRequired < deltaBitsRequired) {
-      format = TABLE_COMPRESSED;
-    } else if (gcd != 0 && gcd != 1) {
-      final long gcdDelta = (maxValue - minValue) / gcd;
-      final long gcdBitsRequired = DirectWriter.unsignedBitsRequired(gcdDelta);
-      format = gcdBitsRequired < deltaBitsRequired ? GCD_COMPRESSED : DELTA_COMPRESSED;
-    } else {
-      format = DELTA_COMPRESSED;
-    }
-    meta.writeVInt(field.number);
-    meta.writeByte(Lucene49DocValuesFormat.NUMERIC);
-    meta.writeVInt(format);
-    if (missing) {
-      meta.writeLong(data.getFilePointer());
-      writeMissingBitset(values);
-    } else {
-      meta.writeLong(-1L);
-    }
-    meta.writeLong(data.getFilePointer());
-    meta.writeVLong(count);
-
-    switch (format) {
-      case GCD_COMPRESSED:
-        meta.writeLong(minValue);
-        meta.writeLong(gcd);
-        final long maxDelta = (maxValue - minValue) / gcd;
-        final int bits = DirectWriter.unsignedBitsRequired(maxDelta);
-        meta.writeVInt(bits);
-        final DirectWriter quotientWriter = DirectWriter.getInstance(data, count, bits);
-        for (Number nv : values) {
-          long value = nv == null ? 0 : nv.longValue();
-          quotientWriter.add((value - minValue) / gcd);
-        }
-        quotientWriter.finish();
-        break;
-      case DELTA_COMPRESSED:
-        final long minDelta = delta < 0 ? 0 : minValue;
-        meta.writeLong(minDelta);
-        meta.writeVInt(deltaBitsRequired);
-        final DirectWriter writer = DirectWriter.getInstance(data, count, deltaBitsRequired);
-        for (Number nv : values) {
-          long v = nv == null ? 0 : nv.longValue();
-          writer.add(v - minDelta);
-        }
-        writer.finish();
-        break;
-      case TABLE_COMPRESSED:
-        final Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);
-        Arrays.sort(decode);
-        final HashMap<Long,Integer> encode = new HashMap<>();
-        meta.writeVInt(decode.length);
-        for (int i = 0; i < decode.length; i++) {
-          meta.writeLong(decode[i]);
-          encode.put(decode[i], i);
-        }
-        meta.writeVInt(tableBitsRequired);
-        final DirectWriter ordsWriter = DirectWriter.getInstance(data, count, tableBitsRequired);
-        for (Number nv : values) {
-          ordsWriter.add(encode.get(nv == null ? 0 : nv.longValue()));
-        }
-        ordsWriter.finish();
-        break;
-      default:
-        throw new AssertionError();
-    }
-    meta.writeLong(data.getFilePointer());
-  }
-  
-  // TODO: in some cases representing missing with minValue-1 wouldn't take up additional space and so on,
-  // but this is very simple, and algorithms only check this for values of 0 anyway (doesnt slow down normal decode)
-  void writeMissingBitset(Iterable<?> values) throws IOException {
-    byte bits = 0;
-    int count = 0;
-    for (Object v : values) {
-      if (count == 8) {
-        data.writeByte(bits);
-        count = 0;
-        bits = 0;
-      }
-      if (v != null) {
-        bits |= 1 << (count & 7);
-      }
-      count++;
-    }
-    if (count > 0) {
-      data.writeByte(bits);
-    }
-  }
-
-  @Override
-  public void addBinaryField(FieldInfo field, Iterable<BytesRef> values) throws IOException {
-    checkCanWrite(field);
-    // write the byte[] data
-    meta.writeVInt(field.number);
-    meta.writeByte(Lucene49DocValuesFormat.BINARY);
-    int minLength = Integer.MAX_VALUE;
-    int maxLength = Integer.MIN_VALUE;
-    final long startFP = data.getFilePointer();
-    long count = 0;
-    boolean missing = false;
-    for(BytesRef v : values) {
-      final int length;
-      if (v == null) {
-        length = 0;
-        missing = true;
-      } else {
-        length = v.length;
-      }
-      minLength = Math.min(minLength, length);
-      maxLength = Math.max(maxLength, length);
-      if (v != null) {
-        data.writeBytes(v.bytes, v.offset, v.length);
-      }
-      count++;
-    }
-    meta.writeVInt(minLength == maxLength ? BINARY_FIXED_UNCOMPRESSED : BINARY_VARIABLE_UNCOMPRESSED);
-    if (missing) {
-      meta.writeLong(data.getFilePointer());
-      writeMissingBitset(values);
-    } else {
-      meta.writeLong(-1L);
-    }
-    meta.writeVInt(minLength);
-    meta.writeVInt(maxLength);
-    meta.writeVLong(count);
-    meta.writeLong(startFP);
-    
-    // if minLength == maxLength, its a fixed-length byte[], we are done (the addresses are implicit)
-    // otherwise, we need to record the length fields...
-    if (minLength != maxLength) {
-      meta.writeLong(data.getFilePointer());
-      meta.writeVInt(PackedInts.VERSION_CURRENT);
-      meta.writeVInt(BLOCK_SIZE);
-
-      final MonotonicBlockPackedWriter writer = new MonotonicBlockPackedWriter(data, BLOCK_SIZE);
-      long addr = 0;
-      writer.add(addr);
-      for (BytesRef v : values) {
-        if (v != null) {
-          addr += v.length;
-        }
-        writer.add(addr);
-      }
-      writer.finish();
-    }
-  }
-  
-  /** expert: writes a value dictionary for a sorted/sortedset field */
-  private void addTermsDict(FieldInfo field, final Iterable<BytesRef> values) throws IOException {
-    // first check if its a "fixed-length" terms dict
-    int minLength = Integer.MAX_VALUE;
-    int maxLength = Integer.MIN_VALUE;
-    for (BytesRef v : values) {
-      minLength = Math.min(minLength, v.length);
-      maxLength = Math.max(maxLength, v.length);
-    }
-    if (minLength == maxLength) {
-      // no index needed: direct addressing by mult
-      addBinaryField(field, values);
-    } else {
-      // header
-      meta.writeVInt(field.number);
-      meta.writeByte(Lucene49DocValuesFormat.BINARY);
-      meta.writeVInt(BINARY_PREFIX_COMPRESSED);
-      meta.writeLong(-1L);
-      // now write the bytes: sharing prefixes within a block
-      final long startFP = data.getFilePointer();
-      // currently, we have to store the delta from expected for every 1/nth term
-      // we could avoid this, but its not much and less overall RAM than the previous approach!
-      RAMOutputStream addressBuffer = new RAMOutputStream();
-      MonotonicBlockPackedWriter termAddresses = new MonotonicBlockPackedWriter(addressBuffer, BLOCK_SIZE);
-      BytesRefBuilder lastTerm = new BytesRefBuilder();
-      lastTerm.grow(Math.max(0, maxLength));
-      long count = 0;
-      for (BytesRef v : values) {
-        if (count % ADDRESS_INTERVAL == 0) {
-          termAddresses.add(data.getFilePointer() - startFP);
-          // force the first term in a block to be abs-encoded
-          lastTerm.clear();
-        }
-        
-        // prefix-code
-        int sharedPrefix = StringHelper.bytesDifference(lastTerm.get(), v);
-        data.writeVInt(sharedPrefix);
-        data.writeVInt(v.length - sharedPrefix);
-        data.writeBytes(v.bytes, v.offset + sharedPrefix, v.length - sharedPrefix);
-        lastTerm.copyBytes(v);
-        count++;
-      }
-      final long indexStartFP = data.getFilePointer();
-      // write addresses of indexed terms
-      termAddresses.finish();
-      addressBuffer.writeTo(data);
-      addressBuffer = null;
-      termAddresses = null;
-      meta.writeVInt(minLength);
-      meta.writeVInt(maxLength);
-      meta.writeVLong(count);
-      meta.writeLong(startFP);
-      meta.writeVInt(ADDRESS_INTERVAL);
-      meta.writeLong(indexStartFP);
-      meta.writeVInt(PackedInts.VERSION_CURRENT);
-      meta.writeVInt(BLOCK_SIZE);
-    }
-  }
-
-  @Override
-  public void addSortedField(FieldInfo field, Iterable<BytesRef> values, Iterable<Number> docToOrd) throws IOException {
-    checkCanWrite(field);
-    meta.writeVInt(field.number);
-    meta.writeByte(Lucene49DocValuesFormat.SORTED);
-    addTermsDict(field, values);
-    addNumericField(field, docToOrd, false);
-  }
-
-  @Override
-  public void addSortedNumericField(FieldInfo field, final Iterable<Number> docToValueCount, final Iterable<Number> values) throws IOException {
-    checkCanWrite(field);
-    meta.writeVInt(field.number);
-    meta.writeByte(Lucene49DocValuesFormat.SORTED_NUMERIC);
-    if (isSingleValued(docToValueCount)) {
-      meta.writeVInt(SORTED_SINGLE_VALUED);
-      // The field is single-valued, we can encode it as NUMERIC
-      addNumericField(field, singletonView(docToValueCount, values, null));
-    } else {
-      meta.writeVInt(SORTED_WITH_ADDRESSES);
-      // write the stream of values as a numeric field
-      addNumericField(field, values, true);
-      // write the doc -> ord count as a absolute index to the stream
-      addAddresses(field, docToValueCount);
-    }
-  }
-
-  @Override
-  public void addSortedSetField(FieldInfo field, Iterable<BytesRef> values, final Iterable<Number> docToOrdCount, final Iterable<Number> ords) throws IOException {
-    checkCanWrite(field);
-    meta.writeVInt(field.number);
-    meta.writeByte(Lucene49DocValuesFormat.SORTED_SET);
-
-    if (isSingleValued(docToOrdCount)) {
-      meta.writeVInt(SORTED_SINGLE_VALUED);
-      // The field is single-valued, we can encode it as SORTED
-      addSortedField(field, values, singletonView(docToOrdCount, ords, -1L));
-    } else {
-      meta.writeVInt(SORTED_WITH_ADDRESSES);
-
-      // write the ord -> byte[] as a binary field
-      addTermsDict(field, values);
-
-      // write the stream of ords as a numeric field
-      // NOTE: we could return an iterator that delta-encodes these within a doc
-      addNumericField(field, ords, false);
-
-      // write the doc -> ord count as a absolute index to the stream
-      addAddresses(field, docToOrdCount);
-    }
-  }
-  
-  // writes addressing information as MONOTONIC_COMPRESSED integer
-  private void addAddresses(FieldInfo field, Iterable<Number> values) throws IOException {
-    meta.writeVInt(field.number);
-    meta.writeByte(Lucene49DocValuesFormat.NUMERIC);
-    meta.writeVInt(MONOTONIC_COMPRESSED);
-    meta.writeLong(-1L);
-    meta.writeLong(data.getFilePointer());
-    meta.writeVLong(maxDoc);
-    meta.writeVInt(PackedInts.VERSION_CURRENT);
-    meta.writeVInt(BLOCK_SIZE);
-
-    final MonotonicBlockPackedWriter writer = new MonotonicBlockPackedWriter(data, BLOCK_SIZE);
-    long addr = 0;
-    writer.add(addr);
-    for (Number v : values) {
-      addr += v.longValue();
-      writer.add(addr);
-    }
-    writer.finish();
-    meta.writeLong(data.getFilePointer());
-  }
-
-  @Override
-  public void close() throws IOException {
-    boolean success = false;
-    try {
-      if (meta != null) {
-        meta.writeVInt(-1); // write EOF marker
-        CodecUtil.writeFooter(meta); // write checksum
-      }
-      if (data != null) {
-        CodecUtil.writeFooter(data); // write checksum
-      }
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(data, meta);
-      } else {
-        IOUtils.closeWhileHandlingException(data, meta);
-      }
-      meta = data = null;
-    }
-  }
-  
-  void checkCanWrite(FieldInfo field) {
-    if ((field.getDocValuesType() == DocValuesType.NUMERIC || 
-        field.getDocValuesType() == DocValuesType.BINARY) && 
-        field.getDocValuesGen() != -1) {
-      // ok
-    } else {
-      throw new UnsupportedOperationException("this codec can only be used for reading");
-    }
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesFormat.java
deleted file mode 100644
index 1108b2a..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesFormat.java
+++ /dev/null
@@ -1,61 +0,0 @@
-package org.apache.lucene.codecs.lucene49;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.codecs.DocValuesProducer;
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-
-/**
- * Lucene 4.9 DocValues format.
- * @deprecated only for old 4.x segments
- */
-@Deprecated
-public class Lucene49DocValuesFormat extends DocValuesFormat {
-
-  /** Sole Constructor */
-  public Lucene49DocValuesFormat() {
-    super("Lucene49");
-  }
-
-  @Override
-  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    return new Lucene49DocValuesConsumer(state, DATA_CODEC, DATA_EXTENSION, META_CODEC, META_EXTENSION);
-  }
-
-  @Override
-  public final DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
-    return new Lucene49DocValuesProducer(state, DATA_CODEC, DATA_EXTENSION, META_CODEC, META_EXTENSION);
-  }
-  
-  static final String DATA_CODEC = "Lucene49DocValuesData";
-  static final String DATA_EXTENSION = "dvd";
-  static final String META_CODEC = "Lucene49ValuesMetadata";
-  static final String META_EXTENSION = "dvm";
-  static final int VERSION_START = 0;
-  static final int VERSION_CURRENT = VERSION_START;
-  static final byte NUMERIC = 0;
-  static final byte BINARY = 1;
-  static final byte SORTED = 2;
-  static final byte SORTED_SET = 3;
-  static final byte SORTED_NUMERIC = 4;
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesProducer.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesProducer.java
deleted file mode 100644
index 284ad17..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesProducer.java
+++ /dev/null
@@ -1,992 +0,0 @@
-package org.apache.lucene.codecs.lucene49;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import static org.apache.lucene.codecs.lucene49.Lucene49DocValuesConsumer.BINARY_FIXED_UNCOMPRESSED;
-import static org.apache.lucene.codecs.lucene49.Lucene49DocValuesConsumer.BINARY_PREFIX_COMPRESSED;
-import static org.apache.lucene.codecs.lucene49.Lucene49DocValuesConsumer.BINARY_VARIABLE_UNCOMPRESSED;
-import static org.apache.lucene.codecs.lucene49.Lucene49DocValuesConsumer.DELTA_COMPRESSED;
-import static org.apache.lucene.codecs.lucene49.Lucene49DocValuesConsumer.GCD_COMPRESSED;
-import static org.apache.lucene.codecs.lucene49.Lucene49DocValuesConsumer.MONOTONIC_COMPRESSED;
-import static org.apache.lucene.codecs.lucene49.Lucene49DocValuesConsumer.SORTED_SINGLE_VALUED;
-import static org.apache.lucene.codecs.lucene49.Lucene49DocValuesConsumer.SORTED_WITH_ADDRESSES;
-import static org.apache.lucene.codecs.lucene49.Lucene49DocValuesConsumer.TABLE_COMPRESSED;
-
-import java.io.Closeable; // javadocs
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.concurrent.atomic.AtomicLong;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.DocValuesProducer;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.NumericDocValues;
-import org.apache.lucene.index.RandomAccessOrds;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SortedDocValues;
-import org.apache.lucene.index.SortedNumericDocValues;
-import org.apache.lucene.index.SortedSetDocValues;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.TermsEnum.SeekStatus;
-import org.apache.lucene.store.ChecksumIndexInput;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.RandomAccessInput;
-import org.apache.lucene.util.Accountable;
-import org.apache.lucene.util.Accountables;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.LongValues;
-import org.apache.lucene.util.RamUsageEstimator;
-import org.apache.lucene.util.packed.DirectReader;
-import org.apache.lucene.util.packed.MonotonicBlockPackedReader;
-
-/** 
- * reader for 4.9 docvalues format
- * @deprecated only for 4.x segments 
- */
-@Deprecated
-class Lucene49DocValuesProducer extends DocValuesProducer implements Closeable {
-  private final Map<String,NumericEntry> numerics = new HashMap<>();
-  private final Map<String,BinaryEntry> binaries = new HashMap<>();
-  private final Map<String,SortedSetEntry> sortedSets = new HashMap<>();
-  private final Map<String,SortedSetEntry> sortedNumerics = new HashMap<>();
-  private final Map<String,NumericEntry> ords = new HashMap<>();
-  private final Map<String,NumericEntry> ordIndexes = new HashMap<>();
-  private final AtomicLong ramBytesUsed;
-  private final IndexInput data;
-  private final int numFields;
-  private final int maxDoc;
-
-  // memory-resident structures
-  private final Map<String,MonotonicBlockPackedReader> addressInstances = new HashMap<>();
-  private final Map<String,MonotonicBlockPackedReader> ordIndexInstances = new HashMap<>();
-  
-  private final boolean merging;
-  
-  // clone for merge: when merging we don't do any instances.put()s
-  Lucene49DocValuesProducer(Lucene49DocValuesProducer original) throws IOException {
-    assert Thread.holdsLock(original);
-    numerics.putAll(original.numerics);
-    binaries.putAll(original.binaries);
-    sortedSets.putAll(original.sortedSets);
-    sortedNumerics.putAll(original.sortedNumerics);
-    ords.putAll(original.ords);
-    ordIndexes.putAll(original.ordIndexes);
-    ramBytesUsed = new AtomicLong(original.ramBytesUsed());
-    data = original.data.clone();
-    numFields = original.numFields;
-    maxDoc = original.maxDoc;
-    addressInstances.putAll(original.addressInstances);
-    ordIndexInstances.putAll(original.ordIndexInstances);
-    merging = true;
-  }
-  
-  /** expert: instantiates a new reader */
-  Lucene49DocValuesProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
-    String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
-    this.maxDoc = state.segmentInfo.getDocCount();
-    merging = false;
-    
-    int version = -1;
-    int numFields = -1;
-    
-    // read in the entries from the metadata file.
-    try (ChecksumIndexInput in = state.directory.openChecksumInput(metaName, state.context)) {
-      Throwable priorE = null;
-      try {
-        version = CodecUtil.checkHeader(in, metaCodec, 
-                                            Lucene49DocValuesFormat.VERSION_START,
-                                            Lucene49DocValuesFormat.VERSION_CURRENT);
-        numFields = readFields(in, state.fieldInfos);
-      } catch (Throwable exception) {
-        priorE = exception;
-      } finally {
-        CodecUtil.checkFooter(in, priorE);
-      }
-    }
-    this.numFields = numFields;
-
-    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
-    this.data = state.directory.openInput(dataName, state.context);
-    boolean success = false;
-    try {
-      final int version2 = CodecUtil.checkHeader(data, dataCodec, 
-                                                 Lucene49DocValuesFormat.VERSION_START,
-                                                 Lucene49DocValuesFormat.VERSION_CURRENT);
-      if (version != version2) {
-        throw new CorruptIndexException("Format versions mismatch: meta=" + version + ", data=" + version2, data);
-      }
-      
-      // NOTE: data file is too costly to verify checksum against all the bytes on open,
-      // but for now we at least verify proper structure of the checksum footer: which looks
-      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
-      // such as file truncation.
-      CodecUtil.retrieveChecksum(data);
-
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(this.data);
-      }
-    }
-    
-    ramBytesUsed = new AtomicLong(RamUsageEstimator.shallowSizeOfInstance(getClass()));
-  }
-
-  private void readSortedField(FieldInfo info, IndexInput meta) throws IOException {
-    // sorted = binary + numeric
-    if (meta.readVInt() != info.number) {
-      throw new CorruptIndexException("sorted entry for field: " + info.name + " is corrupt", meta);
-    }
-    if (meta.readByte() != Lucene49DocValuesFormat.BINARY) {
-      throw new CorruptIndexException("sorted entry for field: " + info.name + " is corrupt", meta);
-    }
-    BinaryEntry b = readBinaryEntry(meta);
-    binaries.put(info.name, b);
-    
-    if (meta.readVInt() != info.number) {
-      throw new CorruptIndexException("sorted entry for field: " + info.name + " is corrupt", meta);
-    }
-    if (meta.readByte() != Lucene49DocValuesFormat.NUMERIC) {
-      throw new CorruptIndexException("sorted entry for field: " + info.name + " is corrupt", meta);
-    }
-    NumericEntry n = readNumericEntry(meta);
-    ords.put(info.name, n);
-  }
-
-  private void readSortedSetFieldWithAddresses(FieldInfo info, IndexInput meta) throws IOException {
-    // sortedset = binary + numeric (addresses) + ordIndex
-    if (meta.readVInt() != info.number) {
-      throw new CorruptIndexException("sortedset entry for field: " + info.name + " is corrupt", meta);
-    }
-    if (meta.readByte() != Lucene49DocValuesFormat.BINARY) {
-      throw new CorruptIndexException("sortedset entry for field: " + info.name + " is corrupt", meta);
-    }
-    BinaryEntry b = readBinaryEntry(meta);
-    binaries.put(info.name, b);
-
-    if (meta.readVInt() != info.number) {
-      throw new CorruptIndexException("sortedset entry for field: " + info.name + " is corrupt", meta);
-    }
-    if (meta.readByte() != Lucene49DocValuesFormat.NUMERIC) {
-      throw new CorruptIndexException("sortedset entry for field: " + info.name + " is corrupt", meta);
-    }
-    NumericEntry n1 = readNumericEntry(meta);
-    ords.put(info.name, n1);
-
-    if (meta.readVInt() != info.number) {
-      throw new CorruptIndexException("sortedset entry for field: " + info.name + " is corrupt", meta);
-    }
-    if (meta.readByte() != Lucene49DocValuesFormat.NUMERIC) {
-      throw new CorruptIndexException("sortedset entry for field: " + info.name + " is corrupt", meta);
-    }
-    NumericEntry n2 = readNumericEntry(meta);
-    ordIndexes.put(info.name, n2);
-  }
-
-  private int readFields(IndexInput meta, FieldInfos infos) throws IOException {
-    int numFields = 0;
-    int fieldNumber = meta.readVInt();
-    while (fieldNumber != -1) {
-      numFields++;
-      FieldInfo info = infos.fieldInfo(fieldNumber);
-      if (info == null) {
-        // trickier to validate more: because we use multiple entries for "composite" types like sortedset, etc.
-        throw new CorruptIndexException("Invalid field number: " + fieldNumber, meta);
-      }
-      byte type = meta.readByte();
-      if (type == Lucene49DocValuesFormat.NUMERIC) {
-        numerics.put(info.name, readNumericEntry(meta));
-      } else if (type == Lucene49DocValuesFormat.BINARY) {
-        BinaryEntry b = readBinaryEntry(meta);
-        binaries.put(info.name, b);
-      } else if (type == Lucene49DocValuesFormat.SORTED) {
-        readSortedField(info, meta);
-      } else if (type == Lucene49DocValuesFormat.SORTED_SET) {
-        SortedSetEntry ss = readSortedSetEntry(meta);
-        sortedSets.put(info.name, ss);
-        if (ss.format == SORTED_WITH_ADDRESSES) {
-          readSortedSetFieldWithAddresses(info, meta);
-        } else if (ss.format == SORTED_SINGLE_VALUED) {
-          if (meta.readVInt() != fieldNumber) {
-            throw new CorruptIndexException("sortedset entry for field: " + info.name + " is corrupt", meta);
-          }
-          if (meta.readByte() != Lucene49DocValuesFormat.SORTED) {
-            throw new CorruptIndexException("sortedset entry for field: " + info.name + " is corrupt", meta);
-          }
-          readSortedField(info, meta);
-        } else {
-          throw new AssertionError();
-        }
-      } else if (type == Lucene49DocValuesFormat.SORTED_NUMERIC) {
-        SortedSetEntry ss = readSortedSetEntry(meta);
-        sortedNumerics.put(info.name, ss);
-        if (meta.readVInt() != fieldNumber) {
-          throw new CorruptIndexException("sortednumeric entry for field: " + info.name + " is corrupt", meta);
-        }
-        if (meta.readByte() != Lucene49DocValuesFormat.NUMERIC) {
-          throw new CorruptIndexException("sortednumeric entry for field: " + info.name + " is corrupt", meta);
-        }
-        numerics.put(info.name, readNumericEntry(meta));
-        if (ss.format == SORTED_WITH_ADDRESSES) {
-          if (meta.readVInt() != fieldNumber) {
-            throw new CorruptIndexException("sortednumeric entry for field: " + info.name + " is corrupt", meta);
-          }
-          if (meta.readByte() != Lucene49DocValuesFormat.NUMERIC) {
-            throw new CorruptIndexException("sortednumeric entry for field: " + info.name + " is corrupt", meta);
-          }
-          NumericEntry ordIndex = readNumericEntry(meta);
-          ordIndexes.put(info.name, ordIndex);
-        } else if (ss.format != SORTED_SINGLE_VALUED) {
-          throw new AssertionError();
-        }
-      } else {
-        throw new CorruptIndexException("invalid type: " + type, meta);
-      }
-      fieldNumber = meta.readVInt();
-    }
-    return numFields;
-  }
-  
-  static NumericEntry readNumericEntry(IndexInput meta) throws IOException {
-    NumericEntry entry = new NumericEntry();
-    entry.format = meta.readVInt();
-    entry.missingOffset = meta.readLong();
-    entry.offset = meta.readLong();
-    entry.count = meta.readVLong();
-    switch(entry.format) {
-      case GCD_COMPRESSED:
-        entry.minValue = meta.readLong();
-        entry.gcd = meta.readLong();
-        entry.bitsPerValue = meta.readVInt();
-        break;
-      case TABLE_COMPRESSED:
-        final int uniqueValues = meta.readVInt();
-        if (uniqueValues > 256) {
-          throw new CorruptIndexException("TABLE_COMPRESSED cannot have more than 256 distinct values, got=" + uniqueValues, meta);
-        }
-        entry.table = new long[uniqueValues];
-        for (int i = 0; i < uniqueValues; ++i) {
-          entry.table[i] = meta.readLong();
-        }
-        entry.bitsPerValue = meta.readVInt();
-        break;
-      case DELTA_COMPRESSED:
-        entry.minValue = meta.readLong();
-        entry.bitsPerValue = meta.readVInt();
-        break;
-      case MONOTONIC_COMPRESSED:
-        entry.packedIntsVersion = meta.readVInt();
-        entry.blockSize = meta.readVInt();
-        break;
-      default:
-        throw new CorruptIndexException("Unknown format: " + entry.format, meta);
-    }
-    entry.endOffset = meta.readLong();
-    return entry;
-  }
-  
-  static BinaryEntry readBinaryEntry(IndexInput meta) throws IOException {
-    BinaryEntry entry = new BinaryEntry();
-    entry.format = meta.readVInt();
-    entry.missingOffset = meta.readLong();
-    entry.minLength = meta.readVInt();
-    entry.maxLength = meta.readVInt();
-    entry.count = meta.readVLong();
-    entry.offset = meta.readLong();
-    switch(entry.format) {
-      case BINARY_FIXED_UNCOMPRESSED:
-        break;
-      case BINARY_PREFIX_COMPRESSED:
-        entry.addressInterval = meta.readVInt();
-        entry.addressesOffset = meta.readLong();
-        entry.packedIntsVersion = meta.readVInt();
-        entry.blockSize = meta.readVInt();
-        break;
-      case BINARY_VARIABLE_UNCOMPRESSED:
-        entry.addressesOffset = meta.readLong();
-        entry.packedIntsVersion = meta.readVInt();
-        entry.blockSize = meta.readVInt();
-        break;
-      default:
-        throw new CorruptIndexException("Unknown format: " + entry.format, meta);
-    }
-    return entry;
-  }
-
-  SortedSetEntry readSortedSetEntry(IndexInput meta) throws IOException {
-    SortedSetEntry entry = new SortedSetEntry();
-    entry.format = meta.readVInt();
-    if (entry.format != SORTED_SINGLE_VALUED && entry.format != SORTED_WITH_ADDRESSES) {
-      throw new CorruptIndexException("Unknown format: " + entry.format, meta);
-    }
-    return entry;
-  }
-
-  @Override
-  public NumericDocValues getNumeric(FieldInfo field) throws IOException {
-    NumericEntry entry = numerics.get(field.name);
-    return getNumeric(entry);
-  }
-  
-  @Override
-  public long ramBytesUsed() {
-    return ramBytesUsed.get();
-  }
-  
-  @Override
-  public synchronized Iterable<? extends Accountable> getChildResources() {
-    List<Accountable> resources = new ArrayList<>();
-    resources.addAll(Accountables.namedAccountables("addresses field", addressInstances));
-    resources.addAll(Accountables.namedAccountables("ord index field", ordIndexInstances));
-    return Collections.unmodifiableList(resources);
-  }
-  
-  @Override
-  public void checkIntegrity() throws IOException {
-    CodecUtil.checksumEntireFile(data);
-  }
-  
-  @Override
-  public String toString() {
-    return getClass().getSimpleName() + "(fields=" + numFields + ")";
-  }
-
-  LongValues getNumeric(NumericEntry entry) throws IOException {
-    RandomAccessInput slice = this.data.randomAccessSlice(entry.offset, entry.endOffset - entry.offset);
-    switch (entry.format) {
-      case DELTA_COMPRESSED:
-        final long delta = entry.minValue;
-        final LongValues values = DirectReader.getInstance(slice, entry.bitsPerValue);
-        return new LongValues() {
-          @Override
-          public long get(long id) {
-            return delta + values.get(id);
-          }
-        };
-      case GCD_COMPRESSED:
-        final long min = entry.minValue;
-        final long mult = entry.gcd;
-        final LongValues quotientReader = DirectReader.getInstance(slice, entry.bitsPerValue);
-        return new LongValues() {
-          @Override
-          public long get(long id) {
-            return min + mult * quotientReader.get(id);
-          }
-        };
-      case TABLE_COMPRESSED:
-        final long table[] = entry.table;
-        final LongValues ords = DirectReader.getInstance(slice, entry.bitsPerValue);
-        return new LongValues() {
-          @Override
-          public long get(long id) {
-            return table[(int) ords.get(id)];
-          }
-        };
-      default:
-        throw new AssertionError();
-    }
-  }
-
-  @Override
-  public BinaryDocValues getBinary(FieldInfo field) throws IOException {
-    BinaryEntry bytes = binaries.get(field.name);
-    switch(bytes.format) {
-      case BINARY_FIXED_UNCOMPRESSED:
-        return getFixedBinary(field, bytes);
-      case BINARY_VARIABLE_UNCOMPRESSED:
-        return getVariableBinary(field, bytes);
-      case BINARY_PREFIX_COMPRESSED:
-        return getCompressedBinary(field, bytes);
-      default:
-        throw new AssertionError();
-    }
-  }
-  
-  private BinaryDocValues getFixedBinary(FieldInfo field, final BinaryEntry bytes) {
-    final IndexInput data = this.data.clone();
-
-    return new LongBinaryDocValues() {
-      final BytesRef term;
-      {
-        term = new BytesRef(bytes.maxLength);
-        term.offset = 0;
-        term.length = bytes.maxLength;
-      }
-      
-      @Override
-      public BytesRef get(long id) {
-        long address = bytes.offset + id * bytes.maxLength;
-        try {
-          data.seek(address);
-          data.readBytes(term.bytes, 0, term.length);
-          return term;
-        } catch (IOException e) {
-          throw new RuntimeException(e);
-        }
-      }
-    };
-  }
-  
-  /** returns an address instance for variable-length binary values. */
-  private synchronized MonotonicBlockPackedReader getAddressInstance(IndexInput data, FieldInfo field, BinaryEntry bytes) throws IOException {
-    final MonotonicBlockPackedReader addresses;
-    MonotonicBlockPackedReader addrInstance = addressInstances.get(field.name);
-    if (addrInstance == null) {
-      data.seek(bytes.addressesOffset);
-      addrInstance = MonotonicBlockPackedReader.of(data, bytes.packedIntsVersion, bytes.blockSize, bytes.count+1, false);
-      if (!merging) {
-        addressInstances.put(field.name, addrInstance);
-        ramBytesUsed.addAndGet(addrInstance.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
-      }
-    }
-    addresses = addrInstance;
-    return addresses;
-  }
-  
-  private BinaryDocValues getVariableBinary(FieldInfo field, final BinaryEntry bytes) throws IOException {
-    final IndexInput data = this.data.clone();
-    
-    final MonotonicBlockPackedReader addresses = getAddressInstance(data, field, bytes);
-
-    return new LongBinaryDocValues() {
-      final BytesRef term = new BytesRef(Math.max(0, bytes.maxLength));
-      
-      @Override
-      public BytesRef get(long id) {
-        long startAddress = bytes.offset + addresses.get(id);
-        long endAddress = bytes.offset + addresses.get(id+1);
-        int length = (int) (endAddress - startAddress);
-        try {
-          data.seek(startAddress);
-          data.readBytes(term.bytes, 0, length);
-          term.length = length;
-          return term;
-        } catch (IOException e) {
-          throw new RuntimeException(e);
-        }
-      }
-    };
-  }
-  
-  /** returns an address instance for prefix-compressed binary values. */
-  private synchronized MonotonicBlockPackedReader getIntervalInstance(IndexInput data, FieldInfo field, BinaryEntry bytes) throws IOException {
-    final MonotonicBlockPackedReader addresses;
-    final long interval = bytes.addressInterval;
-    MonotonicBlockPackedReader addrInstance = addressInstances.get(field.name);
-    if (addrInstance == null) {
-      data.seek(bytes.addressesOffset);
-      final long size;
-      if (bytes.count % interval == 0) {
-        size = bytes.count / interval;
-      } else {
-        size = 1L + bytes.count / interval;
-      }
-      addrInstance = MonotonicBlockPackedReader.of(data, bytes.packedIntsVersion, bytes.blockSize, size, false);
-      if (!merging) {
-        addressInstances.put(field.name, addrInstance);
-        ramBytesUsed.addAndGet(addrInstance.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
-      }
-    }
-    addresses = addrInstance;
-    return addresses;
-  }
-
-
-  private BinaryDocValues getCompressedBinary(FieldInfo field, final BinaryEntry bytes) throws IOException {
-    final IndexInput data = this.data.clone();
-
-    final MonotonicBlockPackedReader addresses = getIntervalInstance(data, field, bytes);
-    
-    return new CompressedBinaryDocValues(bytes, addresses, data);
-  }
-
-  @Override
-  public SortedDocValues getSorted(FieldInfo field) throws IOException {
-    final int valueCount = (int) binaries.get(field.name).count;
-    final BinaryDocValues binary = getBinary(field);
-    NumericEntry entry = ords.get(field.name);
-    final LongValues ordinals = getNumeric(entry);
-    
-    return new SortedDocValues() {
-
-      @Override
-      public int getOrd(int docID) {
-        return (int) ordinals.get(docID);
-      }
-
-      @Override
-      public BytesRef lookupOrd(int ord) {
-        return binary.get(ord);
-      }
-
-      @Override
-      public int getValueCount() {
-        return valueCount;
-      }
-
-      @Override
-      public int lookupTerm(BytesRef key) {
-        if (binary instanceof CompressedBinaryDocValues) {
-          return (int) ((CompressedBinaryDocValues)binary).lookupTerm(key);
-        } else {
-        return super.lookupTerm(key);
-        }
-      }
-
-      @Override
-      public TermsEnum termsEnum() {
-        if (binary instanceof CompressedBinaryDocValues) {
-          return ((CompressedBinaryDocValues)binary).getTermsEnum();
-        } else {
-          return super.termsEnum();
-        }
-      }
-    };
-  }
-  
-  /** returns an address instance for sortedset ordinal lists */
-  private synchronized MonotonicBlockPackedReader getOrdIndexInstance(IndexInput data, FieldInfo field, NumericEntry entry) throws IOException {
-    final MonotonicBlockPackedReader ordIndex;
-    MonotonicBlockPackedReader ordIndexInstance = ordIndexInstances.get(field.name);
-    if (ordIndexInstance == null) {
-      data.seek(entry.offset);
-      ordIndexInstance = MonotonicBlockPackedReader.of(data, entry.packedIntsVersion, entry.blockSize, entry.count+1, false);
-      if (!merging) {
-        ordIndexInstances.put(field.name, ordIndexInstance);
-        ramBytesUsed.addAndGet(ordIndexInstance.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
-      }
-    }
-    ordIndex = ordIndexInstance;
-    return ordIndex;
-  }
-  
-  @Override
-  public SortedNumericDocValues getSortedNumeric(FieldInfo field) throws IOException {
-    SortedSetEntry ss = sortedNumerics.get(field.name);
-    NumericEntry numericEntry = numerics.get(field.name);
-    final LongValues values = getNumeric(numericEntry);
-    if (ss.format == SORTED_SINGLE_VALUED) {
-      final Bits docsWithField = getMissingBits(numericEntry.missingOffset);
-      return DocValues.singleton(values, docsWithField);
-    } else if (ss.format == SORTED_WITH_ADDRESSES) {
-      final IndexInput data = this.data.clone();
-      final MonotonicBlockPackedReader ordIndex = getOrdIndexInstance(data, field, ordIndexes.get(field.name));
-      
-      return new SortedNumericDocValues() {
-        long startOffset;
-        long endOffset;
-        
-        @Override
-        public void setDocument(int doc) {
-          startOffset = ordIndex.get(doc);
-          endOffset = ordIndex.get(doc+1L);
-        }
-
-        @Override
-        public long valueAt(int index) {
-          return values.get(startOffset + index);
-        }
-
-        @Override
-        public int count() {
-          return (int) (endOffset - startOffset);
-        }
-      };
-    } else {
-      throw new AssertionError();
-    }
-  }
-
-  @Override
-  public SortedSetDocValues getSortedSet(FieldInfo field) throws IOException {
-    SortedSetEntry ss = sortedSets.get(field.name);
-    if (ss.format == SORTED_SINGLE_VALUED) {
-      final SortedDocValues values = getSorted(field);
-      return DocValues.singleton(values);
-    } else if (ss.format != SORTED_WITH_ADDRESSES) {
-      throw new AssertionError();
-    }
-
-    final IndexInput data = this.data.clone();
-    final long valueCount = binaries.get(field.name).count;
-    // we keep the byte[]s and list of ords on disk, these could be large
-    final LongBinaryDocValues binary = (LongBinaryDocValues) getBinary(field);
-    final LongValues ordinals = getNumeric(ords.get(field.name));
-    // but the addresses to the ord stream are in RAM
-    final MonotonicBlockPackedReader ordIndex = getOrdIndexInstance(data, field, ordIndexes.get(field.name));
-    
-    return new RandomAccessOrds() {
-      long startOffset;
-      long offset;
-      long endOffset;
-      
-      @Override
-      public long nextOrd() {
-        if (offset == endOffset) {
-          return NO_MORE_ORDS;
-        } else {
-          long ord = ordinals.get(offset);
-          offset++;
-          return ord;
-        }
-      }
-
-      @Override
-      public void setDocument(int docID) {
-        startOffset = offset = ordIndex.get(docID);
-        endOffset = ordIndex.get(docID+1L);
-      }
-
-      @Override
-      public BytesRef lookupOrd(long ord) {
-        return binary.get(ord);
-      }
-
-      @Override
-      public long getValueCount() {
-        return valueCount;
-      }
-      
-      @Override
-      public long lookupTerm(BytesRef key) {
-        if (binary instanceof CompressedBinaryDocValues) {
-          return ((CompressedBinaryDocValues)binary).lookupTerm(key);
-        } else {
-          return super.lookupTerm(key);
-        }
-      }
-
-      @Override
-      public TermsEnum termsEnum() {
-        if (binary instanceof CompressedBinaryDocValues) {
-          return ((CompressedBinaryDocValues)binary).getTermsEnum();
-        } else {
-          return super.termsEnum();
-        }
-      }
-
-      @Override
-      public long ordAt(int index) {
-        return ordinals.get(startOffset + index);
-      }
-
-      @Override
-      public int cardinality() {
-        return (int) (endOffset - startOffset);
-      }
-    };
-  }
-  
-  private Bits getMissingBits(final long offset) throws IOException {
-    if (offset == -1) {
-      return new Bits.MatchAllBits(maxDoc);
-    } else {
-      int length = (int) ((maxDoc + 7L) >>> 3);
-      final RandomAccessInput in = data.randomAccessSlice(offset, length);
-      return new Bits() {
-        @Override
-        public boolean get(int index) {
-          try {
-            return (in.readByte(index >> 3) & (1 << (index & 7))) != 0;
-          } catch (IOException e) {
-            throw new RuntimeException(e);
-          }
-        }
-
-        @Override
-        public int length() {
-          return maxDoc;
-        }
-      };
-    }
-  }
-
-  @Override
-  public Bits getDocsWithField(FieldInfo field) throws IOException {
-    switch(field.getDocValuesType()) {
-      case SORTED_SET:
-        return DocValues.docsWithValue(getSortedSet(field), maxDoc);
-      case SORTED_NUMERIC:
-        return DocValues.docsWithValue(getSortedNumeric(field), maxDoc);
-      case SORTED:
-        return DocValues.docsWithValue(getSorted(field), maxDoc);
-      case BINARY:
-        BinaryEntry be = binaries.get(field.name);
-        return getMissingBits(be.missingOffset);
-      case NUMERIC:
-        NumericEntry ne = numerics.get(field.name);
-        return getMissingBits(ne.missingOffset);
-      default:
-        throw new AssertionError();
-    }
-  }
-
-  @Override
-  public void close() throws IOException {
-    data.close();
-  }
-  
-  @Override
-  public synchronized DocValuesProducer getMergeInstance() throws IOException {
-    return new Lucene49DocValuesProducer(this);
-  }
-
-  /** metadata entry for a numeric docvalues field */
-  static class NumericEntry {
-    private NumericEntry() {}
-    /** offset to the bitset representing docsWithField, or -1 if no documents have missing values */
-    long missingOffset;
-    /** offset to the actual numeric values */
-    public long offset;
-    /** end offset to the actual numeric values */
-    public long endOffset;
-    /** bits per value used to pack the numeric values */
-    public int bitsPerValue;
-
-    int format;
-    /** packed ints version used to encode these numerics */
-    public int packedIntsVersion;
-    /** count of values written */
-    public long count;
-    /** packed ints blocksize */
-    public int blockSize;
-    
-    long minValue;
-    long gcd;
-    long table[];
-  }
-  
-  /** metadata entry for a binary docvalues field */
-  static class BinaryEntry {
-    private BinaryEntry() {}
-    /** offset to the bitset representing docsWithField, or -1 if no documents have missing values */
-    long missingOffset;
-    /** offset to the actual binary values */
-    long offset;
-
-    int format;
-    /** count of values written */
-    public long count;
-    int minLength;
-    int maxLength;
-    /** offset to the addressing data that maps a value to its slice of the byte[] */
-    public long addressesOffset;
-    /** interval of shared prefix chunks (when using prefix-compressed binary) */
-    public long addressInterval;
-    /** packed ints version used to encode addressing information */
-    public int packedIntsVersion;
-    /** packed ints blocksize */
-    public int blockSize;
-  }
-
-  /** metadata entry for a sorted-set docvalues field */
-  static class SortedSetEntry {
-    private SortedSetEntry() {}
-    int format;
-  }
-
-  // internally we compose complex dv (sorted/sortedset) from other ones
-  static abstract class LongBinaryDocValues extends BinaryDocValues {
-    @Override
-    public final BytesRef get(int docID) {
-      return get((long)docID);
-    }
-    
-    abstract BytesRef get(long id);
-  }
-  
-  // in the compressed case, we add a few additional operations for
-  // more efficient reverse lookup and enumeration
-  static class CompressedBinaryDocValues extends LongBinaryDocValues {
-    final BinaryEntry bytes;
-    final long interval;
-    final long numValues;
-    final long numIndexValues;
-    final MonotonicBlockPackedReader addresses;
-    final IndexInput data;
-    final TermsEnum termsEnum;
-    
-    public CompressedBinaryDocValues(BinaryEntry bytes, MonotonicBlockPackedReader addresses, IndexInput data) throws IOException {
-      this.bytes = bytes;
-      this.interval = bytes.addressInterval;
-      this.addresses = addresses;
-      this.data = data;
-      this.numValues = bytes.count;
-      this.numIndexValues = addresses.size();
-      this.termsEnum = getTermsEnum(data);
-    }
-    
-    @Override
-    public BytesRef get(long id) {
-      try {
-        termsEnum.seekExact(id);
-        return termsEnum.term();
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      }
-    }
-    
-    long lookupTerm(BytesRef key) {
-      try {
-        SeekStatus status = termsEnum.seekCeil(key);
-        if (status == SeekStatus.END) {
-          return -numValues-1;
-        } else if (status == SeekStatus.FOUND) {
-          return termsEnum.ord();
-        } else {
-          return -termsEnum.ord()-1;
-        }
-      } catch (IOException bogus) {
-        throw new RuntimeException(bogus);
-      }
-    }
-    
-    TermsEnum getTermsEnum() {
-      try {
-        return getTermsEnum(data.clone());
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      }
-    }
-    
-    private TermsEnum getTermsEnum(final IndexInput input) throws IOException {
-      input.seek(bytes.offset);
-      
-      return new TermsEnum() {
-        private long currentOrd = -1;
-        // TODO: maxLength is negative when all terms are merged away...
-        private final BytesRef term = new BytesRef(bytes.maxLength < 0 ? 0 : bytes.maxLength);
-
-        @Override
-        public BytesRef next() throws IOException {
-          if (++currentOrd >= numValues) {
-            return null;
-          } else {
-            int start = input.readVInt();
-            int suffix = input.readVInt();
-            input.readBytes(term.bytes, start, suffix);
-            term.length = start + suffix;
-            return term;
-          }
-        }
-
-        @Override
-        public SeekStatus seekCeil(BytesRef text) throws IOException {
-          // binary-search just the index values to find the block,
-          // then scan within the block
-          long low = 0;
-          long high = numIndexValues-1;
-
-          while (low <= high) {
-            long mid = (low + high) >>> 1;
-            seekExact(mid * interval);
-            int cmp = term.compareTo(text);
-
-            if (cmp < 0) {
-              low = mid + 1;
-            } else if (cmp > 0) {
-              high = mid - 1;
-            } else {
-              // we got lucky, found an indexed term
-              return SeekStatus.FOUND;
-            }
-          }
-          
-          if (numIndexValues == 0) {
-            return SeekStatus.END;
-          }
-          
-          // block before insertion point
-          long block = low-1;
-          seekExact(block < 0 ? -1 : block * interval);
-          
-          while (next() != null) {
-            int cmp = term.compareTo(text);
-            if (cmp == 0) {
-              return SeekStatus.FOUND;
-            } else if (cmp > 0) {
-              return SeekStatus.NOT_FOUND;
-            }
-          }
-          
-          return SeekStatus.END;
-        }
-
-        @Override
-        public void seekExact(long ord) throws IOException {
-          long block = ord / interval;
-
-          if (ord >= currentOrd && block == currentOrd / interval) {
-            // seek within current block
-          } else {
-            // position before start of block
-            currentOrd = ord - ord % interval - 1;
-            input.seek(bytes.offset + addresses.get(block));
-          }
-          
-          while (currentOrd < ord) {
-            next();
-          }
-        }
-
-        @Override
-        public BytesRef term() throws IOException {
-          return term;
-        }
-
-        @Override
-        public long ord() throws IOException {
-          return currentOrd;
-        }
-
-        @Override
-        public int docFreq() throws IOException {
-          throw new UnsupportedOperationException();
-        }
-
-        @Override
-        public long totalTermFreq() throws IOException {
-          return -1;
-        }
-
-        @Override
-        public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
-          throw new UnsupportedOperationException();
-        }
-
-        @Override
-        public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
-          throw new UnsupportedOperationException();
-        }
-      };
-    }
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49NormsFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49NormsFormat.java
deleted file mode 100644
index f9b5fa7..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49NormsFormat.java
+++ /dev/null
@@ -1,59 +0,0 @@
-package org.apache.lucene.codecs.lucene49;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.NormsConsumer;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.NormsProducer;
-import org.apache.lucene.codecs.UndeadNormsProducer;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-
-/**
- * Lucene 4.9 Score normalization format.
- * @deprecated only for reading 4.9/4.10 indexes
- */
-@Deprecated
-public class Lucene49NormsFormat extends NormsFormat {
-
-  /** Sole Constructor */
-  public Lucene49NormsFormat() {}
-  
-  @Override
-  public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
-    throw new UnsupportedOperationException("this codec can only be used for reading");
-  }
-
-  @Override
-  public final NormsProducer normsProducer(SegmentReadState state) throws IOException {
-    if (UndeadNormsProducer.isUndeadArmy(state.fieldInfos)) {
-      return UndeadNormsProducer.INSTANCE;
-    } else {
-      return new Lucene49NormsProducer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION);
-    }
-  }
-  
-  static final String DATA_CODEC = "Lucene49NormsData";
-  static final String DATA_EXTENSION = "nvd";
-  static final String METADATA_CODEC = "Lucene49NormsMetadata";
-  static final String METADATA_EXTENSION = "nvm";
-  static final int VERSION_START = 0;
-  static final int VERSION_CURRENT = VERSION_START;
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49NormsProducer.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49NormsProducer.java
deleted file mode 100644
index aab8458..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49NormsProducer.java
+++ /dev/null
@@ -1,275 +0,0 @@
-package org.apache.lucene.codecs.lucene49;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-import java.util.concurrent.atomic.AtomicInteger;
-import java.util.concurrent.atomic.AtomicLong;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.NormsProducer;
-import org.apache.lucene.codecs.UndeadNormsProducer;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.NumericDocValues;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.store.ChecksumIndexInput;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.Accountable;
-import org.apache.lucene.util.Accountables;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.RamUsageEstimator;
-import org.apache.lucene.util.packed.BlockPackedReader;
-import org.apache.lucene.util.packed.PackedInts;
-
-import static org.apache.lucene.codecs.lucene49.Lucene49NormsFormat.VERSION_CURRENT;
-import static org.apache.lucene.codecs.lucene49.Lucene49NormsFormat.VERSION_START;
-
-/**
- * Reader for 4.9 norms
- * @deprecated only for reading 4.9/4.10 indexes
- */
-@Deprecated
-final class Lucene49NormsProducer extends NormsProducer {
-  static final byte DELTA_COMPRESSED = 0;
-  static final byte TABLE_COMPRESSED = 1;
-  static final byte CONST_COMPRESSED = 2;
-  static final byte UNCOMPRESSED = 3;
-  static final int BLOCK_SIZE = 16384;
-  
-  // metadata maps (just file pointers and minimal stuff)
-  private final Map<String,NormsEntry> norms = new HashMap<>();
-  private final IndexInput data;
-  
-  // ram instances we have already loaded
-  final Map<String,NumericDocValues> instances = new HashMap<>();
-  final Map<String,Accountable> instancesInfo = new HashMap<>();
-  
-  private final int maxDoc;
-  private final AtomicLong ramBytesUsed;
-  private final AtomicInteger activeCount = new AtomicInteger();
-  
-  private final boolean merging;
-  
-  // clone for merge: when merging we don't do any instances.put()s
-  Lucene49NormsProducer(Lucene49NormsProducer original) {
-    assert Thread.holdsLock(original);
-    norms.putAll(original.norms);
-    data = original.data.clone();
-    instances.putAll(original.instances);
-    instancesInfo.putAll(original.instancesInfo);
-    maxDoc = original.maxDoc;
-    ramBytesUsed = new AtomicLong(original.ramBytesUsed.get());
-    activeCount.set(original.activeCount.get());
-    merging = true;
-  }
-    
-  Lucene49NormsProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
-    maxDoc = state.segmentInfo.getDocCount();
-    merging = false;
-    String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
-    ramBytesUsed = new AtomicLong(RamUsageEstimator.shallowSizeOfInstance(getClass()));
-    int version = -1;
-    
-    // read in the entries from the metadata file.
-    try (ChecksumIndexInput in = state.directory.openChecksumInput(metaName, state.context)) {
-      Throwable priorE = null;
-      try {
-        version = CodecUtil.checkHeader(in, metaCodec, VERSION_START, VERSION_CURRENT);
-        readFields(in, state.fieldInfos);
-      } catch (Throwable exception) {
-        priorE = exception;
-      } finally {
-        CodecUtil.checkFooter(in, priorE);
-      }
-    }
-
-    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
-    this.data = state.directory.openInput(dataName, state.context);
-    boolean success = false;
-    try {
-      final int version2 = CodecUtil.checkHeader(data, dataCodec, VERSION_START, VERSION_CURRENT);
-      if (version != version2) {
-        throw new CorruptIndexException("Format versions mismatch: meta=" + version + ",data=" + version2, data);
-      }
-      
-      // NOTE: data file is too costly to verify checksum against all the bytes on open,
-      // but for now we at least verify proper structure of the checksum footer: which looks
-      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
-      // such as file truncation.
-      CodecUtil.retrieveChecksum(data);
-
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(this.data);
-      }
-    }
-  }
-  
-  private void readFields(IndexInput meta, FieldInfos infos) throws IOException {
-    int fieldNumber = meta.readVInt();
-    while (fieldNumber != -1) {
-      FieldInfo info = infos.fieldInfo(fieldNumber);
-      if (info == null) {
-        throw new CorruptIndexException("Invalid field number: " + fieldNumber, meta);
-      } else if (!info.hasNorms()) {
-        throw new CorruptIndexException("Invalid field: " + info.name, meta);
-      }
-      NormsEntry entry = new NormsEntry();
-      entry.format = meta.readByte();
-      entry.offset = meta.readLong();
-      switch(entry.format) {
-        case CONST_COMPRESSED:
-        case UNCOMPRESSED:
-        case TABLE_COMPRESSED:
-        case DELTA_COMPRESSED:
-          break;
-        default:
-          throw new CorruptIndexException("Unknown format: " + entry.format, meta);
-      }
-      norms.put(info.name, entry);
-      fieldNumber = meta.readVInt();
-    }
-  }
-
-  @Override
-  public synchronized NumericDocValues getNorms(FieldInfo field) throws IOException {
-    if (UndeadNormsProducer.isUndead(field)) {
-      // Bring undead norms back to life; this is set in Lucene46FieldInfosFormat, to emulate pre-5.0 undead norms
-      return DocValues.emptyNumeric();
-    }
-    NumericDocValues instance = instances.get(field.name);
-    if (instance == null) {
-      instance = loadNorms(field);
-      if (!merging) {
-        instances.put(field.name, instance);
-        activeCount.incrementAndGet();
-      }
-    }
-    return instance;
-  }
-  
-  @Override
-  public long ramBytesUsed() {
-    return ramBytesUsed.get();
-  }
-  
-  @Override
-  public synchronized Iterable<? extends Accountable> getChildResources() {
-    return Accountables.namedAccountables("field", instancesInfo);
-  }
-  
-  @Override
-  public void checkIntegrity() throws IOException {
-    CodecUtil.checksumEntireFile(data);
-  }
-
-  private NumericDocValues loadNorms(FieldInfo field) throws IOException {
-    NormsEntry entry = norms.get(field.name);
-    switch(entry.format) {
-      case CONST_COMPRESSED:
-        if (!merging) {
-          instancesInfo.put(field.name, Accountables.namedAccountable("constant", 8));
-          ramBytesUsed.addAndGet(8);
-        }
-        final long v = entry.offset;
-        return new NumericDocValues() {
-          @Override
-          public long get(int docID) {
-            return v;
-          }
-        };
-      case UNCOMPRESSED:
-        data.seek(entry.offset);
-        final byte bytes[] = new byte[maxDoc];
-        data.readBytes(bytes, 0, bytes.length);
-        if (!merging) {
-          ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(bytes));
-          instancesInfo.put(field.name, Accountables.namedAccountable("byte array", maxDoc));
-        }
-        return new NumericDocValues() {
-          @Override
-          public long get(int docID) {
-            return bytes[docID];
-          }
-        };
-      case DELTA_COMPRESSED:
-        data.seek(entry.offset);
-        int packedIntsVersion = data.readVInt();
-        int blockSize = data.readVInt();
-        final BlockPackedReader reader = new BlockPackedReader(data, packedIntsVersion, blockSize, maxDoc, false);
-        if (!merging) {
-          ramBytesUsed.addAndGet(reader.ramBytesUsed());
-          instancesInfo.put(field.name, Accountables.namedAccountable("delta compressed", reader));
-        }
-        return reader;
-      case TABLE_COMPRESSED:
-        data.seek(entry.offset);
-        int packedVersion = data.readVInt();
-        int size = data.readVInt();
-        if (size > 256) {
-          throw new CorruptIndexException("TABLE_COMPRESSED cannot have more than 256 distinct values, got=" + size, data);
-        }
-        final long decode[] = new long[size];
-        for (int i = 0; i < decode.length; i++) {
-          decode[i] = data.readLong();
-        }
-        final int formatID = data.readVInt();
-        final int bitsPerValue = data.readVInt();
-        final PackedInts.Reader ordsReader = PackedInts.getReaderNoHeader(data, PackedInts.Format.byId(formatID), packedVersion, maxDoc, bitsPerValue);
-        if (!merging) {
-          ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(decode) + ordsReader.ramBytesUsed());
-          instancesInfo.put(field.name, Accountables.namedAccountable("table compressed", ordsReader));
-        }
-        return new NumericDocValues() {
-          @Override
-          public long get(int docID) {
-            return decode[(int)ordsReader.get(docID)];
-          }
-        };
-      default:
-        throw new AssertionError();
-    }
-  }
-
-  @Override
-  public void close() throws IOException {
-    data.close();
-  }
-  
-  @Override
-  public synchronized NormsProducer getMergeInstance() throws IOException {
-    return new Lucene49NormsProducer(this);
-  }
-
-  static class NormsEntry {
-    byte format;
-    long offset;
-  }
-
-  @Override
-  public String toString() {
-    return getClass().getSimpleName() + "(fields=" + norms.size() + ",active=" + activeCount.get() + ")";
-  }
-}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/package.html b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/package.html
deleted file mode 100644
index 35c7c09..0000000
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Lucene 4.9 file format.
-</body>
-</html>
diff --git a/lucene/backward-codecs/src/resources/META-INF/services/org.apache.lucene.codecs.Codec b/lucene/backward-codecs/src/resources/META-INF/services/org.apache.lucene.codecs.Codec
index 8dfd608..cccdbfa 100644
--- a/lucene/backward-codecs/src/resources/META-INF/services/org.apache.lucene.codecs.Codec
+++ b/lucene/backward-codecs/src/resources/META-INF/services/org.apache.lucene.codecs.Codec
@@ -13,11 +13,4 @@
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
-org.apache.lucene.codecs.lucene40.Lucene40Codec
-org.apache.lucene.codecs.lucene41.Lucene41Codec
-org.apache.lucene.codecs.lucene42.Lucene42Codec
-org.apache.lucene.codecs.lucene45.Lucene45Codec
-org.apache.lucene.codecs.lucene46.Lucene46Codec
-org.apache.lucene.codecs.lucene49.Lucene49Codec
-org.apache.lucene.codecs.lucene410.Lucene410Codec
 
diff --git a/lucene/backward-codecs/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat b/lucene/backward-codecs/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat
index 574c9c5..cccdbfa 100644
--- a/lucene/backward-codecs/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat
+++ b/lucene/backward-codecs/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat
@@ -13,8 +13,4 @@
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
-org.apache.lucene.codecs.lucene42.Lucene42DocValuesFormat
-org.apache.lucene.codecs.lucene45.Lucene45DocValuesFormat
-org.apache.lucene.codecs.lucene49.Lucene49DocValuesFormat
-org.apache.lucene.codecs.lucene410.Lucene410DocValuesFormat
 
diff --git a/lucene/backward-codecs/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat b/lucene/backward-codecs/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
index 023d9c9..4a812de 100644
--- a/lucene/backward-codecs/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
+++ b/lucene/backward-codecs/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
@@ -13,5 +13,3 @@
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
-org.apache.lucene.codecs.lucene40.Lucene40PostingsFormat
-org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/blocktree/Lucene40BlockTreeTermsWriter.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/blocktree/Lucene40BlockTreeTermsWriter.java
deleted file mode 100644
index 53a6448..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/blocktree/Lucene40BlockTreeTermsWriter.java
+++ /dev/null
@@ -1,1022 +0,0 @@
-package org.apache.lucene.codecs.blocktree;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.codecs.BlockTermState;
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.FieldsConsumer;
-import org.apache.lucene.codecs.PostingsWriterBase;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.RAMOutputStream;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.BytesRefBuilder;
-import org.apache.lucene.util.FixedBitSet;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.IntsRefBuilder;
-import org.apache.lucene.util.StringHelper;
-import org.apache.lucene.util.fst.Builder;
-import org.apache.lucene.util.fst.ByteSequenceOutputs;
-import org.apache.lucene.util.fst.BytesRefFSTEnum;
-import org.apache.lucene.util.fst.FST;
-import org.apache.lucene.util.fst.Outputs;
-import org.apache.lucene.util.fst.Util;
-import org.apache.lucene.util.packed.PackedInts;
-
-/*
-  TODO:
-  
-    - Currently there is a one-to-one mapping of indexed
-      term to term block, but we could decouple the two, ie,
-      put more terms into the index than there are blocks.
-      The index would take up more RAM but then it'd be able
-      to avoid seeking more often and could make PK/FuzzyQ
-      faster if the additional indexed terms could store
-      the offset into the terms block.
-
-    - The blocks are not written in true depth-first
-      order, meaning if you just next() the file pointer will
-      sometimes jump backwards.  For example, block foo* will
-      be written before block f* because it finished before.
-      This could possibly hurt performance if the terms dict is
-      not hot, since OSs anticipate sequential file access.  We
-      could fix the writer to re-order the blocks as a 2nd
-      pass.
-
-    - Each block encodes the term suffixes packed
-      sequentially using a separate vInt per term, which is
-      1) wasteful and 2) slow (must linear scan to find a
-      particular suffix).  We should instead 1) make
-      random-access array so we can directly access the Nth
-      suffix, and 2) bulk-encode this array using bulk int[]
-      codecs; then at search time we can binary search when
-      we seek a particular term.
-*/
-
-/**
- * Block-based terms index and dictionary writer.
- * <p>
- * Writes terms dict and index, block-encoding (column
- * stride) each term's metadata for each set of terms
- * between two index terms.
- * <p>
- * Files:
- * <ul>
- *   <li><tt>.tim</tt>: <a href="#Termdictionary">Term Dictionary</a></li>
- *   <li><tt>.tip</tt>: <a href="#Termindex">Term Index</a></li>
- * </ul>
- * <p>
- * <a name="Termdictionary" id="Termdictionary"></a>
- * <h3>Term Dictionary</h3>
- *
- * <p>The .tim file contains the list of terms in each
- * field along with per-term statistics (such as docfreq)
- * and per-term metadata (typically pointers to the postings list
- * for that term in the inverted index).
- * </p>
- *
- * <p>The .tim is arranged in blocks: with blocks containing
- * a variable number of entries (by default 25-48), where
- * each entry is either a term or a reference to a
- * sub-block.</p>
- *
- * <p>NOTE: The term dictionary can plug into different postings implementations:
- * the postings writer/reader are actually responsible for encoding 
- * and decoding the Postings Metadata and Term Metadata sections.</p>
- *
- * <ul>
- *    <li>TermsDict (.tim) --&gt; Header, <i>PostingsHeader</i>, NodeBlock<sup>NumBlocks</sup>,
- *                               FieldSummary, DirOffset, Footer</li>
- *    <li>NodeBlock --&gt; (OuterNode | InnerNode)</li>
- *    <li>OuterNode --&gt; EntryCount, SuffixLength, Byte<sup>SuffixLength</sup>, StatsLength, &lt; TermStats &gt;<sup>EntryCount</sup>, MetaLength, &lt;<i>TermMetadata</i>&gt;<sup>EntryCount</sup></li>
- *    <li>InnerNode --&gt; EntryCount, SuffixLength[,Sub?], Byte<sup>SuffixLength</sup>, StatsLength, &lt; TermStats ? &gt;<sup>EntryCount</sup>, MetaLength, &lt;<i>TermMetadata ? </i>&gt;<sup>EntryCount</sup></li>
- *    <li>TermStats --&gt; DocFreq, TotalTermFreq </li>
- *    <li>FieldSummary --&gt; NumFields, &lt;FieldNumber, NumTerms, RootCodeLength, Byte<sup>RootCodeLength</sup>,
- *                            SumTotalTermFreq?, SumDocFreq, DocCount, LongsSize, MinTerm, MaxTerm&gt;<sup>NumFields</sup></li>
- *    <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- *    <li>DirOffset --&gt; {@link DataOutput#writeLong Uint64}</li>
- *    <li>MinTerm,MaxTerm --&gt; {@link DataOutput#writeVInt VInt} length followed by the byte[]</li>
- *    <li>EntryCount,SuffixLength,StatsLength,DocFreq,MetaLength,NumFields,
- *        FieldNumber,RootCodeLength,DocCount,LongsSize --&gt; {@link DataOutput#writeVInt VInt}</li>
- *    <li>TotalTermFreq,NumTerms,SumTotalTermFreq,SumDocFreq --&gt; 
- *        {@link DataOutput#writeVLong VLong}</li>
- *    <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
- * </ul>
- * <p>Notes:</p>
- * <ul>
- *    <li>Header is a {@link CodecUtil#writeHeader CodecHeader} storing the version information
- *        for the BlockTree implementation.</li>
- *    <li>DirOffset is a pointer to the FieldSummary section.</li>
- *    <li>DocFreq is the count of documents which contain the term.</li>
- *    <li>TotalTermFreq is the total number of occurrences of the term. This is encoded
- *        as the difference between the total number of occurrences and the DocFreq.</li>
- *    <li>FieldNumber is the fields number from {@link FieldInfos}. (.fnm)</li>
- *    <li>NumTerms is the number of unique terms for the field.</li>
- *    <li>RootCode points to the root block for the field.</li>
- *    <li>SumDocFreq is the total number of postings, the number of term-document pairs across
- *        the entire field.</li>
- *    <li>DocCount is the number of documents that have at least one posting for this field.</li>
- *    <li>LongsSize records how many long values the postings writer/reader record per term
- *        (e.g., to hold freq/prox/doc file offsets).
- *    <li>MinTerm, MaxTerm are the lowest and highest term in this field.</li>
- *    <li>PostingsHeader and TermMetadata are plugged into by the specific postings implementation:
- *        these contain arbitrary per-file data (such as parameters or versioning information) 
- *        and per-term data (such as pointers to inverted files).</li>
- *    <li>For inner nodes of the tree, every entry will steal one bit to mark whether it points
- *        to child nodes(sub-block). If so, the corresponding TermStats and TermMetaData are omitted </li>
- * </ul>
- * <a name="Termindex" id="Termindex"></a>
- * <h3>Term Index</h3>
- * <p>The .tip file contains an index into the term dictionary, so that it can be 
- * accessed randomly.  The index is also used to determine
- * when a given term cannot exist on disk (in the .tim file), saving a disk seek.</p>
- * <ul>
- *   <li>TermsIndex (.tip) --&gt; Header, FSTIndex<sup>NumFields</sup>
- *                                &lt;IndexStartFP&gt;<sup>NumFields</sup>, DirOffset, Footer</li>
- *   <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- *   <li>DirOffset --&gt; {@link DataOutput#writeLong Uint64}</li>
- *   <li>IndexStartFP --&gt; {@link DataOutput#writeVLong VLong}</li>
- *   <!-- TODO: better describe FST output here -->
- *   <li>FSTIndex --&gt; {@link FST FST&lt;byte[]&gt;}</li>
- *   <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
- * </ul>
- * <p>Notes:</p>
- * <ul>
- *   <li>The .tip file contains a separate FST for each
- *       field.  The FST maps a term prefix to the on-disk
- *       block that holds all terms starting with that
- *       prefix.  Each field's IndexStartFP points to its
- *       FST.</li>
- *   <li>DirOffset is a pointer to the start of the IndexStartFPs
- *       for all fields</li>
- *   <li>It's possible that an on-disk block would contain
- *       too many terms (more than the allowed maximum
- *       (default: 48)).  When this happens, the block is
- *       sub-divided into new blocks (called "floor
- *       blocks"), and then the output in the FST for the
- *       block's prefix encodes the leading byte of each
- *       sub-block, and its file pointer.
- * </ul>
- *
- * @see Lucene40BlockTreeTermsReader
- * @lucene.experimental
- * @deprecated Only for 4.x backcompat
- */
-@Deprecated
-public final class Lucene40BlockTreeTermsWriter extends FieldsConsumer {
-
-  /** Suggested default value for the {@code
-   *  minItemsInBlock} parameter to {@link
-   *  #Lucene40BlockTreeTermsWriter(SegmentWriteState,PostingsWriterBase,int,int)}. */
-  public final static int DEFAULT_MIN_BLOCK_SIZE = 25;
-
-  /** Suggested default value for the {@code
-   *  maxItemsInBlock} parameter to {@link
-   *  #Lucene40BlockTreeTermsWriter(SegmentWriteState,PostingsWriterBase,int,int)}. */
-  public final static int DEFAULT_MAX_BLOCK_SIZE = 48;
-
-  // public final static boolean DEBUG = false;
-  //private final static boolean SAVE_DOT_FILES = false;
-
-  private final IndexOutput out;
-  private final IndexOutput indexOut;
-  final int maxDoc;
-  final int minItemsInBlock;
-  final int maxItemsInBlock;
-
-  final PostingsWriterBase postingsWriter;
-  final FieldInfos fieldInfos;
-
-  private static class FieldMetaData {
-    public final FieldInfo fieldInfo;
-    public final BytesRef rootCode;
-    public final long numTerms;
-    public final long indexStartFP;
-    public final long sumTotalTermFreq;
-    public final long sumDocFreq;
-    public final int docCount;
-    private final int longsSize;
-    public final BytesRef minTerm;
-    public final BytesRef maxTerm;
-
-    public FieldMetaData(FieldInfo fieldInfo, BytesRef rootCode, long numTerms, long indexStartFP, long sumTotalTermFreq, long sumDocFreq, int docCount, int longsSize,
-                         BytesRef minTerm, BytesRef maxTerm) {
-      assert numTerms > 0;
-      this.fieldInfo = fieldInfo;
-      assert rootCode != null: "field=" + fieldInfo.name + " numTerms=" + numTerms;
-      this.rootCode = rootCode;
-      this.indexStartFP = indexStartFP;
-      this.numTerms = numTerms;
-      this.sumTotalTermFreq = sumTotalTermFreq;
-      this.sumDocFreq = sumDocFreq;
-      this.docCount = docCount;
-      this.longsSize = longsSize;
-      this.minTerm = minTerm;
-      this.maxTerm = maxTerm;
-    }
-  }
-
-  private final List<FieldMetaData> fields = new ArrayList<>();
-
-  // private final String segment;
-
-  /** Create a new writer.  The number of items (terms or
-   *  sub-blocks) per block will aim to be between
-   *  minItemsPerBlock and maxItemsPerBlock, though in some
-   *  cases the blocks may be smaller than the min. */
-  public Lucene40BlockTreeTermsWriter(
-      SegmentWriteState state,
-      PostingsWriterBase postingsWriter,
-      int minItemsInBlock,
-      int maxItemsInBlock)
-    throws IOException
-  {
-    if (minItemsInBlock <= 1) {
-      throw new IllegalArgumentException("minItemsInBlock must be >= 2; got " + minItemsInBlock);
-    }
-    if (maxItemsInBlock <= 0) {
-      throw new IllegalArgumentException("maxItemsInBlock must be >= 1; got " + maxItemsInBlock);
-    }
-    if (minItemsInBlock > maxItemsInBlock) {
-      throw new IllegalArgumentException("maxItemsInBlock must be >= minItemsInBlock; got maxItemsInBlock=" + maxItemsInBlock + " minItemsInBlock=" + minItemsInBlock);
-    }
-    if (2*(minItemsInBlock-1) > maxItemsInBlock) {
-      throw new IllegalArgumentException("maxItemsInBlock must be at least 2*(minItemsInBlock-1); got maxItemsInBlock=" + maxItemsInBlock + " minItemsInBlock=" + minItemsInBlock);
-    }
-
-    maxDoc = state.segmentInfo.getDocCount();
-
-    final String termsFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, Lucene40BlockTreeTermsReader.TERMS_EXTENSION);
-    out = state.directory.createOutput(termsFileName, state.context);
-    boolean success = false;
-    IndexOutput indexOut = null;
-    try {
-      fieldInfos = state.fieldInfos;
-      this.minItemsInBlock = minItemsInBlock;
-      this.maxItemsInBlock = maxItemsInBlock;
-      writeHeader(out);
-
-      //DEBUG = state.segmentName.equals("_4a");
-
-      final String termsIndexFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, Lucene40BlockTreeTermsReader.TERMS_INDEX_EXTENSION);
-      indexOut = state.directory.createOutput(termsIndexFileName, state.context);
-      writeIndexHeader(indexOut);
-
-      this.postingsWriter = postingsWriter;
-      // segment = state.segmentInfo.name;
-
-      // System.out.println("BTW.init seg=" + state.segmentName);
-
-      postingsWriter.init(out, state);                          // have consumer write its format/header
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(out, indexOut);
-      }
-    }
-    this.indexOut = indexOut;
-  }
-
-  /** Writes the terms file header. */
-  private void writeHeader(IndexOutput out) throws IOException {
-    CodecUtil.writeHeader(out, Lucene40BlockTreeTermsReader.TERMS_CODEC_NAME, Lucene40BlockTreeTermsReader.VERSION_CURRENT);   
-  }
-
-  /** Writes the index file header. */
-  private void writeIndexHeader(IndexOutput out) throws IOException {
-    CodecUtil.writeHeader(out, Lucene40BlockTreeTermsReader.TERMS_INDEX_CODEC_NAME, Lucene40BlockTreeTermsReader.VERSION_CURRENT); 
-  }
-
-  /** Writes the terms file trailer. */
-  private void writeTrailer(IndexOutput out, long dirStart) throws IOException {
-    out.writeLong(dirStart);    
-  }
-
-  /** Writes the index file trailer. */
-  private void writeIndexTrailer(IndexOutput indexOut, long dirStart) throws IOException {
-    indexOut.writeLong(dirStart);    
-  }
-
-  @Override
-  public void write(Fields fields) throws IOException {
-
-    String lastField = null;
-    for(String field : fields) {
-      assert lastField == null || lastField.compareTo(field) < 0;
-      lastField = field;
-
-      Terms terms = fields.terms(field);
-      if (terms == null) {
-        continue;
-      }
-
-      TermsEnum termsEnum = terms.iterator(null);
-
-      TermsWriter termsWriter = new TermsWriter(fieldInfos.fieldInfo(field));
-      while (true) {
-        BytesRef term = termsEnum.next();
-        if (term == null) {
-          break;
-        }
-        termsWriter.write(term, termsEnum);
-      }
-
-      termsWriter.finish();
-    }
-  }
-  
-  static long encodeOutput(long fp, boolean hasTerms, boolean isFloor) {
-    assert fp < (1L << 62);
-    return (fp << 2) | (hasTerms ? Lucene40BlockTreeTermsReader.OUTPUT_FLAG_HAS_TERMS : 0) | (isFloor ? Lucene40BlockTreeTermsReader.OUTPUT_FLAG_IS_FLOOR : 0);
-  }
-
-  private static class PendingEntry {
-    public final boolean isTerm;
-
-    protected PendingEntry(boolean isTerm) {
-      this.isTerm = isTerm;
-    }
-  }
-
-  private static final class PendingTerm extends PendingEntry {
-    public final byte[] termBytes;
-    // stats + metadata
-    public final BlockTermState state;
-
-    public PendingTerm(BytesRef term, BlockTermState state) {
-      super(true);
-      this.termBytes = new byte[term.length];
-      System.arraycopy(term.bytes, term.offset, termBytes, 0, term.length);
-      this.state = state;
-    }
-
-    @Override
-    public String toString() {
-      return brToString(termBytes);
-    }
-  }
-
-  // for debugging
-  @SuppressWarnings("unused")
-  static String brToString(BytesRef b) {
-    try {
-      return b.utf8ToString() + " " + b;
-    } catch (Throwable t) {
-      // If BytesRef isn't actually UTF8, or it's eg a
-      // prefix of UTF8 that ends mid-unicode-char, we
-      // fallback to hex:
-      return b.toString();
-    }
-  }
-
-  // for debugging
-  @SuppressWarnings("unused")
-  static String brToString(byte[] b) {
-    return brToString(new BytesRef(b));
-  }
-
-  private static final class PendingBlock extends PendingEntry {
-    public final BytesRef prefix;
-    public final long fp;
-    public FST<BytesRef> index;
-    public List<FST<BytesRef>> subIndices;
-    public final boolean hasTerms;
-    public final boolean isFloor;
-    public final int floorLeadByte;
-
-    public PendingBlock(BytesRef prefix, long fp, boolean hasTerms, boolean isFloor, int floorLeadByte, List<FST<BytesRef>> subIndices) {
-      super(false);
-      this.prefix = prefix;
-      this.fp = fp;
-      this.hasTerms = hasTerms;
-      this.isFloor = isFloor;
-      this.floorLeadByte = floorLeadByte;
-      this.subIndices = subIndices;
-    }
-
-    @Override
-    public String toString() {
-      return "BLOCK: " + brToString(prefix);
-    }
-
-    public void compileIndex(List<PendingBlock> blocks, RAMOutputStream scratchBytes, IntsRefBuilder scratchIntsRef) throws IOException {
-
-      assert (isFloor && blocks.size() > 1) || (isFloor == false && blocks.size() == 1): "isFloor=" + isFloor + " blocks=" + blocks;
-      assert this == blocks.get(0);
-
-      assert scratchBytes.getFilePointer() == 0;
-
-      // TODO: try writing the leading vLong in MSB order
-      // (opposite of what Lucene does today), for better
-      // outputs sharing in the FST
-      scratchBytes.writeVLong(encodeOutput(fp, hasTerms, isFloor));
-      if (isFloor) {
-        scratchBytes.writeVInt(blocks.size()-1);
-        for (int i=1;i<blocks.size();i++) {
-          PendingBlock sub = blocks.get(i);
-          assert sub.floorLeadByte != -1;
-          //if (DEBUG) {
-          //  System.out.println("    write floorLeadByte=" + Integer.toHexString(sub.floorLeadByte&0xff));
-          //}
-          scratchBytes.writeByte((byte) sub.floorLeadByte);
-          assert sub.fp > fp;
-          scratchBytes.writeVLong((sub.fp - fp) << 1 | (sub.hasTerms ? 1 : 0));
-        }
-      }
-
-      final ByteSequenceOutputs outputs = ByteSequenceOutputs.getSingleton();
-      final Builder<BytesRef> indexBuilder = new Builder<>(FST.INPUT_TYPE.BYTE1,
-                                                           0, 0, true, false, Integer.MAX_VALUE,
-                                                           outputs, false,
-                                                           PackedInts.COMPACT, true, 15);
-      //if (DEBUG) {
-      //  System.out.println("  compile index for prefix=" + prefix);
-      //}
-      //indexBuilder.DEBUG = false;
-      final byte[] bytes = new byte[(int) scratchBytes.getFilePointer()];
-      assert bytes.length > 0;
-      scratchBytes.writeTo(bytes, 0);
-      indexBuilder.add(Util.toIntsRef(prefix, scratchIntsRef), new BytesRef(bytes, 0, bytes.length));
-      scratchBytes.reset();
-
-      // Copy over index for all sub-blocks
-      for(PendingBlock block : blocks) {
-        if (block.subIndices != null) {
-          for(FST<BytesRef> subIndex : block.subIndices) {
-            append(indexBuilder, subIndex, scratchIntsRef);
-          }
-          block.subIndices = null;
-        }
-      }
-
-      index = indexBuilder.finish();
-
-      assert subIndices == null;
-
-      /*
-      Writer w = new OutputStreamWriter(new FileOutputStream("out.dot"));
-      Util.toDot(index, w, false, false);
-      System.out.println("SAVED to out.dot");
-      w.close();
-      */
-    }
-
-    // TODO: maybe we could add bulk-add method to
-    // Builder?  Takes FST and unions it w/ current
-    // FST.
-    private void append(Builder<BytesRef> builder, FST<BytesRef> subIndex, IntsRefBuilder scratchIntsRef) throws IOException {
-      final BytesRefFSTEnum<BytesRef> subIndexEnum = new BytesRefFSTEnum<>(subIndex);
-      BytesRefFSTEnum.InputOutput<BytesRef> indexEnt;
-      while((indexEnt = subIndexEnum.next()) != null) {
-        //if (DEBUG) {
-        //  System.out.println("      add sub=" + indexEnt.input + " " + indexEnt.input + " output=" + indexEnt.output);
-        //}
-        builder.add(Util.toIntsRef(indexEnt.input, scratchIntsRef), indexEnt.output);
-      }
-    }
-  }
-
-  private final RAMOutputStream scratchBytes = new RAMOutputStream();
-  private final IntsRefBuilder scratchIntsRef = new IntsRefBuilder();
-
-  class TermsWriter {
-    private final FieldInfo fieldInfo;
-    private final int longsSize;
-    private long numTerms;
-    final FixedBitSet docsSeen;
-    long sumTotalTermFreq;
-    long sumDocFreq;
-    long indexStartFP;
-
-    // Records index into pending where the current prefix at that
-    // length "started"; for example, if current term starts with 't',
-    // startsByPrefix[0] is the index into pending for the first
-    // term/sub-block starting with 't'.  We use this to figure out when
-    // to write a new block:
-    private final BytesRefBuilder lastTerm = new BytesRefBuilder();
-    private int[] prefixStarts = new int[8];
-
-    private final long[] longs;
-
-    // Pending stack of terms and blocks.  As terms arrive (in sorted order)
-    // we append to this stack, and once the top of the stack has enough
-    // terms starting with a common prefix, we write a new block with
-    // those terms and replace those terms in the stack with a new block:
-    private final List<PendingEntry> pending = new ArrayList<>();
-
-    // Reused in writeBlocks:
-    private final List<PendingBlock> newBlocks = new ArrayList<>();
-
-    private PendingTerm firstPendingTerm;
-    private PendingTerm lastPendingTerm;
-
-    /** Writes the top count entries in pending, using prevTerm to compute the prefix. */
-    void writeBlocks(int prefixLength, int count) throws IOException {
-
-      assert count > 0;
-
-      /*
-      if (DEBUG) {
-        BytesRef br = new BytesRef(lastTerm.bytes);
-        br.offset = lastTerm.offset;
-        br.length = prefixLength;
-        System.out.println("writeBlocks: " + br.utf8ToString() + " count=" + count);
-      }
-      */
-
-      // Root block better write all remaining pending entries:
-      assert prefixLength > 0 || count == pending.size();
-
-      int lastSuffixLeadLabel = -1;
-
-      // True if we saw at least one term in this block (we record if a block
-      // only points to sub-blocks in the terms index so we can avoid seeking
-      // to it when we are looking for a term):
-      boolean hasTerms = false;
-      boolean hasSubBlocks = false;
-
-      int start = pending.size()-count;
-      int end = pending.size();
-      int nextBlockStart = start;
-      int nextFloorLeadLabel = -1;
-
-      for (int i=start; i<end; i++) {
-
-        PendingEntry ent = pending.get(i);
-
-        int suffixLeadLabel;
-
-        if (ent.isTerm) {
-          PendingTerm term = (PendingTerm) ent;
-          if (term.termBytes.length == prefixLength) {
-            // Suffix is 0, i.e. prefix 'foo' and term is
-            // 'foo' so the term has empty string suffix
-            // in this block
-            assert lastSuffixLeadLabel == -1;
-            suffixLeadLabel = -1;
-          } else {
-            suffixLeadLabel = term.termBytes[prefixLength] & 0xff;
-          }
-        } else {
-          PendingBlock block = (PendingBlock) ent;
-          assert block.prefix.length > prefixLength;
-          suffixLeadLabel = block.prefix.bytes[block.prefix.offset + prefixLength] & 0xff;
-        }
-        // if (DEBUG) System.out.println("  i=" + i + " ent=" + ent + " suffixLeadLabel=" + suffixLeadLabel);
-
-        if (suffixLeadLabel != lastSuffixLeadLabel) {
-          int itemsInBlock = i - nextBlockStart;
-          if (itemsInBlock >= minItemsInBlock && end-nextBlockStart > maxItemsInBlock) {
-            // The count is too large for one block, so we must break it into "floor" blocks, where we record
-            // the leading label of the suffix of the first term in each floor block, so at search time we can
-            // jump to the right floor block.  We just use a naive greedy segmenter here: make a new floor
-            // block as soon as we have at least minItemsInBlock.  This is not always best: it often produces
-            // a too-small block as the final block:
-            boolean isFloor = itemsInBlock < count;
-            newBlocks.add(writeBlock(prefixLength, isFloor, nextFloorLeadLabel, nextBlockStart, i, hasTerms, hasSubBlocks));
-
-            hasTerms = false;
-            hasSubBlocks = false;
-            nextFloorLeadLabel = suffixLeadLabel;
-            nextBlockStart = i;
-          }
-
-          lastSuffixLeadLabel = suffixLeadLabel;
-        }
-
-        if (ent.isTerm) {
-          hasTerms = true;
-        } else {
-          hasSubBlocks = true;
-        }
-      }
-
-      // Write last block, if any:
-      if (nextBlockStart < end) {
-        int itemsInBlock = end - nextBlockStart;
-        boolean isFloor = itemsInBlock < count;
-        newBlocks.add(writeBlock(prefixLength, isFloor, nextFloorLeadLabel, nextBlockStart, end, hasTerms, hasSubBlocks));
-      }
-
-      assert newBlocks.isEmpty() == false;
-
-      PendingBlock firstBlock = newBlocks.get(0);
-
-      assert firstBlock.isFloor || newBlocks.size() == 1;
-
-      firstBlock.compileIndex(newBlocks, scratchBytes, scratchIntsRef);
-
-      // Remove slice from the top of the pending stack, that we just wrote:
-      pending.subList(pending.size()-count, pending.size()).clear();
-
-      // Append new block
-      pending.add(firstBlock);
-
-      newBlocks.clear();
-    }
-
-    /** Writes the specified slice (start is inclusive, end is exclusive)
-     *  from pending stack as a new block.  If isFloor is true, there
-     *  were too many (more than maxItemsInBlock) entries sharing the
-     *  same prefix, and so we broke it into multiple floor blocks where
-     *  we record the starting label of the suffix of each floor block. */
-    private PendingBlock writeBlock(int prefixLength, boolean isFloor, int floorLeadLabel, int start, int end, boolean hasTerms, boolean hasSubBlocks) throws IOException {
-
-      assert end > start;
-
-      long startFP = out.getFilePointer();
-
-      boolean hasFloorLeadLabel = isFloor && floorLeadLabel != -1;
-
-      final BytesRef prefix = new BytesRef(prefixLength + (hasFloorLeadLabel ? 1 : 0));
-      System.arraycopy(lastTerm.get().bytes, 0, prefix.bytes, 0, prefixLength);
-      prefix.length = prefixLength;
-
-      // Write block header:
-      int numEntries = end - start;
-      int code = numEntries << 1;
-      if (end == pending.size()) {
-        // Last block:
-        code |= 1;
-      }
-      out.writeVInt(code);
-
-      /*
-      if (DEBUG) {
-        System.out.println("  writeBlock " + (isFloor ? "(floor) " : "") + "seg=" + segment + " pending.size()=" + pending.size() + " prefixLength=" + prefixLength + " indexPrefix=" + brToString(prefix) + " entCount=" + (end-start+1) + " startFP=" + startFP + (isFloor ? (" floorLeadLabel=" + Integer.toHexString(floorLeadLabel)) : ""));
-      }
-      */
-
-      // 1st pass: pack term suffix bytes into byte[] blob
-      // TODO: cutover to bulk int codec... simple64?
-
-      // We optimize the leaf block case (block has only terms), writing a more
-      // compact format in this case:
-      boolean isLeafBlock = hasSubBlocks == false;
-
-      final List<FST<BytesRef>> subIndices;
-
-      boolean absolute = true;
-
-      if (isLeafBlock) {
-        // Only terms:
-        subIndices = null;
-        for (int i=start;i<end;i++) {
-          PendingEntry ent = pending.get(i);
-          assert ent.isTerm: "i=" + i;
-
-          PendingTerm term = (PendingTerm) ent;
-          assert StringHelper.startsWith(term.termBytes, prefix): "term.term=" + term.termBytes + " prefix=" + prefix;
-          BlockTermState state = term.state;
-          final int suffix = term.termBytes.length - prefixLength;
-          /*
-          if (DEBUG) {
-            BytesRef suffixBytes = new BytesRef(suffix);
-            System.arraycopy(term.termBytes, prefixLength, suffixBytes.bytes, 0, suffix);
-            suffixBytes.length = suffix;
-            System.out.println("    write term suffix=" + brToString(suffixBytes));
-          }
-          */
-          // For leaf block we write suffix straight
-          suffixWriter.writeVInt(suffix);
-          suffixWriter.writeBytes(term.termBytes, prefixLength, suffix);
-          assert floorLeadLabel == -1 || (term.termBytes[prefixLength] & 0xff) >= floorLeadLabel;
-
-          // Write term stats, to separate byte[] blob:
-          statsWriter.writeVInt(state.docFreq);
-          if (fieldInfo.getIndexOptions() != IndexOptions.DOCS_ONLY) {
-            assert state.totalTermFreq >= state.docFreq: state.totalTermFreq + " vs " + state.docFreq;
-            statsWriter.writeVLong(state.totalTermFreq - state.docFreq);
-          }
-
-          // Write term meta data
-          postingsWriter.encodeTerm(longs, bytesWriter, fieldInfo, state, absolute);
-          for (int pos = 0; pos < longsSize; pos++) {
-            assert longs[pos] >= 0;
-            metaWriter.writeVLong(longs[pos]);
-          }
-          bytesWriter.writeTo(metaWriter);
-          bytesWriter.reset();
-          absolute = false;
-        }
-      } else {
-        // Mixed terms and sub-blocks:
-        subIndices = new ArrayList<>();
-        for (int i=start;i<end;i++) {
-          PendingEntry ent = pending.get(i);
-          if (ent.isTerm) {
-            PendingTerm term = (PendingTerm) ent;
-            assert StringHelper.startsWith(term.termBytes, prefix): "term.term=" + term.termBytes + " prefix=" + prefix;
-            BlockTermState state = term.state;
-            final int suffix = term.termBytes.length - prefixLength;
-            /*
-            if (DEBUG) {
-              BytesRef suffixBytes = new BytesRef(suffix);
-              System.arraycopy(term.termBytes, prefixLength, suffixBytes.bytes, 0, suffix);
-              suffixBytes.length = suffix;
-              System.out.println("    write term suffix=" + brToString(suffixBytes));
-            }
-            */
-            // For non-leaf block we borrow 1 bit to record
-            // if entry is term or sub-block
-            suffixWriter.writeVInt(suffix<<1);
-            suffixWriter.writeBytes(term.termBytes, prefixLength, suffix);
-            assert floorLeadLabel == -1 || (term.termBytes[prefixLength] & 0xff) >= floorLeadLabel;
-
-            // Write term stats, to separate byte[] blob:
-            statsWriter.writeVInt(state.docFreq);
-            if (fieldInfo.getIndexOptions() != IndexOptions.DOCS_ONLY) {
-              assert state.totalTermFreq >= state.docFreq;
-              statsWriter.writeVLong(state.totalTermFreq - state.docFreq);
-            }
-
-            // TODO: now that terms dict "sees" these longs,
-            // we can explore better column-stride encodings
-            // to encode all long[0]s for this block at
-            // once, all long[1]s, etc., e.g. using
-            // Simple64.  Alternatively, we could interleave
-            // stats + meta ... no reason to have them
-            // separate anymore:
-
-            // Write term meta data
-            postingsWriter.encodeTerm(longs, bytesWriter, fieldInfo, state, absolute);
-            for (int pos = 0; pos < longsSize; pos++) {
-              assert longs[pos] >= 0;
-              metaWriter.writeVLong(longs[pos]);
-            }
-            bytesWriter.writeTo(metaWriter);
-            bytesWriter.reset();
-            absolute = false;
-          } else {
-            PendingBlock block = (PendingBlock) ent;
-            assert StringHelper.startsWith(block.prefix, prefix);
-            final int suffix = block.prefix.length - prefixLength;
-
-            assert suffix > 0;
-
-            // For non-leaf block we borrow 1 bit to record
-            // if entry is term or sub-block
-            suffixWriter.writeVInt((suffix<<1)|1);
-            suffixWriter.writeBytes(block.prefix.bytes, prefixLength, suffix);
-
-            assert floorLeadLabel == -1 || (block.prefix.bytes[prefixLength] & 0xff) >= floorLeadLabel;
-
-            assert block.fp < startFP;
-
-            /*
-            if (DEBUG) {
-              BytesRef suffixBytes = new BytesRef(suffix);
-              System.arraycopy(block.prefix.bytes, prefixLength, suffixBytes.bytes, 0, suffix);
-              suffixBytes.length = suffix;
-              System.out.println("    write sub-block suffix=" + brToString(suffixBytes) + " subFP=" + block.fp + " subCode=" + (startFP-block.fp) + " floor=" + block.isFloor);
-            }
-            */
-
-            suffixWriter.writeVLong(startFP - block.fp);
-            subIndices.add(block.index);
-          }
-        }
-
-        assert subIndices.size() != 0;
-      }
-
-      // TODO: we could block-write the term suffix pointers;
-      // this would take more space but would enable binary
-      // search on lookup
-
-      // Write suffixes byte[] blob to terms dict output:
-      out.writeVInt((int) (suffixWriter.getFilePointer() << 1) | (isLeafBlock ? 1:0));
-      suffixWriter.writeTo(out);
-      suffixWriter.reset();
-
-      // Write term stats byte[] blob
-      out.writeVInt((int) statsWriter.getFilePointer());
-      statsWriter.writeTo(out);
-      statsWriter.reset();
-
-      // Write term meta data byte[] blob
-      out.writeVInt((int) metaWriter.getFilePointer());
-      metaWriter.writeTo(out);
-      metaWriter.reset();
-
-      // if (DEBUG) {
-      //   System.out.println("      fpEnd=" + out.getFilePointer());
-      // }
-
-      if (hasFloorLeadLabel) {
-        // We already allocated to length+1 above:
-        prefix.bytes[prefix.length++] = (byte) floorLeadLabel;
-      }
-
-      return new PendingBlock(prefix, startFP, hasTerms, isFloor, floorLeadLabel, subIndices);
-    }
-
-    TermsWriter(FieldInfo fieldInfo) {
-      this.fieldInfo = fieldInfo;
-      docsSeen = new FixedBitSet(maxDoc);
-
-      this.longsSize = postingsWriter.setField(fieldInfo);
-      this.longs = new long[longsSize];
-    }
-    
-    /** Writes one term's worth of postings. */
-    public void write(BytesRef text, TermsEnum termsEnum) throws IOException {
-      /*
-      if (DEBUG) {
-        int[] tmp = new int[lastTerm.length];
-        System.arraycopy(prefixStarts, 0, tmp, 0, tmp.length);
-        System.out.println("BTTW: write term=" + brToString(text) + " prefixStarts=" + Arrays.toString(tmp) + " pending.size()=" + pending.size());
-      }
-      */
-
-      BlockTermState state = postingsWriter.writeTerm(text, termsEnum, docsSeen);
-      if (state != null) {
-        assert state.docFreq != 0;
-        assert fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY || state.totalTermFreq >= state.docFreq: "postingsWriter=" + postingsWriter;
-        sumDocFreq += state.docFreq;
-        sumTotalTermFreq += state.totalTermFreq;
-        pushTerm(text);
-       
-        PendingTerm term = new PendingTerm(text, state);
-        pending.add(term);
-        numTerms++;
-        if (firstPendingTerm == null) {
-          firstPendingTerm = term;
-        }
-        lastPendingTerm = term;
-      }
-    }
-
-    /** Pushes the new term to the top of the stack, and writes new blocks. */
-    private void pushTerm(BytesRef text) throws IOException {
-      int limit = Math.min(lastTerm.length(), text.length);
-
-      // Find common prefix between last term and current term:
-      int pos = 0;
-      while (pos < limit && lastTerm.byteAt(pos) == text.bytes[text.offset+pos]) {
-        pos++;
-      }
-
-      // if (DEBUG) System.out.println("  shared=" + pos + "  lastTerm.length=" + lastTerm.length);
-
-      // Close the "abandoned" suffix now:
-      for(int i=lastTerm.length()-1;i>=pos;i--) {
-
-        // How many items on top of the stack share the current suffix
-        // we are closing:
-        int prefixTopSize = pending.size() - prefixStarts[i];
-        if (prefixTopSize >= minItemsInBlock) {
-          // if (DEBUG) System.out.println("pushTerm i=" + i + " prefixTopSize=" + prefixTopSize + " minItemsInBlock=" + minItemsInBlock);
-          writeBlocks(i+1, prefixTopSize);
-          prefixStarts[i] -= prefixTopSize-1;
-        }
-      }
-
-      if (prefixStarts.length < text.length) {
-        prefixStarts = ArrayUtil.grow(prefixStarts, text.length);
-      }
-
-      // Init new tail:
-      for(int i=pos;i<text.length;i++) {
-        prefixStarts[i] = pending.size();
-      }
-
-      lastTerm.copyBytes(text);
-    }
-
-    // Finishes all terms in this field
-    public void finish() throws IOException {
-      if (numTerms > 0) {
-        // if (DEBUG) System.out.println("BTTW: finish prefixStarts=" + Arrays.toString(prefixStarts));
-
-        // Add empty term to force closing of all final blocks:
-        pushTerm(new BytesRef());
-
-        // TODO: if pending.size() is already 1 with a non-zero prefix length
-        // we can save writing a "degenerate" root block, but we have to
-        // fix all the places that assume the root block's prefix is the empty string:
-        writeBlocks(0, pending.size());
-
-        // We better have one final "root" block:
-        assert pending.size() == 1 && !pending.get(0).isTerm: "pending.size()=" + pending.size() + " pending=" + pending;
-        final PendingBlock root = (PendingBlock) pending.get(0);
-        assert root.prefix.length == 0;
-        assert root.index.getEmptyOutput() != null;
-
-        // Write FST to index
-        indexStartFP = indexOut.getFilePointer();
-        root.index.save(indexOut);
-        //System.out.println("  write FST " + indexStartFP + " field=" + fieldInfo.name);
-
-        /*
-        if (DEBUG) {
-          final String dotFileName = segment + "_" + fieldInfo.name + ".dot";
-          Writer w = new OutputStreamWriter(new FileOutputStream(dotFileName));
-          Util.toDot(root.index, w, false, false);
-          System.out.println("SAVED to " + dotFileName);
-          w.close();
-        }
-        */
-        assert firstPendingTerm != null;
-        BytesRef minTerm = new BytesRef(firstPendingTerm.termBytes);
-
-        assert lastPendingTerm != null;
-        BytesRef maxTerm = new BytesRef(lastPendingTerm.termBytes);
-
-        fields.add(new FieldMetaData(fieldInfo,
-                                     ((PendingBlock) pending.get(0)).index.getEmptyOutput(),
-                                     numTerms,
-                                     indexStartFP,
-                                     sumTotalTermFreq,
-                                     sumDocFreq,
-                                     docsSeen.cardinality(),
-                                     longsSize,
-                                     minTerm, maxTerm));
-      } else {
-        assert sumTotalTermFreq == 0 || fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY && sumTotalTermFreq == -1;
-        assert sumDocFreq == 0;
-        assert docsSeen.cardinality() == 0;
-      }
-    }
-
-    private final RAMOutputStream suffixWriter = new RAMOutputStream();
-    private final RAMOutputStream statsWriter = new RAMOutputStream();
-    private final RAMOutputStream metaWriter = new RAMOutputStream();
-    private final RAMOutputStream bytesWriter = new RAMOutputStream();
-  }
-
-  @Override
-  public void close() throws IOException {
-
-    boolean success = false;
-    try {
-      
-      final long dirStart = out.getFilePointer();
-      final long indexDirStart = indexOut.getFilePointer();
-
-      out.writeVInt(fields.size());
-      
-      for(FieldMetaData field : fields) {
-        //System.out.println("  field " + field.fieldInfo.name + " " + field.numTerms + " terms");
-        out.writeVInt(field.fieldInfo.number);
-        assert field.numTerms > 0;
-        out.writeVLong(field.numTerms);
-        out.writeVInt(field.rootCode.length);
-        out.writeBytes(field.rootCode.bytes, field.rootCode.offset, field.rootCode.length);
-        if (field.fieldInfo.getIndexOptions() != IndexOptions.DOCS_ONLY) {
-          out.writeVLong(field.sumTotalTermFreq);
-        }
-        out.writeVLong(field.sumDocFreq);
-        out.writeVInt(field.docCount);
-        out.writeVInt(field.longsSize);
-        indexOut.writeVLong(field.indexStartFP);
-        writeBytesRef(out, field.minTerm);
-        writeBytesRef(out, field.maxTerm);
-      }
-      writeTrailer(out, dirStart);
-      CodecUtil.writeFooter(out);
-      writeIndexTrailer(indexOut, indexDirStart);
-      CodecUtil.writeFooter(indexOut);
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(out, indexOut, postingsWriter);
-      } else {
-        IOUtils.closeWhileHandlingException(out, indexOut, postingsWriter);
-      }
-    }
-  }
-
-  private static void writeBytesRef(IndexOutput out, BytesRef bytes) throws IOException {
-    out.writeVInt(bytes.length);
-    out.writeBytes(bytes.bytes, bytes.offset, bytes.length);
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/blocktree/TestLucene40BlockFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/blocktree/TestLucene40BlockFormat.java
deleted file mode 100644
index 2e863a8..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/blocktree/TestLucene40BlockFormat.java
+++ /dev/null
@@ -1,67 +0,0 @@
-package org.apache.lucene.codecs.blocktree;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.blocktree.Lucene40FieldReader;
-import org.apache.lucene.codecs.blocktree.Lucene40Stats;
-import org.apache.lucene.codecs.lucene41.Lucene41RWCodec;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.index.BasePostingsFormatTestCase;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.store.Directory;
-
-/**
- * Tests BlockPostingsFormat
- */
-public class TestLucene40BlockFormat extends BasePostingsFormatTestCase {
-  private final Codec codec = new Lucene41RWCodec();
-
-  @Override
-  protected Codec getCodec() {
-    return codec;
-  }
-  
-  /** Make sure the final sub-block(s) are not skipped. */
-  public void testFinalBlock() throws Exception {
-    Directory d = newDirectory();
-    IndexWriter w = new IndexWriter(d, new IndexWriterConfig(new MockAnalyzer(random())));
-    for(int i=0;i<25;i++) {
-      Document doc = new Document();
-      doc.add(newStringField("field", Character.toString((char) (97+i)), Field.Store.NO));
-      doc.add(newStringField("field", "z" + Character.toString((char) (97+i)), Field.Store.NO));
-      w.addDocument(doc);
-    }
-    w.forceMerge(1);
-
-    DirectoryReader r = DirectoryReader.open(w, true);
-    assertEquals(1, r.leaves().size());
-    Lucene40FieldReader field = (Lucene40FieldReader) r.leaves().get(0).reader().fields().terms("field");
-    // We should see exactly two blocks: one root block (prefix empty string) and one block for z* terms (prefix z):
-    Lucene40Stats stats = field.getStats();
-    assertEquals(0, stats.floorBlockCount);
-    assertEquals(2, stats.nonFloorBlockCount);
-    r.close();
-    w.close();
-    d.close();
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40DocValuesWriter.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40DocValuesWriter.java
deleted file mode 100644
index 5d33f52..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40DocValuesWriter.java
+++ /dev/null
@@ -1,559 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.TreeSet;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.codecs.MissingOrdRemapper;
-import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosFormat.LegacyDocValuesType;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * Writer for 4.0 docvalues format
- * @deprecated for test purposes only
- */
-@Deprecated
-final class Lucene40DocValuesWriter extends DocValuesConsumer {
-  private final Directory dir;
-  private final SegmentWriteState state;
-  private final String legacyKey;
-  private final static String segmentSuffix = "dv";
-
-  // note: intentionally ignores seg suffix
-  Lucene40DocValuesWriter(SegmentWriteState state, String filename, String legacyKey) throws IOException {
-    this.state = state;
-    this.legacyKey = legacyKey;
-    this.dir = new Lucene40CompoundReader(state.directory, filename, state.context, true);
-  }
-  
-  @Override
-  public void addNumericField(FieldInfo field, Iterable<Number> values) throws IOException {
-    if (field.getDocValuesGen() != -1) {
-      throw new UnsupportedOperationException("4.0 does not support dv updates");
-    }
-    // examine the values to determine best type to use
-    long minValue = Long.MAX_VALUE;
-    long maxValue = Long.MIN_VALUE;
-    for (Number n : values) {
-      long v = n == null ? 0 : n.longValue();
-      minValue = Math.min(minValue, v);
-      maxValue = Math.max(maxValue, v);
-    }
-    
-    String fileName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
-    IndexOutput data = dir.createOutput(fileName, state.context);
-    boolean success = false;
-    try {
-      if (minValue >= Byte.MIN_VALUE && maxValue <= Byte.MAX_VALUE && PackedInts.bitsRequired(maxValue-minValue) > 4) {
-        // fits in a byte[], would be more than 4bpv, just write byte[]
-        addBytesField(field, data, values);
-      } else if (minValue >= Short.MIN_VALUE && maxValue <= Short.MAX_VALUE && PackedInts.bitsRequired(maxValue-minValue) > 8) {
-        // fits in a short[], would be more than 8bpv, just write short[]
-        addShortsField(field, data, values);
-      } else if (minValue >= Integer.MIN_VALUE && maxValue <= Integer.MAX_VALUE && PackedInts.bitsRequired(maxValue-minValue) > 16) {
-        // fits in a int[], would be more than 16bpv, just write int[]
-        addIntsField(field, data, values);
-      } else {
-        addVarIntsField(field, data, values, minValue, maxValue);
-      }
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(data);
-      } else {
-        IOUtils.closeWhileHandlingException(data);
-      }
-    }
-  }
-
-  private void addBytesField(FieldInfo field, IndexOutput output, Iterable<Number> values) throws IOException {
-    field.putAttribute(legacyKey, LegacyDocValuesType.FIXED_INTS_8.name());
-    CodecUtil.writeHeader(output, 
-                          Lucene40DocValuesFormat.INTS_CODEC_NAME, 
-                          Lucene40DocValuesFormat.INTS_VERSION_CURRENT);
-    output.writeInt(1); // size
-    for (Number n : values) {
-      output.writeByte(n == null ? 0 : n.byteValue());
-    }
-  }
-  
-  private void addShortsField(FieldInfo field, IndexOutput output, Iterable<Number> values) throws IOException {
-    field.putAttribute(legacyKey, LegacyDocValuesType.FIXED_INTS_16.name());
-    CodecUtil.writeHeader(output, 
-                          Lucene40DocValuesFormat.INTS_CODEC_NAME, 
-                          Lucene40DocValuesFormat.INTS_VERSION_CURRENT);
-    output.writeInt(2); // size
-    for (Number n : values) {
-      output.writeShort(n == null ? 0 : n.shortValue());
-    }
-  }
-  
-  private void addIntsField(FieldInfo field, IndexOutput output, Iterable<Number> values) throws IOException {
-    field.putAttribute(legacyKey, LegacyDocValuesType.FIXED_INTS_32.name());
-    CodecUtil.writeHeader(output, 
-                          Lucene40DocValuesFormat.INTS_CODEC_NAME, 
-                          Lucene40DocValuesFormat.INTS_VERSION_CURRENT);
-    output.writeInt(4); // size
-    for (Number n : values) {
-      output.writeInt(n == null ? 0 : n.intValue());
-    }
-  }
-  
-  private void addVarIntsField(FieldInfo field, IndexOutput output, Iterable<Number> values, long minValue, long maxValue) throws IOException {
-    field.putAttribute(legacyKey, LegacyDocValuesType.VAR_INTS.name());
-    
-    CodecUtil.writeHeader(output, 
-                          Lucene40DocValuesFormat.VAR_INTS_CODEC_NAME, 
-                          Lucene40DocValuesFormat.VAR_INTS_VERSION_CURRENT);
-    
-    final long delta = maxValue - minValue;
-    
-    if (delta < 0) {
-      // writes longs
-      output.writeByte(Lucene40DocValuesFormat.VAR_INTS_FIXED_64);
-      for (Number n : values) {
-        output.writeLong(n == null ? 0 : n.longValue());
-      }
-    } else {
-      // writes packed ints
-      output.writeByte(Lucene40DocValuesFormat.VAR_INTS_PACKED);
-      output.writeLong(minValue);
-      output.writeLong(0 - minValue); // default value (representation of 0)
-      PackedInts.Writer writer = PackedInts.getWriter(output, 
-                                                      state.segmentInfo.getDocCount(),
-                                                      PackedInts.bitsRequired(delta), 
-                                                      PackedInts.DEFAULT);
-      for (Number n : values) {
-        long v = n == null ? 0 : n.longValue();
-        writer.add(v - minValue);
-      }
-      writer.finish();
-    }
-  }
-
-  @Override
-  public void addBinaryField(FieldInfo field, Iterable<BytesRef> values) throws IOException {
-    if (field.getDocValuesGen() != -1) {
-      throw new UnsupportedOperationException("4.0 does not support dv updates");
-    }
-    // examine the values to determine best type to use
-    HashSet<BytesRef> uniqueValues = new HashSet<>();
-    int minLength = Integer.MAX_VALUE;
-    int maxLength = Integer.MIN_VALUE;
-    for (BytesRef b : values) {
-      if (b == null) {
-        b = new BytesRef(); // 4.0 doesnt distinguish
-      }
-      if (b.length > Lucene40DocValuesFormat.MAX_BINARY_FIELD_LENGTH) {
-        throw new IllegalArgumentException("DocValuesField \"" + field.name + "\" is too large, must be <= " + Lucene40DocValuesFormat.MAX_BINARY_FIELD_LENGTH);
-      }
-      minLength = Math.min(minLength, b.length);
-      maxLength = Math.max(maxLength, b.length);
-      if (uniqueValues != null) {
-        if (uniqueValues.add(BytesRef.deepCopyOf(b))) {
-          if (uniqueValues.size() > 256) {
-            uniqueValues = null;
-          }
-        }
-      }
-    }
-    
-    int maxDoc = state.segmentInfo.getDocCount();
-    final boolean fixed = minLength == maxLength;
-    final boolean dedup = uniqueValues != null && uniqueValues.size() * 2 < maxDoc;
-    
-    if (dedup) {
-      // we will deduplicate and deref values
-      boolean success = false;
-      IndexOutput data = null;
-      IndexOutput index = null;
-      String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
-      String indexName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "idx");
-      try {
-        data = dir.createOutput(dataName, state.context);
-        index = dir.createOutput(indexName, state.context);
-        if (fixed) {
-          addFixedDerefBytesField(field, data, index, values, minLength);
-        } else {
-          addVarDerefBytesField(field, data, index, values);
-        }
-        success = true;
-      } finally {
-        if (success) {
-          IOUtils.close(data, index);
-        } else {
-          IOUtils.closeWhileHandlingException(data, index);
-        }
-      }
-    } else {
-      // we dont deduplicate, just write values straight
-      if (fixed) {
-        // fixed byte[]
-        String fileName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
-        IndexOutput data = dir.createOutput(fileName, state.context);
-        boolean success = false;
-        try {
-          addFixedStraightBytesField(field, data, values, minLength);
-          success = true;
-        } finally {
-          if (success) {
-            IOUtils.close(data);
-          } else {
-            IOUtils.closeWhileHandlingException(data);
-          }
-        }
-      } else {
-        // variable byte[]
-        boolean success = false;
-        IndexOutput data = null;
-        IndexOutput index = null;
-        String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
-        String indexName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "idx");
-        try {
-          data = dir.createOutput(dataName, state.context);
-          index = dir.createOutput(indexName, state.context);
-          addVarStraightBytesField(field, data, index, values);
-          success = true;
-        } finally {
-          if (success) {
-            IOUtils.close(data, index);
-          } else {
-            IOUtils.closeWhileHandlingException(data, index);
-          }
-        }
-      }
-    }
-  }
-  
-  private void addFixedStraightBytesField(FieldInfo field, IndexOutput output, Iterable<BytesRef> values, int length) throws IOException {
-    field.putAttribute(legacyKey, LegacyDocValuesType.BYTES_FIXED_STRAIGHT.name());
-
-    CodecUtil.writeHeader(output, 
-                          Lucene40DocValuesFormat.BYTES_FIXED_STRAIGHT_CODEC_NAME,
-                          Lucene40DocValuesFormat.BYTES_FIXED_STRAIGHT_VERSION_CURRENT);
-    
-    output.writeInt(length);
-    for (BytesRef v : values) {
-      if (v != null) {
-        output.writeBytes(v.bytes, v.offset, v.length);
-      }
-    }
-  }
-  
-  // NOTE: 4.0 file format docs are crazy/wrong here...
-  private void addVarStraightBytesField(FieldInfo field, IndexOutput data, IndexOutput index, Iterable<BytesRef> values) throws IOException {
-    field.putAttribute(legacyKey, LegacyDocValuesType.BYTES_VAR_STRAIGHT.name());
-    
-    CodecUtil.writeHeader(data, 
-                          Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_CODEC_NAME_DAT,
-                          Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_VERSION_CURRENT);
-    
-    CodecUtil.writeHeader(index, 
-                          Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_CODEC_NAME_IDX,
-                          Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_VERSION_CURRENT);
-    
-    /* values */
-    
-    final long startPos = data.getFilePointer();
-    
-    for (BytesRef v : values) {
-      if (v != null) {
-        data.writeBytes(v.bytes, v.offset, v.length);
-      }
-    }
-    
-    /* addresses */
-    
-    final long maxAddress = data.getFilePointer() - startPos;
-    index.writeVLong(maxAddress);
-    
-    final int maxDoc = state.segmentInfo.getDocCount();
-    assert maxDoc != Integer.MAX_VALUE; // unsupported by the 4.0 impl
-    
-    final PackedInts.Writer w = PackedInts.getWriter(index, maxDoc+1, PackedInts.bitsRequired(maxAddress), PackedInts.DEFAULT);
-    long currentPosition = 0;
-    for (BytesRef v : values) {
-      w.add(currentPosition);
-      if (v != null) {
-        currentPosition += v.length;
-      }
-    }
-    // write sentinel
-    assert currentPosition == maxAddress;
-    w.add(currentPosition);
-    w.finish();
-  }
-  
-  private void addFixedDerefBytesField(FieldInfo field, IndexOutput data, IndexOutput index, Iterable<BytesRef> values, int length) throws IOException {
-    field.putAttribute(legacyKey, LegacyDocValuesType.BYTES_FIXED_DEREF.name());
-
-    CodecUtil.writeHeader(data, 
-                          Lucene40DocValuesFormat.BYTES_FIXED_DEREF_CODEC_NAME_DAT,
-                          Lucene40DocValuesFormat.BYTES_FIXED_DEREF_VERSION_CURRENT);
-    
-    CodecUtil.writeHeader(index, 
-                          Lucene40DocValuesFormat.BYTES_FIXED_DEREF_CODEC_NAME_IDX,
-                          Lucene40DocValuesFormat.BYTES_FIXED_DEREF_VERSION_CURRENT);
-    
-    // deduplicate
-    TreeSet<BytesRef> dictionary = new TreeSet<>();
-    for (BytesRef v : values) {
-      dictionary.add(v == null ? new BytesRef() : BytesRef.deepCopyOf(v));
-    }
-    
-    /* values */
-    data.writeInt(length);
-    for (BytesRef v : dictionary) {
-      data.writeBytes(v.bytes, v.offset, v.length);
-    }
-    
-    /* ordinals */
-    int valueCount = dictionary.size();
-    assert valueCount > 0;
-    index.writeInt(valueCount);
-    final int maxDoc = state.segmentInfo.getDocCount();
-    final PackedInts.Writer w = PackedInts.getWriter(index, maxDoc, PackedInts.bitsRequired(valueCount-1), PackedInts.DEFAULT);
-
-    for (BytesRef v : values) {
-      if (v == null) {
-        v = new BytesRef();
-      }
-      int ord = dictionary.headSet(v).size();
-      w.add(ord);
-    }
-    w.finish();
-  }
-  
-  private void addVarDerefBytesField(FieldInfo field, IndexOutput data, IndexOutput index, Iterable<BytesRef> values) throws IOException {
-    field.putAttribute(legacyKey, LegacyDocValuesType.BYTES_VAR_DEREF.name());
-
-    CodecUtil.writeHeader(data, 
-                          Lucene40DocValuesFormat.BYTES_VAR_DEREF_CODEC_NAME_DAT,
-                          Lucene40DocValuesFormat.BYTES_VAR_DEREF_VERSION_CURRENT);
-    
-    CodecUtil.writeHeader(index, 
-                          Lucene40DocValuesFormat.BYTES_VAR_DEREF_CODEC_NAME_IDX,
-                          Lucene40DocValuesFormat.BYTES_VAR_DEREF_VERSION_CURRENT);
-    
-    // deduplicate
-    TreeSet<BytesRef> dictionary = new TreeSet<>();
-    for (BytesRef v : values) {
-      dictionary.add(v == null ? new BytesRef() : BytesRef.deepCopyOf(v));
-    }
-    
-    /* values */
-    long startPosition = data.getFilePointer();
-    long currentAddress = 0;
-    HashMap<BytesRef,Long> valueToAddress = new HashMap<>();
-    for (BytesRef v : dictionary) {
-      currentAddress = data.getFilePointer() - startPosition;
-      valueToAddress.put(v, currentAddress);
-      writeVShort(data, v.length);
-      data.writeBytes(v.bytes, v.offset, v.length);
-    }
-    
-    /* ordinals */
-    long totalBytes = data.getFilePointer() - startPosition;
-    index.writeLong(totalBytes);
-    final int maxDoc = state.segmentInfo.getDocCount();
-    final PackedInts.Writer w = PackedInts.getWriter(index, maxDoc, PackedInts.bitsRequired(currentAddress), PackedInts.DEFAULT);
-
-    for (BytesRef v : values) {
-      w.add(valueToAddress.get(v == null ? new BytesRef() : v));
-    }
-    w.finish();
-  }
-  
-  // the little vint encoding used for var-deref
-  private static void writeVShort(IndexOutput o, int i) throws IOException {
-    assert i >= 0 && i <= Short.MAX_VALUE;
-    if (i < 128) {
-      o.writeByte((byte)i);
-    } else {
-      o.writeByte((byte) (0x80 | (i >> 8)));
-      o.writeByte((byte) (i & 0xff));
-    }
-  }
-
-  @Override
-  public void addSortedField(FieldInfo field, Iterable<BytesRef> values, Iterable<Number> docToOrd) throws IOException {
-    if (field.getDocValuesGen() != -1) {
-      throw new UnsupportedOperationException("4.0 does not support dv updates");
-    }
-    // examine the values to determine best type to use
-    int minLength = Integer.MAX_VALUE;
-    int maxLength = Integer.MIN_VALUE;
-    for (BytesRef b : values) {
-      minLength = Math.min(minLength, b.length);
-      maxLength = Math.max(maxLength, b.length);
-    }
-    
-    // but dont use fixed if there are missing values (we are simulating how lucene40 wrote dv...)
-    boolean anyMissing = false;
-    for (Number n : docToOrd) {
-      if (n.longValue() == -1) {
-        anyMissing = true;
-        break;
-      }
-    }
-    
-    boolean success = false;
-    IndexOutput data = null;
-    IndexOutput index = null;
-    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
-    String indexName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "idx");
-    
-    try {
-      data = dir.createOutput(dataName, state.context);
-      index = dir.createOutput(indexName, state.context);
-      if (minLength == maxLength && !anyMissing) {
-        // fixed byte[]
-        addFixedSortedBytesField(field, data, index, values, docToOrd, minLength);
-      } else {
-        // var byte[]
-        // three cases for simulating the old writer:
-        // 1. no missing
-        // 2. missing (and empty string in use): remap ord=-1 -> ord=0
-        // 3. missing (and empty string not in use): remap all ords +1, insert empty string into values
-        if (!anyMissing) {
-          addVarSortedBytesField(field, data, index, values, docToOrd);
-        } else if (minLength == 0) {
-          addVarSortedBytesField(field, data, index, values, MissingOrdRemapper.mapMissingToOrd0(docToOrd));
-        } else {
-          addVarSortedBytesField(field, data, index, MissingOrdRemapper.insertEmptyValue(values), MissingOrdRemapper.mapAllOrds(docToOrd));
-        }
-      }
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(data, index);
-      } else {
-        IOUtils.closeWhileHandlingException(data, index);
-      }
-    }
-  }
-  
-  private void addFixedSortedBytesField(FieldInfo field, IndexOutput data, IndexOutput index, Iterable<BytesRef> values, Iterable<Number> docToOrd, int length) throws IOException {
-    field.putAttribute(legacyKey, LegacyDocValuesType.BYTES_FIXED_SORTED.name());
-
-    CodecUtil.writeHeader(data, 
-                          Lucene40DocValuesFormat.BYTES_FIXED_SORTED_CODEC_NAME_DAT,
-                          Lucene40DocValuesFormat.BYTES_FIXED_SORTED_VERSION_CURRENT);
-    
-    CodecUtil.writeHeader(index, 
-                          Lucene40DocValuesFormat.BYTES_FIXED_SORTED_CODEC_NAME_IDX,
-                          Lucene40DocValuesFormat.BYTES_FIXED_SORTED_VERSION_CURRENT);
-    
-    /* values */
-    
-    data.writeInt(length);
-    int valueCount = 0;
-    for (BytesRef v : values) {
-      data.writeBytes(v.bytes, v.offset, v.length);
-      valueCount++;
-    }
-    
-    /* ordinals */
-    
-    index.writeInt(valueCount);
-    int maxDoc = state.segmentInfo.getDocCount();
-    assert valueCount > 0;
-    final PackedInts.Writer w = PackedInts.getWriter(index, maxDoc, PackedInts.bitsRequired(valueCount-1), PackedInts.DEFAULT);
-    for (Number n : docToOrd) {
-      w.add(n.longValue());
-    }
-    w.finish();
-  }
-  
-  private void addVarSortedBytesField(FieldInfo field, IndexOutput data, IndexOutput index, Iterable<BytesRef> values, Iterable<Number> docToOrd) throws IOException {
-    field.putAttribute(legacyKey, LegacyDocValuesType.BYTES_VAR_SORTED.name());
-    
-    CodecUtil.writeHeader(data, 
-                          Lucene40DocValuesFormat.BYTES_VAR_SORTED_CODEC_NAME_DAT,
-                          Lucene40DocValuesFormat.BYTES_VAR_SORTED_VERSION_CURRENT);
-
-    CodecUtil.writeHeader(index, 
-                          Lucene40DocValuesFormat.BYTES_VAR_SORTED_CODEC_NAME_IDX,
-                          Lucene40DocValuesFormat.BYTES_VAR_SORTED_VERSION_CURRENT);
-
-    /* values */
-    
-    final long startPos = data.getFilePointer();
-    
-    int valueCount = 0;
-    for (BytesRef v : values) {
-      data.writeBytes(v.bytes, v.offset, v.length);
-      valueCount++;
-    }
-    
-    /* addresses */
-    
-    final long maxAddress = data.getFilePointer() - startPos;
-    index.writeLong(maxAddress);
-    
-    assert valueCount != Integer.MAX_VALUE; // unsupported by the 4.0 impl
-    
-    final PackedInts.Writer w = PackedInts.getWriter(index, valueCount+1, PackedInts.bitsRequired(maxAddress), PackedInts.DEFAULT);
-    long currentPosition = 0;
-    for (BytesRef v : values) {
-      w.add(currentPosition);
-      currentPosition += v.length;
-    }
-    // write sentinel
-    assert currentPosition == maxAddress;
-    w.add(currentPosition);
-    w.finish();
-    
-    /* ordinals */
-    
-    final int maxDoc = state.segmentInfo.getDocCount();
-    assert valueCount > 0;
-    final PackedInts.Writer ords = PackedInts.getWriter(index, maxDoc, PackedInts.bitsRequired(valueCount-1), PackedInts.DEFAULT);
-    for (Number n : docToOrd) {
-      ords.add(n.longValue());
-    }
-    ords.finish();
-  }
-  
-  @Override
-  public void addSortedNumericField(FieldInfo field, Iterable<Number> docToValueCount, Iterable<Number> values) throws IOException {
-    throw new UnsupportedOperationException("Lucene 4.0 does not support SortedNumeric docvalues");
-  }
-  
-  @Override
-  public void addSortedSetField(FieldInfo field, Iterable<BytesRef> values, Iterable<Number> docToOrdCount, Iterable<Number> ords) throws IOException {
-    throw new UnsupportedOperationException("Lucene 4.0 does not support SortedSet docvalues");
-  }
-
-  @Override
-  public void close() throws IOException {
-    dir.close();
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40PostingsWriter.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40PostingsWriter.java
deleted file mode 100644
index bac0e4b..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40PostingsWriter.java
+++ /dev/null
@@ -1,320 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** Consumes doc & freq, writing them using the current
- *  index file format */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.BlockTermState;
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.PushPostingsWriterBase;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.DocsEnum;  // javadocs
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Writer for 4.0 postings format
- * @deprecated for test purposes only
- */
-@Deprecated
-final class Lucene40PostingsWriter extends PushPostingsWriterBase {
-
-  final IndexOutput freqOut;
-  final IndexOutput proxOut;
-  final Lucene40SkipListWriter skipListWriter;
-  /** Expert: The fraction of TermDocs entries stored in skip tables,
-   * used to accelerate {@link DocsEnum#advance(int)}.  Larger values result in
-   * smaller indexes, greater acceleration, but fewer accelerable cases, while
-   * smaller values result in bigger indexes, less acceleration and more
-   * accelerable cases. More detailed experiments would be useful here. */
-  static final int DEFAULT_SKIP_INTERVAL = 16;
-  final int skipInterval;
-  
-  /**
-   * Expert: minimum docFreq to write any skip data at all
-   */
-  final int skipMinimum;
-
-  /** Expert: The maximum number of skip levels. Smaller values result in 
-   * slightly smaller indexes, but slower skipping in big posting lists.
-   */
-  final int maxSkipLevels = 10;
-  final int totalNumDocs;
-
-  // Starts a new term
-  long freqStart;
-  long proxStart;
-  int lastPayloadLength;
-  int lastOffsetLength;
-  int lastPosition;
-  int lastOffset;
-
-  final static StandardTermState emptyState = new StandardTermState();
-  StandardTermState lastState;
-
-  // private String segment;
-
-  /** Creates a {@link Lucene40PostingsWriter}, with the
-   *  {@link #DEFAULT_SKIP_INTERVAL}. */
-  public Lucene40PostingsWriter(SegmentWriteState state) throws IOException {
-    this(state, DEFAULT_SKIP_INTERVAL);
-  }
-  
-  /** Creates a {@link Lucene40PostingsWriter}, with the
-   *  specified {@code skipInterval}. */
-  public Lucene40PostingsWriter(SegmentWriteState state, int skipInterval) throws IOException {
-    super();
-    this.skipInterval = skipInterval;
-    this.skipMinimum = skipInterval; /* set to the same for now */
-    // this.segment = state.segmentName;
-    String fileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, Lucene40PostingsFormat.FREQ_EXTENSION);
-    freqOut = state.directory.createOutput(fileName, state.context);
-    boolean success = false;
-    IndexOutput proxOut = null;
-    try {
-      CodecUtil.writeHeader(freqOut, Lucene40PostingsReader.FRQ_CODEC, Lucene40PostingsReader.VERSION_CURRENT);
-      // TODO: this is a best effort, if one of these fields has no postings
-      // then we make an empty prx file, same as if we are wrapped in 
-      // per-field postingsformat. maybe... we shouldn't
-      // bother w/ this opto?  just create empty prx file...?
-      if (state.fieldInfos.hasProx()) {
-        // At least one field does not omit TF, so create the
-        // prox file
-        fileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, Lucene40PostingsFormat.PROX_EXTENSION);
-        proxOut = state.directory.createOutput(fileName, state.context);
-        CodecUtil.writeHeader(proxOut, Lucene40PostingsReader.PRX_CODEC, Lucene40PostingsReader.VERSION_CURRENT);
-      } else {
-        // Every field omits TF so we will write no prox file
-        proxOut = null;
-      }
-      this.proxOut = proxOut;
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(freqOut, proxOut);
-      }
-    }
-
-    totalNumDocs = state.segmentInfo.getDocCount();
-
-    skipListWriter = new Lucene40SkipListWriter(skipInterval,
-                                               maxSkipLevels,
-                                               totalNumDocs,
-                                               freqOut,
-                                               proxOut);
-  }
-
-  @Override
-  public void init(IndexOutput termsOut, SegmentWriteState state) throws IOException {
-    CodecUtil.writeHeader(termsOut, Lucene40PostingsReader.TERMS_CODEC, Lucene40PostingsReader.VERSION_CURRENT);
-    termsOut.writeInt(skipInterval);                // write skipInterval
-    termsOut.writeInt(maxSkipLevels);               // write maxSkipLevels
-    termsOut.writeInt(skipMinimum);                 // write skipMinimum
-  }
-
-  @Override
-  public BlockTermState newTermState() {
-    return new StandardTermState();
-  }
-
-  @Override
-  public void startTerm() {
-    freqStart = freqOut.getFilePointer();
-    //if (DEBUG) System.out.println("SPW: startTerm freqOut.fp=" + freqStart);
-    if (proxOut != null) {
-      proxStart = proxOut.getFilePointer();
-    }
-    // force first payload to write its length
-    lastPayloadLength = -1;
-    // force first offset to write its length
-    lastOffsetLength = -1;
-    skipListWriter.resetSkip();
-  }
-
-  // Currently, this instance is re-used across fields, so
-  // our parent calls setField whenever the field changes
-  @Override
-  public int setField(FieldInfo fieldInfo) {
-    super.setField(fieldInfo);
-    //System.out.println("SPW: setField");
-    /*
-    if (BlockTreeTermsWriter.DEBUG && fieldInfo.name.equals("id")) {
-      DEBUG = true;
-    } else {
-      DEBUG = false;
-    }
-    */
-
-    lastState = emptyState;
-    //System.out.println("  set init blockFreqStart=" + freqStart);
-    //System.out.println("  set init blockProxStart=" + proxStart);
-    return 0;
-  }
-
-  int lastDocID;
-  int df;
-
-  @Override
-  public void startDoc(int docID, int termDocFreq) throws IOException {
-    // if (DEBUG) System.out.println("SPW:   startDoc seg=" + segment + " docID=" + docID + " tf=" + termDocFreq + " freqOut.fp=" + freqOut.getFilePointer());
-
-    final int delta = docID - lastDocID;
-    
-    if (docID < 0 || (df > 0 && delta <= 0)) {
-      throw new CorruptIndexException("docs out of order (" + docID + " <= " + lastDocID + " )", freqOut.toString());
-    }
-
-    if ((++df % skipInterval) == 0) {
-      skipListWriter.setSkipData(lastDocID, writePayloads, lastPayloadLength, writeOffsets, lastOffsetLength);
-      skipListWriter.bufferSkip(df);
-    }
-
-    assert docID < totalNumDocs: "docID=" + docID + " totalNumDocs=" + totalNumDocs;
-
-    lastDocID = docID;
-    if (indexOptions == IndexOptions.DOCS_ONLY) {
-      freqOut.writeVInt(delta);
-    } else if (1 == termDocFreq) {
-      freqOut.writeVInt((delta<<1) | 1);
-    } else {
-      freqOut.writeVInt(delta<<1);
-      freqOut.writeVInt(termDocFreq);
-    }
-
-    lastPosition = 0;
-    lastOffset = 0;
-  }
-
-  /** Add a new position & payload */
-  @Override
-  public void addPosition(int position, BytesRef payload, int startOffset, int endOffset) throws IOException {
-    //if (DEBUG) System.out.println("SPW:     addPos pos=" + position + " payload=" + (payload == null ? "null" : (payload.length + " bytes")) + " proxFP=" + proxOut.getFilePointer());
-    assert indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 : "invalid indexOptions: " + indexOptions;
-    assert proxOut != null;
-
-    final int delta = position - lastPosition;
-    
-    assert delta >= 0: "position=" + position + " lastPosition=" + lastPosition;            // not quite right (if pos=0 is repeated twice we don't catch it)
-
-    lastPosition = position;
-
-    int payloadLength = 0;
-
-    if (writePayloads) {
-      payloadLength = payload == null ? 0 : payload.length;
-
-      if (payloadLength != lastPayloadLength) {
-        lastPayloadLength = payloadLength;
-        proxOut.writeVInt((delta<<1)|1);
-        proxOut.writeVInt(payloadLength);
-      } else {
-        proxOut.writeVInt(delta << 1);
-      }
-    } else {
-      proxOut.writeVInt(delta);
-    }
-    
-    if (writeOffsets) {
-      // don't use startOffset - lastEndOffset, because this creates lots of negative vints for synonyms,
-      // and the numbers aren't that much smaller anyways.
-      int offsetDelta = startOffset - lastOffset;
-      int offsetLength = endOffset - startOffset;
-      assert offsetDelta >= 0 && offsetLength >= 0 : "startOffset=" + startOffset + ",lastOffset=" + lastOffset + ",endOffset=" + endOffset;
-      if (offsetLength != lastOffsetLength) {
-        proxOut.writeVInt(offsetDelta << 1 | 1);
-        proxOut.writeVInt(offsetLength);
-      } else {
-        proxOut.writeVInt(offsetDelta << 1);
-      }
-      lastOffset = startOffset;
-      lastOffsetLength = offsetLength;
-    }
-    
-    if (payloadLength > 0) {
-      proxOut.writeBytes(payload.bytes, payload.offset, payloadLength);
-    }
-  }
-
-  @Override
-  public void finishDoc() {
-  }
-
-  private static class StandardTermState extends BlockTermState {
-    public long freqStart;
-    public long proxStart;
-    public long skipOffset;
-  }
-
-  /** Called when we are done adding docs to this term */
-  @Override
-  public void finishTerm(BlockTermState _state) throws IOException {
-    StandardTermState state = (StandardTermState) _state;
-    // if (DEBUG) System.out.println("SPW: finishTerm seg=" + segment + " freqStart=" + freqStart);
-    assert state.docFreq > 0;
-
-    // TODO: wasteful we are counting this (counting # docs
-    // for this term) in two places?
-    assert state.docFreq == df;
-    state.freqStart = freqStart;
-    state.proxStart = proxStart;
-    if (df >= skipMinimum) {
-      state.skipOffset = skipListWriter.writeSkip(freqOut)-freqStart;
-    } else {
-      state.skipOffset = -1;
-    }
-    lastDocID = 0;
-    df = 0;
-  }
-
-  @Override
-  public void encodeTerm(long[] empty, DataOutput out, FieldInfo fieldInfo, BlockTermState _state, boolean absolute) throws IOException {
-    StandardTermState state = (StandardTermState)_state;
-    if (absolute) {
-      lastState = emptyState;
-    }
-    out.writeVLong(state.freqStart - lastState.freqStart);
-    if (state.skipOffset != -1) {
-      assert state.skipOffset > 0;
-      out.writeVLong(state.skipOffset);
-    }
-    if (indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) {
-      out.writeVLong(state.proxStart - lastState.proxStart);
-    }
-    lastState = state;
-  }
-
-  @Override
-  public void close() throws IOException {
-    try {
-      freqOut.close();
-    } finally {
-      if (proxOut != null) {
-        proxOut.close();
-      }
-    }
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWCodec.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWCodec.java
deleted file mode 100644
index 56f2d39..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWCodec.java
+++ /dev/null
@@ -1,80 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.SegmentInfoFormat;
-import org.apache.lucene.codecs.StoredFieldsFormat;
-import org.apache.lucene.codecs.TermVectorsFormat;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Read-write version of 4.0 codec for testing
- * @deprecated for test purposes only
- */
-@Deprecated
-public final class Lucene40RWCodec extends Lucene40Codec {
-  
-  private final FieldInfosFormat fieldInfos = new Lucene40RWFieldInfosFormat();
-  private final DocValuesFormat docValues = new Lucene40RWDocValuesFormat();
-  private final NormsFormat norms = new Lucene40RWNormsFormat();
-  private final StoredFieldsFormat stored = new Lucene40RWStoredFieldsFormat();
-  private final TermVectorsFormat vectors = new Lucene40RWTermVectorsFormat();
-  private final PostingsFormat postings = new Lucene40RWPostingsFormat();
-  
-  @Override
-  public FieldInfosFormat fieldInfosFormat() {
-    return fieldInfos;
-  }
-
-  @Override
-  public DocValuesFormat docValuesFormat() {
-    return docValues;
-  }
-
-  @Override
-  public NormsFormat normsFormat() {
-    return norms;
-  }
-
-  @Override
-  public StoredFieldsFormat storedFieldsFormat() {
-    return stored;
-  }
-
-  @Override
-  public TermVectorsFormat termVectorsFormat() {
-    return vectors;
-  }
-
-  @Override
-  public PostingsFormat getPostingsFormatForField(String field) {
-    return postings;
-  }
-  
-  private static final SegmentInfoFormat segmentInfos = new Lucene40RWSegmentInfoFormat();
-
-  @Override
-  public SegmentInfoFormat segmentInfoFormat() {
-    return segmentInfos;
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWDocValuesFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWDocValuesFormat.java
deleted file mode 100644
index f6062e5..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWDocValuesFormat.java
+++ /dev/null
@@ -1,40 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-
-/**
- * Read-write version of 4.0 docvalues format for testing
- * @deprecated for test purposes only
- */
-@Deprecated
-public final class Lucene40RWDocValuesFormat extends Lucene40DocValuesFormat {
-
-  @Override
-  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    String filename = IndexFileNames.segmentFileName(state.segmentInfo.name, 
-          "dv", 
-          Lucene40CompoundFormat.COMPOUND_FILE_EXTENSION);
-    return new Lucene40DocValuesWriter(state, filename, Lucene40FieldInfosFormat.LEGACY_DV_TYPE_KEY);
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWFieldInfosFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWFieldInfosFormat.java
deleted file mode 100644
index ea671b8..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWFieldInfosFormat.java
+++ /dev/null
@@ -1,104 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.IOException;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.index.FieldInfo.DocValuesType;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Writer for 4.0 fieldinfos format
- * @deprecated for test purposes only
- */
-@Deprecated
-public final class Lucene40RWFieldInfosFormat extends Lucene40FieldInfosFormat {
-
-  /** Sole constructor. */
-  public Lucene40RWFieldInfosFormat() {
-  }
-  
-  @Override
-  public void write(Directory directory, SegmentInfo segmentInfo, String segmentSuffix, FieldInfos infos, IOContext context) throws IOException {
-    if (!segmentSuffix.isEmpty()) {
-      throw new UnsupportedOperationException("4.0 does not support fieldinfo updates");
-    }
-    final String fileName = IndexFileNames.segmentFileName(segmentInfo.name, "", Lucene40FieldInfosFormat.FIELD_INFOS_EXTENSION);
-    IndexOutput output = directory.createOutput(fileName, context);
-    boolean success = false;
-    try {
-      CodecUtil.writeHeader(output, Lucene40FieldInfosFormat.CODEC_NAME, Lucene40FieldInfosFormat.FORMAT_CURRENT);
-      output.writeVInt(infos.size());
-      for (FieldInfo fi : infos) {
-        IndexOptions indexOptions = fi.getIndexOptions();
-        byte bits = 0x0;
-        if (fi.hasVectors()) bits |= Lucene40FieldInfosFormat.STORE_TERMVECTOR;
-        if (fi.omitsNorms()) bits |= Lucene40FieldInfosFormat.OMIT_NORMS;
-        if (fi.hasPayloads()) bits |= Lucene40FieldInfosFormat.STORE_PAYLOADS;
-        if (fi.isIndexed()) {
-          bits |= Lucene40FieldInfosFormat.IS_INDEXED;
-          assert indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 || !fi.hasPayloads();
-          if (indexOptions == IndexOptions.DOCS_ONLY) {
-            bits |= Lucene40FieldInfosFormat.OMIT_TERM_FREQ_AND_POSITIONS;
-          } else if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) {
-            bits |= Lucene40FieldInfosFormat.STORE_OFFSETS_IN_POSTINGS;
-          } else if (indexOptions == IndexOptions.DOCS_AND_FREQS) {
-            bits |= Lucene40FieldInfosFormat.OMIT_POSITIONS;
-          }
-        }
-        output.writeString(fi.name);
-        output.writeVInt(fi.number);
-        output.writeByte(bits);
-
-        // pack the DV types in one byte
-        final byte dv = docValuesByte(fi.getDocValuesType(), fi.getAttribute(LEGACY_DV_TYPE_KEY));
-        final byte nrm = docValuesByte(fi.hasNorms() ? DocValuesType.NUMERIC : null, fi.getAttribute(LEGACY_NORM_TYPE_KEY));
-        assert (dv & (~0xF)) == 0 && (nrm & (~0x0F)) == 0;
-        byte val = (byte) (0xff & ((nrm << 4) | dv));
-        output.writeByte(val);
-        output.writeStringStringMap(fi.attributes());
-      }
-      success = true;
-    } finally {
-      if (success) {
-        output.close();
-      } else {
-        IOUtils.closeWhileHandlingException(output);
-      }
-    }
-  }
-  
-  /** 4.0-style docvalues byte */
-  public byte docValuesByte(DocValuesType type, String legacyTypeAtt) {
-    if (type == null) {
-      assert legacyTypeAtt == null;
-      return 0;
-    } else {
-      assert legacyTypeAtt != null;
-      return (byte) LegacyDocValuesType.valueOf(legacyTypeAtt).ordinal();
-    }
-  }  
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWNormsFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWNormsFormat.java
deleted file mode 100644
index 49468a2..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWNormsFormat.java
+++ /dev/null
@@ -1,52 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.NormsConsumer;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-
-/**
- * Read-write version of 4.0 norms format for testing
- * @deprecated for test purposes only
- */
-@Deprecated
-public final class Lucene40RWNormsFormat extends Lucene40NormsFormat {
-
-  @Override
-  public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
-    String filename = IndexFileNames.segmentFileName(state.segmentInfo.name, 
-        "nrm", 
-        Lucene40CompoundFormat.COMPOUND_FILE_EXTENSION);
-    final Lucene40DocValuesWriter impl = new Lucene40DocValuesWriter(state, filename, Lucene40FieldInfosFormat.LEGACY_NORM_TYPE_KEY);
-    return new NormsConsumer() {
-      @Override
-      public void addNormsField(FieldInfo field, Iterable<Number> values) throws IOException {
-        impl.addNumericField(field, values);
-      }
-      
-      @Override
-      public void close() throws IOException {
-        impl.close();
-      }
-    };
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWPostingsFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWPostingsFormat.java
deleted file mode 100644
index 4b69af8..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWPostingsFormat.java
+++ /dev/null
@@ -1,58 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.FieldsConsumer;
-import org.apache.lucene.codecs.PostingsWriterBase;
-import org.apache.lucene.codecs.blocktree.Lucene40BlockTreeTermsWriter;
-import org.apache.lucene.index.SegmentWriteState;
-
-/**
- * Read-write version of 4.0 postings format for testing
- * @deprecated for test purposes only
- */
-@Deprecated
-public final class Lucene40RWPostingsFormat extends Lucene40PostingsFormat {
-  
-  /** minimum items (terms or sub-blocks) per block for 4.0 BlockTree */
-  final static int MIN_BLOCK_SIZE = 25;
-  /** maximum items (terms or sub-blocks) per block for 4.0 BlockTree */
-  final static int MAX_BLOCK_SIZE = 48;
-
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    PostingsWriterBase docs = new Lucene40PostingsWriter(state);
-    
-    // TODO: should we make the terms index more easily
-    // pluggable?  Ie so that this codec would record which
-    // index impl was used, and switch on loading?
-    // Or... you must make a new Codec for this?
-    boolean success = false;
-    try {
-      FieldsConsumer ret = new Lucene40BlockTreeTermsWriter(state, docs, MIN_BLOCK_SIZE, MAX_BLOCK_SIZE);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        docs.close();
-      }
-    }
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWSegmentInfoFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWSegmentInfoFormat.java
deleted file mode 100644
index 732e947..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWSegmentInfoFormat.java
+++ /dev/null
@@ -1,68 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collections;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Read-write version of 4.0 segmentinfo format for testing
- * @deprecated for test purposes only
- */
-@Deprecated
-public final class Lucene40RWSegmentInfoFormat extends Lucene40SegmentInfoFormat {
-
-  @Override
-  public void write(Directory dir, SegmentInfo si, IOContext ioContext) throws IOException {
-    final String fileName = IndexFileNames.segmentFileName(si.name, "", Lucene40SegmentInfoFormat.SI_EXTENSION);
-    si.addFile(fileName);
-
-    final IndexOutput output = dir.createOutput(fileName, ioContext);
-
-    boolean success = false;
-    try {
-      CodecUtil.writeHeader(output, Lucene40SegmentInfoFormat.CODEC_NAME, Lucene40SegmentInfoFormat.VERSION_CURRENT);
-      // Write the Lucene version that created this segment, since 3.1
-      output.writeString(si.getVersion().toString());
-      output.writeInt(si.getDocCount());
-
-      output.writeByte((byte) (si.getUseCompoundFile() ? SegmentInfo.YES : SegmentInfo.NO));
-      output.writeStringStringMap(si.getDiagnostics());
-      output.writeStringStringMap(Collections.<String,String>emptyMap());
-      output.writeStringSet(si.files());
-
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(output);
-        // TODO: why must we do this? do we not get tracking dir wrapper?
-        IOUtils.deleteFilesIgnoringExceptions(si.dir, fileName);
-      } else {
-        output.close();
-      }
-    }
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWStoredFieldsFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWStoredFieldsFormat.java
deleted file mode 100644
index 3e4f790..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWStoredFieldsFormat.java
+++ /dev/null
@@ -1,38 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.StoredFieldsWriter;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-/**
- * Read-write version of 4.0 stored fields format for testing
- * @deprecated for test purposes only
- */
-@Deprecated
-final class Lucene40RWStoredFieldsFormat extends Lucene40StoredFieldsFormat {
-
-  @Override
-  public StoredFieldsWriter fieldsWriter(Directory directory, SegmentInfo si, IOContext context) throws IOException {
-    return new Lucene40StoredFieldsWriter(directory, si.name, context);
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWTermVectorsFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWTermVectorsFormat.java
deleted file mode 100644
index 9c62465..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWTermVectorsFormat.java
+++ /dev/null
@@ -1,38 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.TermVectorsWriter;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-/**
- * Read-write version of 4.0 term vectors format for testing
- * @deprecated for test purposes only
- */
-@Deprecated
-public final class Lucene40RWTermVectorsFormat extends Lucene40TermVectorsFormat {
-
-  @Override
-  public TermVectorsWriter vectorsWriter(Directory directory, SegmentInfo segmentInfo, IOContext context) throws IOException {
-    return new Lucene40TermVectorsWriter(directory, segmentInfo.name, context);
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40SkipListWriter.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40SkipListWriter.java
deleted file mode 100644
index 444813b..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40SkipListWriter.java
+++ /dev/null
@@ -1,150 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.codecs.MultiLevelSkipListWriter;
-
-
-/**
- * Writer of 4.0 skip lists for testing
- * @deprecated for test purposes only
- */
-@Deprecated
-final class Lucene40SkipListWriter extends MultiLevelSkipListWriter {
-  private int[] lastSkipDoc;
-  private int[] lastSkipPayloadLength;
-  private int[] lastSkipOffsetLength;
-  private long[] lastSkipFreqPointer;
-  private long[] lastSkipProxPointer;
-  
-  private IndexOutput freqOutput;
-  private IndexOutput proxOutput;
-
-  private int curDoc;
-  private boolean curStorePayloads;
-  private boolean curStoreOffsets;
-  private int curPayloadLength;
-  private int curOffsetLength;
-  private long curFreqPointer;
-  private long curProxPointer;
-
-  /** Sole constructor. */
-  public Lucene40SkipListWriter(int skipInterval, int numberOfSkipLevels, int docCount, IndexOutput freqOutput, IndexOutput proxOutput) {
-    super(skipInterval, numberOfSkipLevels, docCount);
-    this.freqOutput = freqOutput;
-    this.proxOutput = proxOutput;
-    
-    lastSkipDoc = new int[numberOfSkipLevels];
-    lastSkipPayloadLength = new int[numberOfSkipLevels];
-    lastSkipOffsetLength = new int[numberOfSkipLevels];
-    lastSkipFreqPointer = new long[numberOfSkipLevels];
-    lastSkipProxPointer = new long[numberOfSkipLevels];
-  }
-
-  /**
-   * Sets the values for the current skip data. 
-   */
-  public void setSkipData(int doc, boolean storePayloads, int payloadLength, boolean storeOffsets, int offsetLength) {
-    assert storePayloads || payloadLength == -1;
-    assert storeOffsets  || offsetLength == -1;
-    this.curDoc = doc;
-    this.curStorePayloads = storePayloads;
-    this.curPayloadLength = payloadLength;
-    this.curStoreOffsets = storeOffsets;
-    this.curOffsetLength = offsetLength;
-    this.curFreqPointer = freqOutput.getFilePointer();
-    if (proxOutput != null)
-      this.curProxPointer = proxOutput.getFilePointer();
-  }
-
-  @Override
-  public void resetSkip() {
-    super.resetSkip();
-    Arrays.fill(lastSkipDoc, 0);
-    Arrays.fill(lastSkipPayloadLength, -1);  // we don't have to write the first length in the skip list
-    Arrays.fill(lastSkipOffsetLength, -1);  // we don't have to write the first length in the skip list
-    Arrays.fill(lastSkipFreqPointer, freqOutput.getFilePointer());
-    if (proxOutput != null)
-      Arrays.fill(lastSkipProxPointer, proxOutput.getFilePointer());
-  }
-  
-  @Override
-  protected void writeSkipData(int level, IndexOutput skipBuffer) throws IOException {
-    // To efficiently store payloads/offsets in the posting lists we do not store the length of
-    // every payload/offset. Instead we omit the length if the previous lengths were the same
-    //
-    // However, in order to support skipping, the length at every skip point must be known.
-    // So we use the same length encoding that we use for the posting lists for the skip data as well:
-    // Case 1: current field does not store payloads/offsets
-    //           SkipDatum                 --> DocSkip, FreqSkip, ProxSkip
-    //           DocSkip,FreqSkip,ProxSkip --> VInt
-    //           DocSkip records the document number before every SkipInterval th  document in TermFreqs. 
-    //           Document numbers are represented as differences from the previous value in the sequence.
-    // Case 2: current field stores payloads/offsets
-    //           SkipDatum                 --> DocSkip, PayloadLength?,OffsetLength?,FreqSkip,ProxSkip
-    //           DocSkip,FreqSkip,ProxSkip --> VInt
-    //           PayloadLength,OffsetLength--> VInt    
-    //         In this case DocSkip/2 is the difference between
-    //         the current and the previous value. If DocSkip
-    //         is odd, then a PayloadLength encoded as VInt follows,
-    //         if DocSkip is even, then it is assumed that the
-    //         current payload/offset lengths equals the lengths at the previous
-    //         skip point
-    int delta = curDoc - lastSkipDoc[level];
-    
-    if (curStorePayloads || curStoreOffsets) {
-      assert curStorePayloads || curPayloadLength == lastSkipPayloadLength[level];
-      assert curStoreOffsets  || curOffsetLength == lastSkipOffsetLength[level];
-
-      if (curPayloadLength == lastSkipPayloadLength[level] && curOffsetLength == lastSkipOffsetLength[level]) {
-        // the current payload/offset lengths equals the lengths at the previous skip point,
-        // so we don't store the lengths again
-        skipBuffer.writeVInt(delta << 1);
-      } else {
-        // the payload and/or offset length is different from the previous one. We shift the DocSkip, 
-        // set the lowest bit and store the current payload and/or offset lengths as VInts.
-        skipBuffer.writeVInt(delta << 1 | 1);
-
-        if (curStorePayloads) {
-          skipBuffer.writeVInt(curPayloadLength);
-          lastSkipPayloadLength[level] = curPayloadLength;
-        }
-        if (curStoreOffsets) {
-          skipBuffer.writeVInt(curOffsetLength);
-          lastSkipOffsetLength[level] = curOffsetLength;
-        }
-      }
-    } else {
-      // current field does not store payloads or offsets
-      skipBuffer.writeVInt(delta);
-    }
-
-    skipBuffer.writeVInt((int) (curFreqPointer - lastSkipFreqPointer[level]));
-    skipBuffer.writeVInt((int) (curProxPointer - lastSkipProxPointer[level]));
-
-    lastSkipDoc[level] = curDoc;
-    
-    lastSkipFreqPointer[level] = curFreqPointer;
-    lastSkipProxPointer[level] = curProxPointer;
-  }
-
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsWriter.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsWriter.java
deleted file mode 100644
index 71d6de2..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsWriter.java
+++ /dev/null
@@ -1,185 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/**
- * Copyright 2004 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License"); you may not
- * use this file except in compliance with the License. You may obtain a copy of
- * the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.StoredFieldsWriter;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.StorableField;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.RAMOutputStream;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-
-import static org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsReader.*;
-
-
-/**
- * Writer for 4.0 stored fields format for testing
- * @deprecated for test purposes only
- */
-@Deprecated
-final class Lucene40StoredFieldsWriter extends StoredFieldsWriter {
-
-  private final Directory directory;
-  private final String segment;
-  private IndexOutput fieldsStream;
-  private IndexOutput indexStream;
-  private final RAMOutputStream fieldsBuffer = new RAMOutputStream();
-
-  /** Sole constructor. */
-  public Lucene40StoredFieldsWriter(Directory directory, String segment, IOContext context) throws IOException {
-    assert directory != null;
-    this.directory = directory;
-    this.segment = segment;
-
-    boolean success = false;
-    try {
-      fieldsStream = directory.createOutput(IndexFileNames.segmentFileName(segment, "", FIELDS_EXTENSION), context);
-      indexStream = directory.createOutput(IndexFileNames.segmentFileName(segment, "", FIELDS_INDEX_EXTENSION), context);
-
-      CodecUtil.writeHeader(fieldsStream, CODEC_NAME_DAT, VERSION_CURRENT);
-      CodecUtil.writeHeader(indexStream, CODEC_NAME_IDX, VERSION_CURRENT);
-      assert HEADER_LENGTH_DAT == fieldsStream.getFilePointer();
-      assert HEADER_LENGTH_IDX == indexStream.getFilePointer();
-      success = true;
-    } finally {
-      if (!success) {
-        abort();
-      }
-    }
-  }
-
-  int numStoredFields;
-
-  // Writes the contents of buffer into the fields stream
-  // and adds a new entry for this document into the index
-  // stream.  This assumes the buffer was already written
-  // in the correct fields format.
-  @Override
-  public void startDocument() throws IOException {
-    indexStream.writeLong(fieldsStream.getFilePointer());
-  }
-
-  @Override
-  public void finishDocument() throws IOException {
-    fieldsStream.writeVInt(numStoredFields);
-    fieldsBuffer.writeTo(fieldsStream);
-    fieldsBuffer.reset();
-    numStoredFields = 0;
-  }
-
-  @Override
-  public void close() throws IOException {
-    try {
-      IOUtils.close(fieldsStream, indexStream);
-    } finally {
-      fieldsStream = indexStream = null;
-    }
-  }
-
-  @Override
-  public void abort() {
-    try {
-      close();
-    } catch (Throwable ignored) {}
-    IOUtils.deleteFilesIgnoringExceptions(directory,
-        IndexFileNames.segmentFileName(segment, "", FIELDS_EXTENSION),
-        IndexFileNames.segmentFileName(segment, "", FIELDS_INDEX_EXTENSION));
-  }
-
-  @Override
-  public void writeField(FieldInfo info, StorableField field) throws IOException {
-    numStoredFields++;
-
-    fieldsBuffer.writeVInt(info.number);
-    int bits = 0;
-    final BytesRef bytes;
-    final String string;
-    // TODO: maybe a field should serialize itself?
-    // this way we don't bake into indexer all these
-    // specific encodings for different fields?  and apps
-    // can customize...
-
-    Number number = field.numericValue();
-    if (number != null) {
-      if (number instanceof Byte || number instanceof Short || number instanceof Integer) {
-        bits |= FIELD_IS_NUMERIC_INT;
-      } else if (number instanceof Long) {
-        bits |= FIELD_IS_NUMERIC_LONG;
-      } else if (number instanceof Float) {
-        bits |= FIELD_IS_NUMERIC_FLOAT;
-      } else if (number instanceof Double) {
-        bits |= FIELD_IS_NUMERIC_DOUBLE;
-      } else {
-        throw new IllegalArgumentException("cannot store numeric type " + number.getClass());
-      }
-      string = null;
-      bytes = null;
-    } else {
-      bytes = field.binaryValue();
-      if (bytes != null) {
-        bits |= FIELD_IS_BINARY;
-        string = null;
-      } else {
-        string = field.stringValue();
-        if (string == null) {
-          throw new IllegalArgumentException("field " + field.name() + " is stored but does not have binaryValue, stringValue nor numericValue");
-        }
-      }
-    }
-
-    fieldsBuffer.writeByte((byte) bits);
-
-    if (bytes != null) {
-      fieldsBuffer.writeVInt(bytes.length);
-      fieldsBuffer.writeBytes(bytes.bytes, bytes.offset, bytes.length);
-    } else if (string != null) {
-      fieldsBuffer.writeString(field.stringValue());
-    } else {
-      if (number instanceof Byte || number instanceof Short || number instanceof Integer) {
-        fieldsBuffer.writeInt(number.intValue());
-      } else if (number instanceof Long) {
-        fieldsBuffer.writeLong(number.longValue());
-      } else if (number instanceof Float) {
-        fieldsBuffer.writeInt(Float.floatToIntBits(number.floatValue()));
-      } else if (number instanceof Double) {
-        fieldsBuffer.writeLong(Double.doubleToLongBits(number.doubleValue()));
-      } else {
-        throw new AssertionError("Cannot get here");
-      }
-    }
-  }
-
-  @Override
-  public void finish(FieldInfos fis, int numDocs) {
-    long indexFP = indexStream.getFilePointer();
-    if (HEADER_LENGTH_IDX+((long) numDocs)*8 != indexFP)
-      // This is most likely a bug in Sun JRE 1.6.0_04/_05;
-      // we detect that the bug has struck, here, and
-      // throw an exception to prevent the corruption from
-      // entering the index.  See LUCENE-1282 for
-      // details.
-      throw new RuntimeException("fdx size mismatch: docCount is " + numDocs + " but fdx file size is " + indexFP + " (wrote numDocs=" + ((indexFP-HEADER_LENGTH_IDX)/8.0) + " file=" + indexStream.toString() + "; now aborting this merge to prevent index corruption");
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter.java
deleted file mode 100644
index 69ec8c1..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter.java
+++ /dev/null
@@ -1,291 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.TermVectorsWriter;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.store.DataInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.BytesRefBuilder;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.StringHelper;
-
-import static org.apache.lucene.codecs.lucene40.Lucene40TermVectorsReader.*;
-
-/**
- * Writer for 4.0 term vectors format for testing
- * @deprecated for test purposes only
- */
-@Deprecated
-final class Lucene40TermVectorsWriter extends TermVectorsWriter {
-  private final Directory directory;
-  private final String segment;
-  private IndexOutput tvx = null, tvd = null, tvf = null;
-  
-  /** Sole constructor. */
-  public Lucene40TermVectorsWriter(Directory directory, String segment, IOContext context) throws IOException {
-    this.directory = directory;
-    this.segment = segment;
-    boolean success = false;
-    try {
-      // Open files for TermVector storage
-      tvx = directory.createOutput(IndexFileNames.segmentFileName(segment, "", Lucene40TermVectorsReader.VECTORS_INDEX_EXTENSION), context);
-      CodecUtil.writeHeader(tvx, CODEC_NAME_INDEX, VERSION_CURRENT);
-      tvd = directory.createOutput(IndexFileNames.segmentFileName(segment, "", Lucene40TermVectorsReader.VECTORS_DOCUMENTS_EXTENSION), context);
-      CodecUtil.writeHeader(tvd, CODEC_NAME_DOCS, VERSION_CURRENT);
-      tvf = directory.createOutput(IndexFileNames.segmentFileName(segment, "", Lucene40TermVectorsReader.VECTORS_FIELDS_EXTENSION), context);
-      CodecUtil.writeHeader(tvf, CODEC_NAME_FIELDS, VERSION_CURRENT);
-      assert HEADER_LENGTH_INDEX == tvx.getFilePointer();
-      assert HEADER_LENGTH_DOCS == tvd.getFilePointer();
-      assert HEADER_LENGTH_FIELDS == tvf.getFilePointer();
-      success = true;
-    } finally {
-      if (!success) {
-        abort();
-      }
-    }
-  }
- 
-  @Override
-  public void startDocument(int numVectorFields) throws IOException {
-    lastFieldName = null;
-    this.numVectorFields = numVectorFields;
-    tvx.writeLong(tvd.getFilePointer());
-    tvx.writeLong(tvf.getFilePointer());
-    tvd.writeVInt(numVectorFields);
-    fieldCount = 0;
-    fps = ArrayUtil.grow(fps, numVectorFields);
-  }
-  
-  private long fps[] = new long[10]; // pointers to the tvf before writing each field 
-  private int fieldCount = 0;        // number of fields we have written so far for this document
-  private int numVectorFields = 0;   // total number of fields we will write for this document
-  private String lastFieldName;
-
-  @Override
-  public void startField(FieldInfo info, int numTerms, boolean positions, boolean offsets, boolean payloads) throws IOException {
-    assert lastFieldName == null || info.name.compareTo(lastFieldName) > 0: "fieldName=" + info.name + " lastFieldName=" + lastFieldName;
-    lastFieldName = info.name;
-    this.positions = positions;
-    this.offsets = offsets;
-    this.payloads = payloads;
-    lastTerm.clear();
-    lastPayloadLength = -1; // force first payload to write its length
-    fps[fieldCount++] = tvf.getFilePointer();
-    tvd.writeVInt(info.number);
-    tvf.writeVInt(numTerms);
-    byte bits = 0x0;
-    if (positions)
-      bits |= Lucene40TermVectorsReader.STORE_POSITIONS_WITH_TERMVECTOR;
-    if (offsets)
-      bits |= Lucene40TermVectorsReader.STORE_OFFSET_WITH_TERMVECTOR;
-    if (payloads)
-      bits |= Lucene40TermVectorsReader.STORE_PAYLOAD_WITH_TERMVECTOR;
-    tvf.writeByte(bits);
-  }
-  
-  @Override
-  public void finishDocument() throws IOException {
-    assert fieldCount == numVectorFields;
-    for (int i = 1; i < fieldCount; i++) {
-      tvd.writeVLong(fps[i] - fps[i-1]);
-    }
-  }
-
-  private final BytesRefBuilder lastTerm = new BytesRefBuilder();
-
-  // NOTE: we override addProx, so we don't need to buffer when indexing.
-  // we also don't buffer during bulk merges.
-  private int offsetStartBuffer[] = new int[10];
-  private int offsetEndBuffer[] = new int[10];
-  private BytesRefBuilder payloadData = new BytesRefBuilder();
-  private int bufferedIndex = 0;
-  private int bufferedFreq = 0;
-  private boolean positions = false;
-  private boolean offsets = false;
-  private boolean payloads = false;
-
-  @Override
-  public void startTerm(BytesRef term, int freq) throws IOException {
-    final int prefix = StringHelper.bytesDifference(lastTerm.get(), term);
-    final int suffix = term.length - prefix;
-    tvf.writeVInt(prefix);
-    tvf.writeVInt(suffix);
-    tvf.writeBytes(term.bytes, term.offset + prefix, suffix);
-    tvf.writeVInt(freq);
-    lastTerm.copyBytes(term);
-    lastPosition = lastOffset = 0;
-    
-    if (offsets && positions) {
-      // we might need to buffer if its a non-bulk merge
-      offsetStartBuffer = ArrayUtil.grow(offsetStartBuffer, freq);
-      offsetEndBuffer = ArrayUtil.grow(offsetEndBuffer, freq);
-    }
-    bufferedIndex = 0;
-    bufferedFreq = freq;
-    payloadData.clear();
-  }
-
-  int lastPosition = 0;
-  int lastOffset = 0;
-  int lastPayloadLength = -1; // force first payload to write its length
-
-  BytesRefBuilder scratch = new BytesRefBuilder(); // used only by this optimized flush below
-
-  @Override
-  public void addProx(int numProx, DataInput positions, DataInput offsets) throws IOException {
-    if (payloads) {
-      // TODO, maybe overkill and just call super.addProx() in this case?
-      // we do avoid buffering the offsets in RAM though.
-      for (int i = 0; i < numProx; i++) {
-        int code = positions.readVInt();
-        if ((code & 1) == 1) {
-          int length = positions.readVInt();
-          scratch.grow(length);
-          scratch.setLength(length);
-          positions.readBytes(scratch.bytes(), 0, scratch.length());
-          writePosition(code >>> 1, scratch.get());
-        } else {
-          writePosition(code >>> 1, null);
-        }
-      }
-      tvf.writeBytes(payloadData.bytes(), 0, payloadData.length());
-    } else if (positions != null) {
-      // pure positions, no payloads
-      for (int i = 0; i < numProx; i++) {
-        tvf.writeVInt(positions.readVInt() >>> 1);
-      }
-    }
-    
-    if (offsets != null) {
-      for (int i = 0; i < numProx; i++) {
-        tvf.writeVInt(offsets.readVInt());
-        tvf.writeVInt(offsets.readVInt());
-      }
-    }
-  }
-
-  @Override
-  public void addPosition(int position, int startOffset, int endOffset, BytesRef payload) throws IOException {
-    if (positions && (offsets || payloads)) {
-      // write position delta
-      writePosition(position - lastPosition, payload);
-      lastPosition = position;
-      
-      // buffer offsets
-      if (offsets) {
-        offsetStartBuffer[bufferedIndex] = startOffset;
-        offsetEndBuffer[bufferedIndex] = endOffset;
-      }
-      
-      bufferedIndex++;
-    } else if (positions) {
-      // write position delta
-      writePosition(position - lastPosition, payload);
-      lastPosition = position;
-    } else if (offsets) {
-      // write offset deltas
-      tvf.writeVInt(startOffset - lastOffset);
-      tvf.writeVInt(endOffset - startOffset);
-      lastOffset = endOffset;
-    }
-  }
-  
-  @Override
-  public void finishTerm() throws IOException {
-    if (bufferedIndex > 0) {
-      // dump buffer
-      assert positions && (offsets || payloads);
-      assert bufferedIndex == bufferedFreq;
-      if (payloads) {
-        tvf.writeBytes(payloadData.bytes(), 0, payloadData.length());
-      }
-      if (offsets) {
-        for (int i = 0; i < bufferedIndex; i++) {
-          tvf.writeVInt(offsetStartBuffer[i] - lastOffset);
-          tvf.writeVInt(offsetEndBuffer[i] - offsetStartBuffer[i]);
-          lastOffset = offsetEndBuffer[i];
-        }
-      }
-    }
-  }
-
-  private void writePosition(int delta, BytesRef payload) throws IOException {
-    if (payloads) {
-      int payloadLength = payload == null ? 0 : payload.length;
-
-      if (payloadLength != lastPayloadLength) {
-        lastPayloadLength = payloadLength;
-        tvf.writeVInt((delta<<1)|1);
-        tvf.writeVInt(payloadLength);
-      } else {
-        tvf.writeVInt(delta << 1);
-      }
-      if (payloadLength > 0) {
-        if (payloadLength + payloadData.length() < 0) {
-          // we overflowed the payload buffer, just throw UOE
-          // having > Integer.MAX_VALUE bytes of payload for a single term in a single doc is nuts.
-          throw new UnsupportedOperationException("A term cannot have more than Integer.MAX_VALUE bytes of payload data in a single document");
-        }
-        payloadData.append(payload);
-      }
-    } else {
-      tvf.writeVInt(delta);
-    }
-  }
-
-  @Override
-  public void abort() {
-    try {
-      close();
-    } catch (Throwable ignored) {}
-    IOUtils.deleteFilesIgnoringExceptions(directory, IndexFileNames.segmentFileName(segment, "", Lucene40TermVectorsReader.VECTORS_INDEX_EXTENSION),
-        IndexFileNames.segmentFileName(segment, "", Lucene40TermVectorsReader.VECTORS_DOCUMENTS_EXTENSION),
-        IndexFileNames.segmentFileName(segment, "", Lucene40TermVectorsReader.VECTORS_FIELDS_EXTENSION));
-  }
-  
-  @Override
-  public void finish(FieldInfos fis, int numDocs) {
-    long indexFP = tvx.getFilePointer();
-    if (HEADER_LENGTH_INDEX+((long) numDocs)*16 != indexFP)
-      // This is most likely a bug in Sun JRE 1.6.0_04/_05;
-      // we detect that the bug has struck, here, and
-      // throw an exception to prevent the corruption from
-      // entering the index.  See LUCENE-1282 for
-      // details.
-      throw new RuntimeException("tvx size mismatch: mergedDocs is " + numDocs + " but tvx size is " + indexFP + " (wrote numDocs=" + ((indexFP - HEADER_LENGTH_INDEX)/16.0) + " file=" + tvx.toString() + "; now aborting this merge to prevent index corruption");
-  }
-
-  /** Close all streams. */
-  @Override
-  public void close() throws IOException {
-    // make an effort to close all streams we can but remember and re-throw
-    // the first exception encountered in this process
-    IOUtils.close(tvx, tvd, tvf);
-    tvx = tvd = tvf = null;
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestBitVector.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestBitVector.java
deleted file mode 100644
index 0ce482e..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestBitVector.java
+++ /dev/null
@@ -1,282 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.MockDirectoryWrapper;
-import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.TestUtil;
-
-/**
- * <code>TestBitVector</code> tests the <code>BitVector</code>, obviously.
- */
-public class TestBitVector extends LuceneTestCase
-{
-
-    /**
-     * Test the default constructor on BitVectors of various sizes.
-     */
-    public void testConstructSize() throws Exception {
-        doTestConstructOfSize(8);
-        doTestConstructOfSize(20);
-        doTestConstructOfSize(100);
-        doTestConstructOfSize(1000);
-    }
-
-    private void doTestConstructOfSize(int n) {
-        BitVector bv = new BitVector(n);
-        assertEquals(n,bv.size());
-    }
-
-    /**
-     * Test the get() and set() methods on BitVectors of various sizes.
-     */
-    public void testGetSet() throws Exception {
-        doTestGetSetVectorOfSize(8);
-        doTestGetSetVectorOfSize(20);
-        doTestGetSetVectorOfSize(100);
-        doTestGetSetVectorOfSize(1000);
-    }
-
-    private void doTestGetSetVectorOfSize(int n) {
-        BitVector bv = new BitVector(n);
-        for(int i=0;i<bv.size();i++) {
-            // ensure a set bit can be git'
-            assertFalse(bv.get(i));
-            bv.set(i);
-            assertTrue(bv.get(i));
-        }
-    }
-
-    /**
-     * Test the clear() method on BitVectors of various sizes.
-     */
-    public void testClear() throws Exception {
-        doTestClearVectorOfSize(8);
-        doTestClearVectorOfSize(20);
-        doTestClearVectorOfSize(100);
-        doTestClearVectorOfSize(1000);
-    }
-
-    private void doTestClearVectorOfSize(int n) {
-        BitVector bv = new BitVector(n);
-        for(int i=0;i<bv.size();i++) {
-            // ensure a set bit is cleared
-            assertFalse(bv.get(i));
-            bv.set(i);
-            assertTrue(bv.get(i));
-            bv.clear(i);
-            assertFalse(bv.get(i));
-        }
-    }
-
-    /**
-     * Test the count() method on BitVectors of various sizes.
-     */
-    public void testCount() throws Exception {
-        doTestCountVectorOfSize(8);
-        doTestCountVectorOfSize(20);
-        doTestCountVectorOfSize(100);
-        doTestCountVectorOfSize(1000);
-    }
-
-    private void doTestCountVectorOfSize(int n) {
-        BitVector bv = new BitVector(n);
-        // test count when incrementally setting bits
-        for(int i=0;i<bv.size();i++) {
-            assertFalse(bv.get(i));
-            assertEquals(i,bv.count());
-            bv.set(i);
-            assertTrue(bv.get(i));
-            assertEquals(i+1,bv.count());
-        }
-
-        bv = new BitVector(n);
-        // test count when setting then clearing bits
-        for(int i=0;i<bv.size();i++) {
-            assertFalse(bv.get(i));
-            assertEquals(0,bv.count());
-            bv.set(i);
-            assertTrue(bv.get(i));
-            assertEquals(1,bv.count());
-            bv.clear(i);
-            assertFalse(bv.get(i));
-            assertEquals(0,bv.count());
-        }
-    }
-
-    /**
-     * Test writing and construction to/from Directory.
-     */
-    public void testWriteRead() throws Exception {
-        doTestWriteRead(8);
-        doTestWriteRead(20);
-        doTestWriteRead(100);
-        doTestWriteRead(1000);
-    }
-
-    private void doTestWriteRead(int n) throws Exception {
-        MockDirectoryWrapper d = new  MockDirectoryWrapper(random(), new RAMDirectory());
-        d.setPreventDoubleWrite(false);
-        BitVector bv = new BitVector(n);
-        // test count when incrementally setting bits
-        for(int i=0;i<bv.size();i++) {
-            assertFalse(bv.get(i));
-            assertEquals(i,bv.count());
-            bv.set(i);
-            assertTrue(bv.get(i));
-            assertEquals(i+1,bv.count());
-            bv.write(d, "TESTBV", newIOContext(random()));
-            BitVector compare = new BitVector(d, "TESTBV", newIOContext(random()));
-            // compare bit vectors with bits set incrementally
-            assertTrue(doCompare(bv,compare));
-        }
-    }
-    
-    /**
-     * Test r/w when size/count cause switching between bit-set and d-gaps file formats.  
-     */
-    public void testDgaps() throws IOException {
-      doTestDgaps(1,0,1);
-      doTestDgaps(10,0,1);
-      doTestDgaps(100,0,1);
-      doTestDgaps(1000,4,7);
-      doTestDgaps(10000,40,43);
-      doTestDgaps(100000,415,418);
-      doTestDgaps(1000000,3123,3126);
-      // now exercise skipping of fully populated byte in the bitset (they are omitted if bitset is sparse)
-      MockDirectoryWrapper d = new  MockDirectoryWrapper(random(), new RAMDirectory());
-      d.setPreventDoubleWrite(false);
-      BitVector bv = new BitVector(10000);
-      bv.set(0);
-      for (int i = 8; i < 16; i++) {
-        bv.set(i);
-      } // make sure we have once byte full of set bits
-      for (int i = 32; i < 40; i++) {
-        bv.set(i);
-      } // get a second byte full of set bits
-      // add some more bits here 
-      for (int i = 40; i < 10000; i++) {
-        if (random().nextInt(1000) == 0) {
-          bv.set(i);
-        }
-      }
-      bv.write(d, "TESTBV", newIOContext(random()));
-      BitVector compare = new BitVector(d, "TESTBV", newIOContext(random()));
-      assertTrue(doCompare(bv,compare));
-    }
-    
-    private void doTestDgaps(int size, int count1, int count2) throws IOException {
-      MockDirectoryWrapper d = new  MockDirectoryWrapper(random(), new RAMDirectory());
-      d.setPreventDoubleWrite(false);
-      BitVector bv = new BitVector(size);
-      bv.invertAll();
-      for (int i=0; i<count1; i++) {
-        bv.clear(i);
-        assertEquals(i+1,size-bv.count());
-      }
-      bv.write(d, "TESTBV", newIOContext(random()));
-      // gradually increase number of set bits
-      for (int i=count1; i<count2; i++) {
-        BitVector bv2 = new BitVector(d, "TESTBV", newIOContext(random()));
-        assertTrue(doCompare(bv,bv2));
-        bv = bv2;
-        bv.clear(i);
-        assertEquals(i+1, size-bv.count());
-        bv.write(d, "TESTBV", newIOContext(random()));
-      }
-      // now start decreasing number of set bits
-      for (int i=count2-1; i>=count1; i--) {
-        BitVector bv2 = new BitVector(d, "TESTBV", newIOContext(random()));
-        assertTrue(doCompare(bv,bv2));
-        bv = bv2;
-        bv.set(i);
-        assertEquals(i,size-bv.count());
-        bv.write(d, "TESTBV", newIOContext(random()));
-      }
-    }
-
-    public void testSparseWrite() throws IOException {
-      Directory d = newDirectory();
-      final int numBits = 10240;
-      BitVector bv = new BitVector(numBits);
-      bv.invertAll();
-      int numToClear = random().nextInt(5);
-      for(int i=0;i<numToClear;i++) {
-        bv.clear(random().nextInt(numBits));
-      }
-      bv.write(d, "test", newIOContext(random()));
-      final long size = d.fileLength("test");
-      assertTrue("size=" + size, size < 100);
-      d.close();
-    }
-
-    public void testClearedBitNearEnd() throws IOException {
-      Directory d = newDirectory();
-      final int numBits = TestUtil.nextInt(random(), 7, 1000);
-      BitVector bv = new BitVector(numBits);
-      bv.invertAll();
-      bv.clear(numBits- TestUtil.nextInt(random(), 1, 7));
-      bv.write(d, "test", newIOContext(random()));
-      assertEquals(numBits-1, bv.count());
-      d.close();
-    }
-
-    public void testMostlySet() throws IOException {
-      Directory d = newDirectory();
-      final int numBits = TestUtil.nextInt(random(), 30, 1000);
-      for(int numClear=0;numClear<20;numClear++) {
-        BitVector bv = new BitVector(numBits);
-        bv.invertAll();
-        int count = 0;
-        while(count < numClear) {
-          final int bit = random().nextInt(numBits);
-          // Don't use getAndClear, so that count is recomputed
-          if (bv.get(bit)) {
-            bv.clear(bit);
-            count++;
-            assertEquals(numBits-count, bv.count());
-          }
-        }
-      }
-
-      d.close();
-    }
-
-    /**
-     * Compare two BitVectors.
-     * This should really be an equals method on the BitVector itself.
-     * @param bv One bit vector
-     * @param compare The second to compare
-     */
-    private boolean doCompare(BitVector bv, BitVector compare) {
-        boolean equal = true;
-        for(int i=0;i<bv.size();i++) {
-            // bits must be equal
-            if(bv.get(i)!=compare.get(i)) {
-                equal = false;
-                break;
-            }
-        }
-        assertEquals(bv.count(), compare.count());
-        return equal;
-    }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40CompoundFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40CompoundFormat.java
deleted file mode 100644
index e37c56c..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40CompoundFormat.java
+++ /dev/null
@@ -1,157 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.BaseCompoundFormatTestCase;
-import org.apache.lucene.store.MockDirectoryWrapper;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-
-public class TestLucene40CompoundFormat extends BaseCompoundFormatTestCase {
-  private final Codec codec = new Lucene40RWCodec();
-
-  @Override
-  protected Codec getCodec() {
-    return codec;
-  }
-  
-  // LUCENE-3382 test that delegate compound files correctly.
-  public void testCompoundFileAppendTwice() throws IOException {
-    Directory newDir = newFSDirectory(createTempDir("testCompoundFileAppendTwice"));
-    Directory csw = new Lucene40CompoundReader(newDir, "d.cfs", newIOContext(random()), true);
-    createSequenceFile(newDir, "d1", (byte) 0, 15);
-    IndexOutput out = csw.createOutput("d.xyz", newIOContext(random()));
-    out.writeInt(0);
-    out.close();
-    assertEquals(1, csw.listAll().length);
-    assertEquals("d.xyz", csw.listAll()[0]);
-   
-    csw.close();
-
-    Directory cfr = new Lucene40CompoundReader(newDir, "d.cfs", newIOContext(random()), false);
-    assertEquals(1, cfr.listAll().length);
-    assertEquals("d.xyz", cfr.listAll()[0]);
-    cfr.close();
-    newDir.close();
-  }
-  
-  public void testReadNestedCFP() throws IOException {
-    Directory newDir = newDirectory();
-    // manually manipulates directory
-    if (newDir instanceof MockDirectoryWrapper) {
-      ((MockDirectoryWrapper)newDir).setEnableVirusScanner(false);
-    }
-    Lucene40CompoundReader csw = new Lucene40CompoundReader(newDir, "d.cfs", newIOContext(random()), true);
-    Lucene40CompoundReader nested = new Lucene40CompoundReader(newDir, "b.cfs", newIOContext(random()), true);
-    IndexOutput out = nested.createOutput("b.xyz", newIOContext(random()));
-    IndexOutput out1 = nested.createOutput("b_1.xyz", newIOContext(random()));
-    out.writeInt(0);
-    out1.writeInt(1);
-    out.close();
-    out1.close();
-    nested.close();
-    newDir.copy(csw, "b.cfs", "b.cfs", newIOContext(random()));
-    newDir.copy(csw, "b.cfe", "b.cfe", newIOContext(random()));
-    newDir.deleteFile("b.cfs");
-    newDir.deleteFile("b.cfe");
-    csw.close();
-    
-    assertEquals(2, newDir.listAll().length);
-    csw = new Lucene40CompoundReader(newDir, "d.cfs", newIOContext(random()), false);
-    
-    assertEquals(2, csw.listAll().length);
-    nested = new Lucene40CompoundReader(csw, "b.cfs", newIOContext(random()), false);
-    
-    assertEquals(2, nested.listAll().length);
-    IndexInput openInput = nested.openInput("b.xyz", newIOContext(random()));
-    assertEquals(0, openInput.readInt());
-    openInput.close();
-    openInput = nested.openInput("b_1.xyz", newIOContext(random()));
-    assertEquals(1, openInput.readInt());
-    openInput.close();
-    nested.close();
-    csw.close();
-    newDir.close();
-  }
-  
-  public void testAppend() throws IOException {
-    Directory dir = newDirectory();
-    Directory newDir = newDirectory();
-    Directory csw = new Lucene40CompoundReader(newDir, "d.cfs", newIOContext(random()), true);
-    int size = 5 + random().nextInt(128);
-    for (int j = 0; j < 2; j++) {
-      IndexOutput os = csw.createOutput("seg_" + j + "_foo.txt", newIOContext(random()));
-      for (int i = 0; i < size; i++) {
-        os.writeInt(i*j);
-      }
-      os.close();
-      String[] listAll = newDir.listAll();
-      assertEquals(1, listAll.length);
-      assertEquals("d.cfs", listAll[0]);
-    }
-    createSequenceFile(dir, "d1", (byte) 0, 15);
-    dir.copy(csw, "d1", "d1", newIOContext(random()));
-    String[] listAll = newDir.listAll();
-    assertEquals(1, listAll.length);
-    assertEquals("d.cfs", listAll[0]);
-    csw.close();
-    
-    Directory csr = new Lucene40CompoundReader(newDir, "d.cfs", newIOContext(random()), false);
-    for (int j = 0; j < 2; j++) {
-      IndexInput openInput = csr.openInput("seg_" + j + "_foo.txt", newIOContext(random()));
-      assertEquals(size * 4, openInput.length());
-      for (int i = 0; i < size; i++) {
-        assertEquals(i*j, openInput.readInt());
-      }
-      
-      openInput.close();
-    }
-    IndexInput expected = dir.openInput("d1", newIOContext(random()));
-    IndexInput actual = csr.openInput("d1", newIOContext(random()));
-    assertSameStreams("d1", expected, actual);
-    assertSameSeekBehavior("d1", expected, actual);
-    expected.close();
-    actual.close();
-    csr.close();
-    newDir.close();
-    dir.close();
-  }
-  
-  public void testAppendTwice() throws IOException {
-    Directory newDir = newDirectory();
-    Directory csw = new Lucene40CompoundReader(newDir, "d.cfs", newIOContext(random()), true);
-    createSequenceFile(newDir, "d1", (byte) 0, 15);
-    IndexOutput out = csw.createOutput("d.xyz", newIOContext(random()));
-    out.writeInt(0);
-    out.close();
-    assertEquals(1, csw.listAll().length);
-    assertEquals("d.xyz", csw.listAll()[0]);
-    
-    csw.close();
-    
-    Directory cfr = new Lucene40CompoundReader(newDir, "d.cfs", newIOContext(random()), false);
-    assertEquals(1, cfr.listAll().length);
-    assertEquals("d.xyz", cfr.listAll()[0]);
-    cfr.close();
-    newDir.close();
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40DocValuesFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40DocValuesFormat.java
deleted file mode 100644
index 1f138d0..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40DocValuesFormat.java
+++ /dev/null
@@ -1,56 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.BaseDocValuesFormatTestCase;
-
-/**
- * Tests Lucene40DocValuesFormat
- */
-public class TestLucene40DocValuesFormat extends BaseDocValuesFormatTestCase {
-  private final Codec codec = new Lucene40RWCodec();
-
-  @Override
-  protected Codec getCodec() {
-    return codec;
-  }
-
-  // LUCENE-4583: This codec should throw IAE on huge binary values:
-  @Override
-  protected boolean codecAcceptsHugeBinaryValues(String field) {
-    return false;
-  }
-
-  // this codec doesnt support missing (its the same as empty string)
-  @Override
-  protected boolean codecSupportsDocsWithField() {
-    return false;
-  }
-
-  @Override
-  protected boolean codecSupportsSortedSet() {
-    return false;
-  }
-
-  @Override
-  protected boolean codecSupportsSortedNumeric() {
-    return false;
-  }
-
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40FieldInfoFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40FieldInfoFormat.java
deleted file mode 100644
index 7ff112c..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40FieldInfoFormat.java
+++ /dev/null
@@ -1,71 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosFormat.LegacyDocValuesType;
-import org.apache.lucene.index.BaseFieldInfoFormatTestCase;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfo.DocValuesType;
-
-/** Test Lucene 4.0 FieldInfos Format */
-public class TestLucene40FieldInfoFormat extends BaseFieldInfoFormatTestCase {
-  private final Codec codec = new Lucene40RWCodec();
-
-  @Override
-  protected Codec getCodec() {
-    return codec;
-  }
-
-  // we only support these three dv types
-  @Override
-  @Deprecated
-  protected DocValuesType[] getDocValuesTypes() {
-    return new DocValuesType[] {
-      DocValuesType.BINARY,
-      DocValuesType.NUMERIC,
-      DocValuesType.SORTED
-    };
-  }
-
-  // but we have more internal typing information, previously recorded in fieldinfos.
-  // this is exposed via attributes (so our writer expects them to be set by the dv impl)
-  @Override
-  protected void addAttributes(FieldInfo fi) {
-    DocValuesType dvType = fi.getDocValuesType();
-    if (dvType != null) {
-      switch (dvType) {
-        case BINARY: 
-          fi.putAttribute(Lucene40FieldInfosFormat.LEGACY_DV_TYPE_KEY, LegacyDocValuesType.BYTES_FIXED_STRAIGHT.name());
-          break;
-        case NUMERIC:
-          fi.putAttribute(Lucene40FieldInfosFormat.LEGACY_DV_TYPE_KEY, LegacyDocValuesType.FIXED_INTS_32.name());
-          break;
-        case SORTED:
-          fi.putAttribute(Lucene40FieldInfosFormat.LEGACY_DV_TYPE_KEY, LegacyDocValuesType.BYTES_FIXED_SORTED.name());
-          break;
-        default:
-          throw new AssertionError();
-      }
-    }
-    
-    if (fi.hasNorms()) {
-      fi.putAttribute(Lucene40FieldInfosFormat.LEGACY_NORM_TYPE_KEY, LegacyDocValuesType.FIXED_INTS_8.name());
-    }
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40NormsFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40NormsFormat.java
deleted file mode 100644
index 72c01da..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40NormsFormat.java
+++ /dev/null
@@ -1,138 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.InputStream;
-import java.nio.file.Path;
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.BaseNormsFormatTestCase;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.MultiDocValues;
-import org.apache.lucene.index.NumericDocValues;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.FSDirectory;
-import org.apache.lucene.util.TestUtil;
-
-/** Tests Lucene40's norms format */
-public class TestLucene40NormsFormat extends BaseNormsFormatTestCase {
-  final Codec codec = new Lucene40RWCodec();
-  
-  @Override
-  protected Codec getCodec() {
-    return codec;
-  }
-
-  /** Copy this back to /l/400/lucene/CreateUndeadNorms.java, then:
-   *   - ant clean
-   *   - pushd analysis/common; ant jar; popd
-   *   - pushd core; ant jar; popd
-   *   - javac -cp build/analysis/common/lucene-analyzers-common-4.0-SNAPSHOT.jar:build/core/lucene-core-4.0-SNAPSHOT.jar CreateUndeadNorms.java
-   *   - java -cp .:build/analysis/common/lucene-analyzers-common-4.0-SNAPSHOT.jar:build/core/lucene-core-4.0-SNAPSHOT.jar CreateUndeadNorms
-   *   - cd /tmp/undeadnorms  ; zip index.40.undeadnorms.zip *
-
-import java.io.File;
-import java.io.IOException;
-
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.FSDirectory;
-import org.apache.lucene.util.Version;
-
-public class CreateUndeadNorms {
-  public static void main(String[] args) throws Exception {
-    File file = new File("/tmp/undeadnorms");
-    if (file.exists()) {
-      throw new RuntimeException("please remove /tmp/undeadnorms first");
-    }
-    Directory dir = FSDirectory.open(new File("/tmp/undeadnorms"));
-    IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(Version.LUCENE_40, new WhitespaceAnalyzer(Version.LUCENE_40)));
-    Document doc = new Document();
-    doc.add(new StringField("id", "0", Field.Store.NO));
-    w.addDocument(doc);
-    doc = new Document();
-    doc.add(new StringField("id", "1", Field.Store.NO));
-    Field content = new TextField("content", "some content", Field.Store.NO);
-    content.setTokenStream(new TokenStream() {
-        @Override
-        public boolean incrementToken() throws IOException {
-          throw new IOException("brains brains!");
-        }
-      });
-
-    doc.add(content);
-    try {
-      w.addDocument(doc);
-      throw new RuntimeException("didn't hit exception");
-    } catch (IOException ioe) {
-      // perfect
-    }
-    w.close();
-    dir.close();
-  }
-}
-*/
-
-  /** 
-   * LUCENE-6006: Test undead norms.
-   *                                 .....            
-   *                             C C  /            
-   *                            /<   /             
-   *             ___ __________/_#__=o             
-   *            /(- /(\_\________   \              
-   *            \ ) \ )_      \o     \             
-   *            /|\ /|\       |'     |             
-   *                          |     _|             
-   *                          /o   __\             
-   *                         / '     |             
-   *                        / /      |             
-   *                       /_/\______|             
-   *                      (   _(    <              
-   *                       \    \    \             
-   *                        \    \    |            
-   *                         \____\____\           
-   *                         ____\_\__\_\          
-   *                       /`   /`     o\          
-   *                       |___ |_______|
-   *
-   */
-  public void testReadUndeadNorms() throws Exception {
-    InputStream resource = TestLucene40NormsFormat.class.getResourceAsStream("index.40.undeadnorms.zip");
-    assertNotNull(resource);
-    Path path = createTempDir("undeadnorms");
-    TestUtil.unzip(resource, path);
-    Directory dir = FSDirectory.open(path);
-    IndexReader r = DirectoryReader.open(dir);
-    NumericDocValues undeadNorms = MultiDocValues.getNormValues(r, "content");
-    assertNotNull(undeadNorms);
-    assertEquals(2, r.maxDoc());
-    assertEquals(0, undeadNorms.get(0));
-    assertEquals(0, undeadNorms.get(1));
-    dir.close();
-    r.close();
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40PostingsFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40PostingsFormat.java
deleted file mode 100644
index a742d58..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40PostingsFormat.java
+++ /dev/null
@@ -1,34 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.BasePostingsFormatTestCase;
-
-/**
- * Tests Lucene40PostingsFormat
- */
-public class TestLucene40PostingsFormat extends BasePostingsFormatTestCase {
-  private final Codec codec = new Lucene40RWCodec();
-  
-  @Override
-  protected Codec getCodec() {
-    return codec;
-  }
-  
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40PostingsReader.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40PostingsReader.java
deleted file mode 100644
index 3dc2c53..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40PostingsReader.java
+++ /dev/null
@@ -1,125 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.ArrayList;
-import java.util.Collections;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.TestUtil;
-
-public class TestLucene40PostingsReader extends LuceneTestCase {
-  static final String terms[] = new String[100];
-  static {
-    for (int i = 0; i < terms.length; i++) {
-      terms[i] = Integer.toString(i+1);
-    }
-  }
-
-  /** tests terms with different probabilities of being in the document.
-   *  depends heavily on term vectors cross-check at checkIndex
-   */
-  public void testPostings() throws Exception {
-    Directory dir = newFSDirectory(createTempDir("postings"));
-    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));
-    iwc.setCodec(new Lucene40RWCodec());
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
-    
-    Document doc = new Document();
-    
-    // id field
-    FieldType idType = new FieldType(StringField.TYPE_NOT_STORED);
-    idType.setStoreTermVectors(true);
-    Field idField = new Field("id", "", idType);
-    doc.add(idField);
-    
-    // title field: short text field
-    FieldType titleType = new FieldType(TextField.TYPE_NOT_STORED);
-    titleType.setStoreTermVectors(true);
-    titleType.setStoreTermVectorPositions(true);
-    titleType.setStoreTermVectorOffsets(true);
-    titleType.setIndexOptions(indexOptions());
-    Field titleField = new Field("title", "", titleType);
-    doc.add(titleField);
-    
-    // body field: long text field
-    FieldType bodyType = new FieldType(TextField.TYPE_NOT_STORED);
-    bodyType.setStoreTermVectors(true);
-    bodyType.setStoreTermVectorPositions(true);
-    bodyType.setStoreTermVectorOffsets(true);
-    bodyType.setIndexOptions(indexOptions());
-    Field bodyField = new Field("body", "", bodyType);
-    doc.add(bodyField);
-    
-    int numDocs = atLeast(1000);
-    for (int i = 0; i < numDocs; i++) {
-      idField.setStringValue(Integer.toString(i));
-      titleField.setStringValue(fieldValue(1));
-      bodyField.setStringValue(fieldValue(3));
-      iw.addDocument(doc);
-      if (random().nextInt(20) == 0) {
-        iw.deleteDocuments(new Term("id", Integer.toString(i)));
-      }
-    }
-    if (random().nextBoolean()) {
-      // delete 1-100% of docs
-      iw.deleteDocuments(new Term("title", terms[random().nextInt(terms.length)]));
-    }
-    iw.close();
-    dir.close(); // checkindex
-  }
-  
-  IndexOptions indexOptions() {
-    switch(random().nextInt(4)) {
-      case 0: return IndexOptions.DOCS_ONLY;
-      case 1: return IndexOptions.DOCS_AND_FREQS;
-      case 2: return IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
-      default: return IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;
-    }
-  }
-  
-  String fieldValue(int maxTF) {
-    ArrayList<String> shuffled = new ArrayList<>();
-    StringBuilder sb = new StringBuilder();
-    int i = random().nextInt(terms.length);
-    while (i < terms.length) {
-      int tf =  TestUtil.nextInt(random(), 1, maxTF);
-      for (int j = 0; j < tf; j++) {
-        shuffled.add(terms[i]);
-      }
-      i++;
-    }
-    Collections.shuffle(shuffled, random());
-    for (String term : shuffled) {
-      sb.append(term);
-      sb.append(' ');
-    }
-    return sb.toString();
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40SegmentInfoFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40SegmentInfoFormat.java
deleted file mode 100644
index 984b2b8..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40SegmentInfoFormat.java
+++ /dev/null
@@ -1,58 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.BaseSegmentInfoFormatTestCase;
-import org.apache.lucene.util.Version;
-
-/**
- * Tests Lucene40InfoFormat
- */
-public class TestLucene40SegmentInfoFormat extends BaseSegmentInfoFormatTestCase {
-
-  @Override
-  protected Version[] getVersions() {
-    // NOTE: some of these bugfix releases we never actually "wrote",
-    // but staying on the safe side...
-    return new Version[] { 
-        Version.LUCENE_4_0_0_ALPHA, 
-        Version.LUCENE_4_0_0_BETA,
-        Version.LUCENE_4_0_0,
-        Version.LUCENE_4_1_0,
-        Version.LUCENE_4_2_0,
-        Version.LUCENE_4_2_1,
-        Version.LUCENE_4_3_0,
-        Version.LUCENE_4_3_1,
-        Version.LUCENE_4_4_0,
-        Version.LUCENE_4_5_0,
-        Version.LUCENE_4_5_1,
-    };
-  }
-
-  @Override
-  @Deprecated
-  protected void assertIDEquals(byte[] expected, byte[] actual) {
-    assertNull(actual); // we don't support IDs
-  }
-
-  @Override
-  protected Codec getCodec() {
-    return new Lucene40RWCodec();
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40StoredFieldsFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40StoredFieldsFormat.java
deleted file mode 100644
index 72427c0..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40StoredFieldsFormat.java
+++ /dev/null
@@ -1,29 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.BaseStoredFieldsFormatTestCase;
-
-public class TestLucene40StoredFieldsFormat extends BaseStoredFieldsFormatTestCase {
-  
-  @Override
-  protected Codec getCodec() {
-    return new Lucene40RWCodec();
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40TermVectorsFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40TermVectorsFormat.java
deleted file mode 100644
index 1360b2a..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40TermVectorsFormat.java
+++ /dev/null
@@ -1,30 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.BaseTermVectorsFormatTestCase;
-
-public class TestLucene40TermVectorsFormat extends BaseTermVectorsFormatTestCase {
-  
-  @Override
-  protected Codec getCodec() {
-    return new Lucene40RWCodec();
-  }
-  
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum.java
deleted file mode 100644
index 33f36fc..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum.java
+++ /dev/null
@@ -1,193 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.IOException;
-import java.util.IdentityHashMap;
-import java.util.List;
-import java.util.Random;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.LeafReader;
-import org.apache.lucene.index.LeafReaderContext;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.Bits.MatchNoBits;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.LineFileDocs;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.TestUtil;
-
-// TODO: really this should be in BaseTestPF or somewhere else? useful test!
-public class TestReuseDocsEnum extends LuceneTestCase {
-  
-  public void testReuseDocsEnumNoReuse() throws IOException {
-    Directory dir = newDirectory();
-    Codec cp = TestUtil.alwaysPostingsFormat(new Lucene40RWPostingsFormat());
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,
-        newIndexWriterConfig(new MockAnalyzer(random())).setCodec(cp));
-    int numdocs = atLeast(20);
-    createRandomIndex(numdocs, writer, random());
-    writer.commit();
-
-    DirectoryReader open = DirectoryReader.open(dir);
-    for (LeafReaderContext ctx : open.leaves()) {
-      LeafReader indexReader = ctx.reader();
-      Terms terms = indexReader.terms("body");
-      TermsEnum iterator = terms.iterator(null);
-      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<>();
-      MatchNoBits bits = new Bits.MatchNoBits(indexReader.maxDoc());
-      while ((iterator.next()) != null) {
-        DocsEnum docs = iterator.docs(random().nextBoolean() ? bits : new Bits.MatchNoBits(indexReader.maxDoc()), null, random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);
-        enums.put(docs, true);
-      }
-      
-      assertEquals(terms.size(), enums.size());
-    }
-    writer.commit();
-    IOUtils.close(writer, open, dir);
-  }
-  
-  // tests for reuse only if bits are the same either null or the same instance
-  public void testReuseDocsEnumSameBitsOrNull() throws IOException {
-    Directory dir = newDirectory();
-    Codec cp = TestUtil.alwaysPostingsFormat(new Lucene40RWPostingsFormat());
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,
-        newIndexWriterConfig(new MockAnalyzer(random())).setCodec(cp));
-    int numdocs = atLeast(20);
-    createRandomIndex(numdocs, writer, random());
-    writer.commit();
-
-    DirectoryReader open = DirectoryReader.open(dir);
-    for (LeafReaderContext ctx : open.leaves()) {
-      Terms terms = ctx.reader().terms("body");
-      TermsEnum iterator = terms.iterator(null);
-      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<>();
-      MatchNoBits bits = new Bits.MatchNoBits(open.maxDoc());
-      DocsEnum docs = null;
-      while ((iterator.next()) != null) {
-        docs = iterator.docs(bits, docs, random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);
-        enums.put(docs, true);
-      }
-      
-      assertEquals(1, enums.size());
-      enums.clear();
-      iterator = terms.iterator(null);
-      docs = null;
-      while ((iterator.next()) != null) {
-        docs = iterator.docs(new Bits.MatchNoBits(open.maxDoc()), docs, random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);
-        enums.put(docs, true);
-      }
-      assertEquals(terms.size(), enums.size());
-      
-      enums.clear();
-      iterator = terms.iterator(null);
-      docs = null;
-      while ((iterator.next()) != null) {
-        docs = iterator.docs(null, docs, random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);
-        enums.put(docs, true);
-      }
-      assertEquals(1, enums.size());  
-    }
-    writer.close();
-    IOUtils.close(open, dir);
-  }
-  
-  // make sure we never reuse from another reader even if it is the same field & codec etc
-  public void testReuseDocsEnumDifferentReader() throws IOException {
-    Directory dir = newDirectory();
-    Codec cp = TestUtil.alwaysPostingsFormat(new Lucene40RWPostingsFormat());
-    MockAnalyzer analyzer = new MockAnalyzer(random());
-    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));
-
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,
-        newIndexWriterConfig(analyzer).setCodec(cp));
-    int numdocs = atLeast(20);
-    createRandomIndex(numdocs, writer, random());
-    writer.commit();
-
-    DirectoryReader firstReader = DirectoryReader.open(dir);
-    DirectoryReader secondReader = DirectoryReader.open(dir);
-    List<LeafReaderContext> leaves = firstReader.leaves();
-    List<LeafReaderContext> leaves2 = secondReader.leaves();
-    
-    for (LeafReaderContext ctx : leaves) {
-      Terms terms = ctx.reader().terms("body");
-      TermsEnum iterator = terms.iterator(null);
-      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<>();
-      MatchNoBits bits = new Bits.MatchNoBits(firstReader.maxDoc());
-      iterator = terms.iterator(null);
-      DocsEnum docs = null;
-      BytesRef term = null;
-      while ((term = iterator.next()) != null) {
-        docs = iterator.docs(null, randomDocsEnum("body", term, leaves2, bits), random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);
-        enums.put(docs, true);
-      }
-      assertEquals(terms.size(), enums.size());
-      
-      iterator = terms.iterator(null);
-      enums.clear();
-      docs = null;
-      while ((term = iterator.next()) != null) {
-        docs = iterator.docs(bits, randomDocsEnum("body", term, leaves2, bits), random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);
-        enums.put(docs, true);
-      }
-      assertEquals(terms.size(), enums.size());
-    }
-    writer.close();
-    IOUtils.close(firstReader, secondReader, dir);
-  }
-  
-  public DocsEnum randomDocsEnum(String field, BytesRef term, List<LeafReaderContext> readers, Bits bits) throws IOException {
-    if (random().nextInt(10) == 0) {
-      return null;
-    }
-    LeafReader indexReader = readers.get(random().nextInt(readers.size())).reader();
-    Terms terms = indexReader.terms(field);
-    if (terms == null) {
-      return null;
-    }
-    TermsEnum iterator = terms.iterator(null);
-    if (iterator.seekExact(term)) {
-      return iterator.docs(bits, null, random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);
-    }
-    return null;
-  }
-
-  /**
-   * populates a writer with random stuff. this must be fully reproducable with
-   * the seed!
-   */
-  public static void createRandomIndex(int numdocs, RandomIndexWriter writer,
-      Random random) throws IOException {
-    LineFileDocs lineFileDocs = new LineFileDocs(random);
-
-    for (int i = 0; i < numdocs; i++) {
-      writer.addDocument(lineFileDocs.nextDoc());
-    }
-    
-    lineFileDocs.close();
-  }
-
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/index.40.undeadnorms.zip b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/index.40.undeadnorms.zip
deleted file mode 100644
index d2d6a97..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/index.40.undeadnorms.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41PostingsWriter.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41PostingsWriter.java
deleted file mode 100644
index 41d735a..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41PostingsWriter.java
+++ /dev/null
@@ -1,536 +0,0 @@
-package org.apache.lucene.codecs.lucene41;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.BlockTermState;
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.PushPostingsWriterBase;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.packed.PackedInts;
-
-import static org.apache.lucene.codecs.lucene41.ForUtil.MAX_DATA_SIZE;
-import static org.apache.lucene.codecs.lucene41.ForUtil.MAX_ENCODED_SIZE;
-import static org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat.BLOCK_SIZE;
-
-/**
- * Writes 4.1 postings for testing
- * @deprecated for test purposes only
- */
-@Deprecated
-public final class Lucene41PostingsWriter extends PushPostingsWriterBase {
-
-  IndexOutput docOut;
-  IndexOutput posOut;
-  IndexOutput payOut;
-
-  final static IntBlockTermState emptyState = new IntBlockTermState();
-  IntBlockTermState lastState;
-
-  // Holds starting file pointers for current term:
-  private long docStartFP;
-  private long posStartFP;
-  private long payStartFP;
-
-  final int[] docDeltaBuffer;
-  final int[] freqBuffer;
-  private int docBufferUpto;
-
-  final int[] posDeltaBuffer;
-  final int[] payloadLengthBuffer;
-  final int[] offsetStartDeltaBuffer;
-  final int[] offsetLengthBuffer;
-  private int posBufferUpto;
-
-  private byte[] payloadBytes;
-  private int payloadByteUpto;
-
-  private int lastBlockDocID;
-  private long lastBlockPosFP;
-  private long lastBlockPayFP;
-  private int lastBlockPosBufferUpto;
-  private int lastBlockPayloadByteUpto;
-
-  private int lastDocID;
-  private int lastPosition;
-  private int lastStartOffset;
-  private int docCount;
-
-  final byte[] encoded;
-
-  private final ForUtil forUtil;
-  private final Lucene41SkipWriter skipWriter;
-  
-  /** Creates a postings writer with the specified PackedInts overhead ratio */
-  // TODO: does this ctor even make sense?
-  public Lucene41PostingsWriter(SegmentWriteState state, float acceptableOverheadRatio) throws IOException {
-    super();
-
-    docOut = state.directory.createOutput(IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, Lucene41PostingsFormat.DOC_EXTENSION),
-                                                  state.context);
-    IndexOutput posOut = null;
-    IndexOutput payOut = null;
-    boolean success = false;
-    try {
-      CodecUtil.writeHeader(docOut, Lucene41PostingsFormat.DOC_CODEC, Lucene41PostingsFormat.VERSION_CURRENT);
-      forUtil = new ForUtil(acceptableOverheadRatio, docOut);
-      if (state.fieldInfos.hasProx()) {
-        posDeltaBuffer = new int[MAX_DATA_SIZE];
-        posOut = state.directory.createOutput(IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, Lucene41PostingsFormat.POS_EXTENSION),
-                                                      state.context);
-        CodecUtil.writeHeader(posOut, Lucene41PostingsFormat.POS_CODEC, Lucene41PostingsFormat.VERSION_CURRENT);
-
-        if (state.fieldInfos.hasPayloads()) {
-          payloadBytes = new byte[128];
-          payloadLengthBuffer = new int[MAX_DATA_SIZE];
-        } else {
-          payloadBytes = null;
-          payloadLengthBuffer = null;
-        }
-
-        if (state.fieldInfos.hasOffsets()) {
-          offsetStartDeltaBuffer = new int[MAX_DATA_SIZE];
-          offsetLengthBuffer = new int[MAX_DATA_SIZE];
-        } else {
-          offsetStartDeltaBuffer = null;
-          offsetLengthBuffer = null;
-        }
-
-        if (state.fieldInfos.hasPayloads() || state.fieldInfos.hasOffsets()) {
-          payOut = state.directory.createOutput(IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, Lucene41PostingsFormat.PAY_EXTENSION),
-                                                        state.context);
-          CodecUtil.writeHeader(payOut, Lucene41PostingsFormat.PAY_CODEC, Lucene41PostingsFormat.VERSION_CURRENT);
-        }
-      } else {
-        posDeltaBuffer = null;
-        payloadLengthBuffer = null;
-        offsetStartDeltaBuffer = null;
-        offsetLengthBuffer = null;
-        payloadBytes = null;
-      }
-      this.payOut = payOut;
-      this.posOut = posOut;
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(docOut, posOut, payOut);
-      }
-    }
-
-    docDeltaBuffer = new int[MAX_DATA_SIZE];
-    freqBuffer = new int[MAX_DATA_SIZE];
-
-    // TODO: should we try skipping every 2/4 blocks...?
-    skipWriter = new Lucene41SkipWriter(Lucene41PostingsFormat.maxSkipLevels,
-                                     BLOCK_SIZE, 
-                                     state.segmentInfo.getDocCount(),
-                                     docOut,
-                                     posOut,
-                                     payOut);
-
-    encoded = new byte[MAX_ENCODED_SIZE];
-  }
-
-  /** Creates a postings writer with <code>PackedInts.COMPACT</code> */
-  public Lucene41PostingsWriter(SegmentWriteState state) throws IOException {
-    this(state, PackedInts.COMPACT);
-  }
-
-  @Override
-  public IntBlockTermState newTermState() {
-    return new IntBlockTermState();
-  }
-
-  @Override
-  public void init(IndexOutput termsOut, SegmentWriteState state) throws IOException {
-    CodecUtil.writeHeader(termsOut, Lucene41PostingsFormat.TERMS_CODEC, Lucene41PostingsFormat.VERSION_CURRENT);
-    termsOut.writeVInt(BLOCK_SIZE);
-  }
-
-  @Override
-  public int setField(FieldInfo fieldInfo) {
-    super.setField(fieldInfo);
-    skipWriter.setField(writePositions, writeOffsets, writePayloads);
-    lastState = emptyState;
-    if (writePositions) {
-      if (writePayloads || writeOffsets) {
-        return 3;  // doc + pos + pay FP
-      } else {
-        return 2;  // doc + pos FP
-      }
-    } else {
-      return 1;    // doc FP
-    }
-  }
-
-  @Override
-  public void startTerm() {
-    docStartFP = docOut.getFilePointer();
-    if (writePositions) {
-      posStartFP = posOut.getFilePointer();
-      if (writePayloads || writeOffsets) {
-        payStartFP = payOut.getFilePointer();
-      }
-    }
-    lastDocID = 0;
-    lastBlockDocID = -1;
-    // if (DEBUG) {
-    //   System.out.println("FPW.startTerm startFP=" + docStartFP);
-    // }
-    skipWriter.resetSkip();
-  }
-
-  @Override
-  public void startDoc(int docID, int termDocFreq) throws IOException {
-    // if (DEBUG) {
-    //   System.out.println("FPW.startDoc docID["+docBufferUpto+"]=" + docID);
-    // }
-    // Have collected a block of docs, and get a new doc. 
-    // Should write skip data as well as postings list for
-    // current block.
-    if (lastBlockDocID != -1 && docBufferUpto == 0) {
-      // if (DEBUG) {
-      //   System.out.println("  bufferSkip at writeBlock: lastDocID=" + lastBlockDocID + " docCount=" + (docCount-1));
-      // }
-      skipWriter.bufferSkip(lastBlockDocID, docCount, lastBlockPosFP, lastBlockPayFP, lastBlockPosBufferUpto, lastBlockPayloadByteUpto);
-    }
-
-    final int docDelta = docID - lastDocID;
-
-    if (docID < 0 || (docCount > 0 && docDelta <= 0)) {
-      throw new CorruptIndexException("docs out of order (" + docID + " <= " + lastDocID + " )", docOut.toString());
-    }
-
-    docDeltaBuffer[docBufferUpto] = docDelta;
-    // if (DEBUG) {
-    //   System.out.println("  docDeltaBuffer[" + docBufferUpto + "]=" + docDelta);
-    // }
-    if (writeFreqs) {
-      freqBuffer[docBufferUpto] = termDocFreq;
-    }
-    docBufferUpto++;
-    docCount++;
-
-    if (docBufferUpto == BLOCK_SIZE) {
-      // if (DEBUG) {
-      //   System.out.println("  write docDelta block @ fp=" + docOut.getFilePointer());
-      // }
-      forUtil.writeBlock(docDeltaBuffer, encoded, docOut);
-      if (writeFreqs) {
-        // if (DEBUG) {
-        //   System.out.println("  write freq block @ fp=" + docOut.getFilePointer());
-        // }
-        forUtil.writeBlock(freqBuffer, encoded, docOut);
-      }
-      // NOTE: don't set docBufferUpto back to 0 here;
-      // finishDoc will do so (because it needs to see that
-      // the block was filled so it can save skip data)
-    }
-
-
-    lastDocID = docID;
-    lastPosition = 0;
-    lastStartOffset = 0;
-  }
-
-  @Override
-  public void addPosition(int position, BytesRef payload, int startOffset, int endOffset) throws IOException {
-    // if (DEBUG) {
-    //   System.out.println("FPW.addPosition pos=" + position + " posBufferUpto=" + posBufferUpto + (writePayloads ? " payloadByteUpto=" + payloadByteUpto: ""));
-    // }
-    posDeltaBuffer[posBufferUpto] = position - lastPosition;
-    if (writePayloads) {
-      if (payload == null || payload.length == 0) {
-        // no payload
-        payloadLengthBuffer[posBufferUpto] = 0;
-      } else {
-        payloadLengthBuffer[posBufferUpto] = payload.length;
-        if (payloadByteUpto + payload.length > payloadBytes.length) {
-          payloadBytes = ArrayUtil.grow(payloadBytes, payloadByteUpto + payload.length);
-        }
-        System.arraycopy(payload.bytes, payload.offset, payloadBytes, payloadByteUpto, payload.length);
-        payloadByteUpto += payload.length;
-      }
-    }
-
-    if (writeOffsets) {
-      assert startOffset >= lastStartOffset;
-      assert endOffset >= startOffset;
-      offsetStartDeltaBuffer[posBufferUpto] = startOffset - lastStartOffset;
-      offsetLengthBuffer[posBufferUpto] = endOffset - startOffset;
-      lastStartOffset = startOffset;
-    }
-    
-    posBufferUpto++;
-    lastPosition = position;
-    if (posBufferUpto == BLOCK_SIZE) {
-      // if (DEBUG) {
-      //   System.out.println("  write pos bulk block @ fp=" + posOut.getFilePointer());
-      // }
-      forUtil.writeBlock(posDeltaBuffer, encoded, posOut);
-
-      if (writePayloads) {
-        forUtil.writeBlock(payloadLengthBuffer, encoded, payOut);
-        payOut.writeVInt(payloadByteUpto);
-        payOut.writeBytes(payloadBytes, 0, payloadByteUpto);
-        payloadByteUpto = 0;
-      }
-      if (writeOffsets) {
-        forUtil.writeBlock(offsetStartDeltaBuffer, encoded, payOut);
-        forUtil.writeBlock(offsetLengthBuffer, encoded, payOut);
-      }
-      posBufferUpto = 0;
-    }
-  }
-
-  @Override
-  public void finishDoc() throws IOException {
-    // Since we don't know df for current term, we had to buffer
-    // those skip data for each block, and when a new doc comes, 
-    // write them to skip file.
-    if (docBufferUpto == BLOCK_SIZE) {
-      lastBlockDocID = lastDocID;
-      if (posOut != null) {
-        if (payOut != null) {
-          lastBlockPayFP = payOut.getFilePointer();
-        }
-        lastBlockPosFP = posOut.getFilePointer();
-        lastBlockPosBufferUpto = posBufferUpto;
-        lastBlockPayloadByteUpto = payloadByteUpto;
-      }
-      // if (DEBUG) {
-      //   System.out.println("  docBufferUpto="+docBufferUpto+" now get lastBlockDocID="+lastBlockDocID+" lastBlockPosFP=" + lastBlockPosFP + " lastBlockPosBufferUpto=" +  lastBlockPosBufferUpto + " lastBlockPayloadByteUpto=" + lastBlockPayloadByteUpto);
-      // }
-      docBufferUpto = 0;
-    }
-  }
-
-  /** Called when we are done adding docs to this term */
-  @Override
-  public void finishTerm(BlockTermState _state) throws IOException {
-    IntBlockTermState state = (IntBlockTermState) _state;
-    assert state.docFreq > 0;
-
-    // TODO: wasteful we are counting this (counting # docs
-    // for this term) in two places?
-    assert state.docFreq == docCount: state.docFreq + " vs " + docCount;
-
-    // if (DEBUG) {
-    //   System.out.println("FPW.finishTerm docFreq=" + state.docFreq);
-    // }
-
-    // if (DEBUG) {
-    //   if (docBufferUpto > 0) {
-    //     System.out.println("  write doc/freq vInt block (count=" + docBufferUpto + ") at fp=" + docOut.getFilePointer() + " docStartFP=" + docStartFP);
-    //   }
-    // }
-    
-    // docFreq == 1, don't write the single docid/freq to a separate file along with a pointer to it.
-    final int singletonDocID;
-    if (state.docFreq == 1) {
-      // pulse the singleton docid into the term dictionary, freq is implicitly totalTermFreq
-      singletonDocID = docDeltaBuffer[0];
-    } else {
-      singletonDocID = -1;
-      // vInt encode the remaining doc deltas and freqs:
-      for(int i=0;i<docBufferUpto;i++) {
-        final int docDelta = docDeltaBuffer[i];
-        final int freq = freqBuffer[i];
-        if (!writeFreqs) {
-          docOut.writeVInt(docDelta);
-        } else if (freqBuffer[i] == 1) {
-          docOut.writeVInt((docDelta<<1)|1);
-        } else {
-          docOut.writeVInt(docDelta<<1);
-          docOut.writeVInt(freq);
-        }
-      }
-    }
-
-    final long lastPosBlockOffset;
-
-    if (writePositions) {
-      // if (DEBUG) {
-      //   if (posBufferUpto > 0) {
-      //     System.out.println("  write pos vInt block (count=" + posBufferUpto + ") at fp=" + posOut.getFilePointer() + " posStartFP=" + posStartFP + " hasPayloads=" + writePayloads + " hasOffsets=" + writeOffsets);
-      //   }
-      // }
-
-      // totalTermFreq is just total number of positions(or payloads, or offsets)
-      // associated with current term.
-      assert state.totalTermFreq != -1;
-      if (state.totalTermFreq > BLOCK_SIZE) {
-        // record file offset for last pos in last block
-        lastPosBlockOffset = posOut.getFilePointer() - posStartFP;
-      } else {
-        lastPosBlockOffset = -1;
-      }
-      if (posBufferUpto > 0) {       
-        // TODO: should we send offsets/payloads to
-        // .pay...?  seems wasteful (have to store extra
-        // vLong for low (< BLOCK_SIZE) DF terms = vast vast
-        // majority)
-
-        // vInt encode the remaining positions/payloads/offsets:
-        int lastPayloadLength = -1;  // force first payload length to be written
-        int lastOffsetLength = -1;   // force first offset length to be written
-        int payloadBytesReadUpto = 0;
-        for(int i=0;i<posBufferUpto;i++) {
-          final int posDelta = posDeltaBuffer[i];
-          if (writePayloads) {
-            final int payloadLength = payloadLengthBuffer[i];
-            if (payloadLength != lastPayloadLength) {
-              lastPayloadLength = payloadLength;
-              posOut.writeVInt((posDelta<<1)|1);
-              posOut.writeVInt(payloadLength);
-            } else {
-              posOut.writeVInt(posDelta<<1);
-            }
-
-            // if (DEBUG) {
-            //   System.out.println("        i=" + i + " payloadLen=" + payloadLength);
-            // }
-
-            if (payloadLength != 0) {
-              // if (DEBUG) {
-              //   System.out.println("          write payload @ pos.fp=" + posOut.getFilePointer());
-              // }
-              posOut.writeBytes(payloadBytes, payloadBytesReadUpto, payloadLength);
-              payloadBytesReadUpto += payloadLength;
-            }
-          } else {
-            posOut.writeVInt(posDelta);
-          }
-
-          if (writeOffsets) {
-            // if (DEBUG) {
-            //   System.out.println("          write offset @ pos.fp=" + posOut.getFilePointer());
-            // }
-            int delta = offsetStartDeltaBuffer[i];
-            int length = offsetLengthBuffer[i];
-            if (length == lastOffsetLength) {
-              posOut.writeVInt(delta << 1);
-            } else {
-              posOut.writeVInt(delta << 1 | 1);
-              posOut.writeVInt(length);
-              lastOffsetLength = length;
-            }
-          }
-        }
-
-        if (writePayloads) {
-          assert payloadBytesReadUpto == payloadByteUpto;
-          payloadByteUpto = 0;
-        }
-      }
-      // if (DEBUG) {
-      //   System.out.println("  totalTermFreq=" + state.totalTermFreq + " lastPosBlockOffset=" + lastPosBlockOffset);
-      // }
-    } else {
-      lastPosBlockOffset = -1;
-    }
-
-    long skipOffset;
-    if (docCount > BLOCK_SIZE) {
-      skipOffset = skipWriter.writeSkip(docOut) - docStartFP;
-      
-      // if (DEBUG) {
-      //   System.out.println("skip packet " + (docOut.getFilePointer() - (docStartFP + skipOffset)) + " bytes");
-      // }
-    } else {
-      skipOffset = -1;
-      // if (DEBUG) {
-      //   System.out.println("  no skip: docCount=" + docCount);
-      // }
-    }
-    // if (DEBUG) {
-    //   System.out.println("  payStartFP=" + payStartFP);
-    // }
-    state.docStartFP = docStartFP;
-    state.posStartFP = posStartFP;
-    state.payStartFP = payStartFP;
-    state.singletonDocID = singletonDocID;
-    state.skipOffset = skipOffset;
-    state.lastPosBlockOffset = lastPosBlockOffset;
-    docBufferUpto = 0;
-    posBufferUpto = 0;
-    lastDocID = 0;
-    docCount = 0;
-  }
-  
-  @Override
-  public void encodeTerm(long[] longs, DataOutput out, FieldInfo fieldInfo, BlockTermState _state, boolean absolute) throws IOException {
-    IntBlockTermState state = (IntBlockTermState)_state;
-    if (absolute) {
-      lastState = emptyState;
-    }
-    longs[0] = state.docStartFP - lastState.docStartFP;
-    if (writePositions) {
-      longs[1] = state.posStartFP - lastState.posStartFP;
-      if (writePayloads || writeOffsets) {
-        longs[2] = state.payStartFP - lastState.payStartFP;
-      }
-    }
-    if (state.singletonDocID != -1) {
-      out.writeVInt(state.singletonDocID);
-    }
-    if (writePositions) {
-      if (state.lastPosBlockOffset != -1) {
-        out.writeVLong(state.lastPosBlockOffset);
-      }
-    }
-    if (state.skipOffset != -1) {
-      out.writeVLong(state.skipOffset);
-    }
-    lastState = state;
-  }
-
-  @Override
-  public void close() throws IOException {
-    // TODO: add a finish() at least to PushBase? DV too...?
-    boolean success = false;
-    try {
-      if (docOut != null) {
-        CodecUtil.writeFooter(docOut);
-      }
-      if (posOut != null) {
-        CodecUtil.writeFooter(posOut);
-      }
-      if (payOut != null) {
-        CodecUtil.writeFooter(payOut);
-      }
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(docOut, posOut, payOut);
-      } else {
-        IOUtils.closeWhileHandlingException(docOut, posOut, payOut);
-      }
-      docOut = posOut = payOut = null;
-    }
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41RWCodec.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41RWCodec.java
deleted file mode 100644
index 16779c8..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41RWCodec.java
+++ /dev/null
@@ -1,82 +0,0 @@
-package org.apache.lucene.codecs.lucene41;
-
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.SegmentInfoFormat;
-import org.apache.lucene.codecs.StoredFieldsFormat;
-import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40RWDocValuesFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40RWFieldInfosFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40RWNormsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40RWSegmentInfoFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40RWTermVectorsFormat;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Read-write version of 4.1 codec for testing
- * @deprecated for test purposes only
- */
-@Deprecated
-public final class Lucene41RWCodec extends Lucene41Codec {
-  private final StoredFieldsFormat fieldsFormat = new Lucene41RWStoredFieldsFormat();
-  private final FieldInfosFormat fieldInfos = new Lucene40RWFieldInfosFormat();
-  private final DocValuesFormat docValues = new Lucene40RWDocValuesFormat();
-  private final NormsFormat norms = new Lucene40RWNormsFormat();
-  private final TermVectorsFormat vectors = new Lucene40RWTermVectorsFormat();
-  private final PostingsFormat postings = new Lucene41RWPostingsFormat();
-  
-  @Override
-  public PostingsFormat getPostingsFormatForField(String field) {
-    return postings;
-  }
-  
-  @Override
-  public FieldInfosFormat fieldInfosFormat() {
-    return fieldInfos;
-  }
-
-  @Override
-  public StoredFieldsFormat storedFieldsFormat() {
-    return fieldsFormat;
-  }
-
-  @Override
-  public DocValuesFormat docValuesFormat() {
-    return docValues;
-  }
-
-  @Override
-  public NormsFormat normsFormat() {
-    return norms;
-  }
-  
-  @Override
-  public TermVectorsFormat termVectorsFormat() {
-    return vectors;
-  }
-  
-  private static final SegmentInfoFormat segmentInfos = new Lucene40RWSegmentInfoFormat();
-
-  @Override
-  public SegmentInfoFormat segmentInfoFormat() {
-    return segmentInfos;
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41RWPostingsFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41RWPostingsFormat.java
deleted file mode 100644
index 0e32012..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41RWPostingsFormat.java
+++ /dev/null
@@ -1,56 +0,0 @@
-package org.apache.lucene.codecs.lucene41;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.FieldsConsumer;
-import org.apache.lucene.codecs.PostingsWriterBase;
-import org.apache.lucene.codecs.blocktree.Lucene40BlockTreeTermsWriter;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Read-write version of 4.1 postings format for testing
- * @deprecated for test purposes only
- */
-@Deprecated
-public class Lucene41RWPostingsFormat extends Lucene41PostingsFormat {
-  
-  static final int MIN_BLOCK_SIZE = 25;
-  static final int MAX_BLOCK_SIZE = 48;
-      
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    PostingsWriterBase postingsWriter = new Lucene41PostingsWriter(state);
-
-    boolean success = false;
-    try {
-      FieldsConsumer ret = new Lucene40BlockTreeTermsWriter(state, 
-                                                    postingsWriter,
-                                                    MIN_BLOCK_SIZE, 
-                                                    MAX_BLOCK_SIZE);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(postingsWriter);
-      }
-    }
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41RWStoredFieldsFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41RWStoredFieldsFormat.java
deleted file mode 100644
index cf2690f..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41RWStoredFieldsFormat.java
+++ /dev/null
@@ -1,37 +0,0 @@
-package org.apache.lucene.codecs.lucene41;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.StoredFieldsWriter;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-/**
- * Read-write version of 4.1 stored fields format for testing
- * @deprecated for test purposes only
- */
-@Deprecated
-public final class Lucene41RWStoredFieldsFormat extends Lucene41StoredFieldsFormat {
-  @Override
-  public StoredFieldsWriter fieldsWriter(Directory directory, SegmentInfo si, IOContext context) throws IOException {
-    return new Lucene41StoredFieldsWriter(directory, si, SEGMENT_SUFFIX, context, FORMAT_NAME, COMPRESSION_MODE, CHUNK_SIZE);
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41SkipWriter.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41SkipWriter.java
deleted file mode 100644
index fd1b61c..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41SkipWriter.java
+++ /dev/null
@@ -1,160 +0,0 @@
-package org.apache.lucene.codecs.lucene41;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.codecs.MultiLevelSkipListWriter;
-
-/**
- * Writes 4.1 skiplists for testing
- * @deprecated for test purposes only
- */
-@Deprecated
-final class Lucene41SkipWriter extends MultiLevelSkipListWriter {
-  // private boolean DEBUG = Lucene41PostingsReader.DEBUG;
-  
-  private int[] lastSkipDoc;
-  private long[] lastSkipDocPointer;
-  private long[] lastSkipPosPointer;
-  private long[] lastSkipPayPointer;
-  private int[] lastPayloadByteUpto;
-
-  private final IndexOutput docOut;
-  private final IndexOutput posOut;
-  private final IndexOutput payOut;
-
-  private int curDoc;
-  private long curDocPointer;
-  private long curPosPointer;
-  private long curPayPointer;
-  private int curPosBufferUpto;
-  private int curPayloadByteUpto;
-  private boolean fieldHasPositions;
-  private boolean fieldHasOffsets;
-  private boolean fieldHasPayloads;
-
-  public Lucene41SkipWriter(int maxSkipLevels, int blockSize, int docCount, IndexOutput docOut, IndexOutput posOut, IndexOutput payOut) {
-    super(blockSize, 8, maxSkipLevels, docCount);
-    this.docOut = docOut;
-    this.posOut = posOut;
-    this.payOut = payOut;
-    
-    lastSkipDoc = new int[maxSkipLevels];
-    lastSkipDocPointer = new long[maxSkipLevels];
-    if (posOut != null) {
-      lastSkipPosPointer = new long[maxSkipLevels];
-      if (payOut != null) {
-        lastSkipPayPointer = new long[maxSkipLevels];
-      }
-      lastPayloadByteUpto = new int[maxSkipLevels];
-    }
-  }
-
-  public void setField(boolean fieldHasPositions, boolean fieldHasOffsets, boolean fieldHasPayloads) {
-    this.fieldHasPositions = fieldHasPositions;
-    this.fieldHasOffsets = fieldHasOffsets;
-    this.fieldHasPayloads = fieldHasPayloads;
-  }
-  
-  // tricky: we only skip data for blocks (terms with more than 128 docs), but re-init'ing the skipper 
-  // is pretty slow for rare terms in large segments as we have to fill O(log #docs in segment) of junk.
-  // this is the vast majority of terms (worst case: ID field or similar).  so in resetSkip() we save 
-  // away the previous pointers, and lazy-init only if we need to buffer skip data for the term.
-  private boolean initialized;
-  long lastDocFP;
-  long lastPosFP;
-  long lastPayFP;
-
-  @Override
-  public void resetSkip() {
-    lastDocFP = docOut.getFilePointer();
-    if (fieldHasPositions) {
-      lastPosFP = posOut.getFilePointer();
-      if (fieldHasOffsets || fieldHasPayloads) {
-        lastPayFP = payOut.getFilePointer();
-      }
-    }
-    initialized = false;
-  }
-  
-  public void initSkip() {
-    if (!initialized) {
-      super.resetSkip();
-      Arrays.fill(lastSkipDoc, 0);
-      Arrays.fill(lastSkipDocPointer, lastDocFP);
-      if (fieldHasPositions) {
-        Arrays.fill(lastSkipPosPointer, lastPosFP);
-        if (fieldHasPayloads) {
-          Arrays.fill(lastPayloadByteUpto, 0);
-        }
-        if (fieldHasOffsets || fieldHasPayloads) {
-          Arrays.fill(lastSkipPayPointer, lastPayFP);
-        }
-      }
-      initialized = true;
-    }
-  }
-
-  /**
-   * Sets the values for the current skip data. 
-   */
-  public void bufferSkip(int doc, int numDocs, long posFP, long payFP, int posBufferUpto, int payloadByteUpto) throws IOException {
-    initSkip();
-    this.curDoc = doc;
-    this.curDocPointer = docOut.getFilePointer();
-    this.curPosPointer = posFP;
-    this.curPayPointer = payFP;
-    this.curPosBufferUpto = posBufferUpto;
-    this.curPayloadByteUpto = payloadByteUpto;
-    bufferSkip(numDocs);
-  }
-  
-  @Override
-  protected void writeSkipData(int level, IndexOutput skipBuffer) throws IOException {
-    int delta = curDoc - lastSkipDoc[level];
-    // if (DEBUG) {
-    //   System.out.println("writeSkipData level=" + level + " lastDoc=" + curDoc + " delta=" + delta + " curDocPointer=" + curDocPointer);
-    // }
-    skipBuffer.writeVInt(delta);
-    lastSkipDoc[level] = curDoc;
-
-    skipBuffer.writeVInt((int) (curDocPointer - lastSkipDocPointer[level]));
-    lastSkipDocPointer[level] = curDocPointer;
-
-    if (fieldHasPositions) {
-      // if (DEBUG) {
-      //   System.out.println("  curPosPointer=" + curPosPointer + " curPosBufferUpto=" + curPosBufferUpto);
-      // }
-      skipBuffer.writeVInt((int) (curPosPointer - lastSkipPosPointer[level]));
-      lastSkipPosPointer[level] = curPosPointer;
-      skipBuffer.writeVInt(curPosBufferUpto);
-
-      if (fieldHasPayloads) {
-        skipBuffer.writeVInt(curPayloadByteUpto);
-      }
-
-      if (fieldHasOffsets || fieldHasPayloads) {
-        skipBuffer.writeVInt((int) (curPayPointer - lastSkipPayPointer[level]));
-        lastSkipPayPointer[level] = curPayPointer;
-      }
-    }
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsIndexWriter.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsIndexWriter.java
deleted file mode 100644
index 8b2ab4e..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsIndexWriter.java
+++ /dev/null
@@ -1,171 +0,0 @@
-package org.apache.lucene.codecs.lucene41;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import static org.apache.lucene.util.BitUtil.zigZagEncode;
-
-import java.io.Closeable;
-import java.io.IOException;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * Writer for 4.1 stored fields/term vectors index for testing
- * @deprecated for test purposes only
- */
-@Deprecated
-public final class Lucene41StoredFieldsIndexWriter implements Closeable {
-  
-  static final int BLOCK_SIZE = 1024; // number of chunks to serialize at once
-
-  final IndexOutput fieldsIndexOut;
-  int totalDocs;
-  int blockDocs;
-  int blockChunks;
-  long firstStartPointer;
-  long maxStartPointer;
-  final int[] docBaseDeltas;
-  final long[] startPointerDeltas;
-
-  public Lucene41StoredFieldsIndexWriter(IndexOutput indexOutput) throws IOException {
-    this.fieldsIndexOut = indexOutput;
-    reset();
-    totalDocs = 0;
-    docBaseDeltas = new int[BLOCK_SIZE];
-    startPointerDeltas = new long[BLOCK_SIZE];
-    fieldsIndexOut.writeVInt(PackedInts.VERSION_CURRENT);
-  }
-
-  private void reset() {
-    blockChunks = 0;
-    blockDocs = 0;
-    firstStartPointer = -1; // means unset
-  }
-
-  private void writeBlock() throws IOException {
-    assert blockChunks > 0;
-    fieldsIndexOut.writeVInt(blockChunks);
-
-    // The trick here is that we only store the difference from the average start
-    // pointer or doc base, this helps save bits per value.
-    // And in order to prevent a few chunks that would be far from the average to
-    // raise the number of bits per value for all of them, we only encode blocks
-    // of 1024 chunks at once
-    // See LUCENE-4512
-
-    // doc bases
-    final int avgChunkDocs;
-    if (blockChunks == 1) {
-      avgChunkDocs = 0;
-    } else {
-      avgChunkDocs = Math.round((float) (blockDocs - docBaseDeltas[blockChunks - 1]) / (blockChunks - 1));
-    }
-    fieldsIndexOut.writeVInt(totalDocs - blockDocs); // docBase
-    fieldsIndexOut.writeVInt(avgChunkDocs);
-    int docBase = 0;
-    long maxDelta = 0;
-    for (int i = 0; i < blockChunks; ++i) {
-      final int delta = docBase - avgChunkDocs * i;
-      maxDelta |= zigZagEncode(delta);
-      docBase += docBaseDeltas[i];
-    }
-
-    final int bitsPerDocBase = PackedInts.bitsRequired(maxDelta);
-    fieldsIndexOut.writeVInt(bitsPerDocBase);
-    PackedInts.Writer writer = PackedInts.getWriterNoHeader(fieldsIndexOut,
-        PackedInts.Format.PACKED, blockChunks, bitsPerDocBase, 1);
-    docBase = 0;
-    for (int i = 0; i < blockChunks; ++i) {
-      final long delta = docBase - avgChunkDocs * i;
-      assert PackedInts.bitsRequired(zigZagEncode(delta)) <= writer.bitsPerValue();
-      writer.add(zigZagEncode(delta));
-      docBase += docBaseDeltas[i];
-    }
-    writer.finish();
-
-    // start pointers
-    fieldsIndexOut.writeVLong(firstStartPointer);
-    final long avgChunkSize;
-    if (blockChunks == 1) {
-      avgChunkSize = 0;
-    } else {
-      avgChunkSize = (maxStartPointer - firstStartPointer) / (blockChunks - 1);
-    }
-    fieldsIndexOut.writeVLong(avgChunkSize);
-    long startPointer = 0;
-    maxDelta = 0;
-    for (int i = 0; i < blockChunks; ++i) {
-      startPointer += startPointerDeltas[i];
-      final long delta = startPointer - avgChunkSize * i;
-      maxDelta |= zigZagEncode(delta);
-    }
-
-    final int bitsPerStartPointer = PackedInts.bitsRequired(maxDelta);
-    fieldsIndexOut.writeVInt(bitsPerStartPointer);
-    writer = PackedInts.getWriterNoHeader(fieldsIndexOut, PackedInts.Format.PACKED,
-        blockChunks, bitsPerStartPointer, 1);
-    startPointer = 0;
-    for (int i = 0; i < blockChunks; ++i) {
-      startPointer += startPointerDeltas[i];
-      final long delta = startPointer - avgChunkSize * i;
-      assert PackedInts.bitsRequired(zigZagEncode(delta)) <= writer.bitsPerValue();
-      writer.add(zigZagEncode(delta));
-    }
-    writer.finish();
-  }
-
-  public void writeIndex(int numDocs, long startPointer) throws IOException {
-    if (blockChunks == BLOCK_SIZE) {
-      writeBlock();
-      reset();
-    }
-
-    if (firstStartPointer == -1) {
-      firstStartPointer = maxStartPointer = startPointer;
-    }
-    assert firstStartPointer > 0 && startPointer >= firstStartPointer;
-
-    docBaseDeltas[blockChunks] = numDocs;
-    startPointerDeltas[blockChunks] = startPointer - maxStartPointer;
-
-    ++blockChunks;
-    blockDocs += numDocs;
-    totalDocs += numDocs;
-    maxStartPointer = startPointer;
-  }
-
-  public void finish(int numDocs, long maxPointer) throws IOException {
-    if (numDocs != totalDocs) {
-      throw new IllegalStateException("Expected " + numDocs + " docs, but got " + totalDocs);
-    }
-    if (blockChunks > 0) {
-      writeBlock();
-    }
-    fieldsIndexOut.writeVInt(0); // end marker
-    fieldsIndexOut.writeVLong(maxPointer);
-    CodecUtil.writeFooter(fieldsIndexOut);
-  }
-
-  @Override
-  public void close() throws IOException {
-    fieldsIndexOut.close();
-  }
-
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsWriter.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsWriter.java
deleted file mode 100644
index a286d64..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsWriter.java
+++ /dev/null
@@ -1,315 +0,0 @@
-package org.apache.lucene.codecs.lucene41;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import static org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsReader.BYTE_ARR;
-import static org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsReader.CODEC_SFX_DAT;
-import static org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsReader.CODEC_SFX_IDX;
-import static org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsReader.FIELDS_EXTENSION;
-import static org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsReader.FIELDS_INDEX_EXTENSION;
-import static org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsReader.NUMERIC_DOUBLE;
-import static org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsReader.NUMERIC_FLOAT;
-import static org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsReader.NUMERIC_INT;
-import static org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsReader.NUMERIC_LONG;
-import static org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsReader.STRING;
-import static org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsReader.TYPE_BITS;
-import static org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsReader.VERSION_CURRENT;
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.StoredFieldsWriter;
-import org.apache.lucene.codecs.compressing.CompressionMode;
-import org.apache.lucene.codecs.compressing.Compressor;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.StorableField;
-import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.GrowableByteArrayDataOutput;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * Writer for 4.1 stored fields format for testing
- * @deprecated for test purposes only
- */
-@Deprecated
-final class Lucene41StoredFieldsWriter extends StoredFieldsWriter {
-
-  // hard limit on the maximum number of documents per chunk
-  static final int MAX_DOCUMENTS_PER_CHUNK = 128;
-
-  private final Directory directory;
-  private final String segment;
-  private final String segmentSuffix;
-  private Lucene41StoredFieldsIndexWriter indexWriter;
-  private IndexOutput fieldsStream;
-
-  private final Compressor compressor;
-  private final int chunkSize;
-
-  private final GrowableByteArrayDataOutput bufferedDocs;
-  private int[] numStoredFields; // number of stored fields
-  private int[] endOffsets; // end offsets in bufferedDocs
-  private int docBase; // doc ID at the beginning of the chunk
-  private int numBufferedDocs; // docBase + numBufferedDocs == current doc ID
-
-  /** Sole constructor. */
-  public Lucene41StoredFieldsWriter(Directory directory, SegmentInfo si, String segmentSuffix, IOContext context,
-      String formatName, CompressionMode compressionMode, int chunkSize) throws IOException {
-    assert directory != null;
-    this.directory = directory;
-    this.segment = si.name;
-    this.segmentSuffix = segmentSuffix;
-    this.compressor = compressionMode.newCompressor();
-    this.chunkSize = chunkSize;
-    this.docBase = 0;
-    this.bufferedDocs = new GrowableByteArrayDataOutput(chunkSize);
-    this.numStoredFields = new int[16];
-    this.endOffsets = new int[16];
-    this.numBufferedDocs = 0;
-
-    boolean success = false;
-    IndexOutput indexStream = directory.createOutput(IndexFileNames.segmentFileName(segment, segmentSuffix, FIELDS_INDEX_EXTENSION), 
-                                                                     context);
-    try {
-      fieldsStream = directory.createOutput(IndexFileNames.segmentFileName(segment, segmentSuffix, FIELDS_EXTENSION),
-                                                    context);
-
-      final String codecNameIdx = formatName + CODEC_SFX_IDX;
-      final String codecNameDat = formatName + CODEC_SFX_DAT;
-      CodecUtil.writeHeader(indexStream, codecNameIdx, VERSION_CURRENT);
-      CodecUtil.writeHeader(fieldsStream, codecNameDat, VERSION_CURRENT);
-      assert CodecUtil.headerLength(codecNameDat) == fieldsStream.getFilePointer();
-      assert CodecUtil.headerLength(codecNameIdx) == indexStream.getFilePointer();
-
-      indexWriter = new Lucene41StoredFieldsIndexWriter(indexStream);
-      indexStream = null;
-
-      fieldsStream.writeVInt(chunkSize);
-      fieldsStream.writeVInt(PackedInts.VERSION_CURRENT);
-
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(indexStream);
-        abort();
-      }
-    }
-  }
-
-  @Override
-  public void close() throws IOException {
-    try {
-      IOUtils.close(fieldsStream, indexWriter);
-    } finally {
-      fieldsStream = null;
-      indexWriter = null;
-    }
-  }
-
-  private int numStoredFieldsInDoc;
-
-  @Override
-  public void startDocument() throws IOException {
-  }
-
-  @Override
-  public void finishDocument() throws IOException {
-    if (numBufferedDocs == this.numStoredFields.length) {
-      final int newLength = ArrayUtil.oversize(numBufferedDocs + 1, 4);
-      this.numStoredFields = Arrays.copyOf(this.numStoredFields, newLength);
-      endOffsets = Arrays.copyOf(endOffsets, newLength);
-    }
-    this.numStoredFields[numBufferedDocs] = numStoredFieldsInDoc;
-    numStoredFieldsInDoc = 0;
-    endOffsets[numBufferedDocs] = bufferedDocs.length;
-    ++numBufferedDocs;
-    if (triggerFlush()) {
-      flush();
-    }
-  }
-
-  private static void saveInts(int[] values, int length, DataOutput out) throws IOException {
-    assert length > 0;
-    if (length == 1) {
-      out.writeVInt(values[0]);
-    } else {
-      boolean allEqual = true;
-      for (int i = 1; i < length; ++i) {
-        if (values[i] != values[0]) {
-          allEqual = false;
-          break;
-        }
-      }
-      if (allEqual) {
-        out.writeVInt(0);
-        out.writeVInt(values[0]);
-      } else {
-        long max = 0;
-        for (int i = 0; i < length; ++i) {
-          max |= values[i];
-        }
-        final int bitsRequired = PackedInts.bitsRequired(max);
-        out.writeVInt(bitsRequired);
-        final PackedInts.Writer w = PackedInts.getWriterNoHeader(out, PackedInts.Format.PACKED, length, bitsRequired, 1);
-        for (int i = 0; i < length; ++i) {
-          w.add(values[i]);
-        }
-        w.finish();
-      }
-    }
-  }
-
-  private void writeHeader(int docBase, int numBufferedDocs, int[] numStoredFields, int[] lengths) throws IOException {
-    // save docBase and numBufferedDocs
-    fieldsStream.writeVInt(docBase);
-    fieldsStream.writeVInt(numBufferedDocs);
-
-    // save numStoredFields
-    saveInts(numStoredFields, numBufferedDocs, fieldsStream);
-
-    // save lengths
-    saveInts(lengths, numBufferedDocs, fieldsStream);
-  }
-
-  private boolean triggerFlush() {
-    return bufferedDocs.length >= chunkSize || // chunks of at least chunkSize bytes
-        numBufferedDocs >= MAX_DOCUMENTS_PER_CHUNK;
-  }
-
-  private void flush() throws IOException {
-    indexWriter.writeIndex(numBufferedDocs, fieldsStream.getFilePointer());
-
-    // transform end offsets into lengths
-    final int[] lengths = endOffsets;
-    for (int i = numBufferedDocs - 1; i > 0; --i) {
-      lengths[i] = endOffsets[i] - endOffsets[i - 1];
-      assert lengths[i] >= 0;
-    }
-    writeHeader(docBase, numBufferedDocs, numStoredFields, lengths);
-
-    // compress stored fields to fieldsStream
-    if (bufferedDocs.length >= 2 * chunkSize) {
-      // big chunk, slice it
-      for (int compressed = 0; compressed < bufferedDocs.length; compressed += chunkSize) {
-        compressor.compress(bufferedDocs.bytes, compressed, Math.min(chunkSize, bufferedDocs.length - compressed), fieldsStream);
-      }
-    } else {
-      compressor.compress(bufferedDocs.bytes, 0, bufferedDocs.length, fieldsStream);
-    }
-
-    // reset
-    docBase += numBufferedDocs;
-    numBufferedDocs = 0;
-    bufferedDocs.length = 0;
-  }
-
-  @Override
-  public void writeField(FieldInfo info, StorableField field)
-      throws IOException {
-
-    ++numStoredFieldsInDoc;
-
-    int bits = 0;
-    final BytesRef bytes;
-    final String string;
-
-    Number number = field.numericValue();
-    if (number != null) {
-      if (number instanceof Byte || number instanceof Short || number instanceof Integer) {
-        bits = NUMERIC_INT;
-      } else if (number instanceof Long) {
-        bits = NUMERIC_LONG;
-      } else if (number instanceof Float) {
-        bits = NUMERIC_FLOAT;
-      } else if (number instanceof Double) {
-        bits = NUMERIC_DOUBLE;
-      } else {
-        throw new IllegalArgumentException("cannot store numeric type " + number.getClass());
-      }
-      string = null;
-      bytes = null;
-    } else {
-      bytes = field.binaryValue();
-      if (bytes != null) {
-        bits = BYTE_ARR;
-        string = null;
-      } else {
-        bits = STRING;
-        string = field.stringValue();
-        if (string == null) {
-          throw new IllegalArgumentException("field " + field.name() + " is stored but does not have binaryValue, stringValue nor numericValue");
-        }
-      }
-    }
-
-    final long infoAndBits = (((long) info.number) << TYPE_BITS) | bits;
-    bufferedDocs.writeVLong(infoAndBits);
-
-    if (bytes != null) {
-      bufferedDocs.writeVInt(bytes.length);
-      bufferedDocs.writeBytes(bytes.bytes, bytes.offset, bytes.length);
-    } else if (string != null) {
-      bufferedDocs.writeString(field.stringValue());
-    } else {
-      if (number instanceof Byte || number instanceof Short || number instanceof Integer) {
-        bufferedDocs.writeInt(number.intValue());
-      } else if (number instanceof Long) {
-        bufferedDocs.writeLong(number.longValue());
-      } else if (number instanceof Float) {
-        bufferedDocs.writeInt(Float.floatToIntBits(number.floatValue()));
-      } else if (number instanceof Double) {
-        bufferedDocs.writeLong(Double.doubleToLongBits(number.doubleValue()));
-      } else {
-        throw new AssertionError("Cannot get here");
-      }
-    }
-  }
-
-  @Override
-  public void abort() {
-    IOUtils.closeWhileHandlingException(this);
-    IOUtils.deleteFilesIgnoringExceptions(directory,
-        IndexFileNames.segmentFileName(segment, segmentSuffix, FIELDS_EXTENSION),
-        IndexFileNames.segmentFileName(segment, segmentSuffix, FIELDS_INDEX_EXTENSION));
-  }
-
-  @Override
-  public void finish(FieldInfos fis, int numDocs) throws IOException {
-    if (numBufferedDocs > 0) {
-      flush();
-    } else {
-      assert bufferedDocs.length == 0;
-    }
-    if (docBase != numDocs) {
-      throw new RuntimeException("Wrote " + docBase + " docs, finish called with numDocs=" + numDocs);
-    }
-    indexWriter.finish(numDocs, fieldsStream.getFilePointer());
-    CodecUtil.writeFooter(fieldsStream);
-    assert bufferedDocs.length == 0;
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/TestLucene41ForUtil.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/TestLucene41ForUtil.java
deleted file mode 100644
index 437758d..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/TestLucene41ForUtil.java
+++ /dev/null
@@ -1,94 +0,0 @@
-package org.apache.lucene.codecs.lucene41;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import static org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat.BLOCK_SIZE;
-import static org.apache.lucene.codecs.lucene41.ForUtil.MAX_DATA_SIZE;
-import static org.apache.lucene.codecs.lucene41.ForUtil.MAX_ENCODED_SIZE;
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.packed.PackedInts;
-
-import com.carrotsearch.randomizedtesting.generators.RandomInts;
-
-public class TestLucene41ForUtil extends LuceneTestCase {
-
-  public void testEncodeDecode() throws IOException {
-    final int iterations = RandomInts.randomIntBetween(random(), 1, 1000);
-    final float acceptableOverheadRatio = random().nextFloat();
-    final int[] values = new int[(iterations - 1) * BLOCK_SIZE + ForUtil.MAX_DATA_SIZE];
-    for (int i = 0; i < iterations; ++i) {
-      final int bpv = random().nextInt(32);
-      if (bpv == 0) {
-        final int value = RandomInts.randomIntBetween(random(), 0, Integer.MAX_VALUE);
-        for (int j = 0; j < BLOCK_SIZE; ++j) {
-          values[i * BLOCK_SIZE + j] = value;
-        }
-      } else {
-        for (int j = 0; j < BLOCK_SIZE; ++j) {
-          values[i * BLOCK_SIZE + j] = RandomInts.randomIntBetween(random(),
-              0, (int) PackedInts.maxValue(bpv));
-        }
-      }
-    }
-
-    final Directory d = new RAMDirectory();
-    final long endPointer;
-
-    {
-      // encode
-      IndexOutput out = d.createOutput("test.bin", IOContext.DEFAULT);
-      final ForUtil forUtil = new ForUtil(acceptableOverheadRatio, out);
-      
-      for (int i = 0; i < iterations; ++i) {
-        forUtil.writeBlock(
-            Arrays.copyOfRange(values, i * BLOCK_SIZE, values.length),
-            new byte[MAX_ENCODED_SIZE], out);
-      }
-      endPointer = out.getFilePointer();
-      out.close();
-    }
-
-    {
-      // decode
-      IndexInput in = d.openInput("test.bin", IOContext.READONCE);
-      final ForUtil forUtil = new ForUtil(in);
-      for (int i = 0; i < iterations; ++i) {
-        if (random().nextBoolean()) {
-          forUtil.skipBlock(in);
-          continue;
-        }
-        final int[] restored = new int[MAX_DATA_SIZE];
-        forUtil.readBlock(in, new byte[MAX_ENCODED_SIZE], restored);
-        assertArrayEquals(Arrays.copyOfRange(values, i * BLOCK_SIZE, (i + 1) * BLOCK_SIZE),
-            Arrays.copyOf(restored, BLOCK_SIZE));
-      }
-      assertEquals(endPointer, in.getFilePointer());
-      in.close();
-    }
-  }
-
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/TestLucene41PostingsFormat2.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/TestLucene41PostingsFormat2.java
deleted file mode 100644
index 8ebd506..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/TestLucene41PostingsFormat2.java
+++ /dev/null
@@ -1,132 +0,0 @@
-package org.apache.lucene.codecs.lucene41;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.TestUtil;
-
-/** 
- * Tests special cases of BlockPostingsFormat 
- */
-
-public class TestLucene41PostingsFormat2 extends LuceneTestCase {
-  Directory dir;
-  RandomIndexWriter iw;
-  
-  @Override
-  public void setUp() throws Exception {
-    super.setUp();
-    dir = newFSDirectory(createTempDir("testDFBlockSize"));
-    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));
-    iwc.setCodec(new Lucene41RWCodec());
-    iw = new RandomIndexWriter(random(), dir, iwc);
-    iw.setDoRandomForceMerge(false); // we will ourselves
-  }
-  
-  @Override
-  public void tearDown() throws Exception {
-    iw.close();
-    TestUtil.checkIndex(dir); // for some extra coverage, checkIndex before we forceMerge
-    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));
-    iwc.setCodec(new Lucene41RWCodec());
-    iwc.setOpenMode(OpenMode.APPEND);
-    IndexWriter iw = new IndexWriter(dir, iwc);
-    iw.forceMerge(1);
-    iw.close();
-    dir.close(); // just force a checkindex for now
-    super.tearDown();
-  }
-  
-  private Document newDocument() {
-    Document doc = new Document();
-    for (IndexOptions option : FieldInfo.IndexOptions.values()) {
-      FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);
-      // turn on tvs for a cross-check, since we rely upon checkindex in this test (for now)
-      ft.setStoreTermVectors(true);
-      ft.setStoreTermVectorOffsets(true);
-      ft.setStoreTermVectorPositions(true);
-      ft.setStoreTermVectorPayloads(true);
-      ft.setIndexOptions(option);
-      doc.add(new Field(option.toString(), "", ft));
-    }
-    return doc;
-  }
-
-  /** tests terms with df = blocksize */
-  public void testDFBlockSize() throws Exception {
-    Document doc = newDocument();
-    for (int i = 0; i < Lucene41PostingsFormat.BLOCK_SIZE; i++) {
-      for (Field f : doc.getFields()) {
-        f.setStringValue(f.name() + " " + f.name() + "_2");
-      }
-      iw.addDocument(doc);
-    }
-  }
-
-  /** tests terms with df % blocksize = 0 */
-  public void testDFBlockSizeMultiple() throws Exception {
-    Document doc = newDocument();
-    for (int i = 0; i < Lucene41PostingsFormat.BLOCK_SIZE * 16; i++) {
-      for (Field f : doc.getFields()) {
-        f.setStringValue(f.name() + " " + f.name() + "_2");
-      }
-      iw.addDocument(doc);
-    }
-  }
-  
-  /** tests terms with ttf = blocksize */
-  public void testTTFBlockSize() throws Exception {
-    Document doc = newDocument();
-    for (int i = 0; i < Lucene41PostingsFormat.BLOCK_SIZE/2; i++) {
-      for (Field f : doc.getFields()) {
-        f.setStringValue(f.name() + " " + f.name() + " " + f.name() + "_2 " + f.name() + "_2");
-      }
-      iw.addDocument(doc);
-    }
-  }
-  
-  /** tests terms with ttf % blocksize = 0 */
-  public void testTTFBlockSizeMultiple() throws Exception {
-    Document doc = newDocument();
-    for (int i = 0; i < Lucene41PostingsFormat.BLOCK_SIZE/2; i++) {
-      for (Field f : doc.getFields()) {
-        String proto = (f.name() + " " + f.name() + " " + f.name() + " " + f.name() + " " 
-                       + f.name() + "_2 " + f.name() + "_2 " + f.name() + "_2 " + f.name() + "_2");
-        StringBuilder val = new StringBuilder();
-        for (int j = 0; j < 16; j++) {
-          val.append(proto);
-          val.append(" ");
-        }
-        f.setStringValue(val.toString());
-      }
-      iw.addDocument(doc);
-    }
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/TestLucene41PostingsFormat3.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/TestLucene41PostingsFormat3.java
deleted file mode 100644
index 30a9273..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/TestLucene41PostingsFormat3.java
+++ /dev/null
@@ -1,521 +0,0 @@
-package org.apache.lucene.codecs.lucene41;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashSet;
-import java.util.Random;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.MockFixedLengthPayloadFilter;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.analysis.MockVariableLengthPayloadFilter;
-import org.apache.lucene.analysis.TokenFilter;
-import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.LeafReader;
-import org.apache.lucene.index.LeafReaderContext;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import org.apache.lucene.index.TermsEnum.SeekStatus;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.English;
-import org.apache.lucene.util.FixedBitSet;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.TestUtil;
-import org.apache.lucene.util.automaton.AutomatonTestUtil;
-import org.apache.lucene.util.automaton.CompiledAutomaton;
-import org.apache.lucene.util.automaton.RegExp;
-
-/** 
- * Tests partial enumeration (only pulling a subset of the indexed data) 
- */
-public class TestLucene41PostingsFormat3 extends LuceneTestCase {
-  static final int MAXDOC = Lucene41PostingsFormat.BLOCK_SIZE * 20;
-  
-  // creates 8 fields with different options and does "duels" of fields against each other
-  public void test() throws Exception {
-    Directory dir = newDirectory();
-    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {
-      @Override
-      protected TokenStreamComponents createComponents(String fieldName) {
-        Tokenizer tokenizer = new MockTokenizer();
-        if (fieldName.contains("payloadsFixed")) {
-          TokenFilter filter = new MockFixedLengthPayloadFilter(new Random(0), tokenizer, 1);
-          return new TokenStreamComponents(tokenizer, filter);
-        } else if (fieldName.contains("payloadsVariable")) {
-          TokenFilter filter = new MockVariableLengthPayloadFilter(new Random(0), tokenizer);
-          return new TokenStreamComponents(tokenizer, filter);
-        } else {
-          return new TokenStreamComponents(tokenizer);
-        }
-      }
-    };
-    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);
-    iwc.setCodec(new Lucene41RWCodec());
-    // TODO we could actually add more fields implemented with different PFs
-    // or, just put this test into the usual rotation?
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
-    Document doc = new Document();
-    FieldType docsOnlyType = new FieldType(TextField.TYPE_NOT_STORED);
-    // turn this on for a cross-check
-    docsOnlyType.setStoreTermVectors(true);
-    docsOnlyType.setIndexOptions(IndexOptions.DOCS_ONLY);
-    
-    FieldType docsAndFreqsType = new FieldType(TextField.TYPE_NOT_STORED);
-    // turn this on for a cross-check
-    docsAndFreqsType.setStoreTermVectors(true);
-    docsAndFreqsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS);
-    
-    FieldType positionsType = new FieldType(TextField.TYPE_NOT_STORED);
-    // turn these on for a cross-check
-    positionsType.setStoreTermVectors(true);
-    positionsType.setStoreTermVectorPositions(true);
-    positionsType.setStoreTermVectorOffsets(true);
-    positionsType.setStoreTermVectorPayloads(true);
-    FieldType offsetsType = new FieldType(positionsType);
-    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
-    Field field1 = new Field("field1docs", "", docsOnlyType);
-    Field field2 = new Field("field2freqs", "", docsAndFreqsType);
-    Field field3 = new Field("field3positions", "", positionsType);
-    Field field4 = new Field("field4offsets", "", offsetsType);
-    Field field5 = new Field("field5payloadsFixed", "", positionsType);
-    Field field6 = new Field("field6payloadsVariable", "", positionsType);
-    Field field7 = new Field("field7payloadsFixedOffsets", "", offsetsType);
-    Field field8 = new Field("field8payloadsVariableOffsets", "", offsetsType);
-    doc.add(field1);
-    doc.add(field2);
-    doc.add(field3);
-    doc.add(field4);
-    doc.add(field5);
-    doc.add(field6);
-    doc.add(field7);
-    doc.add(field8);
-    for (int i = 0; i < MAXDOC; i++) {
-      String stringValue = Integer.toString(i) + " verycommon " + English.intToEnglish(i).replace('-', ' ') + " " + TestUtil.randomSimpleString(random());
-      field1.setStringValue(stringValue);
-      field2.setStringValue(stringValue);
-      field3.setStringValue(stringValue);
-      field4.setStringValue(stringValue);
-      field5.setStringValue(stringValue);
-      field6.setStringValue(stringValue);
-      field7.setStringValue(stringValue);
-      field8.setStringValue(stringValue);
-      iw.addDocument(doc);
-    }
-    iw.close();
-    verify(dir);
-    TestUtil.checkIndex(dir); // for some extra coverage, checkIndex before we forceMerge
-    iwc = newIndexWriterConfig(analyzer);
-    iwc.setCodec(new Lucene41RWCodec());
-    iwc.setOpenMode(OpenMode.APPEND);
-    IndexWriter iw2 = new IndexWriter(dir, iwc);
-    iw2.forceMerge(1);
-    iw2.close();
-    verify(dir);
-    dir.close();
-  }
-  
-  private void verify(Directory dir) throws Exception {
-    DirectoryReader ir = DirectoryReader.open(dir);
-    for (LeafReaderContext leaf : ir.leaves()) {
-      LeafReader leafReader = leaf.reader();
-      assertTerms(leafReader.terms("field1docs"), leafReader.terms("field2freqs"), true);
-      assertTerms(leafReader.terms("field3positions"), leafReader.terms("field4offsets"), true);
-      assertTerms(leafReader.terms("field4offsets"), leafReader.terms("field5payloadsFixed"), true);
-      assertTerms(leafReader.terms("field5payloadsFixed"), leafReader.terms("field6payloadsVariable"), true);
-      assertTerms(leafReader.terms("field6payloadsVariable"), leafReader.terms("field7payloadsFixedOffsets"), true);
-      assertTerms(leafReader.terms("field7payloadsFixedOffsets"), leafReader.terms("field8payloadsVariableOffsets"), true);
-    }
-    ir.close();
-  }
-  
-  // following code is almost an exact dup of code from TestDuelingCodecs: sorry!
-  
-  public void assertTerms(Terms leftTerms, Terms rightTerms, boolean deep) throws Exception {
-    if (leftTerms == null || rightTerms == null) {
-      assertNull(leftTerms);
-      assertNull(rightTerms);
-      return;
-    }
-    assertTermsStatistics(leftTerms, rightTerms);
-    
-    // NOTE: we don't assert hasOffsets/hasPositions/hasPayloads because they are allowed to be different
-
-    TermsEnum leftTermsEnum = leftTerms.iterator(null);
-    TermsEnum rightTermsEnum = rightTerms.iterator(null);
-    assertTermsEnum(leftTermsEnum, rightTermsEnum, true);
-    
-    assertTermsSeeking(leftTerms, rightTerms);
-    
-    if (deep) {
-      int numIntersections = atLeast(3);
-      for (int i = 0; i < numIntersections; i++) {
-        String re = AutomatonTestUtil.randomRegexp(random());
-        CompiledAutomaton automaton = new CompiledAutomaton(new RegExp(re, RegExp.NONE).toAutomaton());
-        if (automaton.type == CompiledAutomaton.AUTOMATON_TYPE.NORMAL) {
-          // TODO: test start term too
-          TermsEnum leftIntersection = leftTerms.intersect(automaton, null);
-          TermsEnum rightIntersection = rightTerms.intersect(automaton, null);
-          assertTermsEnum(leftIntersection, rightIntersection, rarely());
-        }
-      }
-    }
-  }
-  
-  private void assertTermsSeeking(Terms leftTerms, Terms rightTerms) throws Exception {
-    TermsEnum leftEnum = null;
-    TermsEnum rightEnum = null;
-    
-    // just an upper bound
-    int numTests = atLeast(20);
-    Random random = random();
-    
-    // collect this number of terms from the left side
-    HashSet<BytesRef> tests = new HashSet<>();
-    int numPasses = 0;
-    while (numPasses < 10 && tests.size() < numTests) {
-      leftEnum = leftTerms.iterator(leftEnum);
-      BytesRef term = null;
-      while ((term = leftEnum.next()) != null) {
-        int code = random.nextInt(10);
-        if (code == 0) {
-          // the term
-          tests.add(BytesRef.deepCopyOf(term));
-        } else if (code == 1) {
-          // truncated subsequence of term
-          term = BytesRef.deepCopyOf(term);
-          if (term.length > 0) {
-            // truncate it
-            term.length = random.nextInt(term.length);
-          }
-        } else if (code == 2) {
-          // term, but ensure a non-zero offset
-          byte newbytes[] = new byte[term.length+5];
-          System.arraycopy(term.bytes, term.offset, newbytes, 5, term.length);
-          tests.add(new BytesRef(newbytes, 5, term.length));
-        }
-      }
-      numPasses++;
-    }
-    
-    ArrayList<BytesRef> shuffledTests = new ArrayList<>(tests);
-    Collections.shuffle(shuffledTests, random);
-    
-    for (BytesRef b : shuffledTests) {
-      leftEnum = leftTerms.iterator(leftEnum);
-      rightEnum = rightTerms.iterator(rightEnum);
-      
-      assertEquals(leftEnum.seekExact(b), rightEnum.seekExact(b));
-      assertEquals(leftEnum.seekExact(b), rightEnum.seekExact(b));
-      
-      SeekStatus leftStatus;
-      SeekStatus rightStatus;
-      
-      leftStatus = leftEnum.seekCeil(b);
-      rightStatus = rightEnum.seekCeil(b);
-      assertEquals(leftStatus, rightStatus);
-      if (leftStatus != SeekStatus.END) {
-        assertEquals(leftEnum.term(), rightEnum.term());
-      }
-      
-      leftStatus = leftEnum.seekCeil(b);
-      rightStatus = rightEnum.seekCeil(b);
-      assertEquals(leftStatus, rightStatus);
-      if (leftStatus != SeekStatus.END) {
-        assertEquals(leftEnum.term(), rightEnum.term());
-      }
-    }
-  }
-  
-  /** 
-   * checks collection-level statistics on Terms 
-   */
-  public void assertTermsStatistics(Terms leftTerms, Terms rightTerms) throws Exception {
-    if (leftTerms.getDocCount() != -1 && rightTerms.getDocCount() != -1) {
-      assertEquals(leftTerms.getDocCount(), rightTerms.getDocCount());
-    }
-    if (leftTerms.getSumDocFreq() != -1 && rightTerms.getSumDocFreq() != -1) {
-      assertEquals(leftTerms.getSumDocFreq(), rightTerms.getSumDocFreq());
-    }
-    if (leftTerms.getSumTotalTermFreq() != -1 && rightTerms.getSumTotalTermFreq() != -1) {
-      assertEquals(leftTerms.getSumTotalTermFreq(), rightTerms.getSumTotalTermFreq());
-    }
-    if (leftTerms.size() != -1 && rightTerms.size() != -1) {
-      assertEquals(leftTerms.size(), rightTerms.size());
-    }
-  }
-
-  /** 
-   * checks the terms enum sequentially
-   * if deep is false, it does a 'shallow' test that doesnt go down to the docsenums
-   */
-  public void assertTermsEnum(TermsEnum leftTermsEnum, TermsEnum rightTermsEnum, boolean deep) throws Exception {
-    BytesRef term;
-    Bits randomBits = new RandomBits(MAXDOC, random().nextDouble(), random());
-    DocsAndPositionsEnum leftPositions = null;
-    DocsAndPositionsEnum rightPositions = null;
-    DocsEnum leftDocs = null;
-    DocsEnum rightDocs = null;
-    
-    while ((term = leftTermsEnum.next()) != null) {
-      assertEquals(term, rightTermsEnum.next());
-      assertTermStats(leftTermsEnum, rightTermsEnum);
-      if (deep) {
-        // with payloads + off
-        assertDocsAndPositionsEnum(leftPositions = leftTermsEnum.docsAndPositions(null, leftPositions),
-                                   rightPositions = rightTermsEnum.docsAndPositions(null, rightPositions));
-        assertDocsAndPositionsEnum(leftPositions = leftTermsEnum.docsAndPositions(randomBits, leftPositions),
-                                   rightPositions = rightTermsEnum.docsAndPositions(randomBits, rightPositions));
-
-        assertPositionsSkipping(leftTermsEnum.docFreq(), 
-                                leftPositions = leftTermsEnum.docsAndPositions(null, leftPositions),
-                                rightPositions = rightTermsEnum.docsAndPositions(null, rightPositions));
-        assertPositionsSkipping(leftTermsEnum.docFreq(), 
-                                leftPositions = leftTermsEnum.docsAndPositions(randomBits, leftPositions),
-                                rightPositions = rightTermsEnum.docsAndPositions(randomBits, rightPositions));
-        // with payloads only
-        assertDocsAndPositionsEnum(leftPositions = leftTermsEnum.docsAndPositions(null, leftPositions, DocsAndPositionsEnum.FLAG_PAYLOADS),
-                                   rightPositions = rightTermsEnum.docsAndPositions(null, rightPositions, DocsAndPositionsEnum.FLAG_PAYLOADS));
-        assertDocsAndPositionsEnum(leftPositions = leftTermsEnum.docsAndPositions(randomBits, leftPositions, DocsAndPositionsEnum.FLAG_PAYLOADS),
-                                   rightPositions = rightTermsEnum.docsAndPositions(randomBits, rightPositions, DocsAndPositionsEnum.FLAG_PAYLOADS));
-
-        assertPositionsSkipping(leftTermsEnum.docFreq(), 
-                                leftPositions = leftTermsEnum.docsAndPositions(null, leftPositions, DocsAndPositionsEnum.FLAG_PAYLOADS),
-                                rightPositions = rightTermsEnum.docsAndPositions(null, rightPositions, DocsAndPositionsEnum.FLAG_PAYLOADS));
-        assertPositionsSkipping(leftTermsEnum.docFreq(), 
-                                leftPositions = leftTermsEnum.docsAndPositions(randomBits, leftPositions, DocsAndPositionsEnum.FLAG_PAYLOADS),
-                                rightPositions = rightTermsEnum.docsAndPositions(randomBits, rightPositions, DocsAndPositionsEnum.FLAG_PAYLOADS));
-
-        // with offsets only
-        assertDocsAndPositionsEnum(leftPositions = leftTermsEnum.docsAndPositions(null, leftPositions, DocsAndPositionsEnum.FLAG_OFFSETS),
-                                   rightPositions = rightTermsEnum.docsAndPositions(null, rightPositions, DocsAndPositionsEnum.FLAG_OFFSETS));
-        assertDocsAndPositionsEnum(leftPositions = leftTermsEnum.docsAndPositions(randomBits, leftPositions, DocsAndPositionsEnum.FLAG_OFFSETS),
-                                   rightPositions = rightTermsEnum.docsAndPositions(randomBits, rightPositions, DocsAndPositionsEnum.FLAG_OFFSETS));
-
-        assertPositionsSkipping(leftTermsEnum.docFreq(), 
-                                leftPositions = leftTermsEnum.docsAndPositions(null, leftPositions, DocsAndPositionsEnum.FLAG_OFFSETS),
-                                rightPositions = rightTermsEnum.docsAndPositions(null, rightPositions, DocsAndPositionsEnum.FLAG_OFFSETS));
-        assertPositionsSkipping(leftTermsEnum.docFreq(), 
-                                leftPositions = leftTermsEnum.docsAndPositions(randomBits, leftPositions, DocsAndPositionsEnum.FLAG_OFFSETS),
-                                rightPositions = rightTermsEnum.docsAndPositions(randomBits, rightPositions, DocsAndPositionsEnum.FLAG_OFFSETS));
-        
-        // with positions only
-        assertDocsAndPositionsEnum(leftPositions = leftTermsEnum.docsAndPositions(null, leftPositions, DocsEnum.FLAG_NONE),
-                                   rightPositions = rightTermsEnum.docsAndPositions(null, rightPositions, DocsEnum.FLAG_NONE));
-        assertDocsAndPositionsEnum(leftPositions = leftTermsEnum.docsAndPositions(randomBits, leftPositions, DocsEnum.FLAG_NONE),
-                                   rightPositions = rightTermsEnum.docsAndPositions(randomBits, rightPositions, DocsEnum.FLAG_NONE));
-
-        assertPositionsSkipping(leftTermsEnum.docFreq(), 
-                                leftPositions = leftTermsEnum.docsAndPositions(null, leftPositions, DocsEnum.FLAG_NONE),
-                                rightPositions = rightTermsEnum.docsAndPositions(null, rightPositions, DocsEnum.FLAG_NONE));
-        assertPositionsSkipping(leftTermsEnum.docFreq(), 
-                                leftPositions = leftTermsEnum.docsAndPositions(randomBits, leftPositions, DocsEnum.FLAG_NONE),
-                                rightPositions = rightTermsEnum.docsAndPositions(randomBits, rightPositions, DocsEnum.FLAG_NONE));
-        
-        // with freqs:
-        assertDocsEnum(leftDocs = leftTermsEnum.docs(null, leftDocs),
-            rightDocs = rightTermsEnum.docs(null, rightDocs));
-        assertDocsEnum(leftDocs = leftTermsEnum.docs(randomBits, leftDocs),
-            rightDocs = rightTermsEnum.docs(randomBits, rightDocs));
-
-        // w/o freqs:
-        assertDocsEnum(leftDocs = leftTermsEnum.docs(null, leftDocs, DocsEnum.FLAG_NONE),
-            rightDocs = rightTermsEnum.docs(null, rightDocs, DocsEnum.FLAG_NONE));
-        assertDocsEnum(leftDocs = leftTermsEnum.docs(randomBits, leftDocs, DocsEnum.FLAG_NONE),
-            rightDocs = rightTermsEnum.docs(randomBits, rightDocs, DocsEnum.FLAG_NONE));
-        
-        // with freqs:
-        assertDocsSkipping(leftTermsEnum.docFreq(), 
-            leftDocs = leftTermsEnum.docs(null, leftDocs),
-            rightDocs = rightTermsEnum.docs(null, rightDocs));
-        assertDocsSkipping(leftTermsEnum.docFreq(), 
-            leftDocs = leftTermsEnum.docs(randomBits, leftDocs),
-            rightDocs = rightTermsEnum.docs(randomBits, rightDocs));
-
-        // w/o freqs:
-        assertDocsSkipping(leftTermsEnum.docFreq(), 
-            leftDocs = leftTermsEnum.docs(null, leftDocs, DocsEnum.FLAG_NONE),
-            rightDocs = rightTermsEnum.docs(null, rightDocs, DocsEnum.FLAG_NONE));
-        assertDocsSkipping(leftTermsEnum.docFreq(), 
-            leftDocs = leftTermsEnum.docs(randomBits, leftDocs, DocsEnum.FLAG_NONE),
-            rightDocs = rightTermsEnum.docs(randomBits, rightDocs, DocsEnum.FLAG_NONE));
-      }
-    }
-    assertNull(rightTermsEnum.next());
-  }
-  
-  /**
-   * checks term-level statistics
-   */
-  public void assertTermStats(TermsEnum leftTermsEnum, TermsEnum rightTermsEnum) throws Exception {
-    assertEquals(leftTermsEnum.docFreq(), rightTermsEnum.docFreq());
-    if (leftTermsEnum.totalTermFreq() != -1 && rightTermsEnum.totalTermFreq() != -1) {
-      assertEquals(leftTermsEnum.totalTermFreq(), rightTermsEnum.totalTermFreq());
-    }
-  }
-  
-  /**
-   * checks docs + freqs + positions + payloads, sequentially
-   */
-  public void assertDocsAndPositionsEnum(DocsAndPositionsEnum leftDocs, DocsAndPositionsEnum rightDocs) throws Exception {
-    if (leftDocs == null || rightDocs == null) {
-      assertNull(leftDocs);
-      assertNull(rightDocs);
-      return;
-    }
-    assertEquals(-1, leftDocs.docID());
-    assertEquals(-1, rightDocs.docID());
-    int docid;
-    while ((docid = leftDocs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
-      assertEquals(docid, rightDocs.nextDoc());
-      int freq = leftDocs.freq();
-      assertEquals(freq, rightDocs.freq());
-      for (int i = 0; i < freq; i++) {
-        assertEquals(leftDocs.nextPosition(), rightDocs.nextPosition());
-        // we don't assert offsets/payloads, they are allowed to be different
-      }
-    }
-    assertEquals(DocIdSetIterator.NO_MORE_DOCS, rightDocs.nextDoc());
-  }
-  
-  /**
-   * checks docs + freqs, sequentially
-   */
-  public void assertDocsEnum(DocsEnum leftDocs, DocsEnum rightDocs) throws Exception {
-    if (leftDocs == null) {
-      assertNull(rightDocs);
-      return;
-    }
-    assertEquals(-1, leftDocs.docID());
-    assertEquals(-1, rightDocs.docID());
-    int docid;
-    while ((docid = leftDocs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
-      assertEquals(docid, rightDocs.nextDoc());
-      // we don't assert freqs, they are allowed to be different
-    }
-    assertEquals(DocIdSetIterator.NO_MORE_DOCS, rightDocs.nextDoc());
-  }
-  
-  /**
-   * checks advancing docs
-   */
-  public void assertDocsSkipping(int docFreq, DocsEnum leftDocs, DocsEnum rightDocs) throws Exception {
-    if (leftDocs == null) {
-      assertNull(rightDocs);
-      return;
-    }
-    int docid = -1;
-    int averageGap = MAXDOC / (1+docFreq);
-    int skipInterval = 16;
-
-    while (true) {
-      if (random().nextBoolean()) {
-        // nextDoc()
-        docid = leftDocs.nextDoc();
-        assertEquals(docid, rightDocs.nextDoc());
-      } else {
-        // advance()
-        int skip = docid + (int) Math.ceil(Math.abs(skipInterval + random().nextGaussian() * averageGap));
-        docid = leftDocs.advance(skip);
-        assertEquals(docid, rightDocs.advance(skip));
-      }
-      
-      if (docid == DocIdSetIterator.NO_MORE_DOCS) {
-        return;
-      }
-      // we don't assert freqs, they are allowed to be different
-    }
-  }
-  
-  /**
-   * checks advancing docs + positions
-   */
-  public void assertPositionsSkipping(int docFreq, DocsAndPositionsEnum leftDocs, DocsAndPositionsEnum rightDocs) throws Exception {
-    if (leftDocs == null || rightDocs == null) {
-      assertNull(leftDocs);
-      assertNull(rightDocs);
-      return;
-    }
-    
-    int docid = -1;
-    int averageGap = MAXDOC / (1+docFreq);
-    int skipInterval = 16;
-
-    while (true) {
-      if (random().nextBoolean()) {
-        // nextDoc()
-        docid = leftDocs.nextDoc();
-        assertEquals(docid, rightDocs.nextDoc());
-      } else {
-        // advance()
-        int skip = docid + (int) Math.ceil(Math.abs(skipInterval + random().nextGaussian() * averageGap));
-        docid = leftDocs.advance(skip);
-        assertEquals(docid, rightDocs.advance(skip));
-      }
-      
-      if (docid == DocIdSetIterator.NO_MORE_DOCS) {
-        return;
-      }
-      int freq = leftDocs.freq();
-      assertEquals(freq, rightDocs.freq());
-      for (int i = 0; i < freq; i++) {
-        assertEquals(leftDocs.nextPosition(), rightDocs.nextPosition());
-        // we don't compare the payloads, its allowed that one is empty etc
-      }
-    }
-  }
-  
-  private static class RandomBits implements Bits {
-    FixedBitSet bits;
-    
-    RandomBits(int maxDoc, double pctLive, Random random) {
-      bits = new FixedBitSet(maxDoc);
-      for (int i = 0; i < maxDoc; i++) {
-        if (random.nextDouble() <= pctLive) {        
-          bits.set(i);
-        }
-      }
-    }
-    
-    @Override
-    public boolean get(int index) {
-      return bits.get(index);
-    }
-
-    @Override
-    public int length() {
-      return bits.length();
-    }
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/TestLucene41StoredFieldsFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/TestLucene41StoredFieldsFormat.java
deleted file mode 100644
index 9cee782..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/TestLucene41StoredFieldsFormat.java
+++ /dev/null
@@ -1,28 +0,0 @@
-package org.apache.lucene.codecs.lucene41;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.BaseStoredFieldsFormatTestCase;
-
-public class TestLucene41StoredFieldsFormat extends BaseStoredFieldsFormatTestCase {
-  @Override
-  protected Codec getCodec() {
-    return new Lucene41RWCodec();
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene410/Lucene410RWCodec.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene410/Lucene410RWCodec.java
deleted file mode 100644
index c8da82d..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene410/Lucene410RWCodec.java
+++ /dev/null
@@ -1,80 +0,0 @@
-package org.apache.lucene.codecs.lucene410;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.SegmentInfoFormat;
-import org.apache.lucene.codecs.StoredFieldsFormat;
-import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.codecs.lucene41.Lucene41RWPostingsFormat;
-import org.apache.lucene.codecs.lucene41.Lucene41RWStoredFieldsFormat;
-import org.apache.lucene.codecs.lucene42.Lucene42RWTermVectorsFormat;
-import org.apache.lucene.codecs.lucene46.Lucene46RWSegmentInfoFormat;
-import org.apache.lucene.codecs.lucene49.Lucene49RWNormsFormat;
-
-/**
- * Read-Write version of 4.10 codec for testing
- * @deprecated for test purposes only
- */
-@Deprecated
-public final class Lucene410RWCodec extends Lucene410Codec {
-  
-  private final PostingsFormat postings = new Lucene41RWPostingsFormat();
-  
-  @Override
-  public PostingsFormat getPostingsFormatForField(String field) {
-    return postings;
-  }
-  
-  private static final DocValuesFormat docValues = new Lucene410RWDocValuesFormat();
-  
-  @Override
-  public DocValuesFormat getDocValuesFormatForField(String field) {
-    return docValues;
-  }
-  
-  private static final NormsFormat norms = new Lucene49RWNormsFormat();
-
-  @Override
-  public NormsFormat normsFormat() {
-    return norms;
-  }
-  
-  private static final SegmentInfoFormat segmentInfos = new Lucene46RWSegmentInfoFormat();
-
-  @Override
-  public SegmentInfoFormat segmentInfoFormat() {
-    return segmentInfos;
-  }
-  
-  private static final StoredFieldsFormat storedFields = new Lucene41RWStoredFieldsFormat();
-
-  @Override
-  public StoredFieldsFormat storedFieldsFormat() {
-    return storedFields;
-  }
-  
-  private final TermVectorsFormat vectorsFormat = new Lucene42RWTermVectorsFormat();
-
-  @Override
-  public TermVectorsFormat termVectorsFormat() {
-    return vectorsFormat;
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene410/Lucene410RWDocValuesFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene410/Lucene410RWDocValuesFormat.java
deleted file mode 100644
index 02d7b1e..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene410/Lucene410RWDocValuesFormat.java
+++ /dev/null
@@ -1,42 +0,0 @@
-package org.apache.lucene.codecs.lucene410;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.SegmentWriteState;
-
-/**
- * Read-Write version of 4.10 docvalues format for testing
- * @deprecated for test purposes only
- */
-class Lucene410RWDocValuesFormat extends Lucene410DocValuesFormat {
-
-  @Override
-  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    return new Lucene410DocValuesConsumer(state, DATA_CODEC, DATA_EXTENSION, META_CODEC, META_EXTENSION) {
-      @Override
-      void checkCanWrite(FieldInfo field) {
-        // allow writing all fields 
-      }
-    };
-  }
-  
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene410/TestLucene410DocValuesFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene410/TestLucene410DocValuesFormat.java
deleted file mode 100644
index baa0eb9..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene410/TestLucene410DocValuesFormat.java
+++ /dev/null
@@ -1,273 +0,0 @@
-package org.apache.lucene.codecs.lucene410;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.asserting.AssertingCodec;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.SortedSetDocValuesField;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.LeafReader;
-import org.apache.lucene.index.LeafReaderContext;
-import org.apache.lucene.index.BaseCompressingDocValuesFormatTestCase;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.SerialMergeScheduler;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.TermsEnum.SeekStatus;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.TestUtil;
-
-/**
- * Tests Lucene410DocValuesFormat
- */
-public class TestLucene410DocValuesFormat extends BaseCompressingDocValuesFormatTestCase {
-  private final Codec codec = new Lucene410RWCodec();
-
-  @Override
-  protected Codec getCodec() {
-    return codec;
-  }
-  
-  // TODO: these big methods can easily blow up some of the other ram-hungry codecs...
-  // for now just keep them here, as we want to test this for this format.
-  
-  public void testSortedSetVariableLengthBigVsStoredFields() throws Exception {
-    int numIterations = atLeast(1);
-    for (int i = 0; i < numIterations; i++) {
-      doTestSortedSetVsStoredFields(atLeast(300), 1, 32766, 16);
-    }
-  }
-  
-  @Nightly
-  public void testSortedSetVariableLengthManyVsStoredFields() throws Exception {
-    int numIterations = atLeast(1);
-    for (int i = 0; i < numIterations; i++) {
-      doTestSortedSetVsStoredFields(TestUtil.nextInt(random(), 1024, 2049), 1, 500, 16);
-    }
-  }
-  
-  public void testSortedVariableLengthBigVsStoredFields() throws Exception {
-    int numIterations = atLeast(1);
-    for (int i = 0; i < numIterations; i++) {
-      doTestSortedVsStoredFields(atLeast(300), 1, 32766);
-    }
-  }
-  
-  @Nightly
-  public void testSortedVariableLengthManyVsStoredFields() throws Exception {
-    int numIterations = atLeast(1);
-    for (int i = 0; i < numIterations; i++) {
-      doTestSortedVsStoredFields(TestUtil.nextInt(random(), 1024, 2049), 1, 500);
-    }
-  }
-  
-  public void testTermsEnumFixedWidth() throws Exception {
-    int numIterations = atLeast(1);
-    for (int i = 0; i < numIterations; i++) {
-      doTestTermsEnumRandom(TestUtil.nextInt(random(), 1025, 5121), 10, 10);
-    }
-  }
-  
-  public void testTermsEnumVariableWidth() throws Exception {
-    int numIterations = atLeast(1);
-    for (int i = 0; i < numIterations; i++) {
-      doTestTermsEnumRandom(TestUtil.nextInt(random(), 1025, 5121), 1, 500);
-    }
-  }
-  
-  @Nightly
-  public void testTermsEnumRandomMany() throws Exception {
-    int numIterations = atLeast(1);
-    for (int i = 0; i < numIterations; i++) {
-      doTestTermsEnumRandom(TestUtil.nextInt(random(), 1025, 8121), 1, 500);
-    }
-  }
-  
-  // TODO: try to refactor this and some termsenum tests into the base class.
-  // to do this we need to fix the test class to get a DVF not a Codec so we can setup
-  // the postings format correctly.
-  private void doTestTermsEnumRandom(int numDocs, int minLength, int maxLength) throws Exception {
-    Directory dir = newFSDirectory(createTempDir());
-    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));
-    conf.setMergeScheduler(new SerialMergeScheduler());
-    // set to duel against a codec which has ordinals:
-    final PostingsFormat pf = TestUtil.getPostingsFormatWithOrds(random());
-    final DocValuesFormat dv = new Lucene410RWDocValuesFormat();
-    conf.setCodec(new AssertingCodec() {
-      @Override
-      public PostingsFormat getPostingsFormatForField(String field) {
-        return pf;
-      }
-
-      @Override
-      public DocValuesFormat getDocValuesFormatForField(String field) {
-        return dv;
-      }
-    });
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);
-    
-    // index some docs
-    for (int i = 0; i < numDocs; i++) {
-      Document doc = new Document();
-      Field idField = new StringField("id", Integer.toString(i), Field.Store.NO);
-      doc.add(idField);
-      final int length = TestUtil.nextInt(random(), minLength, maxLength);
-      int numValues = random().nextInt(17);
-      // create a random list of strings
-      List<String> values = new ArrayList<>();
-      for (int v = 0; v < numValues; v++) {
-        values.add(TestUtil.randomSimpleString(random(), minLength, length));
-      }
-      
-      // add in any order to the indexed field
-      ArrayList<String> unordered = new ArrayList<>(values);
-      Collections.shuffle(unordered, random());
-      for (String v : values) {
-        doc.add(newStringField("indexed", v, Field.Store.NO));
-      }
-
-      // add in any order to the dv field
-      ArrayList<String> unordered2 = new ArrayList<>(values);
-      Collections.shuffle(unordered2, random());
-      for (String v : unordered2) {
-        doc.add(new SortedSetDocValuesField("dv", new BytesRef(v)));
-      }
-
-      writer.addDocument(doc);
-      if (random().nextInt(31) == 0) {
-        writer.commit();
-      }
-    }
-    
-    // delete some docs
-    int numDeletions = random().nextInt(numDocs/10);
-    for (int i = 0; i < numDeletions; i++) {
-      int id = random().nextInt(numDocs);
-      writer.deleteDocuments(new Term("id", Integer.toString(id)));
-    }
-    
-    // compare per-segment
-    DirectoryReader ir = writer.getReader();
-    for (LeafReaderContext context : ir.leaves()) {
-      LeafReader r = context.reader();
-      Terms terms = r.terms("indexed");
-      if (terms != null) {
-        assertEquals(terms.size(), r.getSortedSetDocValues("dv").getValueCount());
-        TermsEnum expected = terms.iterator(null);
-        TermsEnum actual = r.getSortedSetDocValues("dv").termsEnum();
-        assertEquals(terms.size(), expected, actual);
-      }
-    }
-    ir.close();
-    
-    writer.forceMerge(1);
-    
-    // now compare again after the merge
-    ir = writer.getReader();
-    LeafReader ar = getOnlySegmentReader(ir);
-    Terms terms = ar.terms("indexed");
-    if (terms != null) {
-      assertEquals(terms.size(), ar.getSortedSetDocValues("dv").getValueCount());
-      TermsEnum expected = terms.iterator(null);
-      TermsEnum actual = ar.getSortedSetDocValues("dv").termsEnum();
-      assertEquals(terms.size(), expected, actual);
-    }
-    ir.close();
-    
-    writer.close();
-    dir.close();
-  }
-  
-  private void assertEquals(long numOrds, TermsEnum expected, TermsEnum actual) throws Exception {
-    BytesRef ref;
-    
-    // sequential next() through all terms
-    while ((ref = expected.next()) != null) {
-      assertEquals(ref, actual.next());
-      assertEquals(expected.ord(), actual.ord());
-      assertEquals(expected.term(), actual.term());
-    }
-    assertNull(actual.next());
-    
-    // sequential seekExact(ord) through all terms
-    for (long i = 0; i < numOrds; i++) {
-      expected.seekExact(i);
-      actual.seekExact(i);
-      assertEquals(expected.ord(), actual.ord());
-      assertEquals(expected.term(), actual.term());
-    }
-    
-    // sequential seekExact(BytesRef) through all terms
-    for (long i = 0; i < numOrds; i++) {
-      expected.seekExact(i);
-      assertTrue(actual.seekExact(expected.term()));
-      assertEquals(expected.ord(), actual.ord());
-      assertEquals(expected.term(), actual.term());
-    }
-    
-    // sequential seekCeil(BytesRef) through all terms
-    for (long i = 0; i < numOrds; i++) {
-      expected.seekExact(i);
-      assertEquals(SeekStatus.FOUND, actual.seekCeil(expected.term()));
-      assertEquals(expected.ord(), actual.ord());
-      assertEquals(expected.term(), actual.term());
-    }
-    
-    // random seekExact(ord)
-    for (long i = 0; i < numOrds; i++) {
-      long randomOrd = TestUtil.nextLong(random(), 0, numOrds - 1);
-      expected.seekExact(randomOrd);
-      actual.seekExact(randomOrd);
-      assertEquals(expected.ord(), actual.ord());
-      assertEquals(expected.term(), actual.term());
-    }
-    
-    // random seekExact(BytesRef)
-    for (long i = 0; i < numOrds; i++) {
-      long randomOrd = TestUtil.nextLong(random(), 0, numOrds - 1);
-      expected.seekExact(randomOrd);
-      actual.seekExact(expected.term());
-      assertEquals(expected.ord(), actual.ord());
-      assertEquals(expected.term(), actual.term());
-    }
-    
-    // random seekCeil(BytesRef)
-    for (long i = 0; i < numOrds; i++) {
-      BytesRef target = new BytesRef(TestUtil.randomUnicodeString(random()));
-      SeekStatus expectedStatus = expected.seekCeil(target);
-      assertEquals(expectedStatus, actual.seekCeil(target));
-      if (expectedStatus != SeekStatus.END) {
-        assertEquals(expected.ord(), actual.ord());
-        assertEquals(expected.term(), actual.term());
-      }
-    }
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42DocValuesConsumer.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42DocValuesConsumer.java
deleted file mode 100644
index 1bbab87..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42DocValuesConsumer.java
+++ /dev/null
@@ -1,398 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.NoSuchElementException;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.codecs.MissingOrdRemapper;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.ByteArrayDataOutput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.IntsRefBuilder;
-import org.apache.lucene.util.MathUtil;
-import org.apache.lucene.util.fst.Builder;
-import org.apache.lucene.util.fst.FST.INPUT_TYPE;
-import org.apache.lucene.util.fst.FST;
-import org.apache.lucene.util.fst.PositiveIntOutputs;
-import org.apache.lucene.util.fst.Util;
-import org.apache.lucene.util.packed.BlockPackedWriter;
-import org.apache.lucene.util.packed.MonotonicBlockPackedWriter;
-import org.apache.lucene.util.packed.PackedInts.FormatAndBits;
-import org.apache.lucene.util.packed.PackedInts;
-
-import static org.apache.lucene.codecs.lucene42.Lucene42DocValuesProducer.VERSION_GCD_COMPRESSION;
-import static org.apache.lucene.codecs.lucene42.Lucene42DocValuesProducer.BLOCK_SIZE;
-import static org.apache.lucene.codecs.lucene42.Lucene42DocValuesProducer.BYTES;
-import static org.apache.lucene.codecs.lucene42.Lucene42DocValuesProducer.NUMBER;
-import static org.apache.lucene.codecs.lucene42.Lucene42DocValuesProducer.FST;
-import static org.apache.lucene.codecs.lucene42.Lucene42DocValuesProducer.DELTA_COMPRESSED;
-import static org.apache.lucene.codecs.lucene42.Lucene42DocValuesProducer.GCD_COMPRESSED;
-import static org.apache.lucene.codecs.lucene42.Lucene42DocValuesProducer.TABLE_COMPRESSED;
-import static org.apache.lucene.codecs.lucene42.Lucene42DocValuesProducer.UNCOMPRESSED;
-
-/**
- * Writer for 4.2 docvalues format for testing
- * @deprecated for test purposes only
- */
-@Deprecated
-final class Lucene42DocValuesConsumer extends DocValuesConsumer {
-  final IndexOutput data, meta;
-  final int maxDoc;
-  final float acceptableOverheadRatio;
-  
-  Lucene42DocValuesConsumer(SegmentWriteState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension, float acceptableOverheadRatio) throws IOException {
-    this.acceptableOverheadRatio = acceptableOverheadRatio;
-    maxDoc = state.segmentInfo.getDocCount();
-    boolean success = false;
-    try {
-      String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
-      data = state.directory.createOutput(dataName, state.context);
-      // this writer writes the format 4.2 did!
-      CodecUtil.writeHeader(data, dataCodec, VERSION_GCD_COMPRESSION);
-      String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
-      meta = state.directory.createOutput(metaName, state.context);
-      CodecUtil.writeHeader(meta, metaCodec, VERSION_GCD_COMPRESSION);
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(this);
-      }
-    }
-  }
-
-  @Override
-  public void addNumericField(FieldInfo field, Iterable<Number> values) throws IOException {
-    if (field.getDocValuesGen() != -1) {
-      throw new UnsupportedOperationException("4.2 does not support dv updates");
-    }
-    addNumericField(field, values, true);
-  }
-
-  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {
-    meta.writeVInt(field.number);
-    meta.writeByte(NUMBER);
-    meta.writeLong(data.getFilePointer());
-    long minValue = Long.MAX_VALUE;
-    long maxValue = Long.MIN_VALUE;
-    long gcd = 0;
-    // TODO: more efficient?
-    HashSet<Long> uniqueValues = null;
-    if (optimizeStorage) {
-      uniqueValues = new HashSet<>();
-
-      long count = 0;
-      for (Number nv : values) {
-        // TODO: support this as MemoryDVFormat (and be smart about missing maybe)
-        final long v = nv == null ? 0 : nv.longValue();
-
-        if (gcd != 1) {
-          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {
-            // in that case v - minValue might overflow and make the GCD computation return
-            // wrong results. Since these extreme values are unlikely, we just discard
-            // GCD computation for them
-            gcd = 1;
-          } else if (count != 0) { // minValue needs to be set first
-            gcd = MathUtil.gcd(gcd, v - minValue);
-          }
-        }
-
-        minValue = Math.min(minValue, v);
-        maxValue = Math.max(maxValue, v);
-
-        if (uniqueValues != null) {
-          if (uniqueValues.add(v)) {
-            if (uniqueValues.size() > 256) {
-              uniqueValues = null;
-            }
-          }
-        }
-
-        ++count;
-      }
-      assert count == maxDoc;
-    }
-
-    if (uniqueValues != null) {
-      // small number of unique values
-      final int bitsPerValue = PackedInts.bitsRequired(uniqueValues.size()-1);
-      FormatAndBits formatAndBits = PackedInts.fastestFormatAndBits(maxDoc, bitsPerValue, acceptableOverheadRatio);
-      if (formatAndBits.bitsPerValue == 8 && minValue >= Byte.MIN_VALUE && maxValue <= Byte.MAX_VALUE) {
-        meta.writeByte(UNCOMPRESSED); // uncompressed
-        for (Number nv : values) {
-          data.writeByte(nv == null ? 0 : (byte) nv.longValue());
-        }
-      } else {
-        meta.writeByte(TABLE_COMPRESSED); // table-compressed
-        Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);
-        final HashMap<Long,Integer> encode = new HashMap<>();
-        data.writeVInt(decode.length);
-        for (int i = 0; i < decode.length; i++) {
-          data.writeLong(decode[i]);
-          encode.put(decode[i], i);
-        }
-
-        meta.writeVInt(PackedInts.VERSION_CURRENT);
-        data.writeVInt(formatAndBits.format.getId());
-        data.writeVInt(formatAndBits.bitsPerValue);
-
-        final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, formatAndBits.format, maxDoc, formatAndBits.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);
-        for(Number nv : values) {
-          writer.add(encode.get(nv == null ? 0 : nv.longValue()));
-        }
-        writer.finish();
-      }
-    } else if (gcd != 0 && gcd != 1) {
-      meta.writeByte(GCD_COMPRESSED);
-      meta.writeVInt(PackedInts.VERSION_CURRENT);
-      data.writeLong(minValue);
-      data.writeLong(gcd);
-      data.writeVInt(BLOCK_SIZE);
-
-      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);
-      for (Number nv : values) {
-        long value = nv == null ? 0 : nv.longValue();
-        writer.add((value - minValue) / gcd);
-      }
-      writer.finish();
-    } else {
-      meta.writeByte(DELTA_COMPRESSED); // delta-compressed
-
-      meta.writeVInt(PackedInts.VERSION_CURRENT);
-      data.writeVInt(BLOCK_SIZE);
-
-      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);
-      for (Number nv : values) {
-        writer.add(nv == null ? 0 : nv.longValue());
-      }
-      writer.finish();
-    }
-  }
-  
-  @Override
-  public void close() throws IOException {
-    boolean success = false;
-    try {
-      if (meta != null) {
-        meta.writeVInt(-1); // write EOF marker
-      }
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(data, meta);
-      } else {
-        IOUtils.closeWhileHandlingException(data, meta);
-      }
-    }
-  }
-
-  @Override
-  public void addBinaryField(FieldInfo field, final Iterable<BytesRef> values) throws IOException {
-    if (field.getDocValuesGen() != -1) {
-      throw new UnsupportedOperationException("4.2 does not support dv updates");
-    }
-    // write the byte[] data
-    meta.writeVInt(field.number);
-    meta.writeByte(BYTES);
-    int minLength = Integer.MAX_VALUE;
-    int maxLength = Integer.MIN_VALUE;
-    final long startFP = data.getFilePointer();
-    for(BytesRef v : values) {
-      final int length = v == null ? 0 : v.length;
-      if (length > Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH) {
-        throw new IllegalArgumentException("DocValuesField \"" + field.name + "\" is too large, must be <= " + Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);
-      }
-      minLength = Math.min(minLength, length);
-      maxLength = Math.max(maxLength, length);
-      if (v != null) {
-        data.writeBytes(v.bytes, v.offset, v.length);
-      }
-    }
-    meta.writeLong(startFP);
-    meta.writeLong(data.getFilePointer() - startFP);
-    meta.writeVInt(minLength);
-    meta.writeVInt(maxLength);
-    
-    // if minLength == maxLength, its a fixed-length byte[], we are done (the addresses are implicit)
-    // otherwise, we need to record the length fields...
-    if (minLength != maxLength) {
-      meta.writeVInt(PackedInts.VERSION_CURRENT);
-      meta.writeVInt(BLOCK_SIZE);
-
-      final MonotonicBlockPackedWriter writer = new MonotonicBlockPackedWriter(data, BLOCK_SIZE);
-      long addr = 0;
-      for (BytesRef v : values) {
-        if (v != null) {
-          addr += v.length;
-        }
-        writer.add(addr);
-      }
-      writer.finish();
-    }
-  }
-  
-  private void writeFST(FieldInfo field, Iterable<BytesRef> values) throws IOException {
-    meta.writeVInt(field.number);
-    meta.writeByte(FST);
-    meta.writeLong(data.getFilePointer());
-    PositiveIntOutputs outputs = PositiveIntOutputs.getSingleton();
-    Builder<Long> builder = new Builder<>(INPUT_TYPE.BYTE1, outputs);
-    IntsRefBuilder scratch = new IntsRefBuilder();
-    long ord = 0;
-    for (BytesRef v : values) {
-      builder.add(Util.toIntsRef(v, scratch), ord);
-      ord++;
-    }
-    FST<Long> fst = builder.finish();
-    if (fst != null) {
-      fst.save(data);
-    }
-    meta.writeVLong(ord);
-  }
-
-  @Override
-  public void addSortedField(FieldInfo field, Iterable<BytesRef> values, Iterable<Number> docToOrd) throws IOException {
-    if (field.getDocValuesGen() != -1) {
-      throw new UnsupportedOperationException("4.2 does not support dv updates");
-    }
-    // three cases for simulating the old writer:
-    // 1. no missing
-    // 2. missing (and empty string in use): remap ord=-1 -> ord=0
-    // 3. missing (and empty string not in use): remap all ords +1, insert empty string into values
-    boolean anyMissing = false;
-    for (Number n : docToOrd) {
-      if (n.longValue() == -1) {
-        anyMissing = true;
-        break;
-      }
-    }
-    
-    boolean hasEmptyString = false;
-    for (BytesRef b : values) {
-      hasEmptyString = b.length == 0;
-      break;
-    }
-    
-    if (!anyMissing) {
-      // nothing to do
-    } else if (hasEmptyString) {
-      docToOrd = MissingOrdRemapper.mapMissingToOrd0(docToOrd);
-    } else {
-      docToOrd = MissingOrdRemapper.mapAllOrds(docToOrd);
-      values = MissingOrdRemapper.insertEmptyValue(values);
-    }
-    
-    // write the ordinals as numerics
-    addNumericField(field, docToOrd, false);
-    
-    // write the values as FST
-    writeFST(field, values);
-  }
-
-  // note: this might not be the most efficient... but its fairly simple
-  @Override
-  public void addSortedSetField(FieldInfo field, Iterable<BytesRef> values, final Iterable<Number> docToOrdCount, final Iterable<Number> ords) throws IOException {
-    assert field.getDocValuesGen() == -1;
-    // write the ordinals as a binary field
-    addBinaryField(field, new Iterable<BytesRef>() {
-      @Override
-      public Iterator<BytesRef> iterator() {
-        return new SortedSetIterator(docToOrdCount.iterator(), ords.iterator());
-      }
-    });
-      
-    // write the values as FST
-    writeFST(field, values);
-  }
-  
-  // per-document vint-encoded byte[]
-  static class SortedSetIterator implements Iterator<BytesRef> {
-    byte[] buffer = new byte[10];
-    ByteArrayDataOutput out = new ByteArrayDataOutput();
-    BytesRef ref = new BytesRef();
-    
-    final Iterator<Number> counts;
-    final Iterator<Number> ords;
-    
-    SortedSetIterator(Iterator<Number> counts, Iterator<Number> ords) {
-      this.counts = counts;
-      this.ords = ords;
-    }
-    
-    @Override
-    public boolean hasNext() {
-      return counts.hasNext();
-    }
-
-    @Override
-    public BytesRef next() {
-      if (!hasNext()) {
-        throw new NoSuchElementException();
-      }
-      
-      int count = counts.next().intValue();
-      int maxSize = count*9; // worst case
-      if (maxSize > buffer.length) {
-        buffer = ArrayUtil.grow(buffer, maxSize);
-      }
-      
-      try {
-        encodeValues(count);
-      } catch (IOException bogus) {
-        throw new RuntimeException(bogus);
-      }
-      
-      ref.bytes = buffer;
-      ref.offset = 0;
-      ref.length = out.getPosition();
-
-      return ref;
-    }
-    
-    // encodes count values to buffer
-    private void encodeValues(int count) throws IOException {
-      out.reset(buffer);
-      long lastOrd = 0;
-      for (int i = 0; i < count; i++) {
-        long ord = ords.next().longValue();
-        out.writeVLong(ord - lastOrd);
-        lastOrd = ord;
-      }
-    }
-
-    @Override
-    public void remove() {
-      throw new UnsupportedOperationException();
-    }
-  }
-
-  @Override
-  public void addSortedNumericField(FieldInfo field, Iterable<Number> docToValueCount, Iterable<Number> values) throws IOException {
-    throw new UnsupportedOperationException("Lucene 4.2 does not support SORTED_NUMERIC");
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42NormsConsumer.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42NormsConsumer.java
deleted file mode 100644
index 0d003c9..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42NormsConsumer.java
+++ /dev/null
@@ -1,198 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.HashSet;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.NormsConsumer;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.MathUtil;
-import org.apache.lucene.util.packed.BlockPackedWriter;
-import org.apache.lucene.util.packed.PackedInts.FormatAndBits;
-import org.apache.lucene.util.packed.PackedInts;
-
-import static org.apache.lucene.codecs.lucene42.Lucene42DocValuesProducer.VERSION_CURRENT;
-
-/**
- * Writer for 4.2 norms format for testing
- * @deprecated for test purposes only
- */
-@Deprecated
-final class Lucene42NormsConsumer extends NormsConsumer { 
-  static final byte NUMBER = 0;
-
-  static final int BLOCK_SIZE = 4096;
-  
-  static final byte DELTA_COMPRESSED = 0;
-  static final byte TABLE_COMPRESSED = 1;
-  static final byte UNCOMPRESSED = 2;
-  static final byte GCD_COMPRESSED = 3;
-
-  IndexOutput data, meta;
-  final int maxDoc;
-  final float acceptableOverheadRatio;
-  
-  Lucene42NormsConsumer(SegmentWriteState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension, float acceptableOverheadRatio) throws IOException {
-    this.acceptableOverheadRatio = acceptableOverheadRatio;
-    maxDoc = state.segmentInfo.getDocCount();
-    boolean success = false;
-    try {
-      String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
-      data = state.directory.createOutput(dataName, state.context);
-      CodecUtil.writeHeader(data, dataCodec, VERSION_CURRENT);
-      String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
-      meta = state.directory.createOutput(metaName, state.context);
-      CodecUtil.writeHeader(meta, metaCodec, VERSION_CURRENT);
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(this);
-      }
-    }
-  }
-
-  @Override
-  public void addNormsField(FieldInfo field, Iterable<Number> values) throws IOException {
-    meta.writeVInt(field.number);
-    meta.writeByte(NUMBER);
-    meta.writeLong(data.getFilePointer());
-    long minValue = Long.MAX_VALUE;
-    long maxValue = Long.MIN_VALUE;
-    long gcd = 0;
-    // TODO: more efficient?
-    HashSet<Long> uniqueValues = null;
-    if (true) {
-      uniqueValues = new HashSet<>();
-
-      long count = 0;
-      for (Number nv : values) {
-        assert nv != null;
-        final long v = nv.longValue();
-
-        if (gcd != 1) {
-          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {
-            // in that case v - minValue might overflow and make the GCD computation return
-            // wrong results. Since these extreme values are unlikely, we just discard
-            // GCD computation for them
-            gcd = 1;
-          } else if (count != 0) { // minValue needs to be set first
-            gcd = MathUtil.gcd(gcd, v - minValue);
-          }
-        }
-
-        minValue = Math.min(minValue, v);
-        maxValue = Math.max(maxValue, v);
-
-        if (uniqueValues != null) {
-          if (uniqueValues.add(v)) {
-            if (uniqueValues.size() > 256) {
-              uniqueValues = null;
-            }
-          }
-        }
-
-        ++count;
-      }
-      assert count == maxDoc;
-    }
-
-    if (uniqueValues != null) {
-      // small number of unique values
-      final int bitsPerValue = PackedInts.bitsRequired(uniqueValues.size()-1);
-      FormatAndBits formatAndBits = PackedInts.fastestFormatAndBits(maxDoc, bitsPerValue, acceptableOverheadRatio);
-      if (formatAndBits.bitsPerValue == 8 && minValue >= Byte.MIN_VALUE && maxValue <= Byte.MAX_VALUE) {
-        meta.writeByte(UNCOMPRESSED); // uncompressed
-        for (Number nv : values) {
-          data.writeByte(nv == null ? 0 : (byte) nv.longValue());
-        }
-      } else {
-        meta.writeByte(TABLE_COMPRESSED); // table-compressed
-        Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);
-        final HashMap<Long,Integer> encode = new HashMap<>();
-        data.writeVInt(decode.length);
-        for (int i = 0; i < decode.length; i++) {
-          data.writeLong(decode[i]);
-          encode.put(decode[i], i);
-        }
-
-        meta.writeVInt(PackedInts.VERSION_CURRENT);
-        data.writeVInt(formatAndBits.format.getId());
-        data.writeVInt(formatAndBits.bitsPerValue);
-
-        final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, formatAndBits.format, maxDoc, formatAndBits.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);
-        for(Number nv : values) {
-          writer.add(encode.get(nv == null ? 0 : nv.longValue()));
-        }
-        writer.finish();
-      }
-    } else if (gcd != 0 && gcd != 1) {
-      meta.writeByte(GCD_COMPRESSED);
-      meta.writeVInt(PackedInts.VERSION_CURRENT);
-      data.writeLong(minValue);
-      data.writeLong(gcd);
-      data.writeVInt(BLOCK_SIZE);
-
-      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);
-      for (Number nv : values) {
-        long value = nv == null ? 0 : nv.longValue();
-        writer.add((value - minValue) / gcd);
-      }
-      writer.finish();
-    } else {
-      meta.writeByte(DELTA_COMPRESSED); // delta-compressed
-
-      meta.writeVInt(PackedInts.VERSION_CURRENT);
-      data.writeVInt(BLOCK_SIZE);
-
-      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);
-      for (Number nv : values) {
-        writer.add(nv == null ? 0 : nv.longValue());
-      }
-      writer.finish();
-    }
-  }
-  
-  @Override
-  public void close() throws IOException {
-    boolean success = false;
-    try {
-      if (meta != null) {
-        meta.writeVInt(-1); // write EOF marker
-        CodecUtil.writeFooter(meta); // write checksum
-      }
-      if (data != null) {
-        CodecUtil.writeFooter(data); // write checksum
-      }
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(data, meta);
-      } else {
-        IOUtils.closeWhileHandlingException(data, meta);
-      }
-      meta = data = null;
-    }
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWCodec.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWCodec.java
deleted file mode 100644
index 9c46571..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWCodec.java
+++ /dev/null
@@ -1,83 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.SegmentInfoFormat;
-import org.apache.lucene.codecs.StoredFieldsFormat;
-import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40RWSegmentInfoFormat;
-import org.apache.lucene.codecs.lucene41.Lucene41RWPostingsFormat;
-import org.apache.lucene.codecs.lucene41.Lucene41RWStoredFieldsFormat;
-
-/**
- * Read-Write version of 4.2 codec for testing
- * @deprecated for test purposes only
- */
-@Deprecated
-public final class Lucene42RWCodec extends Lucene42Codec {
-
-  private static final DocValuesFormat dv = new Lucene42RWDocValuesFormat();
-  private static final NormsFormat norms = new Lucene42RWNormsFormat();
-  private static final StoredFieldsFormat storedFields = new Lucene41RWStoredFieldsFormat();
-  private static final FieldInfosFormat fieldInfosFormat = new Lucene42RWFieldInfosFormat();
-  
-  private final PostingsFormat postings = new Lucene41RWPostingsFormat();
-  
-  @Override
-  public PostingsFormat getPostingsFormatForField(String field) {
-    return postings;
-  }
-
-  @Override
-  public DocValuesFormat getDocValuesFormatForField(String field) {
-    return dv;
-  }
-
-  @Override
-  public NormsFormat normsFormat() {
-    return norms;
-  }
-  
-  @Override
-  public FieldInfosFormat fieldInfosFormat() {
-    return fieldInfosFormat;
-  }  
-  
-  private static final SegmentInfoFormat segmentInfos = new Lucene40RWSegmentInfoFormat();
-
-  @Override
-  public SegmentInfoFormat segmentInfoFormat() {
-    return segmentInfos;
-  }
-
-  @Override
-  public StoredFieldsFormat storedFieldsFormat() {
-    return storedFields;
-  }
-  
-  private final TermVectorsFormat vectorsFormat = new Lucene42RWTermVectorsFormat();
-
-  @Override
-  public TermVectorsFormat termVectorsFormat() {
-    return vectorsFormat;
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWDocValuesFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWDocValuesFormat.java
deleted file mode 100644
index 6ecbd79..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWDocValuesFormat.java
+++ /dev/null
@@ -1,37 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.index.SegmentWriteState;
-
-/**
- * Read-Write version of 4.2 docvalues format for testing
- * @deprecated for test purposes only
- */
-@Deprecated
-public final class Lucene42RWDocValuesFormat extends Lucene42DocValuesFormat {
-  
-  @Override
-  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    // note: we choose DEFAULT here (its reasonably fast, and for small bpv has tiny waste)
-    return new Lucene42DocValuesConsumer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION, acceptableOverheadRatio);
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWFieldInfosFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWFieldInfosFormat.java
deleted file mode 100644
index 3b878e3..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWFieldInfosFormat.java
+++ /dev/null
@@ -1,110 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.index.FieldInfo.DocValuesType;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Writer for 4.2 fieldinfos format for testing
- * @deprecated for test purposes only
- */
-@Deprecated
-public final class Lucene42RWFieldInfosFormat extends Lucene42FieldInfosFormat {
-  
-  /** Sole constructor. */
-  public Lucene42RWFieldInfosFormat() {
-  }
-  
-  @Override
-  public void write(Directory directory, SegmentInfo segmentInfo, String segmentSuffix, FieldInfos infos, IOContext context) throws IOException {
-    if (!segmentSuffix.isEmpty()) {
-      throw new UnsupportedOperationException("4.2 does not support fieldinfo updates");
-    }
-    final String fileName = IndexFileNames.segmentFileName(segmentInfo.name, "", Lucene42FieldInfosFormat.EXTENSION);
-    IndexOutput output = directory.createOutput(fileName, context);
-    boolean success = false;
-    try {
-      CodecUtil.writeHeader(output, Lucene42FieldInfosFormat.CODEC_NAME, Lucene42FieldInfosFormat.FORMAT_CURRENT);
-      output.writeVInt(infos.size());
-      for (FieldInfo fi : infos) {
-        IndexOptions indexOptions = fi.getIndexOptions();
-        byte bits = 0x0;
-        if (fi.hasVectors()) bits |= Lucene42FieldInfosFormat.STORE_TERMVECTOR;
-        if (fi.omitsNorms()) bits |= Lucene42FieldInfosFormat.OMIT_NORMS;
-        if (fi.hasPayloads()) bits |= Lucene42FieldInfosFormat.STORE_PAYLOADS;
-        if (fi.isIndexed()) {
-          bits |= Lucene42FieldInfosFormat.IS_INDEXED;
-          assert indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 || !fi.hasPayloads();
-          if (indexOptions == IndexOptions.DOCS_ONLY) {
-            bits |= Lucene42FieldInfosFormat.OMIT_TERM_FREQ_AND_POSITIONS;
-          } else if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) {
-            bits |= Lucene42FieldInfosFormat.STORE_OFFSETS_IN_POSTINGS;
-          } else if (indexOptions == IndexOptions.DOCS_AND_FREQS) {
-            bits |= Lucene42FieldInfosFormat.OMIT_POSITIONS;
-          }
-        }
-        output.writeString(fi.name);
-        output.writeVInt(fi.number);
-        output.writeByte(bits);
-
-        // pack the DV types in one byte
-        final byte dv = docValuesByte(fi.getDocValuesType());
-        final byte nrm = docValuesByte(fi.hasNorms() ? DocValuesType.NUMERIC : null);
-        assert (dv & (~0xF)) == 0 && (nrm & (~0x0F)) == 0;
-        byte val = (byte) (0xff & ((nrm << 4) | dv));
-        output.writeByte(val);
-        output.writeStringStringMap(fi.attributes());
-      }
-      success = true;
-    } finally {
-      if (success) {
-        output.close();
-      } else {
-        IOUtils.closeWhileHandlingException(output);
-      }
-    }
-  }
-  
-  private static byte docValuesByte(DocValuesType type) {
-    if (type == null) {
-      return 0;
-    } else if (type == DocValuesType.NUMERIC) {
-      return 1;
-    } else if (type == DocValuesType.BINARY) {
-      return 2;
-    } else if (type == DocValuesType.SORTED) {
-      return 3;
-    } else if (type == DocValuesType.SORTED_SET) {
-      return 4;
-    } else {
-      throw new AssertionError();
-    }
-  }  
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWNormsFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWNormsFormat.java
deleted file mode 100644
index ef8165a..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWNormsFormat.java
+++ /dev/null
@@ -1,36 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.NormsConsumer;
-import org.apache.lucene.index.SegmentWriteState;
-
-/**
- * Read-write version of 4.2 norms format for testing
- * @deprecated for test purposes only
- */
-@Deprecated
-public final class Lucene42RWNormsFormat extends Lucene42NormsFormat {
-
-  @Override
-  public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
-    return new Lucene42NormsConsumer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION, acceptableOverheadRatio);
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWTermVectorsFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWTermVectorsFormat.java
deleted file mode 100644
index fdf461f..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWTermVectorsFormat.java
+++ /dev/null
@@ -1,38 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.TermVectorsWriter;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-/**
- * Read-Write version of 4.2 term vectors format for testing
- * @deprecated for test purposes only
- */
-@Deprecated
-public final class Lucene42RWTermVectorsFormat extends Lucene42TermVectorsFormat {
-
-  @Override
-  public TermVectorsWriter vectorsWriter(Directory directory, SegmentInfo segmentInfo, IOContext context) throws IOException {
-    return new Lucene42TermVectorsWriter(directory, segmentInfo, SEGMENT_SUFFIX, context, FORMAT_NAME, COMPRESSION_MODE, CHUNK_SIZE);
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42TermVectorsWriter.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42TermVectorsWriter.java
deleted file mode 100644
index e8c813b..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42TermVectorsWriter.java
+++ /dev/null
@@ -1,714 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import static org.apache.lucene.codecs.lucene42.Lucene42TermVectorsReader.BLOCK_SIZE;
-import static org.apache.lucene.codecs.lucene42.Lucene42TermVectorsReader.CODEC_SFX_DAT;
-import static org.apache.lucene.codecs.lucene42.Lucene42TermVectorsReader.CODEC_SFX_IDX;
-import static org.apache.lucene.codecs.lucene42.Lucene42TermVectorsReader.FLAGS_BITS;
-import static org.apache.lucene.codecs.lucene42.Lucene42TermVectorsReader.OFFSETS;
-import static org.apache.lucene.codecs.lucene42.Lucene42TermVectorsReader.PAYLOADS;
-import static org.apache.lucene.codecs.lucene42.Lucene42TermVectorsReader.POSITIONS;
-import static org.apache.lucene.codecs.lucene42.Lucene42TermVectorsReader.VECTORS_EXTENSION;
-import static org.apache.lucene.codecs.lucene42.Lucene42TermVectorsReader.VECTORS_INDEX_EXTENSION;
-import static org.apache.lucene.codecs.lucene42.Lucene42TermVectorsReader.VERSION_CURRENT;
-
-import java.io.IOException;
-import java.util.ArrayDeque;
-import java.util.Arrays;
-import java.util.Deque;
-import java.util.Iterator;
-import java.util.SortedSet;
-import java.util.TreeSet;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.TermVectorsWriter;
-import org.apache.lucene.codecs.compressing.CompressionMode;
-import org.apache.lucene.codecs.compressing.Compressor;
-import org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsIndexWriter;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.DataInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.GrowableByteArrayDataOutput;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.StringHelper;
-import org.apache.lucene.util.packed.BlockPackedWriter;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * Writer for 4.2 term vectors format for testing
- * @deprecated for test purposes only
- */
-@Deprecated
-final class Lucene42TermVectorsWriter extends TermVectorsWriter {
-
-  // hard limit on the maximum number of documents per chunk
-  static final int MAX_DOCUMENTS_PER_CHUNK = 128;
-
-  private final Directory directory;
-  private final String segment;
-  private final String segmentSuffix;
-  private Lucene41StoredFieldsIndexWriter indexWriter;
-  private IndexOutput vectorsStream;
-
-  private final Compressor compressor;
-  private final int chunkSize;
-
-  /** a pending doc */
-  private class DocData {
-    final int numFields;
-    final Deque<FieldData> fields;
-    final int posStart, offStart, payStart;
-    DocData(int numFields, int posStart, int offStart, int payStart) {
-      this.numFields = numFields;
-      this.fields = new ArrayDeque<>(numFields);
-      this.posStart = posStart;
-      this.offStart = offStart;
-      this.payStart = payStart;
-    }
-    FieldData addField(int fieldNum, int numTerms, boolean positions, boolean offsets, boolean payloads) {
-      final FieldData field;
-      if (fields.isEmpty()) {
-        field = new FieldData(fieldNum, numTerms, positions, offsets, payloads, posStart, offStart, payStart);
-      } else {
-        final FieldData last = fields.getLast();
-        final int posStart = last.posStart + (last.hasPositions ? last.totalPositions : 0);
-        final int offStart = last.offStart + (last.hasOffsets ? last.totalPositions : 0);
-        final int payStart = last.payStart + (last.hasPayloads ? last.totalPositions : 0);
-        field = new FieldData(fieldNum, numTerms, positions, offsets, payloads, posStart, offStart, payStart);
-      }
-      fields.add(field);
-      return field;
-    }
-  }
-
-  private DocData addDocData(int numVectorFields) {
-    FieldData last = null;
-    for (Iterator<DocData> it = pendingDocs.descendingIterator(); it.hasNext(); ) {
-      final DocData doc = it.next();
-      if (!doc.fields.isEmpty()) {
-        last = doc.fields.getLast();
-        break;
-      }
-    }
-    final DocData doc;
-    if (last == null) {
-      doc = new DocData(numVectorFields, 0, 0, 0);
-    } else {
-      final int posStart = last.posStart + (last.hasPositions ? last.totalPositions : 0);
-      final int offStart = last.offStart + (last.hasOffsets ? last.totalPositions : 0);
-      final int payStart = last.payStart + (last.hasPayloads ? last.totalPositions : 0);
-      doc = new DocData(numVectorFields, posStart, offStart, payStart);
-    }
-    pendingDocs.add(doc);
-    return doc;
-  }
-
-  /** a pending field */
-  private class FieldData {
-    final boolean hasPositions, hasOffsets, hasPayloads;
-    final int fieldNum, flags, numTerms;
-    final int[] freqs, prefixLengths, suffixLengths;
-    final int posStart, offStart, payStart;
-    int totalPositions;
-    int ord;
-    FieldData(int fieldNum, int numTerms, boolean positions, boolean offsets, boolean payloads,
-        int posStart, int offStart, int payStart) {
-      this.fieldNum = fieldNum;
-      this.numTerms = numTerms;
-      this.hasPositions = positions;
-      this.hasOffsets = offsets;
-      this.hasPayloads = payloads;
-      this.flags = (positions ? POSITIONS : 0) | (offsets ? OFFSETS : 0) | (payloads ? PAYLOADS : 0);
-      this.freqs = new int[numTerms];
-      this.prefixLengths = new int[numTerms];
-      this.suffixLengths = new int[numTerms];
-      this.posStart = posStart;
-      this.offStart = offStart;
-      this.payStart = payStart;
-      totalPositions = 0;
-      ord = 0;
-    }
-    void addTerm(int freq, int prefixLength, int suffixLength) {
-      freqs[ord] = freq;
-      prefixLengths[ord] = prefixLength;
-      suffixLengths[ord] = suffixLength;
-      ++ord;
-    }
-    void addPosition(int position, int startOffset, int length, int payloadLength) {
-      if (hasPositions) {
-        if (posStart + totalPositions == positionsBuf.length) {
-          positionsBuf = ArrayUtil.grow(positionsBuf);
-        }
-        positionsBuf[posStart + totalPositions] = position;
-      }
-      if (hasOffsets) {
-        if (offStart + totalPositions == startOffsetsBuf.length) {
-          final int newLength = ArrayUtil.oversize(offStart + totalPositions, 4);
-          startOffsetsBuf = Arrays.copyOf(startOffsetsBuf, newLength);
-          lengthsBuf = Arrays.copyOf(lengthsBuf, newLength);
-        }
-        startOffsetsBuf[offStart + totalPositions] = startOffset;
-        lengthsBuf[offStart + totalPositions] = length;
-      }
-      if (hasPayloads) {
-        if (payStart + totalPositions == payloadLengthsBuf.length) {
-          payloadLengthsBuf = ArrayUtil.grow(payloadLengthsBuf);
-        }
-        payloadLengthsBuf[payStart + totalPositions] = payloadLength;
-      }
-      ++totalPositions;
-    }
-  }
-
-  private int numDocs; // total number of docs seen
-  private final Deque<DocData> pendingDocs; // pending docs
-  private DocData curDoc; // current document
-  private FieldData curField; // current field
-  private final BytesRef lastTerm;
-  private int[] positionsBuf, startOffsetsBuf, lengthsBuf, payloadLengthsBuf;
-  private final GrowableByteArrayDataOutput termSuffixes; // buffered term suffixes
-  private final GrowableByteArrayDataOutput payloadBytes; // buffered term payloads
-  private final BlockPackedWriter writer;
-
-  /** Sole constructor. */
-  public Lucene42TermVectorsWriter(Directory directory, SegmentInfo si, String segmentSuffix, IOContext context,
-      String formatName, CompressionMode compressionMode, int chunkSize) throws IOException {
-    assert directory != null;
-    this.directory = directory;
-    this.segment = si.name;
-    this.segmentSuffix = segmentSuffix;
-    this.compressor = compressionMode.newCompressor();
-    this.chunkSize = chunkSize;
-
-    numDocs = 0;
-    pendingDocs = new ArrayDeque<>();
-    termSuffixes = new GrowableByteArrayDataOutput(ArrayUtil.oversize(chunkSize, 1));
-    payloadBytes = new GrowableByteArrayDataOutput(ArrayUtil.oversize(1, 1));
-    lastTerm = new BytesRef(ArrayUtil.oversize(30, 1));
-
-    boolean success = false;
-    IndexOutput indexStream = directory.createOutput(IndexFileNames.segmentFileName(segment, segmentSuffix, VECTORS_INDEX_EXTENSION), 
-                                                                     context);
-    try {
-      vectorsStream = directory.createOutput(IndexFileNames.segmentFileName(segment, segmentSuffix, VECTORS_EXTENSION),
-                                                     context);
-
-      final String codecNameIdx = formatName + CODEC_SFX_IDX;
-      final String codecNameDat = formatName + CODEC_SFX_DAT;
-      CodecUtil.writeHeader(indexStream, codecNameIdx, VERSION_CURRENT);
-      CodecUtil.writeHeader(vectorsStream, codecNameDat, VERSION_CURRENT);
-      assert CodecUtil.headerLength(codecNameDat) == vectorsStream.getFilePointer();
-      assert CodecUtil.headerLength(codecNameIdx) == indexStream.getFilePointer();
-
-      indexWriter = new Lucene41StoredFieldsIndexWriter(indexStream);
-      indexStream = null;
-
-      vectorsStream.writeVInt(PackedInts.VERSION_CURRENT);
-      vectorsStream.writeVInt(chunkSize);
-      writer = new BlockPackedWriter(vectorsStream, BLOCK_SIZE);
-
-      positionsBuf = new int[1024];
-      startOffsetsBuf = new int[1024];
-      lengthsBuf = new int[1024];
-      payloadLengthsBuf = new int[1024];
-
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(indexStream);
-        abort();
-      }
-    }
-  }
-
-  @Override
-  public void close() throws IOException {
-    try {
-      IOUtils.close(vectorsStream, indexWriter);
-    } finally {
-      vectorsStream = null;
-      indexWriter = null;
-    }
-  }
-
-  @Override
-  public void abort() {
-    IOUtils.closeWhileHandlingException(this);
-    IOUtils.deleteFilesIgnoringExceptions(directory,
-        IndexFileNames.segmentFileName(segment, segmentSuffix, VECTORS_EXTENSION),
-        IndexFileNames.segmentFileName(segment, segmentSuffix, VECTORS_INDEX_EXTENSION));
-  }
-
-  @Override
-  public void startDocument(int numVectorFields) throws IOException {
-    curDoc = addDocData(numVectorFields);
-  }
-
-  @Override
-  public void finishDocument() throws IOException {
-    // append the payload bytes of the doc after its terms
-    termSuffixes.writeBytes(payloadBytes.bytes, payloadBytes.length);
-    payloadBytes.length = 0;
-    ++numDocs;
-    if (triggerFlush()) {
-      flush();
-    }
-    curDoc = null;
-  }
-
-  @Override
-  public void startField(FieldInfo info, int numTerms, boolean positions,
-      boolean offsets, boolean payloads) throws IOException {
-    curField = curDoc.addField(info.number, numTerms, positions, offsets, payloads);
-    lastTerm.length = 0;
-  }
-
-  @Override
-  public void finishField() throws IOException {
-    curField = null;
-  }
-
-  @Override
-  public void startTerm(BytesRef term, int freq) throws IOException {
-    assert freq >= 1;
-    final int prefix = StringHelper.bytesDifference(lastTerm, term);
-    curField.addTerm(freq, prefix, term.length - prefix);
-    termSuffixes.writeBytes(term.bytes, term.offset + prefix, term.length - prefix);
-    // copy last term
-    if (lastTerm.bytes.length < term.length) {
-      lastTerm.bytes = new byte[ArrayUtil.oversize(term.length, 1)];
-    }
-    lastTerm.offset = 0;
-    lastTerm.length = term.length;
-    System.arraycopy(term.bytes, term.offset, lastTerm.bytes, 0, term.length);
-  }
-
-  @Override
-  public void addPosition(int position, int startOffset, int endOffset,
-      BytesRef payload) throws IOException {
-    assert curField.flags != 0;
-    curField.addPosition(position, startOffset, endOffset - startOffset, payload == null ? 0 : payload.length);
-    if (curField.hasPayloads && payload != null) {
-      payloadBytes.writeBytes(payload.bytes, payload.offset, payload.length);
-    }
-  }
-
-  private boolean triggerFlush() {
-    return termSuffixes.length >= chunkSize
-        || pendingDocs.size() >= MAX_DOCUMENTS_PER_CHUNK;
-  }
-
-  private void flush() throws IOException {
-    final int chunkDocs = pendingDocs.size();
-    assert chunkDocs > 0 : chunkDocs;
-
-    // write the index file
-    indexWriter.writeIndex(chunkDocs, vectorsStream.getFilePointer());
-
-    final int docBase = numDocs - chunkDocs;
-    vectorsStream.writeVInt(docBase);
-    vectorsStream.writeVInt(chunkDocs);
-
-    // total number of fields of the chunk
-    final int totalFields = flushNumFields(chunkDocs);
-
-    if (totalFields > 0) {
-      // unique field numbers (sorted)
-      final int[] fieldNums = flushFieldNums();
-      // offsets in the array of unique field numbers
-      flushFields(totalFields, fieldNums);
-      // flags (does the field have positions, offsets, payloads?)
-      flushFlags(totalFields, fieldNums);
-      // number of terms of each field
-      flushNumTerms(totalFields);
-      // prefix and suffix lengths for each field
-      flushTermLengths();
-      // term freqs - 1 (because termFreq is always >=1) for each term
-      flushTermFreqs();
-      // positions for all terms, when enabled
-      flushPositions();
-      // offsets for all terms, when enabled
-      flushOffsets(fieldNums);
-      // payload lengths for all terms, when enabled
-      flushPayloadLengths();
-
-      // compress terms and payloads and write them to the output
-      compressor.compress(termSuffixes.bytes, 0, termSuffixes.length, vectorsStream);
-    }
-
-    // reset
-    pendingDocs.clear();
-    curDoc = null;
-    curField = null;
-    termSuffixes.length = 0;
-  }
-
-  private int flushNumFields(int chunkDocs) throws IOException {
-    if (chunkDocs == 1) {
-      final int numFields = pendingDocs.getFirst().numFields;
-      vectorsStream.writeVInt(numFields);
-      return numFields;
-    } else {
-      writer.reset(vectorsStream);
-      int totalFields = 0;
-      for (DocData dd : pendingDocs) {
-        writer.add(dd.numFields);
-        totalFields += dd.numFields;
-      }
-      writer.finish();
-      return totalFields;
-    }
-  }
-
-  /** Returns a sorted array containing unique field numbers */
-  private int[] flushFieldNums() throws IOException {
-    SortedSet<Integer> fieldNums = new TreeSet<>();
-    for (DocData dd : pendingDocs) {
-      for (FieldData fd : dd.fields) {
-        fieldNums.add(fd.fieldNum);
-      }
-    }
-
-    final int numDistinctFields = fieldNums.size();
-    assert numDistinctFields > 0;
-    final int bitsRequired = PackedInts.bitsRequired(fieldNums.last());
-    final int token = (Math.min(numDistinctFields - 1, 0x07) << 5) | bitsRequired;
-    vectorsStream.writeByte((byte) token);
-    if (numDistinctFields - 1 >= 0x07) {
-      vectorsStream.writeVInt(numDistinctFields - 1 - 0x07);
-    }
-    final PackedInts.Writer writer = PackedInts.getWriterNoHeader(vectorsStream, PackedInts.Format.PACKED, fieldNums.size(), bitsRequired, 1);
-    for (Integer fieldNum : fieldNums) {
-      writer.add(fieldNum);
-    }
-    writer.finish();
-
-    int[] fns = new int[fieldNums.size()];
-    int i = 0;
-    for (Integer key : fieldNums) {
-      fns[i++] = key;
-    }
-    return fns;
-  }
-
-  private void flushFields(int totalFields, int[] fieldNums) throws IOException {
-    final PackedInts.Writer writer = PackedInts.getWriterNoHeader(vectorsStream, PackedInts.Format.PACKED, totalFields, PackedInts.bitsRequired(fieldNums.length - 1), 1);
-    for (DocData dd : pendingDocs) {
-      for (FieldData fd : dd.fields) {
-        final int fieldNumIndex = Arrays.binarySearch(fieldNums, fd.fieldNum);
-        assert fieldNumIndex >= 0;
-        writer.add(fieldNumIndex);
-      }
-    }
-    writer.finish();
-  }
-
-  private void flushFlags(int totalFields, int[] fieldNums) throws IOException {
-    // check if fields always have the same flags
-    boolean nonChangingFlags = true;
-    int[] fieldFlags = new int[fieldNums.length];
-    Arrays.fill(fieldFlags, -1);
-    outer:
-    for (DocData dd : pendingDocs) {
-      for (FieldData fd : dd.fields) {
-        final int fieldNumOff = Arrays.binarySearch(fieldNums, fd.fieldNum);
-        assert fieldNumOff >= 0;
-        if (fieldFlags[fieldNumOff] == -1) {
-          fieldFlags[fieldNumOff] = fd.flags;
-        } else if (fieldFlags[fieldNumOff] != fd.flags) {
-          nonChangingFlags = false;
-          break outer;
-        }
-      }
-    }
-
-    if (nonChangingFlags) {
-      // write one flag per field num
-      vectorsStream.writeVInt(0);
-      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(vectorsStream, PackedInts.Format.PACKED, fieldFlags.length, FLAGS_BITS, 1);
-      for (int flags : fieldFlags) {
-        assert flags >= 0;
-        writer.add(flags);
-      }
-      assert writer.ord() == fieldFlags.length - 1;
-      writer.finish();
-    } else {
-      // write one flag for every field instance
-      vectorsStream.writeVInt(1);
-      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(vectorsStream, PackedInts.Format.PACKED, totalFields, FLAGS_BITS, 1);
-      for (DocData dd : pendingDocs) {
-        for (FieldData fd : dd.fields) {
-          writer.add(fd.flags);
-        }
-      }
-      assert writer.ord() == totalFields - 1;
-      writer.finish();
-    }
-  }
-
-  private void flushNumTerms(int totalFields) throws IOException {
-    int maxNumTerms = 0;
-    for (DocData dd : pendingDocs) {
-      for (FieldData fd : dd.fields) {
-        maxNumTerms |= fd.numTerms;
-      }
-    }
-    final int bitsRequired = PackedInts.bitsRequired(maxNumTerms);
-    vectorsStream.writeVInt(bitsRequired);
-    final PackedInts.Writer writer = PackedInts.getWriterNoHeader(
-        vectorsStream, PackedInts.Format.PACKED, totalFields, bitsRequired, 1);
-    for (DocData dd : pendingDocs) {
-      for (FieldData fd : dd.fields) {
-        writer.add(fd.numTerms);
-      }
-    }
-    assert writer.ord() == totalFields - 1;
-    writer.finish();
-  }
-
-  private void flushTermLengths() throws IOException {
-    writer.reset(vectorsStream);
-    for (DocData dd : pendingDocs) {
-      for (FieldData fd : dd.fields) {
-        for (int i = 0; i < fd.numTerms; ++i) {
-          writer.add(fd.prefixLengths[i]);
-        }
-      }
-    }
-    writer.finish();
-    writer.reset(vectorsStream);
-    for (DocData dd : pendingDocs) {
-      for (FieldData fd : dd.fields) {
-        for (int i = 0; i < fd.numTerms; ++i) {
-          writer.add(fd.suffixLengths[i]);
-        }
-      }
-    }
-    writer.finish();
-  }
-
-  private void flushTermFreqs() throws IOException {
-    writer.reset(vectorsStream);
-    for (DocData dd : pendingDocs) {
-      for (FieldData fd : dd.fields) {
-        for (int i = 0; i < fd.numTerms; ++i) {
-          writer.add(fd.freqs[i] - 1);
-        }
-      }
-    }
-    writer.finish();
-  }
-
-  private void flushPositions() throws IOException {
-    writer.reset(vectorsStream);
-    for (DocData dd : pendingDocs) {
-      for (FieldData fd : dd.fields) {
-        if (fd.hasPositions) {
-          int pos = 0;
-          for (int i = 0; i < fd.numTerms; ++i) {
-            int previousPosition = 0;
-            for (int j = 0; j < fd.freqs[i]; ++j) {
-              final int position = positionsBuf[fd .posStart + pos++];
-              writer.add(position - previousPosition);
-              previousPosition = position;
-            }
-          }
-          assert pos == fd.totalPositions;
-        }
-      }
-    }
-    writer.finish();
-  }
-
-  private void flushOffsets(int[] fieldNums) throws IOException {
-    boolean hasOffsets = false;
-    long[] sumPos = new long[fieldNums.length];
-    long[] sumOffsets = new long[fieldNums.length];
-    for (DocData dd : pendingDocs) {
-      for (FieldData fd : dd.fields) {
-        hasOffsets |= fd.hasOffsets;
-        if (fd.hasOffsets && fd.hasPositions) {
-          final int fieldNumOff = Arrays.binarySearch(fieldNums, fd.fieldNum);
-          int pos = 0;
-          for (int i = 0; i < fd.numTerms; ++i) {
-            int previousPos = 0;
-            int previousOff = 0;
-            for (int j = 0; j < fd.freqs[i]; ++j) {
-              final int position = positionsBuf[fd.posStart + pos];
-              final int startOffset = startOffsetsBuf[fd.offStart + pos];
-              sumPos[fieldNumOff] += position - previousPos;
-              sumOffsets[fieldNumOff] += startOffset - previousOff;
-              previousPos = position;
-              previousOff = startOffset;
-              ++pos;
-            }
-          }
-          assert pos == fd.totalPositions;
-        }
-      }
-    }
-
-    if (!hasOffsets) {
-      // nothing to do
-      return;
-    }
-
-    final float[] charsPerTerm = new float[fieldNums.length];
-    for (int i = 0; i < fieldNums.length; ++i) {
-      charsPerTerm[i] = (sumPos[i] <= 0 || sumOffsets[i] <= 0) ? 0 : (float) ((double) sumOffsets[i] / sumPos[i]);
-    }
-
-    // start offsets
-    for (int i = 0; i < fieldNums.length; ++i) {
-      vectorsStream.writeInt(Float.floatToRawIntBits(charsPerTerm[i]));
-    }
-
-    writer.reset(vectorsStream);
-    for (DocData dd : pendingDocs) {
-      for (FieldData fd : dd.fields) {
-        if ((fd.flags & OFFSETS) != 0) {
-          final int fieldNumOff = Arrays.binarySearch(fieldNums, fd.fieldNum);
-          final float cpt = charsPerTerm[fieldNumOff];
-          int pos = 0;
-          for (int i = 0; i < fd.numTerms; ++i) {
-            int previousPos = 0;
-            int previousOff = 0;
-            for (int j = 0; j < fd.freqs[i]; ++j) {
-              final int position = fd.hasPositions ? positionsBuf[fd.posStart + pos] : 0;
-              final int startOffset = startOffsetsBuf[fd.offStart + pos];
-              writer.add(startOffset - previousOff - (int) (cpt * (position - previousPos)));
-              previousPos = position;
-              previousOff = startOffset;
-              ++pos;
-            }
-          }
-        }
-      }
-    }
-    writer.finish();
-
-    // lengths
-    writer.reset(vectorsStream);
-    for (DocData dd : pendingDocs) {
-      for (FieldData fd : dd.fields) {
-        if ((fd.flags & OFFSETS) != 0) {
-          int pos = 0;
-          for (int i = 0; i < fd.numTerms; ++i) {
-            for (int j = 0; j < fd.freqs[i]; ++j) {
-              writer.add(lengthsBuf[fd.offStart + pos++] - fd.prefixLengths[i] - fd.suffixLengths[i]);
-            }
-          }
-          assert pos == fd.totalPositions;
-        }
-      }
-    }
-    writer.finish();
-  }
-
-  private void flushPayloadLengths() throws IOException {
-    writer.reset(vectorsStream);
-    for (DocData dd : pendingDocs) {
-      for (FieldData fd : dd.fields) {
-        if (fd.hasPayloads) {
-          for (int i = 0; i < fd.totalPositions; ++i) {
-            writer.add(payloadLengthsBuf[fd.payStart + i]);
-          }
-        }
-      }
-    }
-    writer.finish();
-  }
-
-  @Override
-  public void finish(FieldInfos fis, int numDocs) throws IOException {
-    if (!pendingDocs.isEmpty()) {
-      flush();
-    }
-    if (numDocs != this.numDocs) {
-      throw new RuntimeException("Wrote " + this.numDocs + " docs, finish called with numDocs=" + numDocs);
-    }
-    indexWriter.finish(numDocs, vectorsStream.getFilePointer());
-    CodecUtil.writeFooter(vectorsStream);
-  }
-
-  @Override
-  public void addProx(int numProx, DataInput positions, DataInput offsets)
-      throws IOException {
-    assert (curField.hasPositions) == (positions != null);
-    assert (curField.hasOffsets) == (offsets != null);
-
-    if (curField.hasPositions) {
-      final int posStart = curField.posStart + curField.totalPositions;
-      if (posStart + numProx > positionsBuf.length) {
-        positionsBuf = ArrayUtil.grow(positionsBuf, posStart + numProx);
-      }
-      int position = 0;
-      if (curField.hasPayloads) {
-        final int payStart = curField.payStart + curField.totalPositions;
-        if (payStart + numProx > payloadLengthsBuf.length) {
-          payloadLengthsBuf = ArrayUtil.grow(payloadLengthsBuf, payStart + numProx);
-        }
-        for (int i = 0; i < numProx; ++i) {
-          final int code = positions.readVInt();
-          if ((code & 1) != 0) {
-            // This position has a payload
-            final int payloadLength = positions.readVInt();
-            payloadLengthsBuf[payStart + i] = payloadLength;
-            payloadBytes.copyBytes(positions, payloadLength);
-          } else {
-            payloadLengthsBuf[payStart + i] = 0;
-          }
-          position += code >>> 1;
-          positionsBuf[posStart + i] = position;
-        }
-      } else {
-        for (int i = 0; i < numProx; ++i) {
-          position += (positions.readVInt() >>> 1);
-          positionsBuf[posStart + i] = position;
-        }
-      }
-    }
-
-    if (curField.hasOffsets) {
-      final int offStart = curField.offStart + curField.totalPositions;
-      if (offStart + numProx > startOffsetsBuf.length) {
-        final int newLength = ArrayUtil.oversize(offStart + numProx, 4);
-        startOffsetsBuf = Arrays.copyOf(startOffsetsBuf, newLength);
-        lengthsBuf = Arrays.copyOf(lengthsBuf, newLength);
-      }
-      int lastOffset = 0, startOffset, endOffset;
-      for (int i = 0; i < numProx; ++i) {
-        startOffset = lastOffset + offsets.readVInt();
-        endOffset = startOffset + offsets.readVInt();
-        lastOffset = endOffset;
-        startOffsetsBuf[offStart + i] = startOffset;
-        lengthsBuf[offStart + i] = endOffset - startOffset;
-      }
-    }
-
-    curField.totalPositions += numProx;
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/TestLucene42DocValuesFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/TestLucene42DocValuesFormat.java
deleted file mode 100644
index 2ebdfc1..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/TestLucene42DocValuesFormat.java
+++ /dev/null
@@ -1,50 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.BaseCompressingDocValuesFormatTestCase;
-import org.junit.BeforeClass;
-
-/**
- * Tests Lucene42DocValuesFormat
- */
-public class TestLucene42DocValuesFormat extends BaseCompressingDocValuesFormatTestCase {
-  private final Codec codec = new Lucene42RWCodec();
-  
-  @Override
-  protected Codec getCodec() {
-    return codec;
-  }
-
-  @Override
-  protected boolean codecAcceptsHugeBinaryValues(String field) {
-    return false;
-  }
-  
-  // this codec doesnt support missing (its the same as empty string)
-  @Override
-  protected boolean codecSupportsDocsWithField() {
-    return false;
-  }
-  
-  @Override
-  protected boolean codecSupportsSortedNumeric() {
-    return false;
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/TestLucene42FieldInfoFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/TestLucene42FieldInfoFormat.java
deleted file mode 100644
index 457b53c..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/TestLucene42FieldInfoFormat.java
+++ /dev/null
@@ -1,44 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.BaseFieldInfoFormatTestCase;
-import org.apache.lucene.index.FieldInfo.DocValuesType;
-
-/** Test Lucene 4.2 FieldInfos Format */
-public class TestLucene42FieldInfoFormat extends BaseFieldInfoFormatTestCase {
-  private final Codec codec = new Lucene42RWCodec();
-
-  @Override
-  protected Codec getCodec() {
-    return codec;
-  }
-
-  // we only support these four dv types
-  @Override
-  @Deprecated
-  protected DocValuesType[] getDocValuesTypes() {
-    return new DocValuesType[] {
-      DocValuesType.BINARY,
-      DocValuesType.NUMERIC,
-      DocValuesType.SORTED,
-      DocValuesType.SORTED_SET
-    };
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/TestLucene42NormsFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/TestLucene42NormsFormat.java
deleted file mode 100644
index 7cbfab5..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/TestLucene42NormsFormat.java
+++ /dev/null
@@ -1,137 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.InputStream;
-import java.nio.file.Path;
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.BaseNormsFormatTestCase;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.MultiDocValues;
-import org.apache.lucene.index.NumericDocValues;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.FSDirectory;
-import org.apache.lucene.util.TestUtil;
-
-/** Tests Lucene42's norms format */
-public class TestLucene42NormsFormat extends BaseNormsFormatTestCase {
-  final Codec codec = new Lucene42RWCodec();
-  
-  @Override
-  protected Codec getCodec() {
-    return codec;
-  }
-
-  /** Copy this back to /l/421/lucene/CreateUndeadNorms.java, then:
-   *   - ant clean
-   *   - pushd analysis/common; ant jar; popd
-   *   - pushd core; ant jar; popd
-   *   - javac -cp build/analysis/common/lucene-analyzers-common-4.2.1-SNAPSHOT.jar:build/core/lucene-core-4.2.1-SNAPSHOT.jar CreateUndeadNorms.java
-   *   - java -cp .:build/analysis/common/lucene-analyzers-common-4.2.1-SNAPSHOT.jar:build/core/lucene-core-4.2.1-SNAPSHOT.jar CreateUndeadNorms
-   *   - cd /tmp/undeadnorms  ; zip index.42.undeadnorms.zip *
-
-import java.io.File;
-import java.io.IOException;
-
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.FSDirectory;
-import org.apache.lucene.util.Version;
-
-public class CreateUndeadNorms {
-  public static void main(String[] args) throws Exception {
-    File file = new File("/tmp/undeadnorms");
-    if (file.exists()) {
-      throw new RuntimeException("please remove /tmp/undeadnorms first");
-    }
-    Directory dir = FSDirectory.open(file);
-    IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(Version.LUCENE_42, new WhitespaceAnalyzer(Version.LUCENE_42)));
-    Document doc = new Document();
-    doc.add(new StringField("id", "0", Field.Store.NO));
-    w.addDocument(doc);
-    doc = new Document();
-    doc.add(new StringField("id", "1", Field.Store.NO));
-    Field content = new TextField("content", "some content", Field.Store.NO);
-    content.setTokenStream(new TokenStream() {
-        @Override
-        public boolean incrementToken() throws IOException {
-          throw new IOException("brains brains!");
-        }
-      });
-
-    doc.add(content);
-    try {
-      w.addDocument(doc);
-      throw new RuntimeException("didn't hit exception");
-    } catch (IOException ioe) {
-      // perfect
-    }
-    w.close();
-    dir.close();
-  }
-}
-*/
-  /** 
-   * LUCENE-6006: Test undead norms.
-   *                                 .....            
-   *                             C C  /            
-   *                            /<   /             
-   *             ___ __________/_#__=o             
-   *            /(- /(\_\________   \              
-   *            \ ) \ )_      \o     \             
-   *            /|\ /|\       |'     |             
-   *                          |     _|             
-   *                          /o   __\             
-   *                         / '     |             
-   *                        / /      |             
-   *                       /_/\______|             
-   *                      (   _(    <              
-   *                       \    \    \             
-   *                        \    \    |            
-   *                         \____\____\           
-   *                         ____\_\__\_\          
-   *                       /`   /`     o\          
-   *                       |___ |_______|
-   *
-   */
-  public void testReadUndeadNorms() throws Exception {
-    InputStream resource = TestLucene42NormsFormat.class.getResourceAsStream("index.42.undeadnorms.zip");
-    assertNotNull(resource);
-    Path path = createTempDir("undeadnorms");
-    TestUtil.unzip(resource, path);
-    Directory dir = FSDirectory.open(path);
-    IndexReader r = DirectoryReader.open(dir);
-    NumericDocValues undeadNorms = MultiDocValues.getNormValues(r, "content");
-    assertNotNull(undeadNorms);
-    assertEquals(2, r.maxDoc());
-    assertEquals(0, undeadNorms.get(0));
-    assertEquals(0, undeadNorms.get(1));
-    dir.close();
-    r.close();
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/TestLucene42TermVectorsFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/TestLucene42TermVectorsFormat.java
deleted file mode 100644
index 2c31c31..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/TestLucene42TermVectorsFormat.java
+++ /dev/null
@@ -1,28 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.BaseTermVectorsFormatTestCase;
-
-public class TestLucene42TermVectorsFormat extends BaseTermVectorsFormatTestCase {
-  @Override
-  protected Codec getCodec() {
-    return new Lucene42RWCodec();
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/index.42.undeadnorms.zip b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/index.42.undeadnorms.zip
deleted file mode 100644
index 44064b2..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/index.42.undeadnorms.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene45/Lucene45RWCodec.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene45/Lucene45RWCodec.java
deleted file mode 100644
index 6f42569..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene45/Lucene45RWCodec.java
+++ /dev/null
@@ -1,89 +0,0 @@
-package org.apache.lucene.codecs.lucene45;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.SegmentInfoFormat;
-import org.apache.lucene.codecs.StoredFieldsFormat;
-import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40RWSegmentInfoFormat;
-import org.apache.lucene.codecs.lucene41.Lucene41RWPostingsFormat;
-import org.apache.lucene.codecs.lucene41.Lucene41RWStoredFieldsFormat;
-import org.apache.lucene.codecs.lucene42.Lucene42RWFieldInfosFormat;
-import org.apache.lucene.codecs.lucene42.Lucene42RWNormsFormat;
-import org.apache.lucene.codecs.lucene42.Lucene42RWTermVectorsFormat;
-
-/**
- * Read-write version of {@link Lucene45Codec} for testing.
- * @deprecated for test purposes only
- */
-@Deprecated
-public final class Lucene45RWCodec extends Lucene45Codec {
-  
-  private final PostingsFormat postings = new Lucene41RWPostingsFormat();
-  
-  @Override
-  public PostingsFormat getPostingsFormatForField(String field) {
-    return postings;
-  }
-  
-  private static final FieldInfosFormat fieldInfosFormat = new Lucene42RWFieldInfosFormat();
-
-  @Override
-  public FieldInfosFormat fieldInfosFormat() {
-    return fieldInfosFormat;
-  }
-  
-  private static final DocValuesFormat docValues = new Lucene45RWDocValuesFormat();
-  
-  @Override
-  public DocValuesFormat getDocValuesFormatForField(String field) {
-    return docValues;
-  }
-
-  private static final NormsFormat norms = new Lucene42RWNormsFormat();
-
-  @Override
-  public NormsFormat normsFormat() {
-    return norms;
-  }
-  
-  private static final SegmentInfoFormat segmentInfos = new Lucene40RWSegmentInfoFormat();
-
-  @Override
-  public SegmentInfoFormat segmentInfoFormat() {
-    return segmentInfos;
-  }
-  
-  private static final StoredFieldsFormat storedFields = new Lucene41RWStoredFieldsFormat();
-
-  @Override
-  public StoredFieldsFormat storedFieldsFormat() {
-    return storedFields;
-  }
-  
-  private final TermVectorsFormat vectorsFormat = new Lucene42RWTermVectorsFormat();
-
-  @Override
-  public TermVectorsFormat termVectorsFormat() {
-    return vectorsFormat;
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene45/Lucene45RWDocValuesFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene45/Lucene45RWDocValuesFormat.java
deleted file mode 100644
index 0b205cc..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene45/Lucene45RWDocValuesFormat.java
+++ /dev/null
@@ -1,42 +0,0 @@
-package org.apache.lucene.codecs.lucene45;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.SegmentWriteState;
-
-/**
- * Read-write version of 4.5 docvalues format for testing
- * @deprecated for test purposes only
- */
-@Deprecated
-public final class Lucene45RWDocValuesFormat extends Lucene45DocValuesFormat {
-
-  @Override
-  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    return new Lucene45DocValuesConsumer(state, DATA_CODEC, DATA_EXTENSION, META_CODEC, META_EXTENSION) {
-      @Override
-      void checkCanWrite(FieldInfo field) {
-        // allow writing all fields 
-      }
-    };
-  }
-}
\ No newline at end of file
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene45/TestLucene45DocValuesFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene45/TestLucene45DocValuesFormat.java
deleted file mode 100644
index 3e7436b..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene45/TestLucene45DocValuesFormat.java
+++ /dev/null
@@ -1,38 +0,0 @@
-package org.apache.lucene.codecs.lucene45;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.BaseCompressingDocValuesFormatTestCase;
-
-/**
- * Tests Lucene45DocValuesFormat
- */
-public class TestLucene45DocValuesFormat extends BaseCompressingDocValuesFormatTestCase {
-  private final Codec codec = new Lucene45RWCodec();
-  
-  @Override
-  protected Codec getCodec() {
-    return codec;
-  }
-  
-  @Override
-  protected boolean codecSupportsSortedNumeric() {
-    return false;
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene46/Lucene46RWCodec.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene46/Lucene46RWCodec.java
deleted file mode 100644
index 62631c9..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene46/Lucene46RWCodec.java
+++ /dev/null
@@ -1,80 +0,0 @@
-package org.apache.lucene.codecs.lucene46;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.SegmentInfoFormat;
-import org.apache.lucene.codecs.StoredFieldsFormat;
-import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.codecs.lucene41.Lucene41RWPostingsFormat;
-import org.apache.lucene.codecs.lucene41.Lucene41RWStoredFieldsFormat;
-import org.apache.lucene.codecs.lucene42.Lucene42RWNormsFormat;
-import org.apache.lucene.codecs.lucene42.Lucene42RWTermVectorsFormat;
-import org.apache.lucene.codecs.lucene45.Lucene45RWDocValuesFormat;
-
-/**
- * Read-write version of 4.6 codec for testing
- * @deprecated for test purposes only
- */
-@Deprecated
-public final class Lucene46RWCodec extends Lucene46Codec {
-  
-  private final PostingsFormat postings = new Lucene41RWPostingsFormat();
-  
-  @Override
-  public PostingsFormat getPostingsFormatForField(String field) {
-    return postings;
-  }
-  
-  private static final DocValuesFormat docValues = new Lucene45RWDocValuesFormat();
-  
-  @Override
-  public DocValuesFormat getDocValuesFormatForField(String field) {
-    return docValues;
-  }
-  
-  private static final NormsFormat norms = new Lucene42RWNormsFormat();
-
-  @Override
-  public NormsFormat normsFormat() {
-    return norms;
-  }
-  
-  private static final SegmentInfoFormat segmentInfos = new Lucene46RWSegmentInfoFormat();
-
-  @Override
-  public SegmentInfoFormat segmentInfoFormat() {
-    return segmentInfos;
-  }
-  
-  private static final StoredFieldsFormat storedFields = new Lucene41RWStoredFieldsFormat();
-
-  @Override
-  public StoredFieldsFormat storedFieldsFormat() {
-    return storedFields;
-  }
-  
-  private final TermVectorsFormat vectorsFormat = new Lucene42RWTermVectorsFormat();
-
-  @Override
-  public TermVectorsFormat termVectorsFormat() {
-    return vectorsFormat;
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene46/Lucene46RWSegmentInfoFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene46/Lucene46RWSegmentInfoFormat.java
deleted file mode 100644
index b5788de..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene46/Lucene46RWSegmentInfoFormat.java
+++ /dev/null
@@ -1,70 +0,0 @@
-package org.apache.lucene.codecs.lucene46;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.Version;
-
-/**
- * Read-Write version of 4.6 segmentinfo format for testing
- * @deprecated for test purposes only
- */
-@Deprecated
-public final class Lucene46RWSegmentInfoFormat extends Lucene46SegmentInfoFormat {
-  @Override
-  public void write(Directory dir, SegmentInfo si, IOContext ioContext) throws IOException {
-    final String fileName = IndexFileNames.segmentFileName(si.name, "", Lucene46SegmentInfoFormat.SI_EXTENSION);
-    si.addFile(fileName);
-
-    final IndexOutput output = dir.createOutput(fileName, ioContext);
-
-    boolean success = false;
-    try {
-      CodecUtil.writeHeader(output, Lucene46SegmentInfoFormat.CODEC_NAME, Lucene46SegmentInfoFormat.VERSION_CURRENT);
-      Version version = si.getVersion();
-      if (version.major < 4) {
-        throw new IllegalArgumentException("invalid major version: should be >= 4 but got: " + version.major + " segment=" + si);
-      }
-      // Write the Lucene version that created this segment, since 3.1
-      output.writeString(version.toString());
-      output.writeInt(si.getDocCount());
-
-      output.writeByte((byte) (si.getUseCompoundFile() ? SegmentInfo.YES : SegmentInfo.NO));
-      output.writeStringStringMap(si.getDiagnostics());
-      output.writeStringSet(si.files());
-      CodecUtil.writeFooter(output);
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(output);
-        // TODO: are we doing this outside of the tracking wrapper? why must SIWriter cleanup like this?
-        IOUtils.deleteFilesIgnoringExceptions(si.dir, fileName);
-      } else {
-        output.close();
-      }
-    }
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene46/TestLucene46FieldInfoFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene46/TestLucene46FieldInfoFormat.java
deleted file mode 100644
index 0ed6d3d..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene46/TestLucene46FieldInfoFormat.java
+++ /dev/null
@@ -1,33 +0,0 @@
-package org.apache.lucene.codecs.lucene46;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.BaseFieldInfoFormatTestCase;
-
-/** Test Lucene 4.2 FieldInfos Format */
-public class TestLucene46FieldInfoFormat extends BaseFieldInfoFormatTestCase {
-  private final Codec codec = new Lucene46RWCodec();
-
-  @Override
-  protected Codec getCodec() {
-    return codec;
-  }
-  
-  // TODO: we actually didnt support SORTED_NUMERIC initially, it was done in a minor rev.
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene46/TestLucene46SegmentInfoFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene46/TestLucene46SegmentInfoFormat.java
deleted file mode 100644
index b902826..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene46/TestLucene46SegmentInfoFormat.java
+++ /dev/null
@@ -1,57 +0,0 @@
-package org.apache.lucene.codecs.lucene46;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.BaseSegmentInfoFormatTestCase;
-import org.apache.lucene.util.Version;
-
-/**
- * Tests Lucene46InfoFormat
- */
-public class TestLucene46SegmentInfoFormat extends BaseSegmentInfoFormatTestCase {
-
-  @Override
-  protected Version[] getVersions() {
-    // NOTE: some of these bugfix releases we never actually "wrote",
-    // but staying on the safe side...
-    return new Version[] { 
-        Version.LUCENE_4_6_0,
-        Version.LUCENE_4_6_1,
-        Version.LUCENE_4_7_0,
-        Version.LUCENE_4_7_1,
-        Version.LUCENE_4_7_2,
-        Version.LUCENE_4_8_0,
-        Version.LUCENE_4_8_1,
-        Version.LUCENE_4_9_0,
-        Version.LUCENE_4_10_0,
-        Version.LUCENE_4_10_1
-    };
-  }
-
-  @Override
-  @Deprecated
-  protected void assertIDEquals(byte[] expected, byte[] actual) {
-    assertNull(actual); // we don't support IDs
-  }
-
-  @Override
-  protected Codec getCodec() {
-    return new Lucene46RWCodec();
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene46/TestLucene46UndeadNorms.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene46/TestLucene46UndeadNorms.java
deleted file mode 100644
index bd9cdcf..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene46/TestLucene46UndeadNorms.java
+++ /dev/null
@@ -1,129 +0,0 @@
-package org.apache.lucene.codecs.lucene46;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.InputStream;
-import java.nio.file.Path;
-
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.MultiDocValues;
-import org.apache.lucene.index.NumericDocValues;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.FSDirectory;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.TestUtil;
-
-public class TestLucene46UndeadNorms extends LuceneTestCase {
-
-  /** Copy this back to /l/461/lucene/CreateUndeadNorms.java, then:
-   *   - ant clean
-   *   - pushd analysis/common; ant jar; popd
-   *   - pushd core; ant jar; popd
-   *   - javac -cp build/analysis/common/lucene-analyzers-common-4.6-SNAPSHOT.jar:build/core/lucene-core-4.6-SNAPSHOT.jar CreateUndeadNorms.java
-   *   - java -cp .:build/analysis/common/lucene-analyzers-common-4.6-SNAPSHOT.jar:build/core/lucene-core-4.6-SNAPSHOT.jar CreateUndeadNorms
-   *   - cd /tmp/undeadnorms  ; zip index.46.undeadnorms.zip *
-
-import java.io.File;
-import java.io.IOException;
-
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.FSDirectory;
-import org.apache.lucene.util.Version;
-
-public class CreateUndeadNorms {
-  public static void main(String[] args) throws Exception {
-    File file = new File("/tmp/undeadnorms");
-    if (file.exists()) {
-      throw new RuntimeException("please remove /tmp/undeadnorms first");
-    }
-    Directory dir = FSDirectory.open(file);
-    IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(Version.LUCENE_46, new WhitespaceAnalyzer(Version.LUCENE_46)));
-    Document doc = new Document();
-    doc.add(new StringField("id", "0", Field.Store.NO));
-    w.addDocument(doc);
-    doc = new Document();
-    doc.add(new StringField("id", "1", Field.Store.NO));
-    Field content = new TextField("content", "some content", Field.Store.NO);
-    content.setTokenStream(new TokenStream() {
-        @Override
-        public boolean incrementToken() throws IOException {
-          throw new IOException("brains brains!");
-        }
-      });
-
-    doc.add(content);
-    try {
-      w.addDocument(doc);
-      throw new RuntimeException("didn't hit exception");
-    } catch (IOException ioe) {
-      // perfect
-    }
-    w.close();
-    dir.close();
-  }
-}
-*/
-  /** 
-   * LUCENE-6006: Test undead norms.
-   *                                 .....            
-   *                             C C  /            
-   *                            /<   /             
-   *             ___ __________/_#__=o             
-   *            /(- /(\_\________   \              
-   *            \ ) \ )_      \o     \             
-   *            /|\ /|\       |'     |             
-   *                          |     _|             
-   *                          /o   __\             
-   *                         / '     |             
-   *                        / /      |             
-   *                       /_/\______|             
-   *                      (   _(    <              
-   *                       \    \    \             
-   *                        \    \    |            
-   *                         \____\____\           
-   *                         ____\_\__\_\          
-   *                       /`   /`     o\          
-   *                       |___ |_______|
-   *
-   */
-  public void testReadUndeadNorms() throws Exception {
-    InputStream resource = TestLucene46UndeadNorms.class.getResourceAsStream("index.46.undeadnorms.zip");
-    assertNotNull(resource);
-    Path path = createTempDir("undeadnorms");
-    TestUtil.unzip(resource, path);
-    Directory dir = FSDirectory.open(path);
-    IndexReader r = DirectoryReader.open(dir);
-    NumericDocValues undeadNorms = MultiDocValues.getNormValues(r, "content");
-    assertNotNull(undeadNorms);
-    assertEquals(2, r.maxDoc());
-    assertEquals(0, undeadNorms.get(0));
-    assertEquals(0, undeadNorms.get(1));
-    dir.close();
-    r.close();
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene46/index.46.undeadnorms.zip b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene46/index.46.undeadnorms.zip
deleted file mode 100644
index 4f332b4..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene46/index.46.undeadnorms.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/Lucene49NormsConsumer.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/Lucene49NormsConsumer.java
deleted file mode 100644
index 6ae09be..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/Lucene49NormsConsumer.java
+++ /dev/null
@@ -1,251 +0,0 @@
-package org.apache.lucene.codecs.lucene49;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import static org.apache.lucene.codecs.lucene49.Lucene49NormsFormat.VERSION_CURRENT;
-import static org.apache.lucene.codecs.lucene49.Lucene49NormsProducer.BLOCK_SIZE;
-import static org.apache.lucene.codecs.lucene49.Lucene49NormsProducer.CONST_COMPRESSED;
-import static org.apache.lucene.codecs.lucene49.Lucene49NormsProducer.DELTA_COMPRESSED;
-import static org.apache.lucene.codecs.lucene49.Lucene49NormsProducer.TABLE_COMPRESSED;
-import static org.apache.lucene.codecs.lucene49.Lucene49NormsProducer.UNCOMPRESSED;
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.NormsConsumer;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.packed.BlockPackedWriter;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * Writer for 4.9 norms
- * @deprecated for test purposes only
- */
-@Deprecated
-final class Lucene49NormsConsumer extends NormsConsumer { 
-  IndexOutput data, meta;
-  final int maxDoc;
-  
-  Lucene49NormsConsumer(SegmentWriteState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
-    maxDoc = state.segmentInfo.getDocCount();
-    boolean success = false;
-    try {
-      String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
-      data = state.directory.createOutput(dataName, state.context);
-      CodecUtil.writeHeader(data, dataCodec, VERSION_CURRENT);
-      String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
-      meta = state.directory.createOutput(metaName, state.context);
-      CodecUtil.writeHeader(meta, metaCodec, VERSION_CURRENT);
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(this);
-      }
-    }
-  }
-  
-  // we explicitly use only certain bits per value and a specified format, so we statically check this will work
-  static {
-    assert PackedInts.Format.PACKED_SINGLE_BLOCK.isSupported(1);
-    assert PackedInts.Format.PACKED_SINGLE_BLOCK.isSupported(2);
-    assert PackedInts.Format.PACKED_SINGLE_BLOCK.isSupported(4);
-  }
-
-  @Override
-  public void addNormsField(FieldInfo field, Iterable<Number> values) throws IOException {
-    meta.writeVInt(field.number);
-    long minValue = Long.MAX_VALUE;
-    long maxValue = Long.MIN_VALUE;
-    // TODO: more efficient?
-    NormMap uniqueValues = new NormMap();
-    
-    long count = 0;
-    for (Number nv : values) {
-      if (nv == null) {
-        throw new IllegalStateException("illegal norms data for field " + field.name + ", got null for value: " + count);
-      }
-      final long v = nv.longValue();
-      
-      minValue = Math.min(minValue, v);
-      maxValue = Math.max(maxValue, v);
-      
-      if (uniqueValues != null) {
-        if (uniqueValues.add(v)) {
-          if (uniqueValues.size > 256) {
-            uniqueValues = null;
-          }
-        }
-      }
-      ++count;
-    }
-    
-    if (count != maxDoc) {
-      throw new IllegalStateException("illegal norms data for field " + field.name + ", expected " + maxDoc + " values, got " + count);
-    }
-    
-    if (uniqueValues != null && uniqueValues.size == 1) {
-      // 0 bpv
-      meta.writeByte(CONST_COMPRESSED);
-      meta.writeLong(minValue);
-    } else if (uniqueValues != null) {
-      // small number of unique values: this is the typical case:
-      // we only use bpv=1,2,4,8     
-      PackedInts.Format format = PackedInts.Format.PACKED_SINGLE_BLOCK;
-      int bitsPerValue = PackedInts.bitsRequired(uniqueValues.size-1);
-      if (bitsPerValue == 3) {
-        bitsPerValue = 4;
-      } else if (bitsPerValue > 4) {
-        bitsPerValue = 8;
-      }
-      
-      if (bitsPerValue == 8 && minValue >= Byte.MIN_VALUE && maxValue <= Byte.MAX_VALUE) {
-        meta.writeByte(UNCOMPRESSED); // uncompressed byte[]
-        meta.writeLong(data.getFilePointer());
-        for (Number nv : values) {
-          data.writeByte(nv == null ? 0 : (byte) nv.longValue());
-        }
-      } else {
-        meta.writeByte(TABLE_COMPRESSED); // table-compressed
-        meta.writeLong(data.getFilePointer());
-        data.writeVInt(PackedInts.VERSION_CURRENT);
-        
-        long[] decode = uniqueValues.getDecodeTable();
-        // upgrade to power of two sized array
-        int size = 1 << bitsPerValue;
-        data.writeVInt(size);
-        for (int i = 0; i < decode.length; i++) {
-          data.writeLong(decode[i]);
-        }
-        for (int i = decode.length; i < size; i++) {
-          data.writeLong(0);
-        }
-
-        data.writeVInt(format.getId());
-        data.writeVInt(bitsPerValue);
-
-        final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, format, maxDoc, bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);
-        for(Number nv : values) {
-          writer.add(uniqueValues.getOrd(nv.longValue()));
-        }
-        writer.finish();
-      }
-    } else {
-      meta.writeByte(DELTA_COMPRESSED); // delta-compressed
-      meta.writeLong(data.getFilePointer());
-      data.writeVInt(PackedInts.VERSION_CURRENT);
-      data.writeVInt(BLOCK_SIZE);
-
-      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);
-      for (Number nv : values) {
-        writer.add(nv.longValue());
-      }
-      writer.finish();
-    }
-  }
-  
-  @Override
-  public void close() throws IOException {
-    boolean success = false;
-    try {
-      if (meta != null) {
-        meta.writeVInt(-1); // write EOF marker
-        CodecUtil.writeFooter(meta); // write checksum
-      }
-      if (data != null) {
-        CodecUtil.writeFooter(data); // write checksum
-      }
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(data, meta);
-      } else {
-        IOUtils.closeWhileHandlingException(data, meta);
-      }
-      meta = data = null;
-    }
-  }
-  
-  // specialized deduplication of long->ord for norms: 99.99999% of the time this will be a single-byte range.
-  static class NormMap {
-    // we use short: at most we will add 257 values to this map before its rejected as too big above.
-    final short[] singleByteRange = new short[256];
-    final Map<Long,Short> other = new HashMap<Long,Short>();
-    int size;
-    
-    {
-      Arrays.fill(singleByteRange, (short)-1);
-    }
-
-    /** adds an item to the mapping. returns true if actually added */
-    public boolean add(long l) {
-      assert size <= 256; // once we add > 256 values, we nullify the map in addNumericField and don't use this strategy
-      if (l >= Byte.MIN_VALUE && l <= Byte.MAX_VALUE) {
-        int index = (int) (l + 128);
-        short previous = singleByteRange[index];
-        if (previous < 0) {
-          singleByteRange[index] = (short) size;
-          size++;
-          return true;
-        } else {
-          return false;
-        }
-      } else {
-        if (!other.containsKey(l)) {
-          other.put(l, (short)size);
-          size++;
-          return true;
-        } else {
-          return false;
-        }
-      }
-    }
-    
-    /** gets the ordinal for a previously added item */
-    public int getOrd(long l) {
-      if (l >= Byte.MIN_VALUE && l <= Byte.MAX_VALUE) {
-        int index = (int) (l + 128);
-        return singleByteRange[index];
-      } else {
-        // NPE if something is screwed up
-        return other.get(l);
-      }
-    }
-    
-    /** retrieves the ordinal table for previously added items */
-    public long[] getDecodeTable() {
-      long decode[] = new long[size];
-      for (int i = 0; i < singleByteRange.length; i++) {
-        short s = singleByteRange[i];
-        if (s >= 0) {
-          decode[s] = i - 128;
-        }
-      }
-      for (Map.Entry<Long,Short> entry : other.entrySet()) {
-        decode[entry.getValue()] = entry.getKey();
-      }
-      return decode;
-    }
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/Lucene49RWCodec.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/Lucene49RWCodec.java
deleted file mode 100644
index c000488..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/Lucene49RWCodec.java
+++ /dev/null
@@ -1,79 +0,0 @@
-package org.apache.lucene.codecs.lucene49;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.SegmentInfoFormat;
-import org.apache.lucene.codecs.StoredFieldsFormat;
-import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.codecs.lucene41.Lucene41RWPostingsFormat;
-import org.apache.lucene.codecs.lucene41.Lucene41RWStoredFieldsFormat;
-import org.apache.lucene.codecs.lucene42.Lucene42RWTermVectorsFormat;
-import org.apache.lucene.codecs.lucene46.Lucene46RWSegmentInfoFormat;
-
-/**
- * Read-Write version of 4.9 codec for testing
- * @deprecated for test purposes only
- */
-@Deprecated
-public final class Lucene49RWCodec extends Lucene49Codec {
-  
-  private final PostingsFormat postings = new Lucene41RWPostingsFormat();
-  
-  @Override
-  public PostingsFormat getPostingsFormatForField(String field) {
-    return postings;
-  }
-  
-  private static final DocValuesFormat docValues = new Lucene49RWDocValuesFormat();
-  
-  @Override
-  public DocValuesFormat getDocValuesFormatForField(String field) {
-    return docValues;
-  }
-  
-  private static final NormsFormat norms = new Lucene49RWNormsFormat();
-
-  @Override
-  public NormsFormat normsFormat() {
-    return norms;
-  }
-  
-  private static final SegmentInfoFormat segmentInfos = new Lucene46RWSegmentInfoFormat();
-
-  @Override
-  public SegmentInfoFormat segmentInfoFormat() {
-    return segmentInfos;
-  }
-  
-  private static final StoredFieldsFormat storedFields = new Lucene41RWStoredFieldsFormat();
-
-  @Override
-  public StoredFieldsFormat storedFieldsFormat() {
-    return storedFields;
-  }
-  
-  private final TermVectorsFormat vectorsFormat = new Lucene42RWTermVectorsFormat();
-
-  @Override
-  public TermVectorsFormat termVectorsFormat() {
-    return vectorsFormat;
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/Lucene49RWDocValuesFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/Lucene49RWDocValuesFormat.java
deleted file mode 100644
index e3c758e..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/Lucene49RWDocValuesFormat.java
+++ /dev/null
@@ -1,43 +0,0 @@
-package org.apache.lucene.codecs.lucene49;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.SegmentWriteState;
-
-/**
- * Read-Write version of 4.9 docvalues format for testing
- * @deprecated for test purposes only
- */
-@Deprecated
-public final class Lucene49RWDocValuesFormat extends Lucene49DocValuesFormat {
-
-  @Override
-  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    return new Lucene49DocValuesConsumer(state, DATA_CODEC, DATA_EXTENSION, META_CODEC, META_EXTENSION) {
-      @Override
-      void checkCanWrite(FieldInfo field) {
-        // allow writing all fields 
-      }
-    };
-  }
-  
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/Lucene49RWNormsFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/Lucene49RWNormsFormat.java
deleted file mode 100644
index c704399..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/Lucene49RWNormsFormat.java
+++ /dev/null
@@ -1,35 +0,0 @@
-package org.apache.lucene.codecs.lucene49;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.NormsConsumer;
-import org.apache.lucene.index.SegmentWriteState;
-
-/**
- * Read-Write version of 4.9 norms format for testing
- * @deprecated for test purposes only
- */
-@Deprecated
-public class Lucene49RWNormsFormat extends Lucene49NormsFormat {
-  @Override
-  public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
-    return new Lucene49NormsConsumer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION);
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/TestLucene49DocValuesFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/TestLucene49DocValuesFormat.java
deleted file mode 100644
index a01ab3f..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/TestLucene49DocValuesFormat.java
+++ /dev/null
@@ -1,33 +0,0 @@
-package org.apache.lucene.codecs.lucene49;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.BaseCompressingDocValuesFormatTestCase;
-
-/**
- * Tests Lucene49DocValuesFormat
- */
-public class TestLucene49DocValuesFormat extends BaseCompressingDocValuesFormatTestCase {
-  private final Codec codec = new Lucene49RWCodec();
-  
-  @Override
-  protected Codec getCodec() {
-    return codec;
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/TestLucene49NormsFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/TestLucene49NormsFormat.java
deleted file mode 100644
index 3c7b0a7..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/TestLucene49NormsFormat.java
+++ /dev/null
@@ -1,140 +0,0 @@
-package org.apache.lucene.codecs.lucene49;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.InputStream;
-import java.nio.file.Path;
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.BaseNormsFormatTestCase;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.MultiDocValues;
-import org.apache.lucene.index.NumericDocValues;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.FSDirectory;
-import org.apache.lucene.util.TestUtil;
-
-/**
- * Tests Lucene49NormsFormat
- */
-public class TestLucene49NormsFormat extends BaseNormsFormatTestCase {
-  private final Codec codec = new Lucene49RWCodec();
-  
-  @Override
-  protected Codec getCodec() {
-    return codec;
-  } 
-
-  /** Copy this back to /l/491/lucene/CreateUndeadNorms.java, then:
-   *   - ant clean
-   *   - pushd analysis/common; ant jar; popd
-   *   - pushd core; ant jar; popd
-   *   - javac -cp build/analysis/common/lucene-analyzers-common-4.9-SNAPSHOT.jar:build/core/lucene-core-4.9-SNAPSHOT.jar CreateUndeadNorms.java
-   *   - java -cp .:build/analysis/common/lucene-analyzers-common-4.9-SNAPSHOT.jar:build/core/lucene-core-4.9-SNAPSHOT.jar CreateUndeadNorms
-   *   - cd /tmp/undeadnorms  ; zip index.49.undeadnorms.zip *
-
-import java.io.File;
-import java.io.IOException;
-
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.FSDirectory;
-import org.apache.lucene.util.Version;
-
-public class CreateUndeadNorms {
-  public static void main(String[] args) throws Exception {
-    File file = new File("/tmp/undeadnorms");
-    if (file.exists()) {
-      throw new RuntimeException("please remove /tmp/undeadnorms first");
-    }
-    Directory dir = FSDirectory.open(file);
-    IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(Version.LUCENE_4_9, new WhitespaceAnalyzer(Version.LUCENE_4_9)));
-    Document doc = new Document();
-    doc.add(new StringField("id", "0", Field.Store.NO));
-    w.addDocument(doc);
-    doc = new Document();
-    doc.add(new StringField("id", "1", Field.Store.NO));
-    Field content = new TextField("content", "some content", Field.Store.NO);
-    content.setTokenStream(new TokenStream() {
-        @Override
-        public boolean incrementToken() throws IOException {
-          throw new IOException("brains brains!");
-        }
-      });
-
-    doc.add(content);
-    try {
-      w.addDocument(doc);
-      throw new RuntimeException("didn't hit exception");
-    } catch (IOException ioe) {
-      // perfect
-    }
-    w.close();
-    dir.close();
-  }
-}
-*/
-
-  /** 
-   * LUCENE-6006: Test undead norms.
-   *                                 .....            
-   *                             C C  /            
-   *                            /<   /             
-   *             ___ __________/_#__=o             
-   *            /(- /(\_\________   \              
-   *            \ ) \ )_      \o     \             
-   *            /|\ /|\       |'     |             
-   *                          |     _|             
-   *                          /o   __\             
-   *                         / '     |             
-   *                        / /      |             
-   *                       /_/\______|             
-   *                      (   _(    <              
-   *                       \    \    \             
-   *                        \    \    |            
-   *                         \____\____\           
-   *                         ____\_\__\_\          
-   *                       /`   /`     o\          
-   *                       |___ |_______|
-   *
-   */
-  public void testReadUndeadNorms() throws Exception {
-    InputStream resource = TestLucene49NormsFormat.class.getResourceAsStream("index.49.undeadnorms.zip");
-    assertNotNull(resource);
-    Path path = createTempDir("undeadnorms");
-    TestUtil.unzip(resource, path);
-    Directory dir = FSDirectory.open(path);
-    IndexReader r = DirectoryReader.open(dir);
-    NumericDocValues undeadNorms = MultiDocValues.getNormValues(r, "content");
-    assertNotNull(undeadNorms);
-    assertEquals(2, r.maxDoc());
-    assertEquals(0, undeadNorms.get(0));
-    assertEquals(0, undeadNorms.get(1));
-    dir.close();
-    r.close();
-  }
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/index.49.undeadnorms.zip b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/index.49.undeadnorms.zip
deleted file mode 100644
index 8d34d41..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/index.49.undeadnorms.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java b/lucene/backward-codecs/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
index f131d3f..de87615 100644
--- a/lucene/backward-codecs/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
@@ -76,6 +76,7 @@ import org.apache.lucene.util.TestUtil;
 import org.apache.lucene.util.Version;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
+import org.junit.Ignore;
 
 /*
   Verify we can read the pre-5.0 file format, do searches
@@ -158,8 +159,8 @@ public class TestBackwardsCompatibility extends LuceneTestCase {
   }
   
   private void updateBinary(IndexWriter writer, String id, String f, String cf, long value) throws IOException {
-    writer.updateBinaryDocValue(new Term("id", id), f, TestDocValuesUpdatesOnOldSegments.toBytes(value));
-    writer.updateBinaryDocValue(new Term("id", id), cf, TestDocValuesUpdatesOnOldSegments.toBytes(value*2));
+    writer.updateBinaryDocValue(new Term("id", id), f, toBytes(value));
+    writer.updateBinaryDocValue(new Term("id", id), cf, toBytes(value*2));
   }
 
   // Creates an index with DocValues updates
@@ -179,10 +180,10 @@ public class TestBackwardsCompatibility extends LuceneTestCase {
       doc.add(new NumericDocValuesField("ndv1_c", i*2));
       doc.add(new NumericDocValuesField("ndv2", i*3));
       doc.add(new NumericDocValuesField("ndv2_c", i*6));
-      doc.add(new BinaryDocValuesField("bdv1", TestDocValuesUpdatesOnOldSegments.toBytes(i)));
-      doc.add(new BinaryDocValuesField("bdv1_c", TestDocValuesUpdatesOnOldSegments.toBytes(i*2)));
-      doc.add(new BinaryDocValuesField("bdv2", TestDocValuesUpdatesOnOldSegments.toBytes(i*3)));
-      doc.add(new BinaryDocValuesField("bdv2_c", TestDocValuesUpdatesOnOldSegments.toBytes(i*6)));
+      doc.add(new BinaryDocValuesField("bdv1", toBytes(i)));
+      doc.add(new BinaryDocValuesField("bdv1_c", toBytes(i*2)));
+      doc.add(new BinaryDocValuesField("bdv2", toBytes(i*3)));
+      doc.add(new BinaryDocValuesField("bdv2_c", toBytes(i*6)));
       writer.addDocument(doc);
       if ((i+1) % 10 == 0) {
         writer.commit(); // flush every 10 docs
@@ -208,50 +209,6 @@ public class TestBackwardsCompatibility extends LuceneTestCase {
   }
 
   final static String[] oldNames = {
-      "4.0.0-cfs",
-      "4.0.0-nocfs",
-      "4.0.0.1-cfs",
-      "4.0.0.1-nocfs",
-      "4.0.0.2-cfs",
-      "4.0.0.2-nocfs",
-      "4.1.0-cfs",
-      "4.1.0-nocfs",
-      "4.2.0-cfs",
-      "4.2.0-nocfs",
-      "4.2.1-cfs",
-      "4.2.1-nocfs",
-      "4.3.0-cfs",
-      "4.3.0-nocfs",
-      "4.3.1-cfs",
-      "4.3.1-nocfs",
-      "4.4.0-cfs",
-      "4.4.0-nocfs",
-      "4.5.0-cfs",
-      "4.5.0-nocfs",
-      "4.5.1-cfs",
-      "4.5.1-nocfs",
-      "4.6.0-cfs",
-      "4.6.0-nocfs",
-      "4.6.1-cfs",
-      "4.6.1-nocfs",
-      "4.7.0-cfs",
-      "4.7.0-nocfs",
-      "4.7.1-cfs",
-      "4.7.1-nocfs",
-      "4.7.2-cfs",
-      "4.7.2-nocfs",
-      "4.8.0-cfs",
-      "4.8.0-nocfs",
-      "4.8.1-cfs",
-      "4.8.1-nocfs",
-      "4.9.0-cfs",
-      "4.9.0-nocfs",
-      "4.9.1-cfs",
-      "4.9.1-nocfs",
-      "4.10.0-cfs",
-      "4.10.0-nocfs",
-      "4.10.1-cfs",
-      "4.10.1-nocfs"
   };
   
   final String[] unsupportedNames = {
@@ -302,11 +259,54 @@ public class TestBackwardsCompatibility extends LuceneTestCase {
       "3.6.1-cfs",
       "3.6.1-nocfs",
       "3.6.2-cfs",
-      "3.6.2-nocfs"
+      "3.6.2-nocfs",
+      "4.0.0-cfs",
+      "4.0.0-nocfs",
+      "4.0.0.1-cfs",
+      "4.0.0.1-nocfs",
+      "4.0.0.2-cfs",
+      "4.0.0.2-nocfs",
+      "4.1.0-cfs",
+      "4.1.0-nocfs",
+      "4.2.0-cfs",
+      "4.2.0-nocfs",
+      "4.2.1-cfs",
+      "4.2.1-nocfs",
+      "4.3.0-cfs",
+      "4.3.0-nocfs",
+      "4.3.1-cfs",
+      "4.3.1-nocfs",
+      "4.4.0-cfs",
+      "4.4.0-nocfs",
+      "4.5.0-cfs",
+      "4.5.0-nocfs",
+      "4.5.1-cfs",
+      "4.5.1-nocfs",
+      "4.6.0-cfs",
+      "4.6.0-nocfs",
+      "4.6.1-cfs",
+      "4.6.1-nocfs",
+      "4.7.0-cfs",
+      "4.7.0-nocfs",
+      "4.7.1-cfs",
+      "4.7.1-nocfs",
+      "4.7.2-cfs",
+      "4.7.2-nocfs",
+      "4.8.0-cfs",
+      "4.8.0-nocfs",
+      "4.8.1-cfs",
+      "4.8.1-nocfs",
+      "4.9.0-cfs",
+      "4.9.0-nocfs",
+      "4.9.1-cfs",
+      "4.9.1-nocfs",
+      "4.10.0-cfs",
+      "4.10.0-nocfs",
+      "4.10.1-cfs",
+      "4.10.1-nocfs"
   };
   
-  final static String[] oldSingleSegmentNames = {"4.0.0-optimized-cfs",
-                                                 "4.0.0-optimized-nocfs",
+  final static String[] oldSingleSegmentNames = {
   };
   
   static Map<String,Directory> oldIndexDirs;
@@ -1221,8 +1221,9 @@ public class TestBackwardsCompatibility extends LuceneTestCase {
     }
   }
 
-  public static final String moreTermsIndex = "moreterms.4.0.0.zip";
+  public static final String moreTermsIndex = "moreterms.5.0.0.zip";
 
+  @Ignore("needs a 5.0 index once released")
   public void testMoreTerms() throws Exception {
     Path oldIndexDir = createTempDir("moreterms");
     TestUtil.unzip(getDataInputStream(moreTermsIndex), oldIndexDir);
@@ -1233,7 +1234,7 @@ public class TestBackwardsCompatibility extends LuceneTestCase {
     dir.close();
   }
 
-  public static final String dvUpdatesIndex = "dvupdates.4.8.0.zip";
+  public static final String dvUpdatesIndex = "dvupdates.5.0.0.zip";
 
   private void assertNumericDocValues(LeafReader r, String f, String cf) throws IOException {
     NumericDocValues ndvf = r.getNumericDocValues(f);
@@ -1247,7 +1248,7 @@ public class TestBackwardsCompatibility extends LuceneTestCase {
     BinaryDocValues bdvf = r.getBinaryDocValues(f);
     BinaryDocValues bdvcf = r.getBinaryDocValues(cf);
     for (int i = 0; i < r.maxDoc(); i++) {
-      assertEquals(TestDocValuesUpdatesOnOldSegments.getValue(bdvcf, i), TestDocValuesUpdatesOnOldSegments.getValue(bdvf, i)*2);
+      assertEquals(getValue(bdvcf, i), getValue(bdvf, i)*2);
     }
   }
   
@@ -1263,6 +1264,7 @@ public class TestBackwardsCompatibility extends LuceneTestCase {
     reader.close();
   }
   
+  @Ignore("needs a 5.0 index once released")
   public void testDocValuesUpdates() throws Exception {
     Path oldIndexDir = createTempDir("dvupdates");
     TestUtil.unzip(getDataInputStream(dvUpdatesIndex), oldIndexDir);
@@ -1324,4 +1326,27 @@ public class TestBackwardsCompatibility extends LuceneTestCase {
       dir.close();
     }
   }
+  
+  static long getValue(BinaryDocValues bdv, int idx) {
+    BytesRef term = bdv.get(idx);
+    idx = term.offset;
+    byte b = term.bytes[idx++];
+    long value = b & 0x7FL;
+    for (int shift = 7; (b & 0x80L) != 0; shift += 7) {
+      b = term.bytes[idx++];
+      value |= (b & 0x7FL) << shift;
+    }
+    return value;
+  }
+
+  // encodes a long into a BytesRef as VLong so that we get varying number of bytes when we update
+  static BytesRef toBytes(long value) {
+    BytesRef bytes = new BytesRef(10); // negative longs may take 10 bytes
+    while ((value & ~0x7FL) != 0L) {
+      bytes.bytes[bytes.length++] = (byte) ((value & 0x7FL) | 0x80L);
+      value >>>= 7;
+    }
+    bytes.bytes[bytes.length++] = (byte) value;
+    return bytes;
+  }
 }
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/TestDocValuesUpdatesOnOldSegments.java b/lucene/backward-codecs/src/test/org/apache/lucene/index/TestDocValuesUpdatesOnOldSegments.java
deleted file mode 100644
index 188bc0d..0000000
--- a/lucene/backward-codecs/src/test/org/apache/lucene/index/TestDocValuesUpdatesOnOldSegments.java
+++ /dev/null
@@ -1,124 +0,0 @@
-package org.apache.lucene.index;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.lucene40.Lucene40RWCodec;
-import org.apache.lucene.codecs.lucene41.Lucene41RWCodec;
-import org.apache.lucene.codecs.lucene42.Lucene42RWCodec;
-import org.apache.lucene.codecs.lucene45.Lucene45RWCodec;
-import org.apache.lucene.document.BinaryDocValuesField;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field.Store;
-import org.apache.lucene.document.NumericDocValuesField;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.LuceneTestCase;
-
-/** 
- * Tests performing docvalues updates against versions of lucene
- * that did not support it.
- */
-public class TestDocValuesUpdatesOnOldSegments extends LuceneTestCase {
-
-  static long getValue(BinaryDocValues bdv, int idx) {
-    BytesRef term = bdv.get(idx);
-    idx = term.offset;
-    byte b = term.bytes[idx++];
-    long value = b & 0x7FL;
-    for (int shift = 7; (b & 0x80L) != 0; shift += 7) {
-      b = term.bytes[idx++];
-      value |= (b & 0x7FL) << shift;
-    }
-    return value;
-  }
-
-  // encodes a long into a BytesRef as VLong so that we get varying number of bytes when we update
-  static BytesRef toBytes(long value) {
-    BytesRef bytes = new BytesRef(10); // negative longs may take 10 bytes
-    while ((value & ~0x7FL) != 0L) {
-      bytes.bytes[bytes.length++] = (byte) ((value & 0x7FL) | 0x80L);
-      value >>>= 7;
-    }
-    bytes.bytes[bytes.length++] = (byte) value;
-    return bytes;
-  }
-
-  public void testBinaryUpdates() throws Exception {
-    Codec[] oldCodecs = new Codec[] { new Lucene40RWCodec(), new Lucene41RWCodec(), new Lucene42RWCodec(), new Lucene45RWCodec() };
-    
-    for (Codec codec : oldCodecs) {
-      Directory dir = newDirectory();
-      
-      // create a segment with an old Codec
-      IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));
-      conf.setCodec(codec);
-      IndexWriter writer = new IndexWriter(dir, conf);
-      Document doc = new Document();
-      doc.add(new StringField("id", "doc", Store.NO));
-      doc.add(new BinaryDocValuesField("f", toBytes(5L)));
-      writer.addDocument(doc);
-      writer.close();
-      
-      conf = newIndexWriterConfig(new MockAnalyzer(random()));
-      writer = new IndexWriter(dir, conf);
-      writer.updateBinaryDocValue(new Term("id", "doc"), "f", toBytes(4L));
-      try {
-        writer.close();
-        fail("should not have succeeded to update a segment written with an old Codec");
-      } catch (UnsupportedOperationException e) {
-        writer.rollback();
-      }
-      
-      dir.close();
-    }
-  }
-
-  public void testNumericUpdates() throws Exception {
-    Codec[] oldCodecs = new Codec[] { new Lucene40RWCodec(), new Lucene41RWCodec(), new Lucene42RWCodec(), new Lucene45RWCodec() };
-    
-    for (Codec codec : oldCodecs) {
-      Directory dir = newDirectory();
-      
-      // create a segment with an old Codec
-      IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));
-      conf.setCodec(codec);
-      IndexWriter writer = new IndexWriter(dir, conf);
-      Document doc = new Document();
-      doc.add(new StringField("id", "doc", Store.NO));
-      doc.add(new NumericDocValuesField("f", 5));
-      writer.addDocument(doc);
-      writer.close();
-      
-      conf = newIndexWriterConfig(new MockAnalyzer(random()));
-      writer = new IndexWriter(dir, conf);
-      writer.updateNumericDocValue(new Term("id", "doc"), "f", 4L);
-      try {
-        writer.close();
-        fail("should not have succeeded to update a segment written with an old Codec");
-      } catch (UnsupportedOperationException e) {
-        writer.rollback();
-      }
-      
-      dir.close();
-    }
-  }
-
-}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/dvupdates.4.8.0.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/dvupdates.4.8.0.zip
deleted file mode 100755
index e8121ca..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/dvupdates.4.8.0.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.0.0-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.0.0-cfs.zip
deleted file mode 100644
index 4974749..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.0.0-cfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.0.0-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.0.0-nocfs.zip
deleted file mode 100644
index 9699080..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.0.0-nocfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.0.0-optimized-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.0.0-optimized-cfs.zip
deleted file mode 100644
index 209c436..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.0.0-optimized-cfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.0.0-optimized-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.0.0-optimized-nocfs.zip
deleted file mode 100644
index 0eaffd0..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.0.0-optimized-nocfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.0.0.1-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.0.0.1-cfs.zip
deleted file mode 100644
index f33ef34..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.0.0.1-cfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.0.0.1-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.0.0.1-nocfs.zip
deleted file mode 100644
index ecec878..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.0.0.1-nocfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.0.0.2-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.0.0.2-cfs.zip
deleted file mode 100644
index 3dd1ead..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.0.0.2-cfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.0.0.2-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.0.0.2-nocfs.zip
deleted file mode 100644
index 999789a..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.0.0.2-nocfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.1.0-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.1.0-cfs.zip
deleted file mode 100644
index 400f6e5..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.1.0-cfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.1.0-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.1.0-nocfs.zip
deleted file mode 100644
index 96eaff2..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.1.0-nocfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.10.0-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.10.0-cfs.zip
deleted file mode 100644
index a98aee4..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.10.0-cfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.10.0-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.10.0-nocfs.zip
deleted file mode 100644
index 4b280fe..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.10.0-nocfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.10.1-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.10.1-cfs.zip
deleted file mode 100644
index 28ecac3..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.10.1-cfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.10.1-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.10.1-nocfs.zip
deleted file mode 100644
index 0a77788..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.10.1-nocfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.2.0-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.2.0-cfs.zip
deleted file mode 100644
index ecce4a2..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.2.0-cfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.2.0-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.2.0-nocfs.zip
deleted file mode 100644
index 9edcd09..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.2.0-nocfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.2.1-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.2.1-cfs.zip
deleted file mode 100644
index 9d0b6e0..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.2.1-cfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.2.1-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.2.1-nocfs.zip
deleted file mode 100644
index 2e963c6..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.2.1-nocfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.3.0-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.3.0-cfs.zip
deleted file mode 100644
index 59812ee..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.3.0-cfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.3.0-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.3.0-nocfs.zip
deleted file mode 100644
index 3fa3023..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.3.0-nocfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.3.1-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.3.1-cfs.zip
deleted file mode 100644
index 01b66b6..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.3.1-cfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.3.1-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.3.1-nocfs.zip
deleted file mode 100644
index dd80b03..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.3.1-nocfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.4.0-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.4.0-cfs.zip
deleted file mode 100644
index 18ed8ca..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.4.0-cfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.4.0-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.4.0-nocfs.zip
deleted file mode 100644
index ad7dead..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.4.0-nocfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.5.0-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.5.0-cfs.zip
deleted file mode 100644
index 968c3fe..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.5.0-cfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.5.0-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.5.0-nocfs.zip
deleted file mode 100644
index d598689..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.5.0-nocfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.5.1-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.5.1-cfs.zip
deleted file mode 100644
index 68bf61c..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.5.1-cfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.5.1-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.5.1-nocfs.zip
deleted file mode 100644
index 91ae2ea..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.5.1-nocfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.6.0-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.6.0-cfs.zip
deleted file mode 100644
index e1c4801..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.6.0-cfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.6.0-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.6.0-nocfs.zip
deleted file mode 100644
index 758dbf6..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.6.0-nocfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.6.1-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.6.1-cfs.zip
deleted file mode 100644
index 863dab7..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.6.1-cfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.6.1-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.6.1-nocfs.zip
deleted file mode 100644
index 4087748..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.6.1-nocfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.7.0-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.7.0-cfs.zip
deleted file mode 100644
index 1d5fe36..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.7.0-cfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.7.0-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.7.0-nocfs.zip
deleted file mode 100644
index 4a20ef2..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.7.0-nocfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.7.1-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.7.1-cfs.zip
deleted file mode 100644
index b0eb81c..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.7.1-cfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.7.1-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.7.1-nocfs.zip
deleted file mode 100644
index 4684f07..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.7.1-nocfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.7.2-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.7.2-cfs.zip
deleted file mode 100644
index a7964a6..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.7.2-cfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.7.2-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.7.2-nocfs.zip
deleted file mode 100644
index 4593414..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.7.2-nocfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.8.0-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.8.0-cfs.zip
deleted file mode 100644
index b05a294..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.8.0-cfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.8.0-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.8.0-nocfs.zip
deleted file mode 100644
index 49831bc..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.8.0-nocfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.8.1-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.8.1-cfs.zip
deleted file mode 100644
index 3e6afca..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.8.1-cfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.8.1-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.8.1-nocfs.zip
deleted file mode 100644
index beadd10..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.8.1-nocfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.9.0-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.9.0-cfs.zip
deleted file mode 100644
index 7859d79..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.9.0-cfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.9.0-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.9.0-nocfs.zip
deleted file mode 100644
index 817cf1f..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.9.0-nocfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.9.1-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.9.1-cfs.zip
deleted file mode 100644
index c77faa8..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.9.1-cfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.9.1-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.9.1-nocfs.zip
deleted file mode 100644
index 5161de5..0000000
Binary files a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.4.9.1-nocfs.zip and /dev/null differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.0.0-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.0.0-cfs.zip
new file mode 100644
index 0000000..4974749
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.0.0-cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.0.0-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.0.0-nocfs.zip
new file mode 100644
index 0000000..9699080
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.0.0-nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.0.0.1-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.0.0.1-cfs.zip
new file mode 100644
index 0000000..f33ef34
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.0.0.1-cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.0.0.1-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.0.0.1-nocfs.zip
new file mode 100644
index 0000000..ecec878
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.0.0.1-nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.0.0.2-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.0.0.2-cfs.zip
new file mode 100644
index 0000000..3dd1ead
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.0.0.2-cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.0.0.2-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.0.0.2-nocfs.zip
new file mode 100644
index 0000000..999789a
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.0.0.2-nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.1.0-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.1.0-cfs.zip
new file mode 100644
index 0000000..400f6e5
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.1.0-cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.1.0-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.1.0-nocfs.zip
new file mode 100644
index 0000000..96eaff2
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.1.0-nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.10.0-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.10.0-cfs.zip
new file mode 100644
index 0000000..a98aee4
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.10.0-cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.10.0-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.10.0-nocfs.zip
new file mode 100644
index 0000000..4b280fe
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.10.0-nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.10.1-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.10.1-cfs.zip
new file mode 100644
index 0000000..28ecac3
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.10.1-cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.10.1-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.10.1-nocfs.zip
new file mode 100644
index 0000000..0a77788
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.10.1-nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.2.0-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.2.0-cfs.zip
new file mode 100644
index 0000000..ecce4a2
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.2.0-cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.2.0-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.2.0-nocfs.zip
new file mode 100644
index 0000000..9edcd09
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.2.0-nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.2.1-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.2.1-cfs.zip
new file mode 100644
index 0000000..9d0b6e0
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.2.1-cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.2.1-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.2.1-nocfs.zip
new file mode 100644
index 0000000..2e963c6
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.2.1-nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.3.0-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.3.0-cfs.zip
new file mode 100644
index 0000000..59812ee
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.3.0-cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.3.0-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.3.0-nocfs.zip
new file mode 100644
index 0000000..3fa3023
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.3.0-nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.3.1-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.3.1-cfs.zip
new file mode 100644
index 0000000..01b66b6
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.3.1-cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.3.1-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.3.1-nocfs.zip
new file mode 100644
index 0000000..dd80b03
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.3.1-nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.4.0-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.4.0-cfs.zip
new file mode 100644
index 0000000..18ed8ca
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.4.0-cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.4.0-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.4.0-nocfs.zip
new file mode 100644
index 0000000..ad7dead
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.4.0-nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.5.0-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.5.0-cfs.zip
new file mode 100644
index 0000000..968c3fe
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.5.0-cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.5.0-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.5.0-nocfs.zip
new file mode 100644
index 0000000..d598689
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.5.0-nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.5.1-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.5.1-cfs.zip
new file mode 100644
index 0000000..68bf61c
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.5.1-cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.5.1-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.5.1-nocfs.zip
new file mode 100644
index 0000000..91ae2ea
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.5.1-nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.6.0-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.6.0-cfs.zip
new file mode 100644
index 0000000..e1c4801
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.6.0-cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.6.0-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.6.0-nocfs.zip
new file mode 100644
index 0000000..758dbf6
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.6.0-nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.6.1-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.6.1-cfs.zip
new file mode 100644
index 0000000..863dab7
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.6.1-cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.6.1-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.6.1-nocfs.zip
new file mode 100644
index 0000000..4087748
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.6.1-nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.7.0-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.7.0-cfs.zip
new file mode 100644
index 0000000..1d5fe36
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.7.0-cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.7.0-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.7.0-nocfs.zip
new file mode 100644
index 0000000..4a20ef2
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.7.0-nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.7.1-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.7.1-cfs.zip
new file mode 100644
index 0000000..b0eb81c
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.7.1-cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.7.1-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.7.1-nocfs.zip
new file mode 100644
index 0000000..4684f07
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.7.1-nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.7.2-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.7.2-cfs.zip
new file mode 100644
index 0000000..a7964a6
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.7.2-cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.7.2-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.7.2-nocfs.zip
new file mode 100644
index 0000000..4593414
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.7.2-nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.8.0-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.8.0-cfs.zip
new file mode 100644
index 0000000..b05a294
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.8.0-cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.8.0-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.8.0-nocfs.zip
new file mode 100644
index 0000000..49831bc
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.8.0-nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.8.1-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.8.1-cfs.zip
new file mode 100644
index 0000000..3e6afca
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.8.1-cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.8.1-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.8.1-nocfs.zip
new file mode 100644
index 0000000..beadd10
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.8.1-nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.9.0-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.9.0-cfs.zip
new file mode 100644
index 0000000..7859d79
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.9.0-cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.9.0-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.9.0-nocfs.zip
new file mode 100644
index 0000000..817cf1f
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.9.0-nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.9.1-cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.9.1-cfs.zip
new file mode 100644
index 0000000..c77faa8
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.9.1-cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.9.1-nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.9.1-nocfs.zip
new file mode 100644
index 0000000..5161de5
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.4.9.1-nocfs.zip differ
diff --git a/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java b/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
index 5e9de04..ef00821 100644
--- a/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
+++ b/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
@@ -592,7 +592,7 @@ public class CheckIndex implements Closeable {
       segInfoStat.docCount = info.info.getDocCount();
       
       final Version version = info.info.getVersion();
-      if (info.info.getDocCount() <= 0 && version != null && version.onOrAfter(Version.LUCENE_4_5_0)) {
+      if (info.info.getDocCount() <= 0) {
         throw new RuntimeException("illegal number of documents: maxDoc=" + info.info.getDocCount());
       }
 
diff --git a/lucene/core/src/java/org/apache/lucene/index/SegmentDocValuesProducer.java b/lucene/core/src/java/org/apache/lucene/index/SegmentDocValuesProducer.java
index 420ea44..1fd9c6b 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SegmentDocValuesProducer.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SegmentDocValuesProducer.java
@@ -34,7 +34,6 @@ import org.apache.lucene.util.Accountable;
 import org.apache.lucene.util.Accountables;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.RamUsageEstimator;
-import org.apache.lucene.util.Version;
 
 /** Encapsulates multiple producers when there are docvalues updates as one producer */
 // TODO: try to clean up close? no-op?
@@ -52,63 +51,26 @@ class SegmentDocValuesProducer extends DocValuesProducer {
   SegmentDocValuesProducer(SegmentCommitInfo si, Directory dir, FieldInfos fieldInfos, SegmentDocValues segDocValues, DocValuesFormat dvFormat) throws IOException {
     boolean success = false;
     try {
-      Version ver = si.info.getVersion();
-      if (ver != null && ver.onOrAfter(Version.LUCENE_4_9_0)) {
-        DocValuesProducer baseProducer = null;
-        for (FieldInfo fi : fieldInfos) {
-          if (!fi.hasDocValues()) {
-            continue;
-          }
-          long docValuesGen = fi.getDocValuesGen();
-          if (docValuesGen == -1) {
-            if (baseProducer == null) {
-              // the base producer gets all the fields, so the Codec can validate properly
-              baseProducer = segDocValues.getDocValuesProducer(docValuesGen, si, IOContext.READ, dir, dvFormat, fieldInfos);
-              dvGens.add(docValuesGen);
-              dvProducers.add(baseProducer);
-            }
-            dvProducersByField.put(fi.name, baseProducer);
-          } else {
-            assert !dvGens.contains(docValuesGen);
-            final DocValuesProducer dvp = segDocValues.getDocValuesProducer(docValuesGen, si, IOContext.READ, dir, dvFormat, new FieldInfos(new FieldInfo[] { fi }));
-            dvGens.add(docValuesGen);
-            dvProducers.add(dvp);
-            dvProducersByField.put(fi.name, dvp);
-          }
-        }
-      } else {
-        // For pre-4.9 indexes, especially with doc-values updates, multiple
-        // FieldInfos could belong to the same dvGen. Therefore need to make sure
-        // we initialize each DocValuesProducer once per gen.
-        Map<Long,List<FieldInfo>> genInfos = new HashMap<>();
-        for (FieldInfo fi : fieldInfos) {
-          if (!fi.hasDocValues()) {
-            continue;
-          }
-          List<FieldInfo> genFieldInfos = genInfos.get(fi.getDocValuesGen());
-          if (genFieldInfos == null) {
-            genFieldInfos = new ArrayList<>();
-            genInfos.put(fi.getDocValuesGen(), genFieldInfos);
-          }
-          genFieldInfos.add(fi);
+      DocValuesProducer baseProducer = null;
+      for (FieldInfo fi : fieldInfos) {
+        if (!fi.hasDocValues()) {
+          continue;
         }
-      
-        for (Map.Entry<Long,List<FieldInfo>> e : genInfos.entrySet()) {
-          long docValuesGen = e.getKey();
-          List<FieldInfo> infos = e.getValue();
-          final DocValuesProducer dvp;
-          if (docValuesGen == -1) {
-            // we need to send all FieldInfos to gen=-1, but later we need to
-            // record the DVP only for the "true" gen=-1 fields (not updated)
-            dvp = segDocValues.getDocValuesProducer(docValuesGen, si, IOContext.READ, dir, dvFormat, fieldInfos);
-          } else {
-            dvp = segDocValues.getDocValuesProducer(docValuesGen, si, IOContext.READ, dir, dvFormat, new FieldInfos(infos.toArray(new FieldInfo[infos.size()])));
+        long docValuesGen = fi.getDocValuesGen();
+        if (docValuesGen == -1) {
+          if (baseProducer == null) {
+            // the base producer gets all the fields, so the Codec can validate properly
+            baseProducer = segDocValues.getDocValuesProducer(docValuesGen, si, IOContext.READ, dir, dvFormat, fieldInfos);
+            dvGens.add(docValuesGen);
+            dvProducers.add(baseProducer);
           }
+          dvProducersByField.put(fi.name, baseProducer);
+        } else {
+          assert !dvGens.contains(docValuesGen);
+          final DocValuesProducer dvp = segDocValues.getDocValuesProducer(docValuesGen, si, IOContext.READ, dir, dvFormat, new FieldInfos(new FieldInfo[] { fi }));
           dvGens.add(docValuesGen);
           dvProducers.add(dvp);
-          for (FieldInfo fi : infos) {
-            dvProducersByField.put(fi.name, dvp);
-          }
+          dvProducersByField.put(fi.name, dvp);
         }
       }
       success = true;
diff --git a/lucene/core/src/java/org/apache/lucene/index/SegmentInfo.java b/lucene/core/src/java/org/apache/lucene/index/SegmentInfo.java
index 394b8df..2a8f580 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SegmentInfo.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SegmentInfo.java
@@ -100,7 +100,7 @@ public final class SegmentInfo {
     this.codec = codec;
     this.diagnostics = diagnostics;
     this.id = id;
-    if (id != null && id.length != StringHelper.ID_LENGTH) {
+    if (id.length != StringHelper.ID_LENGTH) {
       throw new IllegalArgumentException("invalid id: " + Arrays.toString(id));
     }
   }
@@ -223,7 +223,7 @@ public final class SegmentInfo {
 
   /** Return the id that uniquely identifies this segment. */
   public byte[] getId() {
-    return id == null ? null : id.clone();
+    return id.clone();
   }
 
   private Set<String> setFiles;
diff --git a/lucene/core/src/java/org/apache/lucene/index/SegmentInfos.java b/lucene/core/src/java/org/apache/lucene/index/SegmentInfos.java
index 2210cbf..58abc36 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SegmentInfos.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SegmentInfos.java
@@ -119,18 +119,6 @@ import org.apache.lucene.util.StringHelper;
  */
 public final class SegmentInfos implements Cloneable, Iterable<SegmentCommitInfo> {
 
-  /** The file format version for the segments_N codec header, up to 4.5. */
-  public static final int VERSION_40 = 0;
-
-  /** The file format version for the segments_N codec header, since 4.6+. */
-  public static final int VERSION_46 = 1;
-  
-  /** The file format version for the segments_N codec header, since 4.8+ */
-  public static final int VERSION_48 = 2;
-  
-  /** The file format version for the segments_N codec header, since 4.9+ */
-  public static final int VERSION_49 = 3;
-
   /** The file format version for the segments_N codec header, since 5.0+ */
   public static final int VERSION_50 = 4;
 
@@ -262,9 +250,9 @@ public final class SegmentInfos implements Cloneable, Iterable<SegmentCommitInfo
   }
 
   /** Since Lucene 5.0, every commit (segments_N) writes a unique id.  This will
-   *  return that id, or null if this commit was prior to 5.0. */
+   *  return that id */
   public byte[] getId() {
-    return id == null ? null : id.clone();
+    return id.clone();
   }
 
   /**
@@ -286,15 +274,10 @@ public final class SegmentInfos implements Cloneable, Iterable<SegmentCommitInfo
       if (magic != CodecUtil.CODEC_MAGIC) {
         throw new IndexFormatTooOldException(input, magic, CodecUtil.CODEC_MAGIC, CodecUtil.CODEC_MAGIC);
       }
-      // 4.0+
-      int format = CodecUtil.checkHeaderNoMagic(input, "segments", VERSION_40, VERSION_50);
-      // 5.0+
-      byte id[] = null;
-      if (format >= VERSION_50) {
-        id = new byte[StringHelper.ID_LENGTH];
-        input.readBytes(id, 0, id.length);
-        CodecUtil.checkIndexHeaderSuffix(input, Long.toString(generation, Character.MAX_RADIX));
-      }
+      CodecUtil.checkHeaderNoMagic(input, "segments", VERSION_50, VERSION_50);
+      byte id[] = new byte[StringHelper.ID_LENGTH];
+      input.readBytes(id, 0, id.length);
+      CodecUtil.checkIndexHeaderSuffix(input, Long.toString(generation, Character.MAX_RADIX));
       
       SegmentInfos infos = new SegmentInfos();
       infos.id = id;
@@ -309,18 +292,12 @@ public final class SegmentInfos implements Cloneable, Iterable<SegmentCommitInfo
       for (int seg = 0; seg < numSegments; seg++) {
         String segName = input.readString();
         final byte segmentID[];
-        if (format >= VERSION_50) {
-          byte hasID = input.readByte();
-          if (hasID == 1) {
-            segmentID = new byte[StringHelper.ID_LENGTH];
-            input.readBytes(segmentID, 0, segmentID.length);
-          } else if (hasID == 0) {
-            segmentID = null; // 4.x segment, doesn't have an ID
-          } else {
-            throw new CorruptIndexException("invalid hasID byte, got: " + hasID, input);
-          }
+        byte hasID = input.readByte();
+        if (hasID == 1) {
+          segmentID = new byte[StringHelper.ID_LENGTH];
+          input.readBytes(segmentID, 0, segmentID.length);
         } else {
-          segmentID = null;
+          throw new CorruptIndexException("invalid hasID byte, got: " + hasID, input);
         }
         Codec codec = Codec.forName(input.readString());
         SegmentInfo info = codec.segmentInfoFormat().read(directory, segName, segmentID, IOContext.READ);
@@ -330,65 +307,26 @@ public final class SegmentInfos implements Cloneable, Iterable<SegmentCommitInfo
         if (delCount < 0 || delCount > info.getDocCount()) {
           throw new CorruptIndexException("invalid deletion count: " + delCount + " vs docCount=" + info.getDocCount(), input);
         }
-        long fieldInfosGen = -1;
-        if (format >= VERSION_46) {
-          fieldInfosGen = input.readLong();
-        }
-        long dvGen = -1;
-        if (format >= VERSION_49) {
-          dvGen = input.readLong();
-        } else {
-          dvGen = fieldInfosGen;
-        }
+        long fieldInfosGen = input.readLong();
+        long dvGen = input.readLong();
         SegmentCommitInfo siPerCommit = new SegmentCommitInfo(info, delCount, delGen, fieldInfosGen, dvGen);
-        if (format >= VERSION_46) {
-          if (format < VERSION_49) {
-            // Recorded per-generation files, which were buggy (see
-            // LUCENE-5636). We need to read and keep them so we continue to
-            // reference those files. Unfortunately it means that the files will
-            // be referenced even if the fields are updated again, until the
-            // segment is merged.
-            final int numGensUpdatesFiles = input.readInt();
-            final Map<Long,Set<String>> genUpdatesFiles;
-            if (numGensUpdatesFiles == 0) {
-              genUpdatesFiles = Collections.emptyMap();
-            } else {
-              genUpdatesFiles = new HashMap<>(numGensUpdatesFiles);
-              for (int i = 0; i < numGensUpdatesFiles; i++) {
-                genUpdatesFiles.put(input.readLong(), input.readStringSet());
-              }
-            }
-            siPerCommit.setGenUpdatesFiles(genUpdatesFiles);
-          } else {
-            siPerCommit.setFieldInfosFiles(input.readStringSet());
-            final Map<Integer,Set<String>> dvUpdateFiles;
-            final int numDVFields = input.readInt();
-            if (numDVFields == 0) {
-              dvUpdateFiles = Collections.emptyMap();
-            } else {
-              dvUpdateFiles = new HashMap<>(numDVFields);
-              for (int i = 0; i < numDVFields; i++) {
-                dvUpdateFiles.put(input.readInt(), input.readStringSet());
-              }
-            }
-            siPerCommit.setDocValuesUpdatesFiles(dvUpdateFiles);
+        siPerCommit.setFieldInfosFiles(input.readStringSet());
+        final Map<Integer,Set<String>> dvUpdateFiles;
+        final int numDVFields = input.readInt();
+        if (numDVFields == 0) {
+          dvUpdateFiles = Collections.emptyMap();
+        } else {
+          dvUpdateFiles = new HashMap<>(numDVFields);
+          for (int i = 0; i < numDVFields; i++) {
+            dvUpdateFiles.put(input.readInt(), input.readStringSet());
           }
         }
+        siPerCommit.setDocValuesUpdatesFiles(dvUpdateFiles);
         infos.add(siPerCommit);
       }
       infos.userData = input.readStringStringMap();
 
-      if (format >= VERSION_48) {
-        CodecUtil.checkFooter(input);
-      } else {
-        final long checksumNow = input.getChecksum();
-        final long checksumThen = input.readLong();
-        if (checksumNow != checksumThen) {
-          throw new CorruptIndexException("checksum failed (hardware problem?) : expected=" + Long.toHexString(checksumThen) +  
-                                          " actual=" + Long.toHexString(checksumNow), input);
-        }
-        CodecUtil.checkEOF(input);
-      }
+      CodecUtil.checkFooter(input);
 
       return infos;
     }
diff --git a/lucene/core/src/java/org/apache/lucene/util/Version.java b/lucene/core/src/java/org/apache/lucene/util/Version.java
index 85ad6ca..7601fc1 100644
--- a/lucene/core/src/java/org/apache/lucene/util/Version.java
+++ b/lucene/core/src/java/org/apache/lucene/util/Version.java
@@ -33,153 +33,6 @@ import java.util.Locale;
 public final class Version {
 
   /**
-   * Match settings and bugs in Lucene's 4.0.0-ALPHA release.
-   * @deprecated (5.0) Use latest
-   */
-  @Deprecated
-  public static final Version LUCENE_4_0_0_ALPHA = new Version(4, 0, 0, 0);
-
-  /**
-   * Match settings and bugs in Lucene's 4.0.0-BETA release.
-   * @deprecated (5.0) Use latest
-   */
-  @Deprecated
-  public static final Version LUCENE_4_0_0_BETA = new Version(4, 0, 0, 1);
-
-  /**
-   * Match settings and bugs in Lucene's 4.0.0 release.
-   * @deprecated (5.0) Use latest
-   */
-  @Deprecated
-  public static final Version LUCENE_4_0_0 = new Version(4, 0, 0, 2);
-  
-  /**
-   * Match settings and bugs in Lucene's 4.1.0 release.
-   * @deprecated (5.0) Use latest
-   */
-  @Deprecated
-  public static final Version LUCENE_4_1_0 = new Version(4, 1, 0);
-
-  /**
-   * Match settings and bugs in Lucene's 4.2.0 release.
-   * @deprecated (5.0) Use latest
-   */
-  @Deprecated
-  public static final Version LUCENE_4_2_0 = new Version(4, 2, 0);
-
-  /**
-   * Match settings and bugs in Lucene's 4.2.1 release.
-   * @deprecated (5.0) Use latest
-   */
-  @Deprecated
-  public static final Version LUCENE_4_2_1 = new Version(4, 2, 1);
-
-  /**
-   * Match settings and bugs in Lucene's 4.3.0 release.
-   * @deprecated (5.0) Use latest
-   */
-  @Deprecated
-  public static final Version LUCENE_4_3_0 = new Version(4, 3, 0);
-
-  /**
-   * Match settings and bugs in Lucene's 4.3.1 release.
-   * @deprecated (5.0) Use latest
-   */
-  @Deprecated
-  public static final Version LUCENE_4_3_1 = new Version(4, 3, 1);
-
-  /**
-   * Match settings and bugs in Lucene's 4.4.0 release.
-   * @deprecated (5.0) Use latest
-   */
-  @Deprecated
-  public static final Version LUCENE_4_4_0 = new Version(4, 4, 0);
-
-  /**
-   * Match settings and bugs in Lucene's 4.5.0 release.
-   * @deprecated (5.0) Use latest
-   */
-  @Deprecated
-  public static final Version LUCENE_4_5_0 = new Version(4, 5, 0);
-
-  /**
-   * Match settings and bugs in Lucene's 4.5.1 release.
-   * @deprecated (5.0) Use latest
-   */
-  @Deprecated
-  public static final Version LUCENE_4_5_1 = new Version(4, 5, 1);
-
-  /**
-   * Match settings and bugs in Lucene's 4.6.0 release.
-   * @deprecated (5.0) Use latest
-   */
-  @Deprecated
-  public static final Version LUCENE_4_6_0 = new Version(4, 6, 0);
-
-  /**
-   * Match settings and bugs in Lucene's 4.6.1 release.
-   * @deprecated (5.0) Use latest
-   */
-  @Deprecated
-  public static final Version LUCENE_4_6_1 = new Version(4, 6, 1);
-  
-  /**
-   * Match settings and bugs in Lucene's 4.7.0 release.
-   * @deprecated (5.0) Use latest
-   */
-  @Deprecated
-  public static final Version LUCENE_4_7_0 = new Version(4, 7, 0);
-
-  /**
-   * Match settings and bugs in Lucene's 4.7.1 release.
-   * @deprecated (5.0) Use latest
-   */
-  @Deprecated
-  public static final Version LUCENE_4_7_1 = new Version(4, 7, 1);
-
-  /**
-   * Match settings and bugs in Lucene's 4.7.2 release.
-   * @deprecated (5.0) Use latest
-   */
-  @Deprecated
-  public static final Version LUCENE_4_7_2 = new Version(4, 7, 2);
-  
-  /**
-   * Match settings and bugs in Lucene's 4.8.0 release.
-   * @deprecated (5.0) Use latest
-   */
-  @Deprecated
-  public static final Version LUCENE_4_8_0 = new Version(4, 8, 0);
-
-  /**
-   * Match settings and bugs in Lucene's 4.8.1 release.
-   * @deprecated (5.0) Use latest
-   */
-  @Deprecated
-  public static final Version LUCENE_4_8_1 = new Version(4, 8, 1);
-
-  /**
-   * Match settings and bugs in Lucene's 4.9.0 release.
-   * @deprecated (5.0) Use latest
-   */
-  @Deprecated
-  public static final Version LUCENE_4_9_0 = new Version(4, 9, 0);
-  
-  /**
-   * Match settings and bugs in Lucene's 4.10.0 release.
-   * @deprecated (5.0) Use latest
-   */
-  @Deprecated
-  public static final Version LUCENE_4_10_0 = new Version(4, 10, 0);
-
-  /**
-   * Match settings and bugs in Lucene's 4.10.1 release.
-   * @deprecated (5.0) Use latest
-   */
-  @Deprecated
-  public static final Version LUCENE_4_10_1 = new Version(4, 10, 1);
-
-  /**
    * Match settings and bugs in Lucene's 5.0 release.
    * @deprecated (5.0) Use latest
    */
@@ -221,47 +74,6 @@ public final class Version {
   @Deprecated
   public static final Version LUCENE_CURRENT = LATEST;
 
-  /** @deprecated Bad naming of constant; use {@link #LUCENE_4_0_0} instead (this constant actually points to {@link #LUCENE_4_0_0_ALPHA} to match whole 4.0 series). */
-  @Deprecated
-  public static final Version LUCENE_4_0 = LUCENE_4_0_0_ALPHA;
-
-  /** @deprecated Bad naming of constant; use {@link #LUCENE_4_1_0} instead. */
-  @Deprecated
-  public static final Version LUCENE_4_1 = LUCENE_4_1_0;
-
-  /** @deprecated Bad naming of constant; use {@link #LUCENE_4_2_0} instead. */
-  @Deprecated
-  public static final Version LUCENE_4_2 = LUCENE_4_2_0;
-
-  /** @deprecated Bad naming of constant; use {@link #LUCENE_4_3_0} instead. */
-  @Deprecated
-  public static final Version LUCENE_4_3 = LUCENE_4_3_0;
-
-  /** @deprecated Bad naming of constant; use {@link #LUCENE_4_4_0} instead. */
-  @Deprecated
-  public static final Version LUCENE_4_4 = LUCENE_4_4_0;
-
-  /** @deprecated Bad naming of constant; use {@link #LUCENE_4_5_0} instead. */
-  @Deprecated
-  public static final Version LUCENE_4_5 = LUCENE_4_5_0;
-
-  /** @deprecated Bad naming of constant; use {@link #LUCENE_4_6_0} instead. */
-  @Deprecated
-  public static final Version LUCENE_4_6 = LUCENE_4_6_0;
-
-  /** @deprecated Bad naming of constant; use {@link #LUCENE_4_7_0} instead. */
-  @Deprecated
-  public static final Version LUCENE_4_7 = LUCENE_4_7_0;
-
-  /** @deprecated Bad naming of constant; use {@link #LUCENE_4_8_0} instead. */
-  @Deprecated
-  public static final Version LUCENE_4_8 = LUCENE_4_8_0;
-
-  /** @deprecated Bad naming of constant; use {@link #LUCENE_4_9_0} instead. */
-  @Deprecated
-  public static final Version LUCENE_4_9 = LUCENE_4_9_0;
-
-
   /**
    * Parse a version number of the form {@code "major.minor.bugfix.prerelease"}.
    *
@@ -350,12 +162,6 @@ public final class Version {
       case "LATEST":
       case "LUCENE_CURRENT":
         return LATEST;
-      case "LUCENE_4_0_0":
-        return LUCENE_4_0_0;
-      case "LUCENE_4_0_0_ALPHA":
-        return LUCENE_4_0_0_ALPHA;
-      case "LUCENE_4_0_0_BETA":
-        return LUCENE_4_0_0_BETA;
       default:
         version = version
           .replaceFirst("^LUCENE_(\\d+)_(\\d+)_(\\d+)$", "$1.$2.$3")
diff --git a/lucene/core/src/test/org/apache/lucene/util/TestVersion.java b/lucene/core/src/test/org/apache/lucene/util/TestVersion.java
index 15d58f0..a9643a3 100644
--- a/lucene/core/src/test/org/apache/lucene/util/TestVersion.java
+++ b/lucene/core/src/test/org/apache/lucene/util/TestVersion.java
@@ -32,37 +32,25 @@ public class TestVersion extends LuceneTestCase {
         assertTrue("LATEST must be always onOrAfter("+v+")", Version.LATEST.onOrAfter(v));
       }
     }
-    assertTrue(Version.LUCENE_5_0_0.onOrAfter(Version.LUCENE_4_0_0));
-    assertFalse(Version.LUCENE_4_0_0.onOrAfter(Version.LUCENE_5_0_0));
-    assertTrue(Version.LUCENE_4_0_0_ALPHA.onOrAfter(Version.LUCENE_4_0_0_ALPHA));
-    assertTrue(Version.LUCENE_4_0_0_BETA.onOrAfter(Version.LUCENE_4_0_0_ALPHA));
-    assertTrue(Version.LUCENE_4_0_0.onOrAfter(Version.LUCENE_4_0_0_ALPHA));
-    assertTrue(Version.LUCENE_4_0_0.onOrAfter(Version.LUCENE_4_0_0_BETA));
+    assertTrue(Version.LUCENE_6_0_0.onOrAfter(Version.LUCENE_5_0_0));;
   }
 
   public void testToString() {
-    assertEquals("4.2.0", Version.LUCENE_4_2_0.toString());
-    assertEquals("4.2.0", Version.LUCENE_4_2.toString());
-    assertEquals("4.2.1", Version.LUCENE_4_2_1.toString());
-    assertEquals("4.0.0", Version.LUCENE_4_0_0_ALPHA.toString());
-    assertEquals("4.0.0.1", Version.LUCENE_4_0_0_BETA.toString());
-    assertEquals("4.0.0.2", Version.LUCENE_4_0_0.toString());
+    assertEquals("5.0.0", Version.LUCENE_5_0_0.toString());
+    assertEquals("6.0.0", Version.LUCENE_6_0_0.toString());
   }
 
   public void testParseLeniently() throws Exception {
-    assertEquals(Version.LUCENE_4_9_0, Version.parseLeniently("LUCENE_49"));
-    assertEquals(Version.LUCENE_4_9_0, Version.parseLeniently("LUCENE_4_9"));
-    assertEquals(Version.LUCENE_4_9_0, Version.parseLeniently("LUCENE_4_9_0"));
-    assertEquals(Version.LUCENE_4_9_0, Version.parseLeniently("lucene_49"));
-    assertEquals(Version.LUCENE_4_9_0, Version.parseLeniently("Lucene_4_9"));
-    assertEquals(Version.LUCENE_4_9_0, Version.parseLeniently("Lucene_4_9_0"));
-    assertEquals(Version.LUCENE_4_10_0, Version.parseLeniently("LUCENE_4_10"));
-    assertEquals(Version.LUCENE_4_10_0, Version.parseLeniently("LUCENE_4_10_0"));
-    assertEquals(Version.LUCENE_4_0_0_ALPHA, Version.parseLeniently("4.0"));
-    assertEquals(Version.LUCENE_4_0_0_ALPHA, Version.parseLeniently("4.0.0"));
-    assertEquals(Version.LUCENE_4_0_0_ALPHA, Version.parseLeniently("LUCENE_40"));
-    assertEquals(Version.LUCENE_4_0_0_ALPHA, Version.parseLeniently("LUCENE_4_0"));
-    assertEquals(Version.LUCENE_4_0_0, Version.parseLeniently("LUCENE_4_0_0"));
+    assertEquals(Version.LUCENE_5_0_0, Version.parseLeniently("5.0"));
+    assertEquals(Version.LUCENE_5_0_0, Version.parseLeniently("5.0.0"));
+    assertEquals(Version.LUCENE_5_0_0, Version.parseLeniently("LUCENE_50"));
+    assertEquals(Version.LUCENE_5_0_0, Version.parseLeniently("LUCENE_5_0"));
+    assertEquals(Version.LUCENE_5_0_0, Version.parseLeniently("LUCENE_5_0_0"));
+    assertEquals(Version.LUCENE_6_0_0, Version.parseLeniently("6.0"));
+    assertEquals(Version.LUCENE_6_0_0, Version.parseLeniently("6.0.0"));
+    assertEquals(Version.LUCENE_6_0_0, Version.parseLeniently("LUCENE_60"));
+    assertEquals(Version.LUCENE_6_0_0, Version.parseLeniently("LUCENE_6_0"));
+    assertEquals(Version.LUCENE_6_0_0, Version.parseLeniently("LUCENE_6_0_0"));
     assertEquals(Version.LATEST, Version.parseLeniently("LATEST"));
     assertEquals(Version.LATEST, Version.parseLeniently("latest"));
     assertEquals(Version.LATEST, Version.parseLeniently("LUCENE_CURRENT"));
@@ -78,18 +66,18 @@ public class TestVersion extends LuceneTestCase {
       assertTrue(pe.getMessage().contains("LUCENE"));
     }
     try {
-      Version.parseLeniently("LUCENE_410");
+      Version.parseLeniently("LUCENE_610");
       fail();
     } catch (ParseException pe) {
       // pass
-      assertTrue(pe.getMessage().contains("LUCENE_410"));
+      assertTrue(pe.getMessage().contains("LUCENE_610"));
     }
     try {
-      Version.parseLeniently("LUCENE41");
+      Version.parseLeniently("LUCENE61");
       fail();
     } catch (ParseException pe) {
       // pass
-      assertTrue(pe.getMessage().contains("LUCENE41"));
+      assertTrue(pe.getMessage().contains("LUCENE61"));
     }
     try {
       Version.parseLeniently("LUCENE_6.0.0");
@@ -115,119 +103,113 @@ public class TestVersion extends LuceneTestCase {
   }
 
   public void testParse() throws Exception {
+    assertEquals(Version.LUCENE_6_0_0, Version.parse("6.0.0"));
     assertEquals(Version.LUCENE_5_0_0, Version.parse("5.0.0"));
-    assertEquals(Version.LUCENE_4_1_0, Version.parse("4.1"));
-    assertEquals(Version.LUCENE_4_1_0, Version.parse("4.1.0"));
-    assertEquals(Version.LUCENE_4_0_0_ALPHA, Version.parse("4.0.0"));
-    assertEquals(Version.LUCENE_4_0_0_BETA, Version.parse("4.0.0.1"));
-    assertEquals(Version.LUCENE_4_0_0, Version.parse("4.0.0.2"));
     
     // Version does not pass judgement on the major version:
     assertEquals(1, Version.parse("1.0").major);
-    assertEquals(6, Version.parse("6.0.0").major);
+    assertEquals(7, Version.parse("7.0.0").major);
   }
 
   public void testForwardsCompatibility() throws Exception {
-    assertTrue(Version.parse("4.7.10").onOrAfter(Version.LUCENE_4_7_2));
-    assertTrue(Version.parse("4.20.0").onOrAfter(Version.LUCENE_4_8_1));
     assertTrue(Version.parse("5.10.20").onOrAfter(Version.LUCENE_5_0_0));
   }
 
   public void testParseExceptions() {
     try {
-      Version.parse("LUCENE_4_0_0");
+      Version.parse("LUCENE_6_0_0");
       fail();
     } catch (ParseException pe) {
       // pass
-      assertTrue(pe.getMessage().contains("LUCENE_4_0_0"));
+      assertTrue(pe.getMessage().contains("LUCENE_6_0_0"));
     }
 
     try {
-      Version.parse("4.256");
+      Version.parse("6.256");
       fail();
     } catch (ParseException pe) {
       // pass
-      assertTrue(pe.getMessage().contains("4.256"));
+      assertTrue(pe.getMessage().contains("6.256"));
     }
 
     try {
-      Version.parse("4.-1");
+      Version.parse("6.-1");
       fail();
     } catch (ParseException pe) {
       // pass
-      assertTrue(pe.getMessage().contains("4.-1"));
+      assertTrue(pe.getMessage().contains("6.-1"));
     }
 
     try {
-      Version.parse("4.1.256");
+      Version.parse("6.1.256");
       fail();
     } catch (ParseException pe) {
       // pass
-      assertTrue(pe.getMessage().contains("4.1.256"));
+      assertTrue(pe.getMessage().contains("6.1.256"));
     }
 
     try {
-      Version.parse("4.1.-1");
+      Version.parse("6.1.-1");
       fail();
     } catch (ParseException pe) {
       // pass
-      assertTrue(pe.getMessage().contains("4.1.-1"));
+      assertTrue(pe.getMessage().contains("6.1.-1"));
     }
 
     try {
-      Version.parse("4.1.1.3");
+      Version.parse("6.1.1.3");
       fail();
     } catch (ParseException pe) {
       // pass
-      assertTrue(pe.getMessage().contains("4.1.1.3"));
+      assertTrue(pe.getMessage().contains("6.1.1.3"));
     }
 
     try {
-      Version.parse("4.1.1.-1");
+      Version.parse("6.1.1.-1");
       fail();
     } catch (ParseException pe) {
       // pass
-      assertTrue(pe.getMessage().contains("4.1.1.-1"));
+      assertTrue(pe.getMessage().contains("6.1.1.-1"));
     }
 
     try {
-      Version.parse("4.1.1.1");
+      Version.parse("6.1.1.1");
       fail();
     } catch (ParseException pe) {
       // pass
-      assertTrue(pe.getMessage().contains("4.1.1.1"));
+      assertTrue(pe.getMessage().contains("6.1.1.1"));
     }
 
     try {
-      Version.parse("4.1.1.2");
+      Version.parse("6.1.1.2");
       fail();
     } catch (ParseException pe) {
       // pass
-      assertTrue(pe.getMessage().contains("4.1.1.2"));
+      assertTrue(pe.getMessage().contains("6.1.1.2"));
     }
 
     try {
-      Version.parse("4.0.0.0");
+      Version.parse("6.0.0.0");
       fail();
     } catch (ParseException pe) {
       // pass
-      assertTrue(pe.getMessage().contains("4.0.0.0"));
+      assertTrue(pe.getMessage().contains("6.0.0.0"));
     }
 
     try {
-      Version.parse("4.0.0.1.42");
+      Version.parse("6.0.0.1.42");
       fail();
     } catch (ParseException pe) {
       // pass
-      assertTrue(pe.getMessage().contains("4.0.0.1.42"));
+      assertTrue(pe.getMessage().contains("6.0.0.1.42"));
     }
 
     try {
-      Version.parse("4..0.1");
+      Version.parse("6..0.1");
       fail();
     } catch (ParseException pe) {
       // pass
-      assertTrue(pe.getMessage().contains("4..0.1"));
+      assertTrue(pe.getMessage().contains("6..0.1"));
     }
   }
   
diff --git a/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java b/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java
index d7529ff..28fefe4 100644
--- a/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java
+++ b/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java
@@ -31,7 +31,6 @@ import org.apache.lucene.analysis.AnalyzerWrapper;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.ngram.EdgeNGramTokenFilter;
-import org.apache.lucene.analysis.ngram.Lucene43EdgeNGramTokenFilter;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.document.BinaryDocValuesField;
@@ -296,12 +295,7 @@ public class AnalyzingInfixSuggester extends Lookup implements Closeable {
       protected TokenStreamComponents wrapComponents(String fieldName, TokenStreamComponents components) {
         if (fieldName.equals("textgrams") && minPrefixChars > 0) {
           // TODO: should use an EdgeNGramTokenFilterFactory here
-          TokenFilter filter;
-          if (matchVersion.onOrAfter(Version.LUCENE_4_4_0)) {
-            filter = new EdgeNGramTokenFilter(components.getTokenStream(), 1, minPrefixChars);
-          } else {
-            filter = new Lucene43EdgeNGramTokenFilter(components.getTokenStream(), 1, minPrefixChars);
-          }
+          TokenFilter filter = new EdgeNGramTokenFilter(components.getTokenStream(), 1, minPrefixChars);
           return new TokenStreamComponents(components.getTokenizer(), filter);
         } else {
           return components;
diff --git a/solr/contrib/morphlines-core/src/test-files/solr/collection1/conf/schema.xml b/solr/contrib/morphlines-core/src/test-files/solr/collection1/conf/schema.xml
index f46885f..eb02af4 100644
--- a/solr/contrib/morphlines-core/src/test-files/solr/collection1/conf/schema.xml
+++ b/solr/contrib/morphlines-core/src/test-files/solr/collection1/conf/schema.xml
@@ -899,9 +899,8 @@
     <!-- Thai -->
     <fieldType name="text_th" class="solr.TextField" positionIncrementGap="100">
       <analyzer> 
-        <tokenizer class="solr.StandardTokenizerFactory"/>
+        <tokenizer class="solr.ThaiTokenizerFactory"/>
         <filter class="solr.LowerCaseFilterFactory"/>
-        <filter class="solr.ThaiWordFilterFactory"/>
         <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_th.txt"/>
       </analyzer>
     </fieldType>
diff --git a/solr/contrib/morphlines-core/src/test-files/solr/minimr/conf/schema.xml b/solr/contrib/morphlines-core/src/test-files/solr/minimr/conf/schema.xml
index 74c061b..a7e90be 100644
--- a/solr/contrib/morphlines-core/src/test-files/solr/minimr/conf/schema.xml
+++ b/solr/contrib/morphlines-core/src/test-files/solr/minimr/conf/schema.xml
@@ -913,9 +913,8 @@
     <!-- Thai -->
     <fieldType name="text_th" class="solr.TextField" positionIncrementGap="100">
       <analyzer> 
-        <tokenizer class="solr.StandardTokenizerFactory"/>
+        <tokenizer class="solr.ThaiTokenizerFactory"/>
         <filter class="solr.LowerCaseFilterFactory"/>
-        <filter class="solr.ThaiWordFilterFactory"/>
         <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_th.txt"/>
       </analyzer>
     </fieldType>
diff --git a/solr/contrib/morphlines-core/src/test-files/solr/mrunit/conf/schema.xml b/solr/contrib/morphlines-core/src/test-files/solr/mrunit/conf/schema.xml
index c5beada..9d35f16 100644
--- a/solr/contrib/morphlines-core/src/test-files/solr/mrunit/conf/schema.xml
+++ b/solr/contrib/morphlines-core/src/test-files/solr/mrunit/conf/schema.xml
@@ -913,9 +913,8 @@
     <!-- Thai -->
     <fieldType name="text_th" class="solr.TextField" positionIncrementGap="100">
       <analyzer> 
-        <tokenizer class="solr.StandardTokenizerFactory"/>
+        <tokenizer class="solr.ThaiTokenizerFactory"/>
         <filter class="solr.LowerCaseFilterFactory"/>
-        <filter class="solr.ThaiWordFilterFactory"/>
         <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_th.txt" />
       </analyzer>
     </fieldType>
diff --git a/solr/contrib/morphlines-core/src/test-files/solr/solrcelltest/collection1/conf/schema.xml b/solr/contrib/morphlines-core/src/test-files/solr/solrcelltest/collection1/conf/schema.xml
index 46b3fb0..ae2d709 100644
--- a/solr/contrib/morphlines-core/src/test-files/solr/solrcelltest/collection1/conf/schema.xml
+++ b/solr/contrib/morphlines-core/src/test-files/solr/solrcelltest/collection1/conf/schema.xml
@@ -866,9 +866,8 @@
     <!-- Thai -->
     <fieldType name="text_th" class="solr.TextField" positionIncrementGap="100">
       <analyzer> 
-        <tokenizer class="solr.StandardTokenizerFactory"/>
+        <tokenizer class="solr.ThaiTokenizerFactory"/>
         <filter class="solr.LowerCaseFilterFactory"/>
-        <filter class="solr.ThaiWordFilterFactory"/>
         <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_th.txt"/>
       </analyzer>
     </fieldType>
diff --git a/solr/core/src/java/org/apache/solr/core/SolrConfig.java b/solr/core/src/java/org/apache/solr/core/SolrConfig.java
index 0c86653..3203366 100644
--- a/solr/core/src/java/org/apache/solr/core/SolrConfig.java
+++ b/solr/core/src/java/org/apache/solr/core/SolrConfig.java
@@ -172,20 +172,8 @@ public class SolrConfig extends Config {
     // Old indexDefaults and mainIndex sections are deprecated and fails fast for luceneMatchVersion=>LUCENE_4_0_0.
     // For older solrconfig.xml's we allow the old sections, but never mixed with the new <indexConfig>
     boolean hasDeprecatedIndexConfig = (getNode("indexDefaults", false) != null) || (getNode("mainIndex", false) != null);
-    boolean hasNewIndexConfig = getNode("indexConfig", false) != null;
     if(hasDeprecatedIndexConfig){
-      if(luceneMatchVersion.onOrAfter(Version.LUCENE_4_0_0_ALPHA)) {
-        throw new SolrException(ErrorCode.FORBIDDEN, "<indexDefaults> and <mainIndex> configuration sections are discontinued. Use <indexConfig> instead.");
-      } else {
-        // Still allow the old sections for older LuceneMatchVersion's
-        if(hasNewIndexConfig) {
-          throw new SolrException(ErrorCode.FORBIDDEN, "Cannot specify both <indexDefaults>, <mainIndex> and <indexConfig> at the same time. Please use <indexConfig> only.");
-        }
-        log.warn("<indexDefaults> and <mainIndex> configuration sections are deprecated and will fail for luceneMatchVersion=LUCENE_4_0_0 and later. Please use <indexConfig> instead.");
-        defaultIndexConfig = new SolrIndexConfig(this, "indexDefaults", null);
-        mainIndexConfig = new SolrIndexConfig(this, "mainIndex", defaultIndexConfig);
-        indexConfigPrefix = "mainIndex";
-      }
+      throw new SolrException(ErrorCode.FORBIDDEN, "<indexDefaults> and <mainIndex> configuration sections are discontinued. Use <indexConfig> instead.");
     } else {
       defaultIndexConfig = mainIndexConfig = null;
       indexConfigPrefix = "indexConfig";
diff --git a/solr/core/src/java/org/apache/solr/schema/FieldTypePluginLoader.java b/solr/core/src/java/org/apache/solr/schema/FieldTypePluginLoader.java
index 0347fff..746d90b 100644
--- a/solr/core/src/java/org/apache/solr/schema/FieldTypePluginLoader.java
+++ b/solr/core/src/java/org/apache/solr/schema/FieldTypePluginLoader.java
@@ -398,10 +398,10 @@ public final class FieldTypePluginLoader
     Version version = (configuredVersion != null) ?
             Config.parseLuceneVersionString(configuredVersion) : schema.getDefaultLuceneMatchVersion();
 
-    if (!version.onOrAfter(Version.LUCENE_4_0_0_ALPHA)) {
+    if (!version.onOrAfter(Version.LUCENE_6_0_0)) {
       log.warn(pluginClassName + " is using deprecated " + version +
-        " emulation. You should at some point declare and reindex to at least 4.0, because " +
-        "3.x emulation is deprecated and will be removed in 5.0");
+        " emulation. You should at some point declare and reindex to at least 6.0, because " +
+        "5.x emulation is deprecated and will be removed in 7.0");
     }
     return version;
   }
diff --git a/solr/core/src/java/org/apache/solr/schema/IndexSchema.java b/solr/core/src/java/org/apache/solr/schema/IndexSchema.java
index 1cee8ac..d869cae 100644
--- a/solr/core/src/java/org/apache/solr/schema/IndexSchema.java
+++ b/solr/core/src/java/org/apache/solr/schema/IndexSchema.java
@@ -498,9 +498,6 @@ public class IndexSchema {
         similarityFactory = new DefaultSimilarityFactory();
         final NamedList similarityParams = new NamedList();
         Version luceneVersion = getDefaultLuceneMatchVersion();
-        if (!luceneVersion.onOrAfter(Version.LUCENE_4_7_0)) {
-          similarityParams.add(DefaultSimilarityFactory.DISCOUNT_OVERLAPS, false);
-        }
         similarityFactory.init(SolrParams.toSolrParams(similarityParams));
       } else {
         isExplicitSimilarity = true;
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestLuceneMatchVersion.java b/solr/core/src/test/org/apache/solr/analysis/TestLuceneMatchVersion.java
index 3ec6500..9c6f5f0 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestLuceneMatchVersion.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestLuceneMatchVersion.java
@@ -49,19 +49,9 @@ public class TestLuceneMatchVersion extends SolrTestCaseJ4 {
     assertEquals(DEFAULT_VERSION, (ana.getTokenizerFactory()).getLuceneMatchVersion());
     assertEquals(DEFAULT_VERSION, (ana.getTokenFilterFactories()[2]).getLuceneMatchVersion());
 
-    type = schema.getFieldType("text40");
-    ana = (TokenizerChain) type.getIndexAnalyzer();
-    assertEquals(Version.LUCENE_4_0_0_ALPHA, (ana.getTokenizerFactory()).getLuceneMatchVersion());
-    assertEquals(Version.LUCENE_5_0_0, (ana.getTokenFilterFactories()[2]).getLuceneMatchVersion());
-
     type = schema.getFieldType("textTurkishAnalyzerDefault");
     Analyzer ana1 = type.getIndexAnalyzer();
     assertTrue(ana1 instanceof TurkishAnalyzer);
     assertEquals(DEFAULT_VERSION, ana1.getVersion());
-
-    type = schema.getFieldType("textTurkishAnalyzer40");
-    ana1 = type.getIndexAnalyzer();
-    assertTrue(ana1 instanceof TurkishAnalyzer);
-    assertEquals(Version.LUCENE_4_0_0_ALPHA, ana1.getVersion());
   }
 }
diff --git a/solr/core/src/test/org/apache/solr/search/similarities/TestNonDefinedSimilarityFactory.java b/solr/core/src/test/org/apache/solr/search/similarities/TestNonDefinedSimilarityFactory.java
index f78e8d4..4dec59c 100644
--- a/solr/core/src/test/org/apache/solr/search/similarities/TestNonDefinedSimilarityFactory.java
+++ b/solr/core/src/test/org/apache/solr/search/similarities/TestNonDefinedSimilarityFactory.java
@@ -40,19 +40,4 @@ public class TestNonDefinedSimilarityFactory extends BaseSimilarityTestCase {
     DefaultSimilarity sim = getSimilarity("text", DefaultSimilarity.class);
     assertEquals(true, sim.getDiscountOverlaps());
   }
-
-  public void test47() throws Exception {
-    System.setProperty("tests.luceneMatchVersion", Version.LUCENE_4_7_0.toString());
-    initCore("solrconfig-basic.xml","schema-tiny.xml");
-    DefaultSimilarity sim = getSimilarity("text", DefaultSimilarity.class);
-    assertEquals(true, sim.getDiscountOverlaps());
-  }
-
-  public void test46() throws Exception {
-    System.setProperty("tests.luceneMatchVersion", Version.LUCENE_4_6_0.toString());
-    initCore("solrconfig-basic.xml","schema-tiny.xml");
-    DefaultSimilarity sim = getSimilarity("text", DefaultSimilarity.class);
-    assertEquals(false, sim.getDiscountOverlaps());
-  }
-
 }
diff --git a/solr/example/example-DIH/solr/db/conf/schema.xml b/solr/example/example-DIH/solr/db/conf/schema.xml
index 0e2badc..99b301c 100755
--- a/solr/example/example-DIH/solr/db/conf/schema.xml
+++ b/solr/example/example-DIH/solr/db/conf/schema.xml
@@ -1106,9 +1106,8 @@
     <!-- Thai -->
     <fieldType name="text_th" class="solr.TextField" positionIncrementGap="100">
       <analyzer> 
-        <tokenizer class="solr.StandardTokenizerFactory"/>
+        <tokenizer class="solr.ThaiTokenizerFactory"/>
         <filter class="solr.LowerCaseFilterFactory"/>
-        <filter class="solr.ThaiWordFilterFactory"/>
         <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_th.txt" />
       </analyzer>
     </fieldType>
diff --git a/solr/example/example-DIH/solr/mail/conf/schema.xml b/solr/example/example-DIH/solr/mail/conf/schema.xml
index 9f83427..3a63422 100755
--- a/solr/example/example-DIH/solr/mail/conf/schema.xml
+++ b/solr/example/example-DIH/solr/mail/conf/schema.xml
@@ -1025,9 +1025,8 @@
     <!-- Thai -->
     <fieldType name="text_th" class="solr.TextField" positionIncrementGap="100">
       <analyzer> 
-        <tokenizer class="solr.StandardTokenizerFactory"/>
+        <tokenizer class="solr.ThaiTokenizerFactory"/>
         <filter class="solr.LowerCaseFilterFactory"/>
-        <filter class="solr.ThaiWordFilterFactory"/>
         <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_th.txt" />
       </analyzer>
     </fieldType>
diff --git a/solr/example/example-DIH/solr/rss/conf/schema.xml b/solr/example/example-DIH/solr/rss/conf/schema.xml
index c95c6b2..1c611fa 100755
--- a/solr/example/example-DIH/solr/rss/conf/schema.xml
+++ b/solr/example/example-DIH/solr/rss/conf/schema.xml
@@ -1056,9 +1056,8 @@
     <!-- Thai -->
     <fieldType name="text_th" class="solr.TextField" positionIncrementGap="100">
       <analyzer> 
-        <tokenizer class="solr.StandardTokenizerFactory"/>
+        <tokenizer class="solr.ThaiTokenizerFactory"/>
         <filter class="solr.LowerCaseFilterFactory"/>
-        <filter class="solr.ThaiWordFilterFactory"/>
         <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_th.txt" />
       </analyzer>
     </fieldType>
diff --git a/solr/example/example-DIH/solr/solr/conf/schema.xml b/solr/example/example-DIH/solr/solr/conf/schema.xml
index f5b0ed5..04fb4ce 100755
--- a/solr/example/example-DIH/solr/solr/conf/schema.xml
+++ b/solr/example/example-DIH/solr/solr/conf/schema.xml
@@ -1106,9 +1106,8 @@
     <!-- Thai -->
     <fieldType name="text_th" class="solr.TextField" positionIncrementGap="100">
       <analyzer> 
-        <tokenizer class="solr.StandardTokenizerFactory"/>
+        <tokenizer class="solr.ThaiTokenizerFactory"/>
         <filter class="solr.LowerCaseFilterFactory"/>
-        <filter class="solr.ThaiWordFilterFactory"/>
         <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_th.txt" />
       </analyzer>
     </fieldType>
diff --git a/solr/example/example-DIH/solr/tika/conf/schema.xml b/solr/example/example-DIH/solr/tika/conf/schema.xml
index f58e222..efbacea 100755
--- a/solr/example/example-DIH/solr/tika/conf/schema.xml
+++ b/solr/example/example-DIH/solr/tika/conf/schema.xml
@@ -883,9 +883,8 @@
     <!-- Thai -->
     <fieldType name="text_th" class="solr.TextField" positionIncrementGap="100">
       <analyzer> 
-        <tokenizer class="solr.StandardTokenizerFactory"/>
+        <tokenizer class="solr.ThaiTokenizerFactory"/>
         <filter class="solr.LowerCaseFilterFactory"/>
-        <filter class="solr.ThaiWordFilterFactory"/>
       </analyzer>
     </fieldType>
     
diff --git a/solr/example/example-schemaless/solr/collection1/conf/schema.xml b/solr/example/example-schemaless/solr/collection1/conf/schema.xml
index 432c735..168820b 100755
--- a/solr/example/example-schemaless/solr/collection1/conf/schema.xml
+++ b/solr/example/example-schemaless/solr/collection1/conf/schema.xml
@@ -1021,9 +1021,8 @@
     <!-- Thai -->
     <fieldType name="text_th" class="solr.TextField" positionIncrementGap="100">
       <analyzer> 
-        <tokenizer class="solr.StandardTokenizerFactory"/>
+        <tokenizer class="solr.ThaiTokenizerFactory"/>
         <filter class="solr.LowerCaseFilterFactory"/>
-        <filter class="solr.ThaiWordFilterFactory"/>
         <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_th.txt" />
       </analyzer>
     </fieldType>

