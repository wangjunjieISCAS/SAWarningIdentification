GitDiffStart: 54915fac9a822ad621210178dd6054532be988bf | Sat Aug 1 07:48:53 2015 +0000
diff --git a/lucene/CHANGES.txt b/lucene/CHANGES.txt
index 02f68d9..08369dc 100644
--- a/lucene/CHANGES.txt
+++ b/lucene/CHANGES.txt
@@ -141,6 +141,12 @@ New Features
 * LUCENE-6695: Added a new BlendedTermQuery to blend statistics across several
   terms. (Simon Willnauer, Adrien Grand)
 
+* LUCENE-6697: Add experimental range tree doc values format and
+  queries, based on a 1D version of the spatial BKD tree, for a faster
+  and smaller alternative to postings-based numeric and binary term
+  filtering.  Range trees can also handle values larger than 64 bits.
+  (Adrien Grand, Mike McCandless)
+
 API Changes
 
 * LUCENE-6508: Simplify Lock api, there is now just 
diff --git a/lucene/sandbox/src/java/org/apache/lucene/rangetree/GrowingHeapSliceWriter.java b/lucene/sandbox/src/java/org/apache/lucene/rangetree/GrowingHeapSliceWriter.java
new file mode 100644
index 0000000..f1fe7ac
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/rangetree/GrowingHeapSliceWriter.java
@@ -0,0 +1,84 @@
+package org.apache.lucene.rangetree;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.RamUsageEstimator;
+
+final class GrowingHeapSliceWriter implements SliceWriter {
+  long[] values;
+  int[] docIDs;
+  long[] ords;
+  private int nextWrite;
+  final int maxSize;
+
+  public GrowingHeapSliceWriter(int maxSize) {
+    values = new long[16];
+    docIDs = new int[16];
+    ords = new long[16];
+    this.maxSize = maxSize;
+  }
+
+  private int[] growExact(int[] arr, int size) {
+    assert size > arr.length;
+    int[] newArr = new int[size];
+    System.arraycopy(arr, 0, newArr, 0, arr.length);
+    return newArr;
+  }
+
+  private long[] growExact(long[] arr, int size) {
+    assert size > arr.length;
+    long[] newArr = new long[size];
+    System.arraycopy(arr, 0, newArr, 0, arr.length);
+    return newArr;
+  }
+
+  @Override
+  public void append(long value, long ord, int docID) {
+    assert ord == nextWrite;
+    if (values.length == nextWrite) {
+      int nextSize = Math.min(maxSize, ArrayUtil.oversize(nextWrite+1, RamUsageEstimator.NUM_BYTES_INT));
+      assert nextSize > nextWrite: "nextSize=" + nextSize + " vs nextWrite=" + nextWrite;
+      values = growExact(values, nextSize);
+      ords = growExact(ords, nextSize);
+      docIDs = growExact(docIDs, nextSize);
+    }
+    values[nextWrite] = value;
+    ords[nextWrite] = ord;
+    docIDs[nextWrite] = docID;
+    nextWrite++;
+  }
+
+  @Override
+  public SliceReader getReader(long start) {
+    return new HeapSliceReader(values, ords, docIDs, (int) start, nextWrite);
+  }
+
+  @Override
+  public void close() {
+  }
+
+  @Override
+  public void destroy() {
+  }
+
+  @Override
+  public String toString() {
+    return "GrowingHeapSliceWriter(count=" + nextWrite + " alloc=" + values.length + ")";
+  }
+}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/rangetree/HeapSliceReader.java b/lucene/sandbox/src/java/org/apache/lucene/rangetree/HeapSliceReader.java
new file mode 100644
index 0000000..fff15eb
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/rangetree/HeapSliceReader.java
@@ -0,0 +1,60 @@
+package org.apache.lucene.rangetree;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+final class HeapSliceReader implements SliceReader {
+  private int curRead;
+  final long[] values;
+  final long[] ords;
+  final int[] docIDs;
+  final int end;
+
+  HeapSliceReader(long[] values, long[] ords, int[] docIDs, int start, int end) {
+    this.values = values;
+    this.ords = ords;
+    this.docIDs = docIDs;
+    curRead = start-1;
+    this.end = end;
+  }
+
+  @Override
+  public boolean next() {
+    curRead++;
+    return curRead < end;
+  }
+
+  @Override
+  public long value() {
+    return values[curRead];
+  }
+
+  @Override
+  public int docID() {
+    return docIDs[curRead];
+  }
+
+  @Override
+  public long ord() {
+    return ords[curRead];
+  }
+
+  @Override
+  public void close() {
+  }
+}
+
diff --git a/lucene/sandbox/src/java/org/apache/lucene/rangetree/HeapSliceWriter.java b/lucene/sandbox/src/java/org/apache/lucene/rangetree/HeapSliceWriter.java
new file mode 100644
index 0000000..1b5ab43
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/rangetree/HeapSliceWriter.java
@@ -0,0 +1,60 @@
+package org.apache.lucene.rangetree;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+final class HeapSliceWriter implements SliceWriter {
+  final long[] values;
+  final int[] docIDs;
+  final long[] ords;
+  private int nextWrite;
+
+  public HeapSliceWriter(int count) {
+    values = new long[count];
+    docIDs = new int[count];
+    ords = new long[count];
+  }
+
+  @Override
+  public void append(long value, long ord, int docID) {
+    values[nextWrite] = value;
+    ords[nextWrite] = ord;
+    docIDs[nextWrite] = docID;
+    nextWrite++;
+  }
+
+  @Override
+  public SliceReader getReader(long start) {
+    return new HeapSliceReader(values, ords, docIDs, (int) start, values.length);
+  }
+
+  @Override
+  public void close() {
+    if (nextWrite != values.length) {
+      throw new IllegalStateException("only wrote " + nextWrite + " values, but expected " + values.length);
+    }
+  }
+
+  @Override
+  public void destroy() {
+  }
+
+  @Override
+  public String toString() {
+    return "HeapSliceWriter(count=" + values.length + ")";
+  }
+}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/rangetree/NumericRangeTreeQuery.java b/lucene/sandbox/src/java/org/apache/lucene/rangetree/NumericRangeTreeQuery.java
new file mode 100644
index 0000000..d2118b9
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/rangetree/NumericRangeTreeQuery.java
@@ -0,0 +1,159 @@
+package org.apache.lucene.rangetree;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.document.SortedNumericDocValuesField;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.index.SortedNumericDocValues;
+import org.apache.lucene.search.ConstantScoreScorer;
+import org.apache.lucene.search.ConstantScoreWeight;
+import org.apache.lucene.search.DocIdSet;
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.Scorer;
+import org.apache.lucene.search.Weight;
+import org.apache.lucene.util.ToStringUtils;
+
+import java.io.IOException;
+
+/** Finds all previously indexed long values that fall within the specified range.
+ *
+ * <p>The field must be indexed with {@link RangeTreeDocValuesFormat}, and {@link SortedNumericDocValuesField} added per document.
+ *
+ * @lucene.experimental */
+
+public class NumericRangeTreeQuery extends Query {
+  final String field;
+  final Long minValue;
+  final Long maxValue;
+  final boolean minInclusive;
+  final boolean maxInclusive;
+
+  // TODO: sugar for all numeric conversions?
+
+  /** Matches all values in the specified long range. */ 
+  public NumericRangeTreeQuery(String field, Long minValue, boolean minInclusive, Long maxValue, boolean maxInclusive) {
+    this.field = field;
+    this.minInclusive = minInclusive;
+    this.minValue = minValue;
+    this.maxInclusive = maxInclusive;
+    this.maxValue = maxValue;
+  }
+
+  @Override
+  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {
+
+    // I don't use RandomAccessWeight here: it's no good to approximate with "match all docs"; this is an inverted structure and should be
+    // used in the first pass:
+
+    return new ConstantScoreWeight(this) {
+
+      @Override
+      public Scorer scorer(LeafReaderContext context) throws IOException {
+        LeafReader reader = context.reader();
+        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(field);
+        if (sdv == null) {
+          // No docs in this segment had this field
+          return null;
+        }
+
+        if (sdv instanceof RangeTreeSortedNumericDocValues == false) {
+          throw new IllegalStateException("field \"" + field + "\" was not indexed with RangeTreeDocValuesFormat: got: " + sdv);
+        }
+        RangeTreeSortedNumericDocValues treeDV = (RangeTreeSortedNumericDocValues) sdv;
+        RangeTreeReader tree = treeDV.getRangeTreeReader();
+
+        // lower
+        long minBoundIncl = (minValue == null) ? Long.MIN_VALUE : minValue.longValue();
+
+        if (minInclusive == false && minValue != null) {
+          if (minBoundIncl == Long.MAX_VALUE) {
+            return null;
+          }
+          minBoundIncl++;
+        }
+          
+        // upper
+        long maxBoundIncl = (maxValue == null) ? Long.MAX_VALUE : maxValue.longValue();
+        if (maxInclusive == false && maxValue != null) {
+          if (maxBoundIncl == Long.MIN_VALUE) {
+            return null;
+          }
+          maxBoundIncl--;
+        }
+
+        if (maxBoundIncl < minBoundIncl) {
+          return null;
+        }
+
+        DocIdSet result = tree.intersect(minBoundIncl, maxBoundIncl, treeDV.delegate, context.reader().maxDoc());
+
+        final DocIdSetIterator disi = result.iterator();
+
+        return new ConstantScoreScorer(this, score(), disi);
+      }
+    };
+  }
+
+  @Override
+  public int hashCode() {
+    int hash = super.hashCode();
+    if (minValue != null) hash += minValue.hashCode()^0x14fa55fb;
+    if (maxValue != null) hash += maxValue.hashCode()^0x733fa5fe;
+    return hash +
+      (Boolean.valueOf(minInclusive).hashCode()^0x14fa55fb)+
+      (Boolean.valueOf(maxInclusive).hashCode()^0x733fa5fe);
+  }
+
+  @Override
+  public boolean equals(Object other) {
+    if (super.equals(other)) {
+      final NumericRangeTreeQuery q = (NumericRangeTreeQuery) other;
+      return (
+        (q.minValue == null ? minValue == null : q.minValue.equals(minValue)) &&
+        (q.maxValue == null ? maxValue == null : q.maxValue.equals(maxValue)) &&
+        minInclusive == q.minInclusive &&
+        maxInclusive == q.maxInclusive
+      );
+    }
+
+    return false;
+  }
+
+  @Override
+  public String toString(String field) {
+    final StringBuilder sb = new StringBuilder();
+    sb.append(getClass().getSimpleName());
+    sb.append(':');
+    if (this.field.equals(field) == false) {
+      sb.append("field=");
+      sb.append(this.field);
+      sb.append(':');
+    }
+
+    return sb.append(minInclusive ? '[' : '{')
+      .append((minValue == null) ? "*" : minValue.toString())
+      .append(" TO ")
+      .append((maxValue == null) ? "*" : maxValue.toString())
+      .append(maxInclusive ? ']' : '}')
+      .append(ToStringUtils.boost(getBoost()))
+      .toString();
+  }
+}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/rangetree/OfflineSliceReader.java b/lucene/sandbox/src/java/org/apache/lucene/rangetree/OfflineSliceReader.java
new file mode 100644
index 0000000..9701a1f
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/rangetree/OfflineSliceReader.java
@@ -0,0 +1,82 @@
+package org.apache.lucene.rangetree;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.BufferedInputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.nio.file.Files;
+import java.nio.file.Path;
+
+import org.apache.lucene.store.InputStreamDataInput;
+
+final class OfflineSliceReader implements SliceReader {
+  final InputStreamDataInput in;
+  long countLeft;
+  private long value;
+  private long ord;
+  private int docID;
+
+  OfflineSliceReader(Path tempFile, long start, long count) throws IOException {
+    InputStream fis = Files.newInputStream(tempFile);
+    long seekFP = start * RangeTreeWriter.BYTES_PER_DOC;
+    long skipped = 0;
+    while (skipped < seekFP) {
+      long inc = fis.skip(seekFP - skipped);
+      skipped += inc;
+      if (inc == 0) {
+        throw new RuntimeException("skip returned 0");
+      }
+    }
+    in = new InputStreamDataInput(new BufferedInputStream(fis));
+    this.countLeft = count;
+  }
+
+  @Override
+  public boolean next() throws IOException {
+    if (countLeft == 0) {
+      return false;
+    }
+    countLeft--;
+    value = in.readLong();
+    ord = in.readLong();
+    docID = in.readInt();
+    return true;
+  }
+
+  @Override
+  public long value() {
+    return value;
+  }
+
+  @Override
+  public long ord() {
+    return ord;
+  }
+
+  @Override
+  public int docID() {
+    return docID;
+  }
+
+  @Override
+  public void close() throws IOException {
+    in.close();
+  }
+}
+
diff --git a/lucene/sandbox/src/java/org/apache/lucene/rangetree/OfflineSliceWriter.java b/lucene/sandbox/src/java/org/apache/lucene/rangetree/OfflineSliceWriter.java
new file mode 100644
index 0000000..5ca2549
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/rangetree/OfflineSliceWriter.java
@@ -0,0 +1,75 @@
+package org.apache.lucene.rangetree;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.BufferedOutputStream;
+import java.io.IOException;
+import java.nio.file.Files;
+import java.nio.file.Path;
+
+import org.apache.lucene.store.ByteArrayDataOutput;
+import org.apache.lucene.store.OutputStreamDataOutput;
+import org.apache.lucene.util.IOUtils;
+
+final class OfflineSliceWriter implements SliceWriter {
+
+  final Path tempFile;
+  final byte[] scratchBytes = new byte[RangeTreeWriter.BYTES_PER_DOC];
+  final ByteArrayDataOutput scratchBytesOutput = new ByteArrayDataOutput(scratchBytes);      
+  final OutputStreamDataOutput out;
+  final long count;
+  private long countWritten;
+
+  public OfflineSliceWriter(Path tempDir, long count) throws IOException {
+    tempFile = Files.createTempFile(tempDir, "size" + count + ".", "");
+    out = new OutputStreamDataOutput(new BufferedOutputStream(Files.newOutputStream(tempFile)));
+    this.count = count;
+  }
+    
+  @Override
+  public void append(long value, long ord, int docID) throws IOException {
+    out.writeLong(value);
+    out.writeLong(ord);
+    out.writeInt(docID);
+    countWritten++;
+  }
+
+  @Override
+  public SliceReader getReader(long start) throws IOException {
+    return new OfflineSliceReader(tempFile, start, count-start);
+  }
+
+  @Override
+  public void close() throws IOException {
+    out.close();
+    if (count != countWritten) {
+      throw new IllegalStateException("wrote " + countWritten + " values, but expected " + count);
+    }
+  }
+
+  @Override
+  public void destroy() throws IOException {
+    IOUtils.rm(tempFile);
+  }
+
+  @Override
+  public String toString() {
+    return "OfflineSliceWriter(count=" + count + " tempFile=" + tempFile + ")";
+  }
+}
+
diff --git a/lucene/sandbox/src/java/org/apache/lucene/rangetree/RangeTreeDocValuesConsumer.java b/lucene/sandbox/src/java/org/apache/lucene/rangetree/RangeTreeDocValuesConsumer.java
new file mode 100644
index 0000000..ecf6bb9
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/rangetree/RangeTreeDocValuesConsumer.java
@@ -0,0 +1,148 @@
+package org.apache.lucene.rangetree;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.Map;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+
+class RangeTreeDocValuesConsumer extends DocValuesConsumer implements Closeable {
+  final DocValuesConsumer delegate;
+  final int maxPointsInLeafNode;
+  final int maxPointsSortInHeap;
+  final IndexOutput out;
+  final Map<Integer,Long> fieldIndexFPs = new HashMap<>();
+  final SegmentWriteState state;
+
+  public RangeTreeDocValuesConsumer(DocValuesConsumer delegate, SegmentWriteState state, int maxPointsInLeafNode, int maxPointsSortInHeap) throws IOException {
+    RangeTreeWriter.verifyParams(maxPointsInLeafNode, maxPointsSortInHeap);
+    this.delegate = delegate;
+    this.maxPointsInLeafNode = maxPointsInLeafNode;
+    this.maxPointsSortInHeap = maxPointsSortInHeap;
+    this.state = state;
+    String datFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, RangeTreeDocValuesFormat.DATA_EXTENSION);
+    out = state.directory.createOutput(datFileName, state.context);
+    CodecUtil.writeIndexHeader(out, RangeTreeDocValuesFormat.DATA_CODEC_NAME, RangeTreeDocValuesFormat.DATA_VERSION_CURRENT,
+                               state.segmentInfo.getId(), state.segmentSuffix);
+  }
+
+  @Override
+  public void close() throws IOException {
+    boolean success = false;
+    try {
+      CodecUtil.writeFooter(out);
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(delegate, out);
+      } else {
+        IOUtils.closeWhileHandlingException(delegate, out);
+      }
+    }
+    
+    String metaFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, RangeTreeDocValuesFormat.META_EXTENSION);
+    IndexOutput metaOut = state.directory.createOutput(metaFileName, state.context);
+    success = false;
+    try {
+      CodecUtil.writeIndexHeader(metaOut, RangeTreeDocValuesFormat.META_CODEC_NAME, RangeTreeDocValuesFormat.META_VERSION_CURRENT,
+                                 state.segmentInfo.getId(), state.segmentSuffix);
+      metaOut.writeVInt(fieldIndexFPs.size());
+      for(Map.Entry<Integer,Long> ent : fieldIndexFPs.entrySet()) {       
+        metaOut.writeVInt(ent.getKey());
+        metaOut.writeVLong(ent.getValue());
+      }
+      CodecUtil.writeFooter(metaOut);
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(metaOut);
+      } else {
+        IOUtils.closeWhileHandlingException(metaOut);
+      }
+    }
+  }
+
+  @Override
+  public void addSortedNumericField(FieldInfo field, Iterable<Number> docToValueCount, Iterable<Number> values) throws IOException {
+    delegate.addSortedNumericField(field, docToValueCount, values);
+    RangeTreeWriter writer = new RangeTreeWriter(maxPointsInLeafNode, maxPointsSortInHeap);
+    Iterator<Number> valueIt = values.iterator();
+    Iterator<Number> valueCountIt = docToValueCount.iterator();
+    //System.out.println("\nSNF: field=" + field.name);
+    for (int docID=0;docID<state.segmentInfo.maxDoc();docID++) {
+      assert valueCountIt.hasNext();
+      int count = valueCountIt.next().intValue();
+      for(int i=0;i<count;i++) {
+        assert valueIt.hasNext();
+        writer.add(valueIt.next().longValue(), docID);
+      }
+    }
+
+    long indexStartFP = writer.finish(out);
+
+    fieldIndexFPs.put(field.number, indexStartFP);
+  }
+
+  @Override
+  public void addNumericField(FieldInfo field, Iterable<Number> values) throws IOException {
+    throw new UnsupportedOperationException("use either SortedNumericDocValuesField or SortedSetDocValuesField");
+  }
+
+  @Override
+  public void addBinaryField(FieldInfo field, Iterable<BytesRef> values) {
+    throw new UnsupportedOperationException("use either SortedNumericDocValuesField or SortedSetDocValuesField");
+  }
+
+  @Override
+  public void addSortedField(FieldInfo field, Iterable<BytesRef> values, Iterable<Number> docToOrd) {
+    throw new UnsupportedOperationException("use either SortedNumericDocValuesField or SortedSetDocValuesField");
+  }
+
+  @Override
+  public void addSortedSetField(FieldInfo field, Iterable<BytesRef> values, Iterable<Number> docToOrdCount, Iterable<Number> ords) throws IOException {
+    delegate.addSortedSetField(field, values, docToOrdCount, ords);
+    RangeTreeWriter writer = new RangeTreeWriter(maxPointsInLeafNode, maxPointsSortInHeap);
+    Iterator<Number> docToOrdCountIt = docToOrdCount.iterator();
+    Iterator<Number> ordsIt = ords.iterator();
+    //System.out.println("\nSSF: field=" + field.name);
+    for (int docID=0;docID<state.segmentInfo.maxDoc();docID++) {
+      assert docToOrdCountIt.hasNext();
+      int count = docToOrdCountIt.next().intValue();
+      for(int i=0;i<count;i++) {
+        assert ordsIt.hasNext();
+        long ord = ordsIt.next().longValue();
+        writer.add(ord, docID);
+      }
+    }
+
+    long indexStartFP = writer.finish(out);
+
+    fieldIndexFPs.put(field.number, indexStartFP);
+  }
+}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/rangetree/RangeTreeDocValuesFormat.java b/lucene/sandbox/src/java/org/apache/lucene/rangetree/RangeTreeDocValuesFormat.java
new file mode 100644
index 0000000..4bd62cb
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/rangetree/RangeTreeDocValuesFormat.java
@@ -0,0 +1,112 @@
+package org.apache.lucene.rangetree;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.codecs.lucene50.Lucene50DocValuesFormat;
+import org.apache.lucene.document.SortedNumericDocValuesField;
+import org.apache.lucene.document.SortedSetDocValuesField; // javadocs
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+
+import java.io.IOException;
+
+/**
+ * A {@link DocValuesFormat} to efficiently index numeric values from
+ * from {@link SortedNumericDocValuesField} or BytesRef values from {@link SortedSetDocValuesField}
+ * for numeric range queries using ({@link NumericRangeTreeQuery}) and arbitrary binary
+ * range queries using {@link SortedSetRangeTreeQuery}.
+ *
+ * <p>This wraps {@link Lucene50DocValuesFormat}, but saves its own numeric tree
+ * structures to disk for fast query-time intersection. See <a
+ * href="https://www.cs.duke.edu/~pankaj/publications/papers/bkd-sstd.pdf">this paper</a>
+ * for details.
+ *
+ * <p>The numeric tree slices up 1D space into smaller and
+ * smaller ranges, until the smallest ranges have approximately
+ * between X/2 and X (X default is 1024) values in them, at which point
+ * such leaf cells are written as a block to disk, while the index tree
+ * structure records how space was sub-divided is loaded into HEAP
+ * at search time.  At search time, the tree is recursed based on whether
+ * each of left or right child overlap with the query range, and once
+ * a leaf block is reached, all documents in that leaf block are collected
+ * if the cell is fully enclosed by the query shape, or filtered and then
+ * collected, if not.
+ *
+ * <p>The index is also quite compact, because docs only appear once in
+ * the tree (no "prefix terms").
+ *
+ * <p>In addition to the files written by {@link Lucene50DocValuesFormat}, this format writes:
+ * <ol>
+ *   <li><tt>.ndd</tt>: numeric tree leaf data and index</li>
+ *   <li><tt>.ndm</tt>: numeric tree metadata</li>
+ * </ol>
+ *
+ * <p>The disk format is experimental and free to change suddenly, and this code likely has new and exciting bugs!
+ *
+ * @lucene.experimental */
+
+public class RangeTreeDocValuesFormat extends DocValuesFormat {
+
+  static final String DATA_CODEC_NAME = "NumericTreeData";
+  static final int DATA_VERSION_START = 0;
+  static final int DATA_VERSION_CURRENT = DATA_VERSION_START;
+  static final String DATA_EXTENSION = "ndd";
+
+  static final String META_CODEC_NAME = "NumericTreeMeta";
+  static final int META_VERSION_START = 0;
+  static final int META_VERSION_CURRENT = META_VERSION_START;
+  static final String META_EXTENSION = "ndm";
+
+  private final int maxPointsInLeafNode;
+  private final int maxPointsSortInHeap;
+  
+  private final DocValuesFormat delegate = new Lucene50DocValuesFormat();
+
+  /** Default constructor */
+  public RangeTreeDocValuesFormat() {
+    this(RangeTreeWriter.DEFAULT_MAX_VALUES_IN_LEAF_NODE, RangeTreeWriter.DEFAULT_MAX_VALUES_SORT_IN_HEAP);
+  }
+
+  /** Creates this with custom configuration.
+   *
+   * @param maxPointsInLeafNode Maximum number of points in each leaf cell.  Smaller values create a deeper tree with larger in-heap index and possibly
+   *    faster searching.  The default is 1024.
+   * @param maxPointsSortInHeap Maximum number of points where in-heap sort can be used.  When the number of points exceeds this, a (slower)
+   *    offline sort is used.  The default is 128 * 1024.
+   *
+   * @lucene.experimental */
+  public RangeTreeDocValuesFormat(int maxPointsInLeafNode, int maxPointsSortInHeap) {
+    super("NumericTree");
+    RangeTreeWriter.verifyParams(maxPointsInLeafNode, maxPointsSortInHeap);
+    this.maxPointsInLeafNode = maxPointsInLeafNode;
+    this.maxPointsSortInHeap = maxPointsSortInHeap;
+  }
+
+  @Override
+  public DocValuesConsumer fieldsConsumer(final SegmentWriteState state) throws IOException {
+    return new RangeTreeDocValuesConsumer(delegate.fieldsConsumer(state), state, maxPointsInLeafNode, maxPointsSortInHeap);
+  }
+
+  @Override
+  public DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
+    return new RangeTreeDocValuesProducer(delegate.fieldsProducer(state), state);
+  }
+}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/rangetree/RangeTreeDocValuesProducer.java b/lucene/sandbox/src/java/org/apache/lucene/rangetree/RangeTreeDocValuesProducer.java
new file mode 100644
index 0000000..b569471
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/rangetree/RangeTreeDocValuesProducer.java
@@ -0,0 +1,190 @@
+package org.apache.lucene.rangetree;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.atomic.AtomicLong;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.index.SortedNumericDocValues;
+import org.apache.lucene.index.SortedSetDocValues;
+import org.apache.lucene.store.ChecksumIndexInput;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.Accountable;
+import org.apache.lucene.util.Accountables;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.RamUsageEstimator;
+
+class RangeTreeDocValuesProducer extends DocValuesProducer {
+
+  private final Map<String,RangeTreeReader> treeReaders = new HashMap<>();
+  private final Map<Integer,Long> fieldToIndexFPs = new HashMap<>();
+
+  private final IndexInput datIn;
+  private final AtomicLong ramBytesUsed;
+  private final int maxDoc;
+  private final DocValuesProducer delegate;
+  private final boolean merging;
+
+  public RangeTreeDocValuesProducer(DocValuesProducer delegate, SegmentReadState state) throws IOException {
+    String metaFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, RangeTreeDocValuesFormat.META_EXTENSION);
+    ChecksumIndexInput metaIn = state.directory.openChecksumInput(metaFileName, state.context);
+    CodecUtil.checkIndexHeader(metaIn, RangeTreeDocValuesFormat.META_CODEC_NAME, RangeTreeDocValuesFormat.META_VERSION_START, RangeTreeDocValuesFormat.META_VERSION_CURRENT,
+                               state.segmentInfo.getId(), state.segmentSuffix);
+    int fieldCount = metaIn.readVInt();
+    for(int i=0;i<fieldCount;i++) {
+      int fieldNumber = metaIn.readVInt();
+      long indexFP = metaIn.readVLong();
+      fieldToIndexFPs.put(fieldNumber, indexFP);
+    }
+    CodecUtil.checkFooter(metaIn);
+    metaIn.close();
+
+    String datFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, RangeTreeDocValuesFormat.DATA_EXTENSION);
+    datIn = state.directory.openInput(datFileName, state.context);
+    CodecUtil.checkIndexHeader(datIn, RangeTreeDocValuesFormat.DATA_CODEC_NAME, RangeTreeDocValuesFormat.DATA_VERSION_START, RangeTreeDocValuesFormat.DATA_VERSION_CURRENT,
+                               state.segmentInfo.getId(), state.segmentSuffix);
+    ramBytesUsed = new AtomicLong(RamUsageEstimator.shallowSizeOfInstance(getClass()));
+    maxDoc = state.segmentInfo.maxDoc();
+    this.delegate = delegate;
+    merging = false;
+  }
+
+  // clone for merge: we don't hang onto the NumericTrees we load
+  RangeTreeDocValuesProducer(RangeTreeDocValuesProducer orig) throws IOException {
+    assert Thread.holdsLock(orig);
+    datIn = orig.datIn.clone();
+    ramBytesUsed = new AtomicLong(orig.ramBytesUsed.get());
+    delegate = orig.delegate.getMergeInstance();
+    fieldToIndexFPs.putAll(orig.fieldToIndexFPs);
+    treeReaders.putAll(orig.treeReaders);
+    merging = true;
+    maxDoc = orig.maxDoc;
+  }
+
+  @Override
+  public synchronized SortedNumericDocValues getSortedNumeric(FieldInfo field) throws IOException {
+    RangeTreeReader treeReader = treeReaders.get(field.name);
+    if (treeReader == null) {
+      // Lazy load
+      Long fp = fieldToIndexFPs.get(field.number);
+      // FieldInfos checks has already ensured we are a DV field of this type, and Codec ensures
+      // this DVFormat was used at write time:
+      assert fp != null;
+      datIn.seek(fp);
+      treeReader = new RangeTreeReader(datIn);
+
+      // Only hang onto the reader when we are not merging:
+      if (merging == false) {
+        treeReaders.put(field.name, treeReader);
+        ramBytesUsed.addAndGet(treeReader.ramBytesUsed());
+      }
+    }
+
+    return new RangeTreeSortedNumericDocValues(treeReader, delegate.getSortedNumeric(field));
+  }
+
+  @Override
+  public void close() throws IOException {
+    IOUtils.close(datIn, delegate);
+  }
+
+  @Override
+  public void checkIntegrity() throws IOException {
+    CodecUtil.checksumEntireFile(datIn);
+  }
+
+  @Override
+  public NumericDocValues getNumeric(FieldInfo field) {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public BinaryDocValues getBinary(FieldInfo field) {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public SortedDocValues getSorted(FieldInfo field) {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public synchronized SortedSetDocValues getSortedSet(FieldInfo field) throws IOException {
+    RangeTreeReader treeReader = treeReaders.get(field.name);
+    if (treeReader == null) {
+      // Lazy load
+      Long fp = fieldToIndexFPs.get(field.number);
+
+      // FieldInfos checks has already ensured we are a DV field of this type, and Codec ensures
+      // this DVFormat was used at write time:
+      assert fp != null;
+
+      datIn.seek(fp);
+      //System.out.println("load field=" + field.name);
+      treeReader = new RangeTreeReader(datIn);
+
+      // Only hang onto the reader when we are not merging:
+      if (merging == false) {
+        treeReaders.put(field.name, treeReader);
+        ramBytesUsed.addAndGet(treeReader.ramBytesUsed());
+      }
+    }
+
+    return new RangeTreeSortedSetDocValues(treeReader, delegate.getSortedSet(field));
+  }
+
+  @Override
+  public Bits getDocsWithField(FieldInfo field) throws IOException {
+    return delegate.getDocsWithField(field);
+  }
+
+  @Override
+  public synchronized Collection<Accountable> getChildResources() {
+    List<Accountable> resources = new ArrayList<>();
+    for(Map.Entry<String,RangeTreeReader> ent : treeReaders.entrySet()) {
+      resources.add(Accountables.namedAccountable("field " + ent.getKey(), ent.getValue()));
+    }
+    resources.add(Accountables.namedAccountable("delegate", delegate));
+
+    return resources;
+  }
+
+  @Override
+  public synchronized DocValuesProducer getMergeInstance() throws IOException {
+    return new RangeTreeDocValuesProducer(this);
+  }
+
+  @Override
+  public long ramBytesUsed() {
+    return ramBytesUsed.get() + delegate.ramBytesUsed();
+  }
+}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/rangetree/RangeTreeReader.java b/lucene/sandbox/src/java/org/apache/lucene/rangetree/RangeTreeReader.java
new file mode 100644
index 0000000..5b4b318
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/rangetree/RangeTreeReader.java
@@ -0,0 +1,202 @@
+package org.apache.lucene.rangetree;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.index.SortedNumericDocValues;
+import org.apache.lucene.search.DocIdSet;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.Accountable;
+import org.apache.lucene.util.DocIdSetBuilder;
+import org.apache.lucene.util.RamUsageEstimator;
+
+import java.io.IOException;
+import java.util.Arrays;
+
+/** Handles intersection of a range with a numeric tree previously written with {@link RangeTreeWriter}.
+ *
+ * @lucene.experimental */
+
+final class RangeTreeReader implements Accountable {
+  final private long[] blockFPs;
+  final private long[] blockMinValues;
+  final IndexInput in;
+  final long globalMaxValue;
+  final int approxDocsPerBlock;
+
+  public RangeTreeReader(IndexInput in) throws IOException {
+
+    // Read index:
+    int numLeaves = in.readVInt();
+    approxDocsPerBlock = in.readVInt();
+
+    blockMinValues = new long[numLeaves];
+    for(int i=0;i<numLeaves;i++) {
+      blockMinValues[i] = in.readLong();
+    }
+    blockFPs = new long[numLeaves];
+    for(int i=0;i<numLeaves;i++) {
+      blockFPs[i] = in.readVLong();
+    }
+    globalMaxValue = in.readLong();
+
+    this.in = in;
+  }
+
+  public long getMinValue() {
+    return blockMinValues[0];
+  }
+
+  public long getMaxValue() {
+    return globalMaxValue;
+  }
+
+  private static final class QueryState {
+    final IndexInput in;
+    final DocIdSetBuilder docs;
+    final long minValueIncl;
+    final long maxValueIncl;
+    final SortedNumericDocValues sndv;
+
+    public QueryState(IndexInput in, int maxDoc,
+                      long minValueIncl, long maxValueIncl,
+                      SortedNumericDocValues sndv) {
+      this.in = in;
+      this.docs = new DocIdSetBuilder(maxDoc);
+      this.minValueIncl = minValueIncl;
+      this.maxValueIncl = maxValueIncl;
+      this.sndv = sndv;
+    }
+  }
+
+  public DocIdSet intersect(long minIncl, long maxIncl, SortedNumericDocValues sndv, int maxDoc) throws IOException {
+
+    if (minIncl > maxIncl) {
+      return DocIdSet.EMPTY;
+    }
+
+    if (minIncl > globalMaxValue || maxIncl < blockMinValues[0]) {
+      return DocIdSet.EMPTY;
+    }
+
+    QueryState state = new QueryState(in.clone(), maxDoc,
+                                      minIncl, maxIncl,
+                                      sndv);
+
+    int startBlockIncl = Arrays.binarySearch(blockMinValues, minIncl);
+    if (startBlockIncl >= 0) {
+      // There can be dups here, when the same value is added many
+      // times.  Also, we need the first block whose min is < minIncl:
+      while (startBlockIncl > 0 && blockMinValues[startBlockIncl] == minIncl) {
+        startBlockIncl--;
+      }
+    } else {
+      startBlockIncl = Math.max(-startBlockIncl-2, 0);
+    }
+
+    int endBlockIncl = Arrays.binarySearch(blockMinValues, maxIncl);
+    if (endBlockIncl >= 0) {
+      // There can be dups here, when the same value is added many
+      // times.  Also, we need the first block whose max is > minIncl:
+      while (endBlockIncl < blockMinValues.length-1 && blockMinValues[endBlockIncl] == maxIncl) {
+        endBlockIncl++;
+      }
+    } else {
+      endBlockIncl = Math.max(-endBlockIncl-2, 0);
+    }
+
+    assert startBlockIncl <= endBlockIncl;
+
+    state.in.seek(blockFPs[startBlockIncl]);
+
+    //System.out.println("startBlockIncl=" + startBlockIncl + " endBlockIncl=" + endBlockIncl);
+
+    // Rough estimate of how many hits we'll see.  Note that in the degenerate case
+    // (index same value many times) this could be a big over-estimate, but in the typical
+    // case it's good:
+    state.docs.grow(approxDocsPerBlock * (endBlockIncl - startBlockIncl + 1));
+
+    int hitCount = 0;
+    for (int block=startBlockIncl;block<=endBlockIncl;block++) {
+      boolean doFilter = blockMinValues[block] <= minIncl || block == blockMinValues.length-1 || blockMinValues[block+1] >= maxIncl;
+      //System.out.println("  block=" + block + " min=" + blockMinValues[block] + " doFilter=" + doFilter);
+
+      int newCount;
+      if (doFilter) {
+        // We must filter each hit:
+        newCount = addSome(state);
+      } else {
+        newCount = addAll(state);
+      }
+
+      hitCount += newCount;
+    }
+
+    // NOTE: hitCount is an over-estimate in the multi-valued case:
+    return state.docs.build(hitCount);
+  }
+
+  /** Adds all docs from the current block. */
+  private int addAll(QueryState state) throws IOException {
+    // How many values are stored in this leaf cell:
+    int count = state.in.readVInt();
+    state.docs.grow(count);
+    for(int i=0;i<count;i++) {
+      int docID = state.in.readInt();
+      state.docs.add(docID);
+    }
+
+    return count;
+  }
+
+  /** Adds docs from the current block, filtering each hit against the query min/max.  This
+   *  is only needed on the boundary blocks. */
+  private int addSome(QueryState state) throws IOException {
+    int hitCount = 0;
+
+    // How many points are stored in this leaf cell:
+    int count = state.in.readVInt();
+    state.docs.grow(count);
+    for(int i=0;i<count;i++) {
+      int docID = state.in.readInt();
+      state.sndv.setDocument(docID);
+
+      // How many values this doc has:
+      int docValueCount = state.sndv.count();
+
+      for(int j=0;j<docValueCount;j++) {
+        long value = state.sndv.valueAt(j);
+
+        if (value >= state.minValueIncl && value <= state.maxValueIncl) {
+          state.docs.add(docID);
+          hitCount++;
+
+          // Stop processing values for this doc:
+          break;
+        }
+      }
+    }
+
+    return hitCount;
+  }
+
+  @Override
+  public long ramBytesUsed() {
+    return blockMinValues.length * RamUsageEstimator.NUM_BYTES_LONG + 
+      blockFPs.length * RamUsageEstimator.NUM_BYTES_LONG;
+  }
+}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/rangetree/RangeTreeSortedNumericDocValues.java b/lucene/sandbox/src/java/org/apache/lucene/rangetree/RangeTreeSortedNumericDocValues.java
new file mode 100644
index 0000000..a5cbd15
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/rangetree/RangeTreeSortedNumericDocValues.java
@@ -0,0 +1,49 @@
+package org.apache.lucene.rangetree;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.index.SortedNumericDocValues;
+
+class RangeTreeSortedNumericDocValues extends SortedNumericDocValues {
+  final RangeTreeReader rangeTreeReader;
+  final SortedNumericDocValues delegate;
+
+  public RangeTreeSortedNumericDocValues(RangeTreeReader rangeTreeReader, SortedNumericDocValues delegate) {
+    this.rangeTreeReader = rangeTreeReader;
+    this.delegate = delegate;
+  }
+
+  public RangeTreeReader getRangeTreeReader() {
+    return rangeTreeReader;
+  }
+
+  @Override
+  public void setDocument(int doc) {
+    delegate.setDocument(doc);
+  }
+
+  @Override
+  public long valueAt(int index) {
+    return delegate.valueAt(index);
+  }
+
+  @Override
+  public int count() {
+    return delegate.count();
+  }
+}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/rangetree/RangeTreeSortedSetDocValues.java b/lucene/sandbox/src/java/org/apache/lucene/rangetree/RangeTreeSortedSetDocValues.java
new file mode 100644
index 0000000..b881a89
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/rangetree/RangeTreeSortedSetDocValues.java
@@ -0,0 +1,66 @@
+package org.apache.lucene.rangetree;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.index.SortedSetDocValues;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.util.BytesRef;
+
+class RangeTreeSortedSetDocValues extends SortedSetDocValues {
+  final RangeTreeReader rangeTreeReader;
+  final SortedSetDocValues delegate;
+
+  public RangeTreeSortedSetDocValues(RangeTreeReader rangeTreeReader, SortedSetDocValues delegate) {
+    this.rangeTreeReader = rangeTreeReader;
+    this.delegate = delegate;
+  }
+
+  public RangeTreeReader getRangeTreeReader() {
+    return rangeTreeReader;
+  }
+
+  @Override
+  public long nextOrd() {
+    return delegate.nextOrd();
+  }
+
+  @Override
+  public void setDocument(int doc) {
+    delegate.setDocument(doc);
+  }
+
+  @Override
+  public BytesRef lookupOrd(long ord) {
+    return delegate.lookupOrd(ord);
+  }
+
+  @Override
+  public long getValueCount() {
+    return delegate.getValueCount();
+  }
+
+  @Override
+  public long lookupTerm(BytesRef key) {
+    return delegate.lookupTerm(key);
+  }
+
+  @Override
+  public TermsEnum termsEnum() {
+    return delegate.termsEnum();
+  }
+}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/rangetree/RangeTreeWriter.java b/lucene/sandbox/src/java/org/apache/lucene/rangetree/RangeTreeWriter.java
new file mode 100644
index 0000000..bf297a1
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/rangetree/RangeTreeWriter.java
@@ -0,0 +1,591 @@
+package org.apache.lucene.rangetree;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.nio.file.DirectoryStream;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.util.Arrays;
+import java.util.Comparator;
+
+import org.apache.lucene.store.ByteArrayDataInput;
+import org.apache.lucene.store.ByteArrayDataOutput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.BytesRefBuilder;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.InPlaceMergeSorter;
+import org.apache.lucene.util.OfflineSorter.ByteSequencesWriter;
+import org.apache.lucene.util.OfflineSorter;
+import org.apache.lucene.util.RamUsageEstimator;
+
+// TODO
+//   - could we just "use postings" to map leaf -> docIDs?
+//   - we could also index "auto-prefix terms" here, and use better compression
+//   - the index could be efficiently encoded as an FST, so we don't have wasteful
+//     (monotonic) long[] leafBlockFPs; or we could use MonotonicLongValues ... but then
+//     the index is already plenty small: 60M OSM points --> 1.1 MB with 128 points
+//     per leaf, and you can reduce that by putting more points per leaf
+//   - we can quantize the split values to 2 bytes (short): http://people.csail.mit.edu/tmertens/papers/qkdtree.pdf
+
+/** Recursively builds a 1d BKD tree to assign all incoming {@code long} values to smaller
+ *  and smaller ranges until the number of points in a given
+ *  range is &lt= the <code>maxPointsInLeafNode</code>.  The tree is
+ *  fully balanced, which means the leaf nodes will have between 50% and 100% of
+ *  the requested <code>maxPointsInLeafNode</code>, except for the adversarial case
+ *  of indexing exactly the same value many times.
+ *
+ *  <p>
+ *  See <a href="https://www.cs.duke.edu/~pankaj/publications/papers/bkd-sstd.pdf">this paper</a> for details.
+ *
+ *  <p>This consumes heap during writing: for any nodes with fewer than <code>maxPointsSortInHeap</code>, it holds
+ *  the points in memory as simple java arrays.
+ *
+ *  <p>
+ *  <b>NOTE</b>: This can write at most Integer.MAX_VALUE * <code>maxPointsInLeafNode</code> total values,
+ *  which should be plenty since a Lucene index can have at most Integer.MAX_VALUE-1 documents.
+ *
+ * @lucene.experimental */
+
+class RangeTreeWriter {
+
+  // value (long) + ord (long) + docID (int)
+  static final int BYTES_PER_DOC = 2 * RamUsageEstimator.NUM_BYTES_LONG + RamUsageEstimator.NUM_BYTES_INT;
+
+  public static final int DEFAULT_MAX_VALUES_IN_LEAF_NODE = 1024;
+
+  /** This works out to max of ~10 MB peak heap tied up during writing: */
+  public static final int DEFAULT_MAX_VALUES_SORT_IN_HEAP = 128*1024;;
+
+  private final byte[] scratchBytes = new byte[BYTES_PER_DOC];
+  private final ByteArrayDataOutput scratchBytesOutput = new ByteArrayDataOutput(scratchBytes);
+
+  private OfflineSorter.ByteSequencesWriter writer;
+  private GrowingHeapSliceWriter heapWriter;
+
+  private Path tempInput;
+  private Path tempDir;
+  private final int maxValuesInLeafNode;
+  private final int maxValuesSortInHeap;
+
+  private long valueCount;
+  private long globalMinValue = Long.MAX_VALUE;
+  private long globalMaxValue = Long.MIN_VALUE;
+
+  public RangeTreeWriter() throws IOException {
+    this(DEFAULT_MAX_VALUES_IN_LEAF_NODE, DEFAULT_MAX_VALUES_SORT_IN_HEAP);
+  }
+
+  // TODO: instead of maxValuesSortInHeap, change to maxMBHeap ... the mapping is non-obvious:
+  public RangeTreeWriter(int maxValuesInLeafNode, int maxValuesSortInHeap) throws IOException {
+    verifyParams(maxValuesInLeafNode, maxValuesSortInHeap);
+    this.maxValuesInLeafNode = maxValuesInLeafNode;
+    this.maxValuesSortInHeap = maxValuesSortInHeap;
+
+    // We write first maxValuesSortInHeap in heap, then cutover to offline for additional points:
+    heapWriter = new GrowingHeapSliceWriter(maxValuesSortInHeap);
+  }
+
+  public static void verifyParams(int maxValuesInLeafNode, int maxValuesSortInHeap) {
+    if (maxValuesInLeafNode <= 0) {
+      throw new IllegalArgumentException("maxValuesInLeafNode must be > 0; got " + maxValuesInLeafNode);
+    }
+    if (maxValuesInLeafNode > ArrayUtil.MAX_ARRAY_LENGTH) {
+      throw new IllegalArgumentException("maxValuesInLeafNode must be <= ArrayUtil.MAX_ARRAY_LENGTH (= " + ArrayUtil.MAX_ARRAY_LENGTH + "); got " + maxValuesInLeafNode);
+    }
+    if (maxValuesSortInHeap < maxValuesInLeafNode) {
+      throw new IllegalArgumentException("maxValuesSortInHeap must be >= maxValuesInLeafNode; got " + maxValuesSortInHeap + " vs maxValuesInLeafNode="+ maxValuesInLeafNode);
+    }
+    if (maxValuesSortInHeap > ArrayUtil.MAX_ARRAY_LENGTH) {
+      throw new IllegalArgumentException("maxValuesSortInHeap must be <= ArrayUtil.MAX_ARRAY_LENGTH (= " + ArrayUtil.MAX_ARRAY_LENGTH + "); got " + maxValuesSortInHeap);
+    }
+  }
+
+  /** If the current segment has too many points then we switchover to temp files / offline sort. */
+  private void switchToOffline() throws IOException {
+
+    // OfflineSorter isn't thread safe, but our own private tempDir works around this:
+    tempDir = Files.createTempDirectory(OfflineSorter.defaultTempDir(), RangeTreeWriter.class.getSimpleName());
+
+    // For each .add we just append to this input file, then in .finish we sort this input and resursively build the tree:
+    tempInput = tempDir.resolve("in");
+    writer = new OfflineSorter.ByteSequencesWriter(tempInput);
+    for(int i=0;i<valueCount;i++) {
+      scratchBytesOutput.reset(scratchBytes);
+      scratchBytesOutput.writeLong(heapWriter.values[i]);
+      scratchBytesOutput.writeVInt(heapWriter.docIDs[i]);
+      scratchBytesOutput.writeVLong(i);
+      // TODO: can/should OfflineSorter optimize the fixed-width case?
+      writer.write(scratchBytes, 0, scratchBytes.length);
+    }
+
+    heapWriter = null;
+  }
+
+  void add(long value, int docID) throws IOException {
+    if (valueCount >= maxValuesSortInHeap) {
+      if (writer == null) {
+        switchToOffline();
+      }
+      scratchBytesOutput.reset(scratchBytes);
+      scratchBytesOutput.writeLong(value);
+      scratchBytesOutput.writeVInt(docID);
+      scratchBytesOutput.writeVLong(valueCount);
+      writer.write(scratchBytes, 0, scratchBytes.length);
+    } else {
+      // Not too many points added yet, continue using heap:
+      heapWriter.append(value, valueCount, docID);
+    }
+
+    valueCount++;
+    globalMaxValue = Math.max(value, globalMaxValue);
+    globalMinValue = Math.min(value, globalMinValue);
+  }
+
+  /** Changes incoming {@link ByteSequencesWriter} file to to fixed-width-per-entry file, because we need to be able to slice
+   *  as we recurse in {@link #build}. */
+  private SliceWriter convertToFixedWidth(Path in) throws IOException {
+    BytesRefBuilder scratch = new BytesRefBuilder();
+    scratch.grow(BYTES_PER_DOC);
+    BytesRef bytes = scratch.get();
+    ByteArrayDataInput dataReader = new ByteArrayDataInput();
+
+    OfflineSorter.ByteSequencesReader reader = null;
+    SliceWriter sortedWriter = null;
+    boolean success = false;
+    try {
+      reader = new OfflineSorter.ByteSequencesReader(in);
+      sortedWriter = getWriter(valueCount);
+      for (long i=0;i<valueCount;i++) {
+        boolean result = reader.read(scratch);
+        assert result;
+        dataReader.reset(bytes.bytes, bytes.offset, bytes.length);
+        long value = dataReader.readLong();
+        int docID = dataReader.readVInt();
+        assert docID >= 0: "docID=" + docID;
+        long ord = dataReader.readVLong();
+        sortedWriter.append(value, ord, docID);
+      }
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(sortedWriter, reader);
+      } else {
+        IOUtils.closeWhileHandlingException(reader);
+        try {
+          sortedWriter.destroy();
+        } catch (Throwable t) {
+          // Suppress to keep throwing original exc
+        }
+      }
+    }
+
+    return sortedWriter;
+  }
+
+  private SliceWriter sort() throws IOException {
+    if (heapWriter != null) {
+
+      assert valueCount < Integer.MAX_VALUE;
+
+      // All buffered points are still in heap
+      new InPlaceMergeSorter() {
+        @Override
+        protected void swap(int i, int j) {
+          int docID = heapWriter.docIDs[i];
+          heapWriter.docIDs[i] = heapWriter.docIDs[j];
+          heapWriter.docIDs[j] = docID;
+
+          long ord = heapWriter.ords[i];
+          heapWriter.ords[i] = heapWriter.ords[j];
+          heapWriter.ords[j] = ord;
+
+          long value = heapWriter.values[i];
+          heapWriter.values[i] = heapWriter.values[j];
+          heapWriter.values[j] = value;
+        }
+
+        @Override
+        protected int compare(int i, int j) {
+          int cmp = Long.compare(heapWriter.values[i], heapWriter.values[j]);
+          if (cmp != 0) {
+            return cmp;
+          }
+
+          // Tie-break
+          cmp = Integer.compare(heapWriter.docIDs[i], heapWriter.docIDs[j]);
+          if (cmp != 0) {
+            return cmp;
+          }
+
+          return Long.compare(heapWriter.ords[i], heapWriter.ords[j]);
+        }
+      }.sort(0, (int) valueCount);
+
+      HeapSliceWriter sorted = new HeapSliceWriter((int) valueCount);
+      for(int i=0;i<valueCount;i++) {
+        sorted.append(heapWriter.values[i],
+                      heapWriter.ords[i],
+                      heapWriter.docIDs[i]);
+      }
+
+      return sorted;
+    } else {
+
+      // Offline sort:
+      assert tempDir != null;
+
+      final ByteArrayDataInput reader = new ByteArrayDataInput();
+      Comparator<BytesRef> cmp = new Comparator<BytesRef>() {
+        private final ByteArrayDataInput readerB = new ByteArrayDataInput();
+
+        @Override
+        public int compare(BytesRef a, BytesRef b) {
+          reader.reset(a.bytes, a.offset, a.length);
+          final long valueA = reader.readLong();
+          final int docIDA = reader.readVInt();
+          final long ordA = reader.readVLong();
+
+          reader.reset(b.bytes, b.offset, b.length);
+          final long valueB = reader.readLong();
+          final int docIDB = reader.readVInt();
+          final long ordB = reader.readVLong();
+
+          int cmp = Long.compare(valueA, valueB);
+          if (cmp != 0) {
+            return cmp;
+          }
+
+          // Tie-break
+          cmp = Integer.compare(docIDA, docIDB);
+          if (cmp != 0) {
+            return cmp;
+          }
+
+          return Long.compare(ordA, ordB);
+        }
+      };
+
+      Path sorted = tempDir.resolve("sorted");
+      boolean success = false;
+      try {
+        OfflineSorter sorter = new OfflineSorter(cmp, OfflineSorter.BufferSize.automatic(), tempDir, OfflineSorter.MAX_TEMPFILES);
+        sorter.sort(tempInput, sorted);
+        SliceWriter writer = convertToFixedWidth(sorted);
+        success = true;
+        return writer;
+      } finally {
+        if (success) {
+          IOUtils.rm(sorted);
+        } else {
+          IOUtils.deleteFilesIgnoringExceptions(sorted);
+        }
+      }
+    }
+  }
+
+  /** Writes the 1d BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */
+  public long finish(IndexOutput out) throws IOException {
+
+    if (writer != null) {
+      writer.close();
+    }
+
+    if (valueCount == 0) {
+      throw new IllegalStateException("at least one value must be indexed");
+    }
+
+    // TODO: we should use in-memory sort here, if number of points is small enough:
+
+    long countPerLeaf = valueCount;
+    long innerNodeCount = 1;
+
+    while (countPerLeaf > maxValuesInLeafNode) {
+      countPerLeaf /= 2;
+      innerNodeCount *= 2;
+    }
+
+    //System.out.println("innerNodeCount=" + innerNodeCount);
+
+    if (1+2*innerNodeCount >= Integer.MAX_VALUE) {
+      throw new IllegalStateException("too many nodes; increase maxValuesInLeafNode (currently " + maxValuesInLeafNode + ") and reindex");
+    }
+
+    innerNodeCount--;
+
+    int numLeaves = (int) (innerNodeCount+1);
+
+    // Indexed by nodeID, but first (root) nodeID is 1
+    long[] blockMinValues = new long[numLeaves];
+
+    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)
+    long[] leafBlockFPs = new long[numLeaves];
+
+    // Make sure the math above "worked":
+    assert valueCount / blockMinValues.length <= maxValuesInLeafNode: "valueCount=" + valueCount + " blockMinValues.length=" + blockMinValues.length + " maxValuesInLeafNode=" + maxValuesInLeafNode;
+    //System.out.println("  avg pointsPerLeaf=" + (valueCount/blockMinValues.length));
+
+    // Sort all docs by value:
+    SliceWriter sortedWriter = null;
+
+    boolean success = false;
+    try {
+      sortedWriter = sort();
+      heapWriter = null;
+
+      build(1, numLeaves,
+            new PathSlice(sortedWriter, 0, valueCount),
+            out,
+            globalMinValue, globalMaxValue,
+            blockMinValues,
+            leafBlockFPs);
+      success = true;
+    } finally {
+      if (success) {
+        sortedWriter.destroy();
+        IOUtils.rm(tempInput);
+      } else {
+        try {
+          sortedWriter.destroy();
+        } catch (Throwable t) {
+          // Suppress to keep throwing original exc
+        }
+        IOUtils.deleteFilesIgnoringExceptions(tempInput);
+      }
+    }
+
+    //System.out.println("Total nodes: " + innerNodeCount);
+
+    // Write index:
+    long indexFP = out.getFilePointer();
+    out.writeVInt(numLeaves);
+    out.writeVInt((int) (valueCount / numLeaves));
+
+    for (int i=0;i<blockMinValues.length;i++) {
+      out.writeLong(blockMinValues[i]);
+    }
+    for (int i=0;i<leafBlockFPs.length;i++) {
+      out.writeVLong(leafBlockFPs[i]);
+    }
+    out.writeLong(globalMaxValue);
+
+    if (tempDir != null) {
+      // If we had to go offline, we should have removed all temp files we wrote:
+      assert directoryIsEmpty(tempDir);
+      IOUtils.rm(tempDir);
+    }
+
+    return indexFP;
+  }
+
+  // Called only from assert
+  private boolean directoryIsEmpty(Path in) {
+    try (DirectoryStream<Path> dir = Files.newDirectoryStream(in)) {
+      for (Path path : dir) {
+        assert false: "dir=" + in + " still has file=" + path;
+        return false;
+      }
+    } catch (IOException ioe) {
+      // Just ignore: we are only called from assert
+    }
+    return true;
+  }
+
+  /** Sliced reference to points in an OfflineSorter.ByteSequencesWriter file. */
+  private static final class PathSlice {
+    final SliceWriter writer;
+    final long start;
+    final long count;
+
+    public PathSlice(SliceWriter writer, long start, long count) {
+      this.writer = writer;
+      this.start = start;
+      this.count = count;
+    }
+
+    @Override
+    public String toString() {
+      return "PathSlice(start=" + start + " count=" + count + " writer=" + writer + ")";
+    }
+  }
+
+  private long getSplitValue(PathSlice source, long leftCount, long minValue, long maxValue) throws IOException {
+
+    // Read the split value:
+    SliceReader reader = source.writer.getReader(source.start + leftCount);
+    boolean success = false;
+    long splitValue;
+    try {
+      boolean result = reader.next();
+      assert result;
+      splitValue = reader.value();
+      assert splitValue >= minValue && splitValue <= maxValue: "splitValue=" + splitValue + " minValue=" + minValue + " maxValue=" + maxValue + " reader=" + reader;
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(reader);
+      } else {
+        IOUtils.closeWhileHandlingException(reader);
+      }
+    }
+
+    return splitValue;
+  }
+
+  /** The incoming PathSlice for the dim we will split is already partitioned/sorted. */
+  private void build(int nodeID, int leafNodeOffset,
+                     PathSlice source,
+                     IndexOutput out,
+                     long minValue, long maxValue,
+                     long[] blockMinValues,
+                     long[] leafBlockFPs) throws IOException {
+
+    long count = source.count;
+
+    if (source.writer instanceof OfflineSliceWriter && count <= maxValuesSortInHeap) {
+      // Cutover to heap:
+      SliceWriter writer = new HeapSliceWriter((int) count);
+      SliceReader reader = source.writer.getReader(source.start);
+      for(int i=0;i<count;i++) {
+        boolean hasNext = reader.next();
+        assert hasNext;
+        writer.append(reader.value(), reader.ord(), reader.docID());
+      }
+      source = new PathSlice(writer, 0, count);
+    }
+
+    // We should never hit dead-end nodes on recursion even in the adversarial cases:
+    assert count > 0;
+
+    if (nodeID >= leafNodeOffset) {
+      // Leaf node: write block
+      assert maxValue >= minValue;
+
+      //System.out.println("\nleaf:\n  lat range: " + ((long) maxLatEnc-minLatEnc));
+      //System.out.println("  lon range: " + ((long) maxLonEnc-minLonEnc));
+
+      // Sort by docID in the leaf so we can .or(DISI) at search time:
+      SliceReader reader = source.writer.getReader(source.start);
+
+      int[] docIDs = new int[(int) count];
+
+      boolean success = false;
+      try {
+        for (int i=0;i<source.count;i++) {
+
+          // NOTE: we discard ord at this point; we only needed it temporarily
+          // during building to uniquely identify each point to properly handle
+          // the multi-valued case (one docID having multiple values):
+
+          // We also discard lat/lon, since at search time, we reside on the
+          // wrapped doc values for this:
+
+          boolean result = reader.next();
+          assert result;
+          docIDs[i] = reader.docID();
+        }
+        success = true;
+      } finally {
+        if (success) {
+          IOUtils.close(reader);
+        } else {
+          IOUtils.closeWhileHandlingException(reader);
+        }
+      }
+
+      // TODO: not clear we need to do this anymore (we used to make a DISI over
+      // the block at search time), but maybe it buys some memory
+      // locality/sequentiality at search time?
+      Arrays.sort(docIDs);
+
+      // Dedup docIDs: for the multi-valued case where more than one value for the doc
+      // wound up in this leaf cell, we only need to store the docID once:
+      int lastDocID = -1;
+      int uniqueCount = 0;
+      for(int i=0;i<docIDs.length;i++) {
+        int docID = docIDs[i];
+        if (docID != lastDocID) {
+          uniqueCount++;
+          lastDocID = docID;
+        }
+      }
+      assert uniqueCount <= count;
+
+      // TODO: in theory we could compute exactly what this fp will be, since we fixed-width (writeInt) encode docID, and up-front we know
+      // how many docIDs are in every leaf since we don't do anything special about multiple splitValue boundary case?
+      long startFP = out.getFilePointer();
+      out.writeVInt(uniqueCount);
+
+      // Save the block file pointer:
+      int blockID = nodeID - leafNodeOffset;
+      leafBlockFPs[blockID] = startFP;
+      //System.out.println("    leafFP=" + startFP);
+
+      blockMinValues[blockID] = minValue;
+
+      lastDocID = -1;
+      for (int i=0;i<docIDs.length;i++) {
+        // Absolute int encode; with "vInt of deltas" encoding, the .kdd size dropped from
+        // 697 MB -> 539 MB, but query time for 225 queries went from 1.65 sec -> 2.64 sec.
+        // I think if we also indexed prefix terms here we could do less costly compression
+        // on those lists:
+        int docID = docIDs[i];
+        if (docID != lastDocID) {
+          out.writeInt(docID);
+          lastDocID = docID;
+        }
+      }
+      //long endFP = out.getFilePointer();
+      //System.out.println("  bytes/doc: " + ((endFP - startFP) / count));
+    } else {
+      // Inner node: sort, partition/recurse
+
+      assert nodeID < blockMinValues.length: "nodeID=" + nodeID + " blockMinValues.length=" + blockMinValues.length;
+
+      assert source.count == count;
+
+      long leftCount = source.count / 2;
+
+      // NOTE: we don't tweak leftCount for the boundary cases, which means at search time if we are looking for exactly splitValue then we
+      // must search both left and right trees:
+      long splitValue = getSplitValue(source, leftCount, minValue, maxValue);
+
+      build(2*nodeID, leafNodeOffset,
+            new PathSlice(source.writer, source.start, leftCount),
+            out,
+            minValue, splitValue,
+            blockMinValues, leafBlockFPs);
+
+      build(2*nodeID+1, leafNodeOffset,
+            new PathSlice(source.writer, source.start+leftCount, count-leftCount),
+            out,
+            splitValue, maxValue,
+            blockMinValues, leafBlockFPs);
+    }
+  }
+
+  SliceWriter getWriter(long count) throws IOException {
+    if (count < maxValuesSortInHeap) {
+      return new HeapSliceWriter((int) count);
+    } else {
+      return new OfflineSliceWriter(tempDir, count);
+    }
+  }
+}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/rangetree/SliceReader.java b/lucene/sandbox/src/java/org/apache/lucene/rangetree/SliceReader.java
new file mode 100644
index 0000000..3256fee
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/rangetree/SliceReader.java
@@ -0,0 +1,31 @@
+package org.apache.lucene.rangetree;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable;
+import java.io.IOException;
+
+/** Iterates over one slice of the sorted values.  This abstracts away whether
+ *  OfflineSorter or simple arrays in heap are used. */
+interface SliceReader extends Closeable {
+  boolean next() throws IOException;
+  long value();
+  long ord();
+  int docID();
+}
+
diff --git a/lucene/sandbox/src/java/org/apache/lucene/rangetree/SliceWriter.java b/lucene/sandbox/src/java/org/apache/lucene/rangetree/SliceWriter.java
new file mode 100644
index 0000000..9850f09
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/rangetree/SliceWriter.java
@@ -0,0 +1,29 @@
+package org.apache.lucene.rangetree;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable;
+import java.io.IOException;
+
+/** Abstracts away whether OfflineSorter or simple arrays in heap are used. */
+interface SliceWriter extends Closeable {
+  void append(long value, long ord, int docID) throws IOException;
+  SliceReader getReader(long start) throws IOException;
+  void destroy() throws IOException;
+}
+
diff --git a/lucene/sandbox/src/java/org/apache/lucene/rangetree/SortedSetRangeTreeQuery.java b/lucene/sandbox/src/java/org/apache/lucene/rangetree/SortedSetRangeTreeQuery.java
new file mode 100644
index 0000000..348af84
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/rangetree/SortedSetRangeTreeQuery.java
@@ -0,0 +1,219 @@
+package org.apache.lucene.rangetree;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.document.SortedSetDocValuesField; // javadocs
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.index.SortedNumericDocValues;
+import org.apache.lucene.index.SortedSetDocValues;
+import org.apache.lucene.search.ConstantScoreScorer;
+import org.apache.lucene.search.ConstantScoreWeight;
+import org.apache.lucene.search.DocIdSet;
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.Scorer;
+import org.apache.lucene.search.Weight;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.ToStringUtils;
+
+import java.io.IOException;
+
+/** Finds all previously indexed values that fall within the specified {@link BytesRef} range.
+ *
+ * <p>The field must be indexed with {@link RangeTreeDocValuesFormat}, and {@link SortedSetDocValuesField} added per document.
+ *
+ * @lucene.experimental */
+
+public class SortedSetRangeTreeQuery extends Query {
+  final String field;
+  final BytesRef minValue;
+  final BytesRef maxValue;
+  final boolean minInclusive;
+  final boolean maxInclusive;
+
+  /** Matches all values in the specified {@link BytesRef} range. */ 
+  public SortedSetRangeTreeQuery(String field, BytesRef minValue, boolean minInclusive, BytesRef maxValue, boolean maxInclusive) {
+    this.field = field;
+    this.minInclusive = minInclusive;
+    this.minValue = minValue;
+    this.maxInclusive = maxInclusive;
+    this.maxValue = maxValue;
+  }
+
+  @Override
+  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {
+
+    // I don't use RandomAccessWeight here: it's no good to approximate with "match all docs"; this is an inverted structure and should be
+    // used in the first pass:
+
+    return new ConstantScoreWeight(this) {
+
+      @Override
+      public Scorer scorer(LeafReaderContext context) throws IOException {
+        LeafReader reader = context.reader();
+        final SortedSetDocValues ssdv = reader.getSortedSetDocValues(field);
+        if (ssdv == null) {
+          // No docs in this segment had this field
+          return null;
+        }
+
+        if (ssdv instanceof RangeTreeSortedSetDocValues == false) {
+          throw new IllegalStateException("field \"" + field + "\" was not indexed with RangeTreeDocValuesFormat: got: " + ssdv);
+        }
+        RangeTreeSortedSetDocValues treeDV = (RangeTreeSortedSetDocValues) ssdv;
+        RangeTreeReader tree = treeDV.getRangeTreeReader();
+
+        /*
+        for(int i=0;i<treeDV.getValueCount();i++) {
+          System.out.println("  ord " + i + " -> " + treeDV.lookupOrd(i));
+        }
+        */
+
+        // lower
+        final long minOrdIncl;
+        if (minValue == null) {
+          minOrdIncl = 0;
+        } else {
+          long ord = ssdv.lookupTerm(minValue);
+          if (ord >= 0) {
+            // Exact match
+            if (minInclusive) {
+              minOrdIncl = ord;
+            } else {
+              minOrdIncl = ord+1;
+            }
+          } else {
+            minOrdIncl = -ord-1;
+          }
+        }
+
+        // upper
+        final long maxOrdIncl;
+        if (maxValue == null) {
+          maxOrdIncl = Long.MAX_VALUE;
+        } else {
+          long ord = ssdv.lookupTerm(maxValue);
+          if (ord >= 0) {
+            // Exact match
+            if (maxInclusive) {
+              maxOrdIncl = ord;
+            } else {
+              maxOrdIncl = ord-1;
+            }
+          } else {
+            maxOrdIncl = -ord-2;
+          }
+        }
+
+        if (maxOrdIncl < minOrdIncl) {  
+          // This can happen when the requested range lies entirely between 2 adjacent ords:
+          return null;
+        }
+
+        //System.out.println(reader + ": ORD: " + minOrdIncl + "-" + maxOrdIncl + "; " + minValue + " - " + maxValue);
+        
+        // Just a "view" of only the ords from the SSDV, as an SNDV.  Maybe we
+        // have this view implemented somewhere else already?  It's not so bad that
+        // we are inefficient here (making 2 passes over the ords): this is only
+        // used in at most 2 leaf cells (the boundary cells).
+        SortedNumericDocValues ords = new SortedNumericDocValues() {
+
+            private long[] ords = new long[2];
+            private int count;
+
+            @Override
+            public void setDocument(int doc) {
+              ssdv.setDocument(doc);
+              long ord;
+              count = 0;
+              while ((ord = ssdv.nextOrd()) != SortedSetDocValues.NO_MORE_ORDS) {
+                if (count == ords.length) {
+                  ords = ArrayUtil.grow(ords, count+1);
+                }
+                ords[count++] = ord;
+              }
+            }
+
+            @Override
+            public int count() {
+              return count;
+            }
+
+            @Override
+            public long valueAt(int index) {
+              return ords[index];
+            }
+          };
+
+        DocIdSet result = tree.intersect(minOrdIncl, maxOrdIncl, ords, context.reader().maxDoc());
+
+        final DocIdSetIterator disi = result.iterator();
+
+        return new ConstantScoreScorer(this, score(), disi);
+      }
+    };
+  }
+
+  @Override
+  public int hashCode() {
+    int hash = super.hashCode();
+    if (minValue != null) hash += minValue.hashCode()^0x14fa55fb;
+    if (maxValue != null) hash += maxValue.hashCode()^0x733fa5fe;
+    return hash +
+      (Boolean.valueOf(minInclusive).hashCode()^0x14fa55fb)+
+      (Boolean.valueOf(maxInclusive).hashCode()^0x733fa5fe);
+  }
+
+  @Override
+  public boolean equals(Object other) {
+    if (super.equals(other)) {
+      final SortedSetRangeTreeQuery q = (SortedSetRangeTreeQuery) other;
+      return (
+        (q.minValue == null ? minValue == null : q.minValue.equals(minValue)) &&
+        (q.maxValue == null ? maxValue == null : q.maxValue.equals(maxValue)) &&
+        minInclusive == q.minInclusive &&
+        maxInclusive == q.maxInclusive
+      );
+    }
+
+    return false;
+  }
+
+  @Override
+  public String toString(String field) {
+    final StringBuilder sb = new StringBuilder();
+    sb.append(getClass().getSimpleName());
+    sb.append(':');
+    if (this.field.equals(field) == false) {
+      sb.append("field=");
+      sb.append(this.field);
+      sb.append(':');
+    }
+
+    return sb.append(minInclusive ? '[' : '{')
+      .append((minValue == null) ? "*" : minValue.toString())
+      .append(" TO ")
+      .append((maxValue == null) ? "*" : maxValue.toString())
+      .append(maxInclusive ? ']' : '}')
+      .append(ToStringUtils.boost(getBoost()))
+      .toString();
+  }
+}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/rangetree/package.html b/lucene/sandbox/src/java/org/apache/lucene/rangetree/package.html
new file mode 100644
index 0000000..e657418
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/rangetree/package.html
@@ -0,0 +1,28 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+
+<!-- not a package-info.java, because we already defined this package in core/ -->
+
+<html>
+<head>
+   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
+</head>
+<body>
+This package contains a numeric tree implementation for indexing long values enabling fast range searching.
+</body>
+</html>
diff --git a/lucene/sandbox/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat b/lucene/sandbox/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat
index 49d2b2e..e1bb624 100644
--- a/lucene/sandbox/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat
+++ b/lucene/sandbox/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat
@@ -14,4 +14,5 @@
 #  limitations under the License.
 
 org.apache.lucene.bkdtree.BKDTreeDocValuesFormat
+org.apache.lucene.rangetree.RangeTreeDocValuesFormat
 
diff --git a/lucene/sandbox/src/test/org/apache/lucene/rangetree/TestRangeTree.java b/lucene/sandbox/src/test/org/apache/lucene/rangetree/TestRangeTree.java
new file mode 100644
index 0000000..1641d1f
--- /dev/null
+++ b/lucene/sandbox/src/test/org/apache/lucene/rangetree/TestRangeTree.java
@@ -0,0 +1,764 @@
+package org.apache.lucene.rangetree;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.lucene53.Lucene53Codec;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.document.SortedNumericDocValuesField;
+import org.apache.lucene.document.SortedSetDocValuesField;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.index.MultiDocValues;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.SimpleCollector;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.Accountable;
+import org.apache.lucene.util.Accountables;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.FixedBitSet;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.TestUtil;
+import org.junit.BeforeClass;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Set;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+// TODO: can test framework assert we don't leak temp files?
+
+public class TestRangeTree extends LuceneTestCase {
+
+  // Controls what range of values we randomly generate, so we sometimes test narrow ranges:
+  static long valueMid;
+  static int valueRange;
+
+  @BeforeClass
+  public static void beforeClass() {
+    if (random().nextBoolean()) {
+      valueMid = random().nextLong();
+      if (random().nextBoolean()) {
+        // Wide range
+        valueRange = TestUtil.nextInt(random(), 1, Integer.MAX_VALUE);
+      } else {
+        // Narrow range
+        valueRange = TestUtil.nextInt(random(), 1, 100000);
+      }
+      if (VERBOSE) {
+        System.out.println("TEST: will generate long values " + valueMid + " +/- " + valueRange);
+      }
+    } else {
+      // All longs
+      valueRange = 0;
+      if (VERBOSE) {
+        System.out.println("TEST: will generate all long values");
+      }
+    }
+  }
+
+  public void testAllEqual() throws Exception {
+    int numValues = atLeast(10000);
+    long value = randomValue();
+    long[] values = new long[numValues];
+    FixedBitSet missing = new FixedBitSet(numValues);
+
+    if (VERBOSE) {
+      System.out.println("TEST: use same value=" + value);
+    }
+
+    for(int docID=0;docID<numValues;docID++) {
+      int x = random().nextInt(20);
+      if (x == 17) {
+        // Some docs don't have a point:
+        missing.set(docID);
+        if (VERBOSE) {
+          System.out.println("  doc=" + docID + " is missing");
+        }
+        continue;
+      }
+      values[docID] = value;
+    }
+
+    verify(missing, values);
+  }
+
+  public void testMultiValued() throws Exception {
+    int numValues = atLeast(10000);
+    // Every doc has 2 values:
+    long[] values = new long[2*numValues];
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig();
+
+    // We rely on docID order:
+    iwc.setMergePolicy(newLogMergePolicy());
+    int maxPointsInLeaf = TestUtil.nextInt(random(), 16, 2048);
+    int maxPointsSortInHeap = TestUtil.nextInt(random(), 1024, 1024*1024);
+    Codec codec = TestUtil.alwaysDocValuesFormat(new RangeTreeDocValuesFormat(maxPointsInLeaf, maxPointsSortInHeap));
+    iwc.setCodec(codec);
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+
+    for (int docID=0;docID<numValues;docID++) {
+      Document doc = new Document();
+      values[2*docID] = randomValue();
+      doc.add(new SortedNumericDocValuesField("value", values[2*docID]));
+      values[2*docID+1] = randomValue();
+      doc.add(new SortedNumericDocValuesField("value", values[2*docID+1]));
+      w.addDocument(doc);
+    }
+
+    if (random().nextBoolean()) {
+      w.forceMerge(1);
+    }
+    IndexReader r = w.getReader();
+    w.close();
+    // We can't wrap with "exotic" readers because the NumericRangeTreeQuery must see the NumericTreeDVFormat:
+    IndexSearcher s = newSearcher(r, false);
+
+    int iters = atLeast(100);
+    for (int iter=0;iter<iters;iter++) {
+      long lower = randomValue();
+      long upper = randomValue();
+
+      if (upper < lower) {
+        long x = lower;
+        lower = upper;
+        upper = x;
+      }
+
+      if (VERBOSE) {
+        System.out.println("\nTEST: iter=" + iter + " value=" + lower + " TO " + upper);
+      }
+
+      boolean includeLower = random().nextBoolean();
+      boolean includeUpper = random().nextBoolean();
+      Query query = new NumericRangeTreeQuery("value", lower, includeLower, upper, includeUpper);
+
+      final FixedBitSet hits = new FixedBitSet(r.maxDoc());
+      s.search(query, new SimpleCollector() {
+
+          private int docBase;
+
+          @Override
+          public boolean needsScores() {
+            return false;
+          }
+
+          @Override
+          protected void doSetNextReader(LeafReaderContext context) throws IOException {
+            docBase = context.docBase;
+          }
+
+          @Override
+          public void collect(int doc) {
+            hits.set(docBase+doc);
+          }
+        });
+
+      for(int docID=0;docID<values.length/2;docID++) {
+        long docValue1 = values[2*docID];
+        long docValue2 = values[2*docID+1];
+        boolean expected = matches(lower, includeLower, upper, includeUpper, docValue1) ||
+          matches(lower, includeLower, upper, includeUpper, docValue2);
+
+        if (hits.get(docID) != expected) {
+          fail("docID=" + docID + " docValue1=" + docValue1 + " docValue2=" + docValue2 + " expected " + expected + " but got: " + hits.get(docID));
+        }
+      }
+    }
+    r.close();
+    dir.close();
+  }
+
+  public void testMultiValuedSortedSet() throws Exception {
+    int numValues = atLeast(10000);
+    // Every doc has 2 values:
+    long[] values = new long[2*numValues];
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig();
+
+    // We rely on docID order:
+    iwc.setMergePolicy(newLogMergePolicy());
+    int maxPointsInLeaf = TestUtil.nextInt(random(), 16, 2048);
+    int maxPointsSortInHeap = TestUtil.nextInt(random(), 1024, 1024*1024);
+    Codec codec = TestUtil.alwaysDocValuesFormat(new RangeTreeDocValuesFormat(maxPointsInLeaf, maxPointsSortInHeap));
+    iwc.setCodec(codec);
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+
+    for (int docID=0;docID<numValues;docID++) {
+      Document doc = new Document();
+      values[2*docID] = randomValue();
+      doc.add(new SortedSetDocValuesField("value", longToBytes(values[2*docID])));
+      values[2*docID+1] = randomValue();
+      doc.add(new SortedSetDocValuesField("value", longToBytes(values[2*docID+1])));
+      w.addDocument(doc);
+    }
+
+    if (random().nextBoolean()) {
+      w.forceMerge(1);
+    }
+    IndexReader r = w.getReader();
+    w.close();
+    // We can't wrap with "exotic" readers because the NumericRangeTreeQuery must see the NumericTreeDVFormat:
+    IndexSearcher s = newSearcher(r, false);
+
+    int iters = atLeast(100);
+    for (int iter=0;iter<iters;iter++) {
+      long lower = randomValue();
+      long upper = randomValue();
+
+      if (upper < lower) {
+        long x = lower;
+        lower = upper;
+        upper = x;
+      }
+
+      if (VERBOSE) {
+        System.out.println("\nTEST: iter=" + iter + " value=" + lower + " TO " + upper);
+      }
+
+      boolean includeLower = random().nextBoolean();
+      boolean includeUpper = random().nextBoolean();
+      Query query = new SortedSetRangeTreeQuery("value", longToBytes(lower), includeLower, longToBytes(upper), includeUpper);
+
+      final FixedBitSet hits = new FixedBitSet(r.maxDoc());
+      s.search(query, new SimpleCollector() {
+
+          private int docBase;
+
+          @Override
+          public boolean needsScores() {
+            return false;
+          }
+
+          @Override
+          protected void doSetNextReader(LeafReaderContext context) throws IOException {
+            docBase = context.docBase;
+          }
+
+          @Override
+          public void collect(int doc) {
+            hits.set(docBase+doc);
+          }
+        });
+
+      for(int docID=0;docID<values.length/2;docID++) {
+        long docValue1 = values[2*docID];
+        long docValue2 = values[2*docID+1];
+        boolean expected = matches(lower, includeLower, upper, includeUpper, docValue1) ||
+          matches(lower, includeLower, upper, includeUpper, docValue2);
+
+        if (hits.get(docID) != expected) {
+          fail("docID=" + docID + " docValue1=" + docValue1 + " docValue2=" + docValue2 + " expected " + expected + " but got: " + hits.get(docID));
+        }
+      }
+    }
+    r.close();
+    dir.close();
+  }
+
+  public void testRandomTiny() throws Exception {
+    // Make sure single-leaf-node case is OK:
+    doTestRandom(10);
+  }
+
+  public void testRandomMedium() throws Exception {
+    doTestRandom(10000);
+  }
+
+  @Nightly
+  public void testRandomBig() throws Exception {
+    doTestRandom(200000);
+  }
+
+  private void doTestRandom(int count) throws Exception {
+
+    int numValues = atLeast(count);
+
+    if (VERBOSE) {
+      System.out.println("TEST: numValues=" + numValues);
+    }
+
+    long[] values = new long[numValues];
+    FixedBitSet missing = new FixedBitSet(numValues);
+
+    boolean haveRealDoc = false;
+
+    for (int docID=0;docID<numValues;docID++) {
+      int x = random().nextInt(20);
+      if (x == 17) {
+        // Some docs don't have a point:
+        missing.set(docID);
+        if (VERBOSE) {
+          System.out.println("  doc=" + docID + " is missing");
+        }
+        continue;
+      }
+
+      if (docID > 0 && x == 0 && haveRealDoc) {
+        int oldDocID;
+        while (true) {
+          oldDocID = random().nextInt(docID);
+          if (missing.get(oldDocID) == false) {
+            break;
+          }
+        }
+            
+        // Identical to old value
+        values[docID] = values[oldDocID];
+        if (VERBOSE) {
+          System.out.println("  doc=" + docID + " value=" + values[docID] + " bytes=" + longToBytes(values[docID]) + " (same as doc=" + oldDocID + ")");
+        }
+      } else {
+        values[docID] = randomValue();
+        haveRealDoc = true;
+        if (VERBOSE) {
+          System.out.println("  doc=" + docID + " value=" + values[docID] + " bytes=" + longToBytes(values[docID]));
+        }
+      }
+    }
+
+    verify(missing, values);
+  }
+
+  private static void verify(Bits missing, long[] values) throws Exception {
+    int maxPointsInLeaf = TestUtil.nextInt(random(), 16, 2048);
+    int maxPointsSortInHeap = TestUtil.nextInt(random(), maxPointsInLeaf, 1024*1024);
+    IndexWriterConfig iwc = newIndexWriterConfig();
+
+    // Else we can get O(N^2) merging:
+    int mbd = iwc.getMaxBufferedDocs();
+    if (mbd != -1 && mbd < values.length/100) {
+      iwc.setMaxBufferedDocs(values.length/100);
+    }
+    final DocValuesFormat dvFormat = new RangeTreeDocValuesFormat(maxPointsInLeaf, maxPointsSortInHeap);
+    Codec codec = new Lucene53Codec() {
+        @Override
+        public DocValuesFormat getDocValuesFormatForField(String field) {
+          if (field.equals("sn_value") || field.equals("ss_value")) {
+            return dvFormat;
+          } else {
+            return super.getDocValuesFormatForField(field);
+          }
+        }
+      };
+    iwc.setCodec(codec);
+    Directory dir;
+    if (values.length > 100000) {
+      dir = newFSDirectory(createTempDir("TestRangeTree"));
+    } else {
+      dir = newDirectory();
+    }
+    Set<Integer> deleted = new HashSet<>();
+    // RandomIndexWriter is too slow here:
+    IndexWriter w = new IndexWriter(dir, iwc);
+    for(int id=0;id<values.length;id++) {
+      Document doc = new Document();
+      doc.add(newStringField("id", ""+id, Field.Store.NO));
+      doc.add(new NumericDocValuesField("id", id));
+      if (missing.get(id) == false) {
+        doc.add(new SortedNumericDocValuesField("sn_value", values[id]));
+        doc.add(new SortedSetDocValuesField("ss_value", longToBytes(values[id])));
+      }
+      w.addDocument(doc);
+      if (id > 0 && random().nextInt(100) == 42) {
+        int idToDelete = random().nextInt(id);
+        w.deleteDocuments(new Term("id", ""+idToDelete));
+        deleted.add(idToDelete);
+        if (VERBOSE) {
+          System.out.println("  delete id=" + idToDelete);
+        }
+      }
+    }
+    if (random().nextBoolean()) {
+      w.forceMerge(1);
+    }
+    final IndexReader r = DirectoryReader.open(w, true);
+    w.close();
+
+    // We can't wrap with "exotic" readers because the NumericRangeTreeQuery must see the NumericTreeDVFormat:
+    IndexSearcher s = newSearcher(r, false);
+
+    int numThreads = TestUtil.nextInt(random(), 2, 5);
+
+    if (VERBOSE) {
+      System.out.println("TEST: use " + numThreads + " query threads");
+    }
+
+    List<Thread> threads = new ArrayList<>();
+    final int iters = atLeast(100);
+
+    final CountDownLatch startingGun = new CountDownLatch(1);
+    final AtomicBoolean failed = new AtomicBoolean();
+
+    for(int i=0;i<numThreads;i++) {
+      Thread thread = new Thread() {
+          @Override
+          public void run() {
+            try {
+              _run();
+            } catch (Exception e) {
+              failed.set(true);
+              throw new RuntimeException(e);
+            }
+          }
+
+          private void _run() throws Exception {
+            startingGun.await();
+
+            NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, "id");
+
+            for (int iter=0;iter<iters && failed.get() == false;iter++) {
+              long lower = randomValue();
+              long upper = randomValue();
+
+              if (upper < lower) {
+                long x = lower;
+                lower = upper;
+                upper = x;
+              }
+
+              if (VERBOSE) {
+                System.out.println("\n" + Thread.currentThread().getName() + ": TEST: iter=" + iter + " value=" + lower + " TO " + upper);
+              }
+
+              boolean includeLower = random().nextBoolean();
+              boolean includeUpper = random().nextBoolean();
+              Query query;
+              if (random().nextBoolean()) {
+                query = new NumericRangeTreeQuery("sn_value", lower, includeLower, upper, includeUpper);
+              } else {
+                query = new SortedSetRangeTreeQuery("ss_value", longToBytes(lower), includeLower, longToBytes(upper), includeUpper);
+              }
+
+              if (VERBOSE) {
+                System.out.println(Thread.currentThread().getName() + ":  using query: " + query);
+              }
+
+              final FixedBitSet hits = new FixedBitSet(r.maxDoc());
+              s.search(query, new SimpleCollector() {
+
+                  private int docBase;
+
+                  @Override
+                  public boolean needsScores() {
+                    return false;
+                  }
+
+                  @Override
+                  protected void doSetNextReader(LeafReaderContext context) throws IOException {
+                    docBase = context.docBase;
+                  }
+
+                  @Override
+                  public void collect(int doc) {
+                    hits.set(docBase+doc);
+                  }
+                });
+
+              if (VERBOSE) {
+                System.out.println(Thread.currentThread().getName() + ":  hitCount: " + hits.cardinality());
+              }
+      
+              for(int docID=0;docID<r.maxDoc();docID++) {
+                int id = (int) docIDToID.get(docID);
+                boolean expected = missing.get(id) == false && deleted.contains(id) == false && matches(lower, includeLower, upper, includeUpper, values[id]);
+                if (hits.get(docID) != expected) {
+                  // We do exact quantized comparison so the bbox query should never disagree:
+                  fail(Thread.currentThread().getName() + ": iter=" + iter + " id=" + id + " docID=" + docID + " value=" + values[id] + " (range: " + lower + " TO " + upper + ") expected " + expected + " but got: " + hits.get(docID) + " deleted?=" + deleted.contains(id) + " query=" + query);
+                  }
+                }
+              }
+            }
+        };
+      thread.setName("T" + i);
+      thread.start();
+      threads.add(thread);
+    }
+    startingGun.countDown();
+    for(Thread thread : threads) {
+      thread.join();
+    }
+    IOUtils.close(r, dir);
+  }
+
+  private static boolean matches(long lower, boolean includeLower, long upper, boolean includeUpper, long value) {
+    if (value > lower && value < upper) {
+      return true;
+    }
+    if (value == lower && includeLower) {
+      return true;
+    }
+    if (value == upper && includeUpper) {
+      return true;
+    }
+    return false;
+  }
+
+  private static long randomValue() {
+    if (valueRange == 0) {
+      return random().nextLong();
+    } else {
+      return valueMid + TestUtil.nextInt(random(), -valueRange, valueRange);
+    }
+  }
+
+  public void testAccountableHasDelegate() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig();
+    Codec codec = TestUtil.alwaysDocValuesFormat(new RangeTreeDocValuesFormat());
+    iwc.setCodec(codec);
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+    Document doc = new Document();
+    doc.add(new SortedNumericDocValuesField("value", 187));
+    w.addDocument(doc);
+    IndexReader r = w.getReader();
+
+    // We can't wrap with "exotic" readers because the query must see the NumericTreeDVFormat:
+    IndexSearcher s = newSearcher(r, false);
+    // Need to run a query so the DV field is really loaded:
+    TopDocs hits = s.search(new NumericRangeTreeQuery("value", -30L, true, 187L, true), 1);
+    assertEquals(1, hits.totalHits);
+    assertTrue(Accountables.toString((Accountable) r.leaves().get(0).reader()).contains("delegate"));
+    IOUtils.close(r, w, dir);
+  }
+
+  public void testMinMaxLong() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig();
+    Codec codec = TestUtil.alwaysDocValuesFormat(new RangeTreeDocValuesFormat());
+    iwc.setCodec(codec);
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+    Document doc = new Document();
+    doc.add(new SortedNumericDocValuesField("value", Long.MIN_VALUE));
+    w.addDocument(doc);
+
+    doc = new Document();
+    doc.add(new SortedNumericDocValuesField("value", Long.MAX_VALUE));
+    w.addDocument(doc);
+
+    IndexReader r = w.getReader();
+
+    // We can't wrap with "exotic" readers because the query must see the NumericTreeDVFormat:
+    IndexSearcher s = newSearcher(r, false);
+
+    assertEquals(1, s.count(new NumericRangeTreeQuery("value", Long.MIN_VALUE, true, 0L, true)));
+    assertEquals(1, s.count(new NumericRangeTreeQuery("value", 0L, true, Long.MAX_VALUE, true)));
+    assertEquals(2, s.count(new NumericRangeTreeQuery("value", Long.MIN_VALUE, true, Long.MAX_VALUE, true)));
+
+    IOUtils.close(r, w, dir);
+  }
+
+  public void testBasicSortedSet() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig();
+    Codec codec = TestUtil.alwaysDocValuesFormat(new RangeTreeDocValuesFormat());
+    iwc.setCodec(codec);
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("abc")));
+    w.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("def")));
+    w.addDocument(doc);
+
+    IndexReader r = w.getReader();
+
+    // We can't wrap with "exotic" readers because the query must see the NumericTreeDVFormat:
+    IndexSearcher s = newSearcher(r, false);
+
+    assertEquals(1, s.count(new SortedSetRangeTreeQuery("value", new BytesRef("aaa"), true, new BytesRef("bbb"), true)));
+    assertEquals(1, s.count(new SortedSetRangeTreeQuery("value", new BytesRef("c"), true, new BytesRef("e"), true)));
+    assertEquals(2, s.count(new SortedSetRangeTreeQuery("value", new BytesRef("a"), true, new BytesRef("z"), true)));
+
+    assertEquals(1, s.count(new SortedSetRangeTreeQuery("value", null, true, new BytesRef("abc"), true)));
+    assertEquals(1, s.count(new SortedSetRangeTreeQuery("value", new BytesRef("a"), true, new BytesRef("abc"), true)));
+    assertEquals(0, s.count(new SortedSetRangeTreeQuery("value", new BytesRef("a"), true, new BytesRef("abc"), false)));
+
+    assertEquals(1, s.count(new SortedSetRangeTreeQuery("value", new BytesRef("def"), true, null, false)));
+    assertEquals(1, s.count(new SortedSetRangeTreeQuery("value", new BytesRef("def"), true, new BytesRef("z"), true)));
+    assertEquals(0, s.count(new SortedSetRangeTreeQuery("value", new BytesRef("def"), false, new BytesRef("z"), true)));
+
+    IOUtils.close(r, w, dir);
+  }
+
+  public void testLongMinMaxNumeric() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig();
+    Codec codec = TestUtil.alwaysDocValuesFormat(new RangeTreeDocValuesFormat());
+    iwc.setCodec(codec);
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+    Document doc = new Document();
+    doc.add(new SortedNumericDocValuesField("value", Long.MIN_VALUE));
+    w.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedNumericDocValuesField("value", Long.MAX_VALUE));
+    w.addDocument(doc);
+
+    IndexReader r = w.getReader();
+
+    // We can't wrap with "exotic" readers because the query must see the NumericTreeDVFormat:
+    IndexSearcher s = newSearcher(r, false);
+
+    assertEquals(2, s.count(new NumericRangeTreeQuery("value", Long.MIN_VALUE, true, Long.MAX_VALUE, true)));
+    assertEquals(1, s.count(new NumericRangeTreeQuery("value", Long.MIN_VALUE, true, Long.MAX_VALUE, false)));
+    assertEquals(1, s.count(new NumericRangeTreeQuery("value", Long.MIN_VALUE, false, Long.MAX_VALUE, true)));
+    assertEquals(0, s.count(new NumericRangeTreeQuery("value", Long.MIN_VALUE, false, Long.MAX_VALUE, false)));
+
+    assertEquals(2, s.count(new NumericRangeTreeQuery("value", null, true, null, true)));
+
+    IOUtils.close(r, w, dir);
+  }
+
+  public void testLongMinMaxSortedSet() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig();
+    Codec codec = TestUtil.alwaysDocValuesFormat(new RangeTreeDocValuesFormat());
+    iwc.setCodec(codec);
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", longToBytes(Long.MIN_VALUE)));
+    w.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", longToBytes(Long.MAX_VALUE)));
+    w.addDocument(doc);
+
+    IndexReader r = w.getReader();
+
+    // We can't wrap with "exotic" readers because the query must see the NumericTreeDVFormat:
+    IndexSearcher s = newSearcher(r, false);
+
+    assertEquals(2, s.count(new SortedSetRangeTreeQuery("value", longToBytes(Long.MIN_VALUE), true, longToBytes(Long.MAX_VALUE), true)));
+    assertEquals(1, s.count(new SortedSetRangeTreeQuery("value", longToBytes(Long.MIN_VALUE), true, longToBytes(Long.MAX_VALUE), false)));
+    assertEquals(1, s.count(new SortedSetRangeTreeQuery("value", longToBytes(Long.MIN_VALUE), false, longToBytes(Long.MAX_VALUE), true)));
+    assertEquals(0, s.count(new SortedSetRangeTreeQuery("value", longToBytes(Long.MIN_VALUE), false, longToBytes(Long.MAX_VALUE), false)));
+
+    assertEquals(2, s.count(new SortedSetRangeTreeQuery("value", null, true, null, true)));
+
+    IOUtils.close(r, w, dir);
+  }
+
+  public void testSortedSetNoOrdsMatch() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig();
+    Codec codec = TestUtil.alwaysDocValuesFormat(new RangeTreeDocValuesFormat());
+    iwc.setCodec(codec);
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("a")));
+    w.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("z")));
+    w.addDocument(doc);
+
+    IndexReader r = w.getReader();
+
+    // We can't wrap with "exotic" readers because the query must see the NumericTreeDVFormat:
+    IndexSearcher s = newSearcher(r, false);
+    assertEquals(0, s.count(new SortedSetRangeTreeQuery("value", new BytesRef("m"), true, new BytesRef("n"), false)));
+
+    assertEquals(2, s.count(new SortedSetRangeTreeQuery("value", null, true, null, true)));
+
+    IOUtils.close(r, w, dir);
+  }
+
+  public void testNumericNoValuesMatch() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig();
+    Codec codec = TestUtil.alwaysDocValuesFormat(new RangeTreeDocValuesFormat());
+    iwc.setCodec(codec);
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+    Document doc = new Document();
+    doc.add(new SortedNumericDocValuesField("value", 17));
+    w.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedNumericDocValuesField("value", 22));
+    w.addDocument(doc);
+
+    IndexReader r = w.getReader();
+
+    // We can't wrap with "exotic" readers because the query must see the NumericTreeDVFormat:
+    IndexSearcher s = newSearcher(r, false);
+    assertEquals(0, s.count(new NumericRangeTreeQuery("value", 17L, true, 13L, false)));
+
+    IOUtils.close(r, w, dir);
+  }
+
+  public void testNoDocs() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig();
+    Codec codec = TestUtil.alwaysDocValuesFormat(new RangeTreeDocValuesFormat());
+    iwc.setCodec(codec);
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+    w.addDocument(new Document());
+
+    IndexReader r = w.getReader();
+
+    // We can't wrap with "exotic" readers because the query must see the NumericTreeDVFormat:
+    IndexSearcher s = newSearcher(r, false);
+    assertEquals(0, s.count(new NumericRangeTreeQuery("value", 17L, true, 13L, false)));
+
+    IOUtils.close(r, w, dir);
+  }
+
+  private static BytesRef longToBytes(long v) {
+    // Flip the sign bit so negative longs sort before positive longs:
+    v ^= 0x8000000000000000L;
+    byte[] bytes = new byte[8];
+    bytes[0] = (byte) (v >> 56);
+    bytes[1] = (byte) (v >> 48);
+    bytes[2] = (byte) (v >> 40);
+    bytes[3] = (byte) (v >> 32);
+    bytes[4] = (byte) (v >> 24);
+    bytes[5] = (byte) (v >> 16);
+    bytes[6] = (byte) (v >> 8);
+    bytes[7] = (byte) v;
+    return new BytesRef(bytes);
+  }
+
+  /*
+  private static long bytesToLong(BytesRef bytes) {
+    long v = ((bytes.bytes[bytes.offset]&0xFFL) << 56) |
+      ((bytes.bytes[bytes.offset+1]&0xFFL) << 48) |
+      ((bytes.bytes[bytes.offset+2]&0xFFL) << 40) |
+      ((bytes.bytes[bytes.offset+3]&0xFFL) << 32) |
+      ((bytes.bytes[bytes.offset+4]&0xFFL) << 24) |
+      ((bytes.bytes[bytes.offset+5]&0xFFL) << 16) |
+      ((bytes.bytes[bytes.offset+6]&0xFFL) << 8) |
+      (bytes.bytes[bytes.offset+7]&0xFFL);
+    // Flip the sign bit back:
+    return v ^ 0x8000000000000000L;
+  }
+  */
+}

