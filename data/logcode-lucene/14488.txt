GitDiffStart: 873f1999244555a1e1b3e14698e7f9494f4ef00c | Wed Nov 16 19:09:35 2011 +0000
diff --git a/dev-tools/eclipse/dot.classpath b/dev-tools/eclipse/dot.classpath
index ee6a742..ff557ce 100644
--- a/dev-tools/eclipse/dot.classpath
+++ b/dev-tools/eclipse/dot.classpath
@@ -10,8 +10,6 @@
 	<classpathentry kind="src" path="lucene/contrib/demo/src/test"/>
 	<classpathentry kind="src" path="lucene/contrib/highlighter/src/java"/>
 	<classpathentry kind="src" path="lucene/contrib/highlighter/src/test"/>
-	<classpathentry kind="src" path="lucene/contrib/instantiated/src/java"/>
-	<classpathentry kind="src" path="lucene/contrib/instantiated/src/test"/>
 	<classpathentry kind="src" path="lucene/contrib/memory/src/java"/>
 	<classpathentry kind="src" path="lucene/contrib/memory/src/test"/>
 	<classpathentry kind="src" path="lucene/contrib/misc/src/java"/>
diff --git a/dev-tools/idea/lucene/contrib/instantiated/instantiated.iml b/dev-tools/idea/lucene/contrib/instantiated/instantiated.iml
deleted file mode 100644
index 208c179..0000000
--- a/dev-tools/idea/lucene/contrib/instantiated/instantiated.iml
+++ /dev/null
@@ -1,17 +0,0 @@
-<?xml version="1.0" encoding="UTF-8"?>
-<module type="JAVA_MODULE" version="4">
-  <component name="NewModuleRootManager" inherit-compiler-output="false">
-    <output url="file://$MODULE_DIR$/../../build/contrib/instantiated/classes/java" />
-    <output-test url="file://$MODULE_DIR$/../../build/contrib/instantiated/classes/test" />
-    <exclude-output />
-    <content url="file://$MODULE_DIR$">
-      <sourceFolder url="file://$MODULE_DIR$/src/java" isTestSource="false" />
-      <sourceFolder url="file://$MODULE_DIR$/src/test" isTestSource="true" />
-    </content>
-    <orderEntry type="inheritedJdk" />
-    <orderEntry type="sourceFolder" forTests="false" />
-    <orderEntry type="library" scope="TEST" name="JUnit" level="project" />
-    <orderEntry type="module" module-name="misc" />
-    <orderEntry type="module" module-name="lucene" />
-  </component>
-</module>
diff --git a/dev-tools/maven/lucene/contrib/instantiated/pom.xml.template b/dev-tools/maven/lucene/contrib/instantiated/pom.xml.template
deleted file mode 100644
index 147a60e..0000000
--- a/dev-tools/maven/lucene/contrib/instantiated/pom.xml.template
+++ /dev/null
@@ -1,66 +0,0 @@
-<project xmlns="http://maven.apache.org/POM/4.0.0"
-         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
-  <!--
-    Licensed to the Apache Software Foundation (ASF) under one
-    or more contributor license agreements.  See the NOTICE file
-    distributed with this work for additional information
-    regarding copyright ownership.  The ASF licenses this file
-    to you under the Apache License, Version 2.0 (the
-    "License"); you may not use this file except in compliance
-    with the License.  You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-    Unless required by applicable law or agreed to in writing,
-    software distributed under the License is distributed on an
-    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-    KIND, either express or implied.  See the License for the
-    specific language governing permissions and limitations
-    under the License.
-  -->
-  <modelVersion>4.0.0</modelVersion>
-  <parent>
-    <groupId>org.apache.lucene</groupId>
-    <artifactId>lucene-parent</artifactId>
-    <version>@version@</version>
-    <relativePath>../../pom.xml</relativePath>
-  </parent>
-  <groupId>org.apache.lucene</groupId>
-  <artifactId>lucene-instantiated</artifactId>
-  <packaging>jar</packaging>
-  <name>Lucene InstantiatedIndex</name>
-  <description>InstantiatedIndex, alternative RAM store for small corpora.</description>
-  <properties>
-    <module-directory>lucene/contrib/instantiated</module-directory>
-    <build-directory>../../build/contrib/instantiated</build-directory>
-  </properties>
-  <dependencies>
-    <dependency>
-      <groupId>${project.groupId}</groupId>
-      <artifactId>lucene-core</artifactId>
-      <version>${project.version}</version>
-    </dependency>
-    <dependency>
-      <groupId>${project.groupId}</groupId>
-      <artifactId>lucene-test-framework</artifactId>
-      <version>${project.version}</version>
-      <scope>test</scope>
-    </dependency>
-  </dependencies>
-  <build>
-    <directory>${build-directory}</directory>
-    <outputDirectory>${build-directory}/classes/java</outputDirectory>
-    <testOutputDirectory>${build-directory}/classes/test</testOutputDirectory>
-    <sourceDirectory>src/java</sourceDirectory>
-    <testSourceDirectory>src/test</testSourceDirectory>
-    <testResources>
-      <testResource>
-        <directory>${project.build.testSourceDirectory}</directory>
-        <excludes>
-          <exclude>**/*.java</exclude>
-        </excludes>
-      </testResource>
-    </testResources>
-  </build>
-</project>
diff --git a/dev-tools/maven/lucene/contrib/pom.xml.template b/dev-tools/maven/lucene/contrib/pom.xml.template
index a7b9806..2910450 100644
--- a/dev-tools/maven/lucene/contrib/pom.xml.template
+++ b/dev-tools/maven/lucene/contrib/pom.xml.template
@@ -33,7 +33,6 @@
   <modules>
     <module>demo</module>
     <module>highlighter</module>
-    <module>instantiated</module>
     <module>memory</module>
     <module>misc</module>
     <module>sandbox</module>
diff --git a/lucene/CHANGES.txt b/lucene/CHANGES.txt
index 87c6d14..07b2a6e 100644
--- a/lucene/CHANGES.txt
+++ b/lucene/CHANGES.txt
@@ -32,6 +32,11 @@ Changes in backwards compatibility policy
     now specify the skipDocs explicitly (typically this will be the
     deleted docs, but in general you can provide any Bits).
 
+  - The term vectors APIs (TermFreqVector, TermPositionVector, 
+    TermVectorMapper) have been removed in favor of the above
+    flexible indexing APIs, presenting a single-document inverted
+    index of the document from the term vectors.
+
   - MultiReader ctor now throws IOException
 
   - Directory.copy/Directory.copyTo now copies all files (not just
diff --git a/lucene/build.xml b/lucene/build.xml
index 21ac390..6923933 100644
--- a/lucene/build.xml
+++ b/lucene/build.xml
@@ -252,7 +252,6 @@
 
           <packageset dir="contrib/demo/src/java"/>
           <packageset dir="contrib/highlighter/src/java"/>
-          <packageset dir="contrib/instantiated/src/java"/>
           <packageset dir="contrib/memory/src/java"/>
           <packageset dir="contrib/misc/src/java"/>
           <packageset dir="contrib/sandbox/src/java"/>
@@ -268,7 +267,6 @@
           <group title="contrib: Demo" packages="org.apache.lucene.demo*"/>
           <group title="contrib: ICU" packages="org.apache.lucene.collation*"/>
           <group title="contrib: Highlighter" packages="org.apache.lucene.search.highlight*:org.apache.lucene.search.vectorhighlight*"/>
-          <group title="contrib: Instantiated" packages="org.apache.lucene.store.instantiated*"/>
           <group title="contrib: Memory" packages="org.apache.lucene.index.memory*"/>
           <group title="contrib: Misc " packages="org.apache.lucene.misc*"/>
           <group title="contrib: Sandbox" packages="org.apache.lucene.sandbox*"/>
diff --git a/lucene/common-build.xml b/lucene/common-build.xml
index 3271f80..56dbbfa 100644
--- a/lucene/common-build.xml
+++ b/lucene/common-build.xml
@@ -808,7 +808,7 @@
         <!-- <compilerarg line="-Xmaxwarns 10000000"/>
         <compilerarg line="-Xmaxerrs 10000000"/> -->
         <!-- for generics in Java 1.5: -->
-        <compilerarg line="-Xlint -Xlint:-deprecation -Xlint:-serial"/>
+	<compilerarg line="-Xlint -Xlint:-deprecation -Xlint:-serial"/>
       </javac>
     </sequential>
   </macrodef>
diff --git a/lucene/contrib/CHANGES.txt b/lucene/contrib/CHANGES.txt
index 9d7d965..69340d8 100644
--- a/lucene/contrib/CHANGES.txt
+++ b/lucene/contrib/CHANGES.txt
@@ -13,6 +13,8 @@ Changes in Runtime Behavior
    provide an analyzer that returns 1 from the offsetGap method.
    (Mike McCandless)
 
+ * LUCENE-2621: Removed contrib/instantiated.  (Robert Muir)
+
 Build
 
  * LUCENE-2845: Moved contrib/benchmark to modules.
diff --git a/lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources.java b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources.java
index c3f11c3..7ca1b99 100644
--- a/lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources.java
+++ b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources.java
@@ -32,10 +32,11 @@ import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.document.Document;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.Fields;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.TermFreqVector;
-import org.apache.lucene.index.TermPositionVector;
-import org.apache.lucene.index.TermVectorOffsetInfo;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.util.ArrayUtil;
 import org.apache.lucene.util.BytesRef;
 
@@ -69,12 +70,14 @@ public class TokenSources {
       String field, Document doc, Analyzer analyzer) throws IOException {
     TokenStream ts = null;
 
-    TermFreqVector tfv = reader.getTermFreqVector(docId, field);
-    if (tfv != null) {
-      if (tfv instanceof TermPositionVector) {
-        ts = getTokenStream((TermPositionVector) tfv);
+    Fields vectors = reader.getTermVectors(docId);
+    if (vectors != null) {
+      Terms vector = vectors.terms(field);
+      if (vector != null) {
+        ts = getTokenStream(vector);
       }
     }
+
     // No token info stored so fall back to analyzing raw content
     if (ts == null) {
       ts = getTokenStream(doc, field, analyzer);
@@ -99,12 +102,14 @@ public class TokenSources {
       String field, Analyzer analyzer) throws IOException {
     TokenStream ts = null;
 
-    TermFreqVector tfv = reader.getTermFreqVector(docId, field);
-    if (tfv != null) {
-      if (tfv instanceof TermPositionVector) {
-        ts = getTokenStream((TermPositionVector) tfv);
+    Fields vectors = reader.getTermVectors(docId);
+    if (vectors != null) {
+      Terms vector = vectors.terms(field);
+      if (vector != null) {
+        ts = getTokenStream(vector);
       }
     }
+
     // No token info stored so fall back to analyzing raw content
     if (ts == null) {
       ts = getTokenStream(reader, docId, field, analyzer);
@@ -112,10 +117,25 @@ public class TokenSources {
     return ts;
   }
 
-  public static TokenStream getTokenStream(TermPositionVector tpv) {
+  public static TokenStream getTokenStream(Terms vector) throws IOException {
     // assumes the worst and makes no assumptions about token position
     // sequences.
-    return getTokenStream(tpv, false);
+    return getTokenStream(vector, false);
+  }
+
+  private static boolean hasPositions(Terms vector) throws IOException {
+    final TermsEnum termsEnum = vector.iterator(null);
+    if (termsEnum.next() != null) {
+      DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null);
+      if (dpEnum != null) {
+        int pos = dpEnum.nextPosition();
+        if (pos >= 0) {
+          return true;
+        }
+      }
+    }
+
+    return false;
   }
 
   /**
@@ -144,9 +164,10 @@ public class TokenSources {
    *        numbers have no overlaps or gaps. If looking to eek out the last
    *        drops of performance, set to true. If in doubt, set to false.
    */
-  public static TokenStream getTokenStream(TermPositionVector tpv,
-      boolean tokenPositionsGuaranteedContiguous) {
-    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {
+  public static TokenStream getTokenStream(Terms tpv,
+      boolean tokenPositionsGuaranteedContiguous) 
+  throws IOException {
+    if (!tokenPositionsGuaranteedContiguous && hasPositions(tpv)) {
       return new TokenStreamFromTermPositionVector(tpv);
     }
 
@@ -186,56 +207,57 @@ public class TokenSources {
       }
     }
     // code to reconstruct the original sequence of Tokens
-    BytesRef[] terms = tpv.getTerms();
-    int[] freq = tpv.getTermFrequencies();
+    TermsEnum termsEnum = tpv.iterator(null);
     int totalTokens = 0;
-    for (int t = 0; t < freq.length; t++) {
-      totalTokens += freq[t];
+    while(termsEnum.next() != null) {
+      totalTokens += (int) termsEnum.totalTermFreq();
     }
     Token tokensInOriginalOrder[] = new Token[totalTokens];
     ArrayList<Token> unsortedTokens = null;
-    for (int t = 0; t < freq.length; t++) {
-      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);
-      if (offsets == null) {
+    termsEnum = tpv.iterator(null);
+    BytesRef text;
+    DocsAndPositionsEnum dpEnum = null;
+    while ((text = termsEnum.next()) != null) {
+
+      dpEnum = termsEnum.docsAndPositions(null, dpEnum);
+      if (dpEnum == null || (!dpEnum.attributes().hasAttribute(OffsetAttribute.class))) {
         throw new IllegalArgumentException(
             "Required TermVector Offset information was not found");
       }
 
-      int[] pos = null;
-      if (tokenPositionsGuaranteedContiguous) {
-        // try get the token position info to speed up assembly of tokens into
-        // sorted sequence
-        pos = tpv.getTermPositions(t);
-      }
-      if (pos == null) {
-        // tokens NOT stored with positions or not guaranteed contiguous - must
-        // add to list and sort later
-        if (unsortedTokens == null) {
-          unsortedTokens = new ArrayList<Token>();
-        }
-        for (int tp = 0; tp < offsets.length; tp++) {
-          Token token = new Token(terms[t].utf8ToString(),
-              offsets[tp].getStartOffset(), offsets[tp].getEndOffset());
+      final String term = text.utf8ToString();
+
+      final OffsetAttribute offsetAtt = dpEnum.attributes().getAttribute(OffsetAttribute.class);
+      dpEnum.nextDoc();
+      final int freq = dpEnum.freq();
+      for(int posUpto=0;posUpto<freq;posUpto++) {
+        final int pos = dpEnum.nextPosition();
+        final Token token = new Token(term,
+                                      offsetAtt.startOffset(),
+                                      offsetAtt.endOffset());
+        if (tokenPositionsGuaranteedContiguous && pos != -1) {
+          // We have positions stored and a guarantee that the token position
+          // information is contiguous
+
+          // This may be fast BUT wont work if Tokenizers used which create >1
+          // token in same position or
+          // creates jumps in position numbers - this code would fail under those
+          // circumstances
+
+          // tokens stored with positions - can use this to index straight into
+          // sorted array
+          tokensInOriginalOrder[pos] = token;
+        } else {
+          // tokens NOT stored with positions or not guaranteed contiguous - must
+          // add to list and sort later
+          if (unsortedTokens == null) {
+            unsortedTokens = new ArrayList<Token>();
+          }
           unsortedTokens.add(token);
         }
-      } else {
-        // We have positions stored and a guarantee that the token position
-        // information is contiguous
-
-        // This may be fast BUT wont work if Tokenizers used which create >1
-        // token in same position or
-        // creates jumps in position numbers - this code would fail under those
-        // circumstances
-
-        // tokens stored with positions - can use this to index straight into
-        // sorted array
-        for (int tp = 0; tp < pos.length; tp++) {
-          Token token = new Token(terms[t].utf8ToString(),
-              offsets[tp].getStartOffset(), offsets[tp].getEndOffset());
-          tokensInOriginalOrder[pos[tp]] = token;
-        }
       }
     }
+
     // If the field has been stored without position data we must perform a sort
     if (unsortedTokens != null) {
       tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens
@@ -253,18 +275,25 @@ public class TokenSources {
 
   public static TokenStream getTokenStream(IndexReader reader, int docId,
       String field) throws IOException {
-    TermFreqVector tfv = reader.getTermFreqVector(docId, field);
-    if (tfv == null) {
+
+    Fields vectors = reader.getTermVectors(docId);
+    if (vectors == null) {
+      throw new IllegalArgumentException(field + " in doc #" + docId
+          + "does not have any term position data stored");
+    }
+
+    Terms vector = vectors.terms(field);
+    if (vector == null) {
       throw new IllegalArgumentException(field + " in doc #" + docId
           + "does not have any term position data stored");
     }
-    if (tfv instanceof TermPositionVector) {
-      TermPositionVector tpv = (TermPositionVector) reader.getTermFreqVector(
-          docId, field);
-      return getTokenStream(tpv);
+
+    if (!hasPositions(vector)) {
+      throw new IllegalArgumentException(field + " in doc #" + docId
+          + "does not have any term position data stored");
     }
-    throw new IllegalArgumentException(field + " in doc #" + docId
-        + "does not have any term position data stored");
+    
+    return getTokenStream(vector);
   }
 
   // convenience method
diff --git a/lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermPositionVector.java b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermPositionVector.java
index a24508b..b431187 100644
--- a/lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermPositionVector.java
+++ b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermPositionVector.java
@@ -27,8 +27,9 @@ import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.index.TermPositionVector;
-import org.apache.lucene.index.TermVectorOffsetInfo;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CollectionUtil;
 
@@ -47,31 +48,42 @@ public final class TokenStreamFromTermPositionVector extends TokenStream {
   /**
    * Constructor.
    * 
-   * @param termPositionVector TermPositionVector that contains the data for
+   * @param vector Terms that contains the data for
    *        creating the TokenStream. Must have positions and offsets.
    */
   public TokenStreamFromTermPositionVector(
-      final TermPositionVector termPositionVector) {
+      final Terms vector) throws IOException {
     termAttribute = addAttribute(CharTermAttribute.class);
     positionIncrementAttribute = addAttribute(PositionIncrementAttribute.class);
     offsetAttribute = addAttribute(OffsetAttribute.class);
-    final BytesRef[] terms = termPositionVector.getTerms();
-    for (int i = 0; i < terms.length; i++) {
-      final TermVectorOffsetInfo[] offsets = termPositionVector.getOffsets(i);
-      final int[] termPositions = termPositionVector.getTermPositions(i);
-      for (int j = 0; j < termPositions.length; j++) {
+    final TermsEnum termsEnum = vector.iterator(null);
+    BytesRef text;
+    DocsAndPositionsEnum dpEnum = null;
+    while((text = termsEnum.next()) != null) {
+      dpEnum = termsEnum.docsAndPositions(null, dpEnum);
+      dpEnum.nextDoc();
+      final int freq = dpEnum.freq();
+      final OffsetAttribute offsetAtt;
+      if (dpEnum.attributes().hasAttribute(OffsetAttribute.class)) {
+        offsetAtt = dpEnum.attributes().getAttribute(OffsetAttribute.class);
+      } else {
+        offsetAtt = null;
+      }
+      for (int j = 0; j < freq; j++) {
+        int pos = dpEnum.nextPosition();
         Token token;
-        if (offsets != null) {
-          token = new Token(terms[i].utf8ToString(),
-              offsets[j].getStartOffset(), offsets[j].getEndOffset());
+        if (offsetAtt != null) {
+          token = new Token(text.utf8ToString(),
+                            offsetAtt.startOffset(),
+                            offsetAtt.endOffset());
         } else {
           token = new Token();
-          token.setEmpty().append(terms[i].utf8ToString());
+          token.setEmpty().append(text.utf8ToString());
         }
         // Yes - this is the position, not the increment! This is for
         // sorting. This value
         // will be corrected before use.
-        token.setPositionIncrement(termPositions[j]);
+        token.setPositionIncrement(pos);
         this.positionedTokens.add(token);
       }
     }
diff --git a/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldTermStack.java b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldTermStack.java
index ede19b1..b0dc0cf 100644
--- a/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldTermStack.java
+++ b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldTermStack.java
@@ -21,10 +21,12 @@ import java.util.Collections;
 import java.util.LinkedList;
 import java.util.Set;
 
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.Fields;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.TermFreqVector;
-import org.apache.lucene.index.TermPositionVector;
-import org.apache.lucene.index.TermVectorOffsetInfo;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CharsRef;
 
@@ -76,30 +78,55 @@ public class FieldTermStack {
     // just return to make null snippet if un-matched fieldName specified when fieldMatch == true
     if( termSet == null ) return;
 
-    TermFreqVector tfv = reader.getTermFreqVector( docId, fieldName );
-    if( tfv == null ) return; // just return to make null snippets
-    TermPositionVector tpv = null;
-    try{
-      tpv = (TermPositionVector)tfv;
+    final Fields vectors = reader.getTermVectors(docId);
+    if (vectors == null) {
+      // null snippet
+      return;
     }
-    catch( ClassCastException e ){
-      return; // just return to make null snippets
+
+    final Terms vector = vectors.terms(fieldName);
+    if (vector == null) {
+      // null snippet
+      return;
     }
-    
+
     final CharsRef spare = new CharsRef();
-    for( BytesRef term : tpv.getTerms() ){
-      if( !termSet.contains( term.utf8ToChars(spare).toString() ) ) continue;
-      int index = tpv.indexOf( term );
-      TermVectorOffsetInfo[] tvois = tpv.getOffsets( index );
-      if( tvois == null ) return; // just return to make null snippets
-      int[] poss = tpv.getTermPositions( index );
-      if( poss == null ) return; // just return to make null snippets
-      for( int i = 0; i < tvois.length; i++ )
-        termList.add( new TermInfo( term.utf8ToChars(spare).toString(), tvois[i].getStartOffset(), tvois[i].getEndOffset(), poss[i] ) );
+    final TermsEnum termsEnum = vector.iterator(null);
+    DocsAndPositionsEnum dpEnum = null;
+    BytesRef text;
+    while ((text = termsEnum.next()) != null) {
+      final String term = text.utf8ToChars(spare).toString();
+      if (!termSet.contains(term)) {
+        continue;
+      }
+      dpEnum = termsEnum.docsAndPositions(null, dpEnum);
+      if (dpEnum == null) {
+        // null snippet
+        return;
+      }
+
+      if (!dpEnum.attributes().hasAttribute(OffsetAttribute.class)) {
+        // null snippet
+        return;
+      }
+      dpEnum.nextDoc();
+
+      final OffsetAttribute offsetAtt = dpEnum.attributes().getAttribute(OffsetAttribute.class);
+
+      final int freq = dpEnum.freq();
+      
+      for(int i = 0;i < freq;i++) {
+        final int pos = dpEnum.nextPosition();
+        if (pos == -1) {
+          // null snippet
+          return;
+        }
+        termList.add(new TermInfo(term, offsetAtt.startOffset(), offsetAtt.endOffset(), pos));
+      }
     }
     
     // sort by position
-    Collections.sort( termList );
+    Collections.sort(termList);
   }
 
   /**
diff --git a/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/package.html b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/package.html
index 8afb201..d713fa2 100644
--- a/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/package.html
+++ b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/package.html
@@ -95,7 +95,7 @@ will be generated:</p>
 </pre>
 
 <h3>Step 2.</h3>
-<p>In Step 2, Fast Vector Highlighter generates {@link org.apache.lucene.search.vectorhighlight.FieldTermStack}. Fast Vector Highlighter uses {@link org.apache.lucene.index.TermFreqVector} data
+<p>In Step 2, Fast Vector Highlighter generates {@link org.apache.lucene.search.vectorhighlight.FieldTermStack}. Fast Vector Highlighter uses term vector data
 (must be stored {@link org.apache.lucene.document.FieldType#setStoreTermVectorOffsets(boolean)} and {@link org.apache.lucene.document.FieldType#setStoreTermVectorPositions(boolean)})
 to generate it. <code>FieldTermStack</code> keeps the terms in the user query.
 Therefore, in this sample case, Fast Vector Highlighter generates the following <code>FieldTermStack</code>:</p>
diff --git a/lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterPhraseTest.java b/lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterPhraseTest.java
index 990d3c4..eb98aca 100644
--- a/lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterPhraseTest.java
+++ b/lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterPhraseTest.java
@@ -34,7 +34,6 @@ import org.apache.lucene.index.CorruptIndexException;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.index.TermPositionVector;
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.IndexSearcher;
@@ -85,7 +84,7 @@ public class HighlighterPhraseTest extends LuceneTestCase {
             new QueryScorer(phraseQuery));
 
         final TokenStream tokenStream = TokenSources
-            .getTokenStream((TermPositionVector) indexReader.getTermFreqVector(
+            .getTokenStream(indexReader.getTermVector(
                 0, FIELD), false);
         assertEquals(highlighter.getBestFragment(new TokenStreamConcurrent(),
             TEXT), highlighter.getBestFragment(tokenStream, TEXT));
@@ -160,7 +159,7 @@ public class HighlighterPhraseTest extends LuceneTestCase {
             .nextSetBit(position + 1)) {
           assertEquals(0, position);
           final TokenStream tokenStream = TokenSources.getTokenStream(
-              (TermPositionVector) indexReader.getTermFreqVector(position,
+              indexReader.getTermVector(position,
                   FIELD), false);
           assertEquals(highlighter.getBestFragment(new TokenStreamConcurrent(),
               TEXT), highlighter.getBestFragment(tokenStream, TEXT));
@@ -207,7 +206,7 @@ public class HighlighterPhraseTest extends LuceneTestCase {
             new SimpleHTMLFormatter(), new SimpleHTMLEncoder(),
             new QueryScorer(phraseQuery));
         final TokenStream tokenStream = TokenSources
-            .getTokenStream((TermPositionVector) indexReader.getTermFreqVector(
+            .getTokenStream(indexReader.getTermVector(
                 0, FIELD), false);
         assertEquals(
             highlighter.getBestFragment(new TokenStreamSparse(), TEXT),
@@ -253,7 +252,7 @@ public class HighlighterPhraseTest extends LuceneTestCase {
             new SimpleHTMLFormatter(), new SimpleHTMLEncoder(),
             new QueryScorer(phraseQuery));
         final TokenStream tokenStream = TokenSources.getTokenStream(
-            (TermPositionVector) indexReader.getTermFreqVector(0, FIELD), true);
+            indexReader.getTermVector(0, FIELD), true);
         assertEquals("the fox <B>did</B> not <B>jump</B>", highlighter
             .getBestFragment(tokenStream, TEXT));
       } finally {
@@ -297,7 +296,7 @@ public class HighlighterPhraseTest extends LuceneTestCase {
             new SimpleHTMLFormatter(), new SimpleHTMLEncoder(),
             new QueryScorer(phraseQuery));
         final TokenStream tokenStream = TokenSources
-            .getTokenStream((TermPositionVector) indexReader.getTermFreqVector(
+            .getTokenStream(indexReader.getTermVector(
                 0, FIELD), false);
         assertEquals(
             highlighter.getBestFragment(new TokenStreamSparse(), TEXT),
diff --git a/lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest.java b/lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest.java
index 9dbecbd..316923b 100644
--- a/lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest.java
+++ b/lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest.java
@@ -32,7 +32,6 @@ import org.apache.lucene.index.CorruptIndexException;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.index.TermPositionVector;
 import org.apache.lucene.search.DisjunctionMaxQuery;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
@@ -133,7 +132,7 @@ public class TokenSourcesTest extends LuceneTestCase {
             new QueryScorer(query));
         final TokenStream tokenStream = TokenSources
             .getTokenStream(
-                (TermPositionVector) indexReader.getTermFreqVector(0, FIELD),
+                indexReader.getTermVector(0, FIELD),
                 false);
         assertEquals("<B>the fox</B> did not jump",
             highlighter.getBestFragment(tokenStream, TEXT));
@@ -182,7 +181,7 @@ public class TokenSourcesTest extends LuceneTestCase {
             new QueryScorer(query));
         final TokenStream tokenStream = TokenSources
             .getTokenStream(
-                (TermPositionVector) indexReader.getTermFreqVector(0, FIELD),
+                indexReader.getTermVector(0, FIELD),
                 false);
         assertEquals("<B>the fox</B> did not jump",
             highlighter.getBestFragment(tokenStream, TEXT));
@@ -230,7 +229,7 @@ public class TokenSourcesTest extends LuceneTestCase {
             new QueryScorer(phraseQuery));
         final TokenStream tokenStream = TokenSources
             .getTokenStream(
-                (TermPositionVector) indexReader.getTermFreqVector(0, FIELD),
+                indexReader.getTermVector(0, FIELD),
                 false);
         assertEquals("<B>the fox</B> did not jump",
             highlighter.getBestFragment(tokenStream, TEXT));
@@ -279,7 +278,7 @@ public class TokenSourcesTest extends LuceneTestCase {
             new QueryScorer(phraseQuery));
         final TokenStream tokenStream = TokenSources
             .getTokenStream(
-                (TermPositionVector) indexReader.getTermFreqVector(0, FIELD),
+                indexReader.getTermVector(0, FIELD),
                 false);
         assertEquals("<B>the fox</B> did not jump",
             highlighter.getBestFragment(tokenStream, TEXT));
diff --git a/lucene/contrib/instantiated/build.xml b/lucene/contrib/instantiated/build.xml
deleted file mode 100644
index 88fb14f..0000000
--- a/lucene/contrib/instantiated/build.xml
+++ /dev/null
@@ -1,29 +0,0 @@
-<?xml version="1.0"?>
-
-<!--
-    Licensed to the Apache Software Foundation (ASF) under one or more
-    contributor license agreements.  See the NOTICE file distributed with
-    this work for additional information regarding copyright ownership.
-    The ASF licenses this file to You under the Apache License, Version 2.0
-    the "License"); you may not use this file except in compliance with
-    the License.  You may obtain a copy of the License at
- 
-        http://www.apache.org/licenses/LICENSE-2.0
- 
-    Unless required by applicable law or agreed to in writing, software
-    distributed under the License is distributed on an "AS IS" BASIS,
-    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-    See the License for the specific language governing permissions and
-    limitations under the License.
- -->
-
-<project name="instantiated" default="default">
-
-  <description>
-    InstantiatedIndex, an alternative RAM store.
-  </description>
-
-  <import file="../contrib-build.xml"/>
-
-
-</project>
diff --git a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/FieldSetting.java b/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/FieldSetting.java
deleted file mode 100644
index bd2441b..0000000
--- a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/FieldSetting.java
+++ /dev/null
@@ -1,62 +0,0 @@
-package org.apache.lucene.store.instantiated;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * For non package access see {@link org.apache.lucene.index.IndexReader#getFieldNames(org.apache.lucene.index.IndexReader.FieldOption)} 
- */
-class FieldSetting  {
-  String fieldName;
-
-  boolean storeTermVector = false;
-  boolean storeOffsetWithTermVector = false;
-  boolean storePositionWithTermVector = false;
-  boolean storePayloads = false;
-
-  boolean stored = false;
-  boolean indexed = false;
-  boolean tokenized = false;
-
-  FieldSetting() {
-  }
-
-
-  FieldSetting(String fieldName) {
-    this.fieldName = fieldName;
-  }
-
-  @Override
-  public boolean equals(Object o) {
-    if (this == o)
-      return true;
-    if (o == null || getClass() != o.getClass())
-      return false;
-
-    final FieldSetting that = (FieldSetting) o;
-
-    return fieldName.equals(that.fieldName);
-
-  }
-
-  @Override
-  public int hashCode() {
-    return fieldName.hashCode();
-  }
-
-
-}
diff --git a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/FieldSettings.java b/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/FieldSettings.java
deleted file mode 100644
index 4b0662c..0000000
--- a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/FieldSettings.java
+++ /dev/null
@@ -1,92 +0,0 @@
-package org.apache.lucene.store.instantiated;
-
-import java.util.HashMap;
-import java.util.Map;
-import java.util.Collection;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Essentially a Map<FieldName, {@link org.apache.lucene.store.instantiated.FieldSetting}> 
- */
-class FieldSettings {
-
-
-  FieldSettings() {
-  }
-
-  private Map</** field name */String, FieldSetting> fieldSettings = new HashMap<String, FieldSetting>();
-
-  synchronized FieldSetting merge(FieldSetting fieldSetting) {
-    FieldSetting setting = fieldSettings.get(fieldSetting.fieldName);
-
-    if (setting == null) {
-      setting = new FieldSetting(fieldSetting.fieldName);
-      fieldSettings.put(fieldSetting.fieldName, setting);
-    }
-
-    if (fieldSetting.stored) {
-      setting.stored = true;
-    }
-
-    if ("b3".equals(fieldSetting.fieldName)) {
-      System.currentTimeMillis();
-    }
-    if (fieldSetting.indexed) {
-      setting.indexed = true;
-    }
-    if (fieldSetting.tokenized) {
-      setting.tokenized = true;
-    }
-
-    if (fieldSetting.storeTermVector) {
-      setting.storeTermVector = true;
-    }
-    if (fieldSetting.storeOffsetWithTermVector) {
-      setting.storeOffsetWithTermVector = true;
-    }
-    if (fieldSetting.storePositionWithTermVector) {
-      setting.storePositionWithTermVector = true;
-    }
-
-    if (fieldSetting.storePayloads) {
-      setting.storePayloads = true;
-    }
-
-    return setting;
-
-  }
-
-  FieldSetting get(String name) {
-    return fieldSettings.get(name);
-  }
-
-  FieldSetting get(String name, boolean create) {
-    FieldSetting fieldSetting = fieldSettings.get(name);
-    if (create && fieldSetting == null) {
-      fieldSetting = new FieldSetting(name);
-      fieldSettings.put(name, fieldSetting);
-    }
-    return fieldSetting;
-  }
-
-  Collection<FieldSetting> values() {
-    return fieldSettings.values();
-  }
-
-}
diff --git a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedDocsAndPositionsEnum.java b/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedDocsAndPositionsEnum.java
deleted file mode 100644
index bcdeca3..0000000
--- a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedDocsAndPositionsEnum.java
+++ /dev/null
@@ -1,101 +0,0 @@
-package org.apache.lucene.store.instantiated;
-
-/**
- * Copyright 2006 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-
-public class InstantiatedDocsAndPositionsEnum extends DocsAndPositionsEnum {
-  private int upto;
-  private int posUpto;
-  private Bits liveDocs;
-  private InstantiatedTerm term;
-  protected InstantiatedTermDocumentInformation currentDoc;
-  private final BytesRef payload = new BytesRef();
-
-  public InstantiatedDocsAndPositionsEnum reset(Bits liveDocs, InstantiatedTerm term) {
-    this.liveDocs = liveDocs;
-    this.term = term;
-    upto = -1;
-    return this;
-  }
-
-  @Override
-  public int docID() {
-    return currentDoc.getDocument().getDocumentNumber();
-  }
-
-  @Override
-  public int nextDoc() {
-    upto++;
-    if (upto >= term.getAssociatedDocuments().length) {
-      return NO_MORE_DOCS;
-    } else {
-      currentDoc = term.getAssociatedDocuments()[upto];
-      if (liveDocs == null || liveDocs.get(currentDoc.getDocument().getDocumentNumber())) {
-        posUpto = -1;
-        return docID();
-      } else {
-        return nextDoc();
-      }
-    }
-  }
-
-  @Override
-  public int advance(int target) {
-    if (currentDoc.getDocument().getDocumentNumber() >= target) {
-      return nextDoc();
-    }
-
-    int startOffset = upto >= 0 ? upto : 0;
-    upto = term.seekCeilingDocumentInformationIndex(target, startOffset);
-    if (upto == -1) {
-      return NO_MORE_DOCS;
-    }
-    currentDoc = term.getAssociatedDocuments()[upto];
-
-    if (liveDocs != null && !liveDocs.get(currentDoc.getDocument().getDocumentNumber())) {
-      return nextDoc();
-    } else {
-      posUpto = -1;
-      return docID();
-    }
-  }
-
-  @Override
-  public int freq() {
-    return currentDoc.getTermPositions().length;
-  }
-  
-  @Override
-  public int nextPosition() {
-    return currentDoc.getTermPositions()[++posUpto];
-  }
-
-  @Override
-  public boolean hasPayload() {
-    return currentDoc.getPayloads()[posUpto] != null;
-  }
-
-  @Override
-  public BytesRef getPayload() {
-    payload.bytes = currentDoc.getPayloads()[posUpto];
-    payload.length = payload.bytes.length;
-    return payload;
-  }
-}
diff --git a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedDocsEnum.java b/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedDocsEnum.java
deleted file mode 100644
index 2657c51..0000000
--- a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedDocsEnum.java
+++ /dev/null
@@ -1,79 +0,0 @@
-package org.apache.lucene.store.instantiated;
-
-/**
- * Copyright 2006 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.util.Bits;
-
-public class InstantiatedDocsEnum extends DocsEnum {
-  private int upto;
-  private Bits liveDocs;
-  private InstantiatedTerm term;
-  protected InstantiatedTermDocumentInformation currentDoc;
-
-  public InstantiatedDocsEnum reset(Bits liveDocs, InstantiatedTerm term) {
-    this.liveDocs = liveDocs;
-    this.term = term;
-    upto = -1;
-    return this;
-  }
-
-  @Override
-  public int docID() {
-    return currentDoc.getDocument().getDocumentNumber();
-  }
-
-  @Override
-  public int nextDoc() {
-    upto++;
-    if (upto >= term.getAssociatedDocuments().length) {
-      return NO_MORE_DOCS;
-    } else {
-      currentDoc = term.getAssociatedDocuments()[upto];
-      if (liveDocs == null || liveDocs.get(currentDoc.getDocument().getDocumentNumber())) {
-        return docID();
-      } else {
-        return nextDoc();
-      }
-    }
-  }
-
-  @Override
-  public int advance(int target) {
-    if (currentDoc != null && currentDoc.getDocument().getDocumentNumber() >= target) {
-      return nextDoc();
-    }
-
-    int startOffset = upto >= 0 ? upto : 0;
-    upto = term.seekCeilingDocumentInformationIndex(target, startOffset);
-    if (upto == -1) {
-      return NO_MORE_DOCS;
-    }
-    currentDoc = term.getAssociatedDocuments()[upto];
-
-    if (liveDocs != null && !liveDocs.get(currentDoc.getDocument().getDocumentNumber())) {
-      return nextDoc();
-    } else {
-      return docID();
-    }
-  }
-
-  @Override
-  public int freq() {
-    return currentDoc.getTermPositions().length;
-  }
-}
diff --git a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedDocument.java b/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedDocument.java
deleted file mode 100644
index 10b124f..0000000
--- a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedDocument.java
+++ /dev/null
@@ -1,75 +0,0 @@
-package org.apache.lucene.store.instantiated;
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.document.Document;
-
-import java.util.List;
-import java.util.Map;
-
-/**
- * A document in the instantiated index object graph, optionally coupled to the vector space view. 
- *
- * @see org.apache.lucene.document.Document
- */
-public class InstantiatedDocument {
-
-  private Document document;
-
-  public InstantiatedDocument() {
-    this.document = new Document();
-  }
-
-
-  public InstantiatedDocument(Document document) {
-    this.document = document;
-  }
-
-  /** this is the unsafe index order document number. */
-  private Integer documentNumber;
-
-  /** this is the term vector space view */
-  private Map<String /*field name*/, List<InstantiatedTermDocumentInformation>> vectorSpace;
-
-  /**
-   * @return position of document in the index.
-   */
-  public Integer getDocumentNumber() {
-    return documentNumber;
-  }
-
-  void setDocumentNumber(Integer documentNumber) {
-    this.documentNumber = documentNumber;
-  }
-
-  public Map</*field name*/ String, List<InstantiatedTermDocumentInformation>> getVectorSpace() {
-    return vectorSpace;
-  }
-
-  public void setVectorSpace(Map</*field name*/ String, List<InstantiatedTermDocumentInformation>> vectorSpace) {
-    this.vectorSpace = vectorSpace;
-  }
-
-  public Document getDocument() {
-    return document;
-  }
-
-  @Override
-  public String toString() {
-    return document.toString();
-  }
-}
diff --git a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndex.java b/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndex.java
deleted file mode 100644
index e0240d2..0000000
--- a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndex.java
+++ /dev/null
@@ -1,384 +0,0 @@
-package org.apache.lucene.store.instantiated;
-
-/**
- * Copyright 2006 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexableField;
-import org.apache.lucene.index.MultiNorms;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.FieldsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.MultiFields;
-import org.apache.lucene.index.TermPositionVector;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.util.BitVector;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CharsRef;
-
-/**
- * Represented as a coupled graph of class instances, this
- * all-in-memory index store implementation delivers search
- * results up to a 100 times faster than the file-centric RAMDirectory
- * at the cost of greater RAM consumption.
- * <p>
- * @lucene.experimental
- * <p>
- * There are no read and write locks in this store.
- * {@link InstantiatedIndexReader} {@link InstantiatedIndexReader#isCurrent()} all the time
- * and {@link org.apache.lucene.store.instantiated.InstantiatedIndexWriter}
- * will attempt to update instances of the object graph in memory
- * at the same time as a searcher is reading from it.
- *
- * Consider using InstantiatedIndex as if it was immutable.
- */
-public class InstantiatedIndex
-    implements Closeable {
-
-  private long version = System.currentTimeMillis();
-
-  private InstantiatedDocument[] documentsByNumber;
-
-  private BitVector deletedDocuments;
-
-  private Map<String, Map<String, InstantiatedTerm>> termsByFieldAndText;
-  private InstantiatedTerm[] orderedTerms;
-
-  private Map<String, byte[]> normsByFieldNameAndDocumentNumber;
-
-  private FieldSettings fieldSettings;
-
-  /**
-   * Creates an empty instantiated index for you to fill with data using an {@link org.apache.lucene.store.instantiated.InstantiatedIndexWriter}. 
-   */
-  public InstantiatedIndex() {
-    initialize();
-  }
-  
-  void initialize() {
-    // todo: clear index without loosing memory (uncouple stuff)
-    termsByFieldAndText = new HashMap<String, Map<String, InstantiatedTerm>>();
-    fieldSettings = new FieldSettings();
-    orderedTerms = new InstantiatedTerm[0];
-    documentsByNumber = new InstantiatedDocument[0];
-    normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>();
-  }
-
-  
-  /**
-   * Creates a new instantiated index that looks just like the index in a specific state as represented by a reader.
-   *
-   * @param sourceIndexReader the source index this new instantiated index will be copied from.
-   * @throws IOException if the source index is not single-segment, or when accessing the source.
-   */
-  public InstantiatedIndex(IndexReader sourceIndexReader) throws IOException {
-    this(sourceIndexReader, null);
-  }
-  
-
-  
-  /**
-   * Creates a new instantiated index that looks just like the index in a specific state as represented by a reader.
-   *
-   * @param sourceIndexReader the source index this new instantiated index will be copied from.
-   * @param fields fields to be added, or null for all
-   * @throws IOException if the source index is not single-segment, or when accessing the source.
-   */
-  public InstantiatedIndex(IndexReader sourceIndexReader, Set<String> fields) throws IOException {
-
-    if (sourceIndexReader.getSequentialSubReaders().length != 1) {
-      System.out.println(("Source index has more than one segment."));      
-      //throw new IOException("Source index has more than one segment.");
-    }
-
-
-    initialize();
-
-    Collection<String> allFieldNames = sourceIndexReader.getFieldNames(IndexReader.FieldOption.ALL);
-        
-    // load field options
-
-    Collection<String> indexedNames = sourceIndexReader.getFieldNames(IndexReader.FieldOption.INDEXED);
-    for (String name : indexedNames) {
-      FieldSetting setting = fieldSettings.get(name, true);
-      setting.indexed = true;
-    }
-    Collection<String> indexedNoVecNames = sourceIndexReader.getFieldNames(IndexReader.FieldOption.INDEXED_NO_TERMVECTOR);
-    for (String name : indexedNoVecNames) {
-      FieldSetting setting = fieldSettings.get(name, true);
-      setting.storeTermVector = false;
-      setting.indexed = true;
-    }
-    Collection<String> indexedVecNames = sourceIndexReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);
-    for (String name : indexedVecNames) {
-      FieldSetting setting = fieldSettings.get(name, true);
-      setting.storeTermVector = true;
-      setting.indexed = true;
-    }
-    Collection<String> payloadNames = sourceIndexReader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS);
-    for (String name : payloadNames) {
-      FieldSetting setting = fieldSettings.get(name, true);
-      setting.storePayloads = true;
-    }
-    Collection<String> termVecNames = sourceIndexReader.getFieldNames(IndexReader.FieldOption.TERMVECTOR);
-    for (String name : termVecNames) {
-      FieldSetting setting = fieldSettings.get(name, true);
-      setting.storeTermVector = true;
-    }
-    Collection<String> termVecOffsetNames = sourceIndexReader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET);
-    for (String name : termVecOffsetNames) {
-      FieldSetting setting = fieldSettings.get(name, true);
-      setting.storeOffsetWithTermVector = true;
-    }
-    Collection<String> termVecPosNames = sourceIndexReader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION);
-    for (String name : termVecPosNames) {
-      FieldSetting setting = fieldSettings.get(name, true);
-      setting.storePositionWithTermVector = true;
-    }
-    Collection<String> termVecPosOffNames = sourceIndexReader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET);
-    for (String name : termVecPosOffNames) {
-      FieldSetting setting = fieldSettings.get(name, true);
-      setting.storeOffsetWithTermVector = true;
-      setting.storePositionWithTermVector = true;
-    }
-    Collection<String> unindexedNames = sourceIndexReader.getFieldNames(IndexReader.FieldOption.UNINDEXED);
-    for (String name : unindexedNames) {
-      FieldSetting setting = fieldSettings.get(name, true);
-      setting.indexed = false;
-    }
-
-
-    documentsByNumber = new InstantiatedDocument[sourceIndexReader.maxDoc()];
-
-    if (sourceIndexReader.hasDeletions()) {
-      deletedDocuments = new BitVector(sourceIndexReader.maxDoc());
-    }
-
-    // create documents
-    final Bits liveDocs = MultiFields.getLiveDocs(sourceIndexReader);
-    for (int i = 0; i < sourceIndexReader.maxDoc(); i++) {
-      if (liveDocs != null && !liveDocs.get(i)) {
-        deletedDocuments.set(i);
-      } else {
-        InstantiatedDocument document = new InstantiatedDocument();
-        // copy stored fields from source reader
-        Document sourceDocument = sourceIndexReader.document(i);
-        for (IndexableField field : sourceDocument) {
-          if (fields == null || fields.contains(field.name())) {
-            document.getDocument().add(field);
-          }
-        }
-        document.setDocumentNumber(i);
-        documentsByNumber[i] = document;
-        for (IndexableField field : document.getDocument()) {
-          if (fields == null || fields.contains(field.name())) {
-            if (field.fieldType().storeTermVectors()) {
-              if (document.getVectorSpace() == null) {
-                document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>());
-              }
-              document.getVectorSpace().put(field.name(), new ArrayList<InstantiatedTermDocumentInformation>());
-            }
-          }
-        }
-      }
-    }
-
-    // create norms
-    for (String fieldName : allFieldNames) {
-      if (fields == null || fields.contains(fieldName)) {
-        byte norms[] = MultiNorms.norms(sourceIndexReader, fieldName);
-        getNormsByFieldNameAndDocumentNumber().put(fieldName, norms);
-      }
-    }
-
-    // create terms
-    for (String fieldName : allFieldNames) {
-      if (fields == null || fields.contains(fieldName)) {
-        getTermsByFieldAndText().put(fieldName, new HashMap<String, InstantiatedTerm>(5000));
-      }
-    }
-    List<InstantiatedTerm> terms = new ArrayList<InstantiatedTerm>(5000 * getTermsByFieldAndText().size());
-    Fields fieldsC = MultiFields.getFields(sourceIndexReader);
-    if (fieldsC != null) {
-      FieldsEnum fieldsEnum = fieldsC.iterator();
-      String field;
-      final CharsRef spare = new CharsRef();
-      while((field = fieldsEnum.next()) != null) {
-        if (fields == null || fields.contains(field)) {
-          TermsEnum termsEnum = fieldsEnum.terms();
-          BytesRef text;
-          while((text = termsEnum.next()) != null) {
-            String termText = text.utf8ToChars(spare).toString();
-            InstantiatedTerm instantiatedTerm = new InstantiatedTerm(field, termText);
-            final long totalTermFreq = termsEnum.totalTermFreq();
-            if (totalTermFreq != -1) {
-              instantiatedTerm.addPositionsCount(totalTermFreq);
-            }
-            getTermsByFieldAndText().get(field).put(termText, instantiatedTerm);
-            instantiatedTerm.setTermIndex(terms.size());
-            terms.add(instantiatedTerm);
-            instantiatedTerm.setAssociatedDocuments(new InstantiatedTermDocumentInformation[termsEnum.docFreq()]);
-          }
-        }
-      }
-    }
-    orderedTerms = terms.toArray(new InstantiatedTerm[terms.size()]);
-
-    // create term-document informations
-    for (InstantiatedTerm term : orderedTerms) {
-      DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(sourceIndexReader,
-                                                                            MultiFields.getLiveDocs(sourceIndexReader),
-                                                                            term.getTerm().field(),
-                                                                            new BytesRef(term.getTerm().text()));
-      int position = 0;
-      while (termPositions.nextDoc() != DocsEnum.NO_MORE_DOCS) {
-        InstantiatedDocument document = documentsByNumber[termPositions.docID()];
-
-        byte[][] payloads = new byte[termPositions.freq()][];
-        int[] positions = new int[termPositions.freq()];
-        for (int i = 0; i < termPositions.freq(); i++) {
-          positions[i] = termPositions.nextPosition();
-
-          if (termPositions.hasPayload()) {
-            BytesRef br = termPositions.getPayload();
-            payloads[i] = new byte[br.length];
-            System.arraycopy(br.bytes, br.offset, payloads[i], 0, br.length);
-          }
-        }
-
-        InstantiatedTermDocumentInformation termDocumentInformation = new InstantiatedTermDocumentInformation(term, document, positions, payloads);
-        term.getAssociatedDocuments()[position++] = termDocumentInformation;
-
-        if (document.getVectorSpace() != null
-            && document.getVectorSpace().containsKey(term.field())) {
-          document.getVectorSpace().get(term.field()).add(termDocumentInformation);
-        }
-
-//        termDocumentInformation.setIndexFromTerm(indexFromTerm++);
-      }
-    }
-
-    // load offsets to term-document informations
-    for (InstantiatedDocument document : getDocumentsByNumber()) {
-      if (document == null) {
-        continue; // deleted
-      }
-      for (IndexableField field : document.getDocument()) {
-        if (field.fieldType().storeTermVectors() && field.fieldType().storeTermVectorOffsets()) {
-          TermPositionVector termPositionVector = (TermPositionVector) sourceIndexReader.getTermFreqVector(document.getDocumentNumber(), field.name());
-          if (termPositionVector != null) {
-            for (int i = 0; i < termPositionVector.getTerms().length; i++) {
-              String token = termPositionVector.getTerms()[i].utf8ToString();
-              InstantiatedTerm term = findTerm(field.name(), token);
-              InstantiatedTermDocumentInformation termDocumentInformation = term.getAssociatedDocument(document.getDocumentNumber());
-              termDocumentInformation.setTermOffsets(termPositionVector.getOffsets(i));
-            }
-          }
-        }
-      }
-    }
-  }
-
-  public InstantiatedIndexWriter indexWriterFactory(Analyzer analyzer, boolean create) throws IOException {
-    return new InstantiatedIndexWriter(this, analyzer, create);
-  }
-
-  public InstantiatedIndexReader indexReaderFactory() throws IOException {
-    return new InstantiatedIndexReader(this);
-  }
-
-  public void close() throws IOException {
-    // todo: decouple everything
-  }
-
-  InstantiatedTerm findTerm(Term term) {
-    return findTerm(term.field(), term.text());
-  }
-
-  InstantiatedTerm findTerm(String field, String text) {
-    Map<String, InstantiatedTerm> termsByField = termsByFieldAndText.get(field);
-    if (termsByField == null) {
-      return null;
-    } else {
-      return termsByField.get(text);
-    }
-  }
-
-  public Map<String, Map<String, InstantiatedTerm>> getTermsByFieldAndText() {
-    return termsByFieldAndText;
-  }
-
-
-  public InstantiatedTerm[] getOrderedTerms() {
-    return orderedTerms;
-  }
-
-  public InstantiatedDocument[] getDocumentsByNumber() {
-    return documentsByNumber;
-  }
-
-  public Map<String, byte[]> getNormsByFieldNameAndDocumentNumber() {
-    return normsByFieldNameAndDocumentNumber;
-  }
-
-  void setNormsByFieldNameAndDocumentNumber(Map<String, byte[]> normsByFieldNameAndDocumentNumber) {
-    this.normsByFieldNameAndDocumentNumber = normsByFieldNameAndDocumentNumber;
-  }
-
-  public BitVector getDeletedDocuments() {
-    return deletedDocuments;
-  }
-
-  void setDeletedDocuments(BitVector deletedDocuments) {
-    this.deletedDocuments = deletedDocuments;
-  }
-
-  void setOrderedTerms(InstantiatedTerm[] orderedTerms) {
-    this.orderedTerms = orderedTerms;
-  }
-
-  void setDocumentsByNumber(InstantiatedDocument[] documentsByNumber) {
-    this.documentsByNumber = documentsByNumber;
-  }
-
-
-  public long getVersion() {
-    return version;
-  }
-
-  void setVersion(long version) {
-    this.version = version;
-  }
-
-
-  FieldSettings getFieldSettings() {
-    return fieldSettings;
-  }
-}
diff --git a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexReader.java b/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexReader.java
deleted file mode 100644
index 82b518d..0000000
--- a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexReader.java
+++ /dev/null
@@ -1,472 +0,0 @@
-package org.apache.lucene.store.instantiated;
-
-/**
- * Copyright 2006 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.LinkedList;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-import java.util.Comparator;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.index.*;
-import org.apache.lucene.index.codecs.PerDocValues;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.BitVector;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Bits;
-
-/**
- * An InstantiatedIndexReader is not a snapshot in time, it is completely in
- * sync with the latest commit to the store!
- * <p>
- * Consider using InstantiatedIndex as if it was immutable.
- */
-public class InstantiatedIndexReader extends IndexReader {
-
-  private final InstantiatedIndex index;
-  private ReaderContext context = new AtomicReaderContext(this);
-
-
-  public InstantiatedIndexReader(InstantiatedIndex index) {
-    super();
-    this.index = index;
-    readerFinishedListeners = Collections.synchronizedSet(new HashSet<ReaderFinishedListener>());
-  }
-
-  /**
-   * An InstantiatedIndexReader is not a snapshot in time, it is completely in
-   * sync with the latest commit to the store!
-   * 
-   * @return output from {@link InstantiatedIndex#getVersion()} in associated instantiated index.
-   */
-  @Override
-  public long getVersion() {
-    return index.getVersion();
-  }
-
-  @Override
-  public Directory directory() {
-    throw new UnsupportedOperationException();
-  }
-
-  /**
-   * An InstantiatedIndexReader is always current!
-   * 
-   * Check whether this IndexReader is still using the current (i.e., most
-   * recently committed) version of the index. If a writer has committed any
-   * changes to the index since this reader was opened, this will return
-   * <code>false</code>, in which case you must open a new IndexReader in
-   * order to see the changes. See the description of the <a
-   * href="IndexWriter.html#autoCommit"><code>autoCommit</code></a> flag
-   * which controls when the {@link IndexWriter} actually commits changes to the
-   * index.
-   * 
-   * @return always true
-   * @throws CorruptIndexException if the index is corrupt
-   * @throws IOException if there is a low-level IO error
-   * @throws UnsupportedOperationException unless overridden in subclass
-   */
-  @Override
-  public boolean isCurrent() throws IOException {
-    return true;
-  }
-
-  public InstantiatedIndex getIndex() {
-    return index;
-  }
-
-  @Override
-  public Bits getLiveDocs() {
-    return new Bits() {
-      public boolean get(int n) {
-        return !(index.getDeletedDocuments() != null && index.getDeletedDocuments().get(n))
-                   && !(uncommittedDeletedDocuments != null && uncommittedDeletedDocuments.get(n));
-      }
-
-      public int length() {
-        return maxDoc();
-      }
-    };
-  }
-
-  private BitVector uncommittedDeletedDocuments;
-
-  private Map<String,List<NormUpdate>> uncommittedNormsByFieldNameAndDocumentNumber = null;
-
-  private static class NormUpdate {
-    private int doc;
-    private byte value;
-
-    public NormUpdate(int doc, byte value) {
-      this.doc = doc;
-      this.value = value;
-    }
-  }
-
-  @Override
-  public int numDocs() {
-    // todo i suppose this value could be cached, but array#length and bitvector#count is fast.
-    int numDocs = getIndex().getDocumentsByNumber().length;
-    if (uncommittedDeletedDocuments != null) {
-      numDocs -= uncommittedDeletedDocuments.count();
-    }
-    if (index.getDeletedDocuments() != null) {
-      numDocs -= index.getDeletedDocuments().count();
-    }
-    return numDocs;
-  }
-
-  @Override
-  public int maxDoc() {
-    return getIndex().getDocumentsByNumber().length;
-  }
-
-  @Override
-  public boolean hasDeletions() {
-    return index.getDeletedDocuments() != null || uncommittedDeletedDocuments != null;
-  }
-
-  @Override
-  protected void doDelete(int docNum) throws IOException {
-
-    // dont delete if already deleted
-    if ((index.getDeletedDocuments() != null && index.getDeletedDocuments().get(docNum))
-        || (uncommittedDeletedDocuments != null && uncommittedDeletedDocuments.get(docNum))) {
-      return;
-    }
-
-    if (uncommittedDeletedDocuments == null) {
-      uncommittedDeletedDocuments = new BitVector(maxDoc());
-    }
-
-    uncommittedDeletedDocuments.set(docNum);
-  }
-
-  @Override
-  protected void doUndeleteAll() throws IOException {
-    // todo: read/write lock
-    uncommittedDeletedDocuments = null;
-    // todo: read/write unlock
-  }
-
-  @Override
-  protected void doCommit(Map<String,String> commitUserData) throws IOException {
-    // todo: read/write lock
-
-    // 1. update norms
-    if (uncommittedNormsByFieldNameAndDocumentNumber != null) {
-      for (Map.Entry<String,List<NormUpdate>> e : uncommittedNormsByFieldNameAndDocumentNumber.entrySet()) {
-        byte[] norms = getIndex().getNormsByFieldNameAndDocumentNumber().get(e.getKey());
-        for (NormUpdate normUpdate : e.getValue()) {
-          norms[normUpdate.doc] = normUpdate.value;
-        }
-      }
-      uncommittedNormsByFieldNameAndDocumentNumber = null;
-    }
-
-    // 2. remove deleted documents
-    if (uncommittedDeletedDocuments != null) {
-      if (index.getDeletedDocuments() == null) {
-        index.setDeletedDocuments(uncommittedDeletedDocuments);
-      } else {
-        for (int d = 0; d< uncommittedDeletedDocuments.size(); d++) {
-          if (uncommittedDeletedDocuments.get(d)) {
-            index.getDeletedDocuments().set(d);
-          }
-        }
-      }
-      uncommittedDeletedDocuments = null;
-    }
-
-    // todo unlock read/writelock
-  }
-
-  @Override
-  protected void doClose() throws IOException {
-    // ignored
-    // todo perhaps release all associated instances?
-  }
-
-  @Override
-  public Collection<String> getFieldNames(FieldOption fieldOption) {
-    Set<String> fieldSet = new HashSet<String>();
-    for (FieldSetting fi : index.getFieldSettings().values()) {
-      if (fieldOption == IndexReader.FieldOption.ALL) {
-        fieldSet.add(fi.fieldName);
-      } else if (!fi.indexed && fieldOption == IndexReader.FieldOption.UNINDEXED) {
-        fieldSet.add(fi.fieldName);
-      } else if (fi.storePayloads && fieldOption == IndexReader.FieldOption.STORES_PAYLOADS) {
-        fieldSet.add(fi.fieldName);
-      } else if (fi.indexed && fieldOption == IndexReader.FieldOption.INDEXED) {
-        fieldSet.add(fi.fieldName);
-      } else if (fi.indexed && fi.storeTermVector == false && fieldOption == IndexReader.FieldOption.INDEXED_NO_TERMVECTOR) {
-        fieldSet.add(fi.fieldName);
-      } else if (fi.storeTermVector == true && fi.storePositionWithTermVector == false && fi.storeOffsetWithTermVector == false
-          && fieldOption == IndexReader.FieldOption.TERMVECTOR) {
-        fieldSet.add(fi.fieldName);
-      } else if (fi.indexed && fi.storeTermVector && fieldOption == IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR) {
-        fieldSet.add(fi.fieldName);
-      } else if (fi.storePositionWithTermVector && fi.storeOffsetWithTermVector == false
-          && fieldOption == IndexReader.FieldOption.TERMVECTOR_WITH_POSITION) {
-        fieldSet.add(fi.fieldName);
-      } else if (fi.storeOffsetWithTermVector && fi.storePositionWithTermVector == false
-          && fieldOption == IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET) {
-        fieldSet.add(fi.fieldName);
-      } else if ((fi.storeOffsetWithTermVector && fi.storePositionWithTermVector)
-          && fieldOption == IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET) {
-        fieldSet.add(fi.fieldName);
-      } 
-    }
-    return fieldSet;
-  }
-
-  /**
-   * Returns the stored fields of the <code>n</code><sup>th</sup>
-   * <code>Document</code> in this index.
-   * <p>
-   * <b>Warning!</b>
-   * The resulting document is the actual stored document instance
-   * and not a deserialized clone as retuned by an IndexReader
-   * over a {@link org.apache.lucene.store.Directory}.
-   * I.e., if you need to touch the document, clone it first!
-   * <p>
-   * This can also be seen as a feature for live changes of stored values,
-   * but be careful! Adding a field with an name unknown to the index
-   * or to a field with previously no stored values will make
-   * {@link org.apache.lucene.store.instantiated.InstantiatedIndexReader#getFieldNames(org.apache.lucene.index.IndexReader.FieldOption)}
-   * out of sync, causing problems for instance when merging the
-   * instantiated index to another index.
-   *
-   * @throws CorruptIndexException if the index is corrupt
-   * @throws IOException if there is a low-level IO error
-   */
-
-  @Override
-  public Document document(int n) throws IOException {
-    return getIndex().getDocumentsByNumber()[n].getDocument();
-  }
-
-  @Override
-  public void document(int docID, StoredFieldVisitor visitor) throws IOException {
-    throw new UnsupportedOperationException();
-  }
-  
-  /**
-   * never ever touch these values. it is the true values, unless norms have
-   * been touched.
-   */
-  @Override
-  public byte[] norms(String field) throws IOException {
-    byte[] norms = getIndex().getNormsByFieldNameAndDocumentNumber().get(field);
-    if (norms == null) {
-      return new byte[0]; // todo a static final zero length attribute?
-    }
-    if (uncommittedNormsByFieldNameAndDocumentNumber != null) {
-      norms = norms.clone();
-      List<NormUpdate> updated = uncommittedNormsByFieldNameAndDocumentNumber.get(field);
-      if (updated != null) {
-        for (NormUpdate normUpdate : updated) {
-          norms[normUpdate.doc] = normUpdate.value;
-        }
-      }
-    }
-    return norms;
-  }
-
-  @Override
-  protected void doSetNorm(int doc, String field, byte value) throws IOException {
-    if (uncommittedNormsByFieldNameAndDocumentNumber == null) {
-      uncommittedNormsByFieldNameAndDocumentNumber = new HashMap<String,List<NormUpdate>>(getIndex().getNormsByFieldNameAndDocumentNumber().size());
-    }
-    List<NormUpdate> list = uncommittedNormsByFieldNameAndDocumentNumber.get(field);
-    if (list == null) {
-      list = new LinkedList<NormUpdate>();
-      uncommittedNormsByFieldNameAndDocumentNumber.put(field, list);
-    }
-    list.add(new NormUpdate(doc, value));
-  }
-
-  @Override
-  public int docFreq(Term t) throws IOException {
-    InstantiatedTerm term = getIndex().findTerm(t);
-    if (term == null) {
-      return 0;
-    } else {
-      return term.getAssociatedDocuments().length;
-    }
-  }
-
-  @Override
-  public Fields fields() {
-    if (getIndex().getOrderedTerms().length == 0) {
-      return null;
-    }
-
-    return new Fields() {
-      @Override
-      public FieldsEnum iterator() {
-        final InstantiatedTerm[] orderedTerms = getIndex().getOrderedTerms();
-
-        return new FieldsEnum() {
-          int upto = -1;
-          String currentField;
-
-          @Override
-          public String next() {
-            do {
-              upto++;
-              if (upto >= orderedTerms.length) {
-                return null;
-              }
-            } while(orderedTerms[upto].field().equals(currentField));
-            
-            currentField = orderedTerms[upto].field();
-            return currentField;
-          }
-
-          @Override
-          public TermsEnum terms() {
-            return new InstantiatedTermsEnum(orderedTerms, upto, currentField);
-          }
-        };
-      }
-
-      @Override
-      public Terms terms(final String field) {
-        final InstantiatedTerm[] orderedTerms = getIndex().getOrderedTerms();
-        int i = Arrays.binarySearch(orderedTerms, new Term(field), InstantiatedTerm.termComparator);
-        if (i < 0) {
-          i = -i - 1;
-        }
-        if (i >= orderedTerms.length || !orderedTerms[i].field().equals(field)) {
-          // field does not exist
-          return null;
-        }
-        final int startLoc = i;
-
-        // TODO: heavy to do this here; would be better to
-        // do it up front & cache
-        long sum = 0;
-        int upto = i;
-        while(upto < orderedTerms.length && orderedTerms[i].field().equals(field)) {
-          sum += orderedTerms[i].getTotalTermFreq();
-          upto++;
-        }
-        final long sumTotalTermFreq = sum;
-
-        return new Terms() {
-          @Override 
-          public TermsEnum iterator() {
-            return new InstantiatedTermsEnum(orderedTerms, startLoc, field);
-          }
-
-          @Override
-          public long getSumTotalTermFreq() {
-            return sumTotalTermFreq;
-          }
-          
-          // TODO: support this?
-          @Override
-          public long getSumDocFreq() {
-            return -1;
-          }
-
-          // TODO: support this?
-          @Override
-          public int getDocCount() throws IOException {
-            return -1;
-          }
-
-          @Override
-          public long getUniqueTermCount() throws IOException {
-            return -1;
-          }
-
-          @Override
-          public Comparator<BytesRef> getComparator() {
-            return BytesRef.getUTF8SortedAsUnicodeComparator();
-          }
-        };
-      }
-    };
-  }
-  
-  @Override
-  public ReaderContext getTopReaderContext() {
-    return context;
-  }
-
-  @Override
-  public TermFreqVector[] getTermFreqVectors(int docNumber) throws IOException {
-    InstantiatedDocument doc = getIndex().getDocumentsByNumber()[docNumber];
-    if (doc.getVectorSpace() == null) {
-      return null;
-    }
-    TermFreqVector[] ret = new TermFreqVector[doc.getVectorSpace().size()];
-    Iterator<String> it = doc.getVectorSpace().keySet().iterator();
-    for (int i = 0; i < ret.length; i++) {
-      ret[i] = new InstantiatedTermPositionVector(getIndex().getDocumentsByNumber()[docNumber], it.next());
-    }
-    return ret;
-  }
-
-  @Override
-  public TermFreqVector getTermFreqVector(int docNumber, String field) throws IOException {
-    InstantiatedDocument doc = getIndex().getDocumentsByNumber()[docNumber];
-    if (doc.getVectorSpace() == null || doc.getVectorSpace().get(field) == null) {
-      return null;
-    } else {
-      return new InstantiatedTermPositionVector(doc, field);
-    }
-  }
-
-  @Override
-  public void getTermFreqVector(int docNumber, String field, TermVectorMapper mapper) throws IOException {
-    InstantiatedDocument doc = getIndex().getDocumentsByNumber()[docNumber];
-    if (doc.getVectorSpace() != null && doc.getVectorSpace().get(field) == null) {
-      List<InstantiatedTermDocumentInformation> tv = doc.getVectorSpace().get(field);
-      mapper.setExpectations(field, tv.size(), true, true);
-      for (InstantiatedTermDocumentInformation tdi : tv) {
-        mapper.map(tdi.getTerm().getTerm().bytes(), tdi.getTermPositions().length, tdi.getTermOffsets(), tdi.getTermPositions());
-      }
-    }
-  }
-
-  @Override
-  public void getTermFreqVector(int docNumber, TermVectorMapper mapper) throws IOException {
-    InstantiatedDocument doc = getIndex().getDocumentsByNumber()[docNumber];
-    for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> e : doc.getVectorSpace().entrySet()) {
-      mapper.setExpectations(e.getKey(), e.getValue().size(), true, true);
-      for (InstantiatedTermDocumentInformation tdi : e.getValue()) {
-        mapper.map(tdi.getTerm().getTerm().bytes(), tdi.getTermPositions().length, tdi.getTermOffsets(), tdi.getTermPositions());
-      }
-    }
-  }
-
-  @Override
-  public PerDocValues perDocValues() throws IOException {
-    return null;
-  }
-}
diff --git a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter.java b/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter.java
deleted file mode 100644
index f49d8d7..0000000
--- a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter.java
+++ /dev/null
@@ -1,693 +0,0 @@
-package org.apache.lucene.store.instantiated;
-
-/**
- * Copyright 2006 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.PrintStream;
-import java.io.StringReader;
-import java.io.Closeable;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.LinkedHashMap;
-import java.util.LinkedList;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.Token;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.index.FieldInvertState;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexableField;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.TermVectorOffsetInfo;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.similarities.SimilarityProvider;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.CollectionUtil;
-import org.apache.lucene.util.AttributeImpl;
-import org.apache.lucene.util.BitVector;
-
-/**
- * This class, similar to {@link org.apache.lucene.index.IndexWriter}, has no locking mechanism.
- * <p>
- * {@link org.apache.lucene.store.instantiated.InstantiatedIndexReader} is navigating
- * the same instances in memory as this writer is updating so searchers active while
- * you are committing are bound to throw exceptions.
- * <p>
- * Consider using InstantiatedIndex as if it was immutable.
- *
- * @see org.apache.lucene.index.IndexWriter 
- */
-public class InstantiatedIndexWriter implements Closeable {
-
-  private PrintStream infoStream = null;
-
-  private final InstantiatedIndex index;
-  private final Analyzer analyzer;
-
-  private SimilarityProvider similarityProvider = IndexSearcher.getDefaultSimilarityProvider(); // how to normalize;
-
-  private transient Set<String> fieldNameBuffer;
-  /**
-   * linked to ensure chronological order
-   */
-  private Map<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> termDocumentInformationFactoryByDocument = new LinkedHashMap<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>>(2000);
-
-  private Set<InstantiatedDocument> unflushedDocuments = new HashSet<InstantiatedDocument>();
-
-  public InstantiatedIndexWriter(InstantiatedIndex index) throws IOException {
-    this(index, null);
-  }
-
-  public InstantiatedIndexWriter(InstantiatedIndex index, Analyzer analyzer) throws IOException {
-    this(index, analyzer, false);
-  }
-
-  public InstantiatedIndexWriter(InstantiatedIndex index, Analyzer analyzer, boolean create) throws IOException {
-    this.index = index;
-    this.analyzer = analyzer;
-    fieldNameBuffer = new HashSet<String>();
-    if (create) {
-      this.index.initialize();
-    }
-  }
-
-  private int mergeFactor = 2500;
-
-  /**
-   * The sweetspot for this implementation is somewhere around 2500 at 2K text large documents.
-   * <p/>
-   * Benchmark output:
-   * <pre>
-   *  ------------> Report sum by Prefix (MAddDocs) and Round (8 about 8 out of 160153)
-   *  Operation      round  mrg buf cmpnd   runCnt   recsPerRun        rec/s  elapsedSec    avgUsedMem    avgTotalMem
-   *  MAddDocs_20000     0   10  10  true        1        20000         81,4      245,68   200 325 152    268 156 928
-   *  MAddDocs_20000 -   1 1000  10  true -  -   1 -  -   20000 -  -   494,1 -  -  40,47 - 247 119 072 -  347 025 408
-   *  MAddDocs_20000     2   10 100  true        1        20000        104,8      190,81   233 895 552    363 720 704
-   *  MAddDocs_20000 -   3 2000 100  true -  -   1 -  -   20000 -  -   527,2 -  -  37,94 - 266 136 448 -  378 273 792
-   *  MAddDocs_20000     4   10  10 false        1        20000        103,2      193,75   222 089 792    378 273 792
-   *  MAddDocs_20000 -   5 3000  10 false -  -   1 -  -   20000 -  -   545,2 -  -  36,69 - 237 917 152 -  378 273 792
-   *  MAddDocs_20000     6   10 100 false        1        20000        102,7      194,67   237 018 976    378 273 792
-   *  MAddDocs_20000 -   7 4000 100 false -  -   1 -  -   20000 -  -   535,8 -  -  37,33 - 309 680 640 -  501 968 896
-   * </pre>
-   *
-   * @see org.apache.lucene.index.LogMergePolicy#setMergeFactor(int)
-   */
-  public void setMergeFactor(int mergeFactor) {
-    this.mergeFactor = mergeFactor;
-  }
-
-  /**
-   * @see org.apache.lucene.index.LogMergePolicy#getMergeFactor()
-   */
-  public int getMergeFactor() {
-    return mergeFactor;
-  }
-
-
-  /**
-   * If non-null, information about merges and a message when
-   * maxFieldLength is reached could be printed to this -- currently
-   * not yet implemented.
-   */
-  public void setInfoStream(PrintStream infoStream) {
-    this.infoStream = infoStream;
-  }
-
-
-  public void abort() throws IOException {
-    // what not
-  }
-
-
-  public void addIndexes(IndexReader[] readers) {
-    throw new RuntimeException("Not implemented");
-  }
-
-
-  public PrintStream getInfoStream() {
-    return infoStream;
-  }
-
-
-  /**
-   * Flushes all changes to an index and closes all associated files.
-   */
-  public void close() throws IOException {
-    commit();
-  }
-
-  /**
-   * Returns the number of documents currently in this index.
-   */
-  public int docCount() {
-    // todo: not certain. see http://www.nabble.com/IndexWriter.docCount-tf3128882.html#a8669483
-    return index.getDocumentsByNumber().length /* - index.getDeletedDocuments().size() */ + unflushedDocuments.size();
-  }
-
-  /**
-   * Locks the index and commits the buffered documents.
-   */
-  public void commit() throws IOException {
-
-    // todo write lock, unless held by caller
-
-    boolean orderedTermsDirty = false;
-    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);
-    
-    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();
-    for (String fieldName : fieldNameBuffer) {
-      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));
-    }
-
-    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];
-    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);
-    int documentNumber = index.getDocumentsByNumber().length;
-
-    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);
-    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {
-      orderedTerms.add(instantiatedTerm);
-    }
-
-    // update norm array with fake values for new documents
-    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());
-    Set<String> fieldNames = new HashSet<String>(20);
-    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());
-    fieldNames.addAll(fieldNameBuffer);
-    for (String field : index.getTermsByFieldAndText().keySet()) {
-      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];
-      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);
-      if (oldNorms != null) {
-        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);
-        Arrays.fill(norms, oldNorms.length, norms.length, (byte) 0);
-      } else {
-        Arrays.fill(norms, 0, norms.length, (byte) 0);
-      }
-      normsByFieldNameAndDocumentNumber.put(field, norms);
-      fieldNames.remove(field);
-    }
-    for (String field : fieldNames) {
-      //System.out.println(field);
-      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];
-      Arrays.fill(norms, 0, norms.length, (byte) 0);
-      normsByFieldNameAndDocumentNumber.put(field, norms);
-    }
-    fieldNames.clear();
-    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);
-
-    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {
-
-      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();
-
-      // assign document number
-      document.setDocumentNumber(documentNumber++);
-      documentsByNumber[document.getDocumentNumber()] = document;
-
-      // set norms, prepare document and create optimized size collections.
-
-      int numFieldsWithTermVectorsInDocument = 0;
-      int termsInDocument = 0;
-      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {
-        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {
-          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();
-        }
-        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();
-
-        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {
-          final String fieldName = eFieldTermDocInfoFactoriesByTermText.getKey().fieldName;
-          final FieldInvertState invertState = new FieldInvertState();
-          invertState.setBoost(eFieldTermDocInfoFactoriesByTermText.getKey().boost);
-          invertState.setLength(eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);
-          normsByFieldNameAndDocumentNumber.get(fieldName)[document.getDocumentNumber()] = similarityProvider.get(fieldName).computeNorm(invertState);
-        } else {
-          System.currentTimeMillis();
-        }
-
-      }
-
-      /** used for term vectors only, i think.. */
-      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);
-
-
-      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());
-
-      // terms...
-      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {
-        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());
-
-        // find or create term
-        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {
-
-          // get term..
-          InstantiatedTerm term;
-          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);
-          if (termsByText == null) {
-            termsByText = new HashMap<String, InstantiatedTerm>(1000);
-            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);
-            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());
-            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);
-            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);
-            pos = -1 - pos;
-            orderedTerms.add(pos, term);
-            orderedTermsDirty = true;
-          } else {
-            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());
-            if (term == null) {
-              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());
-              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);
-              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);
-              pos = -1 - pos;
-              orderedTerms.add(pos, term);
-              orderedTermsDirty = true;
-            }
-          }
-
-          // create association term document information
-          //
-          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]
-          //
-          //                 |
-          //        [TermDocumentInformation]
-
-          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];
-          for (int i = 0; i < positions.length; i++) {
-            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);
-          }
-
-          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];
-          for (int i = 0; i < payloads.length; i++) {
-            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);
-          }
-
-          // couple
-
-          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);
-
-          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!
-          InstantiatedTermDocumentInformation[] associatedDocuments;
-          if (term.getAssociatedDocuments() != null) {
-            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];
-            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);
-          } else {
-            associatedDocuments = new InstantiatedTermDocumentInformation[1];
-          }
-          associatedDocuments[associatedDocuments.length - 1] = info;          
-          term.setAssociatedDocuments(associatedDocuments);
-          term.addPositionsCount(positions.length);
-
-          // todo optimize, only if term vector?
-          informationByTermOfCurrentDocument.put(term, info);
-
-
-          dirtyTerms.add(term);
-        }
-
-        // term vector offsets
-        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {
-          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {
-            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {
-              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());
-              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));
-            }
-          }
-        }
-      }
-
-      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();
-      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {
-        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());
-        if (termDocumentInformations == null) {
-          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();
-          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);
-        }
-        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());
-      }
-
-      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {
-
-        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);
-
-        // add term vector
-        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {
-          if (document.getVectorSpace() == null) {
-            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));
-          }
-          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());
-        }
-
-      }
-      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);
-    }
-
-    // order document informations in dirty terms
-    for (InstantiatedTerm term : dirtyTerms) {
-      // todo optimize, i believe this is useless, that the natural order is document number?
-      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);
-
-//      // update association class reference for speedy skipTo()
-//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {
-//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);
-//      }
-    }
-
-
-    // flush to writer
-    index.setDocumentsByNumber(documentsByNumber);
-    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));
-
-    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {
-      index.getFieldSettings().merge(fieldSetting);
-    }
-    // set term index
-    if (orderedTermsDirty) {
-      // todo optimize, only update from start position
-      for (int i = 0; i < index.getOrderedTerms().length; i++) {
-        index.getOrderedTerms()[i].setTermIndex(i);
-      }
-
-    }
-
-    // remove deleted documents
-    IndexReader indexDeleter = index.indexReaderFactory();
-    if (unflushedDeletions.size() > 0) {
-      for (Term term : unflushedDeletions) {
-        indexDeleter.deleteDocuments(term);
-      }
-      unflushedDeletions.clear();
-    }
-
-
-    // all done, clear buffers
-    unflushedDocuments.clear();
-    termDocumentInformationFactoryByDocument.clear();
-    fieldNameBuffer.clear();
-
-
-    // update deleted documents bitset
-    if (index.getDeletedDocuments() != null) {
-      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);
-      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {
-        if (index.getDeletedDocuments().get(i)) {
-          deletedDocuments.set(i);
-        }
-      }
-      index.setDeletedDocuments(deletedDocuments);
-    }
-
-    index.setVersion(System.currentTimeMillis());
-
-    // todo unlock
-
-    indexDeleter.close();
-
-  }
-
-  private static final Comparator<InstantiatedTermDocumentInformation> tdComp = new Comparator<InstantiatedTermDocumentInformation>() {
-    public int compare(InstantiatedTermDocumentInformation instantiatedTermDocumentInformation, InstantiatedTermDocumentInformation instantiatedTermDocumentInformation1) {
-      return instantiatedTermDocumentInformation.getTerm().getTerm().compareTo(instantiatedTermDocumentInformation1.getTerm().getTerm());
-    }
-  };
-
-  /**
-   * Adds a document to this index.
-   */
-  public void addDocument(Document doc) throws IOException {
-    addDocument(doc, getAnalyzer());
-  }
-
-  /**
-   * Adds a document to this index, using the provided analyzer instead of the
-   * value of {@link #getAnalyzer()}.
-   *
-   * @param doc
-   * @param analyzer
-   * @throws IOException
-   */
-  public void addDocument(Document doc, Analyzer analyzer) throws IOException {
-    addDocument(new InstantiatedDocument(doc), analyzer);
-  }
-
-  /**
-   * Tokenizes a document and adds it to the buffer.
-   * Try to do all calculations in this method rather than in commit, as this is a non locking method.
-   * Remember, this index implementation expects unlimited memory for maximum speed.
-   *
-   * @param document
-   * @param analyzer
-   * @throws IOException
-   */
-  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {
-
-    if (document.getDocumentNumber() != null) {
-      throw new RuntimeException("Document number already set! Are you trying to add a document that already is bound to this or another index?");
-    }
-
-    // todo: write lock
-
-    // normalize settings per field name in document
-
-    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();
-    for (IndexableField field : document.getDocument()) {
-      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());
-      if (fieldSetting == null) {
-        fieldSetting = new FieldSetting();
-        fieldSetting.fieldName = field.name();
-        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);
-        fieldNameBuffer.add(fieldSetting.fieldName);
-      }
-
-      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.
-      fieldSetting.boost *= field.boost();
-      //fieldSettings.dimensions++;
-
-
-      // once fieldSettings, always fieldSettings.
-      if (field.fieldType().omitNorms()) {
-        fieldSetting.omitNorms = true;
-      }
-      if (field.fieldType().indexed() ) {
-        fieldSetting.indexed = true;
-      }
-      if (field.fieldType().tokenized()) {
-        fieldSetting.tokenized = true;
-      }
-      if (field.fieldType().stored()) {
-        fieldSetting.stored = true;
-      }
-      if (field.binaryValue() != null) {
-        fieldSetting.isBinary = true;
-      }
-      if (field.fieldType().storeTermVectors()) {
-        fieldSetting.storeTermVector = true;
-      }
-      if (field.fieldType().storeTermVectorPositions()) {
-        fieldSetting.storePositionWithTermVector = true;
-      }
-      if (field.fieldType().storeTermVectorOffsets()) {
-        fieldSetting.storeOffsetWithTermVector = true;
-      }
-    }
-
-    Map<IndexableField, LinkedList<Token>> tokensByField = new LinkedHashMap<IndexableField, LinkedList<Token>>(20);
-
-    // tokenize indexed fields.
-    for (Iterator<IndexableField> it = document.getDocument().iterator(); it.hasNext();) {
-
-      IndexableField field = it.next();
-
-      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());
-
-      if (field.fieldType().indexed()) {
-
-        LinkedList<Token> tokens = new LinkedList<Token>();
-        tokensByField.put(field, tokens);
-
-        if (field.fieldType().tokenized()) {
-          final TokenStream tokenStream = field.tokenStream(analyzer);
-          // reset the TokenStream to the first token          
-          tokenStream.reset();
-
-          while (tokenStream.incrementToken()) {
-            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:
-            final Token token = new Token();
-            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {
-              final AttributeImpl att = atts.next();
-              try {
-                att.copyTo(token);
-              } catch (Exception e) {
-                // ignore unsupported attributes,
-                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that
-                // implements basic attributes supported by Token and also other customized ones in one class.
-              }
-            }
-            tokens.add(token); // the vector will be built on commit.
-            fieldSetting.fieldLength++;
-          }
-          tokenStream.end();
-          tokenStream.close();
-        } else {
-          // untokenized
-          String fieldVal = field.stringValue();
-          Token token = new Token(0, fieldVal.length(), "untokenized");
-          token.setEmpty().append(fieldVal);
-          tokens.add(token);
-          fieldSetting.fieldLength++;
-        }
-      }
-
-      if (!field.fieldType().stored()) {
-        //it.remove();
-      }
-    }
-
-
-    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();
-    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);
-
-    // build term vector, term positions and term offsets
-    for (Map.Entry<IndexableField, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {
-      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());
-
-      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));
-      if (termDocumentInformationFactoryByTermText == null) {
-        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();
-        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);
-      }
-
-      int lastOffset = 0;
-
-      // for each new field, move positions a bunch.
-      if (fieldSetting.position > 0) {
-        // todo what if no analyzer set, multiple fields with same name and index without tokenization?
-        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);
-      }
-
-      for (Token token : eField_Tokens.getValue()) {
-
-        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.toString());
-        if (termDocumentInformationFactory == null) {
-          termDocumentInformationFactory = new TermDocumentInformationFactory();
-          termDocumentInformationFactoryByTermText.put(token.toString(), termDocumentInformationFactory);
-        }
-        //termDocumentInformationFactory.termFrequency++;
-
-        fieldSetting.position += (token.getPositionIncrement() - 1);
-        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);
-
-        if (token.getPayload() != null && token.getPayload().length() > 0) {
-          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());
-          fieldSetting.storePayloads = true;
-        } else {
-          termDocumentInformationFactory.payloads.add(null);
-        }
-
-        if (eField_Tokens.getKey().fieldType().storeTermVectorOffsets()) {
-
-          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));
-          lastOffset = fieldSetting.offset + token.endOffset();
-        }
-
-
-      }
-
-      if (eField_Tokens.getKey().fieldType().storeTermVectorOffsets()) {
-        fieldSetting.offset = lastOffset + 1;
-      }
-
-    }
-
-
-    unflushedDocuments.add(document);
-
-    // if too many documents in buffer, commit.
-    if (unflushedDocuments.size() >= getMergeFactor()) {
-      commit(/*lock*/);
-    }
-
-    // todo: unlock write lock
-
-  }
-
-
-  private Set<Term> unflushedDeletions = new HashSet<Term>();
-
-  public void deleteDocuments(Term term) throws IOException {
-    unflushedDeletions.add(term);
-  }
-
-  public void deleteDocuments(Term[] terms) throws IOException {
-    for (Term term : terms) {
-      deleteDocuments(term);
-    }
-  }
-
-  public void updateDocument(Term term, Document doc) throws IOException {
-    updateDocument(term, doc, getAnalyzer());
-  }
-
-  public void updateDocument(Term term, Document doc, Analyzer analyzer) throws IOException {
-    deleteDocuments(term);
-    addDocument(doc, analyzer);
-  }
-
-  public SimilarityProvider getSimilarityProvider() {
-    return similarityProvider;
-  }
-
-  public void setSimilarityProvider(SimilarityProvider similarityProvider) {
-    this.similarityProvider = similarityProvider;
-  }
-
-  public Analyzer getAnalyzer() {
-    return analyzer;
-  }
-
-  private class TermDocumentInformationFactory {
-    private LinkedList<byte[]> payloads = new LinkedList<byte[]>();
-    private LinkedList<Integer> termPositions = new LinkedList<Integer>();
-    private LinkedList<TermVectorOffsetInfo> termOffsets = new LinkedList<TermVectorOffsetInfo>();
-  }
-
-
-  static class FieldSetting extends org.apache.lucene.store.instantiated.FieldSetting {
-
-    float boost = 1;
-    int position = 0;
-    int offset;
-    int fieldLength = 0;
-
-    boolean omitNorms = false;
-    boolean isBinary = false;
-
-    private FieldSetting() {
-    }
-
-    private FieldSetting(String fieldName) {
-      super(fieldName);
-    }
-  }
-
-
-}
diff --git a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedTerm.java b/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedTerm.java
deleted file mode 100644
index 67e7f78..0000000
--- a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedTerm.java
+++ /dev/null
@@ -1,255 +0,0 @@
-package org.apache.lucene.store.instantiated;
-
-/**
- * Copyright 2006 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Comparator;
-
-import org.apache.lucene.index.Term;
-
-/**
- * A term in the inverted index, coupled to the documents it occurs in.
- *
- * @see org.apache.lucene.index.Term
- */
-public class InstantiatedTerm {
-
-  public static final Comparator<InstantiatedTerm> comparator = new Comparator<InstantiatedTerm>() {
-    public int compare(InstantiatedTerm instantiatedTerm, InstantiatedTerm instantiatedTerm1) {
-      return instantiatedTerm.getTerm().compareTo(instantiatedTerm1.getTerm());
-    }
-  };
-
-  public static final Comparator<Object> termComparator = new Comparator<Object>() {
-    public int compare(Object o, Object o1) {
-      return ((InstantiatedTerm)o).getTerm().compareTo((Term)o1);
-    }
-  };
-  
-  private Term term;
-
-  private long totalTermFreq;
-
-  /**
-   * index of term in InstantiatedIndex
-   * @see org.apache.lucene.store.instantiated.InstantiatedIndex#getOrderedTerms() */
-  private int termIndex;
-
-  /**
-   * @return Term associated with this entry of the index object graph
-   */
-  public Term getTerm() {
-    return term;
-  }
-
-  InstantiatedTerm(String field, String text) {
-    this.term = new Term(field, text);
-  }
-
-//  this could speed up TermDocs.skipTo even more
-//  private Map</** document number*/Integer, /** index in associatedDocuments */Integer> associatedDocumentIndexByDocumentNumber = new HashMap<Integer, Integer>();
-//
-//  public Map</** document number*/Integer, /** index in associatedDocuments */Integer> getAssociatedDocumentIndexByDocumentNumber() {
-//    return associatedDocumentIndexByDocumentNumber;
-//  }
-
-  /** Ordered by document number */
-  private InstantiatedTermDocumentInformation[] associatedDocuments;
-
-  /**
-   * Meta data per document in which this term is occurring.
-   * Ordered by document number.
-   *
-   * @return Meta data per document in which this term is occurring.
-   */
-  public InstantiatedTermDocumentInformation[] getAssociatedDocuments() {
-    return associatedDocuments;
-  }
-
-
-  /**
-   * Meta data per document in which this term is occurring.
-   * Ordered by document number.
-   *
-   * @param associatedDocuments meta data per document in which this term is occurring, ordered by document number
-   */
-  void setAssociatedDocuments(InstantiatedTermDocumentInformation[] associatedDocuments) {
-    this.associatedDocuments = associatedDocuments;
-  }
-
-  void addPositionsCount(long count) {
-    totalTermFreq += count;
-  }
-
-  public long getTotalTermFreq() {
-    return totalTermFreq;
-  }
-
-  /**
-   * Finds index to the first beyond the current whose document number is
-   * greater than or equal to <i>target</i>, -1 if there is no such element.
-   *
-   * @param target the document number to match
-   * @return -1 if there is no such element
-   */
-  public int seekCeilingDocumentInformationIndex(int target) {
-    return seekCeilingDocumentInformationIndex(target, 0, getAssociatedDocuments().length);
-  }
-
-  /**
-   * Finds index to the first beyond the current whose document number is
-   * greater than or equal to <i>target</i>, -1 if there is no such element.
-   *
-   * @param target the document number to match
-   * @param startOffset associated documents index start offset
-   * @return -1 if there is no such element
-   */
-  public int seekCeilingDocumentInformationIndex(int target, int startOffset) {
-    return seekCeilingDocumentInformationIndex(target, startOffset, getAssociatedDocuments().length);
-  }
-
-  /**
-   * Finds index to the first beyond the current whose document number is
-   * greater than or equal to <i>target</i>, -1 if there is no such element.
-   *
-   * @param target the document number to match
-   * @param startOffset associated documents index start offset
-   * @param endPosition associated documents index end position
-   * @return -1 if there is no such element
-   */
-  public int seekCeilingDocumentInformationIndex(int target, int startOffset, int endPosition) {
-
-    int pos = binarySearchAssociatedDocuments(target, startOffset, endPosition - startOffset);
-
-//    int pos = Arrays.binarySearch(getAssociatedDocuments(), target, InstantiatedTermDocumentInformation.doumentNumberIntegerComparator);
-
-    if (pos < 0) {
-      pos = -1 - pos;
-    }
-    if (getAssociatedDocuments().length <= pos) {
-      return -1;
-    } else {
-      return pos;
-    }
-  }
-
-  public int binarySearchAssociatedDocuments(int target) {
-    return binarySearchAssociatedDocuments(target, 0);
-  }
-
-  public int binarySearchAssociatedDocuments(int target, int offset) {
-    return binarySearchAssociatedDocuments(target, offset, associatedDocuments.length - offset);
-  }
-
-  /**
-   * @param target value to search for in the array
-   * @param offset index of the first valid value in the array
-   * @param length number of valid values in the array
-   * @return index of an occurrence of key in array, or -(insertionIndex + 1) if key is not contained in array (<i>insertionIndex</i> is then the index at which key could be inserted).
-   */
-  public int binarySearchAssociatedDocuments(int target, int offset, int length) {
-
-    // implementation originally from http://ochafik.free.fr/blog/?p=106
-
-    if (length == 0) {
-      return -1 - offset;
-    }
-    int min = offset, max = offset + length - 1;
-    int minVal = getAssociatedDocuments()[min].getDocument().getDocumentNumber();
-    int maxVal = getAssociatedDocuments()[max].getDocument().getDocumentNumber();
-
-
-    int nPreviousSteps = 0;
-
-    for (; ;) {
-
-      // be careful not to compute key - minVal, for there might be an integer overflow.
-      if (target <= minVal) return target == minVal ? min : -1 - min;
-      if (target >= maxVal) return target == maxVal ? max : -2 - max;
-
-      assert min != max;
-
-      int pivot;
-      // A typical binarySearch algorithm uses pivot = (min + max) / 2.
-      // The pivot we use here tries to be smarter and to choose a pivot close to the expectable location of the key.
-      // This reduces dramatically the number of steps needed to get to the key.
-      // However, it does not work well with a logarithmic distribution of values, for instance.
-      // When the key is not found quickly the smart way, we switch to the standard pivot.
-      if (nPreviousSteps > 2) {
-        pivot = (min + max) >> 1;
-        // stop increasing nPreviousSteps from now on
-      } else {
-        // NOTE: We cannot do the following operations in int precision, because there might be overflows.
-        //       long operations are slower than float operations with the hardware this was tested on (intel core duo 2, JVM 1.6.0).
-        //       Overall, using float proved to be the safest and fastest approach.
-        pivot = min + (int) ((target - (float) minVal) / (maxVal - (float) minVal) * (max - min));
-        nPreviousSteps++;
-      }
-
-      int pivotVal = getAssociatedDocuments()[pivot].getDocument().getDocumentNumber();
-
-      // NOTE: do not store key - pivotVal because of overflows
-      if (target > pivotVal) {
-        min = pivot + 1;
-        max--;
-      } else if (target == pivotVal) {
-        return pivot;
-      } else {
-        min++;
-        max = pivot - 1;
-      }
-      maxVal = getAssociatedDocuments()[max].getDocument().getDocumentNumber();
-      minVal = getAssociatedDocuments()[min].getDocument().getDocumentNumber();
-    }
-  }
-
-
-  /**
-   * Navigates to the view of this occurrences of this term in a specific document. 
-   *
-   * This method is only used by InstantiatedIndex(IndexReader) and
-   * should not be optimized for less CPU at the cost of more RAM.
-   *
-   * @param documentNumber the n:th document in the index
-   * @return view of this term from specified document
-   */
-  public InstantiatedTermDocumentInformation getAssociatedDocument(int documentNumber) {
-    int pos = binarySearchAssociatedDocuments(documentNumber);
-    return pos < 0 ? null : getAssociatedDocuments()[pos];
-  }
-
-  public final String field() {
-    return term.field();
-  }
-
-  public final String text() {
-    return term.text();
-  }
-
-  @Override
-  public String toString() {
-    return term.toString();
-  }
-
-
-  public int getTermIndex() {
-    return termIndex;
-  }
-
-  public void setTermIndex(int termIndex) {
-    this.termIndex = termIndex;
-  }
-}
diff --git a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedTermDocumentInformation.java b/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedTermDocumentInformation.java
deleted file mode 100644
index c8d9138..0000000
--- a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedTermDocumentInformation.java
+++ /dev/null
@@ -1,135 +0,0 @@
-package org.apache.lucene.store.instantiated;
-
-import org.apache.lucene.index.TermVectorOffsetInfo;
-
-import java.util.Comparator;
-
-/**
- * Copyright 2006 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * There is one instance of this class per indexed term in a document
- * and it contains the meta data about each occurrence of a term in a document.
- *
- * It is the inner glue of the inverted index.
- *
- * <pre>
- * [Term]-- {0..*} | {0..*} --(field)[Document]
- *            &lt;&lt;ordered>>
- *                 |
- *    [TermDocumentInformation]
- *       +payloads
- *       +termPositions
- *       +termOffsets
- * </pre>
- * 
- */
-public class InstantiatedTermDocumentInformation {
-
-  public static final Comparator<InstantiatedTermDocumentInformation> termComparator = new Comparator<InstantiatedTermDocumentInformation>() {
-    public int compare(InstantiatedTermDocumentInformation instantiatedTermDocumentInformation, InstantiatedTermDocumentInformation instantiatedTermDocumentInformation1) {
-      return instantiatedTermDocumentInformation.getTerm().getTerm().compareTo(instantiatedTermDocumentInformation1.getTerm().getTerm());
-    }
-  };
-
-  public static final Comparator<InstantiatedTermDocumentInformation> documentNumberComparator = new Comparator<InstantiatedTermDocumentInformation>() {
-    public int compare(InstantiatedTermDocumentInformation instantiatedTermDocumentInformation, InstantiatedTermDocumentInformation instantiatedTermDocumentInformation1) {
-      return instantiatedTermDocumentInformation.getDocument().getDocumentNumber().compareTo(instantiatedTermDocumentInformation1.getDocument().getDocumentNumber());
-    }
-  };
-
-  public static final Comparator doumentNumberIntegerComparator = new Comparator() {
-    public int compare(Object o1, Object o2) {
-      InstantiatedTermDocumentInformation di = (InstantiatedTermDocumentInformation) o1;
-      Integer i = (Integer) o2;
-      return di.getDocument().getDocumentNumber().compareTo(i);
-    }
-  };
-
-
-  private byte[][] payloads;
-  private int[] termPositions;
-  private InstantiatedTerm term;
-  private InstantiatedDocument document;
-  private TermVectorOffsetInfo[] termOffsets;
-
-
-
-  public InstantiatedTermDocumentInformation(InstantiatedTerm term, InstantiatedDocument document, int[] termPositions, byte[][] payloads) {
-    this.term = term;
-    this.document = document;
-    this.termPositions = termPositions;
-    this.payloads = payloads;
-  }
-
-
-// not quite sure why I wanted this.
-//  /**
-//   * [Term]--- {0..* ordered} ->[Info]
-//   */
-//  private int indexFromTerm;
-
-
-//  public int getIndexFromTerm() {
-//    return indexFromTerm;
-//  }
-//
-//  void setIndexFromTerm(int indexFromTerm) {
-//    this.indexFromTerm = indexFromTerm;
-//  }
-
-
-  public int[] getTermPositions() {
-    return termPositions;
-  }
-
-
-  public byte[][] getPayloads() {
-    return payloads;
-  }
-
-  public InstantiatedDocument getDocument() {
-    return document;
-  }
-
-
-
-  public InstantiatedTerm getTerm() {
-    return term;
-  }
-
-
-  void setTermPositions(int[] termPositions) {
-    this.termPositions = termPositions;
-  }
-
-
-  void setTerm(InstantiatedTerm term) {
-    this.term = term;
-  }
-
-  void setDocument(InstantiatedDocument document) {
-    this.document = document;
-  }
-
-  public TermVectorOffsetInfo[] getTermOffsets() {
-    return termOffsets;
-  }
-
-  void setTermOffsets(TermVectorOffsetInfo[] termOffsets) {
-    this.termOffsets = termOffsets;
-  }
-}
diff --git a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedTermFreqVector.java b/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedTermFreqVector.java
deleted file mode 100644
index dcbc491..0000000
--- a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedTermFreqVector.java
+++ /dev/null
@@ -1,111 +0,0 @@
-package org.apache.lucene.store.instantiated;
-
-import org.apache.lucene.index.TermFreqVector;
-import org.apache.lucene.util.BytesRef;
-
-import java.util.Arrays;
-import java.util.List;
-
-/**
- * Copyright 2006 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Vector space view of a document in an {@link InstantiatedIndexReader}.
- *
- * @see org.apache.lucene.index.TermFreqVector
- */
-public class InstantiatedTermFreqVector
-    implements TermFreqVector {
-
-  private final List<InstantiatedTermDocumentInformation> termDocumentInformations;
-  private final String field;
-  private final BytesRef terms[];
-  private final int termFrequencies[];
-
-  public InstantiatedTermFreqVector(InstantiatedDocument document, String field) {
-    this.field = field;
-    termDocumentInformations = document.getVectorSpace().get(field);
-    terms = new BytesRef[termDocumentInformations.size()];
-    termFrequencies = new int[termDocumentInformations.size()];
-
-    for (int i = 0; i < termDocumentInformations.size(); i++) {
-      InstantiatedTermDocumentInformation termDocumentInformation = termDocumentInformations.get(i);
-      terms[i] = termDocumentInformation.getTerm().getTerm().bytes();
-      termFrequencies[i] = termDocumentInformation.getTermPositions().length;
-    }
-  }
-
-  /**
-   * @return The number of the field this vector is associated with
-   */
-  public String getField() {
-    return field;
-  }
-
-  @Override
-  public String toString() {
-    StringBuilder sb = new StringBuilder();
-    sb.append('{');
-    sb.append(field).append(": ");
-    if (terms != null) {
-      for (int i = 0; i < terms.length; i++) {
-        if (i > 0) sb.append(", ");
-        sb.append(terms[i]).append('/').append(termFrequencies[i]);
-      }
-    }
-    sb.append('}');
-
-    return sb.toString();
-  }
-
-  public int size() {
-    return terms == null ? 0 : terms.length;
-  }
-
-  public BytesRef[] getTerms() {
-    return terms;
-  }
-
-  public int[] getTermFrequencies() {
-    return termFrequencies;
-  }
-
-  public int indexOf(BytesRef termText) {
-    if (terms == null)
-      return -1;
-    int res = Arrays.binarySearch(terms, termText);
-    return res >= 0 ? res : -1;
-  }
-
-  public int[] indexesOf(BytesRef[] termNumbers, int start, int len) {
-    // TODO: there must be a more efficient way of doing this.
-    //       At least, we could advance the lower bound of the terms array
-    //       as we find valid indices. Also, it might be possible to leverage
-    //       this even more by starting in the middle of the termNumbers array
-    //       and thus dividing the terms array maybe in half with each found index.
-    int res[] = new int[len];
-
-    for (int i = 0; i < len; i++) {
-      res[i] = indexOf(termNumbers[start + i]);
-    }
-    return res;
-  }
-
-  public List<InstantiatedTermDocumentInformation> getTermDocumentInformations() {
-    return termDocumentInformations;
-  }
-
-}
diff --git a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedTermPositionVector.java b/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedTermPositionVector.java
deleted file mode 100644
index 8d40063..0000000
--- a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedTermPositionVector.java
+++ /dev/null
@@ -1,43 +0,0 @@
-package org.apache.lucene.store.instantiated;
-
-/**
- * Copyright 2006 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.TermPositionVector;
-import org.apache.lucene.index.TermVectorOffsetInfo;
-
-/**
- * Extended vector space view of a document in an {@link InstantiatedIndexReader}.
- *
- * @see org.apache.lucene.index.TermPositionVector
- */
-public class InstantiatedTermPositionVector
-    extends InstantiatedTermFreqVector
-    implements TermPositionVector {
-
-  public InstantiatedTermPositionVector(InstantiatedDocument document, String field) {
-    super(document, field);
-  }
-
-  public int[] getTermPositions(int index) {
-    return getTermDocumentInformations().get(index).getTermPositions();
-  }
-
-  public TermVectorOffsetInfo[] getOffsets(int index) {
-    return getTermDocumentInformations().get(index).getTermOffsets();
-  }
-
-}
diff --git a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedTermsEnum.java b/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedTermsEnum.java
deleted file mode 100644
index c83355c..0000000
--- a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedTermsEnum.java
+++ /dev/null
@@ -1,154 +0,0 @@
-package org.apache.lucene.store.instantiated;
-
-/**
- * Copyright 2006 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.index.OrdTermState;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.TermState;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.Comparator;
-
-public class InstantiatedTermsEnum extends TermsEnum {
-  private final String field;
-  private final InstantiatedTerm[] terms;
-  private final BytesRef br = new BytesRef();
-  private final int start;
-  private int upto;
-
-  public InstantiatedTermsEnum(InstantiatedTerm[] terms, int start, String field) {
-    this.start = start;
-    upto = start-1;
-    this.field = field;
-    this.terms = terms;
-  }
-
-  @Override
-  public boolean seekExact(BytesRef text, boolean useCache) {
-    final Term t = new Term(field, text);
-    int loc = Arrays.binarySearch(terms, t, InstantiatedTerm.termComparator);
-    if (loc < 0) {
-      return false;
-    } else {
-      upto = loc;
-      br.copy(text);
-      return true;
-    }
-  }
-
-  @Override
-  public SeekStatus seekCeil(BytesRef text, boolean useCache) {
-    final Term t = new Term(field, text);
-    int loc = Arrays.binarySearch(terms, t, InstantiatedTerm.termComparator);
-    if (loc < 0) {
-      upto = -loc - 1;
-      if (upto >= terms.length) {
-        return SeekStatus.END;
-      } else {
-        br.copy(terms[upto].getTerm().bytes());
-        return SeekStatus.NOT_FOUND;
-      }
-    } else {
-      upto = loc;
-      br.copy(text);
-      return SeekStatus.FOUND;
-    }
-  }
-
-  @Override
-  public void seekExact(long ord) {
-    assert (start + (int) ord) < terms.length;
-    upto = start + (int) ord;
-    assert field.equals(terms[upto].field());
-  }
-
-  @Override
-  public BytesRef next() {
-    upto++;
-    if (upto >= terms.length) {
-      return null;
-    }
-    if (terms[upto].field().equals(field)) {
-      br.copy(terms[upto].getTerm().text());
-      return br;
-    } else {
-      return null;
-    }
-  }
-
-  @Override
-  public BytesRef term() {
-    return br;
-  }
-
-  @Override
-  public long ord() {
-    return upto - start;
-  }
-
-  @Override
-  public int docFreq() {
-    return terms[upto].getAssociatedDocuments().length;
-  }
-
-  @Override
-  public long totalTermFreq() {
-    final long v = terms[upto].getTotalTermFreq();
-    return v == 0 ? -1 : v;
-  }
-
-  @Override
-  public DocsEnum docs(Bits liveDocs, DocsEnum reuse) {
-    if (reuse == null || !(reuse instanceof InstantiatedDocsEnum)) {
-      reuse = new InstantiatedDocsEnum();
-    }
-    return ((InstantiatedDocsEnum) reuse).reset(liveDocs, terms[upto]);
-  }
-
-  @Override
-  public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse) {
-    if (reuse == null || !(reuse instanceof InstantiatedDocsAndPositionsEnum)) {
-      reuse = new InstantiatedDocsAndPositionsEnum();
-    }
-    return ((InstantiatedDocsAndPositionsEnum) reuse).reset(liveDocs, terms[upto]);
-  }
-
-  @Override
-  public Comparator<BytesRef> getComparator() {
-    return BytesRef.getUTF8SortedAsUnicodeComparator();
-  }
-
-  @Override
-  public TermState termState() throws IOException {
-    final OrdTermState state = new OrdTermState();
-    state.ord = upto - start;
-    return state;
-  }
-
-  @Override
-  public void seekExact(BytesRef term, TermState state) throws IOException {
-    assert state != null && state instanceof OrdTermState;
-    seekExact(((OrdTermState)state).ord); // just use the ord for simplicity
-  }
-}
-
diff --git a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/doc-files/HitCollectionBench.jpg b/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/doc-files/HitCollectionBench.jpg
deleted file mode 100644
index e1d1297..0000000
Binary files a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/doc-files/HitCollectionBench.jpg and /dev/null differ
diff --git a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/doc-files/classdiagram.png b/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/doc-files/classdiagram.png
deleted file mode 100644
index 00156ea..0000000
Binary files a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/doc-files/classdiagram.png and /dev/null differ
diff --git a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/doc-files/classdiagram.uxf b/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/doc-files/classdiagram.uxf
deleted file mode 100644
index 3464fbd..0000000
--- a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/doc-files/classdiagram.uxf
+++ /dev/null
@@ -1,50 +0,0 @@
-<?xml version="1.0" encoding="UTF-8"?><umlet_diagram><help_text>//Uncomment the following line to change the fontsize:
-//fontsize=14
-
-//Welcome to UMLet!
-
-// *Double-click on UML elements to add them to the diagram.
-// *Edit element properties by modifying the text in this panel.
-// *Edit the files in the 'palettes' directory to store your own element palettes.
-// *Press Del or Backspace to remove elements from the diagram.
-// *Hold down Ctrl key to select multiple elements.
-// *Press c to copy the UML diagram to the system clipboard.
-// * This text will be stored with each diagram. Feel free to use the area for notes.
-</help_text><element><type>com.umlet.element.base.Class</type><coordinates><x>310</x><y>540</y><w>310</w><h>110</h></coordinates><panel_attributes>bg=#eeeeee
-fg=#000000
-InstantiatedTermDocumentInformation
---
-+payloads:byte[][]
-+termPositions:int[]
-+termOffsets:TermVectorOffsetInfo[]
-+indexFromTerm:int
---</panel_attributes><additional_attributes></additional_attributes></element><element><type>com.umlet.element.base.Relation</type><coordinates><x>460</x><y>380</y><w>40</w><h>180</h></coordinates><panel_attributes>lt=.</panel_attributes><additional_attributes>20;20;20;160</additional_attributes></element><element><type>com.umlet.element.base.Relation</type><coordinates><x>460</x><y>40</y><w>61</w><h>160</h></coordinates><panel_attributes>lt=&lt;-
-q2=field
-m1=0..1</panel_attributes><additional_attributes>30;20;30;140</additional_attributes></element><element><type>com.umlet.element.base.Class</type><coordinates><x>430</x><y>30</y><w>120</w><h>30</h></coordinates><panel_attributes>bg=#eeeeee
-fg=#000099
-_norm: byte[][]_</panel_attributes><additional_attributes></additional_attributes></element><element><type>com.umlet.element.base.Class</type><coordinates><x>80</x><y>390</y><w>100</w><h>30</h></coordinates><panel_attributes>bg=#eeeeee
-fg=#000099
-Term</panel_attributes><additional_attributes></additional_attributes></element><element><type>com.umlet.element.base.Relation</type><coordinates><x>770</x><y>380</y><w>120</w><h>40</h></coordinates><panel_attributes>lt=-&gt;
-m2=1</panel_attributes><additional_attributes>20;20;100;20</additional_attributes></element><element><type>com.umlet.element.base.Class</type><coordinates><x>870</x><y>390</y><w>100</w><h>30</h></coordinates><panel_attributes>bg=#eeeeee
-fg=#000099
-Document</panel_attributes><additional_attributes></additional_attributes></element><element><type>com.umlet.element.base.Class</type><coordinates><x>590</x><y>370</y><w>200</w><h>60</h></coordinates><panel_attributes>bg=#eeeeee
-fg=#000000
-InstantiatedDocument
---
-+documentNumber:int
---</panel_attributes><additional_attributes></additional_attributes></element><element><type>com.umlet.element.base.Relation</type><coordinates><x>520</x><y>190</y><w>170</w><h>200</h></coordinates><panel_attributes>lt=&lt;-
-m1=0..*
-&lt;&lt;ordered&gt;&gt;</panel_attributes><additional_attributes>150;180;20;20</additional_attributes></element><element><type>com.umlet.element.base.Relation</type><coordinates><x>290</x><y>190</y><w>140</w><h>220</h></coordinates><panel_attributes>lt=&lt;-
-m1=0..*
-&lt;&lt;ordered&gt;&gt;</panel_attributes><additional_attributes>20;200;120;20</additional_attributes></element><element><type>com.umlet.element.base.Class</type><coordinates><x>380</x><y>180</y><w>200</w><h>30</h></coordinates><panel_attributes>bg=#eeeeee
-fg=#000000
-InstantiatedIndex</panel_attributes><additional_attributes></additional_attributes></element><element><type>com.umlet.element.base.Relation</type><coordinates><x>160</x><y>380</y><w>110</w><h>40</h></coordinates><panel_attributes>lt=-&gt;
-m2=1</panel_attributes><additional_attributes>90;20;20;20</additional_attributes></element><element><type>com.umlet.element.base.Class</type><coordinates><x>250</x><y>390</y><w>160</w><h>30</h></coordinates><panel_attributes>bg=#eeeeee
-fg=#000000
-InstantiatedTerm
-</panel_attributes><additional_attributes></additional_attributes></element><element><type>com.umlet.element.base.Relation</type><coordinates><x>380</x><y>190</y><w>146</w><h>220</h></coordinates><panel_attributes>lt=&lt;-
-q2=field, term
-m1=0..1</panel_attributes><additional_attributes>20;200;100;20</additional_attributes></element><element><type>com.umlet.element.base.Relation</type><coordinates><x>390</x><y>380</y><w>220</w><h>40</h></coordinates><panel_attributes>lt=-
-q2=field
-m2=0..*
-m1=0..*</panel_attributes><additional_attributes>20;20;200;20</additional_attributes></element></umlet_diagram>
\ No newline at end of file
diff --git a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/package.html b/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/package.html
deleted file mode 100644
index c5422ed..0000000
--- a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/package.html
+++ /dev/null
@@ -1,95 +0,0 @@
-<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
-<html>
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<head>
-  <title>InstantiatedIndex</title>
-</head>
-<body>
-<p>InstantiatedIndex, alternative RAM store for small corpora.</p>
-
-<p>@lucene.experimental</p>
-<h2>Abstract</h2>
-
-<p>
-  Represented as a coupled graph of class instances, this
-  all-in-memory index store implementation delivers search
-  results up to a 100 times faster than the file-centric RAMDirectory
-  at the cost of greater RAM consumption.
-</p>
-
-<h2>API</h2>
-
-<p>
-  Just as the default store implementation, InstantiatedIndex
-  comes with an IndexReader and IndexWriter. The latter share
-  many method signatures with the file-centric IndexWriter.
-</p>
-
-<p>
-  It is also possible to load the content of another index
-  by passing an IndexReader to the InstantiatedIndex constructor.
-</p>
-
-<h2>Performance</h2>
-
-<p>
-  At a few thousand ~160 characters long documents
-  InstantiatedIndex outperforms RAMDirectory some 50x,
-  15x at 100 documents of 2000 characters length,
-  and is linear to RAMDirectory at 10,000 documents of 2000 characters length.
-</p>
-
-<p>Mileage may vary depending on term saturation.</p>
-
-<img src="doc-files/HitCollectionBench.jpg" alt="benchmark"/>
-
-<p>
-  Populated with a single document InstantiatedIndex is almost, but not quite, as fast as MemoryIndex.    
-</p>
-
-<p>
-  It takes more or less the same time to populate an InstantiatedIndex
-  as it takes to populate a RAMDirectory. Hardly any effort has been put
-  in to optimizing the InstantiatedIndexWriter, only minimizing the amount
-  of time needed to write-lock the index has been considered.
-</p>
-
-<h2>Caveats</h2>
-<ul>
-  <li>No locks! Consider using InstantiatedIndex as if it was immutable.</li>
-  <li>No documents with fields containing readers.</li>
-  <li>No field selection when retrieving documents, as all stored field are available in memory.</li>
-  <li>Any document returned must cloned if they are to be touched.</li>
-  <li>Norms array returned must not be touched.</li>
-</ul>
-
-<h2>Use cases</h2>
-
-<p>
-  Could replace any small index that could do with greater response time.
-  spell check a priori index,
-  the index of new documents exposed to user search agent queries,
-  to compile classifiers in machine learning environments, etc.
-</p>
-
-<h2>Class diagram</h2>
-<a href="doc-files/classdiagram.png"><img width="640px" height="480px" src="doc-files/classdiagram.png" alt="class diagram"></a>
-<br/>
-<a href="doc-files/classdiagram.uxf">Diagram</a> rendered using <a href="http://umlet.com">UMLet</a> 7.1.
-</body>
-</html>
diff --git a/lucene/contrib/instantiated/src/java/overview.html b/lucene/contrib/instantiated/src/java/overview.html
deleted file mode 100644
index eb9b25e..0000000
--- a/lucene/contrib/instantiated/src/java/overview.html
+++ /dev/null
@@ -1,26 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-  <head>
-    <title>
-      instantiated
-    </title>
-  </head>
-  <body>
-  instantiated
-  </body>
-</html>
\ No newline at end of file
diff --git a/lucene/contrib/instantiated/src/test/org/apache/lucene/store/instantiated/TestEmptyIndex.java b/lucene/contrib/instantiated/src/test/org/apache/lucene/store/instantiated/TestEmptyIndex.java
deleted file mode 100644
index 4081190..0000000
--- a/lucene/contrib/instantiated/src/test/org/apache/lucene/store/instantiated/TestEmptyIndex.java
+++ /dev/null
@@ -1,98 +0,0 @@
-/**
- * Copyright 2006 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.lucene.store.instantiated;
-
-import java.io.IOException;
-import java.util.Arrays;
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.MultiFields;
-import org.apache.lucene.index.MultiNorms;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.LuceneTestCase;
-
-public class TestEmptyIndex extends LuceneTestCase {
-
-  public void testSearch() throws Exception {
-
-    InstantiatedIndex ii = new InstantiatedIndex();
-
-    IndexReader r = new InstantiatedIndexReader(ii);
-    IndexSearcher s = newSearcher(r);
-
-    TopDocs td = s.search(new TermQuery(new Term("foo", "bar")), 1);
-
-    assertEquals(0, td.totalHits);
-
-    s.close();
-    r.close();
-    ii.close();
-
-  }
-
-  public void testNorms() throws Exception {
-
-    InstantiatedIndex ii = new InstantiatedIndex();
-    IndexReader r = new InstantiatedIndexReader(ii);
-    testNorms(r);
-    r.close();
-    ii.close();
-
-    // make sure a Directory acts the same
-    Directory d = newDirectory();
-    new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))).close();
-    r = IndexReader.open(d, false);
-    testNorms(r);
-    r.close();
-    d.close();
-
-  }
-
-  private void testNorms(IndexReader r) throws IOException {
-    byte[] norms = MultiNorms.norms(r, "foo");
-    if (norms != null) {
-      assertEquals(0, norms.length);
-    }
-  }
-
-  public void testTermsEnum() throws Exception {
-
-    InstantiatedIndex ii = new InstantiatedIndex();
-    IndexReader r = new InstantiatedIndexReader(ii);
-    termsEnumTest(r);
-    r.close();
-    ii.close();
-
-    // make sure a Directory acts the same
-    Directory d = newDirectory();
-    new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))).close();
-    r = IndexReader.open(d, false);
-    termsEnumTest(r);
-    r.close();
-    d.close();
-  }
-
-  public void termsEnumTest(IndexReader r) throws Exception {
-    assertNull(MultiFields.getFields(r));
-  }
-
-}
diff --git a/lucene/contrib/instantiated/src/test/org/apache/lucene/store/instantiated/TestIndicesEquals.java b/lucene/contrib/instantiated/src/test/org/apache/lucene/store/instantiated/TestIndicesEquals.java
deleted file mode 100644
index 8146b9a..0000000
--- a/lucene/contrib/instantiated/src/test/org/apache/lucene/store/instantiated/TestIndicesEquals.java
+++ /dev/null
@@ -1,583 +0,0 @@
-package org.apache.lucene.store.instantiated;
-/**
- * Copyright 2006 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Comparator;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Random;
-
-import org.apache.lucene.analysis.Token;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.MultiNorms;
-import org.apache.lucene.index.Payload;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.MultiFields;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.FieldsEnum;
-import org.apache.lucene.index.TermFreqVector;
-import org.apache.lucene.index.TermPositionVector;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.AttributeImpl;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Bits;
-
-/**
- * Asserts equality of content and behaviour of two index readers.
- */
-public class TestIndicesEquals extends LuceneTestCase {
-
-//  public void test2() throws Exception {
-//    FSDirectory fsdir = FSDirectory.open(new File("/tmp/fatcorpus"));
-//    IndexReader ir = IndexReader.open(fsdir, false);
-//    InstantiatedIndex ii = new InstantiatedIndex(ir);
-//    ir.close();
-//    testEquals(fsdir, ii);
-//  }
-
-
-  public void testLoadIndexReader() throws Exception {
-    Directory dir = newDirectory();
-
-    // create dir data
-    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(
-        TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));
-    
-    for (int i = 0; i < 20; i++) {
-      Document document = new Document();
-      assembleDocument(document, i);
-      indexWriter.addDocument(document);
-    }
-    indexWriter.close();
-
-    // test load ii from index reader
-    IndexReader ir = IndexReader.open(dir, false);
-    InstantiatedIndex ii = new InstantiatedIndex(ir);
-    ir.close();
-
-    testEqualBehaviour(dir, ii);
-    dir.close();
-  }
-
-
-  public void testInstantiatedIndexWriter() throws Exception {
-
-    Directory dir = newDirectory();
-    InstantiatedIndex ii = new InstantiatedIndex();
-    
-    // we need to pass the "same" random to both, so they surely index the same payload data.
-    long seed = random.nextLong();
-    
-    // create dir data
-    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(
-                                                                        TEST_VERSION_CURRENT, new MockAnalyzer(new Random(seed))).setMergePolicy(newLogMergePolicy()));
-    if (VERBOSE) {
-      System.out.println("TEST: make test index");
-    }
-    for (int i = 0; i < 500; i++) {
-      Document document = new Document();
-      assembleDocument(document, i);
-      indexWriter.addDocument(document);
-    }
-    indexWriter.close();
-
-    // test ii writer
-    InstantiatedIndexWriter instantiatedIndexWriter = ii.indexWriterFactory(new MockAnalyzer(new Random(seed)), true);
-    for (int i = 0; i < 500; i++) {
-      Document document = new Document();
-      assembleDocument(document, i);
-      instantiatedIndexWriter.addDocument(document);
-    }
-    instantiatedIndexWriter.close();
-
-
-    testEqualBehaviour(dir, ii);
-
-    dir.close();
-
-  }
-
-
-  private void testTermDocsSomeMore(Directory aprioriIndex, InstantiatedIndex testIndex) throws Exception {
-
-    IndexReader aprioriReader = IndexReader.open(aprioriIndex, false);
-    IndexReader testReader = testIndex.indexReaderFactory();
-
-    // test seek
-
-    Term t = new Term("c", "danny");
-    TermsEnum aprioriTermEnum = MultiFields.getTerms(aprioriReader, t.field()).iterator();
-    aprioriTermEnum.seekCeil(new BytesRef(t.text()));
-    TermsEnum testTermEnum = MultiFields.getTerms(testReader, t.field()).iterator();
-    testTermEnum.seekCeil(new BytesRef(t.text()));
-    assertEquals(aprioriTermEnum.term(), testTermEnum.term());
-
-    DocsEnum aprioriTermDocs = aprioriTermEnum.docs(MultiFields.getLiveDocs(aprioriReader), null);
-    DocsEnum testTermDocs = testTermEnum.docs(MultiFields.getLiveDocs(testReader), null);
-
-    assertEquals(aprioriTermDocs.nextDoc(), testTermDocs.nextDoc());
-    assertEquals(aprioriTermDocs.freq(), testTermDocs.freq());
-
-    if (aprioriTermDocs.advance(4) != DocsEnum.NO_MORE_DOCS) {
-      assertTrue(testTermDocs.advance(4) != DocsEnum.NO_MORE_DOCS);
-      assertEquals(aprioriTermDocs.freq(), testTermDocs.freq());
-      assertEquals(aprioriTermDocs.docID(), testTermDocs.docID());
-    } else {
-      assertEquals(DocsEnum.NO_MORE_DOCS, testTermDocs.advance(4));
-    }
-
-    if (aprioriTermDocs.nextDoc() != DocsEnum.NO_MORE_DOCS) {
-      assertTrue(testTermDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);
-      assertEquals(aprioriTermDocs.freq(), testTermDocs.freq());
-      assertEquals(aprioriTermDocs.docID(), testTermDocs.docID());
-    } else {
-      assertEquals(DocsEnum.NO_MORE_DOCS, testTermDocs.nextDoc());
-    }
-
-
-    // beyond this point all next and skipto will return false
-
-    if (aprioriTermDocs.advance(100) != DocsEnum.NO_MORE_DOCS) {
-      assertTrue(testTermDocs.advance(100) != DocsEnum.NO_MORE_DOCS);
-      assertEquals(aprioriTermDocs.freq(), testTermDocs.freq());
-      assertEquals(aprioriTermDocs.docID(), testTermDocs.docID());
-    } else {
-      assertEquals(DocsEnum.NO_MORE_DOCS, testTermDocs.advance(100));
-    }
-
-    // start using the API the way one is supposed to use it
-
-    t = new Term("", "");
-    FieldsEnum apFieldsEnum = MultiFields.getFields(aprioriReader).iterator();
-    String apFirstField = apFieldsEnum.next();
-
-    FieldsEnum testFieldsEnum = MultiFields.getFields(testReader).iterator();
-    String testFirstField = testFieldsEnum.next();
-    assertEquals(apFirstField, testFirstField);
-
-    aprioriTermEnum = apFieldsEnum.terms();
-    testTermEnum = testFieldsEnum.terms();
-    
-    assertEquals(aprioriTermEnum.next(), testTermEnum.next());
-    
-    aprioriTermDocs = aprioriTermEnum.docs(MultiFields.getLiveDocs(aprioriReader), aprioriTermDocs);
-    testTermDocs = testTermEnum.docs(MultiFields.getLiveDocs(testReader), testTermDocs);
-
-    while (aprioriTermDocs.nextDoc() != DocsEnum.NO_MORE_DOCS) {
-      assertTrue(testTermDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);
-      assertEquals(aprioriTermDocs.freq(), testTermDocs.freq());
-      assertEquals(aprioriTermDocs.docID(), testTermDocs.docID());
-    }
-    assertEquals(DocsEnum.NO_MORE_DOCS, testTermDocs.nextDoc());
-
-    // clean up
-    aprioriReader.close();
-    testReader.close();
-
-  }
-
-
-  private void assembleDocument(Document document, int i) {
-    FieldType customType = new FieldType(TextField.TYPE_STORED);
-    customType.setStoreTermVectors(true);
-    customType.setStoreTermVectorOffsets(true);
-    customType.setStoreTermVectorPositions(true);
-    //document.add(new Field("a", i + " Do you really want to go and live in that house all winter?", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
-    document.add(new Field("a", i + " Do you really want to go and live in that house all winter?", customType));
-    if (i > 0) {
-      //document.add(new Field("b0", i + " All work and no play makes Jack a dull boy", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
-      document.add(new Field("b0", i + " All work and no play makes Jack a dull boy", customType));
-
-      //document.add(new Field("b1", i + " All work and no play makes Jack a dull boy", Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS, Field.TermVector.NO));
-      FieldType customType2 = new FieldType(TextField.TYPE_STORED);
-      customType2.setTokenized(false);
-      customType2.setOmitNorms(true);
-      document.add(new Field("b1", i + " All work and no play makes Jack a dull boy", customType2));
-      
-      //document.add(new Field("b2", i + " All work and no play makes Jack a dull boy", Field.Store.NO, Field.Index.NOT_ANALYZED, Field.TermVector.NO));
-      FieldType customType3 = new FieldType(TextField.TYPE_UNSTORED);
-      customType3.setTokenized(false);
-      document.add(new Field("b1", i + " All work and no play makes Jack a dull boy", customType3));
-      
-      //document.add(new Field("b3", i + " All work and no play makes Jack a dull boy", Field.Store.YES, Field.Index.NO, Field.TermVector.NO));
-      FieldType customType4 = new FieldType(TextField.TYPE_STORED);
-      customType4.setIndexed(false);
-      customType4.setTokenized(false);
-      document.add(new Field("b1", i + " All work and no play makes Jack a dull boy", customType4));
-      if (i > 1) {
-        //document.add(new Field("c", i + " Redrum redrum", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
-        document.add(new Field("c", i + " Redrum redrum", customType));
-        if (i > 2) {
-          //document.add(new Field("d", i + " Hello Danny, come and play with us... forever and ever. and ever.", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
-          document.add(new Field("d", i + " Hello Danny, come and play with us... forever and ever. and ever.", customType));
-          if (i > 3) {
-            //Field f = new Field("e", i + " Heres Johnny!", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS);
-            //f.setOmitNorms(true);
-            FieldType customType5 = new FieldType(TextField.TYPE_UNSTORED);
-            customType5.setOmitNorms(true);
-            Field f = new Field("e", i + " Heres Johnny!", customType5);
-            document.add(f);
-            if (i > 4) {
-              final List<Token> tokens = new ArrayList<Token>(2);
-              Token t = createToken("the", 0, 2, "text");
-              t.setPayload(new Payload(new byte[]{1, 2, 3}));
-              tokens.add(t);
-              t = createToken("end", 3, 5, "text");
-              t.setPayload(new Payload(new byte[]{2}));
-              tokens.add(t);
-              tokens.add(createToken("fin", 7, 9));
-              TokenStream ts = new TokenStream(Token.TOKEN_ATTRIBUTE_FACTORY) {
-                final AttributeImpl reusableToken = (AttributeImpl) addAttribute(CharTermAttribute.class);
-                Iterator<Token> it = tokens.iterator();
-                
-                @Override
-                public final boolean incrementToken() throws IOException {
-                  if (!it.hasNext()) {
-                    return false;
-                  }
-                  clearAttributes();
-                  it.next().copyTo(reusableToken);
-                  return true;
-                }
-
-                @Override
-                public void reset() throws IOException {
-                  it = tokens.iterator();
-                }
-              };
-              
-              //document.add(new Field("f", ts));
-              document.add(new TextField("f", ts));
-            }
-          }
-        }
-      }
-    }
-  }
-
-
-  /**
-   * Asserts that the content of two index readers equal each other.
-   *
-   * @param aprioriIndex the index that is known to be correct
-   * @param testIndex    the index that is supposed to equals the apriori index.
-   * @throws Exception
-   */
-  protected void testEqualBehaviour(Directory aprioriIndex, InstantiatedIndex testIndex) throws Exception {
-
-
-    testEquals(aprioriIndex,  testIndex);
-
-    // delete a few documents
-    IndexReader air = IndexReader.open(aprioriIndex, false);
-    InstantiatedIndexReader tir = testIndex.indexReaderFactory();
-
-    assertEquals(air.isCurrent(), tir.isCurrent());
-    assertEquals(air.hasDeletions(), tir.hasDeletions());
-    assertEquals(air.maxDoc(), tir.maxDoc());
-    assertEquals(air.numDocs(), tir.numDocs());
-    assertEquals(air.numDeletedDocs(), tir.numDeletedDocs());
-
-    air.deleteDocument(3);
-    tir.deleteDocument(3);
-
-    assertEquals(air.isCurrent(), tir.isCurrent());
-    assertEquals(air.hasDeletions(), tir.hasDeletions());
-    assertEquals(air.maxDoc(), tir.maxDoc());
-    assertEquals(air.numDocs(), tir.numDocs());
-    assertEquals(air.numDeletedDocs(), tir.numDeletedDocs());
-
-    air.deleteDocument(8);
-    tir.deleteDocument(8);
-
-    assertEquals(air.isCurrent(), tir.isCurrent());
-    assertEquals(air.hasDeletions(), tir.hasDeletions());
-    assertEquals(air.maxDoc(), tir.maxDoc());
-    assertEquals(air.numDocs(), tir.numDocs());
-    assertEquals(air.numDeletedDocs(), tir.numDeletedDocs());    
-
-    // this (in 3.0) commits the deletions
-    air.close();
-    tir.close();
-
-    air = IndexReader.open(aprioriIndex, false);
-    tir = testIndex.indexReaderFactory();
-
-    assertEquals(air.isCurrent(), tir.isCurrent());
-    assertEquals(air.hasDeletions(), tir.hasDeletions());
-    assertEquals(air.maxDoc(), tir.maxDoc());
-    assertEquals(air.numDocs(), tir.numDocs());
-    assertEquals(air.numDeletedDocs(), tir.numDeletedDocs());
-
-    final Bits aLiveDocs = MultiFields.getLiveDocs(air);
-    final Bits tLiveDocs = MultiFields.getLiveDocs(tir);
-    assertTrue((aLiveDocs != null && tLiveDocs != null) || 
-               (aLiveDocs == null && tLiveDocs == null));
-    if (aLiveDocs != null) {
-      for (int d =0; d<air.maxDoc(); d++) {
-        assertEquals(aLiveDocs.get(d), tLiveDocs.get(d));
-      }
-    }
-
-    air.close();
-    tir.close();
-
-
-    // make sure they still equal
-    testEquals(aprioriIndex,  testIndex);
-  }
-
-  protected void testEquals(Directory aprioriIndex, InstantiatedIndex testIndex) throws Exception {
-
-    if (VERBOSE) {
-      System.out.println("TEST: testEquals");
-    }
-    testTermDocsSomeMore(aprioriIndex, testIndex);
-
-    IndexReader aprioriReader = IndexReader.open(aprioriIndex, false);
-    IndexReader testReader = testIndex.indexReaderFactory();
-
-    assertEquals(aprioriReader.numDocs(), testReader.numDocs());
-
-    // assert field options
-    assertEquals(aprioriReader.getFieldNames(IndexReader.FieldOption.INDEXED), testReader.getFieldNames(IndexReader.FieldOption.INDEXED));
-    assertEquals(aprioriReader.getFieldNames(IndexReader.FieldOption.INDEXED_NO_TERMVECTOR), testReader.getFieldNames(IndexReader.FieldOption.INDEXED_NO_TERMVECTOR));
-    assertEquals(aprioriReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR), testReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR));
-    assertEquals(aprioriReader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), testReader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS));
-    assertEquals(aprioriReader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), testReader.getFieldNames(IndexReader.FieldOption.TERMVECTOR));
-    assertEquals(aprioriReader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), testReader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET));
-    assertEquals(aprioriReader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), testReader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION));
-    assertEquals(aprioriReader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), testReader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET));
-    assertEquals(aprioriReader.getFieldNames(IndexReader.FieldOption.UNINDEXED), testReader.getFieldNames(IndexReader.FieldOption.UNINDEXED));
-
-    for (Object field : aprioriReader.getFieldNames(IndexReader.FieldOption.ALL)) {
-
-      // test norms as used by normal use
-
-      byte[] aprioriNorms = MultiNorms.norms(aprioriReader, (String) field);
-      byte[] testNorms = MultiNorms.norms(testReader, (String) field);
-
-      if (aprioriNorms != null) {
-        assertEquals(aprioriNorms.length, testNorms.length);
-
-        for (int i = 0; i < aprioriNorms.length; i++) {
-          assertEquals("norms does not equals for field " + field + " in document " + i, aprioriNorms[i], testNorms[i]);
-        }
-      }
-    }
-
-    final Bits apLiveDocs = MultiFields.getLiveDocs(aprioriReader);
-    final Bits testLiveDocs = MultiFields.getLiveDocs(testReader);
-    assertTrue((apLiveDocs != null && testLiveDocs != null) || 
-               (apLiveDocs == null && testLiveDocs == null));
-    if (apLiveDocs != null) {
-      for (int docIndex = 0; docIndex < aprioriReader.numDocs(); docIndex++) {
-        assertEquals(apLiveDocs.get(docIndex), testLiveDocs.get(docIndex));
-      }
-    }
-
-    // compare term enumeration stepping
-
-    FieldsEnum aprioriFieldsEnum = MultiFields.getFields(aprioriReader).iterator();
-    FieldsEnum testFieldsEnum = MultiFields.getFields(testReader).iterator();
-
-    String aprioriField;
-    while((aprioriField = aprioriFieldsEnum.next()) != null) {
-      String testField = testFieldsEnum.next();
-      if (VERBOSE) {
-        System.out.println("TEST: verify field=" + testField);
-      }
-      assertEquals(aprioriField, testField);
-
-      TermsEnum aprioriTermEnum = aprioriFieldsEnum.terms();
-      TermsEnum testTermEnum = testFieldsEnum.terms();
-
-      BytesRef aprioriText;
-      while((aprioriText = aprioriTermEnum.next()) != null) {
-        assertEquals(aprioriText, testTermEnum.next());
-        if (VERBOSE) {
-          System.out.println("TEST:   verify term=" + aprioriText.utf8ToString());
-        }
-
-        assertTrue(aprioriTermEnum.docFreq() == testTermEnum.docFreq());
-        final long totalTermFreq = aprioriTermEnum.totalTermFreq();
-        if (totalTermFreq != -1) {
-          assertEquals(totalTermFreq, testTermEnum.totalTermFreq());
-        }
-
-        // compare termDocs seeking
-
-        DocsEnum aprioriTermDocs = aprioriTermEnum.docs(MultiFields.getLiveDocs(aprioriReader), null);
-        DocsEnum testTermDocs = testTermEnum.docs(MultiFields.getLiveDocs(testReader), null);
-        
-        while (aprioriTermDocs.nextDoc() != DocsEnum.NO_MORE_DOCS) {
-          assertTrue(testTermDocs.advance(aprioriTermDocs.docID()) != DocsEnum.NO_MORE_DOCS);
-          assertEquals(aprioriTermDocs.docID(), testTermDocs.docID());
-        }
-        
-        // compare documents per term
-        
-        assertEquals(aprioriReader.docFreq(aprioriField, aprioriTermEnum.term()), testReader.docFreq(aprioriField, testTermEnum.term()));
-
-        aprioriTermDocs = aprioriTermEnum.docs(MultiFields.getLiveDocs(aprioriReader), aprioriTermDocs);
-        testTermDocs = testTermEnum.docs(MultiFields.getLiveDocs(testReader), testTermDocs);
-
-        while (true) {
-          if (aprioriTermDocs.nextDoc() == DocsEnum.NO_MORE_DOCS) {
-            assertEquals(DocsEnum.NO_MORE_DOCS, testTermDocs.nextDoc());
-            break;
-          }
-          if (VERBOSE) {
-            System.out.println("TEST:     verify doc=" + aprioriTermDocs.docID());
-          }
-
-          assertTrue(testTermDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);
-
-          assertEquals(aprioriTermDocs.docID(), testTermDocs.docID());
-          assertEquals(aprioriTermDocs.freq(), testTermDocs.freq());
-        }
-
-        // compare term positions
-
-        DocsAndPositionsEnum aprioriTermPositions = aprioriTermEnum.docsAndPositions(MultiFields.getLiveDocs(aprioriReader), null);
-        DocsAndPositionsEnum testTermPositions = testTermEnum.docsAndPositions(MultiFields.getLiveDocs(testReader), null);
-
-        if (VERBOSE) {
-          System.out.println("TEST: enum1=" + aprioriTermPositions + " enum2=" + testTermPositions);
-        }
-        if (aprioriTermPositions != null) {
-
-          for (int docIndex = 0; docIndex < aprioriReader.maxDoc(); docIndex++) {
-            boolean hasNext = aprioriTermPositions.nextDoc() != DocsEnum.NO_MORE_DOCS;
-            if (hasNext) {
-              assertTrue(testTermPositions.nextDoc() != DocsEnum.NO_MORE_DOCS);
-
-              if (VERBOSE) {
-                System.out.println("TEST:     verify doc=" + aprioriTermPositions.docID());
-              }
-              
-              assertEquals(aprioriTermPositions.freq(), testTermPositions.freq());
-
-              for (int termPositionIndex = 0; termPositionIndex < aprioriTermPositions.freq(); termPositionIndex++) {
-                int aprioriPos = aprioriTermPositions.nextPosition();
-                int testPos = testTermPositions.nextPosition();
-
-                if (VERBOSE) {
-                  System.out.println("TEST:       verify pos=" + aprioriPos);
-                }
-
-                assertEquals(aprioriPos, testPos);
-
-                assertEquals(aprioriTermPositions.hasPayload(), testTermPositions.hasPayload());
-                if (aprioriTermPositions.hasPayload()) {
-                  BytesRef apPayload = aprioriTermPositions.getPayload();
-                  BytesRef testPayload = testTermPositions.getPayload();
-                  assertEquals(apPayload, testPayload);
-                }
-              }
-            }
-          }
-        }
-      }
-      assertNull(testTermEnum.next());
-    }
-    assertNull(testFieldsEnum.next());
-
-    // compare term vectors and position vectors
-
-    for (int documentNumber = 0; documentNumber < aprioriReader.numDocs(); documentNumber++) {
-
-      if (documentNumber > 0) {
-        assertNotNull(aprioriReader.getTermFreqVector(documentNumber, "b0"));
-        assertNull(aprioriReader.getTermFreqVector(documentNumber, "b1"));
-
-        assertNotNull(testReader.getTermFreqVector(documentNumber, "b0"));
-        assertNull(testReader.getTermFreqVector(documentNumber, "b1"));
-
-      }
-
-      TermFreqVector[] aprioriFreqVectors = aprioriReader.getTermFreqVectors(documentNumber);
-      TermFreqVector[] testFreqVectors = testReader.getTermFreqVectors(documentNumber);
-
-      if (aprioriFreqVectors != null && testFreqVectors != null) {
-
-        Arrays.sort(aprioriFreqVectors, new Comparator<TermFreqVector>() {
-          public int compare(TermFreqVector termFreqVector, TermFreqVector termFreqVector1) {
-            return termFreqVector.getField().compareTo(termFreqVector1.getField());
-          }
-        });
-        Arrays.sort(testFreqVectors, new Comparator<TermFreqVector>() {
-          public int compare(TermFreqVector termFreqVector, TermFreqVector termFreqVector1) {
-            return termFreqVector.getField().compareTo(termFreqVector1.getField());
-          }
-        });
-
-        assertEquals("document " + documentNumber + " vectors does not match", aprioriFreqVectors.length, testFreqVectors.length);
-
-        for (int freqVectorIndex = 0; freqVectorIndex < aprioriFreqVectors.length; freqVectorIndex++) {
-          assertTrue(Arrays.equals(aprioriFreqVectors[freqVectorIndex].getTermFrequencies(), testFreqVectors[freqVectorIndex].getTermFrequencies()));
-          assertTrue(Arrays.equals(aprioriFreqVectors[freqVectorIndex].getTerms(), testFreqVectors[freqVectorIndex].getTerms()));
-
-          if (aprioriFreqVectors[freqVectorIndex] instanceof TermPositionVector) {
-            TermPositionVector aprioriTermPositionVector = (TermPositionVector) aprioriFreqVectors[freqVectorIndex];
-            TermPositionVector testTermPositionVector = (TermPositionVector) testFreqVectors[freqVectorIndex];
-
-            for (int positionVectorIndex = 0; positionVectorIndex < aprioriFreqVectors[freqVectorIndex].getTerms().length; positionVectorIndex++)
-            {
-              if (aprioriTermPositionVector.getOffsets(positionVectorIndex) != null) {
-                assertTrue(Arrays.equals(aprioriTermPositionVector.getOffsets(positionVectorIndex), testTermPositionVector.getOffsets(positionVectorIndex)));
-              }
-
-              if (aprioriTermPositionVector.getTermPositions(positionVectorIndex) != null) {
-                assertTrue(Arrays.equals(aprioriTermPositionVector.getTermPositions(positionVectorIndex), testTermPositionVector.getTermPositions(positionVectorIndex)));
-              }
-            }
-          }
-
-        }
-      }
-    }
-
-    aprioriReader.close();
-    testReader.close();
-  }
-
-  private static Token createToken(String term, int start, int offset)
-  {
-    return new Token(term, start, offset);
-  }
-
-  private static Token createToken(String term, int start, int offset, String type)
-  {
-    return new Token(term, start, offset, type);
-  }
-
-
-}
diff --git a/lucene/contrib/instantiated/src/test/org/apache/lucene/store/instantiated/TestMultiSegmentReaderOnConstructor.java b/lucene/contrib/instantiated/src/test/org/apache/lucene/store/instantiated/TestMultiSegmentReaderOnConstructor.java
deleted file mode 100644
index a139148..0000000
--- a/lucene/contrib/instantiated/src/test/org/apache/lucene/store/instantiated/TestMultiSegmentReaderOnConstructor.java
+++ /dev/null
@@ -1,72 +0,0 @@
-package org.apache.lucene.store.instantiated;
-
-/*
- *  Licensed under the Apache License, Version 2.0 (the "License");
- *  you may not use this file except in compliance with the License.
- *  You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- *  Unless required by applicable law or agreed to in writing, software
- *  distributed under the License is distributed on an "AS IS" BASIS,
- *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- *  See the License for the specific language governing permissions and
- *  limitations under the License.
- *
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.TextField;
-
-/**
- * @since 2009-mar-30 13:15:49
- */
-public class TestMultiSegmentReaderOnConstructor extends LuceneTestCase {
-
-  public void test() throws Exception {
-    Directory dir = newDirectory();
-    IndexWriter iw = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));
-    addDocument(iw, "Hello, world!");
-    addDocument(iw, "All work and no play makes jack a dull boy");
-    iw.close();
-
-    iw = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));
-    addDocument(iw, "Hello, tellus!");
-    addDocument(iw, "All work and no play makes danny a dull boy");
-    iw.close();
-
-    iw = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));
-    addDocument(iw, "Hello, earth!");
-    addDocument(iw, "All work and no play makes wendy a dull girl");
-    iw.close();
-
-    IndexReader multiSegReader = IndexReader.open(dir, false);
-    multiSegReader.deleteDocument(2);
-
-    try {
-      new InstantiatedIndex(multiSegReader);
-    } catch (Exception e) {
-      e.printStackTrace(System.out);
-      fail("No exceptions when loading a multi-seg reader!");
-    }
-
-    // todo some assertations.
-    multiSegReader.close();
-    dir.close();
-  }
-
-  private void addDocument(IndexWriter iw, String text) throws IOException {
-    Document doc = new Document();
-    doc.add(new TextField("field", text));
-    iw.addDocument(doc);
-  }
-}
diff --git a/lucene/contrib/instantiated/src/test/org/apache/lucene/store/instantiated/TestRealTime.java b/lucene/contrib/instantiated/src/test/org/apache/lucene/store/instantiated/TestRealTime.java
deleted file mode 100644
index bd1e719..0000000
--- a/lucene/contrib/instantiated/src/test/org/apache/lucene/store/instantiated/TestRealTime.java
+++ /dev/null
@@ -1,79 +0,0 @@
-/**
- * Copyright 2006 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.lucene.store.instantiated;
-
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.Scorer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.util.LuceneTestCase;
-
-/**
- * Assert that the content of an index 
- * is instantly available
- * for all open searchers
- * also after a commit.
- */
-public class TestRealTime extends LuceneTestCase {
-
-  public void test() throws Exception {
-
-    InstantiatedIndex index = new InstantiatedIndex();
-    InstantiatedIndexReader reader = new InstantiatedIndexReader(index);
-    IndexSearcher searcher = newSearcher(reader, false);
-    InstantiatedIndexWriter writer = new InstantiatedIndexWriter(index);
-
-    Document doc;
-    Collector collector;
-
-    doc = new Document();
-    doc.add(new StringField("f", "a"));
-    writer.addDocument(doc);
-    writer.commit();
-
-    collector = new Collector();
-    searcher.search(new TermQuery(new Term("f", "a")), collector);
-    assertEquals(1, collector.hits);
-
-    doc = new Document();
-    doc.add(new StringField("f", "a"));
-    writer.addDocument(doc);
-    writer.commit();
-
-    collector = new Collector();
-    searcher.search(new TermQuery(new Term("f", "a")), collector);
-    assertEquals(2, collector.hits);
-
-  }
-
-  public static class Collector extends org.apache.lucene.search.Collector {
-    private int hits = 0;
-    @Override
-    public void setScorer(Scorer scorer) {}
-    @Override
-    public void setNextReader(AtomicReaderContext context) {}
-    @Override
-    public boolean acceptsDocsOutOfOrder() { return true; }
-    @Override
-    public void collect(int doc) {
-      hits++;
-    }
-  }
-
-}
diff --git a/lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java b/lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
index 15cbcac..5522709 100644
--- a/lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
+++ b/lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
@@ -45,10 +45,7 @@ import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.OrdTermState;
 import org.apache.lucene.index.StoredFieldVisitor;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.index.TermFreqVector;
-import org.apache.lucene.index.TermPositionVector;
 import org.apache.lucene.index.TermState;
-import org.apache.lucene.index.TermVectorMapper;
 import org.apache.lucene.index.codecs.PerDocValues;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
@@ -782,80 +779,85 @@ public class MemoryIndex {
     public ReaderContext getTopReaderContext() {
       return readerInfos;
     }
-  
-    @Override
-    public Fields fields() {
 
-      sortFields();
+    private class MemoryFields extends Fields {
+      @Override
+      public FieldsEnum iterator() {
+        return new FieldsEnum() {
+          int upto = -1;
+
+          @Override
+          public String next() {
+            upto++;
+            if (upto >= sortedFields.length) {
+              return null;
+            }
+            return sortedFields[upto].getKey();
+          }
 
-      return new Fields() {
-        @Override
-        public FieldsEnum iterator() {
-          return new FieldsEnum() {
-            int upto = -1;
+          @Override
+          public Terms terms() {
+            return MemoryFields.this.terms(sortedFields[upto].getKey());
+          }
+        };
+      }
+
+      @Override
+      public Terms terms(final String field) {
+        int i = Arrays.binarySearch(sortedFields, field, termComparator);
+        if (i < 0) {
+          return null;
+        } else {
+          final Info info = getInfo(i);
+          info.sortTerms();
+
+          return new Terms() {
+            @Override 
+            public TermsEnum iterator(TermsEnum reuse) {
+              return new MemoryTermsEnum(info);
+            }
 
             @Override
-            public String next() {
-              upto++;
-              if (upto >= sortedFields.length) {
-                return null;
-              }
-              return sortedFields[upto].getKey();
+            public Comparator<BytesRef> getComparator() {
+              return BytesRef.getUTF8SortedAsUnicodeComparator();
             }
 
             @Override
-            public TermsEnum terms() {
-              return new MemoryTermsEnum(sortedFields[upto].getValue());
+            public long getUniqueTermCount() {
+              return info.sortedTerms.length;
             }
-          };
-        }
 
-        @Override
-        public Terms terms(final String field) {
-          int i = Arrays.binarySearch(sortedFields, field, termComparator);
-          if (i < 0) {
-            return null;
-          } else {
-            final Info info = getInfo(i);
-            info.sortTerms();
-
-            return new Terms() {
-              @Override 
-              public TermsEnum iterator() {
-                return new MemoryTermsEnum(info);
-              }
-
-              @Override
-              public Comparator<BytesRef> getComparator() {
-                return BytesRef.getUTF8SortedAsUnicodeComparator();
-              }
-
-              @Override
-              public long getUniqueTermCount() {
-                return info.sortedTerms.length;
-              }
-
-              @Override
-              public long getSumTotalTermFreq() {
-                return info.getSumTotalTermFreq();
-              }
-
-              @Override
-              public long getSumDocFreq() throws IOException {
-                // each term has df=1
-                return info.sortedTerms.length;
-              }
-
-              @Override
-              public int getDocCount() throws IOException {
-                return info.sortedTerms.length > 0 ? 1 : 0;
-              }
+            @Override
+            public long getSumTotalTermFreq() {
+              return info.getSumTotalTermFreq();
+            }
+
+            @Override
+            public long getSumDocFreq() throws IOException {
+              // each term has df=1
+              return info.sortedTerms.length;
+            }
+
+            @Override
+            public int getDocCount() throws IOException {
+              return info.sortedTerms.length > 0 ? 1 : 0;
+            }
               
               
-            };
-          }
+          };
         }
-      };
+      }
+
+      @Override
+      public int getUniqueFieldCount() {
+        return sortedFields.length;
+      }
+    }
+  
+    @Override
+    public Fields fields() {
+      sortFields();
+      return new MemoryFields();
     }
 
     private class MemoryTermsEnum extends TermsEnum {
@@ -1062,128 +1064,12 @@ public class MemoryIndex {
     }
     
     @Override
-    public TermFreqVector[] getTermFreqVectors(int docNumber) {
-      if (DEBUG) System.err.println("MemoryIndexReader.getTermFreqVectors");
-      TermFreqVector[] vectors = new TermFreqVector[fields.size()];
-//      if (vectors.length == 0) return null;
-      Iterator<String> iter = fields.keySet().iterator();
-      for (int i=0; i < vectors.length; i++) {
-        vectors[i] = getTermFreqVector(docNumber, iter.next());
-      }
-      return vectors;
-    }
-
-      @Override
-      public void getTermFreqVector(int docNumber, TermVectorMapper mapper) throws IOException
-      {
-          if (DEBUG) System.err.println("MemoryIndexReader.getTermFreqVectors");
-
-    //      if (vectors.length == 0) return null;
-          for (final String fieldName : fields.keySet())
-          {
-            getTermFreqVector(docNumber, fieldName, mapper);
-          }
-      }
-
-      @Override
-      public void getTermFreqVector(int docNumber, String field, TermVectorMapper mapper) throws IOException
-      {
-        if (DEBUG) System.err.println("MemoryIndexReader.getTermFreqVector");
-        final Info info = getInfo(field);
-          if (info == null){
-              return;
-          }
-          info.sortTerms();
-          mapper.setExpectations(field, info.sortedTerms.length, stride != 1, true);
-          for (int i = info.sortedTerms.length; --i >=0;){
-
-              ArrayIntList positions = info.sortedTerms[i].getValue();
-              int size = positions.size();
-              org.apache.lucene.index.TermVectorOffsetInfo[] offsets =
-                new org.apache.lucene.index.TermVectorOffsetInfo[size / stride];
-
-              for (int k=0, j=1; j < size; k++, j += stride) {
-                int start = positions.get(j);
-                int end = positions.get(j+1);
-                offsets[k] = new org.apache.lucene.index.TermVectorOffsetInfo(start, end);
-              }
-              mapper.map(info.sortedTerms[i].getKey(),
-                         numPositions(info.sortedTerms[i].getValue()),
-                         offsets, (info.sortedTerms[i].getValue()).toArray(stride));
-          }
+    public Fields getTermVectors(int docID) {
+      if (docID == 0) {
+        return fields();
+      } else {
+        return null;
       }
-
-      @Override
-      public TermFreqVector getTermFreqVector(int docNumber, final String fieldName) {
-      if (DEBUG) System.err.println("MemoryIndexReader.getTermFreqVector");
-      final Info info = getInfo(fieldName);
-      if (info == null) return null; // TODO: or return empty vector impl???
-      info.sortTerms();
-      
-      return new TermPositionVector() { 
-  
-        private final Map.Entry<BytesRef,ArrayIntList>[] sortedTerms = info.sortedTerms;
-        
-        public String getField() {
-          return fieldName;
-        }
-  
-        public int size() {
-          return sortedTerms.length;
-        }
-  
-        public BytesRef[] getTerms() {
-          BytesRef[] terms = new BytesRef[sortedTerms.length];
-          for (int i=sortedTerms.length; --i >= 0; ) {
-            terms[i] = sortedTerms[i].getKey();
-          }
-          return terms;
-        }
-  
-        public int[] getTermFrequencies() {
-          int[] freqs = new int[sortedTerms.length];
-          for (int i=sortedTerms.length; --i >= 0; ) {
-            freqs[i] = numPositions(sortedTerms[i].getValue());
-          }
-          return freqs;
-        }
-  
-        public int indexOf(BytesRef term) {
-          int i = Arrays.binarySearch(sortedTerms, term, termComparator);
-          return i >= 0 ? i : -1;
-        }
-  
-        public int[] indexesOf(BytesRef[] terms, int start, int len) {
-          int[] indexes = new int[len];
-          for (int i=0; i < len; i++) {
-            indexes[i] = indexOf(terms[start++]);
-          }
-          return indexes;
-        }
-        
-        // lucene >= 1.4.3
-        public int[] getTermPositions(int index) {
-          return sortedTerms[index].getValue().toArray(stride);
-        } 
-        
-        // lucene >= 1.9 (remove this method for lucene-1.4.3)
-        public org.apache.lucene.index.TermVectorOffsetInfo[] getOffsets(int index) {
-          if (stride == 1) return null; // no offsets stored
-          
-          ArrayIntList positions = sortedTerms[index].getValue();
-          int size = positions.size();
-          org.apache.lucene.index.TermVectorOffsetInfo[] offsets = 
-            new org.apache.lucene.index.TermVectorOffsetInfo[size / stride];
-          
-          for (int i=0, j=1; j < size; i++, j += stride) {
-            int start = positions.get(j);
-            int end = positions.get(j+1);
-            offsets[i] = new org.apache.lucene.index.TermVectorOffsetInfo(start, end);
-          }
-          return offsets;
-        }
-
-      };
     }
 
     private SimilarityProvider getSimilarityProvider() {
diff --git a/lucene/contrib/misc/src/java/org/apache/lucene/index/BalancedSegmentMergePolicy.java b/lucene/contrib/misc/src/java/org/apache/lucene/index/BalancedSegmentMergePolicy.java
index cba756a..9e64627 100644
--- a/lucene/contrib/misc/src/java/org/apache/lucene/index/BalancedSegmentMergePolicy.java
+++ b/lucene/contrib/misc/src/java/org/apache/lucene/index/BalancedSegmentMergePolicy.java
@@ -57,7 +57,7 @@ public class BalancedSegmentMergePolicy extends LogByteSizeMergePolicy {
   
   @Override
   protected long size(SegmentInfo info) throws IOException {
-    long byteSize = info.sizeInBytes(true);
+    long byteSize = info.sizeInBytes();
     float delRatio = (info.docCount <= 0 ? 0.0f : ((float)info.getDelCount() / (float)info.docCount));
     return (info.docCount <= 0 ?  byteSize : (long)((1.0f - delRatio) * byteSize));
   }
diff --git a/lucene/contrib/misc/src/java/org/apache/lucene/index/FieldNormModifier.java b/lucene/contrib/misc/src/java/org/apache/lucene/index/FieldNormModifier.java
index 133a29c..ea46caf 100644
--- a/lucene/contrib/misc/src/java/org/apache/lucene/index/FieldNormModifier.java
+++ b/lucene/contrib/misc/src/java/org/apache/lucene/index/FieldNormModifier.java
@@ -127,7 +127,7 @@ public class FieldNormModifier {
         if (fields != null) {
           Terms terms = fields.terms(field);
           if (terms != null) {
-            TermsEnum termsEnum = terms.iterator();
+            TermsEnum termsEnum = terms.iterator(null);
             DocsEnum docs = null;
             while(termsEnum.next() != null) {
               docs = termsEnum.docs(liveDocs, docs);
diff --git a/lucene/contrib/misc/src/java/org/apache/lucene/index/IndexSplitter.java b/lucene/contrib/misc/src/java/org/apache/lucene/index/IndexSplitter.java
index 7fffd7a..c29c704 100644
--- a/lucene/contrib/misc/src/java/org/apache/lucene/index/IndexSplitter.java
+++ b/lucene/contrib/misc/src/java/org/apache/lucene/index/IndexSplitter.java
@@ -105,7 +105,7 @@ public class IndexSplitter {
     DecimalFormat formatter = new DecimalFormat("###,###.###");
     for (int x = 0; x < infos.size(); x++) {
       SegmentInfo info = infos.info(x);
-      String sizeStr = formatter.format(info.sizeInBytes(true));
+      String sizeStr = formatter.format(info.sizeInBytes());
       System.out.println(info.name + " " + sizeStr);
     }
   }
diff --git a/lucene/contrib/misc/src/java/org/apache/lucene/index/TermVectorAccessor.java b/lucene/contrib/misc/src/java/org/apache/lucene/index/TermVectorAccessor.java
deleted file mode 100644
index 993a461..0000000
--- a/lucene/contrib/misc/src/java/org/apache/lucene/index/TermVectorAccessor.java
+++ /dev/null
@@ -1,182 +0,0 @@
-package org.apache.lucene.index;
-
-/*
- *  Licensed under the Apache License, Version 2.0 (the "License");
- *  you may not use this file except in compliance with the License.
- *  You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- *  Unless required by applicable law or agreed to in writing, software
- *  distributed under the License is distributed on an "AS IS" BASIS,
- *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- *  See the License for the specific language governing permissions and
- *  limitations under the License.
- *
- */
-
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-
-/**
- * Transparent access to the vector space model,
- * either via TermFreqVector or by resolving it from the inverted index.
- * <p/>
- * Resolving a term vector from a large index can be a time consuming process.
- * <p/>
- * Warning! This class is not thread safe!
- */
-public class TermVectorAccessor {
-
-  public TermVectorAccessor() {
-  }
-
-  /**
-   * Instance reused to save garbage collector some time
-   */
-  private TermVectorMapperDecorator decoratedMapper = new TermVectorMapperDecorator();
-
-
-  /**
-   * Visits the TermVectorMapper and populates it with terms available for a given document,
-   * either via a vector created at index time or by resolving them from the inverted index.
-   *
-   * @param indexReader    Index source
-   * @param documentNumber Source document to access
-   * @param fieldName      Field to resolve
-   * @param mapper         Mapper to be mapped with data
-   * @throws IOException
-   */
-  public void accept(IndexReader indexReader, int documentNumber, String fieldName, TermVectorMapper mapper) throws IOException {
-    decoratedMapper.decorated = mapper;
-    decoratedMapper.termVectorStored = false;
-
-    indexReader.getTermFreqVector(documentNumber, fieldName, decoratedMapper);
-
-    if (!decoratedMapper.termVectorStored) {
-      mapper.setDocumentNumber(documentNumber);
-      build(indexReader, fieldName, mapper, documentNumber);
-    }
-  }
-
-  /** Instance reused to save garbage collector some time */
-  private List<BytesRef> tokens;
-
-  /** Instance reused to save garbage collector some time */
-  private List<int[]> positions;
-
-  /** Instance reused to save garbage collector some time */
-  private List<Integer> frequencies;
-
-
-  /**
-   * Populates the mapper with terms available for the given field in a document
-   * by resolving the inverted index.
-   *
-   * @param indexReader
-   * @param field interned field name
-   * @param mapper
-   * @param documentNumber
-   * @throws IOException
-   */
-  private void build(IndexReader indexReader, String field, TermVectorMapper mapper, int documentNumber) throws IOException {
-
-    if (tokens == null) {
-      tokens = new ArrayList<BytesRef>(500);
-      positions = new ArrayList<int[]>(500);
-      frequencies = new ArrayList<Integer>(500);
-    } else {
-      tokens.clear();
-      frequencies.clear();
-      positions.clear();
-    }
-
-    final Bits liveDocs = MultiFields.getLiveDocs(indexReader);
-
-    Terms terms = MultiFields.getTerms(indexReader, field);
-    boolean anyTerms = false;
-    if (terms != null) {
-      TermsEnum termsEnum = terms.iterator();
-      DocsEnum docs = null;
-      DocsAndPositionsEnum postings = null;
-      while(true) {
-        BytesRef text = termsEnum.next();
-        if (text != null) {
-          anyTerms = true;
-          if (!mapper.isIgnoringPositions()) {
-            docs = postings = termsEnum.docsAndPositions(liveDocs, postings);
-          } else {
-            docs = termsEnum.docs(liveDocs, docs);
-          }
-
-          int docID = docs.advance(documentNumber);
-          if (docID == documentNumber) {
-
-            frequencies.add(Integer.valueOf(docs.freq()));
-            tokens.add(new BytesRef(text));
-
-            if (!mapper.isIgnoringPositions()) {
-              int[] positions = new int[docs.freq()];
-              for (int i = 0; i < positions.length; i++) {
-                positions[i] = postings.nextPosition();
-              }
-              this.positions.add(positions);
-            } else {
-              positions.add(null);
-            }
-          }
-        } else {
-          break;
-        }
-      }
-
-      if (anyTerms) {
-        mapper.setDocumentNumber(documentNumber);
-        mapper.setExpectations(field, tokens.size(), false, !mapper.isIgnoringPositions());
-        for (int i = 0; i < tokens.size(); i++) {
-          mapper.map(tokens.get(i), frequencies.get(i).intValue(), (TermVectorOffsetInfo[]) null, positions.get(i));
-        }
-      }
-    }
-  }
-
-
-  private static class TermVectorMapperDecorator extends TermVectorMapper {
-
-    private TermVectorMapper decorated;
-
-    @Override
-    public boolean isIgnoringPositions() {
-      return decorated.isIgnoringPositions();
-    }
-
-    @Override
-    public boolean isIgnoringOffsets() {
-      return decorated.isIgnoringOffsets();
-    }
-
-    private boolean termVectorStored = false;
-
-    @Override
-    public void setExpectations(String field, int numTerms, boolean storeOffsets, boolean storePositions) {
-      decorated.setExpectations(field, numTerms, storeOffsets, storePositions);
-      termVectorStored = true;
-    }
-
-    @Override
-    public void map(BytesRef term, int frequency, TermVectorOffsetInfo[] offsets, int[] positions) {
-      decorated.map(term, frequency, offsets, positions);
-    }
-
-    @Override
-    public void setDocumentNumber(int documentNumber) {
-      decorated.setDocumentNumber(documentNumber);
-    }
-  }
-
-}
diff --git a/lucene/contrib/misc/src/java/org/apache/lucene/index/codecs/appending/AppendingCodec.java b/lucene/contrib/misc/src/java/org/apache/lucene/index/codecs/appending/AppendingCodec.java
index ca709ab..408dbbf 100644
--- a/lucene/contrib/misc/src/java/org/apache/lucene/index/codecs/appending/AppendingCodec.java
+++ b/lucene/contrib/misc/src/java/org/apache/lucene/index/codecs/appending/AppendingCodec.java
@@ -19,11 +19,15 @@ package org.apache.lucene.index.codecs.appending;
 
 import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.index.codecs.DefaultDocValuesFormat;
+import org.apache.lucene.index.codecs.DefaultFieldInfosFormat;
 import org.apache.lucene.index.codecs.DefaultStoredFieldsFormat;
+import org.apache.lucene.index.codecs.DefaultTermVectorsFormat;
 import org.apache.lucene.index.codecs.DocValuesFormat;
+import org.apache.lucene.index.codecs.FieldInfosFormat;
 import org.apache.lucene.index.codecs.StoredFieldsFormat;
 import org.apache.lucene.index.codecs.PostingsFormat;
 import org.apache.lucene.index.codecs.SegmentInfosFormat;
+import org.apache.lucene.index.codecs.TermVectorsFormat;
 import org.apache.lucene.index.codecs.lucene40.Lucene40Codec;
 
 /**
@@ -45,6 +49,8 @@ public class AppendingCodec extends Codec {
   private final PostingsFormat postings = new AppendingPostingsFormat();
   private final SegmentInfosFormat infos = new AppendingSegmentInfosFormat();
   private final StoredFieldsFormat fields = new DefaultStoredFieldsFormat();
+  private final FieldInfosFormat fieldInfos = new DefaultFieldInfosFormat();
+  private final TermVectorsFormat vectors = new DefaultTermVectorsFormat();
   private final DocValuesFormat docValues = new DefaultDocValuesFormat();
   
   @Override
@@ -56,6 +62,11 @@ public class AppendingCodec extends Codec {
   public StoredFieldsFormat storedFieldsFormat() {
     return fields;
   }
+  
+  @Override
+  public TermVectorsFormat termVectorsFormat() {
+    return vectors;
+  }
 
   @Override
   public DocValuesFormat docValuesFormat() {
@@ -66,4 +77,9 @@ public class AppendingCodec extends Codec {
   public SegmentInfosFormat segmentInfosFormat() {
     return infos;
   }
+  
+  @Override
+  public FieldInfosFormat fieldInfosFormat() {
+    return fieldInfos;
+  }
 }
diff --git a/lucene/contrib/misc/src/java/org/apache/lucene/misc/HighFreqTerms.java b/lucene/contrib/misc/src/java/org/apache/lucene/misc/HighFreqTerms.java
index 17550ed..33a8709 100644
--- a/lucene/contrib/misc/src/java/org/apache/lucene/misc/HighFreqTerms.java
+++ b/lucene/contrib/misc/src/java/org/apache/lucene/misc/HighFreqTerms.java
@@ -117,7 +117,7 @@ public class HighFreqTerms {
       }
       Terms terms = fields.terms(field);
       if (terms != null) {
-        TermsEnum termsEnum = terms.iterator();
+        TermsEnum termsEnum = terms.iterator(null);
         tiq = new TermStatsQueue(numTerms);
         fillQueue(termsEnum, tiq, field);
       }
@@ -131,8 +131,10 @@ public class HighFreqTerms {
       while (true) {
         field = fieldsEnum.next();
         if (field != null) {
-          TermsEnum terms = fieldsEnum.terms();
-          fillQueue(terms, tiq, field);
+          Terms terms = fieldsEnum.terms();
+          if (terms != null) {
+            fillQueue(terms.iterator(null), tiq, field);
+          }
         } else {
           break;
         }
@@ -185,7 +187,7 @@ public class HighFreqTerms {
       return 0;
     }
 
-    TermsEnum termsEnum = terms.iterator();
+    TermsEnum termsEnum = terms.iterator(null);
     if (termsEnum.seekCeil(termText) != TermsEnum.SeekStatus.FOUND) {
       return 0;
     }
diff --git a/lucene/contrib/misc/src/test/org/apache/lucene/index/TestMultiPassIndexSplitter.java b/lucene/contrib/misc/src/test/org/apache/lucene/index/TestMultiPassIndexSplitter.java
index b9b20d8..141fef4 100644
--- a/lucene/contrib/misc/src/test/org/apache/lucene/index/TestMultiPassIndexSplitter.java
+++ b/lucene/contrib/misc/src/test/org/apache/lucene/index/TestMultiPassIndexSplitter.java
@@ -70,7 +70,7 @@ public class TestMultiPassIndexSplitter extends LuceneTestCase {
     assertTrue(ir.numDocs() - NUM_DOCS / 3 <= 1); // rounding error
     Document doc = ir.document(0);
     assertEquals("0", doc.get("id"));
-    TermsEnum te = MultiFields.getTerms(ir, "id").iterator();
+    TermsEnum te = MultiFields.getTerms(ir, "id").iterator(null);
     assertEquals(TermsEnum.SeekStatus.NOT_FOUND, te.seekCeil(new BytesRef("1")));
     assertNotSame("1", te.term().utf8ToString());
     ir.close();
@@ -78,7 +78,7 @@ public class TestMultiPassIndexSplitter extends LuceneTestCase {
     assertTrue(ir.numDocs() - NUM_DOCS / 3 <= 1);
     doc = ir.document(0);
     assertEquals("1", doc.get("id"));
-    te = MultiFields.getTerms(ir, "id").iterator();
+    te = MultiFields.getTerms(ir, "id").iterator(null);
     assertEquals(TermsEnum.SeekStatus.NOT_FOUND, te.seekCeil(new BytesRef("0")));
 
     assertNotSame("0", te.term().utf8ToString());
@@ -88,7 +88,7 @@ public class TestMultiPassIndexSplitter extends LuceneTestCase {
     doc = ir.document(0);
     assertEquals("2", doc.get("id"));
 
-    te = MultiFields.getTerms(ir, "id").iterator();
+    te = MultiFields.getTerms(ir, "id").iterator(null);
     assertEquals(TermsEnum.SeekStatus.NOT_FOUND, te.seekCeil(new BytesRef("1")));
     assertNotSame("1", te.term());
 
@@ -128,7 +128,7 @@ public class TestMultiPassIndexSplitter extends LuceneTestCase {
     doc = ir.document(0);
     assertEquals(start + "", doc.get("id"));
     // make sure the deleted doc is not here
-    TermsEnum te = MultiFields.getTerms(ir, "id").iterator();
+    TermsEnum te = MultiFields.getTerms(ir, "id").iterator(null);
     Term t = new Term("id", (NUM_DOCS - 1) + "");
     assertEquals(TermsEnum.SeekStatus.NOT_FOUND, te.seekCeil(new BytesRef(t.text())));
     assertNotSame(t.text(), te.term().utf8ToString());
diff --git a/lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor.java b/lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor.java
deleted file mode 100644
index e1a0a8d..0000000
--- a/lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor.java
+++ /dev/null
@@ -1,115 +0,0 @@
-package org.apache.lucene.index;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.LuceneTestCase;
-
-/*
- *  Licensed under the Apache License, Version 2.0 (the "License");
- *  you may not use this file except in compliance with the License.
- *  You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- *  Unless required by applicable law or agreed to in writing, software
- *  distributed under the License is distributed on an "AS IS" BASIS,
- *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- *  See the License for the specific language governing permissions and
- *  limitations under the License.
- *
- */
-
-public class TestTermVectorAccessor extends LuceneTestCase {
-
-  public void test() throws Exception {
-    Directory dir = newDirectory();
-    IndexWriter iw = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));
-
-    Document doc = new Document();
-    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);
-    customType.setStoreTermVectors(true);
-    customType.setStoreTermVectorPositions(true);
-    customType.setStoreTermVectorOffsets(true);
-    doc.add(newField("a", "a b a c a d a e a f a g a h a", customType));
-    doc.add(newField("b", "a b c b d b e b f b g b h b", customType));
-    doc.add(newField("c", "a c b c d c e c f c g c h c", customType));
-    iw.addDocument(doc);
-
-    doc = new Document();
-    FieldType customType2 = new FieldType(TextField.TYPE_UNSTORED);
-    customType2.setStoreTermVectors(true);
-    customType2.setStoreTermVectorPositions(true);
-    doc.add(newField("a", "a b a c a d a e a f a g a h a", customType2));
-    doc.add(newField("b", "a b c b d b e b f b g b h b", customType2));
-    doc.add(newField("c", "a c b c d c e c f c g c h c", customType2));
-    iw.addDocument(doc);
-
-    doc = new Document();
-    FieldType customType3 = new FieldType(TextField.TYPE_UNSTORED);
-    customType3.setStoreTermVectors(true);
-    doc.add(newField("a", "a b a c a d a e a f a g a h a", customType3));
-    doc.add(newField("b", "a b c b d b e b f b g b h b", customType3));
-    doc.add(newField("c", "a c b c d c e c f c g c h c", customType3));
-    iw.addDocument(doc);
-
-    doc = new Document();
-    doc.add(newField("a", "a b a c a d a e a f a g a h a", TextField.TYPE_UNSTORED));
-    doc.add(newField("b", "a b c b d b e b f b g b h b", TextField.TYPE_UNSTORED));
-    doc.add(newField("c", "a c b c d c e c f c g c h c", TextField.TYPE_UNSTORED));
-    iw.addDocument(doc);
-
-    doc = new Document();
-    doc.add(newField("a", "a b a c a d a e a f a g a h a", customType));
-    doc.add(newField("b", "a b c b d b e b f b g b h b", TextField.TYPE_UNSTORED));
-    doc.add(newField("c", "a c b c d c e c f c g c h c", customType3));
-    iw.addDocument(doc);
-
-    iw.close();
-
-    IndexReader ir = IndexReader.open(dir, false);
-
-    TermVectorAccessor accessor = new TermVectorAccessor();
-
-    ParallelArrayTermVectorMapper mapper;
-    TermFreqVector tfv;
-
-    for (int i = 0; i < ir.maxDoc(); i++) {
-
-      mapper = new ParallelArrayTermVectorMapper();
-      accessor.accept(ir, i, "a", mapper);
-      tfv = mapper.materializeVector();
-      assertEquals("doc " + i, "a", tfv.getTerms()[0].utf8ToString());
-      assertEquals("doc " + i, 8, tfv.getTermFrequencies()[0]);
-
-      mapper = new ParallelArrayTermVectorMapper();
-      accessor.accept(ir, i, "b", mapper);
-      tfv = mapper.materializeVector();
-      assertEquals("doc " + i, 8, tfv.getTermFrequencies().length);
-      assertEquals("doc " + i, "b", tfv.getTerms()[1].utf8ToString());
-      assertEquals("doc " + i, 7, tfv.getTermFrequencies()[1]);
-
-      mapper = new ParallelArrayTermVectorMapper();
-      accessor.accept(ir, i, "c", mapper);
-      tfv = mapper.materializeVector();
-      assertEquals("doc " + i, 8, tfv.getTermFrequencies().length);
-      assertEquals("doc " + i, "c", tfv.getTerms()[2].utf8ToString());
-      assertEquals("doc " + i, 7, tfv.getTermFrequencies()[2]);
-
-      mapper = new ParallelArrayTermVectorMapper();
-      accessor.accept(ir, i, "q", mapper);
-      tfv = mapper.materializeVector();
-      assertNull("doc " + i, tfv);
-
-    }
-
-    ir.close();
-
-    dir.close();
-
-
-  }
-
-}
diff --git a/lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec.java b/lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec.java
index a03bec3..3ee5b04 100644
--- a/lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec.java
+++ b/lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec.java
@@ -130,7 +130,7 @@ public class TestAppendingCodec extends LuceneTestCase {
     Fields fields = MultiFields.getFields(reader);
     Terms terms = fields.terms("f");
     assertNotNull(terms);
-    TermsEnum te = terms.iterator();
+    TermsEnum te = terms.iterator(null);
     assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("quick")));
     assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("brown")));
     assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("fox")));
diff --git a/lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/DuplicateFilter.java b/lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/DuplicateFilter.java
index 2c8a031..af85bd9 100644
--- a/lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/DuplicateFilter.java
+++ b/lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/DuplicateFilter.java
@@ -86,7 +86,7 @@ public class DuplicateFilter extends Filter {
       return bits;
     }
 
-    TermsEnum termsEnum = terms.iterator();
+    TermsEnum termsEnum = terms.iterator(null);
     DocsEnum docs = null;
     while (true) {
       BytesRef currTerm = termsEnum.next();
@@ -124,7 +124,7 @@ public class DuplicateFilter extends Filter {
       return bits;
     }
 
-    TermsEnum termsEnum = terms.iterator();
+    TermsEnum termsEnum = terms.iterator(null);
     DocsEnum docs = null;
     while (true) {
       BytesRef currTerm = termsEnum.next();
diff --git a/lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/SlowCollatedTermRangeQuery.java b/lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/SlowCollatedTermRangeQuery.java
index 0488e5e..8914bea 100644
--- a/lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/SlowCollatedTermRangeQuery.java
+++ b/lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/SlowCollatedTermRangeQuery.java
@@ -101,7 +101,7 @@ public class SlowCollatedTermRangeQuery extends MultiTermQuery {
       return TermsEnum.EMPTY;
     }
     
-    TermsEnum tenum = terms.iterator();
+    TermsEnum tenum = terms.iterator(null);
 
     if (lowerTerm == null && upperTerm == null) {
       return tenum;
diff --git a/lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/regex/RegexQuery.java b/lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/regex/RegexQuery.java
index aeb71d1..932e55b 100644
--- a/lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/regex/RegexQuery.java
+++ b/lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/regex/RegexQuery.java
@@ -69,7 +69,7 @@ public class RegexQuery extends MultiTermQuery implements RegexQueryCapable {
 
   @Override
   protected FilteredTermsEnum getTermsEnum(Terms terms, AttributeSource atts) throws IOException {
-    return new RegexTermsEnum(terms.iterator(), term, regexImpl);
+    return new RegexTermsEnum(terms.iterator(null), term, regexImpl);
   }
 
   @Override
diff --git a/lucene/src/java/org/apache/lucene/index/BufferedDeletesStream.java b/lucene/src/java/org/apache/lucene/index/BufferedDeletesStream.java
index eee3784..66e8212 100644
--- a/lucene/src/java/org/apache/lucene/index/BufferedDeletesStream.java
+++ b/lucene/src/java/org/apache/lucene/index/BufferedDeletesStream.java
@@ -373,7 +373,7 @@ class BufferedDeletesStream {
         currentField = term.field();
         Terms terms = fields.terms(currentField);
         if (terms != null) {
-          termsEnum = terms.iterator();
+          termsEnum = terms.iterator(null);
         } else {
           termsEnum = null;
         }
diff --git a/lucene/src/java/org/apache/lucene/index/CheckIndex.java b/lucene/src/java/org/apache/lucene/index/CheckIndex.java
index 6dd0909..80de460 100644
--- a/lucene/src/java/org/apache/lucene/index/CheckIndex.java
+++ b/lucene/src/java/org/apache/lucene/index/CheckIndex.java
@@ -495,7 +495,7 @@ public class CheckIndex {
         segInfoStat.hasProx = info.getHasProx();
         msg("    numFiles=" + info.files().size());
         segInfoStat.numFiles = info.files().size();
-        segInfoStat.sizeMB = info.sizeInBytes(true)/(1024.*1024.);
+        segInfoStat.sizeMB = info.sizeInBytes()/(1024.*1024.);
         msg("    size (MB)=" + nf.format(segInfoStat.sizeMB));
         Map<String,String> diagnostics = info.getDiagnostics();
         segInfoStat.diagnostics = diagnostics;
@@ -676,6 +676,7 @@ public class CheckIndex {
         infoStream.print("    test: terms, freq, prox...");
       }
 
+      int computedFieldCount = 0;
       final Fields fields = reader.fields();
       if (fields == null) {
         msg("OK [no fields/terms]");
@@ -691,9 +692,20 @@ public class CheckIndex {
         if (field == null) {
           break;
         }
+
+        // TODO: really the codec should not return a field
+        // from FieldsEnum if it has to Terms... but we do
+        // this today:
+        // assert fields.terms(field) != null;
+        computedFieldCount++;
         
-        final TermsEnum terms = fieldsEnum.terms();
-        assert terms != null;
+        final Terms terms = fieldsEnum.terms();
+        if (terms == null) {
+          continue;
+        }
+
+        final TermsEnum termsEnum = terms.iterator(null);
+
         boolean hasOrd = true;
         final long termCountStart = status.termCount;
 
@@ -706,7 +718,7 @@ public class CheckIndex {
         FixedBitSet visitedDocs = new FixedBitSet(reader.maxDoc());
         while(true) {
 
-          final BytesRef term = terms.next();
+          final BytesRef term = termsEnum.next();
           if (term == null) {
             break;
           }
@@ -722,20 +734,20 @@ public class CheckIndex {
             lastTerm.copy(term);
           }
 
-          final int docFreq = terms.docFreq();
+          final int docFreq = termsEnum.docFreq();
           if (docFreq <= 0) {
             throw new RuntimeException("docfreq: " + docFreq + " is out of bounds");
           }
           status.totFreq += docFreq;
           sumDocFreq += docFreq;
 
-          docs = terms.docs(liveDocs, docs);
-          postings = terms.docsAndPositions(liveDocs, postings);
+          docs = termsEnum.docs(liveDocs, docs);
+          postings = termsEnum.docsAndPositions(liveDocs, postings);
 
           if (hasOrd) {
             long ord = -1;
             try {
-              ord = terms.ord();
+              ord = termsEnum.ord();
             } catch (UnsupportedOperationException uoe) {
               hasOrd = false;
             }
@@ -804,12 +816,12 @@ public class CheckIndex {
             }
           }
           
-          final long totalTermFreq2 = terms.totalTermFreq();
+          final long totalTermFreq2 = termsEnum.totalTermFreq();
           final boolean hasTotalTermFreq = postings != null && totalTermFreq2 != -1;
 
           // Re-count if there are deleted docs:
           if (reader.hasDeletions()) {
-            final DocsEnum docsNoDel = terms.docs(null, docs);
+            final DocsEnum docsNoDel = termsEnum.docs(null, docs);
             docCount = 0;
             totalTermFreq = 0;
             while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
@@ -836,7 +848,7 @@ public class CheckIndex {
           if (hasPositions) {
             for(int idx=0;idx<7;idx++) {
               final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);
-              postings = terms.docsAndPositions(liveDocs, postings);
+              postings = termsEnum.docsAndPositions(liveDocs, postings);
               final int docID = postings.advance(skipDocID);
               if (docID == DocsEnum.NO_MORE_DOCS) {
                 break;
@@ -872,7 +884,7 @@ public class CheckIndex {
           } else {
             for(int idx=0;idx<7;idx++) {
               final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);
-              docs = terms.docs(liveDocs, docs);
+              docs = termsEnum.docs(liveDocs, docs);
               final int docID = docs.advance(skipDocID);
               if (docID == DocsEnum.NO_MORE_DOCS) {
                 break;
@@ -900,7 +912,8 @@ public class CheckIndex {
           // no terms, eg there used to be terms but all
           // docs got deleted and then merged away):
           // make sure TermsEnum is empty:
-          if (fieldsEnum.terms().next() != null) {
+          final Terms fieldTerms2 = fieldsEnum.terms();
+          if (fieldTerms2 != null && fieldTerms2.iterator(null).next() != null) {
             throw new RuntimeException("Fields.terms(field=" + field + ") returned null yet the field appears to have terms");
           }
         } else {
@@ -927,16 +940,16 @@ public class CheckIndex {
             }
           }
         
-        if (fieldTerms != null) {
-          final int v = fieldTerms.getDocCount();
-          if (v != -1 && visitedDocs.cardinality() != v) {
-            throw new RuntimeException("docCount for field " + field + "=" + v + " != recomputed docCount=" + visitedDocs.cardinality());
+          if (fieldTerms != null) {
+            final int v = fieldTerms.getDocCount();
+            if (v != -1 && visitedDocs.cardinality() != v) {
+              throw new RuntimeException("docCount for field " + field + "=" + v + " != recomputed docCount=" + visitedDocs.cardinality());
+            }
           }
-        }
 
           // Test seek to last term:
           if (lastTerm != null) {
-            if (terms.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { 
+            if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { 
               throw new RuntimeException("seek to last term " + lastTerm + " failed");
             }
 
@@ -963,18 +976,18 @@ public class CheckIndex {
               // Seek by ord
               for(int i=seekCount-1;i>=0;i--) {
                 long ord = i*(termCount/seekCount);
-                terms.seekExact(ord);
-                seekTerms[i] = new BytesRef(terms.term());
+                termsEnum.seekExact(ord);
+                seekTerms[i] = new BytesRef(termsEnum.term());
               }
 
               // Seek by term
               long totDocCount = 0;
               for(int i=seekCount-1;i>=0;i--) {
-                if (terms.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {
+                if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {
                   throw new RuntimeException("seek to existing term " + seekTerms[i] + " failed");
                 }
               
-                docs = terms.docs(liveDocs, docs);
+                docs = termsEnum.docs(liveDocs, docs);
                 if (docs == null) {
                   throw new RuntimeException("null DocsEnum from to existing term " + seekTerms[i]);
                 }
@@ -997,6 +1010,17 @@ public class CheckIndex {
           }
         }
       }
+      
+      int fieldCount = fields.getUniqueFieldCount();
+      
+      if (fieldCount != -1) {
+        if (fieldCount < 0) {
+          throw new RuntimeException("invalid fieldCount: " + fieldCount);
+        }
+        if (fieldCount != computedFieldCount) {
+          throw new RuntimeException("fieldCount mismatch " + fieldCount + " vs recomputed field count " + computedFieldCount);
+        }
+      }
 
       // for most implementations, this is boring (just the sum across all fields)
       // but codecs that don't work per-field like preflex actually implement this,
@@ -1137,18 +1161,136 @@ public class CheckIndex {
   private Status.TermVectorStatus testTermVectors(SegmentInfo info, SegmentReader reader, NumberFormat format) {
     final Status.TermVectorStatus status = new Status.TermVectorStatus();
     
+    TermsEnum termsEnum = null;
     try {
       if (infoStream != null) {
         infoStream.print("    test: term vectors........");
       }
 
+      // TODO: maybe we can factor out testTermIndex and reuse here?
+      DocsEnum docs = null;
+      DocsAndPositionsEnum postings = null;
       final Bits liveDocs = reader.getLiveDocs();
       for (int j = 0; j < info.docCount; ++j) {
         if (liveDocs == null || liveDocs.get(j)) {
           status.docCount++;
-          TermFreqVector[] tfv = reader.getTermFreqVectors(j);
+          Fields tfv = reader.getTermVectors(j);
           if (tfv != null) {
-            status.totVectors += tfv.length;
+            int tfvComputedFieldCount = 0;
+            long tfvComputedTermCount = 0;
+
+            FieldsEnum fieldsEnum = tfv.iterator();
+            String field = null;
+            String lastField = null;
+            while((field = fieldsEnum.next()) != null) {
+              status.totVectors++;
+              tfvComputedFieldCount++;
+
+              if (lastField == null) {
+                lastField = field;
+              } else if (lastField.compareTo(field) > 0) {
+                throw new RuntimeException("vector fields are out of order: lastField=" + lastField + " field=" + field + " doc=" + j);
+              }
+              
+              Terms terms = tfv.terms(field);
+              termsEnum = terms.iterator(termsEnum);
+              
+              long tfvComputedTermCountForField = 0;
+              long tfvComputedSumTotalTermFreq = 0;
+              
+              BytesRef term = null;
+              while ((term = termsEnum.next()) != null) {
+                tfvComputedTermCountForField++;
+                
+                if (termsEnum.docFreq() != 1) {
+                  throw new RuntimeException("vector docFreq for doc " + j + ", field " + field + ", term" + term + " != 1");
+                }
+                
+                long totalTermFreq = termsEnum.totalTermFreq();
+                
+                if (totalTermFreq != -1 && totalTermFreq <= 0) {
+                  throw new RuntimeException("totalTermFreq: " + totalTermFreq + " is out of bounds");
+                }
+                
+                DocsEnum docsEnum;
+                DocsAndPositionsEnum dp = termsEnum.docsAndPositions(null, postings);
+                if (dp == null) {
+                  DocsEnum d = termsEnum.docs(null, docs);
+                  docsEnum = docs = d;
+                } else {
+                  docsEnum = postings = dp;
+                }
+                  
+                final int doc = docsEnum.nextDoc();
+                  
+                if (doc != 0) {
+                  throw new RuntimeException("vector for doc " + j + " didn't return docID=0: got docID=" + doc);
+                }
+                  
+                final int tf = docsEnum.freq();
+                tfvComputedSumTotalTermFreq += tf;
+                
+                if (tf <= 0) {
+                  throw new RuntimeException("vector freq " + tf + " is out of bounds");
+                }
+                
+                if (totalTermFreq != -1 && totalTermFreq != tf) {
+                  throw new RuntimeException("vector totalTermFreq " + totalTermFreq + " != tf " + tf);
+                }
+                
+                if (dp != null) {
+                  int lastPosition = -1;
+                  for (int i = 0; i < tf; i++) {
+                    int pos = dp.nextPosition();
+                    if (pos != -1 && pos < 0) {
+                      throw new RuntimeException("vector position " + pos + " is out of bounds");
+                    }
+                    
+                    if (pos < lastPosition) {
+                      throw new RuntimeException("vector position " + pos + " < lastPos " + lastPosition);
+                    }
+                    
+                    lastPosition = pos;
+                  }
+                }
+                  
+                if (docsEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
+                  throw new RuntimeException("vector for doc " + j + " references multiple documents!");
+                }
+              }
+              
+              long uniqueTermCount = terms.getUniqueTermCount();
+              if (uniqueTermCount != -1 && uniqueTermCount != tfvComputedTermCountForField) {
+                throw new RuntimeException("vector term count for doc " + j + ", field " + field + " = " + uniqueTermCount + " != recomputed term count=" + tfvComputedTermCountForField);
+              }
+              
+              int docCount = terms.getDocCount();
+              if (docCount != -1 && docCount != 1) {
+                throw new RuntimeException("vector doc count for doc " + j + ", field " + field + " = " + docCount + " != 1");
+              }
+              
+              long sumDocFreq = terms.getSumDocFreq();
+              if (sumDocFreq != -1 && sumDocFreq != tfvComputedTermCountForField) {
+                throw new RuntimeException("vector postings count for doc " + j + ", field " + field + " = " + sumDocFreq + " != recomputed postings count=" + tfvComputedTermCountForField);
+              }
+              
+              long sumTotalTermFreq = terms.getSumTotalTermFreq();
+              if (sumTotalTermFreq != -1 && sumTotalTermFreq != tfvComputedSumTotalTermFreq) {
+                throw new RuntimeException("vector sumTotalTermFreq for doc " + j + ", field " + field + " = " + sumTotalTermFreq + " != recomputed sumTotalTermFreq=" + tfvComputedSumTotalTermFreq);
+              }
+              
+              tfvComputedTermCount += tfvComputedTermCountForField;
+            }
+            
+            int tfvUniqueFieldCount = tfv.getUniqueFieldCount();
+            if (tfvUniqueFieldCount != -1 && tfvUniqueFieldCount != tfvComputedFieldCount) {
+              throw new RuntimeException("vector field count for doc " + j + "=" + tfvUniqueFieldCount + " != recomputed uniqueFieldCount=" + tfvComputedFieldCount);
+            }
+            
+            long tfvUniqueTermCount = tfv.getUniqueTermCount();
+            if (tfvUniqueTermCount != -1 && tfvUniqueTermCount != tfvComputedTermCount) {
+              throw new RuntimeException("vector term count for doc " + j + "=" + tfvUniqueTermCount + " != recomputed uniqueTermCount=" + tfvComputedTermCount);
+            }
           }
         }
       }
diff --git a/lucene/src/java/org/apache/lucene/index/DirectoryReader.java b/lucene/src/java/org/apache/lucene/index/DirectoryReader.java
index ad4f8bf..df06b28 100644
--- a/lucene/src/java/org/apache/lucene/index/DirectoryReader.java
+++ b/lucene/src/java/org/apache/lucene/index/DirectoryReader.java
@@ -494,33 +494,10 @@ class DirectoryReader extends IndexReader implements Cloneable {
   }
 
   @Override
-  public TermFreqVector[] getTermFreqVectors(int n) throws IOException {
+  public Fields getTermVectors(int docID) throws IOException {
     ensureOpen();
-    int i = readerIndex(n);        // find segment num
-    return subReaders[i].getTermFreqVectors(n - starts[i]); // dispatch to segment
-  }
-
-  @Override
-  public TermFreqVector getTermFreqVector(int n, String field)
-      throws IOException {
-    ensureOpen();
-    int i = readerIndex(n);        // find segment num
-    return subReaders[i].getTermFreqVector(n - starts[i], field);
-  }
-
-
-  @Override
-  public void getTermFreqVector(int docNumber, String field, TermVectorMapper mapper) throws IOException {
-    ensureOpen();
-    int i = readerIndex(docNumber);        // find segment num
-    subReaders[i].getTermFreqVector(docNumber - starts[i], field, mapper);
-  }
-
-  @Override
-  public void getTermFreqVector(int docNumber, TermVectorMapper mapper) throws IOException {
-    ensureOpen();
-    int i = readerIndex(docNumber);        // find segment num
-    subReaders[i].getTermFreqVector(docNumber - starts[i], mapper);
+    int i = readerIndex(docID);        // find segment num
+    return subReaders[i].getTermVectors(docID - starts[i]); // dispatch to segment
   }
 
   @Override
diff --git a/lucene/src/java/org/apache/lucene/index/DocFieldProcessor.java b/lucene/src/java/org/apache/lucene/index/DocFieldProcessor.java
index 28ae82c..a45d52c 100644
--- a/lucene/src/java/org/apache/lucene/index/DocFieldProcessor.java
+++ b/lucene/src/java/org/apache/lucene/index/DocFieldProcessor.java
@@ -25,10 +25,13 @@ import java.util.HashSet;
 import java.util.Map;
 
 import org.apache.lucene.index.DocumentsWriterPerThread.DocState;
+import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.index.codecs.DocValuesFormat;
 import org.apache.lucene.index.codecs.DocValuesConsumer;
+import org.apache.lucene.index.codecs.FieldInfosWriter;
 import org.apache.lucene.index.codecs.PerDocConsumer;
 import org.apache.lucene.index.values.PerDocFieldValues;
+import org.apache.lucene.store.IOContext;
 import org.apache.lucene.util.ArrayUtil;
 import org.apache.lucene.util.IOUtils;
 
@@ -45,6 +48,7 @@ final class DocFieldProcessor extends DocConsumer {
 
   final DocFieldConsumer consumer;
   final StoredFieldsConsumer fieldsWriter;
+  final Codec codec;
 
   // Holds all fields seen in current doc
   DocFieldProcessorPerField[] fields = new DocFieldProcessorPerField[1];
@@ -61,6 +65,7 @@ final class DocFieldProcessor extends DocConsumer {
 
   public DocFieldProcessor(DocumentsWriterPerThread docWriter, DocFieldConsumer consumer) {
     this.docState = docWriter.docState;
+    this.codec = docWriter.codec;
     this.consumer = consumer;
     fieldsWriter = new StoredFieldsConsumer(docWriter);
   }
@@ -81,8 +86,8 @@ final class DocFieldProcessor extends DocConsumer {
     // consumer can alter the FieldInfo* if necessary.  EG,
     // FreqProxTermsWriter does this with
     // FieldInfo.storePayload.
-    final String fileName = IndexFileNames.segmentFileName(state.segmentName, "", IndexFileNames.FIELD_INFOS_EXTENSION);
-    state.fieldInfos.write(state.directory, fileName);
+    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();
+    infosWriter.write(state.directory, state.segmentName, state.fieldInfos, IOContext.DEFAULT);
     for (DocValuesConsumerAndDocID consumers : docValues.values()) {
       consumers.docValuesConsumer.finish(state.numDocs);
     }
diff --git a/lucene/src/java/org/apache/lucene/index/DocTermOrds.java b/lucene/src/java/org/apache/lucene/index/DocTermOrds.java
index 42d0b91..586e98f 100644
--- a/lucene/src/java/org/apache/lucene/index/DocTermOrds.java
+++ b/lucene/src/java/org/apache/lucene/index/DocTermOrds.java
@@ -200,7 +200,7 @@ public class DocTermOrds {
       //System.out.println("GET normal enum");
       final Terms terms = MultiFields.getTerms(reader, field);
       if (terms != null) {
-        return terms.iterator();
+        return terms.iterator(null);
       } else {
         return null;
       }
@@ -234,7 +234,7 @@ public class DocTermOrds {
       return;
     }
 
-    final TermsEnum te = terms.iterator();
+    final TermsEnum te = terms.iterator(null);
     final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();
     //System.out.println("seekStart=" + seekStart.utf8ToString());
     if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {
@@ -644,7 +644,7 @@ public class DocTermOrds {
     public OrdWrappedTermsEnum(IndexReader reader) throws IOException {
       this.reader = reader;
       assert indexedTermsArray != null;
-      termsEnum = MultiFields.getTerms(reader, field).iterator();
+      termsEnum = MultiFields.getTerms(reader, field).iterator(null);
     }
 
     @Override
diff --git a/lucene/src/java/org/apache/lucene/index/DocumentsWriter.java b/lucene/src/java/org/apache/lucene/index/DocumentsWriter.java
index 1729113..1ad37c0 100644
--- a/lucene/src/java/org/apache/lucene/index/DocumentsWriter.java
+++ b/lucene/src/java/org/apache/lucene/index/DocumentsWriter.java
@@ -46,7 +46,7 @@ import org.apache.lucene.util.InfoStream;
  * which in turn processes the document and interacts with
  * other consumers in the indexing chain.  Certain
  * consumers, like {@link StoredFieldsConsumer} and {@link
- * TermVectorsTermsWriter}, digest a document and
+ * TermVectorsConsumer}, digest a document and
  * immediately write bytes to the "doc store" files (ie,
  * they do not consume RAM per document, except while they
  * are processing the document).
diff --git a/lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java b/lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java
index ff84cd4..f87254e 100644
--- a/lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java
+++ b/lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java
@@ -74,7 +74,7 @@ public class DocumentsWriterPerThread {
 
     // Build up indexing chain:
 
-      final TermsHashConsumer termVectorsWriter = new TermVectorsTermsWriter(documentsWriterPerThread);
+      final TermsHashConsumer termVectorsWriter = new TermVectorsConsumer(documentsWriterPerThread);
       final TermsHashConsumer freqProxWriter = new FreqProxTermsWriter();
 
       final InvertedDocConsumer  termsHash = new TermsHash(documentsWriterPerThread, freqProxWriter, true,
@@ -414,7 +414,7 @@ public class DocumentsWriterPerThread {
   private void doAfterFlush() throws IOException {
     segment = null;
     consumer.doAfterFlush();
-    fieldInfos = new FieldInfos(fieldInfos);
+    fieldInfos = FieldInfos.from(fieldInfos);
     parent.subtractFlushedNumDocs(numDocsInRAM);
     numDocsInRAM = 0;
   }
diff --git a/lucene/src/java/org/apache/lucene/index/FieldInfo.java b/lucene/src/java/org/apache/lucene/index/FieldInfo.java
index 2a6ec56..b27c9e8 100644
--- a/lucene/src/java/org/apache/lucene/index/FieldInfo.java
+++ b/lucene/src/java/org/apache/lucene/index/FieldInfo.java
@@ -51,12 +51,15 @@ public final class FieldInfo {
     DOCS_AND_FREQS_AND_POSITIONS 
   };
 
-  FieldInfo(String na, boolean tk, int nu, boolean storeTermVector, 
+  /**
+   * @lucene.experimental
+   */
+  public FieldInfo(String name, boolean isIndexed, int number, boolean storeTermVector, 
             boolean storePositionWithTermVector,  boolean storeOffsetWithTermVector, 
             boolean omitNorms, boolean storePayloads, IndexOptions indexOptions, ValueType docValues) {
-    name = na;
-    isIndexed = tk;
-    number = nu;
+    this.name = name;
+    this.isIndexed = isIndexed;
+    this.number = number;
     this.docValues = docValues;
     if (isIndexed) {
       this.storeTermVector = storeTermVector;
diff --git a/lucene/src/java/org/apache/lucene/index/FieldInfos.java b/lucene/src/java/org/apache/lucene/index/FieldInfos.java
index 365291a..7d88c16 100644
--- a/lucene/src/java/org/apache/lucene/index/FieldInfos.java
+++ b/lucene/src/java/org/apache/lucene/index/FieldInfos.java
@@ -31,7 +31,6 @@ import java.util.Map.Entry;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.index.values.ValueType;
 import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.store.IndexOutput;
 import org.apache.lucene.util.CodecUtil;
@@ -185,57 +184,27 @@ public final class FieldInfos implements Iterable<FieldInfo> {
   private final HashMap<String,FieldInfo> byName = new HashMap<String,FieldInfo>();
   private final FieldNumberBiMap globalFieldNumbers;
   
-  // First used in 2.9; prior to 2.9 there was no format header
-  public static final int FORMAT_START = -2;
-  // First used in 3.4: omit only positional information
-  public static final int FORMAT_OMIT_POSITIONS = -3;
-  // per-field codec support, records index values for fields
-  public static final int FORMAT_FLEX = -4;
-
-  // whenever you add a new format, make it 1 smaller (negative version logic)!
-  static final int FORMAT_CURRENT = FORMAT_FLEX;
-  
-  static final int FORMAT_MINIMUM = FORMAT_START;
-  
-  static final byte IS_INDEXED = 0x1;
-  static final byte STORE_TERMVECTOR = 0x2;
-  static final byte STORE_POSITIONS_WITH_TERMVECTOR = 0x4;
-  static final byte STORE_OFFSET_WITH_TERMVECTOR = 0x8;
-  static final byte OMIT_NORMS = 0x10;
-  static final byte STORE_PAYLOADS = 0x20;
-  static final byte OMIT_TERM_FREQ_AND_POSITIONS = 0x40;
-  static final byte OMIT_POSITIONS = -128;
-
-  private int format;
   private boolean hasFreq; // only set if readonly
   private boolean hasProx; // only set if readonly
   private boolean hasVectors; // only set if readonly
   private long version; // internal use to track changes
-  
 
   /**
-   * Creates a new {@link FieldInfos} instance with a private
-   * {@link org.apache.lucene.index.FieldInfos.FieldNumberBiMap} 
-   * <p>
-   * Note: this ctor should not be used during indexing use
-   * {@link FieldInfos#FieldInfos(FieldInfos)} or
-   * {@link FieldInfos#FieldInfos(FieldNumberBiMap)}
-   * instead.
-   */
-  public FieldInfos() {
-    this(new FieldNumberBiMap());
-  }
-  
-  /**
-   * Creates a new {@link FieldInfo} instance from the given instance. If the given instance is
-   * read-only this instance will be read-only too.
+   * Creates a new read-only FieldInfos: only public to be accessible
+   * from the codecs package
    * 
-   * @see #isReadOnly()
+   * @lucene.internal
    */
-  FieldInfos(FieldInfos other) {
-    this(other.globalFieldNumbers);
+  public FieldInfos(FieldInfo[] infos, boolean hasFreq, boolean hasProx, boolean hasVectors) {
+    this(null);
+    this.hasFreq = hasFreq;
+    this.hasProx = hasProx;
+    this.hasVectors = hasVectors;
+    for (FieldInfo info : infos) {
+      putInternal(info);
+    }
   }
-  
+
   /**
    * Creates a new FieldInfos instance with the given {@link FieldNumberBiMap}. 
    * If the {@link FieldNumberBiMap} is <code>null</code> this instance will be read-only.
@@ -244,26 +213,6 @@ public final class FieldInfos implements Iterable<FieldInfo> {
   FieldInfos(FieldNumberBiMap globalFieldNumbers) {
     this.globalFieldNumbers = globalFieldNumbers;
   }
-
-  /**
-   * Construct a FieldInfos object using the directory and the name of the file
-   * IndexInput. 
-   * <p>
-   * Note: The created instance will be read-only
-   * 
-   * @param d The directory to open the IndexInput from
-   * @param name The name of the file to open the IndexInput from in the Directory
-   * @throws IOException
-   */
-  public FieldInfos(Directory d, String name) throws IOException {
-    this((FieldNumberBiMap)null); // use null here to make this FIs Read-Only
-    final IndexInput input = d.openInput(name, IOContext.READONCE);
-    try {
-      read(input, name);
-    } finally {
-      input.close();
-    }
-  }
   
   /**
    * adds the given field to this FieldInfos name / number mapping. The given FI
@@ -293,7 +242,6 @@ public final class FieldInfos implements Iterable<FieldInfo> {
   @Override
   synchronized public Object clone() {
     FieldInfos fis = new FieldInfos(globalFieldNumbers);
-    fis.format = format;
     fis.hasFreq = hasFreq;
     fis.hasProx = hasProx;
     fis.hasVectors = hasVectors;
@@ -549,19 +497,10 @@ public final class FieldInfos implements Iterable<FieldInfo> {
     return false;
   }
 
-  public void write(Directory d, String name) throws IOException {
-    IndexOutput output = d.createOutput(name, IOContext.READONCE);
-    try {
-      write(output);
-    } finally {
-      output.close();
-    }
-  }
-  
   /**
    * Returns <code>true</code> iff this instance is not backed by a
    * {@link org.apache.lucene.index.FieldInfos.FieldNumberBiMap}. Instances read from a directory via
-   * {@link FieldInfos#FieldInfos(Directory, String)} will always be read-only
+   * {@link FieldInfos#FieldInfos(FieldInfo[], boolean, boolean, boolean)} will always be read-only
    * since no {@link org.apache.lucene.index.FieldInfos.FieldNumberBiMap} is supplied, otherwise 
    * <code>false</code>.
    */
@@ -572,182 +511,6 @@ public final class FieldInfos implements Iterable<FieldInfo> {
   synchronized final long getVersion() {
     return version;
   }
-
-  public void write(IndexOutput output) throws IOException {
-    output.writeVInt(FORMAT_CURRENT);
-    output.writeVInt(size());
-    for (FieldInfo fi : this) {
-      assert fi.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS || !fi.storePayloads;
-      byte bits = 0x0;
-      if (fi.isIndexed) bits |= IS_INDEXED;
-      if (fi.storeTermVector) bits |= STORE_TERMVECTOR;
-      if (fi.storePositionWithTermVector) bits |= STORE_POSITIONS_WITH_TERMVECTOR;
-      if (fi.storeOffsetWithTermVector) bits |= STORE_OFFSET_WITH_TERMVECTOR;
-      if (fi.omitNorms) bits |= OMIT_NORMS;
-      if (fi.storePayloads) bits |= STORE_PAYLOADS;
-      if (fi.indexOptions == IndexOptions.DOCS_ONLY)
-        bits |= OMIT_TERM_FREQ_AND_POSITIONS;
-      else if (fi.indexOptions == IndexOptions.DOCS_AND_FREQS)
-        bits |= OMIT_POSITIONS;
-      output.writeString(fi.name);
-      output.writeInt(fi.number);
-      output.writeByte(bits);
-
-      final byte b;
-
-      if (fi.docValues == null) {
-        b = 0;
-      } else {
-        switch(fi.docValues) {
-        case VAR_INTS:
-          b = 1;
-          break;
-        case FLOAT_32:
-          b = 2;
-          break;
-        case FLOAT_64:
-          b = 3;
-          break;
-        case BYTES_FIXED_STRAIGHT:
-          b = 4;
-          break;
-        case BYTES_FIXED_DEREF:
-          b = 5;
-          break;
-        case BYTES_VAR_STRAIGHT:
-          b = 6;
-          break;
-        case BYTES_VAR_DEREF:
-          b = 7;
-          break;
-        case FIXED_INTS_16:
-          b = 8;
-          break;
-        case FIXED_INTS_32:
-          b = 9;
-          break;
-        case FIXED_INTS_64:
-          b = 10;
-          break;
-        case FIXED_INTS_8:
-          b = 11;
-          break;
-        case BYTES_FIXED_SORTED:
-          b = 12;
-          break;
-        case BYTES_VAR_SORTED:
-          b = 13;
-          break;
-        default:
-          throw new IllegalStateException("unhandled indexValues type " + fi.docValues);
-        }
-      }
-      output.writeByte(b);
-    }
-  }
-
-  private void read(IndexInput input, String fileName) throws IOException {
-    format = input.readVInt();
-
-    if (format > FORMAT_MINIMUM) {
-      throw new IndexFormatTooOldException(input, format, FORMAT_MINIMUM, FORMAT_CURRENT);
-    }
-    if (format < FORMAT_CURRENT) {
-      throw new IndexFormatTooNewException(input, format, FORMAT_MINIMUM, FORMAT_CURRENT);
-    }
-
-    final int size = input.readVInt(); //read in the size
-
-    for (int i = 0; i < size; i++) {
-      String name = input.readString();
-      final int fieldNumber = format <= FORMAT_FLEX? input.readInt():i;
-      byte bits = input.readByte();
-      boolean isIndexed = (bits & IS_INDEXED) != 0;
-      boolean storeTermVector = (bits & STORE_TERMVECTOR) != 0;
-      boolean storePositionsWithTermVector = (bits & STORE_POSITIONS_WITH_TERMVECTOR) != 0;
-      boolean storeOffsetWithTermVector = (bits & STORE_OFFSET_WITH_TERMVECTOR) != 0;
-      boolean omitNorms = (bits & OMIT_NORMS) != 0;
-      boolean storePayloads = (bits & STORE_PAYLOADS) != 0;
-      final IndexOptions indexOptions;
-      if ((bits & OMIT_TERM_FREQ_AND_POSITIONS) != 0) {
-        indexOptions = IndexOptions.DOCS_ONLY;
-      } else if ((bits & OMIT_POSITIONS) != 0) {
-        if (format <= FORMAT_OMIT_POSITIONS) {
-          indexOptions = IndexOptions.DOCS_AND_FREQS;
-        } else {
-          throw new CorruptIndexException("Corrupt fieldinfos, OMIT_POSITIONS set but format=" + format + " (resource: " + input + ")");
-        }
-      } else {
-        indexOptions = IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
-      }
-
-      // LUCENE-3027: past indices were able to write
-      // storePayloads=true when omitTFAP is also true,
-      // which is invalid.  We correct that, here:
-      if (indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
-        storePayloads = false;
-      }
-      hasVectors |= storeTermVector;
-      hasProx |= isIndexed && indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
-      hasFreq |= isIndexed && indexOptions != IndexOptions.DOCS_ONLY;
-      ValueType docValuesType = null;
-      if (format <= FORMAT_FLEX) {
-        final byte b = input.readByte();
-        switch(b) {
-        case 0:
-          docValuesType = null;
-          break;
-        case 1:
-          docValuesType = ValueType.VAR_INTS;
-          break;
-        case 2:
-          docValuesType = ValueType.FLOAT_32;
-          break;
-        case 3:
-          docValuesType = ValueType.FLOAT_64;
-          break;
-        case 4:
-          docValuesType = ValueType.BYTES_FIXED_STRAIGHT;
-          break;
-        case 5:
-          docValuesType = ValueType.BYTES_FIXED_DEREF;
-          break;
-        case 6:
-          docValuesType = ValueType.BYTES_VAR_STRAIGHT;
-          break;
-        case 7:
-          docValuesType = ValueType.BYTES_VAR_DEREF;
-          break;
-        case 8:
-          docValuesType = ValueType.FIXED_INTS_16;
-          break;
-        case 9:
-          docValuesType = ValueType.FIXED_INTS_32;
-          break;
-        case 10:
-          docValuesType = ValueType.FIXED_INTS_64;
-          break;
-        case 11:
-          docValuesType = ValueType.FIXED_INTS_8;
-          break;
-        case 12:
-          docValuesType = ValueType.BYTES_FIXED_SORTED;
-          break;
-        case 13:
-          docValuesType = ValueType.BYTES_VAR_SORTED;
-          break;
-        
-        default:
-          throw new IllegalStateException("unhandled indexValues type " + b);
-        }
-      }
-      addInternal(name, fieldNumber, isIndexed, storeTermVector, storePositionsWithTermVector, storeOffsetWithTermVector, omitNorms, storePayloads, indexOptions, docValuesType);
-    }
-
-    if (input.getFilePointer() != input.length()) {
-      throw new CorruptIndexException("did not read all bytes from file \"" + fileName + "\": read " + input.getFilePointer() + " vs size " + input.length() + " (resource: " + input + ")");
-    }    
-  }
   
   /**
    * Reverts all uncommitted changes 
@@ -783,4 +546,14 @@ public final class FieldInfos implements Iterable<FieldInfo> {
 
     return false;
   }
+  
+  /**
+   * Creates a new {@link FieldInfo} instance from the given instance. If the given instance is
+   * read-only this instance will be read-only too.
+   * 
+   * @see #isReadOnly()
+   */
+  static FieldInfos from(FieldInfos other) {
+    return new FieldInfos(other.globalFieldNumbers);
+  }
 }
diff --git a/lucene/src/java/org/apache/lucene/index/FieldSortedTermVectorMapper.java b/lucene/src/java/org/apache/lucene/index/FieldSortedTermVectorMapper.java
deleted file mode 100644
index cf11b9f..0000000
--- a/lucene/src/java/org/apache/lucene/index/FieldSortedTermVectorMapper.java
+++ /dev/null
@@ -1,74 +0,0 @@
-package org.apache.lucene.index;
-
-import java.util.*;
-
-import org.apache.lucene.util.BytesRef;
-
-/**
- * Copyright 2007 The Apache Software Foundation
- * <p/>
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * For each Field, store a sorted collection of {@link TermVectorEntry}s
- * <p/>
- * This is not thread-safe.
- */
-public class FieldSortedTermVectorMapper extends TermVectorMapper{
-  private Map<String,SortedSet<TermVectorEntry>> fieldToTerms = new HashMap<String,SortedSet<TermVectorEntry>>();
-  private SortedSet<TermVectorEntry> currentSet;
-  private String currentField;
-  private Comparator<TermVectorEntry> comparator;
-
-  /**
-   *
-   * @param comparator A Comparator for sorting {@link TermVectorEntry}s
-   */
-  public FieldSortedTermVectorMapper(Comparator<TermVectorEntry> comparator) {
-    this(false, false, comparator);
-  }
-
-
-  public FieldSortedTermVectorMapper(boolean ignoringPositions, boolean ignoringOffsets, Comparator<TermVectorEntry> comparator) {
-    super(ignoringPositions, ignoringOffsets);
-    this.comparator = comparator;
-  }
-
-  @Override
-  public void map(BytesRef term, int frequency, TermVectorOffsetInfo[] offsets, int[] positions) {
-    TermVectorEntry entry = new TermVectorEntry(currentField, term, frequency, offsets, positions);
-    currentSet.add(entry);
-  }
-
-  @Override
-  public void setExpectations(String field, int numTerms, boolean storeOffsets, boolean storePositions) {
-    currentSet = new TreeSet<TermVectorEntry>(comparator);
-    currentField = field;
-    fieldToTerms.put(field, currentSet);
-  }
-
-  /**
-   * Get the mapping between fields and terms, sorted by the comparator
-   *
-   * @return A map between field names and {@link java.util.SortedSet}s per field.  SortedSet entries are {@link TermVectorEntry}
-   */
-  public Map<String,SortedSet<TermVectorEntry>> getFieldToTerms() {
-    return fieldToTerms;
-  }
-
-
-  public Comparator<TermVectorEntry> getComparator() {
-    return comparator;
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/Fields.java b/lucene/src/java/org/apache/lucene/index/Fields.java
index 43050fb..81b1dbd 100644
--- a/lucene/src/java/org/apache/lucene/index/Fields.java
+++ b/lucene/src/java/org/apache/lucene/index/Fields.java
@@ -31,6 +31,12 @@ public abstract class Fields {
   /** Get the {@link Terms} for this field.  This will return
    *  null if the field does not exist. */
   public abstract Terms terms(String field) throws IOException;
+
+  /** Returns the number of terms for all fields, or -1 if this 
+   *  measure isn't stored by the codec. Note that, just like 
+   *  other term measures, this measure does not take deleted 
+   *  documents into account. */
+  public abstract int getUniqueFieldCount() throws IOException;
   
   /** Returns the number of terms for all fields, or -1 if this 
    *  measure isn't stored by the codec. Note that, just like 
diff --git a/lucene/src/java/org/apache/lucene/index/FieldsEnum.java b/lucene/src/java/org/apache/lucene/index/FieldsEnum.java
index 62317fe..a0afada 100644
--- a/lucene/src/java/org/apache/lucene/index/FieldsEnum.java
+++ b/lucene/src/java/org/apache/lucene/index/FieldsEnum.java
@@ -47,12 +47,19 @@ public abstract class FieldsEnum {
    * null when there are no more fields.*/
   public abstract String next() throws IOException;
 
-  /** Get {@link TermsEnum} for the current field.  You
-   *  should not call {@link #next} until you're done using
-   *  this {@link TermsEnum}.  After {@link #next} returns
-   *  null this method should not be called. This method
-   *  will not return null. */
-  public abstract TermsEnum terms() throws IOException;
+  // TODO: would be nice to require/fix all impls so they
+  // never return null here... we have to fix the writers to
+  // never write 0-terms fields... or maybe allow a non-null
+  // Terms instance in just this case
+
+  /** Get {@link Terms} for the current field.  After {@link #next} returns
+   *  null this method should not be called. This method may
+   *  return null in some cases, which means the provided
+   *  field does not have any terms. */
+  public abstract Terms terms() throws IOException;
+
+  // TODO: should we allow pulling Terms as well?  not just
+  // the iterator?
   
   public final static FieldsEnum[] EMPTY_ARRAY = new FieldsEnum[0];
 
@@ -65,7 +72,7 @@ public abstract class FieldsEnum {
     }
 
     @Override
-    public TermsEnum terms() {
+    public Terms terms() {
       throw new IllegalStateException("this method should never be called");
     }
   };
diff --git a/lucene/src/java/org/apache/lucene/index/FilterIndexReader.java b/lucene/src/java/org/apache/lucene/index/FilterIndexReader.java
index 09004bb..8206860 100644
--- a/lucene/src/java/org/apache/lucene/index/FilterIndexReader.java
+++ b/lucene/src/java/org/apache/lucene/index/FilterIndexReader.java
@@ -58,6 +58,11 @@ public class FilterIndexReader extends IndexReader {
     public Terms terms(String field) throws IOException {
       return in.terms(field);
     }
+
+    @Override
+    public int getUniqueFieldCount() throws IOException {
+      return in.getUniqueFieldCount();
+    }
   }
 
   /** Base class for filtering {@link Terms}
@@ -70,8 +75,8 @@ public class FilterIndexReader extends IndexReader {
     }
 
     @Override
-    public TermsEnum iterator() throws IOException {
-      return in.iterator();
+    public TermsEnum iterator(TermsEnum reuse) throws IOException {
+      return in.iterator(reuse);
     }
 
     @Override
@@ -128,7 +133,7 @@ public class FilterIndexReader extends IndexReader {
     }
 
     @Override
-    public TermsEnum terms() throws IOException {
+    public Terms terms() throws IOException {
       return in.terms();
     }
   }
@@ -316,30 +321,10 @@ public class FilterIndexReader extends IndexReader {
   }
   
   @Override
-  public TermFreqVector[] getTermFreqVectors(int docNumber)
-          throws IOException {
-    ensureOpen();
-    return in.getTermFreqVectors(docNumber);
-  }
-
-  @Override
-  public TermFreqVector getTermFreqVector(int docNumber, String field)
+  public Fields getTermVectors(int docID)
           throws IOException {
     ensureOpen();
-    return in.getTermFreqVector(docNumber, field);
-  }
-
-
-  @Override
-  public void getTermFreqVector(int docNumber, String field, TermVectorMapper mapper) throws IOException {
-    ensureOpen();
-    in.getTermFreqVector(docNumber, field, mapper);
-  }
-
-  @Override
-  public void getTermFreqVector(int docNumber, TermVectorMapper mapper) throws IOException {
-    ensureOpen();
-    in.getTermFreqVector(docNumber, mapper);
+    return in.getTermVectors(docID);
   }
 
   @Override
diff --git a/lucene/src/java/org/apache/lucene/index/IndexFileNames.java b/lucene/src/java/org/apache/lucene/index/IndexFileNames.java
index e5a75fe..d42e830 100644
--- a/lucene/src/java/org/apache/lucene/index/IndexFileNames.java
+++ b/lucene/src/java/org/apache/lucene/index/IndexFileNames.java
@@ -19,7 +19,7 @@ package org.apache.lucene.index;
 
 import java.util.regex.Pattern;
 
-import org.apache.lucene.index.codecs.PostingsFormat;  // for javadocs
+import org.apache.lucene.index.codecs.Codec;  // for javadocs
 
 // TODO: put all files under codec and remove all the static extensions here
 
@@ -33,7 +33,7 @@ import org.apache.lucene.index.codecs.PostingsFormat;  // for javadocs
  * {@link #segmentFileName(String, String, String) segmentFileName}).
  *
  * <p><b>NOTE</b>: extensions used by codecs are not
- * listed here.  You must interact with the {@link PostingsFormat}
+ * listed here.  You must interact with the {@link Codec}
  * directly.
  *
  * @lucene.internal
@@ -53,15 +53,6 @@ public final class IndexFileNames {
   /** Extension of norms file */
   public static final String NORMS_EXTENSION = "nrm";
 
-  /** Extension of vectors fields file */
-  public static final String VECTORS_FIELDS_EXTENSION = "tvf";
-
-  /** Extension of vectors documents file */
-  public static final String VECTORS_DOCUMENTS_EXTENSION = "tvd";
-
-  /** Extension of vectors index file */
-  public static final String VECTORS_INDEX_EXTENSION = "tvx";
-
   /** Extension of compound file */
   public static final String COMPOUND_FILE_EXTENSION = "cfs";
   
@@ -74,9 +65,6 @@ public final class IndexFileNames {
   /** Extension of deletes */
   public static final String DELETES_EXTENSION = "del";
 
-  /** Extension of field infos */
-  public static final String FIELD_INFOS_EXTENSION = "fnm";
-
   /** Extension of separate norms */
   public static final String SEPARATE_NORMS_EXTENSION = "s";
 
@@ -93,38 +81,16 @@ public final class IndexFileNames {
   public static final String INDEX_EXTENSIONS[] = new String[] {
     COMPOUND_FILE_EXTENSION,
     COMPOUND_FILE_ENTRIES_EXTENSION,
-    FIELD_INFOS_EXTENSION,
     DELETES_EXTENSION,
-    VECTORS_INDEX_EXTENSION,
-    VECTORS_DOCUMENTS_EXTENSION,
-    VECTORS_FIELDS_EXTENSION,
     GEN_EXTENSION,
     NORMS_EXTENSION,
     COMPOUND_FILE_STORE_EXTENSION,
     GLOBAL_FIELD_NUM_MAP_EXTENSION,
   };
 
-  public static final String[] STORE_INDEX_EXTENSIONS = new String[] {
-    VECTORS_INDEX_EXTENSION,
-    VECTORS_FIELDS_EXTENSION,
-    VECTORS_DOCUMENTS_EXTENSION,
-  };
-
   public static final String[] NON_STORE_INDEX_EXTENSIONS = new String[] {
-    FIELD_INFOS_EXTENSION,
     NORMS_EXTENSION
   };
-  
-  static final String COMPOUND_EXTENSIONS_NOT_CODEC[] = new String[] {
-    FIELD_INFOS_EXTENSION,
-  };
-  
-  /** File extensions for term vector support */
-  public static final String VECTOR_EXTENSIONS[] = new String[] {
-    VECTORS_INDEX_EXTENSION,
-    VECTORS_DOCUMENTS_EXTENSION,
-    VECTORS_FIELDS_EXTENSION
-  };
 
   /**
    * Computes the full file name from base, extension and generation. If the
@@ -157,21 +123,6 @@ public final class IndexFileNames {
   }
 
   /**
-   * Returns true if the provided filename is one of the doc store files (ends
-   * with an extension in {@link #STORE_INDEX_EXTENSIONS}).
-   */
-  // TODO: this method is stupid.
-  public static boolean isDocStoreFile(String fileName) {
-    if (fileName.endsWith(COMPOUND_FILE_STORE_EXTENSION))
-      return true;
-    for (String ext : STORE_INDEX_EXTENSIONS) {
-      if (fileName.endsWith(ext))
-        return true;
-    }
-    return false;
-  }
-
-  /**
    * Returns a file name that includes the given segment name, your own custom
    * name and extension. The format of the filename is:
    * &lt;segmentName&gt;(_&lt;name&gt;)(.&lt;ext&gt;).
diff --git a/lucene/src/java/org/apache/lucene/index/IndexReader.java b/lucene/src/java/org/apache/lucene/index/IndexReader.java
index d3bc8c6..5388ae8 100644
--- a/lucene/src/java/org/apache/lucene/index/IndexReader.java
+++ b/lucene/src/java/org/apache/lucene/index/IndexReader.java
@@ -803,58 +803,25 @@ public abstract class IndexReader implements Cloneable,Closeable {
     throw new UnsupportedOperationException("This reader does not support this method.");
   }
 
-  /**
-   * Return an array of term frequency vectors for the specified document.
-   * The array contains a vector for each vectorized field in the document.
-   * Each vector contains terms and frequencies for all terms in a given vectorized field.
-   * If no such fields existed, the method returns null. The term vectors that are
-   * returned may either be of type {@link TermFreqVector}
-   * or of type {@link TermPositionVector} if
-   * positions or offsets have been stored.
-   * 
-   * @param docNumber document for which term frequency vectors are returned
-   * @return array of term frequency vectors. May be null if no term vectors have been
-   *  stored for the specified document.
-   * @throws IOException if index cannot be accessed
-   */
-  abstract public TermFreqVector[] getTermFreqVectors(int docNumber)
-          throws IOException;
-
-
-  /**
-   * Return a term frequency vector for the specified document and field. The
-   * returned vector contains terms and frequencies for the terms in
-   * the specified field of this document, if the field had the storeTermVector
-   * flag set. If termvectors had been stored with positions or offsets, a 
-   * {@link TermPositionVector} is returned.
-   * 
-   * @param docNumber document for which the term frequency vector is returned
-   * @param field field for which the term frequency vector is returned.
-   * @return term frequency vector May be null if field does not exist in the specified
-   * document or term vector was not stored.
-   * @throws IOException if index cannot be accessed
-   */
-  abstract public TermFreqVector getTermFreqVector(int docNumber, String field)
+  /** Retrieve term vectors for this document, or null if
+   *  term vectors were not indexed.  The returned Fields
+   *  instance acts like a single-document inverted index
+   *  (the docID will be 0). */
+  abstract public Fields getTermVectors(int docID)
           throws IOException;
 
-  /**
-   * Load the Term Vector into a user-defined data structure instead of relying on the parallel arrays of
-   * the {@link TermFreqVector}.
-   * @param docNumber The number of the document to load the vector for
-   * @param field The name of the field to load
-   * @param mapper The {@link TermVectorMapper} to process the vector.  Must not be null
-   * @throws IOException if term vectors cannot be accessed or if they do not exist on the field and doc. specified.
-   * 
-   */
-  abstract public void getTermFreqVector(int docNumber, String field, TermVectorMapper mapper) throws IOException;
-
-  /**
-   * Map all the term vectors for all fields in a Document
-   * @param docNumber The number of the document to load the vector for
-   * @param mapper The {@link TermVectorMapper} to process the vector.  Must not be null
-   * @throws IOException if term vectors cannot be accessed or if they do not exist on the field and doc. specified.
-   */
-  abstract public void getTermFreqVector(int docNumber, TermVectorMapper mapper) throws IOException;
+  /** Retrieve term vector for this document and field, or
+   *  null if term vectors were not indexed.  The returned
+   *  Fields instance acts like a single-document inverted
+   *  index (the docID will be 0). */
+  public Terms getTermVector(int docID, String field)
+    throws IOException {
+    Fields vectors = getTermVectors(docID);
+    if (vectors == null) {
+      return null;
+    }
+    return vectors.terms(field);
+  }
 
   /**
    * Returns <code>true</code> if an index exists at the specified directory.
diff --git a/lucene/src/java/org/apache/lucene/index/IndexWriter.java b/lucene/src/java/org/apache/lucene/index/IndexWriter.java
index 2207fbf..9d20483 100644
--- a/lucene/src/java/org/apache/lucene/index/IndexWriter.java
+++ b/lucene/src/java/org/apache/lucene/index/IndexWriter.java
@@ -36,6 +36,7 @@ import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.index.DocumentsWriterPerThread.FlushedSegment;
 import org.apache.lucene.index.FieldInfos.FieldNumberBiMap;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
+import org.apache.lucene.index.MergeState.CheckAbort;
 import org.apache.lucene.index.PayloadProcessorProvider.DirPayloadProcessor;
 import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.search.Query;
@@ -2177,7 +2178,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit {
 
     setDiagnostics(newSegment, "flush");
     
-    IOContext context = new IOContext(new FlushInfo(newSegment.docCount, newSegment.sizeInBytes(true)));
+    IOContext context = new IOContext(new FlushInfo(newSegment.docCount, newSegment.sizeInBytes()));
 
     boolean success = false;
     try {
@@ -2409,7 +2410,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit {
                 && versionComparator.compare(info.getVersion(), "3.1") >= 0;
           }
           
-          IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(true), true, -1));
+          IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(), true, -1));
           
           if (createCFS) {
             copySegmentIntoCFS(info, newSegName, context);
@@ -2498,7 +2499,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit {
 
       // Now create the compound file if needed
       if (useCompoundFile) {
-        merger.createCompoundFile(IndexFileNames.segmentFileName(mergedName, "", IndexFileNames.COMPOUND_FILE_EXTENSION), info, context);
+        createCompoundFile(directory, IndexFileNames.segmentFileName(mergedName, "", IndexFileNames.COMPOUND_FILE_EXTENSION), MergeState.CheckAbort.NONE, info, context);
 
         // delete new non cfs files directly: they were never
         // registered with IFD
@@ -2572,10 +2573,11 @@ public class IndexWriter implements Closeable, TwoPhaseCommit {
       newDsName = segName;
     }
     
+    Set<String> codecDocStoreFiles = info.codecDocStoreFiles();
     // Copy the segment files
     for (String file: info.files()) {
       final String newFileName;
-      if (IndexFileNames.isDocStoreFile(file)) {
+      if (codecDocStoreFiles.contains(file) || file.endsWith(IndexFileNames.COMPOUND_FILE_STORE_EXTENSION)) {
         newFileName = newDsName + IndexFileNames.stripSegmentName(file);
         if (dsFilesCopied.contains(newFileName)) {
           continue;
@@ -3412,7 +3414,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit {
         final int delCount = numDeletedDocs(info);
         assert delCount <= info.docCount;
         final double delRatio = ((double) delCount)/info.docCount;
-        merge.estimatedMergeBytes += info.sizeInBytes(true) * (1.0 - delRatio);
+        merge.estimatedMergeBytes += info.sizeInBytes() * (1.0 - delRatio);
       }
     }
 
@@ -3614,7 +3616,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit {
           if (infoStream != null) {
             infoStream.message("IW", "create compound file " + compoundFileName);
           }
-          merger.createCompoundFile(compoundFileName, merge.info, new IOContext(merge.getMergeInfo()));
+          createCompoundFile(directory, compoundFileName, checkAbort, merge.info, new IOContext(merge.getMergeInfo()));
           success = true;
         } catch (IOException ioe) {
           synchronized(this) {
@@ -3663,7 +3665,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit {
       }
 
       if (infoStream != null) {
-        infoStream.message("IW", String.format("merged segment size=%.3f MB vs estimate=%.3f MB", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));
+        infoStream.message("IW", String.format("merged segment size=%.3f MB vs estimate=%.3f MB", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));
       }
 
       final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();
@@ -4060,4 +4062,32 @@ public class IndexWriter implements Closeable, TwoPhaseCommit {
     ensureOpen();
     return payloadProcessorProvider;
   }
+  
+  /**
+   * NOTE: this method creates a compound file for all files returned by
+   * info.files(). While, generally, this may include separate norms and
+   * deletion files, this SegmentInfo must not reference such files when this
+   * method is called, because they are not allowed within a compound file.
+   */
+  static final Collection<String> createCompoundFile(Directory directory, String fileName, CheckAbort checkAbort, final SegmentInfo info, IOContext context)
+          throws IOException {
+
+    // Now merge all added files
+    Collection<String> files = info.files();
+    CompoundFileDirectory cfsDir = new CompoundFileDirectory(directory, fileName, context, true);
+    try {
+      for (String file : files) {
+        assert !IndexFileNames.matchesExtension(file, IndexFileNames.DELETES_EXTENSION) 
+                  : ".del file is not allowed in .cfs: " + file;
+        assert !IndexFileNames.isSeparateNormsFile(file) 
+                  : "separate norms file (.s[0-9]+) is not allowed in .cfs: " + file;
+        directory.copy(cfsDir, file, file, context);
+        checkAbort.work(directory.fileLength(file));
+      }
+    } finally {
+      cfsDir.close();
+    }
+
+    return files;
+  }
 }
diff --git a/lucene/src/java/org/apache/lucene/index/LogMergePolicy.java b/lucene/src/java/org/apache/lucene/index/LogMergePolicy.java
index ae460ba..ae3e3b1 100644
--- a/lucene/src/java/org/apache/lucene/index/LogMergePolicy.java
+++ b/lucene/src/java/org/apache/lucene/index/LogMergePolicy.java
@@ -196,7 +196,7 @@ public abstract class LogMergePolicy extends MergePolicy {
   }
   
   protected long sizeBytes(SegmentInfo info) throws IOException {
-    long byteSize = info.sizeInBytes(true);
+    long byteSize = info.sizeInBytes();
     if (calibrateSizeByDeletes) {
       int delCount = writer.get().numDeletedDocs(info);
       double delRatio = (info.docCount <= 0 ? 0.0f : ((float)delCount / (float)info.docCount));
diff --git a/lucene/src/java/org/apache/lucene/index/MergePolicy.java b/lucene/src/java/org/apache/lucene/index/MergePolicy.java
index 531a74a..988db92 100644
--- a/lucene/src/java/org/apache/lucene/index/MergePolicy.java
+++ b/lucene/src/java/org/apache/lucene/index/MergePolicy.java
@@ -173,7 +173,7 @@ public abstract class MergePolicy implements java.io.Closeable {
     public long totalBytesSize() throws IOException {
       long total = 0;
       for (SegmentInfo info : segments) {
-        total += info.sizeInBytes(true);
+        total += info.sizeInBytes();
       }
       return total;
     }
diff --git a/lucene/src/java/org/apache/lucene/index/MultiFields.java b/lucene/src/java/org/apache/lucene/index/MultiFields.java
index e266d03..7a8c425 100644
--- a/lucene/src/java/org/apache/lucene/index/MultiFields.java
+++ b/lucene/src/java/org/apache/lucene/index/MultiFields.java
@@ -193,7 +193,8 @@ public final class MultiFields extends Fields {
     if (fieldsEnums.size() == 0) {
       return FieldsEnum.EMPTY;
     } else {
-      return new MultiFieldsEnum(fieldsEnums.toArray(FieldsEnum.EMPTY_ARRAY),
+      return new MultiFieldsEnum(this,
+                                 fieldsEnums.toArray(FieldsEnum.EMPTY_ARRAY),
                                  fieldsSlices.toArray(ReaderUtil.Slice.EMPTY_ARRAY));
     }
   }
@@ -231,5 +232,10 @@ public final class MultiFields extends Fields {
 
     return result;
   }
+
+  @Override
+  public int getUniqueFieldCount() {
+    return terms.size();
+  }
 }
 
diff --git a/lucene/src/java/org/apache/lucene/index/MultiFieldsEnum.java b/lucene/src/java/org/apache/lucene/index/MultiFieldsEnum.java
index 66a2ad8..79757a8 100644
--- a/lucene/src/java/org/apache/lucene/index/MultiFieldsEnum.java
+++ b/lucene/src/java/org/apache/lucene/index/MultiFieldsEnum.java
@@ -47,12 +47,14 @@ public final  class MultiFieldsEnum extends FieldsEnum {
   private final MultiTermsEnum terms;
   private final MultiIndexDocValues docValues;
 
+  private final Fields fields;
 
   private String currentField;
 
   /** The subs array must be newly initialized FieldsEnum
    *  (ie, {@link FieldsEnum#next} has not been called. */
-  public MultiFieldsEnum(FieldsEnum[] subs, ReaderUtil.Slice[] subSlices) throws IOException {
+  public MultiFieldsEnum(MultiFields fields, FieldsEnum[] subs, ReaderUtil.Slice[] subSlices) throws IOException {
+    this.fields = fields;
     terms = new MultiTermsEnum(subSlices);
     queue = new FieldMergeQueue(subs.length);
     docValues = new MultiIndexDocValues();
@@ -107,20 +109,9 @@ public final  class MultiFieldsEnum extends FieldsEnum {
   }
 
   @Override
-  public TermsEnum terms() throws IOException {
-    final List<MultiTermsEnum.TermsEnumIndex> termsEnums = new ArrayList<MultiTermsEnum.TermsEnumIndex>();
-    for(int i=0;i<numTop;i++) {
-      final TermsEnum terms = top[i].fields.terms();
-      if (terms != null) {
-        termsEnums.add(new MultiTermsEnum.TermsEnumIndex(terms, top[i].index));
-      }
-    }
-
-    if (termsEnums.size() == 0) {
-      return TermsEnum.EMPTY;
-    } else {
-      return terms.reset(termsEnums.toArray(MultiTermsEnum.TermsEnumIndex.EMPTY_ARRAY));
-    }
+  public Terms terms() throws IOException {
+    // Ask our parent MultiFields:
+    return fields.terms(currentField);
   }
 
   public final static class FieldsEnumWithSlice {
diff --git a/lucene/src/java/org/apache/lucene/index/MultiReader.java b/lucene/src/java/org/apache/lucene/index/MultiReader.java
index 785889a..59416dc 100644
--- a/lucene/src/java/org/apache/lucene/index/MultiReader.java
+++ b/lucene/src/java/org/apache/lucene/index/MultiReader.java
@@ -204,33 +204,10 @@ public class MultiReader extends IndexReader implements Cloneable {
   }
 
   @Override
-  public TermFreqVector[] getTermFreqVectors(int n) throws IOException {
+  public Fields getTermVectors(int docID) throws IOException {
     ensureOpen();
-    int i = readerIndex(n);        // find segment num
-    return subReaders[i].getTermFreqVectors(n - starts[i]); // dispatch to segment
-  }
-
-  @Override
-  public TermFreqVector getTermFreqVector(int n, String field)
-      throws IOException {
-    ensureOpen();
-    int i = readerIndex(n);        // find segment num
-    return subReaders[i].getTermFreqVector(n - starts[i], field);
-  }
-
-
-  @Override
-  public void getTermFreqVector(int docNumber, String field, TermVectorMapper mapper) throws IOException {
-    ensureOpen();
-    int i = readerIndex(docNumber);        // find segment num
-    subReaders[i].getTermFreqVector(docNumber - starts[i], field, mapper);
-  }
-
-  @Override
-  public void getTermFreqVector(int docNumber, TermVectorMapper mapper) throws IOException {
-    ensureOpen();
-    int i = readerIndex(docNumber);        // find segment num
-    subReaders[i].getTermFreqVector(docNumber - starts[i], mapper);
+    int i = readerIndex(docID);        // find segment num
+    return subReaders[i].getTermVectors(docID - starts[i]); // dispatch to segment
   }
 
   @Override
diff --git a/lucene/src/java/org/apache/lucene/index/MultiTerms.java b/lucene/src/java/org/apache/lucene/index/MultiTerms.java
index cabfc25..14c1f5d 100644
--- a/lucene/src/java/org/apache/lucene/index/MultiTerms.java
+++ b/lucene/src/java/org/apache/lucene/index/MultiTerms.java
@@ -78,11 +78,11 @@ public final class MultiTerms extends Terms {
   }
 
   @Override
-  public TermsEnum iterator() throws IOException {
+  public TermsEnum iterator(TermsEnum reuse) throws IOException {
 
     final List<MultiTermsEnum.TermsEnumIndex> termsEnums = new ArrayList<MultiTermsEnum.TermsEnumIndex>();
     for(int i=0;i<subs.length;i++) {
-      final TermsEnum termsEnum = subs[i].iterator();
+      final TermsEnum termsEnum = subs[i].iterator(null);
       if (termsEnum != null) {
         termsEnums.add(new MultiTermsEnum.TermsEnumIndex(termsEnum, i));
       }
diff --git a/lucene/src/java/org/apache/lucene/index/ParallelReader.java b/lucene/src/java/org/apache/lucene/index/ParallelReader.java
index 01f84d2..e094482 100644
--- a/lucene/src/java/org/apache/lucene/index/ParallelReader.java
+++ b/lucene/src/java/org/apache/lucene/index/ParallelReader.java
@@ -131,9 +131,9 @@ public class ParallelReader extends IndexReader {
     for (final String field : fields) {               // update fieldToReader map
       if (fieldToReader.get(field) == null) {
         fieldToReader.put(field, reader);
+        this.fields.addField(field, MultiFields.getFields(reader).terms(field));
+        this.perDocs.addField(field, reader);
       }
-      this.fields.addField(field, reader);
-      this.perDocs.addField(field, reader);
     }
 
     if (!ignoreStoredFields)
@@ -151,10 +151,11 @@ public class ParallelReader extends IndexReader {
 
   private class ParallelFieldsEnum extends FieldsEnum {
     String currentField;
-    IndexReader currentReader;
     Iterator<String> keys;
+    private final Fields fields;
 
-    ParallelFieldsEnum() {
+    ParallelFieldsEnum(Fields fields) {
+      this.fields = fields;
       keys = fieldToReader.keySet().iterator();
     }
 
@@ -162,23 +163,15 @@ public class ParallelReader extends IndexReader {
     public String next() throws IOException {
       if (keys.hasNext()) {
         currentField = keys.next();
-        currentReader = fieldToReader.get(currentField);
       } else {
         currentField = null;
-        currentReader = null;
       }
       return currentField;
     }
 
     @Override
-    public TermsEnum terms() throws IOException {
-      assert currentReader != null;
-      Terms terms = MultiFields.getTerms(currentReader, currentField);
-      if (terms != null) {
-        return terms.iterator();
-      } else {
-        return TermsEnum.EMPTY;
-      }
+    public Terms terms() throws IOException {
+      return fields.terms(currentField);
     }
 
   }
@@ -187,19 +180,24 @@ public class ParallelReader extends IndexReader {
   private class ParallelFields extends Fields {
     final HashMap<String,Terms> fields = new HashMap<String,Terms>();
 
-    public void addField(String field, IndexReader r) throws IOException {
-      Fields multiFields = MultiFields.getFields(r);
-      fields.put(field, multiFields.terms(field));
+    public void addField(String fieldName, Terms terms) throws IOException {
+      fields.put(fieldName, terms);
     }
 
     @Override
     public FieldsEnum iterator() throws IOException {
-      return new ParallelFieldsEnum();
+      return new ParallelFieldsEnum(this);
     }
+
     @Override
     public Terms terms(String field) throws IOException {
       return fields.get(field);
     }
+
+    @Override
+    public int getUniqueFieldCount() throws IOException {
+      return fields.size();
+    }
   }
   
   @Override
@@ -362,49 +360,18 @@ public class ParallelReader extends IndexReader {
 
   // get all vectors
   @Override
-  public TermFreqVector[] getTermFreqVectors(int n) throws IOException {
-    ensureOpen();
-    ArrayList<TermFreqVector> results = new ArrayList<TermFreqVector>();
-    for (final Map.Entry<String,IndexReader> e: fieldToReader.entrySet()) {
-
-      String field = e.getKey();
-      IndexReader reader = e.getValue();
-      TermFreqVector vector = reader.getTermFreqVector(n, field);
-      if (vector != null)
-        results.add(vector);
-    }
-    return results.toArray(new TermFreqVector[results.size()]);
-  }
-
-  @Override
-  public TermFreqVector getTermFreqVector(int n, String field)
-    throws IOException {
-    ensureOpen();
-    IndexReader reader = fieldToReader.get(field);
-    return reader==null ? null : reader.getTermFreqVector(n, field);
-  }
-
-
-  @Override
-  public void getTermFreqVector(int docNumber, String field, TermVectorMapper mapper) throws IOException {
-    ensureOpen();
-    IndexReader reader = fieldToReader.get(field);
-    if (reader != null) {
-      reader.getTermFreqVector(docNumber, field, mapper); 
-    }
-  }
-
-  @Override
-  public void getTermFreqVector(int docNumber, TermVectorMapper mapper) throws IOException {
+  public Fields getTermVectors(int docID) throws IOException {
     ensureOpen();
-
-    for (final Map.Entry<String,IndexReader> e : fieldToReader.entrySet()) {
-
-      String field = e.getKey();
-      IndexReader reader = e.getValue();
-      reader.getTermFreqVector(docNumber, field, mapper);
+    ParallelFields fields = new ParallelFields();
+    for (Map.Entry<String,IndexReader> ent : fieldToReader.entrySet()) {
+      String fieldName = ent.getKey();
+      Terms vector = ent.getValue().getTermVector(docID, fieldName);
+      if (vector != null) {
+        fields.addField(fieldName, vector);
+      }
     }
 
+    return fields;
   }
 
   @Override
diff --git a/lucene/src/java/org/apache/lucene/index/PositionBasedTermVectorMapper.java b/lucene/src/java/org/apache/lucene/index/PositionBasedTermVectorMapper.java
deleted file mode 100644
index 3dd0a03..0000000
--- a/lucene/src/java/org/apache/lucene/index/PositionBasedTermVectorMapper.java
+++ /dev/null
@@ -1,172 +0,0 @@
-package org.apache.lucene.index;
-/**
- * Copyright 2007 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.lucene.util.BytesRef;
-
-/**
- * For each Field, store position by position information.  It ignores frequency information
- * <p/>
- * This is not thread-safe.
- */
-public class PositionBasedTermVectorMapper extends TermVectorMapper{
-  private Map<String, Map<Integer,TVPositionInfo>> fieldToTerms;
-
-  private String currentField;
-  /**
-   * A Map of Integer and TVPositionInfo
-   */
-  private Map<Integer,TVPositionInfo> currentPositions;
-  private boolean storeOffsets;
-
-  
-
-
-  /**
-   *
-   *
-   */
-  public PositionBasedTermVectorMapper() {
-    super(false, false);
-  }
-
-  public PositionBasedTermVectorMapper(boolean ignoringOffsets)
-  {
-    super(false, ignoringOffsets);
-  }
-
-  /**
-   * Never ignores positions.  This mapper doesn't make much sense unless there are positions
-   * @return false
-   */
-  @Override
-  public boolean isIgnoringPositions() {
-    return false;
-  }
-
-  /**
-   * Callback for the TermVectorReader. 
-   * @param term
-   * @param frequency
-   * @param offsets
-   * @param positions
-   */
-  @Override
-  public void map(BytesRef term, int frequency, TermVectorOffsetInfo[] offsets, int[] positions) {
-    for (int i = 0; i < positions.length; i++) {
-      Integer posVal = Integer.valueOf(positions[i]);
-      TVPositionInfo pos = currentPositions.get(posVal);
-      if (pos == null) {
-        pos = new TVPositionInfo(positions[i], storeOffsets);
-        currentPositions.put(posVal, pos);
-      }
-      pos.addTerm(term, offsets != null ? offsets[i] : null);
-    }
-  }
-
-  /**
-   * Callback mechanism used by the TermVectorReader
-   * @param field  The field being read
-   * @param numTerms The number of terms in the vector
-   * @param storeOffsets Whether offsets are available
-   * @param storePositions Whether positions are available
-   */
-  @Override
-  public void setExpectations(String field, int numTerms, boolean storeOffsets, boolean storePositions) {
-    if (storePositions == false)
-    {
-      throw new RuntimeException("You must store positions in order to use this Mapper");
-    }
-    if (storeOffsets == true)
-    {
-      //ignoring offsets
-    }
-    fieldToTerms = new HashMap<String,Map<Integer,TVPositionInfo>>(numTerms);
-    this.storeOffsets = storeOffsets;
-    currentField = field;
-    currentPositions = new HashMap<Integer,TVPositionInfo>();
-    fieldToTerms.put(currentField, currentPositions);
-  }
-
-  /**
-   * Get the mapping between fields and terms, sorted by the comparator
-   *
-   * @return A map between field names and a Map.  The sub-Map key is the position as the integer, the value is {@link org.apache.lucene.index.PositionBasedTermVectorMapper.TVPositionInfo}.
-   */
-  public Map<String,Map<Integer,TVPositionInfo>>  getFieldToTerms() {
-    return fieldToTerms;
-  }
-
-  /**
-   * Container for a term at a position
-   */
-  public static class TVPositionInfo{
-    private int position;
-
-    private List<BytesRef> terms;
-
-    private List<TermVectorOffsetInfo> offsets;
-
-
-    public TVPositionInfo(int position, boolean storeOffsets) {
-      this.position = position;
-      terms = new ArrayList<BytesRef>();
-      if (storeOffsets) {
-        offsets = new ArrayList<TermVectorOffsetInfo>();
-      }
-    }
-
-    void addTerm(BytesRef term, TermVectorOffsetInfo info)
-    {
-      terms.add(term);
-      if (offsets != null) {
-        offsets.add(info);
-      }
-    }
-
-    /**
-     *
-     * @return The position of the term
-     */
-    public int getPosition() {
-      return position;
-    }
-
-    /**
-     * Note, there may be multiple terms at the same position
-     * @return A List of BytesRefs
-     */
-    public List<BytesRef> getTerms() {
-      return terms;
-    }
-
-    /**
-     * Parallel list (to {@link #getTerms()}) of TermVectorOffsetInfo objects.  There may be multiple entries since there may be multiple terms at a position
-     * @return A List of TermVectorOffsetInfo objects, if offsets are stored.
-     */
-    public List<TermVectorOffsetInfo> getOffsets() {
-      return offsets;
-    }
-  }
-
-
-}
diff --git a/lucene/src/java/org/apache/lucene/index/SegmentCoreReaders.java b/lucene/src/java/org/apache/lucene/index/SegmentCoreReaders.java
index d4f5d83..825ec3e 100644
--- a/lucene/src/java/org/apache/lucene/index/SegmentCoreReaders.java
+++ b/lucene/src/java/org/apache/lucene/index/SegmentCoreReaders.java
@@ -25,6 +25,7 @@ import org.apache.lucene.index.codecs.PostingsFormat;
 import org.apache.lucene.index.codecs.FieldsProducer;
 import org.apache.lucene.index.codecs.StoredFieldsReader;
 import org.apache.lucene.index.codecs.PerDocValues;
+import org.apache.lucene.index.codecs.TermVectorsReader;
 import org.apache.lucene.store.CompoundFileDirectory;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
@@ -165,11 +166,10 @@ final class SegmentCoreReaders {
         assert storeDir != null;
       }
       
-      final String storesSegment = si.getDocStoreSegment();
       fieldsReaderOrig = si.getCodec().storedFieldsFormat().fieldsReader(storeDir, si, fieldInfos, context);
  
       if (si.getHasVectors()) { // open term vector files only as needed
-        termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, context, si.getDocStoreOffset(), si.docCount);
+        termVectorsReaderOrig = si.getCodec().termVectorsFormat().vectorsReader(storeDir, si, fieldInfos, context);
       }
     }
   }
diff --git a/lucene/src/java/org/apache/lucene/index/SegmentInfo.java b/lucene/src/java/org/apache/lucene/index/SegmentInfo.java
index a15ccaa..b808e44 100644
--- a/lucene/src/java/org/apache/lucene/index/SegmentInfo.java
+++ b/lucene/src/java/org/apache/lucene/index/SegmentInfo.java
@@ -28,12 +28,10 @@ import java.util.Map.Entry;
 import java.util.Set;
 
 import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.DefaultSegmentInfosWriter;
+import org.apache.lucene.index.codecs.FieldInfosReader;
 import org.apache.lucene.store.CompoundFileDirectory;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
 import org.apache.lucene.util.Constants;
 import org.apache.lucene.util.StringHelper;
 
@@ -45,9 +43,11 @@ import org.apache.lucene.util.StringHelper;
  */
 public final class SegmentInfo implements Cloneable {
   // TODO: remove with hasVector and hasProx
-  private static final int CHECK_FIELDINFO = -2;
-  static final int NO = -1;          // e.g. no norms; no deletes;
-  static final int YES = 1;          // e.g. have norms; have deletes;
+  public static final int CHECK_FIELDINFO = -2;
+  
+  // TODO: remove these from this class, for now this is the representation
+  public static final int NO = -1;          // e.g. no norms; no deletes;
+  public static final int YES = 1;          // e.g. have norms; have deletes;
   static final int WITHOUT_GEN = 0;  // a file name that has no GEN in it.
 
   public String name;				  // unique name in dir
@@ -167,100 +167,30 @@ public final class SegmentInfo implements Cloneable {
   }
 
   /**
-   * Construct a new SegmentInfo instance by reading a
-   * previously saved SegmentInfo from input.
+   * Construct a new complete SegmentInfo instance from input.
    * <p>Note: this is public only to allow access from
    * the codecs package.</p>
-   *
-   * @param dir directory to load from
-   * @param format format of the segments info file
-   * @param input input handle to read segment info from
    */
-  public SegmentInfo(Directory dir, int format, IndexInput input) throws IOException {
+  public SegmentInfo(Directory dir, String version, String name, int docCount, long delGen, int docStoreOffset,
+      String docStoreSegment, boolean docStoreIsCompoundFile, Map<Integer,Long> normGen, boolean isCompoundFile,
+      int delCount, int hasProx, Codec codec, Map<String,String> diagnostics, int hasVectors) {
     this.dir = dir;
-    if (format <= DefaultSegmentInfosWriter.FORMAT_3_1) {
-      version = input.readString();
-    }
-    name = input.readString();
-    docCount = input.readInt();
-    delGen = input.readLong();
-    docStoreOffset = input.readInt();
-    if (docStoreOffset != -1) {
-      docStoreSegment = input.readString();
-      docStoreIsCompoundFile = input.readByte() == YES;
-    } else {
-      docStoreSegment = name;
-      docStoreIsCompoundFile = false;
-    }
-
-    if (format > DefaultSegmentInfosWriter.FORMAT_4_0) {
-      // pre-4.0 indexes write a byte if there is a single norms file
-      byte b = input.readByte();
-      assert 1 == b;
-    }
-
-    int numNormGen = input.readInt();
-    if (numNormGen == NO) {
-      normGen = null;
-    } else {
-      normGen = new HashMap<Integer, Long>();
-      for(int j=0;j<numNormGen;j++) {
-        int fieldNumber = j;
-        if (format <= DefaultSegmentInfosWriter.FORMAT_4_0) {
-          fieldNumber = input.readInt();
-        }
-
-        normGen.put(fieldNumber, input.readLong());
-      }
-    }
-    isCompoundFile = input.readByte() == YES;
-
-    delCount = input.readInt();
-    assert delCount <= docCount;
-
-    hasProx = input.readByte();
-
-    
-    // System.out.println(Thread.currentThread().getName() + ": si.read hasProx=" + hasProx + " seg=" + name);
-    // note: if the codec is not available: Codec.forName will throw an exception.
-    if (format <= DefaultSegmentInfosWriter.FORMAT_4_0) {
-      codec = Codec.forName(input.readString());
-    } else {
-      codec = Codec.forName("Lucene3x");
-    }
-    diagnostics = input.readStringStringMap();
-
-    if (format <= DefaultSegmentInfosWriter.FORMAT_HAS_VECTORS) {
-      hasVectors = input.readByte();
-    } else {
-      final String storesSegment;
-      final String ext;
-      final boolean isCompoundFile;
-      if (docStoreOffset != -1) {
-        storesSegment = docStoreSegment;
-        isCompoundFile = docStoreIsCompoundFile;
-        ext = IndexFileNames.COMPOUND_FILE_STORE_EXTENSION;
-      } else {
-        storesSegment = name;
-        isCompoundFile = getUseCompoundFile();
-        ext = IndexFileNames.COMPOUND_FILE_EXTENSION;
-      }
-      final Directory dirToTest;
-      if (isCompoundFile) {
-        dirToTest = new CompoundFileDirectory(dir, IndexFileNames.segmentFileName(storesSegment, "", ext), IOContext.READONCE, false);
-      } else {
-        dirToTest = dir;
-      }
-      try {
-        hasVectors = dirToTest.fileExists(IndexFileNames.segmentFileName(storesSegment, "", IndexFileNames.VECTORS_INDEX_EXTENSION)) ? YES : NO;
-      } finally {
-        if (isCompoundFile) {
-          dirToTest.close();
-        }
-      }
-    }
+    this.version = version;
+    this.name = name;
+    this.docCount = docCount;
+    this.delGen = delGen;
+    this.docStoreOffset = docStoreOffset;
+    this.docStoreSegment = docStoreSegment;
+    this.docStoreIsCompoundFile = docStoreIsCompoundFile;
+    this.normGen = normGen;
+    this.isCompoundFile = isCompoundFile;
+    this.delCount = delCount;
+    this.hasProx = hasProx;
+    this.codec = codec;
+    this.diagnostics = diagnostics;
+    this.hasVectors = hasVectors;
   }
-  
+
   synchronized void loadFieldInfos(Directory dir, boolean checkCompoundFile) throws IOException {
     if (fieldInfos == null) {
       Directory dir0 = dir;
@@ -269,8 +199,8 @@ public final class SegmentInfo implements Cloneable {
             "", IndexFileNames.COMPOUND_FILE_EXTENSION), IOContext.READONCE, false);
       }
       try {
-        fieldInfos = new FieldInfos(dir0, IndexFileNames.segmentFileName(name,
-            "", IndexFileNames.FIELD_INFOS_EXTENSION));
+        FieldInfosReader reader = codec.fieldInfosFormat().getFieldInfosReader();
+        fieldInfos = reader.read(dir0, name, IOContext.READONCE);
       } finally {
         if (dir != dir0) {
           dir0.close();
@@ -280,11 +210,23 @@ public final class SegmentInfo implements Cloneable {
   }
 
   /**
+   * Returns total size in bytes of all of files used by this segment
+   */
+  public long sizeInBytes() throws IOException {
+    return sizeInBytes(true);
+  }
+  
+  /**
    * Returns total size in bytes of all of files used by this segment (if
    * {@code includeDocStores} is true), or the size of all files except the
    * store files otherwise.
+   * <p>
+   * NOTE: includeDocStores=false should only be used for debugging.
+   * Theoretically a codec could combine its files however it wants (after-
+   * the-fact or something), and this calculation is not particularly
+   * efficient.
    */
-  public long sizeInBytes(boolean includeDocStores) throws IOException {
+  long sizeInBytes(boolean includeDocStores) throws IOException {
     // TODO: based on how this is used, can't we just forget about all this docstore crap?
     // its really an abstraction violation into the codec
     if (includeDocStores) {
@@ -317,11 +259,17 @@ public final class SegmentInfo implements Cloneable {
     }
   }
   
-  // TODO: a little messy, but sizeInBytes above that uses this is the real problem.
-  private boolean isDocStoreFile(String fileName) throws IOException {
+  Set<String> codecDocStoreFiles() throws IOException {
     Set<String> docStoreFiles = new HashSet<String>();
     codec.storedFieldsFormat().files(dir, this, docStoreFiles);
-    return IndexFileNames.isDocStoreFile(fileName) || docStoreFiles.contains(fileName);
+    codec.termVectorsFormat().files(dir, this, docStoreFiles);
+    return docStoreFiles;
+  }
+
+  // TODO: a little messy, but sizeInBytes above that uses this is the real problem.
+  private boolean isDocStoreFile(String fileName) throws IOException {
+    Set<String> docStoreFiles = codecDocStoreFiles();
+    return fileName.endsWith(IndexFileNames.COMPOUND_FILE_STORE_EXTENSION) || docStoreFiles.contains(fileName);
   }
 
   public boolean getHasVectors() throws IOException {
@@ -551,41 +499,6 @@ public final class SegmentInfo implements Cloneable {
     this.docStoreSegment = docStoreSegment;
   }
 
-  /** Save this segment's info. */
-  public void write(IndexOutput output)
-    throws IOException {
-    assert delCount <= docCount: "delCount=" + delCount + " docCount=" + docCount + " segment=" + name;
-    // Write the Lucene version that created this segment, since 3.1
-    output.writeString(version);
-    output.writeString(name);
-    output.writeInt(docCount);
-    output.writeLong(delGen);
-
-    output.writeInt(docStoreOffset);
-    if (docStoreOffset != -1) {
-      output.writeString(docStoreSegment);
-      output.writeByte((byte) (docStoreIsCompoundFile ? 1:0));
-    }
-
-
-    if (normGen == null) {
-      output.writeInt(NO);
-    } else {
-      output.writeInt(normGen.size());
-      for (Entry<Integer,Long> entry : normGen.entrySet()) {
-        output.writeInt(entry.getKey());
-        output.writeLong(entry.getValue());
-      }
-    }
-
-    output.writeByte((byte) (isCompoundFile ? YES : NO));
-    output.writeInt(delCount);
-    output.writeByte((byte) (hasProx));
-    output.writeString(codec.getName());
-    output.writeStringStringMap(diagnostics);
-    output.writeByte((byte) (hasVectors));
-  }
-
   public boolean getHasProx() throws IOException {
     return hasProx == CHECK_FIELDINFO ? getFieldInfos().hasProx() : hasProx == YES;
   }
@@ -599,7 +512,7 @@ public final class SegmentInfo implements Cloneable {
     this.codec = codec;
   }
 
-  Codec getCodec() {
+  public Codec getCodec() {
     return codec;
   }
 
@@ -646,18 +559,6 @@ public final class SegmentInfo implements Cloneable {
       // TODO: push this out into preflex fieldsFormat?
       if (docStoreIsCompoundFile) {
         fileSet.add(IndexFileNames.segmentFileName(docStoreSegment, "", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION));
-      } else {
-        if (getHasVectors()) {
-          fileSet.add(IndexFileNames.segmentFileName(docStoreSegment, "", IndexFileNames.VECTORS_INDEX_EXTENSION));
-          fileSet.add(IndexFileNames.segmentFileName(docStoreSegment, "", IndexFileNames.VECTORS_DOCUMENTS_EXTENSION));
-          fileSet.add(IndexFileNames.segmentFileName(docStoreSegment, "", IndexFileNames.VECTORS_FIELDS_EXTENSION));
-        }
-      }
-    } else if (!useCompoundFile) {
-      if (getHasVectors()) {
-        fileSet.add(IndexFileNames.segmentFileName(name, "", IndexFileNames.VECTORS_INDEX_EXTENSION));
-        fileSet.add(IndexFileNames.segmentFileName(name, "", IndexFileNames.VECTORS_DOCUMENTS_EXTENSION));
-        fileSet.add(IndexFileNames.segmentFileName(name, "", IndexFileNames.VECTORS_FIELDS_EXTENSION));
       }
     }
 
@@ -795,4 +696,27 @@ public final class SegmentInfo implements Cloneable {
   void setBufferedDeletesGen(long v) {
     bufferedDeletesGen = v;
   }
+  
+  /** @lucene.internal */
+  public long getDelGen() {
+    return delGen;
+  }
+  
+  /** @lucene.internal */
+  public Map<Integer,Long> getNormGen() {
+    return normGen;
+  }
+  
+  // TODO: clean up this SI/FI stuff here
+  /** returns the 'real' value for hasProx (doesn't consult fieldinfos) 
+   * @lucene.internal */
+  public int getHasProxInternal() {
+    return hasProx;
+  }
+  
+  /** returns the 'real' value for hasVectors (doesn't consult fieldinfos) 
+   * @lucene.internal */
+  public int getHasVectorsInternal() {
+    return hasVectors;
+  }
 }
diff --git a/lucene/src/java/org/apache/lucene/index/SegmentMerger.java b/lucene/src/java/org/apache/lucene/index/SegmentMerger.java
index fe30b01..25f7a55 100644
--- a/lucene/src/java/org/apache/lucene/index/SegmentMerger.java
+++ b/lucene/src/java/org/apache/lucene/index/SegmentMerger.java
@@ -25,12 +25,12 @@ import java.util.List;
 
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.index.IndexReader.FieldOption;
-import org.apache.lucene.index.MergePolicy.MergeAbortedException;
 import org.apache.lucene.index.codecs.Codec;
+import org.apache.lucene.index.codecs.FieldInfosWriter;
 import org.apache.lucene.index.codecs.FieldsConsumer;
 import org.apache.lucene.index.codecs.StoredFieldsWriter;
 import org.apache.lucene.index.codecs.PerDocConsumer;
-import org.apache.lucene.store.CompoundFileDirectory;
+import org.apache.lucene.index.codecs.TermVectorsWriter;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexOutput;
@@ -52,10 +52,6 @@ final class SegmentMerger {
   private final String segment;
   private final int termIndexInterval;
 
-  /** Maximum number of contiguous documents to bulk-copy
-      when merging term vectors */
-  private final static int MAX_RAW_MERGE_DOCS = 4192;
-
   private final Codec codec;
   
   private final IOContext context;
@@ -128,41 +124,15 @@ final class SegmentMerger {
     mergeNorms();
 
     if (mergeState.fieldInfos.hasVectors()) {
-      mergeVectors(segmentWriteState);
+      int numMerged = mergeVectors();
+      assert numMerged == mergeState.mergedDocCount;
     }
     // write FIS once merge is done. IDV might change types or drops fields
-    mergeState.fieldInfos.write(directory, segment + "." + IndexFileNames.FIELD_INFOS_EXTENSION);
+    FieldInfosWriter fieldInfosWriter = codec.fieldInfosFormat().getFieldInfosWriter();
+    fieldInfosWriter.write(directory, segment, mergeState.fieldInfos, context);
     return mergeState;
   }
 
-  /**
-   * NOTE: this method creates a compound file for all files returned by
-   * info.files(). While, generally, this may include separate norms and
-   * deletion files, this SegmentInfo must not reference such files when this
-   * method is called, because they are not allowed within a compound file.
-   */
-  final Collection<String> createCompoundFile(String fileName, final SegmentInfo info, IOContext context)
-          throws IOException {
-
-    // Now merge all added files
-    Collection<String> files = info.files();
-    CompoundFileDirectory cfsDir = new CompoundFileDirectory(directory, fileName, context, true);
-    try {
-      for (String file : files) {
-        assert !IndexFileNames.matchesExtension(file, IndexFileNames.DELETES_EXTENSION) 
-                  : ".del file is not allowed in .cfs: " + file;
-        assert !IndexFileNames.isSeparateNormsFile(file) 
-                  : "separate norms file (.s[0-9]+) is not allowed in .cfs: " + file;
-        directory.copy(cfsDir, file, file, context);
-        mergeState.checkAbort.work(directory.fileLength(file));
-      }
-    } finally {
-      cfsDir.close();
-    }
-
-    return files;
-  }
-  
   private static void addIndexed(IndexReader reader, FieldInfos fInfos,
       Collection<String> names, boolean storeTermVectors,
       boolean storePositionWithTermVector, boolean storeOffsetWithTermVector,
@@ -244,138 +214,27 @@ final class SegmentMerger {
    * @throws IOException if there is a low-level IO error
    */
   private int mergeFields() throws CorruptIndexException, IOException {
-    int docCount = 0;
-
     final StoredFieldsWriter fieldsWriter = codec.storedFieldsFormat().fieldsWriter(directory, segment, context);
+    
     try {
-      docCount = fieldsWriter.merge(mergeState);
+      return fieldsWriter.merge(mergeState);
     } finally {
       fieldsWriter.close();
     }
-
-    return docCount;
   }
 
   /**
    * Merge the TermVectors from each of the segments into the new one.
    * @throws IOException
    */
-  private final void mergeVectors(SegmentWriteState segmentWriteState) throws IOException {
-    TermVectorsWriter termVectorsWriter = new TermVectorsWriter(directory, segment, mergeState.fieldInfos, context);
-    // Used for bulk-reading raw bytes for term vectors
-    int rawDocLengths[] = new int[MAX_RAW_MERGE_DOCS];
-    int rawDocLengths2[] = new int[MAX_RAW_MERGE_DOCS];
+  private final int mergeVectors() throws IOException {
+    final TermVectorsWriter termVectorsWriter = codec.termVectorsFormat().vectorsWriter(directory, segment, context);
+    
     try {
-      int idx = 0;
-      for (final MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {
-        final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];
-        TermVectorsReader matchingVectorsReader = null;
-        if (matchingSegmentReader != null) {
-          TermVectorsReader vectorsReader = matchingSegmentReader.getTermVectorsReader();
-
-          // If the TV* files are an older format then they cannot read raw docs:
-          if (vectorsReader != null && vectorsReader.canReadRawDocs()) {
-            matchingVectorsReader = vectorsReader;
-          }
-        }
-        if (reader.liveDocs != null) {
-          copyVectorsWithDeletions(termVectorsWriter, matchingVectorsReader, reader, rawDocLengths, rawDocLengths2);
-        } else {
-          copyVectorsNoDeletions(termVectorsWriter, matchingVectorsReader, reader, rawDocLengths, rawDocLengths2);
-        }
-      }
+      return termVectorsWriter.merge(mergeState);
     } finally {
       termVectorsWriter.close();
     }
-
-    final String fileName = IndexFileNames.segmentFileName(segment, "", IndexFileNames.VECTORS_INDEX_EXTENSION);
-    final long tvxSize = directory.fileLength(fileName);
-    final int mergedDocs = segmentWriteState.numDocs;
-
-    if (4+((long) mergedDocs)*16 != tvxSize)
-      // This is most likely a bug in Sun JRE 1.6.0_04/_05;
-      // we detect that the bug has struck, here, and
-      // throw an exception to prevent the corruption from
-      // entering the index.  See LUCENE-1282 for
-      // details.
-      throw new RuntimeException("mergeVectors produced an invalid result: mergedDocs is " + mergedDocs + " but tvx size is " + tvxSize + " file=" + fileName + " file exists?=" + directory.fileExists(fileName) + "; now aborting this merge to prevent index corruption");
-  }
-
-  private void copyVectorsWithDeletions(final TermVectorsWriter termVectorsWriter,
-                                        final TermVectorsReader matchingVectorsReader,
-                                        final MergeState.IndexReaderAndLiveDocs reader,
-                                        int rawDocLengths[],
-                                        int rawDocLengths2[])
-    throws IOException, MergeAbortedException {
-    final int maxDoc = reader.reader.maxDoc();
-    final Bits liveDocs = reader.liveDocs;
-    if (matchingVectorsReader != null) {
-      // We can bulk-copy because the fieldInfos are "congruent"
-      for (int docNum = 0; docNum < maxDoc;) {
-        if (!liveDocs.get(docNum)) {
-          // skip deleted docs
-          ++docNum;
-          continue;
-        }
-        // We can optimize this case (doing a bulk byte copy) since the field
-        // numbers are identical
-        int start = docNum, numDocs = 0;
-        do {
-          docNum++;
-          numDocs++;
-          if (docNum >= maxDoc) break;
-          if (!liveDocs.get(docNum)) {
-            docNum++;
-            break;
-          }
-        } while(numDocs < MAX_RAW_MERGE_DOCS);
-
-        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, start, numDocs);
-        termVectorsWriter.addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, numDocs);
-        mergeState.checkAbort.work(300 * numDocs);
-      }
-    } else {
-      for (int docNum = 0; docNum < maxDoc; docNum++) {
-        if (!liveDocs.get(docNum)) {
-          // skip deleted docs
-          continue;
-        }
-
-        // NOTE: it's very important to first assign to vectors then pass it to
-        // termVectorsWriter.addAllDocVectors; see LUCENE-1282
-        TermFreqVector[] vectors = reader.reader.getTermFreqVectors(docNum);
-        termVectorsWriter.addAllDocVectors(vectors);
-        mergeState.checkAbort.work(300);
-      }
-    }
-  }
-
-  private void copyVectorsNoDeletions(final TermVectorsWriter termVectorsWriter,
-                                      final TermVectorsReader matchingVectorsReader,
-                                      final MergeState.IndexReaderAndLiveDocs reader,
-                                      int rawDocLengths[],
-                                      int rawDocLengths2[])
-      throws IOException, MergeAbortedException {
-    final int maxDoc = reader.reader.maxDoc();
-    if (matchingVectorsReader != null) {
-      // We can bulk-copy because the fieldInfos are "congruent"
-      int docCount = 0;
-      while (docCount < maxDoc) {
-        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);
-        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, docCount, len);
-        termVectorsWriter.addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, len);
-        docCount += len;
-        mergeState.checkAbort.work(300 * len);
-      }
-    } else {
-      for (int docNum = 0; docNum < maxDoc; docNum++) {
-        // NOTE: it's very important to first assign to vectors then pass it to
-        // termVectorsWriter.addAllDocVectors; see LUCENE-1282
-        TermFreqVector[] vectors = reader.reader.getTermFreqVectors(docNum);
-        termVectorsWriter.addAllDocVectors(vectors);
-        mergeState.checkAbort.work(300);
-      }
-    }
   }
 
   private final void mergeTerms(SegmentWriteState segmentWriteState) throws CorruptIndexException, IOException {
diff --git a/lucene/src/java/org/apache/lucene/index/SegmentReader.java b/lucene/src/java/org/apache/lucene/index/SegmentReader.java
index 6a3a45b..7718a3d 100644
--- a/lucene/src/java/org/apache/lucene/index/SegmentReader.java
+++ b/lucene/src/java/org/apache/lucene/index/SegmentReader.java
@@ -32,6 +32,7 @@ import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.index.codecs.StoredFieldsReader;
 import org.apache.lucene.index.codecs.PerDocValues;
+import org.apache.lucene.index.codecs.TermVectorsReader;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.util.BitVector;
 import org.apache.lucene.util.Bits;
@@ -659,19 +660,16 @@ public class SegmentReader extends IndexReader implements Cloneable {
   /**
    * Create a clone from the initial TermVectorsReader and store it in the ThreadLocal.
    * @return TermVectorsReader
+   * @lucene.internal
    */
-  TermVectorsReader getTermVectorsReader() {
+  public TermVectorsReader getTermVectorsReader() {
     TermVectorsReader tvReader = termVectorsLocal.get();
     if (tvReader == null) {
       TermVectorsReader orig = core.getTermVectorsReaderOrig();
       if (orig == null) {
         return null;
       } else {
-        try {
-          tvReader = (TermVectorsReader) orig.clone();
-        } catch (CloneNotSupportedException cnse) {
-          return null;
-        }
+        tvReader = orig.clone();
       }
       termVectorsLocal.set(tvReader);
     }
@@ -689,67 +687,15 @@ public class SegmentReader extends IndexReader implements Cloneable {
    * @throws IOException
    */
   @Override
-  public TermFreqVector getTermFreqVector(int docNumber, String field) throws IOException {
-    // Check if this field is invalid or has no stored term vector
+  public Fields getTermVectors(int docID) throws IOException {
     ensureOpen();
-    FieldInfo fi = core.fieldInfos.fieldInfo(field);
-    if (fi == null || !fi.storeTermVector) 
-      return null;
-    
-    TermVectorsReader termVectorsReader = getTermVectorsReader();
-    if (termVectorsReader == null)
-      return null;
-    
-    return termVectorsReader.get(docNumber, field);
-  }
-
-
-  @Override
-  public void getTermFreqVector(int docNumber, String field, TermVectorMapper mapper) throws IOException {
-    ensureOpen();
-    FieldInfo fi = core.fieldInfos.fieldInfo(field);
-    if (fi == null || !fi.storeTermVector)
-      return;
-
     TermVectorsReader termVectorsReader = getTermVectorsReader();
     if (termVectorsReader == null) {
-      return;
+      return null;
     }
-
-
-    termVectorsReader.get(docNumber, field, mapper);
+    return termVectorsReader.get(docID);
   }
 
-
-  @Override
-  public void getTermFreqVector(int docNumber, TermVectorMapper mapper) throws IOException {
-    ensureOpen();
-
-    TermVectorsReader termVectorsReader = getTermVectorsReader();
-    if (termVectorsReader == null)
-      return;
-
-    termVectorsReader.get(docNumber, mapper);
-  }
-
-  /** Return an array of term frequency vectors for the specified document.
-   *  The array contains a vector for each vectorized field in the document.
-   *  Each vector vector contains term numbers and frequencies for all terms
-   *  in a given vectorized field.
-   *  If no such fields existed, the method returns null.
-   * @throws IOException
-   */
-  @Override
-  public TermFreqVector[] getTermFreqVectors(int docNumber) throws IOException {
-    ensureOpen();
-    
-    TermVectorsReader termVectorsReader = getTermVectorsReader();
-    if (termVectorsReader == null)
-      return null;
-    
-    return termVectorsReader.get(docNumber);
-  }
-  
   /** {@inheritDoc} */
   @Override
   public String toString() {
diff --git a/lucene/src/java/org/apache/lucene/index/SegmentTermPositionVector.java b/lucene/src/java/org/apache/lucene/index/SegmentTermPositionVector.java
deleted file mode 100644
index bba528d..0000000
--- a/lucene/src/java/org/apache/lucene/index/SegmentTermPositionVector.java
+++ /dev/null
@@ -1,67 +0,0 @@
-package org.apache.lucene.index;
-
-import org.apache.lucene.util.BytesRef;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-class SegmentTermPositionVector extends SegmentTermVector implements TermPositionVector {
-  protected int[][] positions;
-  protected TermVectorOffsetInfo[][] offsets;
-  public static final int[] EMPTY_TERM_POS = new int[0];
-  
-  public SegmentTermPositionVector(String field, BytesRef terms[], int termFreqs[], int[][] positions, TermVectorOffsetInfo[][] offsets) {
-    super(field, terms, termFreqs);
-    this.offsets = offsets;
-    this.positions = positions;
-  }
-
-  /**
-   * Returns an array of TermVectorOffsetInfo in which the term is found.
-   *
-   * @param index The position in the array to get the offsets from
-   * @return An array of TermVectorOffsetInfo objects or the empty list
-   * @see org.apache.lucene.analysis.Token
-   */
-  public TermVectorOffsetInfo[] getOffsets(int index) {
-    TermVectorOffsetInfo[] result = TermVectorOffsetInfo.EMPTY_OFFSET_INFO;
-    if(offsets == null)
-      return null;
-    if (index >=0 && index < offsets.length)
-    {
-      result = offsets[index];
-    }
-    return result;
-  }
-  
-  /**
-   * Returns an array of positions in which the term is found.
-   * Terms are identified by the index at which its number appears in the
-   * term String array obtained from the <code>indexOf</code> method.
-   */
-  public int[] getTermPositions(int index) {
-    int[] result = EMPTY_TERM_POS;
-    if(positions == null)
-      return null;
-    if (index >=0 && index < positions.length)
-    {
-      result = positions[index];
-    }
-    
-    return result;
-  }
-}
\ No newline at end of file
diff --git a/lucene/src/java/org/apache/lucene/index/SegmentTermVector.java b/lucene/src/java/org/apache/lucene/index/SegmentTermVector.java
deleted file mode 100644
index 145608e..0000000
--- a/lucene/src/java/org/apache/lucene/index/SegmentTermVector.java
+++ /dev/null
@@ -1,92 +0,0 @@
-package org.apache.lucene.index;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.*;
-
-import org.apache.lucene.util.BytesRef;
-
-
-class SegmentTermVector implements TermFreqVector {
-  private String field;
-  private BytesRef terms[];
-  private int termFreqs[];
-  
-  SegmentTermVector(String field, BytesRef terms[], int termFreqs[]) {
-    this.field = field;
-    this.terms = terms;
-    this.termFreqs = termFreqs;
-  }
-
-  /**
-   * 
-   * @return The number of the field this vector is associated with
-   */
-  public String getField() {
-    return field;
-  }
-
-  @Override
-  public String toString() {
-    StringBuilder sb = new StringBuilder();
-    sb.append('{');
-    sb.append(field).append(": ");
-    if(terms != null){
-      for (int i=0; i<terms.length; i++) {
-        if (i>0) sb.append(", ");
-        sb.append(terms[i]).append('/').append(termFreqs[i]);
-      }
-    }
-    sb.append('}');
-    
-    return sb.toString();
-  }
-
-  public int size() {
-    return terms == null ? 0 : terms.length;
-  }
-
-  public BytesRef [] getTerms() {
-    return terms;
-  }
-
-  public int[] getTermFrequencies() {
-    return termFreqs;
-  }
-
-  public int indexOf(BytesRef termBytes) {
-    if(terms == null)
-      return -1;
-    int res = Arrays.binarySearch(terms, termBytes);
-    return res >= 0 ? res : -1;
-  }
-
-  public int[] indexesOf(BytesRef [] termNumbers, int start, int len) {
-    // TODO: there must be a more efficient way of doing this.
-    //       At least, we could advance the lower bound of the terms array
-    //       as we find valid indexes. Also, it might be possible to leverage
-    //       this even more by starting in the middle of the termNumbers array
-    //       and thus dividing the terms array maybe in half with each found index.
-    int res[] = new int[len];
-
-    for (int i=0; i < len; i++) {
-      res[i] = indexOf(termNumbers[start+ i]);
-    }
-    return res;
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/SortedTermVectorMapper.java b/lucene/src/java/org/apache/lucene/index/SortedTermVectorMapper.java
deleted file mode 100644
index b5ea136..0000000
--- a/lucene/src/java/org/apache/lucene/index/SortedTermVectorMapper.java
+++ /dev/null
@@ -1,133 +0,0 @@
-package org.apache.lucene.index;
-/**
- * Copyright 2007 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.*;
-
-import org.apache.lucene.util.BytesRef;
-
-/**
- * Store a sorted collection of {@link org.apache.lucene.index.TermVectorEntry}s.  Collects all term information
- * into a single, SortedSet.
- * <br/>
- * NOTE: This Mapper ignores all Field information for the Document.  This means that if you are using offset/positions you will not
- * know what Fields they correlate with.
- *  <br/>
- * This is not thread-safe  
- */
-public class SortedTermVectorMapper extends TermVectorMapper{
-
-
-  private SortedSet<TermVectorEntry> currentSet;
-  private Map<BytesRef,TermVectorEntry> termToTVE = new HashMap<BytesRef,TermVectorEntry>();
-  private boolean storeOffsets;
-  private boolean storePositions;
-  /**
-   * Stand-in name for the field in {@link TermVectorEntry}.
-   */
-  public static final String ALL = "_ALL_";
-
-  /**
-   *
-   * @param comparator A Comparator for sorting {@link TermVectorEntry}s
-   */
-  public SortedTermVectorMapper(Comparator<TermVectorEntry> comparator) {
-    this(false, false, comparator);
-  }
-
-
-  public SortedTermVectorMapper(boolean ignoringPositions, boolean ignoringOffsets, Comparator<TermVectorEntry> comparator) {
-    super(ignoringPositions, ignoringOffsets);
-    currentSet = new TreeSet<TermVectorEntry>(comparator);
-  }
-
-  /**
-   *
-   * @param term The term to map
-   * @param frequency The frequency of the term
-   * @param offsets Offset information, may be null
-   * @param positions Position information, may be null
-   */
-  //We need to combine any previous mentions of the term
-  @Override
-  public void map(BytesRef term, int frequency, TermVectorOffsetInfo[] offsets, int[] positions) {
-    TermVectorEntry entry =  termToTVE.get(term);
-    if (entry == null) {
-      entry = new TermVectorEntry(ALL, term, frequency, 
-              storeOffsets == true ? offsets : null,
-              storePositions == true ? positions : null);
-      termToTVE.put(term, entry);
-      currentSet.add(entry);
-    } else {
-      entry.setFrequency(entry.getFrequency() + frequency);
-      if (storeOffsets)
-      {
-        TermVectorOffsetInfo [] existingOffsets = entry.getOffsets();
-        //A few diff. cases here:  offsets is null, existing offsets is null, both are null, same for positions
-        if (existingOffsets != null && offsets != null && offsets.length > 0)
-        {
-          //copy over the existing offsets
-          TermVectorOffsetInfo [] newOffsets = new TermVectorOffsetInfo[existingOffsets.length + offsets.length];
-          System.arraycopy(existingOffsets, 0, newOffsets, 0, existingOffsets.length);
-          System.arraycopy(offsets, 0, newOffsets, existingOffsets.length, offsets.length);
-          entry.setOffsets(newOffsets);
-        }
-        else if (existingOffsets == null && offsets != null && offsets.length > 0)
-        {
-          entry.setOffsets(offsets);
-        }
-        //else leave it alone
-      }
-      if (storePositions)
-      {
-        int [] existingPositions = entry.getPositions();
-        if (existingPositions != null && positions != null && positions.length > 0)
-        {
-          int [] newPositions = new int[existingPositions.length + positions.length];
-          System.arraycopy(existingPositions, 0, newPositions, 0, existingPositions.length);
-          System.arraycopy(positions, 0, newPositions, existingPositions.length, positions.length);
-          entry.setPositions(newPositions);
-        }
-        else if (existingPositions == null && positions != null && positions.length > 0)
-        {
-          entry.setPositions(positions);
-        }
-      }
-    }
-
-
-  }
-
-  @Override
-  public void setExpectations(String field, int numTerms, boolean storeOffsets, boolean storePositions) {
-
-    this.storeOffsets = storeOffsets;
-    this.storePositions = storePositions;
-  }
-
-  /**
-   * The TermVectorEntrySet.  A SortedSet of {@link TermVectorEntry} objects.  Sort is by the comparator passed into the constructor.
-   *<br/>
-   * This set will be empty until after the mapping process takes place.
-   *
-   * @return The SortedSet of {@link TermVectorEntry}.
-   */
-  public SortedSet<TermVectorEntry> getTermVectorEntrySet()
-  {
-    return currentSet;
-  }
-
-}
diff --git a/lucene/src/java/org/apache/lucene/index/StoredFieldsConsumer.java b/lucene/src/java/org/apache/lucene/index/StoredFieldsConsumer.java
index 6138a40..874020c 100644
--- a/lucene/src/java/org/apache/lucene/index/StoredFieldsConsumer.java
+++ b/lucene/src/java/org/apache/lucene/index/StoredFieldsConsumer.java
@@ -45,12 +45,12 @@ final class StoredFieldsConsumer {
 
   private int numStoredFields;
   private IndexableField[] storedFields;
-  private int[] fieldNumbers;
+  private FieldInfo[] fieldInfos;
 
   public void reset() {
     numStoredFields = 0;
     storedFields = new IndexableField[1];
-    fieldNumbers = new int[1];
+    fieldInfos = new FieldInfo[1];
   }
 
   public void startDocument() {
@@ -116,7 +116,7 @@ final class StoredFieldsConsumer {
     if (fieldsWriter != null && numStoredFields > 0) {
       fieldsWriter.startDocument(numStoredFields);
       for (int i = 0; i < numStoredFields; i++) {
-        fieldsWriter.writeField(fieldNumbers[i], storedFields[i]);
+        fieldsWriter.writeField(fieldInfos[i], storedFields[i]);
       }
       lastDocID++;
     }
@@ -131,14 +131,14 @@ final class StoredFieldsConsumer {
       IndexableField[] newArray = new IndexableField[newSize];
       System.arraycopy(storedFields, 0, newArray, 0, numStoredFields);
       storedFields = newArray;
-    }
-
-    if (numStoredFields == fieldNumbers.length) {
-      fieldNumbers = ArrayUtil.grow(fieldNumbers);
+      
+      FieldInfo[] newInfoArray = new FieldInfo[newSize];
+      System.arraycopy(fieldInfos, 0, newInfoArray, 0, numStoredFields);
+      fieldInfos = newInfoArray;
     }
 
     storedFields[numStoredFields] = field;
-    fieldNumbers[numStoredFields] = fieldInfo.number;
+    fieldInfos[numStoredFields] = fieldInfo;
     numStoredFields++;
 
     assert docState.testPoint("StoredFieldsWriterPerThread.processFields.writeField");
diff --git a/lucene/src/java/org/apache/lucene/index/TermFreqVector.java b/lucene/src/java/org/apache/lucene/index/TermFreqVector.java
deleted file mode 100644
index 23f01ac..0000000
--- a/lucene/src/java/org/apache/lucene/index/TermFreqVector.java
+++ /dev/null
@@ -1,73 +0,0 @@
-package org.apache.lucene.index;
-
-import org.apache.lucene.util.BytesRef;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** Provides access to stored term vector of 
- *  a document field.  The vector consists of the name of the field, an array of the terms that occur in the field of the
- * {@link org.apache.lucene.document.Document} and a parallel array of frequencies.  Thus, getTermFrequencies()[5] corresponds with the
- * frequency of getTerms()[5], assuming there are at least 5 terms in the Document.
- */
-public interface TermFreqVector {
-  /**
-   * The {@link org.apache.lucene.index.IndexableField} name. 
-   * @return The name of the field this vector is associated with.
-   * 
-   */ 
-  public String getField();
-  
-  /** 
-   * @return The number of terms in the term vector.
-   */
-  public int size();
-
-  /** 
-   * @return An Array of term texts in ascending order.
-   */
-  public BytesRef[] getTerms();
-
-
-  /** Array of term frequencies. Locations of the array correspond one to one
-   *  to the terms in the array obtained from <code>getTerms</code>
-   *  method. Each location in the array contains the number of times this
-   *  term occurs in the document or the document field.
-   */
-  public int[] getTermFrequencies();
-  
-
-  /** Return an index in the term numbers array returned from
-   *  <code>getTerms</code> at which the term with the specified
-   *  <code>term</code> appears. If this term does not appear in the array,
-   *  return -1.
-   */
-  public int indexOf(BytesRef term);
-
-
-  /** Just like <code>indexOf(int)</code> but searches for a number of terms
-   *  at the same time. Returns an array that has the same size as the number
-   *  of terms searched for, each slot containing the result of searching for
-   *  that term number.
-   *
-   *  @param terms array containing terms to look for
-   *  @param start index in the array where the list of terms starts
-   *  @param len the number of terms in the list
-   */
-  public int[] indexesOf(BytesRef[] terms, int start, int len);
-
-}
diff --git a/lucene/src/java/org/apache/lucene/index/TermPositionVector.java b/lucene/src/java/org/apache/lucene/index/TermPositionVector.java
deleted file mode 100644
index fa6b88e..0000000
--- a/lucene/src/java/org/apache/lucene/index/TermPositionVector.java
+++ /dev/null
@@ -1,43 +0,0 @@
-package org.apache.lucene.index;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** Extends <code>TermFreqVector</code> to provide additional information about
- *  positions in which each of the terms is found. A TermPositionVector not necessarily
- * contains both positions and offsets, but at least one of these arrays exists.
- */
-public interface TermPositionVector extends TermFreqVector {
-  
-    /** Returns an array of positions in which the term is found.
-     *  Terms are identified by the index at which its number appears in the
-     *  term String array obtained from the <code>indexOf</code> method.
-     *  May return null if positions have not been stored.
-     */
-    public int[] getTermPositions(int index);
-  
-    /**
-     * Returns an array of TermVectorOffsetInfo in which the term is found.
-     * May return null if offsets have not been stored.
-     * 
-     * @see org.apache.lucene.analysis.Token
-     * 
-     * @param index The position in the array to get the offsets from
-     * @return An array of TermVectorOffsetInfo objects or the empty list
-     */ 
-    public TermVectorOffsetInfo [] getOffsets(int index);
-}
\ No newline at end of file
diff --git a/lucene/src/java/org/apache/lucene/index/TermVectorEntry.java b/lucene/src/java/org/apache/lucene/index/TermVectorEntry.java
deleted file mode 100644
index 85c7340..0000000
--- a/lucene/src/java/org/apache/lucene/index/TermVectorEntry.java
+++ /dev/null
@@ -1,103 +0,0 @@
-package org.apache.lucene.index;
-
-import org.apache.lucene.util.BytesRef;
-
-/**
- * Copyright 2007 The Apache Software Foundation
- * <p/>
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Convenience class for holding TermVector information.
- */
-public class TermVectorEntry {
-  private String field;
-  private BytesRef term;
-  private int frequency;
-  private TermVectorOffsetInfo [] offsets;
-  int [] positions;
-
-
-  public TermVectorEntry() {
-  }
-
-  public TermVectorEntry(String field, BytesRef term, int frequency, TermVectorOffsetInfo[] offsets, int[] positions) {
-    this.field = field;
-    this.term = term;
-    this.frequency = frequency;
-    this.offsets = offsets;
-    this.positions = positions;
-  }
-
-
-  public String getField() {
-    return field;
-  }
-
-  public int getFrequency() {
-    return frequency;
-  }
-
-  public TermVectorOffsetInfo[] getOffsets() {
-    return offsets;
-  }
-
-  public int[] getPositions() {
-    return positions;
-  }
-
-  public BytesRef getTerm() {
-    return term;
-  }
-
-  //Keep package local
-  void setFrequency(int frequency) {
-    this.frequency = frequency;
-  }
-
-  void setOffsets(TermVectorOffsetInfo[] offsets) {
-    this.offsets = offsets;
-  }
-
-  void setPositions(int[] positions) {
-    this.positions = positions;
-  }
-
-
-  @Override
-  public boolean equals(Object o) {
-    if (this == o) return true;
-    if (o == null || getClass() != o.getClass()) return false;
-
-    TermVectorEntry that = (TermVectorEntry) o;
-
-    if (term != null ? !term.equals(that.term) : that.term != null) return false;
-
-    return true;
-  }
-
-  @Override
-  public int hashCode() {
-    return (term != null ? term.hashCode() : 0);
-  }
-
-  @Override
-  public String toString() {
-    return "TermVectorEntry{" +
-            "field='" + field + '\'' +
-            ", term='" + term + '\'' +
-            ", frequency=" + frequency +
-            '}';
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/TermVectorEntryFreqSortedComparator.java b/lucene/src/java/org/apache/lucene/index/TermVectorEntryFreqSortedComparator.java
deleted file mode 100644
index 3ff26d8..0000000
--- a/lucene/src/java/org/apache/lucene/index/TermVectorEntryFreqSortedComparator.java
+++ /dev/null
@@ -1,40 +0,0 @@
-package org.apache.lucene.index;
-/**
- * Copyright 2007 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-import java.util.Comparator;
-
-/**
- * Compares {@link org.apache.lucene.index.TermVectorEntry}s first by frequency and then by
- * the term (case-sensitive)
- *
- **/
-public class TermVectorEntryFreqSortedComparator implements Comparator<TermVectorEntry> {
-  public int compare(TermVectorEntry entry, TermVectorEntry entry1) {
-    int result = 0;
-    result = entry1.getFrequency() - entry.getFrequency();
-    if (result == 0)
-    {
-      result = entry.getTerm().compareTo(entry1.getTerm());
-      if (result == 0)
-      {
-        result = entry.getField().compareTo(entry1.getField());
-      }
-    }
-    return result;
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/TermVectorMapper.java b/lucene/src/java/org/apache/lucene/index/TermVectorMapper.java
deleted file mode 100644
index bdca632..0000000
--- a/lucene/src/java/org/apache/lucene/index/TermVectorMapper.java
+++ /dev/null
@@ -1,104 +0,0 @@
-package org.apache.lucene.index;
-
-import org.apache.lucene.util.BytesRef;
-
-/**
- * Copyright 2007 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-/**
- * The TermVectorMapper can be used to map Term Vectors into your own
- * structure instead of the parallel array structure used by
- * {@link org.apache.lucene.index.IndexReader#getTermFreqVector(int,String)}.
- * <p/>
- * It is up to the implementation to make sure it is thread-safe.
- *
- *
- **/
-public abstract class TermVectorMapper {
-
-  private boolean ignoringPositions;
-  private boolean ignoringOffsets;
-
-
-  protected TermVectorMapper() {
-  }
-
-  /**
-   *
-   * @param ignoringPositions true if this mapper should tell Lucene to ignore positions even if they are stored
-   * @param ignoringOffsets similar to ignoringPositions
-   */
-  protected TermVectorMapper(boolean ignoringPositions, boolean ignoringOffsets) {
-    this.ignoringPositions = ignoringPositions;
-    this.ignoringOffsets = ignoringOffsets;
-  }
-
-  /**
-   * Tell the mapper what to expect in regards to field, number of terms, offset and position storage.
-   * This method will be called once before retrieving the vector for a field.
-   *
-   * This method will be called before {@link #map(BytesRef,int,TermVectorOffsetInfo[],int[])}.
-   * @param field The field the vector is for
-   * @param numTerms The number of terms that need to be mapped
-   * @param storeOffsets true if the mapper should expect offset information
-   * @param storePositions true if the mapper should expect positions info
-   */
-  public abstract void setExpectations(String field, int numTerms, boolean storeOffsets, boolean storePositions);
-  /**
-   * Map the Term Vector information into your own structure
-   * @param term The term to add to the vector
-   * @param frequency The frequency of the term in the document
-   * @param offsets null if the offset is not specified, otherwise the offset into the field of the term
-   * @param positions null if the position is not specified, otherwise the position in the field of the term
-   */
-  public abstract void map(BytesRef term, int frequency, TermVectorOffsetInfo [] offsets, int [] positions);
-
-  /**
-   * Indicate to Lucene that even if there are positions stored, this mapper is not interested in them and they
-   * can be skipped over.  Derived classes should set this to true if they want to ignore positions.  The default
-   * is false, meaning positions will be loaded if they are stored.
-   * @return false
-   */
-  public boolean isIgnoringPositions()
-  {
-    return ignoringPositions;
-  }
-
-  /**
-   *
-   * @see #isIgnoringPositions() Same principal as {@link #isIgnoringPositions()}, but applied to offsets.  false by default.
-   * @return false
-   */
-  public boolean isIgnoringOffsets()
-  {
-    return ignoringOffsets;
-  }
-
-  /**
-   * Passes down the index of the document whose term vector is currently being mapped,
-   * once for each top level call to a term vector reader.
-   *<p/>
-   * Default implementation IGNORES the document number.  Override if your implementation needs the document number.
-   * <p/> 
-   * NOTE: Document numbers are internal to Lucene and subject to change depending on indexing operations.
-   *
-   * @param documentNumber index of document currently being mapped
-   */
-  public void setDocumentNumber(int documentNumber) {
-  }
-
-}
diff --git a/lucene/src/java/org/apache/lucene/index/TermVectorOffsetInfo.java b/lucene/src/java/org/apache/lucene/index/TermVectorOffsetInfo.java
deleted file mode 100644
index 4288945..0000000
--- a/lucene/src/java/org/apache/lucene/index/TermVectorOffsetInfo.java
+++ /dev/null
@@ -1,91 +0,0 @@
-package org.apache.lucene.index;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * The TermVectorOffsetInfo class holds information pertaining to a Term in a {@link org.apache.lucene.index.TermPositionVector}'s
- * offset information.  This offset information is the character offset as set during the Analysis phase (and thus may not be the actual offset in the
- * original content).
- */
-public class TermVectorOffsetInfo {
-  /**
-   * Convenience declaration when creating a {@link org.apache.lucene.index.TermPositionVector} that stores only position information.
-   */
-  public transient static final TermVectorOffsetInfo[] EMPTY_OFFSET_INFO = new TermVectorOffsetInfo[0];
-  private int startOffset;
-  private int endOffset;
-
-  public TermVectorOffsetInfo() {
-  }
-
-  public TermVectorOffsetInfo(int startOffset, int endOffset) {
-    this.endOffset = endOffset;
-    this.startOffset = startOffset;
-  }
-
-  /**
-   * The accessor for the ending offset for the term
-   * @return The offset
-   */
-  public int getEndOffset() {
-    return endOffset;
-  }
-
-  public void setEndOffset(int endOffset) {
-    this.endOffset = endOffset;
-  }
-
-  /**
-   * The accessor for the starting offset of the term.
-   *
-   * @return The offset
-   */
-  public int getStartOffset() {
-    return startOffset;
-  }
-
-  public void setStartOffset(int startOffset) {
-    this.startOffset = startOffset;
-  }
-
-  /**
-   * Two TermVectorOffsetInfos are equals if both the start and end offsets are the same
-   * @param o The comparison Object
-   * @return true if both {@link #getStartOffset()} and {@link #getEndOffset()} are the same for both objects.
-   */
-  @Override
-  public boolean equals(Object o) {
-    if (this == o) return true;
-    if (!(o instanceof TermVectorOffsetInfo)) return false;
-
-    final TermVectorOffsetInfo termVectorOffsetInfo = (TermVectorOffsetInfo) o;
-
-    if (endOffset != termVectorOffsetInfo.endOffset) return false;
-    if (startOffset != termVectorOffsetInfo.startOffset) return false;
-
-    return true;
-  }
-
-  @Override
-  public int hashCode() {
-    int result;
-    result = startOffset;
-    result = 29 * result + endOffset;
-    return result;
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/TermVectorsConsumer.java b/lucene/src/java/org/apache/lucene/index/TermVectorsConsumer.java
new file mode 100644
index 0000000..93c8c02
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/index/TermVectorsConsumer.java
@@ -0,0 +1,184 @@
+package org.apache.lucene.index;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Map;
+
+import org.apache.lucene.index.codecs.TermVectorsWriter;
+import org.apache.lucene.store.FlushInfo;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.RamUsageEstimator;
+
+final class TermVectorsConsumer extends TermsHashConsumer {
+
+  TermVectorsWriter writer;
+  final DocumentsWriterPerThread docWriter;
+  int freeCount;
+  int lastDocID;
+
+  final DocumentsWriterPerThread.DocState docState;
+  final BytesRef flushTerm = new BytesRef();
+
+  // Used by perField when serializing the term vectors
+  final ByteSliceReader vectorSliceReaderPos = new ByteSliceReader();
+  final ByteSliceReader vectorSliceReaderOff = new ByteSliceReader();
+  boolean hasVectors;
+
+  public TermVectorsConsumer(DocumentsWriterPerThread docWriter) {
+    this.docWriter = docWriter;
+    docState = docWriter.docState;
+  }
+
+  @Override
+  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {
+    if (writer != null) {
+      // At least one doc in this run had term vectors enabled
+      try {
+        fill(state.numDocs);
+        assert state.segmentName != null;
+        writer.finish(state.numDocs);
+      } finally {
+        IOUtils.close(writer);
+        writer = null;
+
+        lastDocID = 0;
+        hasVectors = false;
+      }
+    }
+
+    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {
+      TermVectorsConsumerPerField perField = (TermVectorsConsumerPerField) field;
+      perField.termsHashPerField.reset();
+      perField.shrinkHash();
+    }
+  }
+
+  /** Fills in no-term-vectors for all docs we haven't seen
+   *  since the last doc that had term vectors. */
+  void fill(int docID) throws IOException {
+    while(lastDocID < docID) {
+      writer.startDocument(0);
+      lastDocID++;
+    }
+  }
+
+  private final void initTermVectorsWriter() throws IOException {
+    if (writer == null) {
+      IOContext context = new IOContext(new FlushInfo(docWriter.getNumDocsInRAM(), docWriter.bytesUsed()));
+      writer = docWriter.codec.termVectorsFormat().vectorsWriter(docWriter.directory, docWriter.getSegment(), context);
+      lastDocID = 0;
+    }
+  }
+
+  @Override
+  void finishDocument(TermsHash termsHash) throws IOException {
+
+    assert docWriter.writer.testPoint("TermVectorsTermsWriter.finishDocument start");
+
+    if (!hasVectors) {
+      return;
+    }
+
+    initTermVectorsWriter();
+
+    fill(docState.docID);
+
+    // Append term vectors to the real outputs:
+    writer.startDocument(numVectorFields);
+    for (int i = 0; i < numVectorFields; i++) {
+      perFields[i].finishDocument();
+    }
+
+    assert lastDocID == docState.docID: "lastDocID=" + lastDocID + " docState.docID=" + docState.docID;
+
+    lastDocID++;
+
+    termsHash.reset();
+    reset();
+    assert docWriter.writer.testPoint("TermVectorsTermsWriter.finishDocument end");
+  }
+
+  @Override
+  public void abort() {
+    hasVectors = false;
+
+    if (writer != null) {
+      writer.abort();
+      writer = null;
+    }
+
+    lastDocID = 0;
+
+    reset();
+  }
+
+  int numVectorFields;
+
+  TermVectorsConsumerPerField[] perFields;
+
+  void reset() {
+    numVectorFields = 0;
+    perFields = new TermVectorsConsumerPerField[1];
+  }
+
+  @Override
+  public TermsHashConsumerPerField addField(TermsHashPerField termsHashPerField, FieldInfo fieldInfo) {
+    return new TermVectorsConsumerPerField(termsHashPerField, this, fieldInfo);
+  }
+
+  void addFieldToFlush(TermVectorsConsumerPerField fieldToFlush) {
+    if (numVectorFields == perFields.length) {
+      int newSize = ArrayUtil.oversize(numVectorFields + 1, RamUsageEstimator.NUM_BYTES_OBJECT_REF);
+      TermVectorsConsumerPerField[] newArray = new TermVectorsConsumerPerField[newSize];
+      System.arraycopy(perFields, 0, newArray, 0, numVectorFields);
+      perFields = newArray;
+    }
+
+    perFields[numVectorFields++] = fieldToFlush;
+  }
+
+  @Override
+  void startDocument() throws IOException {
+    assert clearLastVectorFieldName();
+    reset();
+  }
+
+  // Called only by assert
+  final boolean clearLastVectorFieldName() {
+    lastVectorFieldName = null;
+    return true;
+  }
+
+  // Called only by assert
+  String lastVectorFieldName;
+  final boolean vectorFieldsInOrder(FieldInfo fi) {
+    try {
+      if (lastVectorFieldName != null)
+        return lastVectorFieldName.compareTo(fi.name) < 0;
+      else
+        return true;
+    } finally {
+      lastVectorFieldName = fi.name;
+    }
+  }
+
+}
diff --git a/lucene/src/java/org/apache/lucene/index/TermVectorsConsumerPerField.java b/lucene/src/java/org/apache/lucene/index/TermVectorsConsumerPerField.java
new file mode 100644
index 0000000..2e83733
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/index/TermVectorsConsumerPerField.java
@@ -0,0 +1,259 @@
+package org.apache.lucene.index;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.index.codecs.TermVectorsWriter;
+import org.apache.lucene.util.ByteBlockPool;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.RamUsageEstimator;
+
+final class TermVectorsConsumerPerField extends TermsHashConsumerPerField {
+
+  final TermsHashPerField termsHashPerField;
+  final TermVectorsConsumer termsWriter;
+  final FieldInfo fieldInfo;
+  final DocumentsWriterPerThread.DocState docState;
+  final FieldInvertState fieldState;
+
+  boolean doVectors;
+  boolean doVectorPositions;
+  boolean doVectorOffsets;
+
+  int maxNumPostings;
+  OffsetAttribute offsetAttribute = null;
+
+  public TermVectorsConsumerPerField(TermsHashPerField termsHashPerField, TermVectorsConsumer termsWriter, FieldInfo fieldInfo) {
+    this.termsHashPerField = termsHashPerField;
+    this.termsWriter = termsWriter;
+    this.fieldInfo = fieldInfo;
+    docState = termsHashPerField.docState;
+    fieldState = termsHashPerField.fieldState;
+  }
+
+  @Override
+  int getStreamCount() {
+    return 2;
+  }
+
+  @Override
+  boolean start(IndexableField[] fields, int count) {
+    doVectors = false;
+    doVectorPositions = false;
+    doVectorOffsets = false;
+
+    for(int i=0;i<count;i++) {
+      IndexableField field = fields[i];
+      if (field.fieldType().indexed() && field.fieldType().storeTermVectors()) {
+        doVectors = true;
+        doVectorPositions |= field.fieldType().storeTermVectorPositions();
+        doVectorOffsets |= field.fieldType().storeTermVectorOffsets();
+      }
+    }
+
+    if (doVectors) {
+      termsWriter.hasVectors = true;
+      if (termsHashPerField.bytesHash.size() != 0) {
+        // Only necessary if previous doc hit a
+        // non-aborting exception while writing vectors in
+        // this field:
+        termsHashPerField.reset();
+      }
+    }
+
+    // TODO: only if needed for performance
+    //perThread.postingsCount = 0;
+
+    return doVectors;
+  }
+
+  public void abort() {}
+
+  /** Called once per field per document if term vectors
+   *  are enabled, to write the vectors to
+   *  RAMOutputStream, which is then quickly flushed to
+   *  the real term vectors files in the Directory. */  @Override
+  void finish() throws IOException {
+    if (!doVectors || termsHashPerField.bytesHash.size() == 0)
+      return;
+
+    termsWriter.addFieldToFlush(this);
+  }
+
+  void finishDocument() throws IOException {
+    assert docState.testPoint("TermVectorsTermsWriterPerField.finish start");
+
+    final int numPostings = termsHashPerField.bytesHash.size();
+
+    final BytesRef flushTerm = termsWriter.flushTerm;
+
+    assert numPostings >= 0;
+
+    if (numPostings > maxNumPostings)
+      maxNumPostings = numPostings;
+
+    // This is called once, after inverting all occurrences
+    // of a given field in the doc.  At this point we flush
+    // our hash into the DocWriter.
+
+    assert fieldInfo.storeTermVector;
+    assert termsWriter.vectorFieldsInOrder(fieldInfo);
+
+    TermVectorsPostingsArray postings = (TermVectorsPostingsArray) termsHashPerField.postingsArray;
+    final TermVectorsWriter tv = termsWriter.writer;
+
+    // TODO: we may want to make this sort in same order
+    // as Codec's terms dict?
+    final int[] termIDs = termsHashPerField.sortPostings(BytesRef.getUTF8SortedAsUnicodeComparator());
+
+    tv.startField(fieldInfo, numPostings, doVectorPositions, doVectorOffsets);
+    
+    final ByteSliceReader posReader = doVectorPositions ? termsWriter.vectorSliceReaderPos : null;
+    final ByteSliceReader offReader = doVectorOffsets ? termsWriter.vectorSliceReaderOff : null;
+    
+    final ByteBlockPool termBytePool = termsHashPerField.termBytePool;
+
+    for(int j=0;j<numPostings;j++) {
+      final int termID = termIDs[j];
+      final int freq = postings.freqs[termID];
+
+      // Get BytesRef
+      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);
+      tv.startTerm(flushTerm, freq);
+      
+      if (doVectorPositions || doVectorOffsets) {
+        if (posReader != null) {
+          termsHashPerField.initReader(posReader, termID, 0);
+        }
+        if (offReader != null) {
+          termsHashPerField.initReader(offReader, termID, 1);
+        }
+        tv.addProx(freq, posReader, offReader);
+      }
+    }
+
+    termsHashPerField.reset();
+    // commit the termVectors once successful success - FI will otherwise reset them
+    fieldInfo.commitVectors();
+  }
+
+  void shrinkHash() {
+    termsHashPerField.shrinkHash(maxNumPostings);
+    maxNumPostings = 0;
+  }
+
+  @Override
+  void start(IndexableField f) {
+    if (doVectorOffsets) {
+      offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);
+    } else {
+      offsetAttribute = null;
+    }
+  }
+
+  @Override
+  void newTerm(final int termID) {
+    assert docState.testPoint("TermVectorsTermsWriterPerField.newTerm start");
+    TermVectorsPostingsArray postings = (TermVectorsPostingsArray) termsHashPerField.postingsArray;
+
+    postings.freqs[termID] = 1;
+
+    if (doVectorOffsets) {
+      int startOffset = fieldState.offset + offsetAttribute.startOffset();
+      int endOffset = fieldState.offset + offsetAttribute.endOffset();
+
+      termsHashPerField.writeVInt(1, startOffset);
+      termsHashPerField.writeVInt(1, endOffset - startOffset);
+      postings.lastOffsets[termID] = endOffset;
+    }
+
+    if (doVectorPositions) {
+      termsHashPerField.writeVInt(0, fieldState.position);
+      postings.lastPositions[termID] = fieldState.position;
+    }
+  }
+
+  @Override
+  void addTerm(final int termID) {
+
+    assert docState.testPoint("TermVectorsTermsWriterPerField.addTerm start");
+
+    TermVectorsPostingsArray postings = (TermVectorsPostingsArray) termsHashPerField.postingsArray;
+
+    postings.freqs[termID]++;
+
+    if (doVectorOffsets) {
+      int startOffset = fieldState.offset + offsetAttribute.startOffset();
+      int endOffset = fieldState.offset + offsetAttribute.endOffset();
+
+      termsHashPerField.writeVInt(1, startOffset - postings.lastOffsets[termID]);
+      termsHashPerField.writeVInt(1, endOffset - startOffset);
+      postings.lastOffsets[termID] = endOffset;
+    }
+
+    if (doVectorPositions) {
+      termsHashPerField.writeVInt(0, fieldState.position - postings.lastPositions[termID]);
+      postings.lastPositions[termID] = fieldState.position;
+    }
+  }
+
+  @Override
+  void skippingLongTerm() {}
+
+  @Override
+  ParallelPostingsArray createPostingsArray(int size) {
+    return new TermVectorsPostingsArray(size);
+  }
+
+  static final class TermVectorsPostingsArray extends ParallelPostingsArray {
+    public TermVectorsPostingsArray(int size) {
+      super(size);
+      freqs = new int[size];
+      lastOffsets = new int[size];
+      lastPositions = new int[size];
+    }
+
+    int[] freqs;                                       // How many times this term occurred in the current doc
+    int[] lastOffsets;                                 // Last offset we saw
+    int[] lastPositions;                               // Last position where this term occurred
+
+    @Override
+    ParallelPostingsArray newInstance(int size) {
+      return new TermVectorsPostingsArray(size);
+    }
+
+    @Override
+    void copyTo(ParallelPostingsArray toArray, int numToCopy) {
+      assert toArray instanceof TermVectorsPostingsArray;
+      TermVectorsPostingsArray to = (TermVectorsPostingsArray) toArray;
+
+      super.copyTo(toArray, numToCopy);
+
+      System.arraycopy(freqs, 0, to.freqs, 0, size);
+      System.arraycopy(lastOffsets, 0, to.lastOffsets, 0, size);
+      System.arraycopy(lastPositions, 0, to.lastPositions, 0, size);
+    }
+
+    @Override
+    int bytesPerPosting() {
+      return super.bytesPerPosting() + 3 * RamUsageEstimator.NUM_BYTES_INT;
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/index/TermVectorsReader.java b/lucene/src/java/org/apache/lucene/index/TermVectorsReader.java
deleted file mode 100644
index 9be9071..0000000
--- a/lucene/src/java/org/apache/lucene/index/TermVectorsReader.java
+++ /dev/null
@@ -1,538 +0,0 @@
-package org.apache.lucene.index;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-
-import java.io.Closeable;
-import java.io.IOException;
-import java.util.Arrays;
-
-class TermVectorsReader implements Cloneable, Closeable {
-
-  // NOTE: if you make a new format, it must be larger than
-  // the current format
-
-  // Changed strings to UTF8 with length-in-bytes not length-in-chars
-  static final int FORMAT_UTF8_LENGTH_IN_BYTES = 4;
-
-  // NOTE: always change this if you switch to a new format!
-  // whenever you add a new format, make it 1 larger (positive version logic)!
-  static final int FORMAT_CURRENT = FORMAT_UTF8_LENGTH_IN_BYTES;
-  
-  // when removing support for old versions, leave the last supported version here
-  static final int FORMAT_MINIMUM = FORMAT_UTF8_LENGTH_IN_BYTES;
-
-  //The size in bytes that the FORMAT_VERSION will take up at the beginning of each file 
-  static final int FORMAT_SIZE = 4;
-
-  static final byte STORE_POSITIONS_WITH_TERMVECTOR = 0x1;
-  static final byte STORE_OFFSET_WITH_TERMVECTOR = 0x2;
-  
-  private FieldInfos fieldInfos;
-
-  private IndexInput tvx;
-  private IndexInput tvd;
-  private IndexInput tvf;
-  private int size;
-  private int numTotalDocs;
-
-  // The docID offset where our docs begin in the index
-  // file.  This will be 0 if we have our own private file.
-  private int docStoreOffset;
-  
-  private final int format;
-
-  TermVectorsReader(Directory d, String segment, FieldInfos fieldInfos, IOContext context)
-    throws CorruptIndexException, IOException {
-    this(d, segment, fieldInfos, context, -1, 0);
-  }
-    
-  TermVectorsReader(Directory d, String segment, FieldInfos fieldInfos, IOContext context, int docStoreOffset, int size)
-    throws CorruptIndexException, IOException {
-    boolean success = false;
-
-    try {
-      String idxName = IndexFileNames.segmentFileName(segment, "", IndexFileNames.VECTORS_INDEX_EXTENSION);
-      tvx = d.openInput(idxName, context);
-      format = checkValidFormat(tvx, idxName);
-      String fn = IndexFileNames.segmentFileName(segment, "", IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);
-      tvd = d.openInput(fn, context);
-      final int tvdFormat = checkValidFormat(tvd, fn);
-      fn = IndexFileNames.segmentFileName(segment, "", IndexFileNames.VECTORS_FIELDS_EXTENSION);
-      tvf = d.openInput(fn, context);
-      final int tvfFormat = checkValidFormat(tvf, fn);
-
-      assert format == tvdFormat;
-      assert format == tvfFormat;
-
-      numTotalDocs = (int) (tvx.length() >> 4);
-
-      if (-1 == docStoreOffset) {
-        this.docStoreOffset = 0;
-        this.size = numTotalDocs;
-        assert size == 0 || numTotalDocs == size;
-      } else {
-        this.docStoreOffset = docStoreOffset;
-        this.size = size;
-        // Verify the file is long enough to hold all of our
-        // docs
-        assert numTotalDocs >= size + docStoreOffset: "numTotalDocs=" + numTotalDocs + " size=" + size + " docStoreOffset=" + docStoreOffset;
-      }
-
-      this.fieldInfos = fieldInfos;
-      success = true;
-    } finally {
-      // With lock-less commits, it's entirely possible (and
-      // fine) to hit a FileNotFound exception above. In
-      // this case, we want to explicitly close any subset
-      // of things that were opened so that we don't have to
-      // wait for a GC to do so.
-      if (!success) {
-        close();
-      }
-    }
-  }
-
-  // Used for bulk copy when merging
-  IndexInput getTvdStream() {
-    return tvd;
-  }
-
-  // Used for bulk copy when merging
-  IndexInput getTvfStream() {
-    return tvf;
-  }
-
-  private void seekTvx(final int docNum) throws IOException {
-    tvx.seek((docNum + docStoreOffset) * 16L + FORMAT_SIZE);
-  }
-
-  boolean canReadRawDocs() {
-    // we can always read raw docs, unless the term vectors
-    // didn't exist
-    return format != 0;
-  }
-
-  /** Retrieve the length (in bytes) of the tvd and tvf
-   *  entries for the next numDocs starting with
-   *  startDocID.  This is used for bulk copying when
-   *  merging segments, if the field numbers are
-   *  congruent.  Once this returns, the tvf & tvd streams
-   *  are seeked to the startDocID. */
-  final void rawDocs(int[] tvdLengths, int[] tvfLengths, int startDocID, int numDocs) throws IOException {
-
-    if (tvx == null) {
-      Arrays.fill(tvdLengths, 0);
-      Arrays.fill(tvfLengths, 0);
-      return;
-    }
-
-    seekTvx(startDocID);
-
-    long tvdPosition = tvx.readLong();
-    tvd.seek(tvdPosition);
-
-    long tvfPosition = tvx.readLong();
-    tvf.seek(tvfPosition);
-
-    long lastTvdPosition = tvdPosition;
-    long lastTvfPosition = tvfPosition;
-
-    int count = 0;
-    while (count < numDocs) {
-      final int docID = docStoreOffset + startDocID + count + 1;
-      assert docID <= numTotalDocs;
-      if (docID < numTotalDocs)  {
-        tvdPosition = tvx.readLong();
-        tvfPosition = tvx.readLong();
-      } else {
-        tvdPosition = tvd.length();
-        tvfPosition = tvf.length();
-        assert count == numDocs-1;
-      }
-      tvdLengths[count] = (int) (tvdPosition-lastTvdPosition);
-      tvfLengths[count] = (int) (tvfPosition-lastTvfPosition);
-      count++;
-      lastTvdPosition = tvdPosition;
-      lastTvfPosition = tvfPosition;
-    }
-  }
-
-  private int checkValidFormat(IndexInput in, String fn) throws CorruptIndexException, IOException
-  {
-    int format = in.readInt();
-    if (format < FORMAT_MINIMUM)
-      throw new IndexFormatTooOldException(in, format, FORMAT_MINIMUM, FORMAT_CURRENT);
-    if (format > FORMAT_CURRENT)
-      throw new IndexFormatTooNewException(in, format, FORMAT_MINIMUM, FORMAT_CURRENT);
-    return format;
-  }
-
-  public void close() throws IOException {
-    IOUtils.close(tvx, tvd, tvf);
-  }
-
-  /**
-   * 
-   * @return The number of documents in the reader
-   */
-  int size() {
-    return size;
-  }
-
-  public void get(int docNum, String field, TermVectorMapper mapper) throws IOException {
-    if (tvx != null) {
-      int fieldNumber = fieldInfos.fieldNumber(field);
-      //We need to account for the FORMAT_SIZE at when seeking in the tvx
-      //We don't need to do this in other seeks because we already have the
-      // file pointer
-      //that was written in another file
-      seekTvx(docNum);
-      //System.out.println("TVX Pointer: " + tvx.getFilePointer());
-      long tvdPosition = tvx.readLong();
-
-      tvd.seek(tvdPosition);
-      int fieldCount = tvd.readVInt();
-      //System.out.println("Num Fields: " + fieldCount);
-      // There are only a few fields per document. We opt for a full scan
-      // rather then requiring that they be ordered. We need to read through
-      // all of the fields anyway to get to the tvf pointers.
-      int number = 0;
-      int found = -1;
-      for (int i = 0; i < fieldCount; i++) {
-        number = tvd.readVInt();
-        if (number == fieldNumber)
-          found = i;
-      }
-
-      // This field, although valid in the segment, was not found in this
-      // document
-      if (found != -1) {
-        // Compute position in the tvf file
-        long position = tvx.readLong();
-        for (int i = 1; i <= found; i++)
-          position += tvd.readVLong();
-
-        mapper.setDocumentNumber(docNum);
-        readTermVector(field, position, mapper);
-      } else {
-        //System.out.println("Fieldable not found");
-      }
-    } else {
-      //System.out.println("No tvx file");
-    }
-  }
-
-
-
-  /**
-   * Retrieve the term vector for the given document and field
-   * @param docNum The document number to retrieve the vector for
-   * @param field The field within the document to retrieve
-   * @return The TermFreqVector for the document and field or null if there is no termVector for this field.
-   * @throws IOException if there is an error reading the term vector files
-   */ 
-  TermFreqVector get(int docNum, String field) throws IOException {
-    // Check if no term vectors are available for this segment at all
-    ParallelArrayTermVectorMapper mapper = new ParallelArrayTermVectorMapper();
-    get(docNum, field, mapper);
-
-    return mapper.materializeVector();
-  }
-
-  // Reads the String[] fields; you have to pre-seek tvd to
-  // the right point
-  private String[] readFields(int fieldCount) throws IOException {
-    int number = 0;
-    String[] fields = new String[fieldCount];
-
-    for (int i = 0; i < fieldCount; i++) {
-      number = tvd.readVInt();
-      fields[i] = fieldInfos.fieldName(number);
-    }
-
-    return fields;
-  }
-
-  // Reads the long[] offsets into TVF; you have to pre-seek
-  // tvx/tvd to the right point
-  private long[] readTvfPointers(int fieldCount) throws IOException {
-    // Compute position in the tvf file
-    long position = tvx.readLong();
-
-    long[] tvfPointers = new long[fieldCount];
-    tvfPointers[0] = position;
-
-    for (int i = 1; i < fieldCount; i++) {
-      position += tvd.readVLong();
-      tvfPointers[i] = position;
-    }
-
-    return tvfPointers;
-  }
-
-  /**
-   * Return all term vectors stored for this document or null if the could not be read in.
-   * 
-   * @param docNum The document number to retrieve the vector for
-   * @return All term frequency vectors
-   * @throws IOException if there is an error reading the term vector files 
-   */
-  TermFreqVector[] get(int docNum) throws IOException {
-    TermFreqVector[] result = null;
-    if (tvx != null) {
-      //We need to offset by
-      seekTvx(docNum);
-      long tvdPosition = tvx.readLong();
-
-      tvd.seek(tvdPosition);
-      int fieldCount = tvd.readVInt();
-
-      // No fields are vectorized for this document
-      if (fieldCount != 0) {
-        final String[] fields = readFields(fieldCount);
-        final long[] tvfPointers = readTvfPointers(fieldCount);
-        result = readTermVectors(docNum, fields, tvfPointers);
-      }
-    } else {
-      //System.out.println("No tvx file");
-    }
-    return result;
-  }
-
-  public void get(int docNumber, TermVectorMapper mapper) throws IOException {
-    // Check if no term vectors are available for this segment at all
-    if (tvx != null) {
-      //We need to offset by
-
-      seekTvx(docNumber);
-      long tvdPosition = tvx.readLong();
-
-      tvd.seek(tvdPosition);
-      int fieldCount = tvd.readVInt();
-
-      // No fields are vectorized for this document
-      if (fieldCount != 0) {
-        final String[] fields = readFields(fieldCount);
-        final long[] tvfPointers = readTvfPointers(fieldCount);
-        mapper.setDocumentNumber(docNumber);
-        readTermVectors(fields, tvfPointers, mapper);
-      }
-    } else {
-      //System.out.println("No tvx file");
-    }
-  }
-
-
-  private SegmentTermVector[] readTermVectors(int docNum, String fields[], long tvfPointers[])
-          throws IOException {
-    SegmentTermVector res[] = new SegmentTermVector[fields.length];
-    for (int i = 0; i < fields.length; i++) {
-      ParallelArrayTermVectorMapper mapper = new ParallelArrayTermVectorMapper();
-      mapper.setDocumentNumber(docNum);
-      readTermVector(fields[i], tvfPointers[i], mapper);
-      res[i] = (SegmentTermVector) mapper.materializeVector();
-    }
-    return res;
-  }
-
-  private void readTermVectors(String fields[], long tvfPointers[], TermVectorMapper mapper)
-          throws IOException {
-    for (int i = 0; i < fields.length; i++) {
-      readTermVector(fields[i], tvfPointers[i], mapper);
-    }
-  }
-
-
-  /**
-   * 
-   * @param field The field to read in
-   * @param tvfPointer The pointer within the tvf file where we should start reading
-   * @param mapper The mapper used to map the TermVector
-   * @throws IOException
-   */ 
-  private void readTermVector(String field, long tvfPointer, TermVectorMapper mapper)
-          throws IOException {
-
-    // Now read the data from specified position
-    //We don't need to offset by the FORMAT here since the pointer already includes the offset
-    tvf.seek(tvfPointer);
-
-    int numTerms = tvf.readVInt();
-    //System.out.println("Num Terms: " + numTerms);
-    // If no terms - return a constant empty termvector. However, this should never occur!
-    if (numTerms == 0) 
-      return;
-    
-    boolean storePositions;
-    boolean storeOffsets;
-    
-    byte bits = tvf.readByte();
-    storePositions = (bits & STORE_POSITIONS_WITH_TERMVECTOR) != 0;
-    storeOffsets = (bits & STORE_OFFSET_WITH_TERMVECTOR) != 0;
-
-    mapper.setExpectations(field, numTerms, storeOffsets, storePositions);
-    int start = 0;
-    int deltaLength = 0;
-    int totalLength = 0;
-    byte[] byteBuffer;
-
-    // init the buffer
-    byteBuffer = new byte[20];
-
-    for (int i = 0; i < numTerms; i++) {
-      start = tvf.readVInt();
-      deltaLength = tvf.readVInt();
-      totalLength = start + deltaLength;
-
-      final BytesRef term = new BytesRef(totalLength);
-      
-      // Term stored as utf8 bytes
-      if (byteBuffer.length < totalLength) {
-        byteBuffer = ArrayUtil.grow(byteBuffer, totalLength);
-      }
-      tvf.readBytes(byteBuffer, start, deltaLength);
-      System.arraycopy(byteBuffer, 0, term.bytes, 0, totalLength);
-      term.length = totalLength;
-      int freq = tvf.readVInt();
-      int [] positions = null;
-      if (storePositions) { //read in the positions
-        //does the mapper even care about positions?
-        if (!mapper.isIgnoringPositions()) {
-          positions = new int[freq];
-          int prevPosition = 0;
-          for (int j = 0; j < freq; j++)
-          {
-            positions[j] = prevPosition + tvf.readVInt();
-            prevPosition = positions[j];
-          }
-        } else {
-          //we need to skip over the positions.  Since these are VInts, I don't believe there is anyway to know for sure how far to skip
-          //
-          for (int j = 0; j < freq; j++)
-          {
-            tvf.readVInt();
-          }
-        }
-      }
-      TermVectorOffsetInfo[] offsets = null;
-      if (storeOffsets) {
-        //does the mapper even care about offsets?
-        if (!mapper.isIgnoringOffsets()) {
-          offsets = new TermVectorOffsetInfo[freq];
-          int prevOffset = 0;
-          for (int j = 0; j < freq; j++) {
-            int startOffset = prevOffset + tvf.readVInt();
-            int endOffset = startOffset + tvf.readVInt();
-            offsets[j] = new TermVectorOffsetInfo(startOffset, endOffset);
-            prevOffset = endOffset;
-          }
-        } else {
-          for (int j = 0; j < freq; j++){
-            tvf.readVInt();
-            tvf.readVInt();
-          }
-        }
-      }
-      mapper.map(term, freq, offsets, positions);
-    }
-  }
-
-  @Override
-  protected Object clone() throws CloneNotSupportedException {
-    
-    final TermVectorsReader clone = (TermVectorsReader) super.clone();
-
-    // These are null when a TermVectorsReader was created
-    // on a segment that did not have term vectors saved
-    if (tvx != null && tvd != null && tvf != null) {
-      clone.tvx = (IndexInput) tvx.clone();
-      clone.tvd = (IndexInput) tvd.clone();
-      clone.tvf = (IndexInput) tvf.clone();
-    }
-    
-    return clone;
-  }
-}
-
-
-/**
- * Models the existing parallel array structure
- */
-class ParallelArrayTermVectorMapper extends TermVectorMapper
-{
-
-  private BytesRef[] terms;
-  private int[] termFreqs;
-  private int positions[][];
-  private TermVectorOffsetInfo offsets[][];
-  private int currentPosition;
-  private boolean storingOffsets;
-  private boolean storingPositions;
-  private String field;
-
-  @Override
-  public void setExpectations(String field, int numTerms, boolean storeOffsets, boolean storePositions) {
-    this.field = field;
-    terms = new BytesRef[numTerms];
-    termFreqs = new int[numTerms];
-    this.storingOffsets = storeOffsets;
-    this.storingPositions = storePositions;
-    if(storePositions)
-      this.positions = new int[numTerms][];
-    if(storeOffsets)
-      this.offsets = new TermVectorOffsetInfo[numTerms][];
-  }
-
-  @Override
-  public void map(BytesRef term, int frequency, TermVectorOffsetInfo[] offsets, int[] positions) {
-    terms[currentPosition] = term;
-    termFreqs[currentPosition] = frequency;
-    if (storingOffsets)
-    {
-      this.offsets[currentPosition] = offsets;
-    }
-    if (storingPositions)
-    {
-      this.positions[currentPosition] = positions; 
-    }
-    currentPosition++;
-  }
-
-  /**
-   * Construct the vector
-   * @return The {@link TermFreqVector} based on the mappings.
-   */
-  public TermFreqVector materializeVector() {
-    SegmentTermVector tv = null;
-    if (field != null && terms != null) {
-      if (storingPositions || storingOffsets) {
-        tv = new SegmentTermPositionVector(field, terms, termFreqs, positions, offsets);
-      } else {
-        tv = new SegmentTermVector(field, terms, termFreqs);
-      }
-    }
-    return tv;
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter.java b/lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter.java
deleted file mode 100644
index 7bafcc9..0000000
--- a/lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter.java
+++ /dev/null
@@ -1,242 +0,0 @@
-package org.apache.lucene.index;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Map;
-
-import org.apache.lucene.store.FlushInfo;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.IOContext.Context;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.RamUsageEstimator;
-
-final class TermVectorsTermsWriter extends TermsHashConsumer {
-
-  final DocumentsWriterPerThread docWriter;
-  int freeCount;
-  IndexOutput tvx;
-  IndexOutput tvd;
-  IndexOutput tvf;
-  int lastDocID;
-
-  final DocumentsWriterPerThread.DocState docState;
-  final BytesRef flushTerm = new BytesRef();
-
-  // Used by perField when serializing the term vectors
-  final ByteSliceReader vectorSliceReader = new ByteSliceReader();
-  boolean hasVectors;
-
-  public TermVectorsTermsWriter(DocumentsWriterPerThread docWriter) {
-    this.docWriter = docWriter;
-    docState = docWriter.docState;
-  }
-
-  @Override
-  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {
-    if (tvx != null) {
-      // At least one doc in this run had term vectors enabled
-      fill(state.numDocs);
-      assert state.segmentName != null;
-      String idxName = IndexFileNames.segmentFileName(state.segmentName, "", IndexFileNames.VECTORS_INDEX_EXTENSION);
-      IOUtils.close(tvx, tvf, tvd);
-      tvx = tvd = tvf = null;
-      if (4+((long) state.numDocs)*16 != state.directory.fileLength(idxName)) {
-        throw new RuntimeException("after flush: tvx size mismatch: " + state.numDocs + " docs vs " + state.directory.fileLength(idxName) + " length in bytes of " + idxName + " file exists?=" + state.directory.fileExists(idxName));
-      }
-
-      lastDocID = 0;
-      hasVectors = false;
-    }
-
-    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {
-      TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;
-      perField.termsHashPerField.reset();
-      perField.shrinkHash();
-    }
-  }
-
-  /** Fills in no-term-vectors for all docs we haven't seen
-   *  since the last doc that had term vectors. */
-  void fill(int docID) throws IOException {
-    if (lastDocID < docID) {
-      final long tvfPosition = tvf.getFilePointer();
-      while(lastDocID < docID) {
-        tvx.writeLong(tvd.getFilePointer());
-        tvd.writeVInt(0);
-        tvx.writeLong(tvfPosition);
-        lastDocID++;
-      }
-    }
-  }
-
-  private final void initTermVectorsWriter() throws IOException {
-    if (tvx == null) {
-      boolean success = false;
-      try {
-        IOContext context = new IOContext(new FlushInfo(docWriter.getNumDocsInRAM(), docWriter.bytesUsed()));
-        // If we hit an exception while init'ing the term
-        // vector output files, we must abort this segment
-        // because those files will be in an unknown
-        // state:
-        tvx = docWriter.directory.createOutput(IndexFileNames.segmentFileName(docWriter.getSegment(), "", IndexFileNames.VECTORS_INDEX_EXTENSION), context);
-        tvd = docWriter.directory.createOutput(IndexFileNames.segmentFileName(docWriter.getSegment(), "", IndexFileNames.VECTORS_DOCUMENTS_EXTENSION), context);
-        tvf = docWriter.directory.createOutput(IndexFileNames.segmentFileName(docWriter.getSegment(), "", IndexFileNames.VECTORS_FIELDS_EXTENSION), context);
-
-        tvx.writeInt(TermVectorsReader.FORMAT_CURRENT);
-        tvd.writeInt(TermVectorsReader.FORMAT_CURRENT);
-        tvf.writeInt(TermVectorsReader.FORMAT_CURRENT);
-        success = true;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(tvx, tvd, tvf);
-        }
-      }
-
-      lastDocID = 0;
-    }
-  }
-
-  @Override
-  void finishDocument(TermsHash termsHash) throws IOException {
-
-    assert docWriter.writer.testPoint("TermVectorsTermsWriter.finishDocument start");
-
-    if (!hasVectors) {
-      return;
-    }
-
-    initTermVectorsWriter();
-
-    fill(docState.docID);
-
-    // Append term vectors to the real outputs:
-    tvx.writeLong(tvd.getFilePointer());
-    tvx.writeLong(tvf.getFilePointer());
-    tvd.writeVInt(numVectorFields);
-    if (numVectorFields > 0) {
-      for(int i=0;i<numVectorFields;i++) {
-        tvd.writeVInt(perFields[i].fieldInfo.number);
-      }
-      long lastPos = tvf.getFilePointer();
-      perFields[0].finishDocument();
-      perFields[0].fieldInfo.commitVectors();
-      for(int i=1;i<numVectorFields;i++) {
-        long pos = tvf.getFilePointer();
-        tvd.writeVLong(pos-lastPos);
-        lastPos = pos;
-        perFields[i].finishDocument();
-        // commit the termVectors once successful success - FI will otherwise reset them
-        perFields[i].fieldInfo.commitVectors();
-      }
-    }
-
-    assert lastDocID == docState.docID: "lastDocID=" + lastDocID + " docState.docID=" + docState.docID;
-
-    lastDocID++;
-
-    termsHash.reset();
-    reset();
-    assert docWriter.writer.testPoint("TermVectorsTermsWriter.finishDocument end");
-  }
-
-  @Override
-  public void abort() {
-    hasVectors = false;
-    try {
-      IOUtils.closeWhileHandlingException(tvx, tvd, tvf);
-    } catch (IOException e) {
-      // cannot happen since we suppress exceptions
-      throw new RuntimeException(e);
-    }
-    
-    try {
-      docWriter.directory.deleteFile(IndexFileNames.segmentFileName(docWriter.getSegment(), "", IndexFileNames.VECTORS_INDEX_EXTENSION));
-    } catch (IOException ignored) {
-    }
-    
-    try {
-      docWriter.directory.deleteFile(IndexFileNames.segmentFileName(docWriter.getSegment(), "", IndexFileNames.VECTORS_DOCUMENTS_EXTENSION));
-    } catch (IOException ignored) {
-    }
-    
-    try {
-      docWriter.directory.deleteFile(IndexFileNames.segmentFileName(docWriter.getSegment(), "", IndexFileNames.VECTORS_FIELDS_EXTENSION));
-    } catch (IOException ignored) {
-    }
-    
-    tvx = tvd = tvf = null;
-    lastDocID = 0;
-
-    reset();
-  }
-
-  int numVectorFields;
-
-  TermVectorsTermsWriterPerField[] perFields;
-
-  void reset() {
-    numVectorFields = 0;
-    perFields = new TermVectorsTermsWriterPerField[1];
-  }
-
-  @Override
-  public TermsHashConsumerPerField addField(TermsHashPerField termsHashPerField, FieldInfo fieldInfo) {
-    return new TermVectorsTermsWriterPerField(termsHashPerField, this, fieldInfo);
-  }
-
-  void addFieldToFlush(TermVectorsTermsWriterPerField fieldToFlush) {
-    if (numVectorFields == perFields.length) {
-      int newSize = ArrayUtil.oversize(numVectorFields + 1, RamUsageEstimator.NUM_BYTES_OBJECT_REF);
-      TermVectorsTermsWriterPerField[] newArray = new TermVectorsTermsWriterPerField[newSize];
-      System.arraycopy(perFields, 0, newArray, 0, numVectorFields);
-      perFields = newArray;
-    }
-
-    perFields[numVectorFields++] = fieldToFlush;
-  }
-
-  @Override
-  void startDocument() throws IOException {
-    assert clearLastVectorFieldName();
-    reset();
-  }
-
-  // Called only by assert
-  final boolean clearLastVectorFieldName() {
-    lastVectorFieldName = null;
-    return true;
-  }
-
-  // Called only by assert
-  String lastVectorFieldName;
-  final boolean vectorFieldsInOrder(FieldInfo fi) {
-    try {
-      if (lastVectorFieldName != null)
-        return lastVectorFieldName.compareTo(fi.name) < 0;
-      else
-        return true;
-    } finally {
-      lastVectorFieldName = fi.name;
-    }
-  }
-
-}
diff --git a/lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriterPerField.java b/lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriterPerField.java
deleted file mode 100644
index 9e51a6a..0000000
--- a/lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriterPerField.java
+++ /dev/null
@@ -1,286 +0,0 @@
-package org.apache.lucene.index;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.ByteBlockPool;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.RamUsageEstimator;
-
-final class TermVectorsTermsWriterPerField extends TermsHashConsumerPerField {
-
-  final TermsHashPerField termsHashPerField;
-  final TermVectorsTermsWriter termsWriter;
-  final FieldInfo fieldInfo;
-  final DocumentsWriterPerThread.DocState docState;
-  final FieldInvertState fieldState;
-
-  boolean doVectors;
-  boolean doVectorPositions;
-  boolean doVectorOffsets;
-
-  int maxNumPostings;
-  OffsetAttribute offsetAttribute = null;
-
-  public TermVectorsTermsWriterPerField(TermsHashPerField termsHashPerField, TermVectorsTermsWriter termsWriter, FieldInfo fieldInfo) {
-    this.termsHashPerField = termsHashPerField;
-    this.termsWriter = termsWriter;
-    this.fieldInfo = fieldInfo;
-    docState = termsHashPerField.docState;
-    fieldState = termsHashPerField.fieldState;
-  }
-
-  @Override
-  int getStreamCount() {
-    return 2;
-  }
-
-  @Override
-  boolean start(IndexableField[] fields, int count) {
-    doVectors = false;
-    doVectorPositions = false;
-    doVectorOffsets = false;
-
-    for(int i=0;i<count;i++) {
-      IndexableField field = fields[i];
-      if (field.fieldType().indexed() && field.fieldType().storeTermVectors()) {
-        doVectors = true;
-        doVectorPositions |= field.fieldType().storeTermVectorPositions();
-        doVectorOffsets |= field.fieldType().storeTermVectorOffsets();
-      }
-    }
-
-    if (doVectors) {
-      termsWriter.hasVectors = true;
-      if (termsHashPerField.bytesHash.size() != 0) {
-        // Only necessary if previous doc hit a
-        // non-aborting exception while writing vectors in
-        // this field:
-        termsHashPerField.reset();
-      }
-    }
-
-    // TODO: only if needed for performance
-    //perThread.postingsCount = 0;
-
-    return doVectors;
-  }
-
-  public void abort() {}
-
-  /** Called once per field per document if term vectors
-   *  are enabled, to write the vectors to
-   *  RAMOutputStream, which is then quickly flushed to
-   *  the real term vectors files in the Directory. */  @Override
-  void finish() throws IOException {
-    if (!doVectors || termsHashPerField.bytesHash.size() == 0)
-      return;
-
-    termsWriter.addFieldToFlush(this);
-  }
-
-  void finishDocument() throws IOException {
-    assert docState.testPoint("TermVectorsTermsWriterPerField.finish start");
-
-    final int numPostings = termsHashPerField.bytesHash.size();
-
-    final BytesRef flushTerm = termsWriter.flushTerm;
-
-    assert numPostings >= 0;
-
-    if (numPostings > maxNumPostings)
-      maxNumPostings = numPostings;
-
-    // This is called once, after inverting all occurrences
-    // of a given field in the doc.  At this point we flush
-    // our hash into the DocWriter.
-
-    assert fieldInfo.storeTermVector;
-    assert termsWriter.vectorFieldsInOrder(fieldInfo);
-
-    TermVectorsPostingsArray postings = (TermVectorsPostingsArray) termsHashPerField.postingsArray;
-    final IndexOutput tvf = termsWriter.tvf;
-
-    // TODO: we may want to make this sort in same order
-    // as Codec's terms dict?
-    final int[] termIDs = termsHashPerField.sortPostings(BytesRef.getUTF8SortedAsUnicodeComparator());
-
-    tvf.writeVInt(numPostings);
-    byte bits = 0x0;
-    if (doVectorPositions)
-      bits |= TermVectorsReader.STORE_POSITIONS_WITH_TERMVECTOR;
-    if (doVectorOffsets)
-      bits |= TermVectorsReader.STORE_OFFSET_WITH_TERMVECTOR;
-    tvf.writeByte(bits);
-
-    int lastLen = 0;
-    byte[] lastBytes = null;
-    int lastStart = 0;
-
-    final ByteSliceReader reader = termsWriter.vectorSliceReader;
-    final ByteBlockPool termBytePool = termsHashPerField.termBytePool;
-
-    for(int j=0;j<numPostings;j++) {
-      final int termID = termIDs[j];
-      final int freq = postings.freqs[termID];
-
-      // Get BytesRef
-      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);
-
-      // Compute common byte prefix between last term and
-      // this term
-      int prefix = 0;
-      if (j > 0) {
-        while(prefix < lastLen && prefix < flushTerm.length) {
-          if (lastBytes[lastStart+prefix] != flushTerm.bytes[flushTerm.offset+prefix]) {
-            break;
-          }
-          prefix++;
-        }
-      }
-
-      lastLen = flushTerm.length;
-      lastBytes = flushTerm.bytes;
-      lastStart = flushTerm.offset;
-
-      final int suffix = flushTerm.length - prefix;
-      tvf.writeVInt(prefix);
-      tvf.writeVInt(suffix);
-      tvf.writeBytes(flushTerm.bytes, lastStart+prefix, suffix);
-      tvf.writeVInt(freq);
-
-      if (doVectorPositions) {
-        termsHashPerField.initReader(reader, termID, 0);
-        reader.writeTo(tvf);
-      }
-
-      if (doVectorOffsets) {
-        termsHashPerField.initReader(reader, termID, 1);
-        reader.writeTo(tvf);
-      }
-    }
-
-    termsHashPerField.reset();
-  }
-
-  void shrinkHash() {
-    termsHashPerField.shrinkHash(maxNumPostings);
-    maxNumPostings = 0;
-  }
-
-  @Override
-  void start(IndexableField f) {
-    if (doVectorOffsets) {
-      offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);
-    } else {
-      offsetAttribute = null;
-    }
-  }
-
-  @Override
-  void newTerm(final int termID) {
-    assert docState.testPoint("TermVectorsTermsWriterPerField.newTerm start");
-    TermVectorsPostingsArray postings = (TermVectorsPostingsArray) termsHashPerField.postingsArray;
-
-    postings.freqs[termID] = 1;
-
-    if (doVectorOffsets) {
-      int startOffset = fieldState.offset + offsetAttribute.startOffset();
-      int endOffset = fieldState.offset + offsetAttribute.endOffset();
-
-      termsHashPerField.writeVInt(1, startOffset);
-      termsHashPerField.writeVInt(1, endOffset - startOffset);
-      postings.lastOffsets[termID] = endOffset;
-    }
-
-    if (doVectorPositions) {
-      termsHashPerField.writeVInt(0, fieldState.position);
-      postings.lastPositions[termID] = fieldState.position;
-    }
-  }
-
-  @Override
-  void addTerm(final int termID) {
-
-    assert docState.testPoint("TermVectorsTermsWriterPerField.addTerm start");
-
-    TermVectorsPostingsArray postings = (TermVectorsPostingsArray) termsHashPerField.postingsArray;
-
-    postings.freqs[termID]++;
-
-    if (doVectorOffsets) {
-      int startOffset = fieldState.offset + offsetAttribute.startOffset();
-      int endOffset = fieldState.offset + offsetAttribute.endOffset();
-
-      termsHashPerField.writeVInt(1, startOffset - postings.lastOffsets[termID]);
-      termsHashPerField.writeVInt(1, endOffset - startOffset);
-      postings.lastOffsets[termID] = endOffset;
-    }
-
-    if (doVectorPositions) {
-      termsHashPerField.writeVInt(0, fieldState.position - postings.lastPositions[termID]);
-      postings.lastPositions[termID] = fieldState.position;
-    }
-  }
-
-  @Override
-  void skippingLongTerm() {}
-
-  @Override
-  ParallelPostingsArray createPostingsArray(int size) {
-    return new TermVectorsPostingsArray(size);
-  }
-
-  static final class TermVectorsPostingsArray extends ParallelPostingsArray {
-    public TermVectorsPostingsArray(int size) {
-      super(size);
-      freqs = new int[size];
-      lastOffsets = new int[size];
-      lastPositions = new int[size];
-    }
-
-    int[] freqs;                                       // How many times this term occurred in the current doc
-    int[] lastOffsets;                                 // Last offset we saw
-    int[] lastPositions;                               // Last position where this term occurred
-
-    @Override
-    ParallelPostingsArray newInstance(int size) {
-      return new TermVectorsPostingsArray(size);
-    }
-
-    @Override
-    void copyTo(ParallelPostingsArray toArray, int numToCopy) {
-      assert toArray instanceof TermVectorsPostingsArray;
-      TermVectorsPostingsArray to = (TermVectorsPostingsArray) toArray;
-
-      super.copyTo(toArray, numToCopy);
-
-      System.arraycopy(freqs, 0, to.freqs, 0, size);
-      System.arraycopy(lastOffsets, 0, to.lastOffsets, 0, size);
-      System.arraycopy(lastPositions, 0, to.lastPositions, 0, size);
-    }
-
-    @Override
-    int bytesPerPosting() {
-      return super.bytesPerPosting() + 3 * RamUsageEstimator.NUM_BYTES_INT;
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/TermVectorsWriter.java b/lucene/src/java/org/apache/lucene/index/TermVectorsWriter.java
deleted file mode 100644
index 6b20f82..0000000
--- a/lucene/src/java/org/apache/lucene/index/TermVectorsWriter.java
+++ /dev/null
@@ -1,199 +0,0 @@
-package org.apache.lucene.index;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.StringHelper;
-
-import java.io.IOException;
-
-final class TermVectorsWriter {
-
-  private IndexOutput tvx = null, tvd = null, tvf = null;
-  private FieldInfos fieldInfos;
-
-  public TermVectorsWriter(Directory directory, String segment,
-                           FieldInfos fieldInfos, IOContext context) throws IOException {
-    boolean success = false;
-    try {
-      // Open files for TermVector storage
-      tvx = directory.createOutput(IndexFileNames.segmentFileName(segment, "", IndexFileNames.VECTORS_INDEX_EXTENSION), context);
-      tvx.writeInt(TermVectorsReader.FORMAT_CURRENT);
-      tvd = directory.createOutput(IndexFileNames.segmentFileName(segment, "", IndexFileNames.VECTORS_DOCUMENTS_EXTENSION), context);
-      tvd.writeInt(TermVectorsReader.FORMAT_CURRENT);
-      tvf = directory.createOutput(IndexFileNames.segmentFileName(segment, "", IndexFileNames.VECTORS_FIELDS_EXTENSION), context);
-      tvf.writeInt(TermVectorsReader.FORMAT_CURRENT);
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(tvx, tvd, tvf);
-      }
-    }
-
-    this.fieldInfos = fieldInfos;
-  }
-
-  /**
-   * Add a complete document specified by all its term vectors. If document has no
-   * term vectors, add value for tvx.
-   *
-   * @param vectors
-   * @throws IOException
-   */
-  public final void addAllDocVectors(TermFreqVector[] vectors) throws IOException {
-
-    tvx.writeLong(tvd.getFilePointer());
-    tvx.writeLong(tvf.getFilePointer());
-
-    if (vectors != null) {
-      final int numFields = vectors.length;
-      tvd.writeVInt(numFields);
-
-      long[] fieldPointers = new long[numFields];
-
-      for (int i=0; i<numFields; i++) {
-        fieldPointers[i] = tvf.getFilePointer();
-
-        final int fieldNumber = fieldInfos.fieldNumber(vectors[i].getField());
-
-        // 1st pass: write field numbers to tvd
-        tvd.writeVInt(fieldNumber);
-
-        final int numTerms = vectors[i].size();
-        tvf.writeVInt(numTerms);
-
-        final TermPositionVector tpVector;
-
-        final byte bits;
-        final boolean storePositions;
-        final boolean storeOffsets;
-
-        if (vectors[i] instanceof TermPositionVector) {
-          // May have positions & offsets
-          tpVector = (TermPositionVector) vectors[i];
-          storePositions = tpVector.size() > 0 && tpVector.getTermPositions(0) != null;
-          storeOffsets = tpVector.size() > 0 && tpVector.getOffsets(0) != null;
-          bits = (byte) ((storePositions ? TermVectorsReader.STORE_POSITIONS_WITH_TERMVECTOR : 0) +
-                         (storeOffsets ? TermVectorsReader.STORE_OFFSET_WITH_TERMVECTOR : 0));
-        } else {
-          tpVector = null;
-          bits = 0;
-          storePositions = false;
-          storeOffsets = false;
-        }
-
-        tvf.writeVInt(bits);
-
-        final BytesRef[] terms = vectors[i].getTerms();
-        final int[] freqs = vectors[i].getTermFrequencies();
-
-        for (int j=0; j<numTerms; j++) {
-
-          int start = j == 0 ? 0 : StringHelper.bytesDifference(terms[j-1].bytes,
-                                                   terms[j-1].length,
-                                                   terms[j].bytes,
-                                                   terms[j].length);
-          int length = terms[j].length - start;
-          tvf.writeVInt(start);       // write shared prefix length
-          tvf.writeVInt(length);        // write delta length
-          tvf.writeBytes(terms[j].bytes, start, length);  // write delta bytes
-
-          final int termFreq = freqs[j];
-
-          tvf.writeVInt(termFreq);
-
-          if (storePositions) {
-            final int[] positions = tpVector.getTermPositions(j);
-            if (positions == null)
-              throw new IllegalStateException("Trying to write positions that are null!");
-            assert positions.length == termFreq;
-
-            // use delta encoding for positions
-            int lastPosition = 0;
-            for(int k=0;k<positions.length;k++) {
-              final int position = positions[k];
-              tvf.writeVInt(position-lastPosition);
-              lastPosition = position;
-            }
-          }
-
-          if (storeOffsets) {
-            final TermVectorOffsetInfo[] offsets = tpVector.getOffsets(j);
-            if (offsets == null)
-              throw new IllegalStateException("Trying to write offsets that are null!");
-            assert offsets.length == termFreq;
-
-            // use delta encoding for offsets
-            int lastEndOffset = 0;
-            for(int k=0;k<offsets.length;k++) {
-              final int startOffset = offsets[k].getStartOffset();
-              final int endOffset = offsets[k].getEndOffset();
-              tvf.writeVInt(startOffset-lastEndOffset);
-              tvf.writeVInt(endOffset-startOffset);
-              lastEndOffset = endOffset;
-            }
-          }
-        }
-      }
-
-      // 2nd pass: write field pointers to tvd
-      if (numFields > 1) {
-        long lastFieldPointer = fieldPointers[0];
-        for (int i=1; i<numFields; i++) {
-          final long fieldPointer = fieldPointers[i];
-          tvd.writeVLong(fieldPointer-lastFieldPointer);
-          lastFieldPointer = fieldPointer;
-        }
-      }
-    } else
-      tvd.writeVInt(0);
-  }
-
-  /**
-   * Do a bulk copy of numDocs documents from reader to our
-   * streams.  This is used to expedite merging, if the
-   * field numbers are congruent.
-   */
-  final void addRawDocuments(TermVectorsReader reader, int[] tvdLengths, int[] tvfLengths, int numDocs) throws IOException {
-    long tvdPosition = tvd.getFilePointer();
-    long tvfPosition = tvf.getFilePointer();
-    long tvdStart = tvdPosition;
-    long tvfStart = tvfPosition;
-    for(int i=0;i<numDocs;i++) {
-      tvx.writeLong(tvdPosition);
-      tvdPosition += tvdLengths[i];
-      tvx.writeLong(tvfPosition);
-      tvfPosition += tvfLengths[i];
-    }
-    tvd.copyBytes(reader.getTvdStream(), tvdPosition-tvdStart);
-    tvf.copyBytes(reader.getTvfStream(), tvfPosition-tvfStart);
-    assert tvd.getFilePointer() == tvdPosition;
-    assert tvf.getFilePointer() == tvfPosition;
-  }
-
-  /** Close all streams. */
-  final void close() throws IOException {
-    // make an effort to close all streams we can but remember and re-throw
-    // the first exception encountered in this process
-    IOUtils.close(tvx, tvd, tvf);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/Terms.java b/lucene/src/java/org/apache/lucene/index/Terms.java
index a6ddce5..873639b 100644
--- a/lucene/src/java/org/apache/lucene/index/Terms.java
+++ b/lucene/src/java/org/apache/lucene/index/Terms.java
@@ -37,8 +37,11 @@ public abstract class Terms {
   private final CloseableThreadLocal<TermsEnum> threadEnums = new CloseableThreadLocal<TermsEnum>();
 
   /** Returns an iterator that will step through all
-   *  terms. This method will not return null.*/
-  public abstract TermsEnum iterator() throws IOException;
+   *  terms. This method will not return null.  If you have
+   *  a previous TermsEnum, for example from a different
+   *  field, you can pass it for possible reuse if the
+   *  implementation can do so. */
+  public abstract TermsEnum iterator(TermsEnum reuse) throws IOException;
 
   /** Returns a TermsEnum that iterates over all terms that
    *  are accepted by the provided {@link
@@ -59,9 +62,9 @@ public abstract class Terms {
       throw new IllegalArgumentException("please use CompiledAutomaton.getTermsEnum instead");
     }
     if (startTerm == null) {
-      return new AutomatonTermsEnum(iterator(), compiled);
+      return new AutomatonTermsEnum(iterator(null), compiled);
     } else {
-      return new AutomatonTermsEnum(iterator(), compiled) {
+      return new AutomatonTermsEnum(iterator(null), compiled) {
         @Override
         protected BytesRef nextSeekTerm(BytesRef term) throws IOException {
           if (term == null) {
@@ -186,7 +189,7 @@ public abstract class Terms {
   /**
    * Returns a thread-private {@link TermsEnum} instance. Obtaining
    * {@link TermsEnum} from this method might be more efficient than using
-   * {@link #iterator()} directly since this method doesn't necessarily create a
+   * {@link #iterator(TermsEnum)} directly since this method doesn't necessarily create a
    * new {@link TermsEnum} instance.
    * <p>
    * NOTE: {@link TermsEnum} instances obtained from this method must not be
@@ -201,7 +204,7 @@ public abstract class Terms {
   public TermsEnum getThreadTermsEnum() throws IOException {
     TermsEnum termsEnum = threadEnums.get();
     if (termsEnum == null) {
-      termsEnum = iterator();
+      termsEnum = iterator(null);
       threadEnums.set(termsEnum);
     }
     return termsEnum;
diff --git a/lucene/src/java/org/apache/lucene/index/TermsHash.java b/lucene/src/java/org/apache/lucene/index/TermsHash.java
index d352215..3ed8fc5 100644
--- a/lucene/src/java/org/apache/lucene/index/TermsHash.java
+++ b/lucene/src/java/org/apache/lucene/index/TermsHash.java
@@ -29,7 +29,7 @@ import org.apache.lucene.util.BytesRef;
  *  field.  It stores these tokens in a hash table, and
  *  allocates separate byte streams per token.  Consumers of
  *  this class, eg {@link FreqProxTermsWriter} and {@link
- *  TermVectorsTermsWriter}, write their own byte streams
+ *  TermVectorsConsumer}, write their own byte streams
  *  under each term.
  */
 final class TermsHash extends InvertedDocConsumer {
diff --git a/lucene/src/java/org/apache/lucene/index/TieredMergePolicy.java b/lucene/src/java/org/apache/lucene/index/TieredMergePolicy.java
index ced2836..22341c7 100644
--- a/lucene/src/java/org/apache/lucene/index/TieredMergePolicy.java
+++ b/lucene/src/java/org/apache/lucene/index/TieredMergePolicy.java
@@ -338,7 +338,7 @@ public class TieredMergePolicy extends MergePolicy {
       for(int idx = tooBigCount; idx<infosSorted.size(); idx++) {
         final SegmentInfo info = infosSorted.get(idx);
         if (merging.contains(info)) {
-          mergingBytes += info.sizeInBytes(true);
+          mergingBytes += info.sizeInBytes();
         } else if (!toBeMerged.contains(info)) {
           eligible.add(info);
         }
@@ -430,7 +430,7 @@ public class TieredMergePolicy extends MergePolicy {
       final long segBytes = size(info);
       totAfterMergeBytes += segBytes;
       totAfterMergeBytesFloored += floorSize(segBytes);
-      totBeforeMergeBytes += info.sizeInBytes(true);
+      totBeforeMergeBytes += info.sizeInBytes();
     }
 
     // Measure "skew" of the merge, which can range
@@ -632,7 +632,7 @@ public class TieredMergePolicy extends MergePolicy {
 
   // Segment size in bytes, pro-rated by % deleted
   private long size(SegmentInfo info) throws IOException {
-    final long byteSize = info.sizeInBytes(true);    
+    final long byteSize = info.sizeInBytes();    
     final int delCount = writer.get().numDeletedDocs(info);
     final double delRatio = (info.docCount <= 0 ? 0.0f : ((double)delCount / (double)info.docCount));    
     assert delRatio <= 1.0;
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/BlockTermsReader.java b/lucene/src/java/org/apache/lucene/index/codecs/BlockTermsReader.java
index 5497c69..c94f2c5 100644
--- a/lucene/src/java/org/apache/lucene/index/codecs/BlockTermsReader.java
+++ b/lucene/src/java/org/apache/lucene/index/codecs/BlockTermsReader.java
@@ -130,7 +130,6 @@ public class BlockTermsReader extends FieldsProducer {
       seekDir(in, dirOffset);
 
       final int numFields = in.readVInt();
-
       for(int i=0;i<numFields;i++) {
         final int field = in.readVInt();
         final long numTerms = in.readVLong();
@@ -208,6 +207,11 @@ public class BlockTermsReader extends FieldsProducer {
     return fields.get(field);
   }
 
+  @Override
+  public int getUniqueFieldCount() {
+    return fields.size();
+  }
+
   // Iterates through all fields
   private class TermFieldsEnum extends FieldsEnum {
     final Iterator<FieldReader> it;
@@ -229,8 +233,8 @@ public class BlockTermsReader extends FieldsProducer {
     }
     
     @Override
-    public TermsEnum terms() throws IOException {
-      return current.iterator();
+    public Terms terms() throws IOException {
+      return current;
     }
   }
 
@@ -263,7 +267,7 @@ public class BlockTermsReader extends FieldsProducer {
     }
     
     @Override
-    public TermsEnum iterator() throws IOException {
+    public TermsEnum iterator(TermsEnum reuse) throws IOException {
       return new SegmentTermsEnum();
     }
 
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/BlockTreeTermsReader.java b/lucene/src/java/org/apache/lucene/index/codecs/BlockTreeTermsReader.java
index b409246..70bbe49 100644
--- a/lucene/src/java/org/apache/lucene/index/codecs/BlockTreeTermsReader.java
+++ b/lucene/src/java/org/apache/lucene/index/codecs/BlockTreeTermsReader.java
@@ -19,11 +19,8 @@ package org.apache.lucene.index.codecs;
 
 import java.io.ByteArrayOutputStream;
 import java.io.Closeable;
-import java.io.FileOutputStream;
 import java.io.IOException;
-import java.io.OutputStreamWriter;
 import java.io.PrintStream;
-import java.io.Writer;
 import java.util.Collection;
 import java.util.Comparator;
 import java.util.Iterator;
@@ -221,6 +218,11 @@ public class BlockTreeTermsReader extends FieldsProducer {
     return fields.get(field);
   }
 
+  @Override
+  public int getUniqueFieldCount() {
+    return fields.size();
+  }
+
   // Iterates through all fields
   private class TermFieldsEnum extends FieldsEnum {
     final Iterator<FieldReader> it;
@@ -242,8 +244,8 @@ public class BlockTreeTermsReader extends FieldsProducer {
     }
     
     @Override
-    public TermsEnum terms() throws IOException {
-      return current.iterator();
+    public Terms terms() throws IOException {
+      return current;
     }
   }
 
@@ -454,7 +456,7 @@ public class BlockTreeTermsReader extends FieldsProducer {
     }
     
     @Override
-    public TermsEnum iterator() throws IOException {
+    public TermsEnum iterator(TermsEnum reuse) throws IOException {
       return new SegmentTermsEnum();
     }
 
@@ -914,8 +916,6 @@ public class BlockTreeTermsReader extends FieldsProducer {
 
         for(int idx=0;idx<=target.length;idx++) {
 
-          boolean lastIsSubBlock = false;
-
           while (true) {
             final int savePos = currentFrame.suffixesReader.getPosition();
             final int saveStartBytePos = currentFrame.startBytePos;
@@ -950,7 +950,6 @@ public class BlockTreeTermsReader extends FieldsProducer {
                     return;
                   }
                 }
-                lastIsSubBlock = isSubBlock;
                 continue;
               } else if (cmp == 0) {
                 //if (DEBUG) System.out.println("  return term=" + brToString(term));
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/Codec.java b/lucene/src/java/org/apache/lucene/index/codecs/Codec.java
index 26233ac..de52f39 100644
--- a/lucene/src/java/org/apache/lucene/index/codecs/Codec.java
+++ b/lucene/src/java/org/apache/lucene/index/codecs/Codec.java
@@ -46,6 +46,10 @@ public abstract class Codec implements NamedSPILoader.NamedSPI {
   public void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
     postingsFormat().files(dir, info, "", files);
     storedFieldsFormat().files(dir, info, files);
+    termVectorsFormat().files(dir, info, files);
+    fieldInfosFormat().files(dir, info, files);
+    // TODO: segmentInfosFormat should be allowed to declare additional files
+    // if it wants, in addition to segments_N
     docValuesFormat().files(dir, info, files);
   }
   
@@ -58,6 +62,12 @@ public abstract class Codec implements NamedSPILoader.NamedSPI {
   /** Encodes/decodes stored fields */
   public abstract StoredFieldsFormat storedFieldsFormat();
   
+  /** Encodes/decodes term vectors */
+  public abstract TermVectorsFormat termVectorsFormat();
+  
+  /** Encodes/decodes field infos file */
+  public abstract FieldInfosFormat fieldInfosFormat();
+  
   /** Encodes/decodes segments file */
   public abstract SegmentInfosFormat segmentInfosFormat();
   
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/DefaultFieldInfosFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/DefaultFieldInfosFormat.java
new file mode 100644
index 0000000..1967bdf
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/index/codecs/DefaultFieldInfosFormat.java
@@ -0,0 +1,47 @@
+package org.apache.lucene.index.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+
+/**
+ * @lucene.experimental
+ */
+public class DefaultFieldInfosFormat extends FieldInfosFormat {
+  private final FieldInfosReader reader = new DefaultFieldInfosReader();
+  private final FieldInfosWriter writer = new DefaultFieldInfosWriter();
+  
+  @Override
+  public FieldInfosReader getFieldInfosReader() throws IOException {
+    return reader;
+  }
+
+  @Override
+  public FieldInfosWriter getFieldInfosWriter() throws IOException {
+    return writer;
+  }
+
+  @Override
+  public void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    DefaultFieldInfosReader.files(dir, info, files);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/DefaultFieldInfosReader.java b/lucene/src/java/org/apache/lucene/index/codecs/DefaultFieldInfosReader.java
new file mode 100644
index 0000000..0d2d0ac
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/index/codecs/DefaultFieldInfosReader.java
@@ -0,0 +1,166 @@
+package org.apache.lucene.index.codecs;
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.IndexFormatTooNewException;
+import org.apache.lucene.index.IndexFormatTooOldException;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.values.ValueType;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * @lucene.experimental
+ */
+public class DefaultFieldInfosReader extends FieldInfosReader {
+
+  static final int FORMAT_MINIMUM = DefaultFieldInfosWriter.FORMAT_START;
+
+  @Override
+  public FieldInfos read(Directory directory, String segmentName, IOContext iocontext) throws IOException {
+    final String fileName = IndexFileNames.segmentFileName(segmentName, "", DefaultFieldInfosWriter.FIELD_INFOS_EXTENSION);
+    IndexInput input = directory.openInput(fileName, iocontext);
+
+    boolean hasVectors = false;
+    boolean hasFreq = false;
+    boolean hasProx = false;
+    
+    try {
+      final int format = input.readVInt();
+
+      if (format > FORMAT_MINIMUM) {
+        throw new IndexFormatTooOldException(input, format, FORMAT_MINIMUM, DefaultFieldInfosWriter.FORMAT_CURRENT);
+      }
+      if (format < DefaultFieldInfosWriter.FORMAT_CURRENT) {
+        throw new IndexFormatTooNewException(input, format, FORMAT_MINIMUM, DefaultFieldInfosWriter.FORMAT_CURRENT);
+      }
+
+      final int size = input.readVInt(); //read in the size
+      FieldInfo infos[] = new FieldInfo[size];
+
+      for (int i = 0; i < size; i++) {
+        String name = input.readString();
+        final int fieldNumber = format <= DefaultFieldInfosWriter.FORMAT_FLEX? input.readInt():i;
+        byte bits = input.readByte();
+        boolean isIndexed = (bits & DefaultFieldInfosWriter.IS_INDEXED) != 0;
+        boolean storeTermVector = (bits & DefaultFieldInfosWriter.STORE_TERMVECTOR) != 0;
+        boolean storePositionsWithTermVector = (bits & DefaultFieldInfosWriter.STORE_POSITIONS_WITH_TERMVECTOR) != 0;
+        boolean storeOffsetWithTermVector = (bits & DefaultFieldInfosWriter.STORE_OFFSET_WITH_TERMVECTOR) != 0;
+        boolean omitNorms = (bits & DefaultFieldInfosWriter.OMIT_NORMS) != 0;
+        boolean storePayloads = (bits & DefaultFieldInfosWriter.STORE_PAYLOADS) != 0;
+        final IndexOptions indexOptions;
+        if ((bits & DefaultFieldInfosWriter.OMIT_TERM_FREQ_AND_POSITIONS) != 0) {
+          indexOptions = IndexOptions.DOCS_ONLY;
+        } else if ((bits & DefaultFieldInfosWriter.OMIT_POSITIONS) != 0) {
+          if (format <= DefaultFieldInfosWriter.FORMAT_OMIT_POSITIONS) {
+            indexOptions = IndexOptions.DOCS_AND_FREQS;
+          } else {
+            throw new CorruptIndexException("Corrupt fieldinfos, OMIT_POSITIONS set but format=" + format + " (resource: " + input + ")");
+          }
+        } else {
+          indexOptions = IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
+        }
+
+        // LUCENE-3027: past indices were able to write
+        // storePayloads=true when omitTFAP is also true,
+        // which is invalid.  We correct that, here:
+        if (indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+          storePayloads = false;
+        }
+        hasVectors |= storeTermVector;
+        hasProx |= isIndexed && indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
+        hasFreq |= isIndexed && indexOptions != IndexOptions.DOCS_ONLY;
+        ValueType docValuesType = null;
+        if (format <= DefaultFieldInfosWriter.FORMAT_FLEX) {
+          final byte b = input.readByte();
+          switch(b) {
+            case 0:
+              docValuesType = null;
+              break;
+            case 1:
+              docValuesType = ValueType.VAR_INTS;
+              break;
+            case 2:
+              docValuesType = ValueType.FLOAT_32;
+              break;
+            case 3:
+              docValuesType = ValueType.FLOAT_64;
+              break;
+            case 4:
+              docValuesType = ValueType.BYTES_FIXED_STRAIGHT;
+              break;
+            case 5:
+              docValuesType = ValueType.BYTES_FIXED_DEREF;
+              break;
+            case 6:
+              docValuesType = ValueType.BYTES_VAR_STRAIGHT;
+              break;
+            case 7:
+              docValuesType = ValueType.BYTES_VAR_DEREF;
+              break;
+            case 8:
+              docValuesType = ValueType.FIXED_INTS_16;
+              break;
+            case 9:
+              docValuesType = ValueType.FIXED_INTS_32;
+              break;
+            case 10:
+              docValuesType = ValueType.FIXED_INTS_64;
+              break;
+            case 11:
+              docValuesType = ValueType.FIXED_INTS_8;
+              break;
+            case 12:
+              docValuesType = ValueType.BYTES_FIXED_SORTED;
+              break;
+            case 13:
+              docValuesType = ValueType.BYTES_VAR_SORTED;
+              break;
+        
+            default:
+              throw new IllegalStateException("unhandled indexValues type " + b);
+          }
+        }
+        infos[i] = new FieldInfo(name, isIndexed, fieldNumber, storeTermVector, 
+          storePositionsWithTermVector, storeOffsetWithTermVector, 
+          omitNorms, storePayloads, indexOptions, docValuesType);
+      }
+
+      if (input.getFilePointer() != input.length()) {
+        throw new CorruptIndexException("did not read all bytes from file \"" + fileName + "\": read " + input.getFilePointer() + " vs size " + input.length() + " (resource: " + input + ")");
+      }
+      
+      return new FieldInfos(infos, hasFreq, hasProx, hasVectors);
+    } finally {
+      input.close();
+    }
+  }
+  
+  public static void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    files.add(IndexFileNames.segmentFileName(info.name, "", DefaultFieldInfosWriter.FIELD_INFOS_EXTENSION));
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/DefaultFieldInfosWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/DefaultFieldInfosWriter.java
new file mode 100644
index 0000000..007311e
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/index/codecs/DefaultFieldInfosWriter.java
@@ -0,0 +1,136 @@
+package org.apache.lucene.index.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+
+/**
+ * @lucene.experimental
+ */
+public class DefaultFieldInfosWriter extends FieldInfosWriter {
+  
+  /** Extension of field infos */
+  static final String FIELD_INFOS_EXTENSION = "fnm";
+  
+  // First used in 2.9; prior to 2.9 there was no format header
+  static final int FORMAT_START = -2;
+  // First used in 3.4: omit only positional information
+  static final int FORMAT_OMIT_POSITIONS = -3;
+  // per-field codec support, records index values for fields
+  static final int FORMAT_FLEX = -4;
+
+  // whenever you add a new format, make it 1 smaller (negative version logic)!
+  static final int FORMAT_CURRENT = FORMAT_FLEX;
+  
+  static final byte IS_INDEXED = 0x1;
+  static final byte STORE_TERMVECTOR = 0x2;
+  static final byte STORE_POSITIONS_WITH_TERMVECTOR = 0x4;
+  static final byte STORE_OFFSET_WITH_TERMVECTOR = 0x8;
+  static final byte OMIT_NORMS = 0x10;
+  static final byte STORE_PAYLOADS = 0x20;
+  static final byte OMIT_TERM_FREQ_AND_POSITIONS = 0x40;
+  static final byte OMIT_POSITIONS = -128;
+  
+  @Override
+  public void write(Directory directory, String segmentName, FieldInfos infos, IOContext context) throws IOException {
+    final String fileName = IndexFileNames.segmentFileName(segmentName, "", FIELD_INFOS_EXTENSION);
+    IndexOutput output = directory.createOutput(fileName, context);
+    try {
+      output.writeVInt(FORMAT_CURRENT);
+      output.writeVInt(infos.size());
+      for (FieldInfo fi : infos) {
+        assert fi.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS || !fi.storePayloads;
+        byte bits = 0x0;
+        if (fi.isIndexed) bits |= IS_INDEXED;
+        if (fi.storeTermVector) bits |= STORE_TERMVECTOR;
+        if (fi.storePositionWithTermVector) bits |= STORE_POSITIONS_WITH_TERMVECTOR;
+        if (fi.storeOffsetWithTermVector) bits |= STORE_OFFSET_WITH_TERMVECTOR;
+        if (fi.omitNorms) bits |= OMIT_NORMS;
+        if (fi.storePayloads) bits |= STORE_PAYLOADS;
+        if (fi.indexOptions == IndexOptions.DOCS_ONLY)
+          bits |= OMIT_TERM_FREQ_AND_POSITIONS;
+        else if (fi.indexOptions == IndexOptions.DOCS_AND_FREQS)
+          bits |= OMIT_POSITIONS;
+        output.writeString(fi.name);
+        output.writeInt(fi.number);
+        output.writeByte(bits);
+
+        final byte b;
+
+        if (!fi.hasDocValues()) {
+          b = 0;
+        } else {
+          switch(fi.getDocValues()) {
+          case VAR_INTS:
+            b = 1;
+            break;
+          case FLOAT_32:
+            b = 2;
+            break;
+          case FLOAT_64:
+            b = 3;
+            break;
+          case BYTES_FIXED_STRAIGHT:
+            b = 4;
+            break;
+          case BYTES_FIXED_DEREF:
+            b = 5;
+            break;
+          case BYTES_VAR_STRAIGHT:
+            b = 6;
+            break;
+          case BYTES_VAR_DEREF:
+            b = 7;
+            break;
+          case FIXED_INTS_16:
+            b = 8;
+            break;
+          case FIXED_INTS_32:
+            b = 9;
+            break;
+          case FIXED_INTS_64:
+            b = 10;
+            break;
+          case FIXED_INTS_8:
+            b = 11;
+            break;
+          case BYTES_FIXED_SORTED:
+            b = 12;
+            break;
+          case BYTES_VAR_SORTED:
+            b = 13;
+            break;
+          default:
+            throw new IllegalStateException("unhandled indexValues type " + fi.getDocValues());
+          }
+        }
+        output.writeByte(b);
+      }
+    } finally {
+      output.close();
+    }
+  }
+  
+}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/DefaultSegmentInfosReader.java b/lucene/src/java/org/apache/lucene/index/codecs/DefaultSegmentInfosReader.java
index f597571..534666b 100644
--- a/lucene/src/java/org/apache/lucene/index/codecs/DefaultSegmentInfosReader.java
+++ b/lucene/src/java/org/apache/lucene/index/codecs/DefaultSegmentInfosReader.java
@@ -18,6 +18,8 @@ package org.apache.lucene.index.codecs;
  */
 
 import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
 
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.IndexFormatTooOldException;
@@ -51,7 +53,7 @@ public class DefaultSegmentInfosReader extends SegmentInfosReader {
       infos.setGlobalFieldMapVersion(input.readLong());
     }
     for (int i = input.readInt(); i > 0; i--) { // read segmentInfos
-      SegmentInfo si = new SegmentInfo(directory, format, input);
+      SegmentInfo si = readSegmentInfo(directory, format, input);
       if (si.getVersion() == null) {
         // Could be a 3.0 - try to open the doc stores - if it fails, it's a
         // 2.x segment, and an IndexFormatTooOldException will be thrown,
@@ -90,4 +92,100 @@ public class DefaultSegmentInfosReader extends SegmentInfosReader {
       
     infos.userData = input.readStringStringMap();
   }
+  
+  // if we make a preflex impl we can remove a lot of this hair...
+  public SegmentInfo readSegmentInfo(Directory dir, int format, ChecksumIndexInput input) throws IOException {
+    final String version;
+    if (format <= DefaultSegmentInfosWriter.FORMAT_3_1) {
+      version = input.readString();
+    } else {
+      version = null;
+    }
+    final String name = input.readString();
+    final int docCount = input.readInt();
+    final long delGen = input.readLong();
+    final int docStoreOffset = input.readInt();
+    final String docStoreSegment;
+    final boolean docStoreIsCompoundFile;
+    if (docStoreOffset != -1) {
+      docStoreSegment = input.readString();
+      docStoreIsCompoundFile = input.readByte() == SegmentInfo.YES;
+    } else {
+      docStoreSegment = name;
+      docStoreIsCompoundFile = false;
+    }
+
+    if (format > DefaultSegmentInfosWriter.FORMAT_4_0) {
+      // pre-4.0 indexes write a byte if there is a single norms file
+      byte b = input.readByte();
+      assert 1 == b;
+    }
+
+    final int numNormGen = input.readInt();
+    final Map<Integer,Long> normGen;
+    if (numNormGen == SegmentInfo.NO) {
+      normGen = null;
+    } else {
+      normGen = new HashMap<Integer, Long>();
+      for(int j=0;j<numNormGen;j++) {
+        int fieldNumber = j;
+        if (format <= DefaultSegmentInfosWriter.FORMAT_4_0) {
+          fieldNumber = input.readInt();
+        }
+
+        normGen.put(fieldNumber, input.readLong());
+      }
+    }
+    final boolean isCompoundFile = input.readByte() == SegmentInfo.YES;
+
+    final int delCount = input.readInt();
+    assert delCount <= docCount;
+
+    final int hasProx = input.readByte();
+
+    final Codec codec;
+    // note: if the codec is not available: Codec.forName will throw an exception.
+    if (format <= DefaultSegmentInfosWriter.FORMAT_4_0) {
+      codec = Codec.forName(input.readString());
+    } else {
+      codec = Codec.forName("Lucene3x");
+    }
+    final Map<String,String> diagnostics = input.readStringStringMap();
+
+    final int hasVectors;
+    if (format <= DefaultSegmentInfosWriter.FORMAT_HAS_VECTORS) {
+      hasVectors = input.readByte();
+    } else {
+      final String storesSegment;
+      final String ext;
+      final boolean storeIsCompoundFile;
+      if (docStoreOffset != -1) {
+        storesSegment = docStoreSegment;
+        storeIsCompoundFile = docStoreIsCompoundFile;
+        ext = IndexFileNames.COMPOUND_FILE_STORE_EXTENSION;
+      } else {
+        storesSegment = name;
+        storeIsCompoundFile = isCompoundFile;
+        ext = IndexFileNames.COMPOUND_FILE_EXTENSION;
+      }
+      final Directory dirToTest;
+      if (storeIsCompoundFile) {
+        dirToTest = new CompoundFileDirectory(dir, IndexFileNames.segmentFileName(storesSegment, "", ext), IOContext.READONCE, false);
+      } else {
+        dirToTest = dir;
+      }
+      try {
+        // TODO: remove this manual file check or push to preflex codec
+        hasVectors = dirToTest.fileExists(IndexFileNames.segmentFileName(storesSegment, "", DefaultTermVectorsReader.VECTORS_INDEX_EXTENSION)) ? SegmentInfo.YES : SegmentInfo.NO;
+      } finally {
+        if (isCompoundFile) {
+          dirToTest.close();
+        }
+      }
+    }
+    
+    return new SegmentInfo(dir, version, name, docCount, delGen, docStoreOffset,
+      docStoreSegment, docStoreIsCompoundFile, normGen, isCompoundFile,
+      delCount, hasProx, codec, diagnostics, hasVectors);
+  }
 }
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/DefaultSegmentInfosWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/DefaultSegmentInfosWriter.java
index cf69f56..b16dba6 100644
--- a/lucene/src/java/org/apache/lucene/index/codecs/DefaultSegmentInfosWriter.java
+++ b/lucene/src/java/org/apache/lucene/index/codecs/DefaultSegmentInfosWriter.java
@@ -18,6 +18,8 @@ package org.apache.lucene.index.codecs;
  */
 
 import java.io.IOException;
+import java.util.Map;
+import java.util.Map.Entry;
 
 import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.SegmentInfos;
@@ -50,6 +52,7 @@ public class DefaultSegmentInfosWriter extends SegmentInfosWriter {
 
   /** This must always point to the most recent file format.
    * whenever you add a new format, make it 1 smaller (negative version logic)! */
+  // TODO: move this, as its currently part of required preamble
   public static final int FORMAT_CURRENT = FORMAT_4_0;
   
   /** This must always point to the first supported file format. */
@@ -68,7 +71,7 @@ public class DefaultSegmentInfosWriter extends SegmentInfosWriter {
       out.writeLong(infos.getGlobalFieldMapVersion());
       out.writeInt(infos.size()); // write infos
       for (SegmentInfo si : infos) {
-        si.write(out);
+        writeInfo(out, si);
       }
       out.writeStringStringMap(infos.getUserData());
       success = true;
@@ -80,6 +83,40 @@ public class DefaultSegmentInfosWriter extends SegmentInfosWriter {
     }
   }
   
+  /** Save a single segment's info. */
+  private void writeInfo(IndexOutput output, SegmentInfo si) throws IOException {
+    assert si.getDelCount() <= si.docCount: "delCount=" + si.getDelCount() + " docCount=" + si.docCount + " segment=" + si.name;
+    // Write the Lucene version that created this segment, since 3.1
+    output.writeString(si.getVersion());
+    output.writeString(si.name);
+    output.writeInt(si.docCount);
+    output.writeLong(si.getDelGen());
+
+    output.writeInt(si.getDocStoreOffset());
+    if (si.getDocStoreOffset() != -1) {
+      output.writeString(si.getDocStoreSegment());
+      output.writeByte((byte) (si.getDocStoreIsCompoundFile() ? 1:0));
+    }
+
+    Map<Integer,Long> normGen = si.getNormGen();
+    if (normGen == null) {
+      output.writeInt(SegmentInfo.NO);
+    } else {
+      output.writeInt(normGen.size());
+      for (Entry<Integer,Long> entry : normGen.entrySet()) {
+        output.writeInt(entry.getKey());
+        output.writeLong(entry.getValue());
+      }
+    }
+
+    output.writeByte((byte) (si.getUseCompoundFile() ? SegmentInfo.YES : SegmentInfo.NO));
+    output.writeInt(si.getDelCount());
+    output.writeByte((byte) (si.getHasProxInternal()));
+    output.writeString(si.getCodec().getName());
+    output.writeStringStringMap(si.getDiagnostics());
+    output.writeByte((byte) (si.getHasVectorsInternal()));
+  }
+  
   protected IndexOutput createOutput(Directory dir, String segmentFileName, IOContext context)
       throws IOException {
     IndexOutput plainOut = dir.createOutput(segmentFileName, context);
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/DefaultStoredFieldsReader.java b/lucene/src/java/org/apache/lucene/index/codecs/DefaultStoredFieldsReader.java
index d56fd5f..d5ae277 100644
--- a/lucene/src/java/org/apache/lucene/index/codecs/DefaultStoredFieldsReader.java
+++ b/lucene/src/java/org/apache/lucene/index/codecs/DefaultStoredFieldsReader.java
@@ -32,7 +32,6 @@ import org.apache.lucene.store.AlreadyClosedException;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.CloseableThreadLocal;
 import org.apache.lucene.util.IOUtils;
 
 import java.io.Closeable;
@@ -50,16 +49,7 @@ public final class DefaultStoredFieldsReader extends StoredFieldsReader implemen
   private final static int FORMAT_SIZE = 4;
 
   private final FieldInfos fieldInfos;
-  private CloseableThreadLocal<IndexInput> fieldsStreamTL = new CloseableThreadLocal<IndexInput>();
-  
-  // The main fieldStream, used only for cloning.
-  private final IndexInput cloneableFieldsStream;
-
-  // This is a clone of cloneableFieldsStream used for reading documents.
-  // It should not be cloned outside of a synchronized context.
   private final IndexInput fieldsStream;
-
-  private final IndexInput cloneableIndexStream;
   private final IndexInput indexStream;
   private int numTotalDocs;
   private int size;
@@ -70,8 +60,6 @@ public final class DefaultStoredFieldsReader extends StoredFieldsReader implemen
   // file.  This will be 0 if we have our own private file.
   private int docStoreOffset;
 
-  private boolean isOriginal = false;
-
   /** Returns a cloned FieldsReader that shares open
    *  IndexInputs with the original one.  It is the caller's
    *  job not to close the original FieldsReader until all
@@ -80,7 +68,7 @@ public final class DefaultStoredFieldsReader extends StoredFieldsReader implemen
   @Override
   public DefaultStoredFieldsReader clone() {
     ensureOpen();
-    return new DefaultStoredFieldsReader(fieldInfos, numTotalDocs, size, format, docStoreOffset, cloneableFieldsStream, cloneableIndexStream);
+    return new DefaultStoredFieldsReader(fieldInfos, numTotalDocs, size, format, docStoreOffset, (IndexInput)fieldsStream.clone(), (IndexInput)indexStream.clone());
   }
 
   /** Verifies that the code version which wrote the segment is supported. */
@@ -102,16 +90,14 @@ public final class DefaultStoredFieldsReader extends StoredFieldsReader implemen
   
   // Used only by clone
   private DefaultStoredFieldsReader(FieldInfos fieldInfos, int numTotalDocs, int size, int format, int docStoreOffset,
-                       IndexInput cloneableFieldsStream, IndexInput cloneableIndexStream) {
+                       IndexInput fieldsStream, IndexInput indexStream) {
     this.fieldInfos = fieldInfos;
     this.numTotalDocs = numTotalDocs;
     this.size = size;
     this.format = format;
     this.docStoreOffset = docStoreOffset;
-    this.cloneableFieldsStream = cloneableFieldsStream;
-    this.cloneableIndexStream = cloneableIndexStream;
-    fieldsStream = (IndexInput) cloneableFieldsStream.clone();
-    indexStream = (IndexInput) cloneableIndexStream.clone();
+    this.fieldsStream = fieldsStream;
+    this.indexStream = indexStream;
   }
 
   public DefaultStoredFieldsReader(Directory d, SegmentInfo si, FieldInfos fn, IOContext context) throws IOException {
@@ -119,24 +105,20 @@ public final class DefaultStoredFieldsReader extends StoredFieldsReader implemen
     final int docStoreOffset = si.getDocStoreOffset();
     final int size = si.docCount;
     boolean success = false;
-    isOriginal = true;
+    fieldInfos = fn;
     try {
-      fieldInfos = fn;
-
-      cloneableFieldsStream = d.openInput(IndexFileNames.segmentFileName(segment, "", DefaultStoredFieldsWriter.FIELDS_EXTENSION), context);
+      fieldsStream = d.openInput(IndexFileNames.segmentFileName(segment, "", DefaultStoredFieldsWriter.FIELDS_EXTENSION), context);
       final String indexStreamFN = IndexFileNames.segmentFileName(segment, "", DefaultStoredFieldsWriter.FIELDS_INDEX_EXTENSION);
-      cloneableIndexStream = d.openInput(indexStreamFN, context);
+      indexStream = d.openInput(indexStreamFN, context);
       
-      format = cloneableIndexStream.readInt();
+      format = indexStream.readInt();
 
       if (format < DefaultStoredFieldsWriter.FORMAT_MINIMUM)
-        throw new IndexFormatTooOldException(cloneableIndexStream, format, DefaultStoredFieldsWriter.FORMAT_MINIMUM, DefaultStoredFieldsWriter.FORMAT_CURRENT);
+        throw new IndexFormatTooOldException(indexStream, format, DefaultStoredFieldsWriter.FORMAT_MINIMUM, DefaultStoredFieldsWriter.FORMAT_CURRENT);
       if (format > DefaultStoredFieldsWriter.FORMAT_CURRENT)
-        throw new IndexFormatTooNewException(cloneableIndexStream, format, DefaultStoredFieldsWriter.FORMAT_MINIMUM, DefaultStoredFieldsWriter.FORMAT_CURRENT);
+        throw new IndexFormatTooNewException(indexStream, format, DefaultStoredFieldsWriter.FORMAT_MINIMUM, DefaultStoredFieldsWriter.FORMAT_CURRENT);
 
-      fieldsStream = (IndexInput) cloneableFieldsStream.clone();
-
-      final long indexSize = cloneableIndexStream.length() - FORMAT_SIZE;
+      final long indexSize = indexStream.length() - FORMAT_SIZE;
       
       if (docStoreOffset != -1) {
         // We read only a slice out of this shared fields file
@@ -154,8 +136,6 @@ public final class DefaultStoredFieldsReader extends StoredFieldsReader implemen
           throw new CorruptIndexException("doc counts differ for segment " + segment + ": fieldsReader shows " + this.size + " but segmentInfo shows " + si.docCount);
         }
       }
-
-      indexStream = (IndexInput) cloneableIndexStream.clone();
       numTotalDocs = (int) (indexSize >> 3);
       success = true;
     } finally {
@@ -180,18 +160,14 @@ public final class DefaultStoredFieldsReader extends StoredFieldsReader implemen
   }
 
   /**
-   * Closes the underlying {@link org.apache.lucene.store.IndexInput} streams, including any ones associated with a
-   * lazy implementation of a Field.  This means that the Fields values will not be accessible.
+   * Closes the underlying {@link org.apache.lucene.store.IndexInput} streams.
+   * This means that the Fields values will not be accessible.
    *
    * @throws IOException
    */
   public final void close() throws IOException {
     if (!closed) {
-      if (isOriginal) {
-        IOUtils.close(fieldsStream, indexStream, fieldsStreamTL, cloneableFieldsStream, cloneableIndexStream);
-      } else {
-        IOUtils.close(fieldsStream, indexStream, fieldsStreamTL);
-      }
+      IOUtils.close(fieldsStream, indexStream);
       closed = true;
     }
   }
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/DefaultStoredFieldsWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/DefaultStoredFieldsWriter.java
index d262755..b2ef769 100644
--- a/lucene/src/java/org/apache/lucene/index/codecs/DefaultStoredFieldsWriter.java
+++ b/lucene/src/java/org/apache/lucene/index/codecs/DefaultStoredFieldsWriter.java
@@ -20,6 +20,7 @@ import java.io.IOException;
 
 import org.apache.lucene.document.Document;
 import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.IndexableField;
 import org.apache.lucene.index.MergeState;
@@ -72,8 +73,8 @@ public final class DefaultStoredFieldsWriter extends StoredFieldsWriter {
   /** Extension of stored fields index file */
   public static final String FIELDS_INDEX_EXTENSION = "fdx";
 
-  private Directory directory;
-  private String segment;
+  private final Directory directory;
+  private final String segment;
   private IndexOutput fieldsStream;
   private IndexOutput indexStream;
 
@@ -118,20 +119,19 @@ public final class DefaultStoredFieldsWriter extends StoredFieldsWriter {
   public void abort() {
     try {
       close();
-    } catch (IOException ignored) {
-    }
+    } catch (IOException ignored) {}
+    
     try {
       directory.deleteFile(IndexFileNames.segmentFileName(segment, "", FIELDS_EXTENSION));
-    } catch (IOException ignored) {
-    }
+    } catch (IOException ignored) {}
+    
     try {
       directory.deleteFile(IndexFileNames.segmentFileName(segment, "", FIELDS_INDEX_EXTENSION));
-    } catch (IOException ignored) {
-    }
+    } catch (IOException ignored) {}
   }
 
-  public final void writeField(int fieldNumber, IndexableField field) throws IOException {
-    fieldsStream.writeVInt(fieldNumber);
+  public final void writeField(FieldInfo info, IndexableField field) throws IOException {
+    fieldsStream.writeVInt(info.number);
     int bits = 0;
     final BytesRef bytes;
     final String string;
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/DefaultTermVectorsFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/DefaultTermVectorsFormat.java
new file mode 100644
index 0000000..41fcd02
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/index/codecs/DefaultTermVectorsFormat.java
@@ -0,0 +1,44 @@
+package org.apache.lucene.index.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+public class DefaultTermVectorsFormat extends TermVectorsFormat {
+
+  @Override
+  public TermVectorsReader vectorsReader(Directory directory, SegmentInfo segmentInfo, FieldInfos fieldInfos, IOContext context) throws IOException {
+    return new DefaultTermVectorsReader(directory, segmentInfo, fieldInfos, context);
+  }
+
+  @Override
+  public TermVectorsWriter vectorsWriter(Directory directory, String segment, IOContext context) throws IOException {
+    return new DefaultTermVectorsWriter(directory, segment, context);
+  }
+
+  @Override
+  public void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    DefaultTermVectorsReader.files(dir, info, files);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/DefaultTermVectorsReader.java b/lucene/src/java/org/apache/lucene/index/codecs/DefaultTermVectorsReader.java
new file mode 100644
index 0000000..c02a78c
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/index/codecs/DefaultTermVectorsReader.java
@@ -0,0 +1,736 @@
+package org.apache.lucene.index.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.Comparator;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.Fields;
+import org.apache.lucene.index.FieldsEnum;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.IndexFormatTooNewException;
+import org.apache.lucene.index.IndexFormatTooOldException;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+
+public class DefaultTermVectorsReader extends TermVectorsReader {
+
+  // NOTE: if you make a new format, it must be larger than
+  // the current format
+
+  // Changed strings to UTF8 with length-in-bytes not length-in-chars
+  static final int FORMAT_UTF8_LENGTH_IN_BYTES = 4;
+
+  // NOTE: always change this if you switch to a new format!
+  // whenever you add a new format, make it 1 larger (positive version logic)!
+  static final int FORMAT_CURRENT = FORMAT_UTF8_LENGTH_IN_BYTES;
+  
+  // when removing support for old versions, leave the last supported version here
+  static final int FORMAT_MINIMUM = FORMAT_UTF8_LENGTH_IN_BYTES;
+
+  //The size in bytes that the FORMAT_VERSION will take up at the beginning of each file 
+  static final int FORMAT_SIZE = 4;
+
+  static final byte STORE_POSITIONS_WITH_TERMVECTOR = 0x1;
+
+  static final byte STORE_OFFSET_WITH_TERMVECTOR = 0x2;
+  
+  /** Extension of vectors fields file */
+  static final String VECTORS_FIELDS_EXTENSION = "tvf";
+
+  /** Extension of vectors documents file */
+  static final String VECTORS_DOCUMENTS_EXTENSION = "tvd";
+
+  /** Extension of vectors index file */
+  static final String VECTORS_INDEX_EXTENSION = "tvx";
+
+  private FieldInfos fieldInfos;
+
+  private IndexInput tvx;
+  private IndexInput tvd;
+  private IndexInput tvf;
+  private int size;
+  private int numTotalDocs;
+
+  // The docID offset where our docs begin in the index
+  // file.  This will be 0 if we have our own private file.
+  private int docStoreOffset;
+  
+  private final int format;
+
+  // used by clone
+  DefaultTermVectorsReader(FieldInfos fieldInfos, IndexInput tvx, IndexInput tvd, IndexInput tvf, int size, int numTotalDocs, int docStoreOffset, int format) {
+    this.fieldInfos = fieldInfos;
+    this.tvx = tvx;
+    this.tvd = tvd;
+    this.tvf = tvf;
+    this.size = size;
+    this.numTotalDocs = numTotalDocs;
+    this.docStoreOffset = docStoreOffset;
+    this.format = format;
+  }
+    
+  public DefaultTermVectorsReader(Directory d, SegmentInfo si, FieldInfos fieldInfos, IOContext context)
+    throws CorruptIndexException, IOException {
+    final String segment = si.getDocStoreSegment();
+    final int docStoreOffset = si.getDocStoreOffset();
+    final int size = si.docCount;
+    
+    boolean success = false;
+
+    try {
+      String idxName = IndexFileNames.segmentFileName(segment, "", VECTORS_INDEX_EXTENSION);
+      tvx = d.openInput(idxName, context);
+      format = checkValidFormat(tvx);
+      String fn = IndexFileNames.segmentFileName(segment, "", VECTORS_DOCUMENTS_EXTENSION);
+      tvd = d.openInput(fn, context);
+      final int tvdFormat = checkValidFormat(tvd);
+      fn = IndexFileNames.segmentFileName(segment, "", VECTORS_FIELDS_EXTENSION);
+      tvf = d.openInput(fn, context);
+      final int tvfFormat = checkValidFormat(tvf);
+
+      assert format == tvdFormat;
+      assert format == tvfFormat;
+
+      numTotalDocs = (int) (tvx.length() >> 4);
+
+      if (-1 == docStoreOffset) {
+        this.docStoreOffset = 0;
+        this.size = numTotalDocs;
+        assert size == 0 || numTotalDocs == size;
+      } else {
+        this.docStoreOffset = docStoreOffset;
+        this.size = size;
+        // Verify the file is long enough to hold all of our
+        // docs
+        assert numTotalDocs >= size + docStoreOffset: "numTotalDocs=" + numTotalDocs + " size=" + size + " docStoreOffset=" + docStoreOffset;
+      }
+
+      this.fieldInfos = fieldInfos;
+      success = true;
+    } finally {
+      // With lock-less commits, it's entirely possible (and
+      // fine) to hit a FileNotFound exception above. In
+      // this case, we want to explicitly close any subset
+      // of things that were opened so that we don't have to
+      // wait for a GC to do so.
+      if (!success) {
+        close();
+      }
+    }
+  }
+
+  // Used for bulk copy when merging
+  IndexInput getTvdStream() {
+    return tvd;
+  }
+
+  // Used for bulk copy when merging
+  IndexInput getTvfStream() {
+    return tvf;
+  }
+
+  private void seekTvx(final int docNum) throws IOException {
+    tvx.seek((docNum + docStoreOffset) * 16L + FORMAT_SIZE);
+  }
+
+  boolean canReadRawDocs() {
+    // we can always read raw docs, unless the term vectors
+    // didn't exist
+    return format != 0;
+  }
+
+  /** Retrieve the length (in bytes) of the tvd and tvf
+   *  entries for the next numDocs starting with
+   *  startDocID.  This is used for bulk copying when
+   *  merging segments, if the field numbers are
+   *  congruent.  Once this returns, the tvf & tvd streams
+   *  are seeked to the startDocID. */
+  final void rawDocs(int[] tvdLengths, int[] tvfLengths, int startDocID, int numDocs) throws IOException {
+
+    if (tvx == null) {
+      Arrays.fill(tvdLengths, 0);
+      Arrays.fill(tvfLengths, 0);
+      return;
+    }
+
+    seekTvx(startDocID);
+
+    long tvdPosition = tvx.readLong();
+    tvd.seek(tvdPosition);
+
+    long tvfPosition = tvx.readLong();
+    tvf.seek(tvfPosition);
+
+    long lastTvdPosition = tvdPosition;
+    long lastTvfPosition = tvfPosition;
+
+    int count = 0;
+    while (count < numDocs) {
+      final int docID = docStoreOffset + startDocID + count + 1;
+      assert docID <= numTotalDocs;
+      if (docID < numTotalDocs)  {
+        tvdPosition = tvx.readLong();
+        tvfPosition = tvx.readLong();
+      } else {
+        tvdPosition = tvd.length();
+        tvfPosition = tvf.length();
+        assert count == numDocs-1;
+      }
+      tvdLengths[count] = (int) (tvdPosition-lastTvdPosition);
+      tvfLengths[count] = (int) (tvfPosition-lastTvfPosition);
+      count++;
+      lastTvdPosition = tvdPosition;
+      lastTvfPosition = tvfPosition;
+    }
+  }
+
+  private int checkValidFormat(IndexInput in) throws CorruptIndexException, IOException
+  {
+    int format = in.readInt();
+    if (format < FORMAT_MINIMUM)
+      throw new IndexFormatTooOldException(in, format, FORMAT_MINIMUM, FORMAT_CURRENT);
+    if (format > FORMAT_CURRENT)
+      throw new IndexFormatTooNewException(in, format, FORMAT_MINIMUM, FORMAT_CURRENT);
+    return format;
+  }
+
+  public void close() throws IOException {
+    IOUtils.close(tvx, tvd, tvf);
+  }
+
+  /**
+   * 
+   * @return The number of documents in the reader
+   */
+  int size() {
+    return size;
+  }
+
+  private class TVFields extends Fields {
+    private final int[] fieldNumbers;
+    private final long[] fieldFPs;
+    private final Map<Integer,Integer> fieldNumberToIndex = new HashMap<Integer,Integer>();
+
+    public TVFields(int docID) throws IOException {
+      seekTvx(docID);
+      tvd.seek(tvx.readLong());
+      
+      final int fieldCount = tvd.readVInt();
+      assert fieldCount >= 0;
+      if (fieldCount != 0) {
+        fieldNumbers = new int[fieldCount];
+        fieldFPs = new long[fieldCount];
+        for(int fieldUpto=0;fieldUpto<fieldCount;fieldUpto++) {
+          final int fieldNumber = tvd.readVInt();
+          fieldNumbers[fieldUpto] = fieldNumber;
+          fieldNumberToIndex.put(fieldNumber, fieldUpto);
+        }
+
+        long position = tvx.readLong();
+        fieldFPs[0] = position;
+        for(int fieldUpto=1;fieldUpto<fieldCount;fieldUpto++) {
+          position += tvd.readVLong();
+          fieldFPs[fieldUpto] = position;
+        }
+      } else {
+        // TODO: we can improve writer here, eg write 0 into
+        // tvx file, so we know on first read from tvx that
+        // this doc has no TVs
+        fieldNumbers = null;
+        fieldFPs = null;
+      }
+    }
+    
+    @Override
+    public FieldsEnum iterator() throws IOException {
+
+      return new FieldsEnum() {
+        private int fieldUpto;
+
+        @Override
+        public String next() throws IOException {
+          if (fieldNumbers != null && fieldUpto < fieldNumbers.length) {
+            return fieldInfos.fieldName(fieldNumbers[fieldUpto++]);
+          } else {
+            return null;
+          }
+        }
+
+        @Override
+        public Terms terms() throws IOException {
+          return TVFields.this.terms(fieldInfos.fieldName(fieldNumbers[fieldUpto-1]));
+        }
+      };
+    }
+
+    @Override
+    public Terms terms(String field) throws IOException {
+      final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
+      if (fieldInfo == null) {
+        // No such field
+        return null;
+      }
+
+      final Integer fieldIndex = fieldNumberToIndex.get(fieldInfo.number);
+      if (fieldIndex == null) {
+        // Term vectors were not indexed for this field
+        return null;
+      }
+
+      return new TVTerms(fieldFPs[fieldIndex]);
+    }
+
+    @Override
+    public int getUniqueFieldCount() {
+      if (fieldNumbers == null) {
+        return 0;
+      } else {
+        return fieldNumbers.length;
+      }
+    }
+  }
+
+  private class TVTerms extends Terms {
+    private final int numTerms;
+    private final long tvfFPStart;
+
+    public TVTerms(long tvfFP) throws IOException {
+      tvf.seek(tvfFP);
+      numTerms = tvf.readVInt();
+      tvfFPStart = tvf.getFilePointer();
+    }
+
+    @Override
+    public TermsEnum iterator(TermsEnum reuse) throws IOException {
+      TVTermsEnum termsEnum;
+      if (reuse instanceof TVTermsEnum) {
+        termsEnum = (TVTermsEnum) reuse;
+        if (!termsEnum.canReuse(tvf)) {
+          termsEnum = new TVTermsEnum();
+        }
+      } else {
+        termsEnum = new TVTermsEnum();
+      }
+      termsEnum.reset(numTerms, tvfFPStart);
+      return termsEnum;
+    }
+
+    @Override
+    public long getUniqueTermCount() {
+      return numTerms;
+    }
+
+    @Override
+    public long getSumTotalTermFreq() {
+      return -1;
+    }
+
+    @Override
+    public long getSumDocFreq() {
+      // Every term occurs in just one doc:
+      return numTerms;
+    }
+
+    @Override
+    public int getDocCount() {
+      return 1;
+    }
+
+    @Override
+    public Comparator<BytesRef> getComparator() {
+      // TODO: really indexer hardwires
+      // this...?  I guess codec could buffer and re-sort...
+      return BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+  }
+
+  private class TVTermsEnum extends TermsEnum {
+    private final IndexInput origTVF;
+    private final IndexInput tvf;
+    private int numTerms;
+    private int nextTerm;
+    private int freq;
+    private BytesRef lastTerm = new BytesRef();
+    private BytesRef term = new BytesRef();
+    private boolean storePositions;
+    private boolean storeOffsets;
+    private long tvfFP;
+
+    private int[] positions;
+    private int[] startOffsets;
+    private int[] endOffsets;
+
+    // NOTE: tvf is pre-positioned by caller
+    public TVTermsEnum() throws IOException {
+      this.origTVF = DefaultTermVectorsReader.this.tvf;
+      tvf = (IndexInput) origTVF.clone();
+    }
+
+    public boolean canReuse(IndexInput tvf) {
+      return tvf == origTVF;
+    }
+
+    public void reset(int numTerms, long tvfFPStart) throws IOException {
+      this.numTerms = numTerms;
+      nextTerm = 0;
+      tvf.seek(tvfFPStart);
+      final byte bits = tvf.readByte();
+      storePositions = (bits & STORE_POSITIONS_WITH_TERMVECTOR) != 0;
+      storeOffsets = (bits & STORE_OFFSET_WITH_TERMVECTOR) != 0;
+      tvfFP = 1+tvfFPStart;
+      positions = null;
+      startOffsets = null;
+      endOffsets = null;
+    }
+
+    // NOTE: slow!  (linear scan)
+    @Override
+    public SeekStatus seekCeil(BytesRef text, boolean useCache)
+      throws IOException {
+      if (nextTerm != 0 && text.compareTo(term) < 0) {
+        nextTerm = 0;
+        tvf.seek(tvfFP);
+      }
+
+      while (next() != null) {
+        final int cmp = text.compareTo(term);
+        if (cmp < 0) {
+          return SeekStatus.NOT_FOUND;
+        } else if (cmp == 0) {
+          return SeekStatus.FOUND;
+        }
+      }
+
+      return SeekStatus.END;
+    }
+
+    @Override
+    public void seekExact(long ord) {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public BytesRef next() throws IOException {
+      if (nextTerm >= numTerms) {
+        return null;
+      }
+      term.copy(lastTerm);
+      final int start = tvf.readVInt();
+      final int deltaLen = tvf.readVInt();
+      term.length = start + deltaLen;
+      term.grow(term.length);
+      tvf.readBytes(term.bytes, start, deltaLen);
+      freq = tvf.readVInt();
+
+      if (storePositions) {
+        // TODO: we could maybe reuse last array, if we can
+        // somehow be careful about consumer never using two
+        // D&PEnums at once...
+        positions = new int[freq];
+        int pos = 0;
+        for(int posUpto=0;posUpto<freq;posUpto++) {
+          pos += tvf.readVInt();
+          positions[posUpto] = pos;
+        }
+      }
+
+      if (storeOffsets) {
+        startOffsets = new int[freq];
+        endOffsets = new int[freq];
+        int offset = 0;
+        for(int posUpto=0;posUpto<freq;posUpto++) {
+          startOffsets[posUpto] = offset + tvf.readVInt();
+          offset = endOffsets[posUpto] = startOffsets[posUpto] + tvf.readVInt();
+        }
+      }
+
+      lastTerm.copy(term);
+      nextTerm++;
+      return term;
+    }
+
+    @Override
+    public BytesRef term() {
+      return term;
+    }
+
+    @Override
+    public long ord() {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public int docFreq() {
+      return 1;
+    }
+
+    @Override
+    public long totalTermFreq() {
+      return freq;
+    }
+
+    @Override
+    public DocsEnum docs(Bits liveDocs, DocsEnum reuse) throws IOException {
+      TVDocsEnum docsEnum;
+      if (reuse != null && reuse instanceof TVDocsEnum) {
+        docsEnum = (TVDocsEnum) reuse;
+      } else {
+        docsEnum = new TVDocsEnum();
+      }
+      docsEnum.reset(liveDocs, freq);
+      return docsEnum;
+    }
+
+    @Override
+    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse) throws IOException {
+      if (!storePositions && !storeOffsets) {
+        return null;
+      }
+      
+      TVDocsAndPositionsEnum docsAndPositionsEnum;
+      if (reuse != null) {
+        docsAndPositionsEnum = (TVDocsAndPositionsEnum) reuse;
+        if (docsAndPositionsEnum.canReuse(storeOffsets)) {
+          docsAndPositionsEnum = (TVDocsAndPositionsEnum) reuse;
+        } else {
+          docsAndPositionsEnum = new TVDocsAndPositionsEnum(storeOffsets);
+        }
+      } else {
+        docsAndPositionsEnum = new TVDocsAndPositionsEnum(storeOffsets);
+      }
+      docsAndPositionsEnum.reset(liveDocs, positions, startOffsets, endOffsets);
+      return docsAndPositionsEnum;
+    }
+
+    @Override
+    public Comparator<BytesRef> getComparator() {
+      // TODO: really indexer hardwires
+      // this...?  I guess codec could buffer and re-sort...
+      return BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+  }
+
+  // NOTE: sort of a silly class, since you can get the
+  // freq() already by TermsEnum.totalTermFreq
+  private static class TVDocsEnum extends DocsEnum {
+    private boolean didNext;
+    private int freq;
+    private Bits liveDocs;
+
+    @Override
+    public int freq() {
+      return freq;
+    }
+
+    @Override
+    public int docID() {
+      return 0;
+    }
+
+    @Override
+    public int nextDoc() {
+      if (!didNext && (liveDocs == null || liveDocs.get(0))) {
+        didNext = true;
+        return 0;
+      } else {
+        return NO_MORE_DOCS;
+      }
+    }
+
+    @Override
+    public int advance(int target) {
+      if (!didNext && target == 0) {
+        return nextDoc();
+      } else {
+        return NO_MORE_DOCS;
+      }
+    }
+
+    public void reset(Bits liveDocs, int freq) {
+      this.liveDocs = liveDocs;
+      this.freq = freq;
+      didNext = false;
+    }
+  }
+
+  private static class TVDocsAndPositionsEnum extends DocsAndPositionsEnum {
+    private final OffsetAttribute offsetAtt;
+    private boolean didNext;
+    private int nextPos;
+    private Bits liveDocs;
+    private int[] positions;
+    private int[] startOffsets;
+    private int[] endOffsets;
+
+    public TVDocsAndPositionsEnum(boolean storeOffsets) {
+      if (storeOffsets) {
+        offsetAtt = attributes().addAttribute(OffsetAttribute.class);
+      } else {
+        offsetAtt = null;
+      }
+    }
+
+    public boolean canReuse(boolean storeOffsets) {
+      return storeOffsets == (offsetAtt != null);
+    }
+
+    @Override
+    public int freq() {
+      if (positions != null) {
+        return positions.length;
+      } else {
+        assert startOffsets != null;
+        return startOffsets.length;
+      }
+    }
+
+    @Override
+    public int docID() {
+      return 0;
+    }
+
+    @Override
+    public int nextDoc() {
+      if (!didNext && (liveDocs == null || liveDocs.get(0))) {
+        didNext = true;
+        return 0;
+      } else {
+        return NO_MORE_DOCS;
+      }
+    }
+
+    @Override
+    public int advance(int target) {
+      if (!didNext && target == 0) {
+        return nextDoc();
+      } else {
+        return NO_MORE_DOCS;
+      }
+    }
+
+    public void reset(Bits liveDocs, int[] positions, int[] startOffsets, int[] endOffsets) {
+      this.liveDocs = liveDocs;
+      this.positions = positions;
+      this.startOffsets = startOffsets;
+      assert (offsetAtt != null) == (startOffsets != null);
+      this.endOffsets = endOffsets;
+      didNext = false;
+      nextPos = 0;
+    }
+
+    @Override
+    public BytesRef getPayload() {
+      return null;
+    }
+
+    @Override
+    public boolean hasPayload() {
+      return false;
+    }
+
+    @Override
+    public int nextPosition() {
+      assert (positions != null && nextPos < positions.length) ||
+        startOffsets != null && nextPos < startOffsets.length;
+
+      if (startOffsets != null) {
+        offsetAtt.setOffset(startOffsets[nextPos],
+                            endOffsets[nextPos]);
+      }
+      if (positions != null) {
+        return positions[nextPos++];
+      } else {
+        nextPos++;
+        return -1;
+      }
+    }
+  }
+
+  @Override
+  public Fields get(int docID) throws IOException {
+    if (docID < 0 || docID >= numTotalDocs) {
+      throw new IllegalArgumentException("doID=" + docID + " is out of bounds [0.." + (numTotalDocs-1) + "]");
+    }
+    if (tvx != null) {
+      Fields fields = new TVFields(docID);
+      if (fields.getUniqueFieldCount() == 0) {
+        // TODO: we can improve writer here, eg write 0 into
+        // tvx file, so we know on first read from tvx that
+        // this doc has no TVs
+        return null;
+      } else {
+        return fields;
+      }
+    } else {
+      return null;
+    }
+  }
+
+  @Override
+  public TermVectorsReader clone() {
+    IndexInput cloneTvx = null;
+    IndexInput cloneTvd = null;
+    IndexInput cloneTvf = null;
+
+    // These are null when a TermVectorsReader was created
+    // on a segment that did not have term vectors saved
+    if (tvx != null && tvd != null && tvf != null) {
+      cloneTvx = (IndexInput) tvx.clone();
+      cloneTvd = (IndexInput) tvd.clone();
+      cloneTvf = (IndexInput) tvf.clone();
+    }
+    
+    return new DefaultTermVectorsReader(fieldInfos, cloneTvx, cloneTvd, cloneTvf, size, numTotalDocs, docStoreOffset, format);
+  }
+  
+  public static void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    if (info.getHasVectors()) {
+      if (info.getDocStoreOffset() != -1) {
+        assert info.getDocStoreSegment() != null;
+        if (!info.getDocStoreIsCompoundFile()) {
+          files.add(IndexFileNames.segmentFileName(info.getDocStoreSegment(), "", VECTORS_INDEX_EXTENSION));
+          files.add(IndexFileNames.segmentFileName(info.getDocStoreSegment(), "", VECTORS_FIELDS_EXTENSION));
+          files.add(IndexFileNames.segmentFileName(info.getDocStoreSegment(), "", VECTORS_DOCUMENTS_EXTENSION));
+        }
+      } else {
+        files.add(IndexFileNames.segmentFileName(info.name, "", VECTORS_INDEX_EXTENSION));
+        files.add(IndexFileNames.segmentFileName(info.name, "", VECTORS_FIELDS_EXTENSION));
+        files.add(IndexFileNames.segmentFileName(info.name, "", VECTORS_DOCUMENTS_EXTENSION));
+      }
+    }
+  }
+}
+
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/DefaultTermVectorsWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/DefaultTermVectorsWriter.java
new file mode 100644
index 0000000..1219c9d
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/index/codecs/DefaultTermVectorsWriter.java
@@ -0,0 +1,376 @@
+package org.apache.lucene.index.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.Fields;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.MergePolicy.MergeAbortedException;
+import org.apache.lucene.index.MergeState;
+import org.apache.lucene.index.SegmentReader;
+import org.apache.lucene.store.DataInput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.StringHelper;
+
+// TODO: make a new 4.0 TV format that encodes better
+//   - use startOffset (not endOffset) as base for delta on
+//     next startOffset because today for syns or ngrams or
+//     WDF or shingles etc. we are encoding negative vints
+//     (= slow, 5 bytes per)
+//   - if doc has no term vectors, write 0 into the tvx
+//     file; saves a seek to tvd only to read a 0 vint (and
+//     saves a byte in tvd)
+
+public final class DefaultTermVectorsWriter extends TermVectorsWriter {
+  private final Directory directory;
+  private final String segment;
+  private IndexOutput tvx = null, tvd = null, tvf = null;
+
+  public DefaultTermVectorsWriter(Directory directory, String segment, IOContext context) throws IOException {
+    this.directory = directory;
+    this.segment = segment;
+    boolean success = false;
+    try {
+      // Open files for TermVector storage
+      tvx = directory.createOutput(IndexFileNames.segmentFileName(segment, "", DefaultTermVectorsReader.VECTORS_INDEX_EXTENSION), context);
+      tvx.writeInt(DefaultTermVectorsReader.FORMAT_CURRENT);
+      tvd = directory.createOutput(IndexFileNames.segmentFileName(segment, "", DefaultTermVectorsReader.VECTORS_DOCUMENTS_EXTENSION), context);
+      tvd.writeInt(DefaultTermVectorsReader.FORMAT_CURRENT);
+      tvf = directory.createOutput(IndexFileNames.segmentFileName(segment, "", DefaultTermVectorsReader.VECTORS_FIELDS_EXTENSION), context);
+      tvf.writeInt(DefaultTermVectorsReader.FORMAT_CURRENT);
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(tvx, tvd, tvf);
+      }
+    }
+  }
+ 
+  @Override
+  public void startDocument(int numVectorFields) throws IOException {
+    lastFieldName = null;
+    this.numVectorFields = numVectorFields;
+    tvx.writeLong(tvd.getFilePointer());
+    tvx.writeLong(tvf.getFilePointer());
+    tvd.writeVInt(numVectorFields);
+    fieldCount = 0;
+    fps = ArrayUtil.grow(fps, numVectorFields);
+  }
+  
+  private long fps[] = new long[10]; // pointers to the tvf before writing each field 
+  private int fieldCount = 0;        // number of fields we have written so far for this document
+  private int numVectorFields = 0;   // total number of fields we will write for this document
+  private String lastFieldName;
+
+  @Override
+  public void startField(FieldInfo info, int numTerms, boolean positions, boolean offsets) throws IOException {
+    assert lastFieldName == null || info.name.compareTo(lastFieldName) > 0: "fieldName=" + info.name + " lastFieldName=" + lastFieldName;
+    lastFieldName = info.name;
+    this.positions = positions;
+    this.offsets = offsets;
+    lastTerm.length = 0;
+    fps[fieldCount++] = tvf.getFilePointer();
+    tvd.writeVInt(info.number);
+    tvf.writeVInt(numTerms);
+    byte bits = 0x0;
+    if (positions)
+      bits |= DefaultTermVectorsReader.STORE_POSITIONS_WITH_TERMVECTOR;
+    if (offsets)
+      bits |= DefaultTermVectorsReader.STORE_OFFSET_WITH_TERMVECTOR;
+    tvf.writeByte(bits);
+    
+    assert fieldCount <= numVectorFields;
+    if (fieldCount == numVectorFields) {
+      // last field of the document
+      // this is crazy because the file format is crazy!
+      for (int i = 1; i < fieldCount; i++) {
+        tvd.writeVLong(fps[i] - fps[i-1]);
+      }
+    }
+  }
+  
+  private final BytesRef lastTerm = new BytesRef(10);
+
+  // NOTE: we override addProx, so we don't need to buffer when indexing.
+  // we also don't buffer during bulk merges.
+  private int offsetStartBuffer[] = new int[10];
+  private int offsetEndBuffer[] = new int[10];
+  private int offsetIndex = 0;
+  private int offsetFreq = 0;
+  private boolean positions = false;
+  private boolean offsets = false;
+
+  @Override
+  public void startTerm(BytesRef term, int freq) throws IOException {
+    final int prefix = StringHelper.bytesDifference(lastTerm.bytes, lastTerm.offset, lastTerm.length, 
+                                                    term.bytes, term.offset, term.length);
+    final int suffix = term.length - prefix;
+    tvf.writeVInt(prefix);
+    tvf.writeVInt(suffix);
+    tvf.writeBytes(term.bytes, term.offset + prefix, suffix);
+    tvf.writeVInt(freq);
+    lastTerm.copy(term);
+    lastPosition = lastOffset = 0;
+    
+    if (offsets && positions) {
+      // we might need to buffer if its a non-bulk merge
+      offsetStartBuffer = ArrayUtil.grow(offsetStartBuffer, freq);
+      offsetEndBuffer = ArrayUtil.grow(offsetEndBuffer, freq);
+      offsetIndex = 0;
+      offsetFreq = freq;
+    }
+  }
+
+  int lastPosition = 0;
+  int lastOffset = 0;
+
+  @Override
+  public void addProx(int numProx, DataInput positions, DataInput offsets) throws IOException {
+    // TODO: technically we could just copy bytes and not re-encode if we knew the length...
+    if (positions != null) {
+      for (int i = 0; i < numProx; i++) {
+        tvf.writeVInt(positions.readVInt());
+      }
+    }
+    
+    if (offsets != null) {
+      for (int i = 0; i < numProx; i++) {
+        tvf.writeVInt(offsets.readVInt());
+        tvf.writeVInt(offsets.readVInt());
+      }
+    }
+  }
+
+  @Override
+  public void addPosition(int position, int startOffset, int endOffset) throws IOException {
+    if (positions && offsets) {
+      // write position delta
+      tvf.writeVInt(position - lastPosition);
+      lastPosition = position;
+      
+      // buffer offsets
+      offsetStartBuffer[offsetIndex] = startOffset;
+      offsetEndBuffer[offsetIndex] = endOffset;
+      offsetIndex++;
+      
+      // dump buffer if we are done
+      if (offsetIndex == offsetFreq) {
+        for (int i = 0; i < offsetIndex; i++) {
+          tvf.writeVInt(offsetStartBuffer[i] - lastOffset);
+          tvf.writeVInt(offsetEndBuffer[i] - offsetStartBuffer[i]);
+          lastOffset = offsetEndBuffer[i];
+        }
+      }
+    } else if (positions) {
+      // write position delta
+      tvf.writeVInt(position - lastPosition);
+      lastPosition = position;
+    } else if (offsets) {
+      // write offset deltas
+      tvf.writeVInt(startOffset - lastOffset);
+      tvf.writeVInt(endOffset - startOffset);
+      lastOffset = endOffset;
+    }
+  }
+
+  @Override
+  public void abort() {
+    try {
+      close();
+    } catch (IOException ignored) {}
+    
+    try {
+      directory.deleteFile(IndexFileNames.segmentFileName(segment, "", DefaultTermVectorsReader.VECTORS_INDEX_EXTENSION));
+    } catch (IOException ignored) {}
+    
+    try {
+      directory.deleteFile(IndexFileNames.segmentFileName(segment, "", DefaultTermVectorsReader.VECTORS_DOCUMENTS_EXTENSION));
+    } catch (IOException ignored) {}
+    
+    try {
+      directory.deleteFile(IndexFileNames.segmentFileName(segment, "", DefaultTermVectorsReader.VECTORS_FIELDS_EXTENSION));
+    } catch (IOException ignored) {}
+  }
+
+  /**
+   * Do a bulk copy of numDocs documents from reader to our
+   * streams.  This is used to expedite merging, if the
+   * field numbers are congruent.
+   */
+  private void addRawDocuments(DefaultTermVectorsReader reader, int[] tvdLengths, int[] tvfLengths, int numDocs) throws IOException {
+    long tvdPosition = tvd.getFilePointer();
+    long tvfPosition = tvf.getFilePointer();
+    long tvdStart = tvdPosition;
+    long tvfStart = tvfPosition;
+    for(int i=0;i<numDocs;i++) {
+      tvx.writeLong(tvdPosition);
+      tvdPosition += tvdLengths[i];
+      tvx.writeLong(tvfPosition);
+      tvfPosition += tvfLengths[i];
+    }
+    tvd.copyBytes(reader.getTvdStream(), tvdPosition-tvdStart);
+    tvf.copyBytes(reader.getTvfStream(), tvfPosition-tvfStart);
+    assert tvd.getFilePointer() == tvdPosition;
+    assert tvf.getFilePointer() == tvfPosition;
+  }
+
+  @Override
+  public final int merge(MergeState mergeState) throws IOException {
+    // Used for bulk-reading raw bytes for term vectors
+    int rawDocLengths[] = new int[MAX_RAW_MERGE_DOCS];
+    int rawDocLengths2[] = new int[MAX_RAW_MERGE_DOCS];
+
+    int idx = 0;
+    int numDocs = 0;
+    for (final MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {
+      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];
+      DefaultTermVectorsReader matchingVectorsReader = null;
+      if (matchingSegmentReader != null) {
+        TermVectorsReader vectorsReader = matchingSegmentReader.getTermVectorsReader();
+
+        if (vectorsReader != null && vectorsReader instanceof DefaultTermVectorsReader) {
+          // If the TV* files are an older format then they cannot read raw docs:
+          if (((DefaultTermVectorsReader)vectorsReader).canReadRawDocs()) {
+            matchingVectorsReader = (DefaultTermVectorsReader) vectorsReader;
+          }
+        }
+      }
+      if (reader.liveDocs != null) {
+        numDocs += copyVectorsWithDeletions(mergeState, matchingVectorsReader, reader, rawDocLengths, rawDocLengths2);
+      } else {
+        numDocs += copyVectorsNoDeletions(mergeState, matchingVectorsReader, reader, rawDocLengths, rawDocLengths2);
+      }
+    }
+    finish(numDocs);
+    return numDocs;
+  }
+
+  /** Maximum number of contiguous documents to bulk-copy
+      when merging term vectors */
+  private final static int MAX_RAW_MERGE_DOCS = 4192;
+
+  private int copyVectorsWithDeletions(MergeState mergeState,
+                                        final DefaultTermVectorsReader matchingVectorsReader,
+                                        final MergeState.IndexReaderAndLiveDocs reader,
+                                        int rawDocLengths[],
+                                        int rawDocLengths2[])
+          throws IOException, MergeAbortedException {
+    final int maxDoc = reader.reader.maxDoc();
+    final Bits liveDocs = reader.liveDocs;
+    int totalNumDocs = 0;
+    if (matchingVectorsReader != null) {
+      // We can bulk-copy because the fieldInfos are "congruent"
+      for (int docNum = 0; docNum < maxDoc;) {
+        if (!liveDocs.get(docNum)) {
+          // skip deleted docs
+          ++docNum;
+          continue;
+        }
+        // We can optimize this case (doing a bulk byte copy) since the field
+        // numbers are identical
+        int start = docNum, numDocs = 0;
+        do {
+          docNum++;
+          numDocs++;
+          if (docNum >= maxDoc) break;
+          if (!liveDocs.get(docNum)) {
+            docNum++;
+            break;
+          }
+        } while(numDocs < MAX_RAW_MERGE_DOCS);
+        
+        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, start, numDocs);
+        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, numDocs);
+        totalNumDocs += numDocs;
+        mergeState.checkAbort.work(300 * numDocs);
+      }
+    } else {
+      for (int docNum = 0; docNum < maxDoc; docNum++) {
+        if (!liveDocs.get(docNum)) {
+          // skip deleted docs
+          continue;
+        }
+        
+        // NOTE: it's very important to first assign to vectors then pass it to
+        // termVectorsWriter.addAllDocVectors; see LUCENE-1282
+        Fields vectors = reader.reader.getTermVectors(docNum);
+        addAllDocVectors(vectors, mergeState.fieldInfos);
+        totalNumDocs++;
+        mergeState.checkAbort.work(300);
+      }
+    }
+    return totalNumDocs;
+  }
+  
+  private int copyVectorsNoDeletions(MergeState mergeState,
+                                      final DefaultTermVectorsReader matchingVectorsReader,
+                                      final MergeState.IndexReaderAndLiveDocs reader,
+                                      int rawDocLengths[],
+                                      int rawDocLengths2[])
+          throws IOException, MergeAbortedException {
+    final int maxDoc = reader.reader.maxDoc();
+    if (matchingVectorsReader != null) {
+      // We can bulk-copy because the fieldInfos are "congruent"
+      int docCount = 0;
+      while (docCount < maxDoc) {
+        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);
+        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, docCount, len);
+        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, len);
+        docCount += len;
+        mergeState.checkAbort.work(300 * len);
+      }
+    } else {
+      for (int docNum = 0; docNum < maxDoc; docNum++) {
+        // NOTE: it's very important to first assign to vectors then pass it to
+        // termVectorsWriter.addAllDocVectors; see LUCENE-1282
+        Fields vectors = reader.reader.getTermVectors(docNum);
+        addAllDocVectors(vectors, mergeState.fieldInfos);
+        mergeState.checkAbort.work(300);
+      }
+    }
+    return maxDoc;
+  }
+  
+  @Override
+  public void finish(int numDocs) throws IOException {
+    if (4+((long) numDocs)*16 != tvx.getFilePointer())
+      // This is most likely a bug in Sun JRE 1.6.0_04/_05;
+      // we detect that the bug has struck, here, and
+      // throw an exception to prevent the corruption from
+      // entering the index.  See LUCENE-1282 for
+      // details.
+      throw new RuntimeException("mergeVectors produced an invalid result: mergedDocs is " + numDocs + " but tvx size is " + tvx.getFilePointer() + " file=" + tvx.toString() + "; now aborting this merge to prevent index corruption");
+  }
+
+  /** Close all streams. */
+  @Override
+  public void close() throws IOException {
+    // make an effort to close all streams we can but remember and re-throw
+    // the first exception encountered in this process
+    IOUtils.close(tvx, tvd, tvf);
+    tvx = tvd = tvf = null;
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/FieldInfosFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/FieldInfosFormat.java
new file mode 100644
index 0000000..d4b66d7
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/index/codecs/FieldInfosFormat.java
@@ -0,0 +1,33 @@
+package org.apache.lucene.index.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+
+/**
+ * @lucene.experimental
+ */
+public abstract class FieldInfosFormat {
+  public abstract FieldInfosReader getFieldInfosReader() throws IOException;
+  public abstract FieldInfosWriter getFieldInfosWriter() throws IOException;
+  public abstract void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException;
+}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/FieldInfosReader.java b/lucene/src/java/org/apache/lucene/index/codecs/FieldInfosReader.java
new file mode 100644
index 0000000..151b212
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/index/codecs/FieldInfosReader.java
@@ -0,0 +1,31 @@
+package org.apache.lucene.index.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+/**
+ * @lucene.experimental
+ */
+public abstract class FieldInfosReader {
+  public abstract FieldInfos read(Directory directory, String segmentName, IOContext iocontext) throws IOException;
+}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/FieldInfosWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/FieldInfosWriter.java
new file mode 100644
index 0000000..be46a49
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/index/codecs/FieldInfosWriter.java
@@ -0,0 +1,31 @@
+package org.apache.lucene.index.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+/**
+ * @lucene.experimental
+ */
+public abstract class FieldInfosWriter {
+  public abstract void write(Directory directory, String segmentName, FieldInfos infos, IOContext context) throws IOException;
+}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/FieldsConsumer.java b/lucene/src/java/org/apache/lucene/index/codecs/FieldsConsumer.java
index 357e7a2..4abe89d 100644
--- a/lucene/src/java/org/apache/lucene/index/codecs/FieldsConsumer.java
+++ b/lucene/src/java/org/apache/lucene/index/codecs/FieldsConsumer.java
@@ -17,14 +17,14 @@ package org.apache.lucene.index.codecs;
  * limitations under the License.
  */
 
+import java.io.Closeable;
+import java.io.IOException;
+
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.Fields;
 import org.apache.lucene.index.FieldsEnum;
 import org.apache.lucene.index.MergeState;
-import org.apache.lucene.index.TermsEnum;
-
-import java.io.IOException;
-import java.io.Closeable;
+import org.apache.lucene.index.Terms;
 
 /** Abstract API that consumes terms, doc, freq, prox and
  *  payloads postings.  Concrete implementations of this
@@ -48,10 +48,10 @@ public abstract class FieldsConsumer implements Closeable {
     while((field = fieldsEnum.next()) != null) {
       mergeState.fieldInfo = mergeState.fieldInfos.fieldInfo(field);
       assert mergeState.fieldInfo != null : "FieldInfo for field is null: "+ field;
-      TermsEnum terms = fieldsEnum.terms();
+      Terms terms = fieldsEnum.terms();
       if (terms != null) {
         final TermsConsumer termsConsumer = addField(mergeState.fieldInfo);
-        termsConsumer.merge(mergeState, terms);
+        termsConsumer.merge(mergeState, terms.iterator(null));
       }
     }
   }
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/StoredFieldsReader.java b/lucene/src/java/org/apache/lucene/index/codecs/StoredFieldsReader.java
index b88cdd7..91af63b 100644
--- a/lucene/src/java/org/apache/lucene/index/codecs/StoredFieldsReader.java
+++ b/lucene/src/java/org/apache/lucene/index/codecs/StoredFieldsReader.java
@@ -1,11 +1,5 @@
 package org.apache.lucene.index.codecs;
 
-import java.io.Closeable;
-import java.io.IOException;
-
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.StoredFieldVisitor;
-
 /**
  * Copyright 2004 The Apache Software Foundation
  *
@@ -22,6 +16,12 @@ import org.apache.lucene.index.StoredFieldVisitor;
  * the License.
  */
 
+import java.io.Closeable;
+import java.io.IOException;
+
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.StoredFieldVisitor;
+
 /**
  * Codec API for reading stored fields:
  * 
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/StoredFieldsWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/StoredFieldsWriter.java
index 266190c..47237d7 100644
--- a/lucene/src/java/org/apache/lucene/index/codecs/StoredFieldsWriter.java
+++ b/lucene/src/java/org/apache/lucene/index/codecs/StoredFieldsWriter.java
@@ -4,6 +4,7 @@ import java.io.Closeable;
 import java.io.IOException;
 
 import org.apache.lucene.document.Document;
+import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.IndexableField;
 import org.apache.lucene.index.MergeState;
@@ -31,7 +32,7 @@ import org.apache.lucene.util.Bits;
  * <ol>
  *   <li>For every document, {@link #startDocument(int)} is called,
  *       informing the Codec how many fields will be written.
- *   <li>{@link #writeField(int, IndexableField)} is called for 
+ *   <li>{@link #writeField(FieldInfo, IndexableField)} is called for 
  *       each field in the document.
  *   <li>After all documents have been written, {@link #finish(int)} 
  *       is called for verification/sanity-checks.
@@ -43,14 +44,14 @@ import org.apache.lucene.util.Bits;
 public abstract class StoredFieldsWriter implements Closeable {
   
   /** Called before writing the stored fields of the document.
-   *  {@link #writeField(int, IndexableField)} will be called
+   *  {@link #writeField(FieldInfo, IndexableField)} will be called
    *  <code>numStoredFields</code> times. Note that this is
    *  called even if the document has no stored fields, in
    *  this case <code>numStoredFields</code> will be zero. */
   public abstract void startDocument(int numStoredFields) throws IOException;
   
   /** Writes a single stored field. */
-  public abstract void writeField(int fieldNumber, IndexableField field) throws IOException;
+  public abstract void writeField(FieldInfo info, IndexableField field) throws IOException;
 
   /** Aborts writing entirely, implementation should remove
    *  any partially-written files, etc. */
@@ -67,7 +68,7 @@ public abstract class StoredFieldsWriter implements Closeable {
   /** Merges in the stored fields from the readers in 
    *  <code>mergeState</code>. The default implementation skips
    *  over deleted documents, and uses {@link #startDocument(int)},
-   *  {@link #writeField(int, IndexableField)}, and {@link #finish(int)},
+   *  {@link #writeField(FieldInfo, IndexableField)}, and {@link #finish(int)},
    *  returning the number of documents that were written.
    *  Implementations can override this method for more sophisticated
    *  merging (bulk-byte copying, etc). */
@@ -110,7 +111,7 @@ public abstract class StoredFieldsWriter implements Closeable {
 
     for (IndexableField field : doc) {
       if (field.fieldType().stored()) {
-        writeField(fieldInfos.fieldNumber(field.name()), field);
+        writeField(fieldInfos.fieldInfo(field.name()), field);
       }
     }
   }
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/TermVectorsFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/TermVectorsFormat.java
new file mode 100644
index 0000000..ecbf415
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/index/codecs/TermVectorsFormat.java
@@ -0,0 +1,35 @@
+package org.apache.lucene.index.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+/**
+ * Controls the format of term vectors
+ */
+public abstract class TermVectorsFormat {
+  public abstract TermVectorsReader vectorsReader(Directory directory, SegmentInfo segmentInfo, FieldInfos fieldInfos, IOContext context) throws IOException;
+  public abstract TermVectorsWriter vectorsWriter(Directory directory, String segment, IOContext context) throws IOException;
+  public abstract void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException;
+}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/TermVectorsReader.java b/lucene/src/java/org/apache/lucene/index/codecs/TermVectorsReader.java
new file mode 100644
index 0000000..45580c5
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/index/codecs/TermVectorsReader.java
@@ -0,0 +1,43 @@
+package org.apache.lucene.index.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable;
+import java.io.IOException;
+
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute; // javadocs
+import org.apache.lucene.index.DocsAndPositionsEnum; // javadocs
+import org.apache.lucene.index.Fields;
+
+/**
+ * Codec API for reading term vectors:
+ * 
+ * @lucene.experimental
+ */
+public abstract class TermVectorsReader implements Cloneable,Closeable {
+
+  /** Returns term vectors for this document, or null if
+   *  term vectors were not indexed. If offsets are
+   *  available they are in an {@link OffsetAttribute}
+   *  available from the {@link DocsAndPositionsEnum}. */
+  public abstract Fields get(int doc) throws IOException;
+
+  /** Create a clone that one caller at a time may use to
+   *  read term vectors. */
+  public abstract TermVectorsReader clone();
+}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/TermVectorsWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/TermVectorsWriter.java
new file mode 100644
index 0000000..5eeaacc
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/index/codecs/TermVectorsWriter.java
@@ -0,0 +1,278 @@
+package org.apache.lucene.index.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable;
+import java.io.IOException;
+
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.Fields;
+import org.apache.lucene.index.FieldsEnum;
+import org.apache.lucene.index.MergeState;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.DataInput;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+
+/**
+ * Codec API for writing term vectors:
+ * <p>
+ * <ol>
+ *   <li>For every document, {@link #startDocument(int)} is called,
+ *       informing the Codec how many fields will be written.
+ *   <li>{@link #startField(FieldInfo, int, boolean, boolean)} is called for 
+ *       each field in the document, informing the codec how many terms
+ *       will be written for that field, and whether or not positions
+ *       or offsets are enabled.
+ *   <li>Within each field, {@link #startTerm(BytesRef, int)} is called
+ *       for each term.
+ *   <li>If offsets and/or positions are enabled, then 
+ *       {@link #addPosition(int, int, int)} will be called for each term
+ *       occurrence.
+ *   <li>After all documents have been written, {@link #finish(int)} 
+ *       is called for verification/sanity-checks.
+ *   <li>Finally the writer is closed ({@link #close()})
+ * </ol>
+ * 
+ * @lucene.experimental
+ */
+public abstract class TermVectorsWriter implements Closeable {
+  
+  /** Called before writing the term vectors of the document.
+   *  {@link #startField(FieldInfo, int, boolean, boolean)} will 
+   *  be called <code>numVectorFields</code> times. Note that if term 
+   *  vectors are enabled, this is called even if the document 
+   *  has no vector fields, in this case <code>numVectorFields</code> 
+   *  will be zero. */
+  public abstract void startDocument(int numVectorFields) throws IOException;
+  
+  /** Called before writing the terms of the field.
+   *  {@link #startTerm(BytesRef, int)} will be called <code>numTerms</code> times. */
+  public abstract void startField(FieldInfo info, int numTerms, boolean positions, boolean offsets) throws IOException;
+  
+  /** Adds a term and its term frequency <code>freq</code>.
+   * If this field has positions and/or offsets enabled, then
+   * {@link #addPosition(int, int, int)} will be called 
+   * <code>freq</code> times respectively.
+   */
+  public abstract void startTerm(BytesRef term, int freq) throws IOException;
+  
+  /** Adds a term position and offsets */
+  public abstract void addPosition(int position, int startOffset, int endOffset) throws IOException;
+  
+  /** Aborts writing entirely, implementation should remove
+   *  any partially-written files, etc. */
+  public abstract void abort();
+
+  /** Called before {@link #close()}, passing in the number
+   *  of documents that were written. Note that this is 
+   *  intentionally redundant (equivalent to the number of
+   *  calls to {@link #startDocument(int)}, but a Codec should
+   *  check that this is the case to detect the JRE bug described 
+   *  in LUCENE-1282. */
+  public abstract void finish(int numDocs) throws IOException;
+  
+  /** 
+   * Called by IndexWriter when writing new segments.
+   * <p>
+   * This is an expert API that allows the codec to consume 
+   * positions and offsets directly from the indexer.
+   * <p>
+   * The default implementation calls {@link #addPosition(int, int, int)},
+   * but subclasses can override this if they want to efficiently write 
+   * all the positions, then all the offsets, for example.
+   * <p>
+   * NOTE: This API is extremely expert and subject to change or removal!!!
+   * @lucene.internal
+   */
+  // TODO: we should probably nuke this and make a more efficient 4.x format
+  // PreFlex-RW could then be slow and buffer (its only used in tests...)
+  public void addProx(int numProx, DataInput positions, DataInput offsets) throws IOException {
+    int position = 0;
+    int lastOffset = 0;
+
+    for (int i = 0; i < numProx; i++) {
+      final int startOffset;
+      final int endOffset;
+      
+      if (positions == null) {
+        position = -1;
+      } else {
+        position += positions.readVInt();
+      }
+      
+      if (offsets == null) {
+        startOffset = endOffset = -1;
+      } else {
+        startOffset = lastOffset + offsets.readVInt();
+        endOffset = startOffset + offsets.readVInt();
+        lastOffset = endOffset;
+      }
+      addPosition(position, startOffset, endOffset);
+    }
+  }
+  
+  /** Merges in the term vectors from the readers in 
+   *  <code>mergeState</code>. The default implementation skips
+   *  over deleted documents, and uses {@link #startDocument(int)},
+   *  {@link #startField(FieldInfo, int, boolean, boolean)}, 
+   *  {@link #startTerm(BytesRef, int)}, {@link #addPosition(int, int, int)},
+   *  and {@link #finish(int)},
+   *  returning the number of documents that were written.
+   *  Implementations can override this method for more sophisticated
+   *  merging (bulk-byte copying, etc). */
+  public int merge(MergeState mergeState) throws IOException {
+    int docCount = 0;
+    for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {
+      final int maxDoc = reader.reader.maxDoc();
+      final Bits liveDocs = reader.liveDocs;
+      for (int docID = 0; docID < maxDoc; docID++) {
+        if (liveDocs != null && !liveDocs.get(docID)) {
+          // skip deleted docs
+          continue;
+        }
+        // NOTE: it's very important to first assign to vectors then pass it to
+        // termVectorsWriter.addAllDocVectors; see LUCENE-1282
+        Fields vectors = reader.reader.getTermVectors(docID);
+        addAllDocVectors(vectors, mergeState.fieldInfos);
+        docCount++;
+        mergeState.checkAbort.work(300);
+      }
+    }
+    finish(docCount);
+    return docCount;
+  }
+  
+  /** Safe (but, slowish) default method to write every
+   *  vector field in the document.  This default
+   *  implementation requires that the vectors implement
+   *  both Fields.getUniqueFieldCount and
+   *  Terms.getUniqueTermCount. */
+  protected final void addAllDocVectors(Fields vectors, FieldInfos fieldInfos) throws IOException {
+    if (vectors == null) {
+      startDocument(0);
+      return;
+    }
+
+    final int numFields = vectors.getUniqueFieldCount();
+    if (numFields == -1) {
+      throw new IllegalStateException("vectors.getUniqueFieldCount() must be implemented (it returned -1)");
+    }
+    startDocument(numFields);
+    
+    final FieldsEnum fieldsEnum = vectors.iterator();
+    String fieldName;
+    String lastFieldName = null;
+
+    while((fieldName = fieldsEnum.next()) != null) {
+      
+      final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldName);
+
+      assert lastFieldName == null || fieldName.compareTo(lastFieldName) > 0: "lastFieldName=" + lastFieldName + " fieldName=" + fieldName;
+      lastFieldName = fieldName;
+
+      final Terms terms = fieldsEnum.terms();
+      if (terms == null) {
+        // FieldsEnum shouldn't lie...
+        continue;
+      }
+      final int numTerms = (int) terms.getUniqueTermCount();
+      if (numTerms == -1) {
+        throw new IllegalStateException("vector.getUniqueTermCount() must be implemented (it returned -1)");
+      }
+
+      final boolean positions;
+
+      OffsetAttribute offsetAtt;
+
+      final TermsEnum termsEnum = terms.iterator(null);
+
+      DocsAndPositionsEnum docsAndPositionsEnum = null;
+
+      if (termsEnum.next() != null) {
+        assert numTerms > 0;
+        docsAndPositionsEnum = termsEnum.docsAndPositions(null, null);
+        if (docsAndPositionsEnum != null) {
+          // has positions
+          positions = true;
+          if (docsAndPositionsEnum.attributes().hasAttribute(OffsetAttribute.class)) {
+            offsetAtt = docsAndPositionsEnum.attributes().getAttribute(OffsetAttribute.class);
+          } else {
+            offsetAtt = null;
+          }
+        } else {
+          positions = false;
+          offsetAtt = null;
+        }
+      } else {
+        // no terms in this field (hmm why is field present
+        // then...?)
+        assert numTerms == 0;
+        positions = false;
+        offsetAtt = null;
+      }
+      
+      startField(fieldInfo, numTerms, positions, offsetAtt != null);
+
+      int termCount = 1;
+
+      // NOTE: we already .next()'d the TermsEnum above, to
+      // peek @ first term to see if positions/offsets are
+      // present
+      while(true) {
+        final int freq = (int) termsEnum.totalTermFreq();
+        startTerm(termsEnum.term(), freq);
+
+        if (positions || offsetAtt != null) {
+          DocsAndPositionsEnum dp = termsEnum.docsAndPositions(null, docsAndPositionsEnum);
+          // TODO: add startOffset()/endOffset() to d&pEnum... this is insanity
+          if (dp != docsAndPositionsEnum) {
+            // producer didnt reuse, must re-pull attributes
+            if (offsetAtt != null) {
+              assert dp.attributes().hasAttribute(OffsetAttribute.class);
+              offsetAtt = dp.attributes().getAttribute(OffsetAttribute.class);
+            }
+          }
+          docsAndPositionsEnum = dp;
+          final int docID = docsAndPositionsEnum.nextDoc();
+          assert docID != DocsEnum.NO_MORE_DOCS;
+          assert docsAndPositionsEnum.freq() == freq;
+
+          for(int posUpto=0; posUpto<freq; posUpto++) {
+            final int pos = docsAndPositionsEnum.nextPosition();
+            final int startOffset = offsetAtt == null ? -1 : offsetAtt.startOffset();
+            final int endOffset = offsetAtt == null ? -1 : offsetAtt.endOffset();
+            
+            addPosition(pos, startOffset, endOffset);
+          }
+        }
+        
+        if (termsEnum.next() == null) {
+          assert termCount == numTerms;
+          break;
+        }
+        termCount++;
+      }
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/Lucene3xCodec.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/Lucene3xCodec.java
index 4d7fb38..c976ad6 100644
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/Lucene3xCodec.java
+++ b/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/Lucene3xCodec.java
@@ -24,14 +24,18 @@ import org.apache.lucene.index.PerDocWriteState;
 import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.codecs.Codec;
+import org.apache.lucene.index.codecs.DefaultFieldInfosFormat;
 import org.apache.lucene.index.codecs.DefaultStoredFieldsFormat;
 import org.apache.lucene.index.codecs.DefaultSegmentInfosFormat;
+import org.apache.lucene.index.codecs.DefaultTermVectorsFormat;
 import org.apache.lucene.index.codecs.DocValuesFormat;
+import org.apache.lucene.index.codecs.FieldInfosFormat;
 import org.apache.lucene.index.codecs.StoredFieldsFormat;
 import org.apache.lucene.index.codecs.PerDocConsumer;
 import org.apache.lucene.index.codecs.PerDocValues;
 import org.apache.lucene.index.codecs.PostingsFormat;
 import org.apache.lucene.index.codecs.SegmentInfosFormat;
+import org.apache.lucene.index.codecs.TermVectorsFormat;
 import org.apache.lucene.store.Directory;
 
 /**
@@ -48,6 +52,12 @@ public class Lucene3xCodec extends Codec {
   private final StoredFieldsFormat fieldsFormat = new DefaultStoredFieldsFormat();
   
   // TODO: this should really be a different impl
+  private final TermVectorsFormat vectorsFormat = new DefaultTermVectorsFormat();
+  
+  // TODO: this should really be a different impl
+  private final FieldInfosFormat fieldInfosFormat = new DefaultFieldInfosFormat();
+
+  // TODO: this should really be a different impl
   // also if we want preflex to *really* be read-only it should throw exception for the writer?
   // this way IR.commit fails on delete/undelete/setNorm/etc ?
   private final SegmentInfosFormat infosFormat = new DefaultSegmentInfosFormat();
@@ -82,6 +92,16 @@ public class Lucene3xCodec extends Codec {
   public StoredFieldsFormat storedFieldsFormat() {
     return fieldsFormat;
   }
+  
+  @Override
+  public TermVectorsFormat termVectorsFormat() {
+    return vectorsFormat;
+  }
+  
+  @Override
+  public FieldInfosFormat fieldInfosFormat() {
+    return fieldInfosFormat;
+  }
 
   @Override
   public SegmentInfosFormat segmentInfosFormat() {
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/Lucene3xFields.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/Lucene3xFields.java
index 194eb3c..7fb9282 100644
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/Lucene3xFields.java
+++ b/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/Lucene3xFields.java
@@ -25,7 +25,6 @@ import java.util.Iterator;
 import java.util.Map;
 import java.util.TreeMap;
 
-import org.apache.lucene.index.CorruptIndexException;
 import org.apache.lucene.index.DocsAndPositionsEnum;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.FieldInfo;
@@ -38,7 +37,6 @@ import org.apache.lucene.index.Term;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.index.codecs.FieldsProducer;
-import org.apache.lucene.store.CompoundFileDirectory;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexInput;
@@ -163,6 +161,11 @@ public class Lucene3xFields extends FieldsProducer {
   }
 
   @Override
+  public int getUniqueFieldCount() {
+    return preTerms.size();
+  }
+
+  @Override
   public long getUniqueTermCount() throws IOException {
     return getTermsDict().size();
   }
@@ -196,12 +199,10 @@ public class Lucene3xFields extends FieldsProducer {
 
   private class PreFlexFieldsEnum extends FieldsEnum {
     final Iterator<FieldInfo> it;
-    private final PreTermsEnum termsEnum;
     FieldInfo current;
 
     public PreFlexFieldsEnum() throws IOException {
       it = fields.values().iterator();
-      termsEnum = new PreTermsEnum();
     }
 
     @Override
@@ -215,9 +216,8 @@ public class Lucene3xFields extends FieldsProducer {
     }
 
     @Override
-    public TermsEnum terms() throws IOException {
-      termsEnum.reset(current);
-      return termsEnum;
+    public Terms terms() throws IOException {
+      return Lucene3xFields.this.terms(current.name);
     }
   }
   
@@ -228,7 +228,7 @@ public class Lucene3xFields extends FieldsProducer {
     }
 
     @Override
-    public TermsEnum iterator() throws IOException {    
+    public TermsEnum iterator(TermsEnum reuse) throws IOException {    
       PreTermsEnum termsEnum = new PreTermsEnum();
       termsEnum.reset(fieldInfo);
       return termsEnum;
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40Codec.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40Codec.java
index 66118df..e766435 100644
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40Codec.java
+++ b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40Codec.java
@@ -19,12 +19,16 @@ package org.apache.lucene.index.codecs.lucene40;
 
 import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.index.codecs.DefaultDocValuesFormat;
+import org.apache.lucene.index.codecs.DefaultFieldInfosFormat;
 import org.apache.lucene.index.codecs.DefaultStoredFieldsFormat;
 import org.apache.lucene.index.codecs.DefaultSegmentInfosFormat;
+import org.apache.lucene.index.codecs.DefaultTermVectorsFormat;
 import org.apache.lucene.index.codecs.DocValuesFormat;
+import org.apache.lucene.index.codecs.FieldInfosFormat;
 import org.apache.lucene.index.codecs.StoredFieldsFormat;
 import org.apache.lucene.index.codecs.PostingsFormat;
 import org.apache.lucene.index.codecs.SegmentInfosFormat;
+import org.apache.lucene.index.codecs.TermVectorsFormat;
 import org.apache.lucene.index.codecs.perfield.PerFieldPostingsFormat;
 
 /**
@@ -39,7 +43,9 @@ import org.apache.lucene.index.codecs.perfield.PerFieldPostingsFormat;
 // (it writes a minor version, etc).
 public class Lucene40Codec extends Codec {
   private final StoredFieldsFormat fieldsFormat = new DefaultStoredFieldsFormat();
+  private final TermVectorsFormat vectorsFormat = new DefaultTermVectorsFormat();
   private final DocValuesFormat docValuesFormat = new DefaultDocValuesFormat();
+  private final FieldInfosFormat fieldInfosFormat = new DefaultFieldInfosFormat();
   private final SegmentInfosFormat infosFormat = new DefaultSegmentInfosFormat();
   private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
     @Override
@@ -56,6 +62,11 @@ public class Lucene40Codec extends Codec {
   public StoredFieldsFormat storedFieldsFormat() {
     return fieldsFormat;
   }
+  
+  @Override
+  public TermVectorsFormat termVectorsFormat() {
+    return vectorsFormat;
+  }
 
   @Override
   public DocValuesFormat docValuesFormat() {
@@ -68,6 +79,11 @@ public class Lucene40Codec extends Codec {
   }
   
   @Override
+  public FieldInfosFormat fieldInfosFormat() {
+    return fieldInfosFormat;
+  }
+  
+  @Override
   public SegmentInfosFormat segmentInfosFormat() {
     return infosFormat;
   }
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/memory/MemoryPostingsFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/memory/MemoryPostingsFormat.java
index 0b6c070..16a32be 100644
--- a/lucene/src/java/org/apache/lucene/index/codecs/memory/MemoryPostingsFormat.java
+++ b/lucene/src/java/org/apache/lucene/index/codecs/memory/MemoryPostingsFormat.java
@@ -723,7 +723,7 @@ public class MemoryPostingsFormat extends PostingsFormat {
     }
 
     @Override
-    public TermsEnum iterator() {
+    public TermsEnum iterator(TermsEnum reuse) {
       return new FSTTermsEnum(field, fst);
     }
 
@@ -768,8 +768,9 @@ public class MemoryPostingsFormat extends PostingsFormat {
             return current.field.name;
           }
 
-          public TermsEnum terms() {
-            return current.iterator();
+          @Override
+          public Terms terms() {
+            return current;
           }
         };
       }
@@ -780,6 +781,11 @@ public class MemoryPostingsFormat extends PostingsFormat {
       }
       
       @Override
+      public int getUniqueFieldCount() {
+        return fields.size();
+      }
+
+      @Override
       public void close() {
         // Drop ref to FST:
         for(TermsReader termsReader : fields.values()) {
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/perfield/PerFieldPostingsFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/perfield/PerFieldPostingsFormat.java
index 98bb641..9fa8948 100644
--- a/lucene/src/java/org/apache/lucene/index/codecs/perfield/PerFieldPostingsFormat.java
+++ b/lucene/src/java/org/apache/lucene/index/codecs/perfield/PerFieldPostingsFormat.java
@@ -34,7 +34,6 @@ import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.SegmentWriteState;
 import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.index.codecs.FieldsConsumer;
 import org.apache.lucene.index.codecs.FieldsProducer;
 import org.apache.lucene.index.codecs.PostingsFormat;
@@ -213,7 +212,7 @@ public abstract class PerFieldPostingsFormat extends PostingsFormat {
       }
 
       @Override
-      public String next() {
+      public String next() throws IOException {
         if (it.hasNext()) {
           current = it.next();
         } else {
@@ -224,13 +223,8 @@ public abstract class PerFieldPostingsFormat extends PostingsFormat {
       }
 
       @Override
-      public TermsEnum terms() throws IOException {
-        final Terms terms = fields.get(current).terms(current);
-        if (terms != null) {
-          return terms.iterator();
-        } else {
-          return TermsEnum.EMPTY;
-        }
+      public Terms terms() throws IOException {
+        return fields.get(current).terms(current);
       }
     }
 
@@ -246,6 +240,11 @@ public abstract class PerFieldPostingsFormat extends PostingsFormat {
     }
     
     @Override
+    public int getUniqueFieldCount() {
+      return fields.size();
+    }
+
+    @Override
     public void close() throws IOException {
       IOUtils.close(formats.values());
     }
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextCodec.java b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextCodec.java
index 52b1012..ee573a6 100644
--- a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextCodec.java
+++ b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextCodec.java
@@ -19,11 +19,12 @@ package org.apache.lucene.index.codecs.simpletext;
 
 import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.index.codecs.DefaultDocValuesFormat;
-import org.apache.lucene.index.codecs.DefaultSegmentInfosFormat;
 import org.apache.lucene.index.codecs.DocValuesFormat;
+import org.apache.lucene.index.codecs.FieldInfosFormat;
 import org.apache.lucene.index.codecs.PostingsFormat;
 import org.apache.lucene.index.codecs.SegmentInfosFormat;
 import org.apache.lucene.index.codecs.StoredFieldsFormat;
+import org.apache.lucene.index.codecs.TermVectorsFormat;
 
 /**
  * plain text index format.
@@ -34,11 +35,11 @@ import org.apache.lucene.index.codecs.StoredFieldsFormat;
 public final class SimpleTextCodec extends Codec {
   private final PostingsFormat postings = new SimpleTextPostingsFormat();
   private final StoredFieldsFormat storedFields = new SimpleTextStoredFieldsFormat();
-  
+  private final SegmentInfosFormat segmentInfos = new SimpleTextSegmentInfosFormat();
+  private final FieldInfosFormat fieldInfosFormat = new SimpleTextFieldInfosFormat();
+  private final TermVectorsFormat vectorsFormat = new SimpleTextTermVectorsFormat();
   // TODO: need a plain-text impl
   private final DocValuesFormat docValues = new DefaultDocValuesFormat();
-  // TODO: need a plain-text impl
-  private final SegmentInfosFormat segmentInfos = new DefaultSegmentInfosFormat();
   
   public SimpleTextCodec() {
     super("SimpleText");
@@ -58,6 +59,16 @@ public final class SimpleTextCodec extends Codec {
   public StoredFieldsFormat storedFieldsFormat() {
     return storedFields;
   }
+  
+  @Override
+  public TermVectorsFormat termVectorsFormat() {
+    return vectorsFormat;
+  }
+  
+  @Override
+  public FieldInfosFormat fieldInfosFormat() {
+    return fieldInfosFormat;
+  }
 
   @Override
   public SegmentInfosFormat segmentInfosFormat() {
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldInfosFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldInfosFormat.java
new file mode 100644
index 0000000..731c8ec
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldInfosFormat.java
@@ -0,0 +1,53 @@
+package org.apache.lucene.index.codecs.simpletext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.codecs.FieldInfosFormat;
+import org.apache.lucene.index.codecs.FieldInfosReader;
+import org.apache.lucene.index.codecs.FieldInfosWriter;
+import org.apache.lucene.store.Directory;
+
+/**
+ * plaintext field infos format
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextFieldInfosFormat extends FieldInfosFormat {
+  private final FieldInfosReader reader = new SimpleTextFieldInfosReader();
+  private final FieldInfosWriter writer = new SimpleTextFieldInfosWriter();
+
+  @Override
+  public FieldInfosReader getFieldInfosReader() throws IOException {
+    return reader;
+  }
+
+  @Override
+  public FieldInfosWriter getFieldInfosWriter() throws IOException {
+    return writer;
+  }
+
+  @Override
+  public void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    SimpleTextFieldInfosReader.files(dir, info, files);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldInfosReader.java b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldInfosReader.java
new file mode 100644
index 0000000..fa771f6
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldInfosReader.java
@@ -0,0 +1,138 @@
+package org.apache.lucene.index.codecs.simpletext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.codecs.FieldInfosReader;
+import org.apache.lucene.index.values.ValueType;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+
+import static org.apache.lucene.index.codecs.simpletext.SimpleTextFieldInfosWriter.*;
+
+/**
+ * reads plaintext field infos files
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextFieldInfosReader extends FieldInfosReader {
+
+  @Override
+  public FieldInfos read(Directory directory, String segmentName, IOContext iocontext) throws IOException {
+    final String fileName = IndexFileNames.segmentFileName(segmentName, "", FIELD_INFOS_EXTENSION);
+    IndexInput input = directory.openInput(fileName, iocontext);
+    BytesRef scratch = new BytesRef();
+    
+    boolean hasVectors = false;
+    boolean hasFreq = false;
+    boolean hasProx = false;
+    
+    try {
+      
+      SimpleTextUtil.readLine(input, scratch);
+      assert scratch.startsWith(NUMFIELDS);
+      final int size = Integer.parseInt(readString(NUMFIELDS.length, scratch));
+      FieldInfo infos[] = new FieldInfo[size];
+
+      for (int i = 0; i < size; i++) {
+        SimpleTextUtil.readLine(input, scratch);
+        assert scratch.startsWith(NAME);
+        String name = readString(NAME.length, scratch);
+        
+        SimpleTextUtil.readLine(input, scratch);
+        assert scratch.startsWith(NUMBER);
+        int fieldNumber = Integer.parseInt(readString(NUMBER.length, scratch));
+
+        SimpleTextUtil.readLine(input, scratch);
+        assert scratch.startsWith(ISINDEXED);
+        boolean isIndexed = Boolean.parseBoolean(readString(ISINDEXED.length, scratch));
+        
+        SimpleTextUtil.readLine(input, scratch);
+        assert scratch.startsWith(STORETV);
+        boolean storeTermVector = Boolean.parseBoolean(readString(STORETV.length, scratch));
+        
+        SimpleTextUtil.readLine(input, scratch);
+        assert scratch.startsWith(STORETVPOS);
+        boolean storePositionsWithTermVector = Boolean.parseBoolean(readString(STORETVPOS.length, scratch));
+        
+        SimpleTextUtil.readLine(input, scratch);
+        assert scratch.startsWith(STORETVOFF);
+        boolean storeOffsetWithTermVector = Boolean.parseBoolean(readString(STORETVOFF.length, scratch));
+        
+        SimpleTextUtil.readLine(input, scratch);
+        assert scratch.startsWith(PAYLOADS);
+        boolean storePayloads = Boolean.parseBoolean(readString(PAYLOADS.length, scratch));
+        
+        SimpleTextUtil.readLine(input, scratch);
+        assert scratch.startsWith(NORMS);
+        boolean omitNorms = !Boolean.parseBoolean(readString(NORMS.length, scratch));
+
+        SimpleTextUtil.readLine(input, scratch);
+        assert scratch.startsWith(DOCVALUES);
+        String dvType = readString(DOCVALUES.length, scratch);
+        final ValueType docValuesType;
+        
+        if ("false".equals(dvType)) {
+          docValuesType = null;
+        } else {
+          docValuesType = ValueType.valueOf(dvType);
+        }
+        
+        SimpleTextUtil.readLine(input, scratch);
+        assert scratch.startsWith(INDEXOPTIONS);
+        IndexOptions indexOptions = IndexOptions.valueOf(readString(INDEXOPTIONS.length, scratch));
+
+        hasVectors |= storeTermVector;
+        hasProx |= isIndexed && indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
+        hasFreq |= isIndexed && indexOptions != IndexOptions.DOCS_ONLY;
+        
+        infos[i] = new FieldInfo(name, isIndexed, fieldNumber, storeTermVector, 
+          storePositionsWithTermVector, storeOffsetWithTermVector, 
+          omitNorms, storePayloads, indexOptions, docValuesType);
+      }
+
+      if (input.getFilePointer() != input.length()) {
+        throw new CorruptIndexException("did not read all bytes from file \"" + fileName + "\": read " + input.getFilePointer() + " vs size " + input.length() + " (resource: " + input + ")");
+      }
+      
+      return new FieldInfos(infos, hasFreq, hasProx, hasVectors);
+    } finally {
+      input.close();
+    }
+  }
+  
+  private String readString(int offset, BytesRef scratch) {
+    return new String(scratch.bytes, scratch.offset+offset, scratch.length-offset, IOUtils.CHARSET_UTF_8);
+  }
+  
+  public static void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    files.add(IndexFileNames.segmentFileName(info.name, "", FIELD_INFOS_EXTENSION));
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldInfosWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldInfosWriter.java
new file mode 100644
index 0000000..abc2c7a
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldInfosWriter.java
@@ -0,0 +1,115 @@
+package org.apache.lucene.index.codecs.simpletext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.codecs.FieldInfosWriter;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+
+/**
+ * writes plaintext field infos files
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextFieldInfosWriter extends FieldInfosWriter {
+  
+  /** Extension of field infos */
+  static final String FIELD_INFOS_EXTENSION = "inf";
+  
+  static final BytesRef NUMFIELDS       =  new BytesRef("number of fields ");
+  static final BytesRef NAME            =  new BytesRef("  name ");
+  static final BytesRef NUMBER          =  new BytesRef("  number ");
+  static final BytesRef ISINDEXED       =  new BytesRef("  indexed ");
+  static final BytesRef STORETV         =  new BytesRef("  term vectors ");
+  static final BytesRef STORETVPOS      =  new BytesRef("  term vector positions ");
+  static final BytesRef STORETVOFF      =  new BytesRef("  term vector offsets ");
+  static final BytesRef PAYLOADS        =  new BytesRef("  payloads ");
+  static final BytesRef NORMS           =  new BytesRef("  norms ");
+  static final BytesRef DOCVALUES       =  new BytesRef("  doc values ");
+  static final BytesRef INDEXOPTIONS    =  new BytesRef("  index options ");
+  
+  @Override
+  public void write(Directory directory, String segmentName, FieldInfos infos, IOContext context) throws IOException {
+    final String fileName = IndexFileNames.segmentFileName(segmentName, "", FIELD_INFOS_EXTENSION);
+    IndexOutput out = directory.createOutput(fileName, context);
+    BytesRef scratch = new BytesRef();
+    try {
+      SimpleTextUtil.write(out, NUMFIELDS);
+      SimpleTextUtil.write(out, Integer.toString(infos.size()), scratch);
+      SimpleTextUtil.writeNewline(out);
+      
+      for (FieldInfo fi : infos) {
+        assert fi.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS || !fi.storePayloads;
+
+        SimpleTextUtil.write(out, NAME);
+        SimpleTextUtil.write(out, fi.name, scratch);
+        SimpleTextUtil.writeNewline(out);
+        
+        SimpleTextUtil.write(out, NUMBER);
+        SimpleTextUtil.write(out, Integer.toString(fi.number), scratch);
+        SimpleTextUtil.writeNewline(out);
+        
+        SimpleTextUtil.write(out, ISINDEXED);
+        SimpleTextUtil.write(out, Boolean.toString(fi.isIndexed), scratch);
+        SimpleTextUtil.writeNewline(out);
+        
+        SimpleTextUtil.write(out, STORETV);
+        SimpleTextUtil.write(out, Boolean.toString(fi.storeTermVector), scratch);
+        SimpleTextUtil.writeNewline(out);
+        
+        SimpleTextUtil.write(out, STORETVPOS);
+        SimpleTextUtil.write(out, Boolean.toString(fi.storePositionWithTermVector), scratch);
+        SimpleTextUtil.writeNewline(out);
+
+        SimpleTextUtil.write(out, STORETVOFF);
+        SimpleTextUtil.write(out, Boolean.toString(fi.storeOffsetWithTermVector), scratch);
+        SimpleTextUtil.writeNewline(out);
+        
+        SimpleTextUtil.write(out, PAYLOADS);
+        SimpleTextUtil.write(out, Boolean.toString(fi.storePayloads), scratch);
+        SimpleTextUtil.writeNewline(out);
+               
+        SimpleTextUtil.write(out, NORMS);
+        SimpleTextUtil.write(out, Boolean.toString(!fi.omitNorms), scratch);
+        SimpleTextUtil.writeNewline(out);
+        
+        SimpleTextUtil.write(out, DOCVALUES);
+        if (!fi.hasDocValues()) {
+          SimpleTextUtil.write(out, "false", scratch);
+        } else {
+          SimpleTextUtil.write(out, fi.getDocValues().toString(), scratch);
+        }
+        SimpleTextUtil.writeNewline(out);
+        
+        SimpleTextUtil.write(out, INDEXOPTIONS);
+        SimpleTextUtil.write(out, fi.indexOptions.toString(), scratch);
+        SimpleTextUtil.writeNewline(out);
+      }
+    } finally {
+      out.close();
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldsReader.java b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldsReader.java
index 954e04b..fbe2779 100644
--- a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldsReader.java
+++ b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldsReader.java
@@ -87,8 +87,8 @@ class SimpleTextFieldsReader extends FieldsProducer {
     }
 
     @Override
-    public TermsEnum terms() throws IOException {
-      return SimpleTextFieldsReader.this.terms(current).iterator();
+    public Terms terms() throws IOException {
+      return SimpleTextFieldsReader.this.terms(current);
     }
   }
 
@@ -523,7 +523,7 @@ class SimpleTextFieldsReader extends FieldsProducer {
     }
 
     @Override
-    public TermsEnum iterator() throws IOException {
+    public TermsEnum iterator(TermsEnum reuse) throws IOException {
       if (fst != null) {
         return new SimpleTextTermsEnum(fst, indexOptions);
       } else {
@@ -582,6 +582,11 @@ class SimpleTextFieldsReader extends FieldsProducer {
   }
 
   @Override
+  public int getUniqueFieldCount() {
+    return -1;
+  }
+
+  @Override
   public void close() throws IOException {
     in.close();
   }
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextSegmentInfosFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextSegmentInfosFormat.java
new file mode 100644
index 0000000..9ed197a
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextSegmentInfosFormat.java
@@ -0,0 +1,43 @@
+package org.apache.lucene.index.codecs.simpletext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.index.codecs.SegmentInfosFormat;
+import org.apache.lucene.index.codecs.SegmentInfosReader;
+import org.apache.lucene.index.codecs.SegmentInfosWriter;
+
+/**
+ * plain text segments file format.
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextSegmentInfosFormat extends SegmentInfosFormat {
+  private final SegmentInfosReader reader = new SimpleTextSegmentInfosReader();
+  private final SegmentInfosWriter writer = new SimpleTextSegmentInfosWriter();
+  
+  @Override
+  public SegmentInfosReader getSegmentInfosReader() {
+    return reader;
+  }
+
+  @Override
+  public SegmentInfosWriter getSegmentInfosWriter() {
+    return writer;
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextSegmentInfosReader.java b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextSegmentInfosReader.java
new file mode 100644
index 0000000..06b2bcd
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextSegmentInfosReader.java
@@ -0,0 +1,193 @@
+package org.apache.lucene.index.codecs.simpletext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentInfos;
+import org.apache.lucene.index.codecs.Codec;
+import org.apache.lucene.index.codecs.SegmentInfosReader;
+import org.apache.lucene.store.ChecksumIndexInput;
+import org.apache.lucene.store.DataInput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+
+import static org.apache.lucene.index.codecs.simpletext.SimpleTextSegmentInfosWriter.*;
+
+/**
+ * reads plaintext segments files
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextSegmentInfosReader extends SegmentInfosReader {
+
+  @Override
+  public void read(Directory directory, String segmentsFileName, ChecksumIndexInput input, SegmentInfos infos, IOContext context) throws IOException {
+    final BytesRef scratch = new BytesRef();
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert scratch.startsWith(VERSION);
+    infos.version = Long.parseLong(readString(VERSION.length, scratch));
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert scratch.startsWith(COUNTER);
+    infos.counter = Integer.parseInt(readString(COUNTER.length, scratch));
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert scratch.startsWith(FNX_VERSION);
+    infos.setGlobalFieldMapVersion(Long.parseLong(readString(FNX_VERSION.length, scratch)));
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert scratch.startsWith(NUM_USERDATA);
+    int numUserData = Integer.parseInt(readString(NUM_USERDATA.length, scratch));
+    infos.userData = new HashMap<String,String>();
+
+    for (int i = 0; i < numUserData; i++) {
+      SimpleTextUtil.readLine(input, scratch);
+      assert scratch.startsWith(USERDATA_KEY);
+      String key = readString(USERDATA_KEY.length, scratch);
+      
+      SimpleTextUtil.readLine(input, scratch);
+      assert scratch.startsWith(USERDATA_VALUE);
+      String value = readString(USERDATA_VALUE.length, scratch);
+      infos.userData.put(key, value);
+    }
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert scratch.startsWith(NUM_SEGMENTS);
+    int numSegments = Integer.parseInt(readString(NUM_SEGMENTS.length, scratch));
+    
+    for (int i = 0; i < numSegments; i++) {
+      infos.add(readSegmentInfo(directory, input, scratch));
+    }
+  }
+  
+  public SegmentInfo readSegmentInfo(Directory directory, DataInput input, BytesRef scratch) throws IOException {
+    SimpleTextUtil.readLine(input, scratch);
+    assert scratch.startsWith(SI_NAME);
+    final String name = readString(SI_NAME.length, scratch);
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert scratch.startsWith(SI_CODEC);
+    final Codec codec = Codec.forName(readString(SI_CODEC.length, scratch));
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert scratch.startsWith(SI_VERSION);
+    final String version = readString(SI_VERSION.length, scratch);
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert scratch.startsWith(SI_DOCCOUNT);
+    final int docCount = Integer.parseInt(readString(SI_DOCCOUNT.length, scratch));
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert scratch.startsWith(SI_DELCOUNT);
+    final int delCount = Integer.parseInt(readString(SI_DELCOUNT.length, scratch));
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert scratch.startsWith(SI_HASPROX);
+    final int hasProx = readTernary(SI_HASPROX.length, scratch);
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert scratch.startsWith(SI_HASVECTORS);
+    final int hasVectors = readTernary(SI_HASVECTORS.length, scratch);
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert scratch.startsWith(SI_USECOMPOUND);
+    final boolean isCompoundFile = Boolean.parseBoolean(readString(SI_USECOMPOUND.length, scratch));
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert scratch.startsWith(SI_DSOFFSET);
+    final int dsOffset = Integer.parseInt(readString(SI_DSOFFSET.length, scratch));
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert scratch.startsWith(SI_DSSEGMENT);
+    final String dsSegment = readString(SI_DSSEGMENT.length, scratch);
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert scratch.startsWith(SI_DSCOMPOUND);
+    final boolean dsCompoundFile = Boolean.parseBoolean(readString(SI_DSCOMPOUND.length, scratch));
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert scratch.startsWith(SI_DELGEN);
+    final long delGen = Long.parseLong(readString(SI_DELGEN.length, scratch));
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert scratch.startsWith(SI_NUM_NORMGEN);
+    final int numNormGen = Integer.parseInt(readString(SI_NUM_NORMGEN.length, scratch));
+    final Map<Integer,Long> normGen;
+    if (numNormGen == 0) {
+      normGen = null;
+    } else {
+      normGen = new HashMap<Integer,Long>();
+      for (int i = 0; i < numNormGen; i++) {
+        SimpleTextUtil.readLine(input, scratch);
+        assert scratch.startsWith(SI_NORMGEN_KEY);
+        int key = Integer.parseInt(readString(SI_NORMGEN_KEY.length, scratch));
+        
+        SimpleTextUtil.readLine(input, scratch);
+        assert scratch.startsWith(SI_NORMGEN_VALUE);
+        long value = Long.parseLong(readString(SI_NORMGEN_VALUE.length, scratch));
+        normGen.put(key, value);
+      }
+    }
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert scratch.startsWith(SI_NUM_DIAG);
+    int numDiag = Integer.parseInt(readString(SI_NUM_DIAG.length, scratch));
+    Map<String,String> diagnostics = new HashMap<String,String>();
+
+    for (int i = 0; i < numDiag; i++) {
+      SimpleTextUtil.readLine(input, scratch);
+      assert scratch.startsWith(SI_DIAG_KEY);
+      String key = readString(SI_DIAG_KEY.length, scratch);
+      
+      SimpleTextUtil.readLine(input, scratch);
+      assert scratch.startsWith(SI_DIAG_VALUE);
+      String value = readString(SI_DIAG_VALUE.length, scratch);
+      diagnostics.put(key, value);
+    }
+    
+    return new SegmentInfo(directory, version, name, docCount, delGen, dsOffset,
+        dsSegment, dsCompoundFile, normGen, isCompoundFile,
+        delCount, hasProx, codec, diagnostics, hasVectors);
+  }
+  
+  private String readString(int offset, BytesRef scratch) {
+    return new String(scratch.bytes, scratch.offset+offset, scratch.length-offset, IOUtils.CHARSET_UTF_8);
+  }
+  
+  private int readTernary(int offset, BytesRef scratch) throws IOException {
+    String s = readString(offset, scratch);
+    if ("check fieldinfo".equals(s)) {
+      return SegmentInfo.CHECK_FIELDINFO;
+    } else if ("true".equals(s)) {
+      return SegmentInfo.YES;
+    } else if ("false".equals(s)) {
+      return 0;
+    } else {
+      throw new CorruptIndexException("invalid ternary value: " + s);
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextSegmentInfosWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextSegmentInfosWriter.java
new file mode 100644
index 0000000..bb08421
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextSegmentInfosWriter.java
@@ -0,0 +1,241 @@
+package org.apache.lucene.index.codecs.simpletext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Map;
+import java.util.Map.Entry;
+
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentInfos;
+import org.apache.lucene.index.codecs.DefaultSegmentInfosWriter;
+import org.apache.lucene.index.codecs.SegmentInfosWriter;
+import org.apache.lucene.store.ChecksumIndexOutput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.FlushInfo;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * writes plaintext segments files
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextSegmentInfosWriter extends SegmentInfosWriter {
+
+  final static BytesRef VERSION             = new BytesRef("version ");
+  final static BytesRef COUNTER             = new BytesRef("counter ");
+  final static BytesRef FNX_VERSION         = new BytesRef("global field map version ");
+  final static BytesRef NUM_USERDATA        = new BytesRef("user data entries ");
+  final static BytesRef USERDATA_KEY        = new BytesRef("  key ");
+  final static BytesRef USERDATA_VALUE      = new BytesRef("  value ");
+  final static BytesRef NUM_SEGMENTS        = new BytesRef("number of segments ");
+  final static BytesRef SI_NAME             = new BytesRef("  name ");
+  final static BytesRef SI_CODEC            = new BytesRef("    codec ");
+  final static BytesRef SI_VERSION          = new BytesRef("    version ");
+  final static BytesRef SI_DOCCOUNT         = new BytesRef("    number of documents ");
+  final static BytesRef SI_DELCOUNT         = new BytesRef("    number of deletions ");
+  final static BytesRef SI_HASPROX          = new BytesRef("    has prox ");
+  final static BytesRef SI_HASVECTORS       = new BytesRef("    has vectors ");
+  final static BytesRef SI_USECOMPOUND      = new BytesRef("    uses compound file ");
+  final static BytesRef SI_DSOFFSET         = new BytesRef("    docstore offset ");
+  final static BytesRef SI_DSSEGMENT        = new BytesRef("    docstore segment ");
+  final static BytesRef SI_DSCOMPOUND       = new BytesRef("    docstore is compound file ");
+  final static BytesRef SI_DELGEN           = new BytesRef("    deletion generation ");
+  final static BytesRef SI_NUM_NORMGEN      = new BytesRef("    norms generations ");
+  final static BytesRef SI_NORMGEN_KEY      = new BytesRef("      key ");
+  final static BytesRef SI_NORMGEN_VALUE    = new BytesRef("      value ");
+  final static BytesRef SI_NUM_DIAG         = new BytesRef("    diagnostics ");
+  final static BytesRef SI_DIAG_KEY         = new BytesRef("      key ");
+  final static BytesRef SI_DIAG_VALUE       = new BytesRef("      value ");
+  
+  @Override
+  public IndexOutput writeInfos(Directory dir, String segmentsFileName, String codecID, SegmentInfos infos, IOContext context) throws IOException {
+    BytesRef scratch = new BytesRef();
+    IndexOutput out = new ChecksumIndexOutput(dir.createOutput(segmentsFileName, new IOContext(new FlushInfo(infos.size(), infos.totalDocCount()))));
+    boolean success = false;
+    try {
+      // required preamble
+      out.writeInt(DefaultSegmentInfosWriter.FORMAT_CURRENT); // write FORMAT
+      out.writeString(codecID); // write codecID
+      // end preamble
+      
+      // version
+      SimpleTextUtil.write(out, VERSION);
+      SimpleTextUtil.write(out, Long.toString(infos.version), scratch);
+      SimpleTextUtil.writeNewline(out);
+
+      // counter
+      SimpleTextUtil.write(out, COUNTER);
+      SimpleTextUtil.write(out, Integer.toString(infos.counter), scratch);
+      SimpleTextUtil.writeNewline(out);
+      
+      // global field map version
+      SimpleTextUtil.write(out, FNX_VERSION);
+      SimpleTextUtil.write(out, Long.toString(infos.getGlobalFieldMapVersion()), scratch);
+      SimpleTextUtil.writeNewline(out);
+
+      // user data
+      int numUserDataEntries = infos.getUserData() == null ? 0 : infos.getUserData().size();
+      SimpleTextUtil.write(out, NUM_USERDATA);
+      SimpleTextUtil.write(out, Integer.toString(numUserDataEntries), scratch);
+      SimpleTextUtil.writeNewline(out);
+      
+      if (numUserDataEntries > 0) {
+        for (Map.Entry<String,String> userEntry : infos.getUserData().entrySet()) {
+          SimpleTextUtil.write(out, USERDATA_KEY);
+          SimpleTextUtil.write(out, userEntry.getKey(), scratch);
+          SimpleTextUtil.writeNewline(out);
+          
+          SimpleTextUtil.write(out, USERDATA_VALUE);
+          SimpleTextUtil.write(out, userEntry.getValue(), scratch);
+          SimpleTextUtil.writeNewline(out);
+        }
+      }
+      
+      // infos size
+      SimpleTextUtil.write(out, NUM_SEGMENTS);
+      SimpleTextUtil.write(out, Integer.toString(infos.size()), scratch);
+      SimpleTextUtil.writeNewline(out);
+
+      for (SegmentInfo si : infos) {
+        writeInfo(out, si);
+      } 
+
+      success = true;
+      return out;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(out);
+      }
+    }
+  }
+  
+  private void writeInfo(IndexOutput output, SegmentInfo si) throws IOException {
+    assert si.getDelCount() <= si.docCount: "delCount=" + si.getDelCount() + " docCount=" + si.docCount + " segment=" + si.name;
+    BytesRef scratch = new BytesRef();
+    
+    SimpleTextUtil.write(output, SI_NAME);
+    SimpleTextUtil.write(output, si.name, scratch);
+    SimpleTextUtil.writeNewline(output);
+    
+    SimpleTextUtil.write(output, SI_CODEC);
+    SimpleTextUtil.write(output, si.getCodec().getName(), scratch);
+    SimpleTextUtil.writeNewline(output);
+    
+    SimpleTextUtil.write(output, SI_VERSION);
+    SimpleTextUtil.write(output, si.getVersion(), scratch);
+    SimpleTextUtil.writeNewline(output);
+    
+    SimpleTextUtil.write(output, SI_DOCCOUNT);
+    SimpleTextUtil.write(output, Integer.toString(si.docCount), scratch);
+    SimpleTextUtil.writeNewline(output);
+    
+    SimpleTextUtil.write(output, SI_DELCOUNT);
+    SimpleTextUtil.write(output, Integer.toString(si.getDelCount()), scratch);
+    SimpleTextUtil.writeNewline(output);
+    
+    SimpleTextUtil.write(output, SI_HASPROX);
+    switch(si.getHasProxInternal()) {
+      case SegmentInfo.YES: SimpleTextUtil.write(output, "true", scratch); break;
+      case SegmentInfo.CHECK_FIELDINFO: SimpleTextUtil.write(output, "check fieldinfo", scratch); break;
+      // its "NO" if its 'anything but YES'... such as 0
+      default: SimpleTextUtil.write(output, "false", scratch); break;
+    }
+    SimpleTextUtil.writeNewline(output);
+    
+    SimpleTextUtil.write(output, SI_HASVECTORS);
+    switch(si.getHasVectorsInternal()) {
+      case SegmentInfo.YES: SimpleTextUtil.write(output, "true", scratch); break;
+      case SegmentInfo.CHECK_FIELDINFO: SimpleTextUtil.write(output, "check fieldinfo", scratch); break;
+      // its "NO" if its 'anything but YES'... such as 0
+      default: SimpleTextUtil.write(output, "false", scratch); break;
+    }
+    SimpleTextUtil.writeNewline(output);
+    
+    SimpleTextUtil.write(output, SI_USECOMPOUND);
+    SimpleTextUtil.write(output, Boolean.toString(si.getUseCompoundFile()), scratch);
+    SimpleTextUtil.writeNewline(output);
+    
+    SimpleTextUtil.write(output, SI_DSOFFSET);
+    SimpleTextUtil.write(output, Integer.toString(si.getDocStoreOffset()), scratch);
+    SimpleTextUtil.writeNewline(output);
+    
+    SimpleTextUtil.write(output, SI_DSSEGMENT);
+    SimpleTextUtil.write(output, si.getDocStoreSegment(), scratch);
+    SimpleTextUtil.writeNewline(output);
+    
+    SimpleTextUtil.write(output, SI_DSCOMPOUND);
+    SimpleTextUtil.write(output, Boolean.toString(si.getDocStoreIsCompoundFile()), scratch);
+    SimpleTextUtil.writeNewline(output);
+    
+    SimpleTextUtil.write(output, SI_DELGEN);
+    SimpleTextUtil.write(output, Long.toString(si.getDelGen()), scratch);
+    SimpleTextUtil.writeNewline(output);
+    
+    Map<Integer,Long> normGen = si.getNormGen();
+    int numNormGen = normGen == null ? 0 : normGen.size();
+    SimpleTextUtil.write(output, SI_NUM_NORMGEN);
+    SimpleTextUtil.write(output, Integer.toString(numNormGen), scratch);
+    SimpleTextUtil.writeNewline(output);
+    
+    if (numNormGen > 0) {
+      for (Entry<Integer,Long> entry : normGen.entrySet()) {
+        SimpleTextUtil.write(output, SI_NORMGEN_KEY);
+        SimpleTextUtil.write(output, Integer.toString(entry.getKey()), scratch);
+        SimpleTextUtil.writeNewline(output);
+        
+        SimpleTextUtil.write(output, SI_NORMGEN_VALUE);
+        SimpleTextUtil.write(output, Long.toString(entry.getValue()), scratch);
+        SimpleTextUtil.writeNewline(output);
+      }
+    }
+    
+    Map<String,String> diagnostics = si.getDiagnostics();
+    int numDiagnostics = diagnostics == null ? 0 : diagnostics.size();
+    SimpleTextUtil.write(output, SI_NUM_DIAG);
+    SimpleTextUtil.write(output, Integer.toString(numDiagnostics), scratch);
+    SimpleTextUtil.writeNewline(output);
+    
+    if (numDiagnostics > 0) {
+      for (Map.Entry<String,String> diagEntry : diagnostics.entrySet()) {
+        SimpleTextUtil.write(output, SI_DIAG_KEY);
+        SimpleTextUtil.write(output, diagEntry.getKey(), scratch);
+        SimpleTextUtil.writeNewline(output);
+        
+        SimpleTextUtil.write(output, SI_DIAG_VALUE);
+        SimpleTextUtil.write(output, diagEntry.getValue(), scratch);
+        SimpleTextUtil.writeNewline(output);
+      }
+    }
+  }
+
+  @Override
+  public void prepareCommit(IndexOutput out) throws IOException {
+    ((ChecksumIndexOutput)out).prepareCommit();
+  }
+
+  @Override
+  public void finishCommit(IndexOutput out) throws IOException {
+    ((ChecksumIndexOutput)out).finishCommit();
+    out.close();
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextStoredFieldsWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextStoredFieldsWriter.java
index 0ddc140..6fe87eb 100644
--- a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextStoredFieldsWriter.java
+++ b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextStoredFieldsWriter.java
@@ -19,6 +19,7 @@ package org.apache.lucene.index.codecs.simpletext;
 
 import java.io.IOException;
 
+import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.IndexableField;
 import org.apache.lucene.index.codecs.StoredFieldsWriter;
@@ -87,9 +88,9 @@ public class SimpleTextStoredFieldsWriter extends StoredFieldsWriter {
   }
 
   @Override
-  public void writeField(int fieldNumber, IndexableField field) throws IOException {
+  public void writeField(FieldInfo info, IndexableField field) throws IOException {
     write(FIELD);
-    write(Integer.toString(fieldNumber));
+    write(Integer.toString(info.number));
     newLine();
     
     write(NAME);
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextTermVectorsFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextTermVectorsFormat.java
new file mode 100644
index 0000000..d78849d
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextTermVectorsFormat.java
@@ -0,0 +1,53 @@
+package org.apache.lucene.index.codecs.simpletext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.codecs.TermVectorsFormat;
+import org.apache.lucene.index.codecs.TermVectorsReader;
+import org.apache.lucene.index.codecs.TermVectorsWriter;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+/**
+ * plain text term vectors format.
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextTermVectorsFormat extends TermVectorsFormat {
+
+  @Override
+  public TermVectorsReader vectorsReader(Directory directory, SegmentInfo segmentInfo, FieldInfos fieldInfos, IOContext context) throws IOException {
+    return new SimpleTextTermVectorsReader(directory, segmentInfo, fieldInfos, context);
+  }
+
+  @Override
+  public TermVectorsWriter vectorsWriter(Directory directory, String segment, IOContext context) throws IOException {
+    return new SimpleTextTermVectorsWriter(directory, segment, context);
+  }
+
+  @Override
+  public void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    SimpleTextTermVectorsReader.files(dir, info, files);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextTermVectorsReader.java b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextTermVectorsReader.java
new file mode 100644
index 0000000..4b33a55
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextTermVectorsReader.java
@@ -0,0 +1,526 @@
+package org.apache.lucene.index.codecs.simpletext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Comparator;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.Set;
+import java.util.SortedMap;
+import java.util.TreeMap;
+
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.Fields;
+import org.apache.lucene.index.FieldsEnum;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.index.codecs.TermVectorsReader;
+import org.apache.lucene.store.AlreadyClosedException;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.CharsRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.UnicodeUtil;
+
+import static org.apache.lucene.index.codecs.simpletext.SimpleTextTermVectorsWriter.*;
+
+/**
+ * Reads plain-text term vectors.
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextTermVectorsReader extends TermVectorsReader {
+  private ArrayList<Long> offsets; /* docid -> offset in .vec file */
+  private IndexInput in;
+  private BytesRef scratch = new BytesRef();
+  private CharsRef scratchUTF16 = new CharsRef();
+  
+  public SimpleTextTermVectorsReader(Directory directory, SegmentInfo si, FieldInfos fieldInfos, IOContext context) throws IOException {
+    boolean success = false;
+    try {
+      in = directory.openInput(IndexFileNames.segmentFileName(si.name, "", VECTORS_EXTENSION), context);
+      success = true;
+    } finally {
+      if (!success) {
+        close();
+      }
+    }
+    readIndex();
+  }
+  
+  // used by clone
+  SimpleTextTermVectorsReader(ArrayList<Long> offsets, IndexInput in) {
+    this.offsets = offsets;
+    this.in = in;
+  }
+  
+  // we don't actually write a .tvx-like index, instead we read the 
+  // vectors file in entirety up-front and save the offsets 
+  // so we can seek to the data later.
+  private void readIndex() throws IOException {
+    offsets = new ArrayList<Long>();
+    while (!scratch.equals(END)) {
+      readLine();
+      if (scratch.startsWith(DOC)) {
+        offsets.add(in.getFilePointer());
+      }
+    }
+  }
+  
+  @Override
+  public Fields get(int doc) throws IOException {
+    // TestTV tests for this in testBadParams... but is this
+    // really guaranteed by the API?
+    if (doc < 0 || doc >= offsets.size()) {
+      throw new IllegalArgumentException("doc id out of range");
+    }
+
+    SortedMap<String,SimpleTVTerms> fields = new TreeMap<String,SimpleTVTerms>();
+    in.seek(offsets.get(doc));
+    readLine();
+    assert scratch.startsWith(NUMFIELDS);
+    int numFields = parseIntAt(NUMFIELDS.length);
+    if (numFields == 0) {
+      return null; // no vectors for this doc
+    }
+    for (int i = 0; i < numFields; i++) {
+      readLine();
+      assert scratch.startsWith(FIELD);
+      int fieldNumber = parseIntAt(FIELD.length);
+      
+      readLine();
+      assert scratch.startsWith(FIELDNAME);
+      String fieldName = readString(FIELDNAME.length, scratch);
+      
+      readLine();
+      assert scratch.startsWith(FIELDPOSITIONS);
+      boolean positions = Boolean.parseBoolean(readString(FIELDPOSITIONS.length, scratch));
+      
+      readLine();
+      assert scratch.startsWith(FIELDOFFSETS);
+      boolean offsets = Boolean.parseBoolean(readString(FIELDOFFSETS.length, scratch));
+      
+      readLine();
+      assert scratch.startsWith(FIELDTERMCOUNT);
+      int termCount = parseIntAt(FIELDTERMCOUNT.length);
+      
+      SimpleTVTerms terms = new SimpleTVTerms();
+      fields.put(fieldName, terms);
+      
+      for (int j = 0; j < termCount; j++) {
+        readLine();
+        assert scratch.startsWith(TERMTEXT);
+        BytesRef term = new BytesRef();
+        int termLength = scratch.length - TERMTEXT.length;
+        term.grow(termLength);
+        term.length = termLength;
+        System.arraycopy(scratch.bytes, scratch.offset+TERMTEXT.length, term.bytes, term.offset, termLength);
+        
+        SimpleTVPostings postings = new SimpleTVPostings();
+        terms.terms.put(term, postings);
+        
+        readLine();
+        assert scratch.startsWith(TERMFREQ);
+        postings.freq = parseIntAt(TERMFREQ.length);
+        
+        if (positions || offsets) {
+          if (positions) {
+            postings.positions = new int[postings.freq];
+          }
+        
+          if (offsets) {
+            postings.startOffsets = new int[postings.freq];
+            postings.endOffsets = new int[postings.freq];
+          }
+          
+          for (int k = 0; k < postings.freq; k++) {
+            if (positions) {
+              readLine();
+              assert scratch.startsWith(POSITION);
+              postings.positions[k] = parseIntAt(POSITION.length);
+            }
+            
+            if (offsets) {
+              readLine();
+              assert scratch.startsWith(STARTOFFSET);
+              postings.startOffsets[k] = parseIntAt(STARTOFFSET.length);
+              
+              readLine();
+              assert scratch.startsWith(ENDOFFSET);
+              postings.endOffsets[k] = parseIntAt(ENDOFFSET.length);
+            }
+          }
+        }
+      }
+    }
+    return new SimpleTVFields(fields);
+  }
+
+  @Override
+  public TermVectorsReader clone() {
+    if (in == null) {
+      throw new AlreadyClosedException("this TermVectorsReader is closed");
+    }
+    return new SimpleTextTermVectorsReader(offsets, (IndexInput) in.clone());
+  }
+  
+  @Override
+  public void close() throws IOException {
+    try {
+      IOUtils.close(in); 
+    } finally {
+      in = null;
+      offsets = null;
+    }
+  }
+  
+  public static void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    if (info.getHasVectors()) {
+      files.add(IndexFileNames.segmentFileName(info.name, "", VECTORS_EXTENSION));
+    }
+  }
+  
+  private void readLine() throws IOException {
+    SimpleTextUtil.readLine(in, scratch);
+  }
+  
+  private int parseIntAt(int offset) throws IOException {
+    UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+offset, scratch.length-offset, scratchUTF16);
+    return ArrayUtil.parseInt(scratchUTF16.chars, 0, scratchUTF16.length);
+  }
+  
+  private String readString(int offset, BytesRef scratch) {
+    UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+offset, scratch.length-offset, scratchUTF16);
+    return scratchUTF16.toString();
+  }
+  
+  private class SimpleTVFields extends Fields {
+    private final SortedMap<String,SimpleTVTerms> fields;
+    
+    SimpleTVFields(SortedMap<String,SimpleTVTerms> fields) throws IOException {
+      this.fields = fields;
+    }
+
+    @Override
+    public FieldsEnum iterator() throws IOException {
+      return new FieldsEnum() {
+        private Iterator<Map.Entry<String,SimpleTVTerms>> iterator = fields.entrySet().iterator();
+        private Map.Entry<String,SimpleTVTerms> current = null;
+        
+        @Override
+        public String next() throws IOException {
+          if (!iterator.hasNext()) {
+            return null;
+          } else {
+            current = iterator.next();
+            return current.getKey();
+          }
+        }
+
+        @Override
+        public Terms terms() throws IOException {
+          return current.getValue();
+        }
+      };
+    }
+
+    @Override
+    public Terms terms(String field) throws IOException {
+      return fields.get(field);
+    }
+
+    @Override
+    public int getUniqueFieldCount() throws IOException {
+      return fields.size();
+    }
+  }
+  
+  private static class SimpleTVTerms extends Terms {
+    final SortedMap<BytesRef,SimpleTVPostings> terms;
+    
+    SimpleTVTerms() {
+      terms = new TreeMap<BytesRef,SimpleTVPostings>();
+    }
+    
+    @Override
+    public TermsEnum iterator(TermsEnum reuse) throws IOException {
+      // TODO: reuse
+      return new SimpleTVTermsEnum(terms);
+    }
+
+    @Override
+    public Comparator<BytesRef> getComparator() throws IOException {
+      return BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+
+    @Override
+    public long getUniqueTermCount() throws IOException {
+      return terms.size();
+    }
+
+    @Override
+    public long getSumTotalTermFreq() throws IOException {
+      return -1;
+    }
+
+    @Override
+    public long getSumDocFreq() throws IOException {
+      return terms.size();
+    }
+
+    @Override
+    public int getDocCount() throws IOException {
+      return 1;
+    }
+  }
+  
+  private static class SimpleTVPostings {
+    private int freq;
+    private int positions[];
+    private int startOffsets[];
+    private int endOffsets[];
+  }
+  
+  private static class SimpleTVTermsEnum extends TermsEnum {
+    SortedMap<BytesRef,SimpleTVPostings> terms;
+    Iterator<Map.Entry<BytesRef,SimpleTextTermVectorsReader.SimpleTVPostings>> iterator;
+    Map.Entry<BytesRef,SimpleTextTermVectorsReader.SimpleTVPostings> current;
+    
+    SimpleTVTermsEnum(SortedMap<BytesRef,SimpleTVPostings> terms) {
+      this.terms = terms;
+      this.iterator = terms.entrySet().iterator();
+    }
+    
+    @Override
+    public SeekStatus seekCeil(BytesRef text, boolean useCache) throws IOException {
+      iterator = terms.tailMap(text).entrySet().iterator();
+      if (!iterator.hasNext()) {
+        return SeekStatus.END;
+      } else {
+        return next().equals(text) ? SeekStatus.FOUND : SeekStatus.NOT_FOUND;
+      }
+    }
+
+    @Override
+    public void seekExact(long ord) throws IOException {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public BytesRef next() throws IOException {
+      if (!iterator.hasNext()) {
+        return null;
+      } else {
+        current = iterator.next();
+        return current.getKey();
+      }
+    }
+
+    @Override
+    public BytesRef term() throws IOException {
+      return current.getKey();
+    }
+
+    @Override
+    public long ord() throws IOException {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public int docFreq() throws IOException {
+      return 1;
+    }
+
+    @Override
+    public long totalTermFreq() throws IOException {
+      return current.getValue().freq;
+    }
+
+    @Override
+    public DocsEnum docs(Bits liveDocs, DocsEnum reuse) throws IOException {
+      // TODO: reuse
+      SimpleTVDocsEnum e = new SimpleTVDocsEnum();
+      e.reset(liveDocs, current.getValue().freq);
+      return e;
+    }
+
+    @Override
+    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse) throws IOException {
+      SimpleTVPostings postings = current.getValue();
+      if (postings.positions == null && postings.startOffsets == null) {
+        return null;
+      }
+      // TODO: reuse
+      SimpleTVDocsAndPositionsEnum e = new SimpleTVDocsAndPositionsEnum(postings.startOffsets != null);
+      e.reset(liveDocs, postings.positions, postings.startOffsets, postings.endOffsets);
+      return e;
+    }
+
+    @Override
+    public Comparator<BytesRef> getComparator() throws IOException {
+      return BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+  }
+  
+  // note: these two enum classes are exactly like the Default impl...
+  private static class SimpleTVDocsEnum extends DocsEnum {
+    private boolean didNext;
+    private int freq;
+    private Bits liveDocs;
+
+    @Override
+    public int freq() {
+      return freq;
+    }
+
+    @Override
+    public int docID() {
+      return 0;
+    }
+
+    @Override
+    public int nextDoc() {
+      if (!didNext && (liveDocs == null || liveDocs.get(0))) {
+        didNext = true;
+        return 0;
+      } else {
+        return NO_MORE_DOCS;
+      }
+    }
+
+    @Override
+    public int advance(int target) {
+      if (!didNext && target == 0) {
+        return nextDoc();
+      } else {
+        return NO_MORE_DOCS;
+      }
+    }
+
+    public void reset(Bits liveDocs, int freq) {
+      this.liveDocs = liveDocs;
+      this.freq = freq;
+      didNext = false;
+    }
+  }
+  
+  private static class SimpleTVDocsAndPositionsEnum extends DocsAndPositionsEnum {
+    private final OffsetAttribute offsetAtt;
+    private boolean didNext;
+    private int nextPos;
+    private Bits liveDocs;
+    private int[] positions;
+    private int[] startOffsets;
+    private int[] endOffsets;
+
+    public SimpleTVDocsAndPositionsEnum(boolean storeOffsets) {
+      if (storeOffsets) {
+        offsetAtt = attributes().addAttribute(OffsetAttribute.class);
+      } else {
+        offsetAtt = null;
+      }
+    }
+
+    public boolean canReuse(boolean storeOffsets) {
+      return storeOffsets == (offsetAtt != null);
+    }
+
+    @Override
+    public int freq() {
+      if (positions != null) {
+        return positions.length;
+      } else {
+        assert startOffsets != null;
+        return startOffsets.length;
+      }
+    }
+
+    @Override
+    public int docID() {
+      return 0;
+    }
+
+    @Override
+    public int nextDoc() {
+      if (!didNext && (liveDocs == null || liveDocs.get(0))) {
+        didNext = true;
+        return 0;
+      } else {
+        return NO_MORE_DOCS;
+      }
+    }
+
+    @Override
+    public int advance(int target) {
+      if (!didNext && target == 0) {
+        return nextDoc();
+      } else {
+        return NO_MORE_DOCS;
+      }
+    }
+
+    public void reset(Bits liveDocs, int[] positions, int[] startOffsets, int[] endOffsets) {
+      this.liveDocs = liveDocs;
+      this.positions = positions;
+      this.startOffsets = startOffsets;
+      assert (offsetAtt != null) == (startOffsets != null);
+      this.endOffsets = endOffsets;
+      didNext = false;
+      nextPos = 0;
+    }
+
+    @Override
+    public BytesRef getPayload() {
+      return null;
+    }
+
+    @Override
+    public boolean hasPayload() {
+      return false;
+    }
+
+    @Override
+    public int nextPosition() {
+      assert (positions != null && nextPos < positions.length) ||
+        startOffsets != null && nextPos < startOffsets.length;
+
+      if (startOffsets != null) {
+        offsetAtt.setOffset(startOffsets[nextPos],
+                            endOffsets[nextPos]);
+      }
+      if (positions != null) {
+        return positions[nextPos++];
+      } else {
+        nextPos++;
+        return -1;
+      }
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextTermVectorsWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextTermVectorsWriter.java
new file mode 100644
index 0000000..be59151
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextTermVectorsWriter.java
@@ -0,0 +1,179 @@
+package org.apache.lucene.index.codecs.simpletext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.codecs.TermVectorsWriter;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Writes plain-text term vectors.
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextTermVectorsWriter extends TermVectorsWriter {
+  
+  static final BytesRef END                = new BytesRef("END");
+  static final BytesRef DOC                = new BytesRef("doc ");
+  static final BytesRef NUMFIELDS          = new BytesRef("  numfields ");
+  static final BytesRef FIELD              = new BytesRef("  field ");
+  static final BytesRef FIELDNAME          = new BytesRef("    name ");
+  static final BytesRef FIELDPOSITIONS     = new BytesRef("    positions ");
+  static final BytesRef FIELDOFFSETS       = new BytesRef("    offsets   ");
+  static final BytesRef FIELDTERMCOUNT     = new BytesRef("    numterms ");
+  static final BytesRef TERMTEXT           = new BytesRef("    term ");
+  static final BytesRef TERMFREQ           = new BytesRef("      freq ");
+  static final BytesRef POSITION           = new BytesRef("      position ");
+  static final BytesRef STARTOFFSET        = new BytesRef("        startoffset ");
+  static final BytesRef ENDOFFSET          = new BytesRef("        endoffset ");
+
+  static final String VECTORS_EXTENSION = "vec";
+  
+  private final Directory directory;
+  private final String segment;
+  private IndexOutput out;
+  private int numDocsWritten = 0;
+  private final BytesRef scratch = new BytesRef();
+  private boolean offsets;
+  private boolean positions;
+
+  public SimpleTextTermVectorsWriter(Directory directory, String segment, IOContext context) throws IOException {
+    this.directory = directory;
+    this.segment = segment;
+    out = directory.createOutput(IndexFileNames.segmentFileName(segment, "", VECTORS_EXTENSION), context);
+  }
+  
+  @Override
+  public void startDocument(int numVectorFields) throws IOException {
+    write(DOC);
+    write(Integer.toString(numDocsWritten));
+    newLine();
+    
+    write(NUMFIELDS);
+    write(Integer.toString(numVectorFields));
+    newLine();
+    numDocsWritten++;
+  }
+
+  @Override
+  public void startField(FieldInfo info, int numTerms, boolean positions, boolean offsets) throws IOException {  
+    write(FIELD);
+    write(Integer.toString(info.number));
+    newLine();
+    
+    write(FIELDNAME);
+    write(info.name);
+    newLine();
+    
+    write(FIELDPOSITIONS);
+    write(Boolean.toString(positions));
+    newLine();
+    
+    write(FIELDOFFSETS);
+    write(Boolean.toString(offsets));
+    newLine();
+    
+    write(FIELDTERMCOUNT);
+    write(Integer.toString(numTerms));
+    newLine();
+    
+    this.positions = positions;
+    this.offsets = offsets;
+  }
+
+  @Override
+  public void startTerm(BytesRef term, int freq) throws IOException {
+    write(TERMTEXT);
+    write(term);
+    newLine();
+    
+    write(TERMFREQ);
+    write(Integer.toString(freq));
+    newLine();
+  }
+
+  @Override
+  public void addPosition(int position, int startOffset, int endOffset) throws IOException {
+    assert positions || offsets;
+    
+    if (positions) {
+      write(POSITION);
+      write(Integer.toString(position));
+      newLine();
+    }
+    
+    if (offsets) {
+      write(STARTOFFSET);
+      write(Integer.toString(startOffset));
+      newLine();
+      
+      write(ENDOFFSET);
+      write(Integer.toString(endOffset));
+      newLine();
+    }
+  }
+
+  @Override
+  public void abort() {
+    try {
+      close();
+    } catch (IOException ignored) {}
+    
+    try {
+      directory.deleteFile(IndexFileNames.segmentFileName(segment, "", VECTORS_EXTENSION));
+    } catch (IOException ignored) {}
+  }
+
+  @Override
+  public void finish(int numDocs) throws IOException {
+    if (numDocsWritten != numDocs) {
+      throw new RuntimeException("mergeVectors produced an invalid result: mergedDocs is " + numDocs + " but vec numDocs is " + numDocsWritten + " file=" + out.toString() + "; now aborting this merge to prevent index corruption");
+    }
+    write(END);
+    newLine();
+  }
+  
+  @Override
+  public void close() throws IOException {
+    try {
+      IOUtils.close(out);
+    } finally {
+      out = null;
+    }
+  }
+  
+  private void write(String s) throws IOException {
+    SimpleTextUtil.write(out, s, scratch);
+  }
+  
+  private void write(BytesRef bytes) throws IOException {
+    SimpleTextUtil.write(out, bytes);
+  }
+  
+  private void newLine() throws IOException {
+    SimpleTextUtil.writeNewline(out);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java b/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java
index ff791b2..e93c638 100644
--- a/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java
+++ b/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java
@@ -333,7 +333,7 @@ class FieldCacheImpl implements FieldCache {
             setDocsWithField = false;
           }
         }
-        final TermsEnum termsEnum = terms.iterator();
+        final TermsEnum termsEnum = terms.iterator(null);
         DocsEnum docs = null;
         try {
           while(true) {
@@ -406,7 +406,7 @@ class FieldCacheImpl implements FieldCache {
             setDocsWithField = false;
           }
         }
-        final TermsEnum termsEnum = terms.iterator();
+        final TermsEnum termsEnum = terms.iterator(null);
         DocsEnum docs = null;
         try {
           while(true) {
@@ -505,7 +505,7 @@ class FieldCacheImpl implements FieldCache {
             setDocsWithField = false;
           }
         }
-        final TermsEnum termsEnum = terms.iterator();
+        final TermsEnum termsEnum = terms.iterator(null);
         DocsEnum docs = null;
         try {
           while(true) {
@@ -574,7 +574,7 @@ class FieldCacheImpl implements FieldCache {
           // Fast case: all docs have this field:
           return new Bits.MatchAllBits(maxDoc);
         }
-        final TermsEnum termsEnum = terms.iterator();
+        final TermsEnum termsEnum = terms.iterator(null);
         DocsEnum docs = null;
         while(true) {
           final BytesRef term = termsEnum.next();
@@ -655,7 +655,7 @@ class FieldCacheImpl implements FieldCache {
             setDocsWithField = false;
           }
         }
-        final TermsEnum termsEnum = terms.iterator();
+        final TermsEnum termsEnum = terms.iterator(null);
         DocsEnum docs = null;
         try {
           while(true) {
@@ -743,7 +743,7 @@ class FieldCacheImpl implements FieldCache {
             setDocsWithField = false;
           }
         }
-        final TermsEnum termsEnum = terms.iterator();
+        final TermsEnum termsEnum = terms.iterator(null);
         DocsEnum docs = null;
         try {
           while(true) {
@@ -832,7 +832,7 @@ class FieldCacheImpl implements FieldCache {
             setDocsWithField = false;
           }
         }
-        final TermsEnum termsEnum = terms.iterator();
+        final TermsEnum termsEnum = terms.iterator(null);
         DocsEnum docs = null;
         try {
           while(true) {
@@ -1128,7 +1128,7 @@ class FieldCacheImpl implements FieldCache {
       int termOrd = 1;
 
       if (terms != null) {
-        final TermsEnum termsEnum = terms.iterator();
+        final TermsEnum termsEnum = terms.iterator(null);
         DocsEnum docs = null;
 
         while(true) {
@@ -1253,7 +1253,7 @@ class FieldCacheImpl implements FieldCache {
 
       if (terms != null) {
         int termCount = 0;
-        final TermsEnum termsEnum = terms.iterator();
+        final TermsEnum termsEnum = terms.iterator(null);
         DocsEnum docs = null;
         while(true) {
           if (termCount++ == termCountHardLimit) {
diff --git a/lucene/src/java/org/apache/lucene/search/FuzzyQuery.java b/lucene/src/java/org/apache/lucene/search/FuzzyQuery.java
index c0a4333..25fabe2 100644
--- a/lucene/src/java/org/apache/lucene/search/FuzzyQuery.java
+++ b/lucene/src/java/org/apache/lucene/search/FuzzyQuery.java
@@ -139,7 +139,7 @@ public class FuzzyQuery extends MultiTermQuery {
   @Override
   protected TermsEnum getTermsEnum(Terms terms, AttributeSource atts) throws IOException {
     if (!termLongEnough) {  // can only match if it's exact
-      return new SingleTermsEnum(terms.iterator(), term.bytes());
+      return new SingleTermsEnum(terms.iterator(null), term.bytes());
     }
     return new FuzzyTermsEnum(terms, atts, getTerm(), minimumSimilarity, prefixLength);
   }
diff --git a/lucene/src/java/org/apache/lucene/search/FuzzyTermsEnum.java b/lucene/src/java/org/apache/lucene/search/FuzzyTermsEnum.java
index 568b365..a84a2da 100644
--- a/lucene/src/java/org/apache/lucene/search/FuzzyTermsEnum.java
+++ b/lucene/src/java/org/apache/lucene/search/FuzzyTermsEnum.java
@@ -406,7 +406,7 @@ public final class FuzzyTermsEnum extends TermsEnum {
      * @throws IOException
      */
     public LinearFuzzyTermsEnum() throws IOException {
-      super(terms.iterator());
+      super(terms.iterator(null));
 
       this.text = new int[termLength - realPrefixLength];
       System.arraycopy(termText, realPrefixLength, text, 0, text.length);
diff --git a/lucene/src/java/org/apache/lucene/search/NumericRangeQuery.java b/lucene/src/java/org/apache/lucene/search/NumericRangeQuery.java
index b12bbb2..538cdd2 100644
--- a/lucene/src/java/org/apache/lucene/search/NumericRangeQuery.java
+++ b/lucene/src/java/org/apache/lucene/search/NumericRangeQuery.java
@@ -305,7 +305,7 @@ public final class NumericRangeQuery<T extends Number> extends MultiTermQuery {
     // very strange: java.lang.Number itsself is not Comparable, but all subclasses used here are
     return (min != null && max != null && ((Comparable<T>) min).compareTo(max) > 0) ?
       TermsEnum.EMPTY :
-      new NumericRangeTermsEnum(terms.iterator());
+      new NumericRangeTermsEnum(terms.iterator(null));
   }
 
   /** Returns <code>true</code> if the lower endpoint is inclusive */
diff --git a/lucene/src/java/org/apache/lucene/search/PrefixQuery.java b/lucene/src/java/org/apache/lucene/search/PrefixQuery.java
index 42cdebf..31e88ea 100644
--- a/lucene/src/java/org/apache/lucene/search/PrefixQuery.java
+++ b/lucene/src/java/org/apache/lucene/search/PrefixQuery.java
@@ -45,7 +45,7 @@ public class PrefixQuery extends MultiTermQuery {
   
   @Override  
   protected TermsEnum getTermsEnum(Terms terms, AttributeSource atts) throws IOException {
-    TermsEnum tenum = terms.iterator();
+    TermsEnum tenum = terms.iterator(null);
     
     if (prefix.bytes().length == 0) {
       // no prefix -- match all terms for this field:
diff --git a/lucene/src/java/org/apache/lucene/search/QueryTermVector.java b/lucene/src/java/org/apache/lucene/search/QueryTermVector.java
deleted file mode 100644
index 2eb7575..0000000
--- a/lucene/src/java/org/apache/lucene/search/QueryTermVector.java
+++ /dev/null
@@ -1,157 +0,0 @@
-package org.apache.lucene.search;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.StringReader;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashMap;
-
-import java.util.List;
-import java.util.Map;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;
-import org.apache.lucene.index.TermFreqVector;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-
-/**
- *
- *
- **/
-public class QueryTermVector implements TermFreqVector {
-  private BytesRef [] terms = new BytesRef[0];
-  private int [] termFreqs = new int[0];
-
-  public String getField() { return null;  }
-
-  /**
-   * 
-   * @param queryTerms The original list of terms from the query, can contain duplicates
-   */ 
-  public QueryTermVector(BytesRef [] queryTerms) {
-
-    processTerms(queryTerms);
-  }
-
-  public QueryTermVector(String queryString, Analyzer analyzer) {    
-    if (analyzer != null)
-    {
-      TokenStream stream;
-      try {
-        stream = analyzer.tokenStream("", new StringReader(queryString));
-      } catch (IOException e1) {
-        stream = null;
-      }
-      if (stream != null)
-      {
-        List<BytesRef> terms = new ArrayList<BytesRef>();
-        try {
-          boolean hasMoreTokens = false;
-          
-          stream.reset(); 
-          final TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);
-
-          hasMoreTokens = stream.incrementToken();
-          BytesRef bytes = termAtt.getBytesRef();
-          while (hasMoreTokens) {
-            termAtt.fillBytesRef();
-            terms.add(new BytesRef(bytes));
-            hasMoreTokens = stream.incrementToken();
-          }
-          processTerms(terms.toArray(new BytesRef[terms.size()]));
-        } catch (IOException e) {
-        }
-      }
-    }                                                              
-  }
-  
-  private void processTerms(BytesRef[] queryTerms) {
-    if (queryTerms != null) {
-      ArrayUtil.quickSort(queryTerms);
-      Map<BytesRef,Integer> tmpSet = new HashMap<BytesRef,Integer>(queryTerms.length);
-      //filter out duplicates
-      List<BytesRef> tmpList = new ArrayList<BytesRef>(queryTerms.length);
-      List<Integer> tmpFreqs = new ArrayList<Integer>(queryTerms.length);
-      int j = 0;
-      for (int i = 0; i < queryTerms.length; i++) {
-        BytesRef term = queryTerms[i];
-        Integer position = tmpSet.get(term);
-        if (position == null) {
-          tmpSet.put(term, Integer.valueOf(j++));
-          tmpList.add(term);
-          tmpFreqs.add(Integer.valueOf(1));
-        }       
-        else {
-          Integer integer = tmpFreqs.get(position.intValue());
-          tmpFreqs.set(position.intValue(), Integer.valueOf(integer.intValue() + 1));          
-        }
-      }
-      terms = tmpList.toArray(terms);
-      //termFreqs = (int[])tmpFreqs.toArray(termFreqs);
-      termFreqs = new int[tmpFreqs.size()];
-      int i = 0;
-      for (final Integer integer : tmpFreqs) {
-        termFreqs[i++] = integer.intValue();
-      }
-    }
-  }
-  
-  @Override
-  public final String toString() {
-        StringBuilder sb = new StringBuilder();
-        sb.append('{');
-        for (int i=0; i<terms.length; i++) {
-            if (i>0) sb.append(", ");
-            sb.append(terms[i].utf8ToString()).append('/').append(termFreqs[i]);
-        }
-        sb.append('}');
-        return sb.toString();
-    }
-  
-
-  public int size() {
-    return terms.length;
-  }
-
-  public BytesRef[] getTerms() {
-    return terms;
-  }
-
-  public int[] getTermFrequencies() {
-    return termFreqs;
-  }
-
-  public int indexOf(BytesRef term) {
-    int res = Arrays.binarySearch(terms, term);
-        return res >= 0 ? res : -1;
-  }
-
-  public int[] indexesOf(BytesRef[] terms, int start, int len) {
-    int res[] = new int[len];
-
-    for (int i=0; i < len; i++) {
-        res[i] = indexOf(terms[i]);
-    }
-    return res;                  
-  }
-
-}
diff --git a/lucene/src/java/org/apache/lucene/search/TermRangeQuery.java b/lucene/src/java/org/apache/lucene/search/TermRangeQuery.java
index 084b970..c55a433 100644
--- a/lucene/src/java/org/apache/lucene/search/TermRangeQuery.java
+++ b/lucene/src/java/org/apache/lucene/search/TermRangeQuery.java
@@ -103,7 +103,7 @@ public class TermRangeQuery extends MultiTermQuery {
       return TermsEnum.EMPTY;
     }
     
-    TermsEnum tenum = terms.iterator();
+    TermsEnum tenum = terms.iterator(null);
     
     if ((lowerTerm == null || (includeLower && lowerTerm.length == 0)) && upperTerm == null) {
       return tenum;
diff --git a/lucene/src/java/org/apache/lucene/util/StringHelper.java b/lucene/src/java/org/apache/lucene/util/StringHelper.java
index d57bb3a..fa4ae2b 100644
--- a/lucene/src/java/org/apache/lucene/util/StringHelper.java
+++ b/lucene/src/java/org/apache/lucene/util/StringHelper.java
@@ -43,6 +43,22 @@ public abstract class StringHelper {
         return i;
     return len;
   }
+  
+  /**
+   * Compares two byte[] arrays, element by element, and returns the
+   * number of elements common to both arrays.
+   *
+   * @param bytes1 The first byte[] to compare
+   * @param bytes2 The second byte[] to compare
+   * @return The number of common elements.
+   */
+  public static int bytesDifference(byte[] bytes1, int off1, int len1, byte[] bytes2, int off2, int len2) {
+    int len = len1 < len2 ? len1 : len2;
+    for (int i = 0; i < len; i++)
+      if (bytes1[i+off1] != bytes2[i+off2])
+        return i;
+    return len;
+  }
 
   private StringHelper() {
   }
diff --git a/lucene/src/java/org/apache/lucene/util/automaton/CompiledAutomaton.java b/lucene/src/java/org/apache/lucene/util/automaton/CompiledAutomaton.java
index 092c6d3..86b087e 100644
--- a/lucene/src/java/org/apache/lucene/util/automaton/CompiledAutomaton.java
+++ b/lucene/src/java/org/apache/lucene/util/automaton/CompiledAutomaton.java
@@ -198,13 +198,13 @@ public class CompiledAutomaton {
     case NONE:
       return TermsEnum.EMPTY;
     case ALL:
-      return terms.iterator();
+      return terms.iterator(null);
     case SINGLE:
-      return new SingleTermsEnum(terms.iterator(), term);
+      return new SingleTermsEnum(terms.iterator(null), term);
     case PREFIX:
       // TODO: this is very likely faster than .intersect,
       // but we should test and maybe cutover
-      return new PrefixTermsEnum(terms.iterator(), term);
+      return new PrefixTermsEnum(terms.iterator(null), term);
     case NORMAL:
       return terms.intersect(this, null);
     default:
diff --git a/lucene/src/test-framework/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase.java b/lucene/src/test-framework/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase.java
index 6858348..3ce2d9e 100644
--- a/lucene/src/test-framework/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase.java
+++ b/lucene/src/test-framework/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase.java
@@ -348,7 +348,7 @@ public abstract class ThreadedIndexingAndSearchingTestCase extends LuceneTestCas
                     if (terms == null) {
                       continue;
                     }
-                    TermsEnum termsEnum = terms.iterator();
+                    TermsEnum termsEnum = terms.iterator(null);
                     int seenTermCount = 0;
                     int shift;
                     int trigger; 
diff --git a/lucene/src/test-framework/java/org/apache/lucene/index/codecs/ramonly/RAMOnlyPostingsFormat.java b/lucene/src/test-framework/java/org/apache/lucene/index/codecs/ramonly/RAMOnlyPostingsFormat.java
index f259b54..1891d24 100644
--- a/lucene/src/test-framework/java/org/apache/lucene/index/codecs/ramonly/RAMOnlyPostingsFormat.java
+++ b/lucene/src/test-framework/java/org/apache/lucene/index/codecs/ramonly/RAMOnlyPostingsFormat.java
@@ -109,6 +109,11 @@ public class RAMOnlyPostingsFormat extends PostingsFormat {
     }
 
     @Override
+    public int getUniqueFieldCount() {
+      return fieldToTerms.size();
+    }
+
+    @Override
     public FieldsEnum iterator() {
       return new RAMFieldsEnum(this);
     }
@@ -150,7 +155,7 @@ public class RAMOnlyPostingsFormat extends PostingsFormat {
     }
 
     @Override
-    public TermsEnum iterator() {
+    public TermsEnum iterator(TermsEnum reuse) {
       return new RAMTermsEnum(RAMOnlyPostingsFormat.RAMField.this);
     }
 
@@ -300,8 +305,8 @@ public class RAMOnlyPostingsFormat extends PostingsFormat {
     }
 
     @Override
-    public TermsEnum terms() {
-      return new RAMTermsEnum(postings.fieldToTerms.get(current));
+    public Terms terms() {
+      return postings.fieldToTerms.get(current);
     }
   }
 
diff --git a/lucene/src/test-framework/java/org/apache/lucene/store/MockDirectoryWrapper.java b/lucene/src/test-framework/java/org/apache/lucene/store/MockDirectoryWrapper.java
index 55b6724..072492e 100644
--- a/lucene/src/test-framework/java/org/apache/lucene/store/MockDirectoryWrapper.java
+++ b/lucene/src/test-framework/java/org/apache/lucene/store/MockDirectoryWrapper.java
@@ -149,8 +149,7 @@ public class MockDirectoryWrapper extends Directory {
   @Override
   public synchronized void sync(Collection<String> names) throws IOException {
     maybeYield();
-    for (String name : names)
-      maybeThrowDeterministicException();
+    maybeThrowDeterministicException();
     if (crashed)
       throw new IOException("cannot sync after crash");
     unSyncedFiles.removeAll(names);
@@ -349,9 +348,18 @@ public class MockDirectoryWrapper extends Directory {
     return new HashSet<String>(openFilesDeleted);
   }
 
+  private boolean failOnCreateOutput = true;
+
+  public void setFailOnCreateOutput(boolean v) {
+    failOnCreateOutput = v;
+  }
+  
   @Override
   public synchronized IndexOutput createOutput(String name, IOContext context) throws IOException {
     maybeYield();
+    if (failOnCreateOutput) {
+      maybeThrowDeterministicException();
+    }
     if (crashed)
       throw new IOException("cannot createOutput after crash");
     init();
@@ -416,10 +424,19 @@ public class MockDirectoryWrapper extends Directory {
     
     openFileHandles.put(c, new RuntimeException("unclosed Index" + handle.name() + ": " + name));
   }
-  
+
+  private boolean failOnOpenInput = true;
+
+  public void setFailOnOpenInput(boolean v) {
+    failOnOpenInput = v;
+  }
+
   @Override
   public synchronized IndexInput openInput(String name, IOContext context) throws IOException {
     maybeYield();
+    if (failOnOpenInput) {
+      maybeThrowDeterministicException();
+    }
     if (!delegate.fileExists(name))
       throw new FileNotFoundException(name);
 
diff --git a/lucene/src/test/org/apache/lucene/index/Test2BTerms.java b/lucene/src/test/org/apache/lucene/index/Test2BTerms.java
index 3ec43ac..cda2b60 100644
--- a/lucene/src/test/org/apache/lucene/index/Test2BTerms.java
+++ b/lucene/src/test/org/apache/lucene/index/Test2BTerms.java
@@ -225,7 +225,7 @@ public class Test2BTerms extends LuceneTestCase {
 
   private List<BytesRef> findTerms(IndexReader r) throws IOException {
     System.out.println("TEST: findTerms");
-    final TermsEnum termsEnum = MultiFields.getTerms(r, "field").iterator();
+    final TermsEnum termsEnum = MultiFields.getTerms(r, "field").iterator(null);
     final List<BytesRef> savedTerms = new ArrayList<BytesRef>();
     int nextSave = _TestUtil.nextInt(random, 500000, 1000000);
     BytesRef term;
@@ -243,7 +243,7 @@ public class Test2BTerms extends LuceneTestCase {
     System.out.println("TEST: run " + terms.size() + " terms on reader=" + r);
     IndexSearcher s = new IndexSearcher(r);
     Collections.shuffle(terms);
-    TermsEnum termsEnum = MultiFields.getTerms(r, "field").iterator();
+    TermsEnum termsEnum = MultiFields.getTerms(r, "field").iterator(null);
     boolean failed = false;
     for(int iter=0;iter<10*terms.size();iter++) {
       final BytesRef term = terms.get(random.nextInt(terms.size()));
diff --git a/lucene/src/test/org/apache/lucene/index/TestAddIndexes.java b/lucene/src/test/org/apache/lucene/index/TestAddIndexes.java
index c06a313..547d870 100755
--- a/lucene/src/test/org/apache/lucene/index/TestAddIndexes.java
+++ b/lucene/src/test/org/apache/lucene/index/TestAddIndexes.java
@@ -32,12 +32,16 @@ import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
 import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.index.codecs.DefaultDocValuesFormat;
+import org.apache.lucene.index.codecs.DefaultFieldInfosFormat;
 import org.apache.lucene.index.codecs.DefaultStoredFieldsFormat;
 import org.apache.lucene.index.codecs.DefaultSegmentInfosFormat;
+import org.apache.lucene.index.codecs.DefaultTermVectorsFormat;
 import org.apache.lucene.index.codecs.DocValuesFormat;
+import org.apache.lucene.index.codecs.FieldInfosFormat;
 import org.apache.lucene.index.codecs.StoredFieldsFormat;
 import org.apache.lucene.index.codecs.PostingsFormat;
 import org.apache.lucene.index.codecs.SegmentInfosFormat;
+import org.apache.lucene.index.codecs.TermVectorsFormat;
 import org.apache.lucene.index.codecs.lucene40.Lucene40Codec;
 import org.apache.lucene.index.codecs.pulsing.Pulsing40PostingsFormat;
 import org.apache.lucene.search.DocIdSetIterator;
@@ -1160,6 +1164,16 @@ public class TestAddIndexes extends LuceneTestCase {
     public StoredFieldsFormat storedFieldsFormat() {
       return new DefaultStoredFieldsFormat();
     }
+    
+    @Override
+    public TermVectorsFormat termVectorsFormat() {
+      return new DefaultTermVectorsFormat();
+    }
+    
+    @Override
+    public FieldInfosFormat fieldInfosFormat() {
+      return new DefaultFieldInfosFormat();
+    }
 
     @Override
     public SegmentInfosFormat segmentInfosFormat() {
diff --git a/lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java b/lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
index 53d234c..62e0775 100644
--- a/lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
+++ b/lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
@@ -35,6 +35,8 @@ import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
+import org.apache.lucene.index.codecs.Codec;
+import org.apache.lucene.index.codecs.FieldInfosReader;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.IndexSearcher;
@@ -44,6 +46,7 @@ import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.search.similarities.DefaultSimilarity;
 import org.apache.lucene.store.CompoundFileDirectory;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
@@ -272,7 +275,7 @@ public class TestBackwardsCompatibility extends LuceneTestCase {
     assertEquals("wrong number of hits", expectedCount, hitCount);
     for(int i=0;i<hitCount;i++) {
       reader.document(hits[i].doc);
-      reader.getTermFreqVectors(hits[i].doc);
+      reader.getTermVectors(hits[i].doc);
     }
   }
 
@@ -311,9 +314,8 @@ public class TestBackwardsCompatibility extends LuceneTestCase {
           assertEquals("field with non-ascii name", f.stringValue());
         }
 
-        TermFreqVector tfv = reader.getTermFreqVector(i, "utf8");
+        Terms tfv = reader.getTermVectors(i).terms("utf8");
         assertNotNull("docID=" + i + " index=" + indexDir.getName(), tfv);
-        assertTrue(tfv instanceof TermPositionVector);
       } else
         // Only ID 7 is deleted
         assertEquals(7, i);
@@ -564,7 +566,8 @@ public class TestBackwardsCompatibility extends LuceneTestCase {
       // "content", and then set our expected file names below
       // accordingly:
       CompoundFileDirectory cfsReader = new CompoundFileDirectory(dir, "_0.cfs", newIOContext(random), false);
-      FieldInfos fieldInfos = new FieldInfos(cfsReader, "_0.fnm");
+      FieldInfosReader infosReader = Codec.getDefault().fieldInfosFormat().getFieldInfosReader();
+      FieldInfos fieldInfos = infosReader.read(cfsReader, "_0", IOContext.READONCE);
       int contentFieldIndex = -1;
       for (FieldInfo fi : fieldInfos) {
         if (fi.name.equals("content")) {
@@ -654,7 +657,7 @@ public class TestBackwardsCompatibility extends LuceneTestCase {
     	_TestUtil.unzip(getDataFile("index." + oldNames[i] + ".zip"), oldIndexDir);
       Directory dir = newFSDirectory(oldIndexDir);
       IndexReader r = IndexReader.open(dir);
-      TermsEnum terms = MultiFields.getFields(r).terms("content").iterator();
+      TermsEnum terms = MultiFields.getFields(r).terms("content").iterator(null);
       BytesRef t = terms.next();
       assertNotNull(t);
 
diff --git a/lucene/src/test/org/apache/lucene/index/TestCodecs.java b/lucene/src/test/org/apache/lucene/index/TestCodecs.java
index 51ff6eb..ee4ba8a 100644
--- a/lucene/src/test/org/apache/lucene/index/TestCodecs.java
+++ b/lucene/src/test/org/apache/lucene/index/TestCodecs.java
@@ -249,7 +249,7 @@ public class TestCodecs extends LuceneTestCase {
       terms[i] = new TermData(text, docs, null);
     }
 
-    final FieldInfos fieldInfos = new FieldInfos();
+    final FieldInfos fieldInfos = new FieldInfos(new FieldInfos.FieldNumberBiMap());
 
     final FieldData field = new FieldData("field", fieldInfos, terms, true, false);
     final FieldData[] fields = new FieldData[] {field};
@@ -264,7 +264,10 @@ public class TestCodecs extends LuceneTestCase {
 
     final FieldsEnum fieldsEnum = reader.iterator();
     assertNotNull(fieldsEnum.next());
-    final TermsEnum termsEnum = fieldsEnum.terms();
+    final Terms terms2 = fieldsEnum.terms();
+    assertNotNull(terms2);
+
+    final TermsEnum termsEnum = terms2.iterator(null);
 
     DocsEnum docsEnum = null;
     for(int i=0;i<NUM_TERMS;i++) {
@@ -293,7 +296,7 @@ public class TestCodecs extends LuceneTestCase {
   }
 
   public void testRandomPostings() throws Throwable {
-    final FieldInfos fieldInfos = new FieldInfos();
+    final FieldInfos fieldInfos = new FieldInfos(new FieldInfos.FieldNumberBiMap());
 
     final FieldData[] fields = new FieldData[NUM_FIELDS];
     for(int i=0;i<NUM_FIELDS;i++) {
@@ -452,7 +455,7 @@ public class TestCodecs extends LuceneTestCase {
 
       for(int iter=0;iter<NUM_TEST_ITER;iter++) {
         final FieldData field = fields[TestCodecs.random.nextInt(fields.length)];
-        final TermsEnum termsEnum = termsDict.terms(field.fieldInfo.name).iterator();
+        final TermsEnum termsEnum = termsDict.terms(field.fieldInfo.name).iterator(null);
         if (si.getCodec() instanceof Lucene3xCodec) {
           // code below expects unicode sort order
           continue;
diff --git a/lucene/src/test/org/apache/lucene/index/TestDirectoryReader.java b/lucene/src/test/org/apache/lucene/index/TestDirectoryReader.java
index 6963dc5..f267f68 100644
--- a/lucene/src/test/org/apache/lucene/index/TestDirectoryReader.java
+++ b/lucene/src/test/org/apache/lucene/index/TestDirectoryReader.java
@@ -85,8 +85,8 @@ public class TestDirectoryReader extends LuceneTestCase {
     Document newDoc2 = reader.document(1);
     assertTrue(newDoc2 != null);
     assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());
-    TermFreqVector vector = reader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);
-    assertTrue(vector != null);
+    Terms vector = reader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);
+    assertNotNull(vector);
     TestSegmentReader.checkNorms(reader);
     reader.close();
   }
@@ -165,14 +165,14 @@ public class TestDirectoryReader extends LuceneTestCase {
     MultiReader mr3 = new MultiReader(readers2);
 
     // test mixing up TermDocs and TermEnums from different readers.
-    TermsEnum te2 = MultiFields.getTerms(mr2, "body").iterator();
+    TermsEnum te2 = MultiFields.getTerms(mr2, "body").iterator(null);
     te2.seekCeil(new BytesRef("wow"));
     DocsEnum td = MultiFields.getTermDocsEnum(mr2,
                                               MultiFields.getLiveDocs(mr2),
                                               "body",
                                               te2.term());
 
-    TermsEnum te3 = MultiFields.getTerms(mr3, "body").iterator();
+    TermsEnum te3 = MultiFields.getTerms(mr3, "body").iterator(null);
     te3.seekCeil(new BytesRef("wow"));
     td = te3.docs(MultiFields.getLiveDocs(mr3),
                   td);
diff --git a/lucene/src/test/org/apache/lucene/index/TestDoc.java b/lucene/src/test/org/apache/lucene/index/TestDoc.java
index 89ac7b5..f59e564 100644
--- a/lucene/src/test/org/apache/lucene/index/TestDoc.java
+++ b/lucene/src/test/org/apache/lucene/index/TestDoc.java
@@ -130,13 +130,13 @@ public class TestDoc extends LuceneTestCase {
       printSegment(out, si2);
       writer.close();
 
-      SegmentInfo siMerge = merge(si1, si2, "merge", false);
+      SegmentInfo siMerge = merge(directory, si1, si2, "merge", false);
       printSegment(out, siMerge);
 
-      SegmentInfo siMerge2 = merge(si1, si2, "merge2", false);
+      SegmentInfo siMerge2 = merge(directory, si1, si2, "merge2", false);
       printSegment(out, siMerge2);
 
-      SegmentInfo siMerge3 = merge(siMerge, siMerge2, "merge3", false);
+      SegmentInfo siMerge3 = merge(directory, siMerge, siMerge2, "merge3", false);
       printSegment(out, siMerge3);
       
       directory.close();
@@ -164,13 +164,13 @@ public class TestDoc extends LuceneTestCase {
       printSegment(out, si2);
       writer.close();
 
-      siMerge = merge(si1, si2, "merge", true);
+      siMerge = merge(directory, si1, si2, "merge", true);
       printSegment(out, siMerge);
 
-      siMerge2 = merge(si1, si2, "merge2", true);
+      siMerge2 = merge(directory, si1, si2, "merge2", true);
       printSegment(out, siMerge2);
 
-      siMerge3 = merge(siMerge, siMerge2, "merge3", true);
+      siMerge3 = merge(directory, siMerge, siMerge2, "merge3", true);
       printSegment(out, siMerge3);
       
       directory.close();
@@ -193,14 +193,14 @@ public class TestDoc extends LuceneTestCase {
    }
 
 
-   private SegmentInfo merge(SegmentInfo si1, SegmentInfo si2, String merged, boolean useCompoundFile)
+   private SegmentInfo merge(Directory dir, SegmentInfo si1, SegmentInfo si2, String merged, boolean useCompoundFile)
    throws Exception {
       IOContext context = newIOContext(random);
       SegmentReader r1 = SegmentReader.get(true, si1, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, context);
       SegmentReader r2 = SegmentReader.get(true, si2, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, context);
 
       final Codec codec = Codec.getDefault();
-      SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), si1.dir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, merged, MergeState.CheckAbort.NONE, null, new FieldInfos(), codec, context);
+      SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), si1.dir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, merged, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, context);
 
       merger.add(r1);
       merger.add(r2);
@@ -212,7 +212,7 @@ public class TestDoc extends LuceneTestCase {
                                                false, codec, fieldInfos);
       
       if (useCompoundFile) {
-        Collection<String> filesToDelete = merger.createCompoundFile(merged + ".cfs", info, newIOContext(random));
+        Collection<String> filesToDelete = IndexWriter.createCompoundFile(dir, merged + ".cfs", MergeState.CheckAbort.NONE, info, newIOContext(random));
         info.setUseCompoundFile(true);
         for (final String fileToDelete : filesToDelete) 
           si1.dir.deleteFile(fileToDelete);
@@ -232,7 +232,9 @@ public class TestDoc extends LuceneTestCase {
       FieldsEnum fis = reader.fields().iterator();
       String field = fis.next();
       while(field != null)  {
-        TermsEnum tis = fis.terms();
+        Terms terms = fis.terms();
+        assertNotNull(terms);
+        TermsEnum tis = terms.iterator(null);
         while(tis.next() != null) {
 
           out.print("  term=" + field + ":" + tis.term());
diff --git a/lucene/src/test/org/apache/lucene/index/TestDocCount.java b/lucene/src/test/org/apache/lucene/index/TestDocCount.java
index ec7e5b5..cd52631 100644
--- a/lucene/src/test/org/apache/lucene/index/TestDocCount.java
+++ b/lucene/src/test/org/apache/lucene/index/TestDocCount.java
@@ -70,7 +70,7 @@ public class TestDocCount extends LuceneTestCase {
       Terms terms = fields.terms(field);
       int docCount = terms.getDocCount();
       FixedBitSet visited = new FixedBitSet(ir.maxDoc());
-      TermsEnum te = terms.iterator();
+      TermsEnum te = terms.iterator(null);
       while (te.next() != null) {
         DocsEnum de = te.docs(null, null);
         while (de.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
diff --git a/lucene/src/test/org/apache/lucene/index/TestDocTermOrds.java b/lucene/src/test/org/apache/lucene/index/TestDocTermOrds.java
index 89ffc82..ffd81e4 100644
--- a/lucene/src/test/org/apache/lucene/index/TestDocTermOrds.java
+++ b/lucene/src/test/org/apache/lucene/index/TestDocTermOrds.java
@@ -314,7 +314,7 @@ public class TestDocTermOrds extends LuceneTestCase {
     if (VERBOSE) {
       System.out.println("TEST: verify prefix=" + (prefixRef==null ? "null" : prefixRef.utf8ToString()));
       System.out.println("TEST: all TERMS:");
-      TermsEnum allTE = MultiFields.getTerms(r, "field").iterator();
+      TermsEnum allTE = MultiFields.getTerms(r, "field").iterator(null);
       int ord = 0;
       while(allTE.next() != null) {
         System.out.println("  ord=" + (ord++) + " term=" + allTE.term().utf8ToString());
@@ -329,7 +329,7 @@ public class TestDocTermOrds extends LuceneTestCase {
       } else {
         Terms terms = MultiFields.getTerms(r, "field");
         if (terms != null) {
-          TermsEnum termsEnum = terms.iterator();
+          TermsEnum termsEnum = terms.iterator(null);
           TermsEnum.SeekStatus result = termsEnum.seekCeil(prefixRef, false);
           if (result != TermsEnum.SeekStatus.END) {
             assertFalse("term=" + termsEnum.term().utf8ToString() + " matches prefix=" + prefixRef.utf8ToString(), termsEnum.term().startsWith(prefixRef));
diff --git a/lucene/src/test/org/apache/lucene/index/TestDocumentWriter.java b/lucene/src/test/org/apache/lucene/index/TestDocumentWriter.java
index cdaea16..ab3bf33 100644
--- a/lucene/src/test/org/apache/lucene/index/TestDocumentWriter.java
+++ b/lucene/src/test/org/apache/lucene/index/TestDocumentWriter.java
@@ -287,13 +287,13 @@ public class TestDocumentWriter extends LuceneTestCase {
 
     IndexReader reader = IndexReader.open(dir, true);
     // f1
-    TermFreqVector tfv1 = reader.getTermFreqVector(0, "f1");
+    Terms tfv1 = reader.getTermVectors(0).terms("f1");
     assertNotNull(tfv1);
-    assertEquals("the 'with_tv' setting should rule!",2,tfv1.getTerms().length);
+    assertEquals("the 'with_tv' setting should rule!",2,tfv1.getUniqueTermCount());
     // f2
-    TermFreqVector tfv2 = reader.getTermFreqVector(0, "f2");
+    Terms tfv2 = reader.getTermVectors(0).terms("f2");
     assertNotNull(tfv2);
-    assertEquals("the 'with_tv' setting should rule!",2,tfv2.getTerms().length);
+    assertEquals("the 'with_tv' setting should rule!",2,tfv2.getUniqueTermCount());
     reader.close();
   }
 
diff --git a/lucene/src/test/org/apache/lucene/index/TestFieldInfos.java b/lucene/src/test/org/apache/lucene/index/TestFieldInfos.java
index a4cb6ee..c95c0a1 100644
--- a/lucene/src/test/org/apache/lucene/index/TestFieldInfos.java
+++ b/lucene/src/test/org/apache/lucene/index/TestFieldInfos.java
@@ -21,8 +21,13 @@ import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util._TestUtil;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.codecs.Codec;
+import org.apache.lucene.index.codecs.FieldInfosReader;
+import org.apache.lucene.index.codecs.FieldInfosWriter;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexOutput;
+import org.junit.Ignore;
 
 import java.io.IOException;
 import java.util.Arrays;
@@ -42,7 +47,7 @@ public class TestFieldInfos extends LuceneTestCase {
   public FieldInfos createAndWriteFieldInfos(Directory dir, String filename) throws IOException{
   //Positive test of FieldInfos
     assertTrue(testDoc != null);
-    FieldInfos fieldInfos = new FieldInfos();
+    FieldInfos fieldInfos = new FieldInfos(new FieldInfos.FieldNumberBiMap());
     _TestUtil.add(testDoc, fieldInfos);
     //Since the complement is stored as well in the fields map
     assertTrue(fieldInfos.size() == DocHelper.all.size()); //this is all b/c we are using the no-arg constructor
@@ -52,16 +57,23 @@ public class TestFieldInfos extends LuceneTestCase {
     assertTrue(output != null);
     //Use a RAMOutputStream
   
-    fieldInfos.write(output);
+    FieldInfosWriter writer = Codec.getDefault().fieldInfosFormat().getFieldInfosWriter();
+    writer.write(dir, filename, fieldInfos, IOContext.DEFAULT);
     output.close();
     return fieldInfos;
   }
+  
+  public FieldInfos readFieldInfos(Directory dir, String filename) throws IOException {
+    FieldInfosReader reader = Codec.getDefault().fieldInfosFormat().getFieldInfosReader();
+    return reader.read(dir, filename, IOContext.DEFAULT);
+  }
+  
   public void test() throws IOException {
     String name = "testFile";
     Directory dir = newDirectory();
     FieldInfos fieldInfos = createAndWriteFieldInfos(dir, name);
-    assertTrue(dir.fileLength(name) > 0);
-    FieldInfos readIn = new FieldInfos(dir, name);
+
+    FieldInfos readIn = readFieldInfos(dir, name);
     assertTrue(fieldInfos.size() == readIn.size());
     FieldInfo info = readIn.fieldInfo("textField1");
     assertTrue(info != null);
@@ -90,7 +102,7 @@ public class TestFieldInfos extends LuceneTestCase {
     String name = "testFile";
     Directory dir = newDirectory();
     FieldInfos fieldInfos = createAndWriteFieldInfos(dir, name);
-    FieldInfos readOnly = new FieldInfos(dir, name);
+    FieldInfos readOnly = readFieldInfos(dir, name);
     assertReadOnly(readOnly, fieldInfos);
     FieldInfos readOnlyClone = (FieldInfos)readOnly.clone();
     assertNotSame(readOnly, readOnlyClone);
diff --git a/lucene/src/test/org/apache/lucene/index/TestFieldsReader.java b/lucene/src/test/org/apache/lucene/index/TestFieldsReader.java
index 80dd7e8..981c86a 100644
--- a/lucene/src/test/org/apache/lucene/index/TestFieldsReader.java
+++ b/lucene/src/test/org/apache/lucene/index/TestFieldsReader.java
@@ -48,7 +48,7 @@ public class TestFieldsReader extends LuceneTestCase {
 
   @BeforeClass
   public static void beforeClass() throws Exception {
-    fieldInfos = new FieldInfos();
+    fieldInfos = new FieldInfos(new FieldInfos.FieldNumberBiMap());
     DocHelper.setupDoc(testDoc);
     _TestUtil.add(testDoc, fieldInfos);
     dir = newDirectory();
diff --git a/lucene/src/test/org/apache/lucene/index/TestFilterIndexReader.java b/lucene/src/test/org/apache/lucene/index/TestFilterIndexReader.java
index e1ac46b..6b593d4 100644
--- a/lucene/src/test/org/apache/lucene/index/TestFilterIndexReader.java
+++ b/lucene/src/test/org/apache/lucene/index/TestFilterIndexReader.java
@@ -54,8 +54,8 @@ public class TestFilterIndexReader extends LuceneTestCase {
       }
 
       @Override
-      public TermsEnum iterator() throws IOException {
-        return new TestTermsEnum(super.iterator());
+      public TermsEnum iterator(TermsEnum reuse) throws IOException {
+        return new TestTermsEnum(super.iterator(reuse));
       }
     }
 
@@ -65,8 +65,8 @@ public class TestFilterIndexReader extends LuceneTestCase {
       }
 
       @Override
-      public TermsEnum terms() throws IOException {
-        return new TestTermsEnum(super.terms());
+      public Terms terms() throws IOException {
+        return new TestTerms(super.terms());
       }
     }
 
@@ -150,7 +150,7 @@ public class TestFilterIndexReader extends LuceneTestCase {
     reader.close();
     reader = IndexReader.open(target, true);
     
-    TermsEnum terms = MultiFields.getTerms(reader, "default").iterator();
+    TermsEnum terms = MultiFields.getTerms(reader, "default").iterator(null);
     while (terms.next() != null) {
       assertTrue(terms.term().utf8ToString().indexOf('e') != -1);
     }
diff --git a/lucene/src/test/org/apache/lucene/index/TestFlex.java b/lucene/src/test/org/apache/lucene/index/TestFlex.java
index caf17bb..484547c 100644
--- a/lucene/src/test/org/apache/lucene/index/TestFlex.java
+++ b/lucene/src/test/org/apache/lucene/index/TestFlex.java
@@ -53,7 +53,7 @@ public class TestFlex extends LuceneTestCase {
 
       IndexReader r = w.getReader();
       
-      TermsEnum terms = MultiFields.getTerms(r, "field3").iterator();
+      TermsEnum terms = MultiFields.getTerms(r, "field3").iterator(null);
       assertEquals(TermsEnum.SeekStatus.END, terms.seekCeil(new BytesRef("abc")));
       r.close();
     }
@@ -70,7 +70,7 @@ public class TestFlex extends LuceneTestCase {
     doc.add(newField("f", "a b c", TextField.TYPE_UNSTORED));
     w.addDocument(doc);
     IndexReader r = w.getReader();
-    TermsEnum terms = r.getSequentialSubReaders()[0].fields().terms("f").iterator();
+    TermsEnum terms = r.getSequentialSubReaders()[0].fields().terms("f").iterator(null);
     assertTrue(terms.next() != null);
     try {
       assertEquals(0, terms.ord());
diff --git a/lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter.java b/lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter.java
index 7a0a42d..0d0625c 100644
--- a/lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter.java
+++ b/lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter.java
@@ -25,9 +25,12 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
+import org.apache.lucene.index.codecs.Codec;
+import org.apache.lucene.index.codecs.FieldInfosReader;
 import org.apache.lucene.search.similarities.DefaultSimilarity;
 import org.apache.lucene.store.CompoundFileDirectory;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.store.IndexOutput;
 import org.apache.lucene.store.MockDirectoryWrapper;
@@ -91,7 +94,8 @@ public class TestIndexFileDeleter extends LuceneTestCase {
     // "content", and then set our expected file names below
     // accordingly:
     CompoundFileDirectory cfsReader = new CompoundFileDirectory(dir, "_2.cfs", newIOContext(random), false);
-    FieldInfos fieldInfos = new FieldInfos(cfsReader, "_2.fnm");
+    FieldInfosReader infosReader = Codec.getDefault().fieldInfosFormat().getFieldInfosReader();
+    FieldInfos fieldInfos = infosReader.read(cfsReader, "2", IOContext.READONCE);
     int contentFieldIndex = -1;
     for (FieldInfo fi : fieldInfos) {
       if (fi.name.equals("content")) {
diff --git a/lucene/src/test/org/apache/lucene/index/TestIndexReader.java b/lucene/src/test/org/apache/lucene/index/TestIndexReader.java
index 46f7354..2da0975 100644
--- a/lucene/src/test/org/apache/lucene/index/TestIndexReader.java
+++ b/lucene/src/test/org/apache/lucene/index/TestIndexReader.java
@@ -28,8 +28,6 @@ import java.util.List;
 import java.util.Map;
 import java.util.HashMap;
 import java.util.Random;
-import java.util.Set;
-import java.util.SortedSet;
 import org.junit.Assume;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.BinaryField;
@@ -314,19 +312,6 @@ public class TestIndexReader extends LuceneTestCase
         writer.addDocument(doc);
     }
     writer.close();
-    IndexReader reader = IndexReader.open(d, false);
-    FieldSortedTermVectorMapper mapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());
-    reader.getTermFreqVector(0, mapper);
-    Map<String,SortedSet<TermVectorEntry>> map = mapper.getFieldToTerms();
-    assertTrue("map is null and it shouldn't be", map != null);
-    assertTrue("map Size: " + map.size() + " is not: " + 4, map.size() == 4);
-    Set<TermVectorEntry> set = map.get("termvector");
-    for (Iterator<TermVectorEntry> iterator = set.iterator(); iterator.hasNext();) {
-      TermVectorEntry entry =  iterator.next();
-      assertTrue("entry is null and it shouldn't be", entry != null);
-      if (VERBOSE) System.out.println("Entry: " + entry);
-    }
-    reader.close();
     d.close();
   }
 
@@ -914,8 +899,17 @@ public class TestIndexReader extends LuceneTestCase
       Bits liveDocs = MultiFields.getLiveDocs(index1);
       while((field1=fenum1.next()) != null) {
         assertEquals("Different fields", field1, fenum2.next());
-        TermsEnum enum1 = fenum1.terms();
-        TermsEnum enum2 = fenum2.terms();
+        Terms terms1 = fenum1.terms();
+        if (terms1 == null) {
+          assertNull(fenum2.terms());
+          continue;
+        }
+        TermsEnum enum1 = terms1.iterator(null);
+
+        Terms terms2 = fenum2.terms();
+        assertNotNull(terms2);
+        TermsEnum enum2 = terms2.iterator(null);
+
         while(enum1.next() != null) {
           assertEquals("Different terms", enum1.term(), enum2.next());
           DocsAndPositionsEnum tp1 = enum1.docsAndPositions(liveDocs, null);
diff --git a/lucene/src/test/org/apache/lucene/index/TestIndexWriter.java b/lucene/src/test/org/apache/lucene/index/TestIndexWriter.java
index d3ebaff..e2f121f 100644
--- a/lucene/src/test/org/apache/lucene/index/TestIndexWriter.java
+++ b/lucene/src/test/org/apache/lucene/index/TestIndexWriter.java
@@ -25,10 +25,12 @@ import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.HashMap;
+import java.util.HashSet;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 import java.util.Random;
+import java.util.Set;
 
 import org.apache.lucene.analysis.*;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
@@ -41,6 +43,7 @@ import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
 import org.apache.lucene.index.codecs.Codec;
+import org.apache.lucene.index.codecs.DefaultTermVectorsReader;
 import org.apache.lucene.index.codecs.simpletext.SimpleTextCodec;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.FieldCache;
@@ -885,7 +888,7 @@ public class TestIndexWriter extends LuceneTestCase {
     writer.close();
     IndexReader reader = IndexReader.open(dir, true);
     IndexReader subreader = getOnlySegmentReader(reader);
-    TermsEnum te = subreader.fields().terms("").iterator();
+    TermsEnum te = subreader.fields().terms("").iterator(null);
     assertEquals(new BytesRef("a"), te.next());
     assertEquals(new BytesRef("b"), te.next());
     assertEquals(new BytesRef("c"), te.next());
@@ -906,7 +909,7 @@ public class TestIndexWriter extends LuceneTestCase {
     writer.close();
     IndexReader reader = IndexReader.open(dir, true);
     IndexReader subreader = getOnlySegmentReader(reader);
-    TermsEnum te = subreader.fields().terms("").iterator();
+    TermsEnum te = subreader.fields().terms("").iterator(null);
     assertEquals(new BytesRef(""), te.next());
     assertEquals(new BytesRef("a"), te.next());
     assertEquals(new BytesRef("b"), te.next());
@@ -1071,13 +1074,23 @@ public class TestIndexWriter extends LuceneTestCase {
     w.close();
 
     IndexReader r = IndexReader.open(dir, true);
-    TermPositionVector tpv = ((TermPositionVector) r.getTermFreqVector(0, "field"));
-    int[] poss = tpv.getTermPositions(0);
-    assertEquals(1, poss.length);
-    assertEquals(100, poss[0]);
-    poss = tpv.getTermPositions(1);
-    assertEquals(1, poss.length);
-    assertEquals(101, poss[0]);
+    Terms tpv = r.getTermVectors(0).terms("field");
+    TermsEnum termsEnum = tpv.iterator(null);
+    assertNotNull(termsEnum.next());
+    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null);
+    assertNotNull(dpEnum);
+    assertTrue(dpEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);
+    assertEquals(1, dpEnum.freq());
+    assertEquals(100, dpEnum.nextPosition());
+
+    assertNotNull(termsEnum.next());
+    dpEnum = termsEnum.docsAndPositions(null, dpEnum);
+    assertNotNull(dpEnum);
+    assertTrue(dpEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);
+    assertEquals(1, dpEnum.freq());
+    assertEquals(101, dpEnum.nextPosition());
+    assertNull(termsEnum.next());
+
     r.close();
     dir.close();
   }
@@ -1402,7 +1415,7 @@ public class TestIndexWriter extends LuceneTestCase {
     w.addDocument(d);
 
     IndexReader r = w.getReader().getSequentialSubReaders()[0];
-    TermsEnum t = r.fields().terms("field").iterator();
+    TermsEnum t = r.fields().terms("field").iterator(null);
     int count = 0;
     while(t.next() != null) {
       final DocsEnum docs = t.docs(null, null);
@@ -1740,13 +1753,16 @@ public class TestIndexWriter extends LuceneTestCase {
     _TestUtil.checkIndex(dir);
 
     assertNoUnreferencedFiles(dir, "no tv files");
-    String[] files = dir.listAll();
-    for(String file : files) {
-      assertTrue(!file.endsWith(IndexFileNames.VECTORS_FIELDS_EXTENSION));
-      assertTrue(!file.endsWith(IndexFileNames.VECTORS_INDEX_EXTENSION));
-      assertTrue(!file.endsWith(IndexFileNames.VECTORS_DOCUMENTS_EXTENSION));
+    IndexReader r0 = IndexReader.open(dir);
+    for (IndexReader r : r0.getSequentialSubReaders()) {
+      SegmentInfo s = ((SegmentReader) r).getSegmentInfo();
+      assertFalse(s.getHasVectors());
+      Set<String> files = new HashSet<String>();
+      s.getCodec().termVectorsFormat().files(dir, s, files);
+      assertTrue(files.isEmpty());
     }
-
+    
+    r0.close();
     dir.close();
   }
 
diff --git a/lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java b/lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java
index 169056d..9162a93 100644
--- a/lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java
+++ b/lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java
@@ -535,7 +535,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
         boolean sawAppend = false;
         boolean sawFlush = false;
         for (int i = 0; i < trace.length; i++) {
-          if ("org.apache.lucene.index.FreqProxTermsWriterPerField".equals(trace[i].getClassName()) && "flush".equals(trace[i].getMethodName()))
+          if (FreqProxTermsWriterPerField.class.getName().equals(trace[i].getClassName()) && "flush".equals(trace[i].getMethodName()))
             sawAppend = true;
           if ("flush".equals(trace[i].getMethodName()))
             sawFlush = true;
@@ -641,7 +641,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
             numDel++;
           else {
             reader.document(j);
-            reader.getTermFreqVectors(j);
+            reader.getTermVectors(j);
           }
         }
         assertEquals(1, numDel);
@@ -665,7 +665,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
       assertNull(MultiFields.getLiveDocs(reader));
       for(int j=0;j<reader.maxDoc();j++) {
         reader.document(j);
-        reader.getTermFreqVectors(j);
+        reader.getTermVectors(j);
       }
       reader.close();
       assertEquals(0, numDel);
@@ -755,7 +755,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
           numDel++;
         else {
           reader.document(j);
-          reader.getTermFreqVectors(j);
+          reader.getTermVectors(j);
         }
       }
       reader.close();
@@ -778,7 +778,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
       assertNull(MultiFields.getLiveDocs(reader));
       for(int j=0;j<reader.maxDoc();j++) {
         reader.document(j);
-        reader.getTermFreqVectors(j);
+        reader.getTermVectors(j);
       }
       reader.close();
 
@@ -794,7 +794,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
       if (doFail) {
         StackTraceElement[] trace = new Exception().getStackTrace();
         for (int i = 0; i < trace.length; i++) {
-          if (doFail && "org.apache.lucene.store.MockDirectoryWrapper".equals(trace[i].getClassName()) && "sync".equals(trace[i].getMethodName())) {
+          if (doFail && MockDirectoryWrapper.class.getName().equals(trace[i].getClassName()) && "sync".equals(trace[i].getMethodName())) {
             didFail = true;
             throw new IOException("now failing on purpose during sync");
           }
@@ -868,11 +868,11 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
       boolean isDelete = false;
       boolean isInGlobalFieldMap = false;
       for (int i = 0; i < trace.length; i++) {
-        if ("org.apache.lucene.index.SegmentInfos".equals(trace[i].getClassName()) && stage.equals(trace[i].getMethodName()))
+        if (SegmentInfos.class.getName().equals(trace[i].getClassName()) && stage.equals(trace[i].getMethodName()))
           isCommit = true;
-        if ("org.apache.lucene.store.MockDirectoryWrapper".equals(trace[i].getClassName()) && "deleteFile".equals(trace[i].getMethodName()))
+        if (MockDirectoryWrapper.class.getName().equals(trace[i].getClassName()) && "deleteFile".equals(trace[i].getMethodName()))
           isDelete = true;
-        if ("org.apache.lucene.index.SegmentInfos".equals(trace[i].getClassName()) && "writeGlobalFieldMap".equals(trace[i].getMethodName()))
+        if (SegmentInfos.class.getName().equals(trace[i].getClassName()) && "writeGlobalFieldMap".equals(trace[i].getMethodName()))
           isInGlobalFieldMap = true;
           
       }
@@ -901,6 +901,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
     
     for (FailOnlyInCommit failure : failures) {
       MockDirectoryWrapper dir = newDirectory();
+      dir.setFailOnCreateOutput(false);
       IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(
           TEST_VERSION_CURRENT, new MockAnalyzer(random)));
       Document doc = new Document();
@@ -1302,20 +1303,17 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
 
     @Override
     public void eval(MockDirectoryWrapper dir)  throws IOException {
+
       StackTraceElement[] trace = new Exception().getStackTrace();
-      boolean failOnInit = false;
-      boolean failOnfinish = false;
+      boolean fail = false;
       for (int i = 0; i < trace.length; i++) {
-        if ("org.apache.lucene.index.TermVectorsTermsWriter".equals(trace[i].getClassName()) && stage.equals(trace[i].getMethodName()))
-          failOnInit = true;
-        if ("org.apache.lucene.index.TermVectorsTermsWriter".equals(trace[i].getClassName()) && stage.equals(trace[i].getMethodName()))
-          failOnfinish = true;
+        if (TermVectorsConsumer.class.getName().equals(trace[i].getClassName()) && stage.equals(trace[i].getMethodName())) {
+          fail = true;
+        }
       }
       
-      if (failOnInit) {
-        throw new RuntimeException(EXC_MSG + " fail on init");
-      } else if (failOnfinish) {
-        throw new RuntimeException(EXC_MSG + " fail on finishDoc");
+      if (fail) {
+        throw new RuntimeException(EXC_MSG);
       }
     }
   }
diff --git a/lucene/src/test/org/apache/lucene/index/TestIndexWriterUnicode.java b/lucene/src/test/org/apache/lucene/index/TestIndexWriterUnicode.java
index c3c3343..8197cad 100644
--- a/lucene/src/test/org/apache/lucene/index/TestIndexWriterUnicode.java
+++ b/lucene/src/test/org/apache/lucene/index/TestIndexWriterUnicode.java
@@ -135,7 +135,7 @@ public class TestIndexWriterUnicode extends LuceneTestCase {
   }
 
   private void checkTermsOrder(IndexReader r, Set<String> allTerms, boolean isTop) throws IOException {
-    TermsEnum terms = MultiFields.getFields(r).terms("f").iterator();
+    TermsEnum terms = MultiFields.getFields(r).terms("f").iterator(null);
 
     BytesRef last = new BytesRef();
 
diff --git a/lucene/src/test/org/apache/lucene/index/TestIndexWriterWithThreads.java b/lucene/src/test/org/apache/lucene/index/TestIndexWriterWithThreads.java
index 278ef70..40f5ae5 100644
--- a/lucene/src/test/org/apache/lucene/index/TestIndexWriterWithThreads.java
+++ b/lucene/src/test/org/apache/lucene/index/TestIndexWriterWithThreads.java
@@ -278,7 +278,7 @@ public class TestIndexWriterWithThreads extends LuceneTestCase {
         for(int j=0;j<reader.maxDoc();j++) {
           if (delDocs == null || !delDocs.get(j)) {
             reader.document(j);
-            reader.getTermFreqVectors(j);
+            reader.getTermVectors(j);
           }
         }
         reader.close();
diff --git a/lucene/src/test/org/apache/lucene/index/TestIndexableField.java b/lucene/src/test/org/apache/lucene/index/TestIndexableField.java
index deac743..5a42dfd 100644
--- a/lucene/src/test/org/apache/lucene/index/TestIndexableField.java
+++ b/lucene/src/test/org/apache/lucene/index/TestIndexableField.java
@@ -288,33 +288,31 @@ public class TestIndexableField extends LuceneTestCase {
         
         if (indexed) {
           final boolean tv = counter % 2 == 1 && fieldID != 9;
-          final TermFreqVector tfv = r.getTermFreqVector(docID, name);
           if (tv) {
+            final Terms tfv = r.getTermVectors(docID).terms(name);
             assertNotNull(tfv);
-            assertTrue(tfv instanceof TermPositionVector);
-            final TermPositionVector tpv = (TermPositionVector) tfv;
-            final BytesRef[] terms = tpv.getTerms();
-            assertEquals(2, terms.length);
-            assertEquals(new BytesRef(""+counter), terms[0]);
-            assertEquals(new BytesRef("text"), terms[1]);
-
-            final int[] freqs = tpv.getTermFrequencies();
-            assertEquals(2, freqs.length);
-            assertEquals(1, freqs[0]);
-            assertEquals(1, freqs[1]);
-
-            int[] positions = tpv.getTermPositions(0);
-            assertEquals(1, positions.length);
-            assertEquals(1, positions[0]);
-
-            positions = tpv.getTermPositions(1);
-            assertEquals(1, positions.length);
-            assertEquals(0, positions[0]);
+            TermsEnum termsEnum = tfv.iterator(null);
+            assertEquals(new BytesRef(""+counter), termsEnum.next());
+            assertEquals(1, termsEnum.totalTermFreq());
+            DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null);
+            assertTrue(dpEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);
+            assertEquals(1, dpEnum.freq());
+            assertEquals(1, dpEnum.nextPosition());
+
+            assertEquals(new BytesRef("text"), termsEnum.next());
+            assertEquals(1, termsEnum.totalTermFreq());
+            dpEnum = termsEnum.docsAndPositions(null, dpEnum);
+            assertTrue(dpEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);
+            assertEquals(1, dpEnum.freq());
+            assertEquals(0, dpEnum.nextPosition());
+
+            assertNull(termsEnum.next());
 
             // TODO: offsets
             
           } else {
-            assertNull(tfv);
+            Fields vectors = r.getTermVectors(docID);
+            assertTrue(vectors == null || vectors.terms(name) == null);
           }
 
           if (numeric) {
diff --git a/lucene/src/test/org/apache/lucene/index/TestParallelTermEnum.java b/lucene/src/test/org/apache/lucene/index/TestParallelTermEnum.java
index 9b63b7c..97fb557 100755
--- a/lucene/src/test/org/apache/lucene/index/TestParallelTermEnum.java
+++ b/lucene/src/test/org/apache/lucene/index/TestParallelTermEnum.java
@@ -84,7 +84,8 @@ public class TestParallelTermEnum extends LuceneTestCase {
         f = fe.next();
         assertEquals("field1", f);
 
-        TermsEnum te = fe.terms();
+        Terms terms = fe.terms();
+        TermsEnum te = terms.iterator(null);
 
         assertEquals("brown", te.next().utf8ToString());
         DocsEnum td = te.docs(liveDocs, null);
@@ -119,7 +120,9 @@ public class TestParallelTermEnum extends LuceneTestCase {
         assertNull(te.next());
         f = fe.next();
         assertEquals("field2", f);
-        te = fe.terms();
+        terms = fe.terms();
+        assertNotNull(terms);
+        te = terms.iterator(null);
 
         assertEquals("brown", te.next().utf8ToString());
         td = te.docs(liveDocs, td);
@@ -154,7 +157,9 @@ public class TestParallelTermEnum extends LuceneTestCase {
         assertNull(te.next());
         f = fe.next();
         assertEquals("field3", f);
-        te = fe.terms();
+        terms = fe.terms();
+        assertNotNull(terms);
+        te = terms.iterator(null);
 
         assertEquals("dog", te.next().utf8ToString());
         td = te.docs(liveDocs, td);
diff --git a/lucene/src/test/org/apache/lucene/index/TestPayloads.java b/lucene/src/test/org/apache/lucene/index/TestPayloads.java
index 735f172..d276d24 100644
--- a/lucene/src/test/org/apache/lucene/index/TestPayloads.java
+++ b/lucene/src/test/org/apache/lucene/index/TestPayloads.java
@@ -527,7 +527,7 @@ public class TestPayloads extends LuceneTestCase {
         }
         writer.close();
         IndexReader reader = IndexReader.open(dir, true);
-        TermsEnum terms = MultiFields.getFields(reader).terms(field).iterator();
+        TermsEnum terms = MultiFields.getFields(reader).terms(field).iterator(null);
         Bits liveDocs = MultiFields.getLiveDocs(reader);
         DocsAndPositionsEnum tp = null;
         while (terms.next() != null) {
diff --git a/lucene/src/test/org/apache/lucene/index/TestPerSegmentDeletes.java b/lucene/src/test/org/apache/lucene/index/TestPerSegmentDeletes.java
index 06ce930..2137284 100644
--- a/lucene/src/test/org/apache/lucene/index/TestPerSegmentDeletes.java
+++ b/lucene/src/test/org/apache/lucene/index/TestPerSegmentDeletes.java
@@ -222,7 +222,7 @@ public class TestPerSegmentDeletes extends LuceneTestCase {
       throws IOException {
     Fields fields = MultiFields.getFields(reader);
     Terms cterms = fields.terms(term.field);
-    TermsEnum ctermsEnum = cterms.iterator();
+    TermsEnum ctermsEnum = cterms.iterator(null);
     if (ctermsEnum.seekExact(new BytesRef(term.text()), false)) {
       DocsEnum docsEnum = ctermsEnum.docs(bits, null);
       return toArray(docsEnum);
diff --git a/lucene/src/test/org/apache/lucene/index/TestPositionBasedTermVectorMapper.java b/lucene/src/test/org/apache/lucene/index/TestPositionBasedTermVectorMapper.java
deleted file mode 100644
index 1575c86..0000000
--- a/lucene/src/test/org/apache/lucene/index/TestPositionBasedTermVectorMapper.java
+++ /dev/null
@@ -1,100 +0,0 @@
-package org.apache.lucene.index;
-/**
- * Copyright 2005 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.LuceneTestCase;
-
-import java.io.IOException;
-import java.util.BitSet;
-import java.util.Map;
-
-public class TestPositionBasedTermVectorMapper extends LuceneTestCase {
-  protected String[] tokens;
-  protected int[][] thePositions;
-  protected TermVectorOffsetInfo[][] offsets;
-  protected int numPositions;
-
-  @Override
-  public void setUp() throws Exception {
-    super.setUp();
-    tokens = new String[]{"here", "is", "some", "text", "to", "test", "extra"};
-    thePositions = new int[tokens.length][];
-    offsets = new TermVectorOffsetInfo[tokens.length][];
-    numPositions = 0;
-    //save off the last one so we can add it with the same positions as some of the others, but in a predictable way
-    for (int i = 0; i < tokens.length - 1; i++)
-    {
-      thePositions[i] = new int[2 * i + 1];//give 'em all some positions
-      for (int j = 0; j < thePositions[i].length; j++)
-      {
-        thePositions[i][j] = numPositions++;
-      }
-      offsets[i] = new TermVectorOffsetInfo[thePositions[i].length];
-      for (int j = 0; j < offsets[i].length; j++) {
-        offsets[i][j] = new TermVectorOffsetInfo(j, j + 1);//the actual value here doesn't much matter
-      }
-    }
-    thePositions[tokens.length - 1] = new int[1];
-    thePositions[tokens.length - 1][0] = 0;//put this at the same position as "here"
-    offsets[tokens.length - 1] = new TermVectorOffsetInfo[1];
-    offsets[tokens.length - 1][0] = new TermVectorOffsetInfo(0, 1);
-  }
-
-  public void test() throws IOException {
-    PositionBasedTermVectorMapper mapper = new PositionBasedTermVectorMapper();
-    
-    mapper.setExpectations("test", tokens.length, true, true);
-    //Test single position
-    for (int i = 0; i < tokens.length; i++) {
-      String token = tokens[i];
-      mapper.map(new BytesRef(token), 1, null, thePositions[i]);
-
-    }
-    Map<String,Map<Integer,PositionBasedTermVectorMapper.TVPositionInfo>> map = mapper.getFieldToTerms();
-    assertTrue("map is null and it shouldn't be", map != null);
-    assertTrue("map Size: " + map.size() + " is not: " + 1, map.size() == 1);
-    Map<Integer,PositionBasedTermVectorMapper.TVPositionInfo> positions = map.get("test");
-    assertTrue("thePositions is null and it shouldn't be", positions != null);
-    
-    assertTrue("thePositions Size: " + positions.size() + " is not: " + numPositions, positions.size() == numPositions);
-    BitSet bits = new BitSet(numPositions);
-    for (Map.Entry<Integer,PositionBasedTermVectorMapper.TVPositionInfo> entry : positions.entrySet()) {
-    
-      PositionBasedTermVectorMapper.TVPositionInfo info = entry.getValue();
-      assertTrue("info is null and it shouldn't be", info != null);
-      int pos = entry.getKey().intValue();
-      bits.set(pos);
-      assertTrue(info.getPosition() + " does not equal: " + pos, info.getPosition() == pos);
-      assertTrue("info.getOffsets() is null and it shouldn't be", info.getOffsets() != null);
-      if (pos == 0)
-      {
-        assertTrue("info.getTerms() Size: " + info.getTerms().size() + " is not: " + 2, info.getTerms().size() == 2);//need a test for multiple terms at one pos
-        assertTrue("info.getOffsets() Size: " + info.getOffsets().size() + " is not: " + 2, info.getOffsets().size() == 2);
-      }
-      else
-      {
-        assertTrue("info.getTerms() Size: " + info.getTerms().size() + " is not: " + 1, info.getTerms().size() == 1);//need a test for multiple terms at one pos
-        assertTrue("info.getOffsets() Size: " + info.getOffsets().size() + " is not: " + 1, info.getOffsets().size() == 1);
-      }
-    }
-    assertTrue("Bits are not all on", bits.cardinality() == numPositions);
-  }
-
-
-
-  
-}
diff --git a/lucene/src/test/org/apache/lucene/index/TestSegmentMerger.java b/lucene/src/test/org/apache/lucene/index/TestSegmentMerger.java
index adc99fe..b9b00f2 100644
--- a/lucene/src/test/org/apache/lucene/index/TestSegmentMerger.java
+++ b/lucene/src/test/org/apache/lucene/index/TestSegmentMerger.java
@@ -77,7 +77,7 @@ public class TestSegmentMerger extends LuceneTestCase {
 
   public void testMerge() throws IOException {
     final Codec codec = Codec.getDefault();
-    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(), codec, newIOContext(random));
+    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random));
     merger.add(reader1);
     merger.add(reader2);
     MergeState mergeState = merger.merge();
@@ -110,23 +110,19 @@ public class TestSegmentMerger extends LuceneTestCase {
     //System.out.println("stored size: " + stored.size());
     assertTrue("We do not have 3 fields that were indexed with term vector",stored.size() == 3);
 
-    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);
-    assertTrue(vector != null);
-    BytesRef [] terms = vector.getTerms();
-    assertTrue(terms != null);
-    //System.out.println("Terms size: " + terms.length);
-    assertTrue(terms.length == 3);
-    int [] freqs = vector.getTermFrequencies();
-    assertTrue(freqs != null);
-    //System.out.println("Freqs size: " + freqs.length);
-    assertTrue(vector instanceof TermPositionVector == true);
+    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);
+    assertNotNull(vector);
+    assertEquals(3, vector.getUniqueTermCount());
+    TermsEnum termsEnum = vector.iterator(null);
 
-    for (int i = 0; i < terms.length; i++) {
-      String term = terms[i].utf8ToString();
-      int freq = freqs[i];
+    int i = 0;
+    while (termsEnum.next() != null) {
+      String term = termsEnum.term().utf8ToString();
+      int freq = (int) termsEnum.totalTermFreq();
       //System.out.println("Term: " + term + " Freq: " + freq);
       assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);
       assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);
+      i++;
     }
 
     TestSegmentReader.checkNorms(mergedReader);
@@ -152,7 +148,7 @@ public class TestSegmentMerger extends LuceneTestCase {
     SegmentMerger sm = new SegmentMerger(InfoStream.getDefault(), dir, 1, "a", MergeState.CheckAbort.NONE, null, null, Codec.getDefault(), newIOContext(random));
     boolean doFail = false;
     try {
-      sm.createCompoundFile("b1", w.segmentInfos.info(0), newIOContext(random));
+      IndexWriter.createCompoundFile(dir, "b1", MergeState.CheckAbort.NONE, w.segmentInfos.info(0), newIOContext(random));
       doFail = true; // should never get here
     } catch (AssertionError e) {
       // expected
@@ -173,7 +169,7 @@ public class TestSegmentMerger extends LuceneTestCase {
     SegmentInfos sis = new SegmentInfos();
     sis.read(dir);
     try {
-      sm.createCompoundFile("b2", sis.info(0), newIOContext(random));
+      IndexWriter.createCompoundFile(dir, "b2", MergeState.CheckAbort.NONE, sis.info(0), newIOContext(random));
       doFail = true; // should never get here
     } catch (AssertionError e) {
       // expected
diff --git a/lucene/src/test/org/apache/lucene/index/TestSegmentReader.java b/lucene/src/test/org/apache/lucene/index/TestSegmentReader.java
index ed39886..e68a880 100644
--- a/lucene/src/test/org/apache/lucene/index/TestSegmentReader.java
+++ b/lucene/src/test/org/apache/lucene/index/TestSegmentReader.java
@@ -121,9 +121,11 @@ public class TestSegmentReader extends LuceneTestCase {
     FieldsEnum fields = MultiFields.getFields(reader).iterator();
     String field;
     while((field = fields.next()) != null) {
-      TermsEnum terms = fields.terms();
-      while(terms.next() != null) {
-        BytesRef term = terms.term();
+      Terms terms = fields.terms();
+      assertNotNull(terms);
+      TermsEnum termsEnum = terms.iterator(null);
+      while(termsEnum.next() != null) {
+        BytesRef term = termsEnum.term();
         assertTrue(term != null);
         String fieldValue = (String) DocHelper.nameValues.get(field);
         assertTrue(fieldValue.indexOf(term.utf8ToString()) != -1);
@@ -188,20 +190,19 @@ public class TestSegmentReader extends LuceneTestCase {
   }
   
   public void testTermVectors() throws IOException {
-    TermFreqVector result = reader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);
-    assertTrue(result != null);
-    BytesRef [] terms = result.getTerms();
-    int [] freqs = result.getTermFrequencies();
-    assertTrue(terms != null && terms.length == 3 && freqs != null && freqs.length == 3);
-    for (int i = 0; i < terms.length; i++) {
-      String term = terms[i].utf8ToString();
-      int freq = freqs[i];
+    Terms result = reader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);
+    assertNotNull(result);
+    assertEquals(3, result.getUniqueTermCount());
+    TermsEnum termsEnum = result.iterator(null);
+    while(termsEnum.next() != null) {
+      String term = termsEnum.term().utf8ToString();
+      int freq = (int) termsEnum.totalTermFreq();
       assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);
       assertTrue(freq > 0);
     }
 
-    TermFreqVector [] results = reader.getTermFreqVectors(0);
+    Fields results = reader.getTermVectors(0);
     assertTrue(results != null);
-    assertTrue("We do not have 3 term freq vectors, we have: " + results.length, results.length == 3);      
+    assertEquals("We do not have 3 term freq vectors", 3, results.getUniqueFieldCount());      
   }    
 }
diff --git a/lucene/src/test/org/apache/lucene/index/TestSegmentTermDocs.java b/lucene/src/test/org/apache/lucene/index/TestSegmentTermDocs.java
index 5d2384c..430ff59 100644
--- a/lucene/src/test/org/apache/lucene/index/TestSegmentTermDocs.java
+++ b/lucene/src/test/org/apache/lucene/index/TestSegmentTermDocs.java
@@ -59,7 +59,7 @@ public class TestSegmentTermDocs extends LuceneTestCase {
     assertTrue(reader != null);
     assertEquals(indexDivisor, reader.getTermInfosIndexDivisor());
 
-    TermsEnum terms = reader.fields().terms(DocHelper.TEXT_FIELD_2_KEY).iterator();
+    TermsEnum terms = reader.fields().terms(DocHelper.TEXT_FIELD_2_KEY).iterator(null);
     terms.seekCeil(new BytesRef("field"));
     DocsEnum termDocs = terms.docs(reader.getLiveDocs(), null);
     if (termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS)    {
diff --git a/lucene/src/test/org/apache/lucene/index/TestSegmentTermEnum.java b/lucene/src/test/org/apache/lucene/index/TestSegmentTermEnum.java
index 77d6483..eca0aa7 100644
--- a/lucene/src/test/org/apache/lucene/index/TestSegmentTermEnum.java
+++ b/lucene/src/test/org/apache/lucene/index/TestSegmentTermEnum.java
@@ -79,7 +79,7 @@ public class TestSegmentTermEnum extends LuceneTestCase {
     addDoc(writer, "aaa bbb");
     writer.close();
     SegmentReader reader = getOnlySegmentReader(IndexReader.open(dir, false));
-    TermsEnum terms = reader.fields().terms("content").iterator();
+    TermsEnum terms = reader.fields().terms("content").iterator(null);
     assertNotNull(terms.next());
     assertEquals("aaa", terms.term().utf8ToString());
     assertNotNull(terms.next());
@@ -103,7 +103,7 @@ public class TestSegmentTermEnum extends LuceneTestCase {
       throws IOException
   {
       IndexReader reader = IndexReader.open(dir, true);
-      TermsEnum termEnum = MultiFields.getTerms(reader, "content").iterator();
+      TermsEnum termEnum = MultiFields.getTerms(reader, "content").iterator(null);
 
     // create enumeration of all terms
     // go to the first term (aaa)
diff --git a/lucene/src/test/org/apache/lucene/index/TestSizeBoundedForceMerge.java b/lucene/src/test/org/apache/lucene/index/TestSizeBoundedForceMerge.java
index 6b157cf..e16e1e8 100644
--- a/lucene/src/test/org/apache/lucene/index/TestSizeBoundedForceMerge.java
+++ b/lucene/src/test/org/apache/lucene/index/TestSizeBoundedForceMerge.java
@@ -59,7 +59,7 @@ public class TestSizeBoundedForceMerge extends LuceneTestCase {
 
     SegmentInfos sis = new SegmentInfos();
     sis.read(dir);
-    double min = sis.info(0).sizeInBytes(true);
+    double min = sis.info(0).sizeInBytes();
 
     conf = newWriterConfig();
     LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();
diff --git a/lucene/src/test/org/apache/lucene/index/TestStressAdvance.java b/lucene/src/test/org/apache/lucene/index/TestStressAdvance.java
index 8e6d2b1..243a219 100644
--- a/lucene/src/test/org/apache/lucene/index/TestStressAdvance.java
+++ b/lucene/src/test/org/apache/lucene/index/TestStressAdvance.java
@@ -67,7 +67,7 @@ public class TestStressAdvance extends LuceneTestCase {
           bDocIDs.add(docID);
         }
       }
-      final TermsEnum te = r.getSequentialSubReaders()[0].fields().terms("field").iterator();
+      final TermsEnum te = r.getSequentialSubReaders()[0].fields().terms("field").iterator(null);
       
       DocsEnum de = null;
       for(int iter2=0;iter2<10;iter2++) {
diff --git a/lucene/src/test/org/apache/lucene/index/TestStressIndexing2.java b/lucene/src/test/org/apache/lucene/index/TestStressIndexing2.java
index 26ece46..65269a1 100644
--- a/lucene/src/test/org/apache/lucene/index/TestStressIndexing2.java
+++ b/lucene/src/test/org/apache/lucene/index/TestStressIndexing2.java
@@ -24,11 +24,11 @@ import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 import java.util.Random;
-import org.apache.lucene.util.*;
 
 import junit.framework.Assert;
 
 import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
@@ -36,6 +36,7 @@ import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.*;
 
 public class TestStressIndexing2 extends LuceneTestCase {
   static int maxFields=4;
@@ -322,7 +323,7 @@ public class TestStressIndexing2 extends LuceneTestCase {
                  MultiFields.getFields(r2).terms(idField) == null);
       return;
     }
-    final TermsEnum termsEnum = terms1.iterator();
+    final TermsEnum termsEnum = terms1.iterator(null);
 
     final Bits liveDocs1 = MultiFields.getLiveDocs(r1);
     final Bits liveDocs2 = MultiFields.getLiveDocs(r2);
@@ -382,24 +383,79 @@ public class TestStressIndexing2 extends LuceneTestCase {
 
       try {
         // verify term vectors are equivalent        
-        verifyEquals(r1.getTermFreqVectors(id1), r2.getTermFreqVectors(id2));
+        verifyEquals(r1.getTermVectors(id1), r2.getTermVectors(id2));
       } catch (Throwable e) {
         System.out.println("FAILED id=" + term + " id1=" + id1 + " id2=" + id2);
-        TermFreqVector[] tv1 = r1.getTermFreqVectors(id1);
+        Fields tv1 = r1.getTermVectors(id1);
         System.out.println("  d1=" + tv1);
-        if (tv1 != null)
-          for(int i=0;i<tv1.length;i++)
-            System.out.println("    " + i + ": " + tv1[i]);
+        if (tv1 != null) {
+          FieldsEnum fieldsEnum = tv1.iterator();
+          String field;
+          DocsAndPositionsEnum dpEnum = null;
+          DocsEnum dEnum = null;
+          while ((field=fieldsEnum.next()) != null) {
+            System.out.println("    " + field + ":");
+            Terms terms3 = fieldsEnum.terms();
+            assertNotNull(terms3);
+            TermsEnum termsEnum2 = terms3.iterator(null);
+            BytesRef term2;
+            while((term2 = termsEnum2.next()) != null) {
+              System.out.println("      " + term2.utf8ToString() + ": freq=" + termsEnum2.totalTermFreq());
+              dpEnum = termsEnum2.docsAndPositions(null, dpEnum);
+              if (dpEnum != null) {
+                assertTrue(dpEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);
+                final int freq = dpEnum.freq();
+                System.out.println("        doc=" + dpEnum.docID() + " freq=" + freq);
+                for(int posUpto=0;posUpto<freq;posUpto++) {
+                  System.out.println("          pos=" + dpEnum.nextPosition());
+                }
+              } else {
+                dEnum = termsEnum2.docs(null, dEnum);
+                assertNotNull(dEnum);
+                assertTrue(dEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);
+                final int freq = dEnum.freq();
+                System.out.println("        doc=" + dEnum.docID() + " freq=" + freq);
+              }
+            }
+          }
+        }
         
-        TermFreqVector[] tv2 = r2.getTermFreqVectors(id2);
+        Fields tv2 = r2.getTermVectors(id2);
         System.out.println("  d2=" + tv2);
-        if (tv2 != null)
-          for(int i=0;i<tv2.length;i++)
-            System.out.println("    " + i + ": " + tv2[i]);
+        if (tv2 != null) {
+          FieldsEnum fieldsEnum = tv2.iterator();
+          String field;
+          DocsAndPositionsEnum dpEnum = null;
+          DocsEnum dEnum = null;
+          while ((field=fieldsEnum.next()) != null) {
+            System.out.println("    " + field + ":");
+            Terms terms3 = fieldsEnum.terms();
+            assertNotNull(terms3);
+            TermsEnum termsEnum2 = terms3.iterator(null);
+            BytesRef term2;
+            while((term2 = termsEnum2.next()) != null) {
+              System.out.println("      " + term2.utf8ToString() + ": freq=" + termsEnum2.totalTermFreq());
+              dpEnum = termsEnum2.docsAndPositions(null, dpEnum);
+              if (dpEnum != null) {
+                assertTrue(dpEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);
+                final int freq = dpEnum.freq();
+                System.out.println("        doc=" + dpEnum.docID() + " freq=" + freq);
+                for(int posUpto=0;posUpto<freq;posUpto++) {
+                  System.out.println("          pos=" + dpEnum.nextPosition());
+                }
+              } else {
+                dEnum = termsEnum2.docs(null, dEnum);
+                assertNotNull(dEnum);
+                assertTrue(dEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);
+                final int freq = dEnum.freq();
+                System.out.println("        doc=" + dEnum.docID() + " freq=" + freq);
+              }
+            }
+          }
+        }
         
         throw e;
       }
-
     }
 
     //System.out.println("TEST: done match id");
@@ -430,7 +486,11 @@ public class TestStressIndexing2 extends LuceneTestCase {
           if (field1 == null) {
             break;
           } else {
-            termsEnum1 = fields1.terms();
+            Terms terms = fields1.terms();
+            if (terms == null) {
+              continue;
+            }
+            termsEnum1 = terms.iterator(null);
           }
         }
         term1 = termsEnum1.next();
@@ -460,7 +520,11 @@ public class TestStressIndexing2 extends LuceneTestCase {
           if (field2 == null) {
             break;
           } else {
-            termsEnum2 = fields2.terms();
+            Terms terms = fields2.terms();
+            if (terms == null) {
+              continue;
+            }
+            termsEnum2 = terms.iterator(null);
           }
         }
         term2 = termsEnum2.next();
@@ -527,62 +591,98 @@ public class TestStressIndexing2 extends LuceneTestCase {
       }
     }
 
-  public static void verifyEquals(TermFreqVector[] d1, TermFreqVector[] d2) {
+  public static void verifyEquals(Fields d1, Fields d2) throws IOException {
     if (d1 == null) {
-      assertTrue(d2 == null);
+      assertTrue(d2 == null || d2.getUniqueFieldCount() == 0);
       return;
     }
     assertTrue(d2 != null);
 
-    assertEquals(d1.length, d2.length);
-    for(int i=0;i<d1.length;i++) {
-      TermFreqVector v1 = d1[i];
-      TermFreqVector v2 = d2[i];
-      if (v1 == null || v2 == null)
-        System.out.println("v1=" + v1 + " v2=" + v2 + " i=" + i + " of " + d1.length);
-      assertEquals(v1.size(), v2.size());
-      int numTerms = v1.size();
-      BytesRef[] terms1 = v1.getTerms();
-      BytesRef[] terms2 = v2.getTerms();
-      int[] freq1 = v1.getTermFrequencies();
-      int[] freq2 = v2.getTermFrequencies();
-      for(int j=0;j<numTerms;j++) {
-        if (!terms1[j].equals(terms2[j]))
-          assertEquals(terms1[j], terms2[j]);
-        assertEquals(freq1[j], freq2[j]);
-      }
-      if (v1 instanceof TermPositionVector) {
-        assertTrue(v2 instanceof TermPositionVector);
-        TermPositionVector tpv1 = (TermPositionVector) v1;
-        TermPositionVector tpv2 = (TermPositionVector) v2;
-        for(int j=0;j<numTerms;j++) {
-          int[] pos1 = tpv1.getTermPositions(j);
-          int[] pos2 = tpv2.getTermPositions(j);
-          if (pos1 == null) {
-            assertNull(pos2);
+    FieldsEnum fieldsEnum1 = d1.iterator();
+    FieldsEnum fieldsEnum2 = d2.iterator();
+    String field1;
+    while ((field1 = fieldsEnum1.next()) != null) {
+      String field2 = fieldsEnum2.next();
+      assertEquals(field1, field2);
+
+      Terms terms1 = fieldsEnum1.terms();
+      assertNotNull(terms1);
+      TermsEnum termsEnum1 = terms1.iterator(null);
+
+      Terms terms2 = fieldsEnum2.terms();
+      assertNotNull(terms2);
+      TermsEnum termsEnum2 = terms2.iterator(null);
+
+      DocsAndPositionsEnum dpEnum1 = null;
+      DocsAndPositionsEnum dpEnum2 = null;
+      DocsEnum dEnum1 = null;
+      DocsEnum dEnum2 = null;
+      
+      BytesRef term1;
+      while ((term1 = termsEnum1.next()) != null) {
+        BytesRef term2 = termsEnum2.next();
+        assertEquals(term1, term2);
+        assertEquals(termsEnum1.totalTermFreq(),
+                     termsEnum2.totalTermFreq());
+        
+        dpEnum1 = termsEnum1.docsAndPositions(null, dpEnum1);
+        dpEnum2 = termsEnum2.docsAndPositions(null, dpEnum2);
+        if (dpEnum1 != null) {
+          assertNotNull(dpEnum2);
+          int docID1 = dpEnum1.nextDoc();
+          dpEnum2.nextDoc();
+          // docIDs are not supposed to be equal
+          //int docID2 = dpEnum2.nextDoc();
+          //assertEquals(docID1, docID2);
+          assertTrue(docID1 != DocsEnum.NO_MORE_DOCS);
+          
+          int freq1 = dpEnum1.freq();
+          int freq2 = dpEnum2.freq();
+          assertEquals(freq1, freq2);
+          OffsetAttribute offsetAtt1 = dpEnum1.attributes().hasAttribute(OffsetAttribute.class) ? dpEnum1.attributes().getAttribute(OffsetAttribute.class) : null;
+          OffsetAttribute offsetAtt2 = dpEnum2.attributes().hasAttribute(OffsetAttribute.class) ? dpEnum2.attributes().getAttribute(OffsetAttribute.class) : null;
+
+          if (offsetAtt1 != null) {
+            assertNotNull(offsetAtt2);
           } else {
-            assertNotNull(pos1);
-            assertNotNull(pos2);
-            assertEquals(pos1.length, pos2.length);
-            TermVectorOffsetInfo[] offsets1 = tpv1.getOffsets(j);
-            TermVectorOffsetInfo[] offsets2 = tpv2.getOffsets(j);
-            if (offsets1 == null)
-              assertTrue(offsets2 == null);
-            else
-              assertTrue(offsets2 != null);
-            for(int k=0;k<pos1.length;k++) {
-              assertEquals(pos1[k], pos2[k]);
-              if (offsets1 != null) {
-                assertEquals(offsets1[k].getStartOffset(),
-                             offsets2[k].getStartOffset());
-                assertEquals(offsets1[k].getEndOffset(),
-                             offsets2[k].getEndOffset());
-              }
+            assertNull(offsetAtt2);
+          }
+
+          for(int posUpto=0;posUpto<freq1;posUpto++) {
+            int pos1 = dpEnum1.nextPosition();
+            int pos2 = dpEnum2.nextPosition();
+            assertEquals(pos1, pos2);
+            if (offsetAtt1 != null) {
+              assertEquals(offsetAtt1.startOffset(),
+                           offsetAtt2.startOffset());
+              assertEquals(offsetAtt1.endOffset(),
+                           offsetAtt2.endOffset());
             }
           }
+          assertEquals(DocsEnum.NO_MORE_DOCS, dpEnum1.nextDoc());
+          assertEquals(DocsEnum.NO_MORE_DOCS, dpEnum2.nextDoc());
+        } else {
+          dEnum1 = termsEnum1.docs(null, dEnum1);
+          dEnum2 = termsEnum2.docs(null, dEnum2);
+          assertNotNull(dEnum1);
+          assertNotNull(dEnum2);
+          int docID1 = dEnum1.nextDoc();
+          dEnum2.nextDoc();
+          // docIDs are not supposed to be equal
+          //int docID2 = dEnum2.nextDoc();
+          //assertEquals(docID1, docID2);
+          assertTrue(docID1 != DocsEnum.NO_MORE_DOCS);
+          int freq1 = dEnum1.freq();
+          int freq2 = dEnum2.freq();
+          assertEquals(freq1, freq2);
+          assertEquals(DocsEnum.NO_MORE_DOCS, dEnum1.nextDoc());
+          assertEquals(DocsEnum.NO_MORE_DOCS, dEnum2.nextDoc());
         }
       }
+
+      assertNull(termsEnum2.next());
     }
+    assertNull(fieldsEnum2.next());
   }
 
   private class IndexingThread extends Thread {
diff --git a/lucene/src/test/org/apache/lucene/index/TestSumDocFreq.java b/lucene/src/test/org/apache/lucene/index/TestSumDocFreq.java
index 145834a..adf5892 100644
--- a/lucene/src/test/org/apache/lucene/index/TestSumDocFreq.java
+++ b/lucene/src/test/org/apache/lucene/index/TestSumDocFreq.java
@@ -92,7 +92,7 @@ public class TestSumDocFreq extends LuceneTestCase {
       }
       
       long computedSumDocFreq = 0;
-      TermsEnum termsEnum = terms.iterator();
+      TermsEnum termsEnum = terms.iterator(null);
       while (termsEnum.next() != null) {
         computedSumDocFreq += termsEnum.docFreq();
       }
diff --git a/lucene/src/test/org/apache/lucene/index/TestTermVectorsReader.java b/lucene/src/test/org/apache/lucene/index/TestTermVectorsReader.java
index e4160d0..ce9d071 100644
--- a/lucene/src/test/org/apache/lucene/index/TestTermVectorsReader.java
+++ b/lucene/src/test/org/apache/lucene/index/TestTermVectorsReader.java
@@ -20,9 +20,8 @@ package org.apache.lucene.index;
 import java.io.IOException;
 import java.io.Reader;
 import java.util.Arrays;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.SortedSet;
+import java.util.HashSet;
+import java.util.Set;
 
 import org.apache.lucene.analysis.*;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
@@ -32,6 +31,9 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.codecs.Codec;
+import org.apache.lucene.index.codecs.DefaultTermVectorsReader;
+import org.apache.lucene.index.codecs.TermVectorsReader;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.LuceneTestCase;
@@ -43,10 +45,9 @@ public class TestTermVectorsReader extends LuceneTestCase {
   private boolean[] testFieldsStoreOff = {true, false, false, true};
   private String[] testTerms = {"this", "is", "a", "test"};
   private int[][] positions = new int[testTerms.length][];
-  private TermVectorOffsetInfo[][] offsets = new TermVectorOffsetInfo[testTerms.length][];
   private Directory dir;
-  private String seg;
-  private FieldInfos fieldInfos = new FieldInfos();
+  private SegmentInfo seg;
+  private FieldInfos fieldInfos = new FieldInfos(new FieldInfos.FieldNumberBiMap());
   private static int TERM_FREQ = 3;
 
   private class TestToken implements Comparable<TestToken> {
@@ -74,18 +75,15 @@ public class TestTermVectorsReader extends LuceneTestCase {
     int tokenUpto = 0;
     for (int i = 0; i < testTerms.length; i++) {
       positions[i] = new int[TERM_FREQ];
-      offsets[i] = new TermVectorOffsetInfo[TERM_FREQ];
       // first position must be 0
       for (int j = 0; j < TERM_FREQ; j++) {
         // positions are always sorted in increasing order
         positions[i][j] = (int) (j * 10 + Math.random() * 10);
-        // offsets are always sorted in increasing order
-        offsets[i][j] = new TermVectorOffsetInfo(j * 10, j * 10 + testTerms[i].length());
         TestToken token = tokens[tokenUpto++] = new TestToken();
         token.text = testTerms[i];
         token.pos = positions[i][j];
-        token.startOffset = offsets[i][j].getStartOffset();
-        token.endOffset = offsets[i][j].getEndOffset();
+        token.startOffset = j * 10;
+        token.endOffset = j * 10 + testTerms[i].length();
       }
     }
     Arrays.sort(tokens);
@@ -122,13 +120,14 @@ public class TestTermVectorsReader extends LuceneTestCase {
 
     //Create 5 documents for testing, they all have the same
     //terms
-    for(int j=0;j<5;j++)
+    for(int j=0;j<5;j++) {
       writer.addDocument(doc);
+    }
     writer.commit();
-    seg = writer.newestSegment().name;
+    seg = writer.newestSegment();
     writer.close();
 
-    fieldInfos = new FieldInfos(dir, IndexFileNames.segmentFileName(seg, "", IndexFileNames.FIELD_INFOS_EXTENSION));
+    fieldInfos = seg.getFieldInfos(); //new FieldInfos(dir, IndexFileNames.segmentFileName(seg.name, "", IndexFileNames.FIELD_INFOS_EXTENSION));
   }
   
   @Override
@@ -152,9 +151,9 @@ public class TestTermVectorsReader extends LuceneTestCase {
     
     @Override
     public boolean incrementToken() {
-      if (tokenUpto >= tokens.length)
+      if (tokenUpto >= tokens.length) {
         return false;
-      else {
+      } else {
         final TestToken testToken = tokens[tokenUpto++];
         clearAttributes();
         termAtt.append(testToken.text);
@@ -184,282 +183,150 @@ public class TestTermVectorsReader extends LuceneTestCase {
 
   public void test() throws IOException {
     //Check to see the files were created properly in setup
-    assertTrue(dir.fileExists(IndexFileNames.segmentFileName(seg, "", IndexFileNames.VECTORS_DOCUMENTS_EXTENSION)));
-    assertTrue(dir.fileExists(IndexFileNames.segmentFileName(seg, "", IndexFileNames.VECTORS_INDEX_EXTENSION)));
+    IndexReader reader = IndexReader.open(dir);
+    for (IndexReader r : reader.getSequentialSubReaders()) {
+      SegmentInfo s = ((SegmentReader) r).getSegmentInfo();
+      assertTrue(s.getHasVectors());
+      Set<String> files = new HashSet<String>();
+      s.getCodec().termVectorsFormat().files(dir, s, files);
+      assertFalse(files.isEmpty());
+      for (String file : files) {
+        assertTrue(dir.fileExists(file));
+      }
+    }
+    reader.close();
   }
 
   public void testReader() throws IOException {
-    TermVectorsReader reader = new TermVectorsReader(dir, seg, fieldInfos, newIOContext(random));
+    TermVectorsReader reader = Codec.getDefault().termVectorsFormat().vectorsReader(dir, seg, fieldInfos, newIOContext(random));
     for (int j = 0; j < 5; j++) {
-      TermFreqVector vector = reader.get(j, testFields[0]);
-      assertTrue(vector != null);
-      BytesRef[] terms = vector.getTerms();
-      assertTrue(terms != null);
-      assertTrue(terms.length == testTerms.length);
-      for (int i = 0; i < terms.length; i++) {
-        String term = terms[i].utf8ToString();
+      Terms vector = reader.get(j).terms(testFields[0]);
+      assertNotNull(vector);
+      assertEquals(testTerms.length, vector.getUniqueTermCount());
+      TermsEnum termsEnum = vector.iterator(null);
+      for (int i = 0; i < testTerms.length; i++) {
+        final BytesRef text = termsEnum.next();
+        assertNotNull(text);
+        String term = text.utf8ToString();
         //System.out.println("Term: " + term);
-        assertTrue(term.equals(testTerms[i]));
+        assertEquals(testTerms[i], term);
       }
+      assertNull(termsEnum.next());
     }
     reader.close();
   }
 
   public void testPositionReader() throws IOException {
-    TermVectorsReader reader = new TermVectorsReader(dir, seg, fieldInfos, newIOContext(random));
-    TermPositionVector vector;
+    TermVectorsReader reader = Codec.getDefault().termVectorsFormat().vectorsReader(dir, seg, fieldInfos, newIOContext(random));
     BytesRef[] terms;
-    vector = (TermPositionVector) reader.get(0, testFields[0]);
-    assertTrue(vector != null);
-    terms = vector.getTerms();
-    assertTrue(terms != null);
-    assertTrue(terms.length == testTerms.length);
-    for (int i = 0; i < terms.length; i++) {
-      String term = terms[i].utf8ToString();
+    Terms vector = reader.get(0).terms(testFields[0]);
+    assertNotNull(vector);
+    assertEquals(testTerms.length, vector.getUniqueTermCount());
+    TermsEnum termsEnum = vector.iterator(null);
+    DocsAndPositionsEnum dpEnum = null;
+    for (int i = 0; i < testTerms.length; i++) {
+      final BytesRef text = termsEnum.next();
+      assertNotNull(text);
+      String term = text.utf8ToString();
       //System.out.println("Term: " + term);
-      assertTrue(term.equals(testTerms[i]));
-      int[] positions = vector.getTermPositions(i);
-      assertTrue(positions != null);
-      assertTrue(positions.length == this.positions[i].length);
-      for (int j = 0; j < positions.length; j++) {
-        int position = positions[j];
-        assertTrue(position == this.positions[i][j]);
+      assertEquals(testTerms[i], term);
+
+      dpEnum = termsEnum.docsAndPositions(null, dpEnum);
+      assertNotNull(dpEnum);
+      assertTrue(dpEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);
+      assertEquals(dpEnum.freq(), positions[i].length);
+      for (int j = 0; j < positions[i].length; j++) {
+        assertEquals(positions[i][j], dpEnum.nextPosition());
       }
-      TermVectorOffsetInfo[] offset = vector.getOffsets(i);
-      assertTrue(offset != null);
-      assertTrue(offset.length == this.offsets[i].length);
-      for (int j = 0; j < offset.length; j++) {
-        TermVectorOffsetInfo termVectorOffsetInfo = offset[j];
-        assertTrue(termVectorOffsetInfo.equals(offsets[i][j]));
+      assertEquals(DocsEnum.NO_MORE_DOCS, dpEnum.nextDoc());
+
+      dpEnum = termsEnum.docsAndPositions(null, dpEnum);
+      assertTrue(dpEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);
+      assertNotNull(dpEnum);
+      final OffsetAttribute offsetAtt = dpEnum.attributes().getAttribute(OffsetAttribute.class);
+      assertNotNull(offsetAtt);
+      assertEquals(dpEnum.freq(), positions[i].length);
+      for (int j = 0; j < positions[i].length; j++) {
+        assertEquals(positions[i][j], dpEnum.nextPosition());
+        assertEquals(j*10, offsetAtt.startOffset());
+        assertEquals(j*10 + testTerms[i].length(), offsetAtt.endOffset());
       }
+      assertEquals(DocsEnum.NO_MORE_DOCS, dpEnum.nextDoc());
     }
 
-    TermFreqVector freqVector = reader.get(0, testFields[1]); //no pos, no offset
-    assertTrue(freqVector != null);
-    assertTrue(freqVector instanceof TermPositionVector == false);
-    terms = freqVector.getTerms();
-    assertTrue(terms != null);
-    assertTrue(terms.length == testTerms.length);
-    for (int i = 0; i < terms.length; i++) {
-      String term = terms[i].utf8ToString();
+    Terms freqVector = reader.get(0).terms(testFields[1]); //no pos, no offset
+    assertNotNull(freqVector);
+    assertEquals(testTerms.length, freqVector.getUniqueTermCount());
+    termsEnum = freqVector.iterator(null);
+    assertNotNull(termsEnum);
+    for (int i = 0; i < testTerms.length; i++) {
+      final BytesRef text = termsEnum.next();
+      assertNotNull(text);
+      String term = text.utf8ToString();
       //System.out.println("Term: " + term);
-      assertTrue(term.equals(testTerms[i]));
+      assertEquals(testTerms[i], term);
     }
     reader.close();
   }
 
   public void testOffsetReader() throws IOException {
-    TermVectorsReader reader = new TermVectorsReader(dir, seg, fieldInfos, newIOContext(random));
-    TermPositionVector vector = (TermPositionVector) reader.get(0, testFields[0]);
-    assertTrue(vector != null);
-    BytesRef[] terms = vector.getTerms();
-    assertTrue(terms != null);
-    assertTrue(terms.length == testTerms.length);
-    for (int i = 0; i < terms.length; i++) {
-      String term = terms[i].utf8ToString();
-      //System.out.println("Term: " + term);
-      assertTrue(term.equals(testTerms[i]));
-      int[] positions = vector.getTermPositions(i);
-      assertTrue(positions != null);
-      assertTrue(positions.length == this.positions[i].length);
-      for (int j = 0; j < positions.length; j++) {
-        int position = positions[j];
-        assertTrue(position == this.positions[i][j]);
+    TermVectorsReader reader = Codec.getDefault().termVectorsFormat().vectorsReader(dir, seg, fieldInfos, newIOContext(random));
+    Terms vector = reader.get(0).terms(testFields[0]);
+    assertNotNull(vector);
+    TermsEnum termsEnum = vector.iterator(null);
+    assertNotNull(termsEnum);
+    assertEquals(testTerms.length, vector.getUniqueTermCount());
+    DocsAndPositionsEnum dpEnum = null;
+    for (int i = 0; i < testTerms.length; i++) {
+      final BytesRef text = termsEnum.next();
+      assertNotNull(text);
+      String term = text.utf8ToString();
+      assertEquals(testTerms[i], term);
+
+      dpEnum = termsEnum.docsAndPositions(null, dpEnum);
+      assertNotNull(dpEnum);
+      assertTrue(dpEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);
+      assertEquals(dpEnum.freq(), positions[i].length);
+      for (int j = 0; j < positions[i].length; j++) {
+        assertEquals(positions[i][j], dpEnum.nextPosition());
       }
-      TermVectorOffsetInfo[] offset = vector.getOffsets(i);
-      assertTrue(offset != null);
-      assertTrue(offset.length == this.offsets[i].length);
-      for (int j = 0; j < offset.length; j++) {
-        TermVectorOffsetInfo termVectorOffsetInfo = offset[j];
-        assertTrue(termVectorOffsetInfo.equals(offsets[i][j]));
+      assertEquals(DocsEnum.NO_MORE_DOCS, dpEnum.nextDoc());
+
+      dpEnum = termsEnum.docsAndPositions(null, dpEnum);
+      assertTrue(dpEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);
+      final OffsetAttribute offsetAtt = dpEnum.attributes().getAttribute(OffsetAttribute.class);
+      assertNotNull(offsetAtt);
+      assertNotNull(dpEnum);
+      assertEquals(dpEnum.freq(), positions[i].length);
+      for (int j = 0; j < positions[i].length; j++) {
+        assertEquals(positions[i][j], dpEnum.nextPosition());
+        assertEquals(j*10, offsetAtt.startOffset());
+        assertEquals(j*10 + testTerms[i].length(), offsetAtt.endOffset());
       }
+      assertEquals(DocsEnum.NO_MORE_DOCS, dpEnum.nextDoc());
     }
     reader.close();
   }
 
-  public void testMapper() throws IOException {
-    TermVectorsReader reader = new TermVectorsReader(dir, seg, fieldInfos, newIOContext(random));
-    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());
-    reader.get(0, mapper);
-    SortedSet<TermVectorEntry> set = mapper.getTermVectorEntrySet();
-    assertTrue("set is null and it shouldn't be", set != null);
-    //three fields, 4 terms, all terms are the same
-    assertTrue("set Size: " + set.size() + " is not: " + 4, set.size() == 4);
-    //Check offsets and positions
-    for (Iterator<TermVectorEntry> iterator = set.iterator(); iterator.hasNext();) {
-      TermVectorEntry tve =  iterator.next();
-      assertTrue("tve is null and it shouldn't be", tve != null);
-      assertTrue("tve.getOffsets() is null and it shouldn't be", tve.getOffsets() != null);
-      assertTrue("tve.getPositions() is null and it shouldn't be", tve.getPositions() != null);
-
-    }
-
-    mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());
-    reader.get(1, mapper);
-    set = mapper.getTermVectorEntrySet();
-    assertTrue("set is null and it shouldn't be", set != null);
-    //three fields, 4 terms, all terms are the same
-    assertTrue("set Size: " + set.size() + " is not: " + 4, set.size() == 4);
-    //Should have offsets and positions b/c we are munging all the fields together
-    for (Iterator<TermVectorEntry> iterator = set.iterator(); iterator.hasNext();) {
-      TermVectorEntry tve = iterator.next();
-      assertTrue("tve is null and it shouldn't be", tve != null);
-      assertTrue("tve.getOffsets() is null and it shouldn't be", tve.getOffsets() != null);
-      assertTrue("tve.getPositions() is null and it shouldn't be", tve.getPositions() != null);
-
-    }
-
-
-    FieldSortedTermVectorMapper fsMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());
-    reader.get(0, fsMapper);
-    Map<String,SortedSet<TermVectorEntry>> map = fsMapper.getFieldToTerms();
-    assertTrue("map Size: " + map.size() + " is not: " + testFields.length, map.size() == testFields.length);
-    for (Map.Entry<String,SortedSet<TermVectorEntry>> entry : map.entrySet()) {
-      SortedSet<TermVectorEntry> sortedSet =  entry.getValue();
-      assertTrue("sortedSet Size: " + sortedSet.size() + " is not: " + 4, sortedSet.size() == 4);
-      for (final TermVectorEntry tve : sortedSet) {
-        assertTrue("tve is null and it shouldn't be", tve != null);
-        //Check offsets and positions.
-        assertTrue("tve is null and it shouldn't be", tve != null);
-        String field = tve.getField();
-        if (field.equals(testFields[0])) {
-          //should have offsets
-
-          assertTrue("tve.getOffsets() is null and it shouldn't be", tve.getOffsets() != null);
-          assertTrue("tve.getPositions() is null and it shouldn't be", tve.getPositions() != null);
-        }
-        else if (field.equals(testFields[1])) {
-          //should not have offsets
-
-          assertTrue("tve.getOffsets() is not null and it shouldn't be", tve.getOffsets() == null);
-          assertTrue("tve.getPositions() is not null and it shouldn't be", tve.getPositions() == null);
-        }
-      }
-    }
-    //Try mapper that ignores offs and positions
-    fsMapper = new FieldSortedTermVectorMapper(true, true, new TermVectorEntryFreqSortedComparator());
-    reader.get(0, fsMapper);
-    map = fsMapper.getFieldToTerms();
-    assertTrue("map Size: " + map.size() + " is not: " + testFields.length, map.size() == testFields.length);
-    for (final Map.Entry<String,SortedSet<TermVectorEntry>> entry : map.entrySet()) {
-      SortedSet<TermVectorEntry> sortedSet =  entry.getValue();
-      assertTrue("sortedSet Size: " + sortedSet.size() + " is not: " + 4, sortedSet.size() == 4);
-      for (final TermVectorEntry tve : sortedSet) {
-        assertTrue("tve is null and it shouldn't be", tve != null);
-        //Check offsets and positions.
-        assertTrue("tve is null and it shouldn't be", tve != null);
-        String field = tve.getField();
-        if (field.equals(testFields[0])) {
-          //should have offsets
-
-          assertTrue("tve.getOffsets() is null and it shouldn't be", tve.getOffsets() == null);
-          assertTrue("tve.getPositions() is null and it shouldn't be", tve.getPositions() == null);
-        }
-        else if (field.equals(testFields[1])) {
-          //should not have offsets
-
-          assertTrue("tve.getOffsets() is not null and it shouldn't be", tve.getOffsets() == null);
-          assertTrue("tve.getPositions() is not null and it shouldn't be", tve.getPositions() == null);
-        }
-      }
-    }
-
-    // test setDocumentNumber()
-    IndexReader ir = IndexReader.open(dir, true);
-    DocNumAwareMapper docNumAwareMapper = new DocNumAwareMapper();
-    assertEquals(-1, docNumAwareMapper.getDocumentNumber());
-
-    ir.getTermFreqVector(0, docNumAwareMapper);
-    assertEquals(0, docNumAwareMapper.getDocumentNumber());
-    docNumAwareMapper.setDocumentNumber(-1);
-
-    ir.getTermFreqVector(1, docNumAwareMapper);
-    assertEquals(1, docNumAwareMapper.getDocumentNumber());
-    docNumAwareMapper.setDocumentNumber(-1);
-
-    ir.getTermFreqVector(0, "f1", docNumAwareMapper);
-    assertEquals(0, docNumAwareMapper.getDocumentNumber());
-    docNumAwareMapper.setDocumentNumber(-1);
-
-    ir.getTermFreqVector(1, "f2", docNumAwareMapper);
-    assertEquals(1, docNumAwareMapper.getDocumentNumber());
-    docNumAwareMapper.setDocumentNumber(-1);
-
-    ir.getTermFreqVector(0, "f1", docNumAwareMapper);
-    assertEquals(0, docNumAwareMapper.getDocumentNumber());
-
-    ir.close();
-    reader.close();
-  }
-
-
   /**
    * Make sure exceptions and bad params are handled appropriately
    */
   public void testBadParams() throws IOException {
     TermVectorsReader reader = null;
     try {
-      reader = new TermVectorsReader(dir, seg, fieldInfos, newIOContext(random));
+      reader = Codec.getDefault().termVectorsFormat().vectorsReader(dir, seg, fieldInfos, newIOContext(random));
       //Bad document number, good field number
-      reader.get(50, testFields[0]);
-      fail();
-    } catch (IOException e) {
-      // expected exception
-    } finally {
-      reader.close();
-    }
-    try {
-      reader = new TermVectorsReader(dir, seg, fieldInfos, newIOContext(random));
-      //Bad document number, no field
       reader.get(50);
       fail();
-    } catch (IOException e) {
+    } catch (IllegalArgumentException e) {
       // expected exception
     } finally {
       reader.close();
     }
-    try {
-      reader = new TermVectorsReader(dir, seg, fieldInfos, newIOContext(random));
-      //good document number, bad field number
-      TermFreqVector vector = reader.get(0, "f50");
-      assertTrue(vector == null);
-      reader.close();
-    } catch (IOException e) {
-      fail();
-    } finally {
-      reader.close();
-    }
-  }
-
-
-  public static class DocNumAwareMapper extends TermVectorMapper {
-
-    public DocNumAwareMapper() {
-    }
-
-    private int documentNumber = -1;
-
-    @Override
-    public void setExpectations(String field, int numTerms, boolean storeOffsets, boolean storePositions) {
-      if (documentNumber == -1) {
-        throw new RuntimeException("Documentnumber should be set at this point!");
-      }
-    }
-
-    @Override
-    public void map(BytesRef term, int frequency, TermVectorOffsetInfo[] offsets, int[] positions) {
-      if (documentNumber == -1) {
-        throw new RuntimeException("Documentnumber should be set at this point!");
-      }
-    }
-
-    public int getDocumentNumber() {
-      return documentNumber;
-    }
-
-    @Override
-    public void setDocumentNumber(int documentNumber) {
-      this.documentNumber = documentNumber;
-    }
+    reader = Codec.getDefault().termVectorsFormat().vectorsReader(dir, seg, fieldInfos, newIOContext(random));
+    //good document number, bad field
+    Terms vector = reader.get(0).terms("f50");
+    assertNull(vector);
+    reader.close();
   }
 }
diff --git a/lucene/src/test/org/apache/lucene/index/TestTermVectorsWriter.java b/lucene/src/test/org/apache/lucene/index/TestTermVectorsWriter.java
index 8319302..2802792 100644
--- a/lucene/src/test/org/apache/lucene/index/TestTermVectorsWriter.java
+++ b/lucene/src/test/org/apache/lucene/index/TestTermVectorsWriter.java
@@ -26,6 +26,7 @@ import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenFilter;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
@@ -34,6 +35,7 @@ import org.apache.lucene.document.TextField;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.MockDirectoryWrapper;
 import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.LuceneTestCase;
 
 /** tests for writing term vectors */
@@ -58,22 +60,46 @@ public class TestTermVectorsWriter extends LuceneTestCase {
     w.close();
 
     IndexReader r = IndexReader.open(dir, true);
-    TermVectorOffsetInfo[] termOffsets = ((TermPositionVector) r.getTermFreqVector(0, "field")).getOffsets(0);
+    Terms vector = r.getTermVectors(0).terms("field");
+    assertNotNull(vector);
+    TermsEnum termsEnum = vector.iterator(null);
+    assertNotNull(termsEnum.next());
+    assertEquals("", termsEnum.term().utf8ToString());
 
     // Token "" occurred once
-    assertEquals(1, termOffsets.length);
-    assertEquals(8, termOffsets[0].getStartOffset());
-    assertEquals(8, termOffsets[0].getEndOffset());
+    assertEquals(1, termsEnum.totalTermFreq());
+
+    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null);
+    OffsetAttribute offsetAtt = dpEnum.attributes().getAttribute(OffsetAttribute.class);
+    assertNotNull(offsetAtt);
+    assertTrue(dpEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);
+    dpEnum.nextPosition();
+    assertEquals(8, offsetAtt.startOffset());
+    assertEquals(8, offsetAtt.endOffset());
+    assertEquals(DocsEnum.NO_MORE_DOCS, dpEnum.nextDoc());
 
     // Token "abcd" occurred three times
-    termOffsets = ((TermPositionVector) r.getTermFreqVector(0, "field")).getOffsets(1);
-    assertEquals(3, termOffsets.length);
-    assertEquals(0, termOffsets[0].getStartOffset());
-    assertEquals(4, termOffsets[0].getEndOffset());
-    assertEquals(4, termOffsets[1].getStartOffset());
-    assertEquals(8, termOffsets[1].getEndOffset());
-    assertEquals(8, termOffsets[2].getStartOffset());
-    assertEquals(12, termOffsets[2].getEndOffset());
+    assertEquals(new BytesRef("abcd"), termsEnum.next());
+    dpEnum = termsEnum.docsAndPositions(null, dpEnum);
+    offsetAtt = dpEnum.attributes().getAttribute(OffsetAttribute.class);
+    assertNotNull(offsetAtt);
+    assertEquals(3, termsEnum.totalTermFreq());
+
+    assertTrue(dpEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);
+    dpEnum.nextPosition();
+    assertEquals(0, offsetAtt.startOffset());
+    assertEquals(4, offsetAtt.endOffset());
+
+    dpEnum.nextPosition();
+    assertEquals(4, offsetAtt.startOffset());
+    assertEquals(8, offsetAtt.endOffset());
+
+    dpEnum.nextPosition();
+    assertEquals(8, offsetAtt.startOffset());
+    assertEquals(12, offsetAtt.endOffset());
+
+    assertEquals(DocsEnum.NO_MORE_DOCS, dpEnum.nextDoc());
+    assertNull(termsEnum.next());
     r.close();
     dir.close();
   }
@@ -94,12 +120,23 @@ public class TestTermVectorsWriter extends LuceneTestCase {
     w.close();
 
     IndexReader r = IndexReader.open(dir, true);
-    TermVectorOffsetInfo[] termOffsets = ((TermPositionVector) r.getTermFreqVector(0, "field")).getOffsets(0);
-    assertEquals(2, termOffsets.length);
-    assertEquals(0, termOffsets[0].getStartOffset());
-    assertEquals(4, termOffsets[0].getEndOffset());
-    assertEquals(5, termOffsets[1].getStartOffset());
-    assertEquals(9, termOffsets[1].getEndOffset());
+    TermsEnum termsEnum = r.getTermVectors(0).terms("field").iterator(null);
+    assertNotNull(termsEnum.next());
+    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null);
+    OffsetAttribute offsetAtt = dpEnum.attributes().getAttribute(OffsetAttribute.class);
+    assertNotNull(offsetAtt);
+    assertEquals(2, termsEnum.totalTermFreq());
+
+    assertTrue(dpEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);
+    dpEnum.nextPosition();
+    assertEquals(0, offsetAtt.startOffset());
+    assertEquals(4, offsetAtt.endOffset());
+
+    dpEnum.nextPosition();
+    assertEquals(5, offsetAtt.startOffset());
+    assertEquals(9, offsetAtt.endOffset());
+    assertEquals(DocsEnum.NO_MORE_DOCS, dpEnum.nextDoc());
+
     r.close();
     dir.close();
   }
@@ -120,12 +157,23 @@ public class TestTermVectorsWriter extends LuceneTestCase {
     w.close();
 
     IndexReader r = IndexReader.open(dir, true);
-    TermVectorOffsetInfo[] termOffsets = ((TermPositionVector) r.getTermFreqVector(0, "field")).getOffsets(0);
-    assertEquals(2, termOffsets.length);
-    assertEquals(0, termOffsets[0].getStartOffset());
-    assertEquals(4, termOffsets[0].getEndOffset());
-    assertEquals(8, termOffsets[1].getStartOffset());
-    assertEquals(12, termOffsets[1].getEndOffset());
+    TermsEnum termsEnum = r.getTermVectors(0).terms("field").iterator(null);
+    assertNotNull(termsEnum.next());
+    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null);
+    OffsetAttribute offsetAtt = dpEnum.attributes().getAttribute(OffsetAttribute.class);
+    assertNotNull(offsetAtt);
+    assertEquals(2, termsEnum.totalTermFreq());
+
+    assertTrue(dpEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);
+    dpEnum.nextPosition();
+    assertEquals(0, offsetAtt.startOffset());
+    assertEquals(4, offsetAtt.endOffset());
+
+    dpEnum.nextPosition();
+    assertEquals(8, offsetAtt.startOffset());
+    assertEquals(12, offsetAtt.endOffset());
+    assertEquals(DocsEnum.NO_MORE_DOCS, dpEnum.nextDoc());
+
     r.close();
     dir.close();
   }
@@ -137,7 +185,7 @@ public class TestTermVectorsWriter extends LuceneTestCase {
     IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer));
     Document doc = new Document();
     TokenStream stream = analyzer.tokenStream("field", new StringReader("abcd   "));
-    stream.reset(); // TODO: wierd to reset before wrapping with CachingTokenFilter... correct?
+    stream.reset(); // TODO: weird to reset before wrapping with CachingTokenFilter... correct?
     stream = new CachingTokenFilter(stream);
     FieldType customType = new FieldType(TextField.TYPE_UNSTORED);
     customType.setStoreTermVectors(true);
@@ -150,12 +198,23 @@ public class TestTermVectorsWriter extends LuceneTestCase {
     w.close();
 
     IndexReader r = IndexReader.open(dir, true);
-    TermVectorOffsetInfo[] termOffsets = ((TermPositionVector) r.getTermFreqVector(0, "field")).getOffsets(0);
-    assertEquals(2, termOffsets.length);
-    assertEquals(0, termOffsets[0].getStartOffset());
-    assertEquals(4, termOffsets[0].getEndOffset());
-    assertEquals(8, termOffsets[1].getStartOffset());
-    assertEquals(12, termOffsets[1].getEndOffset());
+    TermsEnum termsEnum = r.getTermVectors(0).terms("field").iterator(null);
+    assertNotNull(termsEnum.next());
+    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null);
+    OffsetAttribute offsetAtt = dpEnum.attributes().getAttribute(OffsetAttribute.class);
+    assertNotNull(offsetAtt);
+    assertEquals(2, termsEnum.totalTermFreq());
+
+    assertTrue(dpEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);
+    dpEnum.nextPosition();
+    assertEquals(0, offsetAtt.startOffset());
+    assertEquals(4, offsetAtt.endOffset());
+
+    dpEnum.nextPosition();
+    assertEquals(8, offsetAtt.startOffset());
+    assertEquals(12, offsetAtt.endOffset());
+    assertEquals(DocsEnum.NO_MORE_DOCS, dpEnum.nextDoc());
+
     r.close();
     dir.close();
   }
@@ -177,12 +236,23 @@ public class TestTermVectorsWriter extends LuceneTestCase {
     w.close();
 
     IndexReader r = IndexReader.open(dir, true);
-    TermVectorOffsetInfo[] termOffsets = ((TermPositionVector) r.getTermFreqVector(0, "field")).getOffsets(0);
-    assertEquals(2, termOffsets.length);
-    assertEquals(0, termOffsets[0].getStartOffset());
-    assertEquals(4, termOffsets[0].getEndOffset());
-    assertEquals(9, termOffsets[1].getStartOffset());
-    assertEquals(13, termOffsets[1].getEndOffset());
+    TermsEnum termsEnum = r.getTermVectors(0).terms("field").iterator(null);
+    assertNotNull(termsEnum.next());
+    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null);
+    OffsetAttribute offsetAtt = dpEnum.attributes().getAttribute(OffsetAttribute.class);
+    assertNotNull(offsetAtt);
+    assertEquals(2, termsEnum.totalTermFreq());
+
+    assertTrue(dpEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);
+    dpEnum.nextPosition();
+    assertEquals(0, offsetAtt.startOffset());
+    assertEquals(4, offsetAtt.endOffset());
+
+    dpEnum.nextPosition();
+    assertEquals(9, offsetAtt.startOffset());
+    assertEquals(13, offsetAtt.endOffset());
+    assertEquals(DocsEnum.NO_MORE_DOCS, dpEnum.nextDoc());
+
     r.close();
     dir.close();
   }
@@ -205,17 +275,35 @@ public class TestTermVectorsWriter extends LuceneTestCase {
     w.close();
 
     IndexReader r = IndexReader.open(dir, true);
-    TermPositionVector tpv = ((TermPositionVector) r.getTermFreqVector(0, "field"));
-    TermVectorOffsetInfo[] termOffsets = tpv.getOffsets(0);
-    assertEquals(1, termOffsets.length);
-    assertEquals(0, termOffsets[0].getStartOffset());
-    assertEquals(4, termOffsets[0].getEndOffset());
-    termOffsets = tpv.getOffsets(1);
-    assertEquals(11, termOffsets[0].getStartOffset());
-    assertEquals(17, termOffsets[0].getEndOffset());
-    termOffsets = tpv.getOffsets(2);
-    assertEquals(18, termOffsets[0].getStartOffset());
-    assertEquals(21, termOffsets[0].getEndOffset());
+    TermsEnum termsEnum = r.getTermVectors(0).terms("field").iterator(null);
+    assertNotNull(termsEnum.next());
+    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null);
+    OffsetAttribute offsetAtt = dpEnum.attributes().getAttribute(OffsetAttribute.class);
+    assertNotNull(offsetAtt);
+
+    assertTrue(dpEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);
+    dpEnum.nextPosition();
+    assertEquals(0, offsetAtt.startOffset());
+    assertEquals(4, offsetAtt.endOffset());
+
+    assertNotNull(termsEnum.next());
+    dpEnum = termsEnum.docsAndPositions(null, dpEnum);
+    offsetAtt = dpEnum.attributes().getAttribute(OffsetAttribute.class);
+    assertNotNull(offsetAtt);
+    assertTrue(dpEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);
+    dpEnum.nextPosition();
+    assertEquals(11, offsetAtt.startOffset());
+    assertEquals(17, offsetAtt.endOffset());
+
+    assertNotNull(termsEnum.next());
+    dpEnum = termsEnum.docsAndPositions(null, dpEnum);
+    offsetAtt = dpEnum.attributes().getAttribute(OffsetAttribute.class);
+    assertNotNull(offsetAtt);
+    assertTrue(dpEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);
+    dpEnum.nextPosition();
+    assertEquals(18, offsetAtt.startOffset());
+    assertEquals(21, offsetAtt.endOffset());
+
     r.close();
     dir.close();
   }
@@ -238,14 +326,27 @@ public class TestTermVectorsWriter extends LuceneTestCase {
     w.close();
 
     IndexReader r = IndexReader.open(dir, true);
-    TermPositionVector tpv = ((TermPositionVector) r.getTermFreqVector(0, "field"));
-    TermVectorOffsetInfo[] termOffsets = tpv.getOffsets(0);
-    assertEquals(1, termOffsets.length);
-    assertEquals(1, termOffsets[0].getStartOffset());
-    assertEquals(7, termOffsets[0].getEndOffset());
-    termOffsets = tpv.getOffsets(1);
-    assertEquals(8, termOffsets[0].getStartOffset());
-    assertEquals(11, termOffsets[0].getEndOffset());
+    TermsEnum termsEnum = r.getTermVectors(0).terms("field").iterator(null);
+    assertNotNull(termsEnum.next());
+    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null);
+    OffsetAttribute offsetAtt = dpEnum.attributes().getAttribute(OffsetAttribute.class);
+    assertNotNull(offsetAtt);
+
+    assertEquals(1, (int) termsEnum.totalTermFreq());
+    assertTrue(dpEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);
+    dpEnum.nextPosition();
+    assertEquals(1, offsetAtt.startOffset());
+    assertEquals(7, offsetAtt.endOffset());
+
+    assertNotNull(termsEnum.next());
+    dpEnum = termsEnum.docsAndPositions(null, dpEnum);
+    offsetAtt = dpEnum.attributes().getAttribute(OffsetAttribute.class);
+    assertNotNull(offsetAtt);
+    assertTrue(dpEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);
+    dpEnum.nextPosition();
+    assertEquals(8, offsetAtt.startOffset());
+    assertEquals(11, offsetAtt.endOffset());
+
     r.close();
     dir.close();
   }
@@ -272,14 +373,28 @@ public class TestTermVectorsWriter extends LuceneTestCase {
     w.close();
 
     IndexReader r = IndexReader.open(dir, true);
-    TermPositionVector tpv = ((TermPositionVector) r.getTermFreqVector(0, "field"));
-    TermVectorOffsetInfo[] termOffsets = tpv.getOffsets(0);
-    assertEquals(1, termOffsets.length);
-    assertEquals(0, termOffsets[0].getStartOffset());
-    assertEquals(4, termOffsets[0].getEndOffset());
-    termOffsets = tpv.getOffsets(1);
-    assertEquals(6, termOffsets[0].getStartOffset());
-    assertEquals(12, termOffsets[0].getEndOffset());
+    TermsEnum termsEnum = r.getTermVectors(0).terms("field").iterator(null);
+    assertNotNull(termsEnum.next());
+    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null);
+    OffsetAttribute offsetAtt = dpEnum.attributes().getAttribute(OffsetAttribute.class);
+    assertNotNull(offsetAtt);
+
+    assertEquals(1, (int) termsEnum.totalTermFreq());
+    assertTrue(dpEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);
+    dpEnum.nextPosition();
+    assertEquals(0, offsetAtt.startOffset());
+    assertEquals(4, offsetAtt.endOffset());
+
+    assertNotNull(termsEnum.next());
+    dpEnum = termsEnum.docsAndPositions(null, dpEnum);
+    offsetAtt = dpEnum.attributes().getAttribute(OffsetAttribute.class);
+    assertNotNull(offsetAtt);
+    assertTrue(dpEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);
+    dpEnum.nextPosition();
+    assertEquals(6, offsetAtt.startOffset());
+    assertEquals(12, offsetAtt.endOffset());
+
+
     r.close();
     dir.close();
   }
@@ -321,7 +436,7 @@ public class TestTermVectorsWriter extends LuceneTestCase {
       IndexReader reader = IndexReader.open(dir, true);
       for(int i=0;i<reader.numDocs();i++) {
         reader.document(i);
-        reader.getTermFreqVectors(i);
+        reader.getTermVectors(i);
       }
       reader.close();
 
@@ -373,9 +488,9 @@ public class TestTermVectorsWriter extends LuceneTestCase {
       writer.close();
 
       IndexReader reader = IndexReader.open(dir, true);
-      assertTrue(reader.getTermFreqVectors(0)==null);
-      assertTrue(reader.getTermFreqVectors(1)==null);
-      assertTrue(reader.getTermFreqVectors(2)!=null);
+      assertNull(reader.getTermVectors(0));
+      assertNull(reader.getTermVectors(1));
+      assertNotNull(reader.getTermVectors(2));
       reader.close();
     }
     dir.close();
@@ -419,7 +534,7 @@ public class TestTermVectorsWriter extends LuceneTestCase {
 
     IndexReader reader = IndexReader.open(dir, true);
     for(int i=0;i<10;i++) {
-      reader.getTermFreqVectors(i);
+      reader.getTermVectors(i);
       reader.document(i);
     }
     reader.close();
@@ -446,7 +561,6 @@ public class TestTermVectorsWriter extends LuceneTestCase {
 
     FieldType customType = new FieldType(StringField.TYPE_UNSTORED);
     customType.setStoreTermVectors(true);
-    Field termVectorField = newField("termVector", "termVector", customType2);
     document.add(newField("tvtest", "a b c", customType));
     iw.addDocument(document);
     // Make 2nd segment
@@ -465,7 +579,6 @@ public class TestTermVectorsWriter extends LuceneTestCase {
     Document document = new Document();
     FieldType customType = new FieldType(StringField.TYPE_UNSTORED);
     customType.setStoreTermVectors(true);
-    Field termVectorField = newField("termVector", "termVector", customType);
     document.add(newField("tvtest", "a b c", customType));
     iw.addDocument(document);
     iw.commit();
diff --git a/lucene/src/test/org/apache/lucene/index/TestTermdocPerf.java b/lucene/src/test/org/apache/lucene/index/TestTermdocPerf.java
index 71ef02e..729aeac 100644
--- a/lucene/src/test/org/apache/lucene/index/TestTermdocPerf.java
+++ b/lucene/src/test/org/apache/lucene/index/TestTermdocPerf.java
@@ -113,7 +113,7 @@ public class TestTermdocPerf extends LuceneTestCase {
 
     IndexReader reader = IndexReader.open(dir, true);
 
-    TermsEnum tenum = MultiFields.getTerms(reader, "foo").iterator();
+    TermsEnum tenum = MultiFields.getTerms(reader, "foo").iterator(null);
 
     start = System.currentTimeMillis();
 
diff --git a/lucene/src/test/org/apache/lucene/index/TestTermsEnum.java b/lucene/src/test/org/apache/lucene/index/TestTermsEnum.java
index cc1cff4..b39c138 100644
--- a/lucene/src/test/org/apache/lucene/index/TestTermsEnum.java
+++ b/lucene/src/test/org/apache/lucene/index/TestTermsEnum.java
@@ -62,7 +62,7 @@ public class TestTermsEnum extends LuceneTestCase {
     w.close();
 
     final List<BytesRef> terms = new ArrayList<BytesRef>();
-    final TermsEnum termsEnum = MultiFields.getTerms(r, "body").iterator();
+    final TermsEnum termsEnum = MultiFields.getTerms(r, "body").iterator(null);
     BytesRef term;
     while((term = termsEnum.next()) != null) {
       terms.add(new BytesRef(term));
@@ -485,7 +485,7 @@ public class TestTermsEnum extends LuceneTestCase {
     assertEquals(1, docFreq(r, "xx"));
     assertEquals(1, docFreq(r, "aa4"));
 
-    final TermsEnum te = MultiFields.getTerms(r, FIELD).iterator();
+    final TermsEnum te = MultiFields.getTerms(r, FIELD).iterator(null);
     while(te.next() != null) {
       //System.out.println("TEST: next term=" + te.term().utf8ToString());
     }
@@ -517,7 +517,7 @@ public class TestTermsEnum extends LuceneTestCase {
     assertEquals(1, r.maxDoc());
     Terms terms = MultiFields.getTerms(r, "field");
     if (terms != null) {
-      assertNull(terms.iterator().next());
+      assertNull(terms.iterator(null).next());
     }
     r.close();
     d.close();
@@ -617,7 +617,7 @@ public class TestTermsEnum extends LuceneTestCase {
         System.out.println("  " + t.utf8ToString() + " " + t);
       }
     }
-    final TermsEnum te = MultiFields.getTerms(r, FIELD).iterator();
+    final TermsEnum te = MultiFields.getTerms(r, FIELD).iterator(null);
 
     final int END_LOC = -validTerms.length-1;
     
diff --git a/lucene/src/test/org/apache/lucene/index/TestTermsEnum2.java b/lucene/src/test/org/apache/lucene/index/TestTermsEnum2.java
index 2d3d334..3f4c414 100644
--- a/lucene/src/test/org/apache/lucene/index/TestTermsEnum2.java
+++ b/lucene/src/test/org/apache/lucene/index/TestTermsEnum2.java
@@ -117,7 +117,7 @@ public class TestTermsEnum2 extends LuceneTestCase {
     for (int i = 0; i < numIterations; i++) {
       String reg = AutomatonTestUtil.randomRegexp(random);
       Automaton automaton = new RegExp(reg, RegExp.NONE).toAutomaton();
-      TermsEnum te = MultiFields.getTerms(reader, "field").iterator();
+      TermsEnum te = MultiFields.getTerms(reader, "field").iterator(null);
       ArrayList<BytesRef> unsortedTerms = new ArrayList<BytesRef>(terms);
       Collections.shuffle(unsortedTerms, random);
 
@@ -140,7 +140,7 @@ public class TestTermsEnum2 extends LuceneTestCase {
   /** mixes up seek and next for all terms */
   public void testSeekingAndNexting() throws Exception {
     for (int i = 0; i < numIterations; i++) {
-      TermsEnum te = MultiFields.getTerms(reader, "field").iterator();
+      TermsEnum te = MultiFields.getTerms(reader, "field").iterator(null);
 
       for (BytesRef term : terms) {
         int c = random.nextInt(3);
diff --git a/lucene/src/test/org/apache/lucene/index/TestTransactions.java b/lucene/src/test/org/apache/lucene/index/TestTransactions.java
index 0be1cb7..5f84820 100644
--- a/lucene/src/test/org/apache/lucene/index/TestTransactions.java
+++ b/lucene/src/test/org/apache/lucene/index/TestTransactions.java
@@ -211,6 +211,8 @@ public class TestTransactions extends LuceneTestCase {
     dir2.setPreventDoubleWrite(false);
     dir1.failOn(new RandomFailure());
     dir2.failOn(new RandomFailure());
+    dir1.setFailOnOpenInput(false);
+    dir2.setFailOnOpenInput(false);
 
     initIndex(dir1);
     initIndex(dir2);
diff --git a/lucene/src/test/org/apache/lucene/index/codecs/lucene3x/TestSurrogates.java b/lucene/src/test/org/apache/lucene/index/codecs/lucene3x/TestSurrogates.java
index fdf3a42..03bdb7f 100644
--- a/lucene/src/test/org/apache/lucene/index/codecs/lucene3x/TestSurrogates.java
+++ b/lucene/src/test/org/apache/lucene/index/codecs/lucene3x/TestSurrogates.java
@@ -108,7 +108,9 @@ public class TestSurrogates extends LuceneTestCase {
       String field;
       int termCount = 0;
       while((field = fieldsEnum.next()) != null) {
-        TermsEnum termsEnum = fieldsEnum.terms();
+        Terms terms = fieldsEnum.terms();
+        assertNotNull(terms);
+        TermsEnum termsEnum = terms.iterator(null);
         BytesRef text;
         BytesRef lastText = null;
         while((text = termsEnum.next()) != null) {
@@ -162,7 +164,7 @@ public class TestSurrogates extends LuceneTestCase {
       // seek to it
       TermsEnum te = tes.get(field);
       if (te == null) {
-        te = MultiFields.getTerms(reader, field).iterator();
+        te = MultiFields.getTerms(reader, field).iterator(null);
         tes.put(field, te);
       }
 
@@ -227,7 +229,7 @@ public class TestSurrogates extends LuceneTestCase {
           // term does not exist:
           TermsEnum te = tes.get(field);
           if (te == null) {
-            te = MultiFields.getTerms(reader, field).iterator();
+            te = MultiFields.getTerms(reader, field).iterator(null);
             tes.put(field, te);
           }
 
diff --git a/lucene/src/test/org/apache/lucene/index/codecs/lucene3x/TestTermInfosReaderIndex.java b/lucene/src/test/org/apache/lucene/index/codecs/lucene3x/TestTermInfosReaderIndex.java
index b40ca74..827e789 100644
--- a/lucene/src/test/org/apache/lucene/index/codecs/lucene3x/TestTermInfosReaderIndex.java
+++ b/lucene/src/test/org/apache/lucene/index/codecs/lucene3x/TestTermInfosReaderIndex.java
@@ -38,12 +38,16 @@ import org.apache.lucene.index.MultiFields;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.SegmentReader;
 import org.apache.lucene.index.Term;
+import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.index.codecs.Codec;
+import org.apache.lucene.index.codecs.FieldInfosReader;
 import org.apache.lucene.index.codecs.preflexrw.PreFlexRWCodec;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.search.TopDocs;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.store.LockObtainFailedException;
 import org.apache.lucene.util.LuceneTestCase;
@@ -80,7 +84,8 @@ public class TestTermInfosReaderIndex extends LuceneTestCase {
     String segment = r.getSegmentName();
     r.close();
 
-    FieldInfos fieldInfos = new FieldInfos(directory, IndexFileNames.segmentFileName(segment, "", IndexFileNames.FIELD_INFOS_EXTENSION));
+    FieldInfosReader infosReader = new PreFlexRWCodec().fieldInfosFormat().getFieldInfosReader();
+    FieldInfos fieldInfos = infosReader.read(directory, segment, IOContext.READONCE);
     String segmentFileName = IndexFileNames.segmentFileName(segment, "", Lucene3xPostingsFormat.TERMS_INDEX_EXTENSION);
     long tiiFileLength = directory.fileLength(segmentFileName);
     IndexInput input = directory.openInput(segmentFileName, newIOContext(random));
@@ -140,13 +145,15 @@ public class TestTermInfosReaderIndex extends LuceneTestCase {
     FieldsEnum fieldsEnum = MultiFields.getFields(reader).iterator();
     String field;
     while((field = fieldsEnum.next()) != null) {
-      TermsEnum terms = fieldsEnum.terms();
-      while (terms.next() != null) {
+      Terms terms = fieldsEnum.terms();
+      assertNotNull(terms);
+      TermsEnum termsEnum = terms.iterator(null);
+      while (termsEnum.next() != null) {
         if (sample.size() >= size) {
           int pos = random.nextInt(size);
-          sample.set(pos, new Term(field, terms.term()));
+          sample.set(pos, new Term(field, termsEnum.term()));
         } else {
-          sample.add(new Term(field, terms.term()));
+          sample.add(new Term(field, termsEnum.term()));
         }
       }
     }
diff --git a/lucene/src/test/org/apache/lucene/index/codecs/pulsing/Test10KPulsings.java b/lucene/src/test/org/apache/lucene/index/codecs/pulsing/Test10KPulsings.java
index 4ce4a0d..c775fa4 100644
--- a/lucene/src/test/org/apache/lucene/index/codecs/pulsing/Test10KPulsings.java
+++ b/lucene/src/test/org/apache/lucene/index/codecs/pulsing/Test10KPulsings.java
@@ -35,13 +35,10 @@ import org.apache.lucene.index.MultiFields;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.PostingsFormat;
-import org.apache.lucene.index.codecs.lucene40.Lucene40PostingsBaseFormat;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.store.MockDirectoryWrapper;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util._TestUtil;
-import org.junit.Ignore;
 
 /**
  * Pulses 10k terms/docs, 
@@ -82,7 +79,7 @@ public class Test10KPulsings extends LuceneTestCase {
     IndexReader ir = iw.getReader();
     iw.close();
 
-    TermsEnum te = MultiFields.getTerms(ir, "field").iterator();
+    TermsEnum te = MultiFields.getTerms(ir, "field").iterator(null);
     DocsEnum de = null;
     
     for (int i = 0; i < 10050; i++) {
@@ -140,7 +137,7 @@ public class Test10KPulsings extends LuceneTestCase {
     IndexReader ir = iw.getReader();
     iw.close();
 
-    TermsEnum te = MultiFields.getTerms(ir, "field").iterator();
+    TermsEnum te = MultiFields.getTerms(ir, "field").iterator(null);
     DocsEnum de = null;
     
     for (int i = 0; i < 10050; i++) {
diff --git a/lucene/src/test/org/apache/lucene/index/codecs/pulsing/TestPulsingReuse.java b/lucene/src/test/org/apache/lucene/index/codecs/pulsing/TestPulsingReuse.java
index c1addf5..a9cd508 100644
--- a/lucene/src/test/org/apache/lucene/index/codecs/pulsing/TestPulsingReuse.java
+++ b/lucene/src/test/org/apache/lucene/index/codecs/pulsing/TestPulsingReuse.java
@@ -57,7 +57,7 @@ public class TestPulsingReuse extends LuceneTestCase {
     IndexReader segment = ir.getSequentialSubReaders()[0];
     DocsEnum reuse = null;
     Map<DocsEnum,Boolean> allEnums = new IdentityHashMap<DocsEnum,Boolean>();
-    TermsEnum te = segment.terms("foo").iterator();
+    TermsEnum te = segment.terms("foo").iterator(null);
     while (te.next() != null) {
       reuse = te.docs(null, reuse);
       allEnums.put(reuse, true);
@@ -67,7 +67,7 @@ public class TestPulsingReuse extends LuceneTestCase {
     
     allEnums.clear();
     DocsAndPositionsEnum posReuse = null;
-    te = segment.terms("foo").iterator();
+    te = segment.terms("foo").iterator(null);
     while (te.next() != null) {
       posReuse = te.docsAndPositions(null, posReuse);
       allEnums.put(posReuse, true);
@@ -99,7 +99,7 @@ public class TestPulsingReuse extends LuceneTestCase {
     IndexReader segment = ir.getSequentialSubReaders()[0];
     DocsEnum reuse = null;
     Map<DocsEnum,Boolean> allEnums = new IdentityHashMap<DocsEnum,Boolean>();
-    TermsEnum te = segment.terms("foo").iterator();
+    TermsEnum te = segment.terms("foo").iterator(null);
     while (te.next() != null) {
       reuse = te.docs(null, reuse);
       allEnums.put(reuse, true);
@@ -109,7 +109,7 @@ public class TestPulsingReuse extends LuceneTestCase {
     
     allEnums.clear();
     DocsAndPositionsEnum posReuse = null;
-    te = segment.terms("foo").iterator();
+    te = segment.terms("foo").iterator(null);
     while (te.next() != null) {
       posReuse = te.docsAndPositions(null, posReuse);
       allEnums.put(posReuse, true);
diff --git a/lucene/src/test/org/apache/lucene/search/FieldCacheRewriteMethod.java b/lucene/src/test/org/apache/lucene/search/FieldCacheRewriteMethod.java
index 0315b68..e998be0 100644
--- a/lucene/src/test/org/apache/lucene/search/FieldCacheRewriteMethod.java
+++ b/lucene/src/test/org/apache/lucene/search/FieldCacheRewriteMethod.java
@@ -99,7 +99,7 @@ public final class FieldCacheRewriteMethod extends MultiTermQuery.RewriteMethod
         }
         
         @Override
-        public TermsEnum iterator() throws IOException {
+        public TermsEnum iterator(TermsEnum reuse) throws IOException {
           return fcsi.getTermsEnum();
         }
 
@@ -127,12 +127,10 @@ public final class FieldCacheRewriteMethod extends MultiTermQuery.RewriteMethod
       assert termsEnum != null;
       if (termsEnum.next() != null) {
         // fill into a OpenBitSet
-        int termCount = 0;
         do {
           long ord = termsEnum.ord();
           if (ord > 0) {
             termSet.set(ord);
-            termCount++;
           }
         } while (termsEnum.next() != null);
       } else {
diff --git a/lucene/src/test/org/apache/lucene/search/TestMultiPhraseQuery.java b/lucene/src/test/org/apache/lucene/search/TestMultiPhraseQuery.java
index ccdf6de..f866e4a 100644
--- a/lucene/src/test/org/apache/lucene/search/TestMultiPhraseQuery.java
+++ b/lucene/src/test/org/apache/lucene/search/TestMultiPhraseQuery.java
@@ -80,7 +80,7 @@ public class TestMultiPhraseQuery extends LuceneTestCase {
     
     // this TermEnum gives "piccadilly", "pie" and "pizza".
     String prefix = "pi";
-    TermsEnum te = MultiFields.getFields(reader).terms("body").iterator();
+    TermsEnum te = MultiFields.getFields(reader).terms("body").iterator(null);
     te.seekCeil(new BytesRef(prefix));
     do {
       String s = te.term().utf8ToString();
@@ -587,4 +587,3 @@ public class TestMultiPhraseQuery extends LuceneTestCase {
   }
   
 }
-
diff --git a/lucene/src/test/org/apache/lucene/search/TestMultiTermQueryRewrites.java b/lucene/src/test/org/apache/lucene/search/TestMultiTermQueryRewrites.java
index 50ae518..917a9b0 100644
--- a/lucene/src/test/org/apache/lucene/search/TestMultiTermQueryRewrites.java
+++ b/lucene/src/test/org/apache/lucene/search/TestMultiTermQueryRewrites.java
@@ -157,7 +157,7 @@ public class TestMultiTermQueryRewrites extends LuceneTestCase {
     final MultiTermQuery mtq = new MultiTermQuery("data") {
       @Override
       protected TermsEnum getTermsEnum(Terms terms, AttributeSource atts) throws IOException {
-        return new TermRangeTermsEnum(terms.iterator(), new BytesRef("2"), new BytesRef("7"), true, true) {
+        return new TermRangeTermsEnum(terms.iterator(null), new BytesRef("2"), new BytesRef("7"), true, true) {
           final BoostAttribute boostAtt =
             attributes().addAttribute(BoostAttribute.class);
         
diff --git a/lucene/src/test/org/apache/lucene/search/TestMultiThreadTermVectors.java b/lucene/src/test/org/apache/lucene/search/TestMultiThreadTermVectors.java
index 4285c5a..106d17b 100644
--- a/lucene/src/test/org/apache/lucene/search/TestMultiThreadTermVectors.java
+++ b/lucene/src/test/org/apache/lucene/search/TestMultiThreadTermVectors.java
@@ -17,17 +17,19 @@ package org.apache.lucene.search;
  * limitations under the License.
  */
 
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.LuceneTestCase;
+import java.io.IOException;
+
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.*;
+import org.apache.lucene.index.Fields;
+import org.apache.lucene.index.FieldsEnum;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.TermFreqVector;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.English;
-
-import java.io.IOException;
+import org.apache.lucene.util.LuceneTestCase;
 
 public class TestMultiThreadTermVectors extends LuceneTestCase {
   private Directory directory;
@@ -161,34 +163,34 @@ class MultiThreadTermVectorsReader implements Runnable {
     long start = 0L;
     for (int docId = 0; docId < numDocs; docId++) {
       start = System.currentTimeMillis();
-      TermFreqVector [] vectors = reader.getTermFreqVectors(docId);
+      Fields vectors = reader.getTermVectors(docId);
       timeElapsed += System.currentTimeMillis()-start;
       
       // verify vectors result
       verifyVectors(vectors, docId);
       
       start = System.currentTimeMillis();
-      TermFreqVector vector = reader.getTermFreqVector(docId, "field");
+      Terms vector = reader.getTermVectors(docId).terms("field");
       timeElapsed += System.currentTimeMillis()-start;
       
-      vectors = new TermFreqVector[1];
-      vectors[0] = vector;
-      
-      verifyVectors(vectors, docId);
-      
+      verifyVector(vector.iterator(null), docId);
     }
   }
   
-  private void verifyVectors(TermFreqVector[] vectors, int num) {
+  private void verifyVectors(Fields vectors, int num) throws IOException {
+    FieldsEnum fieldsEnum = vectors.iterator();
+    while(fieldsEnum.next() != null) {
+      Terms terms = fieldsEnum.terms();
+      assert terms != null;
+      verifyVector(terms.iterator(null), num);
+    }
+  }
+
+  private void verifyVector(TermsEnum vector, int num) throws IOException {
     StringBuilder temp = new StringBuilder();
-    BytesRef[] terms = null;
-    for (int i = 0; i < vectors.length; i++) {
-      terms = vectors[i].getTerms();
-      for (int z = 0; z < terms.length; z++) {
-        temp.append(terms[z].utf8ToString());
-      }
+    while(vector.next() != null) {
+      temp.append(vector.term().utf8ToString());
     }
-    
     if (!English.intToEnglish(num).trim().equals(temp.toString().trim()))
         System.out.println("wrong term result");
   }
diff --git a/lucene/src/test/org/apache/lucene/search/TestPhrasePrefixQuery.java b/lucene/src/test/org/apache/lucene/search/TestPhrasePrefixQuery.java
index 74e2c21..7b443fb 100644
--- a/lucene/src/test/org/apache/lucene/search/TestPhrasePrefixQuery.java
+++ b/lucene/src/test/org/apache/lucene/search/TestPhrasePrefixQuery.java
@@ -73,7 +73,7 @@ public class TestPhrasePrefixQuery extends LuceneTestCase {
     
     // this TermEnum gives "piccadilly", "pie" and "pizza".
     String prefix = "pi";
-    TermsEnum te = MultiFields.getFields(reader).terms("body").iterator();
+    TermsEnum te = MultiFields.getFields(reader).terms("body").iterator(null);
     te.seekCeil(new BytesRef(prefix));
     do {
       String s = te.term().utf8ToString();
diff --git a/lucene/src/test/org/apache/lucene/search/TestPrefixRandom.java b/lucene/src/test/org/apache/lucene/search/TestPrefixRandom.java
index e43ab23..619e94d 100644
--- a/lucene/src/test/org/apache/lucene/search/TestPrefixRandom.java
+++ b/lucene/src/test/org/apache/lucene/search/TestPrefixRandom.java
@@ -90,7 +90,7 @@ public class TestPrefixRandom extends LuceneTestCase {
     
     @Override
     protected TermsEnum getTermsEnum(Terms terms, AttributeSource atts) throws IOException {
-      return new SimplePrefixTermsEnum(terms.iterator(), prefix);
+      return new SimplePrefixTermsEnum(terms.iterator(null), prefix);
     }
 
     private class SimplePrefixTermsEnum extends FilteredTermsEnum {
diff --git a/lucene/src/test/org/apache/lucene/search/TestQueryTermVector.java b/lucene/src/test/org/apache/lucene/search/TestQueryTermVector.java
deleted file mode 100644
index 488b886..0000000
--- a/lucene/src/test/org/apache/lucene/search/TestQueryTermVector.java
+++ /dev/null
@@ -1,56 +0,0 @@
-package org.apache.lucene.search;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.analysis.MockAnalyzer;
-
-public class TestQueryTermVector extends LuceneTestCase {
-
-  public void testConstructor() {
-    BytesRef [] queryTerm = {new BytesRef("foo"), new BytesRef("bar"), new BytesRef("foo"), 
-        new BytesRef("again"), new BytesRef("foo"), new BytesRef("bar"), new BytesRef("go"),
-        new BytesRef("go"), new BytesRef("go")};
-    //Items are sorted lexicographically
-    BytesRef [] gold = {new BytesRef("again"), new BytesRef("bar"), new BytesRef("foo"), new BytesRef("go")};
-    int [] goldFreqs = {1, 2, 3, 3};
-    QueryTermVector result = new QueryTermVector(queryTerm);
-    BytesRef [] terms = result.getTerms();
-    assertTrue(terms.length == 4);
-    int [] freq = result.getTermFrequencies();
-    assertTrue(freq.length == 4);
-    checkGold(terms, gold, freq, goldFreqs);
-    result = new QueryTermVector(null);
-    assertTrue(result.getTerms().length == 0);
-    
-    result = new QueryTermVector("foo bar foo again foo bar go go go", new MockAnalyzer(random));
-    terms = result.getTerms();
-    assertTrue(terms.length == 4);
-    freq = result.getTermFrequencies();
-    assertTrue(freq.length == 4);
-    checkGold(terms, gold, freq, goldFreqs);
-  }
-
-  private void checkGold(BytesRef[] terms, BytesRef[] gold, int[] freq, int[] goldFreqs) {
-    for (int i = 0; i < terms.length; i++) {
-      assertTrue(terms[i].equals(gold[i]));
-      assertTrue(freq[i] == goldFreqs[i]);
-    }
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/search/TestRegexpRandom2.java b/lucene/src/test/org/apache/lucene/search/TestRegexpRandom2.java
index 9fdd6a9..56c695b 100644
--- a/lucene/src/test/org/apache/lucene/search/TestRegexpRandom2.java
+++ b/lucene/src/test/org/apache/lucene/search/TestRegexpRandom2.java
@@ -113,7 +113,7 @@ public class TestRegexpRandom2 extends LuceneTestCase {
     
     @Override
     protected TermsEnum getTermsEnum(Terms terms, AttributeSource atts) throws IOException {
-      return new SimpleAutomatonTermsEnum(terms.iterator());
+      return new SimpleAutomatonTermsEnum(terms.iterator(null));
     }
 
     private class SimpleAutomatonTermsEnum extends FilteredTermsEnum {
diff --git a/lucene/src/test/org/apache/lucene/search/TestTermVectors.java b/lucene/src/test/org/apache/lucene/search/TestTermVectors.java
index 003086d..0b2bd94 100644
--- a/lucene/src/test/org/apache/lucene/search/TestTermVectors.java
+++ b/lucene/src/test/org/apache/lucene/search/TestTermVectors.java
@@ -17,11 +17,13 @@ package org.apache.lucene.search;
  * limitations under the License.
  */
 
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.LuceneTestCase;
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
@@ -31,11 +33,8 @@ import org.apache.lucene.index.IndexWriterConfig.OpenMode;
 import org.apache.lucene.search.similarities.DefaultSimilarityProvider;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.English;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-import java.util.SortedSet;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.LuceneTestCase;
 
 public class TestTermVectors extends LuceneTestCase {
   private IndexSearcher searcher;
@@ -95,17 +94,13 @@ public class TestTermVectors extends LuceneTestCase {
     assertEquals(100, hits.length);
       
     for (int i = 0; i < hits.length; i++) {
-      TermFreqVector [] vector = searcher.reader.getTermFreqVectors(hits[i].doc);
-      assertTrue(vector != null);
-      assertEquals("doc=" + hits[i].doc + " tv=" + vector, 1, vector.length);
+      Fields vectors = searcher.reader.getTermVectors(hits[i].doc);
+      assertNotNull(vectors);
+      assertEquals("doc=" + hits[i].doc + " tv=" + vectors, 1, vectors.getUniqueFieldCount());
     }
-    TermFreqVector vector;
-    vector = searcher.reader.getTermFreqVector(hits[0].doc, "noTV");
+    Terms vector;
+    vector = searcher.reader.getTermVectors(hits[0].doc).terms("noTV");
     assertNull(vector);
-
-    TestTermVectorMapper mapper = new TestTermVectorMapper();
-    searcher.reader.getTermFreqVector(hits[0].doc, "noTV", mapper);
-    assertNull(mapper.field);
   }
   
   public void testTermVectorsFieldOrder() throws IOException {
@@ -123,23 +118,38 @@ public class TestTermVectors extends LuceneTestCase {
     writer.addDocument(doc);
     IndexReader reader = writer.getReader();
     writer.close();
-    TermFreqVector[] v = reader.getTermFreqVectors(0);
-    assertEquals(4, v.length);
+    Fields v = reader.getTermVectors(0);
+    assertEquals(4, v.getUniqueFieldCount());
     String[] expectedFields = new String[]{"a", "b", "c", "x"};
     int[] expectedPositions = new int[]{1, 2, 0};
-    for(int i=0;i<v.length;i++) {
-      TermPositionVector posVec = (TermPositionVector) v[i];
-      assertEquals(expectedFields[i], posVec.getField());
-      BytesRef[] terms = posVec.getTerms();
-      assertEquals(3, terms.length);
-      assertEquals("content", terms[0].utf8ToString());
-      assertEquals("here", terms[1].utf8ToString());
-      assertEquals("some", terms[2].utf8ToString());
-      for(int j=0;j<3;j++) {
-        int[] positions = posVec.getTermPositions(j);
-        assertEquals(1, positions.length);
-        assertEquals(expectedPositions[j], positions[0]);
-      }
+    FieldsEnum fieldsEnum = v.iterator();
+    for(int i=0;i<expectedFields.length;i++) {
+      assertEquals(expectedFields[i], fieldsEnum.next());
+      assertEquals(3, v.terms(expectedFields[i]).getUniqueTermCount());
+
+      DocsAndPositionsEnum dpEnum = null;
+      Terms terms = fieldsEnum.terms();
+      assertNotNull(terms);
+      TermsEnum termsEnum = terms.iterator(null);
+      assertEquals("content", termsEnum.next().utf8ToString());
+      dpEnum = termsEnum.docsAndPositions(null, dpEnum);
+      assertTrue(dpEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);
+      assertEquals(1, dpEnum.freq());
+      assertEquals(expectedPositions[0], dpEnum.nextPosition());
+
+      assertEquals("here", termsEnum.next().utf8ToString());
+      dpEnum = termsEnum.docsAndPositions(null, dpEnum);
+      assertTrue(dpEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);
+      assertEquals(1, dpEnum.freq());
+      assertEquals(expectedPositions[1], dpEnum.nextPosition());
+
+      assertEquals("some", termsEnum.next().utf8ToString());
+      dpEnum = termsEnum.docsAndPositions(null, dpEnum);
+      assertTrue(dpEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);
+      assertEquals(1, dpEnum.freq());
+      assertEquals(expectedPositions[2], dpEnum.nextPosition());
+
+      assertNull(termsEnum.next());
     }
     reader.close();
     dir.close();
@@ -149,68 +159,61 @@ public class TestTermVectors extends LuceneTestCase {
     Query query = new TermQuery(new Term("field", "zero"));
     ScoreDoc[] hits = searcher.search(query, null, 1000).scoreDocs;
     assertEquals(1, hits.length);
-    
+
+    DocsAndPositionsEnum dpEnum = null;
     for (int i = 0; i < hits.length; i++) {
-      TermFreqVector[] vector = searcher.reader.getTermFreqVectors(hits[i].doc);
-      assertTrue(vector != null);
-      assertTrue(vector.length == 1);
+      Fields vectors = searcher.reader.getTermVectors(hits[i].doc);
+      assertNotNull(vectors);
+      assertEquals(1, vectors.getUniqueFieldCount());
       
-      boolean shouldBePosVector = (hits[i].doc % 2 == 0) ? true : false;
-      assertTrue((shouldBePosVector == false)
-          || (shouldBePosVector == true && (vector[0] instanceof TermPositionVector == true)));
+      TermsEnum termsEnum = vectors.terms("field").iterator(null);
+      assertNotNull(termsEnum.next());
+      dpEnum = termsEnum.docsAndPositions(null, dpEnum);
+      OffsetAttribute offsetAtt = dpEnum == null ? null : dpEnum.attributes().getAttribute(OffsetAttribute.class);
+
+      boolean shouldBePosVector = hits[i].doc % 2 == 0;
+      assertTrue(!shouldBePosVector
+                 || (shouldBePosVector && dpEnum != null));
       
-      boolean shouldBeOffVector = (hits[i].doc % 3 == 0) ? true : false;
-      assertTrue((shouldBeOffVector == false)
-          || (shouldBeOffVector == true && (vector[0] instanceof TermPositionVector == true)));
+      boolean shouldBeOffVector = hits[i].doc % 3 == 0;
+      assertTrue(!shouldBeOffVector
+                 || (shouldBeOffVector && offsetAtt != null));
       
       if (shouldBePosVector || shouldBeOffVector) {
-        TermPositionVector posVec = (TermPositionVector) vector[0];
-        BytesRef[] terms = posVec.getTerms();
-        assertTrue(terms != null && terms.length > 0);
-        
-        for (int j = 0; j < terms.length; j++) {
-          int[] positions = posVec.getTermPositions(j);
-          TermVectorOffsetInfo[] offsets = posVec.getOffsets(j);
-          
+        while(true) {
+          dpEnum = termsEnum.docsAndPositions(null, dpEnum);
+          assertNotNull(dpEnum);
+          offsetAtt = dpEnum.attributes().getAttribute(OffsetAttribute.class);
+
           if (shouldBePosVector) {
-            assertTrue(positions != null);
-            assertTrue(positions.length > 0);
-          } else assertTrue(positions == null);
+            assertTrue(dpEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);
+          }
           
           if (shouldBeOffVector) {
-            assertTrue(offsets != null);
-            assertTrue(offsets.length > 0);
-          } else assertTrue(offsets == null);
+            assertNotNull(offsetAtt);
+          } else {
+            assertNull(offsetAtt);
+          }
+
+          if (termsEnum.next() == null) {
+            break;
+          }
         }
       } else {
-        try {
-          assertTrue(false);
-        } catch (ClassCastException ignore) {
-          TermFreqVector freqVec = vector[0];
-          BytesRef[] terms = freqVec.getTerms();
-          assertTrue(terms != null && terms.length > 0);
-        }
-        
+        fail();
       }
     }
   }
   
-  public void testTermOffsetVectors() {
+  public void testTermOffsetVectors() throws IOException {
     Query query = new TermQuery(new Term("field", "fifty"));
-    try {
-      ScoreDoc[] hits = searcher.search(query, null, 1000).scoreDocs;
-      assertEquals(100, hits.length);
+    ScoreDoc[] hits = searcher.search(query, null, 1000).scoreDocs;
+    assertEquals(100, hits.length);
       
-      for (int i = 0; i < hits.length; i++)
-      {
-        TermFreqVector [] vector = searcher.reader.getTermFreqVectors(hits[i].doc);
-        assertTrue(vector != null);
-        assertTrue(vector.length == 1);
-        
-        //assertTrue();
-      }
-    } catch (IOException e) {
-      assertTrue(false);
+    for (int i = 0; i < hits.length; i++) {
+      Fields vectors = searcher.reader.getTermVectors(hits[i].doc);
+      assertNotNull(vectors);
+      assertEquals(1, vectors.getUniqueFieldCount());
     }
   }
 
@@ -260,16 +263,19 @@ public class TestTermVectors extends LuceneTestCase {
     
     DocsEnum docs = null;
     while(fields.next() != null) {
-      TermsEnum terms = fields.terms();
-      while(terms.next() != null) {
-        String text = terms.term().utf8ToString();
-        docs = terms.docs(MultiFields.getLiveDocs(knownSearcher.reader), docs);
+      Terms terms = fields.terms();
+      assertNotNull(terms);
+      TermsEnum termsEnum = terms.iterator(null);
+
+      while (termsEnum.next() != null) {
+        String text = termsEnum.term().utf8ToString();
+        docs = termsEnum.docs(MultiFields.getLiveDocs(knownSearcher.reader), docs);
         
         while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {
           int docId = docs.docID();
           int freq = docs.freq();
           //System.out.println("Doc Id: " + docId + " freq " + freq);
-          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, "field");
+          Terms vector = knownSearcher.reader.getTermVectors(docId).terms("field");
           //float tf = sim.tf(freq);
           //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());
           //float qNorm = sim.queryNorm()
@@ -277,14 +283,12 @@ public class TestTermVectors extends LuceneTestCase {
           //float lNorm = sim.lengthNorm("field", vector.getTerms().length);
           //float coord = sim.coord()
           //System.out.println("TF: " + tf + " IDF: " + idf + " LenNorm: " + lNorm);
-          assertTrue(vector != null);
-          BytesRef[] vTerms = vector.getTerms();
-          int [] freqs = vector.getTermFrequencies();
-          for (int i = 0; i < vTerms.length; i++)
-          {
-            if (text.equals(vTerms[i].utf8ToString()))
-            {
-              assertTrue(freqs[i] == freq);
+          assertNotNull(vector);
+          TermsEnum termsEnum2 = vector.iterator(null);
+
+          while(termsEnum2.next() != null) {
+            if (text.equals(termsEnum2.term().utf8ToString())) {
+              assertEquals(freq, termsEnum2.totalTermFreq());
             }
           }
         }
@@ -304,46 +308,20 @@ public class TestTermVectors extends LuceneTestCase {
     assertTrue(hits[0].doc == 2);
     assertTrue(hits[1].doc == 3);
     assertTrue(hits[2].doc == 0);
-    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, "field");
-    assertTrue(vector != null);
+    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms("field");
+    assertNotNull(vector);
     //System.out.println("Vector: " + vector);
-    BytesRef[] terms = vector.getTerms();
-    int [] freqs = vector.getTermFrequencies();
-    assertTrue(terms != null && terms.length == 10);
-    for (int i = 0; i < terms.length; i++) {
-      String term = terms[i].utf8ToString();
+    assertEquals(10, vector.getUniqueTermCount());
+    TermsEnum termsEnum = vector.iterator(null);
+    while(termsEnum.next() != null) {
+      String term = termsEnum.term().utf8ToString();
       //System.out.println("Term: " + term);
-      int freq = freqs[i];
+      int freq = (int) termsEnum.totalTermFreq();
       assertTrue(test4.indexOf(term) != -1);
       Integer freqInt = test4Map.get(term);
       assertTrue(freqInt != null);
-      assertTrue(freqInt.intValue() == freq);        
-    }
-    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());
-    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);
-    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();
-    assertTrue("mapper.getTermVectorEntrySet() Size: " + vectorEntrySet.size() + " is not: " + 10, vectorEntrySet.size() == 10);
-    TermVectorEntry last = null;
-    for (final TermVectorEntry tve : vectorEntrySet) {
-      if (tve != null && last != null)
-      {
-        assertTrue("terms are not properly sorted", last.getFrequency() >= tve.getFrequency());
-        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());
-        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields
-        assertTrue("Frequency is not correct:", tve.getFrequency() == 2*expectedFreq.intValue());
-      }
-      last = tve;
-      
+      assertEquals(freqInt.intValue(), freq);
     }
-    
-    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());
-    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);
-    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();
-    assertTrue("map Size: " + map.size() + " is not: " + 2, map.size() == 2);
-    vectorEntrySet = map.get("field");
-    assertTrue("vectorEntrySet is null and it shouldn't be", vectorEntrySet != null);
-    assertTrue("vectorEntrySet Size: " + vectorEntrySet.size() + " is not: " + 10, vectorEntrySet.size() == 10);
-    knownSearcher.close();
     reader.close();
     dir.close();
   } 
@@ -399,9 +377,9 @@ public class TestTermVectors extends LuceneTestCase {
     assertEquals(10, hits.length);
     for (int i = 0; i < hits.length; i++) {
 
-      TermFreqVector [] vector = searcher.reader.getTermFreqVectors(hits[i].doc);
-      assertTrue(vector != null);
-      assertTrue(vector.length == 1);
+      Fields vectors = searcher.reader.getTermVectors(hits[i].doc);
+      assertNotNull(vectors);
+      assertEquals(1, vectors.getUniqueFieldCount());
     }
     reader.close();
   }
@@ -446,41 +424,36 @@ public class TestTermVectors extends LuceneTestCase {
     ScoreDoc[] hits = searcher.search(query, null, 1000).scoreDocs;
     assertEquals(1, hits.length);
 
-    TermFreqVector [] vector = searcher.reader.getTermFreqVectors(hits[0].doc);
-    assertTrue(vector != null);
-    assertTrue(vector.length == 1);
-    TermPositionVector tfv = (TermPositionVector) vector[0];
-    assertTrue(tfv.getField().equals("field"));
-    BytesRef[] terms = tfv.getTerms();
-    assertEquals(1, terms.length);
-    assertEquals(terms[0].utf8ToString(), "one");
-    assertEquals(5, tfv.getTermFrequencies()[0]);
-
-    int[] positions = tfv.getTermPositions(0);
-    assertEquals(5, positions.length);
-    for(int i=0;i<5;i++)
-      assertEquals(i, positions[i]);
-    TermVectorOffsetInfo[] offsets = tfv.getOffsets(0);
-    assertEquals(5, offsets.length);
+    Fields vectors = searcher.reader.getTermVectors(hits[0].doc);
+    assertNotNull(vectors);
+    assertEquals(1, vectors.getUniqueFieldCount());
+    Terms vector = vectors.terms("field");
+    assertNotNull(vector);
+    assertEquals(1, vector.getUniqueTermCount());
+    TermsEnum termsEnum = vector.iterator(null);
+    assertNotNull(termsEnum.next());
+    assertEquals("one", termsEnum.term().utf8ToString());
+    assertEquals(5, termsEnum.totalTermFreq());
+    DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null);
+    assertNotNull(dpEnum);
+    assertTrue(dpEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);
+    assertEquals(5, dpEnum.freq());
     for(int i=0;i<5;i++) {
-      assertEquals(4*i, offsets[i].getStartOffset());
-      assertEquals(4*i+3, offsets[i].getEndOffset());
+      assertEquals(i, dpEnum.nextPosition());
     }
-    reader.close();
-  }
-
-  private static class TestTermVectorMapper extends TermVectorMapper {
-    public String field = null;
-
-    @Override
-    public void setExpectations(String field, int numTerms, boolean storeOffsets, boolean storePositions) {
-      this.field = field;
-    }
-
-    @Override
-    public void map(BytesRef term, int frequency, TermVectorOffsetInfo[] offsets, int[] positions) {
 
+    dpEnum = termsEnum.docsAndPositions(null, dpEnum);
+    assertNotNull(dpEnum);
+    OffsetAttribute offsetAtt = dpEnum.attributes().getAttribute(OffsetAttribute.class);
+    assertNotNull(offsetAtt);
+    assertTrue(dpEnum.nextDoc() != DocsEnum.NO_MORE_DOCS);
+    assertEquals(5, dpEnum.freq());
+    for(int i=0;i<5;i++) {
+      dpEnum.nextPosition();
+      assertEquals(4*i, offsetAtt.startOffset());
+      assertEquals(4*i+3, offsetAtt.endOffset());
     }
+    reader.close();
   }
 
   private IndexWriter createWriter(Directory dir) throws IOException {
@@ -508,8 +481,7 @@ public class TestTermVectors extends LuceneTestCase {
     IndexReader r = IndexReader.open(dir);
     int numDocs = r.numDocs();
     for (int i = 0; i < numDocs; i++) {
-      TermFreqVector tfv = r.getTermFreqVector(i, "c");
-      assertNotNull("term vectors should not have been null for document " + i, tfv);
+      assertNotNull("term vectors should not have been null for document " + i, r.getTermVectors(i).terms("c"));
     }
     r.close();
   }
diff --git a/lucene/src/test/org/apache/lucene/util/fst/TestFSTs.java b/lucene/src/test/org/apache/lucene/util/fst/TestFSTs.java
index 55ec364..47cd025 100644
--- a/lucene/src/test/org/apache/lucene/util/fst/TestFSTs.java
+++ b/lucene/src/test/org/apache/lucene/util/fst/TestFSTs.java
@@ -1050,7 +1050,7 @@ public class TestFSTs extends LuceneTestCase {
     }
     Terms terms = MultiFields.getTerms(r, "body");
     if (terms != null) {
-      final TermsEnum termsEnum = terms.iterator();
+      final TermsEnum termsEnum = terms.iterator(null);
       if (VERBOSE) {
         System.out.println("TEST: got termsEnum=" + termsEnum);
       }
@@ -1501,7 +1501,7 @@ public class TestFSTs extends LuceneTestCase {
       }
 
       // Verify w/ MultiTermsEnum
-      final TermsEnum termsEnum = MultiFields.getTerms(r, "id").iterator();
+      final TermsEnum termsEnum = MultiFields.getTerms(r, "id").iterator(null);
       for(int iter=0;iter<2*NUM_IDS;iter++) {
         final String id;
         final String nextID;
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer.java
index 791e994..92098e9 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer.java
@@ -154,7 +154,7 @@ public final class QueryAutoStopWordAnalyzer extends AnalyzerWrapper {
       Terms terms = MultiFields.getTerms(indexReader, field);
       CharsRef spare = new CharsRef();
       if (terms != null) {
-        TermsEnum te = terms.iterator();
+        TermsEnum te = terms.iterator(null);
         BytesRef text;
         while ((text = te.next()) != null) {
           if (te.docFreq() > maxDocFreq) {
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter.java
index 4a90180..38f4e53 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter.java
@@ -24,15 +24,18 @@ import org.apache.lucene.analysis.core.LowerCaseFilter;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.TermPositionVector;
-import org.apache.lucene.index.TermVectorOffsetInfo;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.AttributeSource;
 import org.apache.lucene.util.English;
@@ -102,12 +105,22 @@ public class TestTeeSinkTokenFilter extends BaseTokenStreamTestCase {
     w.close();
 
     IndexReader r = IndexReader.open(dir, true);
-    TermVectorOffsetInfo[] termOffsets = ((TermPositionVector) r.getTermFreqVector(0, "field")).getOffsets(0);
-    assertEquals(2, termOffsets.length);
-    assertEquals(0, termOffsets[0].getStartOffset());
-    assertEquals(4, termOffsets[0].getEndOffset());
-    assertEquals(8, termOffsets[1].getStartOffset());
-    assertEquals(12, termOffsets[1].getEndOffset());
+    Terms vector = r.getTermVectors(0).terms("field");
+    assertEquals(1, vector.getUniqueTermCount());
+    TermsEnum termsEnum = vector.iterator(null);
+    termsEnum.next();
+    assertEquals(2, termsEnum.totalTermFreq());
+    DocsAndPositionsEnum positions = termsEnum.docsAndPositions(null, null);
+    OffsetAttribute offsetAtt = positions.attributes().getAttribute(OffsetAttribute.class);
+    assertTrue(positions.nextDoc() != DocsEnum.NO_MORE_DOCS);
+    assertEquals(2, positions.freq());
+    positions.nextPosition();
+    assertEquals(0, offsetAtt.startOffset());
+    assertEquals(4, offsetAtt.endOffset());
+    positions.nextPosition();
+    assertEquals(8, offsetAtt.startOffset());
+    assertEquals(12, offsetAtt.endOffset());
+    assertEquals(DocsEnum.NO_MORE_DOCS, positions.nextDoc());
     r.close();
     dir.close();
   }
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/QualityQueriesFinder.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/QualityQueriesFinder.java
index e9206ed..fc146a1 100755
--- a/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/QualityQueriesFinder.java
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/QualityQueriesFinder.java
@@ -91,7 +91,7 @@ public class QualityQueriesFinder {
       int threshold = ir.maxDoc() / 10; // ignore words too common.
       Terms terms = MultiFields.getTerms(ir, field);
       if (terms != null) {
-        TermsEnum termsEnum = terms.iterator();
+        TermsEnum termsEnum = terms.iterator(null);
         while (termsEnum.next() != null) {
           int df = termsEnum.docFreq();
           if (df<threshold) {
diff --git a/modules/benchmark/src/test/org/apache/lucene/benchmark/BenchmarkTestCase.java b/modules/benchmark/src/test/org/apache/lucene/benchmark/BenchmarkTestCase.java
index 4fbac12..2af8f93 100644
--- a/modules/benchmark/src/test/org/apache/lucene/benchmark/BenchmarkTestCase.java
+++ b/modules/benchmark/src/test/org/apache/lucene/benchmark/BenchmarkTestCase.java
@@ -26,12 +26,28 @@ import java.io.StringReader;
 
 import org.apache.lucene.benchmark.byTask.Benchmark;
 import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
 
 /** Base class for all Benchmark unit tests. */
 public abstract class BenchmarkTestCase extends LuceneTestCase {
+  private static File WORKDIR;
+  
+  @BeforeClass
+  public static void beforeClassBenchmarkTestCase() throws Exception {
+    WORKDIR = _TestUtil.getTempDir("benchmark");
+    WORKDIR.delete();
+    WORKDIR.mkdirs();
+  }
+  
+  @AfterClass
+  public static void afterClassBenchmarkTestCase() throws Exception {
+    WORKDIR = null;
+  }
   
   public File getWorkDir() {
-    return TEMP_DIR;
+    return WORKDIR;
   }
   
   /** Copy a resource into the workdir */
diff --git a/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic.java b/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic.java
index b2c2f58..ae73bcf 100755
--- a/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic.java
+++ b/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic.java
@@ -38,9 +38,10 @@ import org.apache.lucene.benchmark.byTask.tasks.CountingHighlighterTestTask;
 import org.apache.lucene.benchmark.byTask.tasks.CountingSearchTestTask;
 import org.apache.lucene.benchmark.byTask.tasks.WriteLineDocTask;
 import org.apache.lucene.collation.CollationKeyAnalyzer;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.Fields;
 import org.apache.lucene.index.FieldsEnum;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
@@ -50,7 +51,7 @@ import org.apache.lucene.index.LogMergePolicy;
 import org.apache.lucene.index.MultiFields;
 import org.apache.lucene.index.SegmentInfos;
 import org.apache.lucene.index.SerialMergeScheduler;
-import org.apache.lucene.index.TermFreqVector;
+import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.search.FieldCache.DocTermsIndex;
 import org.apache.lucene.search.FieldCache;
@@ -485,10 +486,14 @@ public class TestPerfTasksLogic extends BenchmarkTestCase {
       if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {
         continue;
       }
-      TermsEnum terms = fields.terms();
+      Terms terms = fields.terms();
+      if (terms == null) {
+        continue;
+      }
+      TermsEnum termsEnum = terms.iterator(null);
       DocsEnum docs = null;
-      while(terms.next() != null) {
-        docs = terms.docs(MultiFields.getLiveDocs(reader), docs);
+      while(termsEnum.next() != null) {
+        docs = termsEnum.docs(MultiFields.getLiveDocs(reader), docs);
         while(docs.nextDoc() != docs.NO_MORE_DOCS) {
           totalTokenCount2 += docs.freq();
         }
@@ -775,9 +780,9 @@ public class TestPerfTasksLogic extends BenchmarkTestCase {
     writer.close();
     Directory dir = benchmark.getRunData().getDirectory();
     IndexReader reader = IndexReader.open(dir, true);
-    TermFreqVector [] tfv = reader.getTermFreqVectors(0);
+    Fields tfv = reader.getTermVectors(0);
     assertNotNull(tfv);
-    assertTrue(tfv.length > 0);
+    assertTrue(tfv.getUniqueFieldCount() > 0);
     reader.close();
   }
 
diff --git a/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java b/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java
index c3cc4c5..3b06ac8 100644
--- a/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java
+++ b/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java
@@ -755,7 +755,7 @@ public class DirectoryTaxonomyWriter implements TaxonomyWriter {
     // executed we're safe, because we only iterate as long as there are next()
     // terms.
     if (terms != null) {
-      TermsEnum termsEnum = terms.iterator();
+      TermsEnum termsEnum = terms.iterator(null);
       Bits liveDocs = MultiFields.getLiveDocs(reader);
       DocsEnum docsEnum = null;
       while (termsEnum.next() != null) {
@@ -831,7 +831,7 @@ public class DirectoryTaxonomyWriter implements TaxonomyWriter {
     // TODO (Facet): can this then go segment-by-segment and avoid MultiDocsEnum etc?
     Terms terms = MultiFields.getTerms(mainreader, Consts.FULL);
     assert terms != null; // TODO (Facet): explicit check / throw exception?
-    TermsEnum mainte = terms.iterator();
+    TermsEnum mainte = terms.iterator(null);
     DocsEnum mainde = null;
 
     IndexReader[] otherreaders = new IndexReader[taxonomies.length];
@@ -841,7 +841,7 @@ public class DirectoryTaxonomyWriter implements TaxonomyWriter {
       otherreaders[i] = IndexReader.open(taxonomies[i]);
       terms = MultiFields.getTerms(otherreaders[i], Consts.FULL);
       assert terms != null; // TODO (Facet): explicit check / throw exception?
-      othertes[i] = terms.iterator();
+      othertes[i] = terms.iterator(null);
       // Also tell the ordinal maps their expected sizes:
       ordinalMaps[i].setSize(otherreaders[i].numDocs());
     }
diff --git a/modules/facet/src/test/org/apache/lucene/facet/FacetTestBase.java b/modules/facet/src/test/org/apache/lucene/facet/FacetTestBase.java
index 92b8b94..7221cf4 100644
--- a/modules/facet/src/test/org/apache/lucene/facet/FacetTestBase.java
+++ b/modules/facet/src/test/org/apache/lucene/facet/FacetTestBase.java
@@ -282,7 +282,7 @@ public abstract class FacetTestBase extends LuceneTestCase {
         continue;
       }
       Bits liveDocs = MultiFields.getLiveDocs(indexReader);
-      TermsEnum te = terms.iterator();
+      TermsEnum te = terms.iterator(null);
       DocsEnum de = null;
       while (te.next() != null) {
         de = te.docs(liveDocs, de);
diff --git a/modules/queries/src/java/org/apache/lucene/queries/TermsFilter.java b/modules/queries/src/java/org/apache/lucene/queries/TermsFilter.java
index bfd7a96..739db6b 100644
--- a/modules/queries/src/java/org/apache/lucene/queries/TermsFilter.java
+++ b/modules/queries/src/java/org/apache/lucene/queries/TermsFilter.java
@@ -74,7 +74,7 @@ public class TermsFilter extends Filter {
         if (termsC == null) {
           return result;
         }
-        termsEnum = termsC.iterator();
+        termsEnum = termsC.iterator(null);
         lastField = term.field();
       }
 
diff --git a/modules/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis.java b/modules/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis.java
index 563cc8a..ee9582d 100644
--- a/modules/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis.java
+++ b/modules/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis.java
@@ -15,14 +15,19 @@
  */
 package org.apache.lucene.queries.mlt;
 
+import java.io.*;
+import java.util.*;
+
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.document.Document;
+import org.apache.lucene.index.Fields;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexableField;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.index.TermFreqVector;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.search.*;
 import org.apache.lucene.search.similarities.DefaultSimilarity;
 import org.apache.lucene.search.similarities.TFIDFSimilarity;
@@ -30,9 +35,6 @@ import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CharsRef;
 import org.apache.lucene.util.PriorityQueue;
 
-import java.io.*;
-import java.util.*;
-
 
 /**
  * Generate "more like this" similarity queries.
@@ -701,7 +703,13 @@ public final class MoreLikeThis {
   public PriorityQueue<Object[]> retrieveTerms(int docNum) throws IOException {
     Map<String, Int> termFreqMap = new HashMap<String, Int>();
     for (String fieldName : fieldNames) {
-      TermFreqVector vector = ir.getTermFreqVector(docNum, fieldName);
+      final Fields vectors = ir.getTermVectors(docNum);
+      final Terms vector;
+      if (vectors != null) {
+        vector = vectors.terms(fieldName);
+      } else {
+        vector = null;
+      }
 
       // field does not store term vector info
       if (vector == null) {
@@ -716,7 +724,6 @@ public final class MoreLikeThis {
       } else {
         addTermFrequencies(termFreqMap, vector);
       }
-
     }
 
     return createQueue(termFreqMap);
@@ -728,24 +735,25 @@ public final class MoreLikeThis {
    * @param termFreqMap a Map of terms and their frequencies
    * @param vector List of terms and their frequencies for a doc/field
    */
-  private void addTermFrequencies(Map<String, Int> termFreqMap, TermFreqVector vector) {
-    BytesRef[] terms = vector.getTerms();
-    int freqs[] = vector.getTermFrequencies();
+  private void addTermFrequencies(Map<String, Int> termFreqMap, Terms vector) throws IOException {
+    final TermsEnum termsEnum = vector.iterator(null);
     final CharsRef spare = new CharsRef();
-    for (int j = 0; j < terms.length; j++) {
-      final String term = terms[j].utf8ToChars(spare).toString();
-
+    BytesRef text;
+    while((text = termsEnum.next()) != null) {
+      final String term = text.utf8ToChars(spare).toString();
       if (isNoiseWord(term)) {
         continue;
       }
+      final int freq = (int) termsEnum.totalTermFreq();
+
       // increment frequency
       Int cnt = termFreqMap.get(term);
       if (cnt == null) {
         cnt = new Int();
         termFreqMap.put(term, cnt);
-        cnt.x = freqs[j];
+        cnt.x = freq;
       } else {
-        cnt.x += freqs[j];
+        cnt.x += freq;
       }
     }
   }
diff --git a/modules/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SrndPrefixQuery.java b/modules/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SrndPrefixQuery.java
index 6af5267..540dbb5 100644
--- a/modules/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SrndPrefixQuery.java
+++ b/modules/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SrndPrefixQuery.java
@@ -60,7 +60,7 @@ public class SrndPrefixQuery extends SimpleTerm {
     /* inspired by PrefixQuery.rewrite(): */
     Terms terms = MultiFields.getTerms(reader, fieldName);
     if (terms != null) {
-      TermsEnum termsEnum = terms.iterator();
+      TermsEnum termsEnum = terms.iterator(null);
 
       boolean skip = false;
       TermsEnum.SeekStatus status = termsEnum.seekCeil(new BytesRef(getPrefix()));
diff --git a/modules/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SrndTermQuery.java b/modules/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SrndTermQuery.java
index b8bb356..a9ff4b7 100644
--- a/modules/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SrndTermQuery.java
+++ b/modules/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SrndTermQuery.java
@@ -51,7 +51,7 @@ public class SrndTermQuery extends SimpleTerm {
     /* check term presence in index here for symmetry with other SimpleTerm's */
     Terms terms = MultiFields.getTerms(reader, fieldName);
     if (terms != null) {
-      TermsEnum termsEnum = terms.iterator();
+      TermsEnum termsEnum = terms.iterator(null);
 
       TermsEnum.SeekStatus status = termsEnum.seekCeil(new BytesRef(getTermText()));
       if (status == TermsEnum.SeekStatus.FOUND) {
diff --git a/modules/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SrndTruncQuery.java b/modules/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SrndTruncQuery.java
index 9ba02a7..4f341f3 100644
--- a/modules/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SrndTruncQuery.java
+++ b/modules/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SrndTruncQuery.java
@@ -93,7 +93,7 @@ public class SrndTruncQuery extends SimpleTerm {
     if (terms != null) {
       Matcher matcher = pattern.matcher("");
       try {
-        TermsEnum termsEnum = terms.iterator();
+        TermsEnum termsEnum = terms.iterator(null);
 
         TermsEnum.SeekStatus status = termsEnum.seekCeil(prefixRef);
         BytesRef text;
diff --git a/modules/suggest/src/java/org/apache/lucene/search/spell/HighFrequencyDictionary.java b/modules/suggest/src/java/org/apache/lucene/search/spell/HighFrequencyDictionary.java
index fda033e..f339924 100644
--- a/modules/suggest/src/java/org/apache/lucene/search/spell/HighFrequencyDictionary.java
+++ b/modules/suggest/src/java/org/apache/lucene/search/spell/HighFrequencyDictionary.java
@@ -24,7 +24,6 @@ import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.MultiFields;
-import org.apache.lucene.search.spell.Dictionary;
 import org.apache.lucene.util.CharsRef;
 import org.apache.lucene.util.BytesRef;
 
@@ -64,7 +63,7 @@ public class HighFrequencyDictionary implements Dictionary {
       try {
         Terms terms = MultiFields.getTerms(reader, field);
         if (terms != null) {
-          termsEnum = terms.iterator();
+          termsEnum = terms.iterator(null);
         }
         minNumDocs = (int)(thresh * (float)reader.numDocs());
       } catch (IOException e) {
diff --git a/modules/suggest/src/java/org/apache/lucene/search/spell/LuceneDictionary.java b/modules/suggest/src/java/org/apache/lucene/search/spell/LuceneDictionary.java
index b114c25..3f0d56c 100755
--- a/modules/suggest/src/java/org/apache/lucene/search/spell/LuceneDictionary.java
+++ b/modules/suggest/src/java/org/apache/lucene/search/spell/LuceneDictionary.java
@@ -62,7 +62,7 @@ public class LuceneDictionary implements Dictionary {
       try {
         final Terms terms = MultiFields.getTerms(reader, field);
         if (terms != null) {
-          termsEnum = terms.iterator();
+          termsEnum = terms.iterator(null);
           pendingTerm = termsEnum.next();
         }
       } catch (IOException e) {
diff --git a/modules/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java b/modules/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java
index a53c6f9..1ba5714 100755
--- a/modules/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java
+++ b/modules/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java
@@ -500,7 +500,7 @@ public class SpellChecker implements java.io.Closeable {
           protected void add(int base, IndexReader r) throws IOException {
             Terms terms = r.terms(F_WORD);
             if (terms != null)
-              termsEnums.add(terms.iterator());
+              termsEnums.add(terms.iterator(null));
           }
         }.run();
       }
diff --git a/solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler.java b/solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler.java
index 3f9ca09..5666089 100644
--- a/solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler.java
+++ b/solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler.java
@@ -29,27 +29,25 @@ import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
+import org.apache.lucene.index.Fields;
+import org.apache.lucene.index.FieldsEnum;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexableField;
+import org.apache.lucene.index.MultiFields;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.FieldsEnum;
+import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.MultiFields;
-import org.apache.lucene.index.TermFreqVector;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.TermRangeQuery;
 import org.apache.lucene.search.TopDocs;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CharsRef;
 import org.apache.lucene.util.PriorityQueue;
-import org.apache.lucene.util.BytesRef;
 import org.apache.solr.analysis.CharFilterFactory;
 import org.apache.solr.analysis.TokenFilterFactory;
 import org.apache.solr.analysis.TokenizerChain;
@@ -58,9 +56,9 @@ import org.apache.solr.common.SolrException;
 import org.apache.solr.common.luke.FieldFlag;
 import org.apache.solr.common.params.CommonParams;
 import org.apache.solr.common.params.SolrParams;
+import org.apache.solr.common.util.Base64;
 import org.apache.solr.common.util.NamedList;
 import org.apache.solr.common.util.SimpleOrderedMap;
-import org.apache.solr.common.util.Base64;
 import org.apache.solr.handler.RequestHandlerBase;
 import org.apache.solr.request.SolrQueryRequest;
 import org.apache.solr.response.SolrQueryResponse;
@@ -68,6 +66,8 @@ import org.apache.solr.schema.FieldType;
 import org.apache.solr.schema.IndexSchema;
 import org.apache.solr.schema.SchemaField;
 import org.apache.solr.search.SolrIndexSearcher;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 /**
  * This handler exposes the internal lucene index.  It is inspired by and 
@@ -266,11 +266,14 @@ public class LukeRequestHandler extends RequestHandlerBase
       // If we have a term vector, return that
       if( field.fieldType().storeTermVectors() ) {
         try {
-          TermFreqVector v = reader.getTermFreqVector( docId, field.name() );
+          Terms v = reader.getTermVector( docId, field.name() );
           if( v != null ) {
             SimpleOrderedMap<Integer> tfv = new SimpleOrderedMap<Integer>();
-            for( int i=0; i<v.size(); i++ ) {
-              tfv.add( v.getTerms()[i].utf8ToChars(spare).toString(), v.getTermFrequencies()[i] );
+            final TermsEnum termsEnum = v.iterator(null);
+            BytesRef text;
+            while((text = termsEnum.next()) != null) {
+              final int freq = (int) termsEnum.totalTermFreq();
+              tfv.add( text.utf8ToChars(spare).toString(), freq );
             }
             f.add( "termVector", tfv );
           }
@@ -482,9 +485,12 @@ public class LukeRequestHandler extends RequestHandlerBase
       if (fields != null) {
         FieldsEnum fieldsEnum = fields.iterator();
         while(fieldsEnum.next() != null) {
-          TermsEnum termsEnum = fieldsEnum.terms();
-          while(termsEnum.next() != null) {
-            numTerms++;
+          Terms terms = fieldsEnum.terms();
+          if (terms != null) {
+            TermsEnum termsEnum = terms.iterator(null);
+            while(termsEnum.next() != null) {
+              numTerms++;
+            }
           }
         }
       }
@@ -636,7 +642,11 @@ public class LukeRequestHandler extends RequestHandlerBase
       String field;
       while((field = fieldsEnum.next()) != null) {
 
-        TermsEnum termsEnum = fieldsEnum.terms();
+        Terms terms = fieldsEnum.terms();
+        if (terms == null) {
+          continue;
+        }
+        TermsEnum termsEnum = terms.iterator(null);
         BytesRef text;
         while((text = termsEnum.next()) != null) {
           String t = text.utf8ToChars(spare).toString();
diff --git a/solr/core/src/java/org/apache/solr/handler/component/TermVectorComponent.java b/solr/core/src/java/org/apache/solr/handler/component/TermVectorComponent.java
index 875e731..501c6e7 100644
--- a/solr/core/src/java/org/apache/solr/handler/component/TermVectorComponent.java
+++ b/solr/core/src/java/org/apache/solr/handler/component/TermVectorComponent.java
@@ -8,16 +8,17 @@ import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.index.DocsAndPositionsEnum;
 import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.Fields;
+import org.apache.lucene.index.FieldsEnum;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.MultiFields;
-import org.apache.lucene.index.StoredFieldVisitor;
 import org.apache.lucene.index.StoredFieldVisitor.Status;
-import org.apache.lucene.index.TermVectorMapper;
-import org.apache.lucene.index.TermVectorOffsetInfo;
+import org.apache.lucene.index.StoredFieldVisitor;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.util.BytesRef;
 import org.apache.solr.common.SolrException;
 import org.apache.solr.common.params.CommonParams;
@@ -225,12 +226,11 @@ public class TermVectorComponent extends SearchComponent implements SolrCoreAwar
       }
     };
 
-    TVMapper mapper = new TVMapper(reader);
-    mapper.fieldOptions = allFields; //this will only stay set if fieldOptions.isEmpty() (in other words, only if the user didn't set any fields)
+    TermsEnum termsEnum = null;
+
     while (iter.hasNext()) {
       Integer docId = iter.next();
       NamedList<Object> docNL = new NamedList<Object>();
-      mapper.docNL = docNL;
       termVectors.add("doc-" + docId, docNL);
 
       if (keyField != null) {
@@ -245,12 +245,91 @@ public class TermVectorComponent extends SearchComponent implements SolrCoreAwar
       }
       if (!fieldOptions.isEmpty()) {
         for (Map.Entry<String, FieldOptions> entry : fieldOptions.entrySet()) {
-          mapper.fieldOptions = entry.getValue();
-          reader.getTermFreqVector(docId, entry.getKey(), mapper);
+          final String field = entry.getKey();
+          final Terms vector = reader.getTermVector(docId, field);
+          if (vector != null) {
+            termsEnum = vector.iterator(termsEnum);
+            mapOneVector(docNL, entry.getValue(), reader, docId, vector.iterator(termsEnum), field);
+          }
         }
       } else {
-        //deal with all fields by using the allFieldMapper
-        reader.getTermFreqVector(docId, mapper);
+        // extract all fields
+        final Fields vectors = reader.getTermVectors(docId);
+        final FieldsEnum fieldsEnum = vectors.iterator();
+        String field;
+        while((field = fieldsEnum.next()) != null) {
+          Terms terms = fieldsEnum.terms();
+          if (terms != null) {
+            termsEnum = terms.iterator(termsEnum);
+            mapOneVector(docNL, allFields, reader, docId, termsEnum, field);
+          }
+        }
+      }
+    }
+  }
+
+  private void mapOneVector(NamedList<Object> docNL, FieldOptions fieldOptions, IndexReader reader, int docID, TermsEnum termsEnum, String field) throws IOException {
+    NamedList<Object> fieldNL = new NamedList<Object>();
+    docNL.add(field, fieldNL);
+
+    BytesRef text;
+    DocsAndPositionsEnum dpEnum = null;
+    while((text = termsEnum.next()) != null) {
+      String term = text.utf8ToString();
+      NamedList<Object> termInfo = new NamedList<Object>();
+      fieldNL.add(term, termInfo);
+      final int freq = (int) termsEnum.totalTermFreq();
+      if (fieldOptions.termFreq == true) {
+        termInfo.add("tf", freq);
+      }
+
+      dpEnum = termsEnum.docsAndPositions(null, dpEnum);
+
+      boolean usePositions = false;
+      boolean useOffsets = false;
+      OffsetAttribute offsetAtt = null;
+      if (dpEnum != null) {
+        dpEnum.nextDoc();
+        usePositions = fieldOptions.positions;
+        if (fieldOptions.offsets && dpEnum.attributes().hasAttribute(OffsetAttribute.class)) {
+          useOffsets = true;
+          offsetAtt = dpEnum.attributes().getAttribute(OffsetAttribute.class);
+        }
+      }
+
+      NamedList<Number> theOffsets = null;
+      if (useOffsets) {
+        theOffsets = new NamedList<Number>();
+        termInfo.add("offsets", theOffsets);
+      }
+
+      NamedList<Integer> positionsNL = null;
+
+      if (usePositions || theOffsets != null) {
+        for (int i = 0; i < freq; i++) {
+          final int pos = dpEnum.nextPosition();
+          if (usePositions && pos >= 0) {
+            if (positionsNL == null) {
+              positionsNL = new NamedList<Integer>();
+              termInfo.add("positions", positionsNL);
+            }
+            positionsNL.add("position", pos);
+          }
+
+          if (theOffsets != null) {
+            theOffsets.add("start", offsetAtt.startOffset());
+            theOffsets.add("end", offsetAtt.endOffset());
+          }
+        }
+      }
+
+      if (fieldOptions.docFreq) {
+        termInfo.add("df", getDocFreq(reader, field, text));
+      }
+
+      if (fieldOptions.tfIdf) {
+        double tfIdfVal = ((double) freq) / getDocFreq(reader, field, text);
+        termInfo.add("tf-idf", tfIdfVal);
       }
     }
   }
@@ -310,90 +389,20 @@ public class TermVectorComponent extends SearchComponent implements SolrCoreAwar
     return result;
   }
 
-  private static class TVMapper extends TermVectorMapper {
-    private IndexReader reader;
-    private NamedList<Object> docNL;
-
-    //needs to be set for each new field
-    FieldOptions fieldOptions;
-
-    //internal vars not passed in by construction
-    private boolean useOffsets, usePositions;
-    //private Map<String, Integer> idfCache;
-    private NamedList<Object> fieldNL;
-    private String field;
-
-
-    public TVMapper(IndexReader reader) {
-      this.reader = reader;
-    }
-
-    @Override
-    public void map(BytesRef term, int frequency, TermVectorOffsetInfo[] offsets, int[] positions) {
-      NamedList<Object> termInfo = new NamedList<Object>();
-      fieldNL.add(term.utf8ToString(), termInfo);
-      if (fieldOptions.termFreq == true) {
-        termInfo.add("tf", frequency);
-      }
-      if (useOffsets) {
-        NamedList<Number> theOffsets = new NamedList<Number>();
-        termInfo.add("offsets", theOffsets);
-        for (int i = 0; i < offsets.length; i++) {
-          TermVectorOffsetInfo offset = offsets[i];
-          theOffsets.add("start", offset.getStartOffset());
-          theOffsets.add("end", offset.getEndOffset());
-        }
-      }
-      if (usePositions) {
-        NamedList<Integer> positionsNL = new NamedList<Integer>();
-        for (int i = 0; i < positions.length; i++) {
-          positionsNL.add("position", positions[i]);
-        }
-        termInfo.add("positions", positionsNL);
-      }
-      if (fieldOptions.docFreq) {
-        termInfo.add("df", getDocFreq(term));
-      }
-      if (fieldOptions.tfIdf) {
-        double tfIdfVal = ((double) frequency) / getDocFreq(term);
-        termInfo.add("tf-idf", tfIdfVal);
-      }
-    }
-
-    private int getDocFreq(BytesRef term) {
-      int result = 1;
-      try {
-        Terms terms = MultiFields.getTerms(reader, field);
-        if (terms != null) {
-          TermsEnum termsEnum = terms.iterator();
-          if (termsEnum.seekExact(term, true)) {
-            result = termsEnum.docFreq();
-          }
+  private static int getDocFreq(IndexReader reader, String field, BytesRef term) {
+    int result = 1;
+    try {
+      Terms terms = MultiFields.getTerms(reader, field);
+      if (terms != null) {
+        TermsEnum termsEnum = terms.iterator(null);
+        if (termsEnum.seekExact(term, true)) {
+          result = termsEnum.docFreq();
         }
-      } catch (IOException e) {
-        throw new RuntimeException(e);
       }
-      return result;
-    }
-
-    @Override
-    public void setExpectations(String field, int numTerms, boolean storeOffsets, boolean storePositions) {
-      this.field = field;
-      useOffsets = storeOffsets && fieldOptions.offsets;
-      usePositions = storePositions && fieldOptions.positions;
-      fieldNL = new NamedList<Object>();
-      docNL.add(field, fieldNL);
-    }
-
-    @Override
-    public boolean isIgnoringPositions() {
-      return !fieldOptions.positions;  // if we are not interested in positions, then return true telling Lucene to skip loading them
-    }
-
-    @Override
-    public boolean isIgnoringOffsets() {
-      return !fieldOptions.offsets;  //  if we are not interested in offsets, then return true telling Lucene to skip loading them
+    } catch (IOException e) {
+      throw new RuntimeException(e);
     }
+    return result;
   }
 
   @Override
diff --git a/solr/core/src/java/org/apache/solr/handler/component/TermsComponent.java b/solr/core/src/java/org/apache/solr/handler/component/TermsComponent.java
index 8379fc8..6b043dd 100644
--- a/solr/core/src/java/org/apache/solr/handler/component/TermsComponent.java
+++ b/solr/core/src/java/org/apache/solr/handler/component/TermsComponent.java
@@ -158,7 +158,7 @@ public class TermsComponent extends SearchComponent {
       }
 
 
-     TermsEnum termsEnum = terms.iterator();
+     TermsEnum termsEnum = terms.iterator(null);
      BytesRef term = null;
 
       if (lowerBytes != null) {
diff --git a/solr/core/src/java/org/apache/solr/request/SimpleFacets.java b/solr/core/src/java/org/apache/solr/request/SimpleFacets.java
index a63c9a2..0e771cc 100644
--- a/solr/core/src/java/org/apache/solr/request/SimpleFacets.java
+++ b/solr/core/src/java/org/apache/solr/request/SimpleFacets.java
@@ -633,7 +633,7 @@ public class SimpleFacets {
     SolrIndexSearcher.DocsEnumState deState = null;
     BytesRef term = null;
     if (terms != null) {
-      termsEnum = terms.iterator();
+      termsEnum = terms.iterator(null);
 
       // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for
       // facet.offset when sorting by index order.
diff --git a/solr/core/src/java/org/apache/solr/search/JoinQParserPlugin.java b/solr/core/src/java/org/apache/solr/search/JoinQParserPlugin.java
index a179c33..fb34f6c 100644
--- a/solr/core/src/java/org/apache/solr/search/JoinQParserPlugin.java
+++ b/solr/core/src/java/org/apache/solr/search/JoinQParserPlugin.java
@@ -270,8 +270,8 @@ class JoinQuery extends Query {
       BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);
 
       BytesRef term = null;
-      TermsEnum  termsEnum = terms.iterator();
-      TermsEnum  toTermsEnum = toTerms.iterator();
+      TermsEnum  termsEnum = terms.iterator(null);
+      TermsEnum  toTermsEnum = toTerms.iterator(null);
       SolrIndexSearcher.DocsEnumState fromDeState = null;
       SolrIndexSearcher.DocsEnumState toDeState = null;
 
diff --git a/solr/core/src/java/org/apache/solr/search/function/FileFloatSource.java b/solr/core/src/java/org/apache/solr/search/function/FileFloatSource.java
index 27b834b..c2ce7d0 100755
--- a/solr/core/src/java/org/apache/solr/search/function/FileFloatSource.java
+++ b/solr/core/src/java/org/apache/solr/search/function/FileFloatSource.java
@@ -241,7 +241,7 @@ public class FileFloatSource extends ValueSource {
     BytesRef internalKey = new BytesRef();
 
     try {
-      TermsEnum termsEnum = MultiFields.getTerms(reader, idName).iterator();
+      TermsEnum termsEnum = MultiFields.getTerms(reader, idName).iterator(null);
       DocsEnum docsEnum = null;
 
       // removing deleted docs shouldn't matter

