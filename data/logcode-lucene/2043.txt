GitDiffStart: 29cb3373770f18d7bbac6b62d510a9eb081e6eb8 | Thu Jan 21 15:58:08 2016 +0000
diff --git a/lucene/CHANGES.txt b/lucene/CHANGES.txt
index 468cbd6..42d1616 100644
--- a/lucene/CHANGES.txt
+++ b/lucene/CHANGES.txt
@@ -39,25 +39,28 @@ New Features
 
 * LUCENE-6825: Add low-level support for block-KD trees (Mike McCandless)
 
-* LUCENE-6852: Add support for dimensionally indexed values to index,
-  document and codec APIs, including a simple text implementation.
-  (Mike McCandless)
+* LUCENE-6852, LUCENE-6975: Add support for points (dimensionally
+  indexed values) to index, document and codec APIs, including a
+  simple text implementation.  (Mike McCandless)
 
-* LUCENE-6861: Create Lucene60Codec, supporting dimensional values.
+* LUCENE-6861: Create Lucene60Codec, supporting points.
   (Mike McCandless)
 
 * LUCENE-6879: Allow to define custom CharTokenizer instances without
   subclassing using Java 8 lambdas or method references. (Uwe Schindler)
 
-* LUCENE-6881: Cutover all BKD implementations to dimensional values
+* LUCENE-6881: Cutover all BKD implementations to points
   (Mike McCandless)
 
 * LUCENE-6837: Add N-best output support to JapaneseTokenizer.
   (Hiroharu Konno via Christian Moen)
 
-* LUCENE-6962: Add per-dimension min/max to dimensional values
+* LUCENE-6962: Add per-dimension min/max to points
   (Mike McCandless)
 
+* LUCENE-6975: Add ExactPointQuery, to match a single N-dimensional
+  point (Robert Muir, Mike McCandless)
+
 API Changes
 
 * LUCENE-6067: Accountable.getChildResources has a default
@@ -82,18 +85,18 @@ API Changes
   McCandless)
 
 * LUCENE-6917: Deprecate and rename NumericXXX classes to
-  LegacyNumericXXX in favor of dimensional values (Mike McCandless)
+  LegacyNumericXXX in favor of points (Mike McCandless)
 
 * LUCENE-6947: SortField.missingValue is now protected. You can read its value
   using the new SortField.getMissingValue getter. (Adrien Grand)
 
 Optimizations
 
-* LUCENE-6891: Use prefix coding when writing dimensional values in 
+* LUCENE-6891: Use prefix coding when writing points in 
   each leaf block in the default codec, to reduce the index
   size (Mike McCandless)
 
-* LUCENE-6901: Optimize dimensional values indexing: use faster
+* LUCENE-6901: Optimize points indexing: use faster
   IntroSorter instead of InPlaceMergeSorter, and specialize 1D
   merging to merge sort the already sorted segments instead of
   re-indexing (Mike McCandless)
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene50/Lucene50Codec.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene50/Lucene50Codec.java
index faf46d0..9579674 100644
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene50/Lucene50Codec.java
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene50/Lucene50Codec.java
@@ -21,7 +21,7 @@ import java.util.Objects;
 
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.CompoundFormat;
-import org.apache.lucene.codecs.DimensionalFormat;
+import org.apache.lucene.codecs.PointFormat;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.FieldInfosFormat;
 import org.apache.lucene.codecs.FilterCodec;
@@ -154,8 +154,8 @@ public class Lucene50Codec extends Codec {
   }
 
   @Override
-  public final DimensionalFormat dimensionalFormat() {
-    return DimensionalFormat.EMPTY;
+  public final PointFormat pointFormat() {
+    return PointFormat.EMPTY;
   }
 
   private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene50");
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene53/Lucene53Codec.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene53/Lucene53Codec.java
index dfd0f22..1ec1406 100644
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene53/Lucene53Codec.java
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene53/Lucene53Codec.java
@@ -21,7 +21,7 @@ import java.util.Objects;
 
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.CompoundFormat;
-import org.apache.lucene.codecs.DimensionalFormat;
+import org.apache.lucene.codecs.PointFormat;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.FieldInfosFormat;
 import org.apache.lucene.codecs.FilterCodec;
@@ -160,8 +160,8 @@ public class Lucene53Codec extends Codec {
   }
 
   @Override
-  public final DimensionalFormat dimensionalFormat() {
-    return DimensionalFormat.EMPTY;
+  public final PointFormat pointFormat() {
+    return PointFormat.EMPTY;
   }
 
   private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene50");
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene54/Lucene54Codec.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene54/Lucene54Codec.java
index bb129ac..4ca2521 100644
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene54/Lucene54Codec.java
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene54/Lucene54Codec.java
@@ -21,7 +21,7 @@ import java.util.Objects;
 
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.CompoundFormat;
-import org.apache.lucene.codecs.DimensionalFormat;
+import org.apache.lucene.codecs.PointFormat;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.FieldInfosFormat;
 import org.apache.lucene.codecs.FilterCodec;
@@ -160,8 +160,8 @@ public class Lucene54Codec extends Codec {
   }
 
   @Override
-  public final DimensionalFormat dimensionalFormat() {
-    return DimensionalFormat.EMPTY;
+  public final PointFormat pointFormat() {
+    return PointFormat.EMPTY;
   }
 
   private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene50");
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextBKDReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextBKDReader.java
index 6e073d6..f07c4a3 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextBKDReader.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextBKDReader.java
@@ -20,16 +20,16 @@ package org.apache.lucene.codecs.simpletext;
 import java.io.IOException;
 import java.nio.charset.StandardCharsets;
 
-import org.apache.lucene.index.DimensionalValues.IntersectVisitor;
+import org.apache.lucene.index.PointValues.IntersectVisitor;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.BytesRefBuilder;
 import org.apache.lucene.util.StringHelper;
 import org.apache.lucene.util.bkd.BKDReader;
 
-import static org.apache.lucene.codecs.simpletext.SimpleTextDimensionalWriter.BLOCK_COUNT;
-import static org.apache.lucene.codecs.simpletext.SimpleTextDimensionalWriter.BLOCK_DOC_ID;
-import static org.apache.lucene.codecs.simpletext.SimpleTextDimensionalWriter.BLOCK_VALUE;
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.BLOCK_COUNT;
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.BLOCK_DOC_ID;
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.BLOCK_VALUE;
 
 class SimpleTextBKDReader extends BKDReader {
 
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java
index f8285c1..89cd859 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java
@@ -19,7 +19,7 @@ package org.apache.lucene.codecs.simpletext;
 
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.CompoundFormat;
-import org.apache.lucene.codecs.DimensionalFormat;
+import org.apache.lucene.codecs.PointFormat;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.FieldInfosFormat;
 import org.apache.lucene.codecs.LiveDocsFormat;
@@ -45,7 +45,7 @@ public final class SimpleTextCodec extends Codec {
   private final LiveDocsFormat liveDocs = new SimpleTextLiveDocsFormat();
   private final DocValuesFormat dvFormat = new SimpleTextDocValuesFormat();
   private final CompoundFormat compoundFormat = new SimpleTextCompoundFormat();
-  private final DimensionalFormat dimensionalFormat = new SimpleTextDimensionalFormat();
+  private final PointFormat pointFormat = new SimpleTextPointFormat();
   
   public SimpleTextCodec() {
     super("SimpleText");
@@ -97,7 +97,7 @@ public final class SimpleTextCodec extends Codec {
   }
 
   @Override
-  public DimensionalFormat dimensionalFormat() {
-    return dimensionalFormat;
+  public PointFormat pointFormat() {
+    return pointFormat;
   }
 }
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDimensionalFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDimensionalFormat.java
deleted file mode 100644
index 56e7579..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDimensionalFormat.java
+++ /dev/null
@@ -1,53 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DimensionalFormat;
-import org.apache.lucene.codecs.DimensionalReader;
-import org.apache.lucene.codecs.DimensionalWriter;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-
-/** For debugging, curiosity, transparency only!!  Do not
- *  use this codec in production.
- *
- *  <p>This codec stores all dimensional data in a single
- *  human-readable text file (_N.dim).  You can view this in
- *  any text editor, and even edit it to alter your index.
- *
- *  @lucene.experimental */
-public final class SimpleTextDimensionalFormat extends DimensionalFormat {
-  
-  @Override
-  public DimensionalWriter fieldsWriter(SegmentWriteState state) throws IOException {
-    return new SimpleTextDimensionalWriter(state);
-  }
-
-  @Override
-  public DimensionalReader fieldsReader(SegmentReadState state) throws IOException {
-    return new SimpleTextDimensionalReader(state);
-  }
-
-  /** Extension of dimensional data file */
-  static final String DIMENSIONAL_EXTENSION = "dim";
-
-  /** Extension of dimensional index file */
-  static final String DIMENSIONAL_INDEX_EXTENSION = "dii";
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDimensionalReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDimensionalReader.java
deleted file mode 100644
index 800e174..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDimensionalReader.java
+++ /dev/null
@@ -1,252 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.nio.charset.StandardCharsets;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.lucene.codecs.DimensionalReader;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.store.BufferedChecksumIndexInput;
-import org.apache.lucene.store.ChecksumIndexInput;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.BytesRefBuilder;
-import org.apache.lucene.util.StringHelper;
-import org.apache.lucene.util.bkd.BKDReader;
-
-import static org.apache.lucene.codecs.simpletext.SimpleTextDimensionalWriter.BLOCK_FP;
-import static org.apache.lucene.codecs.simpletext.SimpleTextDimensionalWriter.BYTES_PER_DIM;
-import static org.apache.lucene.codecs.simpletext.SimpleTextDimensionalWriter.FIELD_COUNT;
-import static org.apache.lucene.codecs.simpletext.SimpleTextDimensionalWriter.FIELD_FP;
-import static org.apache.lucene.codecs.simpletext.SimpleTextDimensionalWriter.FIELD_FP_NAME;
-import static org.apache.lucene.codecs.simpletext.SimpleTextDimensionalWriter.INDEX_COUNT;
-import static org.apache.lucene.codecs.simpletext.SimpleTextDimensionalWriter.MAX_LEAF_POINTS;
-import static org.apache.lucene.codecs.simpletext.SimpleTextDimensionalWriter.MAX_VALUE;
-import static org.apache.lucene.codecs.simpletext.SimpleTextDimensionalWriter.MIN_VALUE;
-import static org.apache.lucene.codecs.simpletext.SimpleTextDimensionalWriter.NUM_DIMS;
-import static org.apache.lucene.codecs.simpletext.SimpleTextDimensionalWriter.SPLIT_COUNT;
-import static org.apache.lucene.codecs.simpletext.SimpleTextDimensionalWriter.SPLIT_DIM;
-import static org.apache.lucene.codecs.simpletext.SimpleTextDimensionalWriter.SPLIT_VALUE;
-
-class SimpleTextDimensionalReader extends DimensionalReader {
-
-  private final IndexInput dataIn;
-  final SegmentReadState readState;
-  final Map<String,BKDReader> readers = new HashMap<>();
-  final BytesRefBuilder scratch = new BytesRefBuilder();
-
-  public SimpleTextDimensionalReader(SegmentReadState readState) throws IOException {
-    // Initialize readers now:
-    String fileName = IndexFileNames.segmentFileName(readState.segmentInfo.name, readState.segmentSuffix, SimpleTextDimensionalFormat.DIMENSIONAL_EXTENSION);
-    dataIn = readState.directory.openInput(fileName, IOContext.DEFAULT);
-    String indexFileName = IndexFileNames.segmentFileName(readState.segmentInfo.name, readState.segmentSuffix, SimpleTextDimensionalFormat.DIMENSIONAL_INDEX_EXTENSION);
-    try (ChecksumIndexInput in = readState.directory.openChecksumInput(indexFileName, IOContext.DEFAULT)) {
-      readLine(in);
-      int count = parseInt(FIELD_COUNT);
-      for(int i=0;i<count;i++) {
-        readLine(in);
-        String fieldName = stripPrefix(FIELD_FP_NAME);
-        readLine(in);
-        long fp = parseLong(FIELD_FP);
-        readers.put(fieldName, initReader(fp));
-      }
-      SimpleTextUtil.checkFooter(in);
-    }
-    this.readState = readState;
-  }
-
-  private BKDReader initReader(long fp) throws IOException {
-    // NOTE: matches what writeIndex does in SimpleTextDimensionalWriter
-    dataIn.seek(fp);
-    readLine(dataIn);
-    int numDims = parseInt(NUM_DIMS);
-
-    readLine(dataIn);
-    int bytesPerDim = parseInt(BYTES_PER_DIM);
-
-    readLine(dataIn);
-    int maxPointsInLeafNode = parseInt(MAX_LEAF_POINTS);
-
-    readLine(dataIn);
-    int count = parseInt(INDEX_COUNT);
-
-    readLine(dataIn);
-    assert startsWith(MIN_VALUE);
-    BytesRef minValue = SimpleTextUtil.fromBytesRefString(stripPrefix(MIN_VALUE));
-    assert minValue.length == numDims*bytesPerDim;
-
-    readLine(dataIn);
-    assert startsWith(MAX_VALUE);
-    BytesRef maxValue = SimpleTextUtil.fromBytesRefString(stripPrefix(MAX_VALUE));
-    assert maxValue.length == numDims*bytesPerDim;
-    
-    long[] leafBlockFPs = new long[count];
-    for(int i=0;i<count;i++) {
-      readLine(dataIn);
-      leafBlockFPs[i] = parseLong(BLOCK_FP);
-    }
-    readLine(dataIn);
-    count = parseInt(SPLIT_COUNT);
-
-    byte[] splitPackedValues = new byte[count * (1 + bytesPerDim)];
-    for(int i=0;i<count;i++) {
-      readLine(dataIn);
-      splitPackedValues[(1 + bytesPerDim) * i] = (byte) parseInt(SPLIT_DIM);
-      readLine(dataIn);
-      assert startsWith(SPLIT_VALUE);
-      BytesRef br = SimpleTextUtil.fromBytesRefString(stripPrefix(SPLIT_VALUE));
-      assert br.length == bytesPerDim;
-      System.arraycopy(br.bytes, br.offset, splitPackedValues, (1 + bytesPerDim) * i + 1, bytesPerDim);
-    }
-
-    return new SimpleTextBKDReader(dataIn, numDims, maxPointsInLeafNode, bytesPerDim, leafBlockFPs, splitPackedValues, minValue.bytes, maxValue.bytes);
-  }
-
-  private void readLine(IndexInput in) throws IOException {
-    SimpleTextUtil.readLine(in, scratch);
-  }
-
-  private boolean startsWith(BytesRef prefix) {
-    return StringHelper.startsWith(scratch.get(), prefix);
-  }
-
-  private int parseInt(BytesRef prefix) {
-    assert startsWith(prefix);
-    return Integer.parseInt(stripPrefix(prefix));
-  }
-
-  private long parseLong(BytesRef prefix) {
-    assert startsWith(prefix);
-    return Long.parseLong(stripPrefix(prefix));
-  }
-
-  private String stripPrefix(BytesRef prefix) {
-    return new String(scratch.bytes(), prefix.length, scratch.length() - prefix.length, StandardCharsets.UTF_8);
-  }
-
-  private BKDReader getBKDReader(String fieldName) {
-    FieldInfo fieldInfo = readState.fieldInfos.fieldInfo(fieldName);
-    if (fieldInfo == null) {
-      throw new IllegalArgumentException("field=\"" + fieldName + "\" is unrecognized");
-    }
-    if (fieldInfo.getDimensionCount() == 0) {
-      throw new IllegalArgumentException("field=\"" + fieldName + "\" did not index dimensional values");
-    }
-    return readers.get(fieldName);
-  }
-
-  /** Finds all documents and points matching the provided visitor */
-  @Override
-  public void intersect(String fieldName, IntersectVisitor visitor) throws IOException {
-    BKDReader bkdReader = getBKDReader(fieldName);
-    if (bkdReader == null) {
-      // Schema ghost corner case!  This field did index dimensional values in the past, but
-      // now all docs having this dimensional field were deleted in this segment:
-      return;
-    }
-    bkdReader.intersect(visitor);
-  }
-
-  @Override
-  public void checkIntegrity() throws IOException {
-    BytesRefBuilder scratch = new BytesRefBuilder();
-    IndexInput clone = dataIn.clone();
-    clone.seek(0);
-
-    // checksum is fixed-width encoded with 20 bytes, plus 1 byte for newline (the space is included in SimpleTextUtil.CHECKSUM):
-    long footerStartPos = dataIn.length() - (SimpleTextUtil.CHECKSUM.length + 21);
-    ChecksumIndexInput input = new BufferedChecksumIndexInput(clone);
-    while (true) {
-      SimpleTextUtil.readLine(input, scratch);
-      if (input.getFilePointer() >= footerStartPos) {
-        // Make sure we landed at precisely the right location:
-        if (input.getFilePointer() != footerStartPos) {
-          throw new CorruptIndexException("SimpleText failure: footer does not start at expected position current=" + input.getFilePointer() + " vs expected=" + footerStartPos, input);
-        }
-        SimpleTextUtil.checkFooter(input);
-        break;
-      }
-    }
-  }
-
-  @Override
-  public long ramBytesUsed() {
-    return 0L;
-  }
-
-  @Override
-  public void close() throws IOException {
-    dataIn.close();
-  }
-
-  @Override
-  public String toString() {
-    return "SimpleTextDimensionalReader(segment=" + readState.segmentInfo.name + " maxDoc=" + readState.segmentInfo.maxDoc() + ")";
-  }
-
-  @Override
-  public byte[] getMinPackedValue(String fieldName) {
-    BKDReader bkdReader = getBKDReader(fieldName);
-    if (bkdReader == null) {
-      // Schema ghost corner case!  This field did index dimensional values in the past, but
-      // now all docs having this dimensional field were deleted in this segment:
-      return null;
-    }
-    return bkdReader.getMinPackedValue();
-  }
-
-  @Override
-  public byte[] getMaxPackedValue(String fieldName) {
-    BKDReader bkdReader = getBKDReader(fieldName);
-    if (bkdReader == null) {
-      // Schema ghost corner case!  This field did index dimensional values in the past, but
-      // now all docs having this dimensional field were deleted in this segment:
-      return null;
-    }
-    return bkdReader.getMaxPackedValue();
-  }
-
-  @Override
-  public int getNumDimensions(String fieldName) {
-    BKDReader bkdReader = getBKDReader(fieldName);
-    if (bkdReader == null) {
-      // Schema ghost corner case!  This field did index dimensional values in the past, but
-      // now all docs having this dimensional field were deleted in this segment:
-      return 0;
-    }
-    return bkdReader.getNumDimensions();
-  }
-
-  @Override
-  public int getBytesPerDimension(String fieldName) {
-    BKDReader bkdReader = getBKDReader(fieldName);
-    if (bkdReader == null) {
-      // Schema ghost corner case!  This field did index dimensional values in the past, but
-      // now all docs having this dimensional field were deleted in this segment:
-      return 0;
-    }
-    return bkdReader.getBytesPerDimension();
-  }
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDimensionalWriter.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDimensionalWriter.java
deleted file mode 100644
index 268fddc..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDimensionalWriter.java
+++ /dev/null
@@ -1,225 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.lucene.codecs.DimensionalReader;
-import org.apache.lucene.codecs.DimensionalWriter;
-import org.apache.lucene.index.DimensionalValues.IntersectVisitor;
-import org.apache.lucene.index.DimensionalValues.Relation;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.BytesRefBuilder;
-import org.apache.lucene.util.bkd.BKDWriter;
-
-class SimpleTextDimensionalWriter extends DimensionalWriter {
-
-  final static BytesRef NUM_DIMS      = new BytesRef("num dims ");
-  final static BytesRef BYTES_PER_DIM = new BytesRef("bytes per dim ");
-  final static BytesRef MAX_LEAF_POINTS = new BytesRef("max leaf points ");
-  final static BytesRef INDEX_COUNT = new BytesRef("index count ");
-  final static BytesRef BLOCK_COUNT   = new BytesRef("block count ");
-  final static BytesRef BLOCK_DOC_ID  = new BytesRef("  doc ");
-  final static BytesRef BLOCK_FP      = new BytesRef("  block fp ");
-  final static BytesRef BLOCK_VALUE   = new BytesRef("  block value ");
-  final static BytesRef SPLIT_COUNT   = new BytesRef("split count ");
-  final static BytesRef SPLIT_DIM     = new BytesRef("  split dim ");
-  final static BytesRef SPLIT_VALUE   = new BytesRef("  split value ");
-  final static BytesRef FIELD_COUNT   = new BytesRef("field count ");
-  final static BytesRef FIELD_FP_NAME = new BytesRef("  field fp name ");
-  final static BytesRef FIELD_FP      = new BytesRef("  field fp ");
-  final static BytesRef MIN_VALUE     = new BytesRef("min value ");
-  final static BytesRef MAX_VALUE     = new BytesRef("max value ");
-
-  private IndexOutput dataOut;
-  final BytesRefBuilder scratch = new BytesRefBuilder();
-  final SegmentWriteState writeState;
-  final Map<String,Long> indexFPs = new HashMap<>();
-
-  public SimpleTextDimensionalWriter(SegmentWriteState writeState) throws IOException {
-    String fileName = IndexFileNames.segmentFileName(writeState.segmentInfo.name, writeState.segmentSuffix, SimpleTextDimensionalFormat.DIMENSIONAL_EXTENSION);
-    dataOut = writeState.directory.createOutput(fileName, writeState.context);
-    this.writeState = writeState;
-  }
-
-  @Override
-  public void writeField(FieldInfo fieldInfo, DimensionalReader values) throws IOException {
-
-    // We use the normal BKDWriter, but subclass to customize how it writes the index and blocks to disk:
-    BKDWriter writer = new BKDWriter(writeState.directory,
-                                     writeState.segmentInfo.name,
-                                     fieldInfo.getDimensionCount(),
-                                     fieldInfo.getDimensionNumBytes(),
-                                     BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,
-                                     BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP) {
-
-        @Override
-        protected void writeIndex(IndexOutput out, long[] leafBlockFPs, byte[] splitPackedValues) throws IOException {
-          write(out, NUM_DIMS);
-          writeInt(out, numDims);
-          newline(out);
-
-          write(out, BYTES_PER_DIM);
-          writeInt(out, bytesPerDim);
-          newline(out);
-
-          write(out, MAX_LEAF_POINTS);
-          writeInt(out, maxPointsInLeafNode);
-          newline(out);
-
-          write(out, INDEX_COUNT);
-          writeInt(out, leafBlockFPs.length);
-          newline(out);
-
-          write(out, MIN_VALUE);
-          BytesRef br = new BytesRef(minPackedValue, 0, minPackedValue.length);
-          write(out, br.toString());
-          newline(out);
-
-          write(out, MAX_VALUE);
-          br = new BytesRef(maxPackedValue, 0, maxPackedValue.length);
-          write(out, br.toString());
-          newline(out);
-
-          for(int i=0;i<leafBlockFPs.length;i++) {
-            write(out, BLOCK_FP);
-            writeLong(out, leafBlockFPs[i]);
-            newline(out);
-          }
-
-          assert (splitPackedValues.length % (1 + fieldInfo.getDimensionNumBytes())) == 0;
-          int count = splitPackedValues.length / (1 + fieldInfo.getDimensionNumBytes());
-          assert count == leafBlockFPs.length;
-
-          write(out, SPLIT_COUNT);
-          writeInt(out, count);
-          newline(out);
-
-          for(int i=0;i<count;i++) {
-            write(out, SPLIT_DIM);
-            writeInt(out, splitPackedValues[i * (1 + fieldInfo.getDimensionNumBytes())] & 0xff);
-            newline(out);
-            write(out, SPLIT_VALUE);
-            br = new BytesRef(splitPackedValues, 1+(i * (1+fieldInfo.getDimensionNumBytes())), fieldInfo.getDimensionNumBytes());
-            write(out, br.toString());
-            newline(out);
-          }
-        }
-
-        @Override
-        protected void writeLeafBlockDocs(IndexOutput out, int[] docIDs, int start, int count) throws IOException {
-          write(out, BLOCK_COUNT);
-          writeInt(out, count);
-          newline(out);
-          for(int i=0;i<count;i++) {
-            write(out, BLOCK_DOC_ID);
-            writeInt(out, docIDs[start+i]);
-            newline(out);
-          }
-        }
-
-        @Override
-        protected void writeCommonPrefixes(IndexOutput out, int[] commonPrefixLengths, byte[] packedValue) {
-          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths
-        }
-
-        @Override
-        protected void writeLeafBlockPackedValue(IndexOutput out, int[] commonPrefixLengths, byte[] bytes) throws IOException {
-          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths
-          assert bytes.length == packedBytesLength;
-          write(out, BLOCK_VALUE);
-          write(out, new BytesRef(bytes, 0, bytes.length).toString());
-          newline(out);
-        }          
-      };
-
-    values.intersect(fieldInfo.name, new IntersectVisitor() {
-        @Override
-        public void visit(int docID) {
-          throw new IllegalStateException();
-        }
-
-        public void visit(int docID, byte[] packedValue) throws IOException {
-          writer.add(packedValue, docID);
-        }
-
-        @Override
-        public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {
-          return Relation.CELL_CROSSES_QUERY;
-        }
-      });
-
-    // We could have 0 points on merge since all docs with dimensional fields may be deleted:
-    if (writer.getPointCount() > 0) {
-      indexFPs.put(fieldInfo.name, writer.finish(dataOut));
-    }
-  }
-
-  private void write(IndexOutput out, String s) throws IOException {
-    SimpleTextUtil.write(out, s, scratch);
-  }
-
-  private void writeInt(IndexOutput out, int x) throws IOException {
-    SimpleTextUtil.write(out, Integer.toString(x), scratch);
-  }
-
-  private void writeLong(IndexOutput out, long x) throws IOException {
-    SimpleTextUtil.write(out, Long.toString(x), scratch);
-  }
-
-  private void write(IndexOutput out, BytesRef b) throws IOException {
-    SimpleTextUtil.write(out, b);
-  }
-
-  private void newline(IndexOutput out) throws IOException {
-    SimpleTextUtil.writeNewline(out);
-  }
-
-  @Override
-  public void close() throws IOException {
-    if (dataOut != null) {
-      SimpleTextUtil.writeChecksum(dataOut, scratch);
-      dataOut.close();
-      dataOut = null;
-
-      // Write index file
-      String fileName = IndexFileNames.segmentFileName(writeState.segmentInfo.name, writeState.segmentSuffix, SimpleTextDimensionalFormat.DIMENSIONAL_INDEX_EXTENSION);
-      try (IndexOutput indexOut = writeState.directory.createOutput(fileName, writeState.context)) {
-        int count = indexFPs.size();
-        write(indexOut, FIELD_COUNT);
-        write(indexOut, Integer.toString(count));
-        newline(indexOut);
-        for(Map.Entry<String,Long> ent : indexFPs.entrySet()) {
-          write(indexOut, FIELD_FP_NAME);
-          write(indexOut, ent.getKey());
-          newline(indexOut);
-          write(indexOut, FIELD_FP);
-          write(indexOut, Long.toString(ent.getValue()));
-          newline(indexOut);
-        }
-        SimpleTextUtil.writeChecksum(indexOut, scratch);
-      }
-    }
-  }
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosFormat.java
index dc68f72..109966a 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosFormat.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosFormat.java
@@ -232,11 +232,11 @@ public class SimpleTextFieldInfosFormat extends FieldInfosFormat {
         }
 
         SimpleTextUtil.write(out, DIM_COUNT);
-        SimpleTextUtil.write(out, Integer.toString(fi.getDimensionCount()), scratch);
+        SimpleTextUtil.write(out, Integer.toString(fi.getPointDimensionCount()), scratch);
         SimpleTextUtil.writeNewline(out);
         
         SimpleTextUtil.write(out, DIM_NUM_BYTES);
-        SimpleTextUtil.write(out, Integer.toString(fi.getDimensionNumBytes()), scratch);
+        SimpleTextUtil.write(out, Integer.toString(fi.getPointNumBytes()), scratch);
         SimpleTextUtil.writeNewline(out);
       }
       SimpleTextUtil.writeChecksum(out, scratch);
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointFormat.java
new file mode 100644
index 0000000..089ba4f
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointFormat.java
@@ -0,0 +1,53 @@
+package org.apache.lucene.codecs.simpletext;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.PointFormat;
+import org.apache.lucene.codecs.PointReader;
+import org.apache.lucene.codecs.PointWriter;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+
+/** For debugging, curiosity, transparency only!!  Do not
+ *  use this codec in production.
+ *
+ *  <p>This codec stores all dimensional data in a single
+ *  human-readable text file (_N.dim).  You can view this in
+ *  any text editor, and even edit it to alter your index.
+ *
+ *  @lucene.experimental */
+public final class SimpleTextPointFormat extends PointFormat {
+  
+  @Override
+  public PointWriter fieldsWriter(SegmentWriteState state) throws IOException {
+    return new SimpleTextPointWriter(state);
+  }
+
+  @Override
+  public PointReader fieldsReader(SegmentReadState state) throws IOException {
+    return new SimpleTextPointReader(state);
+  }
+
+  /** Extension of points data file */
+  static final String POINT_EXTENSION = "dim";
+
+  /** Extension of points index file */
+  static final String POINT_INDEX_EXTENSION = "dii";
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointReader.java
new file mode 100644
index 0000000..222805e
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointReader.java
@@ -0,0 +1,252 @@
+package org.apache.lucene.codecs.simpletext;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.nio.charset.StandardCharsets;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.codecs.PointReader;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.store.BufferedChecksumIndexInput;
+import org.apache.lucene.store.ChecksumIndexInput;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.BytesRefBuilder;
+import org.apache.lucene.util.StringHelper;
+import org.apache.lucene.util.bkd.BKDReader;
+
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.BLOCK_FP;
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.BYTES_PER_DIM;
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.FIELD_COUNT;
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.FIELD_FP;
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.FIELD_FP_NAME;
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.INDEX_COUNT;
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.MAX_LEAF_POINTS;
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.MAX_VALUE;
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.MIN_VALUE;
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.NUM_DIMS;
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.SPLIT_COUNT;
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.SPLIT_DIM;
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.SPLIT_VALUE;
+
+class SimpleTextPointReader extends PointReader {
+
+  private final IndexInput dataIn;
+  final SegmentReadState readState;
+  final Map<String,BKDReader> readers = new HashMap<>();
+  final BytesRefBuilder scratch = new BytesRefBuilder();
+
+  public SimpleTextPointReader(SegmentReadState readState) throws IOException {
+    // Initialize readers now:
+    String fileName = IndexFileNames.segmentFileName(readState.segmentInfo.name, readState.segmentSuffix, SimpleTextPointFormat.POINT_EXTENSION);
+    dataIn = readState.directory.openInput(fileName, IOContext.DEFAULT);
+    String indexFileName = IndexFileNames.segmentFileName(readState.segmentInfo.name, readState.segmentSuffix, SimpleTextPointFormat.POINT_INDEX_EXTENSION);
+    try (ChecksumIndexInput in = readState.directory.openChecksumInput(indexFileName, IOContext.DEFAULT)) {
+      readLine(in);
+      int count = parseInt(FIELD_COUNT);
+      for(int i=0;i<count;i++) {
+        readLine(in);
+        String fieldName = stripPrefix(FIELD_FP_NAME);
+        readLine(in);
+        long fp = parseLong(FIELD_FP);
+        readers.put(fieldName, initReader(fp));
+      }
+      SimpleTextUtil.checkFooter(in);
+    }
+    this.readState = readState;
+  }
+
+  private BKDReader initReader(long fp) throws IOException {
+    // NOTE: matches what writeIndex does in SimpleTextPointWriter
+    dataIn.seek(fp);
+    readLine(dataIn);
+    int numDims = parseInt(NUM_DIMS);
+
+    readLine(dataIn);
+    int bytesPerDim = parseInt(BYTES_PER_DIM);
+
+    readLine(dataIn);
+    int maxPointsInLeafNode = parseInt(MAX_LEAF_POINTS);
+
+    readLine(dataIn);
+    int count = parseInt(INDEX_COUNT);
+
+    readLine(dataIn);
+    assert startsWith(MIN_VALUE);
+    BytesRef minValue = SimpleTextUtil.fromBytesRefString(stripPrefix(MIN_VALUE));
+    assert minValue.length == numDims*bytesPerDim;
+
+    readLine(dataIn);
+    assert startsWith(MAX_VALUE);
+    BytesRef maxValue = SimpleTextUtil.fromBytesRefString(stripPrefix(MAX_VALUE));
+    assert maxValue.length == numDims*bytesPerDim;
+    
+    long[] leafBlockFPs = new long[count];
+    for(int i=0;i<count;i++) {
+      readLine(dataIn);
+      leafBlockFPs[i] = parseLong(BLOCK_FP);
+    }
+    readLine(dataIn);
+    count = parseInt(SPLIT_COUNT);
+
+    byte[] splitPackedValues = new byte[count * (1 + bytesPerDim)];
+    for(int i=0;i<count;i++) {
+      readLine(dataIn);
+      splitPackedValues[(1 + bytesPerDim) * i] = (byte) parseInt(SPLIT_DIM);
+      readLine(dataIn);
+      assert startsWith(SPLIT_VALUE);
+      BytesRef br = SimpleTextUtil.fromBytesRefString(stripPrefix(SPLIT_VALUE));
+      assert br.length == bytesPerDim;
+      System.arraycopy(br.bytes, br.offset, splitPackedValues, (1 + bytesPerDim) * i + 1, bytesPerDim);
+    }
+
+    return new SimpleTextBKDReader(dataIn, numDims, maxPointsInLeafNode, bytesPerDim, leafBlockFPs, splitPackedValues, minValue.bytes, maxValue.bytes);
+  }
+
+  private void readLine(IndexInput in) throws IOException {
+    SimpleTextUtil.readLine(in, scratch);
+  }
+
+  private boolean startsWith(BytesRef prefix) {
+    return StringHelper.startsWith(scratch.get(), prefix);
+  }
+
+  private int parseInt(BytesRef prefix) {
+    assert startsWith(prefix);
+    return Integer.parseInt(stripPrefix(prefix));
+  }
+
+  private long parseLong(BytesRef prefix) {
+    assert startsWith(prefix);
+    return Long.parseLong(stripPrefix(prefix));
+  }
+
+  private String stripPrefix(BytesRef prefix) {
+    return new String(scratch.bytes(), prefix.length, scratch.length() - prefix.length, StandardCharsets.UTF_8);
+  }
+
+  private BKDReader getBKDReader(String fieldName) {
+    FieldInfo fieldInfo = readState.fieldInfos.fieldInfo(fieldName);
+    if (fieldInfo == null) {
+      throw new IllegalArgumentException("field=\"" + fieldName + "\" is unrecognized");
+    }
+    if (fieldInfo.getPointDimensionCount() == 0) {
+      throw new IllegalArgumentException("field=\"" + fieldName + "\" did not index points");
+    }
+    return readers.get(fieldName);
+  }
+
+  /** Finds all documents and points matching the provided visitor */
+  @Override
+  public void intersect(String fieldName, IntersectVisitor visitor) throws IOException {
+    BKDReader bkdReader = getBKDReader(fieldName);
+    if (bkdReader == null) {
+      // Schema ghost corner case!  This field did index points in the past, but
+      // now all docs having this field were deleted in this segment:
+      return;
+    }
+    bkdReader.intersect(visitor);
+  }
+
+  @Override
+  public void checkIntegrity() throws IOException {
+    BytesRefBuilder scratch = new BytesRefBuilder();
+    IndexInput clone = dataIn.clone();
+    clone.seek(0);
+
+    // checksum is fixed-width encoded with 20 bytes, plus 1 byte for newline (the space is included in SimpleTextUtil.CHECKSUM):
+    long footerStartPos = dataIn.length() - (SimpleTextUtil.CHECKSUM.length + 21);
+    ChecksumIndexInput input = new BufferedChecksumIndexInput(clone);
+    while (true) {
+      SimpleTextUtil.readLine(input, scratch);
+      if (input.getFilePointer() >= footerStartPos) {
+        // Make sure we landed at precisely the right location:
+        if (input.getFilePointer() != footerStartPos) {
+          throw new CorruptIndexException("SimpleText failure: footer does not start at expected position current=" + input.getFilePointer() + " vs expected=" + footerStartPos, input);
+        }
+        SimpleTextUtil.checkFooter(input);
+        break;
+      }
+    }
+  }
+
+  @Override
+  public long ramBytesUsed() {
+    return 0L;
+  }
+
+  @Override
+  public void close() throws IOException {
+    dataIn.close();
+  }
+
+  @Override
+  public String toString() {
+    return "SimpleTextPointReader(segment=" + readState.segmentInfo.name + " maxDoc=" + readState.segmentInfo.maxDoc() + ")";
+  }
+
+  @Override
+  public byte[] getMinPackedValue(String fieldName) {
+    BKDReader bkdReader = getBKDReader(fieldName);
+    if (bkdReader == null) {
+      // Schema ghost corner case!  This field did index points in the past, but
+      // now all docs having this field were deleted in this segment:
+      return null;
+    }
+    return bkdReader.getMinPackedValue();
+  }
+
+  @Override
+  public byte[] getMaxPackedValue(String fieldName) {
+    BKDReader bkdReader = getBKDReader(fieldName);
+    if (bkdReader == null) {
+      // Schema ghost corner case!  This field did index points in the past, but
+      // now all docs having this field were deleted in this segment:
+      return null;
+    }
+    return bkdReader.getMaxPackedValue();
+  }
+
+  @Override
+  public int getNumDimensions(String fieldName) {
+    BKDReader bkdReader = getBKDReader(fieldName);
+    if (bkdReader == null) {
+      // Schema ghost corner case!  This field did index points in the past, but
+      // now all docs having this field were deleted in this segment:
+      return 0;
+    }
+    return bkdReader.getNumDimensions();
+  }
+
+  @Override
+  public int getBytesPerDimension(String fieldName) {
+    BKDReader bkdReader = getBKDReader(fieldName);
+    if (bkdReader == null) {
+      // Schema ghost corner case!  This field did index points in the past, but
+      // now all docs having this field were deleted in this segment:
+      return 0;
+    }
+    return bkdReader.getBytesPerDimension();
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointWriter.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointWriter.java
new file mode 100644
index 0000000..fc533da
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointWriter.java
@@ -0,0 +1,225 @@
+package org.apache.lucene.codecs.simpletext;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.codecs.PointReader;
+import org.apache.lucene.codecs.PointWriter;
+import org.apache.lucene.index.PointValues.IntersectVisitor;
+import org.apache.lucene.index.PointValues.Relation;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.BytesRefBuilder;
+import org.apache.lucene.util.bkd.BKDWriter;
+
+class SimpleTextPointWriter extends PointWriter {
+
+  final static BytesRef NUM_DIMS      = new BytesRef("num dims ");
+  final static BytesRef BYTES_PER_DIM = new BytesRef("bytes per dim ");
+  final static BytesRef MAX_LEAF_POINTS = new BytesRef("max leaf points ");
+  final static BytesRef INDEX_COUNT = new BytesRef("index count ");
+  final static BytesRef BLOCK_COUNT   = new BytesRef("block count ");
+  final static BytesRef BLOCK_DOC_ID  = new BytesRef("  doc ");
+  final static BytesRef BLOCK_FP      = new BytesRef("  block fp ");
+  final static BytesRef BLOCK_VALUE   = new BytesRef("  block value ");
+  final static BytesRef SPLIT_COUNT   = new BytesRef("split count ");
+  final static BytesRef SPLIT_DIM     = new BytesRef("  split dim ");
+  final static BytesRef SPLIT_VALUE   = new BytesRef("  split value ");
+  final static BytesRef FIELD_COUNT   = new BytesRef("field count ");
+  final static BytesRef FIELD_FP_NAME = new BytesRef("  field fp name ");
+  final static BytesRef FIELD_FP      = new BytesRef("  field fp ");
+  final static BytesRef MIN_VALUE     = new BytesRef("min value ");
+  final static BytesRef MAX_VALUE     = new BytesRef("max value ");
+
+  private IndexOutput dataOut;
+  final BytesRefBuilder scratch = new BytesRefBuilder();
+  final SegmentWriteState writeState;
+  final Map<String,Long> indexFPs = new HashMap<>();
+
+  public SimpleTextPointWriter(SegmentWriteState writeState) throws IOException {
+    String fileName = IndexFileNames.segmentFileName(writeState.segmentInfo.name, writeState.segmentSuffix, SimpleTextPointFormat.POINT_EXTENSION);
+    dataOut = writeState.directory.createOutput(fileName, writeState.context);
+    this.writeState = writeState;
+  }
+
+  @Override
+  public void writeField(FieldInfo fieldInfo, PointReader values) throws IOException {
+
+    // We use the normal BKDWriter, but subclass to customize how it writes the index and blocks to disk:
+    BKDWriter writer = new BKDWriter(writeState.directory,
+                                     writeState.segmentInfo.name,
+                                     fieldInfo.getPointDimensionCount(),
+                                     fieldInfo.getPointNumBytes(),
+                                     BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,
+                                     BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP) {
+
+        @Override
+        protected void writeIndex(IndexOutput out, long[] leafBlockFPs, byte[] splitPackedValues) throws IOException {
+          write(out, NUM_DIMS);
+          writeInt(out, numDims);
+          newline(out);
+
+          write(out, BYTES_PER_DIM);
+          writeInt(out, bytesPerDim);
+          newline(out);
+
+          write(out, MAX_LEAF_POINTS);
+          writeInt(out, maxPointsInLeafNode);
+          newline(out);
+
+          write(out, INDEX_COUNT);
+          writeInt(out, leafBlockFPs.length);
+          newline(out);
+
+          write(out, MIN_VALUE);
+          BytesRef br = new BytesRef(minPackedValue, 0, minPackedValue.length);
+          write(out, br.toString());
+          newline(out);
+
+          write(out, MAX_VALUE);
+          br = new BytesRef(maxPackedValue, 0, maxPackedValue.length);
+          write(out, br.toString());
+          newline(out);
+
+          for(int i=0;i<leafBlockFPs.length;i++) {
+            write(out, BLOCK_FP);
+            writeLong(out, leafBlockFPs[i]);
+            newline(out);
+          }
+
+          assert (splitPackedValues.length % (1 + fieldInfo.getPointNumBytes())) == 0;
+          int count = splitPackedValues.length / (1 + fieldInfo.getPointNumBytes());
+          assert count == leafBlockFPs.length;
+
+          write(out, SPLIT_COUNT);
+          writeInt(out, count);
+          newline(out);
+
+          for(int i=0;i<count;i++) {
+            write(out, SPLIT_DIM);
+            writeInt(out, splitPackedValues[i * (1 + fieldInfo.getPointNumBytes())] & 0xff);
+            newline(out);
+            write(out, SPLIT_VALUE);
+            br = new BytesRef(splitPackedValues, 1+(i * (1+fieldInfo.getPointNumBytes())), fieldInfo.getPointNumBytes());
+            write(out, br.toString());
+            newline(out);
+          }
+        }
+
+        @Override
+        protected void writeLeafBlockDocs(IndexOutput out, int[] docIDs, int start, int count) throws IOException {
+          write(out, BLOCK_COUNT);
+          writeInt(out, count);
+          newline(out);
+          for(int i=0;i<count;i++) {
+            write(out, BLOCK_DOC_ID);
+            writeInt(out, docIDs[start+i]);
+            newline(out);
+          }
+        }
+
+        @Override
+        protected void writeCommonPrefixes(IndexOutput out, int[] commonPrefixLengths, byte[] packedValue) {
+          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths
+        }
+
+        @Override
+        protected void writeLeafBlockPackedValue(IndexOutput out, int[] commonPrefixLengths, byte[] bytes) throws IOException {
+          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths
+          assert bytes.length == packedBytesLength;
+          write(out, BLOCK_VALUE);
+          write(out, new BytesRef(bytes, 0, bytes.length).toString());
+          newline(out);
+        }          
+      };
+
+    values.intersect(fieldInfo.name, new IntersectVisitor() {
+        @Override
+        public void visit(int docID) {
+          throw new IllegalStateException();
+        }
+
+        public void visit(int docID, byte[] packedValue) throws IOException {
+          writer.add(packedValue, docID);
+        }
+
+        @Override
+        public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {
+          return Relation.CELL_CROSSES_QUERY;
+        }
+      });
+
+    // We could have 0 points on merge since all docs with points may be deleted:
+    if (writer.getPointCount() > 0) {
+      indexFPs.put(fieldInfo.name, writer.finish(dataOut));
+    }
+  }
+
+  private void write(IndexOutput out, String s) throws IOException {
+    SimpleTextUtil.write(out, s, scratch);
+  }
+
+  private void writeInt(IndexOutput out, int x) throws IOException {
+    SimpleTextUtil.write(out, Integer.toString(x), scratch);
+  }
+
+  private void writeLong(IndexOutput out, long x) throws IOException {
+    SimpleTextUtil.write(out, Long.toString(x), scratch);
+  }
+
+  private void write(IndexOutput out, BytesRef b) throws IOException {
+    SimpleTextUtil.write(out, b);
+  }
+
+  private void newline(IndexOutput out) throws IOException {
+    SimpleTextUtil.writeNewline(out);
+  }
+
+  @Override
+  public void close() throws IOException {
+    if (dataOut != null) {
+      SimpleTextUtil.writeChecksum(dataOut, scratch);
+      dataOut.close();
+      dataOut = null;
+
+      // Write index file
+      String fileName = IndexFileNames.segmentFileName(writeState.segmentInfo.name, writeState.segmentSuffix, SimpleTextPointFormat.POINT_INDEX_EXTENSION);
+      try (IndexOutput indexOut = writeState.directory.createOutput(fileName, writeState.context)) {
+        int count = indexFPs.size();
+        write(indexOut, FIELD_COUNT);
+        write(indexOut, Integer.toString(count));
+        newline(indexOut);
+        for(Map.Entry<String,Long> ent : indexFPs.entrySet()) {
+          write(indexOut, FIELD_FP_NAME);
+          write(indexOut, ent.getKey());
+          newline(indexOut);
+          write(indexOut, FIELD_FP);
+          write(indexOut, Long.toString(ent.getValue()));
+          newline(indexOut);
+        }
+        SimpleTextUtil.writeChecksum(indexOut, scratch);
+      }
+    }
+  }
+}
diff --git a/lucene/core/src/java/org/apache/lucene/analysis/LegacyNumericTokenStream.java b/lucene/core/src/java/org/apache/lucene/analysis/LegacyNumericTokenStream.java
index 1a7077d..e7abf1f 100644
--- a/lucene/core/src/java/org/apache/lucene/analysis/LegacyNumericTokenStream.java
+++ b/lucene/core/src/java/org/apache/lucene/analysis/LegacyNumericTokenStream.java
@@ -23,7 +23,6 @@ import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;
 import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
-import org.apache.lucene.index.DimensionalValues;
 import org.apache.lucene.util.Attribute;
 import org.apache.lucene.util.AttributeFactory;
 import org.apache.lucene.util.AttributeImpl;
@@ -86,7 +85,7 @@ import org.apache.lucene.util.LegacyNumericUtils;
  * href="../search/LegacyNumericRangeQuery.html#precisionStepDesc"><code>precisionStep</code></a>
  * parameter as well as how numeric fields work under the hood.</p>
  *
- * @deprecated Please switch to {@link DimensionalValues} instead
+ * @deprecated Please switch to {@link org.apache.lucene.index.PointValues} instead
  *
  * @since 2.9
  */
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/Codec.java b/lucene/core/src/java/org/apache/lucene/codecs/Codec.java
index 2c37fbf..0613441 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/Codec.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/Codec.java
@@ -108,8 +108,8 @@ public abstract class Codec implements NamedSPILoader.NamedSPI {
   /** Encodes/decodes compound files */
   public abstract CompoundFormat compoundFormat();
 
-  /** Encodes/decodes dimensional index */
-  public abstract DimensionalFormat dimensionalFormat();
+  /** Encodes/decodes points index */
+  public abstract PointFormat pointFormat();
   
   /** looks up a codec by name */
   public static Codec forName(String name) {
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/DimensionalFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/DimensionalFormat.java
deleted file mode 100644
index f69b088..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/DimensionalFormat.java
+++ /dev/null
@@ -1,101 +0,0 @@
-package org.apache.lucene.codecs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-
-/** 
- * Encodes/decodes indexed dimensional data.
- *
- * @lucene.experimental */
-public abstract class DimensionalFormat {
-
-  /**
-   * Creates a new dimensional format.
-   */
-  protected DimensionalFormat() {
-  }
-
-  /** Writes a new segment */
-  public abstract DimensionalWriter fieldsWriter(SegmentWriteState state) throws IOException;
-
-  /** Reads a segment.  NOTE: by the time this call
-   *  returns, it must hold open any files it will need to
-   *  use; else, those files may be deleted. 
-   *  Additionally, required files may be deleted during the execution of 
-   *  this call before there is a chance to open them. Under these 
-   *  circumstances an IOException should be thrown by the implementation. 
-   *  IOExceptions are expected and will automatically cause a retry of the 
-   *  segment opening logic with the newly revised segments.
-   *  */
-  public abstract DimensionalReader fieldsReader(SegmentReadState state) throws IOException;
-
-  /** A {@code DimensionalFormat} that has nothing indexed */
-  public static final DimensionalFormat EMPTY = new DimensionalFormat() {
-      @Override
-      public DimensionalWriter fieldsWriter(SegmentWriteState state) {
-        throw new UnsupportedOperationException();
-      }
-
-      @Override
-      public DimensionalReader fieldsReader(SegmentReadState state) {
-        return new DimensionalReader() {
-          @Override
-          public void close() {
-          }
-
-          @Override
-          public long ramBytesUsed() {
-            return 0L;
-          }
-
-          @Override
-          public void checkIntegrity() {
-          }
-
-          @Override
-          public void intersect(String fieldName, IntersectVisitor visitor) {
-            throw new IllegalArgumentException("field=\"" + fieldName + "\" was not indexed with dimensional values");
-          }
-
-          @Override
-          public byte[] getMinPackedValue(String fieldName) {
-            throw new IllegalArgumentException("field=\"" + fieldName + "\" was not indexed with dimensional values");
-          }
-
-          @Override
-          public byte[] getMaxPackedValue(String fieldName) {
-            throw new IllegalArgumentException("field=\"" + fieldName + "\" was not indexed with dimensional values");
-          }
-
-          @Override
-          public int getNumDimensions(String fieldName) {
-            throw new IllegalArgumentException("field=\"" + fieldName + "\" was not indexed with dimensional values");
-          }
-
-          @Override
-          public int getBytesPerDimension(String fieldName) {
-            throw new IllegalArgumentException("field=\"" + fieldName + "\" was not indexed with dimensional values");
-          }
-        };
-      }
-    };
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/DimensionalReader.java b/lucene/core/src/java/org/apache/lucene/codecs/DimensionalReader.java
deleted file mode 100644
index 7d6eb3c..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/DimensionalReader.java
+++ /dev/null
@@ -1,51 +0,0 @@
-package org.apache.lucene.codecs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable;
-import java.io.IOException;
-
-import org.apache.lucene.index.DimensionalValues;
-import org.apache.lucene.util.Accountable;
-
-/** Abstract API to visit dimensional values.
- *
- * @lucene.experimental
- */
-public abstract class DimensionalReader extends DimensionalValues implements Closeable, Accountable {
-
-  /** Sole constructor. (For invocation by subclass constructors, typically implicit.) */
-  protected DimensionalReader() {}
-
-  /** 
-   * Checks consistency of this reader.
-   * <p>
-   * Note that this may be costly in terms of I/O, e.g. 
-   * may involve computing a checksum value against large data files.
-   * @lucene.internal
-   */
-  public abstract void checkIntegrity() throws IOException;
-
-  /** 
-   * Returns an instance optimized for merging.
-   * <p>
-   * The default implementation returns {@code this} */
-  public DimensionalReader getMergeInstance() throws IOException {
-    return this;
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/DimensionalWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/DimensionalWriter.java
deleted file mode 100644
index 32a80ca..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/DimensionalWriter.java
+++ /dev/null
@@ -1,130 +0,0 @@
-package org.apache.lucene.codecs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable;
-import java.io.IOException;
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.MergeState;
-
-/** Abstract API to write dimensional values
- *
- * @lucene.experimental
- */
-
-public abstract class DimensionalWriter implements Closeable {
-  /** Sole constructor. (For invocation by subclass 
-   *  constructors, typically implicit.) */
-  protected DimensionalWriter() {
-  }
-
-  /** Write all values contained in the provided reader */
-  public abstract void writeField(FieldInfo fieldInfo, DimensionalReader values) throws IOException;
-
-  /** Default naive merge implemenation for one field: it just re-indexes all the values
-   *  from the incoming segment.  The default codec overrides this for 1D fields and uses
-   *  a faster but more complex implementation. */
-  protected void mergeOneField(MergeState mergeState, FieldInfo fieldInfo) throws IOException {
-    writeField(fieldInfo,
-               new DimensionalReader() {
-                 @Override
-                 public void intersect(String fieldName, IntersectVisitor mergedVisitor) throws IOException {
-                   if (fieldName.equals(fieldInfo.name) == false) {
-                     throw new IllegalArgumentException("field name must match the field being merged");
-                   }
-                   for (int i=0;i<mergeState.dimensionalReaders.length;i++) {
-                     DimensionalReader dimensionalReader = mergeState.dimensionalReaders[i];
-                     if (dimensionalReader == null) {
-                       // This segment has no dimensional values
-                       continue;
-                     }
-                     MergeState.DocMap docMap = mergeState.docMaps[i];
-                     int docBase = mergeState.docBase[i];
-                     dimensionalReader.intersect(fieldInfo.name,
-                                                 new IntersectVisitor() {
-                                                   @Override
-                                                   public void visit(int docID) {
-                                                     // Should never be called because our compare method never returns Relation.CELL_INSIDE_QUERY
-                                                     throw new IllegalStateException();
-                                                   }
-
-                                                   @Override
-                                                   public void visit(int docID, byte[] packedValue) throws IOException {
-                                                     int newDocID = docMap.get(docID);
-                                                     if (newDocID != -1) {
-                                                       // Not deleted:
-                                                       mergedVisitor.visit(docBase + newDocID, packedValue);
-                                                     }
-                                                   }
-
-                                                   @Override
-                                                   public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {
-                                                     // Forces this segment's DimensionalReader to always visit all docs + values:
-                                                     return Relation.CELL_CROSSES_QUERY;
-                                                   }
-                                                 });
-                   }
-                 }
-
-                 @Override
-                 public void checkIntegrity() {
-                   throw new UnsupportedOperationException();
-                 }
-
-                 @Override
-                 public long ramBytesUsed() {
-                   return 0L;
-                 }
-
-                 @Override
-                 public void close() {
-                 }
-
-                 @Override
-                 public byte[] getMinPackedValue(String fieldName) {
-                   throw new UnsupportedOperationException();
-                 }
-
-                 @Override
-                 public byte[] getMaxPackedValue(String fieldName) {
-                   throw new UnsupportedOperationException();
-                 }
-
-                 @Override
-                 public int getNumDimensions(String fieldName) {
-                   throw new UnsupportedOperationException();
-                 }
-
-                 @Override
-                 public int getBytesPerDimension(String fieldName) {
-                   throw new UnsupportedOperationException();
-                 }
-               });
-  }
-
-  /** Default merge implementation to merge incoming dimensional readers by visiting all their points and
-   *  adding to this writer */
-  public void merge(MergeState mergeState) throws IOException {
-    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {
-      if (fieldInfo.getDimensionCount() != 0) {
-        mergeOneField(mergeState, fieldInfo);
-      }
-    }
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/FilterCodec.java b/lucene/core/src/java/org/apache/lucene/codecs/FilterCodec.java
index 3465450..f188d50 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/FilterCodec.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/FilterCodec.java
@@ -105,7 +105,7 @@ public abstract class FilterCodec extends Codec {
   }
 
   @Override
-  public DimensionalFormat dimensionalFormat() {
-    return delegate.dimensionalFormat();
+  public PointFormat pointFormat() {
+    return delegate.pointFormat();
   }
 }
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/PointFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/PointFormat.java
new file mode 100644
index 0000000..953bc4d
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/PointFormat.java
@@ -0,0 +1,101 @@
+package org.apache.lucene.codecs;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+
+/** 
+ * Encodes/decodes indexed points.
+ *
+ * @lucene.experimental */
+public abstract class PointFormat {
+
+  /**
+   * Creates a new point format.
+   */
+  protected PointFormat() {
+  }
+
+  /** Writes a new segment */
+  public abstract PointWriter fieldsWriter(SegmentWriteState state) throws IOException;
+
+  /** Reads a segment.  NOTE: by the time this call
+   *  returns, it must hold open any files it will need to
+   *  use; else, those files may be deleted. 
+   *  Additionally, required files may be deleted during the execution of 
+   *  this call before there is a chance to open them. Under these 
+   *  circumstances an IOException should be thrown by the implementation. 
+   *  IOExceptions are expected and will automatically cause a retry of the 
+   *  segment opening logic with the newly revised segments.
+   *  */
+  public abstract PointReader fieldsReader(SegmentReadState state) throws IOException;
+
+  /** A {@code PointFormat} that has nothing indexed */
+  public static final PointFormat EMPTY = new PointFormat() {
+      @Override
+      public PointWriter fieldsWriter(SegmentWriteState state) {
+        throw new UnsupportedOperationException();
+      }
+
+      @Override
+      public PointReader fieldsReader(SegmentReadState state) {
+        return new PointReader() {
+          @Override
+          public void close() {
+          }
+
+          @Override
+          public long ramBytesUsed() {
+            return 0L;
+          }
+
+          @Override
+          public void checkIntegrity() {
+          }
+
+          @Override
+          public void intersect(String fieldName, IntersectVisitor visitor) {
+            throw new IllegalArgumentException("field=\"" + fieldName + "\" was not indexed with points");
+          }
+
+          @Override
+          public byte[] getMinPackedValue(String fieldName) {
+            throw new IllegalArgumentException("field=\"" + fieldName + "\" was not indexed with points");
+          }
+
+          @Override
+          public byte[] getMaxPackedValue(String fieldName) {
+            throw new IllegalArgumentException("field=\"" + fieldName + "\" was not indexed with points");
+          }
+
+          @Override
+          public int getNumDimensions(String fieldName) {
+            throw new IllegalArgumentException("field=\"" + fieldName + "\" was not indexed with points");
+          }
+
+          @Override
+          public int getBytesPerDimension(String fieldName) {
+            throw new IllegalArgumentException("field=\"" + fieldName + "\" was not indexed with points");
+          }
+        };
+      }
+    };
+}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/PointReader.java b/lucene/core/src/java/org/apache/lucene/codecs/PointReader.java
new file mode 100644
index 0000000..aa2553f
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/PointReader.java
@@ -0,0 +1,51 @@
+package org.apache.lucene.codecs;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable;
+import java.io.IOException;
+
+import org.apache.lucene.index.PointValues;
+import org.apache.lucene.util.Accountable;
+
+/** Abstract API to visit point values.
+ *
+ * @lucene.experimental
+ */
+public abstract class PointReader extends PointValues implements Closeable, Accountable {
+
+  /** Sole constructor. (For invocation by subclass constructors, typically implicit.) */
+  protected PointReader() {}
+
+  /** 
+   * Checks consistency of this reader.
+   * <p>
+   * Note that this may be costly in terms of I/O, e.g. 
+   * may involve computing a checksum value against large data files.
+   * @lucene.internal
+   */
+  public abstract void checkIntegrity() throws IOException;
+
+  /** 
+   * Returns an instance optimized for merging.
+   * <p>
+   * The default implementation returns {@code this} */
+  public PointReader getMergeInstance() throws IOException {
+    return this;
+  }
+}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/PointWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/PointWriter.java
new file mode 100644
index 0000000..8e94601
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/PointWriter.java
@@ -0,0 +1,130 @@
+package org.apache.lucene.codecs;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable;
+import java.io.IOException;
+
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.MergeState;
+
+/** Abstract API to write points
+ *
+ * @lucene.experimental
+ */
+
+public abstract class PointWriter implements Closeable {
+  /** Sole constructor. (For invocation by subclass 
+   *  constructors, typically implicit.) */
+  protected PointWriter() {
+  }
+
+  /** Write all values contained in the provided reader */
+  public abstract void writeField(FieldInfo fieldInfo, PointReader values) throws IOException;
+
+  /** Default naive merge implemenation for one field: it just re-indexes all the values
+   *  from the incoming segment.  The default codec overrides this for 1D fields and uses
+   *  a faster but more complex implementation. */
+  protected void mergeOneField(MergeState mergeState, FieldInfo fieldInfo) throws IOException {
+    writeField(fieldInfo,
+               new PointReader() {
+                 @Override
+                 public void intersect(String fieldName, IntersectVisitor mergedVisitor) throws IOException {
+                   if (fieldName.equals(fieldInfo.name) == false) {
+                     throw new IllegalArgumentException("field name must match the field being merged");
+                   }
+                   for (int i=0;i<mergeState.pointReaders.length;i++) {
+                     PointReader pointReader = mergeState.pointReaders[i];
+                     if (pointReader == null) {
+                       // This segment has no points
+                       continue;
+                     }
+                     MergeState.DocMap docMap = mergeState.docMaps[i];
+                     int docBase = mergeState.docBase[i];
+                     pointReader.intersect(fieldInfo.name,
+                                                 new IntersectVisitor() {
+                                                   @Override
+                                                   public void visit(int docID) {
+                                                     // Should never be called because our compare method never returns Relation.CELL_INSIDE_QUERY
+                                                     throw new IllegalStateException();
+                                                   }
+
+                                                   @Override
+                                                   public void visit(int docID, byte[] packedValue) throws IOException {
+                                                     int newDocID = docMap.get(docID);
+                                                     if (newDocID != -1) {
+                                                       // Not deleted:
+                                                       mergedVisitor.visit(docBase + newDocID, packedValue);
+                                                     }
+                                                   }
+
+                                                   @Override
+                                                   public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {
+                                                     // Forces this segment's PointReader to always visit all docs + values:
+                                                     return Relation.CELL_CROSSES_QUERY;
+                                                   }
+                                                 });
+                   }
+                 }
+
+                 @Override
+                 public void checkIntegrity() {
+                   throw new UnsupportedOperationException();
+                 }
+
+                 @Override
+                 public long ramBytesUsed() {
+                   return 0L;
+                 }
+
+                 @Override
+                 public void close() {
+                 }
+
+                 @Override
+                 public byte[] getMinPackedValue(String fieldName) {
+                   throw new UnsupportedOperationException();
+                 }
+
+                 @Override
+                 public byte[] getMaxPackedValue(String fieldName) {
+                   throw new UnsupportedOperationException();
+                 }
+
+                 @Override
+                 public int getNumDimensions(String fieldName) {
+                   throw new UnsupportedOperationException();
+                 }
+
+                 @Override
+                 public int getBytesPerDimension(String fieldName) {
+                   throw new UnsupportedOperationException();
+                 }
+               });
+  }
+
+  /** Default merge implementation to merge incoming points readers by visiting all their points and
+   *  adding to this writer */
+  public void merge(MergeState mergeState) throws IOException {
+    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {
+      if (fieldInfo.getPointDimensionCount() != 0) {
+        mergeOneField(mergeState, fieldInfo);
+      }
+    }
+  }
+}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60Codec.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60Codec.java
index 57575a9..435c6ad 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60Codec.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60Codec.java
@@ -21,7 +21,7 @@ import java.util.Objects;
 
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.CompoundFormat;
-import org.apache.lucene.codecs.DimensionalFormat;
+import org.apache.lucene.codecs.PointFormat;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.FieldInfosFormat;
 import org.apache.lucene.codecs.FilterCodec;
@@ -129,8 +129,8 @@ public class Lucene60Codec extends Codec {
   }
 
   @Override
-  public final DimensionalFormat dimensionalFormat() {
-    return new Lucene60DimensionalFormat();
+  public final PointFormat pointFormat() {
+    return new Lucene60PointFormat();
   }
 
   /** Returns the postings format that should be used for writing 
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60DimensionalFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60DimensionalFormat.java
deleted file mode 100644
index 00e9672..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60DimensionalFormat.java
+++ /dev/null
@@ -1,107 +0,0 @@
-package org.apache.lucene.codecs.lucene60;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.DimensionalFormat;
-import org.apache.lucene.codecs.DimensionalReader;
-import org.apache.lucene.codecs.DimensionalWriter;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-
-/**
- * Lucene 6.0 dimensional format, which encodes dimensional values in a block KD-tree structure
- * for fast shape intersection filtering. See <a href="https://www.cs.duke.edu/~pankaj/publications/papers/bkd-sstd.pdf">this paper</a> for details.
- *
- * <p>This data structure is written as a series of blocks on disk, with an in-memory perfectly balanced
- * binary tree of split values referencing those blocks at the leaves.
- *
- * <p>The <code>.dim</code> file has both blocks and the index split
- * values, for each field.  The file starts with {@link CodecUtil#writeIndexHeader}.
- *
- * <p>The blocks are written like this:
- *
- * <ul>
- *  <li> count (vInt)
- *  <li> delta-docID (vInt) <sup>count</sup> (delta coded docIDs, in sorted order)
- *  <li> packedValue<sup>count</sup> (the <code>byte[]</code> value of each dimension packed into a single <code>byte[]</code>)
- * </ul>
- *
- * <p>After all blocks for a field are written, then the index is written:
- * <ul>
- *  <li> numDims (vInt)
- *  <li> maxPointsInLeafNode (vInt)
- *  <li> bytesPerDim (vInt)
- *  <li> count (vInt)
- *  <li> byte[bytesPerDim]<sup>count</sup> (packed <code>byte[]</code> all split values)
- *  <li> delta-blockFP (vLong)<sup>count</sup> (delta-coded file pointers to the on-disk leaf blocks))
- * </ul>
- *
- * <p>After all fields blocks + index data are written, {@link CodecUtil#writeFooter} writes the checksum.
- *
- * <p>The <code>.dii</code> file records the file pointer in the <code>.dim</code> file where each field's
- * index data was written.  It starts with {@link CodecUtil#writeIndexHeader}, then has:
- *
- * <ul>
- *   <li> fieldCount (vInt)
- *   <li> (fieldNumber (vInt), fieldFilePointer (vLong))<sup>fieldCount</sup>
- * </ul>
- *
- * <p> After that, {@link CodecUtil#writeFooter} writes the checksum.
- *
- * <p>After all fields blocks + index data are written, {@link CodecUtil#writeFooter} writes the checksum.
-
- * @lucene.experimental
- */
-
-public final class Lucene60DimensionalFormat extends DimensionalFormat {
-
-  static final String CODEC_NAME = "Lucene60DimensionalFormat";
-
-  /**
-   * Filename extension for the leaf blocks
-   */
-  public static final String DATA_EXTENSION = "dim";
-
-  /**
-   * Filename extension for the index per field
-   */
-  public static final String INDEX_EXTENSION = "dii";
-
-  static final int DATA_VERSION_START = 0;
-  static final int DATA_VERSION_CURRENT = DATA_VERSION_START;
-
-  static final int INDEX_VERSION_START = 0;
-  static final int INDEX_VERSION_CURRENT = INDEX_VERSION_START;
-
-  /** Sole constructor */
-  public Lucene60DimensionalFormat() {
-  }
-
-  @Override
-  public DimensionalWriter fieldsWriter(SegmentWriteState state) throws IOException {
-    return new Lucene60DimensionalWriter(state);
-  }
-
-  @Override
-  public DimensionalReader fieldsReader(SegmentReadState state) throws IOException {
-    return new Lucene60DimensionalReader(state);
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60DimensionalReader.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60DimensionalReader.java
deleted file mode 100644
index c940e17..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60DimensionalReader.java
+++ /dev/null
@@ -1,189 +0,0 @@
-package org.apache.lucene.codecs.lucene60;
-
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.DimensionalReader;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.store.ChecksumIndexInput;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.Accountable;
-import org.apache.lucene.util.Accountables;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.bkd.BKDReader;
-
-/** Reads dimensional values previously written with {@link Lucene60DimensionalWriter} */
-public class Lucene60DimensionalReader extends DimensionalReader implements Closeable {
-  final IndexInput dataIn;
-  final SegmentReadState readState;
-  final Map<Integer,BKDReader> readers = new HashMap<>();
-
-  /** Sole constructor */
-  public Lucene60DimensionalReader(SegmentReadState readState) throws IOException {
-    this.readState = readState;
-    String dataFileName = IndexFileNames.segmentFileName(readState.segmentInfo.name,
-                                                         readState.segmentSuffix,
-                                                         Lucene60DimensionalFormat.DATA_EXTENSION);
-    dataIn = readState.directory.openInput(dataFileName, readState.context);
-    String indexFileName = IndexFileNames.segmentFileName(readState.segmentInfo.name,
-                                                          readState.segmentSuffix,
-                                                          Lucene60DimensionalFormat.INDEX_EXTENSION);
-
-    boolean success = false;
-
-    // Read index file
-    try (ChecksumIndexInput indexIn = readState.directory.openChecksumInput(indexFileName, readState.context)) {
-      CodecUtil.checkIndexHeader(indexIn,
-                                 Lucene60DimensionalFormat.CODEC_NAME,
-                                 Lucene60DimensionalFormat.INDEX_VERSION_START,
-                                 Lucene60DimensionalFormat.INDEX_VERSION_START,
-                                 readState.segmentInfo.getId(),
-                                 readState.segmentSuffix);
-      int count = indexIn.readVInt();
-      for(int i=0;i<count;i++) {
-        int fieldNumber = indexIn.readVInt();
-        long fp = indexIn.readVLong();
-        dataIn.seek(fp);
-        BKDReader reader = new BKDReader(dataIn);
-        readers.put(fieldNumber, reader);
-        //reader.verify(readState.segmentInfo.maxDoc());
-      }
-      CodecUtil.checkFooter(indexIn);
-      success = true;
-    } finally {
-      if (success == false) {
-        IOUtils.closeWhileHandlingException(dataIn);
-      }
-    }
-  }
-
-  private BKDReader getBKDReader(String fieldName) {
-    FieldInfo fieldInfo = readState.fieldInfos.fieldInfo(fieldName);
-    if (fieldInfo == null) {
-      throw new IllegalArgumentException("field=\"" + fieldName + "\" is unrecognized");
-    }
-    if (fieldInfo.getDimensionCount() == 0) {
-      throw new IllegalArgumentException("field=\"" + fieldName + "\" did not index dimensional values");
-    }
-
-    return readers.get(fieldInfo.number);
-  }
-
-  @Override
-  public void intersect(String fieldName, IntersectVisitor visitor) throws IOException {
-    BKDReader bkdReader = getBKDReader(fieldName);
-
-    if (bkdReader == null) {
-      // Schema ghost corner case!  This field did index dimensional values in the past, but
-      // now all docs having this dimensional field were deleted in this segment:
-      return;
-    }
-
-    bkdReader.intersect(visitor);
-  }
-
-  @Override
-  public long ramBytesUsed() {
-    long sizeInBytes = 0;
-    for(BKDReader reader : readers.values()) {
-      sizeInBytes += reader.ramBytesUsed();
-    }
-    return sizeInBytes;
-  }
-
-  @Override
-  public Collection<Accountable> getChildResources() {
-    List<Accountable> resources = new ArrayList<>();
-    for(Map.Entry<Integer,BKDReader> ent : readers.entrySet()) {
-      resources.add(Accountables.namedAccountable(readState.fieldInfos.fieldInfo(ent.getKey()).name,
-                                                  ent.getValue()));
-    }
-    return Collections.unmodifiableList(resources);
-  }
-
-  @Override
-  public void checkIntegrity() throws IOException {
-    CodecUtil.checksumEntireFile(dataIn);
-  }
-
-  @Override
-  public void close() throws IOException {
-    dataIn.close();
-    // Free up heap:
-    readers.clear();
-  }
-
-  @Override
-  public byte[] getMinPackedValue(String fieldName) {
-    BKDReader bkdReader = getBKDReader(fieldName);
-    if (bkdReader == null) {
-      // Schema ghost corner case!  This field did index dimensional values in the past, but
-      // now all docs having this dimensional field were deleted in this segment:
-      return null;
-    }
-
-    return bkdReader.getMinPackedValue();
-  }
-
-  @Override
-  public byte[] getMaxPackedValue(String fieldName) {
-    BKDReader bkdReader = getBKDReader(fieldName);
-    if (bkdReader == null) {
-      // Schema ghost corner case!  This field did index dimensional values in the past, but
-      // now all docs having this dimensional field were deleted in this segment:
-      return null;
-    }
-
-    return bkdReader.getMaxPackedValue();
-  }
-
-  @Override
-  public int getNumDimensions(String fieldName) {
-    BKDReader bkdReader = getBKDReader(fieldName);
-    if (bkdReader == null) {
-      // Schema ghost corner case!  This field did index dimensional values in the past, but
-      // now all docs having this dimensional field were deleted in this segment:
-      return 0;
-    }
-    return bkdReader.getNumDimensions();
-  }
-
-  @Override
-  public int getBytesPerDimension(String fieldName) {
-    BKDReader bkdReader = getBKDReader(fieldName);
-    if (bkdReader == null) {
-      // Schema ghost corner case!  This field did index dimensional values in the past, but
-      // now all docs having this dimensional field were deleted in this segment:
-      return 0;
-    }
-    return bkdReader.getBytesPerDimension();
-  }
-}
-  
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60DimensionalWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60DimensionalWriter.java
deleted file mode 100644
index 58e9aa8..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60DimensionalWriter.java
+++ /dev/null
@@ -1,205 +0,0 @@
-package org.apache.lucene.codecs.lucene60;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.DimensionalReader;
-import org.apache.lucene.codecs.DimensionalWriter;
-import org.apache.lucene.index.DimensionalValues.IntersectVisitor;
-import org.apache.lucene.index.DimensionalValues.Relation;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.MergeState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.bkd.BKDReader;
-import org.apache.lucene.util.bkd.BKDWriter;
-
-/** Writes dimensional values */
-public class Lucene60DimensionalWriter extends DimensionalWriter implements Closeable {
-  
-  final IndexOutput dataOut;
-  final Map<String,Long> indexFPs = new HashMap<>();
-  final SegmentWriteState writeState;
-  final int maxPointsInLeafNode;
-  final double maxMBSortInHeap;
-  private boolean closed;
-
-  /** Full constructor */
-  public Lucene60DimensionalWriter(SegmentWriteState writeState, int maxPointsInLeafNode, double maxMBSortInHeap) throws IOException {
-    assert writeState.fieldInfos.hasDimensionalValues();
-    this.writeState = writeState;
-    this.maxPointsInLeafNode = maxPointsInLeafNode;
-    this.maxMBSortInHeap = maxMBSortInHeap;
-    String dataFileName = IndexFileNames.segmentFileName(writeState.segmentInfo.name,
-                                                         writeState.segmentSuffix,
-                                                         Lucene60DimensionalFormat.DATA_EXTENSION);
-    dataOut = writeState.directory.createOutput(dataFileName, writeState.context);
-    boolean success = false;
-    try {
-      CodecUtil.writeIndexHeader(dataOut,
-                                 Lucene60DimensionalFormat.CODEC_NAME,
-                                 Lucene60DimensionalFormat.DATA_VERSION_CURRENT,
-                                 writeState.segmentInfo.getId(),
-                                 writeState.segmentSuffix);
-      success = true;
-    } finally {
-      if (success == false) {
-        IOUtils.closeWhileHandlingException(dataOut);
-      }
-    }
-  }
-
-  /** Uses the defaults values for {@code maxPointsInLeafNode} (1024) and {@code maxMBSortInHeap} (16.0) */
-  public Lucene60DimensionalWriter(SegmentWriteState writeState) throws IOException {
-    this(writeState, BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE, BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP);
-  }
-
-  @Override
-  public void writeField(FieldInfo fieldInfo, DimensionalReader values) throws IOException {
-
-    try (BKDWriter writer = new BKDWriter(writeState.directory,
-                                          writeState.segmentInfo.name,
-                                          fieldInfo.getDimensionCount(),
-                                          fieldInfo.getDimensionNumBytes(),
-                                          maxPointsInLeafNode,
-                                          maxMBSortInHeap)) {
-
-      values.intersect(fieldInfo.name, new IntersectVisitor() {
-          @Override
-          public void visit(int docID) {
-            throw new IllegalStateException();
-          }
-
-          public void visit(int docID, byte[] packedValue) throws IOException {
-            writer.add(packedValue, docID);
-          }
-
-          @Override
-          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {
-            return Relation.CELL_CROSSES_QUERY;
-          }
-        });
-
-      // We could have 0 points on merge since all docs with dimensional fields may be deleted:
-      if (writer.getPointCount() > 0) {
-        indexFPs.put(fieldInfo.name, writer.finish(dataOut));
-      }
-    }
-  }
-
-  @Override
-  public void merge(MergeState mergeState) throws IOException {
-    for(DimensionalReader reader : mergeState.dimensionalReaders) {
-      if (reader instanceof Lucene60DimensionalReader == false) {
-        // We can only bulk merge when all to-be-merged segments use our format:
-        super.merge(mergeState);
-        return;
-      }
-    }
-
-    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {
-      if (fieldInfo.getDimensionCount() != 0) {
-        if (fieldInfo.getDimensionCount() == 1) {
-          //System.out.println("MERGE: field=" + fieldInfo.name);
-          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the
-          // already sorted incoming segments, instead of trying to sort all points again as if
-          // we were simply reindexing them:
-          try (BKDWriter writer = new BKDWriter(writeState.directory,
-                                                writeState.segmentInfo.name,
-                                                fieldInfo.getDimensionCount(),
-                                                fieldInfo.getDimensionNumBytes(),
-                                                maxPointsInLeafNode,
-                                                maxMBSortInHeap)) {
-            List<BKDReader> bkdReaders = new ArrayList<>();
-            List<MergeState.DocMap> docMaps = new ArrayList<>();
-            List<Integer> docIDBases = new ArrayList<>();
-            for(int i=0;i<mergeState.dimensionalReaders.length;i++) {
-              DimensionalReader reader = mergeState.dimensionalReaders[i];
-
-              Lucene60DimensionalReader reader60 = (Lucene60DimensionalReader) reader;
-              if (reader60 != null) {
-                // TODO: I could just use the merged fieldInfo.number instead of resolving to this
-                // reader's FieldInfo, right?  Field numbers are always consistent across segments,
-                // since when?
-                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];
-                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);
-                if (readerFieldInfo != null) {
-                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);
-                  if (bkdReader != null) {
-                    docIDBases.add(mergeState.docBase[i]);
-                    bkdReaders.add(bkdReader);
-                    docMaps.add(mergeState.docMaps[i]);
-                  }
-                }
-              }
-            }
-
-            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);
-            if (fp != -1) {
-              indexFPs.put(fieldInfo.name, fp);
-            }
-          }
-        } else {
-          mergeOneField(mergeState, fieldInfo);
-        }
-      }
-    } 
-  }  
-
-  @Override
-  public void close() throws IOException {
-    if (closed == false) {
-      CodecUtil.writeFooter(dataOut);
-      dataOut.close();
-      closed = true;
-
-      String indexFileName = IndexFileNames.segmentFileName(writeState.segmentInfo.name,
-                                                            writeState.segmentSuffix,
-                                                            Lucene60DimensionalFormat.INDEX_EXTENSION);
-      // Write index file
-      try (IndexOutput indexOut = writeState.directory.createOutput(indexFileName, writeState.context)) {
-        CodecUtil.writeIndexHeader(indexOut,
-                                   Lucene60DimensionalFormat.CODEC_NAME,
-                                   Lucene60DimensionalFormat.INDEX_VERSION_CURRENT,
-                                   writeState.segmentInfo.getId(),
-                                   writeState.segmentSuffix);
-        int count = indexFPs.size();
-        indexOut.writeVInt(count);
-        for(Map.Entry<String,Long> ent : indexFPs.entrySet()) {
-          FieldInfo fieldInfo = writeState.fieldInfos.fieldInfo(ent.getKey());
-          if (fieldInfo == null) {
-            throw new IllegalStateException("wrote field=\"" + ent.getKey() + "\" but that field doesn't exist in FieldInfos");
-          }
-          indexOut.writeVInt(fieldInfo.number);
-          indexOut.writeVLong(ent.getValue());
-        }
-        CodecUtil.writeFooter(indexOut);
-      }
-    }
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60FieldInfosFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60FieldInfosFormat.java
index 0c105f8..a26883f 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60FieldInfosFormat.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60FieldInfosFormat.java
@@ -24,7 +24,6 @@ import java.util.Map;
 import org.apache.lucene.codecs.CodecUtil;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.document.DimensionalLongField;
 import org.apache.lucene.index.CorruptIndexException;
 import org.apache.lucene.index.DocValuesType;
 import org.apache.lucene.index.FieldInfo;
@@ -96,8 +95,8 @@ import org.apache.lucene.store.IndexOutput;
  *       there are no DocValues updates to that field. Anything above zero means there 
  *       are updates stored by {@link DocValuesFormat}.</li>
  *   <li>Attributes: a key-value map of codec-private attributes.</li>
- *   <li>DimensionCount, DimensionNumBytes: these are non-zero only if the field is
- *       indexed dimensionally, e.g. using {@link DimensionalLongField}</li>
+ *   <li>PointDimensionCount, PointNumBytes: these are non-zero only if the field is
+ *       indexed as points, e.g. using {@link org.apache.lucene.document.LongPoint}</li>
  * </ul>
  *
  * @lucene.experimental
@@ -149,18 +148,18 @@ public final class Lucene60FieldInfosFormat extends FieldInfosFormat {
             attributes = lastAttributes;
           }
           lastAttributes = attributes;
-          int dimensionCount = input.readVInt();
-          int dimensionNumBytes;
-          if (dimensionCount != 0) {
-            dimensionNumBytes = input.readVInt();
+          int pointDimensionCount = input.readVInt();
+          int pointNumBytes;
+          if (pointDimensionCount != 0) {
+            pointNumBytes = input.readVInt();
           } else {
-            dimensionNumBytes = 0;
+            pointNumBytes = 0;
           }
 
           try {
             infos[i] = new FieldInfo(name, fieldNumber, storeTermVector, omitNorms, storePayloads, 
                                      indexOptions, docValuesType, dvGen, attributes,
-                                     dimensionCount, dimensionNumBytes);
+                                     pointDimensionCount, pointNumBytes);
             infos[i].checkConsistency();
           } catch (IllegalStateException e) {
             throw new CorruptIndexException("invalid fieldinfo for field: " + name + ", fieldNumber=" + fieldNumber, input, e);
@@ -286,10 +285,10 @@ public final class Lucene60FieldInfosFormat extends FieldInfosFormat {
         output.writeByte(docValuesByte(fi.getDocValuesType()));
         output.writeLong(fi.getDocValuesGen());
         output.writeMapOfStrings(fi.attributes());
-        int dimensionCount = fi.getDimensionCount();
-        output.writeVInt(dimensionCount);
-        if (dimensionCount != 0) {
-          output.writeVInt(fi.getDimensionNumBytes());
+        int pointDimensionCount = fi.getPointDimensionCount();
+        output.writeVInt(pointDimensionCount);
+        if (pointDimensionCount != 0) {
+          output.writeVInt(fi.getPointNumBytes());
         }
       }
       CodecUtil.writeFooter(output);
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointFormat.java
new file mode 100644
index 0000000..61ce8fb
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointFormat.java
@@ -0,0 +1,107 @@
+package org.apache.lucene.codecs.lucene60;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.PointFormat;
+import org.apache.lucene.codecs.PointReader;
+import org.apache.lucene.codecs.PointWriter;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+
+/**
+ * Lucene 6.0 point format, which encodes dimensional values in a block KD-tree structure
+ * for fast shape intersection filtering. See <a href="https://www.cs.duke.edu/~pankaj/publications/papers/bkd-sstd.pdf">this paper</a> for details.
+ *
+ * <p>This data structure is written as a series of blocks on disk, with an in-memory perfectly balanced
+ * binary tree of split values referencing those blocks at the leaves.
+ *
+ * <p>The <code>.dim</code> file has both blocks and the index split
+ * values, for each field.  The file starts with {@link CodecUtil#writeIndexHeader}.
+ *
+ * <p>The blocks are written like this:
+ *
+ * <ul>
+ *  <li> count (vInt)
+ *  <li> delta-docID (vInt) <sup>count</sup> (delta coded docIDs, in sorted order)
+ *  <li> packedValue<sup>count</sup> (the <code>byte[]</code> value of each dimension packed into a single <code>byte[]</code>)
+ * </ul>
+ *
+ * <p>After all blocks for a field are written, then the index is written:
+ * <ul>
+ *  <li> numDims (vInt)
+ *  <li> maxPointsInLeafNode (vInt)
+ *  <li> bytesPerDim (vInt)
+ *  <li> count (vInt)
+ *  <li> byte[bytesPerDim]<sup>count</sup> (packed <code>byte[]</code> all split values)
+ *  <li> delta-blockFP (vLong)<sup>count</sup> (delta-coded file pointers to the on-disk leaf blocks))
+ * </ul>
+ *
+ * <p>After all fields blocks + index data are written, {@link CodecUtil#writeFooter} writes the checksum.
+ *
+ * <p>The <code>.dii</code> file records the file pointer in the <code>.dim</code> file where each field's
+ * index data was written.  It starts with {@link CodecUtil#writeIndexHeader}, then has:
+ *
+ * <ul>
+ *   <li> fieldCount (vInt)
+ *   <li> (fieldNumber (vInt), fieldFilePointer (vLong))<sup>fieldCount</sup>
+ * </ul>
+ *
+ * <p> After that, {@link CodecUtil#writeFooter} writes the checksum.
+ *
+ * <p>After all fields blocks + index data are written, {@link CodecUtil#writeFooter} writes the checksum.
+
+ * @lucene.experimental
+ */
+
+public final class Lucene60PointFormat extends PointFormat {
+
+  static final String CODEC_NAME = "Lucene60PointFormat";
+
+  /**
+   * Filename extension for the leaf blocks
+   */
+  public static final String DATA_EXTENSION = "dim";
+
+  /**
+   * Filename extension for the index per field
+   */
+  public static final String INDEX_EXTENSION = "dii";
+
+  static final int DATA_VERSION_START = 0;
+  static final int DATA_VERSION_CURRENT = DATA_VERSION_START;
+
+  static final int INDEX_VERSION_START = 0;
+  static final int INDEX_VERSION_CURRENT = INDEX_VERSION_START;
+
+  /** Sole constructor */
+  public Lucene60PointFormat() {
+  }
+
+  @Override
+  public PointWriter fieldsWriter(SegmentWriteState state) throws IOException {
+    return new Lucene60PointWriter(state);
+  }
+
+  @Override
+  public PointReader fieldsReader(SegmentReadState state) throws IOException {
+    return new Lucene60PointReader(state);
+  }
+}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointReader.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointReader.java
new file mode 100644
index 0000000..2e2bddb
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointReader.java
@@ -0,0 +1,189 @@
+package org.apache.lucene.codecs.lucene60;
+
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.PointReader;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.store.ChecksumIndexInput;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.Accountable;
+import org.apache.lucene.util.Accountables;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.bkd.BKDReader;
+
+/** Reads point values previously written with {@link Lucene60PointWriter} */
+public class Lucene60PointReader extends PointReader implements Closeable {
+  final IndexInput dataIn;
+  final SegmentReadState readState;
+  final Map<Integer,BKDReader> readers = new HashMap<>();
+
+  /** Sole constructor */
+  public Lucene60PointReader(SegmentReadState readState) throws IOException {
+    this.readState = readState;
+    String dataFileName = IndexFileNames.segmentFileName(readState.segmentInfo.name,
+                                                         readState.segmentSuffix,
+                                                         Lucene60PointFormat.DATA_EXTENSION);
+    dataIn = readState.directory.openInput(dataFileName, readState.context);
+    String indexFileName = IndexFileNames.segmentFileName(readState.segmentInfo.name,
+                                                          readState.segmentSuffix,
+                                                          Lucene60PointFormat.INDEX_EXTENSION);
+
+    boolean success = false;
+
+    // Read index file
+    try (ChecksumIndexInput indexIn = readState.directory.openChecksumInput(indexFileName, readState.context)) {
+      CodecUtil.checkIndexHeader(indexIn,
+                                 Lucene60PointFormat.CODEC_NAME,
+                                 Lucene60PointFormat.INDEX_VERSION_START,
+                                 Lucene60PointFormat.INDEX_VERSION_START,
+                                 readState.segmentInfo.getId(),
+                                 readState.segmentSuffix);
+      int count = indexIn.readVInt();
+      for(int i=0;i<count;i++) {
+        int fieldNumber = indexIn.readVInt();
+        long fp = indexIn.readVLong();
+        dataIn.seek(fp);
+        BKDReader reader = new BKDReader(dataIn);
+        readers.put(fieldNumber, reader);
+        //reader.verify(readState.segmentInfo.maxDoc());
+      }
+      CodecUtil.checkFooter(indexIn);
+      success = true;
+    } finally {
+      if (success == false) {
+        IOUtils.closeWhileHandlingException(dataIn);
+      }
+    }
+  }
+
+  private BKDReader getBKDReader(String fieldName) {
+    FieldInfo fieldInfo = readState.fieldInfos.fieldInfo(fieldName);
+    if (fieldInfo == null) {
+      throw new IllegalArgumentException("field=\"" + fieldName + "\" is unrecognized");
+    }
+    if (fieldInfo.getPointDimensionCount() == 0) {
+      throw new IllegalArgumentException("field=\"" + fieldName + "\" did not index point values");
+    }
+
+    return readers.get(fieldInfo.number);
+  }
+
+  @Override
+  public void intersect(String fieldName, IntersectVisitor visitor) throws IOException {
+    BKDReader bkdReader = getBKDReader(fieldName);
+
+    if (bkdReader == null) {
+      // Schema ghost corner case!  This field did index points in the past, but
+      // now all docs having this point field were deleted in this segment:
+      return;
+    }
+
+    bkdReader.intersect(visitor);
+  }
+
+  @Override
+  public long ramBytesUsed() {
+    long sizeInBytes = 0;
+    for(BKDReader reader : readers.values()) {
+      sizeInBytes += reader.ramBytesUsed();
+    }
+    return sizeInBytes;
+  }
+
+  @Override
+  public Collection<Accountable> getChildResources() {
+    List<Accountable> resources = new ArrayList<>();
+    for(Map.Entry<Integer,BKDReader> ent : readers.entrySet()) {
+      resources.add(Accountables.namedAccountable(readState.fieldInfos.fieldInfo(ent.getKey()).name,
+                                                  ent.getValue()));
+    }
+    return Collections.unmodifiableList(resources);
+  }
+
+  @Override
+  public void checkIntegrity() throws IOException {
+    CodecUtil.checksumEntireFile(dataIn);
+  }
+
+  @Override
+  public void close() throws IOException {
+    dataIn.close();
+    // Free up heap:
+    readers.clear();
+  }
+
+  @Override
+  public byte[] getMinPackedValue(String fieldName) {
+    BKDReader bkdReader = getBKDReader(fieldName);
+    if (bkdReader == null) {
+      // Schema ghost corner case!  This field did index points in the past, but
+      // now all docs having this point field were deleted in this segment:
+      return null;
+    }
+
+    return bkdReader.getMinPackedValue();
+  }
+
+  @Override
+  public byte[] getMaxPackedValue(String fieldName) {
+    BKDReader bkdReader = getBKDReader(fieldName);
+    if (bkdReader == null) {
+      // Schema ghost corner case!  This field did index points in the past, but
+      // now all docs having this point field were deleted in this segment:
+      return null;
+    }
+
+    return bkdReader.getMaxPackedValue();
+  }
+
+  @Override
+  public int getNumDimensions(String fieldName) {
+    BKDReader bkdReader = getBKDReader(fieldName);
+    if (bkdReader == null) {
+      // Schema ghost corner case!  This field did index points in the past, but
+      // now all docs having this point field were deleted in this segment:
+      return 0;
+    }
+    return bkdReader.getNumDimensions();
+  }
+
+  @Override
+  public int getBytesPerDimension(String fieldName) {
+    BKDReader bkdReader = getBKDReader(fieldName);
+    if (bkdReader == null) {
+      // Schema ghost corner case!  This field did index points in the past, but
+      // now all docs having this point field were deleted in this segment:
+      return 0;
+    }
+    return bkdReader.getBytesPerDimension();
+  }
+}
+  
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointWriter.java
new file mode 100644
index 0000000..318d665
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointWriter.java
@@ -0,0 +1,205 @@
+package org.apache.lucene.codecs.lucene60;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.PointReader;
+import org.apache.lucene.codecs.PointWriter;
+import org.apache.lucene.index.PointValues.IntersectVisitor;
+import org.apache.lucene.index.PointValues.Relation;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.MergeState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.bkd.BKDReader;
+import org.apache.lucene.util.bkd.BKDWriter;
+
+/** Writes dimensional values */
+public class Lucene60PointWriter extends PointWriter implements Closeable {
+  
+  final IndexOutput dataOut;
+  final Map<String,Long> indexFPs = new HashMap<>();
+  final SegmentWriteState writeState;
+  final int maxPointsInLeafNode;
+  final double maxMBSortInHeap;
+  private boolean closed;
+
+  /** Full constructor */
+  public Lucene60PointWriter(SegmentWriteState writeState, int maxPointsInLeafNode, double maxMBSortInHeap) throws IOException {
+    assert writeState.fieldInfos.hasPointValues();
+    this.writeState = writeState;
+    this.maxPointsInLeafNode = maxPointsInLeafNode;
+    this.maxMBSortInHeap = maxMBSortInHeap;
+    String dataFileName = IndexFileNames.segmentFileName(writeState.segmentInfo.name,
+                                                         writeState.segmentSuffix,
+                                                         Lucene60PointFormat.DATA_EXTENSION);
+    dataOut = writeState.directory.createOutput(dataFileName, writeState.context);
+    boolean success = false;
+    try {
+      CodecUtil.writeIndexHeader(dataOut,
+                                 Lucene60PointFormat.CODEC_NAME,
+                                 Lucene60PointFormat.DATA_VERSION_CURRENT,
+                                 writeState.segmentInfo.getId(),
+                                 writeState.segmentSuffix);
+      success = true;
+    } finally {
+      if (success == false) {
+        IOUtils.closeWhileHandlingException(dataOut);
+      }
+    }
+  }
+
+  /** Uses the defaults values for {@code maxPointsInLeafNode} (1024) and {@code maxMBSortInHeap} (16.0) */
+  public Lucene60PointWriter(SegmentWriteState writeState) throws IOException {
+    this(writeState, BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE, BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP);
+  }
+
+  @Override
+  public void writeField(FieldInfo fieldInfo, PointReader values) throws IOException {
+
+    try (BKDWriter writer = new BKDWriter(writeState.directory,
+                                          writeState.segmentInfo.name,
+                                          fieldInfo.getPointDimensionCount(),
+                                          fieldInfo.getPointNumBytes(),
+                                          maxPointsInLeafNode,
+                                          maxMBSortInHeap)) {
+
+      values.intersect(fieldInfo.name, new IntersectVisitor() {
+          @Override
+          public void visit(int docID) {
+            throw new IllegalStateException();
+          }
+
+          public void visit(int docID, byte[] packedValue) throws IOException {
+            writer.add(packedValue, docID);
+          }
+
+          @Override
+          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {
+            return Relation.CELL_CROSSES_QUERY;
+          }
+        });
+
+      // We could have 0 points on merge since all docs with dimensional fields may be deleted:
+      if (writer.getPointCount() > 0) {
+        indexFPs.put(fieldInfo.name, writer.finish(dataOut));
+      }
+    }
+  }
+
+  @Override
+  public void merge(MergeState mergeState) throws IOException {
+    for(PointReader reader : mergeState.pointReaders) {
+      if (reader instanceof Lucene60PointReader == false) {
+        // We can only bulk merge when all to-be-merged segments use our format:
+        super.merge(mergeState);
+        return;
+      }
+    }
+
+    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {
+      if (fieldInfo.getPointDimensionCount() != 0) {
+        if (fieldInfo.getPointDimensionCount() == 1) {
+          //System.out.println("MERGE: field=" + fieldInfo.name);
+          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the
+          // already sorted incoming segments, instead of trying to sort all points again as if
+          // we were simply reindexing them:
+          try (BKDWriter writer = new BKDWriter(writeState.directory,
+                                                writeState.segmentInfo.name,
+                                                fieldInfo.getPointDimensionCount(),
+                                                fieldInfo.getPointNumBytes(),
+                                                maxPointsInLeafNode,
+                                                maxMBSortInHeap)) {
+            List<BKDReader> bkdReaders = new ArrayList<>();
+            List<MergeState.DocMap> docMaps = new ArrayList<>();
+            List<Integer> docIDBases = new ArrayList<>();
+            for(int i=0;i<mergeState.pointReaders.length;i++) {
+              PointReader reader = mergeState.pointReaders[i];
+
+              Lucene60PointReader reader60 = (Lucene60PointReader) reader;
+              if (reader60 != null) {
+                // TODO: I could just use the merged fieldInfo.number instead of resolving to this
+                // reader's FieldInfo, right?  Field numbers are always consistent across segments,
+                // since when?
+                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];
+                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);
+                if (readerFieldInfo != null) {
+                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);
+                  if (bkdReader != null) {
+                    docIDBases.add(mergeState.docBase[i]);
+                    bkdReaders.add(bkdReader);
+                    docMaps.add(mergeState.docMaps[i]);
+                  }
+                }
+              }
+            }
+
+            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);
+            if (fp != -1) {
+              indexFPs.put(fieldInfo.name, fp);
+            }
+          }
+        } else {
+          mergeOneField(mergeState, fieldInfo);
+        }
+      }
+    } 
+  }  
+
+  @Override
+  public void close() throws IOException {
+    if (closed == false) {
+      CodecUtil.writeFooter(dataOut);
+      dataOut.close();
+      closed = true;
+
+      String indexFileName = IndexFileNames.segmentFileName(writeState.segmentInfo.name,
+                                                            writeState.segmentSuffix,
+                                                            Lucene60PointFormat.INDEX_EXTENSION);
+      // Write index file
+      try (IndexOutput indexOut = writeState.directory.createOutput(indexFileName, writeState.context)) {
+        CodecUtil.writeIndexHeader(indexOut,
+                                   Lucene60PointFormat.CODEC_NAME,
+                                   Lucene60PointFormat.INDEX_VERSION_CURRENT,
+                                   writeState.segmentInfo.getId(),
+                                   writeState.segmentSuffix);
+        int count = indexFPs.size();
+        indexOut.writeVInt(count);
+        for(Map.Entry<String,Long> ent : indexFPs.entrySet()) {
+          FieldInfo fieldInfo = writeState.fieldInfos.fieldInfo(ent.getKey());
+          if (fieldInfo == null) {
+            throw new IllegalStateException("wrote field=\"" + ent.getKey() + "\" but that field doesn't exist in FieldInfos");
+          }
+          indexOut.writeVInt(fieldInfo.number);
+          indexOut.writeVLong(ent.getValue());
+        }
+        CodecUtil.writeFooter(indexOut);
+      }
+    }
+  }
+}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene60/package-info.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene60/package-info.java
index 73a7967..a52d6f6 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene60/package-info.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene60/package-info.java
@@ -193,7 +193,7 @@
  * An optional file indicating which documents are live.
  * </li>
  * <li>
- * {@link org.apache.lucene.codecs.lucene60.Lucene60DimensionalFormat Dimensional values}. 
+ * {@link org.apache.lucene.codecs.lucene60.Lucene60PointFormat Point values}.
  * Optional pair of files, recording dimesionally indexed fields, to enable fast
  * numeric range filtering and large numeric values like BigInteger and BigDecimal (1D)
  * and geo shape intersection (2D, 3D).
@@ -322,9 +322,9 @@
  * <td>Info about what files are live</td>
  * </tr>
  * <tr>
- * <td>{@link org.apache.lucene.codecs.lucene60.Lucene60DimensionalFormat Dimensional values}</td>
+ * <td>{@link org.apache.lucene.codecs.lucene60.Lucene60PointFormat Point values}</td>
  * <td>.dii, .dim</td>
- * <td>Holds dimensionally indexed fields, if any</td>
+ * <td>Holds indexed points, if any</td>
  * </tr>
  * </table>
  * </div>
diff --git a/lucene/core/src/java/org/apache/lucene/document/BinaryPoint.java b/lucene/core/src/java/org/apache/lucene/document/BinaryPoint.java
new file mode 100644
index 0000000..a74b17c
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/document/BinaryPoint.java
@@ -0,0 +1,110 @@
+package org.apache.lucene.document;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.util.BytesRef;
+
+/** A binary field that is indexed dimensionally such that finding
+ *  all documents within an N-dimensional shape or range at search time is
+ *  efficient.  Muliple values for the same field in one documents
+ *  is allowed. */
+
+public final class BinaryPoint extends Field {
+
+  private static FieldType getType(byte[][] point) {
+    if (point == null) {
+      throw new IllegalArgumentException("point cannot be null");
+    }
+    if (point.length == 0) {
+      throw new IllegalArgumentException("point cannot be 0 dimensions");
+    }
+    int bytesPerDim = -1;
+    for(int i=0;i<point.length;i++) {
+      byte[] oneDim = point[i];
+      if (oneDim == null) {
+        throw new IllegalArgumentException("point cannot have null values");
+      }
+      if (oneDim.length == 0) {
+        throw new IllegalArgumentException("point cannot have 0-length values");
+      }
+      if (bytesPerDim == -1) {
+        bytesPerDim = oneDim.length;
+      } else if (bytesPerDim != oneDim.length) {
+        throw new IllegalArgumentException("all dimensions must have same bytes length; got " + bytesPerDim + " and " + oneDim.length);
+      }
+    }
+    return getType(point.length, bytesPerDim);
+  }
+
+  private static FieldType getType(int numDims, int bytesPerDim) {
+    FieldType type = new FieldType();
+    type.setDimensions(numDims, bytesPerDim);
+    type.freeze();
+    return type;
+  }
+
+  private static BytesRef pack(byte[]... point) {
+    if (point == null) {
+      throw new IllegalArgumentException("point cannot be null");
+    }
+    if (point.length == 0) {
+      throw new IllegalArgumentException("point cannot be 0 dimensions");
+    }
+    if (point.length == 1) {
+      return new BytesRef(point[0]);
+    }
+    int bytesPerDim = -1;
+    for(byte[] dim : point) {
+      if (dim == null) {
+        throw new IllegalArgumentException("point cannot have null values");
+      }
+      if (bytesPerDim == -1) {
+        if (dim.length == 0) {
+          throw new IllegalArgumentException("point cannot have 0-length values");
+        }
+        bytesPerDim = dim.length;
+      } else if (dim.length != bytesPerDim) {
+        throw new IllegalArgumentException("all dimensions must have same bytes length; got " + bytesPerDim + " and " + dim.length);
+      }
+    }
+    byte[] packed = new byte[bytesPerDim*point.length];
+    for(int i=0;i<point.length;i++) {
+      System.arraycopy(point[i], 0, packed, i*bytesPerDim, bytesPerDim);
+    }
+    return new BytesRef(packed);
+  }
+
+  /** General purpose API: creates a new BinaryPoint, indexing the
+   *  provided N-dimensional binary point.
+   *
+   *  @param name field name
+   *  @param point byte[][] value
+   *  @throws IllegalArgumentException if the field name or value is null.
+   */
+  public BinaryPoint(String name, byte[]... point) {
+    super(name, pack(point), getType(point));
+  }
+
+  /** Expert API */
+  public BinaryPoint(String name, byte[] packedPoint, FieldType type) {
+    super(name, packedPoint, type);
+    if (packedPoint.length != type.pointDimensionCount() * type.pointNumBytes()) {
+      throw new IllegalArgumentException("packedPoint is length=" + packedPoint.length + " but type.pointDimensionCount()=" + type.pointDimensionCount() + " and type.pointNumBytes()=" + type.pointNumBytes());
+    }
+  }
+}
diff --git a/lucene/core/src/java/org/apache/lucene/document/DateTools.java b/lucene/core/src/java/org/apache/lucene/document/DateTools.java
index e8f6622..e378ece 100644
--- a/lucene/core/src/java/org/apache/lucene/document/DateTools.java
+++ b/lucene/core/src/java/org/apache/lucene/document/DateTools.java
@@ -24,7 +24,6 @@ import java.util.Date;
 import java.util.Locale;
 import java.util.TimeZone;
 
-import org.apache.lucene.search.DimensionalRangeQuery;
 import org.apache.lucene.search.PrefixQuery;
 import org.apache.lucene.search.TermRangeQuery;
 
@@ -39,12 +38,12 @@ import org.apache.lucene.search.TermRangeQuery;
  * {@link TermRangeQuery} and {@link PrefixQuery} will require more memory and become slower.
  * 
  * <P>
- * Another approach is {@link DimensionalLongField}, which indexes the
+ * Another approach is {@link LongPoint}, which indexes the
  * values in sorted order.
  * For indexing a {@link Date} or {@link Calendar}, just get the unix timestamp as
  * <code>long</code> using {@link Date#getTime} or {@link Calendar#getTimeInMillis} and
- * index this as a numeric value with {@link DimensionalLongField}
- * and use {@link DimensionalRangeQuery} to query it.
+ * index this as a numeric value with {@link LongPoint}
+ * and use {@link org.apache.lucene.search.PointRangeQuery} to query it.
  */
 public class DateTools {
   
diff --git a/lucene/core/src/java/org/apache/lucene/document/DimensionalBinaryField.java b/lucene/core/src/java/org/apache/lucene/document/DimensionalBinaryField.java
deleted file mode 100644
index 718a101..0000000
--- a/lucene/core/src/java/org/apache/lucene/document/DimensionalBinaryField.java
+++ /dev/null
@@ -1,110 +0,0 @@
-package org.apache.lucene.document;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.util.BytesRef;
-
-/** A binary field that is indexed dimensionally such that finding
- *  all documents within an N-dimensional shape or range at search time is
- *  efficient.  Muliple values for the same field in one documents
- *  is allowed. */
-
-public final class DimensionalBinaryField extends Field {
-
-  private static FieldType getType(byte[][] point) {
-    if (point == null) {
-      throw new IllegalArgumentException("point cannot be null");
-    }
-    if (point.length == 0) {
-      throw new IllegalArgumentException("point cannot be 0 dimensions");
-    }
-    int bytesPerDim = -1;
-    for(int i=0;i<point.length;i++) {
-      byte[] oneDim = point[i];
-      if (oneDim == null) {
-        throw new IllegalArgumentException("point cannot have null values");
-      }
-      if (oneDim.length == 0) {
-        throw new IllegalArgumentException("point cannot have 0-length values");
-      }
-      if (bytesPerDim == -1) {
-        bytesPerDim = oneDim.length;
-      } else if (bytesPerDim != oneDim.length) {
-        throw new IllegalArgumentException("all dimensions must have same bytes length; got " + bytesPerDim + " and " + oneDim.length);
-      }
-    }
-    return getType(point.length, bytesPerDim);
-  }
-
-  private static FieldType getType(int numDims, int bytesPerDim) {
-    FieldType type = new FieldType();
-    type.setDimensions(numDims, bytesPerDim);
-    type.freeze();
-    return type;
-  }
-
-  private static BytesRef pack(byte[]... point) {
-    if (point == null) {
-      throw new IllegalArgumentException("point cannot be null");
-    }
-    if (point.length == 0) {
-      throw new IllegalArgumentException("point cannot be 0 dimensions");
-    }
-    if (point.length == 1) {
-      return new BytesRef(point[0]);
-    }
-    int bytesPerDim = -1;
-    for(byte[] dim : point) {
-      if (dim == null) {
-        throw new IllegalArgumentException("point cannot have null values");
-      }
-      if (bytesPerDim == -1) {
-        if (dim.length == 0) {
-          throw new IllegalArgumentException("point cannot have 0-length values");
-        }
-        bytesPerDim = dim.length;
-      } else if (dim.length != bytesPerDim) {
-        throw new IllegalArgumentException("all dimensions must have same bytes length; got " + bytesPerDim + " and " + dim.length);
-      }
-    }
-    byte[] packed = new byte[bytesPerDim*point.length];
-    for(int i=0;i<point.length;i++) {
-      System.arraycopy(point[i], 0, packed, i*bytesPerDim, bytesPerDim);
-    }
-    return new BytesRef(packed);
-  }
-
-  /** General purpose API: creates a new DimensionalField, indexing the
-   *  provided N-dimensional binary point.
-   *
-   *  @param name field name
-   *  @param point byte[][] value
-   *  @throws IllegalArgumentException if the field name or value is null.
-   */
-  public DimensionalBinaryField(String name, byte[]... point) {
-    super(name, pack(point), getType(point));
-  }
-
-  /** Expert API */
-  public DimensionalBinaryField(String name, byte[] packedPoint, FieldType type) {
-    super(name, packedPoint, type);
-    if (packedPoint.length != type.dimensionCount() * type.dimensionNumBytes()) {
-      throw new IllegalArgumentException("packedPoint is length=" + packedPoint.length + " but type.dimensionCount()=" + type.dimensionCount() + " and type.dimensionNumBytes()=" + type.dimensionNumBytes());
-    }
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/document/DimensionalDoubleField.java b/lucene/core/src/java/org/apache/lucene/document/DimensionalDoubleField.java
deleted file mode 100644
index db93cd4..0000000
--- a/lucene/core/src/java/org/apache/lucene/document/DimensionalDoubleField.java
+++ /dev/null
@@ -1,86 +0,0 @@
-package org.apache.lucene.document;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.NumericUtils;
-import org.apache.lucene.util.RamUsageEstimator;
-
-/** A double field that is indexed dimensionally such that finding
- *  all documents within an N-dimensional shape or range at search time is
- *  efficient.  Muliple values for the same field in one documents
- *  is allowed. */
-
-public final class DimensionalDoubleField extends Field {
-
-  private static FieldType getType(int numDims) {
-    FieldType type = new FieldType();
-    type.setDimensions(numDims, RamUsageEstimator.NUM_BYTES_LONG);
-    type.freeze();
-    return type;
-  }
-
-  @Override
-  public void setDoubleValue(double value) {
-    setDoubleValues(value);
-  }
-
-  /** Change the values of this field */
-  public void setDoubleValues(double... point) {
-    fieldsData = pack(point);
-  }
-
-  @Override
-  public void setBytesValue(BytesRef bytes) {
-    throw new IllegalArgumentException("cannot change value type from double to BytesRef");
-  }
-
-  @Override
-  public Number numericValue() {
-    BytesRef bytes = (BytesRef) fieldsData;
-    assert bytes.length == RamUsageEstimator.NUM_BYTES_LONG;
-    return NumericUtils.sortableLongToDouble(NumericUtils.bytesToLongDirect(bytes.bytes, bytes.offset));
-  }
-
-  private static BytesRef pack(double... point) {
-    if (point == null) {
-      throw new IllegalArgumentException("point cannot be null");
-    }
-    if (point.length == 0) {
-      throw new IllegalArgumentException("point cannot be 0 dimensions");
-    }
-    byte[] packed = new byte[point.length * RamUsageEstimator.NUM_BYTES_LONG];
-    
-    for(int dim=0;dim<point.length;dim++) {
-      NumericUtils.longToBytesDirect(NumericUtils.doubleToSortableLong(point[dim]), packed, dim);
-    }
-
-    return new BytesRef(packed);
-  }
-
-  /** Creates a new DimensionalDoubleField, indexing the
-   *  provided N-dimensional int point.
-   *
-   *  @param name field name
-   *  @param point double[] value
-   *  @throws IllegalArgumentException if the field name or value is null.
-   */
-  public DimensionalDoubleField(String name, double... point) {
-    super(name, pack(point), getType(point.length));
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/document/DimensionalFloatField.java b/lucene/core/src/java/org/apache/lucene/document/DimensionalFloatField.java
deleted file mode 100644
index 1fd6ea8..0000000
--- a/lucene/core/src/java/org/apache/lucene/document/DimensionalFloatField.java
+++ /dev/null
@@ -1,86 +0,0 @@
-package org.apache.lucene.document;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.NumericUtils;
-import org.apache.lucene.util.RamUsageEstimator;
-
-/** A field that is indexed dimensionally such that finding
- *  all documents within an N-dimensional at search time is
- *  efficient.  Muliple values for the same field in one documents
- *  is allowed. */
-
-public final class DimensionalFloatField extends Field {
-
-  private static FieldType getType(int numDims) {
-    FieldType type = new FieldType();
-    type.setDimensions(numDims, RamUsageEstimator.NUM_BYTES_INT);
-    type.freeze();
-    return type;
-  }
-
-  @Override
-  public void setFloatValue(float value) {
-    setFloatValues(value);
-  }
-
-  /** Change the values of this field */
-  public void setFloatValues(float... point) {
-    fieldsData = pack(point);
-  }
-
-  @Override
-  public void setBytesValue(BytesRef bytes) {
-    throw new IllegalArgumentException("cannot change value type from float to BytesRef");
-  }
-
-  @Override
-  public Number numericValue() {
-    BytesRef bytes = (BytesRef) fieldsData;
-    assert bytes.length == RamUsageEstimator.NUM_BYTES_INT;
-    return NumericUtils.sortableIntToFloat(NumericUtils.bytesToIntDirect(bytes.bytes, bytes.offset));
-  }
-
-  private static BytesRef pack(float... point) {
-    if (point == null) {
-      throw new IllegalArgumentException("point cannot be null");
-    }
-    if (point.length == 0) {
-      throw new IllegalArgumentException("point cannot be 0 dimensions");
-    }
-    byte[] packed = new byte[point.length * RamUsageEstimator.NUM_BYTES_INT];
-    
-    for(int dim=0;dim<point.length;dim++) {
-      NumericUtils.intToBytesDirect(NumericUtils.floatToSortableInt(point[dim]), packed, dim);
-    }
-
-    return new BytesRef(packed);
-  }
-
-  /** Creates a new DimensionalFloatField, indexing the
-   *  provided N-dimensional float point.
-   *
-   *  @param name field name
-   *  @param point int[] value
-   *  @throws IllegalArgumentException if the field name or value is null.
-   */
-  public DimensionalFloatField(String name, float... point) {
-    super(name, pack(point), getType(point.length));
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/document/DimensionalIntField.java b/lucene/core/src/java/org/apache/lucene/document/DimensionalIntField.java
deleted file mode 100644
index 83604a9..0000000
--- a/lucene/core/src/java/org/apache/lucene/document/DimensionalIntField.java
+++ /dev/null
@@ -1,86 +0,0 @@
-package org.apache.lucene.document;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.NumericUtils;
-import org.apache.lucene.util.RamUsageEstimator;
-
-/** An int field that is indexed dimensionally such that finding
- *  all documents within an N-dimensional shape or range at search time is
- *  efficient.  Muliple values for the same field in one documents
- *  is allowed. */
-
-public final class DimensionalIntField extends Field {
-
-  private static FieldType getType(int numDims) {
-    FieldType type = new FieldType();
-    type.setDimensions(numDims, RamUsageEstimator.NUM_BYTES_INT);
-    type.freeze();
-    return type;
-  }
-
-  @Override
-  public void setIntValue(int value) {
-    setIntValues(value);
-  }
-
-  /** Change the values of this field */
-  public void setIntValues(int... point) {
-    fieldsData = pack(point);
-  }
-
-  @Override
-  public void setBytesValue(BytesRef bytes) {
-    throw new IllegalArgumentException("cannot change value type from int to BytesRef");
-  }
-
-  @Override
-  public Number numericValue() {
-    BytesRef bytes = (BytesRef) fieldsData;
-    assert bytes.length == RamUsageEstimator.NUM_BYTES_INT;
-    return NumericUtils.bytesToInt(bytes.bytes, bytes.offset);
-  }
-
-  private static BytesRef pack(int... point) {
-    if (point == null) {
-      throw new IllegalArgumentException("point cannot be null");
-    }
-    if (point.length == 0) {
-      throw new IllegalArgumentException("point cannot be 0 dimensions");
-    }
-    byte[] packed = new byte[point.length * RamUsageEstimator.NUM_BYTES_INT];
-    
-    for(int dim=0;dim<point.length;dim++) {
-      NumericUtils.intToBytes(point[dim], packed, dim);
-    }
-
-    return new BytesRef(packed);
-  }
-
-  /** Creates a new DimensionalIntField, indexing the
-   *  provided N-dimensional int point.
-   *
-   *  @param name field name
-   *  @param point int[] value
-   *  @throws IllegalArgumentException if the field name or value is null.
-   */
-  public DimensionalIntField(String name, int... point) {
-    super(name, pack(point), getType(point.length));
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/document/DimensionalLongField.java b/lucene/core/src/java/org/apache/lucene/document/DimensionalLongField.java
deleted file mode 100644
index 752adef..0000000
--- a/lucene/core/src/java/org/apache/lucene/document/DimensionalLongField.java
+++ /dev/null
@@ -1,86 +0,0 @@
-package org.apache.lucene.document;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.NumericUtils;
-import org.apache.lucene.util.RamUsageEstimator;
-
-/** A long field that is indexed dimensionally such that finding
- *  all documents within an N-dimensional shape or range at search time is
- *  efficient.  Muliple values for the same field in one documents
- *  is allowed. */
-
-public final class DimensionalLongField extends Field {
-
-  private static FieldType getType(int numDims) {
-    FieldType type = new FieldType();
-    type.setDimensions(numDims, RamUsageEstimator.NUM_BYTES_LONG);
-    type.freeze();
-    return type;
-  }
-
-  @Override
-  public void setLongValue(long value) {
-    setLongValues(value);
-  }
-
-  /** Change the values of this field */
-  public void setLongValues(long... point) {
-    fieldsData = pack(point);
-  }
-
-  @Override
-  public void setBytesValue(BytesRef bytes) {
-    throw new IllegalArgumentException("cannot change value type from long to BytesRef");
-  }
-
-  @Override
-  public Number numericValue() {
-    BytesRef bytes = (BytesRef) fieldsData;
-    assert bytes.length == RamUsageEstimator.NUM_BYTES_LONG;
-    return NumericUtils.bytesToLong(bytes.bytes, bytes.offset);
-  }
-
-  private static BytesRef pack(long... point) {
-    if (point == null) {
-      throw new IllegalArgumentException("point cannot be null");
-    }
-    if (point.length == 0) {
-      throw new IllegalArgumentException("point cannot be 0 dimensions");
-    }
-    byte[] packed = new byte[point.length * RamUsageEstimator.NUM_BYTES_LONG];
-    
-    for(int dim=0;dim<point.length;dim++) {
-      NumericUtils.longToBytes(point[dim], packed, dim);
-    }
-
-    return new BytesRef(packed);
-  }
-
-  /** Creates a new DimensionalLongField, indexing the
-   *  provided N-dimensional int point.
-   *
-   *  @param name field name
-   *  @param point int[] value
-   *  @throws IllegalArgumentException if the field name or value is null.
-   */
-  public DimensionalLongField(String name, long... point) {
-    super(name, pack(point), getType(point.length));
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/document/DoublePoint.java b/lucene/core/src/java/org/apache/lucene/document/DoublePoint.java
new file mode 100644
index 0000000..a7a63e0
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/document/DoublePoint.java
@@ -0,0 +1,86 @@
+package org.apache.lucene.document;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.NumericUtils;
+import org.apache.lucene.util.RamUsageEstimator;
+
+/** A double field that is indexed dimensionally such that finding
+ *  all documents within an N-dimensional shape or range at search time is
+ *  efficient.  Muliple values for the same field in one documents
+ *  is allowed. */
+
+public final class DoublePoint extends Field {
+
+  private static FieldType getType(int numDims) {
+    FieldType type = new FieldType();
+    type.setDimensions(numDims, RamUsageEstimator.NUM_BYTES_LONG);
+    type.freeze();
+    return type;
+  }
+
+  @Override
+  public void setDoubleValue(double value) {
+    setDoubleValues(value);
+  }
+
+  /** Change the values of this field */
+  public void setDoubleValues(double... point) {
+    fieldsData = pack(point);
+  }
+
+  @Override
+  public void setBytesValue(BytesRef bytes) {
+    throw new IllegalArgumentException("cannot change value type from double to BytesRef");
+  }
+
+  @Override
+  public Number numericValue() {
+    BytesRef bytes = (BytesRef) fieldsData;
+    assert bytes.length == RamUsageEstimator.NUM_BYTES_LONG;
+    return NumericUtils.sortableLongToDouble(NumericUtils.bytesToLongDirect(bytes.bytes, bytes.offset));
+  }
+
+  private static BytesRef pack(double... point) {
+    if (point == null) {
+      throw new IllegalArgumentException("point cannot be null");
+    }
+    if (point.length == 0) {
+      throw new IllegalArgumentException("point cannot be 0 dimensions");
+    }
+    byte[] packed = new byte[point.length * RamUsageEstimator.NUM_BYTES_LONG];
+    
+    for(int dim=0;dim<point.length;dim++) {
+      NumericUtils.longToBytesDirect(NumericUtils.doubleToSortableLong(point[dim]), packed, dim);
+    }
+
+    return new BytesRef(packed);
+  }
+
+  /** Creates a new DoublePoint, indexing the
+   *  provided N-dimensional int point.
+   *
+   *  @param name field name
+   *  @param point double[] value
+   *  @throws IllegalArgumentException if the field name or value is null.
+   */
+  public DoublePoint(String name, double... point) {
+    super(name, pack(point), getType(point.length));
+  }
+}
diff --git a/lucene/core/src/java/org/apache/lucene/document/FieldType.java b/lucene/core/src/java/org/apache/lucene/document/FieldType.java
index 846f853..c6a137b 100644
--- a/lucene/core/src/java/org/apache/lucene/document/FieldType.java
+++ b/lucene/core/src/java/org/apache/lucene/document/FieldType.java
@@ -18,7 +18,6 @@ package org.apache.lucene.document;
  */
 
 import org.apache.lucene.analysis.Analyzer; // javadocs
-import org.apache.lucene.index.DimensionalValues; // javadocs
 import org.apache.lucene.index.DocValuesType;
 import org.apache.lucene.index.IndexOptions;
 import org.apache.lucene.index.IndexableFieldType;
@@ -32,7 +31,7 @@ public class FieldType implements IndexableFieldType  {
   /** Data type of the numeric value
    * @since 3.2
    *
-   * @deprecated Please switch to {@link DimensionalValues} instead
+   * @deprecated Please switch to {@link org.apache.lucene.index.PointValues} instead
    */
   @Deprecated
   public enum LegacyNumericType {
@@ -304,7 +303,7 @@ public class FieldType implements IndexableFieldType  {
    *         future modifications.
    * @see #numericType()
    *
-   * @deprecated Please switch to {@link DimensionalValues} instead
+   * @deprecated Please switch to {@link org.apache.lucene.index.PointValues} instead
    */
   @Deprecated
   public void setNumericType(LegacyNumericType type) {
@@ -320,7 +319,7 @@ public class FieldType implements IndexableFieldType  {
    * The default is <code>null</code> (no numeric type) 
    * @see #setNumericType(org.apache.lucene.document.FieldType.LegacyNumericType)
    *
-   * @deprecated Please switch to {@link DimensionalValues} instead
+   * @deprecated Please switch to {@link org.apache.lucene.index.PointValues} instead
    */
   @Deprecated
   public LegacyNumericType numericType() {
@@ -335,7 +334,7 @@ public class FieldType implements IndexableFieldType  {
    *         future modifications.
    * @see #numericPrecisionStep()
    *
-   * @deprecated Please switch to {@link DimensionalValues} instead
+   * @deprecated Please switch to {@link org.apache.lucene.index.PointValues} instead
    */
   @Deprecated
   public void setNumericPrecisionStep(int precisionStep) {
@@ -354,7 +353,7 @@ public class FieldType implements IndexableFieldType  {
    * The default is {@link org.apache.lucene.util.LegacyNumericUtils#PRECISION_STEP_DEFAULT}
    * @see #setNumericPrecisionStep(int)
    *
-   * @deprecated Please switch to {@link DimensionalValues} instead
+   * @deprecated Please switch to {@link org.apache.lucene.index.PointValues} instead
    */
   @Deprecated
   public int numericPrecisionStep() {
@@ -362,22 +361,22 @@ public class FieldType implements IndexableFieldType  {
   }
 
   /**
-   * Enables dimensional indexing.
+   * Enables points indexing.
    */
   public void setDimensions(int dimensionCount, int dimensionNumBytes) {
     if (dimensionCount < 0) {
-      throw new IllegalArgumentException("dimensionCount must be >= 0; got " + dimensionCount);
+      throw new IllegalArgumentException("pointDimensionCount must be >= 0; got " + dimensionCount);
     }
     if (dimensionNumBytes < 0) {
-      throw new IllegalArgumentException("dimensionNumBytes must be >= 0; got " + dimensionNumBytes);
+      throw new IllegalArgumentException("pointNumBytes must be >= 0; got " + dimensionNumBytes);
     }
     if (dimensionCount == 0) {
       if (dimensionNumBytes != 0) {
-        throw new IllegalArgumentException("when dimensionCount is 0 dimensionNumBytes must 0; got " + dimensionNumBytes);
+        throw new IllegalArgumentException("when pointDimensionCount is 0 pointNumBytes must 0; got " + dimensionNumBytes);
       }
     } else if (dimensionNumBytes == 0) {
       if (dimensionCount != 0) {
-        throw new IllegalArgumentException("when dimensionNumBytes is 0 dimensionCount must 0; got " + dimensionCount);
+        throw new IllegalArgumentException("when pointNumBytes is 0 pointDimensionCount must 0; got " + dimensionCount);
       }
     }
 
@@ -386,12 +385,12 @@ public class FieldType implements IndexableFieldType  {
   }
 
   @Override
-  public int dimensionCount() {
+  public int pointDimensionCount() {
     return dimensionCount;
   }
 
   @Override
-  public int dimensionNumBytes() {
+  public int pointNumBytes() {
     return dimensionNumBytes;
   }
 
@@ -435,9 +434,9 @@ public class FieldType implements IndexableFieldType  {
         result.append(numericPrecisionStep);
       }
       if (dimensionCount != 0) {
-        result.append(",dimensionCount=");
+        result.append(",pointDimensionCount=");
         result.append(dimensionCount);
-        result.append(",dimensionNumBytes=");
+        result.append(",pointNumBytes=");
         result.append(dimensionNumBytes);
       }
     }
diff --git a/lucene/core/src/java/org/apache/lucene/document/FloatPoint.java b/lucene/core/src/java/org/apache/lucene/document/FloatPoint.java
new file mode 100644
index 0000000..a023a4a
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/document/FloatPoint.java
@@ -0,0 +1,86 @@
+package org.apache.lucene.document;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.NumericUtils;
+import org.apache.lucene.util.RamUsageEstimator;
+
+/** A field that is indexed dimensionally such that finding
+ *  all documents within an N-dimensional at search time is
+ *  efficient.  Muliple values for the same field in one documents
+ *  is allowed. */
+
+public final class FloatPoint extends Field {
+
+  private static FieldType getType(int numDims) {
+    FieldType type = new FieldType();
+    type.setDimensions(numDims, RamUsageEstimator.NUM_BYTES_INT);
+    type.freeze();
+    return type;
+  }
+
+  @Override
+  public void setFloatValue(float value) {
+    setFloatValues(value);
+  }
+
+  /** Change the values of this field */
+  public void setFloatValues(float... point) {
+    fieldsData = pack(point);
+  }
+
+  @Override
+  public void setBytesValue(BytesRef bytes) {
+    throw new IllegalArgumentException("cannot change value type from float to BytesRef");
+  }
+
+  @Override
+  public Number numericValue() {
+    BytesRef bytes = (BytesRef) fieldsData;
+    assert bytes.length == RamUsageEstimator.NUM_BYTES_INT;
+    return NumericUtils.sortableIntToFloat(NumericUtils.bytesToIntDirect(bytes.bytes, bytes.offset));
+  }
+
+  private static BytesRef pack(float... point) {
+    if (point == null) {
+      throw new IllegalArgumentException("point cannot be null");
+    }
+    if (point.length == 0) {
+      throw new IllegalArgumentException("point cannot be 0 dimensions");
+    }
+    byte[] packed = new byte[point.length * RamUsageEstimator.NUM_BYTES_INT];
+    
+    for(int dim=0;dim<point.length;dim++) {
+      NumericUtils.intToBytesDirect(NumericUtils.floatToSortableInt(point[dim]), packed, dim);
+    }
+
+    return new BytesRef(packed);
+  }
+
+  /** Creates a new FloatPoint, indexing the
+   *  provided N-dimensional float point.
+   *
+   *  @param name field name
+   *  @param point int[] value
+   *  @throws IllegalArgumentException if the field name or value is null.
+   */
+  public FloatPoint(String name, float... point) {
+    super(name, pack(point), getType(point.length));
+  }
+}
diff --git a/lucene/core/src/java/org/apache/lucene/document/IntPoint.java b/lucene/core/src/java/org/apache/lucene/document/IntPoint.java
new file mode 100644
index 0000000..28f6a55
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/document/IntPoint.java
@@ -0,0 +1,86 @@
+package org.apache.lucene.document;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.NumericUtils;
+import org.apache.lucene.util.RamUsageEstimator;
+
+/** An int field that is indexed dimensionally such that finding
+ *  all documents within an N-dimensional shape or range at search time is
+ *  efficient.  Muliple values for the same field in one documents
+ *  is allowed. */
+
+public final class IntPoint extends Field {
+
+  private static FieldType getType(int numDims) {
+    FieldType type = new FieldType();
+    type.setDimensions(numDims, RamUsageEstimator.NUM_BYTES_INT);
+    type.freeze();
+    return type;
+  }
+
+  @Override
+  public void setIntValue(int value) {
+    setIntValues(value);
+  }
+
+  /** Change the values of this field */
+  public void setIntValues(int... point) {
+    fieldsData = pack(point);
+  }
+
+  @Override
+  public void setBytesValue(BytesRef bytes) {
+    throw new IllegalArgumentException("cannot change value type from int to BytesRef");
+  }
+
+  @Override
+  public Number numericValue() {
+    BytesRef bytes = (BytesRef) fieldsData;
+    assert bytes.length == RamUsageEstimator.NUM_BYTES_INT;
+    return NumericUtils.bytesToInt(bytes.bytes, bytes.offset);
+  }
+
+  private static BytesRef pack(int... point) {
+    if (point == null) {
+      throw new IllegalArgumentException("point cannot be null");
+    }
+    if (point.length == 0) {
+      throw new IllegalArgumentException("point cannot be 0 dimensions");
+    }
+    byte[] packed = new byte[point.length * RamUsageEstimator.NUM_BYTES_INT];
+    
+    for(int dim=0;dim<point.length;dim++) {
+      NumericUtils.intToBytes(point[dim], packed, dim);
+    }
+
+    return new BytesRef(packed);
+  }
+
+  /** Creates a new IntPoint, indexing the
+   *  provided N-dimensional int point.
+   *
+   *  @param name field name
+   *  @param point int[] value
+   *  @throws IllegalArgumentException if the field name or value is null.
+   */
+  public IntPoint(String name, int... point) {
+    super(name, pack(point), getType(point.length));
+  }
+}
diff --git a/lucene/core/src/java/org/apache/lucene/document/LegacyDoubleField.java b/lucene/core/src/java/org/apache/lucene/document/LegacyDoubleField.java
index ed4b2ef..eaebd61 100644
--- a/lucene/core/src/java/org/apache/lucene/document/LegacyDoubleField.java
+++ b/lucene/core/src/java/org/apache/lucene/document/LegacyDoubleField.java
@@ -105,7 +105,7 @@ import org.apache.lucene.index.IndexOptions;
  * class is a wrapper around this token stream type for
  * easier, more intuitive usage.</p>
  *
- * @deprecated Please use {@link DimensionalDoubleField} instead
+ * @deprecated Please use {@link DoublePoint} instead
  *
  * @since 2.9
  */
diff --git a/lucene/core/src/java/org/apache/lucene/document/LegacyFloatField.java b/lucene/core/src/java/org/apache/lucene/document/LegacyFloatField.java
index cda1aa4..e6ac0de 100644
--- a/lucene/core/src/java/org/apache/lucene/document/LegacyFloatField.java
+++ b/lucene/core/src/java/org/apache/lucene/document/LegacyFloatField.java
@@ -105,7 +105,7 @@ import org.apache.lucene.util.LegacyNumericUtils;
  * class is a wrapper around this token stream type for
  * easier, more intuitive usage.</p>
  *
- * @deprecated Please use {@link DimensionalFloatField} instead
+ * @deprecated Please use {@link FloatPoint} instead
  *
  * @since 2.9
  */
diff --git a/lucene/core/src/java/org/apache/lucene/document/LegacyIntField.java b/lucene/core/src/java/org/apache/lucene/document/LegacyIntField.java
index b937f9f..3ad963b 100644
--- a/lucene/core/src/java/org/apache/lucene/document/LegacyIntField.java
+++ b/lucene/core/src/java/org/apache/lucene/document/LegacyIntField.java
@@ -105,7 +105,7 @@ import org.apache.lucene.util.LegacyNumericUtils;
  * class is a wrapper around this token stream type for
  * easier, more intuitive usage.</p>
  *
- * @deprecated Please use {@link DimensionalIntField} instead
+ * @deprecated Please use {@link IntPoint} instead
  *
  * @since 2.9
  */
diff --git a/lucene/core/src/java/org/apache/lucene/document/LegacyLongField.java b/lucene/core/src/java/org/apache/lucene/document/LegacyLongField.java
index e478766..ce5c994 100644
--- a/lucene/core/src/java/org/apache/lucene/document/LegacyLongField.java
+++ b/lucene/core/src/java/org/apache/lucene/document/LegacyLongField.java
@@ -115,7 +115,7 @@ import org.apache.lucene.index.IndexOptions;
  * class is a wrapper around this token stream type for
  * easier, more intuitive usage.</p>
  *
- * @deprecated Please use {@link DimensionalLongField} instead
+ * @deprecated Please use {@link LongPoint} instead
  *
  * @since 2.9
  */
diff --git a/lucene/core/src/java/org/apache/lucene/document/LongPoint.java b/lucene/core/src/java/org/apache/lucene/document/LongPoint.java
new file mode 100644
index 0000000..23fddb2
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/document/LongPoint.java
@@ -0,0 +1,86 @@
+package org.apache.lucene.document;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.NumericUtils;
+import org.apache.lucene.util.RamUsageEstimator;
+
+/** A long field that is indexed dimensionally such that finding
+ *  all documents within an N-dimensional shape or range at search time is
+ *  efficient.  Muliple values for the same field in one documents
+ *  is allowed. */
+
+public final class LongPoint extends Field {
+
+  private static FieldType getType(int numDims) {
+    FieldType type = new FieldType();
+    type.setDimensions(numDims, RamUsageEstimator.NUM_BYTES_LONG);
+    type.freeze();
+    return type;
+  }
+
+  @Override
+  public void setLongValue(long value) {
+    setLongValues(value);
+  }
+
+  /** Change the values of this field */
+  public void setLongValues(long... point) {
+    fieldsData = pack(point);
+  }
+
+  @Override
+  public void setBytesValue(BytesRef bytes) {
+    throw new IllegalArgumentException("cannot change value type from long to BytesRef");
+  }
+
+  @Override
+  public Number numericValue() {
+    BytesRef bytes = (BytesRef) fieldsData;
+    assert bytes.length == RamUsageEstimator.NUM_BYTES_LONG;
+    return NumericUtils.bytesToLong(bytes.bytes, bytes.offset);
+  }
+
+  private static BytesRef pack(long... point) {
+    if (point == null) {
+      throw new IllegalArgumentException("point cannot be null");
+    }
+    if (point.length == 0) {
+      throw new IllegalArgumentException("point cannot be 0 dimensions");
+    }
+    byte[] packed = new byte[point.length * RamUsageEstimator.NUM_BYTES_LONG];
+    
+    for(int dim=0;dim<point.length;dim++) {
+      NumericUtils.longToBytes(point[dim], packed, dim);
+    }
+
+    return new BytesRef(packed);
+  }
+
+  /** Creates a new LongPoint, indexing the
+   *  provided N-dimensional int point.
+   *
+   *  @param name field name
+   *  @param point int[] value
+   *  @throws IllegalArgumentException if the field name or value is null.
+   */
+  public LongPoint(String name, long... point) {
+    super(name, pack(point), getType(point.length));
+  }
+}
diff --git a/lucene/core/src/java/org/apache/lucene/document/package-info.java b/lucene/core/src/java/org/apache/lucene/document/package-info.java
index 901d57c..f3e8e5b 100644
--- a/lucene/core/src/java/org/apache/lucene/document/package-info.java
+++ b/lucene/core/src/java/org/apache/lucene/document/package-info.java
@@ -35,8 +35,8 @@
  *   the process of taking a file and converting it into a Lucene {@link org.apache.lucene.document.Document}.
  * </p>
  * <p>The {@link org.apache.lucene.document.DateTools} is a utility class to make dates and times searchable. {@link
- * org.apache.lucene.document.DimensionalIntField}, {@link org.apache.lucene.document.DimensionalLongField},
- * {@link org.apache.lucene.document.DimensionalFloatField} and {@link org.apache.lucene.document.DimensionalDoubleField} enable indexing
- * of numeric values (and also dates) for fast range queries using {@link org.apache.lucene.search.DimensionalRangeQuery}</p>
+ * org.apache.lucene.document.IntPoint}, {@link org.apache.lucene.document.LongPoint},
+ * {@link org.apache.lucene.document.FloatPoint} and {@link org.apache.lucene.document.DoublePoint} enable indexing
+ * of numeric values (and also dates) for fast range queries using {@link org.apache.lucene.search.PointRangeQuery}</p>
  */
 package org.apache.lucene.document;
diff --git a/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java b/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
index 1571b51..c3aa49f 100644
--- a/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
+++ b/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
@@ -33,7 +33,7 @@ import java.util.Locale;
 import java.util.Map;
 
 import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.DimensionalReader;
+import org.apache.lucene.codecs.PointReader;
 import org.apache.lucene.codecs.DocValuesProducer;
 import org.apache.lucene.codecs.NormsProducer;
 import org.apache.lucene.codecs.PostingsFormat;
@@ -215,8 +215,8 @@ public final class CheckIndex implements Closeable {
       /** Status for testing of DocValues (null if DocValues could not be tested). */
       public DocValuesStatus docValuesStatus;
 
-      /** Status for testing of DimensionalValues (null if DimensionalValues could not be tested). */
-      public DimensionalValuesStatus dimensionalValuesStatus;
+      /** Status for testing of PointValues (null if PointValues could not be tested). */
+      public PointsStatus pointsStatus;
     }
     
     /**
@@ -358,17 +358,17 @@ public final class CheckIndex implements Closeable {
     }
 
     /**
-     * Status from testing DimensionalValues
+     * Status from testing PointValues
      */
-    public static final class DimensionalValuesStatus {
+    public static final class PointsStatus {
 
-      DimensionalValuesStatus() {
+      PointsStatus() {
       }
 
-      /** Total number of dimensional values points tested. */
+      /** Total number of values points tested. */
       public long totalValuePoints;
 
-      /** Total number of fields with dimensional values. */
+      /** Total number of fields with points. */
       public int totalValueFields;
       
       /** Exception thrown during doc values test (null on success) */
@@ -721,8 +721,8 @@ public final class CheckIndex implements Closeable {
           // Test Docvalues
           segInfoStat.docValuesStatus = testDocValues(reader, infoStream, failFast);
 
-          // Test DimensionalValues
-          segInfoStat.dimensionalValuesStatus = testDimensionalValues(reader, infoStream, failFast);
+          // Test PointValues
+          segInfoStat.pointsStatus = testPoints(reader, infoStream, failFast);
 
           // Rethrow the first exception we encountered
           //  This will cause stats for failed segments to be incremented properly
@@ -1681,23 +1681,23 @@ public final class CheckIndex implements Closeable {
   }
 
   /**
-   * Test the dimensional values index.
+   * Test the points index
    * @lucene.experimental
    */
-  public static Status.DimensionalValuesStatus testDimensionalValues(CodecReader reader, PrintStream infoStream, boolean failFast) throws IOException {
+  public static Status.PointsStatus testPoints(CodecReader reader, PrintStream infoStream, boolean failFast) throws IOException {
     FieldInfos fieldInfos = reader.getFieldInfos();
-    Status.DimensionalValuesStatus status = new Status.DimensionalValuesStatus();
+    Status.PointsStatus status = new Status.PointsStatus();
     try {
-      if (fieldInfos.hasDimensionalValues()) {
-        DimensionalReader values = reader.getDimensionalReader();
+      if (fieldInfos.hasPointValues()) {
+        PointReader values = reader.getPointReader();
         if (values == null) {
-          throw new RuntimeException("there are fields with dimensional values, but reader.getDimensionalRader() is null");
+          throw new RuntimeException("there are fields with points, but reader.getPointReader() is null");
         }
         for (FieldInfo fieldInfo : fieldInfos) {
-          if (fieldInfo.getDimensionCount() > 0) {
+          if (fieldInfo.getPointDimensionCount() > 0) {
             status.totalValueFields++;
-            int dimCount = fieldInfo.getDimensionCount();
-            int bytesPerDim = fieldInfo.getDimensionNumBytes();
+            int dimCount = fieldInfo.getPointDimensionCount();
+            int bytesPerDim = fieldInfo.getPointNumBytes();
             byte[] lastMinPackedValue = new byte[dimCount*bytesPerDim];
             BytesRef lastMinPacked = new BytesRef(lastMinPackedValue);
             byte[] lastMaxPackedValue = new byte[dimCount*bytesPerDim];
@@ -1707,7 +1707,7 @@ public final class CheckIndex implements Closeable {
             lastMinPacked.length = bytesPerDim;
             scratch.length = bytesPerDim;
             values.intersect(fieldInfo.name,
-                             new DimensionalValues.IntersectVisitor() {
+                             new PointValues.IntersectVisitor() {
                                @Override
                                public void visit(int docID) {
                                  throw new RuntimeException("codec called IntersectVisitor.visit without a packed value for docID=" + docID);
@@ -1737,7 +1737,7 @@ public final class CheckIndex implements Closeable {
                                }
 
                                @Override
-                               public DimensionalValues.Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {
+                               public PointValues.Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {
                                  checkPackedValue("min packed value", minPackedValue, -1);
                                  System.arraycopy(minPackedValue, 0, lastMinPackedValue, 0, minPackedValue.length);
                                  checkPackedValue("max packed value", maxPackedValue, -1);
@@ -1745,7 +1745,7 @@ public final class CheckIndex implements Closeable {
 
                                  // We always pretend the query shape is so complex that it crosses every cell, so
                                  // that packedValue is passed for every document
-                                 return DimensionalValues.Relation.CELL_CROSSES_QUERY;
+                                 return PointValues.Relation.CELL_CROSSES_QUERY;
                                }
 
                                private void checkPackedValue(String desc, byte[] packedValue, int docID) {
diff --git a/lucene/core/src/java/org/apache/lucene/index/CodecReader.java b/lucene/core/src/java/org/apache/lucene/index/CodecReader.java
index c879b9e..a5642e4 100644
--- a/lucene/core/src/java/org/apache/lucene/index/CodecReader.java
+++ b/lucene/core/src/java/org/apache/lucene/index/CodecReader.java
@@ -25,7 +25,7 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
-import org.apache.lucene.codecs.DimensionalReader;
+import org.apache.lucene.codecs.PointReader;
 import org.apache.lucene.codecs.DocValuesProducer;
 import org.apache.lucene.codecs.FieldsProducer;
 import org.apache.lucene.codecs.NormsProducer;
@@ -77,10 +77,10 @@ public abstract class CodecReader extends LeafReader implements Accountable {
   public abstract FieldsProducer getPostingsReader();
 
   /**
-   * Expert: retrieve underlying DimensionalReader
+   * Expert: retrieve underlying PointReader
    * @lucene.internal
    */
-  public abstract DimensionalReader getDimensionalReader();
+  public abstract PointReader getPointReader();
   
   @Override
   public final void document(int docID, StoredFieldVisitor visitor) throws IOException {
@@ -322,9 +322,9 @@ public abstract class CodecReader extends LeafReader implements Accountable {
       ramBytesUsed += getTermVectorsReader().ramBytesUsed();
     }
 
-    // dimensional values
-    if (getDimensionalReader() != null) {
-      ramBytesUsed += getDimensionalReader().ramBytesUsed();
+    // points
+    if (getPointReader() != null) {
+      ramBytesUsed += getPointReader().ramBytesUsed();
     }
     
     return ramBytesUsed;
@@ -358,9 +358,9 @@ public abstract class CodecReader extends LeafReader implements Accountable {
       resources.add(Accountables.namedAccountable("term vectors", getTermVectorsReader()));
     }
 
-    // dimensional values
-    if (getDimensionalReader() != null) {
-      resources.add(Accountables.namedAccountable("dimensional values", getDimensionalReader()));
+    // points
+    if (getPointReader() != null) {
+      resources.add(Accountables.namedAccountable("points", getPointReader()));
     }
     
     return Collections.unmodifiableList(resources);
diff --git a/lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.java b/lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.java
index 0370008..23dc6f6 100644
--- a/lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.java
+++ b/lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.java
@@ -23,8 +23,8 @@ import java.util.HashMap;
 import java.util.Map;
 
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.codecs.DimensionalFormat;
-import org.apache.lucene.codecs.DimensionalWriter;
+import org.apache.lucene.codecs.PointFormat;
+import org.apache.lucene.codecs.PointWriter;
 import org.apache.lucene.codecs.DocValuesConsumer;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.NormsConsumer;
@@ -93,7 +93,7 @@ final class DefaultIndexingChain extends DocConsumer {
     int maxDoc = state.segmentInfo.maxDoc();
     writeNorms(state);
     writeDocValues(state);
-    writeDimensionalValues(state);
+    writePoints(state);
     
     // it's possible all docs hit non-aborting exceptions...
     initStoredFieldsWriter();
@@ -121,33 +121,33 @@ final class DefaultIndexingChain extends DocConsumer {
     docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, "", state.fieldInfos, IOContext.DEFAULT);
   }
 
-  /** Writes all buffered dimensional values. */
-  private void writeDimensionalValues(SegmentWriteState state) throws IOException {
-    DimensionalWriter dimensionalWriter = null;
+  /** Writes all buffered points. */
+  private void writePoints(SegmentWriteState state) throws IOException {
+    PointWriter pointWriter = null;
     boolean success = false;
     try {
       for (int i=0;i<fieldHash.length;i++) {
         PerField perField = fieldHash[i];
         while (perField != null) {
-          if (perField.dimensionalValuesWriter != null) {
-            if (perField.fieldInfo.getDimensionCount() == 0) {
+          if (perField.pointValuesWriter != null) {
+            if (perField.fieldInfo.getPointDimensionCount() == 0) {
               // BUG
-              throw new AssertionError("segment=" + state.segmentInfo + ": field=\"" + perField.fieldInfo.name + "\" has no dimensional values but wrote them");
+              throw new AssertionError("segment=" + state.segmentInfo + ": field=\"" + perField.fieldInfo.name + "\" has no points but wrote them");
             }
-            if (dimensionalWriter == null) {
+            if (pointWriter == null) {
               // lazy init
-              DimensionalFormat fmt = state.segmentInfo.getCodec().dimensionalFormat();
+              PointFormat fmt = state.segmentInfo.getCodec().pointFormat();
               if (fmt == null) {
-                throw new IllegalStateException("field=\"" + perField.fieldInfo.name + "\" was indexed dimensionally but codec does not support dimensional formats");
+                throw new IllegalStateException("field=\"" + perField.fieldInfo.name + "\" was indexed as points but codec does not support points");
               }
-              dimensionalWriter = fmt.fieldsWriter(state);
+              pointWriter = fmt.fieldsWriter(state);
             }
 
-            perField.dimensionalValuesWriter.flush(state, dimensionalWriter);
-            perField.dimensionalValuesWriter = null;
-          } else if (perField.fieldInfo.getDimensionCount() != 0) {
+            perField.pointValuesWriter.flush(state, pointWriter);
+            perField.pointValuesWriter = null;
+          } else if (perField.fieldInfo.getPointDimensionCount() != 0) {
             // BUG
-            throw new AssertionError("segment=" + state.segmentInfo + ": field=\"" + perField.fieldInfo.name + "\" has dimensional values but did not write them");
+            throw new AssertionError("segment=" + state.segmentInfo + ": field=\"" + perField.fieldInfo.name + "\" has points but did not write them");
           }
           perField = perField.next;
         }
@@ -155,9 +155,9 @@ final class DefaultIndexingChain extends DocConsumer {
       success = true;
     } finally {
       if (success) {
-        IOUtils.close(dimensionalWriter);
+        IOUtils.close(pointWriter);
       } else {
-        IOUtils.closeWhileHandlingException(dimensionalWriter);
+        IOUtils.closeWhileHandlingException(pointWriter);
       }
     }
   }
@@ -419,11 +419,11 @@ final class DefaultIndexingChain extends DocConsumer {
       }
       indexDocValue(fp, dvType, field);
     }
-    if (fieldType.dimensionCount() != 0) {
+    if (fieldType.pointDimensionCount() != 0) {
       if (fp == null) {
         fp = getOrAddField(fieldName, fieldType, false);
       }
-      indexDimensionalValue(fp, field);
+      indexPoint(fp, field);
     }
     
     return fieldCount;
@@ -448,24 +448,24 @@ final class DefaultIndexingChain extends DocConsumer {
     }
   }
 
-  /** Called from processDocument to index one field's dimensional value */
-  private void indexDimensionalValue(PerField fp, IndexableField field) throws IOException {
-    int dimensionCount = field.fieldType().dimensionCount();
+  /** Called from processDocument to index one field's point */
+  private void indexPoint(PerField fp, IndexableField field) throws IOException {
+    int pointDimensionCount = field.fieldType().pointDimensionCount();
 
-    int dimensionNumBytes = field.fieldType().dimensionNumBytes();
+    int dimensionNumBytes = field.fieldType().pointNumBytes();
 
     // Record dimensions for this field; this setter will throw IllegalArgExc if
     // the dimensions were already set to something different:
-    if (fp.fieldInfo.getDimensionCount() == 0) {
-      fieldInfos.globalFieldNumbers.setDimensions(fp.fieldInfo.number, fp.fieldInfo.name, dimensionCount, dimensionNumBytes);
+    if (fp.fieldInfo.getPointDimensionCount() == 0) {
+      fieldInfos.globalFieldNumbers.setDimensions(fp.fieldInfo.number, fp.fieldInfo.name, pointDimensionCount, dimensionNumBytes);
     }
 
-    fp.fieldInfo.setDimensions(dimensionCount, dimensionNumBytes);
+    fp.fieldInfo.setPointDimensions(pointDimensionCount, dimensionNumBytes);
 
-    if (fp.dimensionalValuesWriter == null) {
-      fp.dimensionalValuesWriter = new DimensionalValuesWriter(docWriter, fp.fieldInfo);
+    if (fp.pointValuesWriter == null) {
+      fp.pointValuesWriter = new PointValuesWriter(docWriter, fp.fieldInfo);
     }
-    fp.dimensionalValuesWriter.addPackedValue(docState.docID, field.binaryValue());
+    fp.pointValuesWriter.addPackedValue(docState.docID, field.binaryValue());
   }
 
   /** Called from processDocument to index one field's doc value */
@@ -596,8 +596,8 @@ final class DefaultIndexingChain extends DocConsumer {
     // segment:
     DocValuesWriter docValuesWriter;
 
-    // Non-null if this field ever had dimensional values in this segment:
-    DimensionalValuesWriter dimensionalValuesWriter;
+    // Non-null if this field ever had points in this segment:
+    PointValuesWriter pointValuesWriter;
 
     /** We use this to know when a PerField is seen for the
      *  first time in the current document. */
diff --git a/lucene/core/src/java/org/apache/lucene/index/DimensionalValues.java b/lucene/core/src/java/org/apache/lucene/index/DimensionalValues.java
deleted file mode 100644
index 2f30ab6..0000000
--- a/lucene/core/src/java/org/apache/lucene/index/DimensionalValues.java
+++ /dev/null
@@ -1,93 +0,0 @@
-package org.apache.lucene.index;
-
-import java.io.IOException;
-
-import org.apache.lucene.document.DimensionalBinaryField;
-import org.apache.lucene.document.DimensionalDoubleField;
-import org.apache.lucene.document.DimensionalFloatField;
-import org.apache.lucene.document.DimensionalIntField;
-import org.apache.lucene.document.DimensionalLongField;
-import org.apache.lucene.util.bkd.BKDWriter;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** Allows recursively visiting dimensional values indexed with {@link DimensionalIntField},
- *  {@link DimensionalFloatField}, {@link DimensionalLongField}, {@link DimensionalDoubleField}
- *  or {@link DimensionalBinaryField}.
- *
- *  @lucene.experimental */
-public abstract class DimensionalValues {
-
-  /** Maximum number of bytes for each dimension */
-  public static final int MAX_NUM_BYTES = 16;
-
-  /** Maximum number of dimensions */
-  public static final int MAX_DIMENSIONS = BKDWriter.MAX_DIMS;
-
-  /** Default constructor */
-  protected DimensionalValues() {
-  }
-
-  /** Used by {@link #intersect} to check how each recursive cell corresponds to the query. */
-  public enum Relation {
-    /** Return this if the cell is fully contained by the query */
-    CELL_INSIDE_QUERY,
-    /** Return this if the cell and query do not overlap */
-    CELL_OUTSIDE_QUERY,
-    /** Return this if the cell partially overlapps the query */
-    CELL_CROSSES_QUERY
-  };
-
-  /** We recurse the BKD tree, using a provided instance of this to guide the recursion.
-   *
-   * @lucene.experimental */
-  public interface IntersectVisitor {
-    /** Called for all docs in a leaf cell that's fully contained by the query.  The
-     *  consumer should blindly accept the docID. */
-    void visit(int docID) throws IOException;
-
-    /** Called for all docs in a leaf cell that crosses the query.  The consumer
-     *  should scrutinize the packedValue to decide whether to accept it. */
-    void visit(int docID, byte[] packedValue) throws IOException;
-
-    /** Called for non-leaf cells to test how the cell relates to the query, to
-     *  determine how to further recurse down the treer. */
-    Relation compare(byte[] minPackedValue, byte[] maxPackedValue);
-
-    /** Notifies the caller that this many documents (from one block) are about
-     *  to be visited */
-    default void grow(int count) {};
-  }
-
-  /** Finds all documents and points matching the provided visitor.
-   *  This method does not enforce live docs, so it's up to the caller
-   *  to test whether each document is deleted, if necessary. */
-  public abstract void intersect(String fieldName, IntersectVisitor visitor) throws IOException;
-
-  /** Returns minimum value for each dimension, packed, or null if no points were indexed */
-  public abstract byte[] getMinPackedValue(String fieldName) throws IOException;
-
-  /** Returns maximum value for each dimension, packed, or null if no points were indexed */
-  public abstract byte[] getMaxPackedValue(String fieldName) throws IOException;
-
-  /** Returns how many dimensions were indexed */
-  public abstract int getNumDimensions(String fieldName) throws IOException;
-
-  /** Returns the number of bytes per dimension */
-  public abstract int getBytesPerDimension(String fieldName) throws IOException;
-}
diff --git a/lucene/core/src/java/org/apache/lucene/index/DimensionalValuesWriter.java b/lucene/core/src/java/org/apache/lucene/index/DimensionalValuesWriter.java
deleted file mode 100644
index d3bfd30..0000000
--- a/lucene/core/src/java/org/apache/lucene/index/DimensionalValuesWriter.java
+++ /dev/null
@@ -1,114 +0,0 @@
-package org.apache.lucene.index;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DimensionalReader;
-import org.apache.lucene.codecs.DimensionalWriter;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.ByteBlockPool;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.RamUsageEstimator;
-
-/** Buffers up pending byte[][] value(s) per doc, then flushes when segment flushes. */
-class DimensionalValuesWriter {
-  private final FieldInfo fieldInfo;
-  private final ByteBlockPool bytes;
-  private final Counter iwBytesUsed;
-  private int[] docIDs;
-  private int numDocs;
-  private final byte[] packedValue;
-
-  public DimensionalValuesWriter(DocumentsWriterPerThread docWriter, FieldInfo fieldInfo) {
-    this.fieldInfo = fieldInfo;
-    this.iwBytesUsed = docWriter.bytesUsed;
-    this.bytes = new ByteBlockPool(docWriter.byteBlockAllocator);
-    docIDs = new int[16];
-    iwBytesUsed.addAndGet(16 * RamUsageEstimator.NUM_BYTES_INT);
-    packedValue = new byte[fieldInfo.getDimensionCount() * fieldInfo.getDimensionNumBytes()];
-  }
-
-  public void addPackedValue(int docID, BytesRef value) {
-    if (value == null) {
-      throw new IllegalArgumentException("field=" + fieldInfo.name + ": dimensional value cannot be null");
-    }
-    if (value.length != fieldInfo.getDimensionCount() * fieldInfo.getDimensionNumBytes()) {
-      throw new IllegalArgumentException("field=" + fieldInfo.name + ": this field's value has length=" + value.length + " but should be " + (fieldInfo.getDimensionCount() * fieldInfo.getDimensionNumBytes()));
-    }
-    if (docIDs.length == numDocs) {
-      docIDs = ArrayUtil.grow(docIDs, numDocs+1);
-      iwBytesUsed.addAndGet((docIDs.length - numDocs) * RamUsageEstimator.NUM_BYTES_INT);
-    }
-    bytes.append(value);
-    docIDs[numDocs] = docID;
-    numDocs++;
-  }
-
-  public void flush(SegmentWriteState state, DimensionalWriter writer) throws IOException {
-
-    writer.writeField(fieldInfo,
-                      new DimensionalReader() {
-                        @Override
-                        public void intersect(String fieldName, IntersectVisitor visitor) throws IOException {
-                          if (fieldName.equals(fieldInfo.name) == false) {
-                            throw new IllegalArgumentException("fieldName must be the same");
-                          }
-                          for(int i=0;i<numDocs;i++) {
-                            bytes.readBytes(packedValue.length * i, packedValue, 0, packedValue.length);
-                            visitor.visit(docIDs[i], packedValue);
-                          }
-                        }
-
-                        @Override
-                        public void checkIntegrity() {
-                          throw new UnsupportedOperationException();
-                        }
-
-                        @Override
-                        public long ramBytesUsed() {
-                          return 0L;
-                        }
-
-                        @Override
-                        public void close() {
-                        }
-
-                        @Override
-                        public byte[] getMinPackedValue(String fieldName) {
-                          throw new UnsupportedOperationException();
-                        }
-
-                        @Override
-                        public byte[] getMaxPackedValue(String fieldName) {
-                          throw new UnsupportedOperationException();
-                        }
-
-                        @Override
-                        public int getNumDimensions(String fieldName) {
-                          throw new UnsupportedOperationException();
-                        }
-
-                        @Override
-                        public int getBytesPerDimension(String fieldName) {
-                          throw new UnsupportedOperationException();
-                        }
-                      });
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/index/FieldInfo.java b/lucene/core/src/java/org/apache/lucene/index/FieldInfo.java
index ea5f844..57dd2c5 100644
--- a/lucene/core/src/java/org/apache/lucene/index/FieldInfo.java
+++ b/lucene/core/src/java/org/apache/lucene/index/FieldInfo.java
@@ -20,8 +20,6 @@ package org.apache.lucene.index;
 import java.util.Map;
 import java.util.Objects;
 
-import org.apache.lucene.codecs.DimensionalFormat;
-
 /**
  *  Access to the Field Info file that describes document fields and whether or
  *  not they are indexed. Each segment has a separate Field Info file. Objects
@@ -50,10 +48,10 @@ public final class FieldInfo {
 
   private long dvGen;
 
-  /** If both of these are positive it means this is a dimensionally indexed
-   *  field (see {@link DimensionalFormat}). */
-  private int dimensionCount;
-  private int dimensionNumBytes;
+  /** If both of these are positive it means this field indexed points
+   *  (see {@link org.apache.lucene.codecs.PointFormat}). */
+  private int pointDimensionCount;
+  private int pointNumBytes;
 
   /**
    * Sole constructor.
@@ -62,7 +60,7 @@ public final class FieldInfo {
    */
   public FieldInfo(String name, int number, boolean storeTermVector, boolean omitNorms, 
                    boolean storePayloads, IndexOptions indexOptions, DocValuesType docValues,
-                   long dvGen, Map<String,String> attributes, int dimensionCount, int dimensionNumBytes) {
+                   long dvGen, Map<String,String> attributes, int pointDimensionCount, int pointNumBytes) {
     this.name = Objects.requireNonNull(name);
     this.number = number;
     this.docValuesType = Objects.requireNonNull(docValues, "DocValuesType cannot be null (field: \"" + name + "\")");
@@ -78,8 +76,8 @@ public final class FieldInfo {
     }
     this.dvGen = dvGen;
     this.attributes = Objects.requireNonNull(attributes);
-    this.dimensionCount = dimensionCount;
-    this.dimensionNumBytes = dimensionNumBytes;
+    this.pointDimensionCount = pointDimensionCount;
+    this.pointNumBytes = pointNumBytes;
     assert checkConsistency();
   }
 
@@ -105,20 +103,20 @@ public final class FieldInfo {
       }
     }
 
-    if (dimensionCount < 0) {
-      throw new IllegalStateException("dimensionCount must be >= 0; got " + dimensionCount);
+    if (pointDimensionCount < 0) {
+      throw new IllegalStateException("pointDimensionCount must be >= 0; got " + pointDimensionCount);
     }
 
-    if (dimensionNumBytes < 0) {
-      throw new IllegalStateException("dimensionNumBytes must be >= 0; got " + dimensionNumBytes);
+    if (pointNumBytes < 0) {
+      throw new IllegalStateException("pointNumBytes must be >= 0; got " + pointNumBytes);
     }
 
-    if (dimensionCount != 0 && dimensionNumBytes == 0) {
-      throw new IllegalStateException("dimensionNumBytes must be > 0 when dimensionCount=" + dimensionCount);
+    if (pointDimensionCount != 0 && pointNumBytes == 0) {
+      throw new IllegalStateException("pointNumBytes must be > 0 when pointDimensionCount=" + pointDimensionCount);
     }
 
-    if (dimensionNumBytes != 0 && dimensionCount == 0) {
-      throw new IllegalStateException("dimensionCount must be > 0 when dimensionNumBytes=" + dimensionNumBytes);
+    if (pointNumBytes != 0 && pointDimensionCount == 0) {
+      throw new IllegalStateException("pointDimensionCount must be > 0 when pointNumBytes=" + pointNumBytes);
     }
     
     if (dvGen != -1 && docValuesType == DocValuesType.NONE) {
@@ -144,9 +142,9 @@ public final class FieldInfo {
       }
     }
 
-    if (this.dimensionCount == 0 && dimensionCount != 0) {
-      this.dimensionCount = dimensionCount;
-      this.dimensionNumBytes = dimensionNumBytes;
+    if (this.pointDimensionCount == 0 && dimensionCount != 0) {
+      this.pointDimensionCount = dimensionCount;
+      this.pointNumBytes = dimensionNumBytes;
     }
 
     if (this.indexOptions != IndexOptions.NONE) { // if updated field data is not for indexing, leave the updates out
@@ -165,40 +163,40 @@ public final class FieldInfo {
     assert checkConsistency();
   }
 
-  /** Record that this field is indexed dimensionally, with the
+  /** Record that this field is indexed with points, with the
    *  specified number of dimensions and bytes per dimension. */
-  public void setDimensions(int count, int numBytes) {
+  public void setPointDimensions(int count, int numBytes) {
     if (count <= 0) {
-      throw new IllegalArgumentException("dimension count must be >= 0; got " + count + " for field=\"" + name + "\"");
+      throw new IllegalArgumentException("point dimension count must be >= 0; got " + count + " for field=\"" + name + "\"");
     }
-    if (count > DimensionalValues.MAX_DIMENSIONS) {
-      throw new IllegalArgumentException("dimension count must be < DimensionalValues.MAX_DIMENSIONS (= " + DimensionalValues.MAX_DIMENSIONS + "); got " + count + " for field=\"" + name + "\"");
+    if (count > PointValues.MAX_DIMENSIONS) {
+      throw new IllegalArgumentException("point dimension count must be < PointValues.MAX_DIMENSIONS (= " + PointValues.MAX_DIMENSIONS + "); got " + count + " for field=\"" + name + "\"");
     }
     if (numBytes <= 0) {
-      throw new IllegalArgumentException("dimension numBytes must be >= 0; got " + numBytes + " for field=\"" + name + "\"");
+      throw new IllegalArgumentException("point numBytes must be >= 0; got " + numBytes + " for field=\"" + name + "\"");
     }
-    if (numBytes > DimensionalValues.MAX_NUM_BYTES) {
-      throw new IllegalArgumentException("dimension numBytes must be <= DimensionalValues.MAX_NUM_BYTES (= " + DimensionalValues.MAX_NUM_BYTES + "); got " + numBytes + " for field=\"" + name + "\"");
+    if (numBytes > PointValues.MAX_NUM_BYTES) {
+      throw new IllegalArgumentException("point numBytes must be <= PointValues.MAX_NUM_BYTES (= " + PointValues.MAX_NUM_BYTES + "); got " + numBytes + " for field=\"" + name + "\"");
     }
-    if (dimensionCount != 0 && dimensionCount != count) {
-      throw new IllegalArgumentException("cannot change dimension count from " + dimensionCount + " to " + count + " for field=\"" + name + "\"");
+    if (pointDimensionCount != 0 && pointDimensionCount != count) {
+      throw new IllegalArgumentException("cannot change point dimension count from " + pointDimensionCount + " to " + count + " for field=\"" + name + "\"");
     }
-    if (dimensionNumBytes != 0 && dimensionNumBytes != numBytes) {
-      throw new IllegalArgumentException("cannot change dimension numBytes from " + dimensionNumBytes + " to " + numBytes + " for field=\"" + name + "\"");
+    if (pointNumBytes != 0 && pointNumBytes != numBytes) {
+      throw new IllegalArgumentException("cannot change point numBytes from " + pointNumBytes + " to " + numBytes + " for field=\"" + name + "\"");
     }
 
-    dimensionCount = count;
-    dimensionNumBytes = numBytes;
+    pointDimensionCount = count;
+    pointNumBytes = numBytes;
   }
 
-  /** Return dimension count */
-  public int getDimensionCount() {
-    return dimensionCount;
+  /** Return point dimension count */
+  public int getPointDimensionCount() {
+    return pointDimensionCount;
   }
 
   /** Return number of bytes per dimension */
-  public int getDimensionNumBytes() {
-    return dimensionNumBytes;
+  public int getPointNumBytes() {
+    return pointNumBytes;
   }
 
   void setDocValuesType(DocValuesType type) {
diff --git a/lucene/core/src/java/org/apache/lucene/index/FieldInfos.java b/lucene/core/src/java/org/apache/lucene/index/FieldInfos.java
index 86da865..93f35e7 100644
--- a/lucene/core/src/java/org/apache/lucene/index/FieldInfos.java
+++ b/lucene/core/src/java/org/apache/lucene/index/FieldInfos.java
@@ -39,7 +39,7 @@ public class FieldInfos implements Iterable<FieldInfo> {
   private final boolean hasVectors;
   private final boolean hasNorms;
   private final boolean hasDocValues;
-  private final boolean hasDimensionalValues;
+  private final boolean hasPointValues;
   
   // used only by fieldInfo(int)
   private final FieldInfo[] byNumberTable; // contiguous
@@ -59,7 +59,7 @@ public class FieldInfos implements Iterable<FieldInfo> {
     boolean hasFreq = false;
     boolean hasNorms = false;
     boolean hasDocValues = false;
-    boolean hasDimensionalValues = false;
+    boolean hasPointValues = false;
     
     TreeMap<Integer, FieldInfo> byNumber = new TreeMap<>();
     for (FieldInfo info : infos) {
@@ -82,7 +82,7 @@ public class FieldInfos implements Iterable<FieldInfo> {
       hasNorms |= info.hasNorms();
       hasDocValues |= info.getDocValuesType() != DocValuesType.NONE;
       hasPayloads |= info.hasPayloads();
-      hasDimensionalValues |= (info.getDimensionCount() != 0);
+      hasPointValues |= (info.getPointDimensionCount() != 0);
     }
     
     this.hasVectors = hasVectors;
@@ -92,7 +92,7 @@ public class FieldInfos implements Iterable<FieldInfo> {
     this.hasFreq = hasFreq;
     this.hasNorms = hasNorms;
     this.hasDocValues = hasDocValues;
-    this.hasDimensionalValues = hasDimensionalValues;
+    this.hasPointValues = hasPointValues;
     this.values = Collections.unmodifiableCollection(byNumber.values());
     Integer max = byNumber.isEmpty() ? null : Collections.max(byNumber.keySet());
     
@@ -147,9 +147,9 @@ public class FieldInfos implements Iterable<FieldInfo> {
     return hasDocValues;
   }
 
-  /** Returns true if any fields have DimensionalValues */
-  public boolean hasDimensionalValues() {
-    return hasDimensionalValues;
+  /** Returns true if any fields have PointValues */
+  public boolean hasPointValues() {
+    return hasPointValues;
   }
   
   /** Returns the number of fields */
@@ -249,10 +249,10 @@ public class FieldInfos implements Iterable<FieldInfo> {
         FieldDimensions dims = dimensions.get(fieldName);
         if (dims != null) {
           if (dims.dimensionCount != dimensionCount) {
-            throw new IllegalArgumentException("cannot change dimension count from " + dims.dimensionCount + " to " + dimensionCount + " for field=\"" + fieldName + "\"");
+            throw new IllegalArgumentException("cannot change point dimension count from " + dims.dimensionCount + " to " + dimensionCount + " for field=\"" + fieldName + "\"");
           }
           if (dims.dimensionNumBytes != dimensionNumBytes) {
-            throw new IllegalArgumentException("cannot change dimension numBytes from " + dims.dimensionNumBytes + " to " + dimensionNumBytes + " for field=\"" + fieldName + "\"");
+            throw new IllegalArgumentException("cannot change point numBytes from " + dims.dimensionNumBytes + " to " + dimensionNumBytes + " for field=\"" + fieldName + "\"");
           }
         } else {
           dimensions.put(fieldName, new FieldDimensions(dimensionCount, dimensionNumBytes));
@@ -302,10 +302,10 @@ public class FieldInfos implements Iterable<FieldInfo> {
       FieldDimensions dim = dimensions.get(name);
       if (dim != null) {
         if (dim.dimensionCount != dimensionCount) {
-          throw new IllegalArgumentException("cannot change dimension count from " + dim.dimensionCount + " to " + dimensionCount + " for field=\"" + name + "\"");
+          throw new IllegalArgumentException("cannot change point dimension count from " + dim.dimensionCount + " to " + dimensionCount + " for field=\"" + name + "\"");
         }
         if (dim.dimensionNumBytes != dimensionNumBytes) {
-          throw new IllegalArgumentException("cannot change dimension numBytes from " + dim.dimensionNumBytes + " to " + dimensionNumBytes + " for field=\"" + name + "\"");
+          throw new IllegalArgumentException("cannot change point numBytes from " + dim.dimensionNumBytes + " to " + dimensionNumBytes + " for field=\"" + name + "\"");
         }
       }
     }
@@ -337,11 +337,11 @@ public class FieldInfos implements Iterable<FieldInfo> {
     }
 
     synchronized void setDimensions(int number, String name, int dimensionCount, int dimensionNumBytes) {
-      if (dimensionNumBytes > DimensionalValues.MAX_NUM_BYTES) {
-        throw new IllegalArgumentException("dimension numBytes must be <= DimensionalValues.MAX_NUM_BYTES (= " + DimensionalValues.MAX_NUM_BYTES + "); got " + dimensionNumBytes + " for field=\"" + name + "\"");
+      if (dimensionNumBytes > PointValues.MAX_NUM_BYTES) {
+        throw new IllegalArgumentException("dimension numBytes must be <= PointValues.MAX_NUM_BYTES (= " + PointValues.MAX_NUM_BYTES + "); got " + dimensionNumBytes + " for field=\"" + name + "\"");
       }
-      if (dimensionCount > DimensionalValues.MAX_DIMENSIONS) {
-        throw new IllegalArgumentException("dimensionCount must be <= DimensionalValues.MAX_DIMENSIONS (= " + DimensionalValues.MAX_DIMENSIONS + "); got " + dimensionCount + " for field=\"" + name + "\"");
+      if (dimensionCount > PointValues.MAX_DIMENSIONS) {
+        throw new IllegalArgumentException("pointDimensionCount must be <= PointValues.MAX_DIMENSIONS (= " + PointValues.MAX_DIMENSIONS + "); got " + dimensionCount + " for field=\"" + name + "\"");
       }
       verifyConsistentDimensions(number, name, dimensionCount, dimensionNumBytes);
       dimensions.put(name, new FieldDimensions(dimensionCount, dimensionNumBytes));
@@ -432,7 +432,7 @@ public class FieldInfos implements Iterable<FieldInfo> {
       return addOrUpdateInternal(fi.name, fi.number, fi.hasVectors(),
                                  fi.omitsNorms(), fi.hasPayloads(),
                                  fi.getIndexOptions(), fi.getDocValuesType(),
-                                 fi.getDimensionCount(), fi.getDimensionNumBytes());
+                                 fi.getPointDimensionCount(), fi.getPointNumBytes());
     }
     
     public FieldInfo fieldInfo(String fieldName) {
diff --git a/lucene/core/src/java/org/apache/lucene/index/FilterCodecReader.java b/lucene/core/src/java/org/apache/lucene/index/FilterCodecReader.java
index b55bb88..65275e2 100644
--- a/lucene/core/src/java/org/apache/lucene/index/FilterCodecReader.java
+++ b/lucene/core/src/java/org/apache/lucene/index/FilterCodecReader.java
@@ -19,7 +19,7 @@ package org.apache.lucene.index;
 
 import java.util.Objects;
 
-import org.apache.lucene.codecs.DimensionalReader;
+import org.apache.lucene.codecs.PointReader;
 import org.apache.lucene.codecs.DocValuesProducer;
 import org.apache.lucene.codecs.FieldsProducer;
 import org.apache.lucene.codecs.NormsProducer;
@@ -82,13 +82,13 @@ public class FilterCodecReader extends CodecReader {
   }
 
   @Override
-  public DimensionalReader getDimensionalReader() {
-    return in.getDimensionalReader();
+  public PointReader getPointReader() {
+    return in.getPointReader();
   }
 
   @Override
-  public DimensionalValues getDimensionalValues() {
-    return in.getDimensionalValues();
+  public PointValues getPointValues() {
+    return in.getPointValues();
   }
 
   @Override
diff --git a/lucene/core/src/java/org/apache/lucene/index/FilterLeafReader.java b/lucene/core/src/java/org/apache/lucene/index/FilterLeafReader.java
index 4870532..eadeffa 100644
--- a/lucene/core/src/java/org/apache/lucene/index/FilterLeafReader.java
+++ b/lucene/core/src/java/org/apache/lucene/index/FilterLeafReader.java
@@ -381,8 +381,8 @@ public class FilterLeafReader extends LeafReader {
   }
 
   @Override
-  public DimensionalValues getDimensionalValues() {
-    return in.getDimensionalValues();
+  public PointValues getPointValues() {
+    return in.getPointValues();
   }
 
   @Override
diff --git a/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java b/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
index dce5dd7..b05e15a 100644
--- a/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
@@ -1018,7 +1018,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit, Accountable {
     for(SegmentCommitInfo info : segmentInfos) {
       FieldInfos fis = readFieldInfos(info);
       for(FieldInfo fi : fis) {
-        map.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getDimensionCount(), fi.getDimensionNumBytes());
+        map.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());
       }
     }
 
@@ -2495,7 +2495,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit, Accountable {
             FieldInfos fis = readFieldInfos(info);
             for(FieldInfo fi : fis) {
               // This will throw exceptions if any of the incoming fields have an illegal schema change:
-              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getDimensionCount(), fi.getDimensionNumBytes());
+              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType(), fi.getPointDimensionCount(), fi.getPointNumBytes());
             }
             infos.add(copySegmentAsIs(info, newSegName, context));
           }
diff --git a/lucene/core/src/java/org/apache/lucene/index/IndexableFieldType.java b/lucene/core/src/java/org/apache/lucene/index/IndexableFieldType.java
index 34b86e3..cf87b44 100644
--- a/lucene/core/src/java/org/apache/lucene/index/IndexableFieldType.java
+++ b/lucene/core/src/java/org/apache/lucene/index/IndexableFieldType.java
@@ -98,12 +98,12 @@ public interface IndexableFieldType {
   public DocValuesType docValuesType();
 
   /**
-   * If this is positive, the field is indexed dimensionally.
+   * If this is positive, the field is indexed as a point.
    */
-  public int dimensionCount();
+  public int pointDimensionCount();
 
   /**
    * The number of bytes in each dimension's values.
    */
-  public int dimensionNumBytes();
+  public int pointNumBytes();
 }
diff --git a/lucene/core/src/java/org/apache/lucene/index/LeafReader.java b/lucene/core/src/java/org/apache/lucene/index/LeafReader.java
index 523ee4e..9baaeb6 100644
--- a/lucene/core/src/java/org/apache/lucene/index/LeafReader.java
+++ b/lucene/core/src/java/org/apache/lucene/index/LeafReader.java
@@ -19,7 +19,6 @@ package org.apache.lucene.index;
 
 import java.io.IOException;
 
-import org.apache.lucene.codecs.DimensionalReader;
 import org.apache.lucene.index.IndexReader.ReaderClosedListener;
 import org.apache.lucene.util.Bits;
 
@@ -301,9 +300,9 @@ public abstract class LeafReader extends IndexReader {
    */
   public abstract Bits getLiveDocs();
 
-  /** Returns the {@link DimensionalReader} used for numeric or
-   *  spatial searches, or null if there are no dimensional fields. */
-  public abstract DimensionalValues getDimensionalValues();
+  /** Returns the {@link org.apache.lucene.codecs.PointReader} used for numeric or
+   *  spatial searches, or null if there are no point fields. */
+  public abstract PointValues getPointValues();
 
   /**
    * Checks consistency of this reader.
diff --git a/lucene/core/src/java/org/apache/lucene/index/MergeState.java b/lucene/core/src/java/org/apache/lucene/index/MergeState.java
index a325b3f..09bdfbd 100644
--- a/lucene/core/src/java/org/apache/lucene/index/MergeState.java
+++ b/lucene/core/src/java/org/apache/lucene/index/MergeState.java
@@ -20,7 +20,7 @@ package org.apache.lucene.index;
 import java.io.IOException;
 import java.util.List;
 
-import org.apache.lucene.codecs.DimensionalReader;
+import org.apache.lucene.codecs.PointReader;
 import org.apache.lucene.codecs.DocValuesProducer;
 import org.apache.lucene.codecs.FieldsProducer;
 import org.apache.lucene.codecs.NormsProducer;
@@ -66,8 +66,8 @@ public class MergeState {
   /** Postings to merge */
   public final FieldsProducer[] fieldsProducers;
 
-  /** Dimensional readers to merge */
-  public final DimensionalReader[] dimensionalReaders;
+  /** Point readers to merge */
+  public final PointReader[] pointReaders;
 
   /** New docID base per reader. */
   public final int[] docBase;
@@ -90,7 +90,7 @@ public class MergeState {
     storedFieldsReaders = new StoredFieldsReader[numReaders];
     termVectorsReaders = new TermVectorsReader[numReaders];
     docValuesProducers = new DocValuesProducer[numReaders];
-    dimensionalReaders = new DimensionalReader[numReaders];
+    pointReaders = new PointReader[numReaders];
     fieldInfos = new FieldInfos[numReaders];
     liveDocs = new Bits[numReaders];
 
@@ -122,9 +122,9 @@ public class MergeState {
       }
       
       fieldsProducers[i] = reader.getPostingsReader().getMergeInstance();
-      dimensionalReaders[i] = reader.getDimensionalReader();
-      if (dimensionalReaders[i] != null) {
-        dimensionalReaders[i] = dimensionalReaders[i].getMergeInstance();
+      pointReaders[i] = reader.getPointReader();
+      if (pointReaders[i] != null) {
+        pointReaders[i] = pointReaders[i].getMergeInstance();
       }
     }
 
diff --git a/lucene/core/src/java/org/apache/lucene/index/MultiDimensionalValues.java b/lucene/core/src/java/org/apache/lucene/index/MultiDimensionalValues.java
deleted file mode 100644
index 0acfd6e..0000000
--- a/lucene/core/src/java/org/apache/lucene/index/MultiDimensionalValues.java
+++ /dev/null
@@ -1,170 +0,0 @@
-package org.apache.lucene.index;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.util.StringHelper;
-
-class MultiDimensionalValues extends DimensionalValues {
-
-  private final List<DimensionalValues> subs;
-  private final List<Integer> docBases;
-
-  private MultiDimensionalValues(List<DimensionalValues> subs, List<Integer> docBases) {
-    this.subs = subs;
-    this.docBases = docBases;
-  }
-
-  public static DimensionalValues get(IndexReader r) {
-    final List<LeafReaderContext> leaves = r.leaves();
-    final int size = leaves.size();
-    if (size == 0) {
-      return null;
-    } else if (size == 1) {
-      return leaves.get(0).reader().getDimensionalValues();
-    }
-
-    List<DimensionalValues> values = new ArrayList<>();
-    List<Integer> docBases = new ArrayList<>();
-    for (int i = 0; i < size; i++) {
-      LeafReaderContext context = leaves.get(i);
-      DimensionalValues v = context.reader().getDimensionalValues();
-      if (v != null) {
-        values.add(v);
-        docBases.add(context.docBase);
-      }
-    }
-
-    if (values.isEmpty()) {
-      return null;
-    }
-
-    return new MultiDimensionalValues(values, docBases);
-  }
-
-  /** Finds all documents and points matching the provided visitor */
-  public void intersect(String fieldName, IntersectVisitor visitor) throws IOException {
-    for(int i=0;i<subs.size();i++) {
-      int docBase = docBases.get(i);
-      subs.get(i).intersect(fieldName,
-                        new IntersectVisitor() {
-                          @Override
-                          public void visit(int docID) throws IOException {
-                            visitor.visit(docBase+docID);
-                          }
-                          @Override
-                          public void visit(int docID, byte[] packedValue) throws IOException {
-                            visitor.visit(docBase+docID, packedValue);
-                          }
-                          @Override
-                          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {
-                            return visitor.compare(minPackedValue, maxPackedValue);
-                          }
-                        });
-    }
-  }
-
-  @Override
-  public String toString() {
-    StringBuilder b = new StringBuilder();
-    b.append("MultiDimensionalValues(");
-    for(int i=0;i<subs.size();i++) {
-      if (i > 0) {
-        b.append(", ");
-      }
-      b.append("docBase=");
-      b.append(docBases.get(i));
-      b.append(" sub=" + subs.get(i));
-    }
-    b.append(')');
-    return b.toString();
-  }
-
-  @Override
-  public byte[] getMinPackedValue(String fieldName) throws IOException {
-    byte[] result = null;
-    for(int i=0;i<subs.size();i++) {
-      byte[] minPackedValue = subs.get(i).getMinPackedValue(fieldName);
-      if (result == null) {
-        if (minPackedValue != null) {
-          result = minPackedValue.clone();
-        }
-      } else {
-        int numDims = subs.get(0).getNumDimensions(fieldName);
-        int bytesPerDim = subs.get(0).getBytesPerDimension(fieldName);
-        for(int dim=0;dim<numDims;dim++) {
-          int offset = dim*bytesPerDim;
-          if (StringHelper.compare(bytesPerDim, minPackedValue, offset, result, offset) < 0) {
-            System.arraycopy(minPackedValue, offset, result, offset, bytesPerDim);
-          }
-        }
-      }
-    }
-
-    return result;
-  }
-
-  @Override
-  public byte[] getMaxPackedValue(String fieldName) throws IOException {
-    byte[] result = null;
-    for(int i=0;i<subs.size();i++) {
-      byte[] maxPackedValue = subs.get(i).getMaxPackedValue(fieldName);
-      if (result == null) {
-        if (maxPackedValue != null) {
-          result = maxPackedValue.clone();
-        }
-      } else {
-        int numDims = subs.get(0).getNumDimensions(fieldName);
-        int bytesPerDim = subs.get(0).getBytesPerDimension(fieldName);
-        for(int dim=0;dim<numDims;dim++) {
-          int offset = dim*bytesPerDim;
-          if (StringHelper.compare(bytesPerDim, maxPackedValue, offset, result, offset) > 0) {
-            System.arraycopy(maxPackedValue, offset, result, offset, bytesPerDim);
-          }
-        }
-      }
-    }
-
-    return result;
-  }
-
-  @Override
-  public int getNumDimensions(String fieldName) throws IOException {
-    for(int i=0;i<subs.size();i++) {
-      int result = subs.get(i).getNumDimensions(fieldName);
-      if (result != 0) {
-        return result;
-      }
-    }
-    return 0;
-  }
-
-  @Override
-  public int getBytesPerDimension(String fieldName) throws IOException {
-    for(int i=0;i<subs.size();i++) {
-      int result = subs.get(i).getBytesPerDimension(fieldName);
-      if (result != 0) {
-        return result;
-      }
-    }
-    return 0;
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/index/MultiPointValues.java b/lucene/core/src/java/org/apache/lucene/index/MultiPointValues.java
new file mode 100644
index 0000000..12282e7
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/index/MultiPointValues.java
@@ -0,0 +1,170 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.lucene.util.StringHelper;
+
+class MultiPointValues extends PointValues {
+
+  private final List<PointValues> subs;
+  private final List<Integer> docBases;
+
+  private MultiPointValues(List<PointValues> subs, List<Integer> docBases) {
+    this.subs = subs;
+    this.docBases = docBases;
+  }
+
+  public static PointValues get(IndexReader r) {
+    final List<LeafReaderContext> leaves = r.leaves();
+    final int size = leaves.size();
+    if (size == 0) {
+      return null;
+    } else if (size == 1) {
+      return leaves.get(0).reader().getPointValues();
+    }
+
+    List<PointValues> values = new ArrayList<>();
+    List<Integer> docBases = new ArrayList<>();
+    for (int i = 0; i < size; i++) {
+      LeafReaderContext context = leaves.get(i);
+      PointValues v = context.reader().getPointValues();
+      if (v != null) {
+        values.add(v);
+        docBases.add(context.docBase);
+      }
+    }
+
+    if (values.isEmpty()) {
+      return null;
+    }
+
+    return new MultiPointValues(values, docBases);
+  }
+
+  /** Finds all documents and points matching the provided visitor */
+  public void intersect(String fieldName, IntersectVisitor visitor) throws IOException {
+    for(int i=0;i<subs.size();i++) {
+      int docBase = docBases.get(i);
+      subs.get(i).intersect(fieldName,
+                        new IntersectVisitor() {
+                          @Override
+                          public void visit(int docID) throws IOException {
+                            visitor.visit(docBase+docID);
+                          }
+                          @Override
+                          public void visit(int docID, byte[] packedValue) throws IOException {
+                            visitor.visit(docBase+docID, packedValue);
+                          }
+                          @Override
+                          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {
+                            return visitor.compare(minPackedValue, maxPackedValue);
+                          }
+                        });
+    }
+  }
+
+  @Override
+  public String toString() {
+    StringBuilder b = new StringBuilder();
+    b.append("MultiPointValues(");
+    for(int i=0;i<subs.size();i++) {
+      if (i > 0) {
+        b.append(", ");
+      }
+      b.append("docBase=");
+      b.append(docBases.get(i));
+      b.append(" sub=" + subs.get(i));
+    }
+    b.append(')');
+    return b.toString();
+  }
+
+  @Override
+  public byte[] getMinPackedValue(String fieldName) throws IOException {
+    byte[] result = null;
+    for(int i=0;i<subs.size();i++) {
+      byte[] minPackedValue = subs.get(i).getMinPackedValue(fieldName);
+      if (result == null) {
+        if (minPackedValue != null) {
+          result = minPackedValue.clone();
+        }
+      } else {
+        int numDims = subs.get(0).getNumDimensions(fieldName);
+        int bytesPerDim = subs.get(0).getBytesPerDimension(fieldName);
+        for(int dim=0;dim<numDims;dim++) {
+          int offset = dim*bytesPerDim;
+          if (StringHelper.compare(bytesPerDim, minPackedValue, offset, result, offset) < 0) {
+            System.arraycopy(minPackedValue, offset, result, offset, bytesPerDim);
+          }
+        }
+      }
+    }
+
+    return result;
+  }
+
+  @Override
+  public byte[] getMaxPackedValue(String fieldName) throws IOException {
+    byte[] result = null;
+    for(int i=0;i<subs.size();i++) {
+      byte[] maxPackedValue = subs.get(i).getMaxPackedValue(fieldName);
+      if (result == null) {
+        if (maxPackedValue != null) {
+          result = maxPackedValue.clone();
+        }
+      } else {
+        int numDims = subs.get(0).getNumDimensions(fieldName);
+        int bytesPerDim = subs.get(0).getBytesPerDimension(fieldName);
+        for(int dim=0;dim<numDims;dim++) {
+          int offset = dim*bytesPerDim;
+          if (StringHelper.compare(bytesPerDim, maxPackedValue, offset, result, offset) > 0) {
+            System.arraycopy(maxPackedValue, offset, result, offset, bytesPerDim);
+          }
+        }
+      }
+    }
+
+    return result;
+  }
+
+  @Override
+  public int getNumDimensions(String fieldName) throws IOException {
+    for(int i=0;i<subs.size();i++) {
+      int result = subs.get(i).getNumDimensions(fieldName);
+      if (result != 0) {
+        return result;
+      }
+    }
+    return 0;
+  }
+
+  @Override
+  public int getBytesPerDimension(String fieldName) throws IOException {
+    for(int i=0;i<subs.size();i++) {
+      int result = subs.get(i).getBytesPerDimension(fieldName);
+      if (result != 0) {
+        return result;
+      }
+    }
+    return 0;
+  }
+}
diff --git a/lucene/core/src/java/org/apache/lucene/index/ParallelLeafReader.java b/lucene/core/src/java/org/apache/lucene/index/ParallelLeafReader.java
index 88bc0e5..73c605f 100644
--- a/lucene/core/src/java/org/apache/lucene/index/ParallelLeafReader.java
+++ b/lucene/core/src/java/org/apache/lucene/index/ParallelLeafReader.java
@@ -315,15 +315,15 @@ public class ParallelLeafReader extends LeafReader {
   }
 
   @Override
-  public DimensionalValues getDimensionalValues() {
-    return new DimensionalValues() {
+  public PointValues getPointValues() {
+    return new PointValues() {
       @Override
       public void intersect(String fieldName, IntersectVisitor visitor) throws IOException {
         LeafReader reader = fieldToReader.get(fieldName);
         if (reader == null) {
           return;
         }
-        DimensionalValues dimValues = reader.getDimensionalValues();
+        PointValues dimValues = reader.getPointValues();
         if (dimValues == null) {
           return;
         }
@@ -336,7 +336,7 @@ public class ParallelLeafReader extends LeafReader {
         if (reader == null) {
           return null;
         }
-        DimensionalValues dimValues = reader.getDimensionalValues();
+        PointValues dimValues = reader.getPointValues();
         if (dimValues == null) {
           return null;
         }
@@ -349,7 +349,7 @@ public class ParallelLeafReader extends LeafReader {
         if (reader == null) {
           return null;
         }
-        DimensionalValues dimValues = reader.getDimensionalValues();
+        PointValues dimValues = reader.getPointValues();
         if (dimValues == null) {
           return null;
         }
@@ -362,7 +362,7 @@ public class ParallelLeafReader extends LeafReader {
         if (reader == null) {
           return 0;
         }
-        DimensionalValues dimValues = reader.getDimensionalValues();
+        PointValues dimValues = reader.getPointValues();
         if (dimValues == null) {
           return 0;
         }
@@ -375,7 +375,7 @@ public class ParallelLeafReader extends LeafReader {
         if (reader == null) {
           return 0;
         }
-        DimensionalValues dimValues = reader.getDimensionalValues();
+        PointValues dimValues = reader.getPointValues();
         if (dimValues == null) {
           return 0;
         }
diff --git a/lucene/core/src/java/org/apache/lucene/index/PointValues.java b/lucene/core/src/java/org/apache/lucene/index/PointValues.java
new file mode 100644
index 0000000..e786bad
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/index/PointValues.java
@@ -0,0 +1,88 @@
+package org.apache.lucene.index;
+
+import java.io.IOException;
+
+import org.apache.lucene.util.bkd.BKDWriter;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** Allows recursively visiting point values indexed with {@link org.apache.lucene.document.IntPoint},
+ *  {@link org.apache.lucene.document.FloatPoint}, {@link org.apache.lucene.document.LongPoint}, {@link org.apache.lucene.document.DoublePoint}
+ *  or {@link org.apache.lucene.document.BinaryPoint}.
+ *
+ *  @lucene.experimental */
+public abstract class PointValues {
+
+  /** Maximum number of bytes for each dimension */
+  public static final int MAX_NUM_BYTES = 16;
+
+  /** Maximum number of dimensions */
+  public static final int MAX_DIMENSIONS = BKDWriter.MAX_DIMS;
+
+  /** Default constructor */
+  protected PointValues() {
+  }
+
+  /** Used by {@link #intersect} to check how each recursive cell corresponds to the query. */
+  public enum Relation {
+    /** Return this if the cell is fully contained by the query */
+    CELL_INSIDE_QUERY,
+    /** Return this if the cell and query do not overlap */
+    CELL_OUTSIDE_QUERY,
+    /** Return this if the cell partially overlapps the query */
+    CELL_CROSSES_QUERY
+  };
+
+  /** We recurse the BKD tree, using a provided instance of this to guide the recursion.
+   *
+   * @lucene.experimental */
+  public interface IntersectVisitor {
+    /** Called for all docs in a leaf cell that's fully contained by the query.  The
+     *  consumer should blindly accept the docID. */
+    void visit(int docID) throws IOException;
+
+    /** Called for all docs in a leaf cell that crosses the query.  The consumer
+     *  should scrutinize the packedValue to decide whether to accept it. */
+    void visit(int docID, byte[] packedValue) throws IOException;
+
+    /** Called for non-leaf cells to test how the cell relates to the query, to
+     *  determine how to further recurse down the treer. */
+    Relation compare(byte[] minPackedValue, byte[] maxPackedValue);
+
+    /** Notifies the caller that this many documents (from one block) are about
+     *  to be visited */
+    default void grow(int count) {};
+  }
+
+  /** Finds all documents and points matching the provided visitor.
+   *  This method does not enforce live docs, so it's up to the caller
+   *  to test whether each document is deleted, if necessary. */
+  public abstract void intersect(String fieldName, IntersectVisitor visitor) throws IOException;
+
+  /** Returns minimum value for each dimension, packed, or null if no points were indexed */
+  public abstract byte[] getMinPackedValue(String fieldName) throws IOException;
+
+  /** Returns maximum value for each dimension, packed, or null if no points were indexed */
+  public abstract byte[] getMaxPackedValue(String fieldName) throws IOException;
+
+  /** Returns how many dimensions were indexed */
+  public abstract int getNumDimensions(String fieldName) throws IOException;
+
+  /** Returns the number of bytes per dimension */
+  public abstract int getBytesPerDimension(String fieldName) throws IOException;
+}
diff --git a/lucene/core/src/java/org/apache/lucene/index/PointValuesWriter.java b/lucene/core/src/java/org/apache/lucene/index/PointValuesWriter.java
new file mode 100644
index 0000000..e05f270
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/index/PointValuesWriter.java
@@ -0,0 +1,114 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.PointReader;
+import org.apache.lucene.codecs.PointWriter;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.ByteBlockPool;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.Counter;
+import org.apache.lucene.util.RamUsageEstimator;
+
+/** Buffers up pending byte[][] value(s) per doc, then flushes when segment flushes. */
+class PointValuesWriter {
+  private final FieldInfo fieldInfo;
+  private final ByteBlockPool bytes;
+  private final Counter iwBytesUsed;
+  private int[] docIDs;
+  private int numDocs;
+  private final byte[] packedValue;
+
+  public PointValuesWriter(DocumentsWriterPerThread docWriter, FieldInfo fieldInfo) {
+    this.fieldInfo = fieldInfo;
+    this.iwBytesUsed = docWriter.bytesUsed;
+    this.bytes = new ByteBlockPool(docWriter.byteBlockAllocator);
+    docIDs = new int[16];
+    iwBytesUsed.addAndGet(16 * RamUsageEstimator.NUM_BYTES_INT);
+    packedValue = new byte[fieldInfo.getPointDimensionCount() * fieldInfo.getPointNumBytes()];
+  }
+
+  public void addPackedValue(int docID, BytesRef value) {
+    if (value == null) {
+      throw new IllegalArgumentException("field=" + fieldInfo.name + ": point value cannot be null");
+    }
+    if (value.length != fieldInfo.getPointDimensionCount() * fieldInfo.getPointNumBytes()) {
+      throw new IllegalArgumentException("field=" + fieldInfo.name + ": this field's value has length=" + value.length + " but should be " + (fieldInfo.getPointDimensionCount() * fieldInfo.getPointNumBytes()));
+    }
+    if (docIDs.length == numDocs) {
+      docIDs = ArrayUtil.grow(docIDs, numDocs+1);
+      iwBytesUsed.addAndGet((docIDs.length - numDocs) * RamUsageEstimator.NUM_BYTES_INT);
+    }
+    bytes.append(value);
+    docIDs[numDocs] = docID;
+    numDocs++;
+  }
+
+  public void flush(SegmentWriteState state, PointWriter writer) throws IOException {
+
+    writer.writeField(fieldInfo,
+                      new PointReader() {
+                        @Override
+                        public void intersect(String fieldName, IntersectVisitor visitor) throws IOException {
+                          if (fieldName.equals(fieldInfo.name) == false) {
+                            throw new IllegalArgumentException("fieldName must be the same");
+                          }
+                          for(int i=0;i<numDocs;i++) {
+                            bytes.readBytes(packedValue.length * i, packedValue, 0, packedValue.length);
+                            visitor.visit(docIDs[i], packedValue);
+                          }
+                        }
+
+                        @Override
+                        public void checkIntegrity() {
+                          throw new UnsupportedOperationException();
+                        }
+
+                        @Override
+                        public long ramBytesUsed() {
+                          return 0L;
+                        }
+
+                        @Override
+                        public void close() {
+                        }
+
+                        @Override
+                        public byte[] getMinPackedValue(String fieldName) {
+                          throw new UnsupportedOperationException();
+                        }
+
+                        @Override
+                        public byte[] getMaxPackedValue(String fieldName) {
+                          throw new UnsupportedOperationException();
+                        }
+
+                        @Override
+                        public int getNumDimensions(String fieldName) {
+                          throw new UnsupportedOperationException();
+                        }
+
+                        @Override
+                        public int getBytesPerDimension(String fieldName) {
+                          throw new UnsupportedOperationException();
+                        }
+                      });
+  }
+}
diff --git a/lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java b/lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java
index 1b3a12e..414c707 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java
@@ -24,7 +24,7 @@ import java.util.Set;
 import java.util.concurrent.atomic.AtomicInteger;
 
 import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.DimensionalReader;
+import org.apache.lucene.codecs.PointReader;
 import org.apache.lucene.codecs.FieldsProducer;
 import org.apache.lucene.codecs.NormsProducer;
 import org.apache.lucene.codecs.PostingsFormat;
@@ -54,7 +54,7 @@ final class SegmentCoreReaders {
 
   final StoredFieldsReader fieldsReaderOrig;
   final TermVectorsReader termVectorsReaderOrig;
-  final DimensionalReader dimensionalReader;
+  final PointReader pointReader;
   final Directory cfsReader;
   /** 
    * fieldinfos for this core: means gen=-1.
@@ -124,10 +124,10 @@ final class SegmentCoreReaders {
         termVectorsReaderOrig = null;
       }
 
-      if (coreFieldInfos.hasDimensionalValues()) {
-        dimensionalReader = codec.dimensionalFormat().fieldsReader(segmentReadState);
+      if (coreFieldInfos.hasPointValues()) {
+        pointReader = codec.pointFormat().fieldsReader(segmentReadState);
       } else {
-        dimensionalReader = null;
+        pointReader = null;
       }
       success = true;
     } finally {
@@ -157,7 +157,7 @@ final class SegmentCoreReaders {
       Throwable th = null;
       try {
         IOUtils.close(termVectorsLocal, fieldsReaderLocal, fields, termVectorsReaderOrig, fieldsReaderOrig,
-                      cfsReader, normsProducer, dimensionalReader);
+                      cfsReader, normsProducer, pointReader);
       } catch (Throwable throwable) {
         th = throwable;
       } finally {
diff --git a/lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java b/lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java
index 16fe667..d80646f 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java
@@ -21,7 +21,7 @@ import java.io.IOException;
 import java.util.List;
 
 import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.DimensionalWriter;
+import org.apache.lucene.codecs.PointWriter;
 import org.apache.lucene.codecs.DocValuesConsumer;
 import org.apache.lucene.codecs.FieldsConsumer;
 import org.apache.lucene.codecs.NormsConsumer;
@@ -113,12 +113,12 @@ final class SegmentMerger {
     if (mergeState.infoStream.isEnabled("SM")) {
       t0 = System.nanoTime();
     }
-    if (mergeState.mergeFieldInfos.hasDimensionalValues()) {
-      mergeDimensionalValues(segmentWriteState);
+    if (mergeState.mergeFieldInfos.hasPointValues()) {
+      mergePoints(segmentWriteState);
     }
     if (mergeState.infoStream.isEnabled("SM")) {
       long t1 = System.nanoTime();
-      mergeState.infoStream.message("SM", ((t1-t0)/1000000) + " msec to merge dimensional values [" + numMerged + " docs]");
+      mergeState.infoStream.message("SM", ((t1-t0)/1000000) + " msec to merge points [" + numMerged + " docs]");
     }
     
     if (mergeState.mergeFieldInfos.hasNorms()) {
@@ -163,8 +163,8 @@ final class SegmentMerger {
     }
   }
 
-  private void mergeDimensionalValues(SegmentWriteState segmentWriteState) throws IOException {
-    try (DimensionalWriter writer = codec.dimensionalFormat().fieldsWriter(segmentWriteState)) {
+  private void mergePoints(SegmentWriteState segmentWriteState) throws IOException {
+    try (PointWriter writer = codec.pointFormat().fieldsWriter(segmentWriteState)) {
       writer.merge(mergeState);
     }
   }
diff --git a/lucene/core/src/java/org/apache/lucene/index/SegmentReader.java b/lucene/core/src/java/org/apache/lucene/index/SegmentReader.java
index 57a4b9a..20f87fd 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SegmentReader.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SegmentReader.java
@@ -21,7 +21,7 @@ import java.io.IOException;
 import java.util.Collections;
 
 import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.DimensionalReader;
+import org.apache.lucene.codecs.PointReader;
 import org.apache.lucene.codecs.DocValuesProducer;
 import org.apache.lucene.codecs.FieldInfosFormat;
 import org.apache.lucene.codecs.FieldsProducer;
@@ -218,9 +218,9 @@ public final class SegmentReader extends CodecReader {
   }
   
   @Override
-  public DimensionalValues getDimensionalValues() {
+  public PointValues getPointValues() {
     ensureOpen();
-    return core.dimensionalReader;
+    return core.pointReader;
   }
 
   @Override
@@ -242,9 +242,9 @@ public final class SegmentReader extends CodecReader {
   }
 
   @Override
-  public DimensionalReader getDimensionalReader() {
+  public PointReader getPointReader() {
     ensureOpen();
-    return core.dimensionalReader;
+    return core.pointReader;
   }
 
   @Override
diff --git a/lucene/core/src/java/org/apache/lucene/index/SlowCodecReaderWrapper.java b/lucene/core/src/java/org/apache/lucene/index/SlowCodecReaderWrapper.java
index ba236d5..c925efb 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SlowCodecReaderWrapper.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SlowCodecReaderWrapper.java
@@ -20,7 +20,7 @@ package org.apache.lucene.index;
 import java.io.IOException;
 import java.util.Iterator;
 
-import org.apache.lucene.codecs.DimensionalReader;
+import org.apache.lucene.codecs.PointReader;
 import org.apache.lucene.codecs.DocValuesProducer;
 import org.apache.lucene.codecs.FieldsProducer;
 import org.apache.lucene.codecs.NormsProducer;
@@ -92,13 +92,13 @@ public final class SlowCodecReaderWrapper {
         }
 
         @Override
-        public DimensionalValues getDimensionalValues() {
-          return reader.getDimensionalValues();
+        public PointValues getPointValues() {
+          return reader.getPointValues();
         }
 
         @Override
-        public DimensionalReader getDimensionalReader() {
-          return dimensionalValuesToReader(reader.getDimensionalValues());
+        public PointReader getPointReader() {
+          return pointValuesToReader(reader.getPointValues());
         }
 
         @Override
@@ -129,11 +129,11 @@ public final class SlowCodecReaderWrapper {
     }
   }
 
-  private static DimensionalReader dimensionalValuesToReader(DimensionalValues values) {
+  private static PointReader pointValuesToReader(PointValues values) {
     if (values == null) {
       return null;
     }
-    return new DimensionalReader() {
+    return new PointReader() {
       @Override
       public void intersect(String fieldName, IntersectVisitor visitor) throws IOException {
         values.intersect(fieldName, visitor);
diff --git a/lucene/core/src/java/org/apache/lucene/index/SlowCompositeReaderWrapper.java b/lucene/core/src/java/org/apache/lucene/index/SlowCompositeReaderWrapper.java
index ee00482..0f6cadf 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SlowCompositeReaderWrapper.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SlowCompositeReaderWrapper.java
@@ -234,9 +234,9 @@ public final class SlowCompositeReaderWrapper extends LeafReader {
   }
 
   @Override
-  public DimensionalValues getDimensionalValues() {
+  public PointValues getPointValues() {
     ensureOpen();
-    return MultiDimensionalValues.get(in);
+    return MultiPointValues.get(in);
   }
 
   @Override
diff --git a/lucene/core/src/java/org/apache/lucene/search/DimensionalRangeQuery.java b/lucene/core/src/java/org/apache/lucene/search/DimensionalRangeQuery.java
deleted file mode 100644
index 864a65e..0000000
--- a/lucene/core/src/java/org/apache/lucene/search/DimensionalRangeQuery.java
+++ /dev/null
@@ -1,353 +0,0 @@
-package org.apache.lucene.search;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.Objects;
-
-import org.apache.lucene.document.DimensionalBinaryField;
-import org.apache.lucene.document.DimensionalDoubleField;
-import org.apache.lucene.document.DimensionalFloatField;
-import org.apache.lucene.document.DimensionalIntField;
-import org.apache.lucene.document.DimensionalLongField;
-import org.apache.lucene.index.DimensionalValues;
-import org.apache.lucene.index.DimensionalValues.IntersectVisitor;
-import org.apache.lucene.index.DimensionalValues.Relation;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.LeafReader;
-import org.apache.lucene.index.LeafReaderContext;
-import org.apache.lucene.util.DocIdSetBuilder;
-import org.apache.lucene.util.NumericUtils;
-import org.apache.lucene.util.RamUsageEstimator;
-import org.apache.lucene.util.StringHelper;
-
-/** Searches for ranges in fields previously indexed using dimensional
- *  fields, e.g. {@link DimensionalLongField}.  In a 1D field this is
- *  a simple range query; in a multi-dimensional field it's a box shape. */
-
-public class DimensionalRangeQuery extends Query {
-  final String field;
-  final int numDims;
-  final byte[][] lowerPoint;
-  final boolean[] lowerInclusive;
-  final byte[][] upperPoint;
-  final boolean[] upperInclusive;
-  // This is null only in the "fully open range" case
-  final Integer bytesPerDim;
-
-  public DimensionalRangeQuery(String field,
-                               byte[][] lowerPoint, boolean[] lowerInclusive,
-                               byte[][] upperPoint, boolean[] upperInclusive) {
-    this.field = field;
-    if (lowerPoint == null) {
-      throw new IllegalArgumentException("lowerPoint must not be null");
-    }
-    if (upperPoint == null) {
-      throw new IllegalArgumentException("upperPoint must not be null");
-    }
-    numDims = lowerPoint.length;
-    if (upperPoint.length != numDims) {
-      throw new IllegalArgumentException("lowerPoint has length=" + numDims + " but upperPoint has different length=" + upperPoint.length);
-    }
-    this.lowerPoint = lowerPoint;
-    this.lowerInclusive = lowerInclusive;
-    this.upperPoint = upperPoint;
-    this.upperInclusive = upperInclusive;
-
-    int bytesPerDim = -1;
-    for(byte[] value : lowerPoint) {
-      if (value != null) {
-        if (bytesPerDim == -1) {
-          bytesPerDim = value.length;
-        } else if (value.length != bytesPerDim) {
-          throw new IllegalArgumentException("all dimensions must have same bytes length, but saw " + bytesPerDim + " and " + value.length);
-        }
-      }
-    }
-    for(byte[] value : upperPoint) {
-      if (value != null) {
-        if (bytesPerDim == -1) {
-          bytesPerDim = value.length;
-        } else if (value.length != bytesPerDim) {
-          throw new IllegalArgumentException("all dimensions must have same bytes length, but saw " + bytesPerDim + " and " + value.length);
-        }
-      }
-    }
-    if (bytesPerDim == -1) {
-      this.bytesPerDim = null;
-    } else {
-      this.bytesPerDim = bytesPerDim;
-    }
-  }
-
-  /** Use in the 1D case when you indexed 1D int values using {@link DimensionalIntField} */
-  public static DimensionalRangeQuery new1DIntRange(String field, Integer lowerValue, boolean lowerInclusive, Integer upperValue, boolean upperInclusive) {
-    return new DimensionalRangeQuery(field, pack(lowerValue), new boolean[] {lowerInclusive}, pack(upperValue), new boolean[] {upperInclusive});
-  }
-
-  /** Use in the 1D case when you indexed 1D long values using {@link DimensionalLongField} */
-  public static DimensionalRangeQuery new1DLongRange(String field, Long lowerValue, boolean lowerInclusive, Long upperValue, boolean upperInclusive) {
-    return new DimensionalRangeQuery(field, pack(lowerValue), new boolean[] {lowerInclusive}, pack(upperValue), new boolean[] {upperInclusive});
-  }
-
-  /** Use in the 1D case when you indexed 1D float values using {@link DimensionalFloatField} */
-  public static DimensionalRangeQuery new1DFloatRange(String field, Float lowerValue, boolean lowerInclusive, Float upperValue, boolean upperInclusive) {
-    return new DimensionalRangeQuery(field, pack(lowerValue), new boolean[] {lowerInclusive}, pack(upperValue), new boolean[] {upperInclusive});
-  }
-
-  /** Use in the 1D case when you indexed 1D double values using {@link DimensionalDoubleField} */
-  public static DimensionalRangeQuery new1DDoubleRange(String field, Double lowerValue, boolean lowerInclusive, Double upperValue, boolean upperInclusive) {
-    return new DimensionalRangeQuery(field, pack(lowerValue), new boolean[] {lowerInclusive}, pack(upperValue), new boolean[] {upperInclusive});
-  }
-
-  /** Use in the 1D case when you indexed binary values using {@link DimensionalBinaryField} */
-  public static DimensionalRangeQuery new1DBinaryRange(String field, byte[] lowerValue, boolean lowerInclusive, byte[] upperValue, boolean upperInclusive) {
-    return new DimensionalRangeQuery(field, new byte[][] {lowerValue}, new boolean[] {lowerInclusive}, new byte[][] {upperValue}, new boolean[] {upperInclusive});
-  }
-
-  private static byte[][] pack(Long value) {
-    if (value == null) {
-      // OK: open ended range
-      return new byte[1][];
-    }
-    byte[][] result = new byte[][] {new byte[RamUsageEstimator.NUM_BYTES_LONG]};
-    NumericUtils.longToBytes(value, result[0], 0);
-    return result;
-  }
-
-  private static byte[][] pack(Double value) {
-    if (value == null) {
-      // OK: open ended range
-      return new byte[1][];
-    }
-    byte[][] result = new byte[][] {new byte[RamUsageEstimator.NUM_BYTES_LONG]};
-    NumericUtils.longToBytesDirect(NumericUtils.doubleToSortableLong(value), result[0], 0);
-    return result;
-  }
-
-  private static byte[][] pack(Integer value) {
-    if (value == null) {
-      // OK: open ended range
-      return new byte[1][];
-    }
-    byte[][] result = new byte[][] {new byte[RamUsageEstimator.NUM_BYTES_INT]};
-    NumericUtils.intToBytes(value, result[0], 0);
-    return result;
-  }
-
-  private static byte[][] pack(Float value) {
-    if (value == null) {
-      // OK: open ended range
-      return new byte[1][];
-    }
-    byte[][] result = new byte[][] {new byte[RamUsageEstimator.NUM_BYTES_INT]};
-    NumericUtils.intToBytesDirect(NumericUtils.floatToSortableInt(value), result[0], 0);
-    return result;
-  }
-
-  @Override
-  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {
-
-    // We don't use RandomAccessWeight here: it's no good to approximate with "match all docs".
-    // This is an inverted structure and should be used in the first pass:
-
-    return new ConstantScoreWeight(this) {
-
-      @Override
-      public Scorer scorer(LeafReaderContext context) throws IOException {
-        LeafReader reader = context.reader();
-        DimensionalValues values = reader.getDimensionalValues();
-        if (values == null) {
-          // No docs in this segment indexed any field dimensionally
-          return null;
-        }
-        FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);
-        if (fieldInfo == null) {
-          // No docs in this segment indexed this field at all
-          return null;
-        }
-        if (fieldInfo.getDimensionCount() != numDims) {
-          throw new IllegalArgumentException("field=\"" + field + "\" was indexed with numDims=" + fieldInfo.getDimensionCount() + " but this query has numDims=" + numDims);
-        }
-        if (bytesPerDim != null && bytesPerDim.intValue() != fieldInfo.getDimensionNumBytes()) {
-          throw new IllegalArgumentException("field=\"" + field + "\" was indexed with bytesPerDim=" + fieldInfo.getDimensionNumBytes() + " but this query has bytesPerDim=" + bytesPerDim);
-        }
-        int bytesPerDim = fieldInfo.getDimensionNumBytes();
-
-        byte[] packedLowerIncl = new byte[numDims * bytesPerDim];
-        byte[] packedUpperIncl = new byte[numDims * bytesPerDim];
-
-        byte[] minValue = new byte[bytesPerDim];
-        byte[] maxValue = new byte[bytesPerDim];
-        Arrays.fill(maxValue, (byte) 0xff);
-
-        byte[] one = new byte[bytesPerDim];
-        one[bytesPerDim-1] = 1;
-
-        // Carefully pack lower and upper bounds, taking care of per-dim inclusive:
-        for(int dim=0;dim<numDims;dim++) {
-          if (lowerPoint[dim] != null) {
-            if (lowerInclusive[dim] == false) {
-              if (Arrays.equals(lowerPoint[dim], maxValue)) {
-                return null;
-              } else {
-                byte[] value = new byte[bytesPerDim];
-                NumericUtils.add(bytesPerDim, 0, lowerPoint[dim], one, value);
-                System.arraycopy(value, 0, packedLowerIncl, dim*bytesPerDim, bytesPerDim);
-              }
-            } else {
-              System.arraycopy(lowerPoint[dim], 0, packedLowerIncl, dim*bytesPerDim, bytesPerDim);
-            }
-          } else {
-            // Open-ended range: we just leave 0s in this packed dim for the lower value
-          }
-
-          if (upperPoint[dim] != null) {
-            if (upperInclusive[dim] == false) {
-              if (Arrays.equals(upperPoint[dim], minValue)) {
-                return null;
-              } else {
-                byte[] value = new byte[bytesPerDim];
-                NumericUtils.subtract(bytesPerDim, 0, upperPoint[dim], one, value);
-                System.arraycopy(value, 0, packedUpperIncl, dim*bytesPerDim, bytesPerDim);
-              }
-            } else {
-              System.arraycopy(upperPoint[dim], 0, packedUpperIncl, dim*bytesPerDim, bytesPerDim);
-            }
-          } else {
-            // Open-ended range: fill with max point for this dim:
-            System.arraycopy(maxValue, 0, packedUpperIncl, dim*bytesPerDim, bytesPerDim);
-          }
-        }
-
-        // Now packedLowerIncl and packedUpperIncl are inclusive, and non-empty space:
-
-        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc());
-
-        int[] hitCount = new int[1];
-        values.intersect(field,
-                         new IntersectVisitor() {
-
-                           @Override
-                           public void grow(int count) {
-                             result.grow(count);
-                           }
-
-                           @Override
-                           public void visit(int docID) {
-                             hitCount[0]++;
-                             result.add(docID);
-                           }
-
-                           @Override
-                           public void visit(int docID, byte[] packedValue) {
-                             for(int dim=0;dim<numDims;dim++) {
-                               int offset = dim*bytesPerDim;
-                               if (StringHelper.compare(bytesPerDim, packedValue, offset, packedLowerIncl, offset) < 0) {
-                                 // Doc's value is too low, in this dimension
-                                 return;
-                               }
-                               if (StringHelper.compare(bytesPerDim, packedValue, offset, packedUpperIncl, offset) > 0) {
-                                 // Doc's value is too high, in this dimension
-                                 return;
-                               }
-                             }
-
-                             // Doc is in-bounds
-                             hitCount[0]++;
-                             result.add(docID);
-                           }
-
-                           @Override
-                           public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {
-
-                             boolean crosses = false;
-
-                             for(int dim=0;dim<numDims;dim++) {
-                               int offset = dim*bytesPerDim;
-
-                               if (StringHelper.compare(bytesPerDim, minPackedValue, offset, packedUpperIncl, offset) > 0 ||
-                                   StringHelper.compare(bytesPerDim, maxPackedValue, offset, packedLowerIncl, offset) < 0) {
-                                 return Relation.CELL_OUTSIDE_QUERY;
-                               }
-
-                               crosses |= StringHelper.compare(bytesPerDim, minPackedValue, offset, packedLowerIncl, offset) < 0 ||
-                                 StringHelper.compare(bytesPerDim, maxPackedValue, offset, packedUpperIncl, offset) > 0;
-                             }
-
-                             if (crosses) {
-                               return Relation.CELL_CROSSES_QUERY;
-                             } else {
-                               return Relation.CELL_INSIDE_QUERY;
-                             }
-                           }
-                         });
-
-        // NOTE: hitCount[0] will be over-estimate in multi-valued case
-        return new ConstantScoreScorer(this, score(), result.build(hitCount[0]).iterator());
-      }
-    };
-  }
-
-  @Override
-  public int hashCode() {
-    int hash = super.hashCode();
-    hash += Arrays.hashCode(lowerPoint)^0x14fa55fb;
-    hash += Arrays.hashCode(upperPoint)^0x733fa5fe;
-    hash += Arrays.hashCode(lowerInclusive)^0x14fa55fb;
-    hash += Arrays.hashCode(upperInclusive)^0x733fa5fe;
-    hash += numDims^0x14fa55fb;
-    hash += Objects.hashCode(bytesPerDim);
-    return hash;
-  }
-
-  @Override
-  public boolean equals(Object other) {
-    if (super.equals(other)) {
-      final DimensionalRangeQuery q = (DimensionalRangeQuery) other;
-      return q.numDims == numDims &&
-        q.bytesPerDim == bytesPerDim &&
-        Arrays.equals(lowerPoint, q.lowerPoint) &&
-        Arrays.equals(lowerInclusive, q.lowerInclusive) &&
-        Arrays.equals(upperPoint, q.upperPoint) &&
-        Arrays.equals(upperInclusive, q.upperInclusive);
-    }
-
-    return false;
-  }
-
-  @Override
-  public String toString(String field) {
-    final StringBuilder sb = new StringBuilder();
-    sb.append(getClass().getSimpleName());
-    sb.append(':');
-    if (this.field.equals(field) == false) {
-      sb.append("field=");
-      sb.append(this.field);
-      sb.append(':');
-    }
-
-    return sb.append('[')
-      .append(Arrays.toString(lowerPoint))
-      .append(" TO ")
-      .append(Arrays.toString(upperPoint))
-      .append(']')
-      .toString();
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/search/ExactPointQuery.java b/lucene/core/src/java/org/apache/lucene/search/ExactPointQuery.java
new file mode 100644
index 0000000..21d7357
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/search/ExactPointQuery.java
@@ -0,0 +1,153 @@
+package org.apache.lucene.search;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.Objects;
+
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.index.PointValues.IntersectVisitor;
+import org.apache.lucene.index.PointValues.Relation;
+import org.apache.lucene.index.PointValues;
+import org.apache.lucene.util.DocIdSetBuilder;
+import org.apache.lucene.util.NumericUtils;
+import org.apache.lucene.util.RamUsageEstimator;
+import org.apache.lucene.util.StringHelper;
+
+/** Searches for single points in fields previously indexed using points
+ *  e.g. {@link org.apache.lucene.document.LongPoint}. */
+
+public class ExactPointQuery extends Query {
+  final String field;
+  final int numDims;
+  final byte[][] point;
+  final int bytesPerDim;
+
+  public ExactPointQuery(String field, byte[][] point) {
+    this.field = field;
+    if (point == null) {
+      throw new IllegalArgumentException("point must not be null");
+    }
+    this.point = point;
+    this.numDims = point.length;
+
+    int bytesPerDim = -1;
+    for(byte[] value : point) {
+      if (value == null) {
+        throw new IllegalArgumentException("point's dimensional values must not be null");
+      }
+      if (bytesPerDim == -1) {
+        bytesPerDim = value.length;
+      } else if (value.length != bytesPerDim) {
+        throw new IllegalArgumentException("all dimensions must have same bytes length, but saw " + bytesPerDim + " and " + value.length);
+      }
+    }
+    this.bytesPerDim = bytesPerDim;
+  }
+
+  /** Use in the 1D case when you indexed 1D int values using {@link org.apache.lucene.document.IntPoint} */
+  public static ExactPointQuery new1DIntExact(String field, int value) {
+    return new ExactPointQuery(field, pack(value));
+  }
+
+  /** Use in the 1D case when you indexed 1D long values using {@link org.apache.lucene.document.LongPoint} */
+  public static ExactPointQuery new1DLongExact(String field, long value) {
+    return new ExactPointQuery(field, pack(value));
+  }
+
+  /** Use in the 1D case when you indexed 1D float values using {@link org.apache.lucene.document.FloatPoint} */
+  public static ExactPointQuery new1DFloatExact(String field, float value) {
+    return new ExactPointQuery(field, pack(value));
+  }
+
+  /** Use in the 1D case when you indexed 1D double values using {@link org.apache.lucene.document.DoublePoint} */
+  public static ExactPointQuery new1DDoubleExact(String field, double value) {
+    return new ExactPointQuery(field, pack(value));
+  }
+
+  private static byte[][] pack(long value) {
+    byte[][] result = new byte[][] {new byte[RamUsageEstimator.NUM_BYTES_LONG]};
+    NumericUtils.longToBytes(value, result[0], 0);
+    return result;
+  }
+
+  private static byte[][] pack(double value) {
+    byte[][] result = new byte[][] {new byte[RamUsageEstimator.NUM_BYTES_LONG]};
+    NumericUtils.longToBytesDirect(NumericUtils.doubleToSortableLong(value), result[0], 0);
+    return result;
+  }
+
+  private static byte[][] pack(int value) {
+    byte[][] result = new byte[][] {new byte[RamUsageEstimator.NUM_BYTES_INT]};
+    NumericUtils.intToBytes(value, result[0], 0);
+    return result;
+  }
+
+  private static byte[][] pack(float value) {
+    byte[][] result = new byte[][] {new byte[RamUsageEstimator.NUM_BYTES_INT]};
+    NumericUtils.intToBytesDirect(NumericUtils.floatToSortableInt(value), result[0], 0);
+    return result;
+  }
+
+  @Override
+  public Query rewrite(IndexReader reader) throws IOException {
+    boolean[] inclusive = new boolean[] {true};
+    return new PointRangeQuery(field, point, inclusive, point, inclusive);
+  }
+
+  @Override
+  public int hashCode() {
+    int hash = super.hashCode();
+    hash += Arrays.hashCode(point)^0x14fa55fb;
+    hash += numDims^0x14fa55fb;
+    hash += Objects.hashCode(bytesPerDim);
+    return hash;
+  }
+
+  @Override
+  public boolean equals(Object other) {
+    if (super.equals(other)) {
+      final ExactPointQuery q = (ExactPointQuery) other;
+      return q.numDims == numDims &&
+        q.bytesPerDim == bytesPerDim &&
+        Arrays.equals(point, q.point);
+    }
+
+    return false;
+  }
+
+  @Override
+  public String toString(String field) {
+    final StringBuilder sb = new StringBuilder();
+    sb.append(getClass().getSimpleName());
+    sb.append(':');
+    if (this.field.equals(field) == false) {
+      sb.append("field=");
+      sb.append(this.field);
+      sb.append(':');
+    }
+
+    return sb.append(" point=")
+      .append(Arrays.toString(point))
+      .toString();
+  }
+}
diff --git a/lucene/core/src/java/org/apache/lucene/search/LegacyNumericRangeQuery.java b/lucene/core/src/java/org/apache/lucene/search/LegacyNumericRangeQuery.java
index be1bf68..cebfd14 100644
--- a/lucene/core/src/java/org/apache/lucene/search/LegacyNumericRangeQuery.java
+++ b/lucene/core/src/java/org/apache/lucene/search/LegacyNumericRangeQuery.java
@@ -157,7 +157,7 @@ import org.apache.lucene.index.Term; // for javadocs
  * precision step). This query type was developed for a geographic portal, where the performance for
  * e.g. bounding boxes or exact date/time stamps is important.</p>
  *
- * @deprecated Please use {@link DimensionalRangeQuery} instead
+ * @deprecated Please use {@link PointRangeQuery} instead
  *
  * @since 2.9
  **/
diff --git a/lucene/core/src/java/org/apache/lucene/search/PointRangeQuery.java b/lucene/core/src/java/org/apache/lucene/search/PointRangeQuery.java
new file mode 100644
index 0000000..714ba0d
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/search/PointRangeQuery.java
@@ -0,0 +1,348 @@
+package org.apache.lucene.search;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.Objects;
+
+import org.apache.lucene.index.PointValues;
+import org.apache.lucene.index.PointValues.IntersectVisitor;
+import org.apache.lucene.index.PointValues.Relation;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.util.DocIdSetBuilder;
+import org.apache.lucene.util.NumericUtils;
+import org.apache.lucene.util.RamUsageEstimator;
+import org.apache.lucene.util.StringHelper;
+
+/** Searches for ranges in fields previously indexed using points e.g.
+ *  {@link org.apache.lucene.document.LongPoint}.  In a 1D field this is
+ *  a simple range query; in a multi-dimensional field it's a box shape. */
+
+public class PointRangeQuery extends Query {
+  final String field;
+  final int numDims;
+  final byte[][] lowerPoint;
+  final boolean[] lowerInclusive;
+  final byte[][] upperPoint;
+  final boolean[] upperInclusive;
+  // This is null only in the "fully open range" case
+  final Integer bytesPerDim;
+
+  public PointRangeQuery(String field,
+                         byte[][] lowerPoint, boolean[] lowerInclusive,
+                         byte[][] upperPoint, boolean[] upperInclusive) {
+    this.field = field;
+    if (lowerPoint == null) {
+      throw new IllegalArgumentException("lowerPoint must not be null");
+    }
+    if (upperPoint == null) {
+      throw new IllegalArgumentException("upperPoint must not be null");
+    }
+    numDims = lowerPoint.length;
+    if (upperPoint.length != numDims) {
+      throw new IllegalArgumentException("lowerPoint has length=" + numDims + " but upperPoint has different length=" + upperPoint.length);
+    }
+    this.lowerPoint = lowerPoint;
+    this.lowerInclusive = lowerInclusive;
+    this.upperPoint = upperPoint;
+    this.upperInclusive = upperInclusive;
+
+    int bytesPerDim = -1;
+    for(byte[] value : lowerPoint) {
+      if (value != null) {
+        if (bytesPerDim == -1) {
+          bytesPerDim = value.length;
+        } else if (value.length != bytesPerDim) {
+          throw new IllegalArgumentException("all dimensions must have same bytes length, but saw " + bytesPerDim + " and " + value.length);
+        }
+      }
+    }
+    for(byte[] value : upperPoint) {
+      if (value != null) {
+        if (bytesPerDim == -1) {
+          bytesPerDim = value.length;
+        } else if (value.length != bytesPerDim) {
+          throw new IllegalArgumentException("all dimensions must have same bytes length, but saw " + bytesPerDim + " and " + value.length);
+        }
+      }
+    }
+    if (bytesPerDim == -1) {
+      this.bytesPerDim = null;
+    } else {
+      this.bytesPerDim = bytesPerDim;
+    }
+  }
+
+  /** Use in the 1D case when you indexed 1D int values using {@link org.apache.lucene.document.IntPoint} */
+  public static PointRangeQuery new1DIntRange(String field, Integer lowerValue, boolean lowerInclusive, Integer upperValue, boolean upperInclusive) {
+    return new PointRangeQuery(field, pack(lowerValue), new boolean[] {lowerInclusive}, pack(upperValue), new boolean[] {upperInclusive});
+  }
+
+  /** Use in the 1D case when you indexed 1D long values using {@link org.apache.lucene.document.LongPoint} */
+  public static PointRangeQuery new1DLongRange(String field, Long lowerValue, boolean lowerInclusive, Long upperValue, boolean upperInclusive) {
+    return new PointRangeQuery(field, pack(lowerValue), new boolean[] {lowerInclusive}, pack(upperValue), new boolean[] {upperInclusive});
+  }
+
+  /** Use in the 1D case when you indexed 1D float values using {@link org.apache.lucene.document.FloatPoint} */
+  public static PointRangeQuery new1DFloatRange(String field, Float lowerValue, boolean lowerInclusive, Float upperValue, boolean upperInclusive) {
+    return new PointRangeQuery(field, pack(lowerValue), new boolean[] {lowerInclusive}, pack(upperValue), new boolean[] {upperInclusive});
+  }
+
+  /** Use in the 1D case when you indexed 1D double values using {@link org.apache.lucene.document.DoublePoint} */
+  public static PointRangeQuery new1DDoubleRange(String field, Double lowerValue, boolean lowerInclusive, Double upperValue, boolean upperInclusive) {
+    return new PointRangeQuery(field, pack(lowerValue), new boolean[] {lowerInclusive}, pack(upperValue), new boolean[] {upperInclusive});
+  }
+
+  /** Use in the 1D case when you indexed binary values using {@link org.apache.lucene.document.BinaryPoint} */
+  public static PointRangeQuery new1DBinaryRange(String field, byte[] lowerValue, boolean lowerInclusive, byte[] upperValue, boolean upperInclusive) {
+    return new PointRangeQuery(field, new byte[][] {lowerValue}, new boolean[] {lowerInclusive}, new byte[][] {upperValue}, new boolean[] {upperInclusive});
+  }
+
+  private static byte[][] pack(Long value) {
+    if (value == null) {
+      // OK: open ended range
+      return new byte[1][];
+    }
+    byte[][] result = new byte[][] {new byte[RamUsageEstimator.NUM_BYTES_LONG]};
+    NumericUtils.longToBytes(value, result[0], 0);
+    return result;
+  }
+
+  private static byte[][] pack(Double value) {
+    if (value == null) {
+      // OK: open ended range
+      return new byte[1][];
+    }
+    byte[][] result = new byte[][] {new byte[RamUsageEstimator.NUM_BYTES_LONG]};
+    NumericUtils.longToBytesDirect(NumericUtils.doubleToSortableLong(value), result[0], 0);
+    return result;
+  }
+
+  private static byte[][] pack(Integer value) {
+    if (value == null) {
+      // OK: open ended range
+      return new byte[1][];
+    }
+    byte[][] result = new byte[][] {new byte[RamUsageEstimator.NUM_BYTES_INT]};
+    NumericUtils.intToBytes(value, result[0], 0);
+    return result;
+  }
+
+  private static byte[][] pack(Float value) {
+    if (value == null) {
+      // OK: open ended range
+      return new byte[1][];
+    }
+    byte[][] result = new byte[][] {new byte[RamUsageEstimator.NUM_BYTES_INT]};
+    NumericUtils.intToBytesDirect(NumericUtils.floatToSortableInt(value), result[0], 0);
+    return result;
+  }
+
+  @Override
+  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {
+
+    // We don't use RandomAccessWeight here: it's no good to approximate with "match all docs".
+    // This is an inverted structure and should be used in the first pass:
+
+    return new ConstantScoreWeight(this) {
+
+      @Override
+      public Scorer scorer(LeafReaderContext context) throws IOException {
+        LeafReader reader = context.reader();
+        PointValues values = reader.getPointValues();
+        if (values == null) {
+          // No docs in this segment indexed any points
+          return null;
+        }
+        FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);
+        if (fieldInfo == null) {
+          // No docs in this segment indexed this field at all
+          return null;
+        }
+        if (fieldInfo.getPointDimensionCount() != numDims) {
+          throw new IllegalArgumentException("field=\"" + field + "\" was indexed with numDims=" + fieldInfo.getPointDimensionCount() + " but this query has numDims=" + numDims);
+        }
+        if (bytesPerDim != null && bytesPerDim.intValue() != fieldInfo.getPointNumBytes()) {
+          throw new IllegalArgumentException("field=\"" + field + "\" was indexed with bytesPerDim=" + fieldInfo.getPointNumBytes() + " but this query has bytesPerDim=" + bytesPerDim);
+        }
+        int bytesPerDim = fieldInfo.getPointNumBytes();
+
+        byte[] packedLowerIncl = new byte[numDims * bytesPerDim];
+        byte[] packedUpperIncl = new byte[numDims * bytesPerDim];
+
+        byte[] minValue = new byte[bytesPerDim];
+        byte[] maxValue = new byte[bytesPerDim];
+        Arrays.fill(maxValue, (byte) 0xff);
+
+        byte[] one = new byte[bytesPerDim];
+        one[bytesPerDim-1] = 1;
+
+        // Carefully pack lower and upper bounds, taking care of per-dim inclusive:
+        for(int dim=0;dim<numDims;dim++) {
+          if (lowerPoint[dim] != null) {
+            if (lowerInclusive[dim] == false) {
+              if (Arrays.equals(lowerPoint[dim], maxValue)) {
+                return null;
+              } else {
+                byte[] value = new byte[bytesPerDim];
+                NumericUtils.add(bytesPerDim, 0, lowerPoint[dim], one, value);
+                System.arraycopy(value, 0, packedLowerIncl, dim*bytesPerDim, bytesPerDim);
+              }
+            } else {
+              System.arraycopy(lowerPoint[dim], 0, packedLowerIncl, dim*bytesPerDim, bytesPerDim);
+            }
+          } else {
+            // Open-ended range: we just leave 0s in this packed dim for the lower value
+          }
+
+          if (upperPoint[dim] != null) {
+            if (upperInclusive[dim] == false) {
+              if (Arrays.equals(upperPoint[dim], minValue)) {
+                return null;
+              } else {
+                byte[] value = new byte[bytesPerDim];
+                NumericUtils.subtract(bytesPerDim, 0, upperPoint[dim], one, value);
+                System.arraycopy(value, 0, packedUpperIncl, dim*bytesPerDim, bytesPerDim);
+              }
+            } else {
+              System.arraycopy(upperPoint[dim], 0, packedUpperIncl, dim*bytesPerDim, bytesPerDim);
+            }
+          } else {
+            // Open-ended range: fill with max point for this dim:
+            System.arraycopy(maxValue, 0, packedUpperIncl, dim*bytesPerDim, bytesPerDim);
+          }
+        }
+
+        // Now packedLowerIncl and packedUpperIncl are inclusive, and non-empty space:
+
+        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc());
+
+        int[] hitCount = new int[1];
+        values.intersect(field,
+                         new IntersectVisitor() {
+
+                           @Override
+                           public void grow(int count) {
+                             result.grow(count);
+                           }
+
+                           @Override
+                           public void visit(int docID) {
+                             hitCount[0]++;
+                             result.add(docID);
+                           }
+
+                           @Override
+                           public void visit(int docID, byte[] packedValue) {
+                             for(int dim=0;dim<numDims;dim++) {
+                               int offset = dim*bytesPerDim;
+                               if (StringHelper.compare(bytesPerDim, packedValue, offset, packedLowerIncl, offset) < 0) {
+                                 // Doc's value is too low, in this dimension
+                                 return;
+                               }
+                               if (StringHelper.compare(bytesPerDim, packedValue, offset, packedUpperIncl, offset) > 0) {
+                                 // Doc's value is too high, in this dimension
+                                 return;
+                               }
+                             }
+
+                             // Doc is in-bounds
+                             hitCount[0]++;
+                             result.add(docID);
+                           }
+
+                           @Override
+                           public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {
+
+                             boolean crosses = false;
+
+                             for(int dim=0;dim<numDims;dim++) {
+                               int offset = dim*bytesPerDim;
+
+                               if (StringHelper.compare(bytesPerDim, minPackedValue, offset, packedUpperIncl, offset) > 0 ||
+                                   StringHelper.compare(bytesPerDim, maxPackedValue, offset, packedLowerIncl, offset) < 0) {
+                                 return Relation.CELL_OUTSIDE_QUERY;
+                               }
+
+                               crosses |= StringHelper.compare(bytesPerDim, minPackedValue, offset, packedLowerIncl, offset) < 0 ||
+                                 StringHelper.compare(bytesPerDim, maxPackedValue, offset, packedUpperIncl, offset) > 0;
+                             }
+
+                             if (crosses) {
+                               return Relation.CELL_CROSSES_QUERY;
+                             } else {
+                               return Relation.CELL_INSIDE_QUERY;
+                             }
+                           }
+                         });
+
+        // NOTE: hitCount[0] will be over-estimate in multi-valued case
+        return new ConstantScoreScorer(this, score(), result.build(hitCount[0]).iterator());
+      }
+    };
+  }
+
+  @Override
+  public int hashCode() {
+    int hash = super.hashCode();
+    hash += Arrays.hashCode(lowerPoint)^0x14fa55fb;
+    hash += Arrays.hashCode(upperPoint)^0x733fa5fe;
+    hash += Arrays.hashCode(lowerInclusive)^0x14fa55fb;
+    hash += Arrays.hashCode(upperInclusive)^0x733fa5fe;
+    hash += numDims^0x14fa55fb;
+    hash += Objects.hashCode(bytesPerDim);
+    return hash;
+  }
+
+  @Override
+  public boolean equals(Object other) {
+    if (super.equals(other)) {
+      final PointRangeQuery q = (PointRangeQuery) other;
+      return q.numDims == numDims &&
+        q.bytesPerDim == bytesPerDim &&
+        Arrays.equals(lowerPoint, q.lowerPoint) &&
+        Arrays.equals(lowerInclusive, q.lowerInclusive) &&
+        Arrays.equals(upperPoint, q.upperPoint) &&
+        Arrays.equals(upperInclusive, q.upperInclusive);
+    }
+
+    return false;
+  }
+
+  @Override
+  public String toString(String field) {
+    final StringBuilder sb = new StringBuilder();
+    sb.append(getClass().getSimpleName());
+    sb.append(':');
+    if (this.field.equals(field) == false) {
+      sb.append("field=");
+      sb.append(this.field);
+      sb.append(':');
+    }
+
+    return sb.append('[')
+      .append(Arrays.toString(lowerPoint))
+      .append(" TO ")
+      .append(Arrays.toString(upperPoint))
+      .append(']')
+      .toString();
+  }
+}
diff --git a/lucene/core/src/java/org/apache/lucene/search/Query.java b/lucene/core/src/java/org/apache/lucene/search/Query.java
index 09db16a..9646ff7 100644
--- a/lucene/core/src/java/org/apache/lucene/search/Query.java
+++ b/lucene/core/src/java/org/apache/lucene/search/Query.java
@@ -33,7 +33,7 @@ import org.apache.lucene.index.IndexReader;
     <li> {@link FuzzyQuery}
     <li> {@link RegexpQuery}
     <li> {@link TermRangeQuery}
-    <li> {@link DimensionalRangeQuery}
+    <li> {@link PointRangeQuery}
     <li> {@link ConstantScoreQuery}
     <li> {@link DisjunctionMaxQuery}
     <li> {@link MatchAllDocsQuery}
diff --git a/lucene/core/src/java/org/apache/lucene/search/UsageTrackingQueryCachingPolicy.java b/lucene/core/src/java/org/apache/lucene/search/UsageTrackingQueryCachingPolicy.java
index 3c5e997..24f1652 100644
--- a/lucene/core/src/java/org/apache/lucene/search/UsageTrackingQueryCachingPolicy.java
+++ b/lucene/core/src/java/org/apache/lucene/search/UsageTrackingQueryCachingPolicy.java
@@ -43,7 +43,7 @@ public final class UsageTrackingQueryCachingPolicy implements QueryCachingPolicy
     // DocIdSet in the first place
     return query instanceof MultiTermQuery ||
         query instanceof MultiTermQueryConstantScoreWrapper ||
-        query instanceof DimensionalRangeQuery;
+        query instanceof PointRangeQuery;
   }
 
   static boolean isCheap(Query query) {
diff --git a/lucene/core/src/java/org/apache/lucene/search/package-info.java b/lucene/core/src/java/org/apache/lucene/search/package-info.java
index efaeca6..27f7d55 100644
--- a/lucene/core/src/java/org/apache/lucene/search/package-info.java
+++ b/lucene/core/src/java/org/apache/lucene/search/package-info.java
@@ -160,22 +160,22 @@
  *     and an upper
  *     {@link org.apache.lucene.index.Term Term}
  *     according to {@link org.apache.lucene.util.BytesRef#compareTo BytesRef.compareTo()}. It is not intended
- *     for numerical ranges; use {@link org.apache.lucene.search.DimensionalRangeQuery DimensionalRangeQuery} instead.
+ *     for numerical ranges; use {@link org.apache.lucene.search.PointRangeQuery PointRangeQuery} instead.
  * 
  *     For example, one could find all documents
  *     that have terms beginning with the letters <tt>a</tt> through <tt>c</tt>.
  * 
  * <h3>
- *     {@link org.apache.lucene.search.DimensionalRangeQuery DimensionalRangeQuery}
+ *     {@link org.apache.lucene.search.PointRangeQuery PointRangeQuery}
  * </h3>
  * 
  * <p>The
- *     {@link org.apache.lucene.search.DimensionalRangeQuery DimensionalRangeQuery}
+ *     {@link org.apache.lucene.search.PointRangeQuery PointRangeQuery}
  *     matches all documents that occur in a numeric range.
- *     For DimensionalRangeQuery to work, you must index the values
- *     using a one of the numeric fields ({@link org.apache.lucene.document.DimensionalIntField DimensionalIntField},
- *     {@link org.apache.lucene.document.DimensionalLongField DimensionalLongField}, {@link org.apache.lucene.document.DimensionalFloatField DimensionalFloatField},
- *     or {@link org.apache.lucene.document.DimensionalDoubleField DimensionalDoubleField}).
+ *     For PointRangeQuery to work, you must index the values
+ *     using a one of the numeric fields ({@link org.apache.lucene.document.IntPoint IntPoint},
+ *     {@link org.apache.lucene.document.LongPoint LongPoint}, {@link org.apache.lucene.document.FloatPoint FloatPoint},
+ *     or {@link org.apache.lucene.document.DoublePoint DoublePoint}).
  * 
  * <h3>
  *     {@link org.apache.lucene.search.PrefixQuery PrefixQuery},
diff --git a/lucene/core/src/java/org/apache/lucene/util/LegacyNumericUtils.java b/lucene/core/src/java/org/apache/lucene/util/LegacyNumericUtils.java
index e367082..1cec17f 100644
--- a/lucene/core/src/java/org/apache/lucene/util/LegacyNumericUtils.java
+++ b/lucene/core/src/java/org/apache/lucene/util/LegacyNumericUtils.java
@@ -19,7 +19,6 @@ package org.apache.lucene.util;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.DimensionalValues;
 import org.apache.lucene.index.FilterLeafReader;
 import org.apache.lucene.index.FilteredTermsEnum;
 import org.apache.lucene.index.Terms;
@@ -60,7 +59,7 @@ import org.apache.lucene.index.TermsEnum;
  *
  * @lucene.internal
  *
- * @deprecated Please use {@link DimensionalValues} instead.
+ * @deprecated Please use {@link org.apache.lucene.index.PointValues} instead.
  *
  * @since 2.9, API changed non backwards-compliant in 4.0
  */
diff --git a/lucene/core/src/java/org/apache/lucene/util/bkd/BKDReader.java b/lucene/core/src/java/org/apache/lucene/util/bkd/BKDReader.java
index a685449..81b9814 100644
--- a/lucene/core/src/java/org/apache/lucene/util/bkd/BKDReader.java
+++ b/lucene/core/src/java/org/apache/lucene/util/bkd/BKDReader.java
@@ -21,8 +21,8 @@ import java.io.IOException;
 import java.util.Arrays;
 
 import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.index.DimensionalValues.IntersectVisitor;
-import org.apache.lucene.index.DimensionalValues.Relation;
+import org.apache.lucene.index.PointValues.IntersectVisitor;
+import org.apache.lucene.index.PointValues.Relation;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.util.Accountable;
 import org.apache.lucene.util.BytesRef;
diff --git a/lucene/core/src/test/org/apache/lucene/document/TestFieldType.java b/lucene/core/src/test/org/apache/lucene/document/TestFieldType.java
index 96eb42e..00b0966 100644
--- a/lucene/core/src/test/org/apache/lucene/document/TestFieldType.java
+++ b/lucene/core/src/test/org/apache/lucene/document/TestFieldType.java
@@ -89,7 +89,7 @@ public class TestFieldType extends LuceneTestCase {
       if ((method.getModifiers() & Modifier.PUBLIC) != 0 && method.getName().startsWith("set")) {
         final Class<?>[] parameterTypes = method.getParameterTypes();
         final Object[] args = new Object[parameterTypes.length];
-        if (method.getName().equals("setDimensions")) {
+        if (method.getName().equals("setPointDimensions")) {
           args[0] = 1 + random().nextInt(15);
           args[1] = 1 + random().nextInt(100);
         } else {
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestDemoParallelLeafReader.java b/lucene/core/src/test/org/apache/lucene/index/TestDemoParallelLeafReader.java
index c5eef35..0d25f28 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestDemoParallelLeafReader.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestDemoParallelLeafReader.java
@@ -32,11 +32,11 @@ import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.atomic.AtomicLong;
 import java.util.regex.Pattern;
 
-import org.apache.lucene.document.DimensionalLongField;
+import org.apache.lucene.document.LongPoint;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.NumericDocValuesField;
-import org.apache.lucene.search.DimensionalRangeQuery;
+import org.apache.lucene.search.PointRangeQuery;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.MatchAllDocsQuery;
 import org.apache.lucene.search.ScoreDoc;
@@ -681,7 +681,7 @@ public class TestDemoParallelLeafReader extends LuceneTestCase {
           Document newDoc = new Document();
           long value = Long.parseLong(oldDoc.get("text").split(" ")[1]);
           newDoc.add(new NumericDocValuesField("number", value));
-          newDoc.add(new DimensionalLongField("number", value));
+          newDoc.add(new LongPoint("number", value));
           w.addDocument(newDoc);
         }
 
@@ -737,7 +737,7 @@ public class TestDemoParallelLeafReader extends LuceneTestCase {
             Document newDoc = new Document();
             long value = Long.parseLong(oldDoc.get("text").split(" ")[1]);
             newDoc.add(new NumericDocValuesField("number_" + newSchemaGen, value));
-            newDoc.add(new DimensionalLongField("number", value));
+            newDoc.add(new LongPoint("number", value));
             w.addDocument(newDoc);
           }
         } else {
@@ -832,7 +832,7 @@ public class TestDemoParallelLeafReader extends LuceneTestCase {
             Document newDoc = new Document();
             long value = Long.parseLong(oldDoc.get("text").split(" ")[1]);
             newDoc.add(new NumericDocValuesField("number", newSchemaGen*value));
-            newDoc.add(new DimensionalLongField("number", value));
+            newDoc.add(new LongPoint("number", value));
             w.addDocument(newDoc);
           }
         } else {
@@ -1168,7 +1168,7 @@ public class TestDemoParallelLeafReader extends LuceneTestCase {
       checkAllNumberDVs(r);
       IndexSearcher s = newSearcher(r);
       testNumericDVSort(s);
-      testDimensionalRangeQuery(s);
+      testPointRangeQuery(s);
     } finally {
       reindexer.mgr.release(r);
     }
@@ -1190,7 +1190,7 @@ public class TestDemoParallelLeafReader extends LuceneTestCase {
       checkAllNumberDVs(r);
       IndexSearcher s = newSearcher(r);
       testNumericDVSort(s);
-      testDimensionalRangeQuery(s);
+      testPointRangeQuery(s);
     } finally {
       reindexer.mgr.release(r);
     }
@@ -1209,7 +1209,7 @@ public class TestDemoParallelLeafReader extends LuceneTestCase {
       checkAllNumberDVs(r);
       IndexSearcher s = newSearcher(r);
       testNumericDVSort(s);
-      testDimensionalRangeQuery(s);
+      testPointRangeQuery(s);
     } finally {
       reindexer.mgr.release(r);
     }
@@ -1261,7 +1261,7 @@ public class TestDemoParallelLeafReader extends LuceneTestCase {
           checkAllNumberDVs(r);
           IndexSearcher s = newSearcher(r);
           testNumericDVSort(s);
-          testDimensionalRangeQuery(s);
+          testPointRangeQuery(s);
         } finally {
           reindexer.mgr.release(r);
         }
@@ -1340,7 +1340,7 @@ public class TestDemoParallelLeafReader extends LuceneTestCase {
     }
   }
 
-  private static void testDimensionalRangeQuery(IndexSearcher s) throws IOException {
+  private static void testPointRangeQuery(IndexSearcher s) throws IOException {
     NumericDocValues numbers = MultiDocValues.getNumericValues(s.getIndexReader(), "number");
     for(int i=0;i<100;i++) {
       // Confirm we can range search by the new indexed (numeric) field:
@@ -1352,7 +1352,7 @@ public class TestDemoParallelLeafReader extends LuceneTestCase {
         max = x;
       }
 
-      TopDocs hits = s.search(DimensionalRangeQuery.new1DLongRange("number", min, true, max, true), 100);
+      TopDocs hits = s.search(PointRangeQuery.new1DLongRange("number", min, true, max, true), 100);
       for(ScoreDoc scoreDoc : hits.scoreDocs) {
         long value = Long.parseLong(s.doc(scoreDoc.doc).get("text").split(" ")[1]);
         assertTrue(value >= min);
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestDimensionalValues.java b/lucene/core/src/test/org/apache/lucene/index/TestDimensionalValues.java
deleted file mode 100644
index ec1fa8b..0000000
--- a/lucene/core/src/test/org/apache/lucene/index/TestDimensionalValues.java
+++ /dev/null
@@ -1,1265 +0,0 @@
-package org.apache.lucene.index;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.math.BigInteger;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.BitSet;
-import java.util.List;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.DimensionalFormat;
-import org.apache.lucene.codecs.DimensionalReader;
-import org.apache.lucene.codecs.DimensionalWriter;
-import org.apache.lucene.codecs.FilterCodec;
-import org.apache.lucene.codecs.lucene60.Lucene60DimensionalReader;
-import org.apache.lucene.codecs.lucene60.Lucene60DimensionalWriter;
-import org.apache.lucene.document.DimensionalBinaryField;
-import org.apache.lucene.document.DimensionalIntField;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.NumericDocValuesField;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.DimensionalValues.IntersectVisitor;
-import org.apache.lucene.index.DimensionalValues.Relation;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.MockDirectoryWrapper;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.NumericUtils;
-import org.apache.lucene.util.StringHelper;
-import org.apache.lucene.util.TestUtil;
-
-// TODO: factor out a BaseTestDimensionFormat
-
-public class TestDimensionalValues extends LuceneTestCase {
-  public void testBasic() throws Exception {
-    Directory dir = getDirectory(20);
-    // TODO: randomize codec once others support dimensional format
-    IndexWriterConfig iwc = newIndexWriterConfig();
-    iwc.setMergePolicy(newLogMergePolicy());
-    IndexWriter w = new IndexWriter(dir, iwc);
-    byte[] point = new byte[4];
-    for(int i=0;i<20;i++) {
-      Document doc = new Document();
-      NumericUtils.intToBytes(i, point, 0);
-      doc.add(new DimensionalBinaryField("dim", point));
-      w.addDocument(doc);
-    }
-    w.forceMerge(1);
-    w.close();
-
-    DirectoryReader r = DirectoryReader.open(dir);
-    LeafReader sub = getOnlySegmentReader(r);
-    DimensionalValues values = sub.getDimensionalValues();
-
-    // Simple test: make sure intersect can visit every doc:
-    BitSet seen = new BitSet();
-    values.intersect("dim",
-                     new IntersectVisitor() {
-                       @Override
-                       public Relation compare(byte[] minPacked, byte[] maxPacked) {
-                         return Relation.CELL_CROSSES_QUERY;
-                       }
-                       public void visit(int docID) {
-                         throw new IllegalStateException();
-                       }
-                       public void visit(int docID, byte[] packedValue) {
-                         seen.set(docID);
-                         assertEquals(docID, NumericUtils.bytesToInt(packedValue, 0));
-                       }
-                     });
-    assertEquals(20, seen.cardinality());
-    IOUtils.close(r, dir);
-  }
-
-  public void testMerge() throws Exception {
-    Directory dir = getDirectory(20);
-    IndexWriterConfig iwc = newIndexWriterConfig();
-    iwc.setMergePolicy(newLogMergePolicy());
-    IndexWriter w = new IndexWriter(dir, iwc);
-    byte[] point = new byte[4];
-    for(int i=0;i<20;i++) {
-      Document doc = new Document();
-      NumericUtils.intToBytes(i, point, 0);
-      doc.add(new DimensionalBinaryField("dim", point));
-      w.addDocument(doc);
-      if (i == 10) {
-        w.commit();
-      }
-    }
-    w.forceMerge(1);
-    w.close();
-
-    DirectoryReader r = DirectoryReader.open(dir);
-    LeafReader sub = getOnlySegmentReader(r);
-    DimensionalValues values = sub.getDimensionalValues();
-
-    // Simple test: make sure intersect can visit every doc:
-    BitSet seen = new BitSet();
-    values.intersect("dim",
-                     new IntersectVisitor() {
-                       @Override
-                       public Relation compare(byte[] minPacked, byte[] maxPacked) {
-                         return Relation.CELL_CROSSES_QUERY;
-                       }
-                       public void visit(int docID) {
-                         throw new IllegalStateException();
-                       }
-                       public void visit(int docID, byte[] packedValue) {
-                         seen.set(docID);
-                         assertEquals(docID, NumericUtils.bytesToInt(packedValue, 0));
-                       }
-                     });
-    assertEquals(20, seen.cardinality());
-    IOUtils.close(r, dir);
-  }
-
-  public void testAllDimensionalDocsDeletedInSegment() throws Exception {
-    Directory dir = getDirectory(20);
-    IndexWriterConfig iwc = newIndexWriterConfig();
-    IndexWriter w = new IndexWriter(dir, iwc);
-    byte[] point = new byte[4];
-    for(int i=0;i<10;i++) {
-      Document doc = new Document();
-      NumericUtils.intToBytes(i, point, 0);
-      doc.add(new DimensionalBinaryField("dim", point));
-      doc.add(new NumericDocValuesField("id", i));
-      doc.add(newStringField("x", "x", Field.Store.NO));
-      w.addDocument(doc);
-    }
-    w.addDocument(new Document());
-    w.deleteDocuments(new Term("x", "x"));
-    if (random().nextBoolean()) {
-      w.forceMerge(1);
-    }
-    w.close();
-    DirectoryReader r = DirectoryReader.open(dir);
-    assertEquals(1, r.numDocs());
-    DimensionalValues values = MultiDimensionalValues.get(r);
-    Bits liveDocs = MultiFields.getLiveDocs(r);
-    NumericDocValues idValues = MultiDocValues.getNumericValues(r, "id");
-
-    if (values != null) {
-      BitSet seen = new BitSet();
-      values.intersect("dim",
-                       new IntersectVisitor() {
-                         @Override
-                         public Relation compare(byte[] minPacked, byte[] maxPacked) {
-                           return Relation.CELL_CROSSES_QUERY;
-                         }
-                         public void visit(int docID) {
-                           throw new IllegalStateException();
-                         }
-                         public void visit(int docID, byte[] packedValue) {
-                           if (liveDocs.get(docID)) {
-                             seen.set(docID);
-                           }
-                           assertEquals(idValues.get(docID), NumericUtils.bytesToInt(packedValue, 0));
-                         }
-                       });
-      assertEquals(0, seen.cardinality());
-    }
-    IOUtils.close(r, dir);
-  }
-
-  /** Make sure we close open files, delete temp files, etc., on exception */
-  public void testWithExceptions() throws Exception {
-    int numDocs = atLeast(10000);
-    int numBytesPerDim = TestUtil.nextInt(random(), 2, DimensionalValues.MAX_NUM_BYTES);
-    int numDims = TestUtil.nextInt(random(), 1, DimensionalValues.MAX_DIMENSIONS);
-
-    byte[][][] docValues = new byte[numDocs][][];
-
-    for(int docID=0;docID<numDocs;docID++) {
-      byte[][] values = new byte[numDims][];
-      for(int dim=0;dim<numDims;dim++) {
-        values[dim] = new byte[numBytesPerDim];
-        random().nextBytes(values[dim]);
-      }
-      docValues[docID] = values;
-    }
-
-    // Keep retrying until we 1) we allow a big enough heap, and 2) we hit a random IOExc from MDW:
-    boolean done = false;
-    while (done == false) {
-      try (MockDirectoryWrapper dir = newMockFSDirectory(createTempDir())) {
-        try {
-          dir.setRandomIOExceptionRate(0.05);
-          dir.setRandomIOExceptionRateOnOpen(0.05);
-          if (dir instanceof MockDirectoryWrapper) {
-            dir.setEnableVirusScanner(false);
-          }
-          verify(dir, docValues, null, numDims, numBytesPerDim, true);
-        } catch (IllegalStateException ise) {
-          if (ise.getMessage().contains("this writer hit an unrecoverable error")) {
-            Throwable cause = ise.getCause();
-            if (cause != null && cause.getMessage().contains("a random IOException")) {
-              done = true;
-            } else {
-              throw ise;
-            }
-          } else {
-            throw ise;
-          }
-        } catch (AssertionError ae) {
-          if (ae.getMessage().contains("does not exist; files=")) {
-            // OK: likely we threw the random IOExc when IW was asserting the commit files exist
-            done = true;
-          } else {
-            throw ae;
-          }
-        } catch (IllegalArgumentException iae) {
-          // This just means we got a too-small maxMB for the maxPointsInLeafNode; just retry w/ more heap
-          assertTrue(iae.getMessage().contains("either increase maxMBSortInHeap or decrease maxPointsInLeafNode"));
-        } catch (IOException ioe) {
-          String message = ioe.getMessage();
-          if (message.contains("a random IOException") || message.contains("background merge hit exception")) {
-            // BKDWriter should fully clean up after itself:
-            done = true;
-          } else {
-            throw ioe;
-          }
-        }
-      }
-    }
-  }
-
-  public void testMultiValued() throws Exception {
-    int numBytesPerDim = TestUtil.nextInt(random(), 2, DimensionalValues.MAX_NUM_BYTES);
-    int numDims = TestUtil.nextInt(random(), 1, DimensionalValues.MAX_DIMENSIONS);
-
-    int numDocs = atLeast(1000);
-    List<byte[][]> docValues = new ArrayList<>();
-    List<Integer> docIDs = new ArrayList<>();
-
-    for(int docID=0;docID<numDocs;docID++) {
-      int numValuesInDoc = TestUtil.nextInt(random(), 1, 5);
-      for(int ord=0;ord<numValuesInDoc;ord++) {
-        docIDs.add(docID);
-        byte[][] values = new byte[numDims][];
-        for(int dim=0;dim<numDims;dim++) {
-          values[dim] = new byte[numBytesPerDim];
-          random().nextBytes(values[dim]);
-        }
-        docValues.add(values);
-      }
-    }
-
-    byte[][][] docValuesArray = docValues.toArray(new byte[docValues.size()][][]);
-    int[] docIDsArray = new int[docIDs.size()];
-    for(int i=0;i<docIDsArray.length;i++) {
-      docIDsArray[i] = docIDs.get(i);
-    }
-
-    verify(docValuesArray, docIDsArray, numDims, numBytesPerDim);
-  }
-
-  public void testAllEqual() throws Exception {
-    int numBytesPerDim = TestUtil.nextInt(random(), 2, DimensionalValues.MAX_NUM_BYTES);
-    int numDims = TestUtil.nextInt(random(), 1, DimensionalValues.MAX_DIMENSIONS);
-
-    int numDocs = atLeast(1000);
-    byte[][][] docValues = new byte[numDocs][][];
-
-    for(int docID=0;docID<numDocs;docID++) {
-      if (docID == 0) {
-        byte[][] values = new byte[numDims][];
-        for(int dim=0;dim<numDims;dim++) {
-          values[dim] = new byte[numBytesPerDim];
-          random().nextBytes(values[dim]);
-        }
-        docValues[docID] = values;
-      } else {
-        docValues[docID] = docValues[0];
-      }
-    }
-
-    verify(docValues, null, numDims, numBytesPerDim);
-  }
-
-  public void testOneDimEqual() throws Exception {
-    int numBytesPerDim = TestUtil.nextInt(random(), 2, DimensionalValues.MAX_NUM_BYTES);
-    int numDims = TestUtil.nextInt(random(), 1, DimensionalValues.MAX_DIMENSIONS);
-
-    int numDocs = atLeast(1000);
-    int theEqualDim = random().nextInt(numDims);
-    byte[][][] docValues = new byte[numDocs][][];
-
-    for(int docID=0;docID<numDocs;docID++) {
-      byte[][] values = new byte[numDims][];
-      for(int dim=0;dim<numDims;dim++) {
-        values[dim] = new byte[numBytesPerDim];
-        random().nextBytes(values[dim]);
-      }
-      docValues[docID] = values;
-      if (docID > 0) {
-        docValues[docID][theEqualDim] = docValues[0][theEqualDim];
-      }
-    }
-
-    verify(docValues, null, numDims, numBytesPerDim);
-  }
-
-  // Tests on N-dimensional points where each dimension is a BigInteger
-  public void testBigIntNDims() throws Exception {
-
-    int numDocs = atLeast(1000);
-    try (Directory dir = getDirectory(numDocs)) {
-      int numBytesPerDim = TestUtil.nextInt(random(), 2, DimensionalValues.MAX_NUM_BYTES);
-      int numDims = TestUtil.nextInt(random(), 1, DimensionalValues.MAX_DIMENSIONS);
-      IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));
-      // We rely on docIDs not changing:
-      iwc.setMergePolicy(newLogMergePolicy());
-      RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
-      BigInteger[][] docs = new BigInteger[numDocs][];
-
-      for(int docID=0;docID<numDocs;docID++) {
-        BigInteger[] values = new BigInteger[numDims];
-        if (VERBOSE) {
-          System.out.println("  docID=" + docID);
-        }
-        byte[][] bytes = new byte[numDims][];
-        for(int dim=0;dim<numDims;dim++) {
-          values[dim] = randomBigInt(numBytesPerDim);
-          bytes[dim] = new byte[numBytesPerDim];
-          NumericUtils.bigIntToBytes(values[dim], bytes[dim], 0, numBytesPerDim);
-          if (VERBOSE) {
-            System.out.println("    " + dim + " -> " + values[dim]);
-          }
-        }
-        docs[docID] = values;
-        Document doc = new Document();
-        doc.add(new DimensionalBinaryField("field", bytes));
-        w.addDocument(doc);
-      }
-
-      DirectoryReader r = w.getReader();
-      w.close();
-
-      DimensionalValues dimValues = MultiDimensionalValues.get(r);
-
-      int iters = atLeast(100);
-      for(int iter=0;iter<iters;iter++) {
-        if (VERBOSE) {
-          System.out.println("\nTEST: iter=" + iter);
-        }
-
-        // Random N dims rect query:
-        BigInteger[] queryMin = new BigInteger[numDims];
-        BigInteger[] queryMax = new BigInteger[numDims];    
-        for(int dim=0;dim<numDims;dim++) {
-          queryMin[dim] = randomBigInt(numBytesPerDim);
-          queryMax[dim] = randomBigInt(numBytesPerDim);
-          if (queryMin[dim].compareTo(queryMax[dim]) > 0) {
-            BigInteger x = queryMin[dim];
-            queryMin[dim] = queryMax[dim];
-            queryMax[dim] = x;
-          }
-          if (VERBOSE) {
-            System.out.println("  " + dim + "\n    min=" + queryMin[dim] + "\n    max=" + queryMax[dim]);
-          }
-        }
-
-        final BitSet hits = new BitSet();
-        dimValues.intersect("field", new IntersectVisitor() {
-            @Override
-            public void visit(int docID) {
-              hits.set(docID);
-              //System.out.println("visit docID=" + docID);
-            }
-
-            @Override
-            public void visit(int docID, byte[] packedValue) {
-              //System.out.println("visit check docID=" + docID);
-              for(int dim=0;dim<numDims;dim++) {
-                BigInteger x = NumericUtils.bytesToBigInt(packedValue, dim, numBytesPerDim);
-                if (x.compareTo(queryMin[dim]) < 0 || x.compareTo(queryMax[dim]) > 0) {
-                  //System.out.println("  no");
-                  return;
-                }
-              }
-
-              //System.out.println("  yes");
-              hits.set(docID);
-            }
-
-            @Override
-            public Relation compare(byte[] minPacked, byte[] maxPacked) {
-              boolean crosses = false;
-              for(int dim=0;dim<numDims;dim++) {
-                BigInteger min = NumericUtils.bytesToBigInt(minPacked, dim, numBytesPerDim);
-                BigInteger max = NumericUtils.bytesToBigInt(maxPacked, dim, numBytesPerDim);
-                assert max.compareTo(min) >= 0;
-
-                if (max.compareTo(queryMin[dim]) < 0 || min.compareTo(queryMax[dim]) > 0) {
-                  return Relation.CELL_OUTSIDE_QUERY;
-                } else if (min.compareTo(queryMin[dim]) < 0 || max.compareTo(queryMax[dim]) > 0) {
-                  crosses = true;
-                }
-              }
-
-              if (crosses) {
-                return Relation.CELL_CROSSES_QUERY;
-              } else {
-                return Relation.CELL_INSIDE_QUERY;
-              }
-            }
-          });
-
-        for(int docID=0;docID<numDocs;docID++) {
-          BigInteger[] docValues = docs[docID];
-          boolean expected = true;
-          for(int dim=0;dim<numDims;dim++) {
-            BigInteger x = docValues[dim];
-            if (x.compareTo(queryMin[dim]) < 0 || x.compareTo(queryMax[dim]) > 0) {
-              expected = false;
-              break;
-            }
-          }
-          boolean actual = hits.get(docID);
-          assertEquals("docID=" + docID, expected, actual);
-        }
-      }
-      r.close();
-      }
-  }
-
-  public void testRandomBinaryTiny() throws Exception {
-    doTestRandomBinary(10);
-  }
-
-  public void testRandomBinaryMedium() throws Exception {
-    doTestRandomBinary(10000);
-  }
-
-  @Nightly
-  public void testRandomBinaryBig() throws Exception {
-    assumeFalse("too slow with SimpleText", Codec.getDefault().getName().equals("SimpleText"));
-    doTestRandomBinary(200000);
-  }
-
-  // Suddenly add dimensional values to an existing field:
-  public void testUpgradeFieldToDimensional() throws Exception {
-    Directory dir = getDirectory(1);
-    IndexWriterConfig iwc = newIndexWriterConfig();
-    IndexWriter w = new IndexWriter(dir, iwc);
-    Document doc = new Document();
-    doc.add(newStringField("dim", "foo", Field.Store.NO));
-    w.addDocument(doc);
-    w.close();
-    
-    iwc = newIndexWriterConfig();
-    w = new IndexWriter(dir, iwc);
-    doc.add(new DimensionalBinaryField("dim", new byte[4]));
-    w.close();
-    dir.close();
-  }
-
-  // Illegal schema change tests:
-
-  public void testIllegalDimChangeOneDoc() throws Exception {
-    Directory dir = getDirectory(1);
-    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));
-    IndexWriter w = new IndexWriter(dir, iwc);
-    Document doc = new Document();
-    doc.add(new DimensionalBinaryField("dim", new byte[4]));
-    doc.add(new DimensionalBinaryField("dim", new byte[4], new byte[4]));
-    try {
-      w.addDocument(doc);
-    } catch (IllegalArgumentException iae) {
-      // expected
-      assertEquals("cannot change dimension count from 1 to 2 for field=\"dim\"", iae.getMessage());
-    }
-    w.close();
-    dir.close();
-  }
-
-  public void testIllegalDimChangeTwoDocs() throws Exception {
-    Directory dir = getDirectory(1);
-    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));
-    IndexWriter w = new IndexWriter(dir, iwc);
-    Document doc = new Document();
-    doc.add(new DimensionalBinaryField("dim", new byte[4]));
-    w.addDocument(doc);
-    doc = new Document();
-    doc.add(new DimensionalBinaryField("dim", new byte[4], new byte[4]));
-    try {
-      w.addDocument(doc);
-    } catch (IllegalArgumentException iae) {
-      // expected
-      assertEquals("cannot change dimension count from 1 to 2 for field=\"dim\"", iae.getMessage());
-    }
-    w.close();
-    dir.close();
-  }
-
-  public void testIllegalDimChangeTwoSegments() throws Exception {
-    Directory dir = getDirectory(1);
-    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));
-    IndexWriter w = new IndexWriter(dir, iwc);
-    Document doc = new Document();
-    doc.add(new DimensionalBinaryField("dim", new byte[4]));
-    w.addDocument(doc);
-    w.commit();
-    doc = new Document();
-    doc.add(new DimensionalBinaryField("dim", new byte[4], new byte[4]));
-    try {
-      w.addDocument(doc);
-    } catch (IllegalArgumentException iae) {
-      // expected
-      assertEquals("cannot change dimension count from 1 to 2 for field=\"dim\"", iae.getMessage());
-    }
-    w.close();
-    dir.close();
-  }
-
-  public void testIllegalDimChangeTwoWriters() throws Exception {
-    Directory dir = getDirectory(1);
-    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));
-    IndexWriter w = new IndexWriter(dir, iwc);
-    Document doc = new Document();
-    doc.add(new DimensionalBinaryField("dim", new byte[4]));
-    w.addDocument(doc);
-    w.close();
-    iwc = new IndexWriterConfig(new MockAnalyzer(random()));
-    w = new IndexWriter(dir, iwc);
-    doc = new Document();
-    doc.add(new DimensionalBinaryField("dim", new byte[4], new byte[4]));
-    try {
-      w.addDocument(doc);
-    } catch (IllegalArgumentException iae) {
-      // expected
-      assertEquals("cannot change dimension count from 1 to 2 for field=\"dim\"", iae.getMessage());
-    }
-    w.close();
-    dir.close();
-  }
-
-  public void testIllegalDimChangeViaAddIndexesDirectory() throws Exception {
-    Directory dir = getDirectory(1);
-    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));
-    IndexWriter w = new IndexWriter(dir, iwc);
-    Document doc = new Document();
-    doc.add(new DimensionalBinaryField("dim", new byte[4]));
-    w.addDocument(doc);
-    w.close();
-
-    Directory dir2 = getDirectory(1);
-    iwc = new IndexWriterConfig(new MockAnalyzer(random()));
-    w = new IndexWriter(dir2, iwc);
-    doc = new Document();
-    doc.add(new DimensionalBinaryField("dim", new byte[4], new byte[4]));
-    w.addDocument(doc);
-    try {
-      w.addIndexes(new Directory[] {dir});
-    } catch (IllegalArgumentException iae) {
-      assertEquals("cannot change dimension count from 2 to 1 for field=\"dim\"", iae.getMessage());
-    }
-    IOUtils.close(w, dir, dir2);
-  }
-
-  public void testIllegalDimChangeViaAddIndexesCodecReader() throws Exception {
-    Directory dir = getDirectory(1);
-    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));
-    IndexWriter w = new IndexWriter(dir, iwc);
-    Document doc = new Document();
-    doc.add(new DimensionalBinaryField("dim", new byte[4]));
-    w.addDocument(doc);
-    w.close();
-
-    Directory dir2 = getDirectory(1);
-    iwc = new IndexWriterConfig(new MockAnalyzer(random()));
-    w = new IndexWriter(dir2, iwc);
-    doc = new Document();
-    doc.add(new DimensionalBinaryField("dim", new byte[4], new byte[4]));
-    w.addDocument(doc);
-    DirectoryReader r = DirectoryReader.open(dir);
-    try {
-      w.addIndexes(new CodecReader[] {getOnlySegmentReader(r)});
-    } catch (IllegalArgumentException iae) {
-      assertEquals("cannot change dimension count from 2 to 1 for field=\"dim\"", iae.getMessage());
-    }
-    IOUtils.close(r, w, dir, dir2);
-  }
-
-  public void testIllegalDimChangeViaAddIndexesSlowCodecReader() throws Exception {
-    Directory dir = getDirectory(1);
-    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));
-    IndexWriter w = new IndexWriter(dir, iwc);
-    Document doc = new Document();
-    doc.add(new DimensionalBinaryField("dim", new byte[4]));
-    w.addDocument(doc);
-    w.close();
-
-    Directory dir2 = getDirectory(1);
-    iwc = new IndexWriterConfig(new MockAnalyzer(random()));
-    w = new IndexWriter(dir2, iwc);
-    doc = new Document();
-    doc.add(new DimensionalBinaryField("dim", new byte[4], new byte[4]));
-    w.addDocument(doc);
-    DirectoryReader r = DirectoryReader.open(dir);
-    try {
-      TestUtil.addIndexesSlowly(w, r);
-    } catch (IllegalArgumentException iae) {
-      assertEquals("cannot change dimension count from 2 to 1 for field=\"dim\"", iae.getMessage());
-    }
-    IOUtils.close(r, w, dir, dir2);
-  }
-
-  public void testIllegalNumBytesChangeOneDoc() throws Exception {
-    Directory dir = getDirectory(1);
-    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));
-    IndexWriter w = new IndexWriter(dir, iwc);
-    Document doc = new Document();
-    doc.add(new DimensionalBinaryField("dim", new byte[4]));
-    doc.add(new DimensionalBinaryField("dim", new byte[6]));
-    try {
-      w.addDocument(doc);
-    } catch (IllegalArgumentException iae) {
-      // expected
-      assertEquals("cannot change dimension numBytes from 4 to 6 for field=\"dim\"", iae.getMessage());
-    }
-    w.close();
-    dir.close();
-  }
-
-  public void testIllegalNumBytesChangeTwoDocs() throws Exception {
-    Directory dir = getDirectory(1);
-    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));
-    IndexWriter w = new IndexWriter(dir, iwc);
-    Document doc = new Document();
-    doc.add(new DimensionalBinaryField("dim", new byte[4]));
-    w.addDocument(doc);
-    doc = new Document();
-    doc.add(new DimensionalBinaryField("dim", new byte[6]));
-    try {
-      w.addDocument(doc);
-    } catch (IllegalArgumentException iae) {
-      // expected
-      assertEquals("cannot change dimension numBytes from 4 to 6 for field=\"dim\"", iae.getMessage());
-    }
-    w.close();
-    dir.close();
-  }
-
-  public void testIllegalNumBytesChangeTwoSegments() throws Exception {
-    Directory dir = getDirectory(1);
-    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));
-    IndexWriter w = new IndexWriter(dir, iwc);
-    Document doc = new Document();
-    doc.add(new DimensionalBinaryField("dim", new byte[4]));
-    w.addDocument(doc);
-    w.commit();
-    doc = new Document();
-    doc.add(new DimensionalBinaryField("dim", new byte[6]));
-    try {
-      w.addDocument(doc);
-    } catch (IllegalArgumentException iae) {
-      // expected
-      assertEquals("cannot change dimension numBytes from 4 to 6 for field=\"dim\"", iae.getMessage());
-    }
-    w.close();
-    dir.close();
-  }
-
-  public void testIllegalNumBytesChangeTwoWriters() throws Exception {
-    Directory dir = getDirectory(1);
-    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));
-    IndexWriter w = new IndexWriter(dir, iwc);
-    Document doc = new Document();
-    doc.add(new DimensionalBinaryField("dim", new byte[4]));
-    w.addDocument(doc);
-    w.close();
-    iwc = new IndexWriterConfig(new MockAnalyzer(random()));
-    w = new IndexWriter(dir, iwc);
-    doc = new Document();
-    doc.add(new DimensionalBinaryField("dim", new byte[6]));
-    try {
-      w.addDocument(doc);
-    } catch (IllegalArgumentException iae) {
-      // expected
-      assertEquals("cannot change dimension numBytes from 4 to 6 for field=\"dim\"", iae.getMessage());
-    }
-    w.close();
-    dir.close();
-  }
-
-  public void testIllegalNumBytesChangeViaAddIndexesDirectory() throws Exception {
-    Directory dir = getDirectory(1);
-    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));
-    IndexWriter w = new IndexWriter(dir, iwc);
-    Document doc = new Document();
-    doc.add(new DimensionalBinaryField("dim", new byte[4]));
-    w.addDocument(doc);
-    w.close();
-
-    Directory dir2 = getDirectory(1);
-    iwc = new IndexWriterConfig(new MockAnalyzer(random()));
-    w = new IndexWriter(dir2, iwc);
-    doc = new Document();
-    doc.add(new DimensionalBinaryField("dim", new byte[6]));
-    w.addDocument(doc);
-    try {
-      w.addIndexes(new Directory[] {dir});
-    } catch (IllegalArgumentException iae) {
-      assertEquals("cannot change dimension numBytes from 6 to 4 for field=\"dim\"", iae.getMessage());
-    }
-    IOUtils.close(w, dir, dir2);
-  }
-
-  public void testIllegalNumBytesChangeViaAddIndexesCodecReader() throws Exception {
-    Directory dir = getDirectory(1);
-    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));
-    IndexWriter w = new IndexWriter(dir, iwc);
-    Document doc = new Document();
-    doc.add(new DimensionalBinaryField("dim", new byte[4]));
-    w.addDocument(doc);
-    w.close();
-
-    Directory dir2 = getDirectory(1);
-    iwc = new IndexWriterConfig(new MockAnalyzer(random()));
-    w = new IndexWriter(dir2, iwc);
-    doc = new Document();
-    doc.add(new DimensionalBinaryField("dim", new byte[6]));
-    w.addDocument(doc);
-    DirectoryReader r = DirectoryReader.open(dir);
-    try {
-      w.addIndexes(new CodecReader[] {getOnlySegmentReader(r)});
-    } catch (IllegalArgumentException iae) {
-      assertEquals("cannot change dimension numBytes from 6 to 4 for field=\"dim\"", iae.getMessage());
-    }
-    IOUtils.close(r, w, dir, dir2);
-  }
-
-  public void testIllegalNumBytesChangeViaAddIndexesSlowCodecReader() throws Exception {
-    Directory dir = getDirectory(1);
-    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));
-    IndexWriter w = new IndexWriter(dir, iwc);
-    Document doc = new Document();
-    doc.add(new DimensionalBinaryField("dim", new byte[4]));
-    w.addDocument(doc);
-    w.close();
-
-    Directory dir2 = getDirectory(1);
-    iwc = new IndexWriterConfig(new MockAnalyzer(random()));
-    w = new IndexWriter(dir2, iwc);
-    doc = new Document();
-    doc.add(new DimensionalBinaryField("dim", new byte[6]));
-    w.addDocument(doc);
-    DirectoryReader r = DirectoryReader.open(dir);
-    try {
-      TestUtil.addIndexesSlowly(w, r);
-    } catch (IllegalArgumentException iae) {
-      assertEquals("cannot change dimension numBytes from 6 to 4 for field=\"dim\"", iae.getMessage());
-    }
-    IOUtils.close(r, w, dir, dir2);
-  }
-
-  public void testIllegalTooManyBytes() throws Exception {
-    Directory dir = getDirectory(1);
-    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));
-    IndexWriter w = new IndexWriter(dir, iwc);
-    Document doc = new Document();
-    doc.add(new DimensionalBinaryField("dim", new byte[DimensionalValues.MAX_NUM_BYTES+1]));
-    try {
-      w.addDocument(doc);
-      fail("did not hit exception");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-    doc = new Document();
-    doc.add(new DimensionalIntField("dim", 17));
-    w.addDocument(doc);
-    w.close();
-    dir.close();
-  }
-
-  public void testIllegalTooManyDimensions() throws Exception {
-    Directory dir = getDirectory(1);
-    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));
-    IndexWriter w = new IndexWriter(dir, iwc);
-    Document doc = new Document();
-    byte[][] values = new byte[DimensionalValues.MAX_DIMENSIONS+1][];
-    for(int i=0;i<values.length;i++) {
-      values[i] = new byte[4];
-    }
-    doc.add(new DimensionalBinaryField("dim", values));
-    try {
-      w.addDocument(doc);
-      fail("did not hit exception");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-    doc = new Document();
-    doc.add(new DimensionalIntField("dim", 17));
-    w.addDocument(doc);
-    w.close();
-    dir.close();
-  }
-
-  private void doTestRandomBinary(int count) throws Exception {
-    int numDocs = TestUtil.nextInt(random(), count, count*2);
-    int numBytesPerDim = TestUtil.nextInt(random(), 2, DimensionalValues.MAX_NUM_BYTES);
-    int numDims = TestUtil.nextInt(random(), 1, DimensionalValues.MAX_DIMENSIONS);
-
-    byte[][][] docValues = new byte[numDocs][][];
-
-    for(int docID=0;docID<numDocs;docID++) {
-      byte[][] values = new byte[numDims][];
-      for(int dim=0;dim<numDims;dim++) {
-        values[dim] = new byte[numBytesPerDim];
-        // TODO: sometimes test on a "small" volume too, so we test the high density cases, higher chance of boundary, etc. cases:
-        random().nextBytes(values[dim]);
-      }
-      docValues[docID] = values;
-    }
-
-    verify(docValues, null, numDims, numBytesPerDim);
-  }
-
-  private Codec getCodec() {
-    if (Codec.getDefault().getName().equals("Lucene60")) {
-      int maxPointsInLeafNode = TestUtil.nextInt(random(), 50, 500);
-      double maxMBSortInHeap = 0.1 + (3*random().nextDouble());
-      if (VERBOSE) {
-        System.out.println("TEST: using Lucene60DimensionalFormat with maxPointsInLeafNode=" + maxPointsInLeafNode + " and maxMBSortInHeap=" + maxMBSortInHeap);
-      }
-
-      return new FilterCodec("Lucene60", Codec.getDefault()) {
-        @Override
-        public DimensionalFormat dimensionalFormat() {
-          return new DimensionalFormat() {
-            @Override
-            public DimensionalWriter fieldsWriter(SegmentWriteState writeState) throws IOException {
-              return new Lucene60DimensionalWriter(writeState, maxPointsInLeafNode, maxMBSortInHeap);
-            }
-
-            @Override
-            public DimensionalReader fieldsReader(SegmentReadState readState) throws IOException {
-              return new Lucene60DimensionalReader(readState);
-            }
-          };
-        }
-      };
-    } else {
-      return Codec.getDefault();
-    }
-  }
-
-  /** docIDs can be null, for the single valued case, else it maps value to docID, but all values for one doc must be adjacent */
-  private void verify(byte[][][] docValues, int[] docIDs, int numDims, int numBytesPerDim) throws Exception {
-    try (Directory dir = getDirectory(docValues.length)) {
-      while (true) {
-        try {
-          verify(dir, docValues, docIDs, numDims, numBytesPerDim, false);
-          return;
-        } catch (IllegalArgumentException iae) {
-          // This just means we got a too-small maxMB for the maxPointsInLeafNode; just retry
-          assertTrue(iae.getMessage().contains("either increase maxMBSortInHeap or decrease maxPointsInLeafNode"));
-        }
-      }
-    }
-  }
-
-  private void verify(Directory dir, byte[][][] docValues, int[] ids, int numDims, int numBytesPerDim, boolean expectExceptions) throws Exception {
-    int numValues = docValues.length;
-    if (VERBOSE) {
-      System.out.println("TEST: numValues=" + numValues + " numDims=" + numDims + " numBytesPerDim=" + numBytesPerDim);
-    }
-
-    // RandomIndexWriter is too slow:
-    boolean useRealWriter = docValues.length > 10000;
-
-    IndexWriterConfig iwc;
-    if (useRealWriter) {
-      iwc = new IndexWriterConfig(new MockAnalyzer(random()));
-    } else {
-      iwc = newIndexWriterConfig();
-    }
-    iwc.setCodec(getCodec());
-
-    if (expectExceptions) {
-      MergeScheduler ms = iwc.getMergeScheduler();
-      if (ms instanceof ConcurrentMergeScheduler) {
-        ((ConcurrentMergeScheduler) ms).setSuppressExceptions();
-      }
-    }
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
-    DirectoryReader r = null;
-
-    // Compute actual min/max values:
-    byte[][] expectedMinValues = new byte[numDims][];
-    byte[][] expectedMaxValues = new byte[numDims][];
-    for(int ord=0;ord<docValues.length;ord++) {
-      for(int dim=0;dim<numDims;dim++) {
-        if (ord == 0) {
-          expectedMinValues[dim] = new byte[numBytesPerDim];
-          System.arraycopy(docValues[ord][dim], 0, expectedMinValues[dim], 0, numBytesPerDim);
-          expectedMaxValues[dim] = new byte[numBytesPerDim];
-          System.arraycopy(docValues[ord][dim], 0, expectedMaxValues[dim], 0, numBytesPerDim);
-        } else {
-          // TODO: it's cheating that we use StringHelper.compare for "truth": what if it's buggy?
-          if (StringHelper.compare(numBytesPerDim, docValues[ord][dim], 0, expectedMinValues[dim], 0) < 0) {
-            System.arraycopy(docValues[ord][dim], 0, expectedMinValues[dim], 0, numBytesPerDim);
-          }
-          if (StringHelper.compare(numBytesPerDim, docValues[ord][dim], 0, expectedMaxValues[dim], 0) > 0) {
-            System.arraycopy(docValues[ord][dim], 0, expectedMaxValues[dim], 0, numBytesPerDim);
-          }
-        }
-      }
-    }
-
-    // 20% of the time we add into a separate directory, then at some point use
-    // addIndexes to bring the indexed dimensional values to the main directory:
-    Directory saveDir;
-    RandomIndexWriter saveW;
-    int addIndexesAt;
-    if (random().nextInt(5) == 1) {
-      saveDir = dir;
-      saveW = w;
-      dir = getDirectory(numValues);
-      if (useRealWriter) {
-        iwc = new IndexWriterConfig(new MockAnalyzer(random()));
-      } else {
-        iwc = newIndexWriterConfig();
-      }
-      iwc.setCodec(getCodec());
-      if (expectExceptions) {
-        MergeScheduler ms = iwc.getMergeScheduler();
-        if (ms instanceof ConcurrentMergeScheduler) {
-          ((ConcurrentMergeScheduler) ms).setSuppressExceptions();
-        }
-      }
-      w = new RandomIndexWriter(random(), dir, iwc);
-      addIndexesAt = TestUtil.nextInt(random(), 1, numValues-1);
-    } else {
-      saveW = null;
-      saveDir = null;
-      addIndexesAt = 0;
-    }
-
-    try {
-
-      Document doc = null;
-      int lastID = -1;
-      for(int ord=0;ord<numValues;ord++) {
-        int id;
-        if (ids == null) {
-          id = ord;
-        } else {
-          id = ids[ord];
-        }
-        if (id != lastID) {
-          if (doc != null) {
-            if (useRealWriter) {
-              w.w.addDocument(doc);
-            } else {
-              w.addDocument(doc);
-            }
-          }
-          doc = new Document();
-          doc.add(new NumericDocValuesField("id", id));
-        }
-        doc.add(new DimensionalBinaryField("field", docValues[ord]));
-        lastID = id;
-
-        if (random().nextInt(30) == 17) {
-          // randomly index some documents without this field
-          if (useRealWriter) {
-            w.w.addDocument(new Document());
-          } else {
-            w.addDocument(new Document());
-          }
-          if (VERBOSE) {
-            System.out.println("add empty doc");
-          }
-        }
-
-        if (random().nextInt(30) == 17) {
-          // randomly index some documents with this field, but we will delete them:
-          Document xdoc = new Document();
-          xdoc.add(new DimensionalBinaryField("field", docValues[ord]));
-          xdoc.add(new StringField("nukeme", "yes", Field.Store.NO));
-          if (useRealWriter) {
-            w.w.addDocument(xdoc);
-          } else {
-            w.addDocument(xdoc);
-          }
-          if (VERBOSE) {
-            System.out.println("add doc doc-to-delete");
-          }
-
-          if (random().nextInt(5) == 1) {
-            if (useRealWriter) {
-              w.w.deleteDocuments(new Term("nukeme", "yes"));
-            } else {
-              w.deleteDocuments(new Term("nukeme", "yes"));
-            }
-          }
-        }
-
-        if (VERBOSE) {
-          System.out.println("  ord=" + ord + " id=" + id);
-          for(int dim=0;dim<numDims;dim++) {
-            System.out.println("    dim=" + dim + " value=" + new BytesRef(docValues[ord][dim]));
-          }
-        }
-
-        if (saveW != null && ord >= addIndexesAt) {
-          switchIndex(w, dir, saveW);
-          w = saveW;
-          dir = saveDir;
-          saveW = null;
-          saveDir = null;
-        }
-      }
-      w.addDocument(doc);
-      w.deleteDocuments(new Term("nukeme", "yes"));
-
-      if (random().nextBoolean()) {
-        if (VERBOSE) {
-          System.out.println("\nTEST: now force merge");
-        }
-        w.forceMerge(1);
-      }
-
-      r = w.getReader();
-      w.close();
-
-      if (VERBOSE) {
-        System.out.println("TEST: reader=" + r);
-      }
-
-      DimensionalValues dimValues = MultiDimensionalValues.get(r);
-      if (VERBOSE) {
-        System.out.println("  dimValues=" + dimValues);
-      }
-      assertNotNull(dimValues);
-
-      NumericDocValues idValues = MultiDocValues.getNumericValues(r, "id");
-      Bits liveDocs = MultiFields.getLiveDocs(r);
-
-      // Verify min/max values are correct:
-      byte[] minValues = dimValues.getMinPackedValue("field");
-      byte[] maxValues = dimValues.getMaxPackedValue("field");
-      byte[] scratch = new byte[numBytesPerDim];
-      for(int dim=0;dim<numDims;dim++) {
-        System.arraycopy(minValues, dim*numBytesPerDim, scratch, 0, scratch.length);
-        //System.out.println("dim=" + dim + " expectedMin=" + new BytesRef(expectedMinValues[dim]) + " min=" + new BytesRef(scratch));
-        assertTrue(Arrays.equals(expectedMinValues[dim], scratch));
-        System.arraycopy(maxValues, dim*numBytesPerDim, scratch, 0, scratch.length);
-        //System.out.println("dim=" + dim + " expectedMax=" + new BytesRef(expectedMaxValues[dim]) + " max=" + new BytesRef(scratch));
-        assertTrue(Arrays.equals(expectedMaxValues[dim], scratch));
-      }
-
-      int iters = atLeast(100);
-      for(int iter=0;iter<iters;iter++) {
-        if (VERBOSE) {
-          System.out.println("\nTEST: iter=" + iter);
-        }
-
-        // Random N dims rect query:
-        byte[][] queryMin = new byte[numDims][];
-        byte[][] queryMax = new byte[numDims][];    
-        for(int dim=0;dim<numDims;dim++) {    
-          queryMin[dim] = new byte[numBytesPerDim];
-          random().nextBytes(queryMin[dim]);
-          queryMax[dim] = new byte[numBytesPerDim];
-          random().nextBytes(queryMax[dim]);
-          if (NumericUtils.compare(numBytesPerDim, queryMin[dim], 0, queryMax[dim], 0) > 0) {
-            byte[] x = queryMin[dim];
-            queryMin[dim] = queryMax[dim];
-            queryMax[dim] = x;
-          }
-        }
-
-        if (VERBOSE) {
-          for(int dim=0;dim<numDims;dim++) {
-            System.out.println("  dim=" + dim + "\n    queryMin=" + new BytesRef(queryMin[dim]) + "\n    queryMax=" + new BytesRef(queryMax[dim]));
-          }
-        }
-
-        final BitSet hits = new BitSet();
-
-        dimValues.intersect("field", new DimensionalValues.IntersectVisitor() {
-            @Override
-            public void visit(int docID) {
-              if (liveDocs == null || liveDocs.get(docID)) {
-                hits.set((int) idValues.get(docID));
-              }
-              //System.out.println("visit docID=" + docID);
-            }
-
-            @Override
-            public void visit(int docID, byte[] packedValue) {
-              if (liveDocs != null && liveDocs.get(docID) == false) {
-                return;
-              }
-              //System.out.println("visit check docID=" + docID + " id=" + idValues.get(docID));
-              for(int dim=0;dim<numDims;dim++) {
-                //System.out.println("  dim=" + dim + " value=" + new BytesRef(packedValue, dim*numBytesPerDim, numBytesPerDim));
-                if (NumericUtils.compare(numBytesPerDim, packedValue, dim, queryMin[dim], 0) < 0 ||
-                    NumericUtils.compare(numBytesPerDim, packedValue, dim, queryMax[dim], 0) > 0) {
-                  //System.out.println("  no");
-                  return;
-                }
-              }
-
-              //System.out.println("  yes");
-              hits.set((int) idValues.get(docID));
-            }
-
-            @Override
-            public Relation compare(byte[] minPacked, byte[] maxPacked) {
-              boolean crosses = false;
-              //System.out.println("compare");
-              for(int dim=0;dim<numDims;dim++) {
-                if (NumericUtils.compare(numBytesPerDim, maxPacked, dim, queryMin[dim], 0) < 0 ||
-                    NumericUtils.compare(numBytesPerDim, minPacked, dim, queryMax[dim], 0) > 0) {
-                  //System.out.println("  query_outside_cell");
-                  return Relation.CELL_OUTSIDE_QUERY;
-                } else if (NumericUtils.compare(numBytesPerDim, minPacked, dim, queryMin[dim], 0) < 0 ||
-                           NumericUtils.compare(numBytesPerDim, maxPacked, dim, queryMax[dim], 0) > 0) {
-                  crosses = true;
-                }
-              }
-
-              if (crosses) {
-                //System.out.println("  query_crosses_cell");
-                return Relation.CELL_CROSSES_QUERY;
-              } else {
-                //System.out.println("  cell_inside_query");
-                return Relation.CELL_INSIDE_QUERY;
-              }
-            }
-          });
-
-        BitSet expected = new BitSet();
-        for(int ord=0;ord<numValues;ord++) {
-          boolean matches = true;
-          for(int dim=0;dim<numDims;dim++) {
-            byte[] x = docValues[ord][dim];
-            if (NumericUtils.compare(numBytesPerDim, x, 0, queryMin[dim], 0) < 0 ||
-                NumericUtils.compare(numBytesPerDim, x, 0, queryMax[dim], 0) > 0) {
-              matches = false;
-              break;
-            }
-          }
-
-          if (matches) {
-            int id;
-            if (ids == null) {
-              id = ord;
-            } else {
-              id = ids[ord];
-            }
-            expected.set(id);
-          }
-        }
-
-        int limit = Math.max(expected.length(), hits.length());
-        int failCount = 0;
-        int successCount = 0;
-        for(int id=0;id<limit;id++) {
-          if (expected.get(id) != hits.get(id)) {
-            System.out.println("FAIL: id=" + id);
-            failCount++;
-          } else {
-            successCount++;
-          }
-        }
-
-        if (failCount != 0) {
-          for(int docID=0;docID<r.maxDoc();docID++) {
-            System.out.println("  docID=" + docID + " id=" + idValues.get(docID));
-          }
-
-          fail(failCount + " docs failed; " + successCount + " docs succeeded");
-        }
-      }
-    } finally {
-      IOUtils.closeWhileHandlingException(r, w, saveW, saveDir == null ? null : dir);
-    }
-  }
-
-  private void switchIndex(RandomIndexWriter w, Directory dir, RandomIndexWriter saveW) throws IOException {
-    if (random().nextBoolean()) {
-      // Add via readers:
-      try (DirectoryReader r = w.getReader()) {
-        if (random().nextBoolean()) {
-          // Add via CodecReaders:
-          List<CodecReader> subs = new ArrayList<>();
-          for (LeafReaderContext context : r.leaves()) {
-            subs.add((CodecReader) context.reader());
-          }
-          if (VERBOSE) {
-            System.out.println("TEST: now use addIndexes(CodecReader[]) to switch writers");
-          }
-          saveW.addIndexes(subs.toArray(new CodecReader[subs.size()]));
-        } else {
-          if (VERBOSE) {
-            System.out.println("TEST: now use TestUtil.addIndexesSlowly(DirectoryReader[]) to switch writers");
-          }
-          TestUtil.addIndexesSlowly(saveW.w, r);
-        }
-      }
-    } else {
-      // Add via directory:
-      if (VERBOSE) {
-        System.out.println("TEST: now use addIndexes(Directory[]) to switch writers");
-      }
-      w.close();
-      saveW.addIndexes(new Directory[] {dir});
-    }
-    w.close();
-    dir.close();
-  }
-
-  private BigInteger randomBigInt(int numBytes) {
-    BigInteger x = new BigInteger(numBytes*8-1, random());
-    if (random().nextBoolean()) {
-      x = x.negate();
-    }
-    return x;
-  }
-
-  private static Directory noVirusChecker(Directory dir) {
-    if (dir instanceof MockDirectoryWrapper) {
-      ((MockDirectoryWrapper) dir).setEnableVirusScanner(false);
-    }
-    return dir;
-  }
-
-  private Directory getDirectory(int numPoints) throws IOException {
-    Directory dir;
-    if (numPoints > 100000) {
-      dir = newFSDirectory(createTempDir("TestBKDTree"));
-    } else {
-      dir = newDirectory();
-    }
-    noVirusChecker(dir);
-    //dir = FSDirectory.open(createTempDir());
-    return dir;
-  }
-}
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestIndexableField.java b/lucene/core/src/test/org/apache/lucene/index/TestIndexableField.java
index 0009789..28a9eaf 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestIndexableField.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestIndexableField.java
@@ -92,12 +92,12 @@ public class TestIndexableField extends LuceneTestCase {
       }
 
       @Override
-      public int dimensionCount() {
+      public int pointDimensionCount() {
         return 0;
       }
 
       @Override
-      public int dimensionNumBytes() {
+      public int pointNumBytes() {
         return 0;
       }
     };
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestPointValues.java b/lucene/core/src/test/org/apache/lucene/index/TestPointValues.java
new file mode 100644
index 0000000..f818567
--- /dev/null
+++ b/lucene/core/src/test/org/apache/lucene/index/TestPointValues.java
@@ -0,0 +1,1265 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.math.BigInteger;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.BitSet;
+import java.util.List;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.PointFormat;
+import org.apache.lucene.codecs.PointReader;
+import org.apache.lucene.codecs.PointWriter;
+import org.apache.lucene.codecs.FilterCodec;
+import org.apache.lucene.codecs.lucene60.Lucene60PointReader;
+import org.apache.lucene.codecs.lucene60.Lucene60PointWriter;
+import org.apache.lucene.document.BinaryPoint;
+import org.apache.lucene.document.IntPoint;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.index.PointValues.IntersectVisitor;
+import org.apache.lucene.index.PointValues.Relation;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.MockDirectoryWrapper;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.NumericUtils;
+import org.apache.lucene.util.StringHelper;
+import org.apache.lucene.util.TestUtil;
+
+// TODO: factor out a BaseTestDimensionFormat
+
+public class TestPointValues extends LuceneTestCase {
+  public void testBasic() throws Exception {
+    Directory dir = getDirectory(20);
+    // TODO: randomize codec once others support points format
+    IndexWriterConfig iwc = newIndexWriterConfig();
+    iwc.setMergePolicy(newLogMergePolicy());
+    IndexWriter w = new IndexWriter(dir, iwc);
+    byte[] point = new byte[4];
+    for(int i=0;i<20;i++) {
+      Document doc = new Document();
+      NumericUtils.intToBytes(i, point, 0);
+      doc.add(new BinaryPoint("dim", point));
+      w.addDocument(doc);
+    }
+    w.forceMerge(1);
+    w.close();
+
+    DirectoryReader r = DirectoryReader.open(dir);
+    LeafReader sub = getOnlySegmentReader(r);
+    PointValues values = sub.getPointValues();
+
+    // Simple test: make sure intersect can visit every doc:
+    BitSet seen = new BitSet();
+    values.intersect("dim",
+                     new IntersectVisitor() {
+                       @Override
+                       public Relation compare(byte[] minPacked, byte[] maxPacked) {
+                         return Relation.CELL_CROSSES_QUERY;
+                       }
+                       public void visit(int docID) {
+                         throw new IllegalStateException();
+                       }
+                       public void visit(int docID, byte[] packedValue) {
+                         seen.set(docID);
+                         assertEquals(docID, NumericUtils.bytesToInt(packedValue, 0));
+                       }
+                     });
+    assertEquals(20, seen.cardinality());
+    IOUtils.close(r, dir);
+  }
+
+  public void testMerge() throws Exception {
+    Directory dir = getDirectory(20);
+    IndexWriterConfig iwc = newIndexWriterConfig();
+    iwc.setMergePolicy(newLogMergePolicy());
+    IndexWriter w = new IndexWriter(dir, iwc);
+    byte[] point = new byte[4];
+    for(int i=0;i<20;i++) {
+      Document doc = new Document();
+      NumericUtils.intToBytes(i, point, 0);
+      doc.add(new BinaryPoint("dim", point));
+      w.addDocument(doc);
+      if (i == 10) {
+        w.commit();
+      }
+    }
+    w.forceMerge(1);
+    w.close();
+
+    DirectoryReader r = DirectoryReader.open(dir);
+    LeafReader sub = getOnlySegmentReader(r);
+    PointValues values = sub.getPointValues();
+
+    // Simple test: make sure intersect can visit every doc:
+    BitSet seen = new BitSet();
+    values.intersect("dim",
+                     new IntersectVisitor() {
+                       @Override
+                       public Relation compare(byte[] minPacked, byte[] maxPacked) {
+                         return Relation.CELL_CROSSES_QUERY;
+                       }
+                       public void visit(int docID) {
+                         throw new IllegalStateException();
+                       }
+                       public void visit(int docID, byte[] packedValue) {
+                         seen.set(docID);
+                         assertEquals(docID, NumericUtils.bytesToInt(packedValue, 0));
+                       }
+                     });
+    assertEquals(20, seen.cardinality());
+    IOUtils.close(r, dir);
+  }
+
+  public void testAllPointDocsDeletedInSegment() throws Exception {
+    Directory dir = getDirectory(20);
+    IndexWriterConfig iwc = newIndexWriterConfig();
+    IndexWriter w = new IndexWriter(dir, iwc);
+    byte[] point = new byte[4];
+    for(int i=0;i<10;i++) {
+      Document doc = new Document();
+      NumericUtils.intToBytes(i, point, 0);
+      doc.add(new BinaryPoint("dim", point));
+      doc.add(new NumericDocValuesField("id", i));
+      doc.add(newStringField("x", "x", Field.Store.NO));
+      w.addDocument(doc);
+    }
+    w.addDocument(new Document());
+    w.deleteDocuments(new Term("x", "x"));
+    if (random().nextBoolean()) {
+      w.forceMerge(1);
+    }
+    w.close();
+    DirectoryReader r = DirectoryReader.open(dir);
+    assertEquals(1, r.numDocs());
+    PointValues values = MultiPointValues.get(r);
+    Bits liveDocs = MultiFields.getLiveDocs(r);
+    NumericDocValues idValues = MultiDocValues.getNumericValues(r, "id");
+
+    if (values != null) {
+      BitSet seen = new BitSet();
+      values.intersect("dim",
+                       new IntersectVisitor() {
+                         @Override
+                         public Relation compare(byte[] minPacked, byte[] maxPacked) {
+                           return Relation.CELL_CROSSES_QUERY;
+                         }
+                         public void visit(int docID) {
+                           throw new IllegalStateException();
+                         }
+                         public void visit(int docID, byte[] packedValue) {
+                           if (liveDocs.get(docID)) {
+                             seen.set(docID);
+                           }
+                           assertEquals(idValues.get(docID), NumericUtils.bytesToInt(packedValue, 0));
+                         }
+                       });
+      assertEquals(0, seen.cardinality());
+    }
+    IOUtils.close(r, dir);
+  }
+
+  /** Make sure we close open files, delete temp files, etc., on exception */
+  public void testWithExceptions() throws Exception {
+    int numDocs = atLeast(10000);
+    int numBytesPerDim = TestUtil.nextInt(random(), 2, PointValues.MAX_NUM_BYTES);
+    int numDims = TestUtil.nextInt(random(), 1, PointValues.MAX_DIMENSIONS);
+
+    byte[][][] docValues = new byte[numDocs][][];
+
+    for(int docID=0;docID<numDocs;docID++) {
+      byte[][] values = new byte[numDims][];
+      for(int dim=0;dim<numDims;dim++) {
+        values[dim] = new byte[numBytesPerDim];
+        random().nextBytes(values[dim]);
+      }
+      docValues[docID] = values;
+    }
+
+    // Keep retrying until we 1) we allow a big enough heap, and 2) we hit a random IOExc from MDW:
+    boolean done = false;
+    while (done == false) {
+      try (MockDirectoryWrapper dir = newMockFSDirectory(createTempDir())) {
+        try {
+          dir.setRandomIOExceptionRate(0.05);
+          dir.setRandomIOExceptionRateOnOpen(0.05);
+          if (dir instanceof MockDirectoryWrapper) {
+            dir.setEnableVirusScanner(false);
+          }
+          verify(dir, docValues, null, numDims, numBytesPerDim, true);
+        } catch (IllegalStateException ise) {
+          if (ise.getMessage().contains("this writer hit an unrecoverable error")) {
+            Throwable cause = ise.getCause();
+            if (cause != null && cause.getMessage().contains("a random IOException")) {
+              done = true;
+            } else {
+              throw ise;
+            }
+          } else {
+            throw ise;
+          }
+        } catch (AssertionError ae) {
+          if (ae.getMessage().contains("does not exist; files=")) {
+            // OK: likely we threw the random IOExc when IW was asserting the commit files exist
+            done = true;
+          } else {
+            throw ae;
+          }
+        } catch (IllegalArgumentException iae) {
+          // This just means we got a too-small maxMB for the maxPointsInLeafNode; just retry w/ more heap
+          assertTrue(iae.getMessage().contains("either increase maxMBSortInHeap or decrease maxPointsInLeafNode"));
+        } catch (IOException ioe) {
+          String message = ioe.getMessage();
+          if (message.contains("a random IOException") || message.contains("background merge hit exception")) {
+            // BKDWriter should fully clean up after itself:
+            done = true;
+          } else {
+            throw ioe;
+          }
+        }
+      }
+    }
+  }
+
+  public void testMultiValued() throws Exception {
+    int numBytesPerDim = TestUtil.nextInt(random(), 2, PointValues.MAX_NUM_BYTES);
+    int numDims = TestUtil.nextInt(random(), 1, PointValues.MAX_DIMENSIONS);
+
+    int numDocs = atLeast(1000);
+    List<byte[][]> docValues = new ArrayList<>();
+    List<Integer> docIDs = new ArrayList<>();
+
+    for(int docID=0;docID<numDocs;docID++) {
+      int numValuesInDoc = TestUtil.nextInt(random(), 1, 5);
+      for(int ord=0;ord<numValuesInDoc;ord++) {
+        docIDs.add(docID);
+        byte[][] values = new byte[numDims][];
+        for(int dim=0;dim<numDims;dim++) {
+          values[dim] = new byte[numBytesPerDim];
+          random().nextBytes(values[dim]);
+        }
+        docValues.add(values);
+      }
+    }
+
+    byte[][][] docValuesArray = docValues.toArray(new byte[docValues.size()][][]);
+    int[] docIDsArray = new int[docIDs.size()];
+    for(int i=0;i<docIDsArray.length;i++) {
+      docIDsArray[i] = docIDs.get(i);
+    }
+
+    verify(docValuesArray, docIDsArray, numDims, numBytesPerDim);
+  }
+
+  public void testAllEqual() throws Exception {
+    int numBytesPerDim = TestUtil.nextInt(random(), 2, PointValues.MAX_NUM_BYTES);
+    int numDims = TestUtil.nextInt(random(), 1, PointValues.MAX_DIMENSIONS);
+
+    int numDocs = atLeast(1000);
+    byte[][][] docValues = new byte[numDocs][][];
+
+    for(int docID=0;docID<numDocs;docID++) {
+      if (docID == 0) {
+        byte[][] values = new byte[numDims][];
+        for(int dim=0;dim<numDims;dim++) {
+          values[dim] = new byte[numBytesPerDim];
+          random().nextBytes(values[dim]);
+        }
+        docValues[docID] = values;
+      } else {
+        docValues[docID] = docValues[0];
+      }
+    }
+
+    verify(docValues, null, numDims, numBytesPerDim);
+  }
+
+  public void testOneDimEqual() throws Exception {
+    int numBytesPerDim = TestUtil.nextInt(random(), 2, PointValues.MAX_NUM_BYTES);
+    int numDims = TestUtil.nextInt(random(), 1, PointValues.MAX_DIMENSIONS);
+
+    int numDocs = atLeast(1000);
+    int theEqualDim = random().nextInt(numDims);
+    byte[][][] docValues = new byte[numDocs][][];
+
+    for(int docID=0;docID<numDocs;docID++) {
+      byte[][] values = new byte[numDims][];
+      for(int dim=0;dim<numDims;dim++) {
+        values[dim] = new byte[numBytesPerDim];
+        random().nextBytes(values[dim]);
+      }
+      docValues[docID] = values;
+      if (docID > 0) {
+        docValues[docID][theEqualDim] = docValues[0][theEqualDim];
+      }
+    }
+
+    verify(docValues, null, numDims, numBytesPerDim);
+  }
+
+  // Tests on N-dimensional points where each dimension is a BigInteger
+  public void testBigIntNDims() throws Exception {
+
+    int numDocs = atLeast(1000);
+    try (Directory dir = getDirectory(numDocs)) {
+      int numBytesPerDim = TestUtil.nextInt(random(), 2, PointValues.MAX_NUM_BYTES);
+      int numDims = TestUtil.nextInt(random(), 1, PointValues.MAX_DIMENSIONS);
+      IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));
+      // We rely on docIDs not changing:
+      iwc.setMergePolicy(newLogMergePolicy());
+      RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+      BigInteger[][] docs = new BigInteger[numDocs][];
+
+      for(int docID=0;docID<numDocs;docID++) {
+        BigInteger[] values = new BigInteger[numDims];
+        if (VERBOSE) {
+          System.out.println("  docID=" + docID);
+        }
+        byte[][] bytes = new byte[numDims][];
+        for(int dim=0;dim<numDims;dim++) {
+          values[dim] = randomBigInt(numBytesPerDim);
+          bytes[dim] = new byte[numBytesPerDim];
+          NumericUtils.bigIntToBytes(values[dim], bytes[dim], 0, numBytesPerDim);
+          if (VERBOSE) {
+            System.out.println("    " + dim + " -> " + values[dim]);
+          }
+        }
+        docs[docID] = values;
+        Document doc = new Document();
+        doc.add(new BinaryPoint("field", bytes));
+        w.addDocument(doc);
+      }
+
+      DirectoryReader r = w.getReader();
+      w.close();
+
+      PointValues dimValues = MultiPointValues.get(r);
+
+      int iters = atLeast(100);
+      for(int iter=0;iter<iters;iter++) {
+        if (VERBOSE) {
+          System.out.println("\nTEST: iter=" + iter);
+        }
+
+        // Random N dims rect query:
+        BigInteger[] queryMin = new BigInteger[numDims];
+        BigInteger[] queryMax = new BigInteger[numDims];    
+        for(int dim=0;dim<numDims;dim++) {
+          queryMin[dim] = randomBigInt(numBytesPerDim);
+          queryMax[dim] = randomBigInt(numBytesPerDim);
+          if (queryMin[dim].compareTo(queryMax[dim]) > 0) {
+            BigInteger x = queryMin[dim];
+            queryMin[dim] = queryMax[dim];
+            queryMax[dim] = x;
+          }
+          if (VERBOSE) {
+            System.out.println("  " + dim + "\n    min=" + queryMin[dim] + "\n    max=" + queryMax[dim]);
+          }
+        }
+
+        final BitSet hits = new BitSet();
+        dimValues.intersect("field", new IntersectVisitor() {
+            @Override
+            public void visit(int docID) {
+              hits.set(docID);
+              //System.out.println("visit docID=" + docID);
+            }
+
+            @Override
+            public void visit(int docID, byte[] packedValue) {
+              //System.out.println("visit check docID=" + docID);
+              for(int dim=0;dim<numDims;dim++) {
+                BigInteger x = NumericUtils.bytesToBigInt(packedValue, dim, numBytesPerDim);
+                if (x.compareTo(queryMin[dim]) < 0 || x.compareTo(queryMax[dim]) > 0) {
+                  //System.out.println("  no");
+                  return;
+                }
+              }
+
+              //System.out.println("  yes");
+              hits.set(docID);
+            }
+
+            @Override
+            public Relation compare(byte[] minPacked, byte[] maxPacked) {
+              boolean crosses = false;
+              for(int dim=0;dim<numDims;dim++) {
+                BigInteger min = NumericUtils.bytesToBigInt(minPacked, dim, numBytesPerDim);
+                BigInteger max = NumericUtils.bytesToBigInt(maxPacked, dim, numBytesPerDim);
+                assert max.compareTo(min) >= 0;
+
+                if (max.compareTo(queryMin[dim]) < 0 || min.compareTo(queryMax[dim]) > 0) {
+                  return Relation.CELL_OUTSIDE_QUERY;
+                } else if (min.compareTo(queryMin[dim]) < 0 || max.compareTo(queryMax[dim]) > 0) {
+                  crosses = true;
+                }
+              }
+
+              if (crosses) {
+                return Relation.CELL_CROSSES_QUERY;
+              } else {
+                return Relation.CELL_INSIDE_QUERY;
+              }
+            }
+          });
+
+        for(int docID=0;docID<numDocs;docID++) {
+          BigInteger[] docValues = docs[docID];
+          boolean expected = true;
+          for(int dim=0;dim<numDims;dim++) {
+            BigInteger x = docValues[dim];
+            if (x.compareTo(queryMin[dim]) < 0 || x.compareTo(queryMax[dim]) > 0) {
+              expected = false;
+              break;
+            }
+          }
+          boolean actual = hits.get(docID);
+          assertEquals("docID=" + docID, expected, actual);
+        }
+      }
+      r.close();
+      }
+  }
+
+  public void testRandomBinaryTiny() throws Exception {
+    doTestRandomBinary(10);
+  }
+
+  public void testRandomBinaryMedium() throws Exception {
+    doTestRandomBinary(10000);
+  }
+
+  @Nightly
+  public void testRandomBinaryBig() throws Exception {
+    assumeFalse("too slow with SimpleText", Codec.getDefault().getName().equals("SimpleText"));
+    doTestRandomBinary(200000);
+  }
+
+  // Suddenly add points to an existing field:
+  public void testUpgradeFieldToPoints() throws Exception {
+    Directory dir = getDirectory(1);
+    IndexWriterConfig iwc = newIndexWriterConfig();
+    IndexWriter w = new IndexWriter(dir, iwc);
+    Document doc = new Document();
+    doc.add(newStringField("dim", "foo", Field.Store.NO));
+    w.addDocument(doc);
+    w.close();
+    
+    iwc = newIndexWriterConfig();
+    w = new IndexWriter(dir, iwc);
+    doc.add(new BinaryPoint("dim", new byte[4]));
+    w.close();
+    dir.close();
+  }
+
+  // Illegal schema change tests:
+
+  public void testIllegalDimChangeOneDoc() throws Exception {
+    Directory dir = getDirectory(1);
+    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));
+    IndexWriter w = new IndexWriter(dir, iwc);
+    Document doc = new Document();
+    doc.add(new BinaryPoint("dim", new byte[4]));
+    doc.add(new BinaryPoint("dim", new byte[4], new byte[4]));
+    try {
+      w.addDocument(doc);
+    } catch (IllegalArgumentException iae) {
+      // expected
+      assertEquals("cannot change point dimension count from 1 to 2 for field=\"dim\"", iae.getMessage());
+    }
+    w.close();
+    dir.close();
+  }
+
+  public void testIllegalDimChangeTwoDocs() throws Exception {
+    Directory dir = getDirectory(1);
+    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));
+    IndexWriter w = new IndexWriter(dir, iwc);
+    Document doc = new Document();
+    doc.add(new BinaryPoint("dim", new byte[4]));
+    w.addDocument(doc);
+    doc = new Document();
+    doc.add(new BinaryPoint("dim", new byte[4], new byte[4]));
+    try {
+      w.addDocument(doc);
+    } catch (IllegalArgumentException iae) {
+      // expected
+      assertEquals("cannot change point dimension count from 1 to 2 for field=\"dim\"", iae.getMessage());
+    }
+    w.close();
+    dir.close();
+  }
+
+  public void testIllegalDimChangeTwoSegments() throws Exception {
+    Directory dir = getDirectory(1);
+    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));
+    IndexWriter w = new IndexWriter(dir, iwc);
+    Document doc = new Document();
+    doc.add(new BinaryPoint("dim", new byte[4]));
+    w.addDocument(doc);
+    w.commit();
+    doc = new Document();
+    doc.add(new BinaryPoint("dim", new byte[4], new byte[4]));
+    try {
+      w.addDocument(doc);
+    } catch (IllegalArgumentException iae) {
+      // expected
+      assertEquals("cannot change point dimension count from 1 to 2 for field=\"dim\"", iae.getMessage());
+    }
+    w.close();
+    dir.close();
+  }
+
+  public void testIllegalDimChangeTwoWriters() throws Exception {
+    Directory dir = getDirectory(1);
+    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));
+    IndexWriter w = new IndexWriter(dir, iwc);
+    Document doc = new Document();
+    doc.add(new BinaryPoint("dim", new byte[4]));
+    w.addDocument(doc);
+    w.close();
+    iwc = new IndexWriterConfig(new MockAnalyzer(random()));
+    w = new IndexWriter(dir, iwc);
+    doc = new Document();
+    doc.add(new BinaryPoint("dim", new byte[4], new byte[4]));
+    try {
+      w.addDocument(doc);
+    } catch (IllegalArgumentException iae) {
+      // expected
+      assertEquals("cannot change point dimension count from 1 to 2 for field=\"dim\"", iae.getMessage());
+    }
+    w.close();
+    dir.close();
+  }
+
+  public void testIllegalDimChangeViaAddIndexesDirectory() throws Exception {
+    Directory dir = getDirectory(1);
+    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));
+    IndexWriter w = new IndexWriter(dir, iwc);
+    Document doc = new Document();
+    doc.add(new BinaryPoint("dim", new byte[4]));
+    w.addDocument(doc);
+    w.close();
+
+    Directory dir2 = getDirectory(1);
+    iwc = new IndexWriterConfig(new MockAnalyzer(random()));
+    w = new IndexWriter(dir2, iwc);
+    doc = new Document();
+    doc.add(new BinaryPoint("dim", new byte[4], new byte[4]));
+    w.addDocument(doc);
+    try {
+      w.addIndexes(new Directory[] {dir});
+    } catch (IllegalArgumentException iae) {
+      assertEquals("cannot change point dimension count from 2 to 1 for field=\"dim\"", iae.getMessage());
+    }
+    IOUtils.close(w, dir, dir2);
+  }
+
+  public void testIllegalDimChangeViaAddIndexesCodecReader() throws Exception {
+    Directory dir = getDirectory(1);
+    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));
+    IndexWriter w = new IndexWriter(dir, iwc);
+    Document doc = new Document();
+    doc.add(new BinaryPoint("dim", new byte[4]));
+    w.addDocument(doc);
+    w.close();
+
+    Directory dir2 = getDirectory(1);
+    iwc = new IndexWriterConfig(new MockAnalyzer(random()));
+    w = new IndexWriter(dir2, iwc);
+    doc = new Document();
+    doc.add(new BinaryPoint("dim", new byte[4], new byte[4]));
+    w.addDocument(doc);
+    DirectoryReader r = DirectoryReader.open(dir);
+    try {
+      w.addIndexes(new CodecReader[] {getOnlySegmentReader(r)});
+    } catch (IllegalArgumentException iae) {
+      assertEquals("cannot change point dimension count from 2 to 1 for field=\"dim\"", iae.getMessage());
+    }
+    IOUtils.close(r, w, dir, dir2);
+  }
+
+  public void testIllegalDimChangeViaAddIndexesSlowCodecReader() throws Exception {
+    Directory dir = getDirectory(1);
+    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));
+    IndexWriter w = new IndexWriter(dir, iwc);
+    Document doc = new Document();
+    doc.add(new BinaryPoint("dim", new byte[4]));
+    w.addDocument(doc);
+    w.close();
+
+    Directory dir2 = getDirectory(1);
+    iwc = new IndexWriterConfig(new MockAnalyzer(random()));
+    w = new IndexWriter(dir2, iwc);
+    doc = new Document();
+    doc.add(new BinaryPoint("dim", new byte[4], new byte[4]));
+    w.addDocument(doc);
+    DirectoryReader r = DirectoryReader.open(dir);
+    try {
+      TestUtil.addIndexesSlowly(w, r);
+    } catch (IllegalArgumentException iae) {
+      assertEquals("cannot change point dimension count from 2 to 1 for field=\"dim\"", iae.getMessage());
+    }
+    IOUtils.close(r, w, dir, dir2);
+  }
+
+  public void testIllegalNumBytesChangeOneDoc() throws Exception {
+    Directory dir = getDirectory(1);
+    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));
+    IndexWriter w = new IndexWriter(dir, iwc);
+    Document doc = new Document();
+    doc.add(new BinaryPoint("dim", new byte[4]));
+    doc.add(new BinaryPoint("dim", new byte[6]));
+    try {
+      w.addDocument(doc);
+    } catch (IllegalArgumentException iae) {
+      // expected
+      assertEquals("cannot change point numBytes from 4 to 6 for field=\"dim\"", iae.getMessage());
+    }
+    w.close();
+    dir.close();
+  }
+
+  public void testIllegalNumBytesChangeTwoDocs() throws Exception {
+    Directory dir = getDirectory(1);
+    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));
+    IndexWriter w = new IndexWriter(dir, iwc);
+    Document doc = new Document();
+    doc.add(new BinaryPoint("dim", new byte[4]));
+    w.addDocument(doc);
+    doc = new Document();
+    doc.add(new BinaryPoint("dim", new byte[6]));
+    try {
+      w.addDocument(doc);
+    } catch (IllegalArgumentException iae) {
+      // expected
+      assertEquals("cannot change point numBytes from 4 to 6 for field=\"dim\"", iae.getMessage());
+    }
+    w.close();
+    dir.close();
+  }
+
+  public void testIllegalNumBytesChangeTwoSegments() throws Exception {
+    Directory dir = getDirectory(1);
+    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));
+    IndexWriter w = new IndexWriter(dir, iwc);
+    Document doc = new Document();
+    doc.add(new BinaryPoint("dim", new byte[4]));
+    w.addDocument(doc);
+    w.commit();
+    doc = new Document();
+    doc.add(new BinaryPoint("dim", new byte[6]));
+    try {
+      w.addDocument(doc);
+    } catch (IllegalArgumentException iae) {
+      // expected
+      assertEquals("cannot change point numBytes from 4 to 6 for field=\"dim\"", iae.getMessage());
+    }
+    w.close();
+    dir.close();
+  }
+
+  public void testIllegalNumBytesChangeTwoWriters() throws Exception {
+    Directory dir = getDirectory(1);
+    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));
+    IndexWriter w = new IndexWriter(dir, iwc);
+    Document doc = new Document();
+    doc.add(new BinaryPoint("dim", new byte[4]));
+    w.addDocument(doc);
+    w.close();
+    iwc = new IndexWriterConfig(new MockAnalyzer(random()));
+    w = new IndexWriter(dir, iwc);
+    doc = new Document();
+    doc.add(new BinaryPoint("dim", new byte[6]));
+    try {
+      w.addDocument(doc);
+    } catch (IllegalArgumentException iae) {
+      // expected
+      assertEquals("cannot change point numBytes from 4 to 6 for field=\"dim\"", iae.getMessage());
+    }
+    w.close();
+    dir.close();
+  }
+
+  public void testIllegalNumBytesChangeViaAddIndexesDirectory() throws Exception {
+    Directory dir = getDirectory(1);
+    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));
+    IndexWriter w = new IndexWriter(dir, iwc);
+    Document doc = new Document();
+    doc.add(new BinaryPoint("dim", new byte[4]));
+    w.addDocument(doc);
+    w.close();
+
+    Directory dir2 = getDirectory(1);
+    iwc = new IndexWriterConfig(new MockAnalyzer(random()));
+    w = new IndexWriter(dir2, iwc);
+    doc = new Document();
+    doc.add(new BinaryPoint("dim", new byte[6]));
+    w.addDocument(doc);
+    try {
+      w.addIndexes(new Directory[] {dir});
+    } catch (IllegalArgumentException iae) {
+      assertEquals("cannot change point numBytes from 6 to 4 for field=\"dim\"", iae.getMessage());
+    }
+    IOUtils.close(w, dir, dir2);
+  }
+
+  public void testIllegalNumBytesChangeViaAddIndexesCodecReader() throws Exception {
+    Directory dir = getDirectory(1);
+    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));
+    IndexWriter w = new IndexWriter(dir, iwc);
+    Document doc = new Document();
+    doc.add(new BinaryPoint("dim", new byte[4]));
+    w.addDocument(doc);
+    w.close();
+
+    Directory dir2 = getDirectory(1);
+    iwc = new IndexWriterConfig(new MockAnalyzer(random()));
+    w = new IndexWriter(dir2, iwc);
+    doc = new Document();
+    doc.add(new BinaryPoint("dim", new byte[6]));
+    w.addDocument(doc);
+    DirectoryReader r = DirectoryReader.open(dir);
+    try {
+      w.addIndexes(new CodecReader[] {getOnlySegmentReader(r)});
+    } catch (IllegalArgumentException iae) {
+      assertEquals("cannot change point numBytes from 6 to 4 for field=\"dim\"", iae.getMessage());
+    }
+    IOUtils.close(r, w, dir, dir2);
+  }
+
+  public void testIllegalNumBytesChangeViaAddIndexesSlowCodecReader() throws Exception {
+    Directory dir = getDirectory(1);
+    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));
+    IndexWriter w = new IndexWriter(dir, iwc);
+    Document doc = new Document();
+    doc.add(new BinaryPoint("dim", new byte[4]));
+    w.addDocument(doc);
+    w.close();
+
+    Directory dir2 = getDirectory(1);
+    iwc = new IndexWriterConfig(new MockAnalyzer(random()));
+    w = new IndexWriter(dir2, iwc);
+    doc = new Document();
+    doc.add(new BinaryPoint("dim", new byte[6]));
+    w.addDocument(doc);
+    DirectoryReader r = DirectoryReader.open(dir);
+    try {
+      TestUtil.addIndexesSlowly(w, r);
+    } catch (IllegalArgumentException iae) {
+      assertEquals("cannot change point numBytes from 6 to 4 for field=\"dim\"", iae.getMessage());
+    }
+    IOUtils.close(r, w, dir, dir2);
+  }
+
+  public void testIllegalTooManyBytes() throws Exception {
+    Directory dir = getDirectory(1);
+    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));
+    IndexWriter w = new IndexWriter(dir, iwc);
+    Document doc = new Document();
+    doc.add(new BinaryPoint("dim", new byte[PointValues.MAX_NUM_BYTES+1]));
+    try {
+      w.addDocument(doc);
+      fail("did not hit exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+    doc = new Document();
+    doc.add(new IntPoint("dim", 17));
+    w.addDocument(doc);
+    w.close();
+    dir.close();
+  }
+
+  public void testIllegalTooManyDimensions() throws Exception {
+    Directory dir = getDirectory(1);
+    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));
+    IndexWriter w = new IndexWriter(dir, iwc);
+    Document doc = new Document();
+    byte[][] values = new byte[PointValues.MAX_DIMENSIONS+1][];
+    for(int i=0;i<values.length;i++) {
+      values[i] = new byte[4];
+    }
+    doc.add(new BinaryPoint("dim", values));
+    try {
+      w.addDocument(doc);
+      fail("did not hit exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+    doc = new Document();
+    doc.add(new IntPoint("dim", 17));
+    w.addDocument(doc);
+    w.close();
+    dir.close();
+  }
+
+  private void doTestRandomBinary(int count) throws Exception {
+    int numDocs = TestUtil.nextInt(random(), count, count*2);
+    int numBytesPerDim = TestUtil.nextInt(random(), 2, PointValues.MAX_NUM_BYTES);
+    int numDims = TestUtil.nextInt(random(), 1, PointValues.MAX_DIMENSIONS);
+
+    byte[][][] docValues = new byte[numDocs][][];
+
+    for(int docID=0;docID<numDocs;docID++) {
+      byte[][] values = new byte[numDims][];
+      for(int dim=0;dim<numDims;dim++) {
+        values[dim] = new byte[numBytesPerDim];
+        // TODO: sometimes test on a "small" volume too, so we test the high density cases, higher chance of boundary, etc. cases:
+        random().nextBytes(values[dim]);
+      }
+      docValues[docID] = values;
+    }
+
+    verify(docValues, null, numDims, numBytesPerDim);
+  }
+
+  private Codec getCodec() {
+    if (Codec.getDefault().getName().equals("Lucene60")) {
+      int maxPointsInLeafNode = TestUtil.nextInt(random(), 50, 500);
+      double maxMBSortInHeap = 0.1 + (3*random().nextDouble());
+      if (VERBOSE) {
+        System.out.println("TEST: using Lucene60PointFormat with maxPointsInLeafNode=" + maxPointsInLeafNode + " and maxMBSortInHeap=" + maxMBSortInHeap);
+      }
+
+      return new FilterCodec("Lucene60", Codec.getDefault()) {
+        @Override
+        public PointFormat pointFormat() {
+          return new PointFormat() {
+            @Override
+            public PointWriter fieldsWriter(SegmentWriteState writeState) throws IOException {
+              return new Lucene60PointWriter(writeState, maxPointsInLeafNode, maxMBSortInHeap);
+            }
+
+            @Override
+            public PointReader fieldsReader(SegmentReadState readState) throws IOException {
+              return new Lucene60PointReader(readState);
+            }
+          };
+        }
+      };
+    } else {
+      return Codec.getDefault();
+    }
+  }
+
+  /** docIDs can be null, for the single valued case, else it maps value to docID, but all values for one doc must be adjacent */
+  private void verify(byte[][][] docValues, int[] docIDs, int numDims, int numBytesPerDim) throws Exception {
+    try (Directory dir = getDirectory(docValues.length)) {
+      while (true) {
+        try {
+          verify(dir, docValues, docIDs, numDims, numBytesPerDim, false);
+          return;
+        } catch (IllegalArgumentException iae) {
+          // This just means we got a too-small maxMB for the maxPointsInLeafNode; just retry
+          assertTrue(iae.getMessage().contains("either increase maxMBSortInHeap or decrease maxPointsInLeafNode"));
+        }
+      }
+    }
+  }
+
+  private void verify(Directory dir, byte[][][] docValues, int[] ids, int numDims, int numBytesPerDim, boolean expectExceptions) throws Exception {
+    int numValues = docValues.length;
+    if (VERBOSE) {
+      System.out.println("TEST: numValues=" + numValues + " numDims=" + numDims + " numBytesPerDim=" + numBytesPerDim);
+    }
+
+    // RandomIndexWriter is too slow:
+    boolean useRealWriter = docValues.length > 10000;
+
+    IndexWriterConfig iwc;
+    if (useRealWriter) {
+      iwc = new IndexWriterConfig(new MockAnalyzer(random()));
+    } else {
+      iwc = newIndexWriterConfig();
+    }
+    iwc.setCodec(getCodec());
+
+    if (expectExceptions) {
+      MergeScheduler ms = iwc.getMergeScheduler();
+      if (ms instanceof ConcurrentMergeScheduler) {
+        ((ConcurrentMergeScheduler) ms).setSuppressExceptions();
+      }
+    }
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+    DirectoryReader r = null;
+
+    // Compute actual min/max values:
+    byte[][] expectedMinValues = new byte[numDims][];
+    byte[][] expectedMaxValues = new byte[numDims][];
+    for(int ord=0;ord<docValues.length;ord++) {
+      for(int dim=0;dim<numDims;dim++) {
+        if (ord == 0) {
+          expectedMinValues[dim] = new byte[numBytesPerDim];
+          System.arraycopy(docValues[ord][dim], 0, expectedMinValues[dim], 0, numBytesPerDim);
+          expectedMaxValues[dim] = new byte[numBytesPerDim];
+          System.arraycopy(docValues[ord][dim], 0, expectedMaxValues[dim], 0, numBytesPerDim);
+        } else {
+          // TODO: it's cheating that we use StringHelper.compare for "truth": what if it's buggy?
+          if (StringHelper.compare(numBytesPerDim, docValues[ord][dim], 0, expectedMinValues[dim], 0) < 0) {
+            System.arraycopy(docValues[ord][dim], 0, expectedMinValues[dim], 0, numBytesPerDim);
+          }
+          if (StringHelper.compare(numBytesPerDim, docValues[ord][dim], 0, expectedMaxValues[dim], 0) > 0) {
+            System.arraycopy(docValues[ord][dim], 0, expectedMaxValues[dim], 0, numBytesPerDim);
+          }
+        }
+      }
+    }
+
+    // 20% of the time we add into a separate directory, then at some point use
+    // addIndexes to bring the indexed point values to the main directory:
+    Directory saveDir;
+    RandomIndexWriter saveW;
+    int addIndexesAt;
+    if (random().nextInt(5) == 1) {
+      saveDir = dir;
+      saveW = w;
+      dir = getDirectory(numValues);
+      if (useRealWriter) {
+        iwc = new IndexWriterConfig(new MockAnalyzer(random()));
+      } else {
+        iwc = newIndexWriterConfig();
+      }
+      iwc.setCodec(getCodec());
+      if (expectExceptions) {
+        MergeScheduler ms = iwc.getMergeScheduler();
+        if (ms instanceof ConcurrentMergeScheduler) {
+          ((ConcurrentMergeScheduler) ms).setSuppressExceptions();
+        }
+      }
+      w = new RandomIndexWriter(random(), dir, iwc);
+      addIndexesAt = TestUtil.nextInt(random(), 1, numValues-1);
+    } else {
+      saveW = null;
+      saveDir = null;
+      addIndexesAt = 0;
+    }
+
+    try {
+
+      Document doc = null;
+      int lastID = -1;
+      for(int ord=0;ord<numValues;ord++) {
+        int id;
+        if (ids == null) {
+          id = ord;
+        } else {
+          id = ids[ord];
+        }
+        if (id != lastID) {
+          if (doc != null) {
+            if (useRealWriter) {
+              w.w.addDocument(doc);
+            } else {
+              w.addDocument(doc);
+            }
+          }
+          doc = new Document();
+          doc.add(new NumericDocValuesField("id", id));
+        }
+        doc.add(new BinaryPoint("field", docValues[ord]));
+        lastID = id;
+
+        if (random().nextInt(30) == 17) {
+          // randomly index some documents without this field
+          if (useRealWriter) {
+            w.w.addDocument(new Document());
+          } else {
+            w.addDocument(new Document());
+          }
+          if (VERBOSE) {
+            System.out.println("add empty doc");
+          }
+        }
+
+        if (random().nextInt(30) == 17) {
+          // randomly index some documents with this field, but we will delete them:
+          Document xdoc = new Document();
+          xdoc.add(new BinaryPoint("field", docValues[ord]));
+          xdoc.add(new StringField("nukeme", "yes", Field.Store.NO));
+          if (useRealWriter) {
+            w.w.addDocument(xdoc);
+          } else {
+            w.addDocument(xdoc);
+          }
+          if (VERBOSE) {
+            System.out.println("add doc doc-to-delete");
+          }
+
+          if (random().nextInt(5) == 1) {
+            if (useRealWriter) {
+              w.w.deleteDocuments(new Term("nukeme", "yes"));
+            } else {
+              w.deleteDocuments(new Term("nukeme", "yes"));
+            }
+          }
+        }
+
+        if (VERBOSE) {
+          System.out.println("  ord=" + ord + " id=" + id);
+          for(int dim=0;dim<numDims;dim++) {
+            System.out.println("    dim=" + dim + " value=" + new BytesRef(docValues[ord][dim]));
+          }
+        }
+
+        if (saveW != null && ord >= addIndexesAt) {
+          switchIndex(w, dir, saveW);
+          w = saveW;
+          dir = saveDir;
+          saveW = null;
+          saveDir = null;
+        }
+      }
+      w.addDocument(doc);
+      w.deleteDocuments(new Term("nukeme", "yes"));
+
+      if (random().nextBoolean()) {
+        if (VERBOSE) {
+          System.out.println("\nTEST: now force merge");
+        }
+        w.forceMerge(1);
+      }
+
+      r = w.getReader();
+      w.close();
+
+      if (VERBOSE) {
+        System.out.println("TEST: reader=" + r);
+      }
+
+      PointValues dimValues = MultiPointValues.get(r);
+      if (VERBOSE) {
+        System.out.println("  dimValues=" + dimValues);
+      }
+      assertNotNull(dimValues);
+
+      NumericDocValues idValues = MultiDocValues.getNumericValues(r, "id");
+      Bits liveDocs = MultiFields.getLiveDocs(r);
+
+      // Verify min/max values are correct:
+      byte[] minValues = dimValues.getMinPackedValue("field");
+      byte[] maxValues = dimValues.getMaxPackedValue("field");
+      byte[] scratch = new byte[numBytesPerDim];
+      for(int dim=0;dim<numDims;dim++) {
+        System.arraycopy(minValues, dim*numBytesPerDim, scratch, 0, scratch.length);
+        //System.out.println("dim=" + dim + " expectedMin=" + new BytesRef(expectedMinValues[dim]) + " min=" + new BytesRef(scratch));
+        assertTrue(Arrays.equals(expectedMinValues[dim], scratch));
+        System.arraycopy(maxValues, dim*numBytesPerDim, scratch, 0, scratch.length);
+        //System.out.println("dim=" + dim + " expectedMax=" + new BytesRef(expectedMaxValues[dim]) + " max=" + new BytesRef(scratch));
+        assertTrue(Arrays.equals(expectedMaxValues[dim], scratch));
+      }
+
+      int iters = atLeast(100);
+      for(int iter=0;iter<iters;iter++) {
+        if (VERBOSE) {
+          System.out.println("\nTEST: iter=" + iter);
+        }
+
+        // Random N dims rect query:
+        byte[][] queryMin = new byte[numDims][];
+        byte[][] queryMax = new byte[numDims][];    
+        for(int dim=0;dim<numDims;dim++) {    
+          queryMin[dim] = new byte[numBytesPerDim];
+          random().nextBytes(queryMin[dim]);
+          queryMax[dim] = new byte[numBytesPerDim];
+          random().nextBytes(queryMax[dim]);
+          if (NumericUtils.compare(numBytesPerDim, queryMin[dim], 0, queryMax[dim], 0) > 0) {
+            byte[] x = queryMin[dim];
+            queryMin[dim] = queryMax[dim];
+            queryMax[dim] = x;
+          }
+        }
+
+        if (VERBOSE) {
+          for(int dim=0;dim<numDims;dim++) {
+            System.out.println("  dim=" + dim + "\n    queryMin=" + new BytesRef(queryMin[dim]) + "\n    queryMax=" + new BytesRef(queryMax[dim]));
+          }
+        }
+
+        final BitSet hits = new BitSet();
+
+        dimValues.intersect("field", new PointValues.IntersectVisitor() {
+            @Override
+            public void visit(int docID) {
+              if (liveDocs == null || liveDocs.get(docID)) {
+                hits.set((int) idValues.get(docID));
+              }
+              //System.out.println("visit docID=" + docID);
+            }
+
+            @Override
+            public void visit(int docID, byte[] packedValue) {
+              if (liveDocs != null && liveDocs.get(docID) == false) {
+                return;
+              }
+              //System.out.println("visit check docID=" + docID + " id=" + idValues.get(docID));
+              for(int dim=0;dim<numDims;dim++) {
+                //System.out.println("  dim=" + dim + " value=" + new BytesRef(packedValue, dim*numBytesPerDim, numBytesPerDim));
+                if (NumericUtils.compare(numBytesPerDim, packedValue, dim, queryMin[dim], 0) < 0 ||
+                    NumericUtils.compare(numBytesPerDim, packedValue, dim, queryMax[dim], 0) > 0) {
+                  //System.out.println("  no");
+                  return;
+                }
+              }
+
+              //System.out.println("  yes");
+              hits.set((int) idValues.get(docID));
+            }
+
+            @Override
+            public Relation compare(byte[] minPacked, byte[] maxPacked) {
+              boolean crosses = false;
+              //System.out.println("compare");
+              for(int dim=0;dim<numDims;dim++) {
+                if (NumericUtils.compare(numBytesPerDim, maxPacked, dim, queryMin[dim], 0) < 0 ||
+                    NumericUtils.compare(numBytesPerDim, minPacked, dim, queryMax[dim], 0) > 0) {
+                  //System.out.println("  query_outside_cell");
+                  return Relation.CELL_OUTSIDE_QUERY;
+                } else if (NumericUtils.compare(numBytesPerDim, minPacked, dim, queryMin[dim], 0) < 0 ||
+                           NumericUtils.compare(numBytesPerDim, maxPacked, dim, queryMax[dim], 0) > 0) {
+                  crosses = true;
+                }
+              }
+
+              if (crosses) {
+                //System.out.println("  query_crosses_cell");
+                return Relation.CELL_CROSSES_QUERY;
+              } else {
+                //System.out.println("  cell_inside_query");
+                return Relation.CELL_INSIDE_QUERY;
+              }
+            }
+          });
+
+        BitSet expected = new BitSet();
+        for(int ord=0;ord<numValues;ord++) {
+          boolean matches = true;
+          for(int dim=0;dim<numDims;dim++) {
+            byte[] x = docValues[ord][dim];
+            if (NumericUtils.compare(numBytesPerDim, x, 0, queryMin[dim], 0) < 0 ||
+                NumericUtils.compare(numBytesPerDim, x, 0, queryMax[dim], 0) > 0) {
+              matches = false;
+              break;
+            }
+          }
+
+          if (matches) {
+            int id;
+            if (ids == null) {
+              id = ord;
+            } else {
+              id = ids[ord];
+            }
+            expected.set(id);
+          }
+        }
+
+        int limit = Math.max(expected.length(), hits.length());
+        int failCount = 0;
+        int successCount = 0;
+        for(int id=0;id<limit;id++) {
+          if (expected.get(id) != hits.get(id)) {
+            System.out.println("FAIL: id=" + id);
+            failCount++;
+          } else {
+            successCount++;
+          }
+        }
+
+        if (failCount != 0) {
+          for(int docID=0;docID<r.maxDoc();docID++) {
+            System.out.println("  docID=" + docID + " id=" + idValues.get(docID));
+          }
+
+          fail(failCount + " docs failed; " + successCount + " docs succeeded");
+        }
+      }
+    } finally {
+      IOUtils.closeWhileHandlingException(r, w, saveW, saveDir == null ? null : dir);
+    }
+  }
+
+  private void switchIndex(RandomIndexWriter w, Directory dir, RandomIndexWriter saveW) throws IOException {
+    if (random().nextBoolean()) {
+      // Add via readers:
+      try (DirectoryReader r = w.getReader()) {
+        if (random().nextBoolean()) {
+          // Add via CodecReaders:
+          List<CodecReader> subs = new ArrayList<>();
+          for (LeafReaderContext context : r.leaves()) {
+            subs.add((CodecReader) context.reader());
+          }
+          if (VERBOSE) {
+            System.out.println("TEST: now use addIndexes(CodecReader[]) to switch writers");
+          }
+          saveW.addIndexes(subs.toArray(new CodecReader[subs.size()]));
+        } else {
+          if (VERBOSE) {
+            System.out.println("TEST: now use TestUtil.addIndexesSlowly(DirectoryReader[]) to switch writers");
+          }
+          TestUtil.addIndexesSlowly(saveW.w, r);
+        }
+      }
+    } else {
+      // Add via directory:
+      if (VERBOSE) {
+        System.out.println("TEST: now use addIndexes(Directory[]) to switch writers");
+      }
+      w.close();
+      saveW.addIndexes(new Directory[] {dir});
+    }
+    w.close();
+    dir.close();
+  }
+
+  private BigInteger randomBigInt(int numBytes) {
+    BigInteger x = new BigInteger(numBytes*8-1, random());
+    if (random().nextBoolean()) {
+      x = x.negate();
+    }
+    return x;
+  }
+
+  private static Directory noVirusChecker(Directory dir) {
+    if (dir instanceof MockDirectoryWrapper) {
+      ((MockDirectoryWrapper) dir).setEnableVirusScanner(false);
+    }
+    return dir;
+  }
+
+  private Directory getDirectory(int numPoints) throws IOException {
+    Directory dir;
+    if (numPoints > 100000) {
+      dir = newFSDirectory(createTempDir("TestBKDTree"));
+    } else {
+      dir = newDirectory();
+    }
+    noVirusChecker(dir);
+    //dir = FSDirectory.open(createTempDir());
+    return dir;
+  }
+}
diff --git a/lucene/core/src/test/org/apache/lucene/search/BaseTestRangeFilter.java b/lucene/core/src/test/org/apache/lucene/search/BaseTestRangeFilter.java
index 6943ce2..6679756 100644
--- a/lucene/core/src/test/org/apache/lucene/search/BaseTestRangeFilter.java
+++ b/lucene/core/src/test/org/apache/lucene/search/BaseTestRangeFilter.java
@@ -21,16 +21,12 @@ import java.io.IOException;
 import java.util.Random;
 
 import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.DimensionalDoubleField;
-import org.apache.lucene.document.DimensionalFloatField;
-import org.apache.lucene.document.DimensionalIntField;
-import org.apache.lucene.document.DimensionalLongField;
+import org.apache.lucene.document.DoublePoint;
+import org.apache.lucene.document.FloatPoint;
+import org.apache.lucene.document.IntPoint;
+import org.apache.lucene.document.LongPoint;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
-import org.apache.lucene.document.LegacyDoubleField;
-import org.apache.lucene.document.LegacyFloatField;
-import org.apache.lucene.document.LegacyIntField;
-import org.apache.lucene.document.LegacyLongField;
 import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.index.IndexReader;
@@ -127,13 +123,13 @@ public class BaseTestRangeFilter extends LuceneTestCase {
     Document doc = new Document();
     Field idField = newStringField(random, "id", "", Field.Store.YES);
     Field idDVField = new SortedDocValuesField("id", new BytesRef());
-    Field intIdField = new DimensionalIntField("id_int", 0);
+    Field intIdField = new IntPoint("id_int", 0);
     Field intDVField = new NumericDocValuesField("id_int", 0);
-    Field floatIdField = new DimensionalFloatField("id_float", 0);
+    Field floatIdField = new FloatPoint("id_float", 0);
     Field floatDVField = new NumericDocValuesField("id_float", 0);
-    Field longIdField = new DimensionalLongField("id_long", 0);
+    Field longIdField = new LongPoint("id_long", 0);
     Field longDVField = new NumericDocValuesField("id_long", 0);
-    Field doubleIdField = new DimensionalDoubleField("id_double", 0);
+    Field doubleIdField = new DoublePoint("id_double", 0);
     Field doubleDVField = new NumericDocValuesField("id_double", 0);
     Field randField = newStringField(random, "rand", "", Field.Store.YES);
     Field randDVField = new SortedDocValuesField("rand", new BytesRef());
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestDimensionalRangeQuery.java b/lucene/core/src/test/org/apache/lucene/search/TestDimensionalRangeQuery.java
deleted file mode 100644
index 2db1e0a..0000000
--- a/lucene/core/src/test/org/apache/lucene/search/TestDimensionalRangeQuery.java
+++ /dev/null
@@ -1,1056 +0,0 @@
-package org.apache.lucene.search;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.nio.charset.StandardCharsets;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.BitSet;
-import java.util.List;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.atomic.AtomicBoolean;
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.DimensionalFormat;
-import org.apache.lucene.codecs.DimensionalReader;
-import org.apache.lucene.codecs.DimensionalWriter;
-import org.apache.lucene.codecs.FilterCodec;
-import org.apache.lucene.codecs.lucene60.Lucene60DimensionalReader;
-import org.apache.lucene.codecs.lucene60.Lucene60DimensionalWriter;
-import org.apache.lucene.document.DimensionalBinaryField;
-import org.apache.lucene.document.DimensionalLongField;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.NumericDocValuesField;
-import org.apache.lucene.document.SortedNumericDocValuesField;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.DimensionalValues;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.LeafReaderContext;
-import org.apache.lucene.index.MultiDocValues;
-import org.apache.lucene.index.NumericDocValues;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.MockDirectoryWrapper;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.NumericUtils;
-import org.apache.lucene.util.TestUtil;
-import org.junit.BeforeClass;
-
-public class TestDimensionalRangeQuery extends LuceneTestCase {
-
-  // Controls what range of values we randomly generate, so we sometimes test narrow ranges:
-  static long valueMid;
-  static int valueRange;
-
-  @BeforeClass
-  public static void beforeClass() {
-    if (random().nextBoolean()) {
-      valueMid = random().nextLong();
-      if (random().nextBoolean()) {
-        // Wide range
-        valueRange = TestUtil.nextInt(random(), 1, Integer.MAX_VALUE);
-      } else {
-        // Narrow range
-        valueRange = TestUtil.nextInt(random(), 1, 100000);
-      }
-      if (VERBOSE) {
-        System.out.println("TEST: will generate long values " + valueMid + " +/- " + valueRange);
-      }
-    } else {
-      // All longs
-      valueRange = 0;
-      if (VERBOSE) {
-        System.out.println("TEST: will generate all long values");
-      }
-    }
-  }
-
-  public void testAllEqual() throws Exception {
-    int numValues = atLeast(10000);
-    long value = randomValue(false);
-    long[] values = new long[numValues];
-
-    if (VERBOSE) {
-      System.out.println("TEST: use same value=" + value);
-    }
-    Arrays.fill(values, value);
-
-    verifyLongs(values, null);
-  }
-
-  public void testRandomLongsTiny() throws Exception {
-    // Make sure single-leaf-node case is OK:
-    doTestRandomLongs(10);
-  }
-
-  public void testRandomLongsMedium() throws Exception {
-    doTestRandomLongs(10000);
-  }
-
-  @Nightly
-  public void testRandomLongsBig() throws Exception {
-    doTestRandomLongs(200000);
-  }
-
-  private void doTestRandomLongs(int count) throws Exception {
-
-    int numValues = atLeast(count);
-
-    if (VERBOSE) {
-      System.out.println("TEST: numValues=" + numValues);
-    }
-
-    long[] values = new long[numValues];
-    int[] ids = new int[numValues];
-
-    boolean singleValued = random().nextBoolean();
-
-    int sameValuePct = random().nextInt(100);
-
-    int id = 0;
-    for (int ord=0;ord<numValues;ord++) {
-      if (ord > 0 && random().nextInt(100) < sameValuePct) {
-        // Identical to old value
-        values[ord] = values[random().nextInt(ord)];
-      } else {
-        values[ord] = randomValue(false);
-      }
-
-      ids[ord] = id;
-      if (singleValued || random().nextInt(2) == 1) {
-        id++;
-      }
-    }
-
-    verifyLongs(values, ids);
-  }
-
-  public void testLongEncode() {
-    for(int i=0;i<10000;i++) {
-      long v = random().nextLong();
-      byte[] tmp = new byte[8];
-      NumericUtils.longToBytes(v, tmp, 0);
-      long v2 = NumericUtils.bytesToLong(tmp, 0);
-      assertEquals("got bytes=" + Arrays.toString(tmp), v, v2);
-    }
-  }
-
-  // verify for long values
-  private static void verifyLongs(long[] values, int[] ids) throws Exception {
-    IndexWriterConfig iwc = newIndexWriterConfig();
-
-    // Else we can get O(N^2) merging:
-    int mbd = iwc.getMaxBufferedDocs();
-    if (mbd != -1 && mbd < values.length/100) {
-      iwc.setMaxBufferedDocs(values.length/100);
-    }
-    iwc.setCodec(getCodec());
-    Directory dir;
-    if (values.length > 100000) {
-      dir = noVirusChecker(newFSDirectory(createTempDir("TestRangeTree")));
-    } else {
-      dir = getDirectory();
-    }
-
-    int missingPct = random().nextInt(100);
-    int deletedPct = random().nextInt(100);
-    if (VERBOSE) {
-      System.out.println("  missingPct=" + missingPct);
-      System.out.println("  deletedPct=" + deletedPct);
-    }
-
-    BitSet missing = new BitSet();
-    BitSet deleted = new BitSet();
-
-    Document doc = null;
-    int lastID = -1;
-
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
-    for(int ord=0;ord<values.length;ord++) {
-
-      int id;
-      if (ids == null) {
-        id = ord;
-      } else {
-        id = ids[ord];
-      }
-      if (id != lastID) {
-        if (random().nextInt(100) < missingPct) {
-          missing.set(id);
-          if (VERBOSE) {
-            System.out.println("  missing id=" + id);
-          }
-        }
-
-        if (doc != null) {
-          w.addDocument(doc);
-          if (random().nextInt(100) < deletedPct) {
-            int idToDelete = random().nextInt(id);
-            w.deleteDocuments(new Term("id", ""+idToDelete));
-            deleted.set(idToDelete);
-            if (VERBOSE) {
-              System.out.println("  delete id=" + idToDelete);
-            }
-          }
-        }
-
-        doc = new Document();
-        doc.add(newStringField("id", ""+id, Field.Store.NO));
-        doc.add(new NumericDocValuesField("id", id));
-        lastID = id;
-      }
-
-      if (missing.get(id) == false) {
-        doc.add(new DimensionalLongField("sn_value", values[id]));
-        byte[] bytes = new byte[8];
-        NumericUtils.longToBytes(values[id], bytes, 0);
-        doc.add(new DimensionalBinaryField("ss_value", bytes));
-      }
-    }
-
-    w.addDocument(doc);
-
-    if (random().nextBoolean()) {
-      if (VERBOSE) {
-        System.out.println("  forceMerge(1)");
-      }
-      w.forceMerge(1);
-    }
-    final IndexReader r = w.getReader();
-    w.close();
-
-    IndexSearcher s = newSearcher(r);
-
-    int numThreads = TestUtil.nextInt(random(), 2, 5);
-
-    if (VERBOSE) {
-      System.out.println("TEST: use " + numThreads + " query threads; searcher=" + s);
-    }
-
-    List<Thread> threads = new ArrayList<>();
-    final int iters = atLeast(100);
-
-    final CountDownLatch startingGun = new CountDownLatch(1);
-    final AtomicBoolean failed = new AtomicBoolean();
-
-    for(int i=0;i<numThreads;i++) {
-      Thread thread = new Thread() {
-          @Override
-          public void run() {
-            try {
-              _run();
-            } catch (Exception e) {
-              failed.set(true);
-              throw new RuntimeException(e);
-            }
-          }
-
-          private void _run() throws Exception {
-            startingGun.await();
-
-            NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, "id");
-            for (int iter=0;iter<iters && failed.get() == false;iter++) {
-              Long lower = randomValue(true);
-              Long upper = randomValue(true);
-
-              if (lower != null && upper != null && upper < lower) {
-                long x = lower;
-                lower = upper;
-                upper = x;
-              }
-
-              boolean includeLower = random().nextBoolean();
-              boolean includeUpper = random().nextBoolean();
-              Query query;
-
-              if (VERBOSE) {
-                System.out.println("\n" + Thread.currentThread().getName() + ": TEST: iter=" + iter + " value=" + lower + " (inclusive?=" + includeLower + ") TO " + upper + " (inclusive?=" + includeUpper + ")");
-                byte[] tmp = new byte[8];
-                if (lower != null) {
-                  NumericUtils.longToBytes(lower, tmp, 0);
-                  System.out.println("  lower bytes=" + Arrays.toString(tmp));
-                }
-                if (upper != null) {
-                  NumericUtils.longToBytes(upper, tmp, 0);
-                  System.out.println("  upper bytes=" + Arrays.toString(tmp));
-                }
-              }
-
-              if (random().nextBoolean()) {
-                query = DimensionalRangeQuery.new1DLongRange("sn_value", lower, includeLower, upper, includeUpper);
-              } else {
-                byte[] lowerBytes;
-                if (lower == null) {
-                  lowerBytes = null;
-                } else {
-                  lowerBytes = new byte[8];
-                  NumericUtils.longToBytes(lower, lowerBytes, 0);
-                }
-                byte[] upperBytes;
-                if (upper == null) {
-                  upperBytes = null;
-                } else {
-                  upperBytes = new byte[8];
-                  NumericUtils.longToBytes(upper, upperBytes, 0);
-                }
-                query = DimensionalRangeQuery.new1DBinaryRange("ss_value", lowerBytes, includeLower, upperBytes, includeUpper);
-              }
-
-              if (VERBOSE) {
-                System.out.println(Thread.currentThread().getName() + ":  using query: " + query);
-              }
-
-              final BitSet hits = new BitSet();
-              s.search(query, new SimpleCollector() {
-
-                  private int docBase;
-
-                  @Override
-                  public boolean needsScores() {
-                    return false;
-                  }
-
-                  @Override
-                  protected void doSetNextReader(LeafReaderContext context) throws IOException {
-                    docBase = context.docBase;
-                  }
-
-                  @Override
-                  public void collect(int doc) {
-                    hits.set(docBase+doc);
-                  }
-                });
-
-              if (VERBOSE) {
-                System.out.println(Thread.currentThread().getName() + ":  hitCount: " + hits.cardinality());
-              }
-      
-              for(int docID=0;docID<r.maxDoc();docID++) {
-                int id = (int) docIDToID.get(docID);
-                boolean expected = missing.get(id) == false && deleted.get(id) == false && matches(lower, includeLower, upper, includeUpper, values[id]);
-                if (hits.get(docID) != expected) {
-                  // We do exact quantized comparison so the bbox query should never disagree:
-                  fail(Thread.currentThread().getName() + ": iter=" + iter + " id=" + id + " docID=" + docID + " value=" + values[id] + " (range: " + lower + " TO " + upper + ") expected " + expected + " but got: " + hits.get(docID) + " deleted?=" + deleted.get(id) + " query=" + query);
-                  }
-                }
-              }
-            }
-        };
-      thread.setName("T" + i);
-      thread.start();
-      threads.add(thread);
-    }
-    startingGun.countDown();
-    for(Thread thread : threads) {
-      thread.join();
-    }
-    IOUtils.close(r, dir);
-  }
-
-  public void testRandomBinaryTiny() throws Exception {
-    doTestRandomBinary(10);
-  }
-
-  public void testRandomBinaryMedium() throws Exception {
-    doTestRandomBinary(10000);
-  }
-
-  @Nightly
-  public void testRandomBinaryBig() throws Exception {
-    doTestRandomBinary(200000);
-  }
-
-  private void doTestRandomBinary(int count) throws Exception {
-    int numValues = TestUtil.nextInt(random(), count, count*2);
-    int numBytesPerDim = TestUtil.nextInt(random(), 2, DimensionalValues.MAX_NUM_BYTES);
-    int numDims = TestUtil.nextInt(random(), 1, DimensionalValues.MAX_DIMENSIONS);
-
-    int sameValuePct = random().nextInt(100);
-
-    byte[][][] docValues = new byte[numValues][][];
-
-    boolean singleValued = random().nextBoolean();
-    int[] ids = new int[numValues];
-
-    int id = 0;
-    for(int ord=0;ord<numValues;ord++) {
-      if (ord > 0 && random().nextInt(100) < sameValuePct) {
-        // Identical to old value
-        docValues[ord] = docValues[random().nextInt(ord)];
-      } else {
-        // Make a new random value
-        byte[][] values = new byte[numDims][];
-        for(int dim=0;dim<numDims;dim++) {
-          values[dim] = new byte[numBytesPerDim];
-          random().nextBytes(values[dim]);
-        }
-        docValues[ord] = values;
-      }
-      ids[ord] = id;
-      if (singleValued || random().nextInt(2) == 1) {
-        id++;
-      }
-    }
-
-    verifyBinary(docValues, ids, numBytesPerDim);
-  }
-
-  // verify for byte[][] values
-  private void verifyBinary(byte[][][] docValues, int[] ids, int numBytesPerDim) throws Exception {
-    IndexWriterConfig iwc = newIndexWriterConfig();
-
-    int numDims = docValues[0].length;
-    int bytesPerDim = docValues[0][0].length;
-
-    // Else we can get O(N^2) merging:
-    int mbd = iwc.getMaxBufferedDocs();
-    if (mbd != -1 && mbd < docValues.length/100) {
-      iwc.setMaxBufferedDocs(docValues.length/100);
-    }
-    iwc.setCodec(getCodec());
-
-    Directory dir;
-    if (docValues.length > 100000) {
-      dir = noVirusChecker(newFSDirectory(createTempDir("TestDimensionalRangeQuery")));
-    } else {
-      dir = getDirectory();
-    }
-
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
-
-    int numValues = docValues.length;
-    if (VERBOSE) {
-      System.out.println("TEST: numValues=" + numValues + " numDims=" + numDims + " numBytesPerDim=" + numBytesPerDim);
-    }
-
-    int missingPct = random().nextInt(100);
-    int deletedPct = random().nextInt(100);
-    if (VERBOSE) {
-      System.out.println("  missingPct=" + missingPct);
-      System.out.println("  deletedPct=" + deletedPct);
-    }
-
-    BitSet missing = new BitSet();
-    BitSet deleted = new BitSet();
-
-    Document doc = null;
-    int lastID = -1;
-
-    for(int ord=0;ord<numValues;ord++) {
-      int id = ids[ord];
-      if (id != lastID) {
-        if (random().nextInt(100) < missingPct) {
-          missing.set(id);
-          if (VERBOSE) {
-            System.out.println("  missing id=" + id);
-          }
-        }
-
-        if (doc != null) {
-          w.addDocument(doc);
-          if (random().nextInt(100) < deletedPct) {
-            int idToDelete = random().nextInt(id);
-            w.deleteDocuments(new Term("id", ""+idToDelete));
-            deleted.set(idToDelete);
-            if (VERBOSE) {
-              System.out.println("  delete id=" + idToDelete);
-            }
-          }
-        }
-
-        doc = new Document();
-        doc.add(newStringField("id", ""+id, Field.Store.NO));
-        doc.add(new NumericDocValuesField("id", id));
-        lastID = id;
-      }
-
-      if (missing.get(id) == false) {
-        doc.add(new DimensionalBinaryField("value", docValues[ord]));
-        if (VERBOSE) {
-          System.out.println("id=" + id);
-          for(int dim=0;dim<numDims;dim++) {
-            System.out.println("  dim=" + dim + " value=" + bytesToString(docValues[ord][dim]));
-          }
-        }
-      }
-    }
-
-    w.addDocument(doc);
-
-    if (random().nextBoolean()) {
-      if (VERBOSE) {
-        System.out.println("  forceMerge(1)");
-      }
-      w.forceMerge(1);
-    }
-    final IndexReader r = w.getReader();
-    w.close();
-
-    IndexSearcher s = newSearcher(r);
-
-    // int numThreads = TestUtil.nextInt(random(), 2, 5);
-    int numThreads = 1;
-
-    if (VERBOSE) {
-      System.out.println("TEST: use " + numThreads + " query threads; searcher=" + s);
-    }
-
-    List<Thread> threads = new ArrayList<>();
-    final int iters = atLeast(100);
-
-    final CountDownLatch startingGun = new CountDownLatch(1);
-    final AtomicBoolean failed = new AtomicBoolean();
-
-    for(int i=0;i<numThreads;i++) {
-      Thread thread = new Thread() {
-          @Override
-          public void run() {
-            try {
-              _run();
-            } catch (Exception e) {
-              failed.set(true);
-              throw new RuntimeException(e);
-            }
-          }
-
-          private void _run() throws Exception {
-            startingGun.await();
-
-            NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, "id");
-
-            for (int iter=0;iter<iters && failed.get() == false;iter++) {
-
-              boolean[] includeUpper = new boolean[numDims];
-              boolean[] includeLower = new boolean[numDims];
-              byte[][] lower = new byte[numDims][];
-              byte[][] upper = new byte[numDims][];
-              for(int dim=0;dim<numDims;dim++) {
-                if (random().nextInt(5) != 1) {
-                  lower[dim] = new byte[bytesPerDim];
-                  random().nextBytes(lower[dim]);
-                } else {
-                  // open-ended on the lower bound
-                }
-                if (random().nextInt(5) != 1) {
-                  upper[dim] = new byte[bytesPerDim];
-                  random().nextBytes(upper[dim]);
-                } else {
-                  // open-ended on the upper bound
-                }
-
-                if (lower[dim] != null && upper[dim] != null && NumericUtils.compare(bytesPerDim, lower[dim], 0, upper[dim], 0) > 0) {
-                  byte[] x = lower[dim];
-                  lower[dim] = upper[dim];
-                  upper[dim] = x;
-                }
-
-                includeLower[dim] = random().nextBoolean();
-                includeUpper[dim] = random().nextBoolean();
-              }
-
-              if (VERBOSE) {
-                System.out.println("\n" + Thread.currentThread().getName() + ": TEST: iter=" + iter);
-                for(int dim=0;dim<numDims;dim++) {
-                  System.out.println("  dim=" + dim + " " +
-                                     bytesToString(lower[dim]) +
-                                     " (inclusive?=" + includeLower[dim] + ") TO " +
-                                     bytesToString(upper[dim]) +
-                                     " (inclusive?=" + includeUpper[dim] + ")");
-                }
-              }
-
-              Query query = new DimensionalRangeQuery("value", lower, includeLower, upper, includeUpper);
-
-              if (VERBOSE) {
-                System.out.println(Thread.currentThread().getName() + ":  using query: " + query);
-              }
-
-              final BitSet hits = new BitSet();
-              s.search(query, new SimpleCollector() {
-
-                  private int docBase;
-
-                  @Override
-                  public boolean needsScores() {
-                    return false;
-                  }
-
-                  @Override
-                  protected void doSetNextReader(LeafReaderContext context) throws IOException {
-                    docBase = context.docBase;
-                  }
-
-                  @Override
-                  public void collect(int doc) {
-                    hits.set(docBase+doc);
-                  }
-                });
-
-              if (VERBOSE) {
-                System.out.println(Thread.currentThread().getName() + ":  hitCount: " + hits.cardinality());
-              }
-
-              BitSet expected = new BitSet();
-              for(int ord=0;ord<numValues;ord++) {
-                int id = ids[ord];
-                if (missing.get(id) == false && deleted.get(id) == false && matches(bytesPerDim, lower, includeLower, upper, includeUpper, docValues[ord])) {
-                  expected.set(id);
-                }
-              }
-
-              int failCount = 0;
-              for(int docID=0;docID<r.maxDoc();docID++) {
-                int id = (int) docIDToID.get(docID);
-                if (hits.get(docID) != expected.get(id)) {
-                  System.out.println("FAIL: iter=" + iter + " id=" + id + " docID=" + docID + " expected=" + expected.get(id) + " but got " + hits.get(docID) + " deleted?=" + deleted.get(id) + " missing?=" + missing.get(id));
-                  for(int dim=0;dim<numDims;dim++) {
-                    System.out.println("  dim=" + dim + " range: " + bytesToString(lower[dim]) + " (inclusive?=" + includeLower[dim] + ") TO " + bytesToString(upper[dim]) + " (inclusive?=" + includeUpper[dim] + ")");
-                    failCount++;
-                  }
-                }
-              }
-              if (failCount != 0) {
-                fail(failCount + " hits were wrong");
-              }
-            }
-          }
-        };
-      thread.setName("T" + i);
-      thread.start();
-      threads.add(thread);
-    }
-    startingGun.countDown();
-    for(Thread thread : threads) {
-      thread.join();
-    }
-    IOUtils.close(r, dir);
-  }
-
-  private static boolean matches(Long lower, boolean includeLower, Long upper, boolean includeUpper, long value) {
-    if (includeLower == false && lower != null) {
-      if (lower == Long.MAX_VALUE) {
-        return false;
-      }
-      lower++;
-    }
-    if (includeUpper == false && upper != null) {
-      if (upper == Long.MIN_VALUE) {
-        return false;
-      }
-      upper--;
-    }
-
-    return (lower == null || value >= lower) && (upper == null || value <= upper);
-  }
-
-  static String bytesToString(byte[] bytes) {
-    if (bytes == null) {
-      return "null";
-    }
-    return new BytesRef(bytes).toString();
-  }
-
-  private static boolean matches(int bytesPerDim, byte[][] lower, boolean[] includeLower, byte[][] upper, boolean[] includeUpper, byte[][] value) {
-    int numDims = lower.length;
-    for(int dim=0;dim<numDims;dim++) {
-      int cmp;
-      if (lower[dim] == null) {
-        cmp = 1;
-      } else {
-        cmp = NumericUtils.compare(bytesPerDim, value[dim], 0, lower[dim], 0);
-      }
-
-      if (cmp < 0 || (cmp == 0 && includeLower[dim] == false)) {
-        // Value is below the lower bound, on this dim
-        return false;
-      }
-
-      if (upper[dim] == null) {
-        cmp = -1;
-      } else {
-        cmp = NumericUtils.compare(bytesPerDim, value[dim], 0, upper[dim], 0);
-      }
-
-      if (cmp > 0 || (cmp == 0 && includeUpper[dim] == false)) {
-        // Value is above the upper bound, on this dim
-        return false;
-      }
-    }
-
-    return true;
-  }
-
-  private static Long randomValue(boolean allowNull) {
-    if (valueRange == 0) {
-      if (allowNull && random().nextInt(10) == 1) {
-        return null;
-      } else {
-        return random().nextLong();
-      }
-    } else {
-      return valueMid + TestUtil.nextInt(random(), -valueRange, valueRange);
-    }
-  }
-  
-  public void testMinMaxLong() throws Exception {
-    Directory dir = getDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig();
-    iwc.setCodec(getCodec());
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
-    Document doc = new Document();
-    doc.add(new DimensionalLongField("value", Long.MIN_VALUE));
-    w.addDocument(doc);
-
-    doc = new Document();
-    doc.add(new DimensionalLongField("value", Long.MAX_VALUE));
-    w.addDocument(doc);
-
-    IndexReader r = w.getReader();
-
-    IndexSearcher s = newSearcher(r);
-
-    assertEquals(1, s.count(DimensionalRangeQuery.new1DLongRange("value", Long.MIN_VALUE, true, 0L, true)));
-    assertEquals(1, s.count(DimensionalRangeQuery.new1DLongRange("value", 0L, true, Long.MAX_VALUE, true)));
-    assertEquals(2, s.count(DimensionalRangeQuery.new1DLongRange("value", Long.MIN_VALUE, true, Long.MAX_VALUE, true)));
-
-    IOUtils.close(r, w, dir);
-  }
-
-  private static byte[] toUTF8(String s) {
-    return s.getBytes(StandardCharsets.UTF_8);
-  }
-
-  // Right zero pads:
-  private static byte[] toUTF8(String s, int length) {
-    byte[] bytes = s.getBytes(StandardCharsets.UTF_8);
-    if (length < bytes.length) {
-      throw new IllegalArgumentException("length=" + length + " but string's UTF8 bytes has length=" + bytes.length);
-    }
-    byte[] result = new byte[length];
-    System.arraycopy(bytes, 0, result, 0, bytes.length);
-    return result;
-  }
-
-  public void testBasicSortedSet() throws Exception {
-    Directory dir = getDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig();
-    iwc.setCodec(getCodec());
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
-    Document doc = new Document();
-    doc.add(new DimensionalBinaryField("value", toUTF8("abc")));
-    w.addDocument(doc);
-    doc = new Document();
-    doc.add(new DimensionalBinaryField("value", toUTF8("def")));
-    w.addDocument(doc);
-
-    IndexReader r = w.getReader();
-
-    IndexSearcher s = newSearcher(r);
-
-    assertEquals(1, s.count(DimensionalRangeQuery.new1DBinaryRange("value",
-                                                                   toUTF8("aaa"),
-                                                                   true,
-                                                                   toUTF8("bbb"),
-                                                                   true)));
-    assertEquals(1, s.count(DimensionalRangeQuery.new1DBinaryRange("value",
-                                                                   toUTF8("c", 3),
-                                                                   true,
-                                                                   toUTF8("e", 3),
-                                                                   true)));
-    assertEquals(2, s.count(DimensionalRangeQuery.new1DBinaryRange("value",
-                                                                   toUTF8("a", 3),
-                                                                   true,
-                                                                   toUTF8("z", 3),
-                                                                   true)));
-    assertEquals(1, s.count(DimensionalRangeQuery.new1DBinaryRange("value",
-                                                                   null,
-                                                                   true,
-                                                                   toUTF8("abc"),
-                                                                   true)));
-    assertEquals(1, s.count(DimensionalRangeQuery.new1DBinaryRange("value",
-                                                                   toUTF8("a", 3),
-                                                                   true,
-                                                                   toUTF8("abc"),
-                                                                   true)));
-    assertEquals(0, s.count(DimensionalRangeQuery.new1DBinaryRange("value",
-                                                                   toUTF8("a", 3),
-                                                                   true,
-                                                                   toUTF8("abc"),
-                                                                   false)));
-    assertEquals(1, s.count(DimensionalRangeQuery.new1DBinaryRange("value",
-                                                                   toUTF8("def"),
-                                                                   true,
-                                                                   null,
-                                                                   false)));
-    assertEquals(1, s.count(DimensionalRangeQuery.new1DBinaryRange("value",
-                                                                   toUTF8(("def")),
-                                                                   true,
-                                                                   toUTF8("z", 3),
-                                                                   true)));
-    assertEquals(0, s.count(DimensionalRangeQuery.new1DBinaryRange("value",
-                                                                   toUTF8("def"),
-                                                                   false,
-                                                                   toUTF8("z", 3),
-                                                                   true)));
-
-    IOUtils.close(r, w, dir);
-  }
-
-  public void testLongMinMaxNumeric() throws Exception {
-    Directory dir = getDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig();
-    iwc.setCodec(getCodec());
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
-    Document doc = new Document();
-    doc.add(new DimensionalLongField("value", Long.MIN_VALUE));
-    w.addDocument(doc);
-    doc = new Document();
-    doc.add(new DimensionalLongField("value", Long.MAX_VALUE));
-    w.addDocument(doc);
-
-    IndexReader r = w.getReader();
-
-    IndexSearcher s = newSearcher(r);
-
-    assertEquals(2, s.count(DimensionalRangeQuery.new1DLongRange("value", Long.MIN_VALUE, true, Long.MAX_VALUE, true)));
-    assertEquals(1, s.count(DimensionalRangeQuery.new1DLongRange("value", Long.MIN_VALUE, true, Long.MAX_VALUE, false)));
-    assertEquals(1, s.count(DimensionalRangeQuery.new1DLongRange("value", Long.MIN_VALUE, false, Long.MAX_VALUE, true)));
-    assertEquals(0, s.count(DimensionalRangeQuery.new1DLongRange("value", Long.MIN_VALUE, false, Long.MAX_VALUE, false)));
-
-    assertEquals(2, s.count(DimensionalRangeQuery.new1DBinaryRange("value", (byte[]) null, true, null, true)));
-
-    IOUtils.close(r, w, dir);
-  }
-
-  public void testLongMinMaxSortedSet() throws Exception {
-    Directory dir = getDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig();
-    iwc.setCodec(getCodec());
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
-    Document doc = new Document();
-    doc.add(new DimensionalLongField("value", Long.MIN_VALUE));
-    w.addDocument(doc);
-    doc = new Document();
-    doc.add(new DimensionalLongField("value", Long.MAX_VALUE));
-    w.addDocument(doc);
-
-    IndexReader r = w.getReader();
-
-    // We can't wrap with "exotic" readers because the query must see the RangeTreeDVFormat:
-    IndexSearcher s = newSearcher(r, false);
-
-    assertEquals(2, s.count(DimensionalRangeQuery.new1DLongRange("value", Long.MIN_VALUE, true, Long.MAX_VALUE, true)));
-    assertEquals(1, s.count(DimensionalRangeQuery.new1DLongRange("value", Long.MIN_VALUE, true, Long.MAX_VALUE, false)));
-    assertEquals(1, s.count(DimensionalRangeQuery.new1DLongRange("value", Long.MIN_VALUE, false, Long.MAX_VALUE, true)));
-    assertEquals(0, s.count(DimensionalRangeQuery.new1DLongRange("value", Long.MIN_VALUE, false, Long.MAX_VALUE, false)));
-
-    assertEquals(2, s.count(DimensionalRangeQuery.new1DLongRange("value", (Long) null, true, null, true)));
-
-    IOUtils.close(r, w, dir);
-  }
-
-  public void testSortedSetNoOrdsMatch() throws Exception {
-    Directory dir = getDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig();
-    iwc.setCodec(getCodec());
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
-    Document doc = new Document();
-    doc.add(new DimensionalBinaryField("value", toUTF8("a")));
-    w.addDocument(doc);
-    doc = new Document();
-    doc.add(new DimensionalBinaryField("value", toUTF8("z")));
-    w.addDocument(doc);
-
-    IndexReader r = w.getReader();
-
-    IndexSearcher s = newSearcher(r);
-    assertEquals(0, s.count(DimensionalRangeQuery.new1DBinaryRange("value", toUTF8("m"), true, toUTF8("n"), false)));
-
-    assertEquals(2, s.count(DimensionalRangeQuery.new1DBinaryRange("value", (byte[]) null, true, null, true)));
-
-    IOUtils.close(r, w, dir);
-  }
-
-  public void testNumericNoValuesMatch() throws Exception {
-    Directory dir = getDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig();
-    iwc.setCodec(getCodec());
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
-    Document doc = new Document();
-    doc.add(new SortedNumericDocValuesField("value", 17));
-    w.addDocument(doc);
-    doc = new Document();
-    doc.add(new SortedNumericDocValuesField("value", 22));
-    w.addDocument(doc);
-
-    IndexReader r = w.getReader();
-
-    IndexSearcher s = new IndexSearcher(r);
-    assertEquals(0, s.count(DimensionalRangeQuery.new1DLongRange("value", 17L, true, 13L, false)));
-
-    IOUtils.close(r, w, dir);
-  }
-
-  public void testNoDocs() throws Exception {
-    Directory dir = getDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig();
-    iwc.setCodec(getCodec());
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
-    w.addDocument(new Document());
-
-    IndexReader r = w.getReader();
-
-    IndexSearcher s = newSearcher(r);
-    assertEquals(0, s.count(DimensionalRangeQuery.new1DLongRange("value", 17L, true, 13L, false)));
-
-    IOUtils.close(r, w, dir);
-  }
-
-  public void testWrongNumDims() throws Exception {
-    Directory dir = getDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig();
-    iwc.setCodec(getCodec());
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
-    Document doc = new Document();
-    doc.add(new DimensionalLongField("value", Long.MIN_VALUE));
-    w.addDocument(doc);
-
-    IndexReader r = w.getReader();
-
-    // no wrapping, else the exc might happen in executor thread:
-    IndexSearcher s = new IndexSearcher(r);
-    byte[][] point = new byte[2][];
-    try {
-      s.count(new DimensionalRangeQuery("value", point, new boolean[] {true, true}, point, new boolean[] {true, true}));
-    } catch (IllegalArgumentException iae) {
-      assertEquals("field=\"value\" was indexed with numDims=1 but this query has numDims=2", iae.getMessage());
-    }
-
-    IOUtils.close(r, w, dir);
-  }
-
-  public void testWrongNumBytes() throws Exception {
-    Directory dir = getDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig();
-    iwc.setCodec(getCodec());
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
-    Document doc = new Document();
-    doc.add(new DimensionalLongField("value", Long.MIN_VALUE));
-    w.addDocument(doc);
-
-    IndexReader r = w.getReader();
-
-    // no wrapping, else the exc might happen in executor thread:
-    IndexSearcher s = new IndexSearcher(r);
-    byte[][] point = new byte[1][];
-    point[0] = new byte[10];
-    try {
-      s.count(new DimensionalRangeQuery("value", point, new boolean[] {true}, point, new boolean[] {true}));
-    } catch (IllegalArgumentException iae) {
-      assertEquals("field=\"value\" was indexed with bytesPerDim=8 but this query has bytesPerDim=10", iae.getMessage());
-    }
-
-    IOUtils.close(r, w, dir);
-  }
-
-  public void testAllDimensionalDocsWereDeletedAndThenMergedAgain() throws Exception {
-    Directory dir = getDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig();
-    iwc.setCodec(getCodec());
-    IndexWriter w = new IndexWriter(dir, iwc);
-    Document doc = new Document();
-    doc.add(new StringField("id", "0", Field.Store.NO));
-    doc.add(new DimensionalLongField("value", 0L));
-    w.addDocument(doc);
-
-    // Add document that won't be deleted to avoid IW dropping
-    // segment below since it's 100% deleted:
-    w.addDocument(new Document());
-    w.commit();
-
-    // Need another segment so we invoke BKDWriter.merge
-    doc = new Document();
-    doc.add(new StringField("id", "0", Field.Store.NO));
-    doc.add(new DimensionalLongField("value", 0L));
-    w.addDocument(doc);
-    w.addDocument(new Document());
-
-    w.deleteDocuments(new Term("id", "0"));
-    w.forceMerge(1);
-
-    doc = new Document();
-    doc.add(new StringField("id", "0", Field.Store.NO));
-    doc.add(new DimensionalLongField("value", 0L));
-    w.addDocument(doc);
-    w.addDocument(new Document());
-
-    w.deleteDocuments(new Term("id", "0"));
-    w.forceMerge(1);
-
-    IOUtils.close(w, dir);
-  }
-
-  private static Directory noVirusChecker(Directory dir) {
-    if (dir instanceof MockDirectoryWrapper) {
-      ((MockDirectoryWrapper) dir).setEnableVirusScanner(false);
-    }
-    return dir;
-  }
-
-  private static Directory getDirectory() {     
-    return noVirusChecker(newDirectory());
-  }
-
-  private static Codec getCodec() {
-    if (Codec.getDefault().getName().equals("Lucene60")) {
-      int maxPointsInLeafNode = TestUtil.nextInt(random(), 16, 2048);
-      double maxMBSortInHeap = 3.0 + (3*random().nextDouble());
-      if (VERBOSE) {
-        System.out.println("TEST: using Lucene60DimensionalFormat with maxPointsInLeafNode=" + maxPointsInLeafNode + " and maxMBSortInHeap=" + maxMBSortInHeap);
-      }
-
-      return new FilterCodec("Lucene60", Codec.getDefault()) {
-        @Override
-        public DimensionalFormat dimensionalFormat() {
-          return new DimensionalFormat() {
-            @Override
-            public DimensionalWriter fieldsWriter(SegmentWriteState writeState) throws IOException {
-              return new Lucene60DimensionalWriter(writeState, maxPointsInLeafNode, maxMBSortInHeap);
-            }
-
-            @Override
-            public DimensionalReader fieldsReader(SegmentReadState readState) throws IOException {
-              return new Lucene60DimensionalReader(readState);
-            }
-          };
-        }
-      };
-    } else {
-      return Codec.getDefault();
-    }
-  }
-}
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestPointQueries.java b/lucene/core/src/test/org/apache/lucene/search/TestPointQueries.java
new file mode 100644
index 0000000..15d4a06
--- /dev/null
+++ b/lucene/core/src/test/org/apache/lucene/search/TestPointQueries.java
@@ -0,0 +1,1100 @@
+package org.apache.lucene.search;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.nio.charset.StandardCharsets;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.BitSet;
+import java.util.List;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.FilterCodec;
+import org.apache.lucene.codecs.PointFormat;
+import org.apache.lucene.codecs.PointReader;
+import org.apache.lucene.codecs.PointWriter;
+import org.apache.lucene.codecs.lucene60.Lucene60PointReader;
+import org.apache.lucene.codecs.lucene60.Lucene60PointWriter;
+import org.apache.lucene.document.BinaryPoint;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.DoublePoint;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FloatPoint;
+import org.apache.lucene.document.IntPoint;
+import org.apache.lucene.document.LongPoint;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.document.SortedNumericDocValuesField;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.index.MultiDocValues;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.PointValues;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.MockDirectoryWrapper;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.NumericUtils;
+import org.apache.lucene.util.TestUtil;
+import org.junit.BeforeClass;
+
+public class TestPointQueries extends LuceneTestCase {
+
+  // Controls what range of values we randomly generate, so we sometimes test narrow ranges:
+  static long valueMid;
+  static int valueRange;
+
+  @BeforeClass
+  public static void beforeClass() {
+    if (random().nextBoolean()) {
+      valueMid = random().nextLong();
+      if (random().nextBoolean()) {
+        // Wide range
+        valueRange = TestUtil.nextInt(random(), 1, Integer.MAX_VALUE);
+      } else {
+        // Narrow range
+        valueRange = TestUtil.nextInt(random(), 1, 100000);
+      }
+      if (VERBOSE) {
+        System.out.println("TEST: will generate long values " + valueMid + " +/- " + valueRange);
+      }
+    } else {
+      // All longs
+      valueRange = 0;
+      if (VERBOSE) {
+        System.out.println("TEST: will generate all long values");
+      }
+    }
+  }
+
+  public void testAllEqual() throws Exception {
+    int numValues = atLeast(10000);
+    long value = randomValue(false);
+    long[] values = new long[numValues];
+
+    if (VERBOSE) {
+      System.out.println("TEST: use same value=" + value);
+    }
+    Arrays.fill(values, value);
+
+    verifyLongs(values, null);
+  }
+
+  public void testRandomLongsTiny() throws Exception {
+    // Make sure single-leaf-node case is OK:
+    doTestRandomLongs(10);
+  }
+
+  public void testRandomLongsMedium() throws Exception {
+    doTestRandomLongs(10000);
+  }
+
+  @Nightly
+  public void testRandomLongsBig() throws Exception {
+    doTestRandomLongs(200000);
+  }
+
+  private void doTestRandomLongs(int count) throws Exception {
+
+    int numValues = atLeast(count);
+
+    if (VERBOSE) {
+      System.out.println("TEST: numValues=" + numValues);
+    }
+
+    long[] values = new long[numValues];
+    int[] ids = new int[numValues];
+
+    boolean singleValued = random().nextBoolean();
+
+    int sameValuePct = random().nextInt(100);
+
+    int id = 0;
+    for (int ord=0;ord<numValues;ord++) {
+      if (ord > 0 && random().nextInt(100) < sameValuePct) {
+        // Identical to old value
+        values[ord] = values[random().nextInt(ord)];
+      } else {
+        values[ord] = randomValue(false);
+      }
+
+      ids[ord] = id;
+      if (singleValued || random().nextInt(2) == 1) {
+        id++;
+      }
+    }
+
+    verifyLongs(values, ids);
+  }
+
+  public void testLongEncode() {
+    for(int i=0;i<10000;i++) {
+      long v = random().nextLong();
+      byte[] tmp = new byte[8];
+      NumericUtils.longToBytes(v, tmp, 0);
+      long v2 = NumericUtils.bytesToLong(tmp, 0);
+      assertEquals("got bytes=" + Arrays.toString(tmp), v, v2);
+    }
+  }
+
+  // verify for long values
+  private static void verifyLongs(long[] values, int[] ids) throws Exception {
+    IndexWriterConfig iwc = newIndexWriterConfig();
+
+    // Else we can get O(N^2) merging:
+    int mbd = iwc.getMaxBufferedDocs();
+    if (mbd != -1 && mbd < values.length/100) {
+      iwc.setMaxBufferedDocs(values.length/100);
+    }
+    iwc.setCodec(getCodec());
+    Directory dir;
+    if (values.length > 100000) {
+      dir = noVirusChecker(newFSDirectory(createTempDir("TestRangeTree")));
+    } else {
+      dir = getDirectory();
+    }
+
+    int missingPct = random().nextInt(100);
+    int deletedPct = random().nextInt(100);
+    if (VERBOSE) {
+      System.out.println("  missingPct=" + missingPct);
+      System.out.println("  deletedPct=" + deletedPct);
+    }
+
+    BitSet missing = new BitSet();
+    BitSet deleted = new BitSet();
+
+    Document doc = null;
+    int lastID = -1;
+
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+    for(int ord=0;ord<values.length;ord++) {
+
+      int id;
+      if (ids == null) {
+        id = ord;
+      } else {
+        id = ids[ord];
+      }
+      if (id != lastID) {
+        if (random().nextInt(100) < missingPct) {
+          missing.set(id);
+          if (VERBOSE) {
+            System.out.println("  missing id=" + id);
+          }
+        }
+
+        if (doc != null) {
+          w.addDocument(doc);
+          if (random().nextInt(100) < deletedPct) {
+            int idToDelete = random().nextInt(id);
+            w.deleteDocuments(new Term("id", ""+idToDelete));
+            deleted.set(idToDelete);
+            if (VERBOSE) {
+              System.out.println("  delete id=" + idToDelete);
+            }
+          }
+        }
+
+        doc = new Document();
+        doc.add(newStringField("id", ""+id, Field.Store.NO));
+        doc.add(new NumericDocValuesField("id", id));
+        lastID = id;
+      }
+
+      if (missing.get(id) == false) {
+        doc.add(new LongPoint("sn_value", values[id]));
+        byte[] bytes = new byte[8];
+        NumericUtils.longToBytes(values[id], bytes, 0);
+        doc.add(new BinaryPoint("ss_value", bytes));
+      }
+    }
+
+    w.addDocument(doc);
+
+    if (random().nextBoolean()) {
+      if (VERBOSE) {
+        System.out.println("  forceMerge(1)");
+      }
+      w.forceMerge(1);
+    }
+    final IndexReader r = w.getReader();
+    w.close();
+
+    IndexSearcher s = newSearcher(r);
+
+    int numThreads = TestUtil.nextInt(random(), 2, 5);
+
+    if (VERBOSE) {
+      System.out.println("TEST: use " + numThreads + " query threads; searcher=" + s);
+    }
+
+    List<Thread> threads = new ArrayList<>();
+    final int iters = atLeast(100);
+
+    final CountDownLatch startingGun = new CountDownLatch(1);
+    final AtomicBoolean failed = new AtomicBoolean();
+
+    for(int i=0;i<numThreads;i++) {
+      Thread thread = new Thread() {
+          @Override
+          public void run() {
+            try {
+              _run();
+            } catch (Exception e) {
+              failed.set(true);
+              throw new RuntimeException(e);
+            }
+          }
+
+          private void _run() throws Exception {
+            startingGun.await();
+
+            NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, "id");
+            for (int iter=0;iter<iters && failed.get() == false;iter++) {
+              Long lower = randomValue(true);
+              Long upper = randomValue(true);
+
+              if (lower != null && upper != null && upper < lower) {
+                long x = lower;
+                lower = upper;
+                upper = x;
+              }
+
+              boolean includeLower = random().nextBoolean();
+              boolean includeUpper = random().nextBoolean();
+              Query query;
+
+              if (VERBOSE) {
+                System.out.println("\n" + Thread.currentThread().getName() + ": TEST: iter=" + iter + " value=" + lower + " (inclusive?=" + includeLower + ") TO " + upper + " (inclusive?=" + includeUpper + ")");
+                byte[] tmp = new byte[8];
+                if (lower != null) {
+                  NumericUtils.longToBytes(lower, tmp, 0);
+                  System.out.println("  lower bytes=" + Arrays.toString(tmp));
+                }
+                if (upper != null) {
+                  NumericUtils.longToBytes(upper, tmp, 0);
+                  System.out.println("  upper bytes=" + Arrays.toString(tmp));
+                }
+              }
+
+              if (random().nextBoolean()) {
+                query = PointRangeQuery.new1DLongRange("sn_value", lower, includeLower, upper, includeUpper);
+              } else {
+                byte[] lowerBytes;
+                if (lower == null) {
+                  lowerBytes = null;
+                } else {
+                  lowerBytes = new byte[8];
+                  NumericUtils.longToBytes(lower, lowerBytes, 0);
+                }
+                byte[] upperBytes;
+                if (upper == null) {
+                  upperBytes = null;
+                } else {
+                  upperBytes = new byte[8];
+                  NumericUtils.longToBytes(upper, upperBytes, 0);
+                }
+                query = PointRangeQuery.new1DBinaryRange("ss_value", lowerBytes, includeLower, upperBytes, includeUpper);
+              }
+
+              if (VERBOSE) {
+                System.out.println(Thread.currentThread().getName() + ":  using query: " + query);
+              }
+
+              final BitSet hits = new BitSet();
+              s.search(query, new SimpleCollector() {
+
+                  private int docBase;
+
+                  @Override
+                  public boolean needsScores() {
+                    return false;
+                  }
+
+                  @Override
+                  protected void doSetNextReader(LeafReaderContext context) throws IOException {
+                    docBase = context.docBase;
+                  }
+
+                  @Override
+                  public void collect(int doc) {
+                    hits.set(docBase+doc);
+                  }
+                });
+
+              if (VERBOSE) {
+                System.out.println(Thread.currentThread().getName() + ":  hitCount: " + hits.cardinality());
+              }
+      
+              for(int docID=0;docID<r.maxDoc();docID++) {
+                int id = (int) docIDToID.get(docID);
+                boolean expected = missing.get(id) == false && deleted.get(id) == false && matches(lower, includeLower, upper, includeUpper, values[id]);
+                if (hits.get(docID) != expected) {
+                  // We do exact quantized comparison so the bbox query should never disagree:
+                  fail(Thread.currentThread().getName() + ": iter=" + iter + " id=" + id + " docID=" + docID + " value=" + values[id] + " (range: " + lower + " TO " + upper + ") expected " + expected + " but got: " + hits.get(docID) + " deleted?=" + deleted.get(id) + " query=" + query);
+                  }
+                }
+              }
+            }
+        };
+      thread.setName("T" + i);
+      thread.start();
+      threads.add(thread);
+    }
+    startingGun.countDown();
+    for(Thread thread : threads) {
+      thread.join();
+    }
+    IOUtils.close(r, dir);
+  }
+
+  public void testRandomBinaryTiny() throws Exception {
+    doTestRandomBinary(10);
+  }
+
+  public void testRandomBinaryMedium() throws Exception {
+    doTestRandomBinary(10000);
+  }
+
+  @Nightly
+  public void testRandomBinaryBig() throws Exception {
+    doTestRandomBinary(200000);
+  }
+
+  private void doTestRandomBinary(int count) throws Exception {
+    int numValues = TestUtil.nextInt(random(), count, count*2);
+    int numBytesPerDim = TestUtil.nextInt(random(), 2, PointValues.MAX_NUM_BYTES);
+    int numDims = TestUtil.nextInt(random(), 1, PointValues.MAX_DIMENSIONS);
+
+    int sameValuePct = random().nextInt(100);
+
+    byte[][][] docValues = new byte[numValues][][];
+
+    boolean singleValued = random().nextBoolean();
+    int[] ids = new int[numValues];
+
+    int id = 0;
+    for(int ord=0;ord<numValues;ord++) {
+      if (ord > 0 && random().nextInt(100) < sameValuePct) {
+        // Identical to old value
+        docValues[ord] = docValues[random().nextInt(ord)];
+      } else {
+        // Make a new random value
+        byte[][] values = new byte[numDims][];
+        for(int dim=0;dim<numDims;dim++) {
+          values[dim] = new byte[numBytesPerDim];
+          random().nextBytes(values[dim]);
+        }
+        docValues[ord] = values;
+      }
+      ids[ord] = id;
+      if (singleValued || random().nextInt(2) == 1) {
+        id++;
+      }
+    }
+
+    verifyBinary(docValues, ids, numBytesPerDim);
+  }
+
+  // verify for byte[][] values
+  private void verifyBinary(byte[][][] docValues, int[] ids, int numBytesPerDim) throws Exception {
+    IndexWriterConfig iwc = newIndexWriterConfig();
+
+    int numDims = docValues[0].length;
+    int bytesPerDim = docValues[0][0].length;
+
+    // Else we can get O(N^2) merging:
+    int mbd = iwc.getMaxBufferedDocs();
+    if (mbd != -1 && mbd < docValues.length/100) {
+      iwc.setMaxBufferedDocs(docValues.length/100);
+    }
+    iwc.setCodec(getCodec());
+
+    Directory dir;
+    if (docValues.length > 100000) {
+      dir = noVirusChecker(newFSDirectory(createTempDir("TestPointRangeQuery")));
+    } else {
+      dir = getDirectory();
+    }
+
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+
+    int numValues = docValues.length;
+    if (VERBOSE) {
+      System.out.println("TEST: numValues=" + numValues + " numDims=" + numDims + " numBytesPerDim=" + numBytesPerDim);
+    }
+
+    int missingPct = random().nextInt(100);
+    int deletedPct = random().nextInt(100);
+    if (VERBOSE) {
+      System.out.println("  missingPct=" + missingPct);
+      System.out.println("  deletedPct=" + deletedPct);
+    }
+
+    BitSet missing = new BitSet();
+    BitSet deleted = new BitSet();
+
+    Document doc = null;
+    int lastID = -1;
+
+    for(int ord=0;ord<numValues;ord++) {
+      int id = ids[ord];
+      if (id != lastID) {
+        if (random().nextInt(100) < missingPct) {
+          missing.set(id);
+          if (VERBOSE) {
+            System.out.println("  missing id=" + id);
+          }
+        }
+
+        if (doc != null) {
+          w.addDocument(doc);
+          if (random().nextInt(100) < deletedPct) {
+            int idToDelete = random().nextInt(id);
+            w.deleteDocuments(new Term("id", ""+idToDelete));
+            deleted.set(idToDelete);
+            if (VERBOSE) {
+              System.out.println("  delete id=" + idToDelete);
+            }
+          }
+        }
+
+        doc = new Document();
+        doc.add(newStringField("id", ""+id, Field.Store.NO));
+        doc.add(new NumericDocValuesField("id", id));
+        lastID = id;
+      }
+
+      if (missing.get(id) == false) {
+        doc.add(new BinaryPoint("value", docValues[ord]));
+        if (VERBOSE) {
+          System.out.println("id=" + id);
+          for(int dim=0;dim<numDims;dim++) {
+            System.out.println("  dim=" + dim + " value=" + bytesToString(docValues[ord][dim]));
+          }
+        }
+      }
+    }
+
+    w.addDocument(doc);
+
+    if (random().nextBoolean()) {
+      if (VERBOSE) {
+        System.out.println("  forceMerge(1)");
+      }
+      w.forceMerge(1);
+    }
+    final IndexReader r = w.getReader();
+    w.close();
+
+    IndexSearcher s = newSearcher(r);
+
+    // int numThreads = TestUtil.nextInt(random(), 2, 5);
+    int numThreads = 1;
+
+    if (VERBOSE) {
+      System.out.println("TEST: use " + numThreads + " query threads; searcher=" + s);
+    }
+
+    List<Thread> threads = new ArrayList<>();
+    final int iters = atLeast(100);
+
+    final CountDownLatch startingGun = new CountDownLatch(1);
+    final AtomicBoolean failed = new AtomicBoolean();
+
+    for(int i=0;i<numThreads;i++) {
+      Thread thread = new Thread() {
+          @Override
+          public void run() {
+            try {
+              _run();
+            } catch (Exception e) {
+              failed.set(true);
+              throw new RuntimeException(e);
+            }
+          }
+
+          private void _run() throws Exception {
+            startingGun.await();
+
+            NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, "id");
+
+            for (int iter=0;iter<iters && failed.get() == false;iter++) {
+
+              boolean[] includeUpper = new boolean[numDims];
+              boolean[] includeLower = new boolean[numDims];
+              byte[][] lower = new byte[numDims][];
+              byte[][] upper = new byte[numDims][];
+              for(int dim=0;dim<numDims;dim++) {
+                if (random().nextInt(5) != 1) {
+                  lower[dim] = new byte[bytesPerDim];
+                  random().nextBytes(lower[dim]);
+                } else {
+                  // open-ended on the lower bound
+                }
+                if (random().nextInt(5) != 1) {
+                  upper[dim] = new byte[bytesPerDim];
+                  random().nextBytes(upper[dim]);
+                } else {
+                  // open-ended on the upper bound
+                }
+
+                if (lower[dim] != null && upper[dim] != null && NumericUtils.compare(bytesPerDim, lower[dim], 0, upper[dim], 0) > 0) {
+                  byte[] x = lower[dim];
+                  lower[dim] = upper[dim];
+                  upper[dim] = x;
+                }
+
+                includeLower[dim] = random().nextBoolean();
+                includeUpper[dim] = random().nextBoolean();
+              }
+
+              if (VERBOSE) {
+                System.out.println("\n" + Thread.currentThread().getName() + ": TEST: iter=" + iter);
+                for(int dim=0;dim<numDims;dim++) {
+                  System.out.println("  dim=" + dim + " " +
+                                     bytesToString(lower[dim]) +
+                                     " (inclusive?=" + includeLower[dim] + ") TO " +
+                                     bytesToString(upper[dim]) +
+                                     " (inclusive?=" + includeUpper[dim] + ")");
+                }
+              }
+
+              Query query = new PointRangeQuery("value", lower, includeLower, upper, includeUpper);
+
+              if (VERBOSE) {
+                System.out.println(Thread.currentThread().getName() + ":  using query: " + query);
+              }
+
+              final BitSet hits = new BitSet();
+              s.search(query, new SimpleCollector() {
+
+                  private int docBase;
+
+                  @Override
+                  public boolean needsScores() {
+                    return false;
+                  }
+
+                  @Override
+                  protected void doSetNextReader(LeafReaderContext context) throws IOException {
+                    docBase = context.docBase;
+                  }
+
+                  @Override
+                  public void collect(int doc) {
+                    hits.set(docBase+doc);
+                  }
+                });
+
+              if (VERBOSE) {
+                System.out.println(Thread.currentThread().getName() + ":  hitCount: " + hits.cardinality());
+              }
+
+              BitSet expected = new BitSet();
+              for(int ord=0;ord<numValues;ord++) {
+                int id = ids[ord];
+                if (missing.get(id) == false && deleted.get(id) == false && matches(bytesPerDim, lower, includeLower, upper, includeUpper, docValues[ord])) {
+                  expected.set(id);
+                }
+              }
+
+              int failCount = 0;
+              for(int docID=0;docID<r.maxDoc();docID++) {
+                int id = (int) docIDToID.get(docID);
+                if (hits.get(docID) != expected.get(id)) {
+                  System.out.println("FAIL: iter=" + iter + " id=" + id + " docID=" + docID + " expected=" + expected.get(id) + " but got " + hits.get(docID) + " deleted?=" + deleted.get(id) + " missing?=" + missing.get(id));
+                  for(int dim=0;dim<numDims;dim++) {
+                    System.out.println("  dim=" + dim + " range: " + bytesToString(lower[dim]) + " (inclusive?=" + includeLower[dim] + ") TO " + bytesToString(upper[dim]) + " (inclusive?=" + includeUpper[dim] + ")");
+                    failCount++;
+                  }
+                }
+              }
+              if (failCount != 0) {
+                fail(failCount + " hits were wrong");
+              }
+            }
+          }
+        };
+      thread.setName("T" + i);
+      thread.start();
+      threads.add(thread);
+    }
+    startingGun.countDown();
+    for(Thread thread : threads) {
+      thread.join();
+    }
+    IOUtils.close(r, dir);
+  }
+
+  private static boolean matches(Long lower, boolean includeLower, Long upper, boolean includeUpper, long value) {
+    if (includeLower == false && lower != null) {
+      if (lower == Long.MAX_VALUE) {
+        return false;
+      }
+      lower++;
+    }
+    if (includeUpper == false && upper != null) {
+      if (upper == Long.MIN_VALUE) {
+        return false;
+      }
+      upper--;
+    }
+
+    return (lower == null || value >= lower) && (upper == null || value <= upper);
+  }
+
+  static String bytesToString(byte[] bytes) {
+    if (bytes == null) {
+      return "null";
+    }
+    return new BytesRef(bytes).toString();
+  }
+
+  private static boolean matches(int bytesPerDim, byte[][] lower, boolean[] includeLower, byte[][] upper, boolean[] includeUpper, byte[][] value) {
+    int numDims = lower.length;
+    for(int dim=0;dim<numDims;dim++) {
+      int cmp;
+      if (lower[dim] == null) {
+        cmp = 1;
+      } else {
+        cmp = NumericUtils.compare(bytesPerDim, value[dim], 0, lower[dim], 0);
+      }
+
+      if (cmp < 0 || (cmp == 0 && includeLower[dim] == false)) {
+        // Value is below the lower bound, on this dim
+        return false;
+      }
+
+      if (upper[dim] == null) {
+        cmp = -1;
+      } else {
+        cmp = NumericUtils.compare(bytesPerDim, value[dim], 0, upper[dim], 0);
+      }
+
+      if (cmp > 0 || (cmp == 0 && includeUpper[dim] == false)) {
+        // Value is above the upper bound, on this dim
+        return false;
+      }
+    }
+
+    return true;
+  }
+
+  private static Long randomValue(boolean allowNull) {
+    if (valueRange == 0) {
+      if (allowNull && random().nextInt(10) == 1) {
+        return null;
+      } else {
+        return random().nextLong();
+      }
+    } else {
+      return valueMid + TestUtil.nextInt(random(), -valueRange, valueRange);
+    }
+  }
+  
+  public void testMinMaxLong() throws Exception {
+    Directory dir = getDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig();
+    iwc.setCodec(getCodec());
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+    Document doc = new Document();
+    doc.add(new LongPoint("value", Long.MIN_VALUE));
+    w.addDocument(doc);
+
+    doc = new Document();
+    doc.add(new LongPoint("value", Long.MAX_VALUE));
+    w.addDocument(doc);
+
+    IndexReader r = w.getReader();
+
+    IndexSearcher s = newSearcher(r);
+
+    assertEquals(1, s.count(PointRangeQuery.new1DLongRange("value", Long.MIN_VALUE, true, 0L, true)));
+    assertEquals(1, s.count(PointRangeQuery.new1DLongRange("value", 0L, true, Long.MAX_VALUE, true)));
+    assertEquals(2, s.count(PointRangeQuery.new1DLongRange("value", Long.MIN_VALUE, true, Long.MAX_VALUE, true)));
+
+    IOUtils.close(r, w, dir);
+  }
+
+  private static byte[] toUTF8(String s) {
+    return s.getBytes(StandardCharsets.UTF_8);
+  }
+
+  // Right zero pads:
+  private static byte[] toUTF8(String s, int length) {
+    byte[] bytes = s.getBytes(StandardCharsets.UTF_8);
+    if (length < bytes.length) {
+      throw new IllegalArgumentException("length=" + length + " but string's UTF8 bytes has length=" + bytes.length);
+    }
+    byte[] result = new byte[length];
+    System.arraycopy(bytes, 0, result, 0, bytes.length);
+    return result;
+  }
+
+  public void testBasicSortedSet() throws Exception {
+    Directory dir = getDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig();
+    iwc.setCodec(getCodec());
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+    Document doc = new Document();
+    doc.add(new BinaryPoint("value", toUTF8("abc")));
+    w.addDocument(doc);
+    doc = new Document();
+    doc.add(new BinaryPoint("value", toUTF8("def")));
+    w.addDocument(doc);
+
+    IndexReader r = w.getReader();
+
+    IndexSearcher s = newSearcher(r);
+
+    assertEquals(1, s.count(PointRangeQuery.new1DBinaryRange("value",
+        toUTF8("aaa"),
+        true,
+        toUTF8("bbb"),
+        true)));
+    assertEquals(1, s.count(PointRangeQuery.new1DBinaryRange("value",
+        toUTF8("c", 3),
+        true,
+        toUTF8("e", 3),
+        true)));
+    assertEquals(2, s.count(PointRangeQuery.new1DBinaryRange("value",
+        toUTF8("a", 3),
+        true,
+        toUTF8("z", 3),
+        true)));
+    assertEquals(1, s.count(PointRangeQuery.new1DBinaryRange("value",
+        null,
+        true,
+        toUTF8("abc"),
+        true)));
+    assertEquals(1, s.count(PointRangeQuery.new1DBinaryRange("value",
+        toUTF8("a", 3),
+        true,
+        toUTF8("abc"),
+        true)));
+    assertEquals(0, s.count(PointRangeQuery.new1DBinaryRange("value",
+        toUTF8("a", 3),
+        true,
+        toUTF8("abc"),
+        false)));
+    assertEquals(1, s.count(PointRangeQuery.new1DBinaryRange("value",
+        toUTF8("def"),
+        true,
+        null,
+        false)));
+    assertEquals(1, s.count(PointRangeQuery.new1DBinaryRange("value",
+        toUTF8(("def")),
+        true,
+        toUTF8("z", 3),
+        true)));
+    assertEquals(0, s.count(PointRangeQuery.new1DBinaryRange("value",
+        toUTF8("def"),
+        false,
+        toUTF8("z", 3),
+        true)));
+
+    IOUtils.close(r, w, dir);
+  }
+
+  public void testLongMinMaxNumeric() throws Exception {
+    Directory dir = getDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig();
+    iwc.setCodec(getCodec());
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+    Document doc = new Document();
+    doc.add(new LongPoint("value", Long.MIN_VALUE));
+    w.addDocument(doc);
+    doc = new Document();
+    doc.add(new LongPoint("value", Long.MAX_VALUE));
+    w.addDocument(doc);
+
+    IndexReader r = w.getReader();
+
+    IndexSearcher s = newSearcher(r);
+
+    assertEquals(2, s.count(PointRangeQuery.new1DLongRange("value", Long.MIN_VALUE, true, Long.MAX_VALUE, true)));
+    assertEquals(1, s.count(PointRangeQuery.new1DLongRange("value", Long.MIN_VALUE, true, Long.MAX_VALUE, false)));
+    assertEquals(1, s.count(PointRangeQuery.new1DLongRange("value", Long.MIN_VALUE, false, Long.MAX_VALUE, true)));
+    assertEquals(0, s.count(PointRangeQuery.new1DLongRange("value", Long.MIN_VALUE, false, Long.MAX_VALUE, false)));
+
+    assertEquals(2, s.count(PointRangeQuery.new1DBinaryRange("value", (byte[]) null, true, null, true)));
+
+    IOUtils.close(r, w, dir);
+  }
+
+  public void testLongMinMaxSortedSet() throws Exception {
+    Directory dir = getDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig();
+    iwc.setCodec(getCodec());
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+    Document doc = new Document();
+    doc.add(new LongPoint("value", Long.MIN_VALUE));
+    w.addDocument(doc);
+    doc = new Document();
+    doc.add(new LongPoint("value", Long.MAX_VALUE));
+    w.addDocument(doc);
+
+    IndexReader r = w.getReader();
+
+    // We can't wrap with "exotic" readers because the query must see the RangeTreeDVFormat:
+    IndexSearcher s = newSearcher(r, false);
+
+    assertEquals(2, s.count(PointRangeQuery.new1DLongRange("value", Long.MIN_VALUE, true, Long.MAX_VALUE, true)));
+    assertEquals(1, s.count(PointRangeQuery.new1DLongRange("value", Long.MIN_VALUE, true, Long.MAX_VALUE, false)));
+    assertEquals(1, s.count(PointRangeQuery.new1DLongRange("value", Long.MIN_VALUE, false, Long.MAX_VALUE, true)));
+    assertEquals(0, s.count(PointRangeQuery.new1DLongRange("value", Long.MIN_VALUE, false, Long.MAX_VALUE, false)));
+
+    assertEquals(2, s.count(PointRangeQuery.new1DLongRange("value", (Long) null, true, null, true)));
+
+    IOUtils.close(r, w, dir);
+  }
+
+  public void testSortedSetNoOrdsMatch() throws Exception {
+    Directory dir = getDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig();
+    iwc.setCodec(getCodec());
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+    Document doc = new Document();
+    doc.add(new BinaryPoint("value", toUTF8("a")));
+    w.addDocument(doc);
+    doc = new Document();
+    doc.add(new BinaryPoint("value", toUTF8("z")));
+    w.addDocument(doc);
+
+    IndexReader r = w.getReader();
+
+    IndexSearcher s = newSearcher(r);
+    assertEquals(0, s.count(PointRangeQuery.new1DBinaryRange("value", toUTF8("m"), true, toUTF8("n"), false)));
+
+    assertEquals(2, s.count(PointRangeQuery.new1DBinaryRange("value", (byte[]) null, true, null, true)));
+
+    IOUtils.close(r, w, dir);
+  }
+
+  public void testNumericNoValuesMatch() throws Exception {
+    Directory dir = getDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig();
+    iwc.setCodec(getCodec());
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+    Document doc = new Document();
+    doc.add(new SortedNumericDocValuesField("value", 17));
+    w.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedNumericDocValuesField("value", 22));
+    w.addDocument(doc);
+
+    IndexReader r = w.getReader();
+
+    IndexSearcher s = new IndexSearcher(r);
+    assertEquals(0, s.count(PointRangeQuery.new1DLongRange("value", 17L, true, 13L, false)));
+
+    IOUtils.close(r, w, dir);
+  }
+
+  public void testNoDocs() throws Exception {
+    Directory dir = getDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig();
+    iwc.setCodec(getCodec());
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+    w.addDocument(new Document());
+
+    IndexReader r = w.getReader();
+
+    IndexSearcher s = newSearcher(r);
+    assertEquals(0, s.count(PointRangeQuery.new1DLongRange("value", 17L, true, 13L, false)));
+
+    IOUtils.close(r, w, dir);
+  }
+
+  public void testWrongNumDims() throws Exception {
+    Directory dir = getDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig();
+    iwc.setCodec(getCodec());
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+    Document doc = new Document();
+    doc.add(new LongPoint("value", Long.MIN_VALUE));
+    w.addDocument(doc);
+
+    IndexReader r = w.getReader();
+
+    // no wrapping, else the exc might happen in executor thread:
+    IndexSearcher s = new IndexSearcher(r);
+    byte[][] point = new byte[2][];
+    try {
+      s.count(new PointRangeQuery("value", point, new boolean[] {true, true}, point, new boolean[] {true, true}));
+    } catch (IllegalArgumentException iae) {
+      assertEquals("field=\"value\" was indexed with numDims=1 but this query has numDims=2", iae.getMessage());
+    }
+
+    IOUtils.close(r, w, dir);
+  }
+
+  public void testWrongNumBytes() throws Exception {
+    Directory dir = getDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig();
+    iwc.setCodec(getCodec());
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+    Document doc = new Document();
+    doc.add(new LongPoint("value", Long.MIN_VALUE));
+    w.addDocument(doc);
+
+    IndexReader r = w.getReader();
+
+    // no wrapping, else the exc might happen in executor thread:
+    IndexSearcher s = new IndexSearcher(r);
+    byte[][] point = new byte[1][];
+    point[0] = new byte[10];
+    try {
+      s.count(new PointRangeQuery("value", point, new boolean[] {true}, point, new boolean[] {true}));
+    } catch (IllegalArgumentException iae) {
+      assertEquals("field=\"value\" was indexed with bytesPerDim=8 but this query has bytesPerDim=10", iae.getMessage());
+    }
+
+    IOUtils.close(r, w, dir);
+  }
+
+  public void testAllPointDocsWereDeletedAndThenMergedAgain() throws Exception {
+    Directory dir = getDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig();
+    iwc.setCodec(getCodec());
+    IndexWriter w = new IndexWriter(dir, iwc);
+    Document doc = new Document();
+    doc.add(new StringField("id", "0", Field.Store.NO));
+    doc.add(new LongPoint("value", 0L));
+    w.addDocument(doc);
+
+    // Add document that won't be deleted to avoid IW dropping
+    // segment below since it's 100% deleted:
+    w.addDocument(new Document());
+    w.commit();
+
+    // Need another segment so we invoke BKDWriter.merge
+    doc = new Document();
+    doc.add(new StringField("id", "0", Field.Store.NO));
+    doc.add(new LongPoint("value", 0L));
+    w.addDocument(doc);
+    w.addDocument(new Document());
+
+    w.deleteDocuments(new Term("id", "0"));
+    w.forceMerge(1);
+
+    doc = new Document();
+    doc.add(new StringField("id", "0", Field.Store.NO));
+    doc.add(new LongPoint("value", 0L));
+    w.addDocument(doc);
+    w.addDocument(new Document());
+
+    w.deleteDocuments(new Term("id", "0"));
+    w.forceMerge(1);
+
+    IOUtils.close(w, dir);
+  }
+
+  private static Directory noVirusChecker(Directory dir) {
+    if (dir instanceof MockDirectoryWrapper) {
+      ((MockDirectoryWrapper) dir).setEnableVirusScanner(false);
+    }
+    return dir;
+  }
+
+  private static Directory getDirectory() {     
+    return noVirusChecker(newDirectory());
+  }
+
+  private static Codec getCodec() {
+    if (Codec.getDefault().getName().equals("Lucene60")) {
+      int maxPointsInLeafNode = TestUtil.nextInt(random(), 16, 2048);
+      double maxMBSortInHeap = 3.0 + (3*random().nextDouble());
+      if (VERBOSE) {
+        System.out.println("TEST: using Lucene60PointFormat with maxPointsInLeafNode=" + maxPointsInLeafNode + " and maxMBSortInHeap=" + maxMBSortInHeap);
+      }
+
+      return new FilterCodec("Lucene60", Codec.getDefault()) {
+        @Override
+        public PointFormat pointFormat() {
+          return new PointFormat() {
+            @Override
+            public PointWriter fieldsWriter(SegmentWriteState writeState) throws IOException {
+              return new Lucene60PointWriter(writeState, maxPointsInLeafNode, maxMBSortInHeap);
+            }
+
+            @Override
+            public PointReader fieldsReader(SegmentReadState readState) throws IOException {
+              return new Lucene60PointReader(readState);
+            }
+          };
+        }
+      };
+    } else {
+      return Codec.getDefault();
+    }
+  }
+
+  public void testExactPointQuery() throws Exception {
+    Directory dir = getDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig();
+    iwc.setCodec(getCodec());
+    IndexWriter w = new IndexWriter(dir, iwc);
+
+    Document doc = new Document();
+    doc.add(new LongPoint("long", 5L));
+    w.addDocument(doc);
+
+    doc = new Document();
+    doc.add(new IntPoint("int", 42));
+    w.addDocument(doc);
+
+    doc = new Document();
+    doc.add(new FloatPoint("float", 2.0f));
+    w.addDocument(doc);
+
+    doc = new Document();
+    doc.add(new DoublePoint("double", 1.0));
+    w.addDocument(doc);
+
+    IndexReader r = DirectoryReader.open(w);
+    IndexSearcher s = newSearcher(r);
+    assertEquals(1, s.count(ExactPointQuery.new1DIntExact("int", 42)));
+    assertEquals(0, s.count(ExactPointQuery.new1DIntExact("int", 41)));
+
+    assertEquals(1, s.count(ExactPointQuery.new1DLongExact("long", 5L)));
+    assertEquals(0, s.count(ExactPointQuery.new1DLongExact("long", -1L)));
+
+    assertEquals(1, s.count(ExactPointQuery.new1DFloatExact("float", 2.0f)));
+    assertEquals(0, s.count(ExactPointQuery.new1DFloatExact("float", 1.0f)));
+
+    assertEquals(1, s.count(ExactPointQuery.new1DDoubleExact("double", 1.0)));
+    assertEquals(0, s.count(ExactPointQuery.new1DDoubleExact("double", 2.0)));
+    w.close();
+    r.close();
+    dir.close();
+  }
+}
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestUsageTrackingFilterCachingPolicy.java b/lucene/core/src/test/org/apache/lucene/search/TestUsageTrackingFilterCachingPolicy.java
index d41f57d..5da982e 100644
--- a/lucene/core/src/test/org/apache/lucene/search/TestUsageTrackingFilterCachingPolicy.java
+++ b/lucene/core/src/test/org/apache/lucene/search/TestUsageTrackingFilterCachingPolicy.java
@@ -26,7 +26,7 @@ public class TestUsageTrackingFilterCachingPolicy extends LuceneTestCase {
 
   public void testCostlyFilter() {
     assertTrue(UsageTrackingQueryCachingPolicy.isCostly(new PrefixQuery(new Term("field", "prefix"))));
-    assertTrue(UsageTrackingQueryCachingPolicy.isCostly(DimensionalRangeQuery.new1DIntRange("intField", 1, true, 1000, true)));
+    assertTrue(UsageTrackingQueryCachingPolicy.isCostly(PointRangeQuery.new1DIntRange("intField", 1, true, 1000, true)));
     assertFalse(UsageTrackingQueryCachingPolicy.isCostly(new TermQuery(new Term("field", "value"))));
   }
 
diff --git a/lucene/core/src/test/org/apache/lucene/util/bkd/TestBKD.java b/lucene/core/src/test/org/apache/lucene/util/bkd/TestBKD.java
index 7027f10..3448150 100644
--- a/lucene/core/src/test/org/apache/lucene/util/bkd/TestBKD.java
+++ b/lucene/core/src/test/org/apache/lucene/util/bkd/TestBKD.java
@@ -24,8 +24,8 @@ import java.util.Arrays;
 import java.util.BitSet;
 import java.util.List;
 
-import org.apache.lucene.index.DimensionalValues.IntersectVisitor;
-import org.apache.lucene.index.DimensionalValues.Relation;
+import org.apache.lucene.index.PointValues.IntersectVisitor;
+import org.apache.lucene.index.PointValues.Relation;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexInput;
diff --git a/lucene/demo/src/java/org/apache/lucene/demo/IndexFiles.java b/lucene/demo/src/java/org/apache/lucene/demo/IndexFiles.java
index 69ad8de..b8d8a9e 100644
--- a/lucene/demo/src/java/org/apache/lucene/demo/IndexFiles.java
+++ b/lucene/demo/src/java/org/apache/lucene/demo/IndexFiles.java
@@ -32,7 +32,7 @@ import java.util.Date;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;
-import org.apache.lucene.document.DimensionalLongField;
+import org.apache.lucene.document.LongPoint;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.StringField;
@@ -178,13 +178,13 @@ public class IndexFiles {
       doc.add(pathField);
       
       // Add the last modified date of the file a field named "modified".
-      // Use a DimensionalLongField that is indexed (i.e. efficiently filterable with
-      // DimensionalRangeQuery).  This indexes to milli-second resolution, which
+      // Use a LongPoint that is indexed (i.e. efficiently filterable with
+      // PointRangeQuery).  This indexes to milli-second resolution, which
       // is often too fine.  You could instead create a number based on
       // year/month/day/hour/minutes/seconds, down the resolution you require.
       // For example the long value 2011021714 would mean
       // February 17, 2011, 2-3 PM.
-      doc.add(new DimensionalLongField("modified", lastModified));
+      doc.add(new LongPoint("modified", lastModified));
       
       // Add the contents of the file to a field named "contents".  Specify a Reader,
       // so that the text of the file is tokenized and indexed, but not stored.
diff --git a/lucene/demo/src/java/org/apache/lucene/demo/facet/DistanceFacetsExample.java b/lucene/demo/src/java/org/apache/lucene/demo/facet/DistanceFacetsExample.java
index f61476a..c183074 100644
--- a/lucene/demo/src/java/org/apache/lucene/demo/facet/DistanceFacetsExample.java
+++ b/lucene/demo/src/java/org/apache/lucene/demo/facet/DistanceFacetsExample.java
@@ -18,7 +18,7 @@ package org.apache.lucene.demo.facet;
  */
 
 import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
-import org.apache.lucene.document.DimensionalDoubleField;
+import org.apache.lucene.document.DoublePoint;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.expressions.Expression;
@@ -40,7 +40,7 @@ import org.apache.lucene.index.IndexWriterConfig.OpenMode;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.BooleanClause;
 import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.DimensionalRangeQuery;
+import org.apache.lucene.search.PointRangeQuery;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.MatchAllDocsQuery;
 import org.apache.lucene.search.Query;
@@ -91,25 +91,25 @@ public class DistanceFacetsExample implements Closeable {
     // TODO: we could index in radians instead ... saves all the conversions in getBoundingBoxFilter
 
     // Add documents with latitude/longitude location:
-    // we index these both as DimensionalDoubleFields (for bounding box/ranges) and as NumericDocValuesFields (for scoring)
+    // we index these both as DoublePoints (for bounding box/ranges) and as NumericDocValuesFields (for scoring)
     Document doc = new Document();
-    doc.add(new DimensionalDoubleField("latitude", 40.759011));
+    doc.add(new DoublePoint("latitude", 40.759011));
     doc.add(new NumericDocValuesField("latitude", Double.doubleToRawLongBits(40.759011)));
-    doc.add(new DimensionalDoubleField("longitude", -73.9844722));
+    doc.add(new DoublePoint("longitude", -73.9844722));
     doc.add(new NumericDocValuesField("longitude", Double.doubleToRawLongBits(-73.9844722)));
     writer.addDocument(doc);
     
     doc = new Document();
-    doc.add(new DimensionalDoubleField("latitude", 40.718266));
+    doc.add(new DoublePoint("latitude", 40.718266));
     doc.add(new NumericDocValuesField("latitude", Double.doubleToRawLongBits(40.718266)));
-    doc.add(new DimensionalDoubleField("longitude", -74.007819));
+    doc.add(new DoublePoint("longitude", -74.007819));
     doc.add(new NumericDocValuesField("longitude", Double.doubleToRawLongBits(-74.007819)));
     writer.addDocument(doc);
     
     doc = new Document();
-    doc.add(new DimensionalDoubleField("latitude", 40.7051157));
+    doc.add(new DoublePoint("latitude", 40.7051157));
     doc.add(new NumericDocValuesField("latitude", Double.doubleToRawLongBits(40.7051157)));
-    doc.add(new DimensionalDoubleField("longitude", -74.0088305));
+    doc.add(new DoublePoint("longitude", -74.0088305));
     doc.add(new NumericDocValuesField("longitude", Double.doubleToRawLongBits(-74.0088305)));
     writer.addDocument(doc);
 
@@ -181,7 +181,7 @@ public class DistanceFacetsExample implements Closeable {
     BooleanQuery.Builder f = new BooleanQuery.Builder();
 
     // Add latitude range filter:
-    f.add(DimensionalRangeQuery.new1DDoubleRange("latitude", Math.toDegrees(minLat), true, Math.toDegrees(maxLat), true),
+    f.add(PointRangeQuery.new1DDoubleRange("latitude", Math.toDegrees(minLat), true, Math.toDegrees(maxLat), true),
           BooleanClause.Occur.FILTER);
 
     // Add longitude range filter:
@@ -189,13 +189,13 @@ public class DistanceFacetsExample implements Closeable {
       // The bounding box crosses the international date
       // line:
       BooleanQuery.Builder lonF = new BooleanQuery.Builder();
-      lonF.add(DimensionalRangeQuery.new1DDoubleRange("longitude", Math.toDegrees(minLng), true, null,  true),
+      lonF.add(PointRangeQuery.new1DDoubleRange("longitude", Math.toDegrees(minLng), true, null, true),
                BooleanClause.Occur.SHOULD);
-      lonF.add(DimensionalRangeQuery.new1DDoubleRange("longitude", null, true, Math.toDegrees(maxLng), true),
+      lonF.add(PointRangeQuery.new1DDoubleRange("longitude", null, true, Math.toDegrees(maxLng), true),
                BooleanClause.Occur.SHOULD);
       f.add(lonF.build(), BooleanClause.Occur.MUST);
     } else {
-      f.add(DimensionalRangeQuery.new1DDoubleRange("longitude", Math.toDegrees(minLng), true, Math.toDegrees(maxLng), true),
+      f.add(PointRangeQuery.new1DDoubleRange("longitude", Math.toDegrees(minLng), true, Math.toDegrees(maxLng), true),
             BooleanClause.Occur.FILTER);
     }
 
diff --git a/lucene/demo/src/java/org/apache/lucene/demo/facet/RangeFacetsExample.java b/lucene/demo/src/java/org/apache/lucene/demo/facet/RangeFacetsExample.java
index 2f3f889..6c6b3f4 100644
--- a/lucene/demo/src/java/org/apache/lucene/demo/facet/RangeFacetsExample.java
+++ b/lucene/demo/src/java/org/apache/lucene/demo/facet/RangeFacetsExample.java
@@ -18,7 +18,7 @@ package org.apache.lucene.demo.facet;
  */
 
 import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
-import org.apache.lucene.document.DimensionalLongField;
+import org.apache.lucene.document.LongPoint;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.facet.DrillDownQuery;
@@ -32,7 +32,7 @@ import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import org.apache.lucene.search.DimensionalRangeQuery;
+import org.apache.lucene.search.PointRangeQuery;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.MatchAllDocsQuery;
 import org.apache.lucene.search.TopDocs;
@@ -69,7 +69,7 @@ public class RangeFacetsExample implements Closeable {
       // Add as doc values field, so we can compute range facets:
       doc.add(new NumericDocValuesField("timestamp", then));
       // Add as numeric field so we can drill-down:
-      doc.add(new DimensionalLongField("timestamp", then));
+      doc.add(new LongPoint("timestamp", then));
       indexWriter.addDocument(doc);
     }
 
@@ -107,7 +107,7 @@ public class RangeFacetsExample implements Closeable {
     // documents ("browse only"):
     DrillDownQuery q = new DrillDownQuery(getConfig());
 
-    q.add("timestamp", DimensionalRangeQuery.new1DLongRange("timestamp", range.min, range.minInclusive, range.max, range.maxInclusive));
+    q.add("timestamp", PointRangeQuery.new1DLongRange("timestamp", range.min, range.minInclusive, range.max, range.maxInclusive));
 
     return searcher.search(q, 10);
   }
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/range/Range.java b/lucene/facet/src/java/org/apache/lucene/facet/range/Range.java
index 7e6b746..3abfdde 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/range/Range.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/range/Range.java
@@ -19,7 +19,6 @@ package org.apache.lucene.facet.range;
 
 import org.apache.lucene.facet.DrillDownQuery; // javadocs
 import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.search.DimensionalRangeQuery; // javadocs
 import org.apache.lucene.search.Query;
 
 /** Base class for a single labeled range.
@@ -46,7 +45,7 @@ public abstract class Range {
    *  or when intersected with another query that can lead the
    *  iteration.  If the {@link ValueSource} is static, e.g. an
    *  indexed numeric field, then it may be more efficient to use
-   *  {@link DimensionalRangeQuery}. The provided fastMatchQuery,
+   *  {@link org.apache.lucene.search.PointRangeQuery}. The provided fastMatchQuery,
    *  if non-null, will first be consulted, and only if
    *  that is set for each document will the range then be
    *  checked. */
@@ -60,7 +59,7 @@ public abstract class Range {
    *  or when intersected with another query that can lead the
    *  iteration.  If the {@link ValueSource} is static, e.g. an
    *  indexed numeric field, then it may be more efficient to
-   *  use {@link DimensionalRangeQuery}. */
+   *  use {@link org.apache.lucene.search.PointRangeQuery}. */
   public Query getQuery(ValueSource valueSource) {
     return getQuery(null, valueSource);
   }
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/range/TestRangeFacetCounts.java b/lucene/facet/src/test/org/apache/lucene/facet/range/TestRangeFacetCounts.java
index 69f613c..f41840e 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/range/TestRangeFacetCounts.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/range/TestRangeFacetCounts.java
@@ -23,9 +23,9 @@ import java.util.Map;
 import java.util.Set;
 import java.util.concurrent.atomic.AtomicBoolean;
 
-import org.apache.lucene.document.DimensionalDoubleField;
-import org.apache.lucene.document.DimensionalFloatField;
-import org.apache.lucene.document.DimensionalLongField;
+import org.apache.lucene.document.DoublePoint;
+import org.apache.lucene.document.FloatPoint;
+import org.apache.lucene.document.LongPoint;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.DoubleDocValuesField;
 import org.apache.lucene.document.FloatDocValuesField;
@@ -55,7 +55,7 @@ import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
 import org.apache.lucene.queries.function.valuesource.DoubleFieldSource;
 import org.apache.lucene.queries.function.valuesource.FloatFieldSource;
 import org.apache.lucene.queries.function.valuesource.LongFieldSource;
-import org.apache.lucene.search.DimensionalRangeQuery;
+import org.apache.lucene.search.PointRangeQuery;
 import org.apache.lucene.search.Explanation;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.MatchAllDocsQuery;
@@ -219,7 +219,7 @@ public class TestRangeFacetCounts extends FacetTestCase {
       // For computing range facet counts:
       doc.add(new NumericDocValuesField("field", l));
       // For drill down by numeric range:
-      doc.add(new DimensionalLongField("field", l));
+      doc.add(new LongPoint("field", l));
 
       if ((l&3) == 0) {
         doc.add(new FacetField("dim", "a"));
@@ -295,7 +295,7 @@ public class TestRangeFacetCounts extends FacetTestCase {
 
     // Third search, drill down on "less than or equal to 10":
     ddq = new DrillDownQuery(config);
-    ddq.add("field", DimensionalRangeQuery.new1DLongRange("field", 0L, true, 10L, true));
+    ddq.add("field", PointRangeQuery.new1DLongRange("field", 0L, true, 10L, true));
     dsr = ds.search(null, ddq, 10);
 
     assertEquals(11, dsr.hits.totalHits);
@@ -383,7 +383,7 @@ public class TestRangeFacetCounts extends FacetTestCase {
       long v = random().nextLong();
       values[i] = v;
       doc.add(new NumericDocValuesField("field", v));
-      doc.add(new DimensionalLongField("field", v));
+      doc.add(new LongPoint("field", v));
       w.addDocument(doc);
       minValue = Math.min(minValue, v);
       maxValue = Math.max(maxValue, v);
@@ -475,9 +475,9 @@ public class TestRangeFacetCounts extends FacetTestCase {
       Query fastMatchQuery;
       if (random().nextBoolean()) {
         if (random().nextBoolean()) {
-          fastMatchQuery = DimensionalRangeQuery.new1DLongRange("field", minValue, true, maxValue, true);
+          fastMatchQuery = PointRangeQuery.new1DLongRange("field", minValue, true, maxValue, true);
         } else {
-          fastMatchQuery = DimensionalRangeQuery.new1DLongRange("field", minAcceptedValue, true, maxAcceptedValue, true);
+          fastMatchQuery = PointRangeQuery.new1DLongRange("field", minAcceptedValue, true, maxAcceptedValue, true);
         }
       } else {
         fastMatchQuery = null;
@@ -499,7 +499,7 @@ public class TestRangeFacetCounts extends FacetTestCase {
         // Test drill-down:
         DrillDownQuery ddq = new DrillDownQuery(config);
         if (random().nextBoolean()) {
-          ddq.add("field", DimensionalRangeQuery.new1DLongRange("field", range.min, range.minInclusive, range.max, range.maxInclusive));
+          ddq.add("field", PointRangeQuery.new1DLongRange("field", range.min, range.minInclusive, range.max, range.maxInclusive));
         } else {
           ddq.add("field", range.getQuery(fastMatchQuery, vs));
         }
@@ -524,7 +524,7 @@ public class TestRangeFacetCounts extends FacetTestCase {
       float v = random().nextFloat();
       values[i] = v;
       doc.add(new FloatDocValuesField("field", v));
-      doc.add(new DimensionalFloatField("field", v));
+      doc.add(new FloatPoint("field", v));
       w.addDocument(doc);
       minValue = Math.min(minValue, v);
       maxValue = Math.max(maxValue, v);
@@ -630,9 +630,9 @@ public class TestRangeFacetCounts extends FacetTestCase {
       Query fastMatchQuery;
       if (random().nextBoolean()) {
         if (random().nextBoolean()) {
-          fastMatchQuery = DimensionalRangeQuery.new1DFloatRange("field", minValue, true, maxValue, true);
+          fastMatchQuery = PointRangeQuery.new1DFloatRange("field", minValue, true, maxValue, true);
         } else {
-          fastMatchQuery = DimensionalRangeQuery.new1DFloatRange("field", minAcceptedValue, true, maxAcceptedValue, true);
+          fastMatchQuery = PointRangeQuery.new1DFloatRange("field", minAcceptedValue, true, maxAcceptedValue, true);
         }
       } else {
         fastMatchQuery = null;
@@ -654,7 +654,7 @@ public class TestRangeFacetCounts extends FacetTestCase {
         // Test drill-down:
         DrillDownQuery ddq = new DrillDownQuery(config);
         if (random().nextBoolean()) {
-          ddq.add("field", DimensionalRangeQuery.new1DFloatRange("field", (float) range.min, range.minInclusive, (float) range.max, range.maxInclusive));
+          ddq.add("field", PointRangeQuery.new1DFloatRange("field", (float) range.min, range.minInclusive, (float) range.max, range.maxInclusive));
         } else {
           ddq.add("field", range.getQuery(fastMatchQuery, vs));
         }
@@ -679,7 +679,7 @@ public class TestRangeFacetCounts extends FacetTestCase {
       double v = random().nextDouble();
       values[i] = v;
       doc.add(new DoubleDocValuesField("field", v));
-      doc.add(new DimensionalDoubleField("field", v));
+      doc.add(new DoublePoint("field", v));
       w.addDocument(doc);
       minValue = Math.min(minValue, v);
       maxValue = Math.max(maxValue, v);
@@ -769,9 +769,9 @@ public class TestRangeFacetCounts extends FacetTestCase {
       Query fastMatchFilter;
       if (random().nextBoolean()) {
         if (random().nextBoolean()) {
-          fastMatchFilter = DimensionalRangeQuery.new1DDoubleRange("field", minValue, true, maxValue, true);
+          fastMatchFilter = PointRangeQuery.new1DDoubleRange("field", minValue, true, maxValue, true);
         } else {
-          fastMatchFilter = DimensionalRangeQuery.new1DDoubleRange("field", minAcceptedValue, true, maxAcceptedValue, true);
+          fastMatchFilter = PointRangeQuery.new1DDoubleRange("field", minAcceptedValue, true, maxAcceptedValue, true);
         }
       } else {
         fastMatchFilter = null;
@@ -793,7 +793,7 @@ public class TestRangeFacetCounts extends FacetTestCase {
         // Test drill-down:
         DrillDownQuery ddq = new DrillDownQuery(config);
         if (random().nextBoolean()) {
-          ddq.add("field", DimensionalRangeQuery.new1DDoubleRange("field", range.min, range.minInclusive, range.max, range.maxInclusive));
+          ddq.add("field", PointRangeQuery.new1DDoubleRange("field", range.min, range.minInclusive, range.max, range.maxInclusive));
         } else {
           ddq.add("field", range.getQuery(fastMatchFilter, vs));
         }
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/TermVectorLeafReader.java b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/TermVectorLeafReader.java
index 4e5d28e..3a63702 100644
--- a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/TermVectorLeafReader.java
+++ b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/TermVectorLeafReader.java
@@ -22,7 +22,7 @@ import java.util.Collections;
 import java.util.Iterator;
 
 import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.index.DimensionalValues;
+import org.apache.lucene.index.PointValues;
 import org.apache.lucene.index.DocValuesType;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
@@ -149,7 +149,7 @@ public class TermVectorLeafReader extends LeafReader {
   }
 
   @Override
-  public DimensionalValues getDimensionalValues() {
+  public PointValues getPointValues() {
     return null;
   }
 
diff --git a/lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest.java b/lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest.java
index f17b4a2..bbc5c22 100644
--- a/lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest.java
+++ b/lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest.java
@@ -45,7 +45,7 @@ import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.document.DimensionalIntField;
+import org.apache.lucene.document.IntPoint;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
@@ -63,7 +63,7 @@ import org.apache.lucene.queries.payloads.SpanPayloadCheckQuery;
 import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.DimensionalRangeQuery;
+import org.apache.lucene.search.PointRangeQuery;
 import org.apache.lucene.search.FuzzyQuery;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.MultiPhraseQuery;
@@ -585,7 +585,7 @@ public class HighlighterTest extends BaseTokenStreamTestCase implements Formatte
   
   public void testDimensionalRangeQuery() throws Exception {
     // doesn't currently highlight, but make sure it doesn't cause exception either
-    query = DimensionalRangeQuery.new1DIntRange(NUMERIC_FIELD_NAME, 2, true, 6, true);
+    query = PointRangeQuery.new1DIntRange(NUMERIC_FIELD_NAME, 2, true, 6, true);
     searcher = newSearcher(reader);
     hits = searcher.search(query, 100);
     int maxNumFragmentsRequired = 2;
@@ -2076,22 +2076,22 @@ public class HighlighterTest extends BaseTokenStreamTestCase implements Formatte
 
     // a few tests need other docs...:
     Document doc = new Document();
-    doc.add(new DimensionalIntField(NUMERIC_FIELD_NAME, 1));
+    doc.add(new IntPoint(NUMERIC_FIELD_NAME, 1));
     doc.add(new StoredField(NUMERIC_FIELD_NAME, 1));
     writer.addDocument(doc);
 
     doc = new Document();
-    doc.add(new DimensionalIntField(NUMERIC_FIELD_NAME, 3));
+    doc.add(new IntPoint(NUMERIC_FIELD_NAME, 3));
     doc.add(new StoredField(NUMERIC_FIELD_NAME, 3));
     writer.addDocument(doc);
 
     doc = new Document();
-    doc.add(new DimensionalIntField(NUMERIC_FIELD_NAME, 5));
+    doc.add(new IntPoint(NUMERIC_FIELD_NAME, 5));
     doc.add(new StoredField(NUMERIC_FIELD_NAME, 5));
     writer.addDocument(doc);
 
     doc = new Document();
-    doc.add(new DimensionalIntField(NUMERIC_FIELD_NAME, 7));
+    doc.add(new IntPoint(NUMERIC_FIELD_NAME, 7));
     doc.add(new StoredField(NUMERIC_FIELD_NAME, 7));
     writer.addDocument(doc);
 
diff --git a/lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java b/lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
index 97f5f0c..dec42d5 100644
--- a/lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
+++ b/lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
@@ -34,7 +34,7 @@ import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;
 import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.index.DimensionalValues;
+import org.apache.lucene.index.PointValues;
 import org.apache.lucene.index.DocValuesType;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
@@ -814,7 +814,7 @@ public class MemoryIndex {
     }
 
     @Override
-    public DimensionalValues getDimensionalValues() {
+    public PointValues getPointValues() {
       return null;
     }
 
diff --git a/lucene/misc/src/java/org/apache/lucene/index/MergeReaderWrapper.java b/lucene/misc/src/java/org/apache/lucene/index/MergeReaderWrapper.java
index 5b8636b..3059650 100644
--- a/lucene/misc/src/java/org/apache/lucene/index/MergeReaderWrapper.java
+++ b/lucene/misc/src/java/org/apache/lucene/index/MergeReaderWrapper.java
@@ -211,8 +211,8 @@ class MergeReaderWrapper extends LeafReader {
   }
 
   @Override
-  public DimensionalValues getDimensionalValues() {
-    return in.getDimensionalValues();
+  public PointValues getPointValues() {
+    return in.getPointValues();
   }
 
   @Override
diff --git a/lucene/misc/src/java/org/apache/lucene/index/SortingLeafReader.java b/lucene/misc/src/java/org/apache/lucene/index/SortingLeafReader.java
index dad2cd2..876fa81 100644
--- a/lucene/misc/src/java/org/apache/lucene/index/SortingLeafReader.java
+++ b/lucene/misc/src/java/org/apache/lucene/index/SortingLeafReader.java
@@ -254,12 +254,12 @@ public class SortingLeafReader extends FilterLeafReader {
     }
   }
 
-  private static class SortingDimensionalValues extends DimensionalValues {
+  private static class SortingPointValues extends PointValues {
 
-    private final DimensionalValues in;
+    private final PointValues in;
     private final Sorter.DocMap docMap;
 
-    public SortingDimensionalValues(final DimensionalValues in, Sorter.DocMap docMap) {
+    public SortingPointValues(final PointValues in, Sorter.DocMap docMap) {
       this.in = in;
       this.docMap = docMap;
     }
@@ -851,13 +851,13 @@ public class SortingLeafReader extends FilterLeafReader {
   }
 
   @Override
-  public DimensionalValues getDimensionalValues() {
-    final DimensionalValues inDimensionalValues = in.getDimensionalValues();
-    if (inDimensionalValues == null) {
+  public PointValues getPointValues() {
+    final PointValues inPointValues = in.getPointValues();
+    if (inPointValues == null) {
       return null;
     } else {
       // TODO: this is untested!
-      return new SortingDimensionalValues(inDimensionalValues, docMap);
+      return new SortingPointValues(inPointValues, docMap);
     }
   }
 
diff --git a/lucene/misc/src/java/org/apache/lucene/uninverting/UninvertingReader.java b/lucene/misc/src/java/org/apache/lucene/uninverting/UninvertingReader.java
index 6b6a31b..b269a02 100644
--- a/lucene/misc/src/java/org/apache/lucene/uninverting/UninvertingReader.java
+++ b/lucene/misc/src/java/org/apache/lucene/uninverting/UninvertingReader.java
@@ -19,7 +19,6 @@ package org.apache.lucene.uninverting;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.Map;
 
 import org.apache.lucene.document.BinaryDocValuesField; // javadocs
@@ -213,7 +212,7 @@ public class UninvertingReader extends FilterLeafReader {
       }
       filteredInfos.add(new FieldInfo(fi.name, fi.number, fi.hasVectors(), fi.omitsNorms(),
           fi.hasPayloads(), fi.getIndexOptions(), type, fi.getDocValuesGen(), fi.attributes(),
-          fi.getDimensionCount(), fi.getDimensionNumBytes()));
+          fi.getPointDimensionCount(), fi.getPointNumBytes()));
     }
     fieldInfos = new FieldInfos(filteredInfos.toArray(new FieldInfo[filteredInfos.size()]));
   }
diff --git a/lucene/misc/src/test/org/apache/lucene/index/SorterTestBase.java b/lucene/misc/src/test/org/apache/lucene/index/SorterTestBase.java
index 76f54bf..3c9ff5e 100644
--- a/lucene/misc/src/test/org/apache/lucene/index/SorterTestBase.java
+++ b/lucene/misc/src/test/org/apache/lucene/index/SorterTestBase.java
@@ -30,7 +30,7 @@ import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
 import org.apache.lucene.document.BinaryDocValuesField;
-import org.apache.lucene.document.DimensionalBinaryField;
+import org.apache.lucene.document.BinaryPoint;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field.Store;
 import org.apache.lucene.document.Field;
@@ -41,8 +41,8 @@ import org.apache.lucene.document.SortedNumericDocValuesField;
 import org.apache.lucene.document.SortedSetDocValuesField;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.DimensionalValues.IntersectVisitor;
-import org.apache.lucene.index.DimensionalValues.Relation;
+import org.apache.lucene.index.PointValues.IntersectVisitor;
+import org.apache.lucene.index.PointValues.Relation;
 import org.apache.lucene.index.SortingLeafReader.SortingDocsEnum;
 import org.apache.lucene.index.TermsEnum.SeekStatus;
 import org.apache.lucene.search.CollectionStatistics;
@@ -175,7 +175,7 @@ public abstract class SorterTestBase extends LuceneTestCase {
     doc.add(new Field(TERM_VECTORS_FIELD, Integer.toString(id), TERM_VECTORS_TYPE));
     byte[] bytes = new byte[4];
     NumericUtils.intToBytes(id, bytes, 0);
-    doc.add(new DimensionalBinaryField(DIMENSIONAL_FIELD, bytes));
+    doc.add(new BinaryPoint(DIMENSIONAL_FIELD, bytes));
     return doc;
   }
 
@@ -379,8 +379,8 @@ public abstract class SorterTestBase extends LuceneTestCase {
     }
   }
 
-  public void testDimensionalValues() throws Exception {
-    DimensionalValues values = sortedReader.getDimensionalValues();
+  public void testPoints() throws Exception {
+    PointValues values = sortedReader.getPointValues();
     values.intersect(DIMENSIONAL_FIELD,
                      new IntersectVisitor() {
                        @Override
diff --git a/lucene/misc/src/test/org/apache/lucene/uninverting/TestUninvertingReader.java b/lucene/misc/src/test/org/apache/lucene/uninverting/TestUninvertingReader.java
index f99107c..afcd339 100644
--- a/lucene/misc/src/test/org/apache/lucene/uninverting/TestUninvertingReader.java
+++ b/lucene/misc/src/test/org/apache/lucene/uninverting/TestUninvertingReader.java
@@ -26,7 +26,7 @@ import java.util.Map;
 import java.util.Set;
 import java.util.Collections;
 
-import org.apache.lucene.document.DimensionalIntField;
+import org.apache.lucene.document.IntPoint;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
@@ -356,7 +356,7 @@ public class TestUninvertingReader extends LuceneTestCase {
     doc.add(new StringField("id", idBytes, Store.YES));
     doc.add(new LegacyIntField("int", 5, Store.YES));
     doc.add(new NumericDocValuesField("dv", 5));
-    doc.add(new DimensionalIntField("dint", 5));
+    doc.add(new IntPoint("dint", 5));
     doc.add(new StoredField("stored", 5)); // not indexed
     iw.addDocument(doc);
 
@@ -373,12 +373,12 @@ public class TestUninvertingReader extends LuceneTestCase {
 
     FieldInfo intFInfo = leafReader.getFieldInfos().fieldInfo("int");
     assertEquals(DocValuesType.NUMERIC, intFInfo.getDocValuesType());
-    assertEquals(0, intFInfo.getDimensionCount());
-    assertEquals(0, intFInfo.getDimensionNumBytes());
+    assertEquals(0, intFInfo.getPointDimensionCount());
+    assertEquals(0, intFInfo.getPointNumBytes());
 
     FieldInfo dintFInfo = leafReader.getFieldInfos().fieldInfo("dint");
-    assertEquals(1, dintFInfo.getDimensionCount());
-    assertEquals(4, dintFInfo.getDimensionNumBytes());
+    assertEquals(1, dintFInfo.getPointDimensionCount());
+    assertEquals(4, dintFInfo.getPointNumBytes());
 
     FieldInfo dvFInfo = leafReader.getFieldInfos().fieldInfo("dv");
     assertEquals(DocValuesType.NUMERIC, dvFInfo.getDocValuesType());
diff --git a/lucene/sandbox/src/java/org/apache/lucene/document/DimensionalLatLonField.java b/lucene/sandbox/src/java/org/apache/lucene/document/DimensionalLatLonField.java
deleted file mode 100644
index a1e3199..0000000
--- a/lucene/sandbox/src/java/org/apache/lucene/document/DimensionalLatLonField.java
+++ /dev/null
@@ -1,88 +0,0 @@
-package org.apache.lucene.document;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.GeoUtils;
-import org.apache.lucene.util.NumericUtils;
-
-/** Add this to a document to index lat/lon point dimensionally */
-public final class DimensionalLatLonField extends Field {
-
-  public static final FieldType TYPE = new FieldType();
-  static {
-    TYPE.setDimensions(2, 4);
-    TYPE.freeze();
-  }
-
-  /** 
-   * Creates a new DimensionalLatLonField with the specified lat and lon
-   * @param name field name
-   * @param lat double latitude
-   * @param lon double longitude
-   * @throws IllegalArgumentException if the field name is null or lat or lon are out of bounds
-   */
-  public DimensionalLatLonField(String name, double lat, double lon) {
-    super(name, TYPE);
-    if (GeoUtils.isValidLat(lat) == false) {
-      throw new IllegalArgumentException("invalid lat (" + lat + "): must be -90 to 90");
-    }
-    if (GeoUtils.isValidLon(lon) == false) {
-      throw new IllegalArgumentException("invalid lon (" + lon + "): must be -180 to 180");
-    }
-    byte[] bytes = new byte[8];
-    NumericUtils.intToBytes(encodeLat(lat), bytes, 0);
-    NumericUtils.intToBytes(encodeLon(lon), bytes, 1);
-    fieldsData = new BytesRef(bytes);
-  }
-
-  public static final double TOLERANCE = 1E-7;
-
-  private static final int BITS = 32;
-
-  private static final double LON_SCALE = (0x1L<<BITS)/360.0D;
-  private static final double LAT_SCALE = (0x1L<<BITS)/180.0D;
-
-  /** Quantizes double (64 bit) latitude into 32 bits */
-  public static int encodeLat(double lat) {
-    assert GeoUtils.isValidLat(lat): "lat=" + lat;
-    long x = (long) (lat * LAT_SCALE);
-    assert x < Integer.MAX_VALUE: "lat=" + lat + " mapped to Integer.MAX_VALUE + " + (x - Integer.MAX_VALUE);
-    assert x > Integer.MIN_VALUE: "lat=" + lat + " mapped to Integer.MIN_VALUE";
-    return (int) x;
-  }
-
-  /** Quantizes double (64 bit) longitude into 32 bits */
-  public static int encodeLon(double lon) {
-    assert GeoUtils.isValidLon(lon): "lon=" + lon;
-    long x = (long) (lon * LON_SCALE);
-    assert x < Integer.MAX_VALUE;
-    assert x > Integer.MIN_VALUE;
-    return (int) x;
-  }
-
-  /** Turns quantized value from {@link #encodeLat} back into a double. */
-  public static double decodeLat(int x) {
-    return x / LAT_SCALE;
-  }
-
-  /** Turns quantized value from {@link #encodeLon} back into a double. */
-  public static double decodeLon(int x) {
-    return x / LON_SCALE;
-  }
-}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/document/LatLonPoint.java b/lucene/sandbox/src/java/org/apache/lucene/document/LatLonPoint.java
new file mode 100644
index 0000000..ac52b00
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/document/LatLonPoint.java
@@ -0,0 +1,87 @@
+package org.apache.lucene.document;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.GeoUtils;
+import org.apache.lucene.util.NumericUtils;
+
+/** Add this to a document to index lat/lon point dimensionally */
+public class LatLonPoint extends Field {
+  public static final FieldType TYPE = new FieldType();
+  static {
+    TYPE.setDimensions(2, 4);
+    TYPE.freeze();
+  }
+
+  /** 
+   * Creates a new LatLonPoint with the specified lat and lon
+   * @param name field name
+   * @param lat double latitude
+   * @param lon double longitude
+   * @throws IllegalArgumentException if the field name is null or lat or lon are out of bounds
+   */
+  public LatLonPoint(String name, double lat, double lon) {
+    super(name, TYPE);
+    if (GeoUtils.isValidLat(lat) == false) {
+      throw new IllegalArgumentException("invalid lat (" + lat + "): must be -90 to 90");
+    }
+    if (GeoUtils.isValidLon(lon) == false) {
+      throw new IllegalArgumentException("invalid lon (" + lon + "): must be -180 to 180");
+    }
+    byte[] bytes = new byte[8];
+    NumericUtils.intToBytes(encodeLat(lat), bytes, 0);
+    NumericUtils.intToBytes(encodeLon(lon), bytes, 1);
+    fieldsData = new BytesRef(bytes);
+  }
+
+  public static final double TOLERANCE = 1E-7;
+
+  private static final int BITS = 32;
+
+  private static final double LON_SCALE = (0x1L<<BITS)/360.0D;
+  private static final double LAT_SCALE = (0x1L<<BITS)/180.0D;
+
+  /** Quantizes double (64 bit) latitude into 32 bits */
+  public static int encodeLat(double lat) {
+    assert GeoUtils.isValidLat(lat): "lat=" + lat;
+    long x = (long) (lat * LAT_SCALE);
+    assert x < Integer.MAX_VALUE: "lat=" + lat + " mapped to Integer.MAX_VALUE + " + (x - Integer.MAX_VALUE);
+    assert x > Integer.MIN_VALUE: "lat=" + lat + " mapped to Integer.MIN_VALUE";
+    return (int) x;
+  }
+
+  /** Quantizes double (64 bit) longitude into 32 bits */
+  public static int encodeLon(double lon) {
+    assert GeoUtils.isValidLon(lon): "lon=" + lon;
+    long x = (long) (lon * LON_SCALE);
+    assert x < Integer.MAX_VALUE;
+    assert x > Integer.MIN_VALUE;
+    return (int) x;
+  }
+
+  /** Turns quantized value from {@link #encodeLat} back into a double. */
+  public static double decodeLat(int x) {
+    return x / LAT_SCALE;
+  }
+
+  /** Turns quantized value from {@link #encodeLon} back into a double. */
+  public static double decodeLon(int x) {
+    return x / LON_SCALE;
+  }
+}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/search/DimensionalPointInPolygonQuery.java b/lucene/sandbox/src/java/org/apache/lucene/search/DimensionalPointInPolygonQuery.java
deleted file mode 100644
index 20a1e94..0000000
--- a/lucene/sandbox/src/java/org/apache/lucene/search/DimensionalPointInPolygonQuery.java
+++ /dev/null
@@ -1,212 +0,0 @@
-package org.apache.lucene.search;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.document.DimensionalLatLonField;
-import org.apache.lucene.index.DimensionalValues.IntersectVisitor;
-import org.apache.lucene.index.DimensionalValues.Relation;
-import org.apache.lucene.index.DimensionalValues;
-import org.apache.lucene.index.LeafReader;
-import org.apache.lucene.index.LeafReaderContext;
-import org.apache.lucene.util.DocIdSetBuilder;
-import org.apache.lucene.util.GeoRelationUtils;
-import org.apache.lucene.util.GeoUtils;
-import org.apache.lucene.util.NumericUtils;
-
-/** Finds all previously indexed points that fall within the specified polygon.
- *
- *  <p>The field must be indexed with using {@link DimensionalLatLonField} added per document.
- *
- *  @lucene.experimental */
-
-public class DimensionalPointInPolygonQuery extends Query {
-  final String field;
-  final double minLat;
-  final double maxLat;
-  final double minLon;
-  final double maxLon;
-  final double[] polyLats;
-  final double[] polyLons;
-
-  /** The lats/lons must be clockwise or counter-clockwise. */
-  public DimensionalPointInPolygonQuery(String field, double[] polyLats, double[] polyLons) {
-    this.field = field;
-    if (polyLats.length != polyLons.length) {
-      throw new IllegalArgumentException("polyLats and polyLons must be equal length");
-    }
-    if (polyLats.length < 4) {
-      throw new IllegalArgumentException("at least 4 polygon points required");
-    }
-    if (polyLats[0] != polyLats[polyLats.length-1]) {
-      throw new IllegalArgumentException("first and last points of the polygon must be the same (it must close itself): polyLats[0]=" + polyLats[0] + " polyLats[" + (polyLats.length-1) + "]=" + polyLats[polyLats.length-1]);
-    }
-    if (polyLons[0] != polyLons[polyLons.length-1]) {
-      throw new IllegalArgumentException("first and last points of the polygon must be the same (it must close itself): polyLons[0]=" + polyLons[0] + " polyLons[" + (polyLons.length-1) + "]=" + polyLons[polyLons.length-1]);
-    }
-
-    this.polyLats = polyLats;
-    this.polyLons = polyLons;
-
-    // TODO: we could also compute the maximal innner bounding box, to make relations faster to compute?
-
-    double minLon = Double.POSITIVE_INFINITY;
-    double minLat = Double.POSITIVE_INFINITY;
-    double maxLon = Double.NEGATIVE_INFINITY;
-    double maxLat = Double.NEGATIVE_INFINITY;
-    for(int i=0;i<polyLats.length;i++) {
-      double lat = polyLats[i];
-      if (GeoUtils.isValidLat(lat) == false) {
-        throw new IllegalArgumentException("polyLats[" + i + "]=" + lat + " is not a valid latitude");
-      }
-      minLat = Math.min(minLat, lat);
-      maxLat = Math.max(maxLat, lat);
-      double lon = polyLons[i];
-      if (GeoUtils.isValidLon(lon) == false) {
-        throw new IllegalArgumentException("polyLons[" + i + "]=" + lat + " is not a valid longitude");
-      }
-      minLon = Math.min(minLon, lon);
-      maxLon = Math.max(maxLon, lon);
-    }
-    this.minLon = minLon;
-    this.maxLon = maxLon;
-    this.minLat = minLat;
-    this.maxLat = maxLat;
-  }
-
-  @Override
-  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {
-
-    // I don't use RandomAccessWeight here: it's no good to approximate with "match all docs"; this is an inverted structure and should be
-    // used in the first pass:
-
-    // TODO: except that the polygon verify is costly!  The approximation should be all docs in all overlapping cells, and matches() should
-    // then check the polygon
-
-    return new ConstantScoreWeight(this) {
-
-      @Override
-      public Scorer scorer(LeafReaderContext context) throws IOException {
-        LeafReader reader = context.reader();
-        DimensionalValues values = reader.getDimensionalValues();
-        if (values == null) {
-          // No docs in this segment had any dimensional fields
-          return null;
-        }
-
-        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc());
-        int[] hitCount = new int[1];
-        values.intersect(field,
-                         new IntersectVisitor() {
-                           @Override
-                           public void visit(int docID) {
-                             hitCount[0]++;
-                             result.add(docID);
-                           }
-
-                           @Override
-                           public void visit(int docID, byte[] packedValue) {
-                             assert packedValue.length == 8;
-                             double lat = DimensionalLatLonField.decodeLat(NumericUtils.bytesToInt(packedValue, 0));
-                             double lon = DimensionalLatLonField.decodeLon(NumericUtils.bytesToInt(packedValue, 1));
-                             if (GeoRelationUtils.pointInPolygon(polyLons, polyLats, lat, lon)) {
-                               hitCount[0]++;
-                               result.add(docID);
-                             }
-                           }
-
-                           @Override
-                           public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {
-                             double cellMinLat = DimensionalLatLonField.decodeLat(NumericUtils.bytesToInt(minPackedValue, 0));
-                             double cellMinLon = DimensionalLatLonField.decodeLon(NumericUtils.bytesToInt(minPackedValue, 1));
-                             double cellMaxLat = DimensionalLatLonField.decodeLat(NumericUtils.bytesToInt(maxPackedValue, 0));
-                             double cellMaxLon = DimensionalLatLonField.decodeLon(NumericUtils.bytesToInt(maxPackedValue, 1));
-
-                             if (cellMinLat <= minLat && cellMaxLat >= maxLat && cellMinLon <= minLon && cellMaxLon >= maxLon) {
-                               // Cell fully encloses the query
-                               return Relation.CELL_CROSSES_QUERY;
-                             } else  if (GeoRelationUtils.rectWithinPoly(cellMinLon, cellMinLat, cellMaxLon, cellMaxLat,
-                                                                 polyLons, polyLats,
-                                                                 minLon, minLat, maxLon, maxLat)) {
-                               return Relation.CELL_INSIDE_QUERY;
-                             } else if (GeoRelationUtils.rectCrossesPoly(cellMinLon, cellMinLat, cellMaxLon, cellMaxLat,
-                                                                 polyLons, polyLats,
-                                                                 minLon, minLat, maxLon, maxLat)) {
-                               return Relation.CELL_CROSSES_QUERY;
-                             } else {
-                               return Relation.CELL_OUTSIDE_QUERY;
-                             }
-                           }
-                         });
-
-        // NOTE: hitCount[0] will be over-estimate in multi-valued case
-        return new ConstantScoreScorer(this, score(), result.build(hitCount[0]).iterator());
-      }
-    };
-  }
-
-  @Override
-  @SuppressWarnings({"unchecked","rawtypes"})
-  public boolean equals(Object o) {
-    if (this == o) return true;
-    if (o == null || getClass() != o.getClass()) return false;
-    if (!super.equals(o)) return false;
-
-    DimensionalPointInPolygonQuery that = (DimensionalPointInPolygonQuery) o;
-
-    if (Arrays.equals(polyLons, that.polyLons) == false) {
-      return false;
-    }
-    if (Arrays.equals(polyLats, that.polyLats) == false) {
-      return false;
-    }
-
-    return true;
-  }
-
-  @Override
-  public final int hashCode() {
-    int result = super.hashCode();
-    result = 31 * result + Arrays.hashCode(polyLons);
-    result = 31 * result + Arrays.hashCode(polyLats);
-    return result;
-  }
-
-  @Override
-  public String toString(String field) {
-    final StringBuilder sb = new StringBuilder();
-    sb.append(getClass().getSimpleName());
-    sb.append(':');
-    if (this.field.equals(field) == false) {
-      sb.append(" field=");
-      sb.append(this.field);
-      sb.append(':');
-    }
-    sb.append(" Points: ");
-    for (int i=0; i<polyLons.length; ++i) {
-      sb.append("[")
-        .append(polyLons[i])
-        .append(", ")
-        .append(polyLats[i])
-        .append("] ");
-    }
-    return sb.toString();
-  }
-}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/search/DimensionalPointInRectQuery.java b/lucene/sandbox/src/java/org/apache/lucene/search/DimensionalPointInRectQuery.java
deleted file mode 100644
index b2e8bf3..0000000
--- a/lucene/sandbox/src/java/org/apache/lucene/search/DimensionalPointInRectQuery.java
+++ /dev/null
@@ -1,201 +0,0 @@
-package org.apache.lucene.search;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.document.DimensionalLatLonField;
-import org.apache.lucene.index.DimensionalValues.IntersectVisitor;
-import org.apache.lucene.index.DimensionalValues.Relation;
-import org.apache.lucene.index.DimensionalValues;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.LeafReader;
-import org.apache.lucene.index.LeafReaderContext;
-import org.apache.lucene.util.DocIdSetBuilder;
-import org.apache.lucene.util.GeoUtils;
-import org.apache.lucene.util.NumericUtils;
-
-/** Finds all previously indexed points that fall within the specified boundings box.
- *
- *  <p>The field must be indexed with using {@link DimensionalLatLonField} added per document.
- *
- *  @lucene.experimental */
-
-public class DimensionalPointInRectQuery extends Query {
-  final String field;
-  final double minLat;
-  final double maxLat;
-  final double minLon;
-  final double maxLon;
-
-  /** Matches all points &gt;= minLon, minLat (inclusive) and &lt; maxLon, maxLat (exclusive). */ 
-  public DimensionalPointInRectQuery(String field, double minLat, double maxLat, double minLon, double maxLon) {
-    this.field = field;
-    if (GeoUtils.isValidLat(minLat) == false) {
-      throw new IllegalArgumentException("minLat=" + minLat + " is not a valid latitude");
-    }
-    if (GeoUtils.isValidLat(maxLat) == false) {
-      throw new IllegalArgumentException("maxLat=" + maxLat + " is not a valid latitude");
-    }
-    if (GeoUtils.isValidLon(minLon) == false) {
-      throw new IllegalArgumentException("minLon=" + minLon + " is not a valid longitude");
-    }
-    if (GeoUtils.isValidLon(maxLon) == false) {
-      throw new IllegalArgumentException("maxLon=" + maxLon + " is not a valid longitude");
-    }
-    this.minLon = minLon;
-    this.maxLon = maxLon;
-    this.minLat = minLat;
-    this.maxLat = maxLat;
-  }
-
-  @Override
-  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {
-
-    // I don't use RandomAccessWeight here: it's no good to approximate with "match all docs"; this is an inverted structure and should be
-    // used in the first pass:
-
-    return new ConstantScoreWeight(this) {
-      @Override
-      public Scorer scorer(LeafReaderContext context) throws IOException {
-        LeafReader reader = context.reader();
-        DimensionalValues values = reader.getDimensionalValues();
-        if (values == null) {
-          // No docs in this segment had any dimensional fields
-          return null;
-        }
-
-        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc());
-        int[] hitCount = new int[1];
-        values.intersect(field,
-                         new IntersectVisitor() {
-                           @Override
-                           public void grow(int count) {
-                             result.grow(count);
-                           }
-
-                           @Override
-                           public void visit(int docID) {
-                             hitCount[0]++;
-                             result.add(docID);
-                           }
-
-                           @Override
-                           public void visit(int docID, byte[] packedValue) {
-                             assert packedValue.length == 8;
-                             double lat = DimensionalLatLonField.decodeLat(NumericUtils.bytesToInt(packedValue, 0));
-                             double lon = DimensionalLatLonField.decodeLon(NumericUtils.bytesToInt(packedValue, 1));
-                             if (lat >= minLat && lat <= maxLat && lon >= minLon && lon <= maxLon) {
-                               hitCount[0]++;
-                               result.add(docID);
-                             }
-                           }
-
-                           @Override
-                           public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {
-                             double cellMinLat = DimensionalLatLonField.decodeLat(NumericUtils.bytesToInt(minPackedValue, 0));
-                             double cellMinLon = DimensionalLatLonField.decodeLon(NumericUtils.bytesToInt(minPackedValue, 1));
-                             double cellMaxLat = DimensionalLatLonField.decodeLat(NumericUtils.bytesToInt(maxPackedValue, 0));
-                             double cellMaxLon = DimensionalLatLonField.decodeLon(NumericUtils.bytesToInt(maxPackedValue, 1));
-
-                             if (minLat <= cellMinLat && maxLat >= cellMaxLat && minLon <= cellMinLon && maxLon >= cellMaxLon) {
-                               return Relation.CELL_INSIDE_QUERY;
-                             }
-
-                             if (cellMaxLat < minLat || cellMinLat > maxLat || cellMaxLon < minLon || cellMinLon > maxLon) {
-                               return Relation.CELL_OUTSIDE_QUERY;
-                             }
-
-                             return Relation.CELL_CROSSES_QUERY;
-                           }
-                         });
-
-        // NOTE: hitCount[0] will be over-estimate in multi-valued case
-        return new ConstantScoreScorer(this, score(), result.build(hitCount[0]).iterator());
-      }
-    };
-  }
-
-  @Override
-  public Query rewrite(IndexReader reader) throws IOException {
-    // Crosses date line: we just rewrite into OR of two bboxes:
-    if (maxLon < minLon) {
-
-      // Disable coord here because a multi-valued doc could match both rects and get unfairly boosted:
-      BooleanQuery.Builder q = new BooleanQuery.Builder();
-      q.setDisableCoord(true);
-
-      // E.g.: maxLon = -179, minLon = 179
-      DimensionalPointInRectQuery left = new DimensionalPointInRectQuery(field, minLat, maxLat, GeoUtils.MIN_LON_INCL, maxLon);
-      q.add(new BooleanClause(left, BooleanClause.Occur.SHOULD));
-      DimensionalPointInRectQuery right = new DimensionalPointInRectQuery(field, minLat, maxLat, minLon, GeoUtils.MAX_LON_INCL);
-      q.add(new BooleanClause(right, BooleanClause.Occur.SHOULD));
-      return new ConstantScoreQuery(q.build());
-    } else {
-      return super.rewrite(reader);
-    }
-  }
-
-  @Override
-  public int hashCode() {
-    int hash = super.hashCode();
-    hash += Double.hashCode(minLat)^0x14fa55fb;
-    hash += Double.hashCode(maxLat)^0x733fa5fe;
-    hash += Double.hashCode(minLon)^0x14fa55fb;
-    hash += Double.hashCode(maxLon)^0x733fa5fe;
-    return hash;
-  }
-
-  @Override
-  public boolean equals(Object other) {
-    if (super.equals(other) && other instanceof DimensionalPointInRectQuery) {
-      final DimensionalPointInRectQuery q = (DimensionalPointInRectQuery) other;
-      return field.equals(q.field) &&
-        minLat == q.minLat &&
-        maxLat == q.maxLat &&
-        minLon == q.minLon &&
-        maxLon == q.maxLon;
-    }
-
-    return false;
-  }
-
-  @Override
-  public String toString(String field) {
-    final StringBuilder sb = new StringBuilder();
-    sb.append(getClass().getSimpleName());
-    sb.append(':');
-    if (this.field.equals(field) == false) {
-      sb.append("field=");
-      sb.append(this.field);
-      sb.append(':');
-    }
-
-    return sb.append(" Lower Left: [")
-        .append(minLon)
-        .append(',')
-        .append(minLat)
-        .append(']')
-        .append(" Upper Right: [")
-        .append(maxLon)
-        .append(',')
-        .append(maxLat)
-        .append("]")
-        .toString();
-  }
-}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/search/DocValuesRangeQuery.java b/lucene/sandbox/src/java/org/apache/lucene/search/DocValuesRangeQuery.java
index b443ab3..a45e4b0 100644
--- a/lucene/sandbox/src/java/org/apache/lucene/search/DocValuesRangeQuery.java
+++ b/lucene/sandbox/src/java/org/apache/lucene/search/DocValuesRangeQuery.java
@@ -33,7 +33,7 @@ import org.apache.lucene.util.BytesRef;
  * A range query that works on top of the doc values APIs. Such queries are
  * usually slow since they do not use an inverted index. However, in the
  * dense case where most documents match this query, it <b>might</b> be as
- * fast or faster than a regular {@link DimensionalRangeQuery}.
+ * fast or faster than a regular {@link PointRangeQuery}.
  *
  * <p>
  * <b>NOTE</b>: be very careful using this query: it is
diff --git a/lucene/sandbox/src/java/org/apache/lucene/search/PointInPolygonQuery.java b/lucene/sandbox/src/java/org/apache/lucene/search/PointInPolygonQuery.java
new file mode 100644
index 0000000..abdc5b9
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/search/PointInPolygonQuery.java
@@ -0,0 +1,212 @@
+package org.apache.lucene.search;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.document.LatLonPoint;
+import org.apache.lucene.index.PointValues.IntersectVisitor;
+import org.apache.lucene.index.PointValues.Relation;
+import org.apache.lucene.index.PointValues;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.util.DocIdSetBuilder;
+import org.apache.lucene.util.GeoRelationUtils;
+import org.apache.lucene.util.GeoUtils;
+import org.apache.lucene.util.NumericUtils;
+
+/** Finds all previously indexed points that fall within the specified polygon.
+ *
+ *  <p>The field must be indexed with using {@link org.apache.lucene.document.LatLonPoint} added per document.
+ *
+ *  @lucene.experimental */
+
+public class PointInPolygonQuery extends Query {
+  final String field;
+  final double minLat;
+  final double maxLat;
+  final double minLon;
+  final double maxLon;
+  final double[] polyLats;
+  final double[] polyLons;
+
+  /** The lats/lons must be clockwise or counter-clockwise. */
+  public PointInPolygonQuery(String field, double[] polyLats, double[] polyLons) {
+    this.field = field;
+    if (polyLats.length != polyLons.length) {
+      throw new IllegalArgumentException("polyLats and polyLons must be equal length");
+    }
+    if (polyLats.length < 4) {
+      throw new IllegalArgumentException("at least 4 polygon points required");
+    }
+    if (polyLats[0] != polyLats[polyLats.length-1]) {
+      throw new IllegalArgumentException("first and last points of the polygon must be the same (it must close itself): polyLats[0]=" + polyLats[0] + " polyLats[" + (polyLats.length-1) + "]=" + polyLats[polyLats.length-1]);
+    }
+    if (polyLons[0] != polyLons[polyLons.length-1]) {
+      throw new IllegalArgumentException("first and last points of the polygon must be the same (it must close itself): polyLons[0]=" + polyLons[0] + " polyLons[" + (polyLons.length-1) + "]=" + polyLons[polyLons.length-1]);
+    }
+
+    this.polyLats = polyLats;
+    this.polyLons = polyLons;
+
+    // TODO: we could also compute the maximal innner bounding box, to make relations faster to compute?
+
+    double minLon = Double.POSITIVE_INFINITY;
+    double minLat = Double.POSITIVE_INFINITY;
+    double maxLon = Double.NEGATIVE_INFINITY;
+    double maxLat = Double.NEGATIVE_INFINITY;
+    for(int i=0;i<polyLats.length;i++) {
+      double lat = polyLats[i];
+      if (GeoUtils.isValidLat(lat) == false) {
+        throw new IllegalArgumentException("polyLats[" + i + "]=" + lat + " is not a valid latitude");
+      }
+      minLat = Math.min(minLat, lat);
+      maxLat = Math.max(maxLat, lat);
+      double lon = polyLons[i];
+      if (GeoUtils.isValidLon(lon) == false) {
+        throw new IllegalArgumentException("polyLons[" + i + "]=" + lat + " is not a valid longitude");
+      }
+      minLon = Math.min(minLon, lon);
+      maxLon = Math.max(maxLon, lon);
+    }
+    this.minLon = minLon;
+    this.maxLon = maxLon;
+    this.minLat = minLat;
+    this.maxLat = maxLat;
+  }
+
+  @Override
+  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {
+
+    // I don't use RandomAccessWeight here: it's no good to approximate with "match all docs"; this is an inverted structure and should be
+    // used in the first pass:
+
+    // TODO: except that the polygon verify is costly!  The approximation should be all docs in all overlapping cells, and matches() should
+    // then check the polygon
+
+    return new ConstantScoreWeight(this) {
+
+      @Override
+      public Scorer scorer(LeafReaderContext context) throws IOException {
+        LeafReader reader = context.reader();
+        PointValues values = reader.getPointValues();
+        if (values == null) {
+          // No docs in this segment had any points fields
+          return null;
+        }
+
+        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc());
+        int[] hitCount = new int[1];
+        values.intersect(field,
+                         new IntersectVisitor() {
+                           @Override
+                           public void visit(int docID) {
+                             hitCount[0]++;
+                             result.add(docID);
+                           }
+
+                           @Override
+                           public void visit(int docID, byte[] packedValue) {
+                             assert packedValue.length == 8;
+                             double lat = LatLonPoint.decodeLat(NumericUtils.bytesToInt(packedValue, 0));
+                             double lon = LatLonPoint.decodeLon(NumericUtils.bytesToInt(packedValue, 1));
+                             if (GeoRelationUtils.pointInPolygon(polyLons, polyLats, lat, lon)) {
+                               hitCount[0]++;
+                               result.add(docID);
+                             }
+                           }
+
+                           @Override
+                           public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {
+                             double cellMinLat = LatLonPoint.decodeLat(NumericUtils.bytesToInt(minPackedValue, 0));
+                             double cellMinLon = LatLonPoint.decodeLon(NumericUtils.bytesToInt(minPackedValue, 1));
+                             double cellMaxLat = LatLonPoint.decodeLat(NumericUtils.bytesToInt(maxPackedValue, 0));
+                             double cellMaxLon = LatLonPoint.decodeLon(NumericUtils.bytesToInt(maxPackedValue, 1));
+
+                             if (cellMinLat <= minLat && cellMaxLat >= maxLat && cellMinLon <= minLon && cellMaxLon >= maxLon) {
+                               // Cell fully encloses the query
+                               return Relation.CELL_CROSSES_QUERY;
+                             } else  if (GeoRelationUtils.rectWithinPoly(cellMinLon, cellMinLat, cellMaxLon, cellMaxLat,
+                                                                 polyLons, polyLats,
+                                                                 minLon, minLat, maxLon, maxLat)) {
+                               return Relation.CELL_INSIDE_QUERY;
+                             } else if (GeoRelationUtils.rectCrossesPoly(cellMinLon, cellMinLat, cellMaxLon, cellMaxLat,
+                                                                 polyLons, polyLats,
+                                                                 minLon, minLat, maxLon, maxLat)) {
+                               return Relation.CELL_CROSSES_QUERY;
+                             } else {
+                               return Relation.CELL_OUTSIDE_QUERY;
+                             }
+                           }
+                         });
+
+        // NOTE: hitCount[0] will be over-estimate in multi-valued case
+        return new ConstantScoreScorer(this, score(), result.build(hitCount[0]).iterator());
+      }
+    };
+  }
+
+  @Override
+  @SuppressWarnings({"unchecked","rawtypes"})
+  public boolean equals(Object o) {
+    if (this == o) return true;
+    if (o == null || getClass() != o.getClass()) return false;
+    if (!super.equals(o)) return false;
+
+    PointInPolygonQuery that = (PointInPolygonQuery) o;
+
+    if (Arrays.equals(polyLons, that.polyLons) == false) {
+      return false;
+    }
+    if (Arrays.equals(polyLats, that.polyLats) == false) {
+      return false;
+    }
+
+    return true;
+  }
+
+  @Override
+  public final int hashCode() {
+    int result = super.hashCode();
+    result = 31 * result + Arrays.hashCode(polyLons);
+    result = 31 * result + Arrays.hashCode(polyLats);
+    return result;
+  }
+
+  @Override
+  public String toString(String field) {
+    final StringBuilder sb = new StringBuilder();
+    sb.append(getClass().getSimpleName());
+    sb.append(':');
+    if (this.field.equals(field) == false) {
+      sb.append(" field=");
+      sb.append(this.field);
+      sb.append(':');
+    }
+    sb.append(" Points: ");
+    for (int i=0; i<polyLons.length; ++i) {
+      sb.append("[")
+        .append(polyLons[i])
+        .append(", ")
+        .append(polyLats[i])
+        .append("] ");
+    }
+    return sb.toString();
+  }
+}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/search/PointInRectQuery.java b/lucene/sandbox/src/java/org/apache/lucene/search/PointInRectQuery.java
new file mode 100644
index 0000000..9af6097
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/search/PointInRectQuery.java
@@ -0,0 +1,201 @@
+package org.apache.lucene.search;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.document.LatLonPoint;
+import org.apache.lucene.index.PointValues.IntersectVisitor;
+import org.apache.lucene.index.PointValues.Relation;
+import org.apache.lucene.index.PointValues;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.util.DocIdSetBuilder;
+import org.apache.lucene.util.GeoUtils;
+import org.apache.lucene.util.NumericUtils;
+
+/** Finds all previously indexed points that fall within the specified boundings box.
+ *
+ *  <p>The field must be indexed with using {@link org.apache.lucene.document.LatLonPoint} added per document.
+ *
+ *  @lucene.experimental */
+
+public class PointInRectQuery extends Query {
+  final String field;
+  final double minLat;
+  final double maxLat;
+  final double minLon;
+  final double maxLon;
+
+  /** Matches all points &gt;= minLon, minLat (inclusive) and &lt; maxLon, maxLat (exclusive). */ 
+  public PointInRectQuery(String field, double minLat, double maxLat, double minLon, double maxLon) {
+    this.field = field;
+    if (GeoUtils.isValidLat(minLat) == false) {
+      throw new IllegalArgumentException("minLat=" + minLat + " is not a valid latitude");
+    }
+    if (GeoUtils.isValidLat(maxLat) == false) {
+      throw new IllegalArgumentException("maxLat=" + maxLat + " is not a valid latitude");
+    }
+    if (GeoUtils.isValidLon(minLon) == false) {
+      throw new IllegalArgumentException("minLon=" + minLon + " is not a valid longitude");
+    }
+    if (GeoUtils.isValidLon(maxLon) == false) {
+      throw new IllegalArgumentException("maxLon=" + maxLon + " is not a valid longitude");
+    }
+    this.minLon = minLon;
+    this.maxLon = maxLon;
+    this.minLat = minLat;
+    this.maxLat = maxLat;
+  }
+
+  @Override
+  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {
+
+    // I don't use RandomAccessWeight here: it's no good to approximate with "match all docs"; this is an inverted structure and should be
+    // used in the first pass:
+
+    return new ConstantScoreWeight(this) {
+      @Override
+      public Scorer scorer(LeafReaderContext context) throws IOException {
+        LeafReader reader = context.reader();
+        PointValues values = reader.getPointValues();
+        if (values == null) {
+          // No docs in this segment had any points fields
+          return null;
+        }
+
+        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc());
+        int[] hitCount = new int[1];
+        values.intersect(field,
+                         new IntersectVisitor() {
+                           @Override
+                           public void grow(int count) {
+                             result.grow(count);
+                           }
+
+                           @Override
+                           public void visit(int docID) {
+                             hitCount[0]++;
+                             result.add(docID);
+                           }
+
+                           @Override
+                           public void visit(int docID, byte[] packedValue) {
+                             assert packedValue.length == 8;
+                             double lat = LatLonPoint.decodeLat(NumericUtils.bytesToInt(packedValue, 0));
+                             double lon = LatLonPoint.decodeLon(NumericUtils.bytesToInt(packedValue, 1));
+                             if (lat >= minLat && lat <= maxLat && lon >= minLon && lon <= maxLon) {
+                               hitCount[0]++;
+                               result.add(docID);
+                             }
+                           }
+
+                           @Override
+                           public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {
+                             double cellMinLat = LatLonPoint.decodeLat(NumericUtils.bytesToInt(minPackedValue, 0));
+                             double cellMinLon = LatLonPoint.decodeLon(NumericUtils.bytesToInt(minPackedValue, 1));
+                             double cellMaxLat = LatLonPoint.decodeLat(NumericUtils.bytesToInt(maxPackedValue, 0));
+                             double cellMaxLon = LatLonPoint.decodeLon(NumericUtils.bytesToInt(maxPackedValue, 1));
+
+                             if (minLat <= cellMinLat && maxLat >= cellMaxLat && minLon <= cellMinLon && maxLon >= cellMaxLon) {
+                               return Relation.CELL_INSIDE_QUERY;
+                             }
+
+                             if (cellMaxLat < minLat || cellMinLat > maxLat || cellMaxLon < minLon || cellMinLon > maxLon) {
+                               return Relation.CELL_OUTSIDE_QUERY;
+                             }
+
+                             return Relation.CELL_CROSSES_QUERY;
+                           }
+                         });
+
+        // NOTE: hitCount[0] will be over-estimate in multi-valued case
+        return new ConstantScoreScorer(this, score(), result.build(hitCount[0]).iterator());
+      }
+    };
+  }
+
+  @Override
+  public Query rewrite(IndexReader reader) throws IOException {
+    // Crosses date line: we just rewrite into OR of two bboxes:
+    if (maxLon < minLon) {
+
+      // Disable coord here because a multi-valued doc could match both rects and get unfairly boosted:
+      BooleanQuery.Builder q = new BooleanQuery.Builder();
+      q.setDisableCoord(true);
+
+      // E.g.: maxLon = -179, minLon = 179
+      PointInRectQuery left = new PointInRectQuery(field, minLat, maxLat, GeoUtils.MIN_LON_INCL, maxLon);
+      q.add(new BooleanClause(left, BooleanClause.Occur.SHOULD));
+      PointInRectQuery right = new PointInRectQuery(field, minLat, maxLat, minLon, GeoUtils.MAX_LON_INCL);
+      q.add(new BooleanClause(right, BooleanClause.Occur.SHOULD));
+      return new ConstantScoreQuery(q.build());
+    } else {
+      return super.rewrite(reader);
+    }
+  }
+
+  @Override
+  public int hashCode() {
+    int hash = super.hashCode();
+    hash += Double.hashCode(minLat)^0x14fa55fb;
+    hash += Double.hashCode(maxLat)^0x733fa5fe;
+    hash += Double.hashCode(minLon)^0x14fa55fb;
+    hash += Double.hashCode(maxLon)^0x733fa5fe;
+    return hash;
+  }
+
+  @Override
+  public boolean equals(Object other) {
+    if (super.equals(other) && other instanceof PointInRectQuery) {
+      final PointInRectQuery q = (PointInRectQuery) other;
+      return field.equals(q.field) &&
+        minLat == q.minLat &&
+        maxLat == q.maxLat &&
+        minLon == q.minLon &&
+        maxLon == q.maxLon;
+    }
+
+    return false;
+  }
+
+  @Override
+  public String toString(String field) {
+    final StringBuilder sb = new StringBuilder();
+    sb.append(getClass().getSimpleName());
+    sb.append(':');
+    if (this.field.equals(field) == false) {
+      sb.append("field=");
+      sb.append(this.field);
+      sb.append(':');
+    }
+
+    return sb.append(" Lower Left: [")
+        .append(minLon)
+        .append(',')
+        .append(minLat)
+        .append(']')
+        .append(" Upper Right: [")
+        .append(maxLon)
+        .append(',')
+        .append(maxLat)
+        .append("]")
+        .toString();
+  }
+}
diff --git a/lucene/sandbox/src/test/org/apache/lucene/search/TestDimensionalQueries.java b/lucene/sandbox/src/test/org/apache/lucene/search/TestDimensionalQueries.java
deleted file mode 100644
index 23234ef..0000000
--- a/lucene/sandbox/src/test/org/apache/lucene/search/TestDimensionalQueries.java
+++ /dev/null
@@ -1,125 +0,0 @@
-package org.apache.lucene.search;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.document.DimensionalLatLonField;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.BaseGeoPointTestCase;
-import org.apache.lucene.util.GeoDistanceUtils;
-import org.apache.lucene.util.GeoRect;
-import org.apache.lucene.util.SloppyMath;
-
-public class TestDimensionalQueries extends BaseGeoPointTestCase {
-
-  @Override
-  protected void addPointToDoc(String field, Document doc, double lat, double lon) {
-    doc.add(new DimensionalLatLonField(field, lat, lon));
-  }
-
-  @Override
-  protected Query newRectQuery(String field, GeoRect rect) {
-    return new DimensionalPointInRectQuery(field, rect.minLat, rect.maxLat, rect.minLon, rect.maxLon);
-  }
-
-  @Override
-  protected Query newDistanceQuery(String field, double centerLat, double centerLon, double radiusMeters) {
-    // return new BKDDistanceQuery(field, centerLat, centerLon, radiusMeters);
-    return null;
-  }
-
-  @Override
-  protected Query newDistanceRangeQuery(String field, double centerLat, double centerLon, double minRadiusMeters, double radiusMeters) {
-    return null;
-  }
-
-  @Override
-  protected Query newPolygonQuery(String field, double[] lats, double[] lons) {
-    return new DimensionalPointInPolygonQuery(FIELD_NAME, lats, lons);
-  }
-
-  @Override
-  protected Boolean rectContainsPoint(GeoRect rect, double pointLat, double pointLon) {
-
-    assert Double.isNaN(pointLat) == false;
-
-    int rectLatMinEnc = DimensionalLatLonField.encodeLat(rect.minLat);
-    int rectLatMaxEnc = DimensionalLatLonField.encodeLat(rect.maxLat);
-    int rectLonMinEnc = DimensionalLatLonField.encodeLon(rect.minLon);
-    int rectLonMaxEnc = DimensionalLatLonField.encodeLon(rect.maxLon);
-
-    int pointLatEnc = DimensionalLatLonField.encodeLat(pointLat);
-    int pointLonEnc = DimensionalLatLonField.encodeLon(pointLon);
-
-    if (rect.minLon < rect.maxLon) {
-      return pointLatEnc >= rectLatMinEnc &&
-        pointLatEnc <= rectLatMaxEnc &&
-        pointLonEnc >= rectLonMinEnc &&
-        pointLonEnc <= rectLonMaxEnc;
-    } else {
-      // Rect crosses dateline:
-      return pointLatEnc >= rectLatMinEnc &&
-        pointLatEnc <= rectLatMaxEnc &&
-        (pointLonEnc >= rectLonMinEnc ||
-         pointLonEnc <= rectLonMaxEnc);
-    }
-  }
-
-  private static final double POLY_TOLERANCE = 1e-7;
-
-  @Override
-  protected Boolean polyRectContainsPoint(GeoRect rect, double pointLat, double pointLon) {
-    if (Math.abs(rect.minLat-pointLat) < POLY_TOLERANCE ||
-        Math.abs(rect.maxLat-pointLat) < POLY_TOLERANCE ||
-        Math.abs(rect.minLon-pointLon) < POLY_TOLERANCE ||
-        Math.abs(rect.maxLon-pointLon) < POLY_TOLERANCE) {
-      // The poly check quantizes slightly differently, so we allow for boundary cases to disagree
-      return null;
-    } else {
-      return rectContainsPoint(rect, pointLat, pointLon);
-    }
-  }
-
-  @Override
-  protected Boolean circleContainsPoint(double centerLat, double centerLon, double radiusMeters, double pointLat, double pointLon) {
-    double distanceMeters = GeoDistanceUtils.haversin(centerLat, centerLon, pointLat, pointLon);
-    boolean result = distanceMeters <= radiusMeters;
-    //System.out.println("  shouldMatch?  centerLon=" + centerLon + " centerLat=" + centerLat + " pointLon=" + pointLon + " pointLat=" + pointLat + " result=" + result + " distanceMeters=" + (distanceKM * 1000));
-    return result;
-  }
-
-  @Override
-  protected Boolean distanceRangeContainsPoint(double centerLat, double centerLon, double minRadiusMeters, double radiusMeters, double pointLat, double pointLon) {
-    final double d = GeoDistanceUtils.haversin(centerLat, centerLon, pointLat, pointLon);
-    return d >= minRadiusMeters && d <= radiusMeters;
-  }
-
-  public void testEncodeDecode() throws Exception {
-    int iters = atLeast(10000);
-    boolean small = random().nextBoolean();
-    for(int iter=0;iter<iters;iter++) {
-      double lat = randomLat(small);
-      double latQuantized = DimensionalLatLonField.decodeLat(DimensionalLatLonField.encodeLat(lat));
-      assertEquals(lat, latQuantized, DimensionalLatLonField.TOLERANCE);
-
-      double lon = randomLon(small);
-      double lonQuantized = DimensionalLatLonField.decodeLon(DimensionalLatLonField.encodeLon(lon));
-      assertEquals(lon, lonQuantized, DimensionalLatLonField.TOLERANCE);
-    }
-  }
-}
diff --git a/lucene/sandbox/src/test/org/apache/lucene/search/TestDocValuesRangeQuery.java b/lucene/sandbox/src/test/org/apache/lucene/search/TestDocValuesRangeQuery.java
index aa51a48..694fcda 100644
--- a/lucene/sandbox/src/test/org/apache/lucene/search/TestDocValuesRangeQuery.java
+++ b/lucene/sandbox/src/test/org/apache/lucene/search/TestDocValuesRangeQuery.java
@@ -19,7 +19,7 @@ package org.apache.lucene.search;
 
 import java.io.IOException;
 
-import org.apache.lucene.document.DimensionalLongField;
+import org.apache.lucene.document.LongPoint;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field.Store;
 import org.apache.lucene.document.NumericDocValuesField;
@@ -53,12 +53,12 @@ public class TestDocValuesRangeQuery extends LuceneTestCase {
         for (int j = 0; j < numValues; ++j) {
           final long value = TestUtil.nextLong(random(), -100, 10000);
           doc.add(new SortedNumericDocValuesField("dv", value));
-          doc.add(new DimensionalLongField("idx", value));
+          doc.add(new LongPoint("idx", value));
         }
         iw.addDocument(doc);
       }
       if (random().nextBoolean()) {
-        iw.deleteDocuments(DimensionalRangeQuery.new1DLongRange("idx", 0L, true, 10L, true));
+        iw.deleteDocuments(PointRangeQuery.new1DLongRange("idx", 0L, true, 10L, true));
       }
       iw.commit();
       final IndexReader reader = iw.getReader();
@@ -70,7 +70,7 @@ public class TestDocValuesRangeQuery extends LuceneTestCase {
         final Long max = random().nextBoolean() ? null : TestUtil.nextLong(random(), -100, 1000);
         final boolean minInclusive = random().nextBoolean();
         final boolean maxInclusive = random().nextBoolean();
-        final Query q1 = DimensionalRangeQuery.new1DLongRange("idx", min, minInclusive, max, maxInclusive);
+        final Query q1 = PointRangeQuery.new1DLongRange("idx", min, minInclusive, max, maxInclusive);
         final Query q2 = DocValuesRangeQuery.newLongRange("dv", min, max, minInclusive, maxInclusive);
         assertSameMatches(searcher, q1, q2, false);
       }
@@ -180,13 +180,13 @@ public class TestDocValuesRangeQuery extends LuceneTestCase {
         final long value = TestUtil.nextLong(random(), -100, 10000);
         doc.add(new SortedNumericDocValuesField("dv1", value));
         doc.add(new SortedSetDocValuesField("dv2", toSortableBytes(value)));
-        doc.add(new DimensionalLongField("idx", value));
+        doc.add(new LongPoint("idx", value));
         doc.add(new StringField("f", random().nextBoolean() ? "a" : "b", Store.NO));
       }
       iw.addDocument(doc);
     }
     if (random().nextBoolean()) {
-      iw.deleteDocuments(DimensionalRangeQuery.new1DLongRange("idx", 0L, true, 10L, true));
+      iw.deleteDocuments(PointRangeQuery.new1DLongRange("idx", 0L, true, 10L, true));
     }
     iw.commit();
     final IndexReader reader = iw.getReader();
@@ -200,7 +200,7 @@ public class TestDocValuesRangeQuery extends LuceneTestCase {
       final boolean maxInclusive = random().nextBoolean();
 
       BooleanQuery.Builder ref = new BooleanQuery.Builder();
-      ref.add(DimensionalRangeQuery.new1DLongRange("idx", min, minInclusive, max, maxInclusive), Occur.FILTER);
+      ref.add(PointRangeQuery.new1DLongRange("idx", min, minInclusive, max, maxInclusive), Occur.FILTER);
       ref.add(new TermQuery(new Term("f", "a")), Occur.MUST);
 
       BooleanQuery.Builder bq1 = new BooleanQuery.Builder();
diff --git a/lucene/sandbox/src/test/org/apache/lucene/search/TestLatLonPointQueries.java b/lucene/sandbox/src/test/org/apache/lucene/search/TestLatLonPointQueries.java
new file mode 100644
index 0000000..313ec21
--- /dev/null
+++ b/lucene/sandbox/src/test/org/apache/lucene/search/TestLatLonPointQueries.java
@@ -0,0 +1,123 @@
+package org.apache.lucene.search;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.document.LatLonPoint;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.util.BaseGeoPointTestCase;
+import org.apache.lucene.util.GeoDistanceUtils;
+import org.apache.lucene.util.GeoRect;
+
+public class TestLatLonPointQueries extends BaseGeoPointTestCase {
+
+  @Override
+  protected void addPointToDoc(String field, Document doc, double lat, double lon) {
+    doc.add(new LatLonPoint(field, lat, lon));
+  }
+
+  @Override
+  protected Query newRectQuery(String field, GeoRect rect) {
+    return new PointInRectQuery(field, rect.minLat, rect.maxLat, rect.minLon, rect.maxLon);
+  }
+
+  @Override
+  protected Query newDistanceQuery(String field, double centerLat, double centerLon, double radiusMeters) {
+    // return new BKDDistanceQuery(field, centerLat, centerLon, radiusMeters);
+    return null;
+  }
+
+  @Override
+  protected Query newDistanceRangeQuery(String field, double centerLat, double centerLon, double minRadiusMeters, double radiusMeters) {
+    return null;
+  }
+
+  @Override
+  protected Query newPolygonQuery(String field, double[] lats, double[] lons) {
+    return new PointInPolygonQuery(FIELD_NAME, lats, lons);
+  }
+
+  @Override
+  protected Boolean rectContainsPoint(GeoRect rect, double pointLat, double pointLon) {
+
+    assert Double.isNaN(pointLat) == false;
+
+    int rectLatMinEnc = LatLonPoint.encodeLat(rect.minLat);
+    int rectLatMaxEnc = LatLonPoint.encodeLat(rect.maxLat);
+    int rectLonMinEnc = LatLonPoint.encodeLon(rect.minLon);
+    int rectLonMaxEnc = LatLonPoint.encodeLon(rect.maxLon);
+
+    int pointLatEnc = LatLonPoint.encodeLat(pointLat);
+    int pointLonEnc = LatLonPoint.encodeLon(pointLon);
+
+    if (rect.minLon < rect.maxLon) {
+      return pointLatEnc >= rectLatMinEnc &&
+        pointLatEnc <= rectLatMaxEnc &&
+        pointLonEnc >= rectLonMinEnc &&
+        pointLonEnc <= rectLonMaxEnc;
+    } else {
+      // Rect crosses dateline:
+      return pointLatEnc >= rectLatMinEnc &&
+        pointLatEnc <= rectLatMaxEnc &&
+        (pointLonEnc >= rectLonMinEnc ||
+         pointLonEnc <= rectLonMaxEnc);
+    }
+  }
+
+  private static final double POLY_TOLERANCE = 1e-7;
+
+  @Override
+  protected Boolean polyRectContainsPoint(GeoRect rect, double pointLat, double pointLon) {
+    if (Math.abs(rect.minLat-pointLat) < POLY_TOLERANCE ||
+        Math.abs(rect.maxLat-pointLat) < POLY_TOLERANCE ||
+        Math.abs(rect.minLon-pointLon) < POLY_TOLERANCE ||
+        Math.abs(rect.maxLon-pointLon) < POLY_TOLERANCE) {
+      // The poly check quantizes slightly differently, so we allow for boundary cases to disagree
+      return null;
+    } else {
+      return rectContainsPoint(rect, pointLat, pointLon);
+    }
+  }
+
+  @Override
+  protected Boolean circleContainsPoint(double centerLat, double centerLon, double radiusMeters, double pointLat, double pointLon) {
+    double distanceMeters = GeoDistanceUtils.haversin(centerLat, centerLon, pointLat, pointLon);
+    boolean result = distanceMeters <= radiusMeters;
+    //System.out.println("  shouldMatch?  centerLon=" + centerLon + " centerLat=" + centerLat + " pointLon=" + pointLon + " pointLat=" + pointLat + " result=" + result + " distanceMeters=" + (distanceKM * 1000));
+    return result;
+  }
+
+  @Override
+  protected Boolean distanceRangeContainsPoint(double centerLat, double centerLon, double minRadiusMeters, double radiusMeters, double pointLat, double pointLon) {
+    final double d = GeoDistanceUtils.haversin(centerLat, centerLon, pointLat, pointLon);
+    return d >= minRadiusMeters && d <= radiusMeters;
+  }
+
+  public void testEncodeDecode() throws Exception {
+    int iters = atLeast(10000);
+    boolean small = random().nextBoolean();
+    for(int iter=0;iter<iters;iter++) {
+      double lat = randomLat(small);
+      double latQuantized = LatLonPoint.decodeLat(LatLonPoint.encodeLat(lat));
+      assertEquals(lat, latQuantized, LatLonPoint.TOLERANCE);
+
+      double lon = randomLon(small);
+      double lonQuantized = LatLonPoint.decodeLon(LatLonPoint.encodeLon(lon));
+      assertEquals(lon, lonQuantized, LatLonPoint.TOLERANCE);
+    }
+  }
+}
diff --git a/lucene/spatial3d/src/java/org/apache/lucene/geo3d/Geo3DPoint.java b/lucene/spatial3d/src/java/org/apache/lucene/geo3d/Geo3DPoint.java
new file mode 100644
index 0000000..b3ca7f6
--- /dev/null
+++ b/lucene/spatial3d/src/java/org/apache/lucene/geo3d/Geo3DPoint.java
@@ -0,0 +1,69 @@
+package org.apache.lucene.geo3d;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.NumericUtils;
+import org.apache.lucene.util.RamUsageEstimator;
+
+/** Add this to a document to index lat/lon or x/y/z point, indexed as a dimensional value.
+ *  Multiple values are allowed: just add multiple Geo3DPoint to the document with the
+ *  same field name.
+ *
+ *  @lucene.experimental */
+public final class Geo3DPoint extends Field {
+
+  /** Indexing {@link FieldType}. */
+  public static final FieldType TYPE = new FieldType();
+  static {
+    TYPE.setDimensions(3, RamUsageEstimator.NUM_BYTES_INT);
+    TYPE.freeze();
+  }
+
+  /** 
+   * Creates a new Geo3DPoint field with the specified lat, lon (in radians), given a planet model.
+   *
+   * @throws IllegalArgumentException if the field name is null or lat or lon are out of bounds
+   */
+  public Geo3DPoint(String name, PlanetModel planetModel, double lat, double lon) {
+    super(name, TYPE);
+    // Translate lat/lon to x,y,z:
+    final GeoPoint point = new GeoPoint(planetModel, lat, lon);
+    fillFieldsData(planetModel.getMaximumMagnitude(), point.x, point.y, point.z);
+  }
+
+  /** 
+   * Creates a new Geo3DPoint field with the specified x,y,z.
+   *
+   * @throws IllegalArgumentException if the field name is null or lat or lon are out of bounds
+   */
+  public Geo3DPoint(String name, PlanetModel planetModel, double x, double y, double z) {
+    super(name, TYPE);
+    fillFieldsData(planetModel.getMaximumMagnitude(), x, y, z);
+  }
+
+  private void fillFieldsData(double planetMax, double x, double y, double z) {
+    byte[] bytes = new byte[12];
+    NumericUtils.intToBytes(Geo3DUtil.encodeValue(planetMax, x), bytes, 0);
+    NumericUtils.intToBytes(Geo3DUtil.encodeValue(planetMax, y), bytes, 1);
+    NumericUtils.intToBytes(Geo3DUtil.encodeValue(planetMax, z), bytes, 2);
+    fieldsData = new BytesRef(bytes);
+  }
+}
diff --git a/lucene/spatial3d/src/java/org/apache/lucene/geo3d/Geo3DPointField.java b/lucene/spatial3d/src/java/org/apache/lucene/geo3d/Geo3DPointField.java
deleted file mode 100644
index 1b19d62..0000000
--- a/lucene/spatial3d/src/java/org/apache/lucene/geo3d/Geo3DPointField.java
+++ /dev/null
@@ -1,69 +0,0 @@
-package org.apache.lucene.geo3d;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.NumericUtils;
-import org.apache.lucene.util.RamUsageEstimator;
-
-/** Add this to a document to index lat/lon or x/y/z point, indexed as a dimensional value.
- *  Multiple values are allowed: just add multiple Geo3DPointField to the document with the
- *  same field name.
- *
- *  @lucene.experimental */
-public final class Geo3DPointField extends Field {
-
-  /** Indexing {@link FieldType}. */
-  public static final FieldType TYPE = new FieldType();
-  static {
-    TYPE.setDimensions(3, RamUsageEstimator.NUM_BYTES_INT);
-    TYPE.freeze();
-  }
-
-  /** 
-   * Creates a new Geo3DPointField field with the specified lat, lon (in radians), given a planet model.
-   *
-   * @throws IllegalArgumentException if the field name is null or lat or lon are out of bounds
-   */
-  public Geo3DPointField(String name, PlanetModel planetModel, double lat, double lon) {
-    super(name, TYPE);
-    // Translate lat/lon to x,y,z:
-    final GeoPoint point = new GeoPoint(planetModel, lat, lon);
-    fillFieldsData(planetModel.getMaximumMagnitude(), point.x, point.y, point.z);
-  }
-
-  /** 
-   * Creates a new Geo3DPointField field with the specified x,y,z.
-   *
-   * @throws IllegalArgumentException if the field name is null or lat or lon are out of bounds
-   */
-  public Geo3DPointField(String name, PlanetModel planetModel, double x, double y, double z) {
-    super(name, TYPE);
-    fillFieldsData(planetModel.getMaximumMagnitude(), x, y, z);
-  }
-
-  private void fillFieldsData(double planetMax, double x, double y, double z) {
-    byte[] bytes = new byte[12];
-    NumericUtils.intToBytes(Geo3DUtil.encodeValue(planetMax, x), bytes, 0);
-    NumericUtils.intToBytes(Geo3DUtil.encodeValue(planetMax, y), bytes, 1);
-    NumericUtils.intToBytes(Geo3DUtil.encodeValue(planetMax, z), bytes, 2);
-    fieldsData = new BytesRef(bytes);
-  }
-}
diff --git a/lucene/spatial3d/src/java/org/apache/lucene/geo3d/PointInGeo3DShapeQuery.java b/lucene/spatial3d/src/java/org/apache/lucene/geo3d/PointInGeo3DShapeQuery.java
index 1f465b7..ac8d227 100644
--- a/lucene/spatial3d/src/java/org/apache/lucene/geo3d/PointInGeo3DShapeQuery.java
+++ b/lucene/spatial3d/src/java/org/apache/lucene/geo3d/PointInGeo3DShapeQuery.java
@@ -19,9 +19,9 @@ package org.apache.lucene.geo3d;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.DimensionalValues.IntersectVisitor;
-import org.apache.lucene.index.DimensionalValues.Relation;
-import org.apache.lucene.index.DimensionalValues;
+import org.apache.lucene.index.PointValues.IntersectVisitor;
+import org.apache.lucene.index.PointValues;
+import org.apache.lucene.index.PointValues.Relation;
 import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.ConstantScoreScorer;
@@ -35,7 +35,7 @@ import org.apache.lucene.util.NumericUtils;
 
 /** Finds all previously indexed points that fall within the specified polygon.
  *
- * <p>The field must be indexed using {@link Geo3DPointField}.
+ * <p>The field must be indexed using {@link Geo3DPoint}.
  *
  * @lucene.experimental */
 
@@ -62,7 +62,7 @@ public class PointInGeo3DShapeQuery extends Query {
       @Override
       public Scorer scorer(LeafReaderContext context) throws IOException {
         LeafReader reader = context.reader();
-        DimensionalValues values = reader.getDimensionalValues();
+        PointValues values = reader.getPointValues();
         if (values == null) {
           return null;
         }
diff --git a/lucene/spatial3d/src/test/org/apache/lucene/geo3d/TestGeo3DPoint.java b/lucene/spatial3d/src/test/org/apache/lucene/geo3d/TestGeo3DPoint.java
new file mode 100644
index 0000000..3a3a31f
--- /dev/null
+++ b/lucene/spatial3d/src/test/org/apache/lucene/geo3d/TestGeo3DPoint.java
@@ -0,0 +1,802 @@
+package org.apache.lucene.geo3d;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.io.PrintWriter;
+import java.io.StringWriter;
+import java.util.ArrayList;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Set;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.PointFormat;
+import org.apache.lucene.codecs.PointReader;
+import org.apache.lucene.codecs.PointWriter;
+import org.apache.lucene.codecs.FilterCodec;
+import org.apache.lucene.codecs.lucene60.Lucene60PointReader;
+import org.apache.lucene.codecs.lucene60.Lucene60PointWriter;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.index.MultiDocValues;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.SimpleCollector;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.MockDirectoryWrapper;
+import org.apache.lucene.util.FixedBitSet;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.TestUtil;
+import org.junit.BeforeClass;
+
+import com.carrotsearch.randomizedtesting.generators.RandomInts;
+
+public class TestGeo3DPoint extends LuceneTestCase {
+
+  private static boolean smallBBox;
+  
+  @BeforeClass
+  public static void beforeClass() {
+    smallBBox = random().nextBoolean();
+    if (VERBOSE) {
+      System.err.println("TEST: smallBBox=" + smallBBox);
+    }
+  }
+
+  private static Codec getCodec() {
+    if (Codec.getDefault().getName().equals("Lucene60")) {
+      int maxPointsInLeafNode = TestUtil.nextInt(random(), 16, 2048);
+      double maxMBSortInHeap = 3.0 + (3*random().nextDouble());
+      if (VERBOSE) {
+        System.out.println("TEST: using Lucene60PointFormat with maxPointsInLeafNode=" + maxPointsInLeafNode + " and maxMBSortInHeap=" + maxMBSortInHeap);
+      }
+
+      return new FilterCodec("Lucene60", Codec.getDefault()) {
+        @Override
+        public PointFormat pointFormat() {
+          return new PointFormat() {
+            @Override
+            public PointWriter fieldsWriter(SegmentWriteState writeState) throws IOException {
+              return new Lucene60PointWriter(writeState, maxPointsInLeafNode, maxMBSortInHeap);
+            }
+
+            @Override
+            public PointReader fieldsReader(SegmentReadState readState) throws IOException {
+              return new Lucene60PointReader(readState);
+            }
+          };
+        }
+      };
+    } else {
+      return Codec.getDefault();
+    }
+  }
+
+  public void testBasic() throws Exception {
+    Directory dir = getDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig();
+    iwc.setCodec(getCodec());
+    IndexWriter w = new IndexWriter(dir, iwc);
+    Document doc = new Document();
+    doc.add(new Geo3DPoint("field", PlanetModel.WGS84, toRadians(50.7345267), toRadians(-97.5303555)));
+    w.addDocument(doc);
+    IndexReader r = DirectoryReader.open(w, true);
+    // We can't wrap with "exotic" readers because the query must see the BKD3DDVFormat:
+    IndexSearcher s = newSearcher(r, false);
+    assertEquals(1, s.search(new PointInGeo3DShapeQuery(PlanetModel.WGS84,
+                                                        "field",
+                                                        GeoCircleFactory.makeGeoCircle(PlanetModel.WGS84, toRadians(50), toRadians(-97), Math.PI/180.)), 1).totalHits);
+    w.close();
+    r.close();
+    dir.close();
+  }
+
+  private static double toRadians(double degrees) {
+    return Math.PI*(degrees/360.0);
+  }
+
+  private static PlanetModel getPlanetModel() {
+    if (random().nextBoolean()) {
+      // Use one of the earth models:
+      if (random().nextBoolean()) {
+        return PlanetModel.WGS84;
+      } else {
+        return PlanetModel.SPHERE;
+      }
+    } else {
+      // Make a randomly squashed planet:
+      double oblateness = random().nextDouble() * 0.5 - 0.25;
+      return new PlanetModel(1.0 + oblateness, 1.0 - oblateness);
+    }
+  }
+
+  private static class Cell {
+    static int nextCellID;
+
+    final Cell parent;
+    final int cellID;
+    final int xMinEnc, xMaxEnc;
+    final int yMinEnc, yMaxEnc;
+    final int zMinEnc, zMaxEnc;
+    final int splitCount;
+
+    public Cell(Cell parent,
+                int xMinEnc, int xMaxEnc,
+                int yMinEnc, int yMaxEnc,
+                int zMinEnc, int zMaxEnc,
+                int splitCount) {
+      this.parent = parent;
+      this.xMinEnc = xMinEnc;
+      this.xMaxEnc = xMaxEnc;
+      this.yMinEnc = yMinEnc;
+      this.yMaxEnc = yMaxEnc;
+      this.zMinEnc = zMinEnc;
+      this.zMaxEnc = zMaxEnc;
+      this.cellID = nextCellID++;
+      this.splitCount = splitCount;
+    }
+
+    /** Returns true if the quantized point lies within this cell, inclusive on all bounds. */
+    public boolean contains(double planetMax, GeoPoint point) {
+      int docX = Geo3DUtil.encodeValue(planetMax, point.x);
+      int docY = Geo3DUtil.encodeValue(planetMax, point.y);
+      int docZ = Geo3DUtil.encodeValue(planetMax, point.z);
+
+      return docX >= xMinEnc && docX <= xMaxEnc &&
+        docY >= yMinEnc && docY <= yMaxEnc && 
+        docZ >= zMinEnc && docZ <= zMaxEnc;
+    }
+
+    @Override
+    public String toString() {
+      return "cell=" + cellID + (parent == null ? "" : " parentCellID=" + parent.cellID) + " x: " + xMinEnc + " TO " + xMaxEnc + ", y: " + yMinEnc + " TO " + yMaxEnc + ", z: " + zMinEnc + " TO " + zMaxEnc + ", splits: " + splitCount;
+    }
+  }
+
+  private static GeoPoint quantize(double planetMax, GeoPoint point) {
+    return new GeoPoint(Geo3DUtil.decodeValueCenter(planetMax, Geo3DUtil.encodeValue(planetMax, point.x)),
+                        Geo3DUtil.decodeValueCenter(planetMax, Geo3DUtil.encodeValue(planetMax, point.y)),
+                        Geo3DUtil.decodeValueCenter(planetMax, Geo3DUtil.encodeValue(planetMax, point.z)));
+  }
+
+  /** Tests consistency of GeoArea.getRelationship vs GeoShape.isWithin */
+  public void testGeo3DRelations() throws Exception {
+
+    PlanetModel planetModel = getPlanetModel();
+
+    int numDocs = atLeast(1000);
+    if (VERBOSE) {
+      System.out.println("TEST: " + numDocs + " docs");
+    }
+
+    GeoPoint[] docs = new GeoPoint[numDocs];
+    for(int docID=0;docID<numDocs;docID++) {
+      docs[docID] = new GeoPoint(planetModel, toRadians(randomLat()), toRadians(randomLon()));
+      if (VERBOSE) {
+        System.out.println("  doc=" + docID + ": " + docs[docID]);
+      }
+    }
+
+    double planetMax = planetModel.getMaximumMagnitude();
+
+    int iters = atLeast(10);
+
+    int recurseDepth = RandomInts.randomIntBetween(random(), 5, 15);
+
+    iters = atLeast(50);
+    
+    for(int iter=0;iter<iters;iter++) {
+      GeoShape shape = randomShape(planetModel);
+
+      StringWriter sw = new StringWriter();
+      PrintWriter log = new PrintWriter(sw, true);
+
+      if (VERBOSE) {
+        log.println("TEST: iter=" + iter + " shape=" + shape);
+      }
+
+      XYZBounds bounds = new XYZBounds();
+      shape.getBounds(bounds);
+
+      // Start with the root cell that fully contains the shape:
+      Cell root = new Cell(null,
+                           Geo3DUtil.encodeValueLenient(planetMax, bounds.getMinimumX()),
+                           Geo3DUtil.encodeValueLenient(planetMax, bounds.getMaximumX()),
+                           Geo3DUtil.encodeValueLenient(planetMax, bounds.getMinimumY()),
+                           Geo3DUtil.encodeValueLenient(planetMax, bounds.getMaximumY()),
+                           Geo3DUtil.encodeValueLenient(planetMax, bounds.getMinimumZ()),
+                           Geo3DUtil.encodeValueLenient(planetMax, bounds.getMaximumZ()),
+                           0);
+
+      if (VERBOSE) {
+        log.println("  root cell: " + root);
+      }
+
+      List<Cell> queue = new ArrayList<>();
+      queue.add(root);
+      Set<Integer> hits = new HashSet<>();
+
+      while (queue.size() > 0) {
+        Cell cell = queue.get(queue.size()-1);
+        queue.remove(queue.size()-1);
+        if (VERBOSE) {
+          log.println("  cycle: " + cell + " queue.size()=" + queue.size());
+        }
+
+        if (random().nextInt(10) == 7 || cell.splitCount > recurseDepth) {
+          if (VERBOSE) {
+            log.println("    leaf");
+          }
+          // Leaf cell: brute force check all docs that fall within this cell:
+          for(int docID=0;docID<numDocs;docID++) {
+            GeoPoint point = docs[docID];
+            if (cell.contains(planetMax, point)) {
+              if (shape.isWithin(quantize(planetMax, point))) {
+                if (VERBOSE) {
+                  log.println("    check doc=" + docID + ": match!");
+                }
+                hits.add(docID);
+              } else {
+                if (VERBOSE) {
+                  log.println("    check doc=" + docID + ": no match");
+                }
+              }
+            }
+          }
+        } else {
+          
+          GeoArea xyzSolid = GeoAreaFactory.makeGeoArea(planetModel,
+                                                        Geo3DUtil.decodeValueMin(planetMax, cell.xMinEnc), Geo3DUtil.decodeValueMax(planetMax, cell.xMaxEnc),
+                                                        Geo3DUtil.decodeValueMin(planetMax, cell.yMinEnc), Geo3DUtil.decodeValueMax(planetMax, cell.yMaxEnc),
+                                                        Geo3DUtil.decodeValueMin(planetMax, cell.zMinEnc), Geo3DUtil.decodeValueMax(planetMax, cell.zMaxEnc));
+
+          if (VERBOSE) {
+            log.println("    minx="+Geo3DUtil.decodeValueMin(planetMax, cell.xMinEnc)+" maxx="+Geo3DUtil.decodeValueMax(planetMax, cell.xMaxEnc)+
+              " miny="+Geo3DUtil.decodeValueMin(planetMax, cell.yMinEnc)+" maxy="+Geo3DUtil.decodeValueMax(planetMax, cell.yMaxEnc)+
+              " minz="+Geo3DUtil.decodeValueMin(planetMax, cell.zMinEnc)+" maxz="+Geo3DUtil.decodeValueMax(planetMax, cell.zMaxEnc));
+          }
+
+          switch (xyzSolid.getRelationship(shape)) {          
+          case GeoArea.CONTAINS:
+            // Shape fully contains the cell: blindly add all docs in this cell:
+            if (VERBOSE) {
+              log.println("    GeoArea.CONTAINS: now addAll");
+            }
+            for(int docID=0;docID<numDocs;docID++) {
+              if (cell.contains(planetMax, docs[docID])) {
+                if (VERBOSE) {
+                  log.println("    addAll doc=" + docID);
+                }
+                hits.add(docID);
+              }
+            }
+            continue;
+          case GeoArea.OVERLAPS:
+            if (VERBOSE) {
+              log.println("    GeoArea.OVERLAPS: keep splitting");
+            }
+            // They do overlap but neither contains the other:
+            //log.println("    crosses1");
+            break;
+          case GeoArea.WITHIN:
+            if (VERBOSE) {
+              log.println("    GeoArea.WITHIN: keep splitting");
+            }
+            // Cell fully contains the shape:
+            //log.println("    crosses2");
+            break;
+          case GeoArea.DISJOINT:
+            // They do not overlap at all: don't recurse on this cell
+            //log.println("    outside");
+            if (VERBOSE) {
+              log.println("    GeoArea.DISJOINT: drop this cell");
+              for(int docID=0;docID<numDocs;docID++) {
+                if (cell.contains(planetMax, docs[docID])) {
+                  if (VERBOSE) {
+                    log.println("    skip doc=" + docID);
+                  }
+                }
+              }
+            }
+            continue;
+          default:
+            assert false;
+          }
+
+          // Randomly split:
+          switch(random().nextInt(3)) {
+
+          case 0:
+            // Split on X:
+            {
+              int splitValue = RandomInts.randomIntBetween(random(), cell.xMinEnc, cell.xMaxEnc);
+              if (VERBOSE) {
+                log.println("    now split on x=" + splitValue);
+              }
+              Cell cell1 = new Cell(cell,
+                                 cell.xMinEnc, splitValue,
+                                 cell.yMinEnc, cell.yMaxEnc,
+                                 cell.zMinEnc, cell.zMaxEnc,
+                                 cell.splitCount+1);
+              Cell cell2 = new Cell(cell,
+                                 splitValue, cell.xMaxEnc,
+                                 cell.yMinEnc, cell.yMaxEnc,
+                                 cell.zMinEnc, cell.zMaxEnc,
+                                 cell.splitCount+1);
+              if (VERBOSE) {
+                log.println("    split cell1: " + cell1);
+                log.println("    split cell2: " + cell2);
+              }
+              queue.add(cell1);
+              queue.add(cell2);
+            }
+            break;
+
+          case 1:
+            // Split on Y:
+            {
+              int splitValue = RandomInts.randomIntBetween(random(), cell.yMinEnc, cell.yMaxEnc);
+              if (VERBOSE) {
+                log.println("    now split on y=" + splitValue);
+              }
+              Cell cell1 = new Cell(cell,
+                                 cell.xMinEnc, cell.xMaxEnc,
+                                 cell.yMinEnc, splitValue,
+                                 cell.zMinEnc, cell.zMaxEnc,
+                                 cell.splitCount+1);
+              Cell cell2 = new Cell(cell,
+                                 cell.xMinEnc, cell.xMaxEnc,
+                                 splitValue, cell.yMaxEnc,
+                                 cell.zMinEnc, cell.zMaxEnc,
+                                 cell.splitCount+1);
+              if (VERBOSE) {
+                log.println("    split cell1: " + cell1);
+                log.println("    split cell2: " + cell2);
+              }
+              queue.add(cell1);
+              queue.add(cell2);
+            }
+            break;
+
+          case 2:
+            // Split on Z:
+            {
+              int splitValue = RandomInts.randomIntBetween(random(), cell.zMinEnc, cell.zMaxEnc);
+              if (VERBOSE) {
+                log.println("    now split on z=" + splitValue);
+              }
+              Cell cell1 = new Cell(cell,
+                                 cell.xMinEnc, cell.xMaxEnc,
+                                 cell.yMinEnc, cell.yMaxEnc,
+                                 cell.zMinEnc, splitValue,
+                                 cell.splitCount+1);
+              Cell cell2 = new Cell(cell,
+                                 cell.xMinEnc, cell.xMaxEnc,
+                                 cell.yMinEnc, cell.yMaxEnc,
+                                 splitValue, cell.zMaxEnc,
+                                 cell.splitCount+1);
+              if (VERBOSE) {
+                log.println("    split cell1: " + cell1);
+                log.println("    split cell2: " + cell2);
+              }
+              queue.add(cell1);
+              queue.add(cell2);
+            }
+            break;
+          }
+        }
+      }
+
+      if (VERBOSE) {
+        log.println("  " + hits.size() + " hits");
+      }
+
+      // Done matching, now verify:
+      boolean fail = false;
+      for(int docID=0;docID<numDocs;docID++) {
+        GeoPoint point = docs[docID];
+        GeoPoint quantized = quantize(planetMax, point);
+        boolean expected = shape.isWithin(quantized);
+
+        if (expected != shape.isWithin(point)) {
+          // Quantization changed the result; skip testing this doc:
+          continue;
+        }
+
+        boolean actual = hits.contains(docID);
+        if (actual != expected) {
+          if (actual) {
+            log.println("doc=" + docID + " matched but should not");
+          } else {
+            log.println("doc=" + docID + " did not match but should");
+          }
+          log.println("  point=" + docs[docID]);
+          log.println("  quantized=" + quantize(planetMax, docs[docID]));
+          fail = true;
+        }
+      }
+
+      if (fail) {
+        System.out.print(sw.toString());
+        fail("invalid hits for shape=" + shape);
+      }
+    }
+  }
+
+  public void testRandomTiny() throws Exception {
+    // Make sure single-leaf-node case is OK:
+    doTestRandom(10);
+  }
+
+  public void testRandomMedium() throws Exception {
+    doTestRandom(10000);
+  }
+
+  @Nightly
+  public void testRandomBig() throws Exception {
+    doTestRandom(200000);
+  }
+
+  private void doTestRandom(int count) throws Exception {
+    int numPoints = atLeast(count);
+
+    if (VERBOSE) {
+      System.err.println("TEST: numPoints=" + numPoints);
+    }
+
+    double[] lats = new double[numPoints];
+    double[] lons = new double[numPoints];
+
+    boolean haveRealDoc = false;
+
+    for (int docID=0;docID<numPoints;docID++) {
+      int x = random().nextInt(20);
+      if (x == 17) {
+        // Some docs don't have a point:
+        lats[docID] = Double.NaN;
+        if (VERBOSE) {
+          System.err.println("  doc=" + docID + " is missing");
+        }
+        continue;
+      }
+
+      if (docID > 0 && x < 3 && haveRealDoc) {
+        int oldDocID;
+        while (true) {
+          oldDocID = random().nextInt(docID);
+          if (Double.isNaN(lats[oldDocID]) == false) {
+            break;
+          }
+        }
+            
+        if (x == 0) {
+          // Identical lat to old point
+          lats[docID] = lats[oldDocID];
+          lons[docID] = toRadians(randomLon());
+          if (VERBOSE) {
+            System.err.println("  doc=" + docID + " lat=" + lats[docID] + " lon=" + lons[docID] + " (same lat as doc=" + oldDocID + ")");
+          }
+        } else if (x == 1) {
+          // Identical lon to old point
+          lats[docID] = toRadians(randomLat());
+          lons[docID] = lons[oldDocID];
+          if (VERBOSE) {
+            System.err.println("  doc=" + docID + " lat=" + lats[docID] + " lon=" + lons[docID] + " (same lon as doc=" + oldDocID + ")");
+          }
+        } else {
+          assert x == 2;
+          // Fully identical point:
+          lats[docID] = lats[oldDocID];
+          lons[docID] = lons[oldDocID];
+          if (VERBOSE) {
+            System.err.println("  doc=" + docID + " lat=" + lats[docID] + " lon=" + lons[docID] + " (same lat/lon as doc=" + oldDocID + ")");
+          }
+        }
+      } else {
+        lats[docID] = toRadians(randomLat());
+        lons[docID] = toRadians(randomLon());
+        haveRealDoc = true;
+        if (VERBOSE) {
+          System.err.println("  doc=" + docID + " lat=" + lats[docID] + " lon=" + lons[docID]);
+        }
+      }
+    }
+
+    verify(lats, lons);
+  }
+
+  private static double randomLat() {
+    if (smallBBox) {
+      return 2.0 * (random().nextDouble()-0.5);
+    } else {
+      return -90 + 180.0 * random().nextDouble();
+    }
+  }
+
+  private static double randomLon() {
+    if (smallBBox) {
+      return 2.0 * (random().nextDouble()-0.5);
+    } else {
+      return -180 + 360.0 * random().nextDouble();
+    }
+  }
+
+  // Poached from Geo3dRptTest.randomShape:
+  private static GeoShape randomShape(PlanetModel planetModel) {
+    while (true) {
+      final int shapeType = random().nextInt(4);
+      switch (shapeType) {
+      case 0: {
+        // Polygons
+        final int vertexCount = random().nextInt(3) + 3;
+        final List<GeoPoint> geoPoints = new ArrayList<>();
+        while (geoPoints.size() < vertexCount) {
+          final GeoPoint gPt = new GeoPoint(planetModel, toRadians(randomLat()), toRadians(randomLon()));
+          geoPoints.add(gPt);
+        }
+        final int convexPointIndex = random().nextInt(vertexCount);       //If we get this wrong, hopefully we get IllegalArgumentException
+        try {
+          return GeoPolygonFactory.makeGeoPolygon(planetModel, geoPoints, convexPointIndex);
+        } catch (IllegalArgumentException e) {
+          // This is what happens when we create a shape that is invalid.  Although it is conceivable that there are cases where
+          // the exception is thrown incorrectly, we aren't going to be able to do that in this random test.
+          continue;
+        }
+      }
+
+      case 1: {
+        // Circles
+
+        double lat = toRadians(randomLat());
+        double lon = toRadians(randomLon());
+
+        double angle;
+        if (smallBBox) {
+          angle = random().nextDouble() * Math.PI/360.0;
+        } else {
+          angle = random().nextDouble() * Math.PI/2.0;
+        }
+
+        try {
+          return GeoCircleFactory.makeGeoCircle(planetModel, lat, lon, angle);
+        } catch (IllegalArgumentException iae) {
+          // angle is too small; try again:
+          continue;
+        }
+      }
+
+      case 2: {
+        // Rectangles
+        double lat0 = toRadians(randomLat());
+        double lat1 = toRadians(randomLat());
+        if (lat1 < lat0) {
+          double x = lat0;
+          lat0 = lat1;
+          lat1 = x;
+        }
+        double lon0 = toRadians(randomLon());
+        double lon1 = toRadians(randomLon());
+        if (lon1 < lon0) {
+          double x = lon0;
+          lon0 = lon1;
+          lon1 = x;
+        }
+
+        return GeoBBoxFactory.makeGeoBBox(planetModel, lat1, lat0, lon0, lon1);
+      }
+
+      case 3: {
+        // Paths
+        final int pointCount = random().nextInt(5) + 1;
+        final double width = toRadians(random().nextInt(89)+1);
+        try {
+          final GeoPath path = new GeoPath(planetModel, width);
+          for (int i = 0; i < pointCount; i++) {
+            path.addPoint(toRadians(randomLat()), toRadians(randomLon()));
+          }
+          path.done();
+          return path;
+        } catch (IllegalArgumentException e) {
+          // This is what happens when we create a shape that is invalid.  Although it is conceivable that there are cases where
+          // the exception is thrown incorrectly, we aren't going to be able to do that in this random test.
+          continue;
+        }
+      }
+
+      default:
+        throw new IllegalStateException("Unexpected shape type");
+      }
+    }
+  }
+
+  private static void verify(double[] lats, double[] lons) throws Exception {
+    IndexWriterConfig iwc = newIndexWriterConfig();
+
+    PlanetModel planetModel = getPlanetModel();
+
+    // Else we can get O(N^2) merging:
+    int mbd = iwc.getMaxBufferedDocs();
+    if (mbd != -1 && mbd < lats.length/100) {
+      iwc.setMaxBufferedDocs(lats.length/100);
+    }
+    iwc.setCodec(getCodec());
+    Directory dir;
+    if (lats.length > 100000) {
+      dir = noVirusChecker(newFSDirectory(createTempDir("TestBKDTree")));
+    } else {
+      dir = getDirectory();
+    }
+    Set<Integer> deleted = new HashSet<>();
+    // RandomIndexWriter is too slow here:
+    IndexWriter w = new IndexWriter(dir, iwc);
+    for(int id=0;id<lats.length;id++) {
+      Document doc = new Document();
+      doc.add(newStringField("id", ""+id, Field.Store.NO));
+      doc.add(new NumericDocValuesField("id", id));
+      if (Double.isNaN(lats[id]) == false) {
+        doc.add(new Geo3DPoint("point", planetModel, lats[id], lons[id]));
+      }
+      w.addDocument(doc);
+      if (id > 0 && random().nextInt(100) == 42) {
+        int idToDelete = random().nextInt(id);
+        w.deleteDocuments(new Term("id", ""+idToDelete));
+        deleted.add(idToDelete);
+        if (VERBOSE) {
+          System.err.println("  delete id=" + idToDelete);
+        }
+      }
+    }
+    if (random().nextBoolean()) {
+      w.forceMerge(1);
+    }
+    final IndexReader r = DirectoryReader.open(w, true);
+    w.close();
+
+    // We can't wrap with "exotic" readers because the geo3d query must see the Geo3DDVFormat:
+    IndexSearcher s = newSearcher(r, false);
+
+    int numThreads = TestUtil.nextInt(random(), 2, 5);
+
+    List<Thread> threads = new ArrayList<>();
+    final int iters = atLeast(100);
+
+    final CountDownLatch startingGun = new CountDownLatch(1);
+    final AtomicBoolean failed = new AtomicBoolean();
+
+    for(int i=0;i<numThreads;i++) {
+      Thread thread = new Thread() {
+          @Override
+          public void run() {
+            try {
+              _run();
+            } catch (Exception e) {
+              failed.set(true);
+              throw new RuntimeException(e);
+            }
+          }
+
+          private void _run() throws Exception {
+            startingGun.await();
+
+            NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, "id");
+
+            for (int iter=0;iter<iters && failed.get() == false;iter++) {
+
+              GeoShape shape = randomShape(planetModel);
+
+              if (VERBOSE) {
+                System.err.println("\n" + Thread.currentThread() + ": TEST: iter=" + iter + " shape="+shape);
+              }
+              
+              Query query = new PointInGeo3DShapeQuery(planetModel, "point", shape);
+
+              if (VERBOSE) {
+                System.err.println("  using query: " + query);
+              }
+
+              final FixedBitSet hits = new FixedBitSet(r.maxDoc());
+
+              s.search(query, new SimpleCollector() {
+
+                  private int docBase;
+
+                  @Override
+                  public boolean needsScores() {
+                    return false;
+                  }
+
+                  @Override
+                  protected void doSetNextReader(LeafReaderContext context) throws IOException {
+                    docBase = context.docBase;
+                  }
+
+                  @Override
+                  public void collect(int doc) {
+                    hits.set(docBase+doc);
+                  }
+                });
+
+              if (VERBOSE) {
+                System.err.println("  hitCount: " + hits.cardinality());
+              }
+      
+              for(int docID=0;docID<r.maxDoc();docID++) {
+                int id = (int) docIDToID.get(docID);
+                if (Double.isNaN(lats[id]) == false) {
+
+                  // Accurate point:
+                  GeoPoint point1 = new GeoPoint(planetModel, lats[id], lons[id]);
+
+                  // Quantized point (32 bits per dim):
+                  GeoPoint point2 = quantize(planetModel.getMaximumMagnitude(), point1);
+
+                  if (shape.isWithin(point1) != shape.isWithin(point2)) {
+                    if (VERBOSE) {
+                      System.out.println("  skip checking docID=" + docID + " quantization changed the expected result from " + shape.isWithin(point1) + " to " + shape.isWithin(point2));
+                    }
+                    continue;
+                  }
+
+                  boolean expected = ((deleted.contains(id) == false) && shape.isWithin(point2));
+                  if (hits.get(docID) != expected) {
+                    fail(Thread.currentThread().getName() + ": iter=" + iter + " id=" + id + " docID=" + docID + " lat=" + lats[id] + " lon=" + lons[id] + " expected " + expected + " but got: " + hits.get(docID) + " deleted?=" + deleted.contains(id) + "\n  point1=" + point1 + ", iswithin="+shape.isWithin(point1)+"\n  point2=" + point2 + ", iswithin="+shape.isWithin(point2) + "\n  query=" + query);
+                  }
+                } else {
+                  assertFalse(hits.get(docID));
+                }
+
+              }
+            }
+          }
+        };
+      thread.setName("T" + i);
+      thread.start();
+      threads.add(thread);
+    }
+    startingGun.countDown();
+    for(Thread thread : threads) {
+      thread.join();
+    }
+    IOUtils.close(r, dir);
+  }
+
+  private static Directory noVirusChecker(Directory dir) {
+    if (dir instanceof MockDirectoryWrapper) {
+      ((MockDirectoryWrapper) dir).setEnableVirusScanner(false);
+    }
+    return dir;
+  }
+
+  private static Directory getDirectory() {     
+    return noVirusChecker(newDirectory());
+  }
+}
diff --git a/lucene/spatial3d/src/test/org/apache/lucene/geo3d/TestGeo3DPointField.java b/lucene/spatial3d/src/test/org/apache/lucene/geo3d/TestGeo3DPointField.java
deleted file mode 100644
index 64e92a4..0000000
--- a/lucene/spatial3d/src/test/org/apache/lucene/geo3d/TestGeo3DPointField.java
+++ /dev/null
@@ -1,802 +0,0 @@
-package org.apache.lucene.geo3d;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.io.StringWriter;
-import java.util.ArrayList;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Set;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.atomic.AtomicBoolean;
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.DimensionalFormat;
-import org.apache.lucene.codecs.DimensionalReader;
-import org.apache.lucene.codecs.DimensionalWriter;
-import org.apache.lucene.codecs.FilterCodec;
-import org.apache.lucene.codecs.lucene60.Lucene60DimensionalReader;
-import org.apache.lucene.codecs.lucene60.Lucene60DimensionalWriter;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.NumericDocValuesField;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.LeafReaderContext;
-import org.apache.lucene.index.MultiDocValues;
-import org.apache.lucene.index.NumericDocValues;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.SimpleCollector;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.MockDirectoryWrapper;
-import org.apache.lucene.util.FixedBitSet;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.TestUtil;
-import org.junit.BeforeClass;
-
-import com.carrotsearch.randomizedtesting.generators.RandomInts;
-
-public class TestGeo3DPointField extends LuceneTestCase {
-
-  private static boolean smallBBox;
-  
-  @BeforeClass
-  public static void beforeClass() {
-    smallBBox = random().nextBoolean();
-    if (VERBOSE) {
-      System.err.println("TEST: smallBBox=" + smallBBox);
-    }
-  }
-
-  private static Codec getCodec() {
-    if (Codec.getDefault().getName().equals("Lucene60")) {
-      int maxPointsInLeafNode = TestUtil.nextInt(random(), 16, 2048);
-      double maxMBSortInHeap = 3.0 + (3*random().nextDouble());
-      if (VERBOSE) {
-        System.out.println("TEST: using Lucene60DimensionalFormat with maxPointsInLeafNode=" + maxPointsInLeafNode + " and maxMBSortInHeap=" + maxMBSortInHeap);
-      }
-
-      return new FilterCodec("Lucene60", Codec.getDefault()) {
-        @Override
-        public DimensionalFormat dimensionalFormat() {
-          return new DimensionalFormat() {
-            @Override
-            public DimensionalWriter fieldsWriter(SegmentWriteState writeState) throws IOException {
-              return new Lucene60DimensionalWriter(writeState, maxPointsInLeafNode, maxMBSortInHeap);
-            }
-
-            @Override
-            public DimensionalReader fieldsReader(SegmentReadState readState) throws IOException {
-              return new Lucene60DimensionalReader(readState);
-            }
-          };
-        }
-      };
-    } else {
-      return Codec.getDefault();
-    }
-  }
-
-  public void testBasic() throws Exception {
-    Directory dir = getDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig();
-    iwc.setCodec(getCodec());
-    IndexWriter w = new IndexWriter(dir, iwc);
-    Document doc = new Document();
-    doc.add(new Geo3DPointField("field", PlanetModel.WGS84, toRadians(50.7345267), toRadians(-97.5303555)));
-    w.addDocument(doc);
-    IndexReader r = DirectoryReader.open(w, true);
-    // We can't wrap with "exotic" readers because the query must see the BKD3DDVFormat:
-    IndexSearcher s = newSearcher(r, false);
-    assertEquals(1, s.search(new PointInGeo3DShapeQuery(PlanetModel.WGS84,
-                                                        "field",
-                                                        GeoCircleFactory.makeGeoCircle(PlanetModel.WGS84, toRadians(50), toRadians(-97), Math.PI/180.)), 1).totalHits);
-    w.close();
-    r.close();
-    dir.close();
-  }
-
-  private static double toRadians(double degrees) {
-    return Math.PI*(degrees/360.0);
-  }
-
-  private static PlanetModel getPlanetModel() {
-    if (random().nextBoolean()) {
-      // Use one of the earth models:
-      if (random().nextBoolean()) {
-        return PlanetModel.WGS84;
-      } else {
-        return PlanetModel.SPHERE;
-      }
-    } else {
-      // Make a randomly squashed planet:
-      double oblateness = random().nextDouble() * 0.5 - 0.25;
-      return new PlanetModel(1.0 + oblateness, 1.0 - oblateness);
-    }
-  }
-
-  private static class Cell {
-    static int nextCellID;
-
-    final Cell parent;
-    final int cellID;
-    final int xMinEnc, xMaxEnc;
-    final int yMinEnc, yMaxEnc;
-    final int zMinEnc, zMaxEnc;
-    final int splitCount;
-
-    public Cell(Cell parent,
-                int xMinEnc, int xMaxEnc,
-                int yMinEnc, int yMaxEnc,
-                int zMinEnc, int zMaxEnc,
-                int splitCount) {
-      this.parent = parent;
-      this.xMinEnc = xMinEnc;
-      this.xMaxEnc = xMaxEnc;
-      this.yMinEnc = yMinEnc;
-      this.yMaxEnc = yMaxEnc;
-      this.zMinEnc = zMinEnc;
-      this.zMaxEnc = zMaxEnc;
-      this.cellID = nextCellID++;
-      this.splitCount = splitCount;
-    }
-
-    /** Returns true if the quantized point lies within this cell, inclusive on all bounds. */
-    public boolean contains(double planetMax, GeoPoint point) {
-      int docX = Geo3DUtil.encodeValue(planetMax, point.x);
-      int docY = Geo3DUtil.encodeValue(planetMax, point.y);
-      int docZ = Geo3DUtil.encodeValue(planetMax, point.z);
-
-      return docX >= xMinEnc && docX <= xMaxEnc &&
-        docY >= yMinEnc && docY <= yMaxEnc && 
-        docZ >= zMinEnc && docZ <= zMaxEnc;
-    }
-
-    @Override
-    public String toString() {
-      return "cell=" + cellID + (parent == null ? "" : " parentCellID=" + parent.cellID) + " x: " + xMinEnc + " TO " + xMaxEnc + ", y: " + yMinEnc + " TO " + yMaxEnc + ", z: " + zMinEnc + " TO " + zMaxEnc + ", splits: " + splitCount;
-    }
-  }
-
-  private static GeoPoint quantize(double planetMax, GeoPoint point) {
-    return new GeoPoint(Geo3DUtil.decodeValueCenter(planetMax, Geo3DUtil.encodeValue(planetMax, point.x)),
-                        Geo3DUtil.decodeValueCenter(planetMax, Geo3DUtil.encodeValue(planetMax, point.y)),
-                        Geo3DUtil.decodeValueCenter(planetMax, Geo3DUtil.encodeValue(planetMax, point.z)));
-  }
-
-  /** Tests consistency of GeoArea.getRelationship vs GeoShape.isWithin */
-  public void testGeo3DRelations() throws Exception {
-
-    PlanetModel planetModel = getPlanetModel();
-
-    int numDocs = atLeast(1000);
-    if (VERBOSE) {
-      System.out.println("TEST: " + numDocs + " docs");
-    }
-
-    GeoPoint[] docs = new GeoPoint[numDocs];
-    for(int docID=0;docID<numDocs;docID++) {
-      docs[docID] = new GeoPoint(planetModel, toRadians(randomLat()), toRadians(randomLon()));
-      if (VERBOSE) {
-        System.out.println("  doc=" + docID + ": " + docs[docID]);
-      }
-    }
-
-    double planetMax = planetModel.getMaximumMagnitude();
-
-    int iters = atLeast(10);
-
-    int recurseDepth = RandomInts.randomIntBetween(random(), 5, 15);
-
-    iters = atLeast(50);
-    
-    for(int iter=0;iter<iters;iter++) {
-      GeoShape shape = randomShape(planetModel);
-
-      StringWriter sw = new StringWriter();
-      PrintWriter log = new PrintWriter(sw, true);
-
-      if (VERBOSE) {
-        log.println("TEST: iter=" + iter + " shape=" + shape);
-      }
-
-      XYZBounds bounds = new XYZBounds();
-      shape.getBounds(bounds);
-
-      // Start with the root cell that fully contains the shape:
-      Cell root = new Cell(null,
-                           Geo3DUtil.encodeValueLenient(planetMax, bounds.getMinimumX()),
-                           Geo3DUtil.encodeValueLenient(planetMax, bounds.getMaximumX()),
-                           Geo3DUtil.encodeValueLenient(planetMax, bounds.getMinimumY()),
-                           Geo3DUtil.encodeValueLenient(planetMax, bounds.getMaximumY()),
-                           Geo3DUtil.encodeValueLenient(planetMax, bounds.getMinimumZ()),
-                           Geo3DUtil.encodeValueLenient(planetMax, bounds.getMaximumZ()),
-                           0);
-
-      if (VERBOSE) {
-        log.println("  root cell: " + root);
-      }
-
-      List<Cell> queue = new ArrayList<>();
-      queue.add(root);
-      Set<Integer> hits = new HashSet<>();
-
-      while (queue.size() > 0) {
-        Cell cell = queue.get(queue.size()-1);
-        queue.remove(queue.size()-1);
-        if (VERBOSE) {
-          log.println("  cycle: " + cell + " queue.size()=" + queue.size());
-        }
-
-        if (random().nextInt(10) == 7 || cell.splitCount > recurseDepth) {
-          if (VERBOSE) {
-            log.println("    leaf");
-          }
-          // Leaf cell: brute force check all docs that fall within this cell:
-          for(int docID=0;docID<numDocs;docID++) {
-            GeoPoint point = docs[docID];
-            if (cell.contains(planetMax, point)) {
-              if (shape.isWithin(quantize(planetMax, point))) {
-                if (VERBOSE) {
-                  log.println("    check doc=" + docID + ": match!");
-                }
-                hits.add(docID);
-              } else {
-                if (VERBOSE) {
-                  log.println("    check doc=" + docID + ": no match");
-                }
-              }
-            }
-          }
-        } else {
-          
-          GeoArea xyzSolid = GeoAreaFactory.makeGeoArea(planetModel,
-                                                        Geo3DUtil.decodeValueMin(planetMax, cell.xMinEnc), Geo3DUtil.decodeValueMax(planetMax, cell.xMaxEnc),
-                                                        Geo3DUtil.decodeValueMin(planetMax, cell.yMinEnc), Geo3DUtil.decodeValueMax(planetMax, cell.yMaxEnc),
-                                                        Geo3DUtil.decodeValueMin(planetMax, cell.zMinEnc), Geo3DUtil.decodeValueMax(planetMax, cell.zMaxEnc));
-
-          if (VERBOSE) {
-            log.println("    minx="+Geo3DUtil.decodeValueMin(planetMax, cell.xMinEnc)+" maxx="+Geo3DUtil.decodeValueMax(planetMax, cell.xMaxEnc)+
-              " miny="+Geo3DUtil.decodeValueMin(planetMax, cell.yMinEnc)+" maxy="+Geo3DUtil.decodeValueMax(planetMax, cell.yMaxEnc)+
-              " minz="+Geo3DUtil.decodeValueMin(planetMax, cell.zMinEnc)+" maxz="+Geo3DUtil.decodeValueMax(planetMax, cell.zMaxEnc));
-          }
-
-          switch (xyzSolid.getRelationship(shape)) {          
-          case GeoArea.CONTAINS:
-            // Shape fully contains the cell: blindly add all docs in this cell:
-            if (VERBOSE) {
-              log.println("    GeoArea.CONTAINS: now addAll");
-            }
-            for(int docID=0;docID<numDocs;docID++) {
-              if (cell.contains(planetMax, docs[docID])) {
-                if (VERBOSE) {
-                  log.println("    addAll doc=" + docID);
-                }
-                hits.add(docID);
-              }
-            }
-            continue;
-          case GeoArea.OVERLAPS:
-            if (VERBOSE) {
-              log.println("    GeoArea.OVERLAPS: keep splitting");
-            }
-            // They do overlap but neither contains the other:
-            //log.println("    crosses1");
-            break;
-          case GeoArea.WITHIN:
-            if (VERBOSE) {
-              log.println("    GeoArea.WITHIN: keep splitting");
-            }
-            // Cell fully contains the shape:
-            //log.println("    crosses2");
-            break;
-          case GeoArea.DISJOINT:
-            // They do not overlap at all: don't recurse on this cell
-            //log.println("    outside");
-            if (VERBOSE) {
-              log.println("    GeoArea.DISJOINT: drop this cell");
-              for(int docID=0;docID<numDocs;docID++) {
-                if (cell.contains(planetMax, docs[docID])) {
-                  if (VERBOSE) {
-                    log.println("    skip doc=" + docID);
-                  }
-                }
-              }
-            }
-            continue;
-          default:
-            assert false;
-          }
-
-          // Randomly split:
-          switch(random().nextInt(3)) {
-
-          case 0:
-            // Split on X:
-            {
-              int splitValue = RandomInts.randomIntBetween(random(), cell.xMinEnc, cell.xMaxEnc);
-              if (VERBOSE) {
-                log.println("    now split on x=" + splitValue);
-              }
-              Cell cell1 = new Cell(cell,
-                                 cell.xMinEnc, splitValue,
-                                 cell.yMinEnc, cell.yMaxEnc,
-                                 cell.zMinEnc, cell.zMaxEnc,
-                                 cell.splitCount+1);
-              Cell cell2 = new Cell(cell,
-                                 splitValue, cell.xMaxEnc,
-                                 cell.yMinEnc, cell.yMaxEnc,
-                                 cell.zMinEnc, cell.zMaxEnc,
-                                 cell.splitCount+1);
-              if (VERBOSE) {
-                log.println("    split cell1: " + cell1);
-                log.println("    split cell2: " + cell2);
-              }
-              queue.add(cell1);
-              queue.add(cell2);
-            }
-            break;
-
-          case 1:
-            // Split on Y:
-            {
-              int splitValue = RandomInts.randomIntBetween(random(), cell.yMinEnc, cell.yMaxEnc);
-              if (VERBOSE) {
-                log.println("    now split on y=" + splitValue);
-              }
-              Cell cell1 = new Cell(cell,
-                                 cell.xMinEnc, cell.xMaxEnc,
-                                 cell.yMinEnc, splitValue,
-                                 cell.zMinEnc, cell.zMaxEnc,
-                                 cell.splitCount+1);
-              Cell cell2 = new Cell(cell,
-                                 cell.xMinEnc, cell.xMaxEnc,
-                                 splitValue, cell.yMaxEnc,
-                                 cell.zMinEnc, cell.zMaxEnc,
-                                 cell.splitCount+1);
-              if (VERBOSE) {
-                log.println("    split cell1: " + cell1);
-                log.println("    split cell2: " + cell2);
-              }
-              queue.add(cell1);
-              queue.add(cell2);
-            }
-            break;
-
-          case 2:
-            // Split on Z:
-            {
-              int splitValue = RandomInts.randomIntBetween(random(), cell.zMinEnc, cell.zMaxEnc);
-              if (VERBOSE) {
-                log.println("    now split on z=" + splitValue);
-              }
-              Cell cell1 = new Cell(cell,
-                                 cell.xMinEnc, cell.xMaxEnc,
-                                 cell.yMinEnc, cell.yMaxEnc,
-                                 cell.zMinEnc, splitValue,
-                                 cell.splitCount+1);
-              Cell cell2 = new Cell(cell,
-                                 cell.xMinEnc, cell.xMaxEnc,
-                                 cell.yMinEnc, cell.yMaxEnc,
-                                 splitValue, cell.zMaxEnc,
-                                 cell.splitCount+1);
-              if (VERBOSE) {
-                log.println("    split cell1: " + cell1);
-                log.println("    split cell2: " + cell2);
-              }
-              queue.add(cell1);
-              queue.add(cell2);
-            }
-            break;
-          }
-        }
-      }
-
-      if (VERBOSE) {
-        log.println("  " + hits.size() + " hits");
-      }
-
-      // Done matching, now verify:
-      boolean fail = false;
-      for(int docID=0;docID<numDocs;docID++) {
-        GeoPoint point = docs[docID];
-        GeoPoint quantized = quantize(planetMax, point);
-        boolean expected = shape.isWithin(quantized);
-
-        if (expected != shape.isWithin(point)) {
-          // Quantization changed the result; skip testing this doc:
-          continue;
-        }
-
-        boolean actual = hits.contains(docID);
-        if (actual != expected) {
-          if (actual) {
-            log.println("doc=" + docID + " matched but should not");
-          } else {
-            log.println("doc=" + docID + " did not match but should");
-          }
-          log.println("  point=" + docs[docID]);
-          log.println("  quantized=" + quantize(planetMax, docs[docID]));
-          fail = true;
-        }
-      }
-
-      if (fail) {
-        System.out.print(sw.toString());
-        fail("invalid hits for shape=" + shape);
-      }
-    }
-  }
-
-  public void testRandomTiny() throws Exception {
-    // Make sure single-leaf-node case is OK:
-    doTestRandom(10);
-  }
-
-  public void testRandomMedium() throws Exception {
-    doTestRandom(10000);
-  }
-
-  @Nightly
-  public void testRandomBig() throws Exception {
-    doTestRandom(200000);
-  }
-
-  private void doTestRandom(int count) throws Exception {
-    int numPoints = atLeast(count);
-
-    if (VERBOSE) {
-      System.err.println("TEST: numPoints=" + numPoints);
-    }
-
-    double[] lats = new double[numPoints];
-    double[] lons = new double[numPoints];
-
-    boolean haveRealDoc = false;
-
-    for (int docID=0;docID<numPoints;docID++) {
-      int x = random().nextInt(20);
-      if (x == 17) {
-        // Some docs don't have a point:
-        lats[docID] = Double.NaN;
-        if (VERBOSE) {
-          System.err.println("  doc=" + docID + " is missing");
-        }
-        continue;
-      }
-
-      if (docID > 0 && x < 3 && haveRealDoc) {
-        int oldDocID;
-        while (true) {
-          oldDocID = random().nextInt(docID);
-          if (Double.isNaN(lats[oldDocID]) == false) {
-            break;
-          }
-        }
-            
-        if (x == 0) {
-          // Identical lat to old point
-          lats[docID] = lats[oldDocID];
-          lons[docID] = toRadians(randomLon());
-          if (VERBOSE) {
-            System.err.println("  doc=" + docID + " lat=" + lats[docID] + " lon=" + lons[docID] + " (same lat as doc=" + oldDocID + ")");
-          }
-        } else if (x == 1) {
-          // Identical lon to old point
-          lats[docID] = toRadians(randomLat());
-          lons[docID] = lons[oldDocID];
-          if (VERBOSE) {
-            System.err.println("  doc=" + docID + " lat=" + lats[docID] + " lon=" + lons[docID] + " (same lon as doc=" + oldDocID + ")");
-          }
-        } else {
-          assert x == 2;
-          // Fully identical point:
-          lats[docID] = lats[oldDocID];
-          lons[docID] = lons[oldDocID];
-          if (VERBOSE) {
-            System.err.println("  doc=" + docID + " lat=" + lats[docID] + " lon=" + lons[docID] + " (same lat/lon as doc=" + oldDocID + ")");
-          }
-        }
-      } else {
-        lats[docID] = toRadians(randomLat());
-        lons[docID] = toRadians(randomLon());
-        haveRealDoc = true;
-        if (VERBOSE) {
-          System.err.println("  doc=" + docID + " lat=" + lats[docID] + " lon=" + lons[docID]);
-        }
-      }
-    }
-
-    verify(lats, lons);
-  }
-
-  private static double randomLat() {
-    if (smallBBox) {
-      return 2.0 * (random().nextDouble()-0.5);
-    } else {
-      return -90 + 180.0 * random().nextDouble();
-    }
-  }
-
-  private static double randomLon() {
-    if (smallBBox) {
-      return 2.0 * (random().nextDouble()-0.5);
-    } else {
-      return -180 + 360.0 * random().nextDouble();
-    }
-  }
-
-  // Poached from Geo3dRptTest.randomShape:
-  private static GeoShape randomShape(PlanetModel planetModel) {
-    while (true) {
-      final int shapeType = random().nextInt(4);
-      switch (shapeType) {
-      case 0: {
-        // Polygons
-        final int vertexCount = random().nextInt(3) + 3;
-        final List<GeoPoint> geoPoints = new ArrayList<>();
-        while (geoPoints.size() < vertexCount) {
-          final GeoPoint gPt = new GeoPoint(planetModel, toRadians(randomLat()), toRadians(randomLon()));
-          geoPoints.add(gPt);
-        }
-        final int convexPointIndex = random().nextInt(vertexCount);       //If we get this wrong, hopefully we get IllegalArgumentException
-        try {
-          return GeoPolygonFactory.makeGeoPolygon(planetModel, geoPoints, convexPointIndex);
-        } catch (IllegalArgumentException e) {
-          // This is what happens when we create a shape that is invalid.  Although it is conceivable that there are cases where
-          // the exception is thrown incorrectly, we aren't going to be able to do that in this random test.
-          continue;
-        }
-      }
-
-      case 1: {
-        // Circles
-
-        double lat = toRadians(randomLat());
-        double lon = toRadians(randomLon());
-
-        double angle;
-        if (smallBBox) {
-          angle = random().nextDouble() * Math.PI/360.0;
-        } else {
-          angle = random().nextDouble() * Math.PI/2.0;
-        }
-
-        try {
-          return GeoCircleFactory.makeGeoCircle(planetModel, lat, lon, angle);
-        } catch (IllegalArgumentException iae) {
-          // angle is too small; try again:
-          continue;
-        }
-      }
-
-      case 2: {
-        // Rectangles
-        double lat0 = toRadians(randomLat());
-        double lat1 = toRadians(randomLat());
-        if (lat1 < lat0) {
-          double x = lat0;
-          lat0 = lat1;
-          lat1 = x;
-        }
-        double lon0 = toRadians(randomLon());
-        double lon1 = toRadians(randomLon());
-        if (lon1 < lon0) {
-          double x = lon0;
-          lon0 = lon1;
-          lon1 = x;
-        }
-
-        return GeoBBoxFactory.makeGeoBBox(planetModel, lat1, lat0, lon0, lon1);
-      }
-
-      case 3: {
-        // Paths
-        final int pointCount = random().nextInt(5) + 1;
-        final double width = toRadians(random().nextInt(89)+1);
-        try {
-          final GeoPath path = new GeoPath(planetModel, width);
-          for (int i = 0; i < pointCount; i++) {
-            path.addPoint(toRadians(randomLat()), toRadians(randomLon()));
-          }
-          path.done();
-          return path;
-        } catch (IllegalArgumentException e) {
-          // This is what happens when we create a shape that is invalid.  Although it is conceivable that there are cases where
-          // the exception is thrown incorrectly, we aren't going to be able to do that in this random test.
-          continue;
-        }
-      }
-
-      default:
-        throw new IllegalStateException("Unexpected shape type");
-      }
-    }
-  }
-
-  private static void verify(double[] lats, double[] lons) throws Exception {
-    IndexWriterConfig iwc = newIndexWriterConfig();
-
-    PlanetModel planetModel = getPlanetModel();
-
-    // Else we can get O(N^2) merging:
-    int mbd = iwc.getMaxBufferedDocs();
-    if (mbd != -1 && mbd < lats.length/100) {
-      iwc.setMaxBufferedDocs(lats.length/100);
-    }
-    iwc.setCodec(getCodec());
-    Directory dir;
-    if (lats.length > 100000) {
-      dir = noVirusChecker(newFSDirectory(createTempDir("TestBKDTree")));
-    } else {
-      dir = getDirectory();
-    }
-    Set<Integer> deleted = new HashSet<>();
-    // RandomIndexWriter is too slow here:
-    IndexWriter w = new IndexWriter(dir, iwc);
-    for(int id=0;id<lats.length;id++) {
-      Document doc = new Document();
-      doc.add(newStringField("id", ""+id, Field.Store.NO));
-      doc.add(new NumericDocValuesField("id", id));
-      if (Double.isNaN(lats[id]) == false) {
-        doc.add(new Geo3DPointField("point", planetModel, lats[id], lons[id]));
-      }
-      w.addDocument(doc);
-      if (id > 0 && random().nextInt(100) == 42) {
-        int idToDelete = random().nextInt(id);
-        w.deleteDocuments(new Term("id", ""+idToDelete));
-        deleted.add(idToDelete);
-        if (VERBOSE) {
-          System.err.println("  delete id=" + idToDelete);
-        }
-      }
-    }
-    if (random().nextBoolean()) {
-      w.forceMerge(1);
-    }
-    final IndexReader r = DirectoryReader.open(w, true);
-    w.close();
-
-    // We can't wrap with "exotic" readers because the geo3d query must see the Geo3DDVFormat:
-    IndexSearcher s = newSearcher(r, false);
-
-    int numThreads = TestUtil.nextInt(random(), 2, 5);
-
-    List<Thread> threads = new ArrayList<>();
-    final int iters = atLeast(100);
-
-    final CountDownLatch startingGun = new CountDownLatch(1);
-    final AtomicBoolean failed = new AtomicBoolean();
-
-    for(int i=0;i<numThreads;i++) {
-      Thread thread = new Thread() {
-          @Override
-          public void run() {
-            try {
-              _run();
-            } catch (Exception e) {
-              failed.set(true);
-              throw new RuntimeException(e);
-            }
-          }
-
-          private void _run() throws Exception {
-            startingGun.await();
-
-            NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, "id");
-
-            for (int iter=0;iter<iters && failed.get() == false;iter++) {
-
-              GeoShape shape = randomShape(planetModel);
-
-              if (VERBOSE) {
-                System.err.println("\n" + Thread.currentThread() + ": TEST: iter=" + iter + " shape="+shape);
-              }
-              
-              Query query = new PointInGeo3DShapeQuery(planetModel, "point", shape);
-
-              if (VERBOSE) {
-                System.err.println("  using query: " + query);
-              }
-
-              final FixedBitSet hits = new FixedBitSet(r.maxDoc());
-
-              s.search(query, new SimpleCollector() {
-
-                  private int docBase;
-
-                  @Override
-                  public boolean needsScores() {
-                    return false;
-                  }
-
-                  @Override
-                  protected void doSetNextReader(LeafReaderContext context) throws IOException {
-                    docBase = context.docBase;
-                  }
-
-                  @Override
-                  public void collect(int doc) {
-                    hits.set(docBase+doc);
-                  }
-                });
-
-              if (VERBOSE) {
-                System.err.println("  hitCount: " + hits.cardinality());
-              }
-      
-              for(int docID=0;docID<r.maxDoc();docID++) {
-                int id = (int) docIDToID.get(docID);
-                if (Double.isNaN(lats[id]) == false) {
-
-                  // Accurate point:
-                  GeoPoint point1 = new GeoPoint(planetModel, lats[id], lons[id]);
-
-                  // Quantized point (32 bits per dim):
-                  GeoPoint point2 = quantize(planetModel.getMaximumMagnitude(), point1);
-
-                  if (shape.isWithin(point1) != shape.isWithin(point2)) {
-                    if (VERBOSE) {
-                      System.out.println("  skip checking docID=" + docID + " quantization changed the expected result from " + shape.isWithin(point1) + " to " + shape.isWithin(point2));
-                    }
-                    continue;
-                  }
-
-                  boolean expected = ((deleted.contains(id) == false) && shape.isWithin(point2));
-                  if (hits.get(docID) != expected) {
-                    fail(Thread.currentThread().getName() + ": iter=" + iter + " id=" + id + " docID=" + docID + " lat=" + lats[id] + " lon=" + lons[id] + " expected " + expected + " but got: " + hits.get(docID) + " deleted?=" + deleted.contains(id) + "\n  point1=" + point1 + ", iswithin="+shape.isWithin(point1)+"\n  point2=" + point2 + ", iswithin="+shape.isWithin(point2) + "\n  query=" + query);
-                  }
-                } else {
-                  assertFalse(hits.get(docID));
-                }
-
-              }
-            }
-          }
-        };
-      thread.setName("T" + i);
-      thread.start();
-      threads.add(thread);
-    }
-    startingGun.countDown();
-    for(Thread thread : threads) {
-      thread.join();
-    }
-    IOUtils.close(r, dir);
-  }
-
-  private static Directory noVirusChecker(Directory dir) {
-    if (dir instanceof MockDirectoryWrapper) {
-      ((MockDirectoryWrapper) dir).setEnableVirusScanner(false);
-    }
-    return dir;
-  }
-
-  private static Directory getDirectory() {     
-    return noVirusChecker(newDirectory());
-  }
-}
diff --git a/lucene/suggest/src/test/org/apache/lucene/search/suggest/document/TestSuggestField.java b/lucene/suggest/src/test/org/apache/lucene/search/suggest/document/TestSuggestField.java
index 05bb677..92271f3 100644
--- a/lucene/suggest/src/test/org/apache/lucene/search/suggest/document/TestSuggestField.java
+++ b/lucene/suggest/src/test/org/apache/lucene/search/suggest/document/TestSuggestField.java
@@ -34,7 +34,7 @@ import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.lucene60.Lucene60Codec;
-import org.apache.lucene.document.DimensionalIntField;
+import org.apache.lucene.document.IntPoint;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.StoredField;
@@ -44,7 +44,7 @@ import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.search.DimensionalRangeQuery;
+import org.apache.lucene.search.PointRangeQuery;
 import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.search.TopDocs;
 import org.apache.lucene.search.suggest.BitsProducer;
@@ -305,7 +305,7 @@ public class TestSuggestField extends LuceneTestCase {
       Document document = new Document();
       document.add(new SuggestField("suggest_field", "abc_" + i, i));
       document.add(new StoredField("weight_fld", i));
-      document.add(new DimensionalIntField("weight_fld", i));
+      document.add(new IntPoint("weight_fld", i));
       iw.addDocument(document);
 
       if (usually()) {
@@ -313,7 +313,7 @@ public class TestSuggestField extends LuceneTestCase {
       }
     }
 
-    iw.deleteDocuments(DimensionalRangeQuery.new1DIntRange("weight_fld", 2, true, null, false));
+    iw.deleteDocuments(PointRangeQuery.new1DIntRange("weight_fld", 2, true, null, false));
 
     DirectoryReader reader = DirectoryReader.open(iw);
     SuggestIndexSearcher indexSearcher = new SuggestIndexSearcher(reader);
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingCodec.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingCodec.java
index 9913129..1e32e01 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingCodec.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingCodec.java
@@ -17,7 +17,7 @@ package org.apache.lucene.codecs.asserting;
  * limitations under the License.
  */
 
-import org.apache.lucene.codecs.DimensionalFormat;
+import org.apache.lucene.codecs.PointFormat;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.FilterCodec;
 import org.apache.lucene.codecs.LiveDocsFormat;
@@ -54,7 +54,7 @@ public class AssertingCodec extends FilterCodec {
   private final LiveDocsFormat liveDocs = new AssertingLiveDocsFormat();
   private final PostingsFormat defaultFormat = new AssertingPostingsFormat();
   private final DocValuesFormat defaultDVFormat = new AssertingDocValuesFormat();
-  private final DimensionalFormat dimensionalFormat = new AssertingDimensionalFormat();
+  private final PointFormat pointFormat = new AssertingPointFormat();
 
   public AssertingCodec() {
     super("Asserting", TestUtil.getDefaultCodec());
@@ -91,8 +91,8 @@ public class AssertingCodec extends FilterCodec {
   }
 
   @Override
-  public DimensionalFormat dimensionalFormat() {
-    return dimensionalFormat;
+  public PointFormat pointFormat() {
+    return pointFormat;
   }
 
   @Override
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingDimensionalFormat.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingDimensionalFormat.java
deleted file mode 100644
index 4191f65..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingDimensionalFormat.java
+++ /dev/null
@@ -1,149 +0,0 @@
-package org.apache.lucene.codecs.asserting;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collection;
-
-import org.apache.lucene.codecs.DimensionalFormat;
-import org.apache.lucene.codecs.DimensionalReader;
-import org.apache.lucene.codecs.DimensionalWriter;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.MergeState;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.util.Accountable;
-import org.apache.lucene.util.TestUtil;
-
-/**
- * Just like the default dimensional format but with additional asserts.
- */
-
-public final class AssertingDimensionalFormat extends DimensionalFormat {
-  private final DimensionalFormat in = TestUtil.getDefaultCodec().dimensionalFormat();
-  
-  @Override
-  public DimensionalWriter fieldsWriter(SegmentWriteState state) throws IOException {
-    return new AssertingDimensionalWriter(state, in.fieldsWriter(state));
-  }
-
-  @Override
-  public DimensionalReader fieldsReader(SegmentReadState state) throws IOException {
-    return new AssertingDimensionalReader(in.fieldsReader(state));
-  }
-  
-  static class AssertingDimensionalReader extends DimensionalReader {
-    private final DimensionalReader in;
-    
-    AssertingDimensionalReader(DimensionalReader in) {
-      this.in = in;
-      // do a few simple checks on init
-      assert toString() != null;
-      assert ramBytesUsed() >= 0;
-      assert getChildResources() != null;
-    }
-    
-    @Override
-    public void close() throws IOException {
-      in.close();
-      in.close(); // close again
-    }
-
-    @Override
-    public void intersect(String fieldName, IntersectVisitor visitor) throws IOException {
-      // TODO: wrap the visitor and make sure things are being reasonable
-      in.intersect(fieldName, visitor);
-    }
-
-    @Override
-    public long ramBytesUsed() {
-      long v = in.ramBytesUsed();
-      assert v >= 0;
-      return v;
-    }
-    
-    @Override
-    public Collection<Accountable> getChildResources() {
-      Collection<Accountable> res = in.getChildResources();
-      TestUtil.checkReadOnly(res);
-      return res;
-    }
-
-    @Override
-    public void checkIntegrity() throws IOException {
-      in.checkIntegrity();
-    }
-    
-    @Override
-    public DimensionalReader getMergeInstance() throws IOException {
-      return new AssertingDimensionalReader(in.getMergeInstance());
-    }
-
-    @Override
-    public String toString() {
-      return getClass().getSimpleName() + "(" + in.toString() + ")";
-    }
-
-    @Override
-    public byte[] getMinPackedValue(String fieldName) throws IOException {
-      return in.getMinPackedValue(fieldName);
-    }
-
-    @Override
-    public byte[] getMaxPackedValue(String fieldName) throws IOException {
-      return in.getMaxPackedValue(fieldName);
-    }
-
-    @Override
-    public int getNumDimensions(String fieldName) throws IOException {
-      return in.getNumDimensions(fieldName);
-    }
-
-    @Override
-    public int getBytesPerDimension(String fieldName) throws IOException {
-      return in.getBytesPerDimension(fieldName);
-    }
-  }
-
-  static class AssertingDimensionalWriter extends DimensionalWriter {
-    private final DimensionalWriter in;
-
-    AssertingDimensionalWriter(SegmentWriteState writeState, DimensionalWriter in) {
-      this.in = in;
-    }
-    
-    @Override
-    public void writeField(FieldInfo fieldInfo, DimensionalReader values) throws IOException {
-      if (fieldInfo.getDimensionCount() == 0) {
-        throw new IllegalArgumentException("writing field=\"" + fieldInfo.name + "\" but dimensionalCount is 0");
-      }
-      in.writeField(fieldInfo, values);
-    }
-
-    @Override
-    public void merge(MergeState mergeState) throws IOException {
-      in.merge(mergeState);
-    }
-
-    @Override
-    public void close() throws IOException {
-      in.close();
-      in.close(); // close again
-    }
-  }
-}
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingPointFormat.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingPointFormat.java
new file mode 100644
index 0000000..7b306d7
--- /dev/null
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingPointFormat.java
@@ -0,0 +1,149 @@
+package org.apache.lucene.codecs.asserting;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collection;
+
+import org.apache.lucene.codecs.PointFormat;
+import org.apache.lucene.codecs.PointReader;
+import org.apache.lucene.codecs.PointWriter;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.MergeState;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.util.Accountable;
+import org.apache.lucene.util.TestUtil;
+
+/**
+ * Just like the default point format but with additional asserts.
+ */
+
+public final class AssertingPointFormat extends PointFormat {
+  private final PointFormat in = TestUtil.getDefaultCodec().pointFormat();
+  
+  @Override
+  public PointWriter fieldsWriter(SegmentWriteState state) throws IOException {
+    return new AssertingPointWriter(state, in.fieldsWriter(state));
+  }
+
+  @Override
+  public PointReader fieldsReader(SegmentReadState state) throws IOException {
+    return new AssertingPointReader(in.fieldsReader(state));
+  }
+  
+  static class AssertingPointReader extends PointReader {
+    private final PointReader in;
+    
+    AssertingPointReader(PointReader in) {
+      this.in = in;
+      // do a few simple checks on init
+      assert toString() != null;
+      assert ramBytesUsed() >= 0;
+      assert getChildResources() != null;
+    }
+    
+    @Override
+    public void close() throws IOException {
+      in.close();
+      in.close(); // close again
+    }
+
+    @Override
+    public void intersect(String fieldName, IntersectVisitor visitor) throws IOException {
+      // TODO: wrap the visitor and make sure things are being reasonable
+      in.intersect(fieldName, visitor);
+    }
+
+    @Override
+    public long ramBytesUsed() {
+      long v = in.ramBytesUsed();
+      assert v >= 0;
+      return v;
+    }
+    
+    @Override
+    public Collection<Accountable> getChildResources() {
+      Collection<Accountable> res = in.getChildResources();
+      TestUtil.checkReadOnly(res);
+      return res;
+    }
+
+    @Override
+    public void checkIntegrity() throws IOException {
+      in.checkIntegrity();
+    }
+    
+    @Override
+    public PointReader getMergeInstance() throws IOException {
+      return new AssertingPointReader(in.getMergeInstance());
+    }
+
+    @Override
+    public String toString() {
+      return getClass().getSimpleName() + "(" + in.toString() + ")";
+    }
+
+    @Override
+    public byte[] getMinPackedValue(String fieldName) throws IOException {
+      return in.getMinPackedValue(fieldName);
+    }
+
+    @Override
+    public byte[] getMaxPackedValue(String fieldName) throws IOException {
+      return in.getMaxPackedValue(fieldName);
+    }
+
+    @Override
+    public int getNumDimensions(String fieldName) throws IOException {
+      return in.getNumDimensions(fieldName);
+    }
+
+    @Override
+    public int getBytesPerDimension(String fieldName) throws IOException {
+      return in.getBytesPerDimension(fieldName);
+    }
+  }
+
+  static class AssertingPointWriter extends PointWriter {
+    private final PointWriter in;
+
+    AssertingPointWriter(SegmentWriteState writeState, PointWriter in) {
+      this.in = in;
+    }
+    
+    @Override
+    public void writeField(FieldInfo fieldInfo, PointReader values) throws IOException {
+      if (fieldInfo.getPointDimensionCount() == 0) {
+        throw new IllegalArgumentException("writing field=\"" + fieldInfo.name + "\" but pointDimensionalCount is 0");
+      }
+      in.writeField(fieldInfo, values);
+    }
+
+    @Override
+    public void merge(MergeState mergeState) throws IOException {
+      in.merge(mergeState);
+    }
+
+    @Override
+    public void close() throws IOException {
+      in.close();
+      in.close(); // close again
+    }
+  }
+}
diff --git a/lucene/test-framework/src/java/org/apache/lucene/index/BaseIndexFileFormatTestCase.java b/lucene/test-framework/src/java/org/apache/lucene/index/BaseIndexFileFormatTestCase.java
index f7d9f16..b4b6f7d 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/index/BaseIndexFileFormatTestCase.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/index/BaseIndexFileFormatTestCase.java
@@ -312,7 +312,7 @@ abstract class BaseIndexFileFormatTestCase extends LuceneTestCase {
     FieldInfo proto = oneDocReader.getFieldInfos().fieldInfo("field");
     FieldInfo field = new FieldInfo(proto.name, proto.number, proto.hasVectors(), proto.omitsNorms(), proto.hasPayloads(), 
                                     proto.getIndexOptions(), proto.getDocValuesType(), proto.getDocValuesGen(), new HashMap<>(),
-                                    proto.getDimensionCount(), proto.getDimensionNumBytes());
+                                    proto.getPointDimensionCount(), proto.getPointNumBytes());
 
     FieldInfos fieldInfos = new FieldInfos(new FieldInfo[] { field } );
 
diff --git a/lucene/test-framework/src/java/org/apache/lucene/index/MismatchedLeafReader.java b/lucene/test-framework/src/java/org/apache/lucene/index/MismatchedLeafReader.java
index c57159f..664e76a 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/index/MismatchedLeafReader.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/index/MismatchedLeafReader.java
@@ -67,8 +67,8 @@ public class MismatchedLeafReader extends FilterLeafReader {
                                         oldInfo.getDocValuesType(),  // docValuesType
                                         oldInfo.getDocValuesGen(),   // dvGen
                                         oldInfo.attributes(),        // attributes
-                                        oldInfo.getDimensionCount(),      // dimension count
-                                        oldInfo.getDimensionNumBytes());  // dimension numBytes
+                                        oldInfo.getPointDimensionCount(),      // dimension count
+                                        oldInfo.getPointNumBytes());  // dimension numBytes
       shuffled.set(i, newInfo);
     }
     
diff --git a/lucene/test-framework/src/java/org/apache/lucene/search/QueryUtils.java b/lucene/test-framework/src/java/org/apache/lucene/search/QueryUtils.java
index 734bced..6de6213 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/search/QueryUtils.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/search/QueryUtils.java
@@ -24,7 +24,7 @@ import java.util.List;
 import java.util.Random;
 
 import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.index.DimensionalValues;
+import org.apache.lucene.index.PointValues;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.Fields;
@@ -257,7 +257,7 @@ public class QueryUtils {
       }
 
       @Override
-      public DimensionalValues getDimensionalValues() {
+      public PointValues getPointValues() {
         return null;
       }
 
diff --git a/lucene/test-framework/src/java/org/apache/lucene/util/LineFileDocs.java b/lucene/test-framework/src/java/org/apache/lucene/util/LineFileDocs.java
index 6e18fd7..a198130 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/util/LineFileDocs.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/util/LineFileDocs.java
@@ -34,10 +34,11 @@ import java.util.Random;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.zip.GZIPInputStream;
 
-import org.apache.lucene.document.DimensionalIntField;
+import org.apache.lucene.document.IntPoint;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.IntPoint;
 import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.document.StringField;
@@ -186,7 +187,7 @@ public class LineFileDocs implements Closeable {
       id = new StringField("docid", "", Field.Store.YES);
       doc.add(id);
 
-      idNum = new DimensionalIntField("docid_int", 0);
+      idNum = new IntPoint("docid_int", 0);
       doc.add(idNum);
 
       date = new StringField("date", "", Field.Store.YES);
diff --git a/lucene/test-framework/src/java/org/apache/lucene/util/TestUtil.java b/lucene/test-framework/src/java/org/apache/lucene/util/TestUtil.java
index 04b4220..99d4be3 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/util/TestUtil.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/util/TestUtil.java
@@ -57,7 +57,7 @@ import org.apache.lucene.codecs.lucene60.Lucene60Codec;
 import org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat;
 import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
 import org.apache.lucene.document.BinaryDocValuesField;
-import org.apache.lucene.document.DimensionalBinaryField;
+import org.apache.lucene.document.BinaryPoint;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType.LegacyNumericType;
@@ -1037,7 +1037,7 @@ public final class TestUtil {
       final Field field1 = (Field) f;
       final Field field2;
       final DocValuesType dvType = field1.fieldType().docValuesType();
-      final int dimCount = field1.fieldType().dimensionCount();
+      final int dimCount = field1.fieldType().pointDimensionCount();
       final LegacyNumericType numType = field1.fieldType().numericType();
       if (dvType != DocValuesType.NONE) {
         switch(dvType) {
@@ -1057,7 +1057,7 @@ public final class TestUtil {
         BytesRef br = field1.binaryValue();
         byte[] bytes = new byte[br.length];
         System.arraycopy(br.bytes, br.offset, bytes, 0, br.length);
-        field2 = new DimensionalBinaryField(field1.name(), bytes, field1.fieldType());
+        field2 = new BinaryPoint(field1.name(), bytes, field1.fieldType());
       } else if (numType != null) {
         switch (numType) {
           case INT:
diff --git a/lucene/test-framework/src/test/org/apache/lucene/codecs/compressing/TestCompressingStoredFieldsFormat.java b/lucene/test-framework/src/test/org/apache/lucene/codecs/compressing/TestCompressingStoredFieldsFormat.java
index 3e9db03..93e23cb 100644
--- a/lucene/test-framework/src/test/org/apache/lucene/codecs/compressing/TestCompressingStoredFieldsFormat.java
+++ b/lucene/test-framework/src/test/org/apache/lucene/codecs/compressing/TestCompressingStoredFieldsFormat.java
@@ -22,10 +22,11 @@ import java.util.Random;
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.document.DimensionalIntField;
+import org.apache.lucene.document.IntPoint;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.IntPoint;
 import org.apache.lucene.document.StoredField;
 import org.apache.lucene.index.BaseStoredFieldsFormatTestCase;
 import org.apache.lucene.index.CodecReader;
@@ -69,7 +70,7 @@ public class TestCompressingStoredFieldsFormat extends BaseStoredFieldsFormatTes
     IndexWriter iw = new IndexWriter(dir, iwConf);
 
     final Document validDoc = new Document();
-    validDoc.add(new DimensionalIntField("id", 0));
+    validDoc.add(new IntPoint("id", 0));
     validDoc.add(new StoredField("id", 0));
     iw.addDocument(validDoc);
     iw.commit();
diff --git a/solr/core/src/java/org/apache/solr/handler/component/ExpandComponent.java b/solr/core/src/java/org/apache/solr/handler/component/ExpandComponent.java
index 58ea6cc..694e53a 100644
--- a/solr/core/src/java/org/apache/solr/handler/component/ExpandComponent.java
+++ b/solr/core/src/java/org/apache/solr/handler/component/ExpandComponent.java
@@ -741,8 +741,8 @@ public class ExpandComponent extends SearchComponent implements PluginInfoInitia
               DocValuesType.NONE,
               fieldInfo.getDocValuesGen(),
               fieldInfo.attributes(),
-              fieldInfo.getDimensionCount(), 
-              fieldInfo.getDimensionNumBytes());
+              fieldInfo.getPointDimensionCount(),
+              fieldInfo.getPointNumBytes());
           newInfos.add(f);
 
         } else {
diff --git a/solr/core/src/java/org/apache/solr/search/Insanity.java b/solr/core/src/java/org/apache/solr/search/Insanity.java
index 6043aa1..11920a9 100644
--- a/solr/core/src/java/org/apache/solr/search/Insanity.java
+++ b/solr/core/src/java/org/apache/solr/search/Insanity.java
@@ -67,7 +67,7 @@ public class Insanity {
         if (fi.name.equals(insaneField)) {
           filteredInfos.add(new FieldInfo(fi.name, fi.number, fi.hasVectors(), fi.omitsNorms(),
                                           fi.hasPayloads(), fi.getIndexOptions(), DocValuesType.NONE, -1, Collections.emptyMap(),
-                                          fi.getDimensionCount(), fi.getDimensionNumBytes()));
+                                          fi.getPointDimensionCount(), fi.getPointNumBytes()));
         } else {
           filteredInfos.add(fi);
         }
diff --git a/solr/core/src/test/org/apache/solr/search/TestDocSet.java b/solr/core/src/test/org/apache/solr/search/TestDocSet.java
index 9708e82..f435e6b 100644
--- a/solr/core/src/test/org/apache/solr/search/TestDocSet.java
+++ b/solr/core/src/test/org/apache/solr/search/TestDocSet.java
@@ -23,7 +23,7 @@ import java.util.List;
 import java.util.Random;
 
 import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.index.DimensionalValues;
+import org.apache.lucene.index.PointValues;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.Fields;
@@ -428,7 +428,7 @@ public class TestDocSet extends LuceneTestCase {
       }
 
       @Override
-      public DimensionalValues getDimensionalValues() {
+      public PointValues getPointValues() {
         return null;
       }
 

