GitDiffStart: 2af893ae8ebf8754c2851928d0396d18ae2f56be | Fri Aug 28 14:00:37 2009 +0000
diff --git a/build.xml b/build.xml
index c5db0bb..3eb2d03 100644
--- a/build.xml
+++ b/build.xml
@@ -65,7 +65,7 @@
               excludes="contrib/db/*/lib/,contrib/*/ext-libs/,src/site/build/,contrib/benchmark/temp/**,contrib/benchmark/work/**"
   />
   <patternset id="binary.build.dist.patterns"
-              includes="${final.name}.jar,${demo.war.name}.war,${demo.name}.jar,docs/,contrib/*/*.jar,contrib/*/*.war"
+              includes="${final.name}.jar,${demo.war.name}.war,${demo.name}.jar,docs/,contrib/*/*.jar,contrib/*/*.war, contrib/*/*/*.jar"
   />
   <patternset id="binary.root.dist.patterns"
               includes="src/demo/,src/jsp/,docs/,*.txt,contrib/*/README*,**/CHANGES.txt,lib/servlet-api-*.jar"
diff --git a/contrib/collation/build.xml b/contrib/collation/build.xml
index 8664e54..5f03c20 100644
--- a/contrib/collation/build.xml
+++ b/contrib/collation/build.xml
@@ -39,7 +39,7 @@
 
   <target name="compile-misc">
     <subant target="compile">
-       <fileset dir="${common.dir}/contrib/miscellaneous" includes="build.xml"/>
+       <fileset dir="${common.dir}/contrib/misc" includes="build.xml"/>
     </subant>
   </target>
 
diff --git a/contrib/misc/README.txt b/contrib/misc/README.txt
new file mode 100644
index 0000000..29c5803
--- /dev/null
+++ b/contrib/misc/README.txt
@@ -0,0 +1,3 @@
+contrib/miscellaneous is a home of different Lucene-related classes
+that all belong to org.apache.lucene.misc package, as they are not
+substantial enough to warrant their own package.
diff --git a/contrib/misc/build.xml b/contrib/misc/build.xml
new file mode 100644
index 0000000..ad311b5
--- /dev/null
+++ b/contrib/misc/build.xml
@@ -0,0 +1,43 @@
+<?xml version="1.0"?>
+
+<!--
+    Licensed to the Apache Software Foundation (ASF) under one or more
+    contributor license agreements.  See the NOTICE file distributed with
+    this work for additional information regarding copyright ownership.
+    The ASF licenses this file to You under the Apache License, Version 2.0
+    the "License"); you may not use this file except in compliance with
+    the License.  You may obtain a copy of the License at
+ 
+        http://www.apache.org/licenses/LICENSE-2.0
+ 
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an "AS IS" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
+ -->
+
+<project name="misc" default="default">
+
+  <!-- TODO: add javacc capability for PrecedenceQueryParser -->
+
+  <description>
+    Miscellaneous Lucene extensions
+  </description>
+
+  <import file="../contrib-build.xml"/>
+
+  <property name="javacc.path" location="src/java/org/apache/lucene/queryParser/precedence"/>
+
+  <target name="javacc" depends="javacc-check" description="generate precedence query parser from jj (requires javacc 3.2)">
+    <delete>
+      <fileset dir="${javacc.path}" includes="*.java">
+        <containsregexp expression="Generated.*By.*JavaCC"/>
+      </fileset>
+    </delete>
+    <invoke-javacc target="${javacc.path}/PrecedenceQueryParser.jj"
+                   outputDir="${javacc.path}"
+    />
+  </target>
+
+</project>
diff --git a/contrib/misc/pom.xml.template b/contrib/misc/pom.xml.template
new file mode 100644
index 0000000..0c47d39
--- /dev/null
+++ b/contrib/misc/pom.xml.template
@@ -0,0 +1,36 @@
+<project xmlns="http://maven.apache.org/POM/4.0.0"
+  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+
+  <!--
+    Licensed to the Apache Software Foundation (ASF) under one
+    or more contributor license agreements.  See the NOTICE file
+    distributed with this work for additional information
+    regarding copyright ownership.  The ASF licenses this file
+    to you under the Apache License, Version 2.0 (the
+    "License"); you may not use this file except in compliance
+    with the License.  You may obtain a copy of the License at
+    
+    http://www.apache.org/licenses/LICENSE-2.0
+    
+    Unless required by applicable law or agreed to in writing,
+    software distributed under the License is distributed on an
+    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+    KIND, either express or implied.  See the License for the
+    specific language governing permissions and limitations
+    under the License.
+  -->
+
+  <modelVersion>4.0.0</modelVersion>
+  <parent>
+    <groupId>org.apache.lucene</groupId>
+    <artifactId>lucene-contrib</artifactId>
+    <version>@version@</version>
+  </parent>
+  <groupId>org.apache.lucene</groupId>
+  <artifactId>lucene-misc</artifactId>
+  <name>Lucene Miscellaneous</name>
+  <version>@version@</version>
+  <description>Miscellaneous Lucene extensions</description>
+  <packaging>jar</packaging>
+</project>
diff --git a/contrib/misc/src/java/org/apache/lucene/index/FieldNormModifier.java b/contrib/misc/src/java/org/apache/lucene/index/FieldNormModifier.java
new file mode 100644
index 0000000..9f25b36
--- /dev/null
+++ b/contrib/misc/src/java/org/apache/lucene/index/FieldNormModifier.java
@@ -0,0 +1,160 @@
+package org.apache.lucene.index;
+
+/**
+ * Copyright 2006 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.io.File;
+import java.util.Date;
+
+import org.apache.lucene.search.Similarity;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.FSDirectory;
+import org.apache.lucene.util.StringHelper;
+
+/**
+ * Given a directory and a list of fields, updates the fieldNorms in place for every document.
+ * 
+ * If Similarity class is specified, uses its lengthNorm method to set norms.
+ * If -n command line argument is used, removed field norms, as if 
+ * {@link org.apache.lucene.document.Field.Index}.NO_NORMS was used.
+ *
+ * <p>
+ * NOTE: This will overwrite any length normalization or field/document boosts.
+ * </p>
+ *
+ */
+public class FieldNormModifier {
+
+  /**
+   * Command Line Execution method.
+   *
+   * <pre>
+   * Usage: FieldNormModifier /path/index &lt;package.SimilarityClassName | -n&gt; field1 field2 ...
+   * </pre>
+   */
+  public static void main(String[] args) throws IOException {
+    if (args.length < 3) {
+      System.err.println("Usage: FieldNormModifier <index> <package.SimilarityClassName | -n> <field1> [field2] ...");
+      System.exit(1);
+    }
+
+    Similarity s = null;
+    if (!args[1].equals("-n")) {
+      try {
+        Class simClass = Class.forName(args[1]);
+        s = (Similarity)simClass.newInstance();
+      } catch (Exception e) {
+        System.err.println("Couldn't instantiate similarity with empty constructor: " + args[1]);
+        e.printStackTrace(System.err);
+        System.exit(1);
+      }
+    }
+
+    Directory d = FSDirectory.open(new File(args[0]));
+    FieldNormModifier fnm = new FieldNormModifier(d, s);
+
+    for (int i = 2; i < args.length; i++) {
+      System.out.print("Updating field: " + args[i] + " " + (new Date()).toString() + " ... ");
+      fnm.reSetNorms(args[i]);
+      System.out.println(new Date().toString());
+    }
+    
+    d.close();
+  }
+  
+  
+  private Directory dir;
+  private Similarity sim;
+  
+  /**
+   * Constructor for code that wishes to use this class programatically
+   * If Similarity is null, kill the field norms.
+   *
+   * @param d the Directory to modify
+   * @param s the Similiary to use (can be null)
+   */
+  public FieldNormModifier(Directory d, Similarity s) {
+    dir = d;
+    sim = s;
+  }
+
+  /**
+   * Resets the norms for the specified field.
+   *
+   * <p>
+   * Opens a new IndexReader on the Directory given to this instance,
+   * modifies the norms (either using the Similarity given to this instance, or by using fake norms,
+   * and closes the IndexReader.
+   * </p>
+   *
+   * @param field the field whose norms should be reset
+   */
+  public void reSetNorms(String field) throws IOException {
+    String fieldName = StringHelper.intern(field);
+    int[] termCounts = new int[0];
+    byte[] fakeNorms = new byte[0];
+    
+    IndexReader reader = null;
+    TermEnum termEnum = null;
+    TermDocs termDocs = null;
+    try {
+      reader = IndexReader.open(dir);
+      termCounts = new int[reader.maxDoc()];
+      // if we are killing norms, get fake ones
+      if (sim == null)
+        fakeNorms = SegmentReader.createFakeNorms(reader.maxDoc());
+      try {
+        termEnum = reader.terms(new Term(field));
+        try {
+          termDocs = reader.termDocs();
+          do {
+            Term term = termEnum.term();
+            if (term != null && term.field().equals(fieldName)) {
+              termDocs.seek(termEnum.term());
+              while (termDocs.next()) {
+                termCounts[termDocs.doc()] += termDocs.freq();
+              }
+            }
+          } while (termEnum.next());
+          
+        } finally {
+          if (null != termDocs) termDocs.close();
+        }
+      } finally {
+        if (null != termEnum) termEnum.close();
+      }
+    } finally {
+      if (null != reader) reader.close();
+    }
+    
+    try {
+      reader = IndexReader.open(dir); 
+      for (int d = 0; d < termCounts.length; d++) {
+        if (! reader.isDeleted(d)) {
+          if (sim == null)
+            reader.setNorm(d, fieldName, fakeNorms[0]);
+          else
+            reader.setNorm(d, fieldName, sim.encodeNorm(sim.lengthNorm(fieldName, termCounts[d])));
+        }
+      }
+      
+    } finally {
+      if (null != reader) reader.close();
+    }
+  }
+  
+}
diff --git a/contrib/misc/src/java/org/apache/lucene/index/TermVectorAccessor.java b/contrib/misc/src/java/org/apache/lucene/index/TermVectorAccessor.java
new file mode 100644
index 0000000..314aba4
--- /dev/null
+++ b/contrib/misc/src/java/org/apache/lucene/index/TermVectorAccessor.java
@@ -0,0 +1,170 @@
+package org.apache.lucene.index;
+
+import org.apache.lucene.util.StringHelper;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Collection;
+import java.util.Iterator;
+/*
+ *  Licensed under the Apache License, Version 2.0 (the "License");
+ *  you may not use this file except in compliance with the License.
+ *  You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *  Unless required by applicable law or agreed to in writing, software
+ *  distributed under the License is distributed on an "AS IS" BASIS,
+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *  See the License for the specific language governing permissions and
+ *  limitations under the License.
+ *
+ */
+
+
+/**
+ * Transparent access to the vector space model,
+ * either via TermFreqVector or by resolving it from the inverted index.
+ * <p/>
+ * Resolving a term vector from a large index can be a time consuming process.
+ * <p/>
+ * Warning! This class is not thread safe!
+ */
+public class TermVectorAccessor {
+
+  public TermVectorAccessor() {
+  }
+
+  /**
+   * Instance reused to save garbage collector some time
+   */
+  private TermVectorMapperDecorator decoratedMapper = new TermVectorMapperDecorator();
+
+
+  /**
+   * Visits the TermVectorMapper and populates it with terms available for a given document,
+   * either via a vector created at index time or by resolving them from the inverted index.
+   *
+   * @param indexReader    Index source
+   * @param documentNumber Source document to access
+   * @param fieldName      Field to resolve
+   * @param mapper         Mapper to be mapped with data
+   * @throws IOException
+   */
+  public void accept(IndexReader indexReader, int documentNumber, String fieldName, TermVectorMapper mapper) throws IOException {
+
+    fieldName = StringHelper.intern(fieldName);
+
+    decoratedMapper.decorated = mapper;
+    decoratedMapper.termVectorStored = false;
+
+    indexReader.getTermFreqVector(documentNumber, fieldName, decoratedMapper);
+
+    if (!decoratedMapper.termVectorStored) {
+      mapper.setDocumentNumber(documentNumber);
+      build(indexReader, fieldName, mapper, documentNumber);
+    }
+  }
+
+  /** Instance reused to save garbage collector some time */
+  private List/*<String>*/ tokens;
+
+  /** Instance reused to save garbage collector some time */
+  private List/*<int[]>*/ positions;
+
+  /** Instance reused to save garbage collector some time */
+  private List/*<Integer>*/ frequencies;
+
+
+  /**
+   * Populates the mapper with terms available for the given field in a document
+   * by resolving the inverted index.
+   *
+   * @param indexReader
+   * @param field interned field name
+   * @param mapper
+   * @param documentNumber
+   * @throws IOException
+   */
+  private void build(IndexReader indexReader, String field, TermVectorMapper mapper, int documentNumber) throws IOException {
+
+    if (tokens == null) {
+      tokens = new ArrayList/*<String>*/(500);
+      positions = new ArrayList/*<int[]>*/(500);
+      frequencies = new ArrayList/*<Integer>*/(500);
+    } else {
+      tokens.clear();
+      frequencies.clear();
+      positions.clear();
+    }
+
+    TermEnum termEnum = indexReader.terms();
+    if (termEnum.skipTo(new Term(field, ""))) {
+
+      while (termEnum.term().field() == field) {
+        TermPositions termPositions = indexReader.termPositions(termEnum.term());
+        if (termPositions.skipTo(documentNumber)) {
+
+          frequencies.add(new Integer(termPositions.freq()));
+          tokens.add(termEnum.term().text());
+
+
+          if (!mapper.isIgnoringPositions()) {
+            int[] positions = new int[termPositions.freq()];
+            for (int i = 0; i < positions.length; i++) {
+              positions[i] = termPositions.nextPosition();
+            }
+            this.positions.add(positions);
+          } else {
+            positions.add(null);
+          }
+        }
+        termPositions.close();
+        if (!termEnum.next()) {
+          break;
+        }
+      }
+
+      mapper.setDocumentNumber(documentNumber);
+      mapper.setExpectations(field, tokens.size(), false, !mapper.isIgnoringPositions());
+      for (int i = 0; i < tokens.size(); i++) {
+        mapper.map((String) tokens.get(i), ((Integer) frequencies.get(i)).intValue(), (TermVectorOffsetInfo[]) null, (int[]) positions.get(i));
+      }
+
+    }
+    termEnum.close();
+
+
+  }
+
+
+  private static class TermVectorMapperDecorator extends TermVectorMapper {
+
+    private TermVectorMapper decorated;
+
+    public boolean isIgnoringPositions() {
+      return decorated.isIgnoringPositions();
+    }
+
+    public boolean isIgnoringOffsets() {
+      return decorated.isIgnoringOffsets();
+    }
+
+    private boolean termVectorStored = false;
+
+    public void setExpectations(String field, int numTerms, boolean storeOffsets, boolean storePositions) {
+      decorated.setExpectations(field, numTerms, storeOffsets, storePositions);
+      termVectorStored = true;
+    }
+
+    public void map(String term, int frequency, TermVectorOffsetInfo[] offsets, int[] positions) {
+      decorated.map(term, frequency, offsets, positions);
+    }
+
+    public void setDocumentNumber(int documentNumber) {
+      decorated.setDocumentNumber(documentNumber);
+    }
+  }
+
+}
diff --git a/contrib/misc/src/java/org/apache/lucene/misc/ChainedFilter.java b/contrib/misc/src/java/org/apache/lucene/misc/ChainedFilter.java
new file mode 100644
index 0000000..e18e561
--- /dev/null
+++ b/contrib/misc/src/java/org/apache/lucene/misc/ChainedFilter.java
@@ -0,0 +1,268 @@
+package org.apache.lucene.misc;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.search.DocIdSet;
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.search.Filter;
+import org.apache.lucene.util.OpenBitSet;
+import org.apache.lucene.util.OpenBitSetDISI;
+import org.apache.lucene.util.SortedVIntList;
+
+/**
+ * <p>
+ * Allows multiple {@link Filter}s to be chained.
+ * Logical operations such as <b>NOT</b> and <b>XOR</b>
+ * are applied between filters. One operation can be used
+ * for all filters, or a specific operation can be declared
+ * for each filter.
+ * </p>
+ * <p>
+ * Order in which filters are called depends on
+ * the position of the filter in the chain. It's probably
+ * more efficient to place the most restrictive filters
+ * /least computationally-intensive filters first.
+ * </p>
+ *
+ */
+public class ChainedFilter extends Filter
+{
+    public static final int OR = 0;
+    public static final int AND = 1;
+    public static final int ANDNOT = 2;
+    public static final int XOR = 3;
+    /**
+     * Logical operation when none is declared. Defaults to
+     * OR.
+     */
+    public static int DEFAULT = OR;
+
+    /** The filter chain */
+    private Filter[] chain = null;
+
+    private int[] logicArray;
+
+    private int logic = -1;
+
+    /**
+     * Ctor.
+     * @param chain The chain of filters
+     */
+    public ChainedFilter(Filter[] chain)
+    {
+        this.chain = chain;
+    }
+
+    /**
+     * Ctor.
+     * @param chain The chain of filters
+     * @param logicArray Logical operations to apply between filters
+     */
+    public ChainedFilter(Filter[] chain, int[] logicArray)
+    {
+        this.chain = chain;
+        this.logicArray = logicArray;
+    }
+
+    /**
+     * Ctor.
+     * @param chain The chain of filters
+     * @param logic Logicial operation to apply to ALL filters
+     */
+    public ChainedFilter(Filter[] chain, int logic)
+    {
+        this.chain = chain;
+        this.logic = logic;
+    }
+
+    /**
+     * {@link Filter#getDocIdSet}.
+     */
+    public DocIdSet getDocIdSet(IndexReader reader) throws IOException
+    {
+        int[] index = new int[1]; // use array as reference to modifiable int; 
+        index[0] = 0;             // an object attribute would not be thread safe.
+        if (logic != -1)
+            return getDocIdSet(reader, logic, index);
+        else if (logicArray != null)
+            return getDocIdSet(reader, logicArray, index);
+        else
+            return getDocIdSet(reader, DEFAULT, index);
+    }
+
+    private DocIdSetIterator getDISI(Filter filter, IndexReader reader)
+    throws IOException {
+        DocIdSet docIdSet = filter.getDocIdSet(reader);
+        if (docIdSet == null) {
+          return DocIdSet.EMPTY_DOCIDSET.iterator();
+        } else {
+          DocIdSetIterator iter = docIdSet.iterator();
+          if (iter == null) {
+            return DocIdSet.EMPTY_DOCIDSET.iterator();
+          } else {
+            return iter;
+          }
+        }
+    }
+
+    private OpenBitSetDISI initialResult(IndexReader reader, int logic, int[] index)
+    throws IOException
+    {
+        OpenBitSetDISI result;
+        /**
+         * First AND operation takes place against a completely false
+         * bitset and will always return zero results.
+         */
+        if (logic == AND)
+        {
+            result = new OpenBitSetDISI(getDISI(chain[index[0]], reader), reader.maxDoc());
+            ++index[0];
+        }
+        else if (logic == ANDNOT)
+        {
+            result = new OpenBitSetDISI(getDISI(chain[index[0]], reader), reader.maxDoc());
+            result.flip(0,reader.maxDoc()); // NOTE: may set bits for deleted docs.
+            ++index[0];
+        }
+        else
+        {
+            result = new OpenBitSetDISI(reader.maxDoc());
+        }
+        return result;
+    }
+
+    // TODO: in 3.0, instead of removing this deprecated
+    // method, make it a no-op and mark it final
+    /** Provide a SortedVIntList when it is definitely
+     *  smaller than an OpenBitSet
+     *  @deprecated Either use CachingWrapperFilter, or
+     *  switch to a different DocIdSet implementation yourself. */
+    protected DocIdSet finalResult(OpenBitSetDISI result, int maxDocs) {
+        return (result.cardinality() < (maxDocs / 9))
+              ? (DocIdSet) new SortedVIntList(result)
+              : (DocIdSet) result;
+    }
+        
+
+    /**
+     * Delegates to each filter in the chain.
+     * @param reader IndexReader
+     * @param logic Logical operation
+     * @return DocIdSet
+     */
+    private DocIdSet getDocIdSet(IndexReader reader, int logic, int[] index)
+    throws IOException
+    {
+        OpenBitSetDISI result = initialResult(reader, logic, index);
+        for (; index[0] < chain.length; index[0]++)
+        {
+            doChain(result, logic, chain[index[0]].getDocIdSet(reader));
+        }
+        return finalResult(result, reader.maxDoc());
+    }
+
+    /**
+     * Delegates to each filter in the chain.
+     * @param reader IndexReader
+     * @param logic Logical operation
+     * @return DocIdSet
+     */
+    private DocIdSet getDocIdSet(IndexReader reader, int[] logic, int[] index)
+    throws IOException
+    {
+        if (logic.length != chain.length)
+            throw new IllegalArgumentException("Invalid number of elements in logic array");
+
+        OpenBitSetDISI result = initialResult(reader, logic[0], index);
+        for (; index[0] < chain.length; index[0]++)
+        {
+            doChain(result, logic[index[0]], chain[index[0]].getDocIdSet(reader));
+        }
+        return finalResult(result, reader.maxDoc());
+    }
+
+    public String toString()
+    {
+        StringBuffer sb = new StringBuffer();
+        sb.append("ChainedFilter: [");
+        for (int i = 0; i < chain.length; i++)
+        {
+            sb.append(chain[i]);
+            sb.append(' ');
+        }
+        sb.append(']');
+        return sb.toString();
+    }
+
+    private void doChain(OpenBitSetDISI result, int logic, DocIdSet dis)
+    throws IOException {
+      
+      if (dis instanceof OpenBitSet) {
+        // optimized case for OpenBitSets
+        switch (logic) {
+            case OR:
+                result.or((OpenBitSet) dis);
+                break;
+            case AND:
+                result.and((OpenBitSet) dis);
+                break;
+            case ANDNOT:
+                result.andNot((OpenBitSet) dis);
+                break;
+            case XOR:
+                result.xor((OpenBitSet) dis);
+                break;
+            default:
+                doChain(result, DEFAULT, dis);
+                break;
+        }
+      } else {
+        DocIdSetIterator disi;
+        if (dis == null) {
+          disi = DocIdSet.EMPTY_DOCIDSET.iterator();
+        } else {
+          disi = dis.iterator();
+          if (disi == null) {
+            disi = DocIdSet.EMPTY_DOCIDSET.iterator();            
+          }
+        }
+
+        switch (logic) {
+            case OR:
+                result.inPlaceOr(disi);
+                break;
+            case AND:
+                result.inPlaceAnd(disi);
+                break;
+            case ANDNOT:
+                result.inPlaceNot(disi);
+                break;
+            case XOR:
+                result.inPlaceXor(disi);
+                break;
+            default:
+                doChain(result, DEFAULT, dis);
+                break;
+        }
+      }
+    }
+
+}
diff --git a/contrib/misc/src/java/org/apache/lucene/misc/HighFreqTerms.java b/contrib/misc/src/java/org/apache/lucene/misc/HighFreqTerms.java
new file mode 100644
index 0000000..71c5b5a
--- /dev/null
+++ b/contrib/misc/src/java/org/apache/lucene/misc/HighFreqTerms.java
@@ -0,0 +1,96 @@
+package org.apache.lucene.misc;
+
+/**
+  * Copyright 2004 The Apache Software Foundation
+  *
+  * Licensed under the Apache License, Version 2.0 (the "License");
+  * you may not use this file except in compliance with the License.
+  * You may obtain a copy of the License at
+  *
+  *     http://www.apache.org/licenses/LICENSE-2.0
+  *
+  * Unless required by applicable law or agreed to in writing, software
+  * distributed under the License is distributed on an "AS IS" BASIS,
+  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  * See the License for the specific language governing permissions and
+  * limitations under the License.
+  */
+
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.TermEnum;
+import org.apache.lucene.util.PriorityQueue;
+
+/**
+ * <code>HighFreqTerms</code> class extracts terms and their frequencies out
+ * of an existing Lucene index.
+ *
+ * @version $Id$
+ */
+public class HighFreqTerms {
+  
+  // The top numTerms will be displayed
+  public static final int numTerms = 100;
+
+  public static void main(String[] args) throws Exception {
+    IndexReader reader = null;
+    String field = null;
+    if (args.length == 1) {
+      reader = IndexReader.open(args[0]);
+    } else if (args.length == 2) {
+      reader = IndexReader.open(args[0]);
+      field = args[1];
+    } else {
+      usage();
+      System.exit(1);
+    }
+
+    TermInfoQueue tiq = new TermInfoQueue(numTerms);
+    TermEnum terms = reader.terms();
+
+    if (field != null) { 
+      while (terms.next()) {
+        if (terms.term().field().equals(field)) {
+          tiq.insert(new TermInfo(terms.term(), terms.docFreq()));
+        }
+      }
+    }
+    else {
+      while (terms.next()) {
+        tiq.insert(new TermInfo(terms.term(), terms.docFreq()));
+      }
+    }
+    while (tiq.size() != 0) {
+      TermInfo termInfo = (TermInfo) tiq.pop();
+      System.out.println(termInfo.term + " " + termInfo.docFreq);
+    }
+
+    reader.close();
+  }
+
+  private static void usage() {
+    System.out.println(
+         "\n\n"
+         + "java org.apache.lucene.misc.HighFreqTerms <index dir> [field]\n\n");
+  }
+}
+
+final class TermInfo {
+  TermInfo(Term t, int df) {
+    term = t;
+    docFreq = df;
+  }
+  int docFreq;
+  Term term;
+}
+
+final class TermInfoQueue extends PriorityQueue {
+  TermInfoQueue(int size) {
+    initialize(size);
+  }
+  protected final boolean lessThan(Object a, Object b) {
+    TermInfo termInfoA = (TermInfo) a;
+    TermInfo termInfoB = (TermInfo) b;
+    return termInfoA.docFreq < termInfoB.docFreq;
+  }
+}
diff --git a/contrib/misc/src/java/org/apache/lucene/misc/IndexMergeTool.java b/contrib/misc/src/java/org/apache/lucene/misc/IndexMergeTool.java
new file mode 100644
index 0000000..8ea1886
--- /dev/null
+++ b/contrib/misc/src/java/org/apache/lucene/misc/IndexMergeTool.java
@@ -0,0 +1,55 @@
+package org.apache.lucene.misc;
+
+/**
+  * Copyright 2005 The Apache Software Foundation
+  *
+  * Licensed under the Apache License, Version 2.0 (the "License");
+  * you may not use this file except in compliance with the License.
+  * You may obtain a copy of the License at
+  *
+  *     http://www.apache.org/licenses/LICENSE-2.0
+  *
+  * Unless required by applicable law or agreed to in writing, software
+  * distributed under the License is distributed on an "AS IS" BASIS,
+  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  * See the License for the specific language governing permissions and
+  * limitations under the License.
+  */
+
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.analysis.SimpleAnalyzer;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.FSDirectory;
+
+import java.io.File;
+import java.io.IOException;
+
+/**
+ * Merges indices specified on the command line into the index
+ * specified as the first command line argument.
+ * @version $Id$
+ */
+public class IndexMergeTool {
+  public static void main(String[] args) throws IOException {
+    if (args.length < 3) {
+      System.err.println("Usage: IndexMergeTool <mergedIndex> <index1> <index2> [index3] ...");
+      System.exit(1);
+    }
+    File mergedIndex = new File(args[0]);
+
+    IndexWriter writer = new IndexWriter(mergedIndex, new  SimpleAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+
+    Directory[] indexes = new Directory[args.length - 1];
+    for (int i = 1; i < args.length; i++) {
+      indexes[i  - 1] = FSDirectory.open(new File(args[i]));
+    }
+
+    System.out.println("Merging...");
+    writer.addIndexes(indexes);
+
+    System.out.println("Optimizing...");
+    writer.optimize();
+    writer.close();
+    System.out.println("Done.");
+  }
+}
diff --git a/contrib/misc/src/java/org/apache/lucene/misc/LengthNormModifier.java b/contrib/misc/src/java/org/apache/lucene/misc/LengthNormModifier.java
new file mode 100644
index 0000000..9fdb725
--- /dev/null
+++ b/contrib/misc/src/java/org/apache/lucene/misc/LengthNormModifier.java
@@ -0,0 +1,154 @@
+package org.apache.lucene.misc;
+
+/**
+ * Copyright 2006 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.TermEnum;
+import org.apache.lucene.index.TermDocs;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.search.Similarity;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.FSDirectory;
+import org.apache.lucene.util.StringHelper;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.Date;
+
+/**
+ * Given a directory, a Similarity, and a list of fields, updates the
+ * fieldNorms in place for every document using the Similarity.lengthNorm.
+ *
+ * <p>
+ * NOTE: This only works if you do <b>not</b> use field/document boosts in your
+ * index.
+ * </p>
+ *
+ * @version $Id$
+ * @deprecated Use {@link org.apache.lucene.index.FieldNormModifier}
+ */
+public class LengthNormModifier {
+  
+  /**
+   * Command Line Execution method.
+   *
+   * <pre>
+   * Usage: LengthNormModifier /path/index package.SimilarityClassName field1 field2 ...
+   * </pre>
+   */
+  public static void main(String[] args) throws IOException {
+    if (args.length < 3) {
+      System.err.println("Usage: LengthNormModifier <index> <package.SimilarityClassName> <field1> [field2] ...");
+      System.exit(1);
+    }
+    
+    Similarity s = null;
+    try {
+      Class simClass = Class.forName(args[1]);
+      s = (Similarity)simClass.newInstance();
+    } catch (Exception e) {
+      System.err.println("Couldn't instantiate similarity with empty constructor: " + args[1]);
+      e.printStackTrace(System.err);
+    }
+    
+    File index = new File(args[0]);
+    Directory d = FSDirectory.open(index);
+    
+    LengthNormModifier lnm = new LengthNormModifier(d, s);
+    
+    for (int i = 2; i < args.length; i++) {
+      System.out.print("Updating field: " + args[i] + " " + (new Date()).toString() + " ... ");
+      lnm.reSetNorms(args[i]);
+      System.out.println(new Date().toString());
+    }
+    
+    d.close();
+  }
+  
+  
+  private Directory dir;
+  private Similarity sim;
+  
+  /**
+   * Constructor for code that wishes to use this class progaomatically.
+   *
+   * @param d The Directory to modify
+   * @param s The Similarity to use in <code>reSetNorms</code>
+   */
+  public LengthNormModifier(Directory d, Similarity s) {
+    dir = d;
+    sim = s;
+  }
+  
+  /**
+   * Resets the norms for the specified field.
+   *
+   * <p>
+   * Opens a new IndexReader on the Directory given to this instance,
+   * modifies the norms using the Similarity given to this instance,
+   * and closes the IndexReader.
+   * </p>
+   *
+   * @param field the field whose norms should be reset
+   */
+  public void reSetNorms(String field) throws IOException {
+    String fieldName = StringHelper.intern(field);
+    int[] termCounts = new int[0];
+    
+    IndexReader reader = null;
+    TermEnum termEnum = null;
+    TermDocs termDocs = null;
+    try {
+      reader = IndexReader.open(dir);
+      termCounts = new int[reader.maxDoc()];
+      try {
+        termEnum = reader.terms(new Term(field));
+        try {
+          termDocs = reader.termDocs();
+          do {
+            Term term = termEnum.term();
+            if (term != null && term.field().equals(fieldName)) {
+              termDocs.seek(termEnum.term());
+              while (termDocs.next()) {
+                termCounts[termDocs.doc()] += termDocs.freq();
+              }
+            }
+          } while (termEnum.next());
+        } finally {
+          if (null != termDocs) termDocs.close();
+        }
+      } finally {
+        if (null != termEnum) termEnum.close();
+      }
+    } finally {
+      if (null != reader) reader.close();
+    }
+    
+    try {
+      reader = IndexReader.open(dir); 
+      for (int d = 0; d < termCounts.length; d++) {
+        if (! reader.isDeleted(d)) {
+          byte norm = sim.encodeNorm(sim.lengthNorm(fieldName, termCounts[d]));
+          reader.setNorm(d, fieldName, norm);
+        }
+      }
+    } finally {
+      if (null != reader) reader.close();
+    }
+  }
+  
+}
diff --git a/contrib/misc/src/java/org/apache/lucene/misc/SweetSpotSimilarity.java b/contrib/misc/src/java/org/apache/lucene/misc/SweetSpotSimilarity.java
new file mode 100644
index 0000000..e59ee57
--- /dev/null
+++ b/contrib/misc/src/java/org/apache/lucene/misc/SweetSpotSimilarity.java
@@ -0,0 +1,267 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.misc;
+
+import org.apache.lucene.search.Similarity;
+import org.apache.lucene.search.DefaultSimilarity;
+import org.apache.lucene.index.FieldInvertState;
+
+import java.util.Map;
+import java.util.HashMap;
+
+/**
+ * A similarity with a lengthNorm that provides for a "plateau" of
+ * equally good lengths, and tf helper functions.
+ *
+ * <p>
+ * For lengthNorm, A global min/max can be specified to define the
+ * plateau of lengths that should all have a norm of 1.0.
+ * Below the min, and above the max the lengthNorm drops off in a
+ * sqrt function.
+ * </p>
+ * <p>
+ * A per field min/max can be specified if different fields have
+ * different sweet spots.
+ * </p>
+ *
+ * <p>
+ * For tf, baselineTf and hyperbolicTf functions are provided, which
+ * subclasses can choose between.
+ * </p>
+ *
+ */
+public class SweetSpotSimilarity extends DefaultSimilarity {
+
+  private int ln_min = 1;
+  private int ln_max = 1;
+  private float ln_steep = 0.5f;
+
+  private Map ln_mins = new HashMap(7);
+  private Map ln_maxs = new HashMap(7);
+  private Map ln_steeps = new HashMap(7);
+  private Map ln_overlaps = new HashMap(7);
+
+  private float tf_base = 0.0f;
+  private float tf_min = 0.0f;
+
+  private float tf_hyper_min = 0.0f;
+  private float tf_hyper_max = 2.0f;
+  private double tf_hyper_base = 1.3d;
+  private float tf_hyper_xoffset = 10.0f;
+    
+  public SweetSpotSimilarity() {
+    super();
+  }
+
+  /**
+   * Sets the baseline and minimum function variables for baselineTf
+   *
+   * @see #baselineTf
+   */
+  public void setBaselineTfFactors(float base, float min) {
+    tf_min = min;
+    tf_base = base;
+  }
+  
+  /**
+   * Sets the function variables for the hyperbolicTf functions
+   *
+   * @param min the minimum tf value to ever be returned (default: 0.0)
+   * @param max the maximum tf value to ever be returned (default: 2.0)
+   * @param base the base value to be used in the exponential for the hyperbolic function (default: e)
+   * @param xoffset the midpoint of the hyperbolic function (default: 10.0)
+   * @see #hyperbolicTf
+   */
+  public void setHyperbolicTfFactors(float min, float max,
+                                     double base, float xoffset) {
+    tf_hyper_min = min;
+    tf_hyper_max = max;
+    tf_hyper_base = base;
+    tf_hyper_xoffset = xoffset;
+  }
+    
+  /**
+   * Sets the default function variables used by lengthNorm when no field
+   * specifc variables have been set.
+   *
+   * @see #lengthNorm
+   */
+  public void setLengthNormFactors(int min, int max, float steepness) {
+    this.ln_min = min;
+    this.ln_max = max;
+    this.ln_steep = steepness;
+  }
+
+  /**
+   * Sets the function variables used by lengthNorm for a specific named field.
+   * 
+   * @param field field name
+   * @param min minimum value
+   * @param max maximum value
+   * @param steepness steepness of the curve
+   * @param discountOverlaps if true, <code>numOverlapTokens</code> will be
+   * subtracted from <code>numTokens</code>; if false then
+   * <code>numOverlapTokens</code> will be assumed to be 0 (see
+   * {@link DefaultSimilarity#computeNorm(String, FieldInvertState)} for details).
+   *
+   * @see #lengthNorm
+   */
+  public void setLengthNormFactors(String field, int min, int max,
+                                   float steepness, boolean discountOverlaps) {
+    ln_mins.put(field, new Integer(min));
+    ln_maxs.put(field, new Integer(max));
+    ln_steeps.put(field, new Float(steepness));
+    ln_overlaps.put(field, new Boolean(discountOverlaps));
+  }
+    
+  /**
+   * Implemented as <code> state.getBoost() *
+   * lengthNorm(fieldName, numTokens) </code> where
+   * numTokens does not count overlap tokens if
+   * discountOverlaps is true by default or true for this
+   * specific field. */
+  public float computeNorm(String fieldName, FieldInvertState state) {
+    final int numTokens;
+    boolean overlaps = discountOverlaps;
+    if (ln_overlaps.containsKey(fieldName)) {
+      overlaps = ((Boolean)ln_overlaps.get(fieldName)).booleanValue();
+    }
+    if (overlaps)
+      numTokens = state.getLength() - state.getNumOverlap();
+    else
+      numTokens = state.getLength();
+
+    return state.getBoost() * lengthNorm(fieldName, numTokens);
+  }
+
+  /**
+   * Implemented as:
+   * <code>
+   * 1/sqrt( steepness * (abs(x-min) + abs(x-max) - (max-min)) + 1 )
+   * </code>.
+   *
+   * <p>
+   * This degrades to <code>1/sqrt(x)</code> when min and max are both 1 and
+   * steepness is 0.5
+   * </p>
+   *
+   * <p>
+   * :TODO: potential optimization is to just flat out return 1.0f if numTerms
+   * is between min and max.
+   * </p>
+   *
+   * @see #setLengthNormFactors
+   */
+  public float lengthNorm(String fieldName, int numTerms) {
+    int l = ln_min;
+    int h = ln_max;
+    float s = ln_steep;
+  
+    if (ln_mins.containsKey(fieldName)) {
+      l = ((Number)ln_mins.get(fieldName)).intValue();
+    }
+    if (ln_maxs.containsKey(fieldName)) {
+      h = ((Number)ln_maxs.get(fieldName)).intValue();
+    }
+    if (ln_steeps.containsKey(fieldName)) {
+      s = ((Number)ln_steeps.get(fieldName)).floatValue();
+    }
+  
+    return (float)
+      (1.0f /
+       Math.sqrt
+       (
+        (
+         s *
+         (float)(Math.abs(numTerms - l) + Math.abs(numTerms - h) - (h-l))
+         )
+        + 1.0f
+        )
+       );
+  }
+
+  /**
+   * Delegates to baselineTf
+   *
+   * @see #baselineTf
+   */
+  public float tf(int freq) {
+    return baselineTf(freq);
+  }
+  
+  /**
+   * Implemented as:
+   * <code>
+   *  (x &lt;= min) &#63; base : sqrt(x+(base**2)-min)
+   * </code>
+   * ...but with a special case check for 0.
+   * <p>
+   * This degrates to <code>sqrt(x)</code> when min and base are both 0
+   * </p>
+   *
+   * @see #setBaselineTfFactors
+   */
+  public float baselineTf(float freq) {
+
+    if (0.0f == freq) return 0.0f;
+  
+    return (freq <= tf_min)
+      ? tf_base
+      : (float)Math.sqrt(freq + (tf_base * tf_base) - tf_min);
+  }
+
+  /**
+   * Uses a hyperbolic tangent function that allows for a hard max...
+   *
+   * <code>
+   * tf(x)=min+(max-min)/2*(((base**(x-xoffset)-base**-(x-xoffset))/(base**(x-xoffset)+base**-(x-xoffset)))+1)
+   * </code>
+   *
+   * <p>
+   * This code is provided as a convincience for subclasses that want
+   * to use a hyperbolic tf function.
+   * </p>
+   *
+   * @see #setHyperbolicTfFactors
+   */
+  public float hyperbolicTf(float freq) {
+    if (0.0f == freq) return 0.0f;
+
+    final float min = tf_hyper_min;
+    final float max = tf_hyper_max;
+    final double base = tf_hyper_base;
+    final float xoffset = tf_hyper_xoffset;
+    final double x = (double)(freq - xoffset);
+  
+    final float result = min +
+      (float)(
+              (max-min) / 2.0f
+              *
+              (
+               ( ( Math.pow(base,x) - Math.pow(base,-x) )
+                 / ( Math.pow(base,x) + Math.pow(base,-x) )
+                 )
+               + 1.0d
+               )
+              );
+
+    return Float.isNaN(result) ? max : result;
+    
+  }
+
+}
diff --git a/contrib/misc/src/java/org/apache/lucene/queryParser/analyzing/AnalyzingQueryParser.java b/contrib/misc/src/java/org/apache/lucene/queryParser/analyzing/AnalyzingQueryParser.java
new file mode 100644
index 0000000..c3f686a
--- /dev/null
+++ b/contrib/misc/src/java/org/apache/lucene/queryParser/analyzing/AnalyzingQueryParser.java
@@ -0,0 +1,318 @@
+package org.apache.lucene.queryParser.analyzing;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.io.StringReader;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.Token;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.TermAttribute;
+import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.search.Query;
+
+/**
+ * Overrides Lucene's default QueryParser so that Fuzzy-, Prefix-, Range-, and WildcardQuerys
+ * are also passed through the given analyzer, but wild card characters (like <code>*</code>) 
+ * don't get removed from the search terms.
+ * 
+ * <p><b>Warning:</b> This class should only be used with analyzers that do not use stopwords
+ * or that add tokens. Also, several stemming analyzers are inappropriate: for example, GermanAnalyzer 
+ * will turn <code>H&auml;user</code> into <code>hau</code>, but <code>H?user</code> will 
+ * become <code>h?user</code> when using this parser and thus no match would be found (i.e.
+ * using this parser will be no improvement over QueryParser in such cases). 
+ *
+ * @version $Revision$, $Date$
+ */
+public class AnalyzingQueryParser extends org.apache.lucene.queryParser.QueryParser {
+
+  /**
+   * Constructs a query parser.
+   * @param field    the default field for query terms.
+   * @param analyzer used to find terms in the query text.
+   */
+  public AnalyzingQueryParser(String field, Analyzer analyzer) {
+    super(field, analyzer);
+  }
+
+  /**
+   * Called when parser
+   * parses an input term token that contains one or more wildcard
+   * characters (like <code>*</code>), but is not a prefix term token (one
+   * that has just a single * character at the end).
+   * <p>
+   * Example: will be called for <code>H?user</code> or for <code>H*user</code> 
+   * but not for <code>*user</code>.
+   * <p>
+   * Depending on analyzer and settings, a wildcard term may (most probably will)
+   * be lower-cased automatically. It <b>will</b> go through the default Analyzer.
+   * <p>
+   * Overrides super class, by passing terms through analyzer.
+   *
+   * @param  field   Name of the field query will use.
+   * @param  termStr Term token that contains one or more wild card
+   *                 characters (? or *), but is not simple prefix term
+   *
+   * @return Resulting {@link Query} built for the term
+   * @throws ParseException
+   */
+  protected Query getWildcardQuery(String field, String termStr) throws ParseException {
+    List tlist = new ArrayList();
+    List wlist = new ArrayList();
+    /* somewhat a hack: find/store wildcard chars
+     * in order to put them back after analyzing */
+    boolean isWithinToken = (!termStr.startsWith("?") && !termStr.startsWith("*"));
+    StringBuffer tmpBuffer = new StringBuffer();
+    char[] chars = termStr.toCharArray();
+    for (int i = 0; i < termStr.length(); i++) {
+      if (chars[i] == '?' || chars[i] == '*') {
+        if (isWithinToken) {
+          tlist.add(tmpBuffer.toString());
+          tmpBuffer.setLength(0);
+        }
+        isWithinToken = false;
+      } else {
+        if (!isWithinToken) {
+          wlist.add(tmpBuffer.toString());
+          tmpBuffer.setLength(0);
+        }
+        isWithinToken = true;
+      }
+      tmpBuffer.append(chars[i]);
+    }
+    if (isWithinToken) {
+      tlist.add(tmpBuffer.toString());
+    } else {
+      wlist.add(tmpBuffer.toString());
+    }
+
+    // get Analyzer from superclass and tokenize the term
+    TokenStream source = getAnalyzer().tokenStream(field, new StringReader(termStr));
+    TermAttribute termAtt = (TermAttribute) source.addAttribute(TermAttribute.class);
+    
+    int countTokens = 0;
+    while (true) {
+      try {
+        if (!source.incrementToken()) break;
+      } catch (IOException e) {
+        break;
+      }
+      String term = termAtt.term();
+      if (!"".equals(term)) {
+        try {
+          tlist.set(countTokens++, term);
+        } catch (IndexOutOfBoundsException ioobe) {
+          countTokens = -1;
+        }
+      }
+    }
+    try {
+      source.close();
+    } catch (IOException e) {
+      // ignore
+    }
+
+    if (countTokens != tlist.size()) {
+      /* this means that the analyzer used either added or consumed 
+       * (common for a stemmer) tokens, and we can't build a WildcardQuery */
+      throw new ParseException("Cannot build WildcardQuery with analyzer "
+          + getAnalyzer().getClass() + " - tokens added or lost");
+    }
+
+    if (tlist.size() == 0) {
+      return null;
+    } else if (tlist.size() == 1) {
+      if (wlist != null && wlist.size() == 1) {
+        /* if wlist contains one wildcard, it must be at the end, because:
+         * 1) wildcards are not allowed in 1st position of a term by QueryParser
+         * 2) if wildcard was *not* in end, there would be *two* or more tokens */
+        return super.getWildcardQuery(field, (String) tlist.get(0)
+            + (((String) wlist.get(0)).toString()));
+      } else {
+        /* we should never get here! if so, this method was called
+         * with a termStr containing no wildcard ... */
+        throw new IllegalArgumentException("getWildcardQuery called without wildcard");
+      }
+    } else {
+      /* the term was tokenized, let's rebuild to one token
+       * with wildcards put back in postion */
+      StringBuffer sb = new StringBuffer();
+      for (int i = 0; i < tlist.size(); i++) {
+        sb.append((String) tlist.get(i));
+        if (wlist != null && wlist.size() > i) {
+          sb.append((String) wlist.get(i));
+        }
+      }
+      return super.getWildcardQuery(field, sb.toString());
+    }
+  }
+
+  /**
+   * Called when parser parses an input term
+   * token that uses prefix notation; that is, contains a single '*' wildcard
+   * character as its last character. Since this is a special case
+   * of generic wildcard term, and such a query can be optimized easily,
+   * this usually results in a different query object.
+   * <p>
+   * Depending on analyzer and settings, a prefix term may (most probably will)
+   * be lower-cased automatically. It <b>will</b> go through the default Analyzer.
+   * <p>
+   * Overrides super class, by passing terms through analyzer.
+   *
+   * @param  field   Name of the field query will use.
+   * @param  termStr Term token to use for building term for the query
+   *                 (<b>without</b> trailing '*' character!)
+   *
+   * @return Resulting {@link Query} built for the term
+   * @throws ParseException
+   */
+  protected Query getPrefixQuery(String field, String termStr) throws ParseException {
+    // get Analyzer from superclass and tokenize the term
+    TokenStream source = getAnalyzer().tokenStream(field, new StringReader(termStr));
+    List tlist = new ArrayList();
+    TermAttribute termAtt = (TermAttribute) source.addAttribute(TermAttribute.class);
+    
+    while (true) {
+      try {
+        if (!source.incrementToken()) break;
+      } catch (IOException e) {
+        break;
+      }
+      tlist.add(termAtt.term());
+    }
+
+    try {
+      source.close();
+    } catch (IOException e) {
+      // ignore
+    }
+
+    if (tlist.size() == 1) {
+      return super.getPrefixQuery(field, (String) tlist.get(0));
+    } else {
+      /* this means that the analyzer used either added or consumed
+       * (common for a stemmer) tokens, and we can't build a PrefixQuery */
+      throw new ParseException("Cannot build PrefixQuery with analyzer "
+          + getAnalyzer().getClass()
+          + (tlist.size() > 1 ? " - token(s) added" : " - token consumed"));
+    }
+  }
+
+  /**
+   * Called when parser parses an input term token that has the fuzzy suffix (~) appended.
+   * <p>
+   * Depending on analyzer and settings, a fuzzy term may (most probably will)
+   * be lower-cased automatically. It <b>will</b> go through the default Analyzer.
+   * <p>
+   * Overrides super class, by passing terms through analyzer.
+   *
+   * @param field Name of the field query will use.
+   * @param termStr Term token to use for building term for the query
+   *
+   * @return Resulting {@link Query} built for the term
+   * @exception ParseException
+   */
+  protected Query getFuzzyQuery(String field, String termStr, float minSimilarity)
+      throws ParseException {
+    // get Analyzer from superclass and tokenize the term
+    TokenStream source = getAnalyzer().tokenStream(field, new StringReader(termStr));
+    TermAttribute termAtt = (TermAttribute) source.addAttribute(TermAttribute.class);
+    String nextToken = null;
+    boolean multipleTokens = false;
+    
+    try {
+      if (source.incrementToken()) {
+        nextToken = termAtt.term();
+      }
+      multipleTokens = source.incrementToken();
+    } catch (IOException e) {
+      nextToken = null;
+    }
+
+    try {
+      source.close();
+    } catch (IOException e) {
+      // ignore
+    }
+
+    if (multipleTokens) {
+      throw new ParseException("Cannot build FuzzyQuery with analyzer " + getAnalyzer().getClass()
+          + " - tokens were added");
+    }
+
+    return (nextToken == null) ? null : super.getFuzzyQuery(field, nextToken, minSimilarity);
+  }
+
+  /**
+   * Overrides super class, by passing terms through analyzer.
+   * @exception ParseException
+   */
+  protected Query getRangeQuery(String field, String part1, String part2, boolean inclusive)
+      throws ParseException {
+    // get Analyzer from superclass and tokenize the terms
+    TokenStream source = getAnalyzer().tokenStream(field, new StringReader(part1));
+    TermAttribute termAtt = (TermAttribute) source.addAttribute(TermAttribute.class);
+    boolean multipleTokens = false;
+
+    // part1
+    try {
+      if (source.incrementToken()) {
+        part1 = termAtt.term();
+      }
+      multipleTokens = source.incrementToken();
+    } catch (IOException e) {
+      // ignore
+    }
+    try {
+      source.close();
+    } catch (IOException e) {
+      // ignore
+    }
+    if (multipleTokens) {
+      throw new ParseException("Cannot build RangeQuery with analyzer " + getAnalyzer().getClass()
+          + " - tokens were added to part1");
+    }
+
+    // part2
+    source = getAnalyzer().tokenStream(field, new StringReader(part2));
+    termAtt = (TermAttribute) source.addAttribute(TermAttribute.class);
+    
+    try {
+      if (source.incrementToken()) {
+        part2 = termAtt.term();
+      }
+      multipleTokens = source.incrementToken();
+    } catch (IOException e) {
+      // ignore
+    }
+    try {
+      source.close();
+    } catch (IOException e) {
+      // ignore
+    }
+    if (multipleTokens) {
+      throw new ParseException("Cannot build RangeQuery with analyzer " + getAnalyzer().getClass()
+          + " - tokens were added to part2");
+    }
+    return super.getRangeQuery(field, part1, part2, inclusive);
+  }
+
+}
diff --git a/contrib/misc/src/java/org/apache/lucene/queryParser/complexPhrase/ComplexPhraseQueryParser.java b/contrib/misc/src/java/org/apache/lucene/queryParser/complexPhrase/ComplexPhraseQueryParser.java
new file mode 100644
index 0000000..9db2b2f
--- /dev/null
+++ b/contrib/misc/src/java/org/apache/lucene/queryParser/complexPhrase/ComplexPhraseQueryParser.java
@@ -0,0 +1,389 @@
+package org.apache.lucene.queryParser.complexPhrase;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.MultiTermQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TermRangeQuery;
+import org.apache.lucene.search.spans.SpanNearQuery;
+import org.apache.lucene.search.spans.SpanNotQuery;
+import org.apache.lucene.search.spans.SpanOrQuery;
+import org.apache.lucene.search.spans.SpanQuery;
+import org.apache.lucene.search.spans.SpanTermQuery;
+
+/**
+ * QueryParser which permits complex phrase query syntax e.g. "(john jon
+ * jonathan~) peters*"
+ * 
+ * Performs potentially multiple passes over Query text to parse any nested
+ * logic in PhraseQueries. - First pass takes any PhraseQuery content between
+ * quotes and stores for subsequent pass. All other query content is parsed as
+ * normal - Second pass parses any stored PhraseQuery content, checking all
+ * embedded clauses are referring to the same field and therefore can be
+ * rewritten as Span queries. All PhraseQuery clauses are expressed as
+ * ComplexPhraseQuery objects
+ * 
+ * This could arguably be done in one pass using a new QueryParser but here I am
+ * working within the constraints of the existing parser as a base class. This
+ * currently simply feeds all phrase content through an analyzer to select
+ * phrase terms - any "special" syntax such as * ~ * etc are not given special
+ * status
+ * 
+ * 
+ */
+public class ComplexPhraseQueryParser extends QueryParser {
+  private ArrayList/*<ComplexPhraseQuery>*/complexPhrases = null;
+
+  private boolean isPass2ResolvingPhrases;
+
+  private ComplexPhraseQuery currentPhraseQuery = null;
+
+  public ComplexPhraseQueryParser(String f, Analyzer a) {
+    super(f, a);
+  }
+
+  protected Query getFieldQuery(String field, String queryText, int slop) {
+    ComplexPhraseQuery cpq = new ComplexPhraseQuery(field, queryText, slop);
+    complexPhrases.add(cpq); // add to list of phrases to be parsed once
+    // we
+    // are through with this pass
+    return cpq;
+  }
+
+  public Query parse(String query) throws ParseException {
+    if (isPass2ResolvingPhrases) {
+      MultiTermQuery.RewriteMethod oldMethod = getMultiTermRewriteMethod();
+      try {
+        // Temporarily force BooleanQuery rewrite so that Parser will
+        // generate visible
+        // collection of terms which we can convert into SpanQueries.
+        // ConstantScoreRewrite mode produces an
+        // opaque ConstantScoreQuery object which cannot be interrogated for
+        // terms in the same way a BooleanQuery can.
+        // QueryParser is not guaranteed threadsafe anyway so this temporary
+        // state change should not
+        // present an issue
+        setMultiTermRewriteMethod(MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE);
+        return super.parse(query);
+      } finally {
+        setMultiTermRewriteMethod(oldMethod);
+      }
+    }
+
+    // First pass - parse the top-level query recording any PhraseQuerys
+    // which will need to be resolved
+    complexPhrases = new ArrayList/*<ComplexPhraseQuery>*/();
+    Query q = super.parse(query);
+
+    // Perform second pass, using this QueryParser to parse any nested
+    // PhraseQueries with different
+    // set of syntax restrictions (i.e. all fields must be same)
+    isPass2ResolvingPhrases = true;
+    try {
+      for (Iterator iterator = complexPhrases.iterator(); iterator.hasNext();) {
+        currentPhraseQuery = (ComplexPhraseQuery) iterator.next();
+        // in each phrase, now parse the contents between quotes as a
+        // separate parse operation
+        currentPhraseQuery.parsePhraseElements(this);
+      }
+    } finally {
+      isPass2ResolvingPhrases = false;
+    }
+    return q;
+  }
+
+  // There is No "getTermQuery throws ParseException" method to override so
+  // unfortunately need
+  // to throw a runtime exception here if a term for another field is embedded
+  // in phrase query
+  protected Query newTermQuery(Term term) {
+    if (isPass2ResolvingPhrases) {
+      try {
+        checkPhraseClauseIsForSameField(term.field());
+      } catch (ParseException pe) {
+        throw new RuntimeException("Error parsing complex phrase", pe);
+      }
+    }
+    return super.newTermQuery(term);
+  }
+
+  // Helper method used to report on any clauses that appear in query syntax
+  private void checkPhraseClauseIsForSameField(String field)
+      throws ParseException {
+    if (!field.equals(currentPhraseQuery.field)) {
+      throw new ParseException("Cannot have clause for field \"" + field
+          + "\" nested in phrase " + " for field \"" + currentPhraseQuery.field
+          + "\"");
+    }
+  }
+
+  protected Query getWildcardQuery(String field, String termStr)
+      throws ParseException {
+    if (isPass2ResolvingPhrases) {
+      checkPhraseClauseIsForSameField(field);
+    }
+    return super.getWildcardQuery(field, termStr);
+  }
+
+  protected Query getRangeQuery(String field, String part1, String part2,
+      boolean inclusive) throws ParseException {
+    if (isPass2ResolvingPhrases) {
+      checkPhraseClauseIsForSameField(field);
+    }
+    return super.getRangeQuery(field, part1, part2, inclusive);
+  }
+
+  protected Query newRangeQuery(String field, String part1, String part2,
+      boolean inclusive) {
+    if (isPass2ResolvingPhrases) {
+      // Must use old-style RangeQuery in order to produce a BooleanQuery
+      // that can be turned into SpanOr clause
+      TermRangeQuery rangeQuery = new TermRangeQuery(field, part1, part2, inclusive, inclusive,
+          getRangeCollator());
+      rangeQuery.setRewriteMethod(MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE);
+      return rangeQuery;
+    }
+    return super.newRangeQuery(field, part1, part2, inclusive);
+  }
+
+  protected Query getFuzzyQuery(String field, String termStr,
+      float minSimilarity) throws ParseException {
+    if (isPass2ResolvingPhrases) {
+      checkPhraseClauseIsForSameField(field);
+    }
+    return super.getFuzzyQuery(field, termStr, minSimilarity);
+  }
+
+  /*
+   * Used to handle the query content in between quotes and produced Span-based
+   * interpretations of the clauses.
+   */
+  static class ComplexPhraseQuery extends Query {
+
+    String field;
+
+    String phrasedQueryStringContents;
+
+    int slopFactor;
+
+    private Query contents;
+
+    public ComplexPhraseQuery(String field, String phrasedQueryStringContents,
+        int slopFactor) {
+      super();
+      this.field = field;
+      this.phrasedQueryStringContents = phrasedQueryStringContents;
+      this.slopFactor = slopFactor;
+    }
+
+    // Called by ComplexPhraseQueryParser for each phrase after the main
+    // parse
+    // thread is through
+    protected void parsePhraseElements(QueryParser qp) throws ParseException {
+      // TODO ensure that field-sensitivity is preserved ie the query
+      // string below is parsed as
+      // field+":("+phrasedQueryStringContents+")"
+      // but this will need code in rewrite to unwrap the first layer of
+      // boolean query
+      contents = qp.parse(phrasedQueryStringContents);
+    }
+
+    public Query rewrite(IndexReader reader) throws IOException {
+      // ArrayList spanClauses = new ArrayList();
+      if (contents instanceof TermQuery) {
+        return contents;
+      }
+      // Build a sequence of Span clauses arranged in a SpanNear - child
+      // clauses can be complex
+      // Booleans e.g. nots and ors etc
+      int numNegatives = 0;
+      if (!(contents instanceof BooleanQuery)) {
+        throw new IllegalArgumentException("Unknown query type \""
+            + contents.getClass().getName()
+            + "\" found in phrase query string \"" + phrasedQueryStringContents
+            + "\"");
+      }
+      BooleanQuery bq = (BooleanQuery) contents;
+      BooleanClause[] bclauses = bq.getClauses();
+      SpanQuery[] allSpanClauses = new SpanQuery[bclauses.length];
+      // For all clauses e.g. one* two~
+      for (int i = 0; i < bclauses.length; i++) {
+        // HashSet bclauseterms=new HashSet();
+        Query qc = bclauses[i].getQuery();
+        // Rewrite this clause e.g one* becomes (one OR onerous)
+        qc = qc.rewrite(reader);
+        if (bclauses[i].getOccur().equals(BooleanClause.Occur.MUST_NOT)) {
+          numNegatives++;
+        }
+
+        if (qc instanceof BooleanQuery) {
+          ArrayList sc = new ArrayList();
+          addComplexPhraseClause(sc, (BooleanQuery) qc);
+          if (sc.size() > 0) {
+            allSpanClauses[i] = (SpanQuery) sc.get(0);
+          } else {
+            // Insert fake term e.g. phrase query was for "Fred Smithe*" and
+            // there were no "Smithe*" terms - need to
+            // prevent match on just "Fred".
+            allSpanClauses[i] = new SpanTermQuery(new Term(field,
+                "Dummy clause because no terms found - must match nothing"));
+          }
+        } else {
+          if (qc instanceof TermQuery) {
+            TermQuery tq = (TermQuery) qc;
+            allSpanClauses[i] = new SpanTermQuery(tq.getTerm());
+          } else {
+            throw new IllegalArgumentException("Unknown query type \""
+                + qc.getClass().getName()
+                + "\" found in phrase query string \""
+                + phrasedQueryStringContents + "\"");
+          }
+
+        }
+      }
+      if (numNegatives == 0) {
+        // The simple case - no negative elements in phrase
+        return new SpanNearQuery(allSpanClauses, slopFactor, true);
+      }
+      // Complex case - we have mixed positives and negatives in the
+      // sequence.
+      // Need to return a SpanNotQuery
+      ArrayList positiveClauses = new ArrayList();
+      for (int j = 0; j < allSpanClauses.length; j++) {
+        if (!bclauses[j].getOccur().equals(BooleanClause.Occur.MUST_NOT)) {
+          positiveClauses.add(allSpanClauses[j]);
+        }
+      }
+
+      SpanQuery[] includeClauses = (SpanQuery[]) positiveClauses
+          .toArray(new SpanQuery[positiveClauses.size()]);
+
+      SpanQuery include = null;
+      if (includeClauses.length == 1) {
+        include = includeClauses[0]; // only one positive clause
+      } else {
+        // need to increase slop factor based on gaps introduced by
+        // negatives
+        include = new SpanNearQuery(includeClauses, slopFactor + numNegatives,
+            true);
+      }
+      // Use sequence of positive and negative values as the exclude.
+      SpanNearQuery exclude = new SpanNearQuery(allSpanClauses, slopFactor,
+          true);
+      SpanNotQuery snot = new SpanNotQuery(include, exclude);
+      return snot;
+    }
+
+    private void addComplexPhraseClause(List spanClauses, BooleanQuery qc) {
+      ArrayList ors = new ArrayList();
+      ArrayList nots = new ArrayList();
+      BooleanClause[] bclauses = qc.getClauses();
+
+      // For all clauses e.g. one* two~
+      for (int i = 0; i < bclauses.length; i++) {
+        Query childQuery = bclauses[i].getQuery();
+
+        // select the list to which we will add these options
+        ArrayList chosenList = ors;
+        if (bclauses[i].getOccur() == BooleanClause.Occur.MUST_NOT) {
+          chosenList = nots;
+        }
+
+        if (childQuery instanceof TermQuery) {
+          TermQuery tq = (TermQuery) childQuery;
+          SpanTermQuery stq = new SpanTermQuery(tq.getTerm());
+          stq.setBoost(tq.getBoost());
+          chosenList.add(stq);
+        } else if (childQuery instanceof BooleanQuery) {
+          BooleanQuery cbq = (BooleanQuery) childQuery;
+          addComplexPhraseClause(chosenList, cbq);
+        } else {
+          // TODO alternatively could call extract terms here?
+          throw new IllegalArgumentException("Unknown query type:"
+              + childQuery.getClass().getName());
+        }
+      }
+      if (ors.size() == 0) {
+        return;
+      }
+      SpanOrQuery soq = new SpanOrQuery((SpanQuery[]) ors
+          .toArray(new SpanQuery[ors.size()]));
+      if (nots.size() == 0) {
+        spanClauses.add(soq);
+      } else {
+        SpanOrQuery snqs = new SpanOrQuery((SpanQuery[]) nots
+            .toArray(new SpanQuery[nots.size()]));
+        SpanNotQuery snq = new SpanNotQuery(soq, snqs);
+        spanClauses.add(snq);
+      }
+    }
+
+    public String toString(String field) {
+      return "\"" + phrasedQueryStringContents + "\"";
+    }
+
+    public int hashCode() {
+      final int prime = 31;
+      int result = 1;
+      result = prime * result + ((field == null) ? 0 : field.hashCode());
+      result = prime
+          * result
+          + ((phrasedQueryStringContents == null) ? 0
+              : phrasedQueryStringContents.hashCode());
+      result = prime * result + slopFactor;
+      return result;
+    }
+
+    public boolean equals(Object obj) {
+      if (this == obj)
+        return true;
+      if (obj == null)
+        return false;
+      if (getClass() != obj.getClass())
+        return false;
+      ComplexPhraseQuery other = (ComplexPhraseQuery) obj;
+      if (field == null) {
+        if (other.field != null)
+          return false;
+      } else if (!field.equals(other.field))
+        return false;
+      if (phrasedQueryStringContents == null) {
+        if (other.phrasedQueryStringContents != null)
+          return false;
+      } else if (!phrasedQueryStringContents
+          .equals(other.phrasedQueryStringContents))
+        return false;
+      if (slopFactor != other.slopFactor)
+        return false;
+      return true;
+    }
+  }
+}
diff --git a/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/CharStream.java b/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/CharStream.java
new file mode 100644
index 0000000..bc3ca76
--- /dev/null
+++ b/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/CharStream.java
@@ -0,0 +1,110 @@
+/* Generated By:JavaCC: Do not edit this line. CharStream.java Version 3.0 */
+package org.apache.lucene.queryParser.precedence;
+
+/**
+ * This interface describes a character stream that maintains line and
+ * column number positions of the characters.  It also has the capability
+ * to backup the stream to some extent.  An implementation of this
+ * interface is used in the TokenManager implementation generated by
+ * JavaCCParser.
+ *
+ * All the methods except backup can be implemented in any fashion. backup
+ * needs to be implemented correctly for the correct operation of the lexer.
+ * Rest of the methods are all used to get information like line number,
+ * column number and the String that constitutes a token and are not used
+ * by the lexer. Hence their implementation won't affect the generated lexer's
+ * operation.
+ */
+
+public interface CharStream {
+
+  /**
+   * Returns the next character from the selected input.  The method
+   * of selecting the input is the responsibility of the class
+   * implementing this interface.  Can throw any java.io.IOException.
+   */
+  char readChar() throws java.io.IOException;
+
+  /**
+   * Returns the column position of the character last read.
+   * @deprecated 
+   * @see #getEndColumn
+   */
+  int getColumn();
+
+  /**
+   * Returns the line number of the character last read.
+   * @deprecated 
+   * @see #getEndLine
+   */
+  int getLine();
+
+  /**
+   * Returns the column number of the last character for current token (being
+   * matched after the last call to BeginTOken).
+   */
+  int getEndColumn();
+
+  /**
+   * Returns the line number of the last character for current token (being
+   * matched after the last call to BeginTOken).
+   */
+  int getEndLine();
+
+  /**
+   * Returns the column number of the first character for current token (being
+   * matched after the last call to BeginTOken).
+   */
+  int getBeginColumn();
+
+  /**
+   * Returns the line number of the first character for current token (being
+   * matched after the last call to BeginTOken).
+   */
+  int getBeginLine();
+
+  /**
+   * Backs up the input stream by amount steps. Lexer calls this method if it
+   * had already read some characters, but could not use them to match a
+   * (longer) token. So, they will be used again as the prefix of the next
+   * token and it is the implemetation's responsibility to do this right.
+   */
+  void backup(int amount);
+
+  /**
+   * Returns the next character that marks the beginning of the next token.
+   * All characters must remain in the buffer between two successive calls
+   * to this method to implement backup correctly.
+   */
+  char BeginToken() throws java.io.IOException;
+
+  /**
+   * Returns a string made up of characters from the marked token beginning 
+   * to the current buffer position. Implementations have the choice of returning
+   * anything that they want to. For example, for efficiency, one might decide
+   * to just return null, which is a valid implementation.
+   */
+  String GetImage();
+
+  /**
+   * Returns an array of characters that make up the suffix of length 'len' for
+   * the currently matched token. This is used to build up the matched string
+   * for use in actions in the case of MORE. A simple and inefficient
+   * implementation of this is as follows :
+   *
+   *   {
+   *      String t = GetImage();
+   *      return t.substring(t.length() - len, t.length()).toCharArray();
+   *   }
+   */
+  char[] GetSuffix(int len);
+
+  /**
+   * The lexer calls this function to indicate that it is done with the stream
+   * and hence implementations can free any resources held by this class.
+   * Again, the body of this function can be just empty and it will not
+   * affect the lexer's operation.
+   */
+  void Done();
+
+}
diff --git a/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/FastCharStream.java b/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/FastCharStream.java
new file mode 100644
index 0000000..5a7be60
--- /dev/null
+++ b/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/FastCharStream.java
@@ -0,0 +1,124 @@
+// FastCharStream.java
+package org.apache.lucene.queryParser.precedence;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.queryParser.*;
+
+import java.io.*;
+
+/** An efficient implementation of JavaCC's CharStream interface.  <p>Note that
+ * this does not do line-number counting, but instead keeps track of the
+ * character position of the token in the input, as required by Lucene's {@link
+ * org.apache.lucene.analysis.Token} API. */
+public final class FastCharStream implements CharStream {
+  char[] buffer = null;
+
+  int bufferLength = 0;				  // end of valid chars
+  int bufferPosition = 0;			  // next char to read
+
+  int tokenStart = 0;				  // offset in buffer
+  int bufferStart = 0;				  // position in file of buffer
+
+  Reader input;					  // source of chars
+
+  /** Constructs from a Reader. */
+  public FastCharStream(Reader r) {
+    input = r;
+  }
+
+  public final char readChar() throws IOException {
+    if (bufferPosition >= bufferLength)
+      refill();
+    return buffer[bufferPosition++];
+  }
+
+  private final void refill() throws IOException {
+    int newPosition = bufferLength - tokenStart;
+
+    if (tokenStart == 0) {			  // token won't fit in buffer
+      if (buffer == null) {			  // first time: alloc buffer
+	buffer = new char[2048];
+      } else if (bufferLength == buffer.length) { // grow buffer
+	char[] newBuffer = new char[buffer.length*2];
+	System.arraycopy(buffer, 0, newBuffer, 0, bufferLength);
+	buffer = newBuffer;
+      }
+    } else {					  // shift token to front
+      System.arraycopy(buffer, tokenStart, buffer, 0, newPosition);
+    }
+
+    bufferLength = newPosition;			  // update state
+    bufferPosition = newPosition;
+    bufferStart += tokenStart;
+    tokenStart = 0;
+
+    int charsRead =				  // fill space in buffer
+      input.read(buffer, newPosition, buffer.length-newPosition);
+    if (charsRead == -1)
+      throw new IOException("read past eof");
+    else
+      bufferLength += charsRead;
+  }
+
+  public final char BeginToken() throws IOException {
+    tokenStart = bufferPosition;
+    return readChar();
+  }
+
+  public final void backup(int amount) {
+    bufferPosition -= amount;
+  }
+
+  public final String GetImage() {
+    return new String(buffer, tokenStart, bufferPosition - tokenStart);
+  }
+
+  public final char[] GetSuffix(int len) {
+    char[] value = new char[len];
+    System.arraycopy(buffer, bufferPosition - len, value, 0, len);
+    return value;
+  }
+
+  public final void Done() {
+    try {
+      input.close();
+    } catch (IOException e) {
+      System.err.println("Caught: " + e + "; ignoring.");
+    }
+  }
+
+  public final int getColumn() {
+    return bufferStart + bufferPosition;
+  }
+  public final int getLine() {
+    return 1;
+  }
+  public final int getEndColumn() {
+    return bufferStart + bufferPosition;
+  }
+  public final int getEndLine() {
+    return 1;
+  }
+  public final int getBeginColumn() {
+    return bufferStart + tokenStart;
+  }
+  public final int getBeginLine() {
+    return 1;
+  }
+}
diff --git a/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/ParseException.java b/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/ParseException.java
new file mode 100644
index 0000000..0306496
--- /dev/null
+++ b/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/ParseException.java
@@ -0,0 +1,192 @@
+/* Generated By:JavaCC: Do not edit this line. ParseException.java Version 3.0 */
+package org.apache.lucene.queryParser.precedence;
+
+/**
+ * This exception is thrown when parse errors are encountered.
+ * You can explicitly create objects of this exception type by
+ * calling the method generateParseException in the generated
+ * parser.
+ *
+ * You can modify this class to customize your error reporting
+ * mechanisms so long as you retain the public fields.
+ */
+public class ParseException extends Exception {
+
+  /**
+   * This constructor is used by the method "generateParseException"
+   * in the generated parser.  Calling this constructor generates
+   * a new object of this type with the fields "currentToken",
+   * "expectedTokenSequences", and "tokenImage" set.  The boolean
+   * flag "specialConstructor" is also set to true to indicate that
+   * this constructor was used to create this object.
+   * This constructor calls its super class with the empty string
+   * to force the "toString" method of parent class "Throwable" to
+   * print the error message in the form:
+   *     ParseException: <result of getMessage>
+   */
+  public ParseException(Token currentTokenVal,
+                        int[][] expectedTokenSequencesVal,
+                        String[] tokenImageVal
+                       )
+  {
+    super("");
+    specialConstructor = true;
+    currentToken = currentTokenVal;
+    expectedTokenSequences = expectedTokenSequencesVal;
+    tokenImage = tokenImageVal;
+  }
+
+  /**
+   * The following constructors are for use by you for whatever
+   * purpose you can think of.  Constructing the exception in this
+   * manner makes the exception behave in the normal way - i.e., as
+   * documented in the class "Throwable".  The fields "errorToken",
+   * "expectedTokenSequences", and "tokenImage" do not contain
+   * relevant information.  The JavaCC generated code does not use
+   * these constructors.
+   */
+
+  public ParseException() {
+    super();
+    specialConstructor = false;
+  }
+
+  public ParseException(String message) {
+    super(message);
+    specialConstructor = false;
+  }
+
+  /**
+   * This variable determines which constructor was used to create
+   * this object and thereby affects the semantics of the
+   * "getMessage" method (see below).
+   */
+  protected boolean specialConstructor;
+
+  /**
+   * This is the last token that has been consumed successfully.  If
+   * this object has been created due to a parse error, the token
+   * followng this token will (therefore) be the first error token.
+   */
+  public Token currentToken;
+
+  /**
+   * Each entry in this array is an array of integers.  Each array
+   * of integers represents a sequence of tokens (by their ordinal
+   * values) that is expected at this point of the parse.
+   */
+  public int[][] expectedTokenSequences;
+
+  /**
+   * This is a reference to the "tokenImage" array of the generated
+   * parser within which the parse error occurred.  This array is
+   * defined in the generated ...Constants interface.
+   */
+  public String[] tokenImage;
+
+  /**
+   * This method has the standard behavior when this object has been
+   * created using the standard constructors.  Otherwise, it uses
+   * "currentToken" and "expectedTokenSequences" to generate a parse
+   * error message and returns it.  If this object has been created
+   * due to a parse error, and you do not catch it (it gets thrown
+   * from the parser), then this method is called during the printing
+   * of the final stack trace, and hence the correct error message
+   * gets displayed.
+   */
+  public String getMessage() {
+    if (!specialConstructor) {
+      return super.getMessage();
+    }
+    String expected = "";
+    int maxSize = 0;
+    for (int i = 0; i < expectedTokenSequences.length; i++) {
+      if (maxSize < expectedTokenSequences[i].length) {
+        maxSize = expectedTokenSequences[i].length;
+      }
+      for (int j = 0; j < expectedTokenSequences[i].length; j++) {
+        expected += tokenImage[expectedTokenSequences[i][j]] + " ";
+      }
+      if (expectedTokenSequences[i][expectedTokenSequences[i].length - 1] != 0) {
+        expected += "...";
+      }
+      expected += eol + "    ";
+    }
+    String retval = "Encountered \"";
+    Token tok = currentToken.next;
+    for (int i = 0; i < maxSize; i++) {
+      if (i != 0) retval += " ";
+      if (tok.kind == 0) {
+        retval += tokenImage[0];
+        break;
+      }
+      retval += add_escapes(tok.image);
+      tok = tok.next; 
+    }
+    retval += "\" at line " + currentToken.next.beginLine + ", column " + currentToken.next.beginColumn;
+    retval += "." + eol;
+    if (expectedTokenSequences.length == 1) {
+      retval += "Was expecting:" + eol + "    ";
+    } else {
+      retval += "Was expecting one of:" + eol + "    ";
+    }
+    retval += expected;
+    return retval;
+  }
+
+  /**
+   * The end of line string for this machine.
+   */
+  protected String eol = System.getProperty("line.separator", "\n");
+ 
+  /**
+   * Used to convert raw characters to their escaped version
+   * when these raw version cannot be used as part of an ASCII
+   * string literal.
+   */
+  protected String add_escapes(String str) {
+      StringBuffer retval = new StringBuffer();
+      char ch;
+      for (int i = 0; i < str.length(); i++) {
+        switch (str.charAt(i))
+        {
+           case 0 :
+              continue;
+           case '\b':
+              retval.append("\\b");
+              continue;
+           case '\t':
+              retval.append("\\t");
+              continue;
+           case '\n':
+              retval.append("\\n");
+              continue;
+           case '\f':
+              retval.append("\\f");
+              continue;
+           case '\r':
+              retval.append("\\r");
+              continue;
+           case '\"':
+              retval.append("\\\"");
+              continue;
+           case '\'':
+              retval.append("\\\'");
+              continue;
+           case '\\':
+              retval.append("\\\\");
+              continue;
+           default:
+              if ((ch = str.charAt(i)) < 0x20 || ch > 0x7e) {
+                 String s = "0000" + Integer.toString(ch, 16);
+                 retval.append("\\u" + s.substring(s.length() - 4, s.length()));
+              } else {
+                 retval.append(ch);
+              }
+              continue;
+        }
+      }
+      return retval.toString();
+   }
+
+}
diff --git a/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.java b/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.java
new file mode 100644
index 0000000..2423c5d
--- /dev/null
+++ b/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.java
@@ -0,0 +1,1345 @@
+/* Generated By:JavaCC: Do not edit this line. PrecedenceQueryParser.java */
+package org.apache.lucene.queryParser.precedence;
+
+import java.io.IOException;
+import java.io.StringReader;
+import java.text.DateFormat;
+import java.util.ArrayList;
+import java.util.Date;
+import java.util.List;
+import java.util.Locale;
+import java.util.Vector;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.document.DateTools;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.FuzzyQuery;
+import org.apache.lucene.search.MultiPhraseQuery;
+import org.apache.lucene.search.PhraseQuery;
+import org.apache.lucene.search.PrefixQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.RangeQuery;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.WildcardQuery;
+import org.apache.lucene.util.Parameter;
+
+/**
+ * Experimental query parser variant designed to handle operator precedence
+ * in a more sensible fashion than QueryParser.  There are still some
+ * open issues with this parser. The following tests are currently failing
+ * in TestPrecedenceQueryParser and are disabled to make this test pass:
+ * <ul>
+ * <li> testSimple
+ * <li> testWildcard
+ * <li> testPrecedence
+ * </ul>
+ *
+ * This class is generated by JavaCC.  The only method that clients should need
+ * to call is {@link #parse(String)}.
+ *
+ * The syntax for query strings is as follows:
+ * A Query is a series of clauses.
+ * A clause may be prefixed by:
+ * <ul>
+ * <li> a plus (<code>+</code>) or a minus (<code>-</code>) sign, indicating
+ * that the clause is required or prohibited respectively; or
+ * <li> a term followed by a colon, indicating the field to be searched.
+ * This enables one to construct queries which search multiple fields.
+ * </ul>
+ *
+ * A clause may be either:
+ * <ul>
+ * <li> a term, indicating all the documents that contain this term; or
+ * <li> a nested query, enclosed in parentheses.  Note that this may be used
+ * with a <code>+</code>/<code>-</code> prefix to require any of a set of
+ * terms.
+ * </ul>
+ *
+ * Thus, in BNF, the query grammar is:
+ * <pre>
+ *   Query  ::= ( Clause )*
+ *   Clause ::= ["+", "-"] [&lt;TERM&gt; ":"] ( &lt;TERM&gt; | "(" Query ")" )
+ * </pre>
+ *
+ * <p>
+ * Examples of appropriately formatted queries can be found in the <a
+ * href="../../../../../../../queryparsersyntax.html">query syntax
+ * documentation</a>.
+ * </p>
+ *
+ */
+
+public class PrecedenceQueryParser implements PrecedenceQueryParserConstants {
+
+  private static final int CONJ_NONE   = 0;
+  private static final int CONJ_AND    = 1;
+  private static final int CONJ_OR     = 2;
+
+  private static final int MOD_NONE    = 0;
+  private static final int MOD_NOT     = 10;
+  private static final int MOD_REQ     = 11;
+
+  // make it possible to call setDefaultOperator() without accessing
+  // the nested class:
+  public static final Operator AND_OPERATOR = Operator.AND;
+  public static final Operator OR_OPERATOR = Operator.OR;
+
+  /** The actual operator that parser uses to combine query terms */
+  private Operator operator = OR_OPERATOR;
+
+  boolean lowercaseExpandedTerms = true;
+
+  Analyzer analyzer;
+  String field;
+  int phraseSlop = 0;
+  float fuzzyMinSim = FuzzyQuery.defaultMinSimilarity;
+  int fuzzyPrefixLength = FuzzyQuery.defaultPrefixLength;
+  Locale locale = Locale.getDefault();
+
+  static final class Operator extends Parameter {
+    private Operator(String name) {
+      super(name);
+    }
+    static final Operator OR = new Operator("OR");
+    static final Operator AND = new Operator("AND");
+  }
+
+  /** Constructs a query parser.
+   *  @param f  the default field for query terms.
+   *  @param a   used to find terms in the query text.
+   */
+  public PrecedenceQueryParser(String f, Analyzer a) {
+    this(new FastCharStream(new StringReader("")));
+    analyzer = a;
+    field = f;
+  }
+
+  /** Parses a query string, returning a {@link org.apache.lucene.search.Query}.
+   *  @param expression  the query string to be parsed.
+   *  @throws ParseException if the parsing fails
+   */
+  public Query parse(String expression) throws ParseException {
+    // optimize empty query to be empty BooleanQuery
+    if (expression == null || expression.trim().length() == 0) {
+      return new BooleanQuery();
+    }
+
+    ReInit(new FastCharStream(new StringReader(expression)));
+    try {
+      Query query = Query(field);
+      return (query != null) ? query : new BooleanQuery();
+    }
+    catch (TokenMgrError tme) {
+      throw new ParseException(tme.getMessage());
+    }
+    catch (BooleanQuery.TooManyClauses tmc) {
+      throw new ParseException("Too many boolean clauses");
+    }
+  }
+
+   /**
+   * @return Returns the analyzer.
+   */
+  public Analyzer getAnalyzer() {
+    return analyzer;
+  }
+
+  /**
+   * @return Returns the field.
+   */
+  public String getField() {
+    return field;
+  }
+
+   /**
+   * Get the minimal similarity for fuzzy queries.
+   */
+  public float getFuzzyMinSim() {
+      return fuzzyMinSim;
+  }
+
+  /**
+   * Set the minimum similarity for fuzzy queries.
+   * Default is 0.5f.
+   */
+  public void setFuzzyMinSim(float fuzzyMinSim) {
+      this.fuzzyMinSim = fuzzyMinSim;
+  }
+
+   /**
+   * Get the prefix length for fuzzy queries. 
+   * @return Returns the fuzzyPrefixLength.
+   */
+  public int getFuzzyPrefixLength() {
+    return fuzzyPrefixLength;
+  }
+
+  /**
+   * Set the prefix length for fuzzy queries. Default is 0.
+   * @param fuzzyPrefixLength The fuzzyPrefixLength to set.
+   */
+  public void setFuzzyPrefixLength(int fuzzyPrefixLength) {
+    this.fuzzyPrefixLength = fuzzyPrefixLength;
+  }
+
+  /**
+   * Sets the default slop for phrases.  If zero, then exact phrase matches
+   * are required.  Default value is zero.
+   */
+  public void setPhraseSlop(int phraseSlop) {
+    this.phraseSlop = phraseSlop;
+  }
+
+  /**
+   * Gets the default slop for phrases.
+   */
+  public int getPhraseSlop() {
+    return phraseSlop;
+  }
+
+  /**
+   * Sets the boolean operator of the QueryParser.
+   * In default mode (<code>OR_OPERATOR</code>) terms without any modifiers
+   * are considered optional: for example <code>capital of Hungary</code> is equal to
+   * <code>capital OR of OR Hungary</code>.<br/>
+   * In <code>AND_OPERATOR</code> mode terms are considered to be in conjuction: the
+   * above mentioned query is parsed as <code>capital AND of AND Hungary</code>
+   */
+  public void setDefaultOperator(Operator op) {
+    this.operator = op;
+  }
+
+  /**
+   * Gets implicit operator setting, which will be either AND_OPERATOR
+   * or OR_OPERATOR.
+   */
+  public Operator getDefaultOperator() {
+    return operator;
+  }
+
+  /**
+   * Whether terms of wildcard, prefix, fuzzy and range queries are to be automatically
+   * lower-cased or not.  Default is <code>true</code>.
+   */
+  public void setLowercaseExpandedTerms(boolean lowercaseExpandedTerms) {
+    this.lowercaseExpandedTerms = lowercaseExpandedTerms;
+  }
+
+  /**
+   * @see #setLowercaseExpandedTerms(boolean)
+   */
+  public boolean getLowercaseExpandedTerms() {
+    return lowercaseExpandedTerms;
+  }
+
+  /**
+   * Set locale used by date range parsing.
+   */
+  public void setLocale(Locale locale) {
+    this.locale = locale;
+  }
+
+  /**
+   * Returns current locale, allowing access by subclasses.
+   */
+  public Locale getLocale() {
+    return locale;
+  }
+
+  /**
+   * @deprecated use {@link #addClause(List, int, int, Query)} instead.
+   */
+  protected void addClause(Vector clauses, int conj, int modifier, Query q) {
+    addClause((List) clauses, conj, modifier, q);
+  }
+
+  protected void addClause(List clauses, int conj, int modifier, Query q) {
+    boolean required, prohibited;
+
+    // If this term is introduced by AND, make the preceding term required,
+    // unless it's already prohibited
+    if (clauses.size() > 0 && conj == CONJ_AND) {
+      BooleanClause c = (BooleanClause) clauses.get(clauses.size()-1);
+      if (!c.isProhibited())
+        c.setOccur(BooleanClause.Occur.MUST);
+    }
+
+    if (clauses.size() > 0 && operator == AND_OPERATOR && conj == CONJ_OR) {
+      // If this term is introduced by OR, make the preceding term optional,
+      // unless it's prohibited (that means we leave -a OR b but +a OR b-->a OR b)
+      // notice if the input is a OR b, first term is parsed as required; without
+      // this modification a OR b would parsed as +a OR b
+      BooleanClause c = (BooleanClause) clauses.get(clauses.size()-1);
+      if (!c.isProhibited())
+        c.setOccur(BooleanClause.Occur.SHOULD);
+    }
+
+    // We might have been passed a null query; the term might have been
+    // filtered away by the analyzer.
+    if (q == null)
+      return;
+
+    if (operator == OR_OPERATOR) {
+      // We set REQUIRED if we're introduced by AND or +; PROHIBITED if
+      // introduced by NOT or -; make sure not to set both.
+      prohibited = (modifier == MOD_NOT);
+      required = (modifier == MOD_REQ);
+      if (conj == CONJ_AND && !prohibited) {
+        required = true;
+      }
+    } else {
+      // We set PROHIBITED if we're introduced by NOT or -; We set REQUIRED
+      // if not PROHIBITED and not introduced by OR
+      prohibited = (modifier == MOD_NOT);
+      required   = (!prohibited && conj != CONJ_OR);
+    }
+    if (required && !prohibited)
+      clauses.add(new BooleanClause(q, BooleanClause.Occur.MUST));
+    else if (!required && !prohibited)
+      clauses.add(new BooleanClause(q, BooleanClause.Occur.SHOULD));
+    else if (!required && prohibited)
+      clauses.add(new BooleanClause(q, BooleanClause.Occur.MUST_NOT));
+    else
+      throw new RuntimeException("Clause cannot be both required and prohibited");
+  }
+
+  /**
+   * @exception ParseException throw in overridden method to disallow
+   */
+  protected Query getFieldQuery(String field, String queryText)  throws ParseException {
+    // Use the analyzer to get all the tokens, and then build a TermQuery,
+    // PhraseQuery, or nothing based on the term count
+
+    TokenStream source = analyzer.tokenStream(field, new StringReader(queryText));
+    List list = new ArrayList();
+    final org.apache.lucene.analysis.Token reusableToken = new org.apache.lucene.analysis.Token();
+    org.apache.lucene.analysis.Token nextToken;
+    int positionCount = 0;
+    boolean severalTokensAtSamePosition = false;
+
+    while (true) {
+      try {
+        nextToken = source.next(reusableToken);
+      }
+      catch (IOException e) {
+        nextToken = null;
+      }
+      if (nextToken == null)
+        break;
+      list.add(nextToken.clone());
+      if (nextToken.getPositionIncrement() == 1)
+        positionCount++;
+      else
+        severalTokensAtSamePosition = true;
+    }
+    try {
+      source.close();
+    }
+    catch (IOException e) {
+      // ignore
+    }
+
+    if (list.size() == 0)
+      return null;
+    else if (list.size() == 1) {
+      nextToken = (org.apache.lucene.analysis.Token) list.get(0);
+      return new TermQuery(new Term(field, nextToken.term()));
+    } else {
+      if (severalTokensAtSamePosition) {
+        if (positionCount == 1) {
+          // no phrase query:
+          BooleanQuery q = new BooleanQuery();
+          for (int i = 0; i < list.size(); i++) {
+            nextToken = (org.apache.lucene.analysis.Token) list.get(i);
+            TermQuery currentQuery = new TermQuery(
+                new Term(field, nextToken.term()));
+            q.add(currentQuery, BooleanClause.Occur.SHOULD);
+          }
+          return q;
+        }
+        else {
+          // phrase query:
+          MultiPhraseQuery mpq = new MultiPhraseQuery();
+          List multiTerms = new ArrayList();
+          for (int i = 0; i < list.size(); i++) {
+            nextToken = (org.apache.lucene.analysis.Token) list.get(i);
+            if (nextToken.getPositionIncrement() == 1 && multiTerms.size() > 0) {
+              mpq.add((Term[])multiTerms.toArray(new Term[0]));
+              multiTerms.clear();
+            }
+            multiTerms.add(new Term(field, nextToken.term()));
+          }
+          mpq.add((Term[])multiTerms.toArray(new Term[0]));
+          return mpq;
+        }
+      }
+      else {
+        PhraseQuery q = new PhraseQuery();
+        q.setSlop(phraseSlop);
+        for (int i = 0; i < list.size(); i++) {
+          q.add(new Term(field, ((org.apache.lucene.analysis.Token)
+              list.get(i)).term()));
+        }
+        return q;
+      }
+    }
+  }
+
+  /**
+   * Base implementation delegates to {@link #getFieldQuery(String,String)}.
+   * This method may be overridden, for example, to return
+   * a SpanNearQuery instead of a PhraseQuery.
+   *
+   * @exception ParseException throw in overridden method to disallow
+   */
+  protected Query getFieldQuery(String field, String queryText, int slop)
+        throws ParseException {
+    Query query = getFieldQuery(field, queryText);
+
+    if (query instanceof PhraseQuery) {
+      ((PhraseQuery) query).setSlop(slop);
+    }
+    if (query instanceof MultiPhraseQuery) {
+      ((MultiPhraseQuery) query).setSlop(slop);
+    }
+
+    return query;
+  }
+
+  /**
+   * @exception ParseException throw in overridden method to disallow
+   */
+  protected Query getRangeQuery(String field,
+                                String part1,
+                                String part2,
+                                boolean inclusive) throws ParseException
+  {
+    if (lowercaseExpandedTerms) {
+      part1 = part1.toLowerCase();
+      part2 = part2.toLowerCase();
+    }
+    try {
+      DateFormat df = DateFormat.getDateInstance(DateFormat.SHORT, locale);
+      df.setLenient(true);
+      Date d1 = df.parse(part1);
+      Date d2 = df.parse(part2);
+      part1 = DateTools.dateToString(d1, DateTools.Resolution.DAY);
+      part2 = DateTools.dateToString(d2, DateTools.Resolution.DAY);
+    }
+    catch (Exception e) { }
+
+    return new RangeQuery(new Term(field, part1),
+                          new Term(field, part2),
+                          inclusive);
+  }
+
+  /**
+   * Factory method for generating query, given a set of clauses.
+   * By default creates a boolean query composed of clauses passed in.
+   *
+   * Can be overridden by extending classes, to modify query being
+   * returned.
+   *
+   * @param clauses List that contains {@link BooleanClause} instances
+   *    to join.
+   *
+   * @return Resulting {@link Query} object.
+   * @exception ParseException throw in overridden method to disallow
+   * @deprecated use {@link #getBooleanQuery(List)} instead
+   */
+  protected Query getBooleanQuery(Vector clauses) throws ParseException
+  {
+    return getBooleanQuery((List) clauses, false);
+  }
+
+  /**
+   * Factory method for generating query, given a set of clauses.
+   * By default creates a boolean query composed of clauses passed in.
+   *
+   * Can be overridden by extending classes, to modify query being
+   * returned.
+   *
+   * @param clauses List that contains {@link BooleanClause} instances
+   *    to join.
+   *
+   * @return Resulting {@link Query} object.
+   * @exception ParseException throw in overridden method to disallow
+   */
+  protected Query getBooleanQuery(List clauses) throws ParseException
+  {
+    return getBooleanQuery(clauses, false);
+  }
+
+  /**
+   * Factory method for generating query, given a set of clauses.
+   * By default creates a boolean query composed of clauses passed in.
+   *
+   * Can be overridden by extending classes, to modify query being
+   * returned.
+   *
+   * @param clauses List that contains {@link BooleanClause} instances
+   *    to join.
+   * @param disableCoord true if coord scoring should be disabled.
+   *
+   * @return Resulting {@link Query} object.
+   * @exception ParseException throw in overridden method to disallow
+   * @deprecated use {@link #getBooleanQuery(List, boolean)} instead
+   */
+  protected Query getBooleanQuery(Vector clauses, boolean disableCoord)
+    throws ParseException
+  {
+    return getBooleanQuery((List) clauses, disableCoord);
+  }
+
+  /**
+   * Factory method for generating query, given a set of clauses.
+   * By default creates a boolean query composed of clauses passed in.
+   *
+   * Can be overridden by extending classes, to modify query being
+   * returned.
+   *
+   * @param clauses List that contains {@link BooleanClause} instances
+   *    to join.
+   * @param disableCoord true if coord scoring should be disabled.
+   *
+   * @return Resulting {@link Query} object.
+   * @exception ParseException throw in overridden method to disallow
+   */
+  protected Query getBooleanQuery(List clauses, boolean disableCoord)
+      throws ParseException {
+    if (clauses == null || clauses.size() == 0)
+      return null;
+
+    BooleanQuery query = new BooleanQuery(disableCoord);
+    for (int i = 0; i < clauses.size(); i++) {
+      query.add((BooleanClause)clauses.get(i));
+    }
+    return query;
+  }
+
+  /**
+   * Factory method for generating a query. Called when parser
+   * parses an input term token that contains one or more wildcard
+   * characters (? and *), but is not a prefix term token (one
+   * that has just a single * character at the end)
+   *<p>
+   * Depending on settings, prefix term may be lower-cased
+   * automatically. It will not go through the default Analyzer,
+   * however, since normal Analyzers are unlikely to work properly
+   * with wildcard templates.
+   *<p>
+   * Can be overridden by extending classes, to provide custom handling for
+   * wildcard queries, which may be necessary due to missing analyzer calls.
+   *
+   * @param field Name of the field query will use.
+   * @param termStr Term token that contains one or more wild card
+   *   characters (? or *), but is not simple prefix term
+   *
+   * @return Resulting {@link Query} built for the term
+   * @exception ParseException throw in overridden method to disallow
+   */
+  protected Query getWildcardQuery(String field, String termStr) throws ParseException
+  {
+    if (lowercaseExpandedTerms) {
+      termStr = termStr.toLowerCase();
+    }
+    Term t = new Term(field, termStr);
+    return new WildcardQuery(t);
+  }
+
+  /**
+   * Factory method for generating a query (similar to
+   * {@link #getWildcardQuery}). Called when parser parses an input term
+   * token that uses prefix notation; that is, contains a single '*' wildcard
+   * character as its last character. Since this is a special case
+   * of generic wildcard term, and such a query can be optimized easily,
+   * this usually results in a different query object.
+   *<p>
+   * Depending on settings, a prefix term may be lower-cased
+   * automatically. It will not go through the default Analyzer,
+   * however, since normal Analyzers are unlikely to work properly
+   * with wildcard templates.
+   *<p>
+   * Can be overridden by extending classes, to provide custom handling for
+   * wild card queries, which may be necessary due to missing analyzer calls.
+   *
+   * @param field Name of the field query will use.
+   * @param termStr Term token to use for building term for the query
+   *    (<b>without</b> trailing '*' character!)
+   *
+   * @return Resulting {@link Query} built for the term
+   * @exception ParseException throw in overridden method to disallow
+   */
+  protected Query getPrefixQuery(String field, String termStr) throws ParseException
+  {
+    if (lowercaseExpandedTerms) {
+      termStr = termStr.toLowerCase();
+    }
+    Term t = new Term(field, termStr);
+    return new PrefixQuery(t);
+  }
+
+   /**
+   * Factory method for generating a query (similar to
+   * {@link #getWildcardQuery}). Called when parser parses
+   * an input term token that has the fuzzy suffix (~) appended.
+   *
+   * @param field Name of the field query will use.
+   * @param termStr Term token to use for building term for the query
+   *
+   * @return Resulting {@link Query} built for the term
+   * @exception ParseException throw in overridden method to disallow
+   */
+  protected Query getFuzzyQuery(String field, String termStr, float minSimilarity) throws ParseException
+  {
+    if (lowercaseExpandedTerms) {
+      termStr = termStr.toLowerCase();
+    }
+    Term t = new Term(field, termStr);
+    return new FuzzyQuery(t, minSimilarity, fuzzyPrefixLength);
+  }
+
+  /**
+   * Returns a String where the escape char has been
+   * removed, or kept only once if there was a double escape.
+   */
+  private String discardEscapeChar(String input) {
+    char[] caSource = input.toCharArray();
+    char[] caDest = new char[caSource.length];
+    int j = 0;
+    for (int i = 0; i < caSource.length; i++) {
+      if ((caSource[i] != '\\') || (i > 0 && caSource[i-1] == '\\')) {
+        caDest[j++]=caSource[i];
+      }
+    }
+    return new String(caDest, 0, j);
+  }
+
+  /**
+   * Returns a String where those characters that QueryParser
+   * expects to be escaped are escaped by a preceding <code>\</code>.
+   */
+  public static String escape(String s) {
+    StringBuffer sb = new StringBuffer();
+    for (int i = 0; i < s.length(); i++) {
+      char c = s.charAt(i);
+      // NOTE: keep this in sync with _ESCAPED_CHAR below!
+      if (c == '\\' || c == '+' || c == '-' || c == '!' || c == '(' || c == ')' || c == ':'
+        || c == '^' || c == '[' || c == ']' || c == '\"' || c == '{' || c == '}' || c == '~'
+        || c == '*' || c == '?') {
+        sb.append('\\');
+      }
+      sb.append(c);
+    }
+    return sb.toString();
+  }
+
+  /**
+   * Command line tool to test QueryParser, using {@link org.apache.lucene.analysis.SimpleAnalyzer}.
+   * Usage:<br>
+   * <code>java org.apache.lucene.queryParser.QueryParser &lt;input&gt;</code>
+   */
+  public static void main(String[] args) throws Exception {
+    if (args.length == 0) {
+      System.out.println("Usage: java org.apache.lucene.queryParser.QueryParser <input>");
+      System.exit(0);
+    }
+    PrecedenceQueryParser qp = new PrecedenceQueryParser("field",
+                           new org.apache.lucene.analysis.SimpleAnalyzer());
+    Query q = qp.parse(args[0]);
+    System.out.println(q.toString("field"));
+  }
+
+// *   Query  ::= ( Clause )*
+// *   Clause ::= ["+", "-"] [<TERM> ":"] ( <TERM> | "(" Query ")" )
+  final public int Conjunction() throws ParseException {
+  int ret = CONJ_NONE;
+    switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+    case AND:
+    case OR:
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case AND:
+        jj_consume_token(AND);
+            ret = CONJ_AND;
+        break;
+      case OR:
+        jj_consume_token(OR);
+              ret = CONJ_OR;
+        break;
+      default:
+        jj_la1[0] = jj_gen;
+        jj_consume_token(-1);
+        throw new ParseException();
+      }
+      break;
+    default:
+      jj_la1[1] = jj_gen;
+      ;
+    }
+    {if (true) return ret;}
+    throw new Error("Missing return statement in function");
+  }
+
+  final public int Modifier() throws ParseException {
+  int ret = MOD_NONE;
+    switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+    case NOT:
+    case PLUS:
+    case MINUS:
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case PLUS:
+        jj_consume_token(PLUS);
+              ret = MOD_REQ;
+        break;
+      case MINUS:
+        jj_consume_token(MINUS);
+                 ret = MOD_NOT;
+        break;
+      case NOT:
+        jj_consume_token(NOT);
+               ret = MOD_NOT;
+        break;
+      default:
+        jj_la1[2] = jj_gen;
+        jj_consume_token(-1);
+        throw new ParseException();
+      }
+      break;
+    default:
+      jj_la1[3] = jj_gen;
+      ;
+    }
+    {if (true) return ret;}
+    throw new Error("Missing return statement in function");
+  }
+
+  final public Query Query(String field) throws ParseException {
+  List clauses = new ArrayList();
+  Query q, firstQuery=null;
+  boolean orPresent = false;
+  int modifier;
+    modifier = Modifier();
+    q = andExpression(field);
+    addClause(clauses, CONJ_NONE, modifier, q);
+    if (modifier == MOD_NONE)
+      firstQuery = q;
+    label_1:
+    while (true) {
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case OR:
+      case NOT:
+      case PLUS:
+      case MINUS:
+      case LPAREN:
+      case QUOTED:
+      case TERM:
+      case PREFIXTERM:
+      case WILDTERM:
+      case RANGEIN_START:
+      case RANGEEX_START:
+      case NUMBER:
+        ;
+        break;
+      default:
+        jj_la1[4] = jj_gen;
+        break label_1;
+      }
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case OR:
+        jj_consume_token(OR);
+            orPresent=true;
+        break;
+      default:
+        jj_la1[5] = jj_gen;
+        ;
+      }
+      modifier = Modifier();
+      q = andExpression(field);
+      addClause(clauses, orPresent ? CONJ_OR : CONJ_NONE, modifier, q);
+    }
+      if (clauses.size() == 1 && firstQuery != null)
+        {if (true) return firstQuery;}
+      else {
+        {if (true) return getBooleanQuery(clauses);}
+      }
+    throw new Error("Missing return statement in function");
+  }
+
+  final public Query andExpression(String field) throws ParseException {
+  List clauses = new ArrayList();
+  Query q, firstQuery=null;
+  int modifier;
+    q = Clause(field);
+    addClause(clauses, CONJ_NONE, MOD_NONE, q);
+    firstQuery = q;
+    label_2:
+    while (true) {
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case AND:
+        ;
+        break;
+      default:
+        jj_la1[6] = jj_gen;
+        break label_2;
+      }
+      jj_consume_token(AND);
+      modifier = Modifier();
+      q = Clause(field);
+      addClause(clauses, CONJ_AND, modifier, q);
+    }
+      if (clauses.size() == 1 && firstQuery != null)
+        {if (true) return firstQuery;}
+      else {
+        {if (true) return getBooleanQuery(clauses);}
+      }
+    throw new Error("Missing return statement in function");
+  }
+
+  final public Query Clause(String field) throws ParseException {
+  Query q;
+  Token fieldToken=null, boost=null;
+    if (jj_2_1(2)) {
+      fieldToken = jj_consume_token(TERM);
+      jj_consume_token(COLON);
+      field=discardEscapeChar(fieldToken.image);
+    } else {
+      ;
+    }
+    switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+    case QUOTED:
+    case TERM:
+    case PREFIXTERM:
+    case WILDTERM:
+    case RANGEIN_START:
+    case RANGEEX_START:
+    case NUMBER:
+      q = Term(field);
+      break;
+    case LPAREN:
+      jj_consume_token(LPAREN);
+      q = Query(field);
+      jj_consume_token(RPAREN);
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case CARAT:
+        jj_consume_token(CARAT);
+        boost = jj_consume_token(NUMBER);
+        break;
+      default:
+        jj_la1[7] = jj_gen;
+        ;
+      }
+      break;
+    default:
+      jj_la1[8] = jj_gen;
+      jj_consume_token(-1);
+      throw new ParseException();
+    }
+      if (boost != null) {
+        float f = (float)1.0;
+  try {
+    f = Float.valueOf(boost.image).floatValue();
+          q.setBoost(f);
+  } catch (Exception ignored) { }
+      }
+      {if (true) return q;}
+    throw new Error("Missing return statement in function");
+  }
+
+  final public Query Term(String field) throws ParseException {
+  Token term, boost=null, fuzzySlop=null, goop1, goop2;
+  boolean prefix = false;
+  boolean wildcard = false;
+  boolean fuzzy = false;
+  Query q;
+    switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+    case TERM:
+    case PREFIXTERM:
+    case WILDTERM:
+    case NUMBER:
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case TERM:
+        term = jj_consume_token(TERM);
+        break;
+      case PREFIXTERM:
+        term = jj_consume_token(PREFIXTERM);
+                             prefix=true;
+        break;
+      case WILDTERM:
+        term = jj_consume_token(WILDTERM);
+                           wildcard=true;
+        break;
+      case NUMBER:
+        term = jj_consume_token(NUMBER);
+        break;
+      default:
+        jj_la1[9] = jj_gen;
+        jj_consume_token(-1);
+        throw new ParseException();
+      }
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case FUZZY_SLOP:
+        fuzzySlop = jj_consume_token(FUZZY_SLOP);
+                                fuzzy=true;
+        break;
+      default:
+        jj_la1[10] = jj_gen;
+        ;
+      }
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case CARAT:
+        jj_consume_token(CARAT);
+        boost = jj_consume_token(NUMBER);
+        switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+        case FUZZY_SLOP:
+          fuzzySlop = jj_consume_token(FUZZY_SLOP);
+                                                         fuzzy=true;
+          break;
+        default:
+          jj_la1[11] = jj_gen;
+          ;
+        }
+        break;
+      default:
+        jj_la1[12] = jj_gen;
+        ;
+      }
+       String termImage=discardEscapeChar(term.image);
+       if (wildcard) {
+       q = getWildcardQuery(field, termImage);
+       } else if (prefix) {
+         q = getPrefixQuery(field,
+           discardEscapeChar(term.image.substring
+          (0, term.image.length()-1)));
+       } else if (fuzzy) {
+          float fms = fuzzyMinSim;
+          try {
+            fms = Float.valueOf(fuzzySlop.image.substring(1)).floatValue();
+          } catch (Exception ignored) { }
+         if(fms < 0.0f || fms > 1.0f){
+           {if (true) throw new ParseException("Minimum similarity for a FuzzyQuery has to be between 0.0f and 1.0f !");}
+         }
+         q = getFuzzyQuery(field, termImage, fms);
+       } else {
+         q = getFieldQuery(field, termImage);
+       }
+      break;
+    case RANGEIN_START:
+      jj_consume_token(RANGEIN_START);
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case RANGEIN_GOOP:
+        goop1 = jj_consume_token(RANGEIN_GOOP);
+        break;
+      case RANGEIN_QUOTED:
+        goop1 = jj_consume_token(RANGEIN_QUOTED);
+        break;
+      default:
+        jj_la1[13] = jj_gen;
+        jj_consume_token(-1);
+        throw new ParseException();
+      }
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case RANGEIN_TO:
+        jj_consume_token(RANGEIN_TO);
+        break;
+      default:
+        jj_la1[14] = jj_gen;
+        ;
+      }
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case RANGEIN_GOOP:
+        goop2 = jj_consume_token(RANGEIN_GOOP);
+        break;
+      case RANGEIN_QUOTED:
+        goop2 = jj_consume_token(RANGEIN_QUOTED);
+        break;
+      default:
+        jj_la1[15] = jj_gen;
+        jj_consume_token(-1);
+        throw new ParseException();
+      }
+      jj_consume_token(RANGEIN_END);
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case CARAT:
+        jj_consume_token(CARAT);
+        boost = jj_consume_token(NUMBER);
+        break;
+      default:
+        jj_la1[16] = jj_gen;
+        ;
+      }
+          if (goop1.kind == RANGEIN_QUOTED) {
+            goop1.image = goop1.image.substring(1, goop1.image.length()-1);
+          } else {
+            goop1.image = discardEscapeChar(goop1.image);
+          }
+          if (goop2.kind == RANGEIN_QUOTED) {
+            goop2.image = goop2.image.substring(1, goop2.image.length()-1);
+      } else {
+        goop2.image = discardEscapeChar(goop2.image);
+      }
+          q = getRangeQuery(field, goop1.image, goop2.image, true);
+      break;
+    case RANGEEX_START:
+      jj_consume_token(RANGEEX_START);
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case RANGEEX_GOOP:
+        goop1 = jj_consume_token(RANGEEX_GOOP);
+        break;
+      case RANGEEX_QUOTED:
+        goop1 = jj_consume_token(RANGEEX_QUOTED);
+        break;
+      default:
+        jj_la1[17] = jj_gen;
+        jj_consume_token(-1);
+        throw new ParseException();
+      }
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case RANGEEX_TO:
+        jj_consume_token(RANGEEX_TO);
+        break;
+      default:
+        jj_la1[18] = jj_gen;
+        ;
+      }
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case RANGEEX_GOOP:
+        goop2 = jj_consume_token(RANGEEX_GOOP);
+        break;
+      case RANGEEX_QUOTED:
+        goop2 = jj_consume_token(RANGEEX_QUOTED);
+        break;
+      default:
+        jj_la1[19] = jj_gen;
+        jj_consume_token(-1);
+        throw new ParseException();
+      }
+      jj_consume_token(RANGEEX_END);
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case CARAT:
+        jj_consume_token(CARAT);
+        boost = jj_consume_token(NUMBER);
+        break;
+      default:
+        jj_la1[20] = jj_gen;
+        ;
+      }
+          if (goop1.kind == RANGEEX_QUOTED) {
+            goop1.image = goop1.image.substring(1, goop1.image.length()-1);
+          } else {
+            goop1.image = discardEscapeChar(goop1.image);
+          }
+          if (goop2.kind == RANGEEX_QUOTED) {
+            goop2.image = goop2.image.substring(1, goop2.image.length()-1);
+      } else {
+        goop2.image = discardEscapeChar(goop2.image);
+      }
+
+          q = getRangeQuery(field, goop1.image, goop2.image, false);
+      break;
+    case QUOTED:
+      term = jj_consume_token(QUOTED);
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case FUZZY_SLOP:
+        fuzzySlop = jj_consume_token(FUZZY_SLOP);
+        break;
+      default:
+        jj_la1[21] = jj_gen;
+        ;
+      }
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case CARAT:
+        jj_consume_token(CARAT);
+        boost = jj_consume_token(NUMBER);
+        break;
+      default:
+        jj_la1[22] = jj_gen;
+        ;
+      }
+         int s = phraseSlop;
+
+         if (fuzzySlop != null) {
+           try {
+             s = Float.valueOf(fuzzySlop.image.substring(1)).intValue();
+           }
+           catch (Exception ignored) { }
+         }
+         q = getFieldQuery(field, term.image.substring(1, term.image.length()-1), s);
+      break;
+    default:
+      jj_la1[23] = jj_gen;
+      jj_consume_token(-1);
+      throw new ParseException();
+    }
+    if (boost != null) {
+      float f = (float) 1.0;
+      try {
+        f = Float.valueOf(boost.image).floatValue();
+      }
+      catch (Exception ignored) {
+    /* Should this be handled somehow? (defaults to "no boost", if
+     * boost number is invalid)
+     */
+      }
+
+      // avoid boosting null queries, such as those caused by stop words
+      if (q != null) {
+        q.setBoost(f);
+      }
+    }
+    {if (true) return q;}
+    throw new Error("Missing return statement in function");
+  }
+
+  final private boolean jj_2_1(int xla) {
+    jj_la = xla; jj_lastpos = jj_scanpos = token;
+    try { return !jj_3_1(); }
+    catch(LookaheadSuccess ls) { return true; }
+    finally { jj_save(0, xla); }
+  }
+
+  final private boolean jj_3_1() {
+    if (jj_scan_token(TERM)) return true;
+    if (jj_scan_token(COLON)) return true;
+    return false;
+  }
+
+  public PrecedenceQueryParserTokenManager token_source;
+  public Token token, jj_nt;
+  private int jj_ntk;
+  private Token jj_scanpos, jj_lastpos;
+  private int jj_la;
+  public boolean lookingAhead = false;
+  private boolean jj_semLA;
+  private int jj_gen;
+  final private int[] jj_la1 = new int[24];
+  static private int[] jj_la1_0;
+  static {
+      jj_la1_0();
+   }
+   private static void jj_la1_0() {
+      jj_la1_0 = new int[] {0x180,0x180,0xe00,0xe00,0xfb1f00,0x100,0x80,0x8000,0xfb1000,0x9a0000,0x40000,0x40000,0x8000,0xc000000,0x1000000,0xc000000,0x8000,0xc0000000,0x10000000,0xc0000000,0x8000,0x40000,0x8000,0xfb0000,};
+   }
+  final private JJCalls[] jj_2_rtns = new JJCalls[1];
+  private boolean jj_rescan = false;
+  private int jj_gc = 0;
+
+  public PrecedenceQueryParser(CharStream stream) {
+    token_source = new PrecedenceQueryParserTokenManager(stream);
+    token = new Token();
+    jj_ntk = -1;
+    jj_gen = 0;
+    for (int i = 0; i < 24; i++) jj_la1[i] = -1;
+    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
+  }
+
+  public void ReInit(CharStream stream) {
+    token_source.ReInit(stream);
+    token = new Token();
+    jj_ntk = -1;
+    jj_gen = 0;
+    for (int i = 0; i < 24; i++) jj_la1[i] = -1;
+    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
+  }
+
+  public PrecedenceQueryParser(PrecedenceQueryParserTokenManager tm) {
+    token_source = tm;
+    token = new Token();
+    jj_ntk = -1;
+    jj_gen = 0;
+    for (int i = 0; i < 24; i++) jj_la1[i] = -1;
+    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
+  }
+
+  public void ReInit(PrecedenceQueryParserTokenManager tm) {
+    token_source = tm;
+    token = new Token();
+    jj_ntk = -1;
+    jj_gen = 0;
+    for (int i = 0; i < 24; i++) jj_la1[i] = -1;
+    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
+  }
+
+  final private Token jj_consume_token(int kind) throws ParseException {
+    Token oldToken;
+    if ((oldToken = token).next != null) token = token.next;
+    else token = token.next = token_source.getNextToken();
+    jj_ntk = -1;
+    if (token.kind == kind) {
+      jj_gen++;
+      if (++jj_gc > 100) {
+        jj_gc = 0;
+        for (int i = 0; i < jj_2_rtns.length; i++) {
+          JJCalls c = jj_2_rtns[i];
+          while (c != null) {
+            if (c.gen < jj_gen) c.first = null;
+            c = c.next;
+          }
+        }
+      }
+      return token;
+    }
+    token = oldToken;
+    jj_kind = kind;
+    throw generateParseException();
+  }
+
+  static private final class LookaheadSuccess extends java.lang.Error { }
+  final private LookaheadSuccess jj_ls = new LookaheadSuccess();
+  final private boolean jj_scan_token(int kind) {
+    if (jj_scanpos == jj_lastpos) {
+      jj_la--;
+      if (jj_scanpos.next == null) {
+        jj_lastpos = jj_scanpos = jj_scanpos.next = token_source.getNextToken();
+      } else {
+        jj_lastpos = jj_scanpos = jj_scanpos.next;
+      }
+    } else {
+      jj_scanpos = jj_scanpos.next;
+    }
+    if (jj_rescan) {
+      int i = 0; Token tok = token;
+      while (tok != null && tok != jj_scanpos) { i++; tok = tok.next; }
+      if (tok != null) jj_add_error_token(kind, i);
+    }
+    if (jj_scanpos.kind != kind) return true;
+    if (jj_la == 0 && jj_scanpos == jj_lastpos) throw jj_ls;
+    return false;
+  }
+
+  final public Token getNextToken() {
+    if (token.next != null) token = token.next;
+    else token = token.next = token_source.getNextToken();
+    jj_ntk = -1;
+    jj_gen++;
+    return token;
+  }
+
+  final public Token getToken(int index) {
+    Token t = lookingAhead ? jj_scanpos : token;
+    for (int i = 0; i < index; i++) {
+      if (t.next != null) t = t.next;
+      else t = t.next = token_source.getNextToken();
+    }
+    return t;
+  }
+
+  final private int jj_ntk() {
+    if ((jj_nt=token.next) == null)
+      return (jj_ntk = (token.next=token_source.getNextToken()).kind);
+    else
+      return (jj_ntk = jj_nt.kind);
+  }
+
+  private java.util.Vector jj_expentries = new java.util.Vector();
+  private int[] jj_expentry;
+  private int jj_kind = -1;
+  private int[] jj_lasttokens = new int[100];
+  private int jj_endpos;
+
+  private void jj_add_error_token(int kind, int pos) {
+    if (pos >= 100) return;
+    if (pos == jj_endpos + 1) {
+      jj_lasttokens[jj_endpos++] = kind;
+    } else if (jj_endpos != 0) {
+      jj_expentry = new int[jj_endpos];
+      for (int i = 0; i < jj_endpos; i++) {
+        jj_expentry[i] = jj_lasttokens[i];
+      }
+      boolean exists = false;
+      for (java.util.Enumeration e = jj_expentries.elements(); e.hasMoreElements();) {
+        int[] oldentry = (int[])(e.nextElement());
+        if (oldentry.length == jj_expentry.length) {
+          exists = true;
+          for (int i = 0; i < jj_expentry.length; i++) {
+            if (oldentry[i] != jj_expentry[i]) {
+              exists = false;
+              break;
+            }
+          }
+          if (exists) break;
+        }
+      }
+      if (!exists) jj_expentries.addElement(jj_expentry);
+      if (pos != 0) jj_lasttokens[(jj_endpos = pos) - 1] = kind;
+    }
+  }
+
+  public ParseException generateParseException() {
+    jj_expentries.removeAllElements();
+    boolean[] la1tokens = new boolean[32];
+    for (int i = 0; i < 32; i++) {
+      la1tokens[i] = false;
+    }
+    if (jj_kind >= 0) {
+      la1tokens[jj_kind] = true;
+      jj_kind = -1;
+    }
+    for (int i = 0; i < 24; i++) {
+      if (jj_la1[i] == jj_gen) {
+        for (int j = 0; j < 32; j++) {
+          if ((jj_la1_0[i] & (1<<j)) != 0) {
+            la1tokens[j] = true;
+          }
+        }
+      }
+    }
+    for (int i = 0; i < 32; i++) {
+      if (la1tokens[i]) {
+        jj_expentry = new int[1];
+        jj_expentry[0] = i;
+        jj_expentries.addElement(jj_expentry);
+      }
+    }
+    jj_endpos = 0;
+    jj_rescan_token();
+    jj_add_error_token(0, 0);
+    int[][] exptokseq = new int[jj_expentries.size()][];
+    for (int i = 0; i < jj_expentries.size(); i++) {
+      exptokseq[i] = (int[])jj_expentries.elementAt(i);
+    }
+    return new ParseException(token, exptokseq, tokenImage);
+  }
+
+  final public void enable_tracing() {
+  }
+
+  final public void disable_tracing() {
+  }
+
+  final private void jj_rescan_token() {
+    jj_rescan = true;
+    for (int i = 0; i < 1; i++) {
+      JJCalls p = jj_2_rtns[i];
+      do {
+        if (p.gen > jj_gen) {
+          jj_la = p.arg; jj_lastpos = jj_scanpos = p.first;
+          switch (i) {
+            case 0: jj_3_1(); break;
+          }
+        }
+        p = p.next;
+      } while (p != null);
+    }
+    jj_rescan = false;
+  }
+
+  final private void jj_save(int index, int xla) {
+    JJCalls p = jj_2_rtns[index];
+    while (p.gen > jj_gen) {
+      if (p.next == null) { p = p.next = new JJCalls(); break; }
+      p = p.next;
+    }
+    p.gen = jj_gen + xla - jj_la; p.first = token; p.arg = xla;
+  }
+
+  static final class JJCalls {
+    int gen;
+    Token first;
+    int arg;
+    JJCalls next;
+  }
+
+}
diff --git a/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.jj b/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.jj
new file mode 100644
index 0000000..9d090b8
--- /dev/null
+++ b/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.jj
@@ -0,0 +1,968 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+options {
+  STATIC=false;
+  JAVA_UNICODE_ESCAPE=true;
+  USER_CHAR_STREAM=true;
+}
+
+PARSER_BEGIN(PrecedenceQueryParser)
+
+package org.apache.lucene.queryParser.precedence;
+
+import java.io.IOException;
+import java.io.StringReader;
+import java.text.DateFormat;
+import java.util.ArrayList;
+import java.util.Date;
+import java.util.List;
+import java.util.Locale;
+import java.util.Vector;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.document.DateTools;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.FuzzyQuery;
+import org.apache.lucene.search.MultiPhraseQuery;
+import org.apache.lucene.search.PhraseQuery;
+import org.apache.lucene.search.PrefixQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.RangeQuery;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.WildcardQuery;
+import org.apache.lucene.util.Parameter;
+
+/**
+ * Experimental query parser variant designed to handle operator precedence
+ * in a more sensible fashion than QueryParser.  There are still some
+ * open issues with this parser. The following tests are currently failing
+ * in TestPrecedenceQueryParser and are disabled to make this test pass:
+ * <ul>
+ * <li> testSimple
+ * <li> testWildcard
+ * <li> testPrecedence
+ * </ul>
+ *
+ * This class is generated by JavaCC.  The only method that clients should need
+ * to call is {@link #parse(String)}.
+ *
+ * The syntax for query strings is as follows:
+ * A Query is a series of clauses.
+ * A clause may be prefixed by:
+ * <ul>
+ * <li> a plus (<code>+</code>) or a minus (<code>-</code>) sign, indicating
+ * that the clause is required or prohibited respectively; or
+ * <li> a term followed by a colon, indicating the field to be searched.
+ * This enables one to construct queries which search multiple fields.
+ * </ul>
+ *
+ * A clause may be either:
+ * <ul>
+ * <li> a term, indicating all the documents that contain this term; or
+ * <li> a nested query, enclosed in parentheses.  Note that this may be used
+ * with a <code>+</code>/<code>-</code> prefix to require any of a set of
+ * terms.
+ * </ul>
+ *
+ * Thus, in BNF, the query grammar is:
+ * <pre>
+ *   Query  ::= ( Clause )*
+ *   Clause ::= ["+", "-"] [&lt;TERM&gt; ":"] ( &lt;TERM&gt; | "(" Query ")" )
+ * </pre>
+ *
+ * <p>
+ * Examples of appropriately formatted queries can be found in the <a
+ * href="../../../../../../../queryparsersyntax.html">query syntax
+ * documentation</a>.
+ * </p>
+ *
+ * @author Brian Goetz
+ * @author Peter Halacsy
+ * @author Tatu Saloranta
+ */
+
+public class PrecedenceQueryParser {
+
+  private static final int CONJ_NONE   = 0;
+  private static final int CONJ_AND    = 1;
+  private static final int CONJ_OR     = 2;
+
+  private static final int MOD_NONE    = 0;
+  private static final int MOD_NOT     = 10;
+  private static final int MOD_REQ     = 11;
+
+  // make it possible to call setDefaultOperator() without accessing
+  // the nested class:
+  public static final Operator AND_OPERATOR = Operator.AND;
+  public static final Operator OR_OPERATOR = Operator.OR;
+
+  /** The actual operator that parser uses to combine query terms */
+  private Operator operator = OR_OPERATOR;
+
+  boolean lowercaseExpandedTerms = true;
+
+  Analyzer analyzer;
+  String field;
+  int phraseSlop = 0;
+  float fuzzyMinSim = FuzzyQuery.defaultMinSimilarity;
+  int fuzzyPrefixLength = FuzzyQuery.defaultPrefixLength;
+  Locale locale = Locale.getDefault();
+
+  static final class Operator extends Parameter {
+    private Operator(String name) {
+      super(name);
+    }
+    static final Operator OR = new Operator("OR");
+    static final Operator AND = new Operator("AND");
+  }
+
+  /** Constructs a query parser.
+   *  @param f  the default field for query terms.
+   *  @param a   used to find terms in the query text.
+   */
+  public PrecedenceQueryParser(String f, Analyzer a) {
+    this(new FastCharStream(new StringReader("")));
+    analyzer = a;
+    field = f;
+  }
+
+  /** Parses a query string, returning a {@link org.apache.lucene.search.Query}.
+   *  @param expression  the query string to be parsed.
+   *  @throws ParseException if the parsing fails
+   */
+  public Query parse(String expression) throws ParseException {
+    // optimize empty query to be empty BooleanQuery
+    if (expression == null || expression.trim().length() == 0) {
+      return new BooleanQuery();
+    }
+
+    ReInit(new FastCharStream(new StringReader(expression)));
+    try {
+      Query query = Query(field);
+      return (query != null) ? query : new BooleanQuery();
+    }
+    catch (TokenMgrError tme) {
+      throw new ParseException(tme.getMessage());
+    }
+    catch (BooleanQuery.TooManyClauses tmc) {
+      throw new ParseException("Too many boolean clauses");
+    }
+  }
+
+   /**
+   * @return Returns the analyzer.
+   */
+  public Analyzer getAnalyzer() {
+    return analyzer;
+  }
+
+  /**
+   * @return Returns the field.
+   */
+  public String getField() {
+    return field;
+  }
+
+   /**
+   * Get the minimal similarity for fuzzy queries.
+   */
+  public float getFuzzyMinSim() {
+      return fuzzyMinSim;
+  }
+
+  /**
+   * Set the minimum similarity for fuzzy queries.
+   * Default is 0.5f.
+   */
+  public void setFuzzyMinSim(float fuzzyMinSim) {
+      this.fuzzyMinSim = fuzzyMinSim;
+  }
+
+   /**
+   * Get the prefix length for fuzzy queries. 
+   * @return Returns the fuzzyPrefixLength.
+   */
+  public int getFuzzyPrefixLength() {
+    return fuzzyPrefixLength;
+  }
+
+  /**
+   * Set the prefix length for fuzzy queries. Default is 0.
+   * @param fuzzyPrefixLength The fuzzyPrefixLength to set.
+   */
+  public void setFuzzyPrefixLength(int fuzzyPrefixLength) {
+    this.fuzzyPrefixLength = fuzzyPrefixLength;
+  }
+
+  /**
+   * Sets the default slop for phrases.  If zero, then exact phrase matches
+   * are required.  Default value is zero.
+   */
+  public void setPhraseSlop(int phraseSlop) {
+    this.phraseSlop = phraseSlop;
+  }
+
+  /**
+   * Gets the default slop for phrases.
+   */
+  public int getPhraseSlop() {
+    return phraseSlop;
+  }
+
+  /**
+   * Sets the boolean operator of the QueryParser.
+   * In default mode (<code>OR_OPERATOR</code>) terms without any modifiers
+   * are considered optional: for example <code>capital of Hungary</code> is equal to
+   * <code>capital OR of OR Hungary</code>.<br/>
+   * In <code>AND_OPERATOR</code> mode terms are considered to be in conjuction: the
+   * above mentioned query is parsed as <code>capital AND of AND Hungary</code>
+   */
+  public void setDefaultOperator(Operator op) {
+    this.operator = op;
+  }
+
+  /**
+   * Gets implicit operator setting, which will be either AND_OPERATOR
+   * or OR_OPERATOR.
+   */
+  public Operator getDefaultOperator() {
+    return operator;
+  }
+
+  /**
+   * Whether terms of wildcard, prefix, fuzzy and range queries are to be automatically
+   * lower-cased or not.  Default is <code>true</code>.
+   */
+  public void setLowercaseExpandedTerms(boolean lowercaseExpandedTerms) {
+    this.lowercaseExpandedTerms = lowercaseExpandedTerms;
+  }
+
+  /**
+   * @see #setLowercaseExpandedTerms(boolean)
+   */
+  public boolean getLowercaseExpandedTerms() {
+    return lowercaseExpandedTerms;
+  }
+
+  /**
+   * Set locale used by date range parsing.
+   */
+  public void setLocale(Locale locale) {
+    this.locale = locale;
+  }
+
+  /**
+   * Returns current locale, allowing access by subclasses.
+   */
+  public Locale getLocale() {
+    return locale;
+  }
+
+  /**
+   * @deprecated use {@link #addClause(List, int, int, Query)} instead.
+   */
+  protected void addClause(Vector clauses, int conj, int modifier, Query q) {
+    addClause((List) clauses, conj, modifier, q);
+  }
+
+  protected void addClause(List clauses, int conj, int modifier, Query q) {
+    boolean required, prohibited;
+
+    // If this term is introduced by AND, make the preceding term required,
+    // unless it's already prohibited
+    if (clauses.size() > 0 && conj == CONJ_AND) {
+      BooleanClause c = (BooleanClause) clauses.get(clauses.size()-1);
+      if (!c.isProhibited())
+        c.setOccur(BooleanClause.Occur.MUST);
+    }
+
+    if (clauses.size() > 0 && operator == AND_OPERATOR && conj == CONJ_OR) {
+      // If this term is introduced by OR, make the preceding term optional,
+      // unless it's prohibited (that means we leave -a OR b but +a OR b-->a OR b)
+      // notice if the input is a OR b, first term is parsed as required; without
+      // this modification a OR b would parsed as +a OR b
+      BooleanClause c = (BooleanClause) clauses.get(clauses.size()-1);
+      if (!c.isProhibited())
+        c.setOccur(BooleanClause.Occur.SHOULD);
+    }
+
+    // We might have been passed a null query; the term might have been
+    // filtered away by the analyzer.
+    if (q == null)
+      return;
+
+    if (operator == OR_OPERATOR) {
+      // We set REQUIRED if we're introduced by AND or +; PROHIBITED if
+      // introduced by NOT or -; make sure not to set both.
+      prohibited = (modifier == MOD_NOT);
+      required = (modifier == MOD_REQ);
+      if (conj == CONJ_AND && !prohibited) {
+        required = true;
+      }
+    } else {
+      // We set PROHIBITED if we're introduced by NOT or -; We set REQUIRED
+      // if not PROHIBITED and not introduced by OR
+      prohibited = (modifier == MOD_NOT);
+      required   = (!prohibited && conj != CONJ_OR);
+    }
+    if (required && !prohibited)
+      clauses.add(new BooleanClause(q, BooleanClause.Occur.MUST));
+    else if (!required && !prohibited)
+      clauses.add(new BooleanClause(q, BooleanClause.Occur.SHOULD));
+    else if (!required && prohibited)
+      clauses.add(new BooleanClause(q, BooleanClause.Occur.MUST_NOT));
+    else
+      throw new RuntimeException("Clause cannot be both required and prohibited");
+  }
+
+  /**
+   * @exception ParseException throw in overridden method to disallow
+   */
+  protected Query getFieldQuery(String field, String queryText)  throws ParseException {
+    // Use the analyzer to get all the tokens, and then build a TermQuery,
+    // PhraseQuery, or nothing based on the term count
+
+    TokenStream source = analyzer.tokenStream(field, new StringReader(queryText));
+    List list = new ArrayList();
+    final org.apache.lucene.analysis.Token reusableToken = new org.apache.lucene.analysis.Token();
+    org.apache.lucene.analysis.Token nextToken;
+    int positionCount = 0;
+    boolean severalTokensAtSamePosition = false;
+
+    while (true) {
+      try {
+        nextToken = source.next(reusableToken);
+      }
+      catch (IOException e) {
+        nextToken = null;
+      }
+      if (nextToken == null)
+        break;
+      list.add(nextToken.clone());
+      if (nextToken.getPositionIncrement() == 1)
+        positionCount++;
+      else
+        severalTokensAtSamePosition = true;
+    }
+    try {
+      source.close();
+    }
+    catch (IOException e) {
+      // ignore
+    }
+
+    if (list.size() == 0)
+      return null;
+    else if (list.size() == 1) {
+      nextToken = (org.apache.lucene.analysis.Token) list.get(0);
+      return new TermQuery(new Term(field, nextToken.term()));
+    } else {
+      if (severalTokensAtSamePosition) {
+        if (positionCount == 1) {
+          // no phrase query:
+          BooleanQuery q = new BooleanQuery();
+          for (int i = 0; i < list.size(); i++) {
+            nextToken = (org.apache.lucene.analysis.Token) list.get(i);
+            TermQuery currentQuery = new TermQuery(
+                new Term(field, nextToken.term()));
+            q.add(currentQuery, BooleanClause.Occur.SHOULD);
+          }
+          return q;
+        }
+        else {
+          // phrase query:
+          MultiPhraseQuery mpq = new MultiPhraseQuery();
+          List multiTerms = new ArrayList();
+          for (int i = 0; i < list.size(); i++) {
+            nextToken = (org.apache.lucene.analysis.Token) list.get(i);
+            if (nextToken.getPositionIncrement() == 1 && multiTerms.size() > 0) {
+              mpq.add((Term[])multiTerms.toArray(new Term[0]));
+              multiTerms.clear();
+            }
+            multiTerms.add(new Term(field, nextToken.term()));
+          }
+          mpq.add((Term[])multiTerms.toArray(new Term[0]));
+          return mpq;
+        }
+      }
+      else {
+        PhraseQuery q = new PhraseQuery();
+        q.setSlop(phraseSlop);
+        for (int i = 0; i < list.size(); i++) {
+          q.add(new Term(field, ((org.apache.lucene.analysis.Token)
+              list.get(i)).term()));
+        }
+        return q;
+      }
+    }
+  }
+
+  /**
+   * Base implementation delegates to {@link #getFieldQuery(String,String)}.
+   * This method may be overridden, for example, to return
+   * a SpanNearQuery instead of a PhraseQuery.
+   *
+   * @exception ParseException throw in overridden method to disallow
+   */
+  protected Query getFieldQuery(String field, String queryText, int slop)
+        throws ParseException {
+    Query query = getFieldQuery(field, queryText);
+
+    if (query instanceof PhraseQuery) {
+      ((PhraseQuery) query).setSlop(slop);
+    }
+    if (query instanceof MultiPhraseQuery) {
+      ((MultiPhraseQuery) query).setSlop(slop);
+    }
+
+    return query;
+  }
+
+  /**
+   * @exception ParseException throw in overridden method to disallow
+   */
+  protected Query getRangeQuery(String field,
+                                String part1,
+                                String part2,
+                                boolean inclusive) throws ParseException
+  {
+    if (lowercaseExpandedTerms) {
+      part1 = part1.toLowerCase();
+      part2 = part2.toLowerCase();
+    }
+    try {
+      DateFormat df = DateFormat.getDateInstance(DateFormat.SHORT, locale);
+      df.setLenient(true);
+      Date d1 = df.parse(part1);
+      Date d2 = df.parse(part2);
+      part1 = DateTools.dateToString(d1, DateTools.Resolution.DAY);
+      part2 = DateTools.dateToString(d2, DateTools.Resolution.DAY);
+    }
+    catch (Exception e) { }
+
+    return new RangeQuery(new Term(field, part1),
+                          new Term(field, part2),
+                          inclusive);
+  }
+
+  /**
+   * Factory method for generating query, given a set of clauses.
+   * By default creates a boolean query composed of clauses passed in.
+   *
+   * Can be overridden by extending classes, to modify query being
+   * returned.
+   *
+   * @param clauses List that contains {@link BooleanClause} instances
+   *    to join.
+   *
+   * @return Resulting {@link Query} object.
+   * @exception ParseException throw in overridden method to disallow
+   * @deprecated use {@link #getBooleanQuery(List)} instead
+   */
+  protected Query getBooleanQuery(Vector clauses) throws ParseException
+  {
+    return getBooleanQuery((List) clauses, false);
+  }
+
+  /**
+   * Factory method for generating query, given a set of clauses.
+   * By default creates a boolean query composed of clauses passed in.
+   *
+   * Can be overridden by extending classes, to modify query being
+   * returned.
+   *
+   * @param clauses List that contains {@link BooleanClause} instances
+   *    to join.
+   *
+   * @return Resulting {@link Query} object.
+   * @exception ParseException throw in overridden method to disallow
+   */
+  protected Query getBooleanQuery(List clauses) throws ParseException
+  {
+    return getBooleanQuery(clauses, false);
+  }
+
+  /**
+   * Factory method for generating query, given a set of clauses.
+   * By default creates a boolean query composed of clauses passed in.
+   *
+   * Can be overridden by extending classes, to modify query being
+   * returned.
+   *
+   * @param clauses List that contains {@link BooleanClause} instances
+   *    to join.
+   * @param disableCoord true if coord scoring should be disabled.
+   *
+   * @return Resulting {@link Query} object.
+   * @exception ParseException throw in overridden method to disallow
+   * @deprecated use {@link #getBooleanQuery(List, boolean)} instead
+   */
+  protected Query getBooleanQuery(Vector clauses, boolean disableCoord)
+    throws ParseException
+  {
+    return getBooleanQuery((List) clauses, disableCoord);
+  }
+
+  /**
+   * Factory method for generating query, given a set of clauses.
+   * By default creates a boolean query composed of clauses passed in.
+   *
+   * Can be overridden by extending classes, to modify query being
+   * returned.
+   *
+   * @param clauses List that contains {@link BooleanClause} instances
+   *    to join.
+   * @param disableCoord true if coord scoring should be disabled.
+   *
+   * @return Resulting {@link Query} object.
+   * @exception ParseException throw in overridden method to disallow
+   */
+  protected Query getBooleanQuery(List clauses, boolean disableCoord)
+      throws ParseException {
+    if (clauses == null || clauses.size() == 0)
+      return null;
+
+    BooleanQuery query = new BooleanQuery(disableCoord);
+    for (int i = 0; i < clauses.size(); i++) {
+      query.add((BooleanClause)clauses.get(i));
+    }
+    return query;
+  }
+
+  /**
+   * Factory method for generating a query. Called when parser
+   * parses an input term token that contains one or more wildcard
+   * characters (? and *), but is not a prefix term token (one
+   * that has just a single * character at the end)
+   *<p>
+   * Depending on settings, prefix term may be lower-cased
+   * automatically. It will not go through the default Analyzer,
+   * however, since normal Analyzers are unlikely to work properly
+   * with wildcard templates.
+   *<p>
+   * Can be overridden by extending classes, to provide custom handling for
+   * wildcard queries, which may be necessary due to missing analyzer calls.
+   *
+   * @param field Name of the field query will use.
+   * @param termStr Term token that contains one or more wild card
+   *   characters (? or *), but is not simple prefix term
+   *
+   * @return Resulting {@link Query} built for the term
+   * @exception ParseException throw in overridden method to disallow
+   */
+  protected Query getWildcardQuery(String field, String termStr) throws ParseException
+  {
+    if (lowercaseExpandedTerms) {
+      termStr = termStr.toLowerCase();
+    }
+    Term t = new Term(field, termStr);
+    return new WildcardQuery(t);
+  }
+
+  /**
+   * Factory method for generating a query (similar to
+   * {@link #getWildcardQuery}). Called when parser parses an input term
+   * token that uses prefix notation; that is, contains a single '*' wildcard
+   * character as its last character. Since this is a special case
+   * of generic wildcard term, and such a query can be optimized easily,
+   * this usually results in a different query object.
+   *<p>
+   * Depending on settings, a prefix term may be lower-cased
+   * automatically. It will not go through the default Analyzer,
+   * however, since normal Analyzers are unlikely to work properly
+   * with wildcard templates.
+   *<p>
+   * Can be overridden by extending classes, to provide custom handling for
+   * wild card queries, which may be necessary due to missing analyzer calls.
+   *
+   * @param field Name of the field query will use.
+   * @param termStr Term token to use for building term for the query
+   *    (<b>without</b> trailing '*' character!)
+   *
+   * @return Resulting {@link Query} built for the term
+   * @exception ParseException throw in overridden method to disallow
+   */
+  protected Query getPrefixQuery(String field, String termStr) throws ParseException
+  {
+    if (lowercaseExpandedTerms) {
+      termStr = termStr.toLowerCase();
+    }
+    Term t = new Term(field, termStr);
+    return new PrefixQuery(t);
+  }
+
+   /**
+   * Factory method for generating a query (similar to
+   * {@link #getWildcardQuery}). Called when parser parses
+   * an input term token that has the fuzzy suffix (~) appended.
+   *
+   * @param field Name of the field query will use.
+   * @param termStr Term token to use for building term for the query
+   *
+   * @return Resulting {@link Query} built for the term
+   * @exception ParseException throw in overridden method to disallow
+   */
+  protected Query getFuzzyQuery(String field, String termStr, float minSimilarity) throws ParseException
+  {
+    if (lowercaseExpandedTerms) {
+      termStr = termStr.toLowerCase();
+    }
+    Term t = new Term(field, termStr);
+    return new FuzzyQuery(t, minSimilarity, fuzzyPrefixLength);
+  }
+
+  /**
+   * Returns a String where the escape char has been
+   * removed, or kept only once if there was a double escape.
+   */
+  private String discardEscapeChar(String input) {
+    char[] caSource = input.toCharArray();
+    char[] caDest = new char[caSource.length];
+    int j = 0;
+    for (int i = 0; i < caSource.length; i++) {
+      if ((caSource[i] != '\\') || (i > 0 && caSource[i-1] == '\\')) {
+        caDest[j++]=caSource[i];
+      }
+    }
+    return new String(caDest, 0, j);
+  }
+
+  /**
+   * Returns a String where those characters that QueryParser
+   * expects to be escaped are escaped by a preceding <code>\</code>.
+   */
+  public static String escape(String s) {
+    StringBuffer sb = new StringBuffer();
+    for (int i = 0; i < s.length(); i++) {
+      char c = s.charAt(i);
+      // NOTE: keep this in sync with _ESCAPED_CHAR below!
+      if (c == '\\' || c == '+' || c == '-' || c == '!' || c == '(' || c == ')' || c == ':'
+        || c == '^' || c == '[' || c == ']' || c == '\"' || c == '{' || c == '}' || c == '~'
+        || c == '*' || c == '?') {
+        sb.append('\\');
+      }
+      sb.append(c);
+    }
+    return sb.toString();
+  }
+
+  /**
+   * Command line tool to test QueryParser, using {@link org.apache.lucene.analysis.SimpleAnalyzer}.
+   * Usage:<br>
+   * <code>java org.apache.lucene.queryParser.QueryParser &lt;input&gt;</code>
+   */
+  public static void main(String[] args) throws Exception {
+    if (args.length == 0) {
+      System.out.println("Usage: java org.apache.lucene.queryParser.QueryParser <input>");
+      System.exit(0);
+    }
+    PrecedenceQueryParser qp = new PrecedenceQueryParser("field",
+                           new org.apache.lucene.analysis.SimpleAnalyzer());
+    Query q = qp.parse(args[0]);
+    System.out.println(q.toString("field"));
+  }
+}
+
+PARSER_END(PrecedenceQueryParser)
+
+/* ***************** */
+/* Token Definitions */
+/* ***************** */
+
+<*> TOKEN : {
+  <#_NUM_CHAR:   ["0"-"9"] >
+// NOTE: keep this in sync with escape(String) above!
+| <#_ESCAPED_CHAR: "\\" [ "\\", "+", "-", "!", "(", ")", ":", "^",
+                          "[", "]", "\"", "{", "}", "~", "*", "?" ] >
+| <#_TERM_START_CHAR: ( ~[ " ", "\t", "\n", "\r", "+", "-", "!", "(", ")", ":", "^",
+                           "[", "]", "\"", "{", "}", "~", "*", "?" ]
+                       | <_ESCAPED_CHAR> ) >
+| <#_TERM_CHAR: ( <_TERM_START_CHAR> | <_ESCAPED_CHAR> | "-" | "+" ) >
+| <#_WHITESPACE: ( " " | "\t" | "\n" | "\r") >
+}
+
+<DEFAULT, RangeIn, RangeEx> SKIP : {
+  <<_WHITESPACE>>
+}
+
+// OG: to support prefix queries:
+// http://nagoya.apache.org/bugzilla/show_bug.cgi?id=12137
+// Change from:
+// | <WILDTERM:  <_TERM_START_CHAR>
+//              (<_TERM_CHAR> | ( [ "*", "?" ] ))* >
+// To:
+//
+// | <WILDTERM:  (<_TERM_CHAR> | ( [ "*", "?" ] ))* >
+
+<DEFAULT> TOKEN : {
+  <AND:       ("AND" | "&&") >
+| <OR:        ("OR" | "||") >
+| <NOT:       ("NOT" | "!") >
+| <PLUS:      "+" >
+| <MINUS:     "-" >
+| <LPAREN:    "(" >
+| <RPAREN:    ")" >
+| <COLON:     ":" >
+| <CARAT:     "^" > : Boost
+| <QUOTED:     "\"" (~["\""])+ "\"">
+| <TERM:      <_TERM_START_CHAR> (<_TERM_CHAR>)*  >
+| <FUZZY_SLOP:     "~" ( (<_NUM_CHAR>)+ ( "." (<_NUM_CHAR>)+ )? )? >
+| <PREFIXTERM:  <_TERM_START_CHAR> (<_TERM_CHAR>)* "*" >
+| <WILDTERM:  <_TERM_START_CHAR>
+              (<_TERM_CHAR> | ( [ "*", "?" ] ))* >
+| <RANGEIN_START: "[" > : RangeIn
+| <RANGEEX_START: "{" > : RangeEx
+}
+
+<Boost> TOKEN : {
+<NUMBER:    (<_NUM_CHAR>)+ ( "." (<_NUM_CHAR>)+ )? > : DEFAULT
+}
+
+<RangeIn> TOKEN : {
+<RANGEIN_TO: "TO">
+| <RANGEIN_END: "]"> : DEFAULT
+| <RANGEIN_QUOTED: "\"" (~["\""])+ "\"">
+| <RANGEIN_GOOP: (~[ " ", "]" ])+ >
+}
+
+<RangeEx> TOKEN : {
+<RANGEEX_TO: "TO">
+| <RANGEEX_END: "}"> : DEFAULT
+| <RANGEEX_QUOTED: "\"" (~["\""])+ "\"">
+| <RANGEEX_GOOP: (~[ " ", "}" ])+ >
+}
+
+// *   Query  ::= ( Clause )*
+// *   Clause ::= ["+", "-"] [<TERM> ":"] ( <TERM> | "(" Query ")" )
+
+int Conjunction() : {
+  int ret = CONJ_NONE;
+}
+{
+  [
+    <AND> { ret = CONJ_AND; }
+    | <OR>  { ret = CONJ_OR; }
+  ]
+  { return ret; }
+}
+
+int Modifier() : {
+  int ret = MOD_NONE;
+}
+{
+  [
+     <PLUS> { ret = MOD_REQ; }
+     | <MINUS> { ret = MOD_NOT; }
+     | <NOT> { ret = MOD_NOT; }
+  ]
+  { return ret; }
+}
+
+Query Query(String field) :
+{
+  List clauses = new ArrayList();
+  Query q, firstQuery=null;
+  boolean orPresent = false;
+  int modifier;
+}
+{
+  modifier=Modifier() q=andExpression(field)
+  {
+    addClause(clauses, CONJ_NONE, modifier, q);
+    if (modifier == MOD_NONE)
+      firstQuery = q;
+  }
+  (
+    [<OR> { orPresent=true; }] modifier=Modifier() q=andExpression(field)
+    { addClause(clauses, orPresent ? CONJ_OR : CONJ_NONE, modifier, q); }
+  )*
+    {
+      if (clauses.size() == 1 && firstQuery != null)
+        return firstQuery;
+      else {
+        return getBooleanQuery(clauses);
+      }
+    }
+}
+
+Query andExpression(String field) :
+{
+  List clauses = new ArrayList();
+  Query q, firstQuery=null;
+  int modifier;
+}
+{
+  q=Clause(field)
+  {
+    addClause(clauses, CONJ_NONE, MOD_NONE, q);
+    firstQuery = q;
+  }
+  (
+    <AND> modifier=Modifier() q=Clause(field)
+    { addClause(clauses, CONJ_AND, modifier, q); }
+  )*
+    {
+      if (clauses.size() == 1 && firstQuery != null)
+        return firstQuery;
+      else {
+        return getBooleanQuery(clauses);
+      }
+    }
+}
+
+Query Clause(String field) : {
+  Query q;
+  Token fieldToken=null, boost=null;
+}
+{
+  [
+    LOOKAHEAD(2)
+    fieldToken=<TERM> <COLON> {
+      field=discardEscapeChar(fieldToken.image);
+    }
+  ]
+
+  (
+   q=Term(field)
+   | <LPAREN> q=Query(field) <RPAREN> (<CARAT> boost=<NUMBER>)?
+
+  )
+    {
+      if (boost != null) {
+        float f = (float)1.0;
+  try {
+    f = Float.valueOf(boost.image).floatValue();
+          q.setBoost(f);
+  } catch (Exception ignored) { }
+      }
+      return q;
+    }
+}
+
+
+Query Term(String field) : {
+  Token term, boost=null, fuzzySlop=null, goop1, goop2;
+  boolean prefix = false;
+  boolean wildcard = false;
+  boolean fuzzy = false;
+  Query q;
+}
+{
+  (
+     (
+       term=<TERM>
+       | term=<PREFIXTERM> { prefix=true; }
+       | term=<WILDTERM> { wildcard=true; }
+       | term=<NUMBER>
+     )
+     [ fuzzySlop=<FUZZY_SLOP> { fuzzy=true; } ]
+     [ <CARAT> boost=<NUMBER> [ fuzzySlop=<FUZZY_SLOP> { fuzzy=true; } ] ]
+     {
+       String termImage=discardEscapeChar(term.image);
+       if (wildcard) {
+       q = getWildcardQuery(field, termImage);
+       } else if (prefix) {
+         q = getPrefixQuery(field,
+           discardEscapeChar(term.image.substring
+          (0, term.image.length()-1)));
+       } else if (fuzzy) {
+       	  float fms = fuzzyMinSim;
+       	  try {
+            fms = Float.valueOf(fuzzySlop.image.substring(1)).floatValue();
+       	  } catch (Exception ignored) { }
+       	 if(fms < 0.0f || fms > 1.0f){
+       	   throw new ParseException("Minimum similarity for a FuzzyQuery has to be between 0.0f and 1.0f !");
+       	 }
+         q = getFuzzyQuery(field, termImage, fms);
+       } else {
+         q = getFieldQuery(field, termImage);
+       }
+     }
+     | ( <RANGEIN_START> ( goop1=<RANGEIN_GOOP>|goop1=<RANGEIN_QUOTED> )
+         [ <RANGEIN_TO> ] ( goop2=<RANGEIN_GOOP>|goop2=<RANGEIN_QUOTED> )
+         <RANGEIN_END> )
+       [ <CARAT> boost=<NUMBER> ]
+        {
+          if (goop1.kind == RANGEIN_QUOTED) {
+            goop1.image = goop1.image.substring(1, goop1.image.length()-1);
+          } else {
+            goop1.image = discardEscapeChar(goop1.image);
+          }
+          if (goop2.kind == RANGEIN_QUOTED) {
+            goop2.image = goop2.image.substring(1, goop2.image.length()-1);
+      } else {
+        goop2.image = discardEscapeChar(goop2.image);
+      }
+          q = getRangeQuery(field, goop1.image, goop2.image, true);
+        }
+     | ( <RANGEEX_START> ( goop1=<RANGEEX_GOOP>|goop1=<RANGEEX_QUOTED> )
+         [ <RANGEEX_TO> ] ( goop2=<RANGEEX_GOOP>|goop2=<RANGEEX_QUOTED> )
+         <RANGEEX_END> )
+       [ <CARAT> boost=<NUMBER> ]
+        {
+          if (goop1.kind == RANGEEX_QUOTED) {
+            goop1.image = goop1.image.substring(1, goop1.image.length()-1);
+          } else {
+            goop1.image = discardEscapeChar(goop1.image);
+          }
+          if (goop2.kind == RANGEEX_QUOTED) {
+            goop2.image = goop2.image.substring(1, goop2.image.length()-1);
+      } else {
+        goop2.image = discardEscapeChar(goop2.image);
+      }
+
+          q = getRangeQuery(field, goop1.image, goop2.image, false);
+        }
+     | term=<QUOTED>
+       [ fuzzySlop=<FUZZY_SLOP> ]
+       [ <CARAT> boost=<NUMBER> ]
+       {
+         int s = phraseSlop;
+
+         if (fuzzySlop != null) {
+           try {
+             s = Float.valueOf(fuzzySlop.image.substring(1)).intValue();
+           }
+           catch (Exception ignored) { }
+         }
+         q = getFieldQuery(field, term.image.substring(1, term.image.length()-1), s);
+       }
+  )
+  {
+    if (boost != null) {
+      float f = (float) 1.0;
+      try {
+        f = Float.valueOf(boost.image).floatValue();
+      }
+      catch (Exception ignored) {
+    /* Should this be handled somehow? (defaults to "no boost", if
+     * boost number is invalid)
+     */
+      }
+
+      // avoid boosting null queries, such as those caused by stop words
+      if (q != null) {
+        q.setBoost(f);
+      }
+    }
+    return q;
+  }
+}
diff --git a/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParserConstants.java b/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParserConstants.java
new file mode 100644
index 0000000..c221c6a
--- /dev/null
+++ b/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParserConstants.java
@@ -0,0 +1,78 @@
+/* Generated By:JavaCC: Do not edit this line. PrecedenceQueryParserConstants.java */
+package org.apache.lucene.queryParser.precedence;
+
+public interface PrecedenceQueryParserConstants {
+
+  int EOF = 0;
+  int _NUM_CHAR = 1;
+  int _ESCAPED_CHAR = 2;
+  int _TERM_START_CHAR = 3;
+  int _TERM_CHAR = 4;
+  int _WHITESPACE = 5;
+  int AND = 7;
+  int OR = 8;
+  int NOT = 9;
+  int PLUS = 10;
+  int MINUS = 11;
+  int LPAREN = 12;
+  int RPAREN = 13;
+  int COLON = 14;
+  int CARAT = 15;
+  int QUOTED = 16;
+  int TERM = 17;
+  int FUZZY_SLOP = 18;
+  int PREFIXTERM = 19;
+  int WILDTERM = 20;
+  int RANGEIN_START = 21;
+  int RANGEEX_START = 22;
+  int NUMBER = 23;
+  int RANGEIN_TO = 24;
+  int RANGEIN_END = 25;
+  int RANGEIN_QUOTED = 26;
+  int RANGEIN_GOOP = 27;
+  int RANGEEX_TO = 28;
+  int RANGEEX_END = 29;
+  int RANGEEX_QUOTED = 30;
+  int RANGEEX_GOOP = 31;
+
+  int Boost = 0;
+  int RangeEx = 1;
+  int RangeIn = 2;
+  int DEFAULT = 3;
+
+  String[] tokenImage = {
+    "<EOF>",
+    "<_NUM_CHAR>",
+    "<_ESCAPED_CHAR>",
+    "<_TERM_START_CHAR>",
+    "<_TERM_CHAR>",
+    "<_WHITESPACE>",
+    "<token of kind 6>",
+    "<AND>",
+    "<OR>",
+    "<NOT>",
+    "\"+\"",
+    "\"-\"",
+    "\"(\"",
+    "\")\"",
+    "\":\"",
+    "\"^\"",
+    "<QUOTED>",
+    "<TERM>",
+    "<FUZZY_SLOP>",
+    "<PREFIXTERM>",
+    "<WILDTERM>",
+    "\"[\"",
+    "\"{\"",
+    "<NUMBER>",
+    "\"TO\"",
+    "\"]\"",
+    "<RANGEIN_QUOTED>",
+    "<RANGEIN_GOOP>",
+    "\"TO\"",
+    "\"}\"",
+    "<RANGEEX_QUOTED>",
+    "<RANGEEX_GOOP>",
+  };
+
+}
diff --git a/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParserTokenManager.java b/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParserTokenManager.java
new file mode 100644
index 0000000..884a243
--- /dev/null
+++ b/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParserTokenManager.java
@@ -0,0 +1,1069 @@
+/* Generated By:JavaCC: Do not edit this line. PrecedenceQueryParserTokenManager.java */
+package org.apache.lucene.queryParser.precedence;
+import java.io.IOException;
+import java.io.StringReader;
+import java.text.DateFormat;
+import java.util.ArrayList;
+import java.util.Date;
+import java.util.List;
+import java.util.Locale;
+import java.util.Vector;
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.document.DateTools;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.FuzzyQuery;
+import org.apache.lucene.search.MultiPhraseQuery;
+import org.apache.lucene.search.PhraseQuery;
+import org.apache.lucene.search.PrefixQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.RangeQuery;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.WildcardQuery;
+import org.apache.lucene.util.Parameter;
+
+public class PrecedenceQueryParserTokenManager implements PrecedenceQueryParserConstants
+{
+  public  java.io.PrintStream debugStream = System.out;
+  public  void setDebugStream(java.io.PrintStream ds) { debugStream = ds; }
+private final int jjStopStringLiteralDfa_3(int pos, long active0)
+{
+   switch (pos)
+   {
+      default :
+         return -1;
+   }
+}
+private final int jjStartNfa_3(int pos, long active0)
+{
+   return jjMoveNfa_3(jjStopStringLiteralDfa_3(pos, active0), pos + 1);
+}
+private final int jjStopAtPos(int pos, int kind)
+{
+   jjmatchedKind = kind;
+   jjmatchedPos = pos;
+   return pos + 1;
+}
+private final int jjStartNfaWithStates_3(int pos, int kind, int state)
+{
+   jjmatchedKind = kind;
+   jjmatchedPos = pos;
+   try { curChar = input_stream.readChar(); }
+   catch(java.io.IOException e) { return pos + 1; }
+   return jjMoveNfa_3(state, pos + 1);
+}
+private final int jjMoveStringLiteralDfa0_3()
+{
+   switch(curChar)
+   {
+      case 40:
+         return jjStopAtPos(0, 12);
+      case 41:
+         return jjStopAtPos(0, 13);
+      case 43:
+         return jjStopAtPos(0, 10);
+      case 45:
+         return jjStopAtPos(0, 11);
+      case 58:
+         return jjStopAtPos(0, 14);
+      case 91:
+         return jjStopAtPos(0, 21);
+      case 94:
+         return jjStopAtPos(0, 15);
+      case 123:
+         return jjStopAtPos(0, 22);
+      default :
+         return jjMoveNfa_3(0, 0);
+   }
+}
+private final void jjCheckNAdd(int state)
+{
+   if (jjrounds[state] != jjround)
+   {
+      jjstateSet[jjnewStateCnt++] = state;
+      jjrounds[state] = jjround;
+   }
+}
+private final void jjAddStates(int start, int end)
+{
+   do {
+      jjstateSet[jjnewStateCnt++] = jjnextStates[start];
+   } while (start++ != end);
+}
+private final void jjCheckNAddTwoStates(int state1, int state2)
+{
+   jjCheckNAdd(state1);
+   jjCheckNAdd(state2);
+}
+private final void jjCheckNAddStates(int start, int end)
+{
+   do {
+      jjCheckNAdd(jjnextStates[start]);
+   } while (start++ != end);
+}
+private final void jjCheckNAddStates(int start)
+{
+   jjCheckNAdd(jjnextStates[start]);
+   jjCheckNAdd(jjnextStates[start + 1]);
+}
+static final long[] jjbitVec0 = {
+   0xfffffffffffffffeL, 0xffffffffffffffffL, 0xffffffffffffffffL, 0xffffffffffffffffL
+};
+static final long[] jjbitVec2 = {
+   0x0L, 0x0L, 0xffffffffffffffffL, 0xffffffffffffffffL
+};
+private final int jjMoveNfa_3(int startState, int curPos)
+{
+   int[] nextStates;
+   int startsAt = 0;
+   jjnewStateCnt = 33;
+   int i = 1;
+   jjstateSet[0] = startState;
+   int j, kind = 0x7fffffff;
+   for (;;)
+   {
+      if (++jjround == 0x7fffffff)
+         ReInitRounds();
+      if (curChar < 64)
+      {
+         long l = 1L << curChar;
+         MatchLoop: do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+                  if ((0x7bffd0f8ffffd9ffL & l) != 0L)
+                  {
+                     if (kind > 17)
+                        kind = 17;
+                     jjCheckNAddStates(0, 6);
+                  }
+                  else if ((0x100002600L & l) != 0L)
+                  {
+                     if (kind > 6)
+                        kind = 6;
+                  }
+                  else if (curChar == 34)
+                     jjCheckNAdd(15);
+                  else if (curChar == 33)
+                  {
+                     if (kind > 9)
+                        kind = 9;
+                  }
+                  if (curChar == 38)
+                     jjstateSet[jjnewStateCnt++] = 4;
+                  break;
+               case 4:
+                  if (curChar == 38 && kind > 7)
+                     kind = 7;
+                  break;
+               case 5:
+                  if (curChar == 38)
+                     jjstateSet[jjnewStateCnt++] = 4;
+                  break;
+               case 13:
+                  if (curChar == 33 && kind > 9)
+                     kind = 9;
+                  break;
+               case 14:
+                  if (curChar == 34)
+                     jjCheckNAdd(15);
+                  break;
+               case 15:
+                  if ((0xfffffffbffffffffL & l) != 0L)
+                     jjCheckNAddTwoStates(15, 16);
+                  break;
+               case 16:
+                  if (curChar == 34 && kind > 16)
+                     kind = 16;
+                  break;
+               case 18:
+                  if ((0x3ff000000000000L & l) == 0L)
+                     break;
+                  if (kind > 18)
+                     kind = 18;
+                  jjAddStates(7, 8);
+                  break;
+               case 19:
+                  if (curChar == 46)
+                     jjCheckNAdd(20);
+                  break;
+               case 20:
+                  if ((0x3ff000000000000L & l) == 0L)
+                     break;
+                  if (kind > 18)
+                     kind = 18;
+                  jjCheckNAdd(20);
+                  break;
+               case 21:
+                  if ((0x7bffd0f8ffffd9ffL & l) == 0L)
+                     break;
+                  if (kind > 17)
+                     kind = 17;
+                  jjCheckNAddStates(0, 6);
+                  break;
+               case 22:
+                  if ((0x7bfff8f8ffffd9ffL & l) == 0L)
+                     break;
+                  if (kind > 17)
+                     kind = 17;
+                  jjCheckNAddTwoStates(22, 23);
+                  break;
+               case 24:
+                  if ((0x84002f0600000000L & l) == 0L)
+                     break;
+                  if (kind > 17)
+                     kind = 17;
+                  jjCheckNAddTwoStates(22, 23);
+                  break;
+               case 25:
+                  if ((0x7bfff8f8ffffd9ffL & l) != 0L)
+                     jjCheckNAddStates(9, 11);
+                  break;
+               case 26:
+                  if (curChar == 42 && kind > 19)
+                     kind = 19;
+                  break;
+               case 28:
+                  if ((0x84002f0600000000L & l) != 0L)
+                     jjCheckNAddStates(9, 11);
+                  break;
+               case 29:
+                  if ((0xfbfffcf8ffffd9ffL & l) == 0L)
+                     break;
+                  if (kind > 20)
+                     kind = 20;
+                  jjCheckNAddTwoStates(29, 30);
+                  break;
+               case 31:
+                  if ((0x84002f0600000000L & l) == 0L)
+                     break;
+                  if (kind > 20)
+                     kind = 20;
+                  jjCheckNAddTwoStates(29, 30);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else if (curChar < 128)
+      {
+         long l = 1L << (curChar & 077);
+         MatchLoop: do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+                  if ((0x97ffffff97ffffffL & l) != 0L)
+                  {
+                     if (kind > 17)
+                        kind = 17;
+                     jjCheckNAddStates(0, 6);
+                  }
+                  else if (curChar == 126)
+                  {
+                     if (kind > 18)
+                        kind = 18;
+                     jjstateSet[jjnewStateCnt++] = 18;
+                  }
+                  if (curChar == 92)
+                     jjCheckNAddStates(12, 14);
+                  else if (curChar == 78)
+                     jjstateSet[jjnewStateCnt++] = 11;
+                  else if (curChar == 124)
+                     jjstateSet[jjnewStateCnt++] = 8;
+                  else if (curChar == 79)
+                     jjstateSet[jjnewStateCnt++] = 6;
+                  else if (curChar == 65)
+                     jjstateSet[jjnewStateCnt++] = 2;
+                  break;
+               case 1:
+                  if (curChar == 68 && kind > 7)
+                     kind = 7;
+                  break;
+               case 2:
+                  if (curChar == 78)
+                     jjstateSet[jjnewStateCnt++] = 1;
+                  break;
+               case 3:
+                  if (curChar == 65)
+                     jjstateSet[jjnewStateCnt++] = 2;
+                  break;
+               case 6:
+                  if (curChar == 82 && kind > 8)
+                     kind = 8;
+                  break;
+               case 7:
+                  if (curChar == 79)
+                     jjstateSet[jjnewStateCnt++] = 6;
+                  break;
+               case 8:
+                  if (curChar == 124 && kind > 8)
+                     kind = 8;
+                  break;
+               case 9:
+                  if (curChar == 124)
+                     jjstateSet[jjnewStateCnt++] = 8;
+                  break;
+               case 10:
+                  if (curChar == 84 && kind > 9)
+                     kind = 9;
+                  break;
+               case 11:
+                  if (curChar == 79)
+                     jjstateSet[jjnewStateCnt++] = 10;
+                  break;
+               case 12:
+                  if (curChar == 78)
+                     jjstateSet[jjnewStateCnt++] = 11;
+                  break;
+               case 15:
+                  jjAddStates(15, 16);
+                  break;
+               case 17:
+                  if (curChar != 126)
+                     break;
+                  if (kind > 18)
+                     kind = 18;
+                  jjstateSet[jjnewStateCnt++] = 18;
+                  break;
+               case 21:
+                  if ((0x97ffffff97ffffffL & l) == 0L)
+                     break;
+                  if (kind > 17)
+                     kind = 17;
+                  jjCheckNAddStates(0, 6);
+                  break;
+               case 22:
+                  if ((0x97ffffff97ffffffL & l) == 0L)
+                     break;
+                  if (kind > 17)
+                     kind = 17;
+                  jjCheckNAddTwoStates(22, 23);
+                  break;
+               case 23:
+                  if (curChar == 92)
+                     jjCheckNAddTwoStates(24, 24);
+                  break;
+               case 24:
+                  if ((0x6800000078000000L & l) == 0L)
+                     break;
+                  if (kind > 17)
+                     kind = 17;
+                  jjCheckNAddTwoStates(22, 23);
+                  break;
+               case 25:
+                  if ((0x97ffffff97ffffffL & l) != 0L)
+                     jjCheckNAddStates(9, 11);
+                  break;
+               case 27:
+                  if (curChar == 92)
+                     jjCheckNAddTwoStates(28, 28);
+                  break;
+               case 28:
+                  if ((0x6800000078000000L & l) != 0L)
+                     jjCheckNAddStates(9, 11);
+                  break;
+               case 29:
+                  if ((0x97ffffff97ffffffL & l) == 0L)
+                     break;
+                  if (kind > 20)
+                     kind = 20;
+                  jjCheckNAddTwoStates(29, 30);
+                  break;
+               case 30:
+                  if (curChar == 92)
+                     jjCheckNAddTwoStates(31, 31);
+                  break;
+               case 31:
+                  if ((0x6800000078000000L & l) == 0L)
+                     break;
+                  if (kind > 20)
+                     kind = 20;
+                  jjCheckNAddTwoStates(29, 30);
+                  break;
+               case 32:
+                  if (curChar == 92)
+                     jjCheckNAddStates(12, 14);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else
+      {
+         int hiByte = (int)(curChar >> 8);
+         int i1 = hiByte >> 6;
+         long l1 = 1L << (hiByte & 077);
+         int i2 = (curChar & 0xff) >> 6;
+         long l2 = 1L << (curChar & 077);
+         MatchLoop: do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+                  if (!jjCanMove_0(hiByte, i1, i2, l1, l2))
+                     break;
+                  if (kind > 17)
+                     kind = 17;
+                  jjCheckNAddStates(0, 6);
+                  break;
+               case 15:
+                  if (jjCanMove_0(hiByte, i1, i2, l1, l2))
+                     jjAddStates(15, 16);
+                  break;
+               case 22:
+                  if (!jjCanMove_0(hiByte, i1, i2, l1, l2))
+                     break;
+                  if (kind > 17)
+                     kind = 17;
+                  jjCheckNAddTwoStates(22, 23);
+                  break;
+               case 25:
+                  if (jjCanMove_0(hiByte, i1, i2, l1, l2))
+                     jjCheckNAddStates(9, 11);
+                  break;
+               case 29:
+                  if (!jjCanMove_0(hiByte, i1, i2, l1, l2))
+                     break;
+                  if (kind > 20)
+                     kind = 20;
+                  jjCheckNAddTwoStates(29, 30);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      if (kind != 0x7fffffff)
+      {
+         jjmatchedKind = kind;
+         jjmatchedPos = curPos;
+         kind = 0x7fffffff;
+      }
+      ++curPos;
+      if ((i = jjnewStateCnt) == (startsAt = 33 - (jjnewStateCnt = startsAt)))
+         return curPos;
+      try { curChar = input_stream.readChar(); }
+      catch(java.io.IOException e) { return curPos; }
+   }
+}
+private final int jjStopStringLiteralDfa_1(int pos, long active0)
+{
+   switch (pos)
+   {
+      case 0:
+         if ((active0 & 0x10000000L) != 0L)
+         {
+            jjmatchedKind = 31;
+            return 4;
+         }
+         return -1;
+      default :
+         return -1;
+   }
+}
+private final int jjStartNfa_1(int pos, long active0)
+{
+   return jjMoveNfa_1(jjStopStringLiteralDfa_1(pos, active0), pos + 1);
+}
+private final int jjStartNfaWithStates_1(int pos, int kind, int state)
+{
+   jjmatchedKind = kind;
+   jjmatchedPos = pos;
+   try { curChar = input_stream.readChar(); }
+   catch(java.io.IOException e) { return pos + 1; }
+   return jjMoveNfa_1(state, pos + 1);
+}
+private final int jjMoveStringLiteralDfa0_1()
+{
+   switch(curChar)
+   {
+      case 84:
+         return jjMoveStringLiteralDfa1_1(0x10000000L);
+      case 125:
+         return jjStopAtPos(0, 29);
+      default :
+         return jjMoveNfa_1(0, 0);
+   }
+}
+private final int jjMoveStringLiteralDfa1_1(long active0)
+{
+   try { curChar = input_stream.readChar(); }
+   catch(java.io.IOException e) {
+      jjStopStringLiteralDfa_1(0, active0);
+      return 1;
+   }
+   switch(curChar)
+   {
+      case 79:
+         if ((active0 & 0x10000000L) != 0L)
+            return jjStartNfaWithStates_1(1, 28, 4);
+         break;
+      default :
+         break;
+   }
+   return jjStartNfa_1(0, active0);
+}
+private final int jjMoveNfa_1(int startState, int curPos)
+{
+   int[] nextStates;
+   int startsAt = 0;
+   jjnewStateCnt = 5;
+   int i = 1;
+   jjstateSet[0] = startState;
+   int j, kind = 0x7fffffff;
+   for (;;)
+   {
+      if (++jjround == 0x7fffffff)
+         ReInitRounds();
+      if (curChar < 64)
+      {
+         long l = 1L << curChar;
+         MatchLoop: do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+                  if ((0xfffffffeffffffffL & l) != 0L)
+                  {
+                     if (kind > 31)
+                        kind = 31;
+                     jjCheckNAdd(4);
+                  }
+                  if ((0x100002600L & l) != 0L)
+                  {
+                     if (kind > 6)
+                        kind = 6;
+                  }
+                  else if (curChar == 34)
+                     jjCheckNAdd(2);
+                  break;
+               case 1:
+                  if (curChar == 34)
+                     jjCheckNAdd(2);
+                  break;
+               case 2:
+                  if ((0xfffffffbffffffffL & l) != 0L)
+                     jjCheckNAddTwoStates(2, 3);
+                  break;
+               case 3:
+                  if (curChar == 34 && kind > 30)
+                     kind = 30;
+                  break;
+               case 4:
+                  if ((0xfffffffeffffffffL & l) == 0L)
+                     break;
+                  if (kind > 31)
+                     kind = 31;
+                  jjCheckNAdd(4);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else if (curChar < 128)
+      {
+         long l = 1L << (curChar & 077);
+         MatchLoop: do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+               case 4:
+                  if ((0xdfffffffffffffffL & l) == 0L)
+                     break;
+                  if (kind > 31)
+                     kind = 31;
+                  jjCheckNAdd(4);
+                  break;
+               case 2:
+                  jjAddStates(17, 18);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else
+      {
+         int hiByte = (int)(curChar >> 8);
+         int i1 = hiByte >> 6;
+         long l1 = 1L << (hiByte & 077);
+         int i2 = (curChar & 0xff) >> 6;
+         long l2 = 1L << (curChar & 077);
+         MatchLoop: do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+               case 4:
+                  if (!jjCanMove_0(hiByte, i1, i2, l1, l2))
+                     break;
+                  if (kind > 31)
+                     kind = 31;
+                  jjCheckNAdd(4);
+                  break;
+               case 2:
+                  if (jjCanMove_0(hiByte, i1, i2, l1, l2))
+                     jjAddStates(17, 18);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      if (kind != 0x7fffffff)
+      {
+         jjmatchedKind = kind;
+         jjmatchedPos = curPos;
+         kind = 0x7fffffff;
+      }
+      ++curPos;
+      if ((i = jjnewStateCnt) == (startsAt = 5 - (jjnewStateCnt = startsAt)))
+         return curPos;
+      try { curChar = input_stream.readChar(); }
+      catch(java.io.IOException e) { return curPos; }
+   }
+}
+private final int jjMoveStringLiteralDfa0_0()
+{
+   return jjMoveNfa_0(0, 0);
+}
+private final int jjMoveNfa_0(int startState, int curPos)
+{
+   int[] nextStates;
+   int startsAt = 0;
+   jjnewStateCnt = 3;
+   int i = 1;
+   jjstateSet[0] = startState;
+   int j, kind = 0x7fffffff;
+   for (;;)
+   {
+      if (++jjround == 0x7fffffff)
+         ReInitRounds();
+      if (curChar < 64)
+      {
+         long l = 1L << curChar;
+         MatchLoop: do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+                  if ((0x3ff000000000000L & l) == 0L)
+                     break;
+                  if (kind > 23)
+                     kind = 23;
+                  jjAddStates(19, 20);
+                  break;
+               case 1:
+                  if (curChar == 46)
+                     jjCheckNAdd(2);
+                  break;
+               case 2:
+                  if ((0x3ff000000000000L & l) == 0L)
+                     break;
+                  if (kind > 23)
+                     kind = 23;
+                  jjCheckNAdd(2);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else if (curChar < 128)
+      {
+         long l = 1L << (curChar & 077);
+         MatchLoop: do
+         {
+            switch(jjstateSet[--i])
+            {
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else
+      {
+         int hiByte = (int)(curChar >> 8);
+         int i1 = hiByte >> 6;
+         long l1 = 1L << (hiByte & 077);
+         int i2 = (curChar & 0xff) >> 6;
+         long l2 = 1L << (curChar & 077);
+         MatchLoop: do
+         {
+            switch(jjstateSet[--i])
+            {
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      if (kind != 0x7fffffff)
+      {
+         jjmatchedKind = kind;
+         jjmatchedPos = curPos;
+         kind = 0x7fffffff;
+      }
+      ++curPos;
+      if ((i = jjnewStateCnt) == (startsAt = 3 - (jjnewStateCnt = startsAt)))
+         return curPos;
+      try { curChar = input_stream.readChar(); }
+      catch(java.io.IOException e) { return curPos; }
+   }
+}
+private final int jjStopStringLiteralDfa_2(int pos, long active0)
+{
+   switch (pos)
+   {
+      case 0:
+         if ((active0 & 0x1000000L) != 0L)
+         {
+            jjmatchedKind = 27;
+            return 4;
+         }
+         return -1;
+      default :
+         return -1;
+   }
+}
+private final int jjStartNfa_2(int pos, long active0)
+{
+   return jjMoveNfa_2(jjStopStringLiteralDfa_2(pos, active0), pos + 1);
+}
+private final int jjStartNfaWithStates_2(int pos, int kind, int state)
+{
+   jjmatchedKind = kind;
+   jjmatchedPos = pos;
+   try { curChar = input_stream.readChar(); }
+   catch(java.io.IOException e) { return pos + 1; }
+   return jjMoveNfa_2(state, pos + 1);
+}
+private final int jjMoveStringLiteralDfa0_2()
+{
+   switch(curChar)
+   {
+      case 84:
+         return jjMoveStringLiteralDfa1_2(0x1000000L);
+      case 93:
+         return jjStopAtPos(0, 25);
+      default :
+         return jjMoveNfa_2(0, 0);
+   }
+}
+private final int jjMoveStringLiteralDfa1_2(long active0)
+{
+   try { curChar = input_stream.readChar(); }
+   catch(java.io.IOException e) {
+      jjStopStringLiteralDfa_2(0, active0);
+      return 1;
+   }
+   switch(curChar)
+   {
+      case 79:
+         if ((active0 & 0x1000000L) != 0L)
+            return jjStartNfaWithStates_2(1, 24, 4);
+         break;
+      default :
+         break;
+   }
+   return jjStartNfa_2(0, active0);
+}
+private final int jjMoveNfa_2(int startState, int curPos)
+{
+   int[] nextStates;
+   int startsAt = 0;
+   jjnewStateCnt = 5;
+   int i = 1;
+   jjstateSet[0] = startState;
+   int j, kind = 0x7fffffff;
+   for (;;)
+   {
+      if (++jjround == 0x7fffffff)
+         ReInitRounds();
+      if (curChar < 64)
+      {
+         long l = 1L << curChar;
+         MatchLoop: do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+                  if ((0xfffffffeffffffffL & l) != 0L)
+                  {
+                     if (kind > 27)
+                        kind = 27;
+                     jjCheckNAdd(4);
+                  }
+                  if ((0x100002600L & l) != 0L)
+                  {
+                     if (kind > 6)
+                        kind = 6;
+                  }
+                  else if (curChar == 34)
+                     jjCheckNAdd(2);
+                  break;
+               case 1:
+                  if (curChar == 34)
+                     jjCheckNAdd(2);
+                  break;
+               case 2:
+                  if ((0xfffffffbffffffffL & l) != 0L)
+                     jjCheckNAddTwoStates(2, 3);
+                  break;
+               case 3:
+                  if (curChar == 34 && kind > 26)
+                     kind = 26;
+                  break;
+               case 4:
+                  if ((0xfffffffeffffffffL & l) == 0L)
+                     break;
+                  if (kind > 27)
+                     kind = 27;
+                  jjCheckNAdd(4);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else if (curChar < 128)
+      {
+         long l = 1L << (curChar & 077);
+         MatchLoop: do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+               case 4:
+                  if ((0xffffffffdfffffffL & l) == 0L)
+                     break;
+                  if (kind > 27)
+                     kind = 27;
+                  jjCheckNAdd(4);
+                  break;
+               case 2:
+                  jjAddStates(17, 18);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else
+      {
+         int hiByte = (int)(curChar >> 8);
+         int i1 = hiByte >> 6;
+         long l1 = 1L << (hiByte & 077);
+         int i2 = (curChar & 0xff) >> 6;
+         long l2 = 1L << (curChar & 077);
+         MatchLoop: do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+               case 4:
+                  if (!jjCanMove_0(hiByte, i1, i2, l1, l2))
+                     break;
+                  if (kind > 27)
+                     kind = 27;
+                  jjCheckNAdd(4);
+                  break;
+               case 2:
+                  if (jjCanMove_0(hiByte, i1, i2, l1, l2))
+                     jjAddStates(17, 18);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      if (kind != 0x7fffffff)
+      {
+         jjmatchedKind = kind;
+         jjmatchedPos = curPos;
+         kind = 0x7fffffff;
+      }
+      ++curPos;
+      if ((i = jjnewStateCnt) == (startsAt = 5 - (jjnewStateCnt = startsAt)))
+         return curPos;
+      try { curChar = input_stream.readChar(); }
+      catch(java.io.IOException e) { return curPos; }
+   }
+}
+static final int[] jjnextStates = {
+   22, 25, 26, 29, 30, 27, 23, 18, 19, 25, 26, 27, 24, 28, 31, 15, 
+   16, 2, 3, 0, 1, 
+};
+private static final boolean jjCanMove_0(int hiByte, int i1, int i2, long l1, long l2)
+{
+   switch(hiByte)
+   {
+      case 0:
+         return ((jjbitVec2[i2] & l2) != 0L);
+      default : 
+         if ((jjbitVec0[i1] & l1) != 0L)
+            return true;
+         return false;
+   }
+}
+public static final String[] jjstrLiteralImages = {
+"", null, null, null, null, null, null, null, null, null, "\53", "\55", "\50", 
+"\51", "\72", "\136", null, null, null, null, null, "\133", "\173", null, "\124\117", 
+"\135", null, null, "\124\117", "\175", null, null, };
+public static final String[] lexStateNames = {
+   "Boost", 
+   "RangeEx", 
+   "RangeIn", 
+   "DEFAULT", 
+};
+public static final int[] jjnewLexState = {
+   -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 2, 1, 3, -1, 
+   3, -1, -1, -1, 3, -1, -1, 
+};
+static final long[] jjtoToken = {
+   0xffffff81L, 
+};
+static final long[] jjtoSkip = {
+   0x40L, 
+};
+protected CharStream input_stream;
+private final int[] jjrounds = new int[33];
+private final int[] jjstateSet = new int[66];
+protected char curChar;
+public PrecedenceQueryParserTokenManager(CharStream stream)
+{
+   input_stream = stream;
+}
+public PrecedenceQueryParserTokenManager(CharStream stream, int lexState)
+{
+   this(stream);
+   SwitchTo(lexState);
+}
+public void ReInit(CharStream stream)
+{
+   jjmatchedPos = jjnewStateCnt = 0;
+   curLexState = defaultLexState;
+   input_stream = stream;
+   ReInitRounds();
+}
+private final void ReInitRounds()
+{
+   int i;
+   jjround = 0x80000001;
+   for (i = 33; i-- > 0;)
+      jjrounds[i] = 0x80000000;
+}
+public void ReInit(CharStream stream, int lexState)
+{
+   ReInit(stream);
+   SwitchTo(lexState);
+}
+public void SwitchTo(int lexState)
+{
+   if (lexState >= 4 || lexState < 0)
+      throw new TokenMgrError("Error: Ignoring invalid lexical state : " + lexState + ". State unchanged.", TokenMgrError.INVALID_LEXICAL_STATE);
+   else
+      curLexState = lexState;
+}
+
+protected Token jjFillToken()
+{
+   Token t = Token.newToken(jjmatchedKind);
+   t.kind = jjmatchedKind;
+   String im = jjstrLiteralImages[jjmatchedKind];
+   t.image = (im == null) ? input_stream.GetImage() : im;
+   t.beginLine = input_stream.getBeginLine();
+   t.beginColumn = input_stream.getBeginColumn();
+   t.endLine = input_stream.getEndLine();
+   t.endColumn = input_stream.getEndColumn();
+   return t;
+}
+
+int curLexState = 3;
+int defaultLexState = 3;
+int jjnewStateCnt;
+int jjround;
+int jjmatchedPos;
+int jjmatchedKind;
+
+public Token getNextToken() 
+{
+  int kind;
+  Token specialToken = null;
+  Token matchedToken;
+  int curPos = 0;
+
+  EOFLoop :
+  for (;;)
+  {   
+   try   
+   {     
+      curChar = input_stream.BeginToken();
+   }     
+   catch(java.io.IOException e)
+   {        
+      jjmatchedKind = 0;
+      matchedToken = jjFillToken();
+      return matchedToken;
+   }
+
+   switch(curLexState)
+   {
+     case 0:
+       jjmatchedKind = 0x7fffffff;
+       jjmatchedPos = 0;
+       curPos = jjMoveStringLiteralDfa0_0();
+       break;
+     case 1:
+       jjmatchedKind = 0x7fffffff;
+       jjmatchedPos = 0;
+       curPos = jjMoveStringLiteralDfa0_1();
+       break;
+     case 2:
+       jjmatchedKind = 0x7fffffff;
+       jjmatchedPos = 0;
+       curPos = jjMoveStringLiteralDfa0_2();
+       break;
+     case 3:
+       jjmatchedKind = 0x7fffffff;
+       jjmatchedPos = 0;
+       curPos = jjMoveStringLiteralDfa0_3();
+       break;
+   }
+     if (jjmatchedKind != 0x7fffffff)
+     {
+        if (jjmatchedPos + 1 < curPos)
+           input_stream.backup(curPos - jjmatchedPos - 1);
+        if ((jjtoToken[jjmatchedKind >> 6] & (1L << (jjmatchedKind & 077))) != 0L)
+        {
+           matchedToken = jjFillToken();
+       if (jjnewLexState[jjmatchedKind] != -1)
+         curLexState = jjnewLexState[jjmatchedKind];
+           return matchedToken;
+        }
+        else
+        {
+         if (jjnewLexState[jjmatchedKind] != -1)
+           curLexState = jjnewLexState[jjmatchedKind];
+           continue EOFLoop;
+        }
+     }
+     int error_line = input_stream.getEndLine();
+     int error_column = input_stream.getEndColumn();
+     String error_after = null;
+     boolean EOFSeen = false;
+     try { input_stream.readChar(); input_stream.backup(1); }
+     catch (java.io.IOException e1) {
+        EOFSeen = true;
+        error_after = curPos <= 1 ? "" : input_stream.GetImage();
+        if (curChar == '\n' || curChar == '\r') {
+           error_line++;
+           error_column = 0;
+        }
+        else
+           error_column++;
+     }
+     if (!EOFSeen) {
+        input_stream.backup(1);
+        error_after = curPos <= 1 ? "" : input_stream.GetImage();
+     }
+     throw new TokenMgrError(EOFSeen, curLexState, error_line, error_column, error_after, curChar, TokenMgrError.LEXICAL_ERROR);
+  }
+}
+
+}
diff --git a/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/Token.java b/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/Token.java
new file mode 100644
index 0000000..a1c0ca5
--- /dev/null
+++ b/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/Token.java
@@ -0,0 +1,81 @@
+/* Generated By:JavaCC: Do not edit this line. Token.java Version 3.0 */
+package org.apache.lucene.queryParser.precedence;
+
+/**
+ * Describes the input token stream.
+ */
+
+public class Token {
+
+  /**
+   * An integer that describes the kind of this token.  This numbering
+   * system is determined by JavaCCParser, and a table of these numbers is
+   * stored in the file ...Constants.java.
+   */
+  public int kind;
+
+  /**
+   * beginLine and beginColumn describe the position of the first character
+   * of this token; endLine and endColumn describe the position of the
+   * last character of this token.
+   */
+  public int beginLine, beginColumn, endLine, endColumn;
+
+  /**
+   * The string image of the token.
+   */
+  public String image;
+
+  /**
+   * A reference to the next regular (non-special) token from the input
+   * stream.  If this is the last token from the input stream, or if the
+   * token manager has not read tokens beyond this one, this field is
+   * set to null.  This is true only if this token is also a regular
+   * token.  Otherwise, see below for a description of the contents of
+   * this field.
+   */
+  public Token next;
+
+  /**
+   * This field is used to access special tokens that occur prior to this
+   * token, but after the immediately preceding regular (non-special) token.
+   * If there are no such special tokens, this field is set to null.
+   * When there are more than one such special token, this field refers
+   * to the last of these special tokens, which in turn refers to the next
+   * previous special token through its specialToken field, and so on
+   * until the first special token (whose specialToken field is null).
+   * The next fields of special tokens refer to other special tokens that
+   * immediately follow it (without an intervening regular token).  If there
+   * is no such token, this field is null.
+   */
+  public Token specialToken;
+
+  /**
+   * Returns the image.
+   */
+  public String toString()
+  {
+     return image;
+  }
+
+  /**
+   * Returns a new Token object, by default. However, if you want, you
+   * can create and return subclass objects based on the value of ofKind.
+   * Simply add the cases to the switch for all those special cases.
+   * For example, if you have a subclass of Token called IDToken that
+   * you want to create if ofKind is ID, simlpy add something like :
+   *
+   *    case MyParserConstants.ID : return new IDToken();
+   *
+   * to the following switch statement. Then you can cast matchedToken
+   * variable to the appropriate type and use it in your lexical actions.
+   */
+  public static final Token newToken(int ofKind)
+  {
+     switch(ofKind)
+     {
+       default : return new Token();
+     }
+  }
+
+}
diff --git a/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/TokenMgrError.java b/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/TokenMgrError.java
new file mode 100644
index 0000000..ed4dce2
--- /dev/null
+++ b/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/TokenMgrError.java
@@ -0,0 +1,133 @@
+/* Generated By:JavaCC: Do not edit this line. TokenMgrError.java Version 3.0 */
+package org.apache.lucene.queryParser.precedence;
+
+public class TokenMgrError extends Error
+{
+   /*
+    * Ordinals for various reasons why an Error of this type can be thrown.
+    */
+
+   /**
+    * Lexical error occured.
+    */
+   static final int LEXICAL_ERROR = 0;
+
+   /**
+    * An attempt wass made to create a second instance of a static token manager.
+    */
+   static final int STATIC_LEXER_ERROR = 1;
+
+   /**
+    * Tried to change to an invalid lexical state.
+    */
+   static final int INVALID_LEXICAL_STATE = 2;
+
+   /**
+    * Detected (and bailed out of) an infinite loop in the token manager.
+    */
+   static final int LOOP_DETECTED = 3;
+
+   /**
+    * Indicates the reason why the exception is thrown. It will have
+    * one of the above 4 values.
+    */
+   int errorCode;
+
+   /**
+    * Replaces unprintable characters by their espaced (or unicode escaped)
+    * equivalents in the given string
+    */
+   protected static final String addEscapes(String str) {
+      StringBuffer retval = new StringBuffer();
+      char ch;
+      for (int i = 0; i < str.length(); i++) {
+        switch (str.charAt(i))
+        {
+           case 0 :
+              continue;
+           case '\b':
+              retval.append("\\b");
+              continue;
+           case '\t':
+              retval.append("\\t");
+              continue;
+           case '\n':
+              retval.append("\\n");
+              continue;
+           case '\f':
+              retval.append("\\f");
+              continue;
+           case '\r':
+              retval.append("\\r");
+              continue;
+           case '\"':
+              retval.append("\\\"");
+              continue;
+           case '\'':
+              retval.append("\\\'");
+              continue;
+           case '\\':
+              retval.append("\\\\");
+              continue;
+           default:
+              if ((ch = str.charAt(i)) < 0x20 || ch > 0x7e) {
+                 String s = "0000" + Integer.toString(ch, 16);
+                 retval.append("\\u" + s.substring(s.length() - 4, s.length()));
+              } else {
+                 retval.append(ch);
+              }
+              continue;
+        }
+      }
+      return retval.toString();
+   }
+
+   /**
+    * Returns a detailed message for the Error when it is thrown by the
+    * token manager to indicate a lexical error.
+    * Parameters : 
+    *    EOFSeen     : indicates if EOF caused the lexicl error
+    *    curLexState : lexical state in which this error occured
+    *    errorLine   : line number when the error occured
+    *    errorColumn : column number when the error occured
+    *    errorAfter  : prefix that was seen before this error occured
+    *    curchar     : the offending character
+    * Note: You can customize the lexical error message by modifying this method.
+    */
+   protected static String LexicalError(boolean EOFSeen, int lexState, int errorLine, int errorColumn, String errorAfter, char curChar) {
+      return("Lexical error at line " +
+           errorLine + ", column " +
+           errorColumn + ".  Encountered: " +
+           (EOFSeen ? "<EOF> " : ("\"" + addEscapes(String.valueOf(curChar)) + "\"") + " (" + (int)curChar + "), ") +
+           "after : \"" + addEscapes(errorAfter) + "\"");
+   }
+
+   /**
+    * You can also modify the body of this method to customize your error messages.
+    * For example, cases like LOOP_DETECTED and INVALID_LEXICAL_STATE are not
+    * of end-users concern, so you can return something like : 
+    *
+    *     "Internal Error : Please file a bug report .... "
+    *
+    * from this method for such cases in the release version of your parser.
+    */
+   public String getMessage() {
+      return super.getMessage();
+   }
+
+   /*
+    * Constructors of various flavors follow.
+    */
+
+   public TokenMgrError() {
+   }
+
+   public TokenMgrError(String message, int reason) {
+      super(message);
+      errorCode = reason;
+   }
+
+   public TokenMgrError(boolean EOFSeen, int lexState, int errorLine, int errorColumn, String errorAfter, char curChar, int reason) {
+      this(LexicalError(EOFSeen, lexState, errorLine, errorColumn, errorAfter, curChar), reason);
+   }
+}
diff --git a/contrib/misc/src/java/overview.html b/contrib/misc/src/java/overview.html
new file mode 100644
index 0000000..88e166f
--- /dev/null
+++ b/contrib/misc/src/java/overview.html
@@ -0,0 +1,26 @@
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+  <head>
+    <title>
+      miscellaneous
+    </title>
+  </head>
+  <body>
+  miscellaneous
+  </body>
+</html>
\ No newline at end of file
diff --git a/contrib/misc/src/test/org/apache/lucene/index/TestFieldNormModifier.java b/contrib/misc/src/test/org/apache/lucene/index/TestFieldNormModifier.java
new file mode 100644
index 0000000..023f970
--- /dev/null
+++ b/contrib/misc/src/test/org/apache/lucene/index/TestFieldNormModifier.java
@@ -0,0 +1,242 @@
+package org.apache.lucene.index;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import junit.framework.TestCase;
+
+import org.apache.lucene.analysis.SimpleAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.index.IndexWriter.MaxFieldLength;
+import org.apache.lucene.search.Collector;
+import org.apache.lucene.search.DefaultSimilarity;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Scorer;
+import org.apache.lucene.search.Similarity;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.RAMDirectory;
+
+/**
+ * Tests changing of field norms with a custom similarity and with fake norms.
+ *
+ * @version $Id$
+ */
+public class TestFieldNormModifier extends TestCase {
+  public TestFieldNormModifier(String name) {
+    super(name);
+  }
+  
+  public static byte DEFAULT_NORM = Similarity.encodeNorm(1.0f);
+  
+  public static int NUM_DOCS = 5;
+  
+  public Directory store = new RAMDirectory();
+  
+  /** inverts the normal notion of lengthNorm */
+  public static Similarity s = new DefaultSimilarity() {
+    public float lengthNorm(String fieldName, int numTokens) {
+      return numTokens;
+    }
+  };
+  
+  public void setUp() throws Exception {
+    IndexWriter writer = new IndexWriter(store, new SimpleAnalyzer(), true, MaxFieldLength.UNLIMITED);
+    
+    for (int i = 0; i < NUM_DOCS; i++) {
+      Document d = new Document();
+      d.add(new Field("field", "word", Field.Store.YES, Field.Index.ANALYZED));
+      d.add(new Field("nonorm", "word", Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS));
+      d.add(new Field("untokfield", "20061212 20071212", Field.Store.YES, Field.Index.ANALYZED));
+      
+      for (int j = 1; j <= i; j++) {
+        d.add(new Field("field", "crap", Field.Store.YES, Field.Index.ANALYZED));
+        d.add(new Field("nonorm", "more words", Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS));
+      }
+      writer.addDocument(d);
+    }
+    writer.close();
+  }
+  
+  public void testMissingField() {
+    FieldNormModifier fnm = new FieldNormModifier(store, s);
+    try {
+      fnm.reSetNorms("nobodyherebutuschickens");
+    } catch (Exception e) {
+      assertNull("caught something", e);
+    }
+  }
+  
+  public void testFieldWithNoNorm() throws Exception {
+    
+    IndexReader r = IndexReader.open(store);
+    byte[] norms = r.norms("nonorm");
+    
+    // sanity check, norms should all be 1
+    assertTrue("Whoops we have norms?", !r.hasNorms("nonorm"));
+    if (!r.getDisableFakeNorms()) {
+      for (int i = 0; i< norms.length; i++) {
+        assertEquals(""+i, DEFAULT_NORM, norms[i]);
+      }
+    } else {
+      assertNull(norms);
+    }
+    
+    r.close();
+    
+    FieldNormModifier fnm = new FieldNormModifier(store, s);
+    try {
+      fnm.reSetNorms("nonorm");
+    } catch (Exception e) {
+      assertNull("caught something", e);
+    }
+    
+    // nothing should have changed
+    r = IndexReader.open(store);
+    
+    norms = r.norms("nonorm");
+    assertTrue("Whoops we have norms?", !r.hasNorms("nonorm"));
+    if (!r.getDisableFakeNorms()) {
+      for (int i = 0; i< norms.length; i++) {
+        assertEquals(""+i, DEFAULT_NORM, norms[i]);
+      }
+    } else {
+      assertNull(norms);
+    }
+
+    r.close();
+  }
+  
+  
+  public void testGoodCases() throws Exception {
+    
+    IndexSearcher searcher = new IndexSearcher(store);
+    final float[] scores = new float[NUM_DOCS];
+    float lastScore = 0.0f;
+    
+    // default similarity should put docs with shorter length first
+    searcher.search(new TermQuery(new Term("field", "word")), new Collector() {
+      private int docBase = 0;
+      private Scorer scorer;
+      
+      public final void collect(int doc) throws IOException {
+        scores[doc + docBase] = scorer.score();
+      }
+      public void setNextReader(IndexReader reader, int docBase) {
+        this.docBase = docBase;
+      }
+      public void setScorer(Scorer scorer) throws IOException {
+        this.scorer = scorer;
+      }
+      public boolean acceptsDocsOutOfOrder() {
+        return true;
+      }
+    });
+    searcher.close();
+    
+    lastScore = Float.MAX_VALUE;
+    for (int i = 0; i < NUM_DOCS; i++) {
+      String msg = "i=" + i + ", " + scores[i] + " <= " + lastScore;
+      assertTrue(msg, scores[i] <= lastScore);
+      //System.out.println(msg);
+      lastScore = scores[i];
+    }
+
+    FieldNormModifier fnm = new FieldNormModifier(store, s);
+    fnm.reSetNorms("field");
+    
+    // new norm (with default similarity) should put longer docs first
+    searcher = new IndexSearcher(store);
+    searcher.search(new TermQuery(new Term("field", "word")),  new Collector() {
+      private int docBase = 0;
+      private Scorer scorer;
+      public final void collect(int doc) throws IOException {
+        scores[doc + docBase] = scorer.score();
+      }
+      public void setNextReader(IndexReader reader, int docBase) {
+        this.docBase = docBase;
+      }
+      public void setScorer(Scorer scorer) throws IOException {
+        this.scorer = scorer;
+      }
+      public boolean acceptsDocsOutOfOrder() {
+        return true;
+      }
+    });
+    searcher.close();
+    
+    lastScore = 0.0f;
+    for (int i = 0; i < NUM_DOCS; i++) {
+      String msg = "i=" + i + ", " + scores[i] + " >= " + lastScore;
+      assertTrue(msg, scores[i] >= lastScore);
+      //System.out.println(msg);
+      lastScore = scores[i];
+    }
+  }
+
+  public void testNormKiller() throws IOException {
+
+    IndexReader r = IndexReader.open(store);
+    byte[] oldNorms = r.norms("untokfield");    
+    r.close();
+    
+    FieldNormModifier fnm = new FieldNormModifier(store, s);
+    fnm.reSetNorms("untokfield");
+
+    r = IndexReader.open(store);
+    byte[] newNorms = r.norms("untokfield");
+    r.close();
+    assertFalse(Arrays.equals(oldNorms, newNorms));    
+
+    
+    // verify that we still get documents in the same order as originally
+    IndexSearcher searcher = new IndexSearcher(store);
+    final float[] scores = new float[NUM_DOCS];
+    float lastScore = 0.0f;
+    
+    // default similarity should return the same score for all documents for this query
+    searcher.search(new TermQuery(new Term("untokfield", "20061212")), new Collector() {
+      private int docBase = 0;
+      private Scorer scorer;
+      public final void collect(int doc) throws IOException {
+        scores[doc + docBase] = scorer.score();
+      }
+      public void setNextReader(IndexReader reader, int docBase) {
+        this.docBase = docBase;
+      }
+      public void setScorer(Scorer scorer) throws IOException {
+        this.scorer = scorer;
+      }
+      public boolean acceptsDocsOutOfOrder() {
+        return true;
+      }
+    });
+    searcher.close();
+    
+    lastScore = scores[0];
+    for (int i = 0; i < NUM_DOCS; i++) {
+      String msg = "i=" + i + ", " + scores[i] + " == " + lastScore;
+      assertTrue(msg, scores[i] == lastScore);
+      //System.out.println(msg);
+      lastScore = scores[i];
+    }
+  }
+}
diff --git a/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor.java b/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor.java
new file mode 100644
index 0000000..d43e85e
--- /dev/null
+++ b/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor.java
@@ -0,0 +1,111 @@
+package org.apache.lucene.index;
+
+import junit.framework.TestCase;
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.RAMDirectory;
+
+import java.util.Collections;
+/*
+ *  Licensed under the Apache License, Version 2.0 (the "License");
+ *  you may not use this file except in compliance with the License.
+ *  You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *  Unless required by applicable law or agreed to in writing, software
+ *  distributed under the License is distributed on an "AS IS" BASIS,
+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *  See the License for the specific language governing permissions and
+ *  limitations under the License.
+ *
+ */
+
+
+public class TestTermVectorAccessor extends TestCase {
+
+  public void test() throws Exception {
+
+    Directory dir = new RAMDirectory();
+    IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(Collections.EMPTY_SET), true);
+
+    Document doc;
+
+    doc = new Document();
+    doc.add(new Field("a", "a b a c a d a e a f a g a h a", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
+    doc.add(new Field("b", "a b c b d b e b f b g b h b", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
+    doc.add(new Field("c", "a c b c d c e c f c g c h c", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
+    iw.addDocument(doc);
+
+    doc = new Document();
+    doc.add(new Field("a", "a b a c a d a e a f a g a h a", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));
+    doc.add(new Field("b", "a b c b d b e b f b g b h b", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));
+    doc.add(new Field("c", "a c b c d c e c f c g c h c", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));
+    iw.addDocument(doc);
+
+    doc = new Document();
+    doc.add(new Field("a", "a b a c a d a e a f a g a h a", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));
+    doc.add(new Field("b", "a b c b d b e b f b g b h b", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));
+    doc.add(new Field("c", "a c b c d c e c f c g c h c", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));
+    iw.addDocument(doc);
+
+    doc = new Document();
+    doc.add(new Field("a", "a b a c a d a e a f a g a h a", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));
+    doc.add(new Field("b", "a b c b d b e b f b g b h b", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));
+    doc.add(new Field("c", "a c b c d c e c f c g c h c", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));
+    iw.addDocument(doc);
+
+    doc = new Document();
+    doc.add(new Field("a", "a b a c a d a e a f a g a h a", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
+    doc.add(new Field("b", "a b c b d b e b f b g b h b", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));
+    doc.add(new Field("c", "a c b c d c e c f c g c h c", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));
+    iw.addDocument(doc);
+
+    iw.close();
+
+    IndexReader ir = IndexReader.open(dir);
+
+    TermVectorAccessor accessor = new TermVectorAccessor();
+
+    ParallelArrayTermVectorMapper mapper;
+    TermFreqVector tfv;
+
+    for (int i = 0; i < ir.maxDoc(); i++) {
+
+      mapper = new ParallelArrayTermVectorMapper();
+      accessor.accept(ir, i, "a", mapper);
+      tfv = mapper.materializeVector();
+      assertEquals("doc " + i, "a", tfv.getTerms()[0]);
+      assertEquals("doc " + i, 8, tfv.getTermFrequencies()[0]);
+
+      mapper = new ParallelArrayTermVectorMapper();
+      accessor.accept(ir, i, "b", mapper);
+      tfv = mapper.materializeVector();
+      assertEquals("doc " + i, 8, tfv.getTermFrequencies().length);
+      assertEquals("doc " + i, "b", tfv.getTerms()[1]);
+      assertEquals("doc " + i, 7, tfv.getTermFrequencies()[1]);
+
+      mapper = new ParallelArrayTermVectorMapper();
+      accessor.accept(ir, i, "c", mapper);
+      tfv = mapper.materializeVector();
+      assertEquals("doc " + i, 8, tfv.getTermFrequencies().length);
+      assertEquals("doc " + i, "c", tfv.getTerms()[2]);
+      assertEquals("doc " + i, 7, tfv.getTermFrequencies()[2]);
+
+      mapper = new ParallelArrayTermVectorMapper();
+      accessor.accept(ir, i, "q", mapper);
+      tfv = mapper.materializeVector();
+      assertNull("doc " + i, tfv);
+
+    }
+
+    ir.close();
+
+    dir.close();
+
+
+  }
+
+}
diff --git a/contrib/misc/src/test/org/apache/lucene/misc/ChainedFilterTest.java b/contrib/misc/src/test/org/apache/lucene/misc/ChainedFilterTest.java
new file mode 100644
index 0000000..4b6906c
--- /dev/null
+++ b/contrib/misc/src/test/org/apache/lucene/misc/ChainedFilterTest.java
@@ -0,0 +1,244 @@
+package org.apache.lucene.misc;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import junit.framework.TestCase;
+import java.util.*;
+import java.io.IOException;
+
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.IndexWriter.MaxFieldLength;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.search.*;
+
+public class ChainedFilterTest extends TestCase {
+  public static final int MAX = 500;
+
+  private RAMDirectory directory;
+  private IndexSearcher searcher;
+  private Query query;
+  // private DateFilter dateFilter;   DateFilter was deprecated and removed
+  private TermRangeFilter dateFilter;
+  private QueryWrapperFilter bobFilter;
+  private QueryWrapperFilter sueFilter;
+
+  public void setUp() throws Exception {
+    directory = new RAMDirectory();
+    IndexWriter writer =
+       new IndexWriter(directory, new WhitespaceAnalyzer(), true);
+
+    Calendar cal = Calendar.getInstance();
+    cal.setTimeInMillis(1041397200000L); // 2003 January 01
+
+    for (int i = 0; i < MAX; i++) {
+      Document doc = new Document();
+      doc.add(new Field("key", "" + (i + 1), Field.Store.YES, Field.Index.NOT_ANALYZED));
+      doc.add(new Field("owner", (i < MAX / 2) ? "bob" : "sue", Field.Store.YES, Field.Index.NOT_ANALYZED));
+      doc.add(new Field("date", cal.getTime().toString(), Field.Store.YES, Field.Index.NOT_ANALYZED));
+      writer.addDocument(doc);
+
+      cal.add(Calendar.DATE, 1);
+    }
+
+    writer.close();
+
+    searcher = new IndexSearcher(directory);
+
+    // query for everything to make life easier
+    BooleanQuery bq = new BooleanQuery();
+    bq.add(new TermQuery(new Term("owner", "bob")), BooleanClause.Occur.SHOULD);
+    bq.add(new TermQuery(new Term("owner", "sue")), BooleanClause.Occur.SHOULD);
+    query = bq;
+
+    // date filter matches everything too
+    //Date pastTheEnd = parseDate("2099 Jan 1");
+    // dateFilter = DateFilter.Before("date", pastTheEnd);
+    // just treat dates as strings and select the whole range for now...
+    dateFilter = new TermRangeFilter("date","","ZZZZ",true,true);
+
+    bobFilter = new QueryWrapperFilter(
+        new TermQuery(new Term("owner", "bob")));
+    sueFilter = new QueryWrapperFilter(
+        new TermQuery(new Term("owner", "sue")));
+  }
+
+  private Filter[] getChainWithOldFilters(Filter[] chain) {
+    Filter[] oldFilters = new Filter[chain.length];
+    for (int i = 0; i < chain.length; i++) {
+      final Filter f = chain[i];
+    // create old BitSet-based Filter as wrapper
+      oldFilters[i] = new Filter() {
+        /** @deprecated */
+        public BitSet bits(IndexReader reader) throws IOException {
+          BitSet bits = new BitSet(reader.maxDoc());
+          DocIdSetIterator it = f.getDocIdSet(reader).iterator();  
+          int doc;
+          while((doc = it.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
+            bits.set(doc);
+          }
+          return bits;
+        }
+      };
+    }
+    return oldFilters;
+  }
+  
+  private ChainedFilter getChainedFilter(Filter[] chain, int[] logic, boolean old) {
+    if (old) {
+      chain = getChainWithOldFilters(chain);
+    }
+    
+    if (logic == null) {
+      return new ChainedFilter(chain);
+    } else {
+      return new ChainedFilter(chain, logic);
+    }
+  }
+
+  private ChainedFilter getChainedFilter(Filter[] chain, int logic, boolean old) {
+    if (old) {
+      chain = getChainWithOldFilters(chain);
+    }
+    
+    return new ChainedFilter(chain, logic);
+  }
+
+  
+  public void testSingleFilter() throws Exception {
+    for (int mode = 0; mode < 2; mode++) {
+      boolean old = (mode==0);
+      
+      ChainedFilter chain = getChainedFilter(new Filter[] {dateFilter}, null, old);
+  
+      Hits hits = searcher.search(query, chain);
+      assertEquals(MAX, hits.length());
+  
+      chain = new ChainedFilter(new Filter[] {bobFilter});
+      hits = searcher.search(query, chain);
+      assertEquals(MAX / 2, hits.length());
+      
+      chain = getChainedFilter(new Filter[] {bobFilter}, new int[] {ChainedFilter.AND}, old);
+      hits = searcher.search(query, chain);
+      assertEquals(MAX / 2, hits.length());
+      assertEquals("bob", hits.doc(0).get("owner"));
+      
+      chain = getChainedFilter(new Filter[] {bobFilter}, new int[] {ChainedFilter.ANDNOT}, old);
+      hits = searcher.search(query, chain);
+      assertEquals(MAX / 2, hits.length());
+      assertEquals("sue", hits.doc(0).get("owner"));
+    }
+  }
+
+  public void testOR() throws Exception {
+    for (int mode = 0; mode < 2; mode++) {
+      boolean old = (mode==0);
+      ChainedFilter chain = getChainedFilter(
+        new Filter[] {sueFilter, bobFilter}, null, old);
+  
+      Hits hits = searcher.search(query, chain);
+      assertEquals("OR matches all", MAX, hits.length());
+    }
+  }
+
+  public void testAND() throws Exception {
+    for (int mode = 0; mode < 2; mode++) {
+      boolean old = (mode==0);
+      ChainedFilter chain = getChainedFilter(
+        new Filter[] {dateFilter, bobFilter}, ChainedFilter.AND, old);
+  
+      Hits hits = searcher.search(query, chain);
+      assertEquals("AND matches just bob", MAX / 2, hits.length());
+      assertEquals("bob", hits.doc(0).get("owner"));
+    }
+  }
+
+  public void testXOR() throws Exception {
+    for (int mode = 0; mode < 2; mode++) {
+      boolean old = (mode==0);
+      ChainedFilter chain = getChainedFilter(
+        new Filter[]{dateFilter, bobFilter}, ChainedFilter.XOR, old);
+  
+      Hits hits = searcher.search(query, chain);
+      assertEquals("XOR matches sue", MAX / 2, hits.length());
+      assertEquals("sue", hits.doc(0).get("owner"));
+    }
+  }
+
+  public void testANDNOT() throws Exception {
+    for (int mode = 0; mode < 2; mode++) {
+      boolean old = (mode==0);
+      ChainedFilter chain = getChainedFilter(
+        new Filter[]{dateFilter, sueFilter},
+          new int[] {ChainedFilter.AND, ChainedFilter.ANDNOT}, old);
+  
+      Hits hits = searcher.search(query, chain);
+      assertEquals("ANDNOT matches just bob",
+          MAX / 2, hits.length());
+      assertEquals("bob", hits.doc(0).get("owner"));
+      
+      chain = getChainedFilter(
+          new Filter[]{bobFilter, bobFilter},
+            new int[] {ChainedFilter.ANDNOT, ChainedFilter.ANDNOT}, old);
+  
+        hits = searcher.search(query, chain);
+        assertEquals("ANDNOT bob ANDNOT bob matches all sues",
+            MAX / 2, hits.length());
+        assertEquals("sue", hits.doc(0).get("owner"));
+    }
+  }
+
+  /*
+  private Date parseDate(String s) throws ParseException {
+    return new SimpleDateFormat("yyyy MMM dd", Locale.US).parse(s);
+  }
+  */
+  
+  public void testWithCachingFilter() throws Exception {
+    Directory dir = new RAMDirectory();
+    Analyzer analyzer = new WhitespaceAnalyzer();
+  
+    IndexWriter writer = new IndexWriter(dir, analyzer, true, MaxFieldLength.LIMITED);
+    writer.close();
+  
+    Searcher searcher = new IndexSearcher(dir);
+  
+    Query query = new TermQuery(new Term("none", "none"));
+  
+    QueryWrapperFilter queryFilter = new QueryWrapperFilter(query);
+    CachingWrapperFilter cachingFilter = new CachingWrapperFilter(queryFilter);
+  
+    searcher.search(query, cachingFilter, 1);
+  
+    CachingWrapperFilter cachingFilter2 = new CachingWrapperFilter(queryFilter);
+    Filter[] chain = new Filter[2];
+    chain[0] = cachingFilter;
+    chain[1] = cachingFilter2;
+    ChainedFilter cf = new ChainedFilter(chain);
+  
+    // throws java.lang.ClassCastException: org.apache.lucene.util.OpenBitSet cannot be cast to java.util.BitSet
+    searcher.search(new MatchAllDocsQuery(), cf, 1);
+  }
+
+}
diff --git a/contrib/misc/src/test/org/apache/lucene/misc/SweetSpotSimilarityTest.java b/contrib/misc/src/test/org/apache/lucene/misc/SweetSpotSimilarityTest.java
new file mode 100644
index 0000000..31f25d7
--- /dev/null
+++ b/contrib/misc/src/test/org/apache/lucene/misc/SweetSpotSimilarityTest.java
@@ -0,0 +1,208 @@
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.misc;
+
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.Similarity;
+import org.apache.lucene.search.DefaultSimilarity;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.PhraseQuery;
+import org.apache.lucene.search.DisjunctionMaxQuery;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanClause.Occur;
+
+import junit.framework.Test;
+import junit.framework.TestCase;
+import junit.framework.TestSuite;
+
+import java.io.File;
+import java.math.BigDecimal;
+import java.util.Random;
+import java.util.Date;
+import java.util.List;
+import java.util.Arrays;
+import java.util.Map;
+import java.util.HashMap;
+import java.util.Iterator;
+
+/**
+ * Test of the SweetSpotSimilarity
+ */
+public class SweetSpotSimilarityTest extends TestCase {
+
+  public void testSweetSpotLengthNorm() {
+  
+    SweetSpotSimilarity ss = new SweetSpotSimilarity();
+    ss.setLengthNormFactors(1,1,0.5f);
+
+    Similarity d = new DefaultSimilarity();
+    Similarity s = ss;
+
+
+    // base case, should degrade
+  
+    for (int i = 1; i < 1000; i++) {
+      assertEquals("base case: i="+i,
+                   d.lengthNorm("foo",i), s.lengthNorm("foo",i),
+                   0.0f);
+    }
+
+    // make a sweet spot
+  
+    ss.setLengthNormFactors(3,10,0.5f);
+  
+    for (int i = 3; i <=10; i++) {
+      assertEquals("3,10: spot i="+i,
+                   1.0f, s.lengthNorm("foo",i),
+                   0.0f);
+    }
+  
+    for (int i = 10; i < 1000; i++) {
+      assertEquals("3,10: 10<x : i="+i,
+                   d.lengthNorm("foo",i-9), s.lengthNorm("foo",i),
+                   0.0f);
+    }
+
+
+    // seperate sweet spot for certain fields
+
+    ss.setLengthNormFactors("bar",8,13, 0.5f, false);
+    ss.setLengthNormFactors("yak",6,9, 0.5f, false);
+
+  
+    for (int i = 3; i <=10; i++) {
+      assertEquals("f: 3,10: spot i="+i,
+                   1.0f, s.lengthNorm("foo",i),
+                   0.0f);
+    }
+    for (int i = 10; i < 1000; i++) {
+      assertEquals("f: 3,10: 10<x : i="+i,
+                   d.lengthNorm("foo",i-9), s.lengthNorm("foo",i),
+                   0.0f);
+    }
+    for (int i = 8; i <=13; i++) {
+      assertEquals("f: 8,13: spot i="+i,
+                   1.0f, s.lengthNorm("bar",i),
+                   0.0f);
+    }
+    for (int i = 6; i <=9; i++) {
+      assertEquals("f: 6,9: spot i="+i,
+                   1.0f, s.lengthNorm("yak",i),
+                   0.0f);
+    }
+    for (int i = 13; i < 1000; i++) {
+      assertEquals("f: 8,13: 13<x : i="+i,
+                   d.lengthNorm("foo",i-12), s.lengthNorm("bar",i),
+                   0.0f);
+    }
+    for (int i = 9; i < 1000; i++) {
+      assertEquals("f: 6,9: 9<x : i="+i,
+                   d.lengthNorm("foo",i-8), s.lengthNorm("yak",i),
+                   0.0f);
+    }
+
+
+    // steepness
+
+    ss.setLengthNormFactors("a",5,8,0.5f, false);
+    ss.setLengthNormFactors("b",5,8,0.1f, false);
+
+    for (int i = 9; i < 1000; i++) {
+      assertTrue("s: i="+i+" : a="+ss.lengthNorm("a",i)+
+                 " < b="+ss.lengthNorm("b",i),
+                 ss.lengthNorm("a",i) < s.lengthNorm("b",i));
+    }
+
+  }
+
+  public void testSweetSpotTf() {
+  
+    SweetSpotSimilarity ss = new SweetSpotSimilarity();
+
+    Similarity d = new DefaultSimilarity();
+    Similarity s = ss;
+    
+    // tf equal
+
+    ss.setBaselineTfFactors(0.0f, 0.0f);
+  
+    for (int i = 1; i < 1000; i++) {
+      assertEquals("tf: i="+i,
+                   d.tf(i), s.tf(i), 0.0f);
+    }
+
+    // tf higher
+  
+    ss.setBaselineTfFactors(1.0f, 0.0f);
+  
+    for (int i = 1; i < 1000; i++) {
+      assertTrue("tf: i="+i+" : d="+d.tf(i)+
+                 " < s="+s.tf(i),
+                 d.tf(i) < s.tf(i));
+    }
+
+    // tf flat
+  
+    ss.setBaselineTfFactors(1.0f, 6.0f);
+    for (int i = 1; i <=6; i++) {
+      assertEquals("tf flat1: i="+i, 1.0f, s.tf(i), 0.0f);
+    }
+    ss.setBaselineTfFactors(2.0f, 6.0f);
+    for (int i = 1; i <=6; i++) {
+      assertEquals("tf flat2: i="+i, 2.0f, s.tf(i), 0.0f);
+    }
+    for (int i = 6; i <=1000; i++) {
+      assertTrue("tf: i="+i+" : s="+s.tf(i)+
+                 " < d="+d.tf(i),
+                 s.tf(i) < d.tf(i));
+    }
+
+    // stupidity
+    assertEquals("tf zero", 0.0f, s.tf(0), 0.0f);
+  }
+
+  public void testHyperbolicSweetSpot() {
+  
+    SweetSpotSimilarity ss = new SweetSpotSimilarity() {
+        public float tf(int freq) {
+          return hyperbolicTf(freq);
+        }
+      };
+    ss.setHyperbolicTfFactors(3.3f, 7.7f, Math.E, 5.0f);
+    
+    Similarity s = ss;
+
+    for (int i = 1; i <=1000; i++) {
+      assertTrue("MIN tf: i="+i+" : s="+s.tf(i),
+                 3.3f <= s.tf(i));
+      assertTrue("MAX tf: i="+i+" : s="+s.tf(i),
+                 s.tf(i) <= 7.7f);
+    }
+    assertEquals("MID tf", 3.3f+(7.7f - 3.3f)/2.0f, s.tf(5), 0.00001f);
+    
+    // stupidity
+    assertEquals("tf zero", 0.0f, s.tf(0), 0.0f);
+    
+  }
+
+  
+}
+
diff --git a/contrib/misc/src/test/org/apache/lucene/misc/TestLengthNormModifier.java b/contrib/misc/src/test/org/apache/lucene/misc/TestLengthNormModifier.java
new file mode 100644
index 0000000..a893b41
--- /dev/null
+++ b/contrib/misc/src/test/org/apache/lucene/misc/TestLengthNormModifier.java
@@ -0,0 +1,208 @@
+package org.apache.lucene.misc;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import junit.framework.TestCase;
+
+import org.apache.lucene.analysis.SimpleAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.index.FieldNormModifier;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.IndexWriter.MaxFieldLength;
+import org.apache.lucene.search.Collector;
+import org.apache.lucene.search.DefaultSimilarity;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Scorer;
+import org.apache.lucene.search.Similarity;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.RAMDirectory;
+
+/**
+ * Tests changing the norms after changing the simularity
+ *
+ * @version $Id:$
+ */
+public class TestLengthNormModifier extends TestCase {
+    public TestLengthNormModifier(String name) {
+	super(name);
+    }
+
+    public static byte DEFAULT_NORM = Similarity.encodeNorm(1.0f);
+    
+    public static int NUM_DOCS = 5;
+
+    public Directory store = new RAMDirectory();
+
+    /** inverts the normal notion of lengthNorm */
+    public static Similarity s = new DefaultSimilarity() {
+	    public float lengthNorm(String fieldName, int numTokens) {
+		return numTokens;
+	    }
+	};
+    
+    public void setUp() throws Exception {
+	IndexWriter writer = new IndexWriter(store, new SimpleAnalyzer(), true, MaxFieldLength.UNLIMITED);
+	
+	for (int i = 0; i < NUM_DOCS; i++) {
+	    Document d = new Document();
+	    d.add(new Field("field", "word",
+			    Field.Store.YES, Field.Index.ANALYZED));
+	    d.add(new Field("nonorm", "word",
+			    Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS));
+		
+	    for (int j = 1; j <= i; j++) {
+		d.add(new Field("field", "crap",
+				Field.Store.YES, Field.Index.ANALYZED));
+		d.add(new Field("nonorm", "more words",
+				Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS));
+	    }
+	    writer.addDocument(d);
+	}
+	writer.close();
+    }
+    
+    public void testMissingField() {
+	FieldNormModifier fnm = new FieldNormModifier(store, s);
+	try {
+	    fnm.reSetNorms("nobodyherebutuschickens");
+	} catch (Exception e) {
+	    assertNull("caught something", e);
+	}
+    }
+	
+    public void testFieldWithNoNorm() throws Exception {
+
+	IndexReader r = IndexReader.open(store);
+	byte[] norms = r.norms("nonorm");
+
+	// sanity check, norms should all be 1
+	assertTrue("Whoops we have norms?", !r.hasNorms("nonorm"));
+        if (!r.getDisableFakeNorms()) {
+          for (int i = 0; i< norms.length; i++) {
+	    assertEquals(""+i, DEFAULT_NORM, norms[i]);
+          }
+        } else {
+          assertNull(norms);
+        }
+
+	r.close();
+	
+	FieldNormModifier fnm = new FieldNormModifier(store, s);
+	try {
+	    fnm.reSetNorms("nonorm");
+	} catch (Exception e) {
+	    assertNull("caught something", e);
+	}
+
+	// nothing should have changed
+	r = IndexReader.open(store);
+	
+	norms = r.norms("nonorm");
+	assertTrue("Whoops we have norms?", !r.hasNorms("nonorm"));
+        if (!r.getDisableFakeNorms()) {
+          for (int i = 0; i< norms.length; i++) {
+	    assertEquals(""+i, DEFAULT_NORM, norms[i]);
+          }
+        } else {
+          assertNull(norms);
+        }
+
+	r.close();
+	
+    }
+	
+    
+    public void testGoodCases() throws Exception {
+	
+	IndexSearcher searcher;
+	final float[] scores = new float[NUM_DOCS];
+	float lastScore = 0.0f;
+	
+	// default similarity should put docs with shorter length first
+  searcher = new IndexSearcher(store);
+  searcher.search(new TermQuery(new Term("field", "word")), new Collector() {
+    private int docBase = 0;
+    private Scorer scorer;
+    public final void collect(int doc) throws IOException {
+      scores[doc + docBase] = scorer.score();
+    }
+    public void setNextReader(IndexReader reader, int docBase) {
+      this.docBase = docBase;
+    }
+    public void setScorer(Scorer scorer) throws IOException {
+      this.scorer = scorer;
+    }
+    public boolean acceptsDocsOutOfOrder() {
+      return true;
+    }
+  });
+  searcher.close();
+	
+	lastScore = Float.MAX_VALUE;
+	for (int i = 0; i < NUM_DOCS; i++) {
+	    String msg = "i=" + i + ", "+scores[i]+" <= "+lastScore;
+	    assertTrue(msg, scores[i] <= lastScore);
+	    //System.out.println(msg);
+	    lastScore = scores[i];
+	}
+
+	// override the norms to be inverted
+	Similarity s = new DefaultSimilarity() {
+		public float lengthNorm(String fieldName, int numTokens) {
+		    return numTokens;
+		}
+	    };
+	FieldNormModifier fnm = new FieldNormModifier(store, s);
+	fnm.reSetNorms("field");
+
+	// new norm (with default similarity) should put longer docs first
+	searcher = new IndexSearcher(store);
+	searcher.search(new TermQuery(new Term("field", "word")), new Collector() {
+      private int docBase = 0;
+      private Scorer scorer;
+      public final void collect(int doc) throws IOException {
+        scores[doc + docBase] = scorer.score();
+      }
+      public void setNextReader(IndexReader reader, int docBase) {
+        this.docBase = docBase;
+      }
+      public void setScorer(Scorer scorer) throws IOException {
+        this.scorer = scorer;
+      }
+      public boolean acceptsDocsOutOfOrder() {
+        return true;
+      }
+    });
+    searcher.close();
+	
+	lastScore = 0.0f;
+	for (int i = 0; i < NUM_DOCS; i++) {
+	    String msg = "i=" + i + ", "+scores[i]+" >= "+lastScore;
+	    assertTrue(msg, scores[i] >= lastScore);
+	    //System.out.println(msg);
+	    lastScore = scores[i];
+	}
+	
+    }
+}
diff --git a/contrib/misc/src/test/org/apache/lucene/queryParser/analyzing/TestAnalyzingQueryParser.java b/contrib/misc/src/test/org/apache/lucene/queryParser/analyzing/TestAnalyzingQueryParser.java
new file mode 100644
index 0000000..027d26a
--- /dev/null
+++ b/contrib/misc/src/test/org/apache/lucene/queryParser/analyzing/TestAnalyzingQueryParser.java
@@ -0,0 +1,118 @@
+package org.apache.lucene.queryParser.analyzing;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Reader;
+
+import junit.framework.TestCase;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.ISOLatin1AccentFilter;
+import org.apache.lucene.analysis.LowerCaseFilter;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.standard.StandardFilter;
+import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.queryParser.ParseException;
+
+/**
+ * @version $Revision$, $Date$
+ */
+public class TestAnalyzingQueryParser extends TestCase {
+
+  private Analyzer a;
+
+  private String[] wildcardInput;
+  private String[] wildcardExpected;
+  private String[] prefixInput;
+  private String[] prefixExpected;
+  private String[] rangeInput;
+  private String[] rangeExpected;
+  private String[] fuzzyInput;
+  private String[] fuzzyExpected;
+
+  public void setUp() {
+    wildcardInput = new String[] { "bersetzung ber*ung",
+        "Mtley Cr\u00fce Mtl?* Cr?", "Rene Zellweger Ren?? Zellw?ger" };
+    wildcardExpected = new String[] { "ubersetzung uber*ung", "motley crue motl?* cru?",
+        "renee zellweger ren?? zellw?ger" };
+
+    prefixInput = new String[] { "bersetzung bersetz*",
+        "Mtley Cre Mtl* cr*", "Ren? Zellw*" };
+    prefixExpected = new String[] { "ubersetzung ubersetz*", "motley crue motl* cru*",
+        "rene? zellw*" };
+
+    rangeInput = new String[] { "[aa TO bb]", "{Anas TO Zo}" };
+    rangeExpected = new String[] { "[aa TO bb]", "{anais TO zoe}" };
+
+    fuzzyInput = new String[] { "?bersetzung ?bersetzung~0.9",
+        "Mtley Cre Mtley~0.75 Cre~0.5",
+        "Rene Zellweger Rene~0.9 Zellweger~" };
+    fuzzyExpected = new String[] { "ubersetzung ubersetzung~0.9",
+        "motley crue motley~0.75 crue~0.5", "renee zellweger renee~0.9 zellweger~0.5" };
+
+    a = new ASCIIAnalyzer();
+  }
+
+  public void testWildCardQuery() throws ParseException {
+    for (int i = 0; i < wildcardInput.length; i++) {
+      assertEquals("Testing wildcards with analyzer " + a.getClass() + ", input string: "
+          + wildcardInput[i], wildcardExpected[i], parseWithAnalyzingQueryParser(wildcardInput[i], a));
+    }
+  }
+
+  public void testPrefixQuery() throws ParseException {
+    for (int i = 0; i < prefixInput.length; i++) {
+      assertEquals("Testing prefixes with analyzer " + a.getClass() + ", input string: "
+          + prefixInput[i], prefixExpected[i], parseWithAnalyzingQueryParser(prefixInput[i], a));
+    }
+  }
+
+  public void testRangeQuery() throws ParseException {
+    for (int i = 0; i < rangeInput.length; i++) {
+      assertEquals("Testing ranges with analyzer " + a.getClass() + ", input string: "
+          + rangeInput[i], rangeExpected[i], parseWithAnalyzingQueryParser(rangeInput[i], a));
+    }
+  }
+
+  public void testFuzzyQuery() throws ParseException {
+    for (int i = 0; i < fuzzyInput.length; i++) {
+      assertEquals("Testing fuzzys with analyzer " + a.getClass() + ", input string: "
+          + fuzzyInput[i], fuzzyExpected[i], parseWithAnalyzingQueryParser(fuzzyInput[i], a));
+    }
+  }
+
+  private String parseWithAnalyzingQueryParser(String s, Analyzer a) throws ParseException {
+    AnalyzingQueryParser qp = new AnalyzingQueryParser("field", a);
+    org.apache.lucene.search.Query q = qp.parse(s);
+    return q.toString("field");
+  }
+
+}
+
+class ASCIIAnalyzer extends org.apache.lucene.analysis.Analyzer {
+  public ASCIIAnalyzer() {
+  }
+
+  public TokenStream tokenStream(String fieldName, Reader reader) {
+    TokenStream result = new StandardTokenizer(reader);
+    result = new StandardFilter(result);
+    result = new ISOLatin1AccentFilter(result);
+    result = new LowerCaseFilter(result);
+    return result;
+  }
+}
diff --git a/contrib/misc/src/test/org/apache/lucene/queryParser/complexPhrase/TestComplexPhraseQuery.java b/contrib/misc/src/test/org/apache/lucene/queryParser/complexPhrase/TestComplexPhraseQuery.java
new file mode 100644
index 0000000..c5f75fc
--- /dev/null
+++ b/contrib/misc/src/test/org/apache/lucene/queryParser/complexPhrase/TestComplexPhraseQuery.java
@@ -0,0 +1,144 @@
+package org.apache.lucene.queryParser.complexPhrase;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.HashSet;
+
+import junit.framework.TestCase;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriter.MaxFieldLength;
+import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.RAMDirectory;
+
+public class TestComplexPhraseQuery extends TestCase {
+
+  Analyzer analyzer = new StandardAnalyzer();
+
+  DocData docsContent[] = { new DocData("john smith", "1"),
+      new DocData("johathon smith", "2"),
+      new DocData("john percival smith", "3"),
+      new DocData("jackson waits tom", "4") };
+
+  private IndexSearcher searcher;
+
+  String defaultFieldName = "name";
+
+  public void testComplexPhrases() throws Exception {
+    checkMatches("\"john smith\"", "1"); // Simple multi-term still works
+    checkMatches("\"j*   smyth~\"", "1,2"); // wildcards and fuzzies are OK in
+    // phrases
+    checkMatches("\"(jo* -john)  smith\"", "2"); // boolean logic works
+    checkMatches("\"jo*  smith\"~2", "1,2,3"); // position logic works.
+    checkMatches("\"jo* [sma TO smZ]\" ", "1,2"); // range queries supported
+    checkMatches("\"john\"", "1,3"); // Simple single-term still works
+    checkMatches("\"(john OR johathon)  smith\"", "1,2"); // boolean logic with
+    // brackets works.
+    checkMatches("\"(jo* -john) smyth~\"", "2"); // boolean logic with
+    // brackets works.
+
+    // checkMatches("\"john -percival\"", "1"); // not logic doesn't work
+    // currently :(.
+
+    checkMatches("\"john  nosuchword*\"", ""); // phrases with clauses producing
+    // empty sets
+
+    checkBadQuery("\"jo*  id:1 smith\""); // mixing fields in a phrase is bad
+    checkBadQuery("\"jo* \"smith\" \""); // phrases inside phrases is bad
+  }
+
+  private void checkBadQuery(String qString) {
+    QueryParser qp = new ComplexPhraseQueryParser(defaultFieldName, analyzer);
+    Throwable expected = null;
+    try {
+      qp.parse(qString);
+    } catch (Throwable e) {
+      expected = e;
+    }
+    assertNotNull("Expected parse error in " + qString, expected);
+
+  }
+
+  private void checkMatches(String qString, String expectedVals)
+      throws Exception {
+    QueryParser qp = new ComplexPhraseQueryParser(defaultFieldName, analyzer);
+    qp.setFuzzyPrefixLength(1); // usually a good idea
+
+    Query q = qp.parse(qString);
+
+    HashSet expecteds = new HashSet();
+    String[] vals = expectedVals.split(",");
+    for (int i = 0; i < vals.length; i++) {
+      if (vals[i].length() > 0)
+        expecteds.add(vals[i]);
+    }
+
+    TopDocs td = searcher.search(q, 10);
+    ScoreDoc[] sd = td.scoreDocs;
+    for (int i = 0; i < sd.length; i++) {
+      Document doc = searcher.doc(sd[i].doc);
+      String id = doc.get("id");
+      assertTrue(qString + "matched doc#" + id + " not expected", expecteds
+          .contains(id));
+      expecteds.remove(id);
+    }
+
+    assertEquals(qString + " missing some matches ", 0, expecteds.size());
+
+  }
+
+  protected void setUp() throws Exception {
+    RAMDirectory rd = new RAMDirectory();
+    IndexWriter w = new IndexWriter(rd, analyzer, MaxFieldLength.UNLIMITED);
+    for (int i = 0; i < docsContent.length; i++) {
+      Document doc = new Document();
+      doc.add(new Field("name", docsContent[i].name, Field.Store.YES,
+          Field.Index.ANALYZED));
+      doc.add(new Field("id", docsContent[i].id, Field.Store.YES,
+          Field.Index.ANALYZED));
+      w.addDocument(doc);
+    }
+    w.close();
+    searcher = new IndexSearcher(rd);
+  }
+
+  protected void tearDown() throws Exception {
+    searcher.close();
+  }
+
+  static class DocData {
+    String name;
+
+    String id;
+
+    public DocData(String name, String id) {
+      super();
+      this.name = name;
+      this.id = id;
+    }
+  }
+
+}
diff --git a/contrib/misc/src/test/org/apache/lucene/queryParser/precedence/TestPrecedenceQueryParser.java b/contrib/misc/src/test/org/apache/lucene/queryParser/precedence/TestPrecedenceQueryParser.java
new file mode 100644
index 0000000..5925354
--- /dev/null
+++ b/contrib/misc/src/test/org/apache/lucene/queryParser/precedence/TestPrecedenceQueryParser.java
@@ -0,0 +1,599 @@
+package org.apache.lucene.queryParser.precedence;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.LowerCaseTokenizer;
+import org.apache.lucene.analysis.SimpleAnalyzer;
+import org.apache.lucene.analysis.TokenFilter;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.analysis.tokenattributes.TermAttribute;
+import org.apache.lucene.document.DateTools;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.FuzzyQuery;
+import org.apache.lucene.search.PhraseQuery;
+import org.apache.lucene.search.PrefixQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.RangeQuery;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.WildcardQuery;
+import org.apache.lucene.util.LocalizedTestCase;
+
+import java.io.IOException;
+import java.io.Reader;
+import java.text.DateFormat;
+import java.util.Arrays;
+import java.util.Calendar;
+import java.util.GregorianCalendar;
+import java.util.HashSet;
+
+public class TestPrecedenceQueryParser extends LocalizedTestCase {
+  
+  public TestPrecedenceQueryParser(String name) {
+    super(name, new HashSet(Arrays.asList(new String[]{
+      "testDateRange", "testNumber"
+    })));
+  }
+
+  public static Analyzer qpAnalyzer = new QPTestAnalyzer();
+
+  public static class QPTestFilter extends TokenFilter {
+    /**
+     * Filter which discards the token 'stop' and which expands the
+     * token 'phrase' into 'phrase1 phrase2'
+     */
+    public QPTestFilter(TokenStream in) {
+      super(in);
+    }
+
+    boolean inPhrase = false;
+    int savedStart = 0, savedEnd = 0;
+
+    TermAttribute termAtt = (TermAttribute) addAttribute(TermAttribute.class);
+    OffsetAttribute offsetAtt = (OffsetAttribute) addAttribute(OffsetAttribute.class);
+    
+    public boolean incrementToken() throws IOException {
+      if (inPhrase) {
+        inPhrase = false;
+        termAtt.setTermBuffer("phrase2");
+        offsetAtt.setOffset(savedStart, savedEnd);
+        return true;
+      } else
+        while(input.incrementToken())
+          if (termAtt.term().equals("phrase")) {
+            inPhrase = true;
+            savedStart = offsetAtt.startOffset();
+            savedEnd = offsetAtt.endOffset();
+            termAtt.setTermBuffer("phrase1");
+            offsetAtt.setOffset(savedStart, savedEnd);
+            return true;
+          } else if (!termAtt.term().equals("stop"))
+            return true;
+      return false;
+    }
+  }
+
+  public static class QPTestAnalyzer extends Analyzer {
+
+    /** Filters LowerCaseTokenizer with StopFilter. */
+    public final TokenStream tokenStream(String fieldName, Reader reader) {
+      return new QPTestFilter(new LowerCaseTokenizer(reader));
+    }
+  }
+
+  public static class QPTestParser extends PrecedenceQueryParser {
+    public QPTestParser(String f, Analyzer a) {
+      super(f, a);
+    }
+
+    protected Query getFuzzyQuery(String field, String termStr, float minSimilarity) throws ParseException {
+      throw new ParseException("Fuzzy queries not allowed");
+    }
+
+    protected Query getWildcardQuery(String field, String termStr) throws ParseException {
+      throw new ParseException("Wildcard queries not allowed");
+    }
+  }
+
+  private int originalMaxClauses;
+
+  public void setUp() throws Exception {
+    super.setUp();
+    originalMaxClauses = BooleanQuery.getMaxClauseCount();
+  }
+
+  public PrecedenceQueryParser getParser(Analyzer a) throws Exception {
+    if (a == null)
+      a = new SimpleAnalyzer();
+    PrecedenceQueryParser qp = new PrecedenceQueryParser("field", a);
+    qp.setDefaultOperator(PrecedenceQueryParser.OR_OPERATOR);
+    return qp;
+  }
+
+  public Query getQuery(String query, Analyzer a) throws Exception {
+    return getParser(a).parse(query);
+  }
+
+  public void assertQueryEquals(String query, Analyzer a, String result)
+    throws Exception {
+    Query q = getQuery(query, a);
+    String s = q.toString("field");
+    if (!s.equals(result)) {
+      fail("Query /" + query + "/ yielded /" + s
+           + "/, expecting /" + result + "/");
+    }
+  }
+
+  public void assertWildcardQueryEquals(String query, boolean lowercase, String result)
+    throws Exception {
+    PrecedenceQueryParser qp = getParser(null);
+    qp.setLowercaseExpandedTerms(lowercase);
+    Query q = qp.parse(query);
+    String s = q.toString("field");
+    if (!s.equals(result)) {
+      fail("WildcardQuery /" + query + "/ yielded /" + s
+           + "/, expecting /" + result + "/");
+    }
+  }
+
+  public void assertWildcardQueryEquals(String query, String result) throws Exception {
+    PrecedenceQueryParser qp = getParser(null);
+    Query q = qp.parse(query);
+    String s = q.toString("field");
+    if (!s.equals(result)) {
+      fail("WildcardQuery /" + query + "/ yielded /" + s + "/, expecting /"
+          + result + "/");
+    }
+  }
+
+  public Query getQueryDOA(String query, Analyzer a)
+    throws Exception {
+    if (a == null)
+      a = new SimpleAnalyzer();
+    PrecedenceQueryParser qp = new PrecedenceQueryParser("field", a);
+    qp.setDefaultOperator(PrecedenceQueryParser.AND_OPERATOR);
+    return qp.parse(query);
+  }
+
+  public void assertQueryEqualsDOA(String query, Analyzer a, String result)
+    throws Exception {
+    Query q = getQueryDOA(query, a);
+    String s = q.toString("field");
+    if (!s.equals(result)) {
+      fail("Query /" + query + "/ yielded /" + s
+           + "/, expecting /" + result + "/");
+    }
+  }
+
+  // failing tests disabled since PrecedenceQueryParser
+  // is currently unmaintained
+  public void _testSimple() throws Exception {
+    assertQueryEquals("", null, "");
+
+    assertQueryEquals("term term term", null, "term term term");
+    assertQueryEquals("trm term term", null, "trm term term");
+    assertQueryEquals("mlaut", null, "mlaut");
+
+    assertQueryEquals("+a", null, "+a");
+    assertQueryEquals("-a", null, "-a");
+    assertQueryEquals("a AND b", null, "+a +b");
+    assertQueryEquals("(a AND b)", null, "+a +b");
+    assertQueryEquals("c OR (a AND b)", null, "c (+a +b)");
+    assertQueryEquals("a AND NOT b", null, "+a -b");
+    assertQueryEquals("a AND -b", null, "+a -b");
+    assertQueryEquals("a AND !b", null, "+a -b");
+    assertQueryEquals("a && b", null, "+a +b");
+    assertQueryEquals("a && ! b", null, "+a -b");
+
+    assertQueryEquals("a OR b", null, "a b");
+    assertQueryEquals("a || b", null, "a b");
+
+    assertQueryEquals("+term -term term", null, "+term -term term");
+    assertQueryEquals("foo:term AND field:anotherTerm", null,
+                      "+foo:term +anotherterm");
+    assertQueryEquals("term AND \"phrase phrase\"", null,
+                      "+term +\"phrase phrase\"");
+    assertQueryEquals("\"hello there\"", null, "\"hello there\"");
+    assertTrue(getQuery("a AND b", null) instanceof BooleanQuery);
+    assertTrue(getQuery("hello", null) instanceof TermQuery);
+    assertTrue(getQuery("\"hello there\"", null) instanceof PhraseQuery);
+
+    assertQueryEquals("germ term^2.0", null, "germ term^2.0");
+    assertQueryEquals("(term)^2.0", null, "term^2.0");
+    assertQueryEquals("(germ term)^2.0", null, "(germ term)^2.0");
+    assertQueryEquals("term^2.0", null, "term^2.0");
+    assertQueryEquals("term^2", null, "term^2.0");
+    assertQueryEquals("\"germ term\"^2.0", null, "\"germ term\"^2.0");
+    assertQueryEquals("\"term germ\"^2", null, "\"term germ\"^2.0");
+
+    assertQueryEquals("(foo OR bar) AND (baz OR boo)", null,
+                      "+(foo bar) +(baz boo)");
+    assertQueryEquals("((a OR b) AND NOT c) OR d", null,
+                      "(+(a b) -c) d");
+    assertQueryEquals("+(apple \"steve jobs\") -(foo bar baz)", null,
+                      "+(apple \"steve jobs\") -(foo bar baz)");
+    assertQueryEquals("+title:(dog OR cat) -author:\"bob dole\"", null,
+                      "+(title:dog title:cat) -author:\"bob dole\"");
+    
+    PrecedenceQueryParser qp = new PrecedenceQueryParser("field", new StandardAnalyzer());
+    // make sure OR is the default:
+    assertEquals(PrecedenceQueryParser.OR_OPERATOR, qp.getDefaultOperator());
+    qp.setDefaultOperator(PrecedenceQueryParser.AND_OPERATOR);
+    assertEquals(PrecedenceQueryParser.AND_OPERATOR, qp.getDefaultOperator());
+    qp.setDefaultOperator(PrecedenceQueryParser.OR_OPERATOR);
+    assertEquals(PrecedenceQueryParser.OR_OPERATOR, qp.getDefaultOperator());
+
+    assertQueryEquals("a OR !b", null, "a (-b)");
+    assertQueryEquals("a OR ! b", null, "a (-b)");
+    assertQueryEquals("a OR -b", null, "a (-b)");
+  }
+
+  public void testPunct() throws Exception {
+    Analyzer a = new WhitespaceAnalyzer();
+    assertQueryEquals("a&b", a, "a&b");
+    assertQueryEquals("a&&b", a, "a&&b");
+    assertQueryEquals(".NET", a, ".NET");
+  }
+
+  public void testSlop() throws Exception {
+    assertQueryEquals("\"term germ\"~2", null, "\"term germ\"~2");
+    assertQueryEquals("\"term germ\"~2 flork", null, "\"term germ\"~2 flork");
+    assertQueryEquals("\"term\"~2", null, "term");
+    assertQueryEquals("\" \"~2 germ", null, "germ");
+    assertQueryEquals("\"term germ\"~2^2", null, "\"term germ\"~2^2.0");
+  }
+
+  public void testNumber() throws Exception {
+// The numbers go away because SimpleAnalzyer ignores them
+    assertQueryEquals("3", null, "");
+    assertQueryEquals("term 1.0 1 2", null, "term");
+    assertQueryEquals("term term1 term2", null, "term term term");
+
+    Analyzer a = new StandardAnalyzer();
+    assertQueryEquals("3", a, "3");
+    assertQueryEquals("term 1.0 1 2", a, "term 1.0 1 2");
+    assertQueryEquals("term term1 term2", a, "term term1 term2");
+  }
+
+  // failing tests disabled since PrecedenceQueryParser
+  // is currently unmaintained
+  public void _testWildcard() throws Exception {
+    assertQueryEquals("term*", null, "term*");
+    assertQueryEquals("term*^2", null, "term*^2.0");
+    assertQueryEquals("term~", null, "term~0.5");
+    assertQueryEquals("term~0.7", null, "term~0.7");
+    assertQueryEquals("term~^2", null, "term^2.0~0.5");
+    assertQueryEquals("term^2~", null, "term^2.0~0.5");
+    assertQueryEquals("term*germ", null, "term*germ");
+    assertQueryEquals("term*germ^3", null, "term*germ^3.0");
+
+    assertTrue(getQuery("term*", null) instanceof PrefixQuery);
+    assertTrue(getQuery("term*^2", null) instanceof PrefixQuery);
+    assertTrue(getQuery("term~", null) instanceof FuzzyQuery);
+    assertTrue(getQuery("term~0.7", null) instanceof FuzzyQuery);
+    FuzzyQuery fq = (FuzzyQuery)getQuery("term~0.7", null);
+    assertEquals(0.7f, fq.getMinSimilarity(), 0.1f);
+    assertEquals(FuzzyQuery.defaultPrefixLength, fq.getPrefixLength());
+    fq = (FuzzyQuery)getQuery("term~", null);
+    assertEquals(0.5f, fq.getMinSimilarity(), 0.1f);
+    assertEquals(FuzzyQuery.defaultPrefixLength, fq.getPrefixLength());
+    try {
+      getQuery("term~1.1", null);   // value > 1, throws exception
+      fail();
+    } catch(ParseException pe) {
+      // expected exception
+    }
+    assertTrue(getQuery("term*germ", null) instanceof WildcardQuery);
+
+/* Tests to see that wild card terms are (or are not) properly
+	 * lower-cased with propery parser configuration
+	 */
+// First prefix queries:
+    // by default, convert to lowercase:
+    assertWildcardQueryEquals("Term*", true, "term*");
+    // explicitly set lowercase:
+    assertWildcardQueryEquals("term*", true, "term*");
+    assertWildcardQueryEquals("Term*", true, "term*");
+    assertWildcardQueryEquals("TERM*", true, "term*");
+    // explicitly disable lowercase conversion:
+    assertWildcardQueryEquals("term*", false, "term*");
+    assertWildcardQueryEquals("Term*", false, "Term*");
+    assertWildcardQueryEquals("TERM*", false, "TERM*");
+// Then 'full' wildcard queries:
+    // by default, convert to lowercase:
+    assertWildcardQueryEquals("Te?m", "te?m");
+    // explicitly set lowercase:
+    assertWildcardQueryEquals("te?m", true, "te?m");
+    assertWildcardQueryEquals("Te?m", true, "te?m");
+    assertWildcardQueryEquals("TE?M", true, "te?m");
+    assertWildcardQueryEquals("Te?m*gerM", true, "te?m*germ");
+    // explicitly disable lowercase conversion:
+    assertWildcardQueryEquals("te?m", false, "te?m");
+    assertWildcardQueryEquals("Te?m", false, "Te?m");
+    assertWildcardQueryEquals("TE?M", false, "TE?M");
+    assertWildcardQueryEquals("Te?m*gerM", false, "Te?m*gerM");
+//  Fuzzy queries:
+    assertWildcardQueryEquals("Term~", "term~0.5");
+    assertWildcardQueryEquals("Term~", true, "term~0.5");
+    assertWildcardQueryEquals("Term~", false, "Term~0.5");
+//  Range queries:
+    assertWildcardQueryEquals("[A TO C]", "[a TO c]");
+    assertWildcardQueryEquals("[A TO C]", true, "[a TO c]");
+    assertWildcardQueryEquals("[A TO C]", false, "[A TO C]");
+  }
+
+  public void testQPA() throws Exception {
+    assertQueryEquals("term term term", qpAnalyzer, "term term term");
+    assertQueryEquals("term +stop term", qpAnalyzer, "term term");
+    assertQueryEquals("term -stop term", qpAnalyzer, "term term");
+    assertQueryEquals("drop AND stop AND roll", qpAnalyzer, "+drop +roll");
+    assertQueryEquals("term phrase term", qpAnalyzer,
+                      "term \"phrase1 phrase2\" term");
+    // note the parens in this next assertion differ from the original
+    // QueryParser behavior
+    assertQueryEquals("term AND NOT phrase term", qpAnalyzer,
+                      "(+term -\"phrase1 phrase2\") term");
+    assertQueryEquals("stop", qpAnalyzer, "");
+    assertQueryEquals("stop OR stop AND stop", qpAnalyzer, "");
+    assertTrue(getQuery("term term term", qpAnalyzer) instanceof BooleanQuery);
+    assertTrue(getQuery("term +stop", qpAnalyzer) instanceof TermQuery);
+  }
+
+  public void testRange() throws Exception {
+    assertQueryEquals("[ a TO z]", null, "[a TO z]");
+    assertTrue(getQuery("[ a TO z]", null) instanceof RangeQuery);
+    assertQueryEquals("[ a TO z ]", null, "[a TO z]");
+    assertQueryEquals("{ a TO z}", null, "{a TO z}");
+    assertQueryEquals("{ a TO z }", null, "{a TO z}");
+    assertQueryEquals("{ a TO z }^2.0", null, "{a TO z}^2.0");
+    assertQueryEquals("[ a TO z] OR bar", null, "[a TO z] bar");
+    assertQueryEquals("[ a TO z] AND bar", null, "+[a TO z] +bar");
+    assertQueryEquals("( bar blar { a TO z}) ", null, "bar blar {a TO z}");
+    assertQueryEquals("gack ( bar blar { a TO z}) ", null, "gack (bar blar {a TO z})");
+  }
+  
+  private String escapeDateString(String s) {
+    if (s.contains(" ")) {
+      return "\"" + s + "\"";
+    } else {
+      return s;
+    }
+  }
+
+  public String getDate(String s) throws Exception {
+    DateFormat df = DateFormat.getDateInstance(DateFormat.SHORT);
+    return DateTools.dateToString(df.parse(s), DateTools.Resolution.DAY);
+  }
+
+  public String getLocalizedDate(int year, int month, int day) {
+    DateFormat df = DateFormat.getDateInstance(DateFormat.SHORT);
+    Calendar calendar = new GregorianCalendar();
+    calendar.set(year, month, day);
+    return df.format(calendar.getTime());
+  }
+
+  public void testDateRange() throws Exception {
+    String startDate = getLocalizedDate(2002, 1, 1);
+    String endDate = getLocalizedDate(2002, 1, 4);
+    assertQueryEquals("[ " + escapeDateString(startDate) + " TO " + escapeDateString(endDate) + "]", null,
+                      "[" + getDate(startDate) + " TO " + getDate(endDate) + "]");
+    assertQueryEquals("{  " + escapeDateString(startDate) + "    " + escapeDateString(endDate) + "   }", null,
+                      "{" + getDate(startDate) + " TO " + getDate(endDate) + "}");
+  }
+
+  public void testEscaped() throws Exception {
+    Analyzer a = new WhitespaceAnalyzer();
+    
+    /*assertQueryEquals("\\[brackets", a, "\\[brackets");
+    assertQueryEquals("\\[brackets", null, "brackets");
+    assertQueryEquals("\\\\", a, "\\\\");
+    assertQueryEquals("\\+blah", a, "\\+blah");
+    assertQueryEquals("\\(blah", a, "\\(blah");
+
+    assertQueryEquals("\\-blah", a, "\\-blah");
+    assertQueryEquals("\\!blah", a, "\\!blah");
+    assertQueryEquals("\\{blah", a, "\\{blah");
+    assertQueryEquals("\\}blah", a, "\\}blah");
+    assertQueryEquals("\\:blah", a, "\\:blah");
+    assertQueryEquals("\\^blah", a, "\\^blah");
+    assertQueryEquals("\\[blah", a, "\\[blah");
+    assertQueryEquals("\\]blah", a, "\\]blah");
+    assertQueryEquals("\\\"blah", a, "\\\"blah");
+    assertQueryEquals("\\(blah", a, "\\(blah");
+    assertQueryEquals("\\)blah", a, "\\)blah");
+    assertQueryEquals("\\~blah", a, "\\~blah");
+    assertQueryEquals("\\*blah", a, "\\*blah");
+    assertQueryEquals("\\?blah", a, "\\?blah");
+    //assertQueryEquals("foo \\&\\& bar", a, "foo \\&\\& bar");
+    //assertQueryEquals("foo \\|| bar", a, "foo \\|| bar");
+    //assertQueryEquals("foo \\AND bar", a, "foo \\AND bar");*/
+
+    assertQueryEquals("a\\-b:c", a, "a-b:c");
+    assertQueryEquals("a\\+b:c", a, "a+b:c");
+    assertQueryEquals("a\\:b:c", a, "a:b:c");
+    assertQueryEquals("a\\\\b:c", a, "a\\b:c");
+
+    assertQueryEquals("a:b\\-c", a, "a:b-c");
+    assertQueryEquals("a:b\\+c", a, "a:b+c");
+    assertQueryEquals("a:b\\:c", a, "a:b:c");
+    assertQueryEquals("a:b\\\\c", a, "a:b\\c");
+
+    assertQueryEquals("a:b\\-c*", a, "a:b-c*");
+    assertQueryEquals("a:b\\+c*", a, "a:b+c*");
+    assertQueryEquals("a:b\\:c*", a, "a:b:c*");
+
+    assertQueryEquals("a:b\\\\c*", a, "a:b\\c*");
+
+    assertQueryEquals("a:b\\-?c", a, "a:b-?c");
+    assertQueryEquals("a:b\\+?c", a, "a:b+?c");
+    assertQueryEquals("a:b\\:?c", a, "a:b:?c");
+
+    assertQueryEquals("a:b\\\\?c", a, "a:b\\?c");
+
+    assertQueryEquals("a:b\\-c~", a, "a:b-c~0.5");
+    assertQueryEquals("a:b\\+c~", a, "a:b+c~0.5");
+    assertQueryEquals("a:b\\:c~", a, "a:b:c~0.5");
+    assertQueryEquals("a:b\\\\c~", a, "a:b\\c~0.5");
+
+    assertQueryEquals("[ a\\- TO a\\+ ]", null, "[a- TO a+]");
+    assertQueryEquals("[ a\\: TO a\\~ ]", null, "[a: TO a~]");
+    assertQueryEquals("[ a\\\\ TO a\\* ]", null, "[a\\ TO a*]");
+  }
+
+  public void testTabNewlineCarriageReturn()
+    throws Exception {
+    assertQueryEqualsDOA("+weltbank +worlbank", null,
+      "+weltbank +worlbank");
+
+    assertQueryEqualsDOA("+weltbank\n+worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \n+worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \n +worlbank", null,
+      "+weltbank +worlbank");
+
+    assertQueryEqualsDOA("+weltbank\r+worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \r+worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \r +worlbank", null,
+      "+weltbank +worlbank");
+
+    assertQueryEqualsDOA("+weltbank\r\n+worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \r\n+worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \r\n +worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \r \n +worlbank", null,
+      "+weltbank +worlbank");
+
+    assertQueryEqualsDOA("+weltbank\t+worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \t+worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \t +worlbank", null,
+      "+weltbank +worlbank");
+  }
+
+  public void testSimpleDAO()
+    throws Exception {
+    assertQueryEqualsDOA("term term term", null, "+term +term +term");
+    assertQueryEqualsDOA("term +term term", null, "+term +term +term");
+    assertQueryEqualsDOA("term term +term", null, "+term +term +term");
+    assertQueryEqualsDOA("term +term +term", null, "+term +term +term");
+    assertQueryEqualsDOA("-term term term", null, "-term +term +term");
+  }
+
+  public void testBoost()
+    throws Exception {
+    StandardAnalyzer oneStopAnalyzer = new StandardAnalyzer(new String[]{"on"});
+    PrecedenceQueryParser qp = new PrecedenceQueryParser("field", oneStopAnalyzer);
+    Query q = qp.parse("on^1.0");
+    assertNotNull(q);
+    q = qp.parse("\"hello\"^2.0");
+    assertNotNull(q);
+    assertEquals(q.getBoost(), (float) 2.0, (float) 0.5);
+    q = qp.parse("hello^2.0");
+    assertNotNull(q);
+    assertEquals(q.getBoost(), (float) 2.0, (float) 0.5);
+    q = qp.parse("\"on\"^1.0");
+    assertNotNull(q);
+
+    q = getParser(new StandardAnalyzer()).parse("the^3");
+    assertNotNull(q);
+  }
+
+  public void testException() throws Exception {
+    try {
+      assertQueryEquals("\"some phrase", null, "abc");
+      fail("ParseException expected, not thrown");
+    } catch (ParseException expected) {
+    }
+  }
+
+  public void testCustomQueryParserWildcard() {
+    try {
+      new QPTestParser("contents", new WhitespaceAnalyzer()).parse("a?t");
+    } catch (ParseException expected) {
+      return;
+    }
+    fail("Wildcard queries should not be allowed");
+  }
+
+  public void testCustomQueryParserFuzzy() throws Exception {
+    try {
+      new QPTestParser("contents", new WhitespaceAnalyzer()).parse("xunit~");
+    } catch (ParseException expected) {
+      return;
+    }
+    fail("Fuzzy queries should not be allowed");
+  }
+
+  public void testBooleanQuery() throws Exception {
+    BooleanQuery.setMaxClauseCount(2);
+    try {
+      getParser(new WhitespaceAnalyzer()).parse("one two three");
+      fail("ParseException expected due to too many boolean clauses");
+    } catch (ParseException expected) {
+      // too many boolean clauses, so ParseException is expected
+    }
+  }
+
+  /**
+   * This test differs from the original QueryParser, showing how the
+   * precedence issue has been corrected.
+   */
+  // failing tests disabled since PrecedenceQueryParser
+  // is currently unmaintained
+  public void _testPrecedence() throws Exception {
+    PrecedenceQueryParser parser = getParser(new WhitespaceAnalyzer());
+    Query query1 = parser.parse("A AND B OR C AND D");
+    Query query2 = parser.parse("(A AND B) OR (C AND D)");
+    assertEquals(query1, query2);
+
+    query1 = parser.parse("A OR B C");
+    query2 = parser.parse("A B C");
+    assertEquals(query1, query2);
+
+    query1 = parser.parse("A AND B C");
+    query2 = parser.parse("(+A +B) C");
+    assertEquals(query1, query2);
+
+    query1 = parser.parse("A AND NOT B");
+    query2 = parser.parse("+A -B");
+    assertEquals(query1, query2);
+
+    query1 = parser.parse("A OR NOT B");
+    query2 = parser.parse("A -B");
+    assertEquals(query1, query2);
+
+    query1 = parser.parse("A OR NOT B AND C");
+    query2 = parser.parse("A (-B +C)");
+    assertEquals(query1, query2);
+  }
+
+
+  public void tearDown() {
+    BooleanQuery.setMaxClauseCount(originalMaxClauses);
+  }
+
+}
diff --git a/contrib/miscellaneous/README.txt b/contrib/miscellaneous/README.txt
deleted file mode 100644
index 29c5803..0000000
--- a/contrib/miscellaneous/README.txt
+++ /dev/null
@@ -1,3 +0,0 @@
-contrib/miscellaneous is a home of different Lucene-related classes
-that all belong to org.apache.lucene.misc package, as they are not
-substantial enough to warrant their own package.
diff --git a/contrib/miscellaneous/build.xml b/contrib/miscellaneous/build.xml
deleted file mode 100644
index ad311b5..0000000
--- a/contrib/miscellaneous/build.xml
+++ /dev/null
@@ -1,43 +0,0 @@
-<?xml version="1.0"?>
-
-<!--
-    Licensed to the Apache Software Foundation (ASF) under one or more
-    contributor license agreements.  See the NOTICE file distributed with
-    this work for additional information regarding copyright ownership.
-    The ASF licenses this file to You under the Apache License, Version 2.0
-    the "License"); you may not use this file except in compliance with
-    the License.  You may obtain a copy of the License at
- 
-        http://www.apache.org/licenses/LICENSE-2.0
- 
-    Unless required by applicable law or agreed to in writing, software
-    distributed under the License is distributed on an "AS IS" BASIS,
-    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-    See the License for the specific language governing permissions and
-    limitations under the License.
- -->
-
-<project name="misc" default="default">
-
-  <!-- TODO: add javacc capability for PrecedenceQueryParser -->
-
-  <description>
-    Miscellaneous Lucene extensions
-  </description>
-
-  <import file="../contrib-build.xml"/>
-
-  <property name="javacc.path" location="src/java/org/apache/lucene/queryParser/precedence"/>
-
-  <target name="javacc" depends="javacc-check" description="generate precedence query parser from jj (requires javacc 3.2)">
-    <delete>
-      <fileset dir="${javacc.path}" includes="*.java">
-        <containsregexp expression="Generated.*By.*JavaCC"/>
-      </fileset>
-    </delete>
-    <invoke-javacc target="${javacc.path}/PrecedenceQueryParser.jj"
-                   outputDir="${javacc.path}"
-    />
-  </target>
-
-</project>
diff --git a/contrib/miscellaneous/pom.xml.template b/contrib/miscellaneous/pom.xml.template
deleted file mode 100644
index 0c47d39..0000000
--- a/contrib/miscellaneous/pom.xml.template
+++ /dev/null
@@ -1,36 +0,0 @@
-<project xmlns="http://maven.apache.org/POM/4.0.0"
-  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
-
-  <!--
-    Licensed to the Apache Software Foundation (ASF) under one
-    or more contributor license agreements.  See the NOTICE file
-    distributed with this work for additional information
-    regarding copyright ownership.  The ASF licenses this file
-    to you under the Apache License, Version 2.0 (the
-    "License"); you may not use this file except in compliance
-    with the License.  You may obtain a copy of the License at
-    
-    http://www.apache.org/licenses/LICENSE-2.0
-    
-    Unless required by applicable law or agreed to in writing,
-    software distributed under the License is distributed on an
-    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-    KIND, either express or implied.  See the License for the
-    specific language governing permissions and limitations
-    under the License.
-  -->
-
-  <modelVersion>4.0.0</modelVersion>
-  <parent>
-    <groupId>org.apache.lucene</groupId>
-    <artifactId>lucene-contrib</artifactId>
-    <version>@version@</version>
-  </parent>
-  <groupId>org.apache.lucene</groupId>
-  <artifactId>lucene-misc</artifactId>
-  <name>Lucene Miscellaneous</name>
-  <version>@version@</version>
-  <description>Miscellaneous Lucene extensions</description>
-  <packaging>jar</packaging>
-</project>
diff --git a/contrib/miscellaneous/src/java/org/apache/lucene/index/FieldNormModifier.java b/contrib/miscellaneous/src/java/org/apache/lucene/index/FieldNormModifier.java
deleted file mode 100644
index 9f25b36..0000000
--- a/contrib/miscellaneous/src/java/org/apache/lucene/index/FieldNormModifier.java
+++ /dev/null
@@ -1,160 +0,0 @@
-package org.apache.lucene.index;
-
-/**
- * Copyright 2006 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.File;
-import java.util.Date;
-
-import org.apache.lucene.search.Similarity;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.FSDirectory;
-import org.apache.lucene.util.StringHelper;
-
-/**
- * Given a directory and a list of fields, updates the fieldNorms in place for every document.
- * 
- * If Similarity class is specified, uses its lengthNorm method to set norms.
- * If -n command line argument is used, removed field norms, as if 
- * {@link org.apache.lucene.document.Field.Index}.NO_NORMS was used.
- *
- * <p>
- * NOTE: This will overwrite any length normalization or field/document boosts.
- * </p>
- *
- */
-public class FieldNormModifier {
-
-  /**
-   * Command Line Execution method.
-   *
-   * <pre>
-   * Usage: FieldNormModifier /path/index &lt;package.SimilarityClassName | -n&gt; field1 field2 ...
-   * </pre>
-   */
-  public static void main(String[] args) throws IOException {
-    if (args.length < 3) {
-      System.err.println("Usage: FieldNormModifier <index> <package.SimilarityClassName | -n> <field1> [field2] ...");
-      System.exit(1);
-    }
-
-    Similarity s = null;
-    if (!args[1].equals("-n")) {
-      try {
-        Class simClass = Class.forName(args[1]);
-        s = (Similarity)simClass.newInstance();
-      } catch (Exception e) {
-        System.err.println("Couldn't instantiate similarity with empty constructor: " + args[1]);
-        e.printStackTrace(System.err);
-        System.exit(1);
-      }
-    }
-
-    Directory d = FSDirectory.open(new File(args[0]));
-    FieldNormModifier fnm = new FieldNormModifier(d, s);
-
-    for (int i = 2; i < args.length; i++) {
-      System.out.print("Updating field: " + args[i] + " " + (new Date()).toString() + " ... ");
-      fnm.reSetNorms(args[i]);
-      System.out.println(new Date().toString());
-    }
-    
-    d.close();
-  }
-  
-  
-  private Directory dir;
-  private Similarity sim;
-  
-  /**
-   * Constructor for code that wishes to use this class programatically
-   * If Similarity is null, kill the field norms.
-   *
-   * @param d the Directory to modify
-   * @param s the Similiary to use (can be null)
-   */
-  public FieldNormModifier(Directory d, Similarity s) {
-    dir = d;
-    sim = s;
-  }
-
-  /**
-   * Resets the norms for the specified field.
-   *
-   * <p>
-   * Opens a new IndexReader on the Directory given to this instance,
-   * modifies the norms (either using the Similarity given to this instance, or by using fake norms,
-   * and closes the IndexReader.
-   * </p>
-   *
-   * @param field the field whose norms should be reset
-   */
-  public void reSetNorms(String field) throws IOException {
-    String fieldName = StringHelper.intern(field);
-    int[] termCounts = new int[0];
-    byte[] fakeNorms = new byte[0];
-    
-    IndexReader reader = null;
-    TermEnum termEnum = null;
-    TermDocs termDocs = null;
-    try {
-      reader = IndexReader.open(dir);
-      termCounts = new int[reader.maxDoc()];
-      // if we are killing norms, get fake ones
-      if (sim == null)
-        fakeNorms = SegmentReader.createFakeNorms(reader.maxDoc());
-      try {
-        termEnum = reader.terms(new Term(field));
-        try {
-          termDocs = reader.termDocs();
-          do {
-            Term term = termEnum.term();
-            if (term != null && term.field().equals(fieldName)) {
-              termDocs.seek(termEnum.term());
-              while (termDocs.next()) {
-                termCounts[termDocs.doc()] += termDocs.freq();
-              }
-            }
-          } while (termEnum.next());
-          
-        } finally {
-          if (null != termDocs) termDocs.close();
-        }
-      } finally {
-        if (null != termEnum) termEnum.close();
-      }
-    } finally {
-      if (null != reader) reader.close();
-    }
-    
-    try {
-      reader = IndexReader.open(dir); 
-      for (int d = 0; d < termCounts.length; d++) {
-        if (! reader.isDeleted(d)) {
-          if (sim == null)
-            reader.setNorm(d, fieldName, fakeNorms[0]);
-          else
-            reader.setNorm(d, fieldName, sim.encodeNorm(sim.lengthNorm(fieldName, termCounts[d])));
-        }
-      }
-      
-    } finally {
-      if (null != reader) reader.close();
-    }
-  }
-  
-}
diff --git a/contrib/miscellaneous/src/java/org/apache/lucene/index/TermVectorAccessor.java b/contrib/miscellaneous/src/java/org/apache/lucene/index/TermVectorAccessor.java
deleted file mode 100644
index 314aba4..0000000
--- a/contrib/miscellaneous/src/java/org/apache/lucene/index/TermVectorAccessor.java
+++ /dev/null
@@ -1,170 +0,0 @@
-package org.apache.lucene.index;
-
-import org.apache.lucene.util.StringHelper;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Collection;
-import java.util.Iterator;
-/*
- *  Licensed under the Apache License, Version 2.0 (the "License");
- *  you may not use this file except in compliance with the License.
- *  You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- *  Unless required by applicable law or agreed to in writing, software
- *  distributed under the License is distributed on an "AS IS" BASIS,
- *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- *  See the License for the specific language governing permissions and
- *  limitations under the License.
- *
- */
-
-
-/**
- * Transparent access to the vector space model,
- * either via TermFreqVector or by resolving it from the inverted index.
- * <p/>
- * Resolving a term vector from a large index can be a time consuming process.
- * <p/>
- * Warning! This class is not thread safe!
- */
-public class TermVectorAccessor {
-
-  public TermVectorAccessor() {
-  }
-
-  /**
-   * Instance reused to save garbage collector some time
-   */
-  private TermVectorMapperDecorator decoratedMapper = new TermVectorMapperDecorator();
-
-
-  /**
-   * Visits the TermVectorMapper and populates it with terms available for a given document,
-   * either via a vector created at index time or by resolving them from the inverted index.
-   *
-   * @param indexReader    Index source
-   * @param documentNumber Source document to access
-   * @param fieldName      Field to resolve
-   * @param mapper         Mapper to be mapped with data
-   * @throws IOException
-   */
-  public void accept(IndexReader indexReader, int documentNumber, String fieldName, TermVectorMapper mapper) throws IOException {
-
-    fieldName = StringHelper.intern(fieldName);
-
-    decoratedMapper.decorated = mapper;
-    decoratedMapper.termVectorStored = false;
-
-    indexReader.getTermFreqVector(documentNumber, fieldName, decoratedMapper);
-
-    if (!decoratedMapper.termVectorStored) {
-      mapper.setDocumentNumber(documentNumber);
-      build(indexReader, fieldName, mapper, documentNumber);
-    }
-  }
-
-  /** Instance reused to save garbage collector some time */
-  private List/*<String>*/ tokens;
-
-  /** Instance reused to save garbage collector some time */
-  private List/*<int[]>*/ positions;
-
-  /** Instance reused to save garbage collector some time */
-  private List/*<Integer>*/ frequencies;
-
-
-  /**
-   * Populates the mapper with terms available for the given field in a document
-   * by resolving the inverted index.
-   *
-   * @param indexReader
-   * @param field interned field name
-   * @param mapper
-   * @param documentNumber
-   * @throws IOException
-   */
-  private void build(IndexReader indexReader, String field, TermVectorMapper mapper, int documentNumber) throws IOException {
-
-    if (tokens == null) {
-      tokens = new ArrayList/*<String>*/(500);
-      positions = new ArrayList/*<int[]>*/(500);
-      frequencies = new ArrayList/*<Integer>*/(500);
-    } else {
-      tokens.clear();
-      frequencies.clear();
-      positions.clear();
-    }
-
-    TermEnum termEnum = indexReader.terms();
-    if (termEnum.skipTo(new Term(field, ""))) {
-
-      while (termEnum.term().field() == field) {
-        TermPositions termPositions = indexReader.termPositions(termEnum.term());
-        if (termPositions.skipTo(documentNumber)) {
-
-          frequencies.add(new Integer(termPositions.freq()));
-          tokens.add(termEnum.term().text());
-
-
-          if (!mapper.isIgnoringPositions()) {
-            int[] positions = new int[termPositions.freq()];
-            for (int i = 0; i < positions.length; i++) {
-              positions[i] = termPositions.nextPosition();
-            }
-            this.positions.add(positions);
-          } else {
-            positions.add(null);
-          }
-        }
-        termPositions.close();
-        if (!termEnum.next()) {
-          break;
-        }
-      }
-
-      mapper.setDocumentNumber(documentNumber);
-      mapper.setExpectations(field, tokens.size(), false, !mapper.isIgnoringPositions());
-      for (int i = 0; i < tokens.size(); i++) {
-        mapper.map((String) tokens.get(i), ((Integer) frequencies.get(i)).intValue(), (TermVectorOffsetInfo[]) null, (int[]) positions.get(i));
-      }
-
-    }
-    termEnum.close();
-
-
-  }
-
-
-  private static class TermVectorMapperDecorator extends TermVectorMapper {
-
-    private TermVectorMapper decorated;
-
-    public boolean isIgnoringPositions() {
-      return decorated.isIgnoringPositions();
-    }
-
-    public boolean isIgnoringOffsets() {
-      return decorated.isIgnoringOffsets();
-    }
-
-    private boolean termVectorStored = false;
-
-    public void setExpectations(String field, int numTerms, boolean storeOffsets, boolean storePositions) {
-      decorated.setExpectations(field, numTerms, storeOffsets, storePositions);
-      termVectorStored = true;
-    }
-
-    public void map(String term, int frequency, TermVectorOffsetInfo[] offsets, int[] positions) {
-      decorated.map(term, frequency, offsets, positions);
-    }
-
-    public void setDocumentNumber(int documentNumber) {
-      decorated.setDocumentNumber(documentNumber);
-    }
-  }
-
-}
diff --git a/contrib/miscellaneous/src/java/org/apache/lucene/misc/ChainedFilter.java b/contrib/miscellaneous/src/java/org/apache/lucene/misc/ChainedFilter.java
deleted file mode 100644
index e18e561..0000000
--- a/contrib/miscellaneous/src/java/org/apache/lucene/misc/ChainedFilter.java
+++ /dev/null
@@ -1,268 +0,0 @@
-package org.apache.lucene.misc;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.search.DocIdSet;
-import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.search.Filter;
-import org.apache.lucene.util.OpenBitSet;
-import org.apache.lucene.util.OpenBitSetDISI;
-import org.apache.lucene.util.SortedVIntList;
-
-/**
- * <p>
- * Allows multiple {@link Filter}s to be chained.
- * Logical operations such as <b>NOT</b> and <b>XOR</b>
- * are applied between filters. One operation can be used
- * for all filters, or a specific operation can be declared
- * for each filter.
- * </p>
- * <p>
- * Order in which filters are called depends on
- * the position of the filter in the chain. It's probably
- * more efficient to place the most restrictive filters
- * /least computationally-intensive filters first.
- * </p>
- *
- */
-public class ChainedFilter extends Filter
-{
-    public static final int OR = 0;
-    public static final int AND = 1;
-    public static final int ANDNOT = 2;
-    public static final int XOR = 3;
-    /**
-     * Logical operation when none is declared. Defaults to
-     * OR.
-     */
-    public static int DEFAULT = OR;
-
-    /** The filter chain */
-    private Filter[] chain = null;
-
-    private int[] logicArray;
-
-    private int logic = -1;
-
-    /**
-     * Ctor.
-     * @param chain The chain of filters
-     */
-    public ChainedFilter(Filter[] chain)
-    {
-        this.chain = chain;
-    }
-
-    /**
-     * Ctor.
-     * @param chain The chain of filters
-     * @param logicArray Logical operations to apply between filters
-     */
-    public ChainedFilter(Filter[] chain, int[] logicArray)
-    {
-        this.chain = chain;
-        this.logicArray = logicArray;
-    }
-
-    /**
-     * Ctor.
-     * @param chain The chain of filters
-     * @param logic Logicial operation to apply to ALL filters
-     */
-    public ChainedFilter(Filter[] chain, int logic)
-    {
-        this.chain = chain;
-        this.logic = logic;
-    }
-
-    /**
-     * {@link Filter#getDocIdSet}.
-     */
-    public DocIdSet getDocIdSet(IndexReader reader) throws IOException
-    {
-        int[] index = new int[1]; // use array as reference to modifiable int; 
-        index[0] = 0;             // an object attribute would not be thread safe.
-        if (logic != -1)
-            return getDocIdSet(reader, logic, index);
-        else if (logicArray != null)
-            return getDocIdSet(reader, logicArray, index);
-        else
-            return getDocIdSet(reader, DEFAULT, index);
-    }
-
-    private DocIdSetIterator getDISI(Filter filter, IndexReader reader)
-    throws IOException {
-        DocIdSet docIdSet = filter.getDocIdSet(reader);
-        if (docIdSet == null) {
-          return DocIdSet.EMPTY_DOCIDSET.iterator();
-        } else {
-          DocIdSetIterator iter = docIdSet.iterator();
-          if (iter == null) {
-            return DocIdSet.EMPTY_DOCIDSET.iterator();
-          } else {
-            return iter;
-          }
-        }
-    }
-
-    private OpenBitSetDISI initialResult(IndexReader reader, int logic, int[] index)
-    throws IOException
-    {
-        OpenBitSetDISI result;
-        /**
-         * First AND operation takes place against a completely false
-         * bitset and will always return zero results.
-         */
-        if (logic == AND)
-        {
-            result = new OpenBitSetDISI(getDISI(chain[index[0]], reader), reader.maxDoc());
-            ++index[0];
-        }
-        else if (logic == ANDNOT)
-        {
-            result = new OpenBitSetDISI(getDISI(chain[index[0]], reader), reader.maxDoc());
-            result.flip(0,reader.maxDoc()); // NOTE: may set bits for deleted docs.
-            ++index[0];
-        }
-        else
-        {
-            result = new OpenBitSetDISI(reader.maxDoc());
-        }
-        return result;
-    }
-
-    // TODO: in 3.0, instead of removing this deprecated
-    // method, make it a no-op and mark it final
-    /** Provide a SortedVIntList when it is definitely
-     *  smaller than an OpenBitSet
-     *  @deprecated Either use CachingWrapperFilter, or
-     *  switch to a different DocIdSet implementation yourself. */
-    protected DocIdSet finalResult(OpenBitSetDISI result, int maxDocs) {
-        return (result.cardinality() < (maxDocs / 9))
-              ? (DocIdSet) new SortedVIntList(result)
-              : (DocIdSet) result;
-    }
-        
-
-    /**
-     * Delegates to each filter in the chain.
-     * @param reader IndexReader
-     * @param logic Logical operation
-     * @return DocIdSet
-     */
-    private DocIdSet getDocIdSet(IndexReader reader, int logic, int[] index)
-    throws IOException
-    {
-        OpenBitSetDISI result = initialResult(reader, logic, index);
-        for (; index[0] < chain.length; index[0]++)
-        {
-            doChain(result, logic, chain[index[0]].getDocIdSet(reader));
-        }
-        return finalResult(result, reader.maxDoc());
-    }
-
-    /**
-     * Delegates to each filter in the chain.
-     * @param reader IndexReader
-     * @param logic Logical operation
-     * @return DocIdSet
-     */
-    private DocIdSet getDocIdSet(IndexReader reader, int[] logic, int[] index)
-    throws IOException
-    {
-        if (logic.length != chain.length)
-            throw new IllegalArgumentException("Invalid number of elements in logic array");
-
-        OpenBitSetDISI result = initialResult(reader, logic[0], index);
-        for (; index[0] < chain.length; index[0]++)
-        {
-            doChain(result, logic[index[0]], chain[index[0]].getDocIdSet(reader));
-        }
-        return finalResult(result, reader.maxDoc());
-    }
-
-    public String toString()
-    {
-        StringBuffer sb = new StringBuffer();
-        sb.append("ChainedFilter: [");
-        for (int i = 0; i < chain.length; i++)
-        {
-            sb.append(chain[i]);
-            sb.append(' ');
-        }
-        sb.append(']');
-        return sb.toString();
-    }
-
-    private void doChain(OpenBitSetDISI result, int logic, DocIdSet dis)
-    throws IOException {
-      
-      if (dis instanceof OpenBitSet) {
-        // optimized case for OpenBitSets
-        switch (logic) {
-            case OR:
-                result.or((OpenBitSet) dis);
-                break;
-            case AND:
-                result.and((OpenBitSet) dis);
-                break;
-            case ANDNOT:
-                result.andNot((OpenBitSet) dis);
-                break;
-            case XOR:
-                result.xor((OpenBitSet) dis);
-                break;
-            default:
-                doChain(result, DEFAULT, dis);
-                break;
-        }
-      } else {
-        DocIdSetIterator disi;
-        if (dis == null) {
-          disi = DocIdSet.EMPTY_DOCIDSET.iterator();
-        } else {
-          disi = dis.iterator();
-          if (disi == null) {
-            disi = DocIdSet.EMPTY_DOCIDSET.iterator();            
-          }
-        }
-
-        switch (logic) {
-            case OR:
-                result.inPlaceOr(disi);
-                break;
-            case AND:
-                result.inPlaceAnd(disi);
-                break;
-            case ANDNOT:
-                result.inPlaceNot(disi);
-                break;
-            case XOR:
-                result.inPlaceXor(disi);
-                break;
-            default:
-                doChain(result, DEFAULT, dis);
-                break;
-        }
-      }
-    }
-
-}
diff --git a/contrib/miscellaneous/src/java/org/apache/lucene/misc/HighFreqTerms.java b/contrib/miscellaneous/src/java/org/apache/lucene/misc/HighFreqTerms.java
deleted file mode 100644
index 71c5b5a..0000000
--- a/contrib/miscellaneous/src/java/org/apache/lucene/misc/HighFreqTerms.java
+++ /dev/null
@@ -1,96 +0,0 @@
-package org.apache.lucene.misc;
-
-/**
-  * Copyright 2004 The Apache Software Foundation
-  *
-  * Licensed under the Apache License, Version 2.0 (the "License");
-  * you may not use this file except in compliance with the License.
-  * You may obtain a copy of the License at
-  *
-  *     http://www.apache.org/licenses/LICENSE-2.0
-  *
-  * Unless required by applicable law or agreed to in writing, software
-  * distributed under the License is distributed on an "AS IS" BASIS,
-  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  * See the License for the specific language governing permissions and
-  * limitations under the License.
-  */
-
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.TermEnum;
-import org.apache.lucene.util.PriorityQueue;
-
-/**
- * <code>HighFreqTerms</code> class extracts terms and their frequencies out
- * of an existing Lucene index.
- *
- * @version $Id$
- */
-public class HighFreqTerms {
-  
-  // The top numTerms will be displayed
-  public static final int numTerms = 100;
-
-  public static void main(String[] args) throws Exception {
-    IndexReader reader = null;
-    String field = null;
-    if (args.length == 1) {
-      reader = IndexReader.open(args[0]);
-    } else if (args.length == 2) {
-      reader = IndexReader.open(args[0]);
-      field = args[1];
-    } else {
-      usage();
-      System.exit(1);
-    }
-
-    TermInfoQueue tiq = new TermInfoQueue(numTerms);
-    TermEnum terms = reader.terms();
-
-    if (field != null) { 
-      while (terms.next()) {
-        if (terms.term().field().equals(field)) {
-          tiq.insert(new TermInfo(terms.term(), terms.docFreq()));
-        }
-      }
-    }
-    else {
-      while (terms.next()) {
-        tiq.insert(new TermInfo(terms.term(), terms.docFreq()));
-      }
-    }
-    while (tiq.size() != 0) {
-      TermInfo termInfo = (TermInfo) tiq.pop();
-      System.out.println(termInfo.term + " " + termInfo.docFreq);
-    }
-
-    reader.close();
-  }
-
-  private static void usage() {
-    System.out.println(
-         "\n\n"
-         + "java org.apache.lucene.misc.HighFreqTerms <index dir> [field]\n\n");
-  }
-}
-
-final class TermInfo {
-  TermInfo(Term t, int df) {
-    term = t;
-    docFreq = df;
-  }
-  int docFreq;
-  Term term;
-}
-
-final class TermInfoQueue extends PriorityQueue {
-  TermInfoQueue(int size) {
-    initialize(size);
-  }
-  protected final boolean lessThan(Object a, Object b) {
-    TermInfo termInfoA = (TermInfo) a;
-    TermInfo termInfoB = (TermInfo) b;
-    return termInfoA.docFreq < termInfoB.docFreq;
-  }
-}
diff --git a/contrib/miscellaneous/src/java/org/apache/lucene/misc/IndexMergeTool.java b/contrib/miscellaneous/src/java/org/apache/lucene/misc/IndexMergeTool.java
deleted file mode 100644
index 8ea1886..0000000
--- a/contrib/miscellaneous/src/java/org/apache/lucene/misc/IndexMergeTool.java
+++ /dev/null
@@ -1,55 +0,0 @@
-package org.apache.lucene.misc;
-
-/**
-  * Copyright 2005 The Apache Software Foundation
-  *
-  * Licensed under the Apache License, Version 2.0 (the "License");
-  * you may not use this file except in compliance with the License.
-  * You may obtain a copy of the License at
-  *
-  *     http://www.apache.org/licenses/LICENSE-2.0
-  *
-  * Unless required by applicable law or agreed to in writing, software
-  * distributed under the License is distributed on an "AS IS" BASIS,
-  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  * See the License for the specific language governing permissions and
-  * limitations under the License.
-  */
-
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.analysis.SimpleAnalyzer;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.FSDirectory;
-
-import java.io.File;
-import java.io.IOException;
-
-/**
- * Merges indices specified on the command line into the index
- * specified as the first command line argument.
- * @version $Id$
- */
-public class IndexMergeTool {
-  public static void main(String[] args) throws IOException {
-    if (args.length < 3) {
-      System.err.println("Usage: IndexMergeTool <mergedIndex> <index1> <index2> [index3] ...");
-      System.exit(1);
-    }
-    File mergedIndex = new File(args[0]);
-
-    IndexWriter writer = new IndexWriter(mergedIndex, new  SimpleAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
-
-    Directory[] indexes = new Directory[args.length - 1];
-    for (int i = 1; i < args.length; i++) {
-      indexes[i  - 1] = FSDirectory.open(new File(args[i]));
-    }
-
-    System.out.println("Merging...");
-    writer.addIndexes(indexes);
-
-    System.out.println("Optimizing...");
-    writer.optimize();
-    writer.close();
-    System.out.println("Done.");
-  }
-}
diff --git a/contrib/miscellaneous/src/java/org/apache/lucene/misc/LengthNormModifier.java b/contrib/miscellaneous/src/java/org/apache/lucene/misc/LengthNormModifier.java
deleted file mode 100644
index 9fdb725..0000000
--- a/contrib/miscellaneous/src/java/org/apache/lucene/misc/LengthNormModifier.java
+++ /dev/null
@@ -1,154 +0,0 @@
-package org.apache.lucene.misc;
-
-/**
- * Copyright 2006 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.TermEnum;
-import org.apache.lucene.index.TermDocs;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.search.Similarity;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.FSDirectory;
-import org.apache.lucene.util.StringHelper;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.Date;
-
-/**
- * Given a directory, a Similarity, and a list of fields, updates the
- * fieldNorms in place for every document using the Similarity.lengthNorm.
- *
- * <p>
- * NOTE: This only works if you do <b>not</b> use field/document boosts in your
- * index.
- * </p>
- *
- * @version $Id$
- * @deprecated Use {@link org.apache.lucene.index.FieldNormModifier}
- */
-public class LengthNormModifier {
-  
-  /**
-   * Command Line Execution method.
-   *
-   * <pre>
-   * Usage: LengthNormModifier /path/index package.SimilarityClassName field1 field2 ...
-   * </pre>
-   */
-  public static void main(String[] args) throws IOException {
-    if (args.length < 3) {
-      System.err.println("Usage: LengthNormModifier <index> <package.SimilarityClassName> <field1> [field2] ...");
-      System.exit(1);
-    }
-    
-    Similarity s = null;
-    try {
-      Class simClass = Class.forName(args[1]);
-      s = (Similarity)simClass.newInstance();
-    } catch (Exception e) {
-      System.err.println("Couldn't instantiate similarity with empty constructor: " + args[1]);
-      e.printStackTrace(System.err);
-    }
-    
-    File index = new File(args[0]);
-    Directory d = FSDirectory.open(index);
-    
-    LengthNormModifier lnm = new LengthNormModifier(d, s);
-    
-    for (int i = 2; i < args.length; i++) {
-      System.out.print("Updating field: " + args[i] + " " + (new Date()).toString() + " ... ");
-      lnm.reSetNorms(args[i]);
-      System.out.println(new Date().toString());
-    }
-    
-    d.close();
-  }
-  
-  
-  private Directory dir;
-  private Similarity sim;
-  
-  /**
-   * Constructor for code that wishes to use this class progaomatically.
-   *
-   * @param d The Directory to modify
-   * @param s The Similarity to use in <code>reSetNorms</code>
-   */
-  public LengthNormModifier(Directory d, Similarity s) {
-    dir = d;
-    sim = s;
-  }
-  
-  /**
-   * Resets the norms for the specified field.
-   *
-   * <p>
-   * Opens a new IndexReader on the Directory given to this instance,
-   * modifies the norms using the Similarity given to this instance,
-   * and closes the IndexReader.
-   * </p>
-   *
-   * @param field the field whose norms should be reset
-   */
-  public void reSetNorms(String field) throws IOException {
-    String fieldName = StringHelper.intern(field);
-    int[] termCounts = new int[0];
-    
-    IndexReader reader = null;
-    TermEnum termEnum = null;
-    TermDocs termDocs = null;
-    try {
-      reader = IndexReader.open(dir);
-      termCounts = new int[reader.maxDoc()];
-      try {
-        termEnum = reader.terms(new Term(field));
-        try {
-          termDocs = reader.termDocs();
-          do {
-            Term term = termEnum.term();
-            if (term != null && term.field().equals(fieldName)) {
-              termDocs.seek(termEnum.term());
-              while (termDocs.next()) {
-                termCounts[termDocs.doc()] += termDocs.freq();
-              }
-            }
-          } while (termEnum.next());
-        } finally {
-          if (null != termDocs) termDocs.close();
-        }
-      } finally {
-        if (null != termEnum) termEnum.close();
-      }
-    } finally {
-      if (null != reader) reader.close();
-    }
-    
-    try {
-      reader = IndexReader.open(dir); 
-      for (int d = 0; d < termCounts.length; d++) {
-        if (! reader.isDeleted(d)) {
-          byte norm = sim.encodeNorm(sim.lengthNorm(fieldName, termCounts[d]));
-          reader.setNorm(d, fieldName, norm);
-        }
-      }
-    } finally {
-      if (null != reader) reader.close();
-    }
-  }
-  
-}
diff --git a/contrib/miscellaneous/src/java/org/apache/lucene/misc/SweetSpotSimilarity.java b/contrib/miscellaneous/src/java/org/apache/lucene/misc/SweetSpotSimilarity.java
deleted file mode 100644
index e59ee57..0000000
--- a/contrib/miscellaneous/src/java/org/apache/lucene/misc/SweetSpotSimilarity.java
+++ /dev/null
@@ -1,267 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.lucene.misc;
-
-import org.apache.lucene.search.Similarity;
-import org.apache.lucene.search.DefaultSimilarity;
-import org.apache.lucene.index.FieldInvertState;
-
-import java.util.Map;
-import java.util.HashMap;
-
-/**
- * A similarity with a lengthNorm that provides for a "plateau" of
- * equally good lengths, and tf helper functions.
- *
- * <p>
- * For lengthNorm, A global min/max can be specified to define the
- * plateau of lengths that should all have a norm of 1.0.
- * Below the min, and above the max the lengthNorm drops off in a
- * sqrt function.
- * </p>
- * <p>
- * A per field min/max can be specified if different fields have
- * different sweet spots.
- * </p>
- *
- * <p>
- * For tf, baselineTf and hyperbolicTf functions are provided, which
- * subclasses can choose between.
- * </p>
- *
- */
-public class SweetSpotSimilarity extends DefaultSimilarity {
-
-  private int ln_min = 1;
-  private int ln_max = 1;
-  private float ln_steep = 0.5f;
-
-  private Map ln_mins = new HashMap(7);
-  private Map ln_maxs = new HashMap(7);
-  private Map ln_steeps = new HashMap(7);
-  private Map ln_overlaps = new HashMap(7);
-
-  private float tf_base = 0.0f;
-  private float tf_min = 0.0f;
-
-  private float tf_hyper_min = 0.0f;
-  private float tf_hyper_max = 2.0f;
-  private double tf_hyper_base = 1.3d;
-  private float tf_hyper_xoffset = 10.0f;
-    
-  public SweetSpotSimilarity() {
-    super();
-  }
-
-  /**
-   * Sets the baseline and minimum function variables for baselineTf
-   *
-   * @see #baselineTf
-   */
-  public void setBaselineTfFactors(float base, float min) {
-    tf_min = min;
-    tf_base = base;
-  }
-  
-  /**
-   * Sets the function variables for the hyperbolicTf functions
-   *
-   * @param min the minimum tf value to ever be returned (default: 0.0)
-   * @param max the maximum tf value to ever be returned (default: 2.0)
-   * @param base the base value to be used in the exponential for the hyperbolic function (default: e)
-   * @param xoffset the midpoint of the hyperbolic function (default: 10.0)
-   * @see #hyperbolicTf
-   */
-  public void setHyperbolicTfFactors(float min, float max,
-                                     double base, float xoffset) {
-    tf_hyper_min = min;
-    tf_hyper_max = max;
-    tf_hyper_base = base;
-    tf_hyper_xoffset = xoffset;
-  }
-    
-  /**
-   * Sets the default function variables used by lengthNorm when no field
-   * specifc variables have been set.
-   *
-   * @see #lengthNorm
-   */
-  public void setLengthNormFactors(int min, int max, float steepness) {
-    this.ln_min = min;
-    this.ln_max = max;
-    this.ln_steep = steepness;
-  }
-
-  /**
-   * Sets the function variables used by lengthNorm for a specific named field.
-   * 
-   * @param field field name
-   * @param min minimum value
-   * @param max maximum value
-   * @param steepness steepness of the curve
-   * @param discountOverlaps if true, <code>numOverlapTokens</code> will be
-   * subtracted from <code>numTokens</code>; if false then
-   * <code>numOverlapTokens</code> will be assumed to be 0 (see
-   * {@link DefaultSimilarity#computeNorm(String, FieldInvertState)} for details).
-   *
-   * @see #lengthNorm
-   */
-  public void setLengthNormFactors(String field, int min, int max,
-                                   float steepness, boolean discountOverlaps) {
-    ln_mins.put(field, new Integer(min));
-    ln_maxs.put(field, new Integer(max));
-    ln_steeps.put(field, new Float(steepness));
-    ln_overlaps.put(field, new Boolean(discountOverlaps));
-  }
-    
-  /**
-   * Implemented as <code> state.getBoost() *
-   * lengthNorm(fieldName, numTokens) </code> where
-   * numTokens does not count overlap tokens if
-   * discountOverlaps is true by default or true for this
-   * specific field. */
-  public float computeNorm(String fieldName, FieldInvertState state) {
-    final int numTokens;
-    boolean overlaps = discountOverlaps;
-    if (ln_overlaps.containsKey(fieldName)) {
-      overlaps = ((Boolean)ln_overlaps.get(fieldName)).booleanValue();
-    }
-    if (overlaps)
-      numTokens = state.getLength() - state.getNumOverlap();
-    else
-      numTokens = state.getLength();
-
-    return state.getBoost() * lengthNorm(fieldName, numTokens);
-  }
-
-  /**
-   * Implemented as:
-   * <code>
-   * 1/sqrt( steepness * (abs(x-min) + abs(x-max) - (max-min)) + 1 )
-   * </code>.
-   *
-   * <p>
-   * This degrades to <code>1/sqrt(x)</code> when min and max are both 1 and
-   * steepness is 0.5
-   * </p>
-   *
-   * <p>
-   * :TODO: potential optimization is to just flat out return 1.0f if numTerms
-   * is between min and max.
-   * </p>
-   *
-   * @see #setLengthNormFactors
-   */
-  public float lengthNorm(String fieldName, int numTerms) {
-    int l = ln_min;
-    int h = ln_max;
-    float s = ln_steep;
-  
-    if (ln_mins.containsKey(fieldName)) {
-      l = ((Number)ln_mins.get(fieldName)).intValue();
-    }
-    if (ln_maxs.containsKey(fieldName)) {
-      h = ((Number)ln_maxs.get(fieldName)).intValue();
-    }
-    if (ln_steeps.containsKey(fieldName)) {
-      s = ((Number)ln_steeps.get(fieldName)).floatValue();
-    }
-  
-    return (float)
-      (1.0f /
-       Math.sqrt
-       (
-        (
-         s *
-         (float)(Math.abs(numTerms - l) + Math.abs(numTerms - h) - (h-l))
-         )
-        + 1.0f
-        )
-       );
-  }
-
-  /**
-   * Delegates to baselineTf
-   *
-   * @see #baselineTf
-   */
-  public float tf(int freq) {
-    return baselineTf(freq);
-  }
-  
-  /**
-   * Implemented as:
-   * <code>
-   *  (x &lt;= min) &#63; base : sqrt(x+(base**2)-min)
-   * </code>
-   * ...but with a special case check for 0.
-   * <p>
-   * This degrates to <code>sqrt(x)</code> when min and base are both 0
-   * </p>
-   *
-   * @see #setBaselineTfFactors
-   */
-  public float baselineTf(float freq) {
-
-    if (0.0f == freq) return 0.0f;
-  
-    return (freq <= tf_min)
-      ? tf_base
-      : (float)Math.sqrt(freq + (tf_base * tf_base) - tf_min);
-  }
-
-  /**
-   * Uses a hyperbolic tangent function that allows for a hard max...
-   *
-   * <code>
-   * tf(x)=min+(max-min)/2*(((base**(x-xoffset)-base**-(x-xoffset))/(base**(x-xoffset)+base**-(x-xoffset)))+1)
-   * </code>
-   *
-   * <p>
-   * This code is provided as a convincience for subclasses that want
-   * to use a hyperbolic tf function.
-   * </p>
-   *
-   * @see #setHyperbolicTfFactors
-   */
-  public float hyperbolicTf(float freq) {
-    if (0.0f == freq) return 0.0f;
-
-    final float min = tf_hyper_min;
-    final float max = tf_hyper_max;
-    final double base = tf_hyper_base;
-    final float xoffset = tf_hyper_xoffset;
-    final double x = (double)(freq - xoffset);
-  
-    final float result = min +
-      (float)(
-              (max-min) / 2.0f
-              *
-              (
-               ( ( Math.pow(base,x) - Math.pow(base,-x) )
-                 / ( Math.pow(base,x) + Math.pow(base,-x) )
-                 )
-               + 1.0d
-               )
-              );
-
-    return Float.isNaN(result) ? max : result;
-    
-  }
-
-}
diff --git a/contrib/miscellaneous/src/java/org/apache/lucene/queryParser/analyzing/AnalyzingQueryParser.java b/contrib/miscellaneous/src/java/org/apache/lucene/queryParser/analyzing/AnalyzingQueryParser.java
deleted file mode 100644
index c3f686a..0000000
--- a/contrib/miscellaneous/src/java/org/apache/lucene/queryParser/analyzing/AnalyzingQueryParser.java
+++ /dev/null
@@ -1,318 +0,0 @@
-package org.apache.lucene.queryParser.analyzing;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.StringReader;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.Token;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-import org.apache.lucene.queryParser.ParseException;
-import org.apache.lucene.search.Query;
-
-/**
- * Overrides Lucene's default QueryParser so that Fuzzy-, Prefix-, Range-, and WildcardQuerys
- * are also passed through the given analyzer, but wild card characters (like <code>*</code>) 
- * don't get removed from the search terms.
- * 
- * <p><b>Warning:</b> This class should only be used with analyzers that do not use stopwords
- * or that add tokens. Also, several stemming analyzers are inappropriate: for example, GermanAnalyzer 
- * will turn <code>H&auml;user</code> into <code>hau</code>, but <code>H?user</code> will 
- * become <code>h?user</code> when using this parser and thus no match would be found (i.e.
- * using this parser will be no improvement over QueryParser in such cases). 
- *
- * @version $Revision$, $Date$
- */
-public class AnalyzingQueryParser extends org.apache.lucene.queryParser.QueryParser {
-
-  /**
-   * Constructs a query parser.
-   * @param field    the default field for query terms.
-   * @param analyzer used to find terms in the query text.
-   */
-  public AnalyzingQueryParser(String field, Analyzer analyzer) {
-    super(field, analyzer);
-  }
-
-  /**
-   * Called when parser
-   * parses an input term token that contains one or more wildcard
-   * characters (like <code>*</code>), but is not a prefix term token (one
-   * that has just a single * character at the end).
-   * <p>
-   * Example: will be called for <code>H?user</code> or for <code>H*user</code> 
-   * but not for <code>*user</code>.
-   * <p>
-   * Depending on analyzer and settings, a wildcard term may (most probably will)
-   * be lower-cased automatically. It <b>will</b> go through the default Analyzer.
-   * <p>
-   * Overrides super class, by passing terms through analyzer.
-   *
-   * @param  field   Name of the field query will use.
-   * @param  termStr Term token that contains one or more wild card
-   *                 characters (? or *), but is not simple prefix term
-   *
-   * @return Resulting {@link Query} built for the term
-   * @throws ParseException
-   */
-  protected Query getWildcardQuery(String field, String termStr) throws ParseException {
-    List tlist = new ArrayList();
-    List wlist = new ArrayList();
-    /* somewhat a hack: find/store wildcard chars
-     * in order to put them back after analyzing */
-    boolean isWithinToken = (!termStr.startsWith("?") && !termStr.startsWith("*"));
-    StringBuffer tmpBuffer = new StringBuffer();
-    char[] chars = termStr.toCharArray();
-    for (int i = 0; i < termStr.length(); i++) {
-      if (chars[i] == '?' || chars[i] == '*') {
-        if (isWithinToken) {
-          tlist.add(tmpBuffer.toString());
-          tmpBuffer.setLength(0);
-        }
-        isWithinToken = false;
-      } else {
-        if (!isWithinToken) {
-          wlist.add(tmpBuffer.toString());
-          tmpBuffer.setLength(0);
-        }
-        isWithinToken = true;
-      }
-      tmpBuffer.append(chars[i]);
-    }
-    if (isWithinToken) {
-      tlist.add(tmpBuffer.toString());
-    } else {
-      wlist.add(tmpBuffer.toString());
-    }
-
-    // get Analyzer from superclass and tokenize the term
-    TokenStream source = getAnalyzer().tokenStream(field, new StringReader(termStr));
-    TermAttribute termAtt = (TermAttribute) source.addAttribute(TermAttribute.class);
-    
-    int countTokens = 0;
-    while (true) {
-      try {
-        if (!source.incrementToken()) break;
-      } catch (IOException e) {
-        break;
-      }
-      String term = termAtt.term();
-      if (!"".equals(term)) {
-        try {
-          tlist.set(countTokens++, term);
-        } catch (IndexOutOfBoundsException ioobe) {
-          countTokens = -1;
-        }
-      }
-    }
-    try {
-      source.close();
-    } catch (IOException e) {
-      // ignore
-    }
-
-    if (countTokens != tlist.size()) {
-      /* this means that the analyzer used either added or consumed 
-       * (common for a stemmer) tokens, and we can't build a WildcardQuery */
-      throw new ParseException("Cannot build WildcardQuery with analyzer "
-          + getAnalyzer().getClass() + " - tokens added or lost");
-    }
-
-    if (tlist.size() == 0) {
-      return null;
-    } else if (tlist.size() == 1) {
-      if (wlist != null && wlist.size() == 1) {
-        /* if wlist contains one wildcard, it must be at the end, because:
-         * 1) wildcards are not allowed in 1st position of a term by QueryParser
-         * 2) if wildcard was *not* in end, there would be *two* or more tokens */
-        return super.getWildcardQuery(field, (String) tlist.get(0)
-            + (((String) wlist.get(0)).toString()));
-      } else {
-        /* we should never get here! if so, this method was called
-         * with a termStr containing no wildcard ... */
-        throw new IllegalArgumentException("getWildcardQuery called without wildcard");
-      }
-    } else {
-      /* the term was tokenized, let's rebuild to one token
-       * with wildcards put back in postion */
-      StringBuffer sb = new StringBuffer();
-      for (int i = 0; i < tlist.size(); i++) {
-        sb.append((String) tlist.get(i));
-        if (wlist != null && wlist.size() > i) {
-          sb.append((String) wlist.get(i));
-        }
-      }
-      return super.getWildcardQuery(field, sb.toString());
-    }
-  }
-
-  /**
-   * Called when parser parses an input term
-   * token that uses prefix notation; that is, contains a single '*' wildcard
-   * character as its last character. Since this is a special case
-   * of generic wildcard term, and such a query can be optimized easily,
-   * this usually results in a different query object.
-   * <p>
-   * Depending on analyzer and settings, a prefix term may (most probably will)
-   * be lower-cased automatically. It <b>will</b> go through the default Analyzer.
-   * <p>
-   * Overrides super class, by passing terms through analyzer.
-   *
-   * @param  field   Name of the field query will use.
-   * @param  termStr Term token to use for building term for the query
-   *                 (<b>without</b> trailing '*' character!)
-   *
-   * @return Resulting {@link Query} built for the term
-   * @throws ParseException
-   */
-  protected Query getPrefixQuery(String field, String termStr) throws ParseException {
-    // get Analyzer from superclass and tokenize the term
-    TokenStream source = getAnalyzer().tokenStream(field, new StringReader(termStr));
-    List tlist = new ArrayList();
-    TermAttribute termAtt = (TermAttribute) source.addAttribute(TermAttribute.class);
-    
-    while (true) {
-      try {
-        if (!source.incrementToken()) break;
-      } catch (IOException e) {
-        break;
-      }
-      tlist.add(termAtt.term());
-    }
-
-    try {
-      source.close();
-    } catch (IOException e) {
-      // ignore
-    }
-
-    if (tlist.size() == 1) {
-      return super.getPrefixQuery(field, (String) tlist.get(0));
-    } else {
-      /* this means that the analyzer used either added or consumed
-       * (common for a stemmer) tokens, and we can't build a PrefixQuery */
-      throw new ParseException("Cannot build PrefixQuery with analyzer "
-          + getAnalyzer().getClass()
-          + (tlist.size() > 1 ? " - token(s) added" : " - token consumed"));
-    }
-  }
-
-  /**
-   * Called when parser parses an input term token that has the fuzzy suffix (~) appended.
-   * <p>
-   * Depending on analyzer and settings, a fuzzy term may (most probably will)
-   * be lower-cased automatically. It <b>will</b> go through the default Analyzer.
-   * <p>
-   * Overrides super class, by passing terms through analyzer.
-   *
-   * @param field Name of the field query will use.
-   * @param termStr Term token to use for building term for the query
-   *
-   * @return Resulting {@link Query} built for the term
-   * @exception ParseException
-   */
-  protected Query getFuzzyQuery(String field, String termStr, float minSimilarity)
-      throws ParseException {
-    // get Analyzer from superclass and tokenize the term
-    TokenStream source = getAnalyzer().tokenStream(field, new StringReader(termStr));
-    TermAttribute termAtt = (TermAttribute) source.addAttribute(TermAttribute.class);
-    String nextToken = null;
-    boolean multipleTokens = false;
-    
-    try {
-      if (source.incrementToken()) {
-        nextToken = termAtt.term();
-      }
-      multipleTokens = source.incrementToken();
-    } catch (IOException e) {
-      nextToken = null;
-    }
-
-    try {
-      source.close();
-    } catch (IOException e) {
-      // ignore
-    }
-
-    if (multipleTokens) {
-      throw new ParseException("Cannot build FuzzyQuery with analyzer " + getAnalyzer().getClass()
-          + " - tokens were added");
-    }
-
-    return (nextToken == null) ? null : super.getFuzzyQuery(field, nextToken, minSimilarity);
-  }
-
-  /**
-   * Overrides super class, by passing terms through analyzer.
-   * @exception ParseException
-   */
-  protected Query getRangeQuery(String field, String part1, String part2, boolean inclusive)
-      throws ParseException {
-    // get Analyzer from superclass and tokenize the terms
-    TokenStream source = getAnalyzer().tokenStream(field, new StringReader(part1));
-    TermAttribute termAtt = (TermAttribute) source.addAttribute(TermAttribute.class);
-    boolean multipleTokens = false;
-
-    // part1
-    try {
-      if (source.incrementToken()) {
-        part1 = termAtt.term();
-      }
-      multipleTokens = source.incrementToken();
-    } catch (IOException e) {
-      // ignore
-    }
-    try {
-      source.close();
-    } catch (IOException e) {
-      // ignore
-    }
-    if (multipleTokens) {
-      throw new ParseException("Cannot build RangeQuery with analyzer " + getAnalyzer().getClass()
-          + " - tokens were added to part1");
-    }
-
-    // part2
-    source = getAnalyzer().tokenStream(field, new StringReader(part2));
-    termAtt = (TermAttribute) source.addAttribute(TermAttribute.class);
-    
-    try {
-      if (source.incrementToken()) {
-        part2 = termAtt.term();
-      }
-      multipleTokens = source.incrementToken();
-    } catch (IOException e) {
-      // ignore
-    }
-    try {
-      source.close();
-    } catch (IOException e) {
-      // ignore
-    }
-    if (multipleTokens) {
-      throw new ParseException("Cannot build RangeQuery with analyzer " + getAnalyzer().getClass()
-          + " - tokens were added to part2");
-    }
-    return super.getRangeQuery(field, part1, part2, inclusive);
-  }
-
-}
diff --git a/contrib/miscellaneous/src/java/org/apache/lucene/queryParser/complexPhrase/ComplexPhraseQueryParser.java b/contrib/miscellaneous/src/java/org/apache/lucene/queryParser/complexPhrase/ComplexPhraseQueryParser.java
deleted file mode 100644
index 9db2b2f..0000000
--- a/contrib/miscellaneous/src/java/org/apache/lucene/queryParser/complexPhrase/ComplexPhraseQueryParser.java
+++ /dev/null
@@ -1,389 +0,0 @@
-package org.apache.lucene.queryParser.complexPhrase;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.queryParser.ParseException;
-import org.apache.lucene.queryParser.QueryParser;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.MultiTermQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.TermRangeQuery;
-import org.apache.lucene.search.spans.SpanNearQuery;
-import org.apache.lucene.search.spans.SpanNotQuery;
-import org.apache.lucene.search.spans.SpanOrQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.apache.lucene.search.spans.SpanTermQuery;
-
-/**
- * QueryParser which permits complex phrase query syntax e.g. "(john jon
- * jonathan~) peters*"
- * 
- * Performs potentially multiple passes over Query text to parse any nested
- * logic in PhraseQueries. - First pass takes any PhraseQuery content between
- * quotes and stores for subsequent pass. All other query content is parsed as
- * normal - Second pass parses any stored PhraseQuery content, checking all
- * embedded clauses are referring to the same field and therefore can be
- * rewritten as Span queries. All PhraseQuery clauses are expressed as
- * ComplexPhraseQuery objects
- * 
- * This could arguably be done in one pass using a new QueryParser but here I am
- * working within the constraints of the existing parser as a base class. This
- * currently simply feeds all phrase content through an analyzer to select
- * phrase terms - any "special" syntax such as * ~ * etc are not given special
- * status
- * 
- * 
- */
-public class ComplexPhraseQueryParser extends QueryParser {
-  private ArrayList/*<ComplexPhraseQuery>*/complexPhrases = null;
-
-  private boolean isPass2ResolvingPhrases;
-
-  private ComplexPhraseQuery currentPhraseQuery = null;
-
-  public ComplexPhraseQueryParser(String f, Analyzer a) {
-    super(f, a);
-  }
-
-  protected Query getFieldQuery(String field, String queryText, int slop) {
-    ComplexPhraseQuery cpq = new ComplexPhraseQuery(field, queryText, slop);
-    complexPhrases.add(cpq); // add to list of phrases to be parsed once
-    // we
-    // are through with this pass
-    return cpq;
-  }
-
-  public Query parse(String query) throws ParseException {
-    if (isPass2ResolvingPhrases) {
-      MultiTermQuery.RewriteMethod oldMethod = getMultiTermRewriteMethod();
-      try {
-        // Temporarily force BooleanQuery rewrite so that Parser will
-        // generate visible
-        // collection of terms which we can convert into SpanQueries.
-        // ConstantScoreRewrite mode produces an
-        // opaque ConstantScoreQuery object which cannot be interrogated for
-        // terms in the same way a BooleanQuery can.
-        // QueryParser is not guaranteed threadsafe anyway so this temporary
-        // state change should not
-        // present an issue
-        setMultiTermRewriteMethod(MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE);
-        return super.parse(query);
-      } finally {
-        setMultiTermRewriteMethod(oldMethod);
-      }
-    }
-
-    // First pass - parse the top-level query recording any PhraseQuerys
-    // which will need to be resolved
-    complexPhrases = new ArrayList/*<ComplexPhraseQuery>*/();
-    Query q = super.parse(query);
-
-    // Perform second pass, using this QueryParser to parse any nested
-    // PhraseQueries with different
-    // set of syntax restrictions (i.e. all fields must be same)
-    isPass2ResolvingPhrases = true;
-    try {
-      for (Iterator iterator = complexPhrases.iterator(); iterator.hasNext();) {
-        currentPhraseQuery = (ComplexPhraseQuery) iterator.next();
-        // in each phrase, now parse the contents between quotes as a
-        // separate parse operation
-        currentPhraseQuery.parsePhraseElements(this);
-      }
-    } finally {
-      isPass2ResolvingPhrases = false;
-    }
-    return q;
-  }
-
-  // There is No "getTermQuery throws ParseException" method to override so
-  // unfortunately need
-  // to throw a runtime exception here if a term for another field is embedded
-  // in phrase query
-  protected Query newTermQuery(Term term) {
-    if (isPass2ResolvingPhrases) {
-      try {
-        checkPhraseClauseIsForSameField(term.field());
-      } catch (ParseException pe) {
-        throw new RuntimeException("Error parsing complex phrase", pe);
-      }
-    }
-    return super.newTermQuery(term);
-  }
-
-  // Helper method used to report on any clauses that appear in query syntax
-  private void checkPhraseClauseIsForSameField(String field)
-      throws ParseException {
-    if (!field.equals(currentPhraseQuery.field)) {
-      throw new ParseException("Cannot have clause for field \"" + field
-          + "\" nested in phrase " + " for field \"" + currentPhraseQuery.field
-          + "\"");
-    }
-  }
-
-  protected Query getWildcardQuery(String field, String termStr)
-      throws ParseException {
-    if (isPass2ResolvingPhrases) {
-      checkPhraseClauseIsForSameField(field);
-    }
-    return super.getWildcardQuery(field, termStr);
-  }
-
-  protected Query getRangeQuery(String field, String part1, String part2,
-      boolean inclusive) throws ParseException {
-    if (isPass2ResolvingPhrases) {
-      checkPhraseClauseIsForSameField(field);
-    }
-    return super.getRangeQuery(field, part1, part2, inclusive);
-  }
-
-  protected Query newRangeQuery(String field, String part1, String part2,
-      boolean inclusive) {
-    if (isPass2ResolvingPhrases) {
-      // Must use old-style RangeQuery in order to produce a BooleanQuery
-      // that can be turned into SpanOr clause
-      TermRangeQuery rangeQuery = new TermRangeQuery(field, part1, part2, inclusive, inclusive,
-          getRangeCollator());
-      rangeQuery.setRewriteMethod(MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE);
-      return rangeQuery;
-    }
-    return super.newRangeQuery(field, part1, part2, inclusive);
-  }
-
-  protected Query getFuzzyQuery(String field, String termStr,
-      float minSimilarity) throws ParseException {
-    if (isPass2ResolvingPhrases) {
-      checkPhraseClauseIsForSameField(field);
-    }
-    return super.getFuzzyQuery(field, termStr, minSimilarity);
-  }
-
-  /*
-   * Used to handle the query content in between quotes and produced Span-based
-   * interpretations of the clauses.
-   */
-  static class ComplexPhraseQuery extends Query {
-
-    String field;
-
-    String phrasedQueryStringContents;
-
-    int slopFactor;
-
-    private Query contents;
-
-    public ComplexPhraseQuery(String field, String phrasedQueryStringContents,
-        int slopFactor) {
-      super();
-      this.field = field;
-      this.phrasedQueryStringContents = phrasedQueryStringContents;
-      this.slopFactor = slopFactor;
-    }
-
-    // Called by ComplexPhraseQueryParser for each phrase after the main
-    // parse
-    // thread is through
-    protected void parsePhraseElements(QueryParser qp) throws ParseException {
-      // TODO ensure that field-sensitivity is preserved ie the query
-      // string below is parsed as
-      // field+":("+phrasedQueryStringContents+")"
-      // but this will need code in rewrite to unwrap the first layer of
-      // boolean query
-      contents = qp.parse(phrasedQueryStringContents);
-    }
-
-    public Query rewrite(IndexReader reader) throws IOException {
-      // ArrayList spanClauses = new ArrayList();
-      if (contents instanceof TermQuery) {
-        return contents;
-      }
-      // Build a sequence of Span clauses arranged in a SpanNear - child
-      // clauses can be complex
-      // Booleans e.g. nots and ors etc
-      int numNegatives = 0;
-      if (!(contents instanceof BooleanQuery)) {
-        throw new IllegalArgumentException("Unknown query type \""
-            + contents.getClass().getName()
-            + "\" found in phrase query string \"" + phrasedQueryStringContents
-            + "\"");
-      }
-      BooleanQuery bq = (BooleanQuery) contents;
-      BooleanClause[] bclauses = bq.getClauses();
-      SpanQuery[] allSpanClauses = new SpanQuery[bclauses.length];
-      // For all clauses e.g. one* two~
-      for (int i = 0; i < bclauses.length; i++) {
-        // HashSet bclauseterms=new HashSet();
-        Query qc = bclauses[i].getQuery();
-        // Rewrite this clause e.g one* becomes (one OR onerous)
-        qc = qc.rewrite(reader);
-        if (bclauses[i].getOccur().equals(BooleanClause.Occur.MUST_NOT)) {
-          numNegatives++;
-        }
-
-        if (qc instanceof BooleanQuery) {
-          ArrayList sc = new ArrayList();
-          addComplexPhraseClause(sc, (BooleanQuery) qc);
-          if (sc.size() > 0) {
-            allSpanClauses[i] = (SpanQuery) sc.get(0);
-          } else {
-            // Insert fake term e.g. phrase query was for "Fred Smithe*" and
-            // there were no "Smithe*" terms - need to
-            // prevent match on just "Fred".
-            allSpanClauses[i] = new SpanTermQuery(new Term(field,
-                "Dummy clause because no terms found - must match nothing"));
-          }
-        } else {
-          if (qc instanceof TermQuery) {
-            TermQuery tq = (TermQuery) qc;
-            allSpanClauses[i] = new SpanTermQuery(tq.getTerm());
-          } else {
-            throw new IllegalArgumentException("Unknown query type \""
-                + qc.getClass().getName()
-                + "\" found in phrase query string \""
-                + phrasedQueryStringContents + "\"");
-          }
-
-        }
-      }
-      if (numNegatives == 0) {
-        // The simple case - no negative elements in phrase
-        return new SpanNearQuery(allSpanClauses, slopFactor, true);
-      }
-      // Complex case - we have mixed positives and negatives in the
-      // sequence.
-      // Need to return a SpanNotQuery
-      ArrayList positiveClauses = new ArrayList();
-      for (int j = 0; j < allSpanClauses.length; j++) {
-        if (!bclauses[j].getOccur().equals(BooleanClause.Occur.MUST_NOT)) {
-          positiveClauses.add(allSpanClauses[j]);
-        }
-      }
-
-      SpanQuery[] includeClauses = (SpanQuery[]) positiveClauses
-          .toArray(new SpanQuery[positiveClauses.size()]);
-
-      SpanQuery include = null;
-      if (includeClauses.length == 1) {
-        include = includeClauses[0]; // only one positive clause
-      } else {
-        // need to increase slop factor based on gaps introduced by
-        // negatives
-        include = new SpanNearQuery(includeClauses, slopFactor + numNegatives,
-            true);
-      }
-      // Use sequence of positive and negative values as the exclude.
-      SpanNearQuery exclude = new SpanNearQuery(allSpanClauses, slopFactor,
-          true);
-      SpanNotQuery snot = new SpanNotQuery(include, exclude);
-      return snot;
-    }
-
-    private void addComplexPhraseClause(List spanClauses, BooleanQuery qc) {
-      ArrayList ors = new ArrayList();
-      ArrayList nots = new ArrayList();
-      BooleanClause[] bclauses = qc.getClauses();
-
-      // For all clauses e.g. one* two~
-      for (int i = 0; i < bclauses.length; i++) {
-        Query childQuery = bclauses[i].getQuery();
-
-        // select the list to which we will add these options
-        ArrayList chosenList = ors;
-        if (bclauses[i].getOccur() == BooleanClause.Occur.MUST_NOT) {
-          chosenList = nots;
-        }
-
-        if (childQuery instanceof TermQuery) {
-          TermQuery tq = (TermQuery) childQuery;
-          SpanTermQuery stq = new SpanTermQuery(tq.getTerm());
-          stq.setBoost(tq.getBoost());
-          chosenList.add(stq);
-        } else if (childQuery instanceof BooleanQuery) {
-          BooleanQuery cbq = (BooleanQuery) childQuery;
-          addComplexPhraseClause(chosenList, cbq);
-        } else {
-          // TODO alternatively could call extract terms here?
-          throw new IllegalArgumentException("Unknown query type:"
-              + childQuery.getClass().getName());
-        }
-      }
-      if (ors.size() == 0) {
-        return;
-      }
-      SpanOrQuery soq = new SpanOrQuery((SpanQuery[]) ors
-          .toArray(new SpanQuery[ors.size()]));
-      if (nots.size() == 0) {
-        spanClauses.add(soq);
-      } else {
-        SpanOrQuery snqs = new SpanOrQuery((SpanQuery[]) nots
-            .toArray(new SpanQuery[nots.size()]));
-        SpanNotQuery snq = new SpanNotQuery(soq, snqs);
-        spanClauses.add(snq);
-      }
-    }
-
-    public String toString(String field) {
-      return "\"" + phrasedQueryStringContents + "\"";
-    }
-
-    public int hashCode() {
-      final int prime = 31;
-      int result = 1;
-      result = prime * result + ((field == null) ? 0 : field.hashCode());
-      result = prime
-          * result
-          + ((phrasedQueryStringContents == null) ? 0
-              : phrasedQueryStringContents.hashCode());
-      result = prime * result + slopFactor;
-      return result;
-    }
-
-    public boolean equals(Object obj) {
-      if (this == obj)
-        return true;
-      if (obj == null)
-        return false;
-      if (getClass() != obj.getClass())
-        return false;
-      ComplexPhraseQuery other = (ComplexPhraseQuery) obj;
-      if (field == null) {
-        if (other.field != null)
-          return false;
-      } else if (!field.equals(other.field))
-        return false;
-      if (phrasedQueryStringContents == null) {
-        if (other.phrasedQueryStringContents != null)
-          return false;
-      } else if (!phrasedQueryStringContents
-          .equals(other.phrasedQueryStringContents))
-        return false;
-      if (slopFactor != other.slopFactor)
-        return false;
-      return true;
-    }
-  }
-}
diff --git a/contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/CharStream.java b/contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/CharStream.java
deleted file mode 100644
index bc3ca76..0000000
--- a/contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/CharStream.java
+++ /dev/null
@@ -1,110 +0,0 @@
-/* Generated By:JavaCC: Do not edit this line. CharStream.java Version 3.0 */
-package org.apache.lucene.queryParser.precedence;
-
-/**
- * This interface describes a character stream that maintains line and
- * column number positions of the characters.  It also has the capability
- * to backup the stream to some extent.  An implementation of this
- * interface is used in the TokenManager implementation generated by
- * JavaCCParser.
- *
- * All the methods except backup can be implemented in any fashion. backup
- * needs to be implemented correctly for the correct operation of the lexer.
- * Rest of the methods are all used to get information like line number,
- * column number and the String that constitutes a token and are not used
- * by the lexer. Hence their implementation won't affect the generated lexer's
- * operation.
- */
-
-public interface CharStream {
-
-  /**
-   * Returns the next character from the selected input.  The method
-   * of selecting the input is the responsibility of the class
-   * implementing this interface.  Can throw any java.io.IOException.
-   */
-  char readChar() throws java.io.IOException;
-
-  /**
-   * Returns the column position of the character last read.
-   * @deprecated 
-   * @see #getEndColumn
-   */
-  int getColumn();
-
-  /**
-   * Returns the line number of the character last read.
-   * @deprecated 
-   * @see #getEndLine
-   */
-  int getLine();
-
-  /**
-   * Returns the column number of the last character for current token (being
-   * matched after the last call to BeginTOken).
-   */
-  int getEndColumn();
-
-  /**
-   * Returns the line number of the last character for current token (being
-   * matched after the last call to BeginTOken).
-   */
-  int getEndLine();
-
-  /**
-   * Returns the column number of the first character for current token (being
-   * matched after the last call to BeginTOken).
-   */
-  int getBeginColumn();
-
-  /**
-   * Returns the line number of the first character for current token (being
-   * matched after the last call to BeginTOken).
-   */
-  int getBeginLine();
-
-  /**
-   * Backs up the input stream by amount steps. Lexer calls this method if it
-   * had already read some characters, but could not use them to match a
-   * (longer) token. So, they will be used again as the prefix of the next
-   * token and it is the implemetation's responsibility to do this right.
-   */
-  void backup(int amount);
-
-  /**
-   * Returns the next character that marks the beginning of the next token.
-   * All characters must remain in the buffer between two successive calls
-   * to this method to implement backup correctly.
-   */
-  char BeginToken() throws java.io.IOException;
-
-  /**
-   * Returns a string made up of characters from the marked token beginning 
-   * to the current buffer position. Implementations have the choice of returning
-   * anything that they want to. For example, for efficiency, one might decide
-   * to just return null, which is a valid implementation.
-   */
-  String GetImage();
-
-  /**
-   * Returns an array of characters that make up the suffix of length 'len' for
-   * the currently matched token. This is used to build up the matched string
-   * for use in actions in the case of MORE. A simple and inefficient
-   * implementation of this is as follows :
-   *
-   *   {
-   *      String t = GetImage();
-   *      return t.substring(t.length() - len, t.length()).toCharArray();
-   *   }
-   */
-  char[] GetSuffix(int len);
-
-  /**
-   * The lexer calls this function to indicate that it is done with the stream
-   * and hence implementations can free any resources held by this class.
-   * Again, the body of this function can be just empty and it will not
-   * affect the lexer's operation.
-   */
-  void Done();
-
-}
diff --git a/contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/FastCharStream.java b/contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/FastCharStream.java
deleted file mode 100644
index 5a7be60..0000000
--- a/contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/FastCharStream.java
+++ /dev/null
@@ -1,124 +0,0 @@
-// FastCharStream.java
-package org.apache.lucene.queryParser.precedence;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.queryParser.*;
-
-import java.io.*;
-
-/** An efficient implementation of JavaCC's CharStream interface.  <p>Note that
- * this does not do line-number counting, but instead keeps track of the
- * character position of the token in the input, as required by Lucene's {@link
- * org.apache.lucene.analysis.Token} API. */
-public final class FastCharStream implements CharStream {
-  char[] buffer = null;
-
-  int bufferLength = 0;				  // end of valid chars
-  int bufferPosition = 0;			  // next char to read
-
-  int tokenStart = 0;				  // offset in buffer
-  int bufferStart = 0;				  // position in file of buffer
-
-  Reader input;					  // source of chars
-
-  /** Constructs from a Reader. */
-  public FastCharStream(Reader r) {
-    input = r;
-  }
-
-  public final char readChar() throws IOException {
-    if (bufferPosition >= bufferLength)
-      refill();
-    return buffer[bufferPosition++];
-  }
-
-  private final void refill() throws IOException {
-    int newPosition = bufferLength - tokenStart;
-
-    if (tokenStart == 0) {			  // token won't fit in buffer
-      if (buffer == null) {			  // first time: alloc buffer
-	buffer = new char[2048];
-      } else if (bufferLength == buffer.length) { // grow buffer
-	char[] newBuffer = new char[buffer.length*2];
-	System.arraycopy(buffer, 0, newBuffer, 0, bufferLength);
-	buffer = newBuffer;
-      }
-    } else {					  // shift token to front
-      System.arraycopy(buffer, tokenStart, buffer, 0, newPosition);
-    }
-
-    bufferLength = newPosition;			  // update state
-    bufferPosition = newPosition;
-    bufferStart += tokenStart;
-    tokenStart = 0;
-
-    int charsRead =				  // fill space in buffer
-      input.read(buffer, newPosition, buffer.length-newPosition);
-    if (charsRead == -1)
-      throw new IOException("read past eof");
-    else
-      bufferLength += charsRead;
-  }
-
-  public final char BeginToken() throws IOException {
-    tokenStart = bufferPosition;
-    return readChar();
-  }
-
-  public final void backup(int amount) {
-    bufferPosition -= amount;
-  }
-
-  public final String GetImage() {
-    return new String(buffer, tokenStart, bufferPosition - tokenStart);
-  }
-
-  public final char[] GetSuffix(int len) {
-    char[] value = new char[len];
-    System.arraycopy(buffer, bufferPosition - len, value, 0, len);
-    return value;
-  }
-
-  public final void Done() {
-    try {
-      input.close();
-    } catch (IOException e) {
-      System.err.println("Caught: " + e + "; ignoring.");
-    }
-  }
-
-  public final int getColumn() {
-    return bufferStart + bufferPosition;
-  }
-  public final int getLine() {
-    return 1;
-  }
-  public final int getEndColumn() {
-    return bufferStart + bufferPosition;
-  }
-  public final int getEndLine() {
-    return 1;
-  }
-  public final int getBeginColumn() {
-    return bufferStart + tokenStart;
-  }
-  public final int getBeginLine() {
-    return 1;
-  }
-}
diff --git a/contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/ParseException.java b/contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/ParseException.java
deleted file mode 100644
index 0306496..0000000
--- a/contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/ParseException.java
+++ /dev/null
@@ -1,192 +0,0 @@
-/* Generated By:JavaCC: Do not edit this line. ParseException.java Version 3.0 */
-package org.apache.lucene.queryParser.precedence;
-
-/**
- * This exception is thrown when parse errors are encountered.
- * You can explicitly create objects of this exception type by
- * calling the method generateParseException in the generated
- * parser.
- *
- * You can modify this class to customize your error reporting
- * mechanisms so long as you retain the public fields.
- */
-public class ParseException extends Exception {
-
-  /**
-   * This constructor is used by the method "generateParseException"
-   * in the generated parser.  Calling this constructor generates
-   * a new object of this type with the fields "currentToken",
-   * "expectedTokenSequences", and "tokenImage" set.  The boolean
-   * flag "specialConstructor" is also set to true to indicate that
-   * this constructor was used to create this object.
-   * This constructor calls its super class with the empty string
-   * to force the "toString" method of parent class "Throwable" to
-   * print the error message in the form:
-   *     ParseException: <result of getMessage>
-   */
-  public ParseException(Token currentTokenVal,
-                        int[][] expectedTokenSequencesVal,
-                        String[] tokenImageVal
-                       )
-  {
-    super("");
-    specialConstructor = true;
-    currentToken = currentTokenVal;
-    expectedTokenSequences = expectedTokenSequencesVal;
-    tokenImage = tokenImageVal;
-  }
-
-  /**
-   * The following constructors are for use by you for whatever
-   * purpose you can think of.  Constructing the exception in this
-   * manner makes the exception behave in the normal way - i.e., as
-   * documented in the class "Throwable".  The fields "errorToken",
-   * "expectedTokenSequences", and "tokenImage" do not contain
-   * relevant information.  The JavaCC generated code does not use
-   * these constructors.
-   */
-
-  public ParseException() {
-    super();
-    specialConstructor = false;
-  }
-
-  public ParseException(String message) {
-    super(message);
-    specialConstructor = false;
-  }
-
-  /**
-   * This variable determines which constructor was used to create
-   * this object and thereby affects the semantics of the
-   * "getMessage" method (see below).
-   */
-  protected boolean specialConstructor;
-
-  /**
-   * This is the last token that has been consumed successfully.  If
-   * this object has been created due to a parse error, the token
-   * followng this token will (therefore) be the first error token.
-   */
-  public Token currentToken;
-
-  /**
-   * Each entry in this array is an array of integers.  Each array
-   * of integers represents a sequence of tokens (by their ordinal
-   * values) that is expected at this point of the parse.
-   */
-  public int[][] expectedTokenSequences;
-
-  /**
-   * This is a reference to the "tokenImage" array of the generated
-   * parser within which the parse error occurred.  This array is
-   * defined in the generated ...Constants interface.
-   */
-  public String[] tokenImage;
-
-  /**
-   * This method has the standard behavior when this object has been
-   * created using the standard constructors.  Otherwise, it uses
-   * "currentToken" and "expectedTokenSequences" to generate a parse
-   * error message and returns it.  If this object has been created
-   * due to a parse error, and you do not catch it (it gets thrown
-   * from the parser), then this method is called during the printing
-   * of the final stack trace, and hence the correct error message
-   * gets displayed.
-   */
-  public String getMessage() {
-    if (!specialConstructor) {
-      return super.getMessage();
-    }
-    String expected = "";
-    int maxSize = 0;
-    for (int i = 0; i < expectedTokenSequences.length; i++) {
-      if (maxSize < expectedTokenSequences[i].length) {
-        maxSize = expectedTokenSequences[i].length;
-      }
-      for (int j = 0; j < expectedTokenSequences[i].length; j++) {
-        expected += tokenImage[expectedTokenSequences[i][j]] + " ";
-      }
-      if (expectedTokenSequences[i][expectedTokenSequences[i].length - 1] != 0) {
-        expected += "...";
-      }
-      expected += eol + "    ";
-    }
-    String retval = "Encountered \"";
-    Token tok = currentToken.next;
-    for (int i = 0; i < maxSize; i++) {
-      if (i != 0) retval += " ";
-      if (tok.kind == 0) {
-        retval += tokenImage[0];
-        break;
-      }
-      retval += add_escapes(tok.image);
-      tok = tok.next; 
-    }
-    retval += "\" at line " + currentToken.next.beginLine + ", column " + currentToken.next.beginColumn;
-    retval += "." + eol;
-    if (expectedTokenSequences.length == 1) {
-      retval += "Was expecting:" + eol + "    ";
-    } else {
-      retval += "Was expecting one of:" + eol + "    ";
-    }
-    retval += expected;
-    return retval;
-  }
-
-  /**
-   * The end of line string for this machine.
-   */
-  protected String eol = System.getProperty("line.separator", "\n");
- 
-  /**
-   * Used to convert raw characters to their escaped version
-   * when these raw version cannot be used as part of an ASCII
-   * string literal.
-   */
-  protected String add_escapes(String str) {
-      StringBuffer retval = new StringBuffer();
-      char ch;
-      for (int i = 0; i < str.length(); i++) {
-        switch (str.charAt(i))
-        {
-           case 0 :
-              continue;
-           case '\b':
-              retval.append("\\b");
-              continue;
-           case '\t':
-              retval.append("\\t");
-              continue;
-           case '\n':
-              retval.append("\\n");
-              continue;
-           case '\f':
-              retval.append("\\f");
-              continue;
-           case '\r':
-              retval.append("\\r");
-              continue;
-           case '\"':
-              retval.append("\\\"");
-              continue;
-           case '\'':
-              retval.append("\\\'");
-              continue;
-           case '\\':
-              retval.append("\\\\");
-              continue;
-           default:
-              if ((ch = str.charAt(i)) < 0x20 || ch > 0x7e) {
-                 String s = "0000" + Integer.toString(ch, 16);
-                 retval.append("\\u" + s.substring(s.length() - 4, s.length()));
-              } else {
-                 retval.append(ch);
-              }
-              continue;
-        }
-      }
-      return retval.toString();
-   }
-
-}
diff --git a/contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.java b/contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.java
deleted file mode 100644
index 2423c5d..0000000
--- a/contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.java
+++ /dev/null
@@ -1,1345 +0,0 @@
-/* Generated By:JavaCC: Do not edit this line. PrecedenceQueryParser.java */
-package org.apache.lucene.queryParser.precedence;
-
-import java.io.IOException;
-import java.io.StringReader;
-import java.text.DateFormat;
-import java.util.ArrayList;
-import java.util.Date;
-import java.util.List;
-import java.util.Locale;
-import java.util.Vector;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.document.DateTools;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.FuzzyQuery;
-import org.apache.lucene.search.MultiPhraseQuery;
-import org.apache.lucene.search.PhraseQuery;
-import org.apache.lucene.search.PrefixQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.RangeQuery;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.WildcardQuery;
-import org.apache.lucene.util.Parameter;
-
-/**
- * Experimental query parser variant designed to handle operator precedence
- * in a more sensible fashion than QueryParser.  There are still some
- * open issues with this parser. The following tests are currently failing
- * in TestPrecedenceQueryParser and are disabled to make this test pass:
- * <ul>
- * <li> testSimple
- * <li> testWildcard
- * <li> testPrecedence
- * </ul>
- *
- * This class is generated by JavaCC.  The only method that clients should need
- * to call is {@link #parse(String)}.
- *
- * The syntax for query strings is as follows:
- * A Query is a series of clauses.
- * A clause may be prefixed by:
- * <ul>
- * <li> a plus (<code>+</code>) or a minus (<code>-</code>) sign, indicating
- * that the clause is required or prohibited respectively; or
- * <li> a term followed by a colon, indicating the field to be searched.
- * This enables one to construct queries which search multiple fields.
- * </ul>
- *
- * A clause may be either:
- * <ul>
- * <li> a term, indicating all the documents that contain this term; or
- * <li> a nested query, enclosed in parentheses.  Note that this may be used
- * with a <code>+</code>/<code>-</code> prefix to require any of a set of
- * terms.
- * </ul>
- *
- * Thus, in BNF, the query grammar is:
- * <pre>
- *   Query  ::= ( Clause )*
- *   Clause ::= ["+", "-"] [&lt;TERM&gt; ":"] ( &lt;TERM&gt; | "(" Query ")" )
- * </pre>
- *
- * <p>
- * Examples of appropriately formatted queries can be found in the <a
- * href="../../../../../../../queryparsersyntax.html">query syntax
- * documentation</a>.
- * </p>
- *
- */
-
-public class PrecedenceQueryParser implements PrecedenceQueryParserConstants {
-
-  private static final int CONJ_NONE   = 0;
-  private static final int CONJ_AND    = 1;
-  private static final int CONJ_OR     = 2;
-
-  private static final int MOD_NONE    = 0;
-  private static final int MOD_NOT     = 10;
-  private static final int MOD_REQ     = 11;
-
-  // make it possible to call setDefaultOperator() without accessing
-  // the nested class:
-  public static final Operator AND_OPERATOR = Operator.AND;
-  public static final Operator OR_OPERATOR = Operator.OR;
-
-  /** The actual operator that parser uses to combine query terms */
-  private Operator operator = OR_OPERATOR;
-
-  boolean lowercaseExpandedTerms = true;
-
-  Analyzer analyzer;
-  String field;
-  int phraseSlop = 0;
-  float fuzzyMinSim = FuzzyQuery.defaultMinSimilarity;
-  int fuzzyPrefixLength = FuzzyQuery.defaultPrefixLength;
-  Locale locale = Locale.getDefault();
-
-  static final class Operator extends Parameter {
-    private Operator(String name) {
-      super(name);
-    }
-    static final Operator OR = new Operator("OR");
-    static final Operator AND = new Operator("AND");
-  }
-
-  /** Constructs a query parser.
-   *  @param f  the default field for query terms.
-   *  @param a   used to find terms in the query text.
-   */
-  public PrecedenceQueryParser(String f, Analyzer a) {
-    this(new FastCharStream(new StringReader("")));
-    analyzer = a;
-    field = f;
-  }
-
-  /** Parses a query string, returning a {@link org.apache.lucene.search.Query}.
-   *  @param expression  the query string to be parsed.
-   *  @throws ParseException if the parsing fails
-   */
-  public Query parse(String expression) throws ParseException {
-    // optimize empty query to be empty BooleanQuery
-    if (expression == null || expression.trim().length() == 0) {
-      return new BooleanQuery();
-    }
-
-    ReInit(new FastCharStream(new StringReader(expression)));
-    try {
-      Query query = Query(field);
-      return (query != null) ? query : new BooleanQuery();
-    }
-    catch (TokenMgrError tme) {
-      throw new ParseException(tme.getMessage());
-    }
-    catch (BooleanQuery.TooManyClauses tmc) {
-      throw new ParseException("Too many boolean clauses");
-    }
-  }
-
-   /**
-   * @return Returns the analyzer.
-   */
-  public Analyzer getAnalyzer() {
-    return analyzer;
-  }
-
-  /**
-   * @return Returns the field.
-   */
-  public String getField() {
-    return field;
-  }
-
-   /**
-   * Get the minimal similarity for fuzzy queries.
-   */
-  public float getFuzzyMinSim() {
-      return fuzzyMinSim;
-  }
-
-  /**
-   * Set the minimum similarity for fuzzy queries.
-   * Default is 0.5f.
-   */
-  public void setFuzzyMinSim(float fuzzyMinSim) {
-      this.fuzzyMinSim = fuzzyMinSim;
-  }
-
-   /**
-   * Get the prefix length for fuzzy queries. 
-   * @return Returns the fuzzyPrefixLength.
-   */
-  public int getFuzzyPrefixLength() {
-    return fuzzyPrefixLength;
-  }
-
-  /**
-   * Set the prefix length for fuzzy queries. Default is 0.
-   * @param fuzzyPrefixLength The fuzzyPrefixLength to set.
-   */
-  public void setFuzzyPrefixLength(int fuzzyPrefixLength) {
-    this.fuzzyPrefixLength = fuzzyPrefixLength;
-  }
-
-  /**
-   * Sets the default slop for phrases.  If zero, then exact phrase matches
-   * are required.  Default value is zero.
-   */
-  public void setPhraseSlop(int phraseSlop) {
-    this.phraseSlop = phraseSlop;
-  }
-
-  /**
-   * Gets the default slop for phrases.
-   */
-  public int getPhraseSlop() {
-    return phraseSlop;
-  }
-
-  /**
-   * Sets the boolean operator of the QueryParser.
-   * In default mode (<code>OR_OPERATOR</code>) terms without any modifiers
-   * are considered optional: for example <code>capital of Hungary</code> is equal to
-   * <code>capital OR of OR Hungary</code>.<br/>
-   * In <code>AND_OPERATOR</code> mode terms are considered to be in conjuction: the
-   * above mentioned query is parsed as <code>capital AND of AND Hungary</code>
-   */
-  public void setDefaultOperator(Operator op) {
-    this.operator = op;
-  }
-
-  /**
-   * Gets implicit operator setting, which will be either AND_OPERATOR
-   * or OR_OPERATOR.
-   */
-  public Operator getDefaultOperator() {
-    return operator;
-  }
-
-  /**
-   * Whether terms of wildcard, prefix, fuzzy and range queries are to be automatically
-   * lower-cased or not.  Default is <code>true</code>.
-   */
-  public void setLowercaseExpandedTerms(boolean lowercaseExpandedTerms) {
-    this.lowercaseExpandedTerms = lowercaseExpandedTerms;
-  }
-
-  /**
-   * @see #setLowercaseExpandedTerms(boolean)
-   */
-  public boolean getLowercaseExpandedTerms() {
-    return lowercaseExpandedTerms;
-  }
-
-  /**
-   * Set locale used by date range parsing.
-   */
-  public void setLocale(Locale locale) {
-    this.locale = locale;
-  }
-
-  /**
-   * Returns current locale, allowing access by subclasses.
-   */
-  public Locale getLocale() {
-    return locale;
-  }
-
-  /**
-   * @deprecated use {@link #addClause(List, int, int, Query)} instead.
-   */
-  protected void addClause(Vector clauses, int conj, int modifier, Query q) {
-    addClause((List) clauses, conj, modifier, q);
-  }
-
-  protected void addClause(List clauses, int conj, int modifier, Query q) {
-    boolean required, prohibited;
-
-    // If this term is introduced by AND, make the preceding term required,
-    // unless it's already prohibited
-    if (clauses.size() > 0 && conj == CONJ_AND) {
-      BooleanClause c = (BooleanClause) clauses.get(clauses.size()-1);
-      if (!c.isProhibited())
-        c.setOccur(BooleanClause.Occur.MUST);
-    }
-
-    if (clauses.size() > 0 && operator == AND_OPERATOR && conj == CONJ_OR) {
-      // If this term is introduced by OR, make the preceding term optional,
-      // unless it's prohibited (that means we leave -a OR b but +a OR b-->a OR b)
-      // notice if the input is a OR b, first term is parsed as required; without
-      // this modification a OR b would parsed as +a OR b
-      BooleanClause c = (BooleanClause) clauses.get(clauses.size()-1);
-      if (!c.isProhibited())
-        c.setOccur(BooleanClause.Occur.SHOULD);
-    }
-
-    // We might have been passed a null query; the term might have been
-    // filtered away by the analyzer.
-    if (q == null)
-      return;
-
-    if (operator == OR_OPERATOR) {
-      // We set REQUIRED if we're introduced by AND or +; PROHIBITED if
-      // introduced by NOT or -; make sure not to set both.
-      prohibited = (modifier == MOD_NOT);
-      required = (modifier == MOD_REQ);
-      if (conj == CONJ_AND && !prohibited) {
-        required = true;
-      }
-    } else {
-      // We set PROHIBITED if we're introduced by NOT or -; We set REQUIRED
-      // if not PROHIBITED and not introduced by OR
-      prohibited = (modifier == MOD_NOT);
-      required   = (!prohibited && conj != CONJ_OR);
-    }
-    if (required && !prohibited)
-      clauses.add(new BooleanClause(q, BooleanClause.Occur.MUST));
-    else if (!required && !prohibited)
-      clauses.add(new BooleanClause(q, BooleanClause.Occur.SHOULD));
-    else if (!required && prohibited)
-      clauses.add(new BooleanClause(q, BooleanClause.Occur.MUST_NOT));
-    else
-      throw new RuntimeException("Clause cannot be both required and prohibited");
-  }
-
-  /**
-   * @exception ParseException throw in overridden method to disallow
-   */
-  protected Query getFieldQuery(String field, String queryText)  throws ParseException {
-    // Use the analyzer to get all the tokens, and then build a TermQuery,
-    // PhraseQuery, or nothing based on the term count
-
-    TokenStream source = analyzer.tokenStream(field, new StringReader(queryText));
-    List list = new ArrayList();
-    final org.apache.lucene.analysis.Token reusableToken = new org.apache.lucene.analysis.Token();
-    org.apache.lucene.analysis.Token nextToken;
-    int positionCount = 0;
-    boolean severalTokensAtSamePosition = false;
-
-    while (true) {
-      try {
-        nextToken = source.next(reusableToken);
-      }
-      catch (IOException e) {
-        nextToken = null;
-      }
-      if (nextToken == null)
-        break;
-      list.add(nextToken.clone());
-      if (nextToken.getPositionIncrement() == 1)
-        positionCount++;
-      else
-        severalTokensAtSamePosition = true;
-    }
-    try {
-      source.close();
-    }
-    catch (IOException e) {
-      // ignore
-    }
-
-    if (list.size() == 0)
-      return null;
-    else if (list.size() == 1) {
-      nextToken = (org.apache.lucene.analysis.Token) list.get(0);
-      return new TermQuery(new Term(field, nextToken.term()));
-    } else {
-      if (severalTokensAtSamePosition) {
-        if (positionCount == 1) {
-          // no phrase query:
-          BooleanQuery q = new BooleanQuery();
-          for (int i = 0; i < list.size(); i++) {
-            nextToken = (org.apache.lucene.analysis.Token) list.get(i);
-            TermQuery currentQuery = new TermQuery(
-                new Term(field, nextToken.term()));
-            q.add(currentQuery, BooleanClause.Occur.SHOULD);
-          }
-          return q;
-        }
-        else {
-          // phrase query:
-          MultiPhraseQuery mpq = new MultiPhraseQuery();
-          List multiTerms = new ArrayList();
-          for (int i = 0; i < list.size(); i++) {
-            nextToken = (org.apache.lucene.analysis.Token) list.get(i);
-            if (nextToken.getPositionIncrement() == 1 && multiTerms.size() > 0) {
-              mpq.add((Term[])multiTerms.toArray(new Term[0]));
-              multiTerms.clear();
-            }
-            multiTerms.add(new Term(field, nextToken.term()));
-          }
-          mpq.add((Term[])multiTerms.toArray(new Term[0]));
-          return mpq;
-        }
-      }
-      else {
-        PhraseQuery q = new PhraseQuery();
-        q.setSlop(phraseSlop);
-        for (int i = 0; i < list.size(); i++) {
-          q.add(new Term(field, ((org.apache.lucene.analysis.Token)
-              list.get(i)).term()));
-        }
-        return q;
-      }
-    }
-  }
-
-  /**
-   * Base implementation delegates to {@link #getFieldQuery(String,String)}.
-   * This method may be overridden, for example, to return
-   * a SpanNearQuery instead of a PhraseQuery.
-   *
-   * @exception ParseException throw in overridden method to disallow
-   */
-  protected Query getFieldQuery(String field, String queryText, int slop)
-        throws ParseException {
-    Query query = getFieldQuery(field, queryText);
-
-    if (query instanceof PhraseQuery) {
-      ((PhraseQuery) query).setSlop(slop);
-    }
-    if (query instanceof MultiPhraseQuery) {
-      ((MultiPhraseQuery) query).setSlop(slop);
-    }
-
-    return query;
-  }
-
-  /**
-   * @exception ParseException throw in overridden method to disallow
-   */
-  protected Query getRangeQuery(String field,
-                                String part1,
-                                String part2,
-                                boolean inclusive) throws ParseException
-  {
-    if (lowercaseExpandedTerms) {
-      part1 = part1.toLowerCase();
-      part2 = part2.toLowerCase();
-    }
-    try {
-      DateFormat df = DateFormat.getDateInstance(DateFormat.SHORT, locale);
-      df.setLenient(true);
-      Date d1 = df.parse(part1);
-      Date d2 = df.parse(part2);
-      part1 = DateTools.dateToString(d1, DateTools.Resolution.DAY);
-      part2 = DateTools.dateToString(d2, DateTools.Resolution.DAY);
-    }
-    catch (Exception e) { }
-
-    return new RangeQuery(new Term(field, part1),
-                          new Term(field, part2),
-                          inclusive);
-  }
-
-  /**
-   * Factory method for generating query, given a set of clauses.
-   * By default creates a boolean query composed of clauses passed in.
-   *
-   * Can be overridden by extending classes, to modify query being
-   * returned.
-   *
-   * @param clauses List that contains {@link BooleanClause} instances
-   *    to join.
-   *
-   * @return Resulting {@link Query} object.
-   * @exception ParseException throw in overridden method to disallow
-   * @deprecated use {@link #getBooleanQuery(List)} instead
-   */
-  protected Query getBooleanQuery(Vector clauses) throws ParseException
-  {
-    return getBooleanQuery((List) clauses, false);
-  }
-
-  /**
-   * Factory method for generating query, given a set of clauses.
-   * By default creates a boolean query composed of clauses passed in.
-   *
-   * Can be overridden by extending classes, to modify query being
-   * returned.
-   *
-   * @param clauses List that contains {@link BooleanClause} instances
-   *    to join.
-   *
-   * @return Resulting {@link Query} object.
-   * @exception ParseException throw in overridden method to disallow
-   */
-  protected Query getBooleanQuery(List clauses) throws ParseException
-  {
-    return getBooleanQuery(clauses, false);
-  }
-
-  /**
-   * Factory method for generating query, given a set of clauses.
-   * By default creates a boolean query composed of clauses passed in.
-   *
-   * Can be overridden by extending classes, to modify query being
-   * returned.
-   *
-   * @param clauses List that contains {@link BooleanClause} instances
-   *    to join.
-   * @param disableCoord true if coord scoring should be disabled.
-   *
-   * @return Resulting {@link Query} object.
-   * @exception ParseException throw in overridden method to disallow
-   * @deprecated use {@link #getBooleanQuery(List, boolean)} instead
-   */
-  protected Query getBooleanQuery(Vector clauses, boolean disableCoord)
-    throws ParseException
-  {
-    return getBooleanQuery((List) clauses, disableCoord);
-  }
-
-  /**
-   * Factory method for generating query, given a set of clauses.
-   * By default creates a boolean query composed of clauses passed in.
-   *
-   * Can be overridden by extending classes, to modify query being
-   * returned.
-   *
-   * @param clauses List that contains {@link BooleanClause} instances
-   *    to join.
-   * @param disableCoord true if coord scoring should be disabled.
-   *
-   * @return Resulting {@link Query} object.
-   * @exception ParseException throw in overridden method to disallow
-   */
-  protected Query getBooleanQuery(List clauses, boolean disableCoord)
-      throws ParseException {
-    if (clauses == null || clauses.size() == 0)
-      return null;
-
-    BooleanQuery query = new BooleanQuery(disableCoord);
-    for (int i = 0; i < clauses.size(); i++) {
-      query.add((BooleanClause)clauses.get(i));
-    }
-    return query;
-  }
-
-  /**
-   * Factory method for generating a query. Called when parser
-   * parses an input term token that contains one or more wildcard
-   * characters (? and *), but is not a prefix term token (one
-   * that has just a single * character at the end)
-   *<p>
-   * Depending on settings, prefix term may be lower-cased
-   * automatically. It will not go through the default Analyzer,
-   * however, since normal Analyzers are unlikely to work properly
-   * with wildcard templates.
-   *<p>
-   * Can be overridden by extending classes, to provide custom handling for
-   * wildcard queries, which may be necessary due to missing analyzer calls.
-   *
-   * @param field Name of the field query will use.
-   * @param termStr Term token that contains one or more wild card
-   *   characters (? or *), but is not simple prefix term
-   *
-   * @return Resulting {@link Query} built for the term
-   * @exception ParseException throw in overridden method to disallow
-   */
-  protected Query getWildcardQuery(String field, String termStr) throws ParseException
-  {
-    if (lowercaseExpandedTerms) {
-      termStr = termStr.toLowerCase();
-    }
-    Term t = new Term(field, termStr);
-    return new WildcardQuery(t);
-  }
-
-  /**
-   * Factory method for generating a query (similar to
-   * {@link #getWildcardQuery}). Called when parser parses an input term
-   * token that uses prefix notation; that is, contains a single '*' wildcard
-   * character as its last character. Since this is a special case
-   * of generic wildcard term, and such a query can be optimized easily,
-   * this usually results in a different query object.
-   *<p>
-   * Depending on settings, a prefix term may be lower-cased
-   * automatically. It will not go through the default Analyzer,
-   * however, since normal Analyzers are unlikely to work properly
-   * with wildcard templates.
-   *<p>
-   * Can be overridden by extending classes, to provide custom handling for
-   * wild card queries, which may be necessary due to missing analyzer calls.
-   *
-   * @param field Name of the field query will use.
-   * @param termStr Term token to use for building term for the query
-   *    (<b>without</b> trailing '*' character!)
-   *
-   * @return Resulting {@link Query} built for the term
-   * @exception ParseException throw in overridden method to disallow
-   */
-  protected Query getPrefixQuery(String field, String termStr) throws ParseException
-  {
-    if (lowercaseExpandedTerms) {
-      termStr = termStr.toLowerCase();
-    }
-    Term t = new Term(field, termStr);
-    return new PrefixQuery(t);
-  }
-
-   /**
-   * Factory method for generating a query (similar to
-   * {@link #getWildcardQuery}). Called when parser parses
-   * an input term token that has the fuzzy suffix (~) appended.
-   *
-   * @param field Name of the field query will use.
-   * @param termStr Term token to use for building term for the query
-   *
-   * @return Resulting {@link Query} built for the term
-   * @exception ParseException throw in overridden method to disallow
-   */
-  protected Query getFuzzyQuery(String field, String termStr, float minSimilarity) throws ParseException
-  {
-    if (lowercaseExpandedTerms) {
-      termStr = termStr.toLowerCase();
-    }
-    Term t = new Term(field, termStr);
-    return new FuzzyQuery(t, minSimilarity, fuzzyPrefixLength);
-  }
-
-  /**
-   * Returns a String where the escape char has been
-   * removed, or kept only once if there was a double escape.
-   */
-  private String discardEscapeChar(String input) {
-    char[] caSource = input.toCharArray();
-    char[] caDest = new char[caSource.length];
-    int j = 0;
-    for (int i = 0; i < caSource.length; i++) {
-      if ((caSource[i] != '\\') || (i > 0 && caSource[i-1] == '\\')) {
-        caDest[j++]=caSource[i];
-      }
-    }
-    return new String(caDest, 0, j);
-  }
-
-  /**
-   * Returns a String where those characters that QueryParser
-   * expects to be escaped are escaped by a preceding <code>\</code>.
-   */
-  public static String escape(String s) {
-    StringBuffer sb = new StringBuffer();
-    for (int i = 0; i < s.length(); i++) {
-      char c = s.charAt(i);
-      // NOTE: keep this in sync with _ESCAPED_CHAR below!
-      if (c == '\\' || c == '+' || c == '-' || c == '!' || c == '(' || c == ')' || c == ':'
-        || c == '^' || c == '[' || c == ']' || c == '\"' || c == '{' || c == '}' || c == '~'
-        || c == '*' || c == '?') {
-        sb.append('\\');
-      }
-      sb.append(c);
-    }
-    return sb.toString();
-  }
-
-  /**
-   * Command line tool to test QueryParser, using {@link org.apache.lucene.analysis.SimpleAnalyzer}.
-   * Usage:<br>
-   * <code>java org.apache.lucene.queryParser.QueryParser &lt;input&gt;</code>
-   */
-  public static void main(String[] args) throws Exception {
-    if (args.length == 0) {
-      System.out.println("Usage: java org.apache.lucene.queryParser.QueryParser <input>");
-      System.exit(0);
-    }
-    PrecedenceQueryParser qp = new PrecedenceQueryParser("field",
-                           new org.apache.lucene.analysis.SimpleAnalyzer());
-    Query q = qp.parse(args[0]);
-    System.out.println(q.toString("field"));
-  }
-
-// *   Query  ::= ( Clause )*
-// *   Clause ::= ["+", "-"] [<TERM> ":"] ( <TERM> | "(" Query ")" )
-  final public int Conjunction() throws ParseException {
-  int ret = CONJ_NONE;
-    switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-    case AND:
-    case OR:
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case AND:
-        jj_consume_token(AND);
-            ret = CONJ_AND;
-        break;
-      case OR:
-        jj_consume_token(OR);
-              ret = CONJ_OR;
-        break;
-      default:
-        jj_la1[0] = jj_gen;
-        jj_consume_token(-1);
-        throw new ParseException();
-      }
-      break;
-    default:
-      jj_la1[1] = jj_gen;
-      ;
-    }
-    {if (true) return ret;}
-    throw new Error("Missing return statement in function");
-  }
-
-  final public int Modifier() throws ParseException {
-  int ret = MOD_NONE;
-    switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-    case NOT:
-    case PLUS:
-    case MINUS:
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case PLUS:
-        jj_consume_token(PLUS);
-              ret = MOD_REQ;
-        break;
-      case MINUS:
-        jj_consume_token(MINUS);
-                 ret = MOD_NOT;
-        break;
-      case NOT:
-        jj_consume_token(NOT);
-               ret = MOD_NOT;
-        break;
-      default:
-        jj_la1[2] = jj_gen;
-        jj_consume_token(-1);
-        throw new ParseException();
-      }
-      break;
-    default:
-      jj_la1[3] = jj_gen;
-      ;
-    }
-    {if (true) return ret;}
-    throw new Error("Missing return statement in function");
-  }
-
-  final public Query Query(String field) throws ParseException {
-  List clauses = new ArrayList();
-  Query q, firstQuery=null;
-  boolean orPresent = false;
-  int modifier;
-    modifier = Modifier();
-    q = andExpression(field);
-    addClause(clauses, CONJ_NONE, modifier, q);
-    if (modifier == MOD_NONE)
-      firstQuery = q;
-    label_1:
-    while (true) {
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case OR:
-      case NOT:
-      case PLUS:
-      case MINUS:
-      case LPAREN:
-      case QUOTED:
-      case TERM:
-      case PREFIXTERM:
-      case WILDTERM:
-      case RANGEIN_START:
-      case RANGEEX_START:
-      case NUMBER:
-        ;
-        break;
-      default:
-        jj_la1[4] = jj_gen;
-        break label_1;
-      }
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case OR:
-        jj_consume_token(OR);
-            orPresent=true;
-        break;
-      default:
-        jj_la1[5] = jj_gen;
-        ;
-      }
-      modifier = Modifier();
-      q = andExpression(field);
-      addClause(clauses, orPresent ? CONJ_OR : CONJ_NONE, modifier, q);
-    }
-      if (clauses.size() == 1 && firstQuery != null)
-        {if (true) return firstQuery;}
-      else {
-        {if (true) return getBooleanQuery(clauses);}
-      }
-    throw new Error("Missing return statement in function");
-  }
-
-  final public Query andExpression(String field) throws ParseException {
-  List clauses = new ArrayList();
-  Query q, firstQuery=null;
-  int modifier;
-    q = Clause(field);
-    addClause(clauses, CONJ_NONE, MOD_NONE, q);
-    firstQuery = q;
-    label_2:
-    while (true) {
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case AND:
-        ;
-        break;
-      default:
-        jj_la1[6] = jj_gen;
-        break label_2;
-      }
-      jj_consume_token(AND);
-      modifier = Modifier();
-      q = Clause(field);
-      addClause(clauses, CONJ_AND, modifier, q);
-    }
-      if (clauses.size() == 1 && firstQuery != null)
-        {if (true) return firstQuery;}
-      else {
-        {if (true) return getBooleanQuery(clauses);}
-      }
-    throw new Error("Missing return statement in function");
-  }
-
-  final public Query Clause(String field) throws ParseException {
-  Query q;
-  Token fieldToken=null, boost=null;
-    if (jj_2_1(2)) {
-      fieldToken = jj_consume_token(TERM);
-      jj_consume_token(COLON);
-      field=discardEscapeChar(fieldToken.image);
-    } else {
-      ;
-    }
-    switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-    case QUOTED:
-    case TERM:
-    case PREFIXTERM:
-    case WILDTERM:
-    case RANGEIN_START:
-    case RANGEEX_START:
-    case NUMBER:
-      q = Term(field);
-      break;
-    case LPAREN:
-      jj_consume_token(LPAREN);
-      q = Query(field);
-      jj_consume_token(RPAREN);
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case CARAT:
-        jj_consume_token(CARAT);
-        boost = jj_consume_token(NUMBER);
-        break;
-      default:
-        jj_la1[7] = jj_gen;
-        ;
-      }
-      break;
-    default:
-      jj_la1[8] = jj_gen;
-      jj_consume_token(-1);
-      throw new ParseException();
-    }
-      if (boost != null) {
-        float f = (float)1.0;
-  try {
-    f = Float.valueOf(boost.image).floatValue();
-          q.setBoost(f);
-  } catch (Exception ignored) { }
-      }
-      {if (true) return q;}
-    throw new Error("Missing return statement in function");
-  }
-
-  final public Query Term(String field) throws ParseException {
-  Token term, boost=null, fuzzySlop=null, goop1, goop2;
-  boolean prefix = false;
-  boolean wildcard = false;
-  boolean fuzzy = false;
-  Query q;
-    switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-    case TERM:
-    case PREFIXTERM:
-    case WILDTERM:
-    case NUMBER:
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case TERM:
-        term = jj_consume_token(TERM);
-        break;
-      case PREFIXTERM:
-        term = jj_consume_token(PREFIXTERM);
-                             prefix=true;
-        break;
-      case WILDTERM:
-        term = jj_consume_token(WILDTERM);
-                           wildcard=true;
-        break;
-      case NUMBER:
-        term = jj_consume_token(NUMBER);
-        break;
-      default:
-        jj_la1[9] = jj_gen;
-        jj_consume_token(-1);
-        throw new ParseException();
-      }
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case FUZZY_SLOP:
-        fuzzySlop = jj_consume_token(FUZZY_SLOP);
-                                fuzzy=true;
-        break;
-      default:
-        jj_la1[10] = jj_gen;
-        ;
-      }
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case CARAT:
-        jj_consume_token(CARAT);
-        boost = jj_consume_token(NUMBER);
-        switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-        case FUZZY_SLOP:
-          fuzzySlop = jj_consume_token(FUZZY_SLOP);
-                                                         fuzzy=true;
-          break;
-        default:
-          jj_la1[11] = jj_gen;
-          ;
-        }
-        break;
-      default:
-        jj_la1[12] = jj_gen;
-        ;
-      }
-       String termImage=discardEscapeChar(term.image);
-       if (wildcard) {
-       q = getWildcardQuery(field, termImage);
-       } else if (prefix) {
-         q = getPrefixQuery(field,
-           discardEscapeChar(term.image.substring
-          (0, term.image.length()-1)));
-       } else if (fuzzy) {
-          float fms = fuzzyMinSim;
-          try {
-            fms = Float.valueOf(fuzzySlop.image.substring(1)).floatValue();
-          } catch (Exception ignored) { }
-         if(fms < 0.0f || fms > 1.0f){
-           {if (true) throw new ParseException("Minimum similarity for a FuzzyQuery has to be between 0.0f and 1.0f !");}
-         }
-         q = getFuzzyQuery(field, termImage, fms);
-       } else {
-         q = getFieldQuery(field, termImage);
-       }
-      break;
-    case RANGEIN_START:
-      jj_consume_token(RANGEIN_START);
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case RANGEIN_GOOP:
-        goop1 = jj_consume_token(RANGEIN_GOOP);
-        break;
-      case RANGEIN_QUOTED:
-        goop1 = jj_consume_token(RANGEIN_QUOTED);
-        break;
-      default:
-        jj_la1[13] = jj_gen;
-        jj_consume_token(-1);
-        throw new ParseException();
-      }
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case RANGEIN_TO:
-        jj_consume_token(RANGEIN_TO);
-        break;
-      default:
-        jj_la1[14] = jj_gen;
-        ;
-      }
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case RANGEIN_GOOP:
-        goop2 = jj_consume_token(RANGEIN_GOOP);
-        break;
-      case RANGEIN_QUOTED:
-        goop2 = jj_consume_token(RANGEIN_QUOTED);
-        break;
-      default:
-        jj_la1[15] = jj_gen;
-        jj_consume_token(-1);
-        throw new ParseException();
-      }
-      jj_consume_token(RANGEIN_END);
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case CARAT:
-        jj_consume_token(CARAT);
-        boost = jj_consume_token(NUMBER);
-        break;
-      default:
-        jj_la1[16] = jj_gen;
-        ;
-      }
-          if (goop1.kind == RANGEIN_QUOTED) {
-            goop1.image = goop1.image.substring(1, goop1.image.length()-1);
-          } else {
-            goop1.image = discardEscapeChar(goop1.image);
-          }
-          if (goop2.kind == RANGEIN_QUOTED) {
-            goop2.image = goop2.image.substring(1, goop2.image.length()-1);
-      } else {
-        goop2.image = discardEscapeChar(goop2.image);
-      }
-          q = getRangeQuery(field, goop1.image, goop2.image, true);
-      break;
-    case RANGEEX_START:
-      jj_consume_token(RANGEEX_START);
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case RANGEEX_GOOP:
-        goop1 = jj_consume_token(RANGEEX_GOOP);
-        break;
-      case RANGEEX_QUOTED:
-        goop1 = jj_consume_token(RANGEEX_QUOTED);
-        break;
-      default:
-        jj_la1[17] = jj_gen;
-        jj_consume_token(-1);
-        throw new ParseException();
-      }
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case RANGEEX_TO:
-        jj_consume_token(RANGEEX_TO);
-        break;
-      default:
-        jj_la1[18] = jj_gen;
-        ;
-      }
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case RANGEEX_GOOP:
-        goop2 = jj_consume_token(RANGEEX_GOOP);
-        break;
-      case RANGEEX_QUOTED:
-        goop2 = jj_consume_token(RANGEEX_QUOTED);
-        break;
-      default:
-        jj_la1[19] = jj_gen;
-        jj_consume_token(-1);
-        throw new ParseException();
-      }
-      jj_consume_token(RANGEEX_END);
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case CARAT:
-        jj_consume_token(CARAT);
-        boost = jj_consume_token(NUMBER);
-        break;
-      default:
-        jj_la1[20] = jj_gen;
-        ;
-      }
-          if (goop1.kind == RANGEEX_QUOTED) {
-            goop1.image = goop1.image.substring(1, goop1.image.length()-1);
-          } else {
-            goop1.image = discardEscapeChar(goop1.image);
-          }
-          if (goop2.kind == RANGEEX_QUOTED) {
-            goop2.image = goop2.image.substring(1, goop2.image.length()-1);
-      } else {
-        goop2.image = discardEscapeChar(goop2.image);
-      }
-
-          q = getRangeQuery(field, goop1.image, goop2.image, false);
-      break;
-    case QUOTED:
-      term = jj_consume_token(QUOTED);
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case FUZZY_SLOP:
-        fuzzySlop = jj_consume_token(FUZZY_SLOP);
-        break;
-      default:
-        jj_la1[21] = jj_gen;
-        ;
-      }
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case CARAT:
-        jj_consume_token(CARAT);
-        boost = jj_consume_token(NUMBER);
-        break;
-      default:
-        jj_la1[22] = jj_gen;
-        ;
-      }
-         int s = phraseSlop;
-
-         if (fuzzySlop != null) {
-           try {
-             s = Float.valueOf(fuzzySlop.image.substring(1)).intValue();
-           }
-           catch (Exception ignored) { }
-         }
-         q = getFieldQuery(field, term.image.substring(1, term.image.length()-1), s);
-      break;
-    default:
-      jj_la1[23] = jj_gen;
-      jj_consume_token(-1);
-      throw new ParseException();
-    }
-    if (boost != null) {
-      float f = (float) 1.0;
-      try {
-        f = Float.valueOf(boost.image).floatValue();
-      }
-      catch (Exception ignored) {
-    /* Should this be handled somehow? (defaults to "no boost", if
-     * boost number is invalid)
-     */
-      }
-
-      // avoid boosting null queries, such as those caused by stop words
-      if (q != null) {
-        q.setBoost(f);
-      }
-    }
-    {if (true) return q;}
-    throw new Error("Missing return statement in function");
-  }
-
-  final private boolean jj_2_1(int xla) {
-    jj_la = xla; jj_lastpos = jj_scanpos = token;
-    try { return !jj_3_1(); }
-    catch(LookaheadSuccess ls) { return true; }
-    finally { jj_save(0, xla); }
-  }
-
-  final private boolean jj_3_1() {
-    if (jj_scan_token(TERM)) return true;
-    if (jj_scan_token(COLON)) return true;
-    return false;
-  }
-
-  public PrecedenceQueryParserTokenManager token_source;
-  public Token token, jj_nt;
-  private int jj_ntk;
-  private Token jj_scanpos, jj_lastpos;
-  private int jj_la;
-  public boolean lookingAhead = false;
-  private boolean jj_semLA;
-  private int jj_gen;
-  final private int[] jj_la1 = new int[24];
-  static private int[] jj_la1_0;
-  static {
-      jj_la1_0();
-   }
-   private static void jj_la1_0() {
-      jj_la1_0 = new int[] {0x180,0x180,0xe00,0xe00,0xfb1f00,0x100,0x80,0x8000,0xfb1000,0x9a0000,0x40000,0x40000,0x8000,0xc000000,0x1000000,0xc000000,0x8000,0xc0000000,0x10000000,0xc0000000,0x8000,0x40000,0x8000,0xfb0000,};
-   }
-  final private JJCalls[] jj_2_rtns = new JJCalls[1];
-  private boolean jj_rescan = false;
-  private int jj_gc = 0;
-
-  public PrecedenceQueryParser(CharStream stream) {
-    token_source = new PrecedenceQueryParserTokenManager(stream);
-    token = new Token();
-    jj_ntk = -1;
-    jj_gen = 0;
-    for (int i = 0; i < 24; i++) jj_la1[i] = -1;
-    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
-  }
-
-  public void ReInit(CharStream stream) {
-    token_source.ReInit(stream);
-    token = new Token();
-    jj_ntk = -1;
-    jj_gen = 0;
-    for (int i = 0; i < 24; i++) jj_la1[i] = -1;
-    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
-  }
-
-  public PrecedenceQueryParser(PrecedenceQueryParserTokenManager tm) {
-    token_source = tm;
-    token = new Token();
-    jj_ntk = -1;
-    jj_gen = 0;
-    for (int i = 0; i < 24; i++) jj_la1[i] = -1;
-    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
-  }
-
-  public void ReInit(PrecedenceQueryParserTokenManager tm) {
-    token_source = tm;
-    token = new Token();
-    jj_ntk = -1;
-    jj_gen = 0;
-    for (int i = 0; i < 24; i++) jj_la1[i] = -1;
-    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
-  }
-
-  final private Token jj_consume_token(int kind) throws ParseException {
-    Token oldToken;
-    if ((oldToken = token).next != null) token = token.next;
-    else token = token.next = token_source.getNextToken();
-    jj_ntk = -1;
-    if (token.kind == kind) {
-      jj_gen++;
-      if (++jj_gc > 100) {
-        jj_gc = 0;
-        for (int i = 0; i < jj_2_rtns.length; i++) {
-          JJCalls c = jj_2_rtns[i];
-          while (c != null) {
-            if (c.gen < jj_gen) c.first = null;
-            c = c.next;
-          }
-        }
-      }
-      return token;
-    }
-    token = oldToken;
-    jj_kind = kind;
-    throw generateParseException();
-  }
-
-  static private final class LookaheadSuccess extends java.lang.Error { }
-  final private LookaheadSuccess jj_ls = new LookaheadSuccess();
-  final private boolean jj_scan_token(int kind) {
-    if (jj_scanpos == jj_lastpos) {
-      jj_la--;
-      if (jj_scanpos.next == null) {
-        jj_lastpos = jj_scanpos = jj_scanpos.next = token_source.getNextToken();
-      } else {
-        jj_lastpos = jj_scanpos = jj_scanpos.next;
-      }
-    } else {
-      jj_scanpos = jj_scanpos.next;
-    }
-    if (jj_rescan) {
-      int i = 0; Token tok = token;
-      while (tok != null && tok != jj_scanpos) { i++; tok = tok.next; }
-      if (tok != null) jj_add_error_token(kind, i);
-    }
-    if (jj_scanpos.kind != kind) return true;
-    if (jj_la == 0 && jj_scanpos == jj_lastpos) throw jj_ls;
-    return false;
-  }
-
-  final public Token getNextToken() {
-    if (token.next != null) token = token.next;
-    else token = token.next = token_source.getNextToken();
-    jj_ntk = -1;
-    jj_gen++;
-    return token;
-  }
-
-  final public Token getToken(int index) {
-    Token t = lookingAhead ? jj_scanpos : token;
-    for (int i = 0; i < index; i++) {
-      if (t.next != null) t = t.next;
-      else t = t.next = token_source.getNextToken();
-    }
-    return t;
-  }
-
-  final private int jj_ntk() {
-    if ((jj_nt=token.next) == null)
-      return (jj_ntk = (token.next=token_source.getNextToken()).kind);
-    else
-      return (jj_ntk = jj_nt.kind);
-  }
-
-  private java.util.Vector jj_expentries = new java.util.Vector();
-  private int[] jj_expentry;
-  private int jj_kind = -1;
-  private int[] jj_lasttokens = new int[100];
-  private int jj_endpos;
-
-  private void jj_add_error_token(int kind, int pos) {
-    if (pos >= 100) return;
-    if (pos == jj_endpos + 1) {
-      jj_lasttokens[jj_endpos++] = kind;
-    } else if (jj_endpos != 0) {
-      jj_expentry = new int[jj_endpos];
-      for (int i = 0; i < jj_endpos; i++) {
-        jj_expentry[i] = jj_lasttokens[i];
-      }
-      boolean exists = false;
-      for (java.util.Enumeration e = jj_expentries.elements(); e.hasMoreElements();) {
-        int[] oldentry = (int[])(e.nextElement());
-        if (oldentry.length == jj_expentry.length) {
-          exists = true;
-          for (int i = 0; i < jj_expentry.length; i++) {
-            if (oldentry[i] != jj_expentry[i]) {
-              exists = false;
-              break;
-            }
-          }
-          if (exists) break;
-        }
-      }
-      if (!exists) jj_expentries.addElement(jj_expentry);
-      if (pos != 0) jj_lasttokens[(jj_endpos = pos) - 1] = kind;
-    }
-  }
-
-  public ParseException generateParseException() {
-    jj_expentries.removeAllElements();
-    boolean[] la1tokens = new boolean[32];
-    for (int i = 0; i < 32; i++) {
-      la1tokens[i] = false;
-    }
-    if (jj_kind >= 0) {
-      la1tokens[jj_kind] = true;
-      jj_kind = -1;
-    }
-    for (int i = 0; i < 24; i++) {
-      if (jj_la1[i] == jj_gen) {
-        for (int j = 0; j < 32; j++) {
-          if ((jj_la1_0[i] & (1<<j)) != 0) {
-            la1tokens[j] = true;
-          }
-        }
-      }
-    }
-    for (int i = 0; i < 32; i++) {
-      if (la1tokens[i]) {
-        jj_expentry = new int[1];
-        jj_expentry[0] = i;
-        jj_expentries.addElement(jj_expentry);
-      }
-    }
-    jj_endpos = 0;
-    jj_rescan_token();
-    jj_add_error_token(0, 0);
-    int[][] exptokseq = new int[jj_expentries.size()][];
-    for (int i = 0; i < jj_expentries.size(); i++) {
-      exptokseq[i] = (int[])jj_expentries.elementAt(i);
-    }
-    return new ParseException(token, exptokseq, tokenImage);
-  }
-
-  final public void enable_tracing() {
-  }
-
-  final public void disable_tracing() {
-  }
-
-  final private void jj_rescan_token() {
-    jj_rescan = true;
-    for (int i = 0; i < 1; i++) {
-      JJCalls p = jj_2_rtns[i];
-      do {
-        if (p.gen > jj_gen) {
-          jj_la = p.arg; jj_lastpos = jj_scanpos = p.first;
-          switch (i) {
-            case 0: jj_3_1(); break;
-          }
-        }
-        p = p.next;
-      } while (p != null);
-    }
-    jj_rescan = false;
-  }
-
-  final private void jj_save(int index, int xla) {
-    JJCalls p = jj_2_rtns[index];
-    while (p.gen > jj_gen) {
-      if (p.next == null) { p = p.next = new JJCalls(); break; }
-      p = p.next;
-    }
-    p.gen = jj_gen + xla - jj_la; p.first = token; p.arg = xla;
-  }
-
-  static final class JJCalls {
-    int gen;
-    Token first;
-    int arg;
-    JJCalls next;
-  }
-
-}
diff --git a/contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.jj b/contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.jj
deleted file mode 100644
index 9d090b8..0000000
--- a/contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.jj
+++ /dev/null
@@ -1,968 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-options {
-  STATIC=false;
-  JAVA_UNICODE_ESCAPE=true;
-  USER_CHAR_STREAM=true;
-}
-
-PARSER_BEGIN(PrecedenceQueryParser)
-
-package org.apache.lucene.queryParser.precedence;
-
-import java.io.IOException;
-import java.io.StringReader;
-import java.text.DateFormat;
-import java.util.ArrayList;
-import java.util.Date;
-import java.util.List;
-import java.util.Locale;
-import java.util.Vector;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.document.DateTools;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.FuzzyQuery;
-import org.apache.lucene.search.MultiPhraseQuery;
-import org.apache.lucene.search.PhraseQuery;
-import org.apache.lucene.search.PrefixQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.RangeQuery;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.WildcardQuery;
-import org.apache.lucene.util.Parameter;
-
-/**
- * Experimental query parser variant designed to handle operator precedence
- * in a more sensible fashion than QueryParser.  There are still some
- * open issues with this parser. The following tests are currently failing
- * in TestPrecedenceQueryParser and are disabled to make this test pass:
- * <ul>
- * <li> testSimple
- * <li> testWildcard
- * <li> testPrecedence
- * </ul>
- *
- * This class is generated by JavaCC.  The only method that clients should need
- * to call is {@link #parse(String)}.
- *
- * The syntax for query strings is as follows:
- * A Query is a series of clauses.
- * A clause may be prefixed by:
- * <ul>
- * <li> a plus (<code>+</code>) or a minus (<code>-</code>) sign, indicating
- * that the clause is required or prohibited respectively; or
- * <li> a term followed by a colon, indicating the field to be searched.
- * This enables one to construct queries which search multiple fields.
- * </ul>
- *
- * A clause may be either:
- * <ul>
- * <li> a term, indicating all the documents that contain this term; or
- * <li> a nested query, enclosed in parentheses.  Note that this may be used
- * with a <code>+</code>/<code>-</code> prefix to require any of a set of
- * terms.
- * </ul>
- *
- * Thus, in BNF, the query grammar is:
- * <pre>
- *   Query  ::= ( Clause )*
- *   Clause ::= ["+", "-"] [&lt;TERM&gt; ":"] ( &lt;TERM&gt; | "(" Query ")" )
- * </pre>
- *
- * <p>
- * Examples of appropriately formatted queries can be found in the <a
- * href="../../../../../../../queryparsersyntax.html">query syntax
- * documentation</a>.
- * </p>
- *
- * @author Brian Goetz
- * @author Peter Halacsy
- * @author Tatu Saloranta
- */
-
-public class PrecedenceQueryParser {
-
-  private static final int CONJ_NONE   = 0;
-  private static final int CONJ_AND    = 1;
-  private static final int CONJ_OR     = 2;
-
-  private static final int MOD_NONE    = 0;
-  private static final int MOD_NOT     = 10;
-  private static final int MOD_REQ     = 11;
-
-  // make it possible to call setDefaultOperator() without accessing
-  // the nested class:
-  public static final Operator AND_OPERATOR = Operator.AND;
-  public static final Operator OR_OPERATOR = Operator.OR;
-
-  /** The actual operator that parser uses to combine query terms */
-  private Operator operator = OR_OPERATOR;
-
-  boolean lowercaseExpandedTerms = true;
-
-  Analyzer analyzer;
-  String field;
-  int phraseSlop = 0;
-  float fuzzyMinSim = FuzzyQuery.defaultMinSimilarity;
-  int fuzzyPrefixLength = FuzzyQuery.defaultPrefixLength;
-  Locale locale = Locale.getDefault();
-
-  static final class Operator extends Parameter {
-    private Operator(String name) {
-      super(name);
-    }
-    static final Operator OR = new Operator("OR");
-    static final Operator AND = new Operator("AND");
-  }
-
-  /** Constructs a query parser.
-   *  @param f  the default field for query terms.
-   *  @param a   used to find terms in the query text.
-   */
-  public PrecedenceQueryParser(String f, Analyzer a) {
-    this(new FastCharStream(new StringReader("")));
-    analyzer = a;
-    field = f;
-  }
-
-  /** Parses a query string, returning a {@link org.apache.lucene.search.Query}.
-   *  @param expression  the query string to be parsed.
-   *  @throws ParseException if the parsing fails
-   */
-  public Query parse(String expression) throws ParseException {
-    // optimize empty query to be empty BooleanQuery
-    if (expression == null || expression.trim().length() == 0) {
-      return new BooleanQuery();
-    }
-
-    ReInit(new FastCharStream(new StringReader(expression)));
-    try {
-      Query query = Query(field);
-      return (query != null) ? query : new BooleanQuery();
-    }
-    catch (TokenMgrError tme) {
-      throw new ParseException(tme.getMessage());
-    }
-    catch (BooleanQuery.TooManyClauses tmc) {
-      throw new ParseException("Too many boolean clauses");
-    }
-  }
-
-   /**
-   * @return Returns the analyzer.
-   */
-  public Analyzer getAnalyzer() {
-    return analyzer;
-  }
-
-  /**
-   * @return Returns the field.
-   */
-  public String getField() {
-    return field;
-  }
-
-   /**
-   * Get the minimal similarity for fuzzy queries.
-   */
-  public float getFuzzyMinSim() {
-      return fuzzyMinSim;
-  }
-
-  /**
-   * Set the minimum similarity for fuzzy queries.
-   * Default is 0.5f.
-   */
-  public void setFuzzyMinSim(float fuzzyMinSim) {
-      this.fuzzyMinSim = fuzzyMinSim;
-  }
-
-   /**
-   * Get the prefix length for fuzzy queries. 
-   * @return Returns the fuzzyPrefixLength.
-   */
-  public int getFuzzyPrefixLength() {
-    return fuzzyPrefixLength;
-  }
-
-  /**
-   * Set the prefix length for fuzzy queries. Default is 0.
-   * @param fuzzyPrefixLength The fuzzyPrefixLength to set.
-   */
-  public void setFuzzyPrefixLength(int fuzzyPrefixLength) {
-    this.fuzzyPrefixLength = fuzzyPrefixLength;
-  }
-
-  /**
-   * Sets the default slop for phrases.  If zero, then exact phrase matches
-   * are required.  Default value is zero.
-   */
-  public void setPhraseSlop(int phraseSlop) {
-    this.phraseSlop = phraseSlop;
-  }
-
-  /**
-   * Gets the default slop for phrases.
-   */
-  public int getPhraseSlop() {
-    return phraseSlop;
-  }
-
-  /**
-   * Sets the boolean operator of the QueryParser.
-   * In default mode (<code>OR_OPERATOR</code>) terms without any modifiers
-   * are considered optional: for example <code>capital of Hungary</code> is equal to
-   * <code>capital OR of OR Hungary</code>.<br/>
-   * In <code>AND_OPERATOR</code> mode terms are considered to be in conjuction: the
-   * above mentioned query is parsed as <code>capital AND of AND Hungary</code>
-   */
-  public void setDefaultOperator(Operator op) {
-    this.operator = op;
-  }
-
-  /**
-   * Gets implicit operator setting, which will be either AND_OPERATOR
-   * or OR_OPERATOR.
-   */
-  public Operator getDefaultOperator() {
-    return operator;
-  }
-
-  /**
-   * Whether terms of wildcard, prefix, fuzzy and range queries are to be automatically
-   * lower-cased or not.  Default is <code>true</code>.
-   */
-  public void setLowercaseExpandedTerms(boolean lowercaseExpandedTerms) {
-    this.lowercaseExpandedTerms = lowercaseExpandedTerms;
-  }
-
-  /**
-   * @see #setLowercaseExpandedTerms(boolean)
-   */
-  public boolean getLowercaseExpandedTerms() {
-    return lowercaseExpandedTerms;
-  }
-
-  /**
-   * Set locale used by date range parsing.
-   */
-  public void setLocale(Locale locale) {
-    this.locale = locale;
-  }
-
-  /**
-   * Returns current locale, allowing access by subclasses.
-   */
-  public Locale getLocale() {
-    return locale;
-  }
-
-  /**
-   * @deprecated use {@link #addClause(List, int, int, Query)} instead.
-   */
-  protected void addClause(Vector clauses, int conj, int modifier, Query q) {
-    addClause((List) clauses, conj, modifier, q);
-  }
-
-  protected void addClause(List clauses, int conj, int modifier, Query q) {
-    boolean required, prohibited;
-
-    // If this term is introduced by AND, make the preceding term required,
-    // unless it's already prohibited
-    if (clauses.size() > 0 && conj == CONJ_AND) {
-      BooleanClause c = (BooleanClause) clauses.get(clauses.size()-1);
-      if (!c.isProhibited())
-        c.setOccur(BooleanClause.Occur.MUST);
-    }
-
-    if (clauses.size() > 0 && operator == AND_OPERATOR && conj == CONJ_OR) {
-      // If this term is introduced by OR, make the preceding term optional,
-      // unless it's prohibited (that means we leave -a OR b but +a OR b-->a OR b)
-      // notice if the input is a OR b, first term is parsed as required; without
-      // this modification a OR b would parsed as +a OR b
-      BooleanClause c = (BooleanClause) clauses.get(clauses.size()-1);
-      if (!c.isProhibited())
-        c.setOccur(BooleanClause.Occur.SHOULD);
-    }
-
-    // We might have been passed a null query; the term might have been
-    // filtered away by the analyzer.
-    if (q == null)
-      return;
-
-    if (operator == OR_OPERATOR) {
-      // We set REQUIRED if we're introduced by AND or +; PROHIBITED if
-      // introduced by NOT or -; make sure not to set both.
-      prohibited = (modifier == MOD_NOT);
-      required = (modifier == MOD_REQ);
-      if (conj == CONJ_AND && !prohibited) {
-        required = true;
-      }
-    } else {
-      // We set PROHIBITED if we're introduced by NOT or -; We set REQUIRED
-      // if not PROHIBITED and not introduced by OR
-      prohibited = (modifier == MOD_NOT);
-      required   = (!prohibited && conj != CONJ_OR);
-    }
-    if (required && !prohibited)
-      clauses.add(new BooleanClause(q, BooleanClause.Occur.MUST));
-    else if (!required && !prohibited)
-      clauses.add(new BooleanClause(q, BooleanClause.Occur.SHOULD));
-    else if (!required && prohibited)
-      clauses.add(new BooleanClause(q, BooleanClause.Occur.MUST_NOT));
-    else
-      throw new RuntimeException("Clause cannot be both required and prohibited");
-  }
-
-  /**
-   * @exception ParseException throw in overridden method to disallow
-   */
-  protected Query getFieldQuery(String field, String queryText)  throws ParseException {
-    // Use the analyzer to get all the tokens, and then build a TermQuery,
-    // PhraseQuery, or nothing based on the term count
-
-    TokenStream source = analyzer.tokenStream(field, new StringReader(queryText));
-    List list = new ArrayList();
-    final org.apache.lucene.analysis.Token reusableToken = new org.apache.lucene.analysis.Token();
-    org.apache.lucene.analysis.Token nextToken;
-    int positionCount = 0;
-    boolean severalTokensAtSamePosition = false;
-
-    while (true) {
-      try {
-        nextToken = source.next(reusableToken);
-      }
-      catch (IOException e) {
-        nextToken = null;
-      }
-      if (nextToken == null)
-        break;
-      list.add(nextToken.clone());
-      if (nextToken.getPositionIncrement() == 1)
-        positionCount++;
-      else
-        severalTokensAtSamePosition = true;
-    }
-    try {
-      source.close();
-    }
-    catch (IOException e) {
-      // ignore
-    }
-
-    if (list.size() == 0)
-      return null;
-    else if (list.size() == 1) {
-      nextToken = (org.apache.lucene.analysis.Token) list.get(0);
-      return new TermQuery(new Term(field, nextToken.term()));
-    } else {
-      if (severalTokensAtSamePosition) {
-        if (positionCount == 1) {
-          // no phrase query:
-          BooleanQuery q = new BooleanQuery();
-          for (int i = 0; i < list.size(); i++) {
-            nextToken = (org.apache.lucene.analysis.Token) list.get(i);
-            TermQuery currentQuery = new TermQuery(
-                new Term(field, nextToken.term()));
-            q.add(currentQuery, BooleanClause.Occur.SHOULD);
-          }
-          return q;
-        }
-        else {
-          // phrase query:
-          MultiPhraseQuery mpq = new MultiPhraseQuery();
-          List multiTerms = new ArrayList();
-          for (int i = 0; i < list.size(); i++) {
-            nextToken = (org.apache.lucene.analysis.Token) list.get(i);
-            if (nextToken.getPositionIncrement() == 1 && multiTerms.size() > 0) {
-              mpq.add((Term[])multiTerms.toArray(new Term[0]));
-              multiTerms.clear();
-            }
-            multiTerms.add(new Term(field, nextToken.term()));
-          }
-          mpq.add((Term[])multiTerms.toArray(new Term[0]));
-          return mpq;
-        }
-      }
-      else {
-        PhraseQuery q = new PhraseQuery();
-        q.setSlop(phraseSlop);
-        for (int i = 0; i < list.size(); i++) {
-          q.add(new Term(field, ((org.apache.lucene.analysis.Token)
-              list.get(i)).term()));
-        }
-        return q;
-      }
-    }
-  }
-
-  /**
-   * Base implementation delegates to {@link #getFieldQuery(String,String)}.
-   * This method may be overridden, for example, to return
-   * a SpanNearQuery instead of a PhraseQuery.
-   *
-   * @exception ParseException throw in overridden method to disallow
-   */
-  protected Query getFieldQuery(String field, String queryText, int slop)
-        throws ParseException {
-    Query query = getFieldQuery(field, queryText);
-
-    if (query instanceof PhraseQuery) {
-      ((PhraseQuery) query).setSlop(slop);
-    }
-    if (query instanceof MultiPhraseQuery) {
-      ((MultiPhraseQuery) query).setSlop(slop);
-    }
-
-    return query;
-  }
-
-  /**
-   * @exception ParseException throw in overridden method to disallow
-   */
-  protected Query getRangeQuery(String field,
-                                String part1,
-                                String part2,
-                                boolean inclusive) throws ParseException
-  {
-    if (lowercaseExpandedTerms) {
-      part1 = part1.toLowerCase();
-      part2 = part2.toLowerCase();
-    }
-    try {
-      DateFormat df = DateFormat.getDateInstance(DateFormat.SHORT, locale);
-      df.setLenient(true);
-      Date d1 = df.parse(part1);
-      Date d2 = df.parse(part2);
-      part1 = DateTools.dateToString(d1, DateTools.Resolution.DAY);
-      part2 = DateTools.dateToString(d2, DateTools.Resolution.DAY);
-    }
-    catch (Exception e) { }
-
-    return new RangeQuery(new Term(field, part1),
-                          new Term(field, part2),
-                          inclusive);
-  }
-
-  /**
-   * Factory method for generating query, given a set of clauses.
-   * By default creates a boolean query composed of clauses passed in.
-   *
-   * Can be overridden by extending classes, to modify query being
-   * returned.
-   *
-   * @param clauses List that contains {@link BooleanClause} instances
-   *    to join.
-   *
-   * @return Resulting {@link Query} object.
-   * @exception ParseException throw in overridden method to disallow
-   * @deprecated use {@link #getBooleanQuery(List)} instead
-   */
-  protected Query getBooleanQuery(Vector clauses) throws ParseException
-  {
-    return getBooleanQuery((List) clauses, false);
-  }
-
-  /**
-   * Factory method for generating query, given a set of clauses.
-   * By default creates a boolean query composed of clauses passed in.
-   *
-   * Can be overridden by extending classes, to modify query being
-   * returned.
-   *
-   * @param clauses List that contains {@link BooleanClause} instances
-   *    to join.
-   *
-   * @return Resulting {@link Query} object.
-   * @exception ParseException throw in overridden method to disallow
-   */
-  protected Query getBooleanQuery(List clauses) throws ParseException
-  {
-    return getBooleanQuery(clauses, false);
-  }
-
-  /**
-   * Factory method for generating query, given a set of clauses.
-   * By default creates a boolean query composed of clauses passed in.
-   *
-   * Can be overridden by extending classes, to modify query being
-   * returned.
-   *
-   * @param clauses List that contains {@link BooleanClause} instances
-   *    to join.
-   * @param disableCoord true if coord scoring should be disabled.
-   *
-   * @return Resulting {@link Query} object.
-   * @exception ParseException throw in overridden method to disallow
-   * @deprecated use {@link #getBooleanQuery(List, boolean)} instead
-   */
-  protected Query getBooleanQuery(Vector clauses, boolean disableCoord)
-    throws ParseException
-  {
-    return getBooleanQuery((List) clauses, disableCoord);
-  }
-
-  /**
-   * Factory method for generating query, given a set of clauses.
-   * By default creates a boolean query composed of clauses passed in.
-   *
-   * Can be overridden by extending classes, to modify query being
-   * returned.
-   *
-   * @param clauses List that contains {@link BooleanClause} instances
-   *    to join.
-   * @param disableCoord true if coord scoring should be disabled.
-   *
-   * @return Resulting {@link Query} object.
-   * @exception ParseException throw in overridden method to disallow
-   */
-  protected Query getBooleanQuery(List clauses, boolean disableCoord)
-      throws ParseException {
-    if (clauses == null || clauses.size() == 0)
-      return null;
-
-    BooleanQuery query = new BooleanQuery(disableCoord);
-    for (int i = 0; i < clauses.size(); i++) {
-      query.add((BooleanClause)clauses.get(i));
-    }
-    return query;
-  }
-
-  /**
-   * Factory method for generating a query. Called when parser
-   * parses an input term token that contains one or more wildcard
-   * characters (? and *), but is not a prefix term token (one
-   * that has just a single * character at the end)
-   *<p>
-   * Depending on settings, prefix term may be lower-cased
-   * automatically. It will not go through the default Analyzer,
-   * however, since normal Analyzers are unlikely to work properly
-   * with wildcard templates.
-   *<p>
-   * Can be overridden by extending classes, to provide custom handling for
-   * wildcard queries, which may be necessary due to missing analyzer calls.
-   *
-   * @param field Name of the field query will use.
-   * @param termStr Term token that contains one or more wild card
-   *   characters (? or *), but is not simple prefix term
-   *
-   * @return Resulting {@link Query} built for the term
-   * @exception ParseException throw in overridden method to disallow
-   */
-  protected Query getWildcardQuery(String field, String termStr) throws ParseException
-  {
-    if (lowercaseExpandedTerms) {
-      termStr = termStr.toLowerCase();
-    }
-    Term t = new Term(field, termStr);
-    return new WildcardQuery(t);
-  }
-
-  /**
-   * Factory method for generating a query (similar to
-   * {@link #getWildcardQuery}). Called when parser parses an input term
-   * token that uses prefix notation; that is, contains a single '*' wildcard
-   * character as its last character. Since this is a special case
-   * of generic wildcard term, and such a query can be optimized easily,
-   * this usually results in a different query object.
-   *<p>
-   * Depending on settings, a prefix term may be lower-cased
-   * automatically. It will not go through the default Analyzer,
-   * however, since normal Analyzers are unlikely to work properly
-   * with wildcard templates.
-   *<p>
-   * Can be overridden by extending classes, to provide custom handling for
-   * wild card queries, which may be necessary due to missing analyzer calls.
-   *
-   * @param field Name of the field query will use.
-   * @param termStr Term token to use for building term for the query
-   *    (<b>without</b> trailing '*' character!)
-   *
-   * @return Resulting {@link Query} built for the term
-   * @exception ParseException throw in overridden method to disallow
-   */
-  protected Query getPrefixQuery(String field, String termStr) throws ParseException
-  {
-    if (lowercaseExpandedTerms) {
-      termStr = termStr.toLowerCase();
-    }
-    Term t = new Term(field, termStr);
-    return new PrefixQuery(t);
-  }
-
-   /**
-   * Factory method for generating a query (similar to
-   * {@link #getWildcardQuery}). Called when parser parses
-   * an input term token that has the fuzzy suffix (~) appended.
-   *
-   * @param field Name of the field query will use.
-   * @param termStr Term token to use for building term for the query
-   *
-   * @return Resulting {@link Query} built for the term
-   * @exception ParseException throw in overridden method to disallow
-   */
-  protected Query getFuzzyQuery(String field, String termStr, float minSimilarity) throws ParseException
-  {
-    if (lowercaseExpandedTerms) {
-      termStr = termStr.toLowerCase();
-    }
-    Term t = new Term(field, termStr);
-    return new FuzzyQuery(t, minSimilarity, fuzzyPrefixLength);
-  }
-
-  /**
-   * Returns a String where the escape char has been
-   * removed, or kept only once if there was a double escape.
-   */
-  private String discardEscapeChar(String input) {
-    char[] caSource = input.toCharArray();
-    char[] caDest = new char[caSource.length];
-    int j = 0;
-    for (int i = 0; i < caSource.length; i++) {
-      if ((caSource[i] != '\\') || (i > 0 && caSource[i-1] == '\\')) {
-        caDest[j++]=caSource[i];
-      }
-    }
-    return new String(caDest, 0, j);
-  }
-
-  /**
-   * Returns a String where those characters that QueryParser
-   * expects to be escaped are escaped by a preceding <code>\</code>.
-   */
-  public static String escape(String s) {
-    StringBuffer sb = new StringBuffer();
-    for (int i = 0; i < s.length(); i++) {
-      char c = s.charAt(i);
-      // NOTE: keep this in sync with _ESCAPED_CHAR below!
-      if (c == '\\' || c == '+' || c == '-' || c == '!' || c == '(' || c == ')' || c == ':'
-        || c == '^' || c == '[' || c == ']' || c == '\"' || c == '{' || c == '}' || c == '~'
-        || c == '*' || c == '?') {
-        sb.append('\\');
-      }
-      sb.append(c);
-    }
-    return sb.toString();
-  }
-
-  /**
-   * Command line tool to test QueryParser, using {@link org.apache.lucene.analysis.SimpleAnalyzer}.
-   * Usage:<br>
-   * <code>java org.apache.lucene.queryParser.QueryParser &lt;input&gt;</code>
-   */
-  public static void main(String[] args) throws Exception {
-    if (args.length == 0) {
-      System.out.println("Usage: java org.apache.lucene.queryParser.QueryParser <input>");
-      System.exit(0);
-    }
-    PrecedenceQueryParser qp = new PrecedenceQueryParser("field",
-                           new org.apache.lucene.analysis.SimpleAnalyzer());
-    Query q = qp.parse(args[0]);
-    System.out.println(q.toString("field"));
-  }
-}
-
-PARSER_END(PrecedenceQueryParser)
-
-/* ***************** */
-/* Token Definitions */
-/* ***************** */
-
-<*> TOKEN : {
-  <#_NUM_CHAR:   ["0"-"9"] >
-// NOTE: keep this in sync with escape(String) above!
-| <#_ESCAPED_CHAR: "\\" [ "\\", "+", "-", "!", "(", ")", ":", "^",
-                          "[", "]", "\"", "{", "}", "~", "*", "?" ] >
-| <#_TERM_START_CHAR: ( ~[ " ", "\t", "\n", "\r", "+", "-", "!", "(", ")", ":", "^",
-                           "[", "]", "\"", "{", "}", "~", "*", "?" ]
-                       | <_ESCAPED_CHAR> ) >
-| <#_TERM_CHAR: ( <_TERM_START_CHAR> | <_ESCAPED_CHAR> | "-" | "+" ) >
-| <#_WHITESPACE: ( " " | "\t" | "\n" | "\r") >
-}
-
-<DEFAULT, RangeIn, RangeEx> SKIP : {
-  <<_WHITESPACE>>
-}
-
-// OG: to support prefix queries:
-// http://nagoya.apache.org/bugzilla/show_bug.cgi?id=12137
-// Change from:
-// | <WILDTERM:  <_TERM_START_CHAR>
-//              (<_TERM_CHAR> | ( [ "*", "?" ] ))* >
-// To:
-//
-// | <WILDTERM:  (<_TERM_CHAR> | ( [ "*", "?" ] ))* >
-
-<DEFAULT> TOKEN : {
-  <AND:       ("AND" | "&&") >
-| <OR:        ("OR" | "||") >
-| <NOT:       ("NOT" | "!") >
-| <PLUS:      "+" >
-| <MINUS:     "-" >
-| <LPAREN:    "(" >
-| <RPAREN:    ")" >
-| <COLON:     ":" >
-| <CARAT:     "^" > : Boost
-| <QUOTED:     "\"" (~["\""])+ "\"">
-| <TERM:      <_TERM_START_CHAR> (<_TERM_CHAR>)*  >
-| <FUZZY_SLOP:     "~" ( (<_NUM_CHAR>)+ ( "." (<_NUM_CHAR>)+ )? )? >
-| <PREFIXTERM:  <_TERM_START_CHAR> (<_TERM_CHAR>)* "*" >
-| <WILDTERM:  <_TERM_START_CHAR>
-              (<_TERM_CHAR> | ( [ "*", "?" ] ))* >
-| <RANGEIN_START: "[" > : RangeIn
-| <RANGEEX_START: "{" > : RangeEx
-}
-
-<Boost> TOKEN : {
-<NUMBER:    (<_NUM_CHAR>)+ ( "." (<_NUM_CHAR>)+ )? > : DEFAULT
-}
-
-<RangeIn> TOKEN : {
-<RANGEIN_TO: "TO">
-| <RANGEIN_END: "]"> : DEFAULT
-| <RANGEIN_QUOTED: "\"" (~["\""])+ "\"">
-| <RANGEIN_GOOP: (~[ " ", "]" ])+ >
-}
-
-<RangeEx> TOKEN : {
-<RANGEEX_TO: "TO">
-| <RANGEEX_END: "}"> : DEFAULT
-| <RANGEEX_QUOTED: "\"" (~["\""])+ "\"">
-| <RANGEEX_GOOP: (~[ " ", "}" ])+ >
-}
-
-// *   Query  ::= ( Clause )*
-// *   Clause ::= ["+", "-"] [<TERM> ":"] ( <TERM> | "(" Query ")" )
-
-int Conjunction() : {
-  int ret = CONJ_NONE;
-}
-{
-  [
-    <AND> { ret = CONJ_AND; }
-    | <OR>  { ret = CONJ_OR; }
-  ]
-  { return ret; }
-}
-
-int Modifier() : {
-  int ret = MOD_NONE;
-}
-{
-  [
-     <PLUS> { ret = MOD_REQ; }
-     | <MINUS> { ret = MOD_NOT; }
-     | <NOT> { ret = MOD_NOT; }
-  ]
-  { return ret; }
-}
-
-Query Query(String field) :
-{
-  List clauses = new ArrayList();
-  Query q, firstQuery=null;
-  boolean orPresent = false;
-  int modifier;
-}
-{
-  modifier=Modifier() q=andExpression(field)
-  {
-    addClause(clauses, CONJ_NONE, modifier, q);
-    if (modifier == MOD_NONE)
-      firstQuery = q;
-  }
-  (
-    [<OR> { orPresent=true; }] modifier=Modifier() q=andExpression(field)
-    { addClause(clauses, orPresent ? CONJ_OR : CONJ_NONE, modifier, q); }
-  )*
-    {
-      if (clauses.size() == 1 && firstQuery != null)
-        return firstQuery;
-      else {
-        return getBooleanQuery(clauses);
-      }
-    }
-}
-
-Query andExpression(String field) :
-{
-  List clauses = new ArrayList();
-  Query q, firstQuery=null;
-  int modifier;
-}
-{
-  q=Clause(field)
-  {
-    addClause(clauses, CONJ_NONE, MOD_NONE, q);
-    firstQuery = q;
-  }
-  (
-    <AND> modifier=Modifier() q=Clause(field)
-    { addClause(clauses, CONJ_AND, modifier, q); }
-  )*
-    {
-      if (clauses.size() == 1 && firstQuery != null)
-        return firstQuery;
-      else {
-        return getBooleanQuery(clauses);
-      }
-    }
-}
-
-Query Clause(String field) : {
-  Query q;
-  Token fieldToken=null, boost=null;
-}
-{
-  [
-    LOOKAHEAD(2)
-    fieldToken=<TERM> <COLON> {
-      field=discardEscapeChar(fieldToken.image);
-    }
-  ]
-
-  (
-   q=Term(field)
-   | <LPAREN> q=Query(field) <RPAREN> (<CARAT> boost=<NUMBER>)?
-
-  )
-    {
-      if (boost != null) {
-        float f = (float)1.0;
-  try {
-    f = Float.valueOf(boost.image).floatValue();
-          q.setBoost(f);
-  } catch (Exception ignored) { }
-      }
-      return q;
-    }
-}
-
-
-Query Term(String field) : {
-  Token term, boost=null, fuzzySlop=null, goop1, goop2;
-  boolean prefix = false;
-  boolean wildcard = false;
-  boolean fuzzy = false;
-  Query q;
-}
-{
-  (
-     (
-       term=<TERM>
-       | term=<PREFIXTERM> { prefix=true; }
-       | term=<WILDTERM> { wildcard=true; }
-       | term=<NUMBER>
-     )
-     [ fuzzySlop=<FUZZY_SLOP> { fuzzy=true; } ]
-     [ <CARAT> boost=<NUMBER> [ fuzzySlop=<FUZZY_SLOP> { fuzzy=true; } ] ]
-     {
-       String termImage=discardEscapeChar(term.image);
-       if (wildcard) {
-       q = getWildcardQuery(field, termImage);
-       } else if (prefix) {
-         q = getPrefixQuery(field,
-           discardEscapeChar(term.image.substring
-          (0, term.image.length()-1)));
-       } else if (fuzzy) {
-       	  float fms = fuzzyMinSim;
-       	  try {
-            fms = Float.valueOf(fuzzySlop.image.substring(1)).floatValue();
-       	  } catch (Exception ignored) { }
-       	 if(fms < 0.0f || fms > 1.0f){
-       	   throw new ParseException("Minimum similarity for a FuzzyQuery has to be between 0.0f and 1.0f !");
-       	 }
-         q = getFuzzyQuery(field, termImage, fms);
-       } else {
-         q = getFieldQuery(field, termImage);
-       }
-     }
-     | ( <RANGEIN_START> ( goop1=<RANGEIN_GOOP>|goop1=<RANGEIN_QUOTED> )
-         [ <RANGEIN_TO> ] ( goop2=<RANGEIN_GOOP>|goop2=<RANGEIN_QUOTED> )
-         <RANGEIN_END> )
-       [ <CARAT> boost=<NUMBER> ]
-        {
-          if (goop1.kind == RANGEIN_QUOTED) {
-            goop1.image = goop1.image.substring(1, goop1.image.length()-1);
-          } else {
-            goop1.image = discardEscapeChar(goop1.image);
-          }
-          if (goop2.kind == RANGEIN_QUOTED) {
-            goop2.image = goop2.image.substring(1, goop2.image.length()-1);
-      } else {
-        goop2.image = discardEscapeChar(goop2.image);
-      }
-          q = getRangeQuery(field, goop1.image, goop2.image, true);
-        }
-     | ( <RANGEEX_START> ( goop1=<RANGEEX_GOOP>|goop1=<RANGEEX_QUOTED> )
-         [ <RANGEEX_TO> ] ( goop2=<RANGEEX_GOOP>|goop2=<RANGEEX_QUOTED> )
-         <RANGEEX_END> )
-       [ <CARAT> boost=<NUMBER> ]
-        {
-          if (goop1.kind == RANGEEX_QUOTED) {
-            goop1.image = goop1.image.substring(1, goop1.image.length()-1);
-          } else {
-            goop1.image = discardEscapeChar(goop1.image);
-          }
-          if (goop2.kind == RANGEEX_QUOTED) {
-            goop2.image = goop2.image.substring(1, goop2.image.length()-1);
-      } else {
-        goop2.image = discardEscapeChar(goop2.image);
-      }
-
-          q = getRangeQuery(field, goop1.image, goop2.image, false);
-        }
-     | term=<QUOTED>
-       [ fuzzySlop=<FUZZY_SLOP> ]
-       [ <CARAT> boost=<NUMBER> ]
-       {
-         int s = phraseSlop;
-
-         if (fuzzySlop != null) {
-           try {
-             s = Float.valueOf(fuzzySlop.image.substring(1)).intValue();
-           }
-           catch (Exception ignored) { }
-         }
-         q = getFieldQuery(field, term.image.substring(1, term.image.length()-1), s);
-       }
-  )
-  {
-    if (boost != null) {
-      float f = (float) 1.0;
-      try {
-        f = Float.valueOf(boost.image).floatValue();
-      }
-      catch (Exception ignored) {
-    /* Should this be handled somehow? (defaults to "no boost", if
-     * boost number is invalid)
-     */
-      }
-
-      // avoid boosting null queries, such as those caused by stop words
-      if (q != null) {
-        q.setBoost(f);
-      }
-    }
-    return q;
-  }
-}
diff --git a/contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParserConstants.java b/contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParserConstants.java
deleted file mode 100644
index c221c6a..0000000
--- a/contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParserConstants.java
+++ /dev/null
@@ -1,78 +0,0 @@
-/* Generated By:JavaCC: Do not edit this line. PrecedenceQueryParserConstants.java */
-package org.apache.lucene.queryParser.precedence;
-
-public interface PrecedenceQueryParserConstants {
-
-  int EOF = 0;
-  int _NUM_CHAR = 1;
-  int _ESCAPED_CHAR = 2;
-  int _TERM_START_CHAR = 3;
-  int _TERM_CHAR = 4;
-  int _WHITESPACE = 5;
-  int AND = 7;
-  int OR = 8;
-  int NOT = 9;
-  int PLUS = 10;
-  int MINUS = 11;
-  int LPAREN = 12;
-  int RPAREN = 13;
-  int COLON = 14;
-  int CARAT = 15;
-  int QUOTED = 16;
-  int TERM = 17;
-  int FUZZY_SLOP = 18;
-  int PREFIXTERM = 19;
-  int WILDTERM = 20;
-  int RANGEIN_START = 21;
-  int RANGEEX_START = 22;
-  int NUMBER = 23;
-  int RANGEIN_TO = 24;
-  int RANGEIN_END = 25;
-  int RANGEIN_QUOTED = 26;
-  int RANGEIN_GOOP = 27;
-  int RANGEEX_TO = 28;
-  int RANGEEX_END = 29;
-  int RANGEEX_QUOTED = 30;
-  int RANGEEX_GOOP = 31;
-
-  int Boost = 0;
-  int RangeEx = 1;
-  int RangeIn = 2;
-  int DEFAULT = 3;
-
-  String[] tokenImage = {
-    "<EOF>",
-    "<_NUM_CHAR>",
-    "<_ESCAPED_CHAR>",
-    "<_TERM_START_CHAR>",
-    "<_TERM_CHAR>",
-    "<_WHITESPACE>",
-    "<token of kind 6>",
-    "<AND>",
-    "<OR>",
-    "<NOT>",
-    "\"+\"",
-    "\"-\"",
-    "\"(\"",
-    "\")\"",
-    "\":\"",
-    "\"^\"",
-    "<QUOTED>",
-    "<TERM>",
-    "<FUZZY_SLOP>",
-    "<PREFIXTERM>",
-    "<WILDTERM>",
-    "\"[\"",
-    "\"{\"",
-    "<NUMBER>",
-    "\"TO\"",
-    "\"]\"",
-    "<RANGEIN_QUOTED>",
-    "<RANGEIN_GOOP>",
-    "\"TO\"",
-    "\"}\"",
-    "<RANGEEX_QUOTED>",
-    "<RANGEEX_GOOP>",
-  };
-
-}
diff --git a/contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParserTokenManager.java b/contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParserTokenManager.java
deleted file mode 100644
index 884a243..0000000
--- a/contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParserTokenManager.java
+++ /dev/null
@@ -1,1069 +0,0 @@
-/* Generated By:JavaCC: Do not edit this line. PrecedenceQueryParserTokenManager.java */
-package org.apache.lucene.queryParser.precedence;
-import java.io.IOException;
-import java.io.StringReader;
-import java.text.DateFormat;
-import java.util.ArrayList;
-import java.util.Date;
-import java.util.List;
-import java.util.Locale;
-import java.util.Vector;
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.document.DateTools;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.FuzzyQuery;
-import org.apache.lucene.search.MultiPhraseQuery;
-import org.apache.lucene.search.PhraseQuery;
-import org.apache.lucene.search.PrefixQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.RangeQuery;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.WildcardQuery;
-import org.apache.lucene.util.Parameter;
-
-public class PrecedenceQueryParserTokenManager implements PrecedenceQueryParserConstants
-{
-  public  java.io.PrintStream debugStream = System.out;
-  public  void setDebugStream(java.io.PrintStream ds) { debugStream = ds; }
-private final int jjStopStringLiteralDfa_3(int pos, long active0)
-{
-   switch (pos)
-   {
-      default :
-         return -1;
-   }
-}
-private final int jjStartNfa_3(int pos, long active0)
-{
-   return jjMoveNfa_3(jjStopStringLiteralDfa_3(pos, active0), pos + 1);
-}
-private final int jjStopAtPos(int pos, int kind)
-{
-   jjmatchedKind = kind;
-   jjmatchedPos = pos;
-   return pos + 1;
-}
-private final int jjStartNfaWithStates_3(int pos, int kind, int state)
-{
-   jjmatchedKind = kind;
-   jjmatchedPos = pos;
-   try { curChar = input_stream.readChar(); }
-   catch(java.io.IOException e) { return pos + 1; }
-   return jjMoveNfa_3(state, pos + 1);
-}
-private final int jjMoveStringLiteralDfa0_3()
-{
-   switch(curChar)
-   {
-      case 40:
-         return jjStopAtPos(0, 12);
-      case 41:
-         return jjStopAtPos(0, 13);
-      case 43:
-         return jjStopAtPos(0, 10);
-      case 45:
-         return jjStopAtPos(0, 11);
-      case 58:
-         return jjStopAtPos(0, 14);
-      case 91:
-         return jjStopAtPos(0, 21);
-      case 94:
-         return jjStopAtPos(0, 15);
-      case 123:
-         return jjStopAtPos(0, 22);
-      default :
-         return jjMoveNfa_3(0, 0);
-   }
-}
-private final void jjCheckNAdd(int state)
-{
-   if (jjrounds[state] != jjround)
-   {
-      jjstateSet[jjnewStateCnt++] = state;
-      jjrounds[state] = jjround;
-   }
-}
-private final void jjAddStates(int start, int end)
-{
-   do {
-      jjstateSet[jjnewStateCnt++] = jjnextStates[start];
-   } while (start++ != end);
-}
-private final void jjCheckNAddTwoStates(int state1, int state2)
-{
-   jjCheckNAdd(state1);
-   jjCheckNAdd(state2);
-}
-private final void jjCheckNAddStates(int start, int end)
-{
-   do {
-      jjCheckNAdd(jjnextStates[start]);
-   } while (start++ != end);
-}
-private final void jjCheckNAddStates(int start)
-{
-   jjCheckNAdd(jjnextStates[start]);
-   jjCheckNAdd(jjnextStates[start + 1]);
-}
-static final long[] jjbitVec0 = {
-   0xfffffffffffffffeL, 0xffffffffffffffffL, 0xffffffffffffffffL, 0xffffffffffffffffL
-};
-static final long[] jjbitVec2 = {
-   0x0L, 0x0L, 0xffffffffffffffffL, 0xffffffffffffffffL
-};
-private final int jjMoveNfa_3(int startState, int curPos)
-{
-   int[] nextStates;
-   int startsAt = 0;
-   jjnewStateCnt = 33;
-   int i = 1;
-   jjstateSet[0] = startState;
-   int j, kind = 0x7fffffff;
-   for (;;)
-   {
-      if (++jjround == 0x7fffffff)
-         ReInitRounds();
-      if (curChar < 64)
-      {
-         long l = 1L << curChar;
-         MatchLoop: do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-                  if ((0x7bffd0f8ffffd9ffL & l) != 0L)
-                  {
-                     if (kind > 17)
-                        kind = 17;
-                     jjCheckNAddStates(0, 6);
-                  }
-                  else if ((0x100002600L & l) != 0L)
-                  {
-                     if (kind > 6)
-                        kind = 6;
-                  }
-                  else if (curChar == 34)
-                     jjCheckNAdd(15);
-                  else if (curChar == 33)
-                  {
-                     if (kind > 9)
-                        kind = 9;
-                  }
-                  if (curChar == 38)
-                     jjstateSet[jjnewStateCnt++] = 4;
-                  break;
-               case 4:
-                  if (curChar == 38 && kind > 7)
-                     kind = 7;
-                  break;
-               case 5:
-                  if (curChar == 38)
-                     jjstateSet[jjnewStateCnt++] = 4;
-                  break;
-               case 13:
-                  if (curChar == 33 && kind > 9)
-                     kind = 9;
-                  break;
-               case 14:
-                  if (curChar == 34)
-                     jjCheckNAdd(15);
-                  break;
-               case 15:
-                  if ((0xfffffffbffffffffL & l) != 0L)
-                     jjCheckNAddTwoStates(15, 16);
-                  break;
-               case 16:
-                  if (curChar == 34 && kind > 16)
-                     kind = 16;
-                  break;
-               case 18:
-                  if ((0x3ff000000000000L & l) == 0L)
-                     break;
-                  if (kind > 18)
-                     kind = 18;
-                  jjAddStates(7, 8);
-                  break;
-               case 19:
-                  if (curChar == 46)
-                     jjCheckNAdd(20);
-                  break;
-               case 20:
-                  if ((0x3ff000000000000L & l) == 0L)
-                     break;
-                  if (kind > 18)
-                     kind = 18;
-                  jjCheckNAdd(20);
-                  break;
-               case 21:
-                  if ((0x7bffd0f8ffffd9ffL & l) == 0L)
-                     break;
-                  if (kind > 17)
-                     kind = 17;
-                  jjCheckNAddStates(0, 6);
-                  break;
-               case 22:
-                  if ((0x7bfff8f8ffffd9ffL & l) == 0L)
-                     break;
-                  if (kind > 17)
-                     kind = 17;
-                  jjCheckNAddTwoStates(22, 23);
-                  break;
-               case 24:
-                  if ((0x84002f0600000000L & l) == 0L)
-                     break;
-                  if (kind > 17)
-                     kind = 17;
-                  jjCheckNAddTwoStates(22, 23);
-                  break;
-               case 25:
-                  if ((0x7bfff8f8ffffd9ffL & l) != 0L)
-                     jjCheckNAddStates(9, 11);
-                  break;
-               case 26:
-                  if (curChar == 42 && kind > 19)
-                     kind = 19;
-                  break;
-               case 28:
-                  if ((0x84002f0600000000L & l) != 0L)
-                     jjCheckNAddStates(9, 11);
-                  break;
-               case 29:
-                  if ((0xfbfffcf8ffffd9ffL & l) == 0L)
-                     break;
-                  if (kind > 20)
-                     kind = 20;
-                  jjCheckNAddTwoStates(29, 30);
-                  break;
-               case 31:
-                  if ((0x84002f0600000000L & l) == 0L)
-                     break;
-                  if (kind > 20)
-                     kind = 20;
-                  jjCheckNAddTwoStates(29, 30);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else if (curChar < 128)
-      {
-         long l = 1L << (curChar & 077);
-         MatchLoop: do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-                  if ((0x97ffffff97ffffffL & l) != 0L)
-                  {
-                     if (kind > 17)
-                        kind = 17;
-                     jjCheckNAddStates(0, 6);
-                  }
-                  else if (curChar == 126)
-                  {
-                     if (kind > 18)
-                        kind = 18;
-                     jjstateSet[jjnewStateCnt++] = 18;
-                  }
-                  if (curChar == 92)
-                     jjCheckNAddStates(12, 14);
-                  else if (curChar == 78)
-                     jjstateSet[jjnewStateCnt++] = 11;
-                  else if (curChar == 124)
-                     jjstateSet[jjnewStateCnt++] = 8;
-                  else if (curChar == 79)
-                     jjstateSet[jjnewStateCnt++] = 6;
-                  else if (curChar == 65)
-                     jjstateSet[jjnewStateCnt++] = 2;
-                  break;
-               case 1:
-                  if (curChar == 68 && kind > 7)
-                     kind = 7;
-                  break;
-               case 2:
-                  if (curChar == 78)
-                     jjstateSet[jjnewStateCnt++] = 1;
-                  break;
-               case 3:
-                  if (curChar == 65)
-                     jjstateSet[jjnewStateCnt++] = 2;
-                  break;
-               case 6:
-                  if (curChar == 82 && kind > 8)
-                     kind = 8;
-                  break;
-               case 7:
-                  if (curChar == 79)
-                     jjstateSet[jjnewStateCnt++] = 6;
-                  break;
-               case 8:
-                  if (curChar == 124 && kind > 8)
-                     kind = 8;
-                  break;
-               case 9:
-                  if (curChar == 124)
-                     jjstateSet[jjnewStateCnt++] = 8;
-                  break;
-               case 10:
-                  if (curChar == 84 && kind > 9)
-                     kind = 9;
-                  break;
-               case 11:
-                  if (curChar == 79)
-                     jjstateSet[jjnewStateCnt++] = 10;
-                  break;
-               case 12:
-                  if (curChar == 78)
-                     jjstateSet[jjnewStateCnt++] = 11;
-                  break;
-               case 15:
-                  jjAddStates(15, 16);
-                  break;
-               case 17:
-                  if (curChar != 126)
-                     break;
-                  if (kind > 18)
-                     kind = 18;
-                  jjstateSet[jjnewStateCnt++] = 18;
-                  break;
-               case 21:
-                  if ((0x97ffffff97ffffffL & l) == 0L)
-                     break;
-                  if (kind > 17)
-                     kind = 17;
-                  jjCheckNAddStates(0, 6);
-                  break;
-               case 22:
-                  if ((0x97ffffff97ffffffL & l) == 0L)
-                     break;
-                  if (kind > 17)
-                     kind = 17;
-                  jjCheckNAddTwoStates(22, 23);
-                  break;
-               case 23:
-                  if (curChar == 92)
-                     jjCheckNAddTwoStates(24, 24);
-                  break;
-               case 24:
-                  if ((0x6800000078000000L & l) == 0L)
-                     break;
-                  if (kind > 17)
-                     kind = 17;
-                  jjCheckNAddTwoStates(22, 23);
-                  break;
-               case 25:
-                  if ((0x97ffffff97ffffffL & l) != 0L)
-                     jjCheckNAddStates(9, 11);
-                  break;
-               case 27:
-                  if (curChar == 92)
-                     jjCheckNAddTwoStates(28, 28);
-                  break;
-               case 28:
-                  if ((0x6800000078000000L & l) != 0L)
-                     jjCheckNAddStates(9, 11);
-                  break;
-               case 29:
-                  if ((0x97ffffff97ffffffL & l) == 0L)
-                     break;
-                  if (kind > 20)
-                     kind = 20;
-                  jjCheckNAddTwoStates(29, 30);
-                  break;
-               case 30:
-                  if (curChar == 92)
-                     jjCheckNAddTwoStates(31, 31);
-                  break;
-               case 31:
-                  if ((0x6800000078000000L & l) == 0L)
-                     break;
-                  if (kind > 20)
-                     kind = 20;
-                  jjCheckNAddTwoStates(29, 30);
-                  break;
-               case 32:
-                  if (curChar == 92)
-                     jjCheckNAddStates(12, 14);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else
-      {
-         int hiByte = (int)(curChar >> 8);
-         int i1 = hiByte >> 6;
-         long l1 = 1L << (hiByte & 077);
-         int i2 = (curChar & 0xff) >> 6;
-         long l2 = 1L << (curChar & 077);
-         MatchLoop: do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-                  if (!jjCanMove_0(hiByte, i1, i2, l1, l2))
-                     break;
-                  if (kind > 17)
-                     kind = 17;
-                  jjCheckNAddStates(0, 6);
-                  break;
-               case 15:
-                  if (jjCanMove_0(hiByte, i1, i2, l1, l2))
-                     jjAddStates(15, 16);
-                  break;
-               case 22:
-                  if (!jjCanMove_0(hiByte, i1, i2, l1, l2))
-                     break;
-                  if (kind > 17)
-                     kind = 17;
-                  jjCheckNAddTwoStates(22, 23);
-                  break;
-               case 25:
-                  if (jjCanMove_0(hiByte, i1, i2, l1, l2))
-                     jjCheckNAddStates(9, 11);
-                  break;
-               case 29:
-                  if (!jjCanMove_0(hiByte, i1, i2, l1, l2))
-                     break;
-                  if (kind > 20)
-                     kind = 20;
-                  jjCheckNAddTwoStates(29, 30);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      if (kind != 0x7fffffff)
-      {
-         jjmatchedKind = kind;
-         jjmatchedPos = curPos;
-         kind = 0x7fffffff;
-      }
-      ++curPos;
-      if ((i = jjnewStateCnt) == (startsAt = 33 - (jjnewStateCnt = startsAt)))
-         return curPos;
-      try { curChar = input_stream.readChar(); }
-      catch(java.io.IOException e) { return curPos; }
-   }
-}
-private final int jjStopStringLiteralDfa_1(int pos, long active0)
-{
-   switch (pos)
-   {
-      case 0:
-         if ((active0 & 0x10000000L) != 0L)
-         {
-            jjmatchedKind = 31;
-            return 4;
-         }
-         return -1;
-      default :
-         return -1;
-   }
-}
-private final int jjStartNfa_1(int pos, long active0)
-{
-   return jjMoveNfa_1(jjStopStringLiteralDfa_1(pos, active0), pos + 1);
-}
-private final int jjStartNfaWithStates_1(int pos, int kind, int state)
-{
-   jjmatchedKind = kind;
-   jjmatchedPos = pos;
-   try { curChar = input_stream.readChar(); }
-   catch(java.io.IOException e) { return pos + 1; }
-   return jjMoveNfa_1(state, pos + 1);
-}
-private final int jjMoveStringLiteralDfa0_1()
-{
-   switch(curChar)
-   {
-      case 84:
-         return jjMoveStringLiteralDfa1_1(0x10000000L);
-      case 125:
-         return jjStopAtPos(0, 29);
-      default :
-         return jjMoveNfa_1(0, 0);
-   }
-}
-private final int jjMoveStringLiteralDfa1_1(long active0)
-{
-   try { curChar = input_stream.readChar(); }
-   catch(java.io.IOException e) {
-      jjStopStringLiteralDfa_1(0, active0);
-      return 1;
-   }
-   switch(curChar)
-   {
-      case 79:
-         if ((active0 & 0x10000000L) != 0L)
-            return jjStartNfaWithStates_1(1, 28, 4);
-         break;
-      default :
-         break;
-   }
-   return jjStartNfa_1(0, active0);
-}
-private final int jjMoveNfa_1(int startState, int curPos)
-{
-   int[] nextStates;
-   int startsAt = 0;
-   jjnewStateCnt = 5;
-   int i = 1;
-   jjstateSet[0] = startState;
-   int j, kind = 0x7fffffff;
-   for (;;)
-   {
-      if (++jjround == 0x7fffffff)
-         ReInitRounds();
-      if (curChar < 64)
-      {
-         long l = 1L << curChar;
-         MatchLoop: do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-                  if ((0xfffffffeffffffffL & l) != 0L)
-                  {
-                     if (kind > 31)
-                        kind = 31;
-                     jjCheckNAdd(4);
-                  }
-                  if ((0x100002600L & l) != 0L)
-                  {
-                     if (kind > 6)
-                        kind = 6;
-                  }
-                  else if (curChar == 34)
-                     jjCheckNAdd(2);
-                  break;
-               case 1:
-                  if (curChar == 34)
-                     jjCheckNAdd(2);
-                  break;
-               case 2:
-                  if ((0xfffffffbffffffffL & l) != 0L)
-                     jjCheckNAddTwoStates(2, 3);
-                  break;
-               case 3:
-                  if (curChar == 34 && kind > 30)
-                     kind = 30;
-                  break;
-               case 4:
-                  if ((0xfffffffeffffffffL & l) == 0L)
-                     break;
-                  if (kind > 31)
-                     kind = 31;
-                  jjCheckNAdd(4);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else if (curChar < 128)
-      {
-         long l = 1L << (curChar & 077);
-         MatchLoop: do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-               case 4:
-                  if ((0xdfffffffffffffffL & l) == 0L)
-                     break;
-                  if (kind > 31)
-                     kind = 31;
-                  jjCheckNAdd(4);
-                  break;
-               case 2:
-                  jjAddStates(17, 18);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else
-      {
-         int hiByte = (int)(curChar >> 8);
-         int i1 = hiByte >> 6;
-         long l1 = 1L << (hiByte & 077);
-         int i2 = (curChar & 0xff) >> 6;
-         long l2 = 1L << (curChar & 077);
-         MatchLoop: do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-               case 4:
-                  if (!jjCanMove_0(hiByte, i1, i2, l1, l2))
-                     break;
-                  if (kind > 31)
-                     kind = 31;
-                  jjCheckNAdd(4);
-                  break;
-               case 2:
-                  if (jjCanMove_0(hiByte, i1, i2, l1, l2))
-                     jjAddStates(17, 18);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      if (kind != 0x7fffffff)
-      {
-         jjmatchedKind = kind;
-         jjmatchedPos = curPos;
-         kind = 0x7fffffff;
-      }
-      ++curPos;
-      if ((i = jjnewStateCnt) == (startsAt = 5 - (jjnewStateCnt = startsAt)))
-         return curPos;
-      try { curChar = input_stream.readChar(); }
-      catch(java.io.IOException e) { return curPos; }
-   }
-}
-private final int jjMoveStringLiteralDfa0_0()
-{
-   return jjMoveNfa_0(0, 0);
-}
-private final int jjMoveNfa_0(int startState, int curPos)
-{
-   int[] nextStates;
-   int startsAt = 0;
-   jjnewStateCnt = 3;
-   int i = 1;
-   jjstateSet[0] = startState;
-   int j, kind = 0x7fffffff;
-   for (;;)
-   {
-      if (++jjround == 0x7fffffff)
-         ReInitRounds();
-      if (curChar < 64)
-      {
-         long l = 1L << curChar;
-         MatchLoop: do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-                  if ((0x3ff000000000000L & l) == 0L)
-                     break;
-                  if (kind > 23)
-                     kind = 23;
-                  jjAddStates(19, 20);
-                  break;
-               case 1:
-                  if (curChar == 46)
-                     jjCheckNAdd(2);
-                  break;
-               case 2:
-                  if ((0x3ff000000000000L & l) == 0L)
-                     break;
-                  if (kind > 23)
-                     kind = 23;
-                  jjCheckNAdd(2);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else if (curChar < 128)
-      {
-         long l = 1L << (curChar & 077);
-         MatchLoop: do
-         {
-            switch(jjstateSet[--i])
-            {
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else
-      {
-         int hiByte = (int)(curChar >> 8);
-         int i1 = hiByte >> 6;
-         long l1 = 1L << (hiByte & 077);
-         int i2 = (curChar & 0xff) >> 6;
-         long l2 = 1L << (curChar & 077);
-         MatchLoop: do
-         {
-            switch(jjstateSet[--i])
-            {
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      if (kind != 0x7fffffff)
-      {
-         jjmatchedKind = kind;
-         jjmatchedPos = curPos;
-         kind = 0x7fffffff;
-      }
-      ++curPos;
-      if ((i = jjnewStateCnt) == (startsAt = 3 - (jjnewStateCnt = startsAt)))
-         return curPos;
-      try { curChar = input_stream.readChar(); }
-      catch(java.io.IOException e) { return curPos; }
-   }
-}
-private final int jjStopStringLiteralDfa_2(int pos, long active0)
-{
-   switch (pos)
-   {
-      case 0:
-         if ((active0 & 0x1000000L) != 0L)
-         {
-            jjmatchedKind = 27;
-            return 4;
-         }
-         return -1;
-      default :
-         return -1;
-   }
-}
-private final int jjStartNfa_2(int pos, long active0)
-{
-   return jjMoveNfa_2(jjStopStringLiteralDfa_2(pos, active0), pos + 1);
-}
-private final int jjStartNfaWithStates_2(int pos, int kind, int state)
-{
-   jjmatchedKind = kind;
-   jjmatchedPos = pos;
-   try { curChar = input_stream.readChar(); }
-   catch(java.io.IOException e) { return pos + 1; }
-   return jjMoveNfa_2(state, pos + 1);
-}
-private final int jjMoveStringLiteralDfa0_2()
-{
-   switch(curChar)
-   {
-      case 84:
-         return jjMoveStringLiteralDfa1_2(0x1000000L);
-      case 93:
-         return jjStopAtPos(0, 25);
-      default :
-         return jjMoveNfa_2(0, 0);
-   }
-}
-private final int jjMoveStringLiteralDfa1_2(long active0)
-{
-   try { curChar = input_stream.readChar(); }
-   catch(java.io.IOException e) {
-      jjStopStringLiteralDfa_2(0, active0);
-      return 1;
-   }
-   switch(curChar)
-   {
-      case 79:
-         if ((active0 & 0x1000000L) != 0L)
-            return jjStartNfaWithStates_2(1, 24, 4);
-         break;
-      default :
-         break;
-   }
-   return jjStartNfa_2(0, active0);
-}
-private final int jjMoveNfa_2(int startState, int curPos)
-{
-   int[] nextStates;
-   int startsAt = 0;
-   jjnewStateCnt = 5;
-   int i = 1;
-   jjstateSet[0] = startState;
-   int j, kind = 0x7fffffff;
-   for (;;)
-   {
-      if (++jjround == 0x7fffffff)
-         ReInitRounds();
-      if (curChar < 64)
-      {
-         long l = 1L << curChar;
-         MatchLoop: do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-                  if ((0xfffffffeffffffffL & l) != 0L)
-                  {
-                     if (kind > 27)
-                        kind = 27;
-                     jjCheckNAdd(4);
-                  }
-                  if ((0x100002600L & l) != 0L)
-                  {
-                     if (kind > 6)
-                        kind = 6;
-                  }
-                  else if (curChar == 34)
-                     jjCheckNAdd(2);
-                  break;
-               case 1:
-                  if (curChar == 34)
-                     jjCheckNAdd(2);
-                  break;
-               case 2:
-                  if ((0xfffffffbffffffffL & l) != 0L)
-                     jjCheckNAddTwoStates(2, 3);
-                  break;
-               case 3:
-                  if (curChar == 34 && kind > 26)
-                     kind = 26;
-                  break;
-               case 4:
-                  if ((0xfffffffeffffffffL & l) == 0L)
-                     break;
-                  if (kind > 27)
-                     kind = 27;
-                  jjCheckNAdd(4);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else if (curChar < 128)
-      {
-         long l = 1L << (curChar & 077);
-         MatchLoop: do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-               case 4:
-                  if ((0xffffffffdfffffffL & l) == 0L)
-                     break;
-                  if (kind > 27)
-                     kind = 27;
-                  jjCheckNAdd(4);
-                  break;
-               case 2:
-                  jjAddStates(17, 18);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else
-      {
-         int hiByte = (int)(curChar >> 8);
-         int i1 = hiByte >> 6;
-         long l1 = 1L << (hiByte & 077);
-         int i2 = (curChar & 0xff) >> 6;
-         long l2 = 1L << (curChar & 077);
-         MatchLoop: do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-               case 4:
-                  if (!jjCanMove_0(hiByte, i1, i2, l1, l2))
-                     break;
-                  if (kind > 27)
-                     kind = 27;
-                  jjCheckNAdd(4);
-                  break;
-               case 2:
-                  if (jjCanMove_0(hiByte, i1, i2, l1, l2))
-                     jjAddStates(17, 18);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      if (kind != 0x7fffffff)
-      {
-         jjmatchedKind = kind;
-         jjmatchedPos = curPos;
-         kind = 0x7fffffff;
-      }
-      ++curPos;
-      if ((i = jjnewStateCnt) == (startsAt = 5 - (jjnewStateCnt = startsAt)))
-         return curPos;
-      try { curChar = input_stream.readChar(); }
-      catch(java.io.IOException e) { return curPos; }
-   }
-}
-static final int[] jjnextStates = {
-   22, 25, 26, 29, 30, 27, 23, 18, 19, 25, 26, 27, 24, 28, 31, 15, 
-   16, 2, 3, 0, 1, 
-};
-private static final boolean jjCanMove_0(int hiByte, int i1, int i2, long l1, long l2)
-{
-   switch(hiByte)
-   {
-      case 0:
-         return ((jjbitVec2[i2] & l2) != 0L);
-      default : 
-         if ((jjbitVec0[i1] & l1) != 0L)
-            return true;
-         return false;
-   }
-}
-public static final String[] jjstrLiteralImages = {
-"", null, null, null, null, null, null, null, null, null, "\53", "\55", "\50", 
-"\51", "\72", "\136", null, null, null, null, null, "\133", "\173", null, "\124\117", 
-"\135", null, null, "\124\117", "\175", null, null, };
-public static final String[] lexStateNames = {
-   "Boost", 
-   "RangeEx", 
-   "RangeIn", 
-   "DEFAULT", 
-};
-public static final int[] jjnewLexState = {
-   -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 2, 1, 3, -1, 
-   3, -1, -1, -1, 3, -1, -1, 
-};
-static final long[] jjtoToken = {
-   0xffffff81L, 
-};
-static final long[] jjtoSkip = {
-   0x40L, 
-};
-protected CharStream input_stream;
-private final int[] jjrounds = new int[33];
-private final int[] jjstateSet = new int[66];
-protected char curChar;
-public PrecedenceQueryParserTokenManager(CharStream stream)
-{
-   input_stream = stream;
-}
-public PrecedenceQueryParserTokenManager(CharStream stream, int lexState)
-{
-   this(stream);
-   SwitchTo(lexState);
-}
-public void ReInit(CharStream stream)
-{
-   jjmatchedPos = jjnewStateCnt = 0;
-   curLexState = defaultLexState;
-   input_stream = stream;
-   ReInitRounds();
-}
-private final void ReInitRounds()
-{
-   int i;
-   jjround = 0x80000001;
-   for (i = 33; i-- > 0;)
-      jjrounds[i] = 0x80000000;
-}
-public void ReInit(CharStream stream, int lexState)
-{
-   ReInit(stream);
-   SwitchTo(lexState);
-}
-public void SwitchTo(int lexState)
-{
-   if (lexState >= 4 || lexState < 0)
-      throw new TokenMgrError("Error: Ignoring invalid lexical state : " + lexState + ". State unchanged.", TokenMgrError.INVALID_LEXICAL_STATE);
-   else
-      curLexState = lexState;
-}
-
-protected Token jjFillToken()
-{
-   Token t = Token.newToken(jjmatchedKind);
-   t.kind = jjmatchedKind;
-   String im = jjstrLiteralImages[jjmatchedKind];
-   t.image = (im == null) ? input_stream.GetImage() : im;
-   t.beginLine = input_stream.getBeginLine();
-   t.beginColumn = input_stream.getBeginColumn();
-   t.endLine = input_stream.getEndLine();
-   t.endColumn = input_stream.getEndColumn();
-   return t;
-}
-
-int curLexState = 3;
-int defaultLexState = 3;
-int jjnewStateCnt;
-int jjround;
-int jjmatchedPos;
-int jjmatchedKind;
-
-public Token getNextToken() 
-{
-  int kind;
-  Token specialToken = null;
-  Token matchedToken;
-  int curPos = 0;
-
-  EOFLoop :
-  for (;;)
-  {   
-   try   
-   {     
-      curChar = input_stream.BeginToken();
-   }     
-   catch(java.io.IOException e)
-   {        
-      jjmatchedKind = 0;
-      matchedToken = jjFillToken();
-      return matchedToken;
-   }
-
-   switch(curLexState)
-   {
-     case 0:
-       jjmatchedKind = 0x7fffffff;
-       jjmatchedPos = 0;
-       curPos = jjMoveStringLiteralDfa0_0();
-       break;
-     case 1:
-       jjmatchedKind = 0x7fffffff;
-       jjmatchedPos = 0;
-       curPos = jjMoveStringLiteralDfa0_1();
-       break;
-     case 2:
-       jjmatchedKind = 0x7fffffff;
-       jjmatchedPos = 0;
-       curPos = jjMoveStringLiteralDfa0_2();
-       break;
-     case 3:
-       jjmatchedKind = 0x7fffffff;
-       jjmatchedPos = 0;
-       curPos = jjMoveStringLiteralDfa0_3();
-       break;
-   }
-     if (jjmatchedKind != 0x7fffffff)
-     {
-        if (jjmatchedPos + 1 < curPos)
-           input_stream.backup(curPos - jjmatchedPos - 1);
-        if ((jjtoToken[jjmatchedKind >> 6] & (1L << (jjmatchedKind & 077))) != 0L)
-        {
-           matchedToken = jjFillToken();
-       if (jjnewLexState[jjmatchedKind] != -1)
-         curLexState = jjnewLexState[jjmatchedKind];
-           return matchedToken;
-        }
-        else
-        {
-         if (jjnewLexState[jjmatchedKind] != -1)
-           curLexState = jjnewLexState[jjmatchedKind];
-           continue EOFLoop;
-        }
-     }
-     int error_line = input_stream.getEndLine();
-     int error_column = input_stream.getEndColumn();
-     String error_after = null;
-     boolean EOFSeen = false;
-     try { input_stream.readChar(); input_stream.backup(1); }
-     catch (java.io.IOException e1) {
-        EOFSeen = true;
-        error_after = curPos <= 1 ? "" : input_stream.GetImage();
-        if (curChar == '\n' || curChar == '\r') {
-           error_line++;
-           error_column = 0;
-        }
-        else
-           error_column++;
-     }
-     if (!EOFSeen) {
-        input_stream.backup(1);
-        error_after = curPos <= 1 ? "" : input_stream.GetImage();
-     }
-     throw new TokenMgrError(EOFSeen, curLexState, error_line, error_column, error_after, curChar, TokenMgrError.LEXICAL_ERROR);
-  }
-}
-
-}
diff --git a/contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/Token.java b/contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/Token.java
deleted file mode 100644
index a1c0ca5..0000000
--- a/contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/Token.java
+++ /dev/null
@@ -1,81 +0,0 @@
-/* Generated By:JavaCC: Do not edit this line. Token.java Version 3.0 */
-package org.apache.lucene.queryParser.precedence;
-
-/**
- * Describes the input token stream.
- */
-
-public class Token {
-
-  /**
-   * An integer that describes the kind of this token.  This numbering
-   * system is determined by JavaCCParser, and a table of these numbers is
-   * stored in the file ...Constants.java.
-   */
-  public int kind;
-
-  /**
-   * beginLine and beginColumn describe the position of the first character
-   * of this token; endLine and endColumn describe the position of the
-   * last character of this token.
-   */
-  public int beginLine, beginColumn, endLine, endColumn;
-
-  /**
-   * The string image of the token.
-   */
-  public String image;
-
-  /**
-   * A reference to the next regular (non-special) token from the input
-   * stream.  If this is the last token from the input stream, or if the
-   * token manager has not read tokens beyond this one, this field is
-   * set to null.  This is true only if this token is also a regular
-   * token.  Otherwise, see below for a description of the contents of
-   * this field.
-   */
-  public Token next;
-
-  /**
-   * This field is used to access special tokens that occur prior to this
-   * token, but after the immediately preceding regular (non-special) token.
-   * If there are no such special tokens, this field is set to null.
-   * When there are more than one such special token, this field refers
-   * to the last of these special tokens, which in turn refers to the next
-   * previous special token through its specialToken field, and so on
-   * until the first special token (whose specialToken field is null).
-   * The next fields of special tokens refer to other special tokens that
-   * immediately follow it (without an intervening regular token).  If there
-   * is no such token, this field is null.
-   */
-  public Token specialToken;
-
-  /**
-   * Returns the image.
-   */
-  public String toString()
-  {
-     return image;
-  }
-
-  /**
-   * Returns a new Token object, by default. However, if you want, you
-   * can create and return subclass objects based on the value of ofKind.
-   * Simply add the cases to the switch for all those special cases.
-   * For example, if you have a subclass of Token called IDToken that
-   * you want to create if ofKind is ID, simlpy add something like :
-   *
-   *    case MyParserConstants.ID : return new IDToken();
-   *
-   * to the following switch statement. Then you can cast matchedToken
-   * variable to the appropriate type and use it in your lexical actions.
-   */
-  public static final Token newToken(int ofKind)
-  {
-     switch(ofKind)
-     {
-       default : return new Token();
-     }
-  }
-
-}
diff --git a/contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/TokenMgrError.java b/contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/TokenMgrError.java
deleted file mode 100644
index ed4dce2..0000000
--- a/contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/TokenMgrError.java
+++ /dev/null
@@ -1,133 +0,0 @@
-/* Generated By:JavaCC: Do not edit this line. TokenMgrError.java Version 3.0 */
-package org.apache.lucene.queryParser.precedence;
-
-public class TokenMgrError extends Error
-{
-   /*
-    * Ordinals for various reasons why an Error of this type can be thrown.
-    */
-
-   /**
-    * Lexical error occured.
-    */
-   static final int LEXICAL_ERROR = 0;
-
-   /**
-    * An attempt wass made to create a second instance of a static token manager.
-    */
-   static final int STATIC_LEXER_ERROR = 1;
-
-   /**
-    * Tried to change to an invalid lexical state.
-    */
-   static final int INVALID_LEXICAL_STATE = 2;
-
-   /**
-    * Detected (and bailed out of) an infinite loop in the token manager.
-    */
-   static final int LOOP_DETECTED = 3;
-
-   /**
-    * Indicates the reason why the exception is thrown. It will have
-    * one of the above 4 values.
-    */
-   int errorCode;
-
-   /**
-    * Replaces unprintable characters by their espaced (or unicode escaped)
-    * equivalents in the given string
-    */
-   protected static final String addEscapes(String str) {
-      StringBuffer retval = new StringBuffer();
-      char ch;
-      for (int i = 0; i < str.length(); i++) {
-        switch (str.charAt(i))
-        {
-           case 0 :
-              continue;
-           case '\b':
-              retval.append("\\b");
-              continue;
-           case '\t':
-              retval.append("\\t");
-              continue;
-           case '\n':
-              retval.append("\\n");
-              continue;
-           case '\f':
-              retval.append("\\f");
-              continue;
-           case '\r':
-              retval.append("\\r");
-              continue;
-           case '\"':
-              retval.append("\\\"");
-              continue;
-           case '\'':
-              retval.append("\\\'");
-              continue;
-           case '\\':
-              retval.append("\\\\");
-              continue;
-           default:
-              if ((ch = str.charAt(i)) < 0x20 || ch > 0x7e) {
-                 String s = "0000" + Integer.toString(ch, 16);
-                 retval.append("\\u" + s.substring(s.length() - 4, s.length()));
-              } else {
-                 retval.append(ch);
-              }
-              continue;
-        }
-      }
-      return retval.toString();
-   }
-
-   /**
-    * Returns a detailed message for the Error when it is thrown by the
-    * token manager to indicate a lexical error.
-    * Parameters : 
-    *    EOFSeen     : indicates if EOF caused the lexicl error
-    *    curLexState : lexical state in which this error occured
-    *    errorLine   : line number when the error occured
-    *    errorColumn : column number when the error occured
-    *    errorAfter  : prefix that was seen before this error occured
-    *    curchar     : the offending character
-    * Note: You can customize the lexical error message by modifying this method.
-    */
-   protected static String LexicalError(boolean EOFSeen, int lexState, int errorLine, int errorColumn, String errorAfter, char curChar) {
-      return("Lexical error at line " +
-           errorLine + ", column " +
-           errorColumn + ".  Encountered: " +
-           (EOFSeen ? "<EOF> " : ("\"" + addEscapes(String.valueOf(curChar)) + "\"") + " (" + (int)curChar + "), ") +
-           "after : \"" + addEscapes(errorAfter) + "\"");
-   }
-
-   /**
-    * You can also modify the body of this method to customize your error messages.
-    * For example, cases like LOOP_DETECTED and INVALID_LEXICAL_STATE are not
-    * of end-users concern, so you can return something like : 
-    *
-    *     "Internal Error : Please file a bug report .... "
-    *
-    * from this method for such cases in the release version of your parser.
-    */
-   public String getMessage() {
-      return super.getMessage();
-   }
-
-   /*
-    * Constructors of various flavors follow.
-    */
-
-   public TokenMgrError() {
-   }
-
-   public TokenMgrError(String message, int reason) {
-      super(message);
-      errorCode = reason;
-   }
-
-   public TokenMgrError(boolean EOFSeen, int lexState, int errorLine, int errorColumn, String errorAfter, char curChar, int reason) {
-      this(LexicalError(EOFSeen, lexState, errorLine, errorColumn, errorAfter, curChar), reason);
-   }
-}
diff --git a/contrib/miscellaneous/src/java/overview.html b/contrib/miscellaneous/src/java/overview.html
deleted file mode 100644
index 88e166f..0000000
--- a/contrib/miscellaneous/src/java/overview.html
+++ /dev/null
@@ -1,26 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-  <head>
-    <title>
-      miscellaneous
-    </title>
-  </head>
-  <body>
-  miscellaneous
-  </body>
-</html>
\ No newline at end of file
diff --git a/contrib/miscellaneous/src/test/org/apache/lucene/index/TestFieldNormModifier.java b/contrib/miscellaneous/src/test/org/apache/lucene/index/TestFieldNormModifier.java
deleted file mode 100644
index 023f970..0000000
--- a/contrib/miscellaneous/src/test/org/apache/lucene/index/TestFieldNormModifier.java
+++ /dev/null
@@ -1,242 +0,0 @@
-package org.apache.lucene.index;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import junit.framework.TestCase;
-
-import org.apache.lucene.analysis.SimpleAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.index.IndexWriter.MaxFieldLength;
-import org.apache.lucene.search.Collector;
-import org.apache.lucene.search.DefaultSimilarity;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Scorer;
-import org.apache.lucene.search.Similarity;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.RAMDirectory;
-
-/**
- * Tests changing of field norms with a custom similarity and with fake norms.
- *
- * @version $Id$
- */
-public class TestFieldNormModifier extends TestCase {
-  public TestFieldNormModifier(String name) {
-    super(name);
-  }
-  
-  public static byte DEFAULT_NORM = Similarity.encodeNorm(1.0f);
-  
-  public static int NUM_DOCS = 5;
-  
-  public Directory store = new RAMDirectory();
-  
-  /** inverts the normal notion of lengthNorm */
-  public static Similarity s = new DefaultSimilarity() {
-    public float lengthNorm(String fieldName, int numTokens) {
-      return numTokens;
-    }
-  };
-  
-  public void setUp() throws Exception {
-    IndexWriter writer = new IndexWriter(store, new SimpleAnalyzer(), true, MaxFieldLength.UNLIMITED);
-    
-    for (int i = 0; i < NUM_DOCS; i++) {
-      Document d = new Document();
-      d.add(new Field("field", "word", Field.Store.YES, Field.Index.ANALYZED));
-      d.add(new Field("nonorm", "word", Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS));
-      d.add(new Field("untokfield", "20061212 20071212", Field.Store.YES, Field.Index.ANALYZED));
-      
-      for (int j = 1; j <= i; j++) {
-        d.add(new Field("field", "crap", Field.Store.YES, Field.Index.ANALYZED));
-        d.add(new Field("nonorm", "more words", Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS));
-      }
-      writer.addDocument(d);
-    }
-    writer.close();
-  }
-  
-  public void testMissingField() {
-    FieldNormModifier fnm = new FieldNormModifier(store, s);
-    try {
-      fnm.reSetNorms("nobodyherebutuschickens");
-    } catch (Exception e) {
-      assertNull("caught something", e);
-    }
-  }
-  
-  public void testFieldWithNoNorm() throws Exception {
-    
-    IndexReader r = IndexReader.open(store);
-    byte[] norms = r.norms("nonorm");
-    
-    // sanity check, norms should all be 1
-    assertTrue("Whoops we have norms?", !r.hasNorms("nonorm"));
-    if (!r.getDisableFakeNorms()) {
-      for (int i = 0; i< norms.length; i++) {
-        assertEquals(""+i, DEFAULT_NORM, norms[i]);
-      }
-    } else {
-      assertNull(norms);
-    }
-    
-    r.close();
-    
-    FieldNormModifier fnm = new FieldNormModifier(store, s);
-    try {
-      fnm.reSetNorms("nonorm");
-    } catch (Exception e) {
-      assertNull("caught something", e);
-    }
-    
-    // nothing should have changed
-    r = IndexReader.open(store);
-    
-    norms = r.norms("nonorm");
-    assertTrue("Whoops we have norms?", !r.hasNorms("nonorm"));
-    if (!r.getDisableFakeNorms()) {
-      for (int i = 0; i< norms.length; i++) {
-        assertEquals(""+i, DEFAULT_NORM, norms[i]);
-      }
-    } else {
-      assertNull(norms);
-    }
-
-    r.close();
-  }
-  
-  
-  public void testGoodCases() throws Exception {
-    
-    IndexSearcher searcher = new IndexSearcher(store);
-    final float[] scores = new float[NUM_DOCS];
-    float lastScore = 0.0f;
-    
-    // default similarity should put docs with shorter length first
-    searcher.search(new TermQuery(new Term("field", "word")), new Collector() {
-      private int docBase = 0;
-      private Scorer scorer;
-      
-      public final void collect(int doc) throws IOException {
-        scores[doc + docBase] = scorer.score();
-      }
-      public void setNextReader(IndexReader reader, int docBase) {
-        this.docBase = docBase;
-      }
-      public void setScorer(Scorer scorer) throws IOException {
-        this.scorer = scorer;
-      }
-      public boolean acceptsDocsOutOfOrder() {
-        return true;
-      }
-    });
-    searcher.close();
-    
-    lastScore = Float.MAX_VALUE;
-    for (int i = 0; i < NUM_DOCS; i++) {
-      String msg = "i=" + i + ", " + scores[i] + " <= " + lastScore;
-      assertTrue(msg, scores[i] <= lastScore);
-      //System.out.println(msg);
-      lastScore = scores[i];
-    }
-
-    FieldNormModifier fnm = new FieldNormModifier(store, s);
-    fnm.reSetNorms("field");
-    
-    // new norm (with default similarity) should put longer docs first
-    searcher = new IndexSearcher(store);
-    searcher.search(new TermQuery(new Term("field", "word")),  new Collector() {
-      private int docBase = 0;
-      private Scorer scorer;
-      public final void collect(int doc) throws IOException {
-        scores[doc + docBase] = scorer.score();
-      }
-      public void setNextReader(IndexReader reader, int docBase) {
-        this.docBase = docBase;
-      }
-      public void setScorer(Scorer scorer) throws IOException {
-        this.scorer = scorer;
-      }
-      public boolean acceptsDocsOutOfOrder() {
-        return true;
-      }
-    });
-    searcher.close();
-    
-    lastScore = 0.0f;
-    for (int i = 0; i < NUM_DOCS; i++) {
-      String msg = "i=" + i + ", " + scores[i] + " >= " + lastScore;
-      assertTrue(msg, scores[i] >= lastScore);
-      //System.out.println(msg);
-      lastScore = scores[i];
-    }
-  }
-
-  public void testNormKiller() throws IOException {
-
-    IndexReader r = IndexReader.open(store);
-    byte[] oldNorms = r.norms("untokfield");    
-    r.close();
-    
-    FieldNormModifier fnm = new FieldNormModifier(store, s);
-    fnm.reSetNorms("untokfield");
-
-    r = IndexReader.open(store);
-    byte[] newNorms = r.norms("untokfield");
-    r.close();
-    assertFalse(Arrays.equals(oldNorms, newNorms));    
-
-    
-    // verify that we still get documents in the same order as originally
-    IndexSearcher searcher = new IndexSearcher(store);
-    final float[] scores = new float[NUM_DOCS];
-    float lastScore = 0.0f;
-    
-    // default similarity should return the same score for all documents for this query
-    searcher.search(new TermQuery(new Term("untokfield", "20061212")), new Collector() {
-      private int docBase = 0;
-      private Scorer scorer;
-      public final void collect(int doc) throws IOException {
-        scores[doc + docBase] = scorer.score();
-      }
-      public void setNextReader(IndexReader reader, int docBase) {
-        this.docBase = docBase;
-      }
-      public void setScorer(Scorer scorer) throws IOException {
-        this.scorer = scorer;
-      }
-      public boolean acceptsDocsOutOfOrder() {
-        return true;
-      }
-    });
-    searcher.close();
-    
-    lastScore = scores[0];
-    for (int i = 0; i < NUM_DOCS; i++) {
-      String msg = "i=" + i + ", " + scores[i] + " == " + lastScore;
-      assertTrue(msg, scores[i] == lastScore);
-      //System.out.println(msg);
-      lastScore = scores[i];
-    }
-  }
-}
diff --git a/contrib/miscellaneous/src/test/org/apache/lucene/index/TestTermVectorAccessor.java b/contrib/miscellaneous/src/test/org/apache/lucene/index/TestTermVectorAccessor.java
deleted file mode 100644
index d43e85e..0000000
--- a/contrib/miscellaneous/src/test/org/apache/lucene/index/TestTermVectorAccessor.java
+++ /dev/null
@@ -1,111 +0,0 @@
-package org.apache.lucene.index;
-
-import junit.framework.TestCase;
-import org.apache.lucene.analysis.standard.StandardAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.RAMDirectory;
-
-import java.util.Collections;
-/*
- *  Licensed under the Apache License, Version 2.0 (the "License");
- *  you may not use this file except in compliance with the License.
- *  You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- *  Unless required by applicable law or agreed to in writing, software
- *  distributed under the License is distributed on an "AS IS" BASIS,
- *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- *  See the License for the specific language governing permissions and
- *  limitations under the License.
- *
- */
-
-
-public class TestTermVectorAccessor extends TestCase {
-
-  public void test() throws Exception {
-
-    Directory dir = new RAMDirectory();
-    IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(Collections.EMPTY_SET), true);
-
-    Document doc;
-
-    doc = new Document();
-    doc.add(new Field("a", "a b a c a d a e a f a g a h a", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
-    doc.add(new Field("b", "a b c b d b e b f b g b h b", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
-    doc.add(new Field("c", "a c b c d c e c f c g c h c", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
-    iw.addDocument(doc);
-
-    doc = new Document();
-    doc.add(new Field("a", "a b a c a d a e a f a g a h a", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));
-    doc.add(new Field("b", "a b c b d b e b f b g b h b", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));
-    doc.add(new Field("c", "a c b c d c e c f c g c h c", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));
-    iw.addDocument(doc);
-
-    doc = new Document();
-    doc.add(new Field("a", "a b a c a d a e a f a g a h a", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));
-    doc.add(new Field("b", "a b c b d b e b f b g b h b", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));
-    doc.add(new Field("c", "a c b c d c e c f c g c h c", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));
-    iw.addDocument(doc);
-
-    doc = new Document();
-    doc.add(new Field("a", "a b a c a d a e a f a g a h a", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));
-    doc.add(new Field("b", "a b c b d b e b f b g b h b", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));
-    doc.add(new Field("c", "a c b c d c e c f c g c h c", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));
-    iw.addDocument(doc);
-
-    doc = new Document();
-    doc.add(new Field("a", "a b a c a d a e a f a g a h a", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
-    doc.add(new Field("b", "a b c b d b e b f b g b h b", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));
-    doc.add(new Field("c", "a c b c d c e c f c g c h c", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));
-    iw.addDocument(doc);
-
-    iw.close();
-
-    IndexReader ir = IndexReader.open(dir);
-
-    TermVectorAccessor accessor = new TermVectorAccessor();
-
-    ParallelArrayTermVectorMapper mapper;
-    TermFreqVector tfv;
-
-    for (int i = 0; i < ir.maxDoc(); i++) {
-
-      mapper = new ParallelArrayTermVectorMapper();
-      accessor.accept(ir, i, "a", mapper);
-      tfv = mapper.materializeVector();
-      assertEquals("doc " + i, "a", tfv.getTerms()[0]);
-      assertEquals("doc " + i, 8, tfv.getTermFrequencies()[0]);
-
-      mapper = new ParallelArrayTermVectorMapper();
-      accessor.accept(ir, i, "b", mapper);
-      tfv = mapper.materializeVector();
-      assertEquals("doc " + i, 8, tfv.getTermFrequencies().length);
-      assertEquals("doc " + i, "b", tfv.getTerms()[1]);
-      assertEquals("doc " + i, 7, tfv.getTermFrequencies()[1]);
-
-      mapper = new ParallelArrayTermVectorMapper();
-      accessor.accept(ir, i, "c", mapper);
-      tfv = mapper.materializeVector();
-      assertEquals("doc " + i, 8, tfv.getTermFrequencies().length);
-      assertEquals("doc " + i, "c", tfv.getTerms()[2]);
-      assertEquals("doc " + i, 7, tfv.getTermFrequencies()[2]);
-
-      mapper = new ParallelArrayTermVectorMapper();
-      accessor.accept(ir, i, "q", mapper);
-      tfv = mapper.materializeVector();
-      assertNull("doc " + i, tfv);
-
-    }
-
-    ir.close();
-
-    dir.close();
-
-
-  }
-
-}
diff --git a/contrib/miscellaneous/src/test/org/apache/lucene/misc/ChainedFilterTest.java b/contrib/miscellaneous/src/test/org/apache/lucene/misc/ChainedFilterTest.java
deleted file mode 100644
index 4b6906c..0000000
--- a/contrib/miscellaneous/src/test/org/apache/lucene/misc/ChainedFilterTest.java
+++ /dev/null
@@ -1,244 +0,0 @@
-package org.apache.lucene.misc;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import junit.framework.TestCase;
-import java.util.*;
-import java.io.IOException;
-
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.IndexWriter.MaxFieldLength;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.search.*;
-
-public class ChainedFilterTest extends TestCase {
-  public static final int MAX = 500;
-
-  private RAMDirectory directory;
-  private IndexSearcher searcher;
-  private Query query;
-  // private DateFilter dateFilter;   DateFilter was deprecated and removed
-  private TermRangeFilter dateFilter;
-  private QueryWrapperFilter bobFilter;
-  private QueryWrapperFilter sueFilter;
-
-  public void setUp() throws Exception {
-    directory = new RAMDirectory();
-    IndexWriter writer =
-       new IndexWriter(directory, new WhitespaceAnalyzer(), true);
-
-    Calendar cal = Calendar.getInstance();
-    cal.setTimeInMillis(1041397200000L); // 2003 January 01
-
-    for (int i = 0; i < MAX; i++) {
-      Document doc = new Document();
-      doc.add(new Field("key", "" + (i + 1), Field.Store.YES, Field.Index.NOT_ANALYZED));
-      doc.add(new Field("owner", (i < MAX / 2) ? "bob" : "sue", Field.Store.YES, Field.Index.NOT_ANALYZED));
-      doc.add(new Field("date", cal.getTime().toString(), Field.Store.YES, Field.Index.NOT_ANALYZED));
-      writer.addDocument(doc);
-
-      cal.add(Calendar.DATE, 1);
-    }
-
-    writer.close();
-
-    searcher = new IndexSearcher(directory);
-
-    // query for everything to make life easier
-    BooleanQuery bq = new BooleanQuery();
-    bq.add(new TermQuery(new Term("owner", "bob")), BooleanClause.Occur.SHOULD);
-    bq.add(new TermQuery(new Term("owner", "sue")), BooleanClause.Occur.SHOULD);
-    query = bq;
-
-    // date filter matches everything too
-    //Date pastTheEnd = parseDate("2099 Jan 1");
-    // dateFilter = DateFilter.Before("date", pastTheEnd);
-    // just treat dates as strings and select the whole range for now...
-    dateFilter = new TermRangeFilter("date","","ZZZZ",true,true);
-
-    bobFilter = new QueryWrapperFilter(
-        new TermQuery(new Term("owner", "bob")));
-    sueFilter = new QueryWrapperFilter(
-        new TermQuery(new Term("owner", "sue")));
-  }
-
-  private Filter[] getChainWithOldFilters(Filter[] chain) {
-    Filter[] oldFilters = new Filter[chain.length];
-    for (int i = 0; i < chain.length; i++) {
-      final Filter f = chain[i];
-    // create old BitSet-based Filter as wrapper
-      oldFilters[i] = new Filter() {
-        /** @deprecated */
-        public BitSet bits(IndexReader reader) throws IOException {
-          BitSet bits = new BitSet(reader.maxDoc());
-          DocIdSetIterator it = f.getDocIdSet(reader).iterator();  
-          int doc;
-          while((doc = it.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
-            bits.set(doc);
-          }
-          return bits;
-        }
-      };
-    }
-    return oldFilters;
-  }
-  
-  private ChainedFilter getChainedFilter(Filter[] chain, int[] logic, boolean old) {
-    if (old) {
-      chain = getChainWithOldFilters(chain);
-    }
-    
-    if (logic == null) {
-      return new ChainedFilter(chain);
-    } else {
-      return new ChainedFilter(chain, logic);
-    }
-  }
-
-  private ChainedFilter getChainedFilter(Filter[] chain, int logic, boolean old) {
-    if (old) {
-      chain = getChainWithOldFilters(chain);
-    }
-    
-    return new ChainedFilter(chain, logic);
-  }
-
-  
-  public void testSingleFilter() throws Exception {
-    for (int mode = 0; mode < 2; mode++) {
-      boolean old = (mode==0);
-      
-      ChainedFilter chain = getChainedFilter(new Filter[] {dateFilter}, null, old);
-  
-      Hits hits = searcher.search(query, chain);
-      assertEquals(MAX, hits.length());
-  
-      chain = new ChainedFilter(new Filter[] {bobFilter});
-      hits = searcher.search(query, chain);
-      assertEquals(MAX / 2, hits.length());
-      
-      chain = getChainedFilter(new Filter[] {bobFilter}, new int[] {ChainedFilter.AND}, old);
-      hits = searcher.search(query, chain);
-      assertEquals(MAX / 2, hits.length());
-      assertEquals("bob", hits.doc(0).get("owner"));
-      
-      chain = getChainedFilter(new Filter[] {bobFilter}, new int[] {ChainedFilter.ANDNOT}, old);
-      hits = searcher.search(query, chain);
-      assertEquals(MAX / 2, hits.length());
-      assertEquals("sue", hits.doc(0).get("owner"));
-    }
-  }
-
-  public void testOR() throws Exception {
-    for (int mode = 0; mode < 2; mode++) {
-      boolean old = (mode==0);
-      ChainedFilter chain = getChainedFilter(
-        new Filter[] {sueFilter, bobFilter}, null, old);
-  
-      Hits hits = searcher.search(query, chain);
-      assertEquals("OR matches all", MAX, hits.length());
-    }
-  }
-
-  public void testAND() throws Exception {
-    for (int mode = 0; mode < 2; mode++) {
-      boolean old = (mode==0);
-      ChainedFilter chain = getChainedFilter(
-        new Filter[] {dateFilter, bobFilter}, ChainedFilter.AND, old);
-  
-      Hits hits = searcher.search(query, chain);
-      assertEquals("AND matches just bob", MAX / 2, hits.length());
-      assertEquals("bob", hits.doc(0).get("owner"));
-    }
-  }
-
-  public void testXOR() throws Exception {
-    for (int mode = 0; mode < 2; mode++) {
-      boolean old = (mode==0);
-      ChainedFilter chain = getChainedFilter(
-        new Filter[]{dateFilter, bobFilter}, ChainedFilter.XOR, old);
-  
-      Hits hits = searcher.search(query, chain);
-      assertEquals("XOR matches sue", MAX / 2, hits.length());
-      assertEquals("sue", hits.doc(0).get("owner"));
-    }
-  }
-
-  public void testANDNOT() throws Exception {
-    for (int mode = 0; mode < 2; mode++) {
-      boolean old = (mode==0);
-      ChainedFilter chain = getChainedFilter(
-        new Filter[]{dateFilter, sueFilter},
-          new int[] {ChainedFilter.AND, ChainedFilter.ANDNOT}, old);
-  
-      Hits hits = searcher.search(query, chain);
-      assertEquals("ANDNOT matches just bob",
-          MAX / 2, hits.length());
-      assertEquals("bob", hits.doc(0).get("owner"));
-      
-      chain = getChainedFilter(
-          new Filter[]{bobFilter, bobFilter},
-            new int[] {ChainedFilter.ANDNOT, ChainedFilter.ANDNOT}, old);
-  
-        hits = searcher.search(query, chain);
-        assertEquals("ANDNOT bob ANDNOT bob matches all sues",
-            MAX / 2, hits.length());
-        assertEquals("sue", hits.doc(0).get("owner"));
-    }
-  }
-
-  /*
-  private Date parseDate(String s) throws ParseException {
-    return new SimpleDateFormat("yyyy MMM dd", Locale.US).parse(s);
-  }
-  */
-  
-  public void testWithCachingFilter() throws Exception {
-    Directory dir = new RAMDirectory();
-    Analyzer analyzer = new WhitespaceAnalyzer();
-  
-    IndexWriter writer = new IndexWriter(dir, analyzer, true, MaxFieldLength.LIMITED);
-    writer.close();
-  
-    Searcher searcher = new IndexSearcher(dir);
-  
-    Query query = new TermQuery(new Term("none", "none"));
-  
-    QueryWrapperFilter queryFilter = new QueryWrapperFilter(query);
-    CachingWrapperFilter cachingFilter = new CachingWrapperFilter(queryFilter);
-  
-    searcher.search(query, cachingFilter, 1);
-  
-    CachingWrapperFilter cachingFilter2 = new CachingWrapperFilter(queryFilter);
-    Filter[] chain = new Filter[2];
-    chain[0] = cachingFilter;
-    chain[1] = cachingFilter2;
-    ChainedFilter cf = new ChainedFilter(chain);
-  
-    // throws java.lang.ClassCastException: org.apache.lucene.util.OpenBitSet cannot be cast to java.util.BitSet
-    searcher.search(new MatchAllDocsQuery(), cf, 1);
-  }
-
-}
diff --git a/contrib/miscellaneous/src/test/org/apache/lucene/misc/SweetSpotSimilarityTest.java b/contrib/miscellaneous/src/test/org/apache/lucene/misc/SweetSpotSimilarityTest.java
deleted file mode 100644
index 31f25d7..0000000
--- a/contrib/miscellaneous/src/test/org/apache/lucene/misc/SweetSpotSimilarityTest.java
+++ /dev/null
@@ -1,208 +0,0 @@
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.lucene.misc;
-
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.Similarity;
-import org.apache.lucene.search.DefaultSimilarity;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.PhraseQuery;
-import org.apache.lucene.search.DisjunctionMaxQuery;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanClause.Occur;
-
-import junit.framework.Test;
-import junit.framework.TestCase;
-import junit.framework.TestSuite;
-
-import java.io.File;
-import java.math.BigDecimal;
-import java.util.Random;
-import java.util.Date;
-import java.util.List;
-import java.util.Arrays;
-import java.util.Map;
-import java.util.HashMap;
-import java.util.Iterator;
-
-/**
- * Test of the SweetSpotSimilarity
- */
-public class SweetSpotSimilarityTest extends TestCase {
-
-  public void testSweetSpotLengthNorm() {
-  
-    SweetSpotSimilarity ss = new SweetSpotSimilarity();
-    ss.setLengthNormFactors(1,1,0.5f);
-
-    Similarity d = new DefaultSimilarity();
-    Similarity s = ss;
-
-
-    // base case, should degrade
-  
-    for (int i = 1; i < 1000; i++) {
-      assertEquals("base case: i="+i,
-                   d.lengthNorm("foo",i), s.lengthNorm("foo",i),
-                   0.0f);
-    }
-
-    // make a sweet spot
-  
-    ss.setLengthNormFactors(3,10,0.5f);
-  
-    for (int i = 3; i <=10; i++) {
-      assertEquals("3,10: spot i="+i,
-                   1.0f, s.lengthNorm("foo",i),
-                   0.0f);
-    }
-  
-    for (int i = 10; i < 1000; i++) {
-      assertEquals("3,10: 10<x : i="+i,
-                   d.lengthNorm("foo",i-9), s.lengthNorm("foo",i),
-                   0.0f);
-    }
-
-
-    // seperate sweet spot for certain fields
-
-    ss.setLengthNormFactors("bar",8,13, 0.5f, false);
-    ss.setLengthNormFactors("yak",6,9, 0.5f, false);
-
-  
-    for (int i = 3; i <=10; i++) {
-      assertEquals("f: 3,10: spot i="+i,
-                   1.0f, s.lengthNorm("foo",i),
-                   0.0f);
-    }
-    for (int i = 10; i < 1000; i++) {
-      assertEquals("f: 3,10: 10<x : i="+i,
-                   d.lengthNorm("foo",i-9), s.lengthNorm("foo",i),
-                   0.0f);
-    }
-    for (int i = 8; i <=13; i++) {
-      assertEquals("f: 8,13: spot i="+i,
-                   1.0f, s.lengthNorm("bar",i),
-                   0.0f);
-    }
-    for (int i = 6; i <=9; i++) {
-      assertEquals("f: 6,9: spot i="+i,
-                   1.0f, s.lengthNorm("yak",i),
-                   0.0f);
-    }
-    for (int i = 13; i < 1000; i++) {
-      assertEquals("f: 8,13: 13<x : i="+i,
-                   d.lengthNorm("foo",i-12), s.lengthNorm("bar",i),
-                   0.0f);
-    }
-    for (int i = 9; i < 1000; i++) {
-      assertEquals("f: 6,9: 9<x : i="+i,
-                   d.lengthNorm("foo",i-8), s.lengthNorm("yak",i),
-                   0.0f);
-    }
-
-
-    // steepness
-
-    ss.setLengthNormFactors("a",5,8,0.5f, false);
-    ss.setLengthNormFactors("b",5,8,0.1f, false);
-
-    for (int i = 9; i < 1000; i++) {
-      assertTrue("s: i="+i+" : a="+ss.lengthNorm("a",i)+
-                 " < b="+ss.lengthNorm("b",i),
-                 ss.lengthNorm("a",i) < s.lengthNorm("b",i));
-    }
-
-  }
-
-  public void testSweetSpotTf() {
-  
-    SweetSpotSimilarity ss = new SweetSpotSimilarity();
-
-    Similarity d = new DefaultSimilarity();
-    Similarity s = ss;
-    
-    // tf equal
-
-    ss.setBaselineTfFactors(0.0f, 0.0f);
-  
-    for (int i = 1; i < 1000; i++) {
-      assertEquals("tf: i="+i,
-                   d.tf(i), s.tf(i), 0.0f);
-    }
-
-    // tf higher
-  
-    ss.setBaselineTfFactors(1.0f, 0.0f);
-  
-    for (int i = 1; i < 1000; i++) {
-      assertTrue("tf: i="+i+" : d="+d.tf(i)+
-                 " < s="+s.tf(i),
-                 d.tf(i) < s.tf(i));
-    }
-
-    // tf flat
-  
-    ss.setBaselineTfFactors(1.0f, 6.0f);
-    for (int i = 1; i <=6; i++) {
-      assertEquals("tf flat1: i="+i, 1.0f, s.tf(i), 0.0f);
-    }
-    ss.setBaselineTfFactors(2.0f, 6.0f);
-    for (int i = 1; i <=6; i++) {
-      assertEquals("tf flat2: i="+i, 2.0f, s.tf(i), 0.0f);
-    }
-    for (int i = 6; i <=1000; i++) {
-      assertTrue("tf: i="+i+" : s="+s.tf(i)+
-                 " < d="+d.tf(i),
-                 s.tf(i) < d.tf(i));
-    }
-
-    // stupidity
-    assertEquals("tf zero", 0.0f, s.tf(0), 0.0f);
-  }
-
-  public void testHyperbolicSweetSpot() {
-  
-    SweetSpotSimilarity ss = new SweetSpotSimilarity() {
-        public float tf(int freq) {
-          return hyperbolicTf(freq);
-        }
-      };
-    ss.setHyperbolicTfFactors(3.3f, 7.7f, Math.E, 5.0f);
-    
-    Similarity s = ss;
-
-    for (int i = 1; i <=1000; i++) {
-      assertTrue("MIN tf: i="+i+" : s="+s.tf(i),
-                 3.3f <= s.tf(i));
-      assertTrue("MAX tf: i="+i+" : s="+s.tf(i),
-                 s.tf(i) <= 7.7f);
-    }
-    assertEquals("MID tf", 3.3f+(7.7f - 3.3f)/2.0f, s.tf(5), 0.00001f);
-    
-    // stupidity
-    assertEquals("tf zero", 0.0f, s.tf(0), 0.0f);
-    
-  }
-
-  
-}
-
diff --git a/contrib/miscellaneous/src/test/org/apache/lucene/misc/TestLengthNormModifier.java b/contrib/miscellaneous/src/test/org/apache/lucene/misc/TestLengthNormModifier.java
deleted file mode 100644
index a893b41..0000000
--- a/contrib/miscellaneous/src/test/org/apache/lucene/misc/TestLengthNormModifier.java
+++ /dev/null
@@ -1,208 +0,0 @@
-package org.apache.lucene.misc;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import junit.framework.TestCase;
-
-import org.apache.lucene.analysis.SimpleAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.index.FieldNormModifier;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.IndexWriter.MaxFieldLength;
-import org.apache.lucene.search.Collector;
-import org.apache.lucene.search.DefaultSimilarity;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Scorer;
-import org.apache.lucene.search.Similarity;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.RAMDirectory;
-
-/**
- * Tests changing the norms after changing the simularity
- *
- * @version $Id:$
- */
-public class TestLengthNormModifier extends TestCase {
-    public TestLengthNormModifier(String name) {
-	super(name);
-    }
-
-    public static byte DEFAULT_NORM = Similarity.encodeNorm(1.0f);
-    
-    public static int NUM_DOCS = 5;
-
-    public Directory store = new RAMDirectory();
-
-    /** inverts the normal notion of lengthNorm */
-    public static Similarity s = new DefaultSimilarity() {
-	    public float lengthNorm(String fieldName, int numTokens) {
-		return numTokens;
-	    }
-	};
-    
-    public void setUp() throws Exception {
-	IndexWriter writer = new IndexWriter(store, new SimpleAnalyzer(), true, MaxFieldLength.UNLIMITED);
-	
-	for (int i = 0; i < NUM_DOCS; i++) {
-	    Document d = new Document();
-	    d.add(new Field("field", "word",
-			    Field.Store.YES, Field.Index.ANALYZED));
-	    d.add(new Field("nonorm", "word",
-			    Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS));
-		
-	    for (int j = 1; j <= i; j++) {
-		d.add(new Field("field", "crap",
-				Field.Store.YES, Field.Index.ANALYZED));
-		d.add(new Field("nonorm", "more words",
-				Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS));
-	    }
-	    writer.addDocument(d);
-	}
-	writer.close();
-    }
-    
-    public void testMissingField() {
-	FieldNormModifier fnm = new FieldNormModifier(store, s);
-	try {
-	    fnm.reSetNorms("nobodyherebutuschickens");
-	} catch (Exception e) {
-	    assertNull("caught something", e);
-	}
-    }
-	
-    public void testFieldWithNoNorm() throws Exception {
-
-	IndexReader r = IndexReader.open(store);
-	byte[] norms = r.norms("nonorm");
-
-	// sanity check, norms should all be 1
-	assertTrue("Whoops we have norms?", !r.hasNorms("nonorm"));
-        if (!r.getDisableFakeNorms()) {
-          for (int i = 0; i< norms.length; i++) {
-	    assertEquals(""+i, DEFAULT_NORM, norms[i]);
-          }
-        } else {
-          assertNull(norms);
-        }
-
-	r.close();
-	
-	FieldNormModifier fnm = new FieldNormModifier(store, s);
-	try {
-	    fnm.reSetNorms("nonorm");
-	} catch (Exception e) {
-	    assertNull("caught something", e);
-	}
-
-	// nothing should have changed
-	r = IndexReader.open(store);
-	
-	norms = r.norms("nonorm");
-	assertTrue("Whoops we have norms?", !r.hasNorms("nonorm"));
-        if (!r.getDisableFakeNorms()) {
-          for (int i = 0; i< norms.length; i++) {
-	    assertEquals(""+i, DEFAULT_NORM, norms[i]);
-          }
-        } else {
-          assertNull(norms);
-        }
-
-	r.close();
-	
-    }
-	
-    
-    public void testGoodCases() throws Exception {
-	
-	IndexSearcher searcher;
-	final float[] scores = new float[NUM_DOCS];
-	float lastScore = 0.0f;
-	
-	// default similarity should put docs with shorter length first
-  searcher = new IndexSearcher(store);
-  searcher.search(new TermQuery(new Term("field", "word")), new Collector() {
-    private int docBase = 0;
-    private Scorer scorer;
-    public final void collect(int doc) throws IOException {
-      scores[doc + docBase] = scorer.score();
-    }
-    public void setNextReader(IndexReader reader, int docBase) {
-      this.docBase = docBase;
-    }
-    public void setScorer(Scorer scorer) throws IOException {
-      this.scorer = scorer;
-    }
-    public boolean acceptsDocsOutOfOrder() {
-      return true;
-    }
-  });
-  searcher.close();
-	
-	lastScore = Float.MAX_VALUE;
-	for (int i = 0; i < NUM_DOCS; i++) {
-	    String msg = "i=" + i + ", "+scores[i]+" <= "+lastScore;
-	    assertTrue(msg, scores[i] <= lastScore);
-	    //System.out.println(msg);
-	    lastScore = scores[i];
-	}
-
-	// override the norms to be inverted
-	Similarity s = new DefaultSimilarity() {
-		public float lengthNorm(String fieldName, int numTokens) {
-		    return numTokens;
-		}
-	    };
-	FieldNormModifier fnm = new FieldNormModifier(store, s);
-	fnm.reSetNorms("field");
-
-	// new norm (with default similarity) should put longer docs first
-	searcher = new IndexSearcher(store);
-	searcher.search(new TermQuery(new Term("field", "word")), new Collector() {
-      private int docBase = 0;
-      private Scorer scorer;
-      public final void collect(int doc) throws IOException {
-        scores[doc + docBase] = scorer.score();
-      }
-      public void setNextReader(IndexReader reader, int docBase) {
-        this.docBase = docBase;
-      }
-      public void setScorer(Scorer scorer) throws IOException {
-        this.scorer = scorer;
-      }
-      public boolean acceptsDocsOutOfOrder() {
-        return true;
-      }
-    });
-    searcher.close();
-	
-	lastScore = 0.0f;
-	for (int i = 0; i < NUM_DOCS; i++) {
-	    String msg = "i=" + i + ", "+scores[i]+" >= "+lastScore;
-	    assertTrue(msg, scores[i] >= lastScore);
-	    //System.out.println(msg);
-	    lastScore = scores[i];
-	}
-	
-    }
-}
diff --git a/contrib/miscellaneous/src/test/org/apache/lucene/queryParser/analyzing/TestAnalyzingQueryParser.java b/contrib/miscellaneous/src/test/org/apache/lucene/queryParser/analyzing/TestAnalyzingQueryParser.java
deleted file mode 100644
index 027d26a..0000000
--- a/contrib/miscellaneous/src/test/org/apache/lucene/queryParser/analyzing/TestAnalyzingQueryParser.java
+++ /dev/null
@@ -1,118 +0,0 @@
-package org.apache.lucene.queryParser.analyzing;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Reader;
-
-import junit.framework.TestCase;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.ISOLatin1AccentFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.standard.StandardFilter;
-import org.apache.lucene.analysis.standard.StandardTokenizer;
-import org.apache.lucene.queryParser.ParseException;
-
-/**
- * @version $Revision$, $Date$
- */
-public class TestAnalyzingQueryParser extends TestCase {
-
-  private Analyzer a;
-
-  private String[] wildcardInput;
-  private String[] wildcardExpected;
-  private String[] prefixInput;
-  private String[] prefixExpected;
-  private String[] rangeInput;
-  private String[] rangeExpected;
-  private String[] fuzzyInput;
-  private String[] fuzzyExpected;
-
-  public void setUp() {
-    wildcardInput = new String[] { "bersetzung ber*ung",
-        "Mtley Cr\u00fce Mtl?* Cr?", "Rene Zellweger Ren?? Zellw?ger" };
-    wildcardExpected = new String[] { "ubersetzung uber*ung", "motley crue motl?* cru?",
-        "renee zellweger ren?? zellw?ger" };
-
-    prefixInput = new String[] { "bersetzung bersetz*",
-        "Mtley Cre Mtl* cr*", "Ren? Zellw*" };
-    prefixExpected = new String[] { "ubersetzung ubersetz*", "motley crue motl* cru*",
-        "rene? zellw*" };
-
-    rangeInput = new String[] { "[aa TO bb]", "{Anas TO Zo}" };
-    rangeExpected = new String[] { "[aa TO bb]", "{anais TO zoe}" };
-
-    fuzzyInput = new String[] { "?bersetzung ?bersetzung~0.9",
-        "Mtley Cre Mtley~0.75 Cre~0.5",
-        "Rene Zellweger Rene~0.9 Zellweger~" };
-    fuzzyExpected = new String[] { "ubersetzung ubersetzung~0.9",
-        "motley crue motley~0.75 crue~0.5", "renee zellweger renee~0.9 zellweger~0.5" };
-
-    a = new ASCIIAnalyzer();
-  }
-
-  public void testWildCardQuery() throws ParseException {
-    for (int i = 0; i < wildcardInput.length; i++) {
-      assertEquals("Testing wildcards with analyzer " + a.getClass() + ", input string: "
-          + wildcardInput[i], wildcardExpected[i], parseWithAnalyzingQueryParser(wildcardInput[i], a));
-    }
-  }
-
-  public void testPrefixQuery() throws ParseException {
-    for (int i = 0; i < prefixInput.length; i++) {
-      assertEquals("Testing prefixes with analyzer " + a.getClass() + ", input string: "
-          + prefixInput[i], prefixExpected[i], parseWithAnalyzingQueryParser(prefixInput[i], a));
-    }
-  }
-
-  public void testRangeQuery() throws ParseException {
-    for (int i = 0; i < rangeInput.length; i++) {
-      assertEquals("Testing ranges with analyzer " + a.getClass() + ", input string: "
-          + rangeInput[i], rangeExpected[i], parseWithAnalyzingQueryParser(rangeInput[i], a));
-    }
-  }
-
-  public void testFuzzyQuery() throws ParseException {
-    for (int i = 0; i < fuzzyInput.length; i++) {
-      assertEquals("Testing fuzzys with analyzer " + a.getClass() + ", input string: "
-          + fuzzyInput[i], fuzzyExpected[i], parseWithAnalyzingQueryParser(fuzzyInput[i], a));
-    }
-  }
-
-  private String parseWithAnalyzingQueryParser(String s, Analyzer a) throws ParseException {
-    AnalyzingQueryParser qp = new AnalyzingQueryParser("field", a);
-    org.apache.lucene.search.Query q = qp.parse(s);
-    return q.toString("field");
-  }
-
-}
-
-class ASCIIAnalyzer extends org.apache.lucene.analysis.Analyzer {
-  public ASCIIAnalyzer() {
-  }
-
-  public TokenStream tokenStream(String fieldName, Reader reader) {
-    TokenStream result = new StandardTokenizer(reader);
-    result = new StandardFilter(result);
-    result = new ISOLatin1AccentFilter(result);
-    result = new LowerCaseFilter(result);
-    return result;
-  }
-}
diff --git a/contrib/miscellaneous/src/test/org/apache/lucene/queryParser/complexPhrase/TestComplexPhraseQuery.java b/contrib/miscellaneous/src/test/org/apache/lucene/queryParser/complexPhrase/TestComplexPhraseQuery.java
deleted file mode 100644
index c5f75fc..0000000
--- a/contrib/miscellaneous/src/test/org/apache/lucene/queryParser/complexPhrase/TestComplexPhraseQuery.java
+++ /dev/null
@@ -1,144 +0,0 @@
-package org.apache.lucene.queryParser.complexPhrase;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.HashSet;
-
-import junit.framework.TestCase;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.standard.StandardAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriter.MaxFieldLength;
-import org.apache.lucene.queryParser.QueryParser;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.ScoreDoc;
-import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.store.RAMDirectory;
-
-public class TestComplexPhraseQuery extends TestCase {
-
-  Analyzer analyzer = new StandardAnalyzer();
-
-  DocData docsContent[] = { new DocData("john smith", "1"),
-      new DocData("johathon smith", "2"),
-      new DocData("john percival smith", "3"),
-      new DocData("jackson waits tom", "4") };
-
-  private IndexSearcher searcher;
-
-  String defaultFieldName = "name";
-
-  public void testComplexPhrases() throws Exception {
-    checkMatches("\"john smith\"", "1"); // Simple multi-term still works
-    checkMatches("\"j*   smyth~\"", "1,2"); // wildcards and fuzzies are OK in
-    // phrases
-    checkMatches("\"(jo* -john)  smith\"", "2"); // boolean logic works
-    checkMatches("\"jo*  smith\"~2", "1,2,3"); // position logic works.
-    checkMatches("\"jo* [sma TO smZ]\" ", "1,2"); // range queries supported
-    checkMatches("\"john\"", "1,3"); // Simple single-term still works
-    checkMatches("\"(john OR johathon)  smith\"", "1,2"); // boolean logic with
-    // brackets works.
-    checkMatches("\"(jo* -john) smyth~\"", "2"); // boolean logic with
-    // brackets works.
-
-    // checkMatches("\"john -percival\"", "1"); // not logic doesn't work
-    // currently :(.
-
-    checkMatches("\"john  nosuchword*\"", ""); // phrases with clauses producing
-    // empty sets
-
-    checkBadQuery("\"jo*  id:1 smith\""); // mixing fields in a phrase is bad
-    checkBadQuery("\"jo* \"smith\" \""); // phrases inside phrases is bad
-  }
-
-  private void checkBadQuery(String qString) {
-    QueryParser qp = new ComplexPhraseQueryParser(defaultFieldName, analyzer);
-    Throwable expected = null;
-    try {
-      qp.parse(qString);
-    } catch (Throwable e) {
-      expected = e;
-    }
-    assertNotNull("Expected parse error in " + qString, expected);
-
-  }
-
-  private void checkMatches(String qString, String expectedVals)
-      throws Exception {
-    QueryParser qp = new ComplexPhraseQueryParser(defaultFieldName, analyzer);
-    qp.setFuzzyPrefixLength(1); // usually a good idea
-
-    Query q = qp.parse(qString);
-
-    HashSet expecteds = new HashSet();
-    String[] vals = expectedVals.split(",");
-    for (int i = 0; i < vals.length; i++) {
-      if (vals[i].length() > 0)
-        expecteds.add(vals[i]);
-    }
-
-    TopDocs td = searcher.search(q, 10);
-    ScoreDoc[] sd = td.scoreDocs;
-    for (int i = 0; i < sd.length; i++) {
-      Document doc = searcher.doc(sd[i].doc);
-      String id = doc.get("id");
-      assertTrue(qString + "matched doc#" + id + " not expected", expecteds
-          .contains(id));
-      expecteds.remove(id);
-    }
-
-    assertEquals(qString + " missing some matches ", 0, expecteds.size());
-
-  }
-
-  protected void setUp() throws Exception {
-    RAMDirectory rd = new RAMDirectory();
-    IndexWriter w = new IndexWriter(rd, analyzer, MaxFieldLength.UNLIMITED);
-    for (int i = 0; i < docsContent.length; i++) {
-      Document doc = new Document();
-      doc.add(new Field("name", docsContent[i].name, Field.Store.YES,
-          Field.Index.ANALYZED));
-      doc.add(new Field("id", docsContent[i].id, Field.Store.YES,
-          Field.Index.ANALYZED));
-      w.addDocument(doc);
-    }
-    w.close();
-    searcher = new IndexSearcher(rd);
-  }
-
-  protected void tearDown() throws Exception {
-    searcher.close();
-  }
-
-  static class DocData {
-    String name;
-
-    String id;
-
-    public DocData(String name, String id) {
-      super();
-      this.name = name;
-      this.id = id;
-    }
-  }
-
-}
diff --git a/contrib/miscellaneous/src/test/org/apache/lucene/queryParser/precedence/TestPrecedenceQueryParser.java b/contrib/miscellaneous/src/test/org/apache/lucene/queryParser/precedence/TestPrecedenceQueryParser.java
deleted file mode 100644
index 5925354..0000000
--- a/contrib/miscellaneous/src/test/org/apache/lucene/queryParser/precedence/TestPrecedenceQueryParser.java
+++ /dev/null
@@ -1,599 +0,0 @@
-package org.apache.lucene.queryParser.precedence;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.LowerCaseTokenizer;
-import org.apache.lucene.analysis.SimpleAnalyzer;
-import org.apache.lucene.analysis.TokenFilter;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
-import org.apache.lucene.analysis.standard.StandardAnalyzer;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-import org.apache.lucene.document.DateTools;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.FuzzyQuery;
-import org.apache.lucene.search.PhraseQuery;
-import org.apache.lucene.search.PrefixQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.RangeQuery;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.WildcardQuery;
-import org.apache.lucene.util.LocalizedTestCase;
-
-import java.io.IOException;
-import java.io.Reader;
-import java.text.DateFormat;
-import java.util.Arrays;
-import java.util.Calendar;
-import java.util.GregorianCalendar;
-import java.util.HashSet;
-
-public class TestPrecedenceQueryParser extends LocalizedTestCase {
-  
-  public TestPrecedenceQueryParser(String name) {
-    super(name, new HashSet(Arrays.asList(new String[]{
-      "testDateRange", "testNumber"
-    })));
-  }
-
-  public static Analyzer qpAnalyzer = new QPTestAnalyzer();
-
-  public static class QPTestFilter extends TokenFilter {
-    /**
-     * Filter which discards the token 'stop' and which expands the
-     * token 'phrase' into 'phrase1 phrase2'
-     */
-    public QPTestFilter(TokenStream in) {
-      super(in);
-    }
-
-    boolean inPhrase = false;
-    int savedStart = 0, savedEnd = 0;
-
-    TermAttribute termAtt = (TermAttribute) addAttribute(TermAttribute.class);
-    OffsetAttribute offsetAtt = (OffsetAttribute) addAttribute(OffsetAttribute.class);
-    
-    public boolean incrementToken() throws IOException {
-      if (inPhrase) {
-        inPhrase = false;
-        termAtt.setTermBuffer("phrase2");
-        offsetAtt.setOffset(savedStart, savedEnd);
-        return true;
-      } else
-        while(input.incrementToken())
-          if (termAtt.term().equals("phrase")) {
-            inPhrase = true;
-            savedStart = offsetAtt.startOffset();
-            savedEnd = offsetAtt.endOffset();
-            termAtt.setTermBuffer("phrase1");
-            offsetAtt.setOffset(savedStart, savedEnd);
-            return true;
-          } else if (!termAtt.term().equals("stop"))
-            return true;
-      return false;
-    }
-  }
-
-  public static class QPTestAnalyzer extends Analyzer {
-
-    /** Filters LowerCaseTokenizer with StopFilter. */
-    public final TokenStream tokenStream(String fieldName, Reader reader) {
-      return new QPTestFilter(new LowerCaseTokenizer(reader));
-    }
-  }
-
-  public static class QPTestParser extends PrecedenceQueryParser {
-    public QPTestParser(String f, Analyzer a) {
-      super(f, a);
-    }
-
-    protected Query getFuzzyQuery(String field, String termStr, float minSimilarity) throws ParseException {
-      throw new ParseException("Fuzzy queries not allowed");
-    }
-
-    protected Query getWildcardQuery(String field, String termStr) throws ParseException {
-      throw new ParseException("Wildcard queries not allowed");
-    }
-  }
-
-  private int originalMaxClauses;
-
-  public void setUp() throws Exception {
-    super.setUp();
-    originalMaxClauses = BooleanQuery.getMaxClauseCount();
-  }
-
-  public PrecedenceQueryParser getParser(Analyzer a) throws Exception {
-    if (a == null)
-      a = new SimpleAnalyzer();
-    PrecedenceQueryParser qp = new PrecedenceQueryParser("field", a);
-    qp.setDefaultOperator(PrecedenceQueryParser.OR_OPERATOR);
-    return qp;
-  }
-
-  public Query getQuery(String query, Analyzer a) throws Exception {
-    return getParser(a).parse(query);
-  }
-
-  public void assertQueryEquals(String query, Analyzer a, String result)
-    throws Exception {
-    Query q = getQuery(query, a);
-    String s = q.toString("field");
-    if (!s.equals(result)) {
-      fail("Query /" + query + "/ yielded /" + s
-           + "/, expecting /" + result + "/");
-    }
-  }
-
-  public void assertWildcardQueryEquals(String query, boolean lowercase, String result)
-    throws Exception {
-    PrecedenceQueryParser qp = getParser(null);
-    qp.setLowercaseExpandedTerms(lowercase);
-    Query q = qp.parse(query);
-    String s = q.toString("field");
-    if (!s.equals(result)) {
-      fail("WildcardQuery /" + query + "/ yielded /" + s
-           + "/, expecting /" + result + "/");
-    }
-  }
-
-  public void assertWildcardQueryEquals(String query, String result) throws Exception {
-    PrecedenceQueryParser qp = getParser(null);
-    Query q = qp.parse(query);
-    String s = q.toString("field");
-    if (!s.equals(result)) {
-      fail("WildcardQuery /" + query + "/ yielded /" + s + "/, expecting /"
-          + result + "/");
-    }
-  }
-
-  public Query getQueryDOA(String query, Analyzer a)
-    throws Exception {
-    if (a == null)
-      a = new SimpleAnalyzer();
-    PrecedenceQueryParser qp = new PrecedenceQueryParser("field", a);
-    qp.setDefaultOperator(PrecedenceQueryParser.AND_OPERATOR);
-    return qp.parse(query);
-  }
-
-  public void assertQueryEqualsDOA(String query, Analyzer a, String result)
-    throws Exception {
-    Query q = getQueryDOA(query, a);
-    String s = q.toString("field");
-    if (!s.equals(result)) {
-      fail("Query /" + query + "/ yielded /" + s
-           + "/, expecting /" + result + "/");
-    }
-  }
-
-  // failing tests disabled since PrecedenceQueryParser
-  // is currently unmaintained
-  public void _testSimple() throws Exception {
-    assertQueryEquals("", null, "");
-
-    assertQueryEquals("term term term", null, "term term term");
-    assertQueryEquals("trm term term", null, "trm term term");
-    assertQueryEquals("mlaut", null, "mlaut");
-
-    assertQueryEquals("+a", null, "+a");
-    assertQueryEquals("-a", null, "-a");
-    assertQueryEquals("a AND b", null, "+a +b");
-    assertQueryEquals("(a AND b)", null, "+a +b");
-    assertQueryEquals("c OR (a AND b)", null, "c (+a +b)");
-    assertQueryEquals("a AND NOT b", null, "+a -b");
-    assertQueryEquals("a AND -b", null, "+a -b");
-    assertQueryEquals("a AND !b", null, "+a -b");
-    assertQueryEquals("a && b", null, "+a +b");
-    assertQueryEquals("a && ! b", null, "+a -b");
-
-    assertQueryEquals("a OR b", null, "a b");
-    assertQueryEquals("a || b", null, "a b");
-
-    assertQueryEquals("+term -term term", null, "+term -term term");
-    assertQueryEquals("foo:term AND field:anotherTerm", null,
-                      "+foo:term +anotherterm");
-    assertQueryEquals("term AND \"phrase phrase\"", null,
-                      "+term +\"phrase phrase\"");
-    assertQueryEquals("\"hello there\"", null, "\"hello there\"");
-    assertTrue(getQuery("a AND b", null) instanceof BooleanQuery);
-    assertTrue(getQuery("hello", null) instanceof TermQuery);
-    assertTrue(getQuery("\"hello there\"", null) instanceof PhraseQuery);
-
-    assertQueryEquals("germ term^2.0", null, "germ term^2.0");
-    assertQueryEquals("(term)^2.0", null, "term^2.0");
-    assertQueryEquals("(germ term)^2.0", null, "(germ term)^2.0");
-    assertQueryEquals("term^2.0", null, "term^2.0");
-    assertQueryEquals("term^2", null, "term^2.0");
-    assertQueryEquals("\"germ term\"^2.0", null, "\"germ term\"^2.0");
-    assertQueryEquals("\"term germ\"^2", null, "\"term germ\"^2.0");
-
-    assertQueryEquals("(foo OR bar) AND (baz OR boo)", null,
-                      "+(foo bar) +(baz boo)");
-    assertQueryEquals("((a OR b) AND NOT c) OR d", null,
-                      "(+(a b) -c) d");
-    assertQueryEquals("+(apple \"steve jobs\") -(foo bar baz)", null,
-                      "+(apple \"steve jobs\") -(foo bar baz)");
-    assertQueryEquals("+title:(dog OR cat) -author:\"bob dole\"", null,
-                      "+(title:dog title:cat) -author:\"bob dole\"");
-    
-    PrecedenceQueryParser qp = new PrecedenceQueryParser("field", new StandardAnalyzer());
-    // make sure OR is the default:
-    assertEquals(PrecedenceQueryParser.OR_OPERATOR, qp.getDefaultOperator());
-    qp.setDefaultOperator(PrecedenceQueryParser.AND_OPERATOR);
-    assertEquals(PrecedenceQueryParser.AND_OPERATOR, qp.getDefaultOperator());
-    qp.setDefaultOperator(PrecedenceQueryParser.OR_OPERATOR);
-    assertEquals(PrecedenceQueryParser.OR_OPERATOR, qp.getDefaultOperator());
-
-    assertQueryEquals("a OR !b", null, "a (-b)");
-    assertQueryEquals("a OR ! b", null, "a (-b)");
-    assertQueryEquals("a OR -b", null, "a (-b)");
-  }
-
-  public void testPunct() throws Exception {
-    Analyzer a = new WhitespaceAnalyzer();
-    assertQueryEquals("a&b", a, "a&b");
-    assertQueryEquals("a&&b", a, "a&&b");
-    assertQueryEquals(".NET", a, ".NET");
-  }
-
-  public void testSlop() throws Exception {
-    assertQueryEquals("\"term germ\"~2", null, "\"term germ\"~2");
-    assertQueryEquals("\"term germ\"~2 flork", null, "\"term germ\"~2 flork");
-    assertQueryEquals("\"term\"~2", null, "term");
-    assertQueryEquals("\" \"~2 germ", null, "germ");
-    assertQueryEquals("\"term germ\"~2^2", null, "\"term germ\"~2^2.0");
-  }
-
-  public void testNumber() throws Exception {
-// The numbers go away because SimpleAnalzyer ignores them
-    assertQueryEquals("3", null, "");
-    assertQueryEquals("term 1.0 1 2", null, "term");
-    assertQueryEquals("term term1 term2", null, "term term term");
-
-    Analyzer a = new StandardAnalyzer();
-    assertQueryEquals("3", a, "3");
-    assertQueryEquals("term 1.0 1 2", a, "term 1.0 1 2");
-    assertQueryEquals("term term1 term2", a, "term term1 term2");
-  }
-
-  // failing tests disabled since PrecedenceQueryParser
-  // is currently unmaintained
-  public void _testWildcard() throws Exception {
-    assertQueryEquals("term*", null, "term*");
-    assertQueryEquals("term*^2", null, "term*^2.0");
-    assertQueryEquals("term~", null, "term~0.5");
-    assertQueryEquals("term~0.7", null, "term~0.7");
-    assertQueryEquals("term~^2", null, "term^2.0~0.5");
-    assertQueryEquals("term^2~", null, "term^2.0~0.5");
-    assertQueryEquals("term*germ", null, "term*germ");
-    assertQueryEquals("term*germ^3", null, "term*germ^3.0");
-
-    assertTrue(getQuery("term*", null) instanceof PrefixQuery);
-    assertTrue(getQuery("term*^2", null) instanceof PrefixQuery);
-    assertTrue(getQuery("term~", null) instanceof FuzzyQuery);
-    assertTrue(getQuery("term~0.7", null) instanceof FuzzyQuery);
-    FuzzyQuery fq = (FuzzyQuery)getQuery("term~0.7", null);
-    assertEquals(0.7f, fq.getMinSimilarity(), 0.1f);
-    assertEquals(FuzzyQuery.defaultPrefixLength, fq.getPrefixLength());
-    fq = (FuzzyQuery)getQuery("term~", null);
-    assertEquals(0.5f, fq.getMinSimilarity(), 0.1f);
-    assertEquals(FuzzyQuery.defaultPrefixLength, fq.getPrefixLength());
-    try {
-      getQuery("term~1.1", null);   // value > 1, throws exception
-      fail();
-    } catch(ParseException pe) {
-      // expected exception
-    }
-    assertTrue(getQuery("term*germ", null) instanceof WildcardQuery);
-
-/* Tests to see that wild card terms are (or are not) properly
-	 * lower-cased with propery parser configuration
-	 */
-// First prefix queries:
-    // by default, convert to lowercase:
-    assertWildcardQueryEquals("Term*", true, "term*");
-    // explicitly set lowercase:
-    assertWildcardQueryEquals("term*", true, "term*");
-    assertWildcardQueryEquals("Term*", true, "term*");
-    assertWildcardQueryEquals("TERM*", true, "term*");
-    // explicitly disable lowercase conversion:
-    assertWildcardQueryEquals("term*", false, "term*");
-    assertWildcardQueryEquals("Term*", false, "Term*");
-    assertWildcardQueryEquals("TERM*", false, "TERM*");
-// Then 'full' wildcard queries:
-    // by default, convert to lowercase:
-    assertWildcardQueryEquals("Te?m", "te?m");
-    // explicitly set lowercase:
-    assertWildcardQueryEquals("te?m", true, "te?m");
-    assertWildcardQueryEquals("Te?m", true, "te?m");
-    assertWildcardQueryEquals("TE?M", true, "te?m");
-    assertWildcardQueryEquals("Te?m*gerM", true, "te?m*germ");
-    // explicitly disable lowercase conversion:
-    assertWildcardQueryEquals("te?m", false, "te?m");
-    assertWildcardQueryEquals("Te?m", false, "Te?m");
-    assertWildcardQueryEquals("TE?M", false, "TE?M");
-    assertWildcardQueryEquals("Te?m*gerM", false, "Te?m*gerM");
-//  Fuzzy queries:
-    assertWildcardQueryEquals("Term~", "term~0.5");
-    assertWildcardQueryEquals("Term~", true, "term~0.5");
-    assertWildcardQueryEquals("Term~", false, "Term~0.5");
-//  Range queries:
-    assertWildcardQueryEquals("[A TO C]", "[a TO c]");
-    assertWildcardQueryEquals("[A TO C]", true, "[a TO c]");
-    assertWildcardQueryEquals("[A TO C]", false, "[A TO C]");
-  }
-
-  public void testQPA() throws Exception {
-    assertQueryEquals("term term term", qpAnalyzer, "term term term");
-    assertQueryEquals("term +stop term", qpAnalyzer, "term term");
-    assertQueryEquals("term -stop term", qpAnalyzer, "term term");
-    assertQueryEquals("drop AND stop AND roll", qpAnalyzer, "+drop +roll");
-    assertQueryEquals("term phrase term", qpAnalyzer,
-                      "term \"phrase1 phrase2\" term");
-    // note the parens in this next assertion differ from the original
-    // QueryParser behavior
-    assertQueryEquals("term AND NOT phrase term", qpAnalyzer,
-                      "(+term -\"phrase1 phrase2\") term");
-    assertQueryEquals("stop", qpAnalyzer, "");
-    assertQueryEquals("stop OR stop AND stop", qpAnalyzer, "");
-    assertTrue(getQuery("term term term", qpAnalyzer) instanceof BooleanQuery);
-    assertTrue(getQuery("term +stop", qpAnalyzer) instanceof TermQuery);
-  }
-
-  public void testRange() throws Exception {
-    assertQueryEquals("[ a TO z]", null, "[a TO z]");
-    assertTrue(getQuery("[ a TO z]", null) instanceof RangeQuery);
-    assertQueryEquals("[ a TO z ]", null, "[a TO z]");
-    assertQueryEquals("{ a TO z}", null, "{a TO z}");
-    assertQueryEquals("{ a TO z }", null, "{a TO z}");
-    assertQueryEquals("{ a TO z }^2.0", null, "{a TO z}^2.0");
-    assertQueryEquals("[ a TO z] OR bar", null, "[a TO z] bar");
-    assertQueryEquals("[ a TO z] AND bar", null, "+[a TO z] +bar");
-    assertQueryEquals("( bar blar { a TO z}) ", null, "bar blar {a TO z}");
-    assertQueryEquals("gack ( bar blar { a TO z}) ", null, "gack (bar blar {a TO z})");
-  }
-  
-  private String escapeDateString(String s) {
-    if (s.contains(" ")) {
-      return "\"" + s + "\"";
-    } else {
-      return s;
-    }
-  }
-
-  public String getDate(String s) throws Exception {
-    DateFormat df = DateFormat.getDateInstance(DateFormat.SHORT);
-    return DateTools.dateToString(df.parse(s), DateTools.Resolution.DAY);
-  }
-
-  public String getLocalizedDate(int year, int month, int day) {
-    DateFormat df = DateFormat.getDateInstance(DateFormat.SHORT);
-    Calendar calendar = new GregorianCalendar();
-    calendar.set(year, month, day);
-    return df.format(calendar.getTime());
-  }
-
-  public void testDateRange() throws Exception {
-    String startDate = getLocalizedDate(2002, 1, 1);
-    String endDate = getLocalizedDate(2002, 1, 4);
-    assertQueryEquals("[ " + escapeDateString(startDate) + " TO " + escapeDateString(endDate) + "]", null,
-                      "[" + getDate(startDate) + " TO " + getDate(endDate) + "]");
-    assertQueryEquals("{  " + escapeDateString(startDate) + "    " + escapeDateString(endDate) + "   }", null,
-                      "{" + getDate(startDate) + " TO " + getDate(endDate) + "}");
-  }
-
-  public void testEscaped() throws Exception {
-    Analyzer a = new WhitespaceAnalyzer();
-    
-    /*assertQueryEquals("\\[brackets", a, "\\[brackets");
-    assertQueryEquals("\\[brackets", null, "brackets");
-    assertQueryEquals("\\\\", a, "\\\\");
-    assertQueryEquals("\\+blah", a, "\\+blah");
-    assertQueryEquals("\\(blah", a, "\\(blah");
-
-    assertQueryEquals("\\-blah", a, "\\-blah");
-    assertQueryEquals("\\!blah", a, "\\!blah");
-    assertQueryEquals("\\{blah", a, "\\{blah");
-    assertQueryEquals("\\}blah", a, "\\}blah");
-    assertQueryEquals("\\:blah", a, "\\:blah");
-    assertQueryEquals("\\^blah", a, "\\^blah");
-    assertQueryEquals("\\[blah", a, "\\[blah");
-    assertQueryEquals("\\]blah", a, "\\]blah");
-    assertQueryEquals("\\\"blah", a, "\\\"blah");
-    assertQueryEquals("\\(blah", a, "\\(blah");
-    assertQueryEquals("\\)blah", a, "\\)blah");
-    assertQueryEquals("\\~blah", a, "\\~blah");
-    assertQueryEquals("\\*blah", a, "\\*blah");
-    assertQueryEquals("\\?blah", a, "\\?blah");
-    //assertQueryEquals("foo \\&\\& bar", a, "foo \\&\\& bar");
-    //assertQueryEquals("foo \\|| bar", a, "foo \\|| bar");
-    //assertQueryEquals("foo \\AND bar", a, "foo \\AND bar");*/
-
-    assertQueryEquals("a\\-b:c", a, "a-b:c");
-    assertQueryEquals("a\\+b:c", a, "a+b:c");
-    assertQueryEquals("a\\:b:c", a, "a:b:c");
-    assertQueryEquals("a\\\\b:c", a, "a\\b:c");
-
-    assertQueryEquals("a:b\\-c", a, "a:b-c");
-    assertQueryEquals("a:b\\+c", a, "a:b+c");
-    assertQueryEquals("a:b\\:c", a, "a:b:c");
-    assertQueryEquals("a:b\\\\c", a, "a:b\\c");
-
-    assertQueryEquals("a:b\\-c*", a, "a:b-c*");
-    assertQueryEquals("a:b\\+c*", a, "a:b+c*");
-    assertQueryEquals("a:b\\:c*", a, "a:b:c*");
-
-    assertQueryEquals("a:b\\\\c*", a, "a:b\\c*");
-
-    assertQueryEquals("a:b\\-?c", a, "a:b-?c");
-    assertQueryEquals("a:b\\+?c", a, "a:b+?c");
-    assertQueryEquals("a:b\\:?c", a, "a:b:?c");
-
-    assertQueryEquals("a:b\\\\?c", a, "a:b\\?c");
-
-    assertQueryEquals("a:b\\-c~", a, "a:b-c~0.5");
-    assertQueryEquals("a:b\\+c~", a, "a:b+c~0.5");
-    assertQueryEquals("a:b\\:c~", a, "a:b:c~0.5");
-    assertQueryEquals("a:b\\\\c~", a, "a:b\\c~0.5");
-
-    assertQueryEquals("[ a\\- TO a\\+ ]", null, "[a- TO a+]");
-    assertQueryEquals("[ a\\: TO a\\~ ]", null, "[a: TO a~]");
-    assertQueryEquals("[ a\\\\ TO a\\* ]", null, "[a\\ TO a*]");
-  }
-
-  public void testTabNewlineCarriageReturn()
-    throws Exception {
-    assertQueryEqualsDOA("+weltbank +worlbank", null,
-      "+weltbank +worlbank");
-
-    assertQueryEqualsDOA("+weltbank\n+worlbank", null,
-      "+weltbank +worlbank");
-    assertQueryEqualsDOA("weltbank \n+worlbank", null,
-      "+weltbank +worlbank");
-    assertQueryEqualsDOA("weltbank \n +worlbank", null,
-      "+weltbank +worlbank");
-
-    assertQueryEqualsDOA("+weltbank\r+worlbank", null,
-      "+weltbank +worlbank");
-    assertQueryEqualsDOA("weltbank \r+worlbank", null,
-      "+weltbank +worlbank");
-    assertQueryEqualsDOA("weltbank \r +worlbank", null,
-      "+weltbank +worlbank");
-
-    assertQueryEqualsDOA("+weltbank\r\n+worlbank", null,
-      "+weltbank +worlbank");
-    assertQueryEqualsDOA("weltbank \r\n+worlbank", null,
-      "+weltbank +worlbank");
-    assertQueryEqualsDOA("weltbank \r\n +worlbank", null,
-      "+weltbank +worlbank");
-    assertQueryEqualsDOA("weltbank \r \n +worlbank", null,
-      "+weltbank +worlbank");
-
-    assertQueryEqualsDOA("+weltbank\t+worlbank", null,
-      "+weltbank +worlbank");
-    assertQueryEqualsDOA("weltbank \t+worlbank", null,
-      "+weltbank +worlbank");
-    assertQueryEqualsDOA("weltbank \t +worlbank", null,
-      "+weltbank +worlbank");
-  }
-
-  public void testSimpleDAO()
-    throws Exception {
-    assertQueryEqualsDOA("term term term", null, "+term +term +term");
-    assertQueryEqualsDOA("term +term term", null, "+term +term +term");
-    assertQueryEqualsDOA("term term +term", null, "+term +term +term");
-    assertQueryEqualsDOA("term +term +term", null, "+term +term +term");
-    assertQueryEqualsDOA("-term term term", null, "-term +term +term");
-  }
-
-  public void testBoost()
-    throws Exception {
-    StandardAnalyzer oneStopAnalyzer = new StandardAnalyzer(new String[]{"on"});
-    PrecedenceQueryParser qp = new PrecedenceQueryParser("field", oneStopAnalyzer);
-    Query q = qp.parse("on^1.0");
-    assertNotNull(q);
-    q = qp.parse("\"hello\"^2.0");
-    assertNotNull(q);
-    assertEquals(q.getBoost(), (float) 2.0, (float) 0.5);
-    q = qp.parse("hello^2.0");
-    assertNotNull(q);
-    assertEquals(q.getBoost(), (float) 2.0, (float) 0.5);
-    q = qp.parse("\"on\"^1.0");
-    assertNotNull(q);
-
-    q = getParser(new StandardAnalyzer()).parse("the^3");
-    assertNotNull(q);
-  }
-
-  public void testException() throws Exception {
-    try {
-      assertQueryEquals("\"some phrase", null, "abc");
-      fail("ParseException expected, not thrown");
-    } catch (ParseException expected) {
-    }
-  }
-
-  public void testCustomQueryParserWildcard() {
-    try {
-      new QPTestParser("contents", new WhitespaceAnalyzer()).parse("a?t");
-    } catch (ParseException expected) {
-      return;
-    }
-    fail("Wildcard queries should not be allowed");
-  }
-
-  public void testCustomQueryParserFuzzy() throws Exception {
-    try {
-      new QPTestParser("contents", new WhitespaceAnalyzer()).parse("xunit~");
-    } catch (ParseException expected) {
-      return;
-    }
-    fail("Fuzzy queries should not be allowed");
-  }
-
-  public void testBooleanQuery() throws Exception {
-    BooleanQuery.setMaxClauseCount(2);
-    try {
-      getParser(new WhitespaceAnalyzer()).parse("one two three");
-      fail("ParseException expected due to too many boolean clauses");
-    } catch (ParseException expected) {
-      // too many boolean clauses, so ParseException is expected
-    }
-  }
-
-  /**
-   * This test differs from the original QueryParser, showing how the
-   * precedence issue has been corrected.
-   */
-  // failing tests disabled since PrecedenceQueryParser
-  // is currently unmaintained
-  public void _testPrecedence() throws Exception {
-    PrecedenceQueryParser parser = getParser(new WhitespaceAnalyzer());
-    Query query1 = parser.parse("A AND B OR C AND D");
-    Query query2 = parser.parse("(A AND B) OR (C AND D)");
-    assertEquals(query1, query2);
-
-    query1 = parser.parse("A OR B C");
-    query2 = parser.parse("A B C");
-    assertEquals(query1, query2);
-
-    query1 = parser.parse("A AND B C");
-    query2 = parser.parse("(+A +B) C");
-    assertEquals(query1, query2);
-
-    query1 = parser.parse("A AND NOT B");
-    query2 = parser.parse("+A -B");
-    assertEquals(query1, query2);
-
-    query1 = parser.parse("A OR NOT B");
-    query2 = parser.parse("A -B");
-    assertEquals(query1, query2);
-
-    query1 = parser.parse("A OR NOT B AND C");
-    query2 = parser.parse("A (-B +C)");
-    assertEquals(query1, query2);
-  }
-
-
-  public void tearDown() {
-    BooleanQuery.setMaxClauseCount(originalMaxClauses);
-  }
-
-}
diff --git a/contrib/spatial/build.xml b/contrib/spatial/build.xml
index 060ea02..4de1d9c 100644
--- a/contrib/spatial/build.xml
+++ b/contrib/spatial/build.xml
@@ -41,7 +41,7 @@
 
   <target name="build-misc" unless="memory.jar.present">
     <echo>Misc building dependency ${misc.jar}</echo>
-    <ant antfile="../miscellaneous/build.xml" target="default" inheritall="false" dir="../miscellaneous" />
+    <ant antfile="../misc/build.xml" target="default" inheritall="false" dir="../misc" />
   </target>
   
 </project>

