GitDiffStart: df9c6a835f65fdc42ba43838d40a37f991ef025b | Thu Dec 3 06:27:44 2015 +0000
diff --git a/lucene/CHANGES.txt b/lucene/CHANGES.txt
index 23c128b..34eb106 100644
--- a/lucene/CHANGES.txt
+++ b/lucene/CHANGES.txt
@@ -52,6 +52,9 @@ New Features
 * LUCENE-6881: Cutover all BKD implementations to dimensional values
   (Mike McCandless)
 
+* LUCENE-6837: Add N-best output support to JapaneseTokenizer.
+  (Hiroharu Konno via Christian Moen)
+
 API Changes
 
 * LUCENE-3312: The API of oal.document was restructured to
diff --git a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseTokenizer.java b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseTokenizer.java
index 614c739..a1f9545 100644
--- a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseTokenizer.java
+++ b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseTokenizer.java
@@ -18,10 +18,13 @@ package org.apache.lucene.analysis.ja;
  */
 
 import java.io.IOException;
+import java.io.StringReader;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
+import java.util.Comparator;
 import java.util.EnumMap;
+import java.util.HashMap;
 import java.util.List;
 
 import org.apache.lucene.analysis.Tokenizer;
@@ -60,13 +63,13 @@ import org.apache.lucene.util.fst.FST;
  *       information for inflected forms.
  * </ul>
  * <p>
- * This tokenizer uses a rolling Viterbi search to find the 
- * least cost segmentation (path) of the incoming characters.  
+ * This tokenizer uses a rolling Viterbi search to find the
+ * least cost segmentation (path) of the incoming characters.
  * For tokens that appear to be compound (&gt; length 2 for all
  * Kanji, or &gt; length 7 for non-Kanji), we see if there is a
  * 2nd best segmentation of that token after applying
  * penalties to the long tokens.  If so, and the Mode is
- * {@link Mode#SEARCH}, we output the alternate segmentation 
+ * {@link Mode#SEARCH}, we output the alternate segmentation
  * as well.
  */
 public final class JapaneseTokenizer extends Tokenizer {
@@ -79,14 +82,14 @@ public final class JapaneseTokenizer extends Tokenizer {
     /**
      * Ordinary segmentation: no decomposition for compounds,
      */
-    NORMAL, 
+    NORMAL,
 
     /**
-     * Segmentation geared towards search: this includes a 
+     * Segmentation geared towards search: this includes a
      * decompounding process for long nouns, also including
      * the full compound token as a synonym.
      */
-    SEARCH, 
+    SEARCH,
 
     /**
      * Extended mode outputs unigrams for unknown words.
@@ -156,6 +159,10 @@ public final class JapaneseTokenizer extends Tokenizer {
   private final boolean searchMode;
   private final boolean extendedMode;
   private final boolean outputCompounds;
+  private boolean outputNBest = false;
+
+  // Allowable cost difference for N-best output:
+  private int nBestCost = 0;
 
   // Index of the last character of unknown word:
   private int unknownWordEndIndex = -1;
@@ -189,7 +196,7 @@ public final class JapaneseTokenizer extends Tokenizer {
    * Create a new JapaneseTokenizer.
    * <p>
    * Uses the default AttributeFactory.
-   * 
+   *
    * @param userDictionary Optional: if non-null, user dictionary.
    * @param discardPunctuation true if punctuation tokens should be dropped from the output.
    * @param mode tokenization mode.
@@ -490,6 +497,11 @@ public final class JapaneseTokenizer extends Tokenizer {
     if (token.getPosition() == lastTokenPos) {
       posIncAtt.setPositionIncrement(0);
       posLengthAtt.setPositionLength(token.getPositionLength());
+    } else if (outputNBest) {
+      // The position length is always calculated if outputNBest is true.
+      assert token.getPosition() > lastTokenPos;
+      posIncAtt.setPositionIncrement(1);
+      posLengthAtt.setPositionLength(token.getPositionLength());
     } else {
       assert token.getPosition() > lastTokenPos;
       posIncAtt.setPositionIncrement(1);
@@ -519,7 +531,7 @@ public final class JapaneseTokenizer extends Tokenizer {
 
     // Next position to write:
     private int nextPos;
-    
+
     // How many valid Position instances are held in the
     // positions array:
     private int count;
@@ -644,7 +656,13 @@ public final class JapaneseTokenizer extends Tokenizer {
         // alive, so whatever the eventual best path is must
         // come through this node.  So we can safely commit
         // to the prefix of the best path at this point:
+        if (outputNBest) {
+          backtraceNBest(posData, false);
+        }
         backtrace(posData, 0);
+        if (outputNBest) {
+          fixupPendingList();
+        }
 
         // Re-base cost so we don't risk int overflow:
         posData.costs[0] = 0;
@@ -689,6 +707,10 @@ public final class JapaneseTokenizer extends Tokenizer {
         // We will always have at least one live path:
         assert leastIDX != -1;
 
+        if (outputNBest) {
+          backtraceNBest(leastPosData, false);
+        }
+
         // Second pass: prune all but the best path:
         for(int pos2=pos;pos2<positions.getNextPos();pos2++) {
           final Position posData2 = positions.get(pos2);
@@ -708,6 +730,9 @@ public final class JapaneseTokenizer extends Tokenizer {
         }
 
         backtrace(leastPosData, 0);
+        if (outputNBest) {
+          fixupPendingList();
+        }
 
         // Re-base cost so we don't risk int overflow:
         Arrays.fill(leastPosData.costs, 0, leastPosData.count, 0);
@@ -775,7 +800,7 @@ public final class JapaneseTokenizer extends Tokenizer {
             break;
           }
           //System.out.println("    match " + (char) ch + " posAhead=" + posAhead);
-          
+
           if (fst.findTargetArc(ch, arc, arc, posAhead == posData.pos, fstReader) == null) {
             break;
           }
@@ -870,7 +895,13 @@ public final class JapaneseTokenizer extends Tokenizer {
         }
       }
 
+      if (outputNBest) {
+        backtraceNBest(endPosData, true);
+      }
       backtrace(endPosData, leastIDX);
+      if (outputNBest) {
+        fixupPendingList();
+      }
     } else {
       // No characters in the input string; return no tokens!
     }
@@ -947,7 +978,7 @@ public final class JapaneseTokenizer extends Tokenizer {
           final Dictionary dict2 = getDict(forwardType);
           final int wordID = posData.forwardID[forwardArcIDX];
           final int toPos = posData.forwardPos[forwardArcIDX];
-          final int newCost = pathCost + dict2.getWordCost(wordID) + 
+          final int newCost = pathCost + dict2.getWordCost(wordID) +
             costs.get(rightID, dict2.getLeftId(wordID)) +
             computePenalty(pos, toPos-pos);
           if (VERBOSE) {
@@ -981,6 +1012,557 @@ public final class JapaneseTokenizer extends Tokenizer {
     }
   }
 
+  // yet another lattice data structure
+  private final static class Lattice {
+    char[] fragment;
+    EnumMap<Type, Dictionary> dictionaryMap;
+    boolean useEOS;
+
+    int rootCapacity = 0;
+    int rootSize = 0;
+    int rootBase = 0;
+
+    // root pointers of node chain by leftChain_ that have same start offset.
+    int[] lRoot;
+    // root pointers of node chain by rightChain_ that have same end offset.
+    int[] rRoot;
+
+    int capacity = 0;
+    int nodeCount = 0;
+
+    // The variables below are elements of lattice node that indexed by node number.
+    Type[] nodeDicType;
+    int[] nodeWordID;
+    // nodeMark - -1:excluded, 0:unused, 1:bestpath, 2:2-best-path, ... N:N-best-path
+    int[] nodeMark;
+    int[] nodeLeftID;
+    int[] nodeRightID;
+    int[] nodeWordCost;
+    int[] nodeLeftCost;
+    int[] nodeRightCost;
+    // nodeLeftNode, nodeRightNode - are left/right node number with minimum cost path.
+    int[] nodeLeftNode;
+    int[] nodeRightNode;
+    // nodeLeft, nodeRight - start/end offset
+    int[] nodeLeft;
+    int[] nodeRight;
+    int[] nodeLeftChain;
+    int[] nodeRightChain;
+
+    private void setupRoot(int baseOffset, int lastOffset) {
+      assert baseOffset <= lastOffset;
+      int size = lastOffset - baseOffset + 1;
+      if (rootCapacity < size) {
+        int oversize = ArrayUtil.oversize(size, RamUsageEstimator.NUM_BYTES_INT);
+        lRoot = new int[oversize];
+        rRoot = new int[oversize];
+        rootCapacity = oversize;
+      }
+      Arrays.fill(lRoot, 0, size, -1);
+      Arrays.fill(rRoot, 0, size, -1);
+      rootSize = size;
+      rootBase = baseOffset;
+    }
+
+    // Reserve at least N nodes.
+    private void reserve(int n) {
+      if (capacity < n) {
+        int oversize = ArrayUtil.oversize(n, RamUsageEstimator.NUM_BYTES_INT);
+        nodeDicType = new Type[oversize];
+        nodeWordID = new int[oversize];
+        nodeMark = new int[oversize];
+        nodeLeftID = new int[oversize];
+        nodeRightID = new int[oversize];
+        nodeWordCost = new int[oversize];
+        nodeLeftCost = new int[oversize];
+        nodeRightCost = new int[oversize];
+        nodeLeftNode = new int[oversize];
+        nodeRightNode = new int[oversize];
+        nodeLeft = new int[oversize];
+        nodeRight = new int[oversize];
+        nodeLeftChain = new int[oversize];
+        nodeRightChain = new int[oversize];
+        capacity = oversize;
+      }
+    }
+
+    private void setupNodePool(int n) {
+      reserve(n);
+      nodeCount = 0;
+      if (VERBOSE) {
+        System.out.printf("DEBUG: setupNodePool: n = %d\n", n);
+        System.out.printf("DEBUG: setupNodePool: lattice.capacity = %d\n", capacity);
+      }
+    }
+
+    private int addNode(Type dicType, int wordID, int left, int right) {
+      if (VERBOSE) {
+        System.out.printf("DEBUG: addNode: dicType=%s, wordID=%d, left=%d, right=%d, str=%s\n",
+                          dicType.toString(), wordID, left, right,
+                          left == -1 ? "BOS" : right == -1 ? "EOS" : new String(fragment, left, right - left));
+      }
+      assert nodeCount < capacity;
+      assert left == -1 || right == -1 || left < right;
+      assert left == -1 || (0 <= left && left < rootSize);
+      assert right == -1 || (0 <= right && right < rootSize);
+
+      int node = nodeCount++;
+
+      if (VERBOSE) {
+        System.out.printf("DEBUG: addNode: node=%d\n", node);
+      }
+
+      nodeDicType[node] = dicType;
+      nodeWordID[node] = wordID;
+      nodeMark[node] = 0;
+
+      if (wordID < 0) {
+        nodeWordCost[node] = 0;
+        nodeLeftCost[node] = 0;
+        nodeRightCost[node] = 0;
+        nodeLeftID[node] = 0;
+        nodeRightID[node] = 0;
+      } else {
+        Dictionary dic = dictionaryMap.get(dicType);
+        nodeWordCost[node] = dic.getWordCost(wordID);
+        nodeLeftID[node] = dic.getLeftId(wordID);
+        nodeRightID[node] = dic.getRightId(wordID);
+      }
+
+      if (VERBOSE) {
+        System.out.printf("DEBUG: addNode: wordCost=%d, leftID=%d, rightID=%d\n",
+                          nodeWordCost[node], nodeLeftID[node], nodeRightID[node]);
+      }
+
+      nodeLeft[node] = left;
+      nodeRight[node] = right;
+      if (0 <= left) {
+        nodeLeftChain[node] = lRoot[left];
+        lRoot[left] = node;
+      } else {
+        nodeLeftChain[node] = -1;
+      }
+      if (0 <= right) {
+        nodeRightChain[node] = rRoot[right];
+        rRoot[right] = node;
+      } else {
+        nodeRightChain[node] = -1;
+      }
+      return node;
+    }
+
+    // Sum of positions.get(i).count in [beg, end) range.
+    // using stream:
+    //   return IntStream.range(beg, end).map(i -> positions.get(i).count).sum();
+    private int positionCount(WrappedPositionArray positions, int beg, int end) {
+      int count = 0;
+      for (int i = beg; i < end; ++i) {
+        count += positions.get(i).count;
+      }
+      return count;
+    }
+
+
+    void setup(char[] fragment,
+          EnumMap<Type, Dictionary> dictionaryMap,
+          WrappedPositionArray positions, int prevOffset, int endOffset, boolean useEOS) {
+      assert positions.get(prevOffset).count == 1;
+      if (VERBOSE) {
+        System.out.printf("DEBUG: setup: prevOffset=%d, endOffset=%d\n", prevOffset, endOffset);
+      }
+
+      this.fragment = fragment;
+      this.dictionaryMap = dictionaryMap;
+      this.useEOS = useEOS;
+
+      // Initialize lRoot and rRoot.
+      setupRoot(prevOffset, endOffset);
+
+      // "+ 2" for first/last record.
+      setupNodePool(positionCount(positions, prevOffset + 1, endOffset + 1) + 2);
+
+      // substitute for BOS = 0
+      Position first = positions.get(prevOffset);
+      if (addNode(first.backType[0], first.backID[0], -1, 0) != 0) {
+        assert false;
+      }
+
+      // EOS = 1
+      if (addNode(Type.KNOWN, -1, endOffset - rootBase, -1) != 1) {
+        assert false;
+      }
+
+      for (int offset = endOffset; prevOffset < offset; --offset) {
+        int right = offset - rootBase;
+        // optimize: exclude disconnected nodes.
+        if (0 <= lRoot[right]) {
+          Position pos = positions.get(offset);
+          for (int i = 0; i < pos.count; ++i) {
+            addNode(pos.backType[i], pos.backID[i], pos.backPos[i] - rootBase, right);
+          }
+        }
+      }
+    }
+
+    // set mark = -1 for unreachable nodes.
+    void markUnreachable() {
+      for (int index = 1; index < rootSize - 1; ++index) {
+        if (rRoot[index] < 0) {
+          for (int node = lRoot[index]; 0 <= node; node = nodeLeftChain[node]) {
+            if (VERBOSE) {
+              System.out.printf("DEBUG: markUnreachable: node=%d\n", node);
+            }
+            nodeMark[node] = -1;
+          }
+        }
+      }
+    }
+
+    int connectionCost(ConnectionCosts costs, int left, int right) {
+      int leftID = nodeLeftID[right];
+      return ((leftID == 0 && !useEOS) ? 0 : costs.get(nodeRightID[left], leftID));
+    }
+
+    void calcLeftCost(ConnectionCosts costs) {
+      for (int index = 0; index < rootSize; ++index) {
+        for (int node = lRoot[index]; 0 <= node; node = nodeLeftChain[node]) {
+          if (0 <= nodeMark[node]) {
+            int leastNode = -1;
+            int leastCost = Integer.MAX_VALUE;
+            for (int leftNode = rRoot[index]; 0 <= leftNode; leftNode = nodeRightChain[leftNode]) {
+              if (0 <= nodeMark[leftNode]) {
+                int cost = nodeLeftCost[leftNode] + nodeWordCost[leftNode] + connectionCost(costs, leftNode, node);
+                if (cost < leastCost) {
+                  leastCost = cost;
+                  leastNode = leftNode;
+                }
+              }
+            }
+            assert 0 <= leastNode;
+            nodeLeftNode[node] = leastNode;
+            nodeLeftCost[node] = leastCost;
+            if (VERBOSE) {
+              System.out.printf("DEBUG: calcLeftCost: node=%d, leftNode=%d, leftCost=%d\n",
+                                node, nodeLeftNode[node], nodeLeftCost[node]);
+            }
+          }
+        }
+      }
+    }
+
+    void calcRightCost(ConnectionCosts costs) {
+      for (int index = rootSize - 1; 0 <= index; --index) {
+        for (int node = rRoot[index]; 0 <= node; node = nodeRightChain[node]) {
+          if (0 <= nodeMark[node]) {
+            int leastNode = -1;
+            int leastCost = Integer.MAX_VALUE;
+            for (int rightNode = lRoot[index]; 0 <= rightNode; rightNode = nodeLeftChain[rightNode]) {
+              if (0 <= nodeMark[rightNode]) {
+                int cost = nodeRightCost[rightNode] + nodeWordCost[rightNode] + connectionCost(costs, node, rightNode);
+                if (cost < leastCost) {
+                  leastCost = cost;
+                  leastNode = rightNode;
+                }
+              }
+            }
+            assert 0 <= leastNode;
+            nodeRightNode[node] = leastNode;
+            nodeRightCost[node] = leastCost;
+            if (VERBOSE) {
+              System.out.printf("DEBUG: calcRightCost: node=%d, rightNode=%d, rightCost=%d\n",
+                                node, nodeRightNode[node], nodeRightCost[node]);
+            }
+          }
+        }
+      }
+    }
+
+    // Mark all nodes that have same text and different par-of-speech or reading.
+    void markSameSpanNode(int refNode, int value) {
+      int left = nodeLeft[refNode];
+      int right = nodeRight[refNode];
+      for (int node = lRoot[left]; 0 <= node; node = nodeLeftChain[node]) {
+        if (nodeRight[node] == right) {
+          nodeMark[node] = value;
+        }
+      }
+    }
+
+    List<Integer> bestPathNodeList() {
+      List<Integer> list = new ArrayList<>();
+      for (int node = nodeRightNode[0]; node != 1; node = nodeRightNode[node]) {
+        list.add(node);
+        markSameSpanNode(node, 1);
+      }
+      return list;
+    }
+
+    private int cost(int node) {
+      return nodeLeftCost[node] + nodeWordCost[node] + nodeRightCost[node];
+    }
+
+    List<Integer> nBestNodeList(int N) {
+      List<Integer> list = new ArrayList<>();
+      int leastCost = Integer.MAX_VALUE;
+      int leastLeft = -1;
+      int leastRight = -1;
+      for (int node = 2; node < nodeCount; ++node) {
+        if (nodeMark[node] == 0) {
+          int cost = cost(node);
+          if (cost < leastCost) {
+            leastCost = cost;
+            leastLeft = nodeLeft[node];
+            leastRight = nodeRight[node];
+            list.clear();
+            list.add(node);
+          } else if (cost == leastCost && (nodeLeft[node] != leastLeft || nodeRight[node] != leastRight)) {
+            list.add(node);
+          }
+        }
+      }
+      for (int node : list) {
+        markSameSpanNode(node, N);
+      }
+      return list;
+    }
+
+    int bestCost() {
+      return nodeLeftCost[1];
+    }
+
+    int probeDelta(int start, int end) {
+      int left = start - rootBase;
+      int right = end - rootBase;
+      if (left < 0 || rootSize < right) {
+        return Integer.MAX_VALUE;
+      }
+      int probedCost = Integer.MAX_VALUE;
+      for (int node = lRoot[left]; 0 <= node; node = nodeLeftChain[node]) {
+        if (nodeRight[node] == right) {
+          probedCost = Math.min(probedCost, cost(node));
+        }
+      }
+      return probedCost - bestCost();
+    }
+
+    void debugPrint() {
+      if (VERBOSE) {
+        for (int node = 0; node < nodeCount; ++node) {
+          System.out.printf("DEBUG NODE: node=%d, mark=%d, cost=%d, left=%d, right=%d\n",
+                            node, nodeMark[node], cost(node), nodeLeft[node], nodeRight[node]);
+        }
+      }
+    }
+  }
+
+  private Lattice lattice = null;
+
+  private void registerNode(int node, char[] fragment) {
+    int left = lattice.nodeLeft[node];
+    int right = lattice.nodeRight[node];
+    Type type = lattice.nodeDicType[node];
+    if (!discardPunctuation || !isPunctuation(fragment[left])) {
+      if (type == Type.USER) {
+        // The code below are based on backtrace().
+        //
+        // Expand the phraseID we recorded into the actual segmentation:
+        final int[] wordIDAndLength = userDictionary.lookupSegmentation(lattice.nodeWordID[node]);
+        int wordID = wordIDAndLength[0];
+        pending.add(new Token(wordID,
+                              fragment,
+                              left,
+                              right - left,
+                              Type.USER,
+                              lattice.rootBase + left,
+                              userDictionary));
+        // Output compound
+        int current = 0;
+        for (int j = 1; j < wordIDAndLength.length; j++) {
+          final int len = wordIDAndLength[j];
+          if (len < right - left) {
+            pending.add(new Token(wordID + j - 1,
+                                  fragment,
+                                  current + left,
+                                  len,
+                                  Type.USER,
+                                  lattice.rootBase + current + left,
+                                  userDictionary));
+          }
+          current += len;
+        }
+      } else {
+        pending.add(new Token(lattice.nodeWordID[node],
+                              fragment,
+                              left,
+                              right - left,
+                              type,
+                              lattice.rootBase + left,
+                              getDict(type)));
+      }
+    }
+  }
+
+  // Sort pending tokens, and set position increment values.
+  private void fixupPendingList() {
+    // Sort for removing same tokens.
+    // USER token should be ahead from normal one.
+    Collections.sort(pending,
+                     new Comparator<Token>() {
+                       @Override
+                       public int compare(Token a, Token b) {
+                         int aOff = a.getOffset();
+                         int bOff = b.getOffset();
+                         if (aOff != bOff) {
+                           return aOff - bOff;
+                         }
+                         int aLen = a.getLength();
+                         int bLen = b.getLength();
+                         if (aLen != bLen) {
+                           return aLen - bLen;
+                         }
+                         // order of Type is KNOWN, UNKNOWN, USER,
+                         // so we use reversed comparison here.
+                         return b.getType().ordinal() - a.getType().ordinal();
+                       }
+                     });
+
+    // Remove same token.
+    for (int i = 1; i < pending.size(); ++i) {
+      Token a = pending.get(i - 1);
+      Token b = pending.get(i);
+      if (a.getOffset() == b.getOffset() && a.getLength() == b.getLength()) {
+        pending.remove(i);
+        // It is important to decrement "i" here, because a next may be removed.
+        --i;
+      }
+    }
+
+    // offset=>position map
+    HashMap<Integer, Integer> map = new HashMap<>();
+    for (Token t : pending) {
+      map.put(t.getOffset(), 0);
+      map.put(t.getOffset() + t.getLength(), 0);
+    }
+
+    // Get uniqe and sorted list of all edge position of tokens.
+    Integer[] offsets = map.keySet().toArray(new Integer[0]);
+    Arrays.sort(offsets);
+
+    // setup all value of map.  It specify N-th position from begin.
+    for (int i = 0; i < offsets.length; ++i) {
+      map.put(offsets[i], i);
+    }
+
+    // We got all position length now.
+    for (Token t : pending) {
+      t.setPositionLength(map.get(t.getOffset() + t.getLength()) - map.get(t.getOffset()));
+    }
+
+    // Make PENDING to be reversed order to fit its usage.
+    // If you would like to speedup, you can try reversed order sort
+    // at first of this function.
+    Collections.reverse(pending);
+  }
+
+  private int probeDelta(String inText, String requiredToken) throws IOException {
+    int start = inText.indexOf(requiredToken);
+    if (start < 0) {
+      // -1 when no requiredToken.
+      return -1;
+    }
+
+    int delta = Integer.MAX_VALUE;
+    int saveNBestCost = nBestCost;
+    setReader(new StringReader(inText));
+    reset();
+    try {
+      setNBestCost(1);
+      int prevRootBase = -1;
+      while (incrementToken()) {
+        if (lattice.rootBase != prevRootBase) {
+          prevRootBase = lattice.rootBase;
+          delta = Math.min(delta, lattice.probeDelta(start, start + requiredToken.length()));
+        }
+      }
+    } finally {
+      // reset & end
+      end();
+      // setReader & close
+      close();
+      setNBestCost(saveNBestCost);
+    }
+
+    if (VERBOSE) {
+      System.out.printf("JapaneseTokenizer: delta = %d: %s-%s\n", delta, inText, requiredToken);
+    }
+    return delta == Integer.MAX_VALUE ? -1 : delta;
+  }
+
+  public int calcNBestCost(String examples) {
+    int maxDelta = 0;
+    for (String example : examples.split("/")) {
+      if (!example.isEmpty()) {
+        String[] pair = example.split("-");
+        if (pair.length != 2) {
+          throw new RuntimeException("Unexpected example form: " + example + " (expected two '-')");
+        } else {
+          try {
+            maxDelta = Math.max(maxDelta, probeDelta(pair[0], pair[1]));
+          } catch (IOException e) {
+            throw new RuntimeException("Internal error calculating best costs from examples. Got ", e);
+          }
+        }
+      }
+    }
+    return maxDelta;
+  }
+
+  public void setNBestCost(int value) {
+    nBestCost = value;
+    outputNBest = 0 < nBestCost;
+  }
+
+  private void backtraceNBest(final Position endPosData, final boolean useEOS) throws IOException {
+    if (lattice == null) {
+      lattice = new Lattice();
+    }
+
+    final int endPos = endPosData.pos;
+    char[] fragment = buffer.get(lastBackTracePos, endPos - lastBackTracePos);
+    lattice.setup(fragment, dictionaryMap, positions, lastBackTracePos, endPos, useEOS);
+    lattice.markUnreachable();
+    lattice.calcLeftCost(costs);
+    lattice.calcRightCost(costs);
+
+    int bestCost = lattice.bestCost();
+    if (VERBOSE) {
+      System.out.printf("DEBUG: 1-BEST COST: %d\n", bestCost);
+    }
+    for (int node : lattice.bestPathNodeList()) {
+      registerNode(node, fragment);
+    }
+
+    for (int n = 2;; ++n) {
+      List<Integer> nbest = lattice.nBestNodeList(n);
+      if (nbest.isEmpty()) {
+        break;
+      }
+      int cost = lattice.cost(nbest.get(0));
+      if (VERBOSE) {
+        System.out.printf("DEBUG: %d-BEST COST: %d\n", n, cost);
+      }
+      if (bestCost + nBestCost < cost) {
+        break;
+      }
+      for (int node : nbest) {
+        registerNode(node, fragment);
+      }
+    }
+    if (VERBOSE) {
+      lattice.debugPrint();
+    }
+  }
+
   // Backtrace from the provided position, back to the last
   // time we back-traced, accumulating the resulting tokens to
   // the pending list.  The pending list is then in-reverse
@@ -1029,7 +1611,7 @@ public final class JapaneseTokenizer extends Tokenizer {
       int nextBestIDX = posData.backIndex[bestIDX];
 
       if (outputCompounds && searchMode && altToken == null && backType != Type.USER) {
-        
+
         // In searchMode, if best path had picked a too-long
         // token, we use the "penalty" to compute the allowed
         // max cost of an alternate back-trace.  If we find an
@@ -1039,7 +1621,7 @@ public final class JapaneseTokenizer extends Tokenizer {
         //System.out.println("    2nd best backPos=" + backPos + " pos=" + pos);
 
         final int penalty = computeSecondBestThreshold(backPos, pos-backPos);
-        
+
         if (penalty > 0) {
           if (VERBOSE) {
             System.out.println("  compound=" + new String(buffer.get(backPos, pos-backPos)) + " backPos=" + backPos + " pos=" + pos + " penalty=" + penalty + " cost=" + posData.costs[bestIDX] + " bestIDX=" + bestIDX + " lastLeftID=" + lastLeftWordID);
@@ -1063,7 +1645,7 @@ public final class JapaneseTokenizer extends Tokenizer {
           for(int idx=0;idx<posData.count;idx++) {
             int cost = posData.costs[idx];
             //System.out.println("    idx=" + idx + " prevCost=" + cost);
-            
+
             if (lastLeftWordID != -1) {
               cost += costs.get(getDict(posData.backType[idx]).getRightId(posData.backID[idx]),
                                 lastLeftWordID);
@@ -1108,7 +1690,7 @@ public final class JapaneseTokenizer extends Tokenizer {
             backID = posData.backID[bestIDX];
             backCount = 0;
             //System.out.println("  do alt token!");
-            
+
           } else {
             // I think in theory it's possible there is no
             // 2nd best path, which is fine; in this case we
@@ -1212,7 +1794,7 @@ public final class JapaneseTokenizer extends Tokenizer {
             }
           }
           backCount += unigramTokenCount;
-          
+
         } else if (!discardPunctuation || length == 0 || !isPunctuation(fragment[offset])) {
           pending.add(new Token(backID,
                                 fragment,
diff --git a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseTokenizerFactory.java b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseTokenizerFactory.java
index 13a2de5..6afd54c 100644
--- a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseTokenizerFactory.java
+++ b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseTokenizerFactory.java
@@ -50,23 +50,66 @@ import org.apache.lucene.analysis.util.ResourceLoaderAware;
  *   &lt;/analyzer&gt;
  * &lt;/fieldType&gt;
  * </pre>
+ * <p>
+ * Additional expert user parameters nBestCost and nBestExamples can be
+ * used to include additional searchable tokens that those most likely
+ * according to the statistical model. A typical use-case for this is to
+ * improve recall and make segmentation more resilient to mistakes.
+ * The feature can also be used to get a decompounding effect.
+ * <p>
+ * The nBestCost parameter specifies an additional Viterbi cost, and
+ * when used, JapaneseTokenizer will include all tokens in Viterbi paths
+ * that are within the nBestCost value of the best path.
+ * <p>
+ * Finding a good value for nBestCost can be difficult to do by hand. The
+ * nBestExamples parameter can be used to find an nBestCost value based on
+ * examples with desired segmentation outcomes.
+ * <p>
+ * For example, a value of /ç®±æ?å±?-ç®±æ?/???ç©ºæ¸¯-???/ indicates that in
+ * the texts, ç®±æ?å±? (Mt. Hakone) and ???ç©ºæ¸¯ (Narita Airport) we'd like
+ * a cost that gives is us ç®±æ? (Hakone) and ??? (Narita). Notice that
+ * costs are estimated for each example individually, and the maximum
+ * nBestCost found across all examples is used.
+ * <p>
+ * If both nBestCost and nBestExamples is used in a configuration,
+ * the largest value of the two is used.
+ * <p>
+ * Parameters nBestCost and nBestExamples work with all tokenizer
+ * modes, but it makes the most sense to use them with NORMAL mode.
  */
 public class JapaneseTokenizerFactory extends TokenizerFactory implements ResourceLoaderAware {
   private static final String MODE = "mode";
-  
+
   private static final String USER_DICT_PATH = "userDictionary";
-  
+
   private static final String USER_DICT_ENCODING = "userDictionaryEncoding";
 
   private static final String DISCARD_PUNCTUATION = "discardPunctuation"; // Expert option
 
+  private static final String NBEST_COST = "nBestCost";
+
+  private static final String NBEST_EXAMPLES = "nBestExamples";
+
   private UserDictionary userDictionary;
 
   private final Mode mode;
-  private final boolean discardPunctuation;  
+  private final boolean discardPunctuation;
   private final String userDictionaryPath;
   private final String userDictionaryEncoding;
 
+  /* Example string for NBEST output.
+   * its form as:
+   *   nbestExamples := [ / ] example [ / example ]... [ / ]
+   *   example := TEXT - TOKEN
+   *   TEXT := input text
+   *   TOKEN := token should be in nbest result
+   * Ex. /ç®±æ?å±?-ç®±æ?/???ç©ºæ¸¯-???/
+   * When the result tokens are "ç®±æ?å±?", "???ç©ºæ¸¯" in NORMAL mode,
+   * /ç®±æ?å±?-ç®±æ?/???ç©ºæ¸¯-???/ requests "ç®±æ?" and "???" to be in the result in NBEST output.
+   */
+  private final String nbestExamples;
+  private int nbestCost = -1;
+
   /** Creates a new JapaneseTokenizerFactory */
   public JapaneseTokenizerFactory(Map<String,String> args) {
     super(args);
@@ -74,11 +117,13 @@ public class JapaneseTokenizerFactory extends TokenizerFactory implements Resour
     userDictionaryPath = args.remove(USER_DICT_PATH);
     userDictionaryEncoding = args.remove(USER_DICT_ENCODING);
     discardPunctuation = getBoolean(args, DISCARD_PUNCTUATION, true);
+    nbestCost = getInt(args, NBEST_COST, 0);
+    nbestExamples = args.remove(NBEST_EXAMPLES);
     if (!args.isEmpty()) {
       throw new IllegalArgumentException("Unknown parameters: " + args);
     }
   }
-  
+
   @Override
   public void inform(ResourceLoader loader) throws IOException {
     if (userDictionaryPath != null) {
@@ -96,9 +141,14 @@ public class JapaneseTokenizerFactory extends TokenizerFactory implements Resour
       userDictionary = null;
     }
   }
-  
+
   @Override
   public JapaneseTokenizer create(AttributeFactory factory) {
-    return new JapaneseTokenizer(factory, userDictionary, discardPunctuation, mode);
+    JapaneseTokenizer t = new JapaneseTokenizer(factory, userDictionary, discardPunctuation, mode);
+    if (nbestExamples != null) {
+      nbestCost = Math.max(nbestCost, t.calcNBestCost(nbestExamples));
+    }
+    t.setNBestCost(nbestCost);
+    return t;
   }
 }
diff --git a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/Token.java b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/Token.java
index ad3bcc0..81a10c8 100644
--- a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/Token.java
+++ b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/Token.java
@@ -25,18 +25,18 @@ import org.apache.lucene.analysis.ja.dict.Dictionary;
  */
 public class Token {
   private final Dictionary dictionary;
-  
+
   private final int wordId;
-  
+
   private final char[] surfaceForm;
   private final int offset;
   private final int length;
-  
+
   private final int position;
   private int positionLength;
-  
+
   private final Type type;
-  
+
   public Token(int wordId, char[] surfaceForm, int offset, int length, Type type, int position, Dictionary dictionary) {
     this.wordId = wordId;
     this.surfaceForm = surfaceForm;
@@ -53,77 +53,85 @@ public class Token {
       " posLen=" + positionLength + " type=" + type + " wordId=" + wordId +
       " leftID=" + dictionary.getLeftId(wordId) + ")";
   }
-  
+
   /**
    * @return surfaceForm
    */
   public char[] getSurfaceForm() {
     return surfaceForm;
   }
-  
+
   /**
    * @return offset into surfaceForm
    */
   public int getOffset() {
     return offset;
   }
-  
+
   /**
    * @return length of surfaceForm
    */
   public int getLength() {
     return length;
   }
-  
+
   /**
    * @return surfaceForm as a String
    */
   public String getSurfaceFormString() {
     return new String(surfaceForm, offset, length);
   }
-  
+
   /**
    * @return reading. null if token doesn't have reading.
    */
   public String getReading() {
     return dictionary.getReading(wordId, surfaceForm, offset, length);
   }
-  
+
   /**
    * @return pronunciation. null if token doesn't have pronunciation.
    */
   public String getPronunciation() {
     return dictionary.getPronunciation(wordId, surfaceForm, offset, length);
   }
-  
+
   /**
    * @return part of speech.
    */
   public String getPartOfSpeech() {
     return dictionary.getPartOfSpeech(wordId);
   }
-  
+
   /**
    * @return inflection type or null
    */
   public String getInflectionType() {
     return dictionary.getInflectionType(wordId);
   }
-  
+
   /**
    * @return inflection form or null
    */
   public String getInflectionForm() {
     return dictionary.getInflectionForm(wordId);
   }
-  
+
   /**
    * @return base form or null if token is not inflected
    */
   public String getBaseForm() {
     return dictionary.getBaseForm(wordId, surfaceForm, offset, length);
   }
-  
+
+  /**
+   * Returns the type of this token
+   * @return token type, not null
+   */
+  public Type getType() {
+    return type;
+  }
+
   /**
    * Returns true if this token is known word
    * @return true if this token is in standard dictionary. false if not.
@@ -131,7 +139,7 @@ public class Token {
   public boolean isKnown() {
     return type == Type.KNOWN;
   }
-  
+
   /**
    * Returns true if this token is unknown word
    * @return true if this token is unknown word. false if not.
@@ -139,7 +147,7 @@ public class Token {
   public boolean isUnknown() {
     return type == Type.UNKNOWN;
   }
-  
+
   /**
    * Returns true if this token is defined in user dictionary
    * @return true if this token is in user dictionary. false if not.
@@ -147,7 +155,7 @@ public class Token {
   public boolean isUser() {
     return type == Type.USER;
   }
-  
+
   /**
    * Get index of this token in input text
    * @return position of token
@@ -163,7 +171,7 @@ public class Token {
   public void setPositionLength(int positionLength) {
     this.positionLength = positionLength;
   }
-  
+
   /**
    * Get the length (in tokens) of this token.  For normal
    * tokens this is 1; for compound tokens it's &gt; 1.
diff --git a/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizer.java b/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizer.java
index ba172a0..a707dc7 100644
--- a/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizer.java
+++ b/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizer.java
@@ -24,6 +24,7 @@ import java.io.LineNumberReader;
 import java.io.Reader;
 import java.io.StringReader;
 import java.nio.charset.StandardCharsets;
+import java.util.ArrayList;
 import java.util.Random;
 
 import org.apache.lucene.analysis.Analyzer;
@@ -59,9 +60,22 @@ public class
       throw new RuntimeException(ioe);
     }
   }
-  
-  private Analyzer analyzer, analyzerNormal, analyzerNoPunct, extendedModeAnalyzerNoPunct;
-  
+
+  private Analyzer analyzer, analyzerNormal, analyzerNormalNBest, analyzerNoPunct, extendedModeAnalyzerNoPunct;
+
+  private JapaneseTokenizer makeTokenizer(boolean discardPunctuation, Mode mode) {
+    return new JapaneseTokenizer(newAttributeFactory(), readDict(), discardPunctuation, mode);
+  }
+
+  private Analyzer makeAnalyzer(final Tokenizer t) {
+    return new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        return new TokenStreamComponents(t, t);
+      }
+    };
+  }
+
   @Override
   public void setUp() throws Exception {
     super.setUp();
@@ -79,6 +93,14 @@ public class
         return new TokenStreamComponents(tokenizer, tokenizer);
       }
     };
+    analyzerNormalNBest = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        JapaneseTokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), readDict(), false, Mode.NORMAL);
+        tokenizer.setNBestCost(2000);
+        return new TokenStreamComponents(tokenizer, tokenizer);
+      }
+    };
     analyzerNoPunct = new Analyzer() {
       @Override
       protected TokenStreamComponents createComponents(String fieldName) {
@@ -94,7 +116,7 @@ public class
       }
     };
   }
-  
+
   @Override
   public void tearDown() throws Exception {
     IOUtils.close(analyzer, analyzerNormal, analyzerNoPunct, extendedModeAnalyzerNoPunct);
@@ -107,14 +129,109 @@ public class
                      new String[] {"?·ã??????????????³ã????"});
   }
 
+  public void testNormalModeNbest() throws Exception {
+    JapaneseTokenizer t = makeTokenizer(true, Mode.NORMAL);
+    Analyzer a = makeAnalyzer(t);
+
+    t.setNBestCost(2000);
+    assertAnalyzesTo(a,
+                     "?·ã??????????????³ã????",
+                     new String[] {"?·ã???", "?·ã??????????????³ã????", "?½ã?????§ã?", "?¨ã??¸ã???"});
+
+    t.setNBestCost(5000);
+    assertAnalyzesTo(a,
+                     "?·ã??????????????³ã????",
+                     new String[] {"?·ã???", "?·ã??????????????³ã????", "?½ã???", "?½ã?????§ã?", "?????", "?¨ã??¸ã???"});
+
+    t.setNBestCost(0);
+    assertAnalyzesTo(a,
+                     "?°å??¨é?è°·å?",
+                     new String[] {"?°å?", "?¨é?", "è°·å?"});
+
+    t.setNBestCost(3000);
+    assertAnalyzesTo(a,
+                     "?°å??¨é?è°·å?",
+                     new String[] {"?°å?", "??", "?¨é?", "?·è°·å·?", "è°·å?"});
+
+    t.setNBestCost(0);
+    assertAnalyzesTo(a,
+                     "çµ??å­????",
+                     new String[] {"çµ??", "å­?", "?¨é?"});
+
+    t.setNBestCost(2000);
+    assertAnalyzesTo(a,
+                     "çµ??å­????",
+                     new String[] {"çµ??", "çµ??å­??", "å­?", "?¨é?", "??"});
+
+    t.setNBestCost(0);
+    assertAnalyzesTo(a,
+                     "???ç©ºæ¸¯??±³??²¹æµ??",
+                     new String[] {"???ç©ºæ¸¯", "ç±?", "??²¹", "æµ??"});
+
+    t.setNBestCost(4000);
+    assertAnalyzesTo(a,
+                     "???ç©ºæ¸¯??±³??²¹æµ??",
+                     new String[] {"???ç©ºæ¸¯", "ç±?", "ç±³å?", "??²¹", "æ²?", "æµ??"});
+  }
+
+  public void testSearchModeNbest() throws Exception {
+    JapaneseTokenizer t = makeTokenizer(true, Mode.SEARCH);
+    Analyzer a = makeAnalyzer(t);
+
+    t.setNBestCost(0);
+    assertAnalyzesTo(a,
+                     "???ç©ºæ¸¯??±³??²¹æµ??",
+                     new String[] {"???", "???ç©ºæ¸¯", "ç©ºæ¸¯", "ç±?", "??²¹", "æµ??"});
+
+    t.setNBestCost(4000);
+    assertAnalyzesTo(a,
+                     "???ç©ºæ¸¯??±³??²¹æµ??",
+                     new String[] {"???", "???ç©ºæ¸¯", "ç©ºæ¸¯", "ç±?", "ç±³å?", "??²¹", "æ²?", "æµ??"});
+  }
+
+  private ArrayList<String> makeTokenList(Analyzer a, String in) throws Exception {
+    ArrayList<String> list = new ArrayList<>();
+    TokenStream ts = a.tokenStream("dummy", in);
+    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);
+
+    ts.reset();
+    while (ts.incrementToken()) {
+      list.add(termAtt.toString());
+    }
+    ts.end();
+    ts.close();
+    return list;
+  }
+
+  private boolean checkToken(Analyzer a, String in, String requitedToken) throws Exception {
+    return makeTokenList(a, in).indexOf(requitedToken) != -1;
+  }
+
+  public void testNBestCost() throws Exception {
+    JapaneseTokenizer t = makeTokenizer(true, Mode.NORMAL);
+    Analyzer a = makeAnalyzer(t);
+
+    t.setNBestCost(0);
+    assertFalse("å­?? is not a token of ?°å??¨é?è°·å?", checkToken(a, "?°å??¨é?è°·å?", "å­??"));
+
+    assertTrue("cost calculated /?°å??¨é?è°·å?-å­??/", 0 <= t.calcNBestCost("/?°å??¨é?è°·å?-å­??/"));
+    t.setNBestCost(t.calcNBestCost("/?°å??¨é?è°·å?-å­??/"));
+    assertTrue("å­?? is a token of ?°å??¨é?è°·å?", checkToken(a, "?°å??¨é?è°·å?", "å­??"));
+
+    assertTrue("cost calculated /?°å??¨é?è°·å?-??/???ç©ºæ¸¯-??/", 0 <= t.calcNBestCost("/?°å??¨é?è°·å?-??/???ç©ºæ¸¯-??/"));
+    t.setNBestCost(t.calcNBestCost("/?°å??¨é?è°·å?-??/???ç©ºæ¸¯-??/"));
+    assertTrue("?? is a token of ?°å??¨é?è°·å?", checkToken(a, "?°å??¨é?è°·å?", "??"));
+    assertTrue("?? is a token of ???ç©ºæ¸¯", checkToken(a, "???ç©ºæ¸¯", "??"));
+  }
+
   public void testDecomposition1() throws Exception {
     assertAnalyzesTo(analyzerNoPunct, "???????²§?°å±¤??¥³?§ã?å­????????è­·ã?????????????è¨??????¶åº¦?§ã?????" +
                          "??????ä½??å¾????????©å?åº??????¥ã?????????ç®??ç´?????ï¼????ºº??²»??????????",
-     new String[] { "???", "??",  "è²§å?", "å±?", "??", "å¥³æ??", "??", "å­??", "??", "?»ç?", "ä¿??", "??",      
-                    "???", "???", "???", "??", "?µè¨­", "??", "??", "??", "?¶åº¦", "??", "???",  "??????", 
+     new String[] { "???", "??",  "è²§å?", "å±?", "??", "å¥³æ??", "??", "å­??", "??", "?»ç?", "ä¿??", "??",
+                    "???", "???", "???", "??", "?µè¨­", "??", "??", "??", "?¶åº¦", "??", "???",  "??????",
                     "ä½?", "??å¾?", "??", "?»ç?", "?´å?", "?¶åº¦", "??",  "ä»??", "??", "??",  "???",
                     "äº??", "??", "ç´?", "ï¼?", "???", "ï¼?", "??", "??ºº", "??", "è²»ã???", "??", "???" },
-     new int[] { 0, 2, 4, 6, 7,  8, 10, 11, 13, 14, 16, 18, 19, 21, 23, 25, 26, 28, 29, 30, 
+     new int[] { 0, 2, 4, 6, 7,  8, 10, 11, 13, 14, 16, 18, 19, 21, 23, 25, 26, 28, 29, 30,
                  31, 33, 34, 37, 41, 42, 44, 45, 47, 49, 51, 53, 55, 56, 58, 60,
                  62, 63, 64, 65, 67, 68, 69, 71, 72, 75, 76 },
      new int[] { 2, 3, 6, 7, 8, 10, 11, 13, 14, 16, 18, 19, 21, 23, 25, 26, 28, 29, 30, 31,
@@ -122,7 +239,7 @@ public class
                  63, 64, 65, 67, 68, 69, 71, 72, 75, 76, 78 }
     );
   }
-  
+
   public void testDecomposition2() throws Exception {
     assertAnalyzesTo(analyzerNoPunct, "éº»è????å£²ã??¹ã????çµ¶ã????????°ã??????",
       new String[] { "éº»è?", "??", "å¯?£²", "??", "?¹ã????", "çµ¶ã???", "?????", "??", "???", "???" },
@@ -130,7 +247,7 @@ public class
       new int[] { 2, 3, 5, 6, 10, 13, 16, 17, 19, 21 }
     );
   }
-  
+
   public void testDecomposition3() throws Exception {
     assertAnalyzesTo(analyzerNoPunct, "é­?¥³?©å¤§å°???·ã??¼ã????????¹ã??",
       new String[] { "é­?¥³", "??", "å¤§å?", "????¥ã?",  "????????" },
@@ -154,7 +271,7 @@ public class
     try (TokenStream ts = analyzer.tokenStream("bogus", "????????????????????????????????????????????????????????????")) {
       ts.reset();
       while (ts.incrementToken()) {
-      
+
       }
       ts.end();
     }
@@ -196,13 +313,15 @@ public class
   public void testRandomStrings() throws Exception {
     checkRandomData(random(), analyzer, 500*RANDOM_MULTIPLIER);
     checkRandomData(random(), analyzerNoPunct, 500*RANDOM_MULTIPLIER);
+    checkRandomData(random(), analyzerNormalNBest, 500*RANDOM_MULTIPLIER);
   }
-  
+
   /** blast some random large strings through the analyzer */
   public void testRandomHugeStrings() throws Exception {
     Random random = random();
     checkRandomData(random, analyzer, 20*RANDOM_MULTIPLIER, 8192);
     checkRandomData(random, analyzerNoPunct, 20*RANDOM_MULTIPLIER, 8192);
+    checkRandomData(random, analyzerNormalNBest, 20*RANDOM_MULTIPLIER, 8192);
   }
 
   public void testRandomHugeStringsMockGraphAfter() throws Exception {
@@ -231,13 +350,13 @@ public class
       }
     }
   }
-  
+
   /** simple test for supplementary characters */
   public void testSurrogates() throws IOException {
     assertAnalyzesTo(analyzer, "ð©??±é??¹æ???",
       new String[] { "ð©?", "??", "??", "??", "??", "??" });
   }
-  
+
   /** random test ensuring we don't ever split supplementaries */
   public void testSurrogates2() throws IOException {
     int numIterations = atLeast(10000);
@@ -272,7 +391,7 @@ public class
       ts.end();
     }
   }
-  
+
   // note: test is kinda silly since kuromoji emits punctuation tokens.
   // but, when/if we filter these out it will be useful.
   public void testEnd() throws Exception {
@@ -338,7 +457,7 @@ public class
     );
   }
   */
-  
+
   public void testSegmentation() throws Exception {
     // Skip tests for Michelle Kwan -- UniDic segments Kwan as ?? ???
     //   String input = "????§ã??»ã???????????¾ã??????????¹ã?????·ã??³ã?è¡???¾ã??????????????";
@@ -376,7 +495,7 @@ public class
     assertAnalyzesTo(analyzer,
                      input,
                      surfaceForms);
-    
+
     assertTrue(gv2.finish().indexOf("22.0") != -1);
     analyzer.close();
   }
@@ -406,7 +525,7 @@ public class
       ts.end();
     }
   }
-  
+
   private void assertBaseForms(String input, String... baseForms) throws IOException {
     try (TokenStream ts = analyzer.tokenStream("ignored", input)) {
       BaseFormAttribute baseFormAtt = ts.addAttribute(BaseFormAttribute.class);
@@ -445,7 +564,7 @@ public class
       ts.end();
     }
   }
-  
+
   private void assertPartsOfSpeech(String input, String... partsOfSpeech) throws IOException {
     try (TokenStream ts = analyzer.tokenStream("ignored", input)) {
       PartOfSpeechAttribute partOfSpeechAtt = ts.addAttribute(PartOfSpeechAttribute.class);
@@ -458,7 +577,7 @@ public class
       ts.end();
     }
   }
-  
+
   public void testReadings() throws Exception {
     assertReadings("å¯¿å?????¹ã????????",
                    "?¹ã?",
@@ -468,7 +587,7 @@ public class
                    "???",
                    "??");
   }
-  
+
   public void testReadings2() throws Exception {
     assertReadings("å¤????????è©??????¡ã???",
                    "?????",
@@ -481,7 +600,7 @@ public class
                    "??",
                    "??");
   }
-  
+
   public void testPronunciations() throws Exception {
     assertPronunciations("å¯¿å?????¹ã????????",
                          "?¹ã?",
@@ -491,7 +610,7 @@ public class
                          "???",
                          "??");
   }
-  
+
   public void testPronunciations2() throws Exception {
     // pronunciation differs from reading here
     assertPronunciations("å¤????????è©??????¡ã???",
@@ -505,7 +624,7 @@ public class
                          "??",
                          "??");
   }
-  
+
   public void testBasicForms() throws Exception {
     assertBaseForms("?????????é¨?????????¾ã???",
                     null,
@@ -518,7 +637,7 @@ public class
                     null,
                     null);
   }
-  
+
   public void testInflectionTypes() throws Exception {
     assertInflectionTypes("?????????é¨?????????¾ã???",
                           null,
@@ -531,7 +650,7 @@ public class
                           "?¹æ??»ã???",
                           null);
   }
-  
+
   public void testInflectionForms() throws Exception {
     assertInflectionForms("?????????é¨?????????¾ã???",
                           null,
@@ -544,7 +663,7 @@ public class
                           "?ºæ?å½?",
                           null);
   }
-  
+
   public void testPartOfSpeech() throws Exception {
     assertPartsOfSpeech("?????????é¨?????????¾ã???",
                         "???-ä»£å?è©?-ä¸???",
@@ -626,13 +745,13 @@ public class
   }
   */
 
-  
+
   private void doTestBocchan(int numIterations) throws Exception {
     LineNumberReader reader = new LineNumberReader(new InputStreamReader(
         this.getClass().getResourceAsStream("bocchan.utf-8"), StandardCharsets.UTF_8));
     String line = reader.readLine();
     reader.close();
-    
+
     if (VERBOSE) {
       System.out.println("Test for Bocchan without pre-splitting sentences");
     }
diff --git a/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizerFactory.java b/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizerFactory.java
index a409a4a..1d63c6f 100644
--- a/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizerFactory.java
+++ b/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizerFactory.java
@@ -19,7 +19,6 @@ package org.apache.lucene.analysis.ja;
 
 import java.io.IOException;
 import java.io.StringReader;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.Map;
 
@@ -42,7 +41,7 @@ public class TestJapaneseTokenizerFactory extends BaseTokenStreamTestCase {
         new int[] { 2, 3, 4, 5, 6, 8 }
     );
   }
-  
+
   /**
    * Test that search mode is enabled and working by default
    */
@@ -55,7 +54,7 @@ public class TestJapaneseTokenizerFactory extends BaseTokenStreamTestCase {
                               new String[] { "?·ã???", "?·ã??????????????³ã????", "?½ã?????§ã?", "?¨ã??¸ã???" }
     );
   }
-  
+
   /**
    * Test mode parameter: specifying normal mode
    */
@@ -75,7 +74,7 @@ public class TestJapaneseTokenizerFactory extends BaseTokenStreamTestCase {
    * Test user dictionary
    */
   public void testUserDict() throws IOException {
-    String userDict = 
+    String userDict =
         "# Custom segmentation for long entries\n" +
         "?¥æ?çµ???°è?,?¥æ? çµ?? ?°è?,????? ?±ã??¶ã? ?·ã????,????¿ã????\n" +
         "?¢è¥¿?½é?ç©ºæ¸¯,?¢è¥¿ ?½é? ç©ºæ¸¯,????µã? ?³ã??µã? ????³ã?,??????è©?n" +
@@ -109,7 +108,7 @@ public class TestJapaneseTokenizerFactory extends BaseTokenStreamTestCase {
             "??", "å¯¿å?", "??", "é£??", "???", "??", "??", "??", "??"}
     );
   }
-  
+
   /** Test that bogus arguments result in exception */
   public void testBogusArguments() throws Exception {
     try {
@@ -121,4 +120,31 @@ public class TestJapaneseTokenizerFactory extends BaseTokenStreamTestCase {
       assertTrue(expected.getMessage().contains("Unknown parameters"));
     }
   }
+
+  private TokenStream makeTokenStream(HashMap<String, String> args, String in) throws IOException {
+    JapaneseTokenizerFactory factory = new JapaneseTokenizerFactory(args);
+    factory.inform(new StringMockResourceLoader(""));
+    Tokenizer t = factory.create(newAttributeFactory());
+    t.setReader(new StringReader(in));
+    return t;
+  }
+
+  /**
+   * Test nbestCost parameter
+   */
+  public void testNbestCost() throws IOException {
+    assertTokenStreamContents(makeTokenStream(new HashMap<String, String>() {{put("nBestCost", "2000");}},
+                                              "é³©å±±ç©??"),
+                              new String[] {"é³?", "é³©å±±", "å±±ç???", "ç©??"});
+  }
+
+  /**
+   * Test nbestExamples parameter
+   */
+  public void testNbestExample() throws IOException {
+    assertTokenStreamContents(makeTokenStream(new HashMap<String, String>()
+                                              {{put("nBestExamples", "/é³©å±±ç©??-é³©å±±/é³©å±±ç©??-é³?/");}},
+                                              "é³©å±±ç©??"),
+                              new String[] {"é³?", "é³©å±±", "å±±ç???", "ç©??"});
+  }
 }

