GitDiffStart: 5f30bedccc3f8a2ed14df05e3207ef9ed56965b2 | Mon Jul 11 03:37:00 2011 +0000
diff --git a/dev-tools/eclipse/dot.classpath b/dev-tools/eclipse/dot.classpath
index 103f568..9fc5db2 100644
--- a/dev-tools/eclipse/dot.classpath
+++ b/dev-tools/eclipse/dot.classpath
@@ -49,6 +49,8 @@
 	<classpathentry kind="src" path="modules/grouping/src/test"/>
   <classpathentry kind="src" path="modules/queries/src/java"/>
 	<classpathentry kind="src" path="modules/queries/src/test"/>
+	<classpathentry kind="src" path="modules/queryparser/src/java"/>
+	<classpathentry kind="src" path="modules/queryparser/src/test"/>
 	<classpathentry kind="src" path="modules/suggest/src/java"/>
 	<classpathentry kind="src" path="modules/suggest/src/test"/>
 	<classpathentry kind="src" path="solr/core/src/java"/>
diff --git a/dev-tools/idea/.idea/modules.xml b/dev-tools/idea/.idea/modules.xml
index 92324fe..7179003 100644
--- a/dev-tools/idea/.idea/modules.xml
+++ b/dev-tools/idea/.idea/modules.xml
@@ -25,6 +25,7 @@
       <module filepath="$PROJECT_DIR$/modules/grouping/grouping.iml" />
       <module filepath="$PROJECT_DIR$/modules/join/join.iml" />
       <module filepath="$PROJECT_DIR$/modules/queries/queries.iml" />
+      <module filepath="$PROJECT_DIR$/modules/queryparser/queryparser.iml" />
       <module filepath="$PROJECT_DIR$/modules/suggest/suggest.iml" />
       <module filepath="$PROJECT_DIR$/solr/solr.iml" />
       <module filepath="$PROJECT_DIR$/solr/contrib/analysis-extras/analysis-extras.iml" />
diff --git a/dev-tools/idea/.idea/workspace.xml b/dev-tools/idea/.idea/workspace.xml
index 053978d..3504013 100644
--- a/dev-tools/idea/.idea/workspace.xml
+++ b/dev-tools/idea/.idea/workspace.xml
@@ -141,6 +141,13 @@
       <option name="VM_PARAMETERS" value="-ea -DtempDir=temp" />
       <option name="TEST_SEARCH_SCOPE"><value defaultName="singleModule" /></option>
     </configuration>
+    <configuration default="false" name="queryparser module" type="JUnit" factoryName="JUnit">
+      <module name="queryparser" />
+      <option name="TEST_OBJECT" value="package" />
+      <option name="WORKING_DIRECTORY" value="file://$PROJECT_DIR$/modules/queries/build" />
+      <option name="VM_PARAMETERS" value="-ea -DtempDir=temp" />
+      <option name="TEST_SEARCH_SCOPE"><value defaultName="singleModule" /></option>
+    </configuration>
     <configuration default="false" name="queryparser contrib" type="JUnit" factoryName="JUnit">
       <module name="queryparser-contrib" />
       <option name="TEST_OBJECT" value="package" />
@@ -204,7 +211,7 @@
       <option name="VM_PARAMETERS" value="-ea -DtempDir=temp" />
       <option name="TEST_SEARCH_SCOPE"><value defaultName="singleModule" /></option>
     </configuration>
-    <list size="29">
+    <list size="30">
       <item index="0" class="java.lang.String" itemvalue="JUnit.analysis-extras contrib" />
       <item index="1" class="java.lang.String" itemvalue="JUnit.benchmark module" />
       <item index="2" class="java.lang.String" itemvalue="JUnit.clustering contrib" />
@@ -225,15 +232,16 @@
       <item index="17" class="java.lang.String" itemvalue="JUnit.phonetic analysis module" />
       <item index="18" class="java.lang.String" itemvalue="JUnit.queries contrib" />
       <item index="19" class="java.lang.String" itemvalue="JUnit.queries module" />
-      <item index="20" class="java.lang.String" itemvalue="JUnit.queryparser contrib" />
-      <item index="21" class="java.lang.String" itemvalue="JUnit.smartcn analysis module" />
-      <item index="22" class="java.lang.String" itemvalue="JUnit.solr" />
-      <item index="23" class="java.lang.String" itemvalue="JUnit.spatial contrib" />
-      <item index="24" class="java.lang.String" itemvalue="JUnit.stempel analysis module" />
-      <item index="25" class="java.lang.String" itemvalue="JUnit.suggest module" />
-      <item index="26" class="java.lang.String" itemvalue="JUnit.uima contrib" />
-      <item index="27" class="java.lang.String" itemvalue="JUnit.wordnet contrib" />
-      <item index="28" class="java.lang.String" itemvalue="JUnit.xml-query-parser contrib" />
+      <item index="20" class="java.lang.String" itemvalue="JUnit.queryparser module" />
+      <item index="21" class="java.lang.String" itemvalue="JUnit.queryparser contrib" />
+      <item index="22" class="java.lang.String" itemvalue="JUnit.smartcn analysis module" />
+      <item index="23" class="java.lang.String" itemvalue="JUnit.solr" />
+      <item index="24" class="java.lang.String" itemvalue="JUnit.spatial contrib" />
+      <item index="25" class="java.lang.String" itemvalue="JUnit.stempel analysis module" />
+      <item index="26" class="java.lang.String" itemvalue="JUnit.suggest module" />
+      <item index="27" class="java.lang.String" itemvalue="JUnit.uima contrib" />
+      <item index="28" class="java.lang.String" itemvalue="JUnit.wordnet contrib" />
+      <item index="29" class="java.lang.String" itemvalue="JUnit.xml-query-parser contrib" />
     </list>
   </component>
 </project>
diff --git a/dev-tools/idea/lucene/contrib/demo/demo.iml b/dev-tools/idea/lucene/contrib/demo/demo.iml
index 95824a4..6941a7f 100644
--- a/dev-tools/idea/lucene/contrib/demo/demo.iml
+++ b/dev-tools/idea/lucene/contrib/demo/demo.iml
@@ -13,5 +13,6 @@
     <orderEntry type="library" scope="TEST" name="JUnit" level="project" />
     <orderEntry type="module" module-name="analysis-common" />
     <orderEntry type="module" module-name="lucene" />
+    <orderEntry type="module" module-name="queryparser" />
   </component>
 </module>
diff --git a/dev-tools/idea/lucene/contrib/memory/memory.iml b/dev-tools/idea/lucene/contrib/memory/memory.iml
index d8f31f7..69366de 100644
--- a/dev-tools/idea/lucene/contrib/memory/memory.iml
+++ b/dev-tools/idea/lucene/contrib/memory/memory.iml
@@ -14,5 +14,6 @@
     <orderEntry type="module" module-name="queries-contrib" />
     <orderEntry type="module" module-name="misc" />
     <orderEntry type="module" module-name="lucene" />
+    <orderEntry type="module" module-name="queryparser" />
   </component>
 </module>
diff --git a/dev-tools/idea/lucene/contrib/queryparser/queryparser-contrib.iml b/dev-tools/idea/lucene/contrib/queryparser/queryparser-contrib.iml
index f6b1189..28e1a41 100644
--- a/dev-tools/idea/lucene/contrib/queryparser/queryparser-contrib.iml
+++ b/dev-tools/idea/lucene/contrib/queryparser/queryparser-contrib.iml
@@ -15,5 +15,6 @@
     <orderEntry type="module" module-name="queries-contrib" />
     <orderEntry type="module" module-name="misc" />
     <orderEntry type="module" module-name="lucene" />
+    <orderEntry type="module" module-name="queryparser" />
   </component>
 </module>
diff --git a/dev-tools/idea/lucene/contrib/xml-query-parser/xml-query-parser.iml b/dev-tools/idea/lucene/contrib/xml-query-parser/xml-query-parser.iml
index 62ae859..7f7a200a 100644
--- a/dev-tools/idea/lucene/contrib/xml-query-parser/xml-query-parser.iml
+++ b/dev-tools/idea/lucene/contrib/xml-query-parser/xml-query-parser.iml
@@ -18,5 +18,6 @@
     <orderEntry type="module" module-name="misc" />
     <orderEntry type="module" module-name="analysis-common" />
     <orderEntry type="module" module-name="lucene" />
+    <orderEntry type="module" module-name="queryparser" />
   </component>
 </module>
diff --git a/dev-tools/idea/modules/benchmark/benchmark.iml b/dev-tools/idea/modules/benchmark/benchmark.iml
index 01eccb1..e4820e7 100644
--- a/dev-tools/idea/modules/benchmark/benchmark.iml
+++ b/dev-tools/idea/modules/benchmark/benchmark.iml
@@ -30,5 +30,6 @@
     <orderEntry type="module" module-name="memory" />
     <orderEntry type="module" module-name="analysis-common" />
     <orderEntry type="module" module-name="lucene" />
+    <orderEntry type="module" module-name="queryparser" />
   </component>
 </module>
diff --git a/dev-tools/idea/modules/queryparser/queryparser.iml b/dev-tools/idea/modules/queryparser/queryparser.iml
new file mode 100644
index 0000000..50cb281
--- /dev/null
+++ b/dev-tools/idea/modules/queryparser/queryparser.iml
@@ -0,0 +1,17 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<module type="JAVA_MODULE" version="4">
+  <component name="NewModuleRootManager" inherit-compiler-output="false">
+    <output url="file://$MODULE_DIR$/build/classes/java" />
+    <output-test url="file://$MODULE_DIR$/build/classes/test" />
+    <exclude-output />
+    <content url="file://$MODULE_DIR$">
+      <sourceFolder url="file://$MODULE_DIR$/src/java" isTestSource="false" />
+      <sourceFolder url="file://$MODULE_DIR$/src/test" isTestSource="true" />
+      <excludeFolder url="file://$MODULE_DIR$/work" />
+    </content>
+    <orderEntry type="inheritedJdk" />
+    <orderEntry type="sourceFolder" forTests="false" />
+    <orderEntry type="library" scope="TEST" name="JUnit" level="project" />
+    <orderEntry type="module" module-name="lucene" />
+  </component>
+</module>
diff --git a/dev-tools/idea/solr/solr.iml b/dev-tools/idea/solr/solr.iml
index 9a405c4..0d79071 100644
--- a/dev-tools/idea/solr/solr.iml
+++ b/dev-tools/idea/solr/solr.iml
@@ -31,5 +31,6 @@
     <orderEntry type="module" module-name="suggest" />
     <orderEntry type="module" module-name="analysis-common" />
     <orderEntry type="module" module-name="lucene" />
+    <orderEntry type="module" module-name="queryparser" />
   </component>
 </module>
diff --git a/dev-tools/maven/lucene/contrib/demo/pom.xml.template b/dev-tools/maven/lucene/contrib/demo/pom.xml.template
index 7672424..3834132 100644
--- a/dev-tools/maven/lucene/contrib/demo/pom.xml.template
+++ b/dev-tools/maven/lucene/contrib/demo/pom.xml.template
@@ -53,6 +53,11 @@
       <version>${project.version}</version>
     </dependency>
     <dependency>
+      <groupId>${project.groupId}</groupId>
+      <artifactId>lucene-queryparser</artifactId>
+      <version>${project.version}</version>
+    </dependency>
+    <dependency>
       <groupId>junit</groupId>
       <artifactId>junit</artifactId>
       <scope>test</scope>
diff --git a/dev-tools/maven/lucene/contrib/memory/pom.xml.template b/dev-tools/maven/lucene/contrib/memory/pom.xml.template
index 8017f82..f665fb6 100644
--- a/dev-tools/maven/lucene/contrib/memory/pom.xml.template
+++ b/dev-tools/maven/lucene/contrib/memory/pom.xml.template
@@ -45,6 +45,12 @@
     </dependency>
     <dependency>
       <groupId>${project.groupId}</groupId>
+      <artifactId>lucene-queryparser</artifactId>
+      <version>${project.version}</version>
+      <scope>test</scope>
+    </dependency>
+    <dependency>
+      <groupId>${project.groupId}</groupId>
       <artifactId>lucene-test-framework</artifactId>
       <version>${project.version}</version>
       <scope>test</scope>
diff --git a/dev-tools/maven/lucene/contrib/queryparser/pom.xml.template b/dev-tools/maven/lucene/contrib/queryparser/pom.xml.template
index 132bf14..4f67518 100644
--- a/dev-tools/maven/lucene/contrib/queryparser/pom.xml.template
+++ b/dev-tools/maven/lucene/contrib/queryparser/pom.xml.template
@@ -45,6 +45,11 @@
     </dependency>
     <dependency>
       <groupId>${project.groupId}</groupId>
+      <artifactId>lucene-queryparser</artifactId>
+      <version>${project.version}</version>
+    </dependency>
+    <dependency>
+      <groupId>${project.groupId}</groupId>
       <artifactId>lucene-core</artifactId>
       <version>${project.version}</version>
       <type>test-jar</type>
diff --git a/dev-tools/maven/lucene/contrib/xml-query-parser/pom.xml.template b/dev-tools/maven/lucene/contrib/xml-query-parser/pom.xml.template
index 73bb0d2..539eed6 100644
--- a/dev-tools/maven/lucene/contrib/xml-query-parser/pom.xml.template
+++ b/dev-tools/maven/lucene/contrib/xml-query-parser/pom.xml.template
@@ -53,6 +53,11 @@
       <version>${project.version}</version>
     </dependency>
     <dependency>
+      <groupId>${project.groupId}</groupId>
+      <artifactId>lucene-queryparser</artifactId>
+      <version>${project.version}</version>
+    </dependency>
+    <dependency>
       <groupId>javax.servlet</groupId>
       <artifactId>servlet-api</artifactId>
       <scope>provided</scope>
diff --git a/dev-tools/maven/modules/benchmark/pom.xml.template b/dev-tools/maven/modules/benchmark/pom.xml.template
index 75fb304..2389360 100755
--- a/dev-tools/maven/modules/benchmark/pom.xml.template
+++ b/dev-tools/maven/modules/benchmark/pom.xml.template
@@ -63,6 +63,11 @@
       <version>${project.version}</version>
     </dependency>
     <dependency>
+      <groupId>${project.groupId}</groupId>
+      <artifactId>lucene-queryparser</artifactId>
+      <version>${project.version}</version>
+    </dependency>
+    <dependency>
       <groupId>commons-beanutils</groupId>
       <artifactId>commons-beanutils</artifactId>
     </dependency>
diff --git a/dev-tools/maven/modules/pom.xml.template b/dev-tools/maven/modules/pom.xml.template
index 5fcb3c8..ae2974a 100644
--- a/dev-tools/maven/modules/pom.xml.template
+++ b/dev-tools/maven/modules/pom.xml.template
@@ -37,6 +37,7 @@
     <module>grouping</module>
     <module>join</module>
     <module>queries</module>
+    <module>queryparser</module>
     <module>suggest</module>
   </modules>
   <build>
diff --git a/dev-tools/maven/modules/queryparser/pom.xml.template b/dev-tools/maven/modules/queryparser/pom.xml.template
new file mode 100644
index 0000000..f10bb3b
--- /dev/null
+++ b/dev-tools/maven/modules/queryparser/pom.xml.template
@@ -0,0 +1,71 @@
+<project xmlns="http://maven.apache.org/POM/4.0.0"
+         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+  <!--
+    Licensed to the Apache Software Foundation (ASF) under one
+    or more contributor license agreements.  See the NOTICE file
+    distributed with this work for additional information
+    regarding copyright ownership.  The ASF licenses this file
+    to you under the Apache License, Version 2.0 (the
+    "License"); you may not use this file except in compliance
+    with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing,
+    software distributed under the License is distributed on an
+    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+    KIND, either express or implied.  See the License for the
+    specific language governing permissions and limitations
+    under the License.
+  -->
+  <modelVersion>4.0.0</modelVersion>
+  <parent>
+    <groupId>org.apache.lucene</groupId>
+    <artifactId>lucene-parent</artifactId>
+    <version>@version@</version>
+    <relativePath>../../lucene/pom.xml</relativePath>
+  </parent>
+  <groupId>org.apache.lucene</groupId>
+  <artifactId>lucene-queryparser</artifactId>
+  <packaging>jar</packaging>
+  <name>Lucene QueryParsers</name>
+  <description>Lucene QueryParsers module</description>
+  <properties>
+    <module-directory>modules/queryparser</module-directory>
+    <build-directory>build</build-directory>
+  </properties>
+  <dependencies>
+    <dependency>
+      <groupId>${project.groupId}</groupId>
+      <artifactId>lucene-core</artifactId>
+      <version>${project.version}</version>
+    </dependency>
+    <dependency>
+      <groupId>${project.groupId}</groupId>
+      <artifactId>lucene-test-framework</artifactId>
+      <version>${project.version}</version>
+      <scope>test</scope>
+    </dependency>
+    <dependency>
+      <groupId>junit</groupId>
+      <artifactId>junit</artifactId>
+      <scope>test</scope>
+    </dependency>
+  </dependencies>
+  <build>
+    <directory>${build-directory}</directory>
+    <outputDirectory>${build-directory}/classes/java</outputDirectory>
+    <testOutputDirectory>${build-directory}/classes/test</testOutputDirectory>
+    <sourceDirectory>src/java</sourceDirectory>
+    <testSourceDirectory>src/test</testSourceDirectory>
+    <testResources>
+      <testResource>
+        <directory>${project.build.testSourceDirectory}</directory>
+        <excludes>
+          <exclude>**/*.java</exclude>
+        </excludes>
+      </testResource>
+    </testResources>
+  </build>
+</project>
diff --git a/dev-tools/maven/solr/core/pom.xml.template b/dev-tools/maven/solr/core/pom.xml.template
index 8a883b9..7ef0f03 100644
--- a/dev-tools/maven/solr/core/pom.xml.template
+++ b/dev-tools/maven/solr/core/pom.xml.template
@@ -83,7 +83,7 @@
     </dependency>
     <dependency>
       <groupId>org.apache.lucene</groupId>
-      <artifactId>lucene-queries-contrib</artifactId>
+      <artifactId>lucene-queryparser</artifactId>
       <version>${project.version}</version>
     </dependency>
     <dependency>
diff --git a/lucene/MIGRATE.txt b/lucene/MIGRATE.txt
index 268ca52..9882931 100644
--- a/lucene/MIGRATE.txt
+++ b/lucene/MIGRATE.txt
@@ -392,3 +392,16 @@ LUCENE-1458, LUCENE-2111: Flexible Indexing
   boost from outer queries such as BooleanQuery, instead it takes 2 parameters,
   the outer boost (topLevelBoost) and the norm. Weight.sumOfSquaredWeights has
   been renamed to Weight.getValueForNormalization().
+
+* LUCENE-3283: Lucene's core o.a.l.queryParser QueryParsers have been consolidated into module/queryparser,
+  where other QueryParsers from the codebase will also be placed.  The following classes were moved:
+  - o.a.l.queryParser.CharStream -> o.a.l.queryparser.classic.CharStream
+  - o.a.l.queryParser.FastCharStream -> o.a.l.queryparser.classic.FastCharStream
+  - o.a.l.queryParser.MultiFieldQueryParser -> o.a.l.queryparser.classic.MultiFieldQueryParser
+  - o.a.l.queryParser.ParseException -> o.a.l.queryparser.classic.ParseException
+  - o.a.l.queryParser.QueryParser -> o.a.l.queryparser.classic.QueryParser
+  - o.a.l.queryParser.QueryParserBase -> o.a.l.queryparser.classic.QueryParserBase
+  - o.a.l.queryParser.QueryParserConstants -> o.a.l.queryparser.classic.QueryParserConstants
+  - o.a.l.queryParser.QueryParserTokenManager -> o.a.l.queryparser.classic.QueryParserTokenManager
+  - o.a.l.queryParser.QueryParserToken -> o.a.l.queryparser.classic.Token
+  - o.a.l.queryParser.QueryParserTokenMgrError -> o.a.l.queryparser.classic.TokenMgrError
diff --git a/lucene/build.xml b/lucene/build.xml
index 805734b..7a70a63 100644
--- a/lucene/build.xml
+++ b/lucene/build.xml
@@ -206,6 +206,7 @@
           <include name="**/lib/*.jar"/>
         </fileset>
         <pathelement location="${common.dir}/../modules/analysis/build/common/lucene-analyzers-common-${version}.jar"/>
+        <pathelement location="${common.dir}/../modules/queryparser/build/lucene-queryparser-${version}.jar"/>
       </path>
 
       <invoke-javadoc
@@ -417,9 +418,6 @@
   <!-- ================================================================== -->
   <target name="clean-javacc">
     <delete>
-      <fileset dir="src/java/org/apache/lucene/queryParser" includes="*.java">
-        <containsregexp expression="Generated.*By.*JavaCC"/>
-      </fileset>
       <fileset dir="contrib/queryparser/src/java/org/apache/lucene/queryParser/surround/parser" includes="*.java">
         <containsregexp expression="Generated.*By.*JavaCC"/>
       </fileset>
@@ -429,25 +427,7 @@
     </delete>
   </target>
 
-  <target name="javacc" depends="init,javacc-check,clean-javacc,javacc-QueryParser,javacc-contrib-queryparser,javacc-contrib-demo"/>
-
-  <target name="javacc-QueryParser" depends="init,javacc-check" if="javacc.present">
-    <sequential>
-      <invoke-javacc target="src/java/org/apache/lucene/queryParser/QueryParser.jj"
-                     outputDir="src/java/org/apache/lucene/queryParser"/>
-
-      <!-- Change the inccorrect public ctors for QueryParser to be protected instead -->
-      <replaceregexp file="src/java/org/apache/lucene/queryParser/QueryParser.java"
-		     byline="true"
-		     match="public QueryParser\(CharStream "
-		     replace="protected QueryParser(CharStream "/>
-      <replaceregexp file="src/java/org/apache/lucene/queryParser/QueryParser.java"
-		     byline="true"
-		     match="public QueryParser\(QueryParserTokenManager "
-		     replace="protected QueryParser(QueryParserTokenManager "/>
-
-    </sequential>
-  </target>	
+  <target name="javacc" depends="init,javacc-check,clean-javacc,javacc-contrib-queryparser,javacc-contrib-demo"/>
 	
   <target name="javacc-contrib-queryparser" depends="init,javacc-check" if="javacc.present">
     <ant target="javacc"
diff --git a/lucene/contrib/demo/build.xml b/lucene/contrib/demo/build.xml
index c85e127..517e0a1 100644
--- a/lucene/contrib/demo/build.xml
+++ b/lucene/contrib/demo/build.xml
@@ -29,6 +29,7 @@
 
   <module-uptodate name="analysis/common" jarfile="${common.dir}/../modules/analysis/build/common/lucene-analyzers-common-${version}.jar"
       property="analyzers-common.uptodate" classpath.property="analyzers-common.jar"/>
+  <module-uptodate name="queryparser" property="queryparser.uptodate" classpath.property="queryparser.jar"/>
   
   <property name="lucene.jar" value="${common.dir}/build/lucene-core-${version}.jar"/>
   <target name="lucene-jar-uptodate" unless="lucene.jar.uptodate">
@@ -45,14 +46,22 @@
   
   <path id="classpath">
 	 <pathelement path="${analyzers-common.jar}"/>
+   <pathelement path="${queryparser.jar}"/>
    <pathelement path="${lucene.jar}"/>
   </path>
 
-  <target name="compile-core" depends="jar-analyzers-common,common.compile-core" />
+  <target name="compile-core" depends="jar-analyzers-common,build-queryparser,common.compile-core" />
 
   <target name="jar-analyzers-common" unless="analyzers-common.uptodate">
     <subant target="jar-core">
       <fileset dir="${common.dir}/../modules/analysis/common" includes="build.xml"/>
     </subant>
   </target>
+
+  <target name="build-queryparser" unless="queryparser.uptodate">
+    <echo>Demo building dependency modules/queryparser</echo>
+    <subant target="default">
+      <fileset dir="${common.dir}/../modules/queryparser" includes="build.xml"/>
+    </subant>
+  </target>
 </project>
diff --git a/lucene/contrib/demo/src/java/org/apache/lucene/demo/SearchFiles.java b/lucene/contrib/demo/src/java/org/apache/lucene/demo/SearchFiles.java
index 7d1e929..7653a1e 100644
--- a/lucene/contrib/demo/src/java/org/apache/lucene/demo/SearchFiles.java
+++ b/lucene/contrib/demo/src/java/org/apache/lucene/demo/SearchFiles.java
@@ -27,7 +27,7 @@ import java.util.Date;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.document.Document;
-import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.queryparser.classic.QueryParser;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.ScoreDoc;
diff --git a/lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest.java b/lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest.java
index 396b29e..85ef9ba 100644
--- a/lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest.java
+++ b/lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest.java
@@ -52,7 +52,6 @@ import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import org.apache.lucene.queryParser.ParseException;
 import org.apache.lucene.search.*;
 import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.search.highlight.SynonymTokenizer.TestHighlightRunner;
@@ -1630,7 +1629,7 @@ public class HighlighterTest extends BaseTokenStreamTestCase implements Formatte
   private Directory dir;
   private Analyzer a = new MockAnalyzer(random, MockTokenizer.WHITESPACE, false);
   
-  public void testWeightedTermsWithDeletes() throws IOException, ParseException, InvalidTokenOffsetsException {
+  public void testWeightedTermsWithDeletes() throws IOException, InvalidTokenOffsetsException {
     makeIndex();
     deleteDocument();
     searchIndex();
@@ -1660,7 +1659,7 @@ public class HighlighterTest extends BaseTokenStreamTestCase implements Formatte
     writer.close();
   }
   
-  private void searchIndex() throws IOException, ParseException, InvalidTokenOffsetsException {
+  private void searchIndex() throws IOException, InvalidTokenOffsetsException {
     Query query = new TermQuery(new Term("t_text1", "random"));
     IndexSearcher searcher = new IndexSearcher( dir, true );
     // This scorer can return negative idf -> null fragment
diff --git a/lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase.java b/lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase.java
index d7fbe0b..3bf2207 100644
--- a/lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase.java
+++ b/lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase.java
@@ -42,7 +42,6 @@ import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import org.apache.lucene.queryParser.QueryParser;
 import org.apache.lucene.search.DisjunctionMaxQuery;
 import org.apache.lucene.search.PhraseQuery;
 import org.apache.lucene.search.Query;
diff --git a/lucene/contrib/memory/build.xml b/lucene/contrib/memory/build.xml
index 5ce01f5..8f73d28 100644
--- a/lucene/contrib/memory/build.xml
+++ b/lucene/contrib/memory/build.xml
@@ -24,4 +24,20 @@
   </description>
 
   <import file="../contrib-build.xml"/>
+
+  <module-uptodate name="queryparser" property="queryparser.uptodate" classpath.property="queryparser.jar"/>
+
+  <path id="test.classpath">
+    <pathelement path="${queryparser.jar}"/>
+    <path refid="test.base.classpath"/>
+  </path>
+
+  <target name="compile-core" depends="build-queryparser,common.compile-core" />
+
+  <target name="build-queryparser" unless="queryparser.uptodate">
+    <echo>Memory building dependency modules/queryparser</echo>
+    <subant target="default">
+      <fileset dir="${common.dir}/../modules/queryparser" includes="build.xml"/>
+    </subant>
+  </target>
 </project>
diff --git a/lucene/contrib/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java b/lucene/contrib/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java
index 197721b..1fe710c 100644
--- a/lucene/contrib/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java
+++ b/lucene/contrib/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java
@@ -33,7 +33,7 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.queryparser.classic.QueryParser;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.TopDocs;
 import org.apache.lucene.store.Directory;
diff --git a/lucene/contrib/queryparser/build.xml b/lucene/contrib/queryparser/build.xml
index d12fdef..f1492c8 100644
--- a/lucene/contrib/queryparser/build.xml
+++ b/lucene/contrib/queryparser/build.xml
@@ -24,8 +24,15 @@
   </description>
 
   <import file="../contrib-build.xml"/>
- 
-  <target name="compile-core" depends="javacc-notice, common.compile-core"/>
+
+  <module-uptodate name="queryparser" property="queryparser.uptodate" classpath.property="queryparser.jar"/>
+
+  <path id="classpath">
+    <pathelement path="${queryparser.jar}"/>
+    <path refid="base.classpath"/>
+  </path>
+
+  <target name="compile-core" depends="javacc-notice,build-queryparser,common.compile-core"/>
 
   <!--
     NOTE: see the README.javacc for details on how to fully regenerate the parser
@@ -100,4 +107,11 @@ import org.apache.lucene.queryParser.core.messages.*;"
                    outputDir="src/java/org/apache/lucene/queryParser/surround/parser"
     />
   </target>
+
+  <target name="build-queryparser" unless="queryparser.uptodate">
+    <echo>QueryParser Contrib building dependency modules/queryparser</echo>
+    <subant target="default">
+      <fileset dir="${common.dir}/../modules/queryparser" includes="build.xml"/>
+    </subant>
+  </target>
 </project>
diff --git a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/analyzing/AnalyzingQueryParser.java b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/analyzing/AnalyzingQueryParser.java
index 136d840..ae8defe 100644
--- a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/analyzing/AnalyzingQueryParser.java
+++ b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/analyzing/AnalyzingQueryParser.java
@@ -25,7 +25,7 @@ import java.util.List;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.util.Version;
 
@@ -41,7 +41,7 @@ import org.apache.lucene.util.Version;
  * using this parser will be no improvement over QueryParser in such cases). 
  *
  */
-public class AnalyzingQueryParser extends org.apache.lucene.queryParser.QueryParser {
+public class AnalyzingQueryParser extends org.apache.lucene.queryparser.classic.QueryParser {
 
   /**
    * Constructs a query parser.
diff --git a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/complexPhrase/ComplexPhraseQueryParser.java b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/complexPhrase/ComplexPhraseQueryParser.java
index 9b41392..97deb6f 100644
--- a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/complexPhrase/ComplexPhraseQueryParser.java
+++ b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/complexPhrase/ComplexPhraseQueryParser.java
@@ -25,8 +25,8 @@ import java.util.List;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.queryParser.ParseException;
-import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.queryparser.classic.ParseException;
+import org.apache.lucene.queryparser.classic.QueryParser;
 import org.apache.lucene.search.BooleanClause;
 import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.MultiTermQuery;
diff --git a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/ext/ExtendableQueryParser.java b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/ext/ExtendableQueryParser.java
index 6592c60..c85712b 100644
--- a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/ext/ExtendableQueryParser.java
+++ b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/ext/ExtendableQueryParser.java
@@ -18,9 +18,9 @@ package org.apache.lucene.queryParser.ext;
  */
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.queryParser.ParseException;
-import org.apache.lucene.queryParser.QueryParser;
 import org.apache.lucene.queryParser.ext.Extensions.Pair;
+import org.apache.lucene.queryparser.classic.ParseException;
+import org.apache.lucene.queryparser.classic.QueryParser;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.util.Version;
 
diff --git a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/ext/ExtensionQuery.java b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/ext/ExtensionQuery.java
index f84faf8..423cad3 100644
--- a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/ext/ExtensionQuery.java
+++ b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/ext/ExtensionQuery.java
@@ -1,6 +1,6 @@
 package org.apache.lucene.queryParser.ext;
 
-import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.queryparser.classic.QueryParser;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
diff --git a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/ext/Extensions.java b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/ext/Extensions.java
index edf763d..9e3ee39 100644
--- a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/ext/Extensions.java
+++ b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/ext/Extensions.java
@@ -16,10 +16,11 @@ package org.apache.lucene.queryParser.ext;
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+import org.apache.lucene.queryparser.classic.QueryParser;
+
 import java.util.HashMap;
 import java.util.Map;
 
-import org.apache.lucene.queryParser.QueryParser;
 
 /**
  * The {@link Extensions} class represents an extension mapping to associate
diff --git a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/ext/ParserExtension.java b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/ext/ParserExtension.java
index b173858..77f9ada 100644
--- a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/ext/ParserExtension.java
+++ b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/ext/ParserExtension.java
@@ -17,19 +17,19 @@ package org.apache.lucene.queryParser.ext;
  * limitations under the License.
  */
 
-import org.apache.lucene.queryParser.ParseException;
-import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.lucene.search.Query;
 
 /**
  * This class represents an extension base class to the Lucene standard
- * {@link QueryParser}. The {@link QueryParser} is generated by the JavaCC
+ * {@link org.apache.lucene.queryparser.classic.QueryParser}. The
+ * {@link org.apache.lucene.queryparser.classic.QueryParser} is generated by the JavaCC
  * parser generator. Changing or adding functionality or syntax in the standard
  * query parser requires changes to the JavaCC source file. To enable extending
  * the standard query parser without changing the JavaCC sources and re-generate
  * the parser the {@link ParserExtension} can be customized and plugged into an
  * instance of {@link ExtendableQueryParser}, a direct subclass of
- * {@link QueryParser}.
+ * {@link org.apache.lucene.queryparser.classic.QueryParser}.
  * 
  * @see Extensions
  * @see ExtendableQueryParser
diff --git a/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/analyzing/TestAnalyzingQueryParser.java b/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/analyzing/TestAnalyzingQueryParser.java
index 8613f8e..515d430 100644
--- a/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/analyzing/TestAnalyzingQueryParser.java
+++ b/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/analyzing/TestAnalyzingQueryParser.java
@@ -25,7 +25,7 @@ import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.lucene.util.LuceneTestCase;
 
 /**
diff --git a/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/complexPhrase/TestComplexPhraseQuery.java b/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/complexPhrase/TestComplexPhraseQuery.java
index b8aaae8..f3cd3f3 100644
--- a/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/complexPhrase/TestComplexPhraseQuery.java
+++ b/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/complexPhrase/TestComplexPhraseQuery.java
@@ -24,7 +24,7 @@ import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.queryparser.classic.QueryParser;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.ScoreDoc;
diff --git a/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/ext/ExtensionStub.java b/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/ext/ExtensionStub.java
index 63ce2b3..59f1722 100644
--- a/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/ext/ExtensionStub.java
+++ b/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/ext/ExtensionStub.java
@@ -1,7 +1,7 @@
 package org.apache.lucene.queryParser.ext;
 
 import org.apache.lucene.index.Term;
-import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.TermQuery;
 
@@ -30,4 +30,4 @@ class ExtensionStub extends ParserExtension {
         .getRawQueryString()));
   }
 
-}
\ No newline at end of file
+}
diff --git a/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/ext/TestExtendableQueryParser.java b/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/ext/TestExtendableQueryParser.java
index 366168f..7aedc93 100644
--- a/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/ext/TestExtendableQueryParser.java
+++ b/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/ext/TestExtendableQueryParser.java
@@ -20,26 +20,21 @@ package org.apache.lucene.queryParser.ext;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.queryParser.ParseException;
-import org.apache.lucene.queryParser.QueryParser;
-import org.apache.lucene.queryParser.TestQueryParser;
+import org.apache.lucene.queryparser.classic.ParseException;
+import org.apache.lucene.queryparser.classic.QueryParser;
 import org.apache.lucene.search.BooleanClause;
 import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.util.LuceneTestCase;
 
 /**
  * Testcase for the class {@link ExtendableQueryParser}
  */
-public class TestExtendableQueryParser extends TestQueryParser {
+public class TestExtendableQueryParser extends LuceneTestCase {
   private static char[] DELIMITERS = new char[] {
       Extensions.DEFAULT_EXTENSION_FIELD_DELIMITER, '-', '|' };
 
-  @Override
-  public QueryParser getParser(Analyzer a) throws Exception {
-    return getParser(a, null);
-  }
-
   public QueryParser getParser(Analyzer a, Extensions extensions)
       throws Exception {
     if (a == null)
diff --git a/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/precedence/TestPrecedenceQueryParser.java b/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/precedence/TestPrecedenceQueryParser.java
index 3614d01..b53ec68 100644
--- a/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/precedence/TestPrecedenceQueryParser.java
+++ b/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/precedence/TestPrecedenceQueryParser.java
@@ -35,7 +35,6 @@ import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.document.DateTools;
-import org.apache.lucene.queryParser.TestQueryParser;
 import org.apache.lucene.queryParser.core.QueryNodeException;
 import org.apache.lucene.queryParser.core.QueryNodeParseException;
 import org.apache.lucene.queryParser.standard.config.StandardQueryConfigHandler;
@@ -57,11 +56,11 @@ import org.apache.lucene.util.automaton.CharacterRunAutomaton;
  * This test case tests {@link PrecedenceQueryParser}.
  * </p>
  * <p>
- * It contains all tests from {@link TestQueryParser} with some adjusted to 
- * fit the precedence requirement, plus some precedence test cases. 
+ * It contains all tests from {@link org.apache.lucene.queryparser.classic.TestQueryParser}
+ * with some adjusted to fit the precedence requirement, plus some precedence test cases.
  * </p>
  * 
- * @see TestQueryParser
+ * @see org.apache.lucene.queryparser.classic.TestQueryParser
  */
 public class TestPrecedenceQueryParser extends LuceneTestCase {
 
diff --git a/lucene/contrib/xml-query-parser/build.xml b/lucene/contrib/xml-query-parser/build.xml
index ebe4a96..a86d467 100644
--- a/lucene/contrib/xml-query-parser/build.xml
+++ b/lucene/contrib/xml-query-parser/build.xml
@@ -29,21 +29,24 @@
   <property name="lucene.jar" location="${common.dir}/build/lucene-core-${version}.jar"/>
   <property name="servlet.jar" location="${common.dir}/lib/servlet-api-2.4.jar"/>
   <available property="servlet.jar.present" type="file" file="${servlet.jar}"/>
+  <module-uptodate name="queryparser" property="queryparser.uptodate" classpath.property="queryparser.jar"/>
 
 
   <path id="classpath">
     <pathelement path="${queries.jar}"/>
+    <pathelement path="${queryparser.jar}"/>
     <path refid="base.classpath"/>
   </path>
 
   <path id="web-classpath">
     <pathelement path="${queries.jar}"/>
+    <pathelement path="${queryparser.jar}"/>
     <pathelement path="${servlet.jar}"/>
     <pathelement path="${build.dir}/${final.name}.jar"/>
     <path refid="base.classpath"/>
   </path>
 
-  <target name="compile-core" depends="build-queries, common.compile-core" />
+  <target name="compile-core" depends="build-queries,build-queryparser,common.compile-core" />
 
   <target name="build-queries" unless="queries.uptodate">
     <echo>XML Parser building dependency ${queries.jar}</echo>
@@ -82,4 +85,11 @@
 
   </target>
 
+  <target name="build-queryparser" unless="queryparser.uptodate">
+    <echo>Demo building dependency modules/queryparser</echo>
+    <subant target="default">
+      <fileset dir="${common.dir}/../modules/queryparser" includes="build.xml"/>
+    </subant>
+  </target>
+
 </project>
diff --git a/lucene/contrib/xml-query-parser/src/java/org/apache/lucene/xmlparser/CoreParser.java b/lucene/contrib/xml-query-parser/src/java/org/apache/lucene/xmlparser/CoreParser.java
index c84b90a..080bd0a 100644
--- a/lucene/contrib/xml-query-parser/src/java/org/apache/lucene/xmlparser/CoreParser.java
+++ b/lucene/contrib/xml-query-parser/src/java/org/apache/lucene/xmlparser/CoreParser.java
@@ -6,7 +6,7 @@ import javax.xml.parsers.DocumentBuilder;
 import javax.xml.parsers.DocumentBuilderFactory;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.queryparser.classic.QueryParser;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.xmlparser.builders.*;
 import org.w3c.dom.Document;
diff --git a/lucene/contrib/xml-query-parser/src/java/org/apache/lucene/xmlparser/CorePlusExtensionsParser.java b/lucene/contrib/xml-query-parser/src/java/org/apache/lucene/xmlparser/CorePlusExtensionsParser.java
index 37022b3..cc68dec 100644
--- a/lucene/contrib/xml-query-parser/src/java/org/apache/lucene/xmlparser/CorePlusExtensionsParser.java
+++ b/lucene/contrib/xml-query-parser/src/java/org/apache/lucene/xmlparser/CorePlusExtensionsParser.java
@@ -1,7 +1,7 @@
 package org.apache.lucene.xmlparser;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.queryparser.classic.QueryParser;
 import org.apache.lucene.xmlparser.builders.BooleanFilterBuilder;
 import org.apache.lucene.xmlparser.builders.BoostingQueryBuilder;
 import org.apache.lucene.xmlparser.builders.DuplicateFilterBuilder;
diff --git a/lucene/contrib/xml-query-parser/src/java/org/apache/lucene/xmlparser/builders/UserInputQueryBuilder.java b/lucene/contrib/xml-query-parser/src/java/org/apache/lucene/xmlparser/builders/UserInputQueryBuilder.java
index 7b6d865..0ada2ca 100644
--- a/lucene/contrib/xml-query-parser/src/java/org/apache/lucene/xmlparser/builders/UserInputQueryBuilder.java
+++ b/lucene/contrib/xml-query-parser/src/java/org/apache/lucene/xmlparser/builders/UserInputQueryBuilder.java
@@ -1,8 +1,8 @@
 package org.apache.lucene.xmlparser.builders;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.queryParser.ParseException;
-import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.queryparser.classic.ParseException;
+import org.apache.lucene.queryparser.classic.QueryParser;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.xmlparser.DOMUtils;
 import org.apache.lucene.xmlparser.ParserException;
diff --git a/lucene/src/java/org/apache/lucene/analysis/package.html b/lucene/src/java/org/apache/lucene/analysis/package.html
index 4ce7f49..45d9d73 100644
--- a/lucene/src/java/org/apache/lucene/analysis/package.html
+++ b/lucene/src/java/org/apache/lucene/analysis/package.html
@@ -123,10 +123,8 @@ There are many post tokenization steps that can be done, including (but not limi
         {@link org.apache.lucene.index.IndexWriter#addDocument(org.apache.lucene.document.Document) addDocument(doc)},
         the Analyzer in effect for indexing is invoked for each indexed field of the added document.
     </li>
-    <li>At search, as a consequence of
-        {@link org.apache.lucene.queryParser.QueryParser#parse(java.lang.String) QueryParser.parse(queryText)},
-        the QueryParser may invoke the Analyzer in effect.
-        Note that for some queries analysis does not take place, e.g. wildcard queries.
+    <li>At search, a QueryParser may invoke the Analyzer during parsing.  Note that for some queries, analysis does not
+        take place, e.g. wildcard queries.
     </li>
   </ul>
   However an application might invoke Analysis of any text for testing or for any other purpose, something like:
diff --git a/lucene/src/java/org/apache/lucene/queryParser/CharStream.java b/lucene/src/java/org/apache/lucene/queryParser/CharStream.java
deleted file mode 100644
index fe1c9e7..0000000
--- a/lucene/src/java/org/apache/lucene/queryParser/CharStream.java
+++ /dev/null
@@ -1,112 +0,0 @@
-/* Generated By:JavaCC: Do not edit this line. CharStream.java Version 4.1 */
-/* JavaCCOptions:STATIC=false */
-package org.apache.lucene.queryParser;
-
-/**
- * This interface describes a character stream that maintains line and
- * column number positions of the characters.  It also has the capability
- * to backup the stream to some extent.  An implementation of this
- * interface is used in the TokenManager implementation generated by
- * JavaCCParser.
- *
- * All the methods except backup can be implemented in any fashion. backup
- * needs to be implemented correctly for the correct operation of the lexer.
- * Rest of the methods are all used to get information like line number,
- * column number and the String that constitutes a token and are not used
- * by the lexer. Hence their implementation won't affect the generated lexer's
- * operation.
- */
-
-public interface CharStream {
-
-  /**
-   * Returns the next character from the selected input.  The method
-   * of selecting the input is the responsibility of the class
-   * implementing this interface.  Can throw any java.io.IOException.
-   */
-  char readChar() throws java.io.IOException;
-
-  /**
-   * Returns the column position of the character last read.
-   * @deprecated (gen)
-   * @see #getEndColumn
-   */
-  int getColumn();
-
-  /**
-   * Returns the line number of the character last read.
-   * @deprecated (gen)
-   * @see #getEndLine
-   */
-  int getLine();
-
-  /**
-   * Returns the column number of the last character for current token (being
-   * matched after the last call to BeginTOken).
-   */
-  int getEndColumn();
-
-  /**
-   * Returns the line number of the last character for current token (being
-   * matched after the last call to BeginTOken).
-   */
-  int getEndLine();
-
-  /**
-   * Returns the column number of the first character for current token (being
-   * matched after the last call to BeginTOken).
-   */
-  int getBeginColumn();
-
-  /**
-   * Returns the line number of the first character for current token (being
-   * matched after the last call to BeginTOken).
-   */
-  int getBeginLine();
-
-  /**
-   * Backs up the input stream by amount steps. Lexer calls this method if it
-   * had already read some characters, but could not use them to match a
-   * (longer) token. So, they will be used again as the prefix of the next
-   * token and it is the implemetation's responsibility to do this right.
-   */
-  void backup(int amount);
-
-  /**
-   * Returns the next character that marks the beginning of the next token.
-   * All characters must remain in the buffer between two successive calls
-   * to this method to implement backup correctly.
-   */
-  char BeginToken() throws java.io.IOException;
-
-  /**
-   * Returns a string made up of characters from the marked token beginning
-   * to the current buffer position. Implementations have the choice of returning
-   * anything that they want to. For example, for efficiency, one might decide
-   * to just return null, which is a valid implementation.
-   */
-  String GetImage();
-
-  /**
-   * Returns an array of characters that make up the suffix of length 'len' for
-   * the currently matched token. This is used to build up the matched string
-   * for use in actions in the case of MORE. A simple and inefficient
-   * implementation of this is as follows :
-   *
-   *   {
-   *      String t = GetImage();
-   *      return t.substring(t.length() - len, t.length()).toCharArray();
-   *   }
-   */
-  char[] GetSuffix(int len);
-
-  /**
-   * The lexer calls this function to indicate that it is done with the stream
-   * and hence implementations can free any resources held by this class.
-   * Again, the body of this function can be just empty and it will not
-   * affect the lexer's operation.
-   */
-  void Done();
-
-}
-/* JavaCC - OriginalChecksum=32a89423891f765dde472f7ef0e3ef7b (do not edit this line) */
diff --git a/lucene/src/java/org/apache/lucene/queryParser/FastCharStream.java b/lucene/src/java/org/apache/lucene/queryParser/FastCharStream.java
deleted file mode 100644
index 8c1a1b9..0000000
--- a/lucene/src/java/org/apache/lucene/queryParser/FastCharStream.java
+++ /dev/null
@@ -1,124 +0,0 @@
-// FastCharStream.java
-package org.apache.lucene.queryParser;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- *  
- */
-
-import java.io.*;
-
-/** An efficient implementation of JavaCC's CharStream interface.  <p>Note that
- * this does not do line-number counting, but instead keeps track of the
- * character position of the token in the input, as required by Lucene's {@link
- * org.apache.lucene.analysis.Token} API. 
- * */
-public final class FastCharStream implements CharStream {
-  char[] buffer = null;
-
-  int bufferLength = 0;				  // end of valid chars
-  int bufferPosition = 0;			  // next char to read
-
-  int tokenStart = 0;				  // offset in buffer
-  int bufferStart = 0;				  // position in file of buffer
-
-  Reader input;					  // source of chars
-
-  /** Constructs from a Reader. */
-  public FastCharStream(Reader r) {
-    input = r;
-  }
-
-  public final char readChar() throws IOException {
-    if (bufferPosition >= bufferLength)
-      refill();
-    return buffer[bufferPosition++];
-  }
-
-  private final void refill() throws IOException {
-    int newPosition = bufferLength - tokenStart;
-
-    if (tokenStart == 0) {			  // token won't fit in buffer
-      if (buffer == null) {			  // first time: alloc buffer
-	buffer = new char[2048];
-      } else if (bufferLength == buffer.length) { // grow buffer
-	char[] newBuffer = new char[buffer.length*2];
-	System.arraycopy(buffer, 0, newBuffer, 0, bufferLength);
-	buffer = newBuffer;
-      }
-    } else {					  // shift token to front
-      System.arraycopy(buffer, tokenStart, buffer, 0, newPosition);
-    }
-
-    bufferLength = newPosition;			  // update state
-    bufferPosition = newPosition;
-    bufferStart += tokenStart;
-    tokenStart = 0;
-
-    int charsRead =				  // fill space in buffer
-      input.read(buffer, newPosition, buffer.length-newPosition);
-    if (charsRead == -1)
-      throw new IOException("read past eof");
-    else
-      bufferLength += charsRead;
-  }
-
-  public final char BeginToken() throws IOException {
-    tokenStart = bufferPosition;
-    return readChar();
-  }
-
-  public final void backup(int amount) {
-    bufferPosition -= amount;
-  }
-
-  public final String GetImage() {
-    return new String(buffer, tokenStart, bufferPosition - tokenStart);
-  }
-
-  public final char[] GetSuffix(int len) {
-    char[] value = new char[len];
-    System.arraycopy(buffer, bufferPosition - len, value, 0, len);
-    return value;
-  }
-
-  public final void Done() {
-    try {
-      input.close();
-    } catch (IOException e) {
-      System.err.println("Caught: " + e + "; ignoring.");
-    }
-  }
-
-  public final int getColumn() {
-    return bufferStart + bufferPosition;
-  }
-  public final int getLine() {
-    return 1;
-  }
-  public final int getEndColumn() {
-    return bufferStart + bufferPosition;
-  }
-  public final int getEndLine() {
-    return 1;
-  }
-  public final int getBeginColumn() {
-    return bufferStart + tokenStart;
-  }
-  public final int getBeginLine() {
-    return 1;
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/queryParser/MultiFieldQueryParser.java b/lucene/src/java/org/apache/lucene/queryParser/MultiFieldQueryParser.java
deleted file mode 100644
index 4e0dd68..0000000
--- a/lucene/src/java/org/apache/lucene/queryParser/MultiFieldQueryParser.java
+++ /dev/null
@@ -1,349 +0,0 @@
-package org.apache.lucene.queryParser;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.MultiPhraseQuery;
-import org.apache.lucene.search.PhraseQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.util.Version;
-
-/**
- * A QueryParser which constructs queries to search multiple fields.
- *
- */
-public class MultiFieldQueryParser extends QueryParser
-{
-  protected String[] fields;
-  protected Map<String,Float> boosts;
-
-  /**
-   * Creates a MultiFieldQueryParser. 
-   * Allows passing of a map with term to Boost, and the boost to apply to each term.
-   *
-   * <p>It will, when parse(String query)
-   * is called, construct a query like this (assuming the query consists of
-   * two terms and you specify the two fields <code>title</code> and <code>body</code>):</p>
-   * 
-   * <code>
-   * (title:term1 body:term1) (title:term2 body:term2)
-   * </code>
-   *
-   * <p>When setDefaultOperator(AND_OPERATOR) is set, the result will be:</p>
-   *  
-   * <code>
-   * +(title:term1 body:term1) +(title:term2 body:term2)
-   * </code>
-   * 
-   * <p>When you pass a boost (title=>5 body=>10) you can get </p>
-   * 
-   * <code>
-   * +(title:term1^5.0 body:term1^10.0) +(title:term2^5.0 body:term2^10.0)
-   * </code>
-   *
-   * <p>In other words, all the query's terms must appear, but it doesn't matter in
-   * what fields they appear.</p>
-   */
-  public MultiFieldQueryParser(Version matchVersion, String[] fields, Analyzer analyzer, Map<String,Float> boosts) {
-    this(matchVersion, fields, analyzer);
-    this.boosts = boosts;
-  }
-  
-  /**
-   * Creates a MultiFieldQueryParser.
-   *
-   * <p>It will, when parse(String query)
-   * is called, construct a query like this (assuming the query consists of
-   * two terms and you specify the two fields <code>title</code> and <code>body</code>):</p>
-   * 
-   * <code>
-   * (title:term1 body:term1) (title:term2 body:term2)
-   * </code>
-   *
-   * <p>When setDefaultOperator(AND_OPERATOR) is set, the result will be:</p>
-   *  
-   * <code>
-   * +(title:term1 body:term1) +(title:term2 body:term2)
-   * </code>
-   * 
-   * <p>In other words, all the query's terms must appear, but it doesn't matter in
-   * what fields they appear.</p>
-   */
-  public MultiFieldQueryParser(Version matchVersion, String[] fields, Analyzer analyzer) {
-    super(matchVersion, null, analyzer);
-    this.fields = fields;
-  }
-  
-  @Override
-  protected Query getFieldQuery(String field, String queryText, int slop) throws ParseException {
-    if (field == null) {
-      List<BooleanClause> clauses = new ArrayList<BooleanClause>();
-      for (int i = 0; i < fields.length; i++) {
-        Query q = super.getFieldQuery(fields[i], queryText, true);
-        if (q != null) {
-          //If the user passes a map of boosts
-          if (boosts != null) {
-            //Get the boost from the map and apply them
-            Float boost = boosts.get(fields[i]);
-            if (boost != null) {
-              q.setBoost(boost.floatValue());
-            }
-          }
-          applySlop(q,slop);
-          clauses.add(new BooleanClause(q, BooleanClause.Occur.SHOULD));
-        }
-      }
-      if (clauses.size() == 0)  // happens for stopwords
-        return null;
-      return getBooleanQuery(clauses, true);
-    }
-    Query q = super.getFieldQuery(field, queryText, true);
-    applySlop(q,slop);
-    return q;
-  }
-
-  private void applySlop(Query q, int slop) {
-    if (q instanceof PhraseQuery) {
-      ((PhraseQuery) q).setSlop(slop);
-    } else if (q instanceof MultiPhraseQuery) {
-      ((MultiPhraseQuery) q).setSlop(slop);
-    }
-  }
-  
-
-  @Override
-  protected Query getFieldQuery(String field, String queryText, boolean quoted) throws ParseException {
-    if (field == null) {
-      List<BooleanClause> clauses = new ArrayList<BooleanClause>();
-      for (int i = 0; i < fields.length; i++) {
-        Query q = super.getFieldQuery(fields[i], queryText, quoted);
-        if (q != null) {
-          //If the user passes a map of boosts
-          if (boosts != null) {
-            //Get the boost from the map and apply them
-            Float boost = boosts.get(fields[i]);
-            if (boost != null) {
-              q.setBoost(boost.floatValue());
-            }
-          }
-          clauses.add(new BooleanClause(q, BooleanClause.Occur.SHOULD));
-        }
-      }
-      if (clauses.size() == 0)  // happens for stopwords
-        return null;
-      return getBooleanQuery(clauses, true);
-    }
-    Query q = super.getFieldQuery(field, queryText, quoted);
-    return q;
-  }
-
-
-  @Override
-  protected Query getFuzzyQuery(String field, String termStr, float minSimilarity) throws ParseException
-  {
-    if (field == null) {
-      List<BooleanClause> clauses = new ArrayList<BooleanClause>();
-      for (int i = 0; i < fields.length; i++) {
-        clauses.add(new BooleanClause(getFuzzyQuery(fields[i], termStr, minSimilarity),
-            BooleanClause.Occur.SHOULD));
-      }
-      return getBooleanQuery(clauses, true);
-    }
-    return super.getFuzzyQuery(field, termStr, minSimilarity);
-  }
-
-  @Override
-  protected Query getPrefixQuery(String field, String termStr) throws ParseException
-  {
-    if (field == null) {
-      List<BooleanClause> clauses = new ArrayList<BooleanClause>();
-      for (int i = 0; i < fields.length; i++) {
-        clauses.add(new BooleanClause(getPrefixQuery(fields[i], termStr),
-            BooleanClause.Occur.SHOULD));
-      }
-      return getBooleanQuery(clauses, true);
-    }
-    return super.getPrefixQuery(field, termStr);
-  }
-
-  @Override
-  protected Query getWildcardQuery(String field, String termStr) throws ParseException {
-    if (field == null) {
-      List<BooleanClause> clauses = new ArrayList<BooleanClause>();
-      for (int i = 0; i < fields.length; i++) {
-        clauses.add(new BooleanClause(getWildcardQuery(fields[i], termStr),
-            BooleanClause.Occur.SHOULD));
-      }
-      return getBooleanQuery(clauses, true);
-    }
-    return super.getWildcardQuery(field, termStr);
-  }
-
- 
-  @Override
-  protected Query getRangeQuery(String field, String part1, String part2, boolean startInclusive, boolean endInclusive) throws ParseException {
-    if (field == null) {
-      List<BooleanClause> clauses = new ArrayList<BooleanClause>();
-      for (int i = 0; i < fields.length; i++) {
-        clauses.add(new BooleanClause(getRangeQuery(fields[i], part1, part2, startInclusive, endInclusive),
-            BooleanClause.Occur.SHOULD));
-      }
-      return getBooleanQuery(clauses, true);
-    }
-    return super.getRangeQuery(field, part1, part2, startInclusive, endInclusive);
-  }
-
-  /**
-   * Parses a query which searches on the fields specified.
-   * <p>
-   * If x fields are specified, this effectively constructs:
-   * <pre>
-   * <code>
-   * (field1:query1) (field2:query2) (field3:query3)...(fieldx:queryx)
-   * </code>
-   * </pre>
-   * @param matchVersion Lucene version to match; this is passed through to QueryParser.
-   * @param queries Queries strings to parse
-   * @param fields Fields to search on
-   * @param analyzer Analyzer to use
-   * @throws ParseException if query parsing fails
-   * @throws IllegalArgumentException if the length of the queries array differs
-   *  from the length of the fields array
-   */
-  public static Query parse(Version matchVersion, String[] queries, String[] fields,
-      Analyzer analyzer) throws ParseException
-  {
-    if (queries.length != fields.length)
-      throw new IllegalArgumentException("queries.length != fields.length");
-    BooleanQuery bQuery = new BooleanQuery();
-    for (int i = 0; i < fields.length; i++)
-    {
-      QueryParser qp = new QueryParser(matchVersion, fields[i], analyzer);
-      Query q = qp.parse(queries[i]);
-      if (q!=null && // q never null, just being defensive
-          (!(q instanceof BooleanQuery) || ((BooleanQuery)q).getClauses().length>0)) {
-        bQuery.add(q, BooleanClause.Occur.SHOULD);
-      }
-    }
-    return bQuery;
-  }
-
-  /**
-   * Parses a query, searching on the fields specified.
-   * Use this if you need to specify certain fields as required,
-   * and others as prohibited.
-   * <p><pre>
-   * Usage:
-   * <code>
-   * String[] fields = {"filename", "contents", "description"};
-   * BooleanClause.Occur[] flags = {BooleanClause.Occur.SHOULD,
-   *                BooleanClause.Occur.MUST,
-   *                BooleanClause.Occur.MUST_NOT};
-   * MultiFieldQueryParser.parse("query", fields, flags, analyzer);
-   * </code>
-   * </pre>
-   *<p>
-   * The code above would construct a query:
-   * <pre>
-   * <code>
-   * (filename:query) +(contents:query) -(description:query)
-   * </code>
-   * </pre>
-   *
-   * @param matchVersion Lucene version to match; this is passed through to QueryParser.
-   * @param query Query string to parse
-   * @param fields Fields to search on
-   * @param flags Flags describing the fields
-   * @param analyzer Analyzer to use
-   * @throws ParseException if query parsing fails
-   * @throws IllegalArgumentException if the length of the fields array differs
-   *  from the length of the flags array
-   */
-  public static Query parse(Version matchVersion, String query, String[] fields,
-      BooleanClause.Occur[] flags, Analyzer analyzer) throws ParseException {
-    if (fields.length != flags.length)
-      throw new IllegalArgumentException("fields.length != flags.length");
-    BooleanQuery bQuery = new BooleanQuery();
-    for (int i = 0; i < fields.length; i++) {
-      QueryParser qp = new QueryParser(matchVersion, fields[i], analyzer);
-      Query q = qp.parse(query);
-      if (q!=null && // q never null, just being defensive 
-          (!(q instanceof BooleanQuery) || ((BooleanQuery)q).getClauses().length>0)) {
-        bQuery.add(q, flags[i]);
-      }
-    }
-    return bQuery;
-  }
-
-  /**
-   * Parses a query, searching on the fields specified.
-   * Use this if you need to specify certain fields as required,
-   * and others as prohibited.
-   * <p><pre>
-   * Usage:
-   * <code>
-   * String[] query = {"query1", "query2", "query3"};
-   * String[] fields = {"filename", "contents", "description"};
-   * BooleanClause.Occur[] flags = {BooleanClause.Occur.SHOULD,
-   *                BooleanClause.Occur.MUST,
-   *                BooleanClause.Occur.MUST_NOT};
-   * MultiFieldQueryParser.parse(query, fields, flags, analyzer);
-   * </code>
-   * </pre>
-   *<p>
-   * The code above would construct a query:
-   * <pre>
-   * <code>
-   * (filename:query1) +(contents:query2) -(description:query3)
-   * </code>
-   * </pre>
-   *
-   * @param matchVersion Lucene version to match; this is passed through to QueryParser.
-   * @param queries Queries string to parse
-   * @param fields Fields to search on
-   * @param flags Flags describing the fields
-   * @param analyzer Analyzer to use
-   * @throws ParseException if query parsing fails
-   * @throws IllegalArgumentException if the length of the queries, fields,
-   *  and flags array differ
-   */
-  public static Query parse(Version matchVersion, String[] queries, String[] fields, BooleanClause.Occur[] flags,
-      Analyzer analyzer) throws ParseException
-  {
-    if (!(queries.length == fields.length && queries.length == flags.length))
-      throw new IllegalArgumentException("queries, fields, and flags array have have different length");
-    BooleanQuery bQuery = new BooleanQuery();
-    for (int i = 0; i < fields.length; i++)
-    {
-      QueryParser qp = new QueryParser(matchVersion, fields[i], analyzer);
-      Query q = qp.parse(queries[i]);
-      if (q!=null && // q never null, just being defensive
-          (!(q instanceof BooleanQuery) || ((BooleanQuery)q).getClauses().length>0)) {
-        bQuery.add(q, flags[i]);
-      }
-    }
-    return bQuery;
-  }
-
-}
diff --git a/lucene/src/java/org/apache/lucene/queryParser/ParseException.java b/lucene/src/java/org/apache/lucene/queryParser/ParseException.java
deleted file mode 100644
index b48a446..0000000
--- a/lucene/src/java/org/apache/lucene/queryParser/ParseException.java
+++ /dev/null
@@ -1,198 +0,0 @@
-/* Generated By:JavaCC: Do not edit this line. ParseException.java Version 4.1 */
-/* JavaCCOptions:KEEP_LINE_COL=null */
-package org.apache.lucene.queryParser;
-
-/**
- * This exception is thrown when parse errors are encountered.
- * You can explicitly create objects of this exception type by
- * calling the method generateParseException in the generated
- * parser.
- *
- * You can modify this class to customize your error reporting
- * mechanisms so long as you retain the public fields.
- */
-public class ParseException extends Exception {
-
-  /**
-   * This constructor is used by the method "generateParseException"
-   * in the generated parser.  Calling this constructor generates
-   * a new object of this type with the fields "currentToken",
-   * "expectedTokenSequences", and "tokenImage" set.  The boolean
-   * flag "specialConstructor" is also set to true to indicate that
-   * this constructor was used to create this object.
-   * This constructor calls its super class with the empty string
-   * to force the "toString" method of parent class "Throwable" to
-   * print the error message in the form:
-   *     ParseException: <result of getMessage>
-   */
-  public ParseException(Token currentTokenVal,
-                        int[][] expectedTokenSequencesVal,
-                        String[] tokenImageVal
-                       )
-  {
-    super("");
-    specialConstructor = true;
-    currentToken = currentTokenVal;
-    expectedTokenSequences = expectedTokenSequencesVal;
-    tokenImage = tokenImageVal;
-  }
-
-  /**
-   * The following constructors are for use by you for whatever
-   * purpose you can think of.  Constructing the exception in this
-   * manner makes the exception behave in the normal way - i.e., as
-   * documented in the class "Throwable".  The fields "errorToken",
-   * "expectedTokenSequences", and "tokenImage" do not contain
-   * relevant information.  The JavaCC generated code does not use
-   * these constructors.
-   */
-
-  public ParseException() {
-    super();
-    specialConstructor = false;
-  }
-
-  /** Constructor with message. */
-  public ParseException(String message) {
-    super(message);
-    specialConstructor = false;
-  }
-
-  /**
-   * This variable determines which constructor was used to create
-   * this object and thereby affects the semantics of the
-   * "getMessage" method (see below).
-   */
-  protected boolean specialConstructor;
-
-  /**
-   * This is the last token that has been consumed successfully.  If
-   * this object has been created due to a parse error, the token
-   * followng this token will (therefore) be the first error token.
-   */
-  public Token currentToken;
-
-  /**
-   * Each entry in this array is an array of integers.  Each array
-   * of integers represents a sequence of tokens (by their ordinal
-   * values) that is expected at this point of the parse.
-   */
-  public int[][] expectedTokenSequences;
-
-  /**
-   * This is a reference to the "tokenImage" array of the generated
-   * parser within which the parse error occurred.  This array is
-   * defined in the generated ...Constants interface.
-   */
-  public String[] tokenImage;
-
-  /**
-   * This method has the standard behavior when this object has been
-   * created using the standard constructors.  Otherwise, it uses
-   * "currentToken" and "expectedTokenSequences" to generate a parse
-   * error message and returns it.  If this object has been created
-   * due to a parse error, and you do not catch it (it gets thrown
-   * from the parser), then this method is called during the printing
-   * of the final stack trace, and hence the correct error message
-   * gets displayed.
-   */
-  public String getMessage() {
-    if (!specialConstructor) {
-      return super.getMessage();
-    }
-    StringBuffer expected = new StringBuffer();
-    int maxSize = 0;
-    for (int i = 0; i < expectedTokenSequences.length; i++) {
-      if (maxSize < expectedTokenSequences[i].length) {
-        maxSize = expectedTokenSequences[i].length;
-      }
-      for (int j = 0; j < expectedTokenSequences[i].length; j++) {
-        expected.append(tokenImage[expectedTokenSequences[i][j]]).append(' ');
-      }
-      if (expectedTokenSequences[i][expectedTokenSequences[i].length - 1] != 0) {
-        expected.append("...");
-      }
-      expected.append(eol).append("    ");
-    }
-    String retval = "Encountered \"";
-    Token tok = currentToken.next;
-    for (int i = 0; i < maxSize; i++) {
-      if (i != 0) retval += " ";
-      if (tok.kind == 0) {
-        retval += tokenImage[0];
-        break;
-      }
-      retval += " " + tokenImage[tok.kind];
-      retval += " \"";
-      retval += add_escapes(tok.image);
-      retval += " \"";
-      tok = tok.next;
-    }
-    retval += "\" at line " + currentToken.next.beginLine + ", column " + currentToken.next.beginColumn;
-    retval += "." + eol;
-    if (expectedTokenSequences.length == 1) {
-      retval += "Was expecting:" + eol + "    ";
-    } else {
-      retval += "Was expecting one of:" + eol + "    ";
-    }
-    retval += expected.toString();
-    return retval;
-  }
-
-  /**
-   * The end of line string for this machine.
-   */
-  protected String eol = System.getProperty("line.separator", "\n");
-
-  /**
-   * Used to convert raw characters to their escaped version
-   * when these raw version cannot be used as part of an ASCII
-   * string literal.
-   */
-  protected String add_escapes(String str) {
-      StringBuffer retval = new StringBuffer();
-      char ch;
-      for (int i = 0; i < str.length(); i++) {
-        switch (str.charAt(i))
-        {
-           case 0 :
-              continue;
-           case '\b':
-              retval.append("\\b");
-              continue;
-           case '\t':
-              retval.append("\\t");
-              continue;
-           case '\n':
-              retval.append("\\n");
-              continue;
-           case '\f':
-              retval.append("\\f");
-              continue;
-           case '\r':
-              retval.append("\\r");
-              continue;
-           case '\"':
-              retval.append("\\\"");
-              continue;
-           case '\'':
-              retval.append("\\\'");
-              continue;
-           case '\\':
-              retval.append("\\\\");
-              continue;
-           default:
-              if ((ch = str.charAt(i)) < 0x20 || ch > 0x7e) {
-                 String s = "0000" + Integer.toString(ch, 16);
-                 retval.append("\\u" + s.substring(s.length() - 4, s.length()));
-              } else {
-                 retval.append(ch);
-              }
-              continue;
-        }
-      }
-      return retval.toString();
-   }
-
-}
-/* JavaCC - OriginalChecksum=c7631a240f7446940695eac31d9483ca (do not edit this line) */
diff --git a/lucene/src/java/org/apache/lucene/queryParser/QueryParser.java b/lucene/src/java/org/apache/lucene/queryParser/QueryParser.java
deleted file mode 100644
index f3ed348..0000000
--- a/lucene/src/java/org/apache/lucene/queryParser/QueryParser.java
+++ /dev/null
@@ -1,754 +0,0 @@
-/* Generated By:JavaCC: Do not edit this line. QueryParser.java */
-package org.apache.lucene.queryParser;
-
-import java.io.StringReader;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Locale;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.document.DateTools;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermRangeQuery;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.util.Version;
-
-/**
- * This class is generated by JavaCC.  The most important method is
- * {@link #parse(String)}.
- *
- * The syntax for query strings is as follows:
- * A Query is a series of clauses.
- * A clause may be prefixed by:
- * <ul>
- * <li> a plus (<code>+</code>) or a minus (<code>-</code>) sign, indicating
- * that the clause is required or prohibited respectively; or
- * <li> a term followed by a colon, indicating the field to be searched.
- * This enables one to construct queries which search multiple fields.
- * </ul>
- *
- * A clause may be either:
- * <ul>
- * <li> a term, indicating all the documents that contain this term; or
- * <li> a nested query, enclosed in parentheses.  Note that this may be used
- * with a <code>+</code>/<code>-</code> prefix to require any of a set of
- * terms.
- * </ul>
- *
- * Thus, in BNF, the query grammar is:
- * <pre>
- *   Query  ::= ( Clause )*
- *   Clause ::= ["+", "-"] [&lt;TERM&gt; ":"] ( &lt;TERM&gt; | "(" Query ")" )
- * </pre>
- *
- * <p>
- * Examples of appropriately formatted queries can be found in the <a
- * href="../../../../../../queryparsersyntax.html">query syntax
- * documentation</a>.
- * </p>
- *
- * <p>
- * In {@link TermRangeQuery}s, QueryParser tries to detect date values, e.g.
- * <tt>date:[6/1/2005 TO 6/4/2005]</tt> produces a range query that searches
- * for "date" fields between 2005-06-01 and 2005-06-04. Note that the format
- * of the accepted input depends on {@link #setLocale(Locale) the locale}.
- * A {@link org.apache.lucene.document.DateTools.Resolution} has to be set,
- * if you want to use {@link DateTools} for date conversion.
- * </p>
- * <p>
- * The date resolution that shall be used for RangeQueries can be set
- * using {@link #setDateResolution(DateTools.Resolution)}
- * or {@link #setDateResolution(String, DateTools.Resolution)}. The former
- * sets the default date resolution for all fields, whereas the latter can
- * be used to set field specific date resolutions. Field specific date
- * resolutions take, if set, precedence over the default date resolution.
- * </p>
- * <p>
- * If you don't use {@link DateTools} in your index, you can create your own
- * query parser that inherits QueryParser and overwrites
- * {@link #getRangeQuery(String, String, String, boolean, boolean)} to
- * use a different method for date conversion.
- * </p>
- *
- * <p>Note that QueryParser is <em>not</em> thread-safe.</p> 
- * 
- * <p><b>NOTE</b>: there is a new QueryParser in contrib, which matches
- * the same syntax as this class, but is more modular,
- * enabling substantial customization to how a query is created.
- *
- * <a name="version"/>
- * <p><b>NOTE</b>: You must specify the required {@link Version}
- * compatibility when creating QueryParser:
- * <ul>
- *    <li> As of 3.1, {@link #setAutoGeneratePhraseQueries} is false by
- *         default.
- * </ul>
- */
-public class QueryParser extends QueryParserBase implements QueryParserConstants {
-  /** The default operator for parsing queries.
-   * Use {@link QueryParserBase#setDefaultOperator} to change it.
-   */
-  static public enum Operator { OR, AND }
-
-  /** Create a query parser.
-   *  @param matchVersion  Lucene version to match. See <a href="#version">above</a>.
-   *  @param f  the default field for query terms.
-   *  @param a   used to find terms in the query text.
-   */
-   public QueryParser(Version matchVersion, String f, Analyzer a) {
-    this(new FastCharStream(new StringReader("")));
-    init(matchVersion, f, a);
-  }
-
-// *   Query  ::= ( Clause )*
-// *   Clause ::= ["+", "-"] [<TERM> ":"] ( <TERM> | "(" Query ")" )
-  final public int Conjunction() throws ParseException {
-  int ret = CONJ_NONE;
-    switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-    case AND:
-    case OR:
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case AND:
-        jj_consume_token(AND);
-            ret = CONJ_AND;
-        break;
-      case OR:
-        jj_consume_token(OR);
-              ret = CONJ_OR;
-        break;
-      default:
-        jj_la1[0] = jj_gen;
-        jj_consume_token(-1);
-        throw new ParseException();
-      }
-      break;
-    default:
-      jj_la1[1] = jj_gen;
-      ;
-    }
-    {if (true) return ret;}
-    throw new Error("Missing return statement in function");
-  }
-
-  final public int Modifiers() throws ParseException {
-  int ret = MOD_NONE;
-    switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-    case NOT:
-    case PLUS:
-    case MINUS:
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case PLUS:
-        jj_consume_token(PLUS);
-              ret = MOD_REQ;
-        break;
-      case MINUS:
-        jj_consume_token(MINUS);
-                 ret = MOD_NOT;
-        break;
-      case NOT:
-        jj_consume_token(NOT);
-               ret = MOD_NOT;
-        break;
-      default:
-        jj_la1[2] = jj_gen;
-        jj_consume_token(-1);
-        throw new ParseException();
-      }
-      break;
-    default:
-      jj_la1[3] = jj_gen;
-      ;
-    }
-    {if (true) return ret;}
-    throw new Error("Missing return statement in function");
-  }
-
-// This makes sure that there is no garbage after the query string
-  final public Query TopLevelQuery(String field) throws ParseException {
-        Query q;
-    q = Query(field);
-    jj_consume_token(0);
-                {if (true) return q;}
-    throw new Error("Missing return statement in function");
-  }
-
-  final public Query Query(String field) throws ParseException {
-  List<BooleanClause> clauses = new ArrayList<BooleanClause>();
-  Query q, firstQuery=null;
-  int conj, mods;
-    mods = Modifiers();
-    q = Clause(field);
-    addClause(clauses, CONJ_NONE, mods, q);
-    if (mods == MOD_NONE)
-        firstQuery=q;
-    label_1:
-    while (true) {
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case AND:
-      case OR:
-      case NOT:
-      case PLUS:
-      case MINUS:
-      case BAREOPER:
-      case LPAREN:
-      case STAR:
-      case QUOTED:
-      case TERM:
-      case PREFIXTERM:
-      case WILDTERM:
-      case REGEXPTERM:
-      case RANGEIN_START:
-      case RANGEEX_START:
-      case NUMBER:
-        ;
-        break;
-      default:
-        jj_la1[4] = jj_gen;
-        break label_1;
-      }
-      conj = Conjunction();
-      mods = Modifiers();
-      q = Clause(field);
-      addClause(clauses, conj, mods, q);
-    }
-      if (clauses.size() == 1 && firstQuery != null)
-        {if (true) return firstQuery;}
-      else {
-  {if (true) return getBooleanQuery(clauses);}
-      }
-    throw new Error("Missing return statement in function");
-  }
-
-  final public Query Clause(String field) throws ParseException {
-  Query q;
-  Token fieldToken=null, boost=null;
-    if (jj_2_1(2)) {
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case TERM:
-        fieldToken = jj_consume_token(TERM);
-        jj_consume_token(COLON);
-                               field=discardEscapeChar(fieldToken.image);
-        break;
-      case STAR:
-        jj_consume_token(STAR);
-        jj_consume_token(COLON);
-                      field="*";
-        break;
-      default:
-        jj_la1[5] = jj_gen;
-        jj_consume_token(-1);
-        throw new ParseException();
-      }
-    } else {
-      ;
-    }
-    switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-    case BAREOPER:
-    case STAR:
-    case QUOTED:
-    case TERM:
-    case PREFIXTERM:
-    case WILDTERM:
-    case REGEXPTERM:
-    case RANGEIN_START:
-    case RANGEEX_START:
-    case NUMBER:
-      q = Term(field);
-      break;
-    case LPAREN:
-      jj_consume_token(LPAREN);
-      q = Query(field);
-      jj_consume_token(RPAREN);
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case CARAT:
-        jj_consume_token(CARAT);
-        boost = jj_consume_token(NUMBER);
-        break;
-      default:
-        jj_la1[6] = jj_gen;
-        ;
-      }
-      break;
-    default:
-      jj_la1[7] = jj_gen;
-      jj_consume_token(-1);
-      throw new ParseException();
-    }
-       {if (true) return handleBoost(q, boost);}
-    throw new Error("Missing return statement in function");
-  }
-
-  final public Query Term(String field) throws ParseException {
-  Token term, boost=null, fuzzySlop=null, goop1, goop2;
-  boolean prefix = false;
-  boolean wildcard = false;
-  boolean fuzzy = false;
-  boolean regexp = false;
-  boolean startInc=false;
-  boolean endInc=false;
-  Query q;
-    switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-    case BAREOPER:
-    case STAR:
-    case TERM:
-    case PREFIXTERM:
-    case WILDTERM:
-    case REGEXPTERM:
-    case NUMBER:
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case TERM:
-        term = jj_consume_token(TERM);
-        break;
-      case STAR:
-        term = jj_consume_token(STAR);
-                       wildcard=true;
-        break;
-      case PREFIXTERM:
-        term = jj_consume_token(PREFIXTERM);
-                             prefix=true;
-        break;
-      case WILDTERM:
-        term = jj_consume_token(WILDTERM);
-                           wildcard=true;
-        break;
-      case REGEXPTERM:
-        term = jj_consume_token(REGEXPTERM);
-                             regexp=true;
-        break;
-      case NUMBER:
-        term = jj_consume_token(NUMBER);
-        break;
-      case BAREOPER:
-        term = jj_consume_token(BAREOPER);
-                           term.image = term.image.substring(0,1);
-        break;
-      default:
-        jj_la1[8] = jj_gen;
-        jj_consume_token(-1);
-        throw new ParseException();
-      }
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case FUZZY_SLOP:
-        fuzzySlop = jj_consume_token(FUZZY_SLOP);
-                                fuzzy=true;
-        break;
-      default:
-        jj_la1[9] = jj_gen;
-        ;
-      }
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case CARAT:
-        jj_consume_token(CARAT);
-        boost = jj_consume_token(NUMBER);
-        switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-        case FUZZY_SLOP:
-          fuzzySlop = jj_consume_token(FUZZY_SLOP);
-                                                         fuzzy=true;
-          break;
-        default:
-          jj_la1[10] = jj_gen;
-          ;
-        }
-        break;
-      default:
-        jj_la1[11] = jj_gen;
-        ;
-      }
-       q = handleBareTokenQuery(field, term, fuzzySlop, prefix, wildcard, fuzzy, regexp);
-      break;
-    case RANGEIN_START:
-    case RANGEEX_START:
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case RANGEIN_START:
-        jj_consume_token(RANGEIN_START);
-                            startInc=true;
-        break;
-      case RANGEEX_START:
-        jj_consume_token(RANGEEX_START);
-        break;
-      default:
-        jj_la1[12] = jj_gen;
-        jj_consume_token(-1);
-        throw new ParseException();
-      }
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case RANGE_GOOP:
-        goop1 = jj_consume_token(RANGE_GOOP);
-        break;
-      case RANGE_QUOTED:
-        goop1 = jj_consume_token(RANGE_QUOTED);
-        break;
-      default:
-        jj_la1[13] = jj_gen;
-        jj_consume_token(-1);
-        throw new ParseException();
-      }
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case RANGE_TO:
-        jj_consume_token(RANGE_TO);
-        break;
-      default:
-        jj_la1[14] = jj_gen;
-        ;
-      }
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case RANGE_GOOP:
-        goop2 = jj_consume_token(RANGE_GOOP);
-        break;
-      case RANGE_QUOTED:
-        goop2 = jj_consume_token(RANGE_QUOTED);
-        break;
-      default:
-        jj_la1[15] = jj_gen;
-        jj_consume_token(-1);
-        throw new ParseException();
-      }
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case RANGEIN_END:
-        jj_consume_token(RANGEIN_END);
-                          endInc=true;
-        break;
-      case RANGEEX_END:
-        jj_consume_token(RANGEEX_END);
-        break;
-      default:
-        jj_la1[16] = jj_gen;
-        jj_consume_token(-1);
-        throw new ParseException();
-      }
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case CARAT:
-        jj_consume_token(CARAT);
-        boost = jj_consume_token(NUMBER);
-        break;
-      default:
-        jj_la1[17] = jj_gen;
-        ;
-      }
-          boolean startOpen=false;
-          boolean endOpen=false;
-          if (goop1.kind == RANGE_QUOTED) {
-            goop1.image = goop1.image.substring(1, goop1.image.length()-1);
-          } else if ("*".equals(goop1.image)) {
-            startOpen=true;
-          }
-          if (goop2.kind == RANGE_QUOTED) {
-            goop2.image = goop2.image.substring(1, goop2.image.length()-1);
-          } else if ("*".equals(goop2.image)) {
-            endOpen=true;
-          }
-          q = getRangeQuery(field, startOpen ? null : discardEscapeChar(goop1.image), endOpen ? null : discardEscapeChar(goop2.image), startInc, endInc);
-      break;
-    case QUOTED:
-      term = jj_consume_token(QUOTED);
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case FUZZY_SLOP:
-        fuzzySlop = jj_consume_token(FUZZY_SLOP);
-        break;
-      default:
-        jj_la1[18] = jj_gen;
-        ;
-      }
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case CARAT:
-        jj_consume_token(CARAT);
-        boost = jj_consume_token(NUMBER);
-        break;
-      default:
-        jj_la1[19] = jj_gen;
-        ;
-      }
-         q = handleQuotedTerm(field, term, fuzzySlop);
-      break;
-    default:
-      jj_la1[20] = jj_gen;
-      jj_consume_token(-1);
-      throw new ParseException();
-    }
-    {if (true) return handleBoost(q, boost);}
-    throw new Error("Missing return statement in function");
-  }
-
-  private boolean jj_2_1(int xla) {
-    jj_la = xla; jj_lastpos = jj_scanpos = token;
-    try { return !jj_3_1(); }
-    catch(LookaheadSuccess ls) { return true; }
-    finally { jj_save(0, xla); }
-  }
-
-  private boolean jj_3R_2() {
-    if (jj_scan_token(TERM)) return true;
-    if (jj_scan_token(COLON)) return true;
-    return false;
-  }
-
-  private boolean jj_3_1() {
-    Token xsp;
-    xsp = jj_scanpos;
-    if (jj_3R_2()) {
-    jj_scanpos = xsp;
-    if (jj_3R_3()) return true;
-    }
-    return false;
-  }
-
-  private boolean jj_3R_3() {
-    if (jj_scan_token(STAR)) return true;
-    if (jj_scan_token(COLON)) return true;
-    return false;
-  }
-
-  /** Generated Token Manager. */
-  public QueryParserTokenManager token_source;
-  /** Current token. */
-  public Token token;
-  /** Next token. */
-  public Token jj_nt;
-  private int jj_ntk;
-  private Token jj_scanpos, jj_lastpos;
-  private int jj_la;
-  private int jj_gen;
-  final private int[] jj_la1 = new int[21];
-  static private int[] jj_la1_0;
-  static private int[] jj_la1_1;
-  static {
-      jj_la1_init_0();
-      jj_la1_init_1();
-   }
-   private static void jj_la1_init_0() {
-      jj_la1_0 = new int[] {0x300,0x300,0x1c00,0x1c00,0xfda7f00,0x120000,0x40000,0xfda6000,0x9d22000,0x200000,0x200000,0x40000,0x6000000,0x80000000,0x10000000,0x80000000,0x60000000,0x40000,0x200000,0x40000,0xfda2000,};
-   }
-   private static void jj_la1_init_1() {
-      jj_la1_1 = new int[] {0x0,0x0,0x0,0x0,0x0,0x0,0x0,0x0,0x0,0x0,0x0,0x0,0x0,0x1,0x0,0x1,0x0,0x0,0x0,0x0,0x0,};
-   }
-  final private JJCalls[] jj_2_rtns = new JJCalls[1];
-  private boolean jj_rescan = false;
-  private int jj_gc = 0;
-
-  /** Constructor with user supplied CharStream. */
-  protected QueryParser(CharStream stream) {
-    token_source = new QueryParserTokenManager(stream);
-    token = new Token();
-    jj_ntk = -1;
-    jj_gen = 0;
-    for (int i = 0; i < 21; i++) jj_la1[i] = -1;
-    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
-  }
-
-  /** Reinitialise. */
-  public void ReInit(CharStream stream) {
-    token_source.ReInit(stream);
-    token = new Token();
-    jj_ntk = -1;
-    jj_gen = 0;
-    for (int i = 0; i < 21; i++) jj_la1[i] = -1;
-    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
-  }
-
-  /** Constructor with generated Token Manager. */
-  protected QueryParser(QueryParserTokenManager tm) {
-    token_source = tm;
-    token = new Token();
-    jj_ntk = -1;
-    jj_gen = 0;
-    for (int i = 0; i < 21; i++) jj_la1[i] = -1;
-    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
-  }
-
-  /** Reinitialise. */
-  public void ReInit(QueryParserTokenManager tm) {
-    token_source = tm;
-    token = new Token();
-    jj_ntk = -1;
-    jj_gen = 0;
-    for (int i = 0; i < 21; i++) jj_la1[i] = -1;
-    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
-  }
-
-  private Token jj_consume_token(int kind) throws ParseException {
-    Token oldToken;
-    if ((oldToken = token).next != null) token = token.next;
-    else token = token.next = token_source.getNextToken();
-    jj_ntk = -1;
-    if (token.kind == kind) {
-      jj_gen++;
-      if (++jj_gc > 100) {
-        jj_gc = 0;
-        for (int i = 0; i < jj_2_rtns.length; i++) {
-          JJCalls c = jj_2_rtns[i];
-          while (c != null) {
-            if (c.gen < jj_gen) c.first = null;
-            c = c.next;
-          }
-        }
-      }
-      return token;
-    }
-    token = oldToken;
-    jj_kind = kind;
-    throw generateParseException();
-  }
-
-  static private final class LookaheadSuccess extends java.lang.Error { }
-  final private LookaheadSuccess jj_ls = new LookaheadSuccess();
-  private boolean jj_scan_token(int kind) {
-    if (jj_scanpos == jj_lastpos) {
-      jj_la--;
-      if (jj_scanpos.next == null) {
-        jj_lastpos = jj_scanpos = jj_scanpos.next = token_source.getNextToken();
-      } else {
-        jj_lastpos = jj_scanpos = jj_scanpos.next;
-      }
-    } else {
-      jj_scanpos = jj_scanpos.next;
-    }
-    if (jj_rescan) {
-      int i = 0; Token tok = token;
-      while (tok != null && tok != jj_scanpos) { i++; tok = tok.next; }
-      if (tok != null) jj_add_error_token(kind, i);
-    }
-    if (jj_scanpos.kind != kind) return true;
-    if (jj_la == 0 && jj_scanpos == jj_lastpos) throw jj_ls;
-    return false;
-  }
-
-
-/** Get the next Token. */
-  final public Token getNextToken() {
-    if (token.next != null) token = token.next;
-    else token = token.next = token_source.getNextToken();
-    jj_ntk = -1;
-    jj_gen++;
-    return token;
-  }
-
-/** Get the specific Token. */
-  final public Token getToken(int index) {
-    Token t = token;
-    for (int i = 0; i < index; i++) {
-      if (t.next != null) t = t.next;
-      else t = t.next = token_source.getNextToken();
-    }
-    return t;
-  }
-
-  private int jj_ntk() {
-    if ((jj_nt=token.next) == null)
-      return (jj_ntk = (token.next=token_source.getNextToken()).kind);
-    else
-      return (jj_ntk = jj_nt.kind);
-  }
-
-  private java.util.List<int[]> jj_expentries = new java.util.ArrayList<int[]>();
-  private int[] jj_expentry;
-  private int jj_kind = -1;
-  private int[] jj_lasttokens = new int[100];
-  private int jj_endpos;
-
-  private void jj_add_error_token(int kind, int pos) {
-    if (pos >= 100) return;
-    if (pos == jj_endpos + 1) {
-      jj_lasttokens[jj_endpos++] = kind;
-    } else if (jj_endpos != 0) {
-      jj_expentry = new int[jj_endpos];
-      for (int i = 0; i < jj_endpos; i++) {
-        jj_expentry[i] = jj_lasttokens[i];
-      }
-      jj_entries_loop: for (java.util.Iterator it = jj_expentries.iterator(); it.hasNext();) {
-        int[] oldentry = (int[])(it.next());
-        if (oldentry.length == jj_expentry.length) {
-          for (int i = 0; i < jj_expentry.length; i++) {
-            if (oldentry[i] != jj_expentry[i]) {
-              continue jj_entries_loop;
-            }
-          }
-          jj_expentries.add(jj_expentry);
-          break jj_entries_loop;
-        }
-      }
-      if (pos != 0) jj_lasttokens[(jj_endpos = pos) - 1] = kind;
-    }
-  }
-
-  /** Generate ParseException. */
-  public ParseException generateParseException() {
-    jj_expentries.clear();
-    boolean[] la1tokens = new boolean[33];
-    if (jj_kind >= 0) {
-      la1tokens[jj_kind] = true;
-      jj_kind = -1;
-    }
-    for (int i = 0; i < 21; i++) {
-      if (jj_la1[i] == jj_gen) {
-        for (int j = 0; j < 32; j++) {
-          if ((jj_la1_0[i] & (1<<j)) != 0) {
-            la1tokens[j] = true;
-          }
-          if ((jj_la1_1[i] & (1<<j)) != 0) {
-            la1tokens[32+j] = true;
-          }
-        }
-      }
-    }
-    for (int i = 0; i < 33; i++) {
-      if (la1tokens[i]) {
-        jj_expentry = new int[1];
-        jj_expentry[0] = i;
-        jj_expentries.add(jj_expentry);
-      }
-    }
-    jj_endpos = 0;
-    jj_rescan_token();
-    jj_add_error_token(0, 0);
-    int[][] exptokseq = new int[jj_expentries.size()][];
-    for (int i = 0; i < jj_expentries.size(); i++) {
-      exptokseq[i] = jj_expentries.get(i);
-    }
-    return new ParseException(token, exptokseq, tokenImage);
-  }
-
-  /** Enable tracing. */
-  final public void enable_tracing() {
-  }
-
-  /** Disable tracing. */
-  final public void disable_tracing() {
-  }
-
-  private void jj_rescan_token() {
-    jj_rescan = true;
-    for (int i = 0; i < 1; i++) {
-    try {
-      JJCalls p = jj_2_rtns[i];
-      do {
-        if (p.gen > jj_gen) {
-          jj_la = p.arg; jj_lastpos = jj_scanpos = p.first;
-          switch (i) {
-            case 0: jj_3_1(); break;
-          }
-        }
-        p = p.next;
-      } while (p != null);
-      } catch(LookaheadSuccess ls) { }
-    }
-    jj_rescan = false;
-  }
-
-  private void jj_save(int index, int xla) {
-    JJCalls p = jj_2_rtns[index];
-    while (p.gen > jj_gen) {
-      if (p.next == null) { p = p.next = new JJCalls(); break; }
-      p = p.next;
-    }
-    p.gen = jj_gen + xla - jj_la; p.first = token; p.arg = xla;
-  }
-
-  static final class JJCalls {
-    int gen;
-    Token first;
-    int arg;
-    JJCalls next;
-  }
-
-}
diff --git a/lucene/src/java/org/apache/lucene/queryParser/QueryParser.jj b/lucene/src/java/org/apache/lucene/queryParser/QueryParser.jj
deleted file mode 100644
index 21087bd..0000000
--- a/lucene/src/java/org/apache/lucene/queryParser/QueryParser.jj
+++ /dev/null
@@ -1,324 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-options {
-  STATIC=false;
-  JAVA_UNICODE_ESCAPE=true;
-  USER_CHAR_STREAM=true;
-}
-
-PARSER_BEGIN(QueryParser)
-
-package org.apache.lucene.queryParser;
-
-import java.io.StringReader;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Locale;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.document.DateTools;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermRangeQuery;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.util.Version;
-
-/**
- * This class is generated by JavaCC.  The most important method is
- * {@link #parse(String)}.
- *
- * The syntax for query strings is as follows:
- * A Query is a series of clauses.
- * A clause may be prefixed by:
- * <ul>
- * <li> a plus (<code>+</code>) or a minus (<code>-</code>) sign, indicating
- * that the clause is required or prohibited respectively; or
- * <li> a term followed by a colon, indicating the field to be searched.
- * This enables one to construct queries which search multiple fields.
- * </ul>
- *
- * A clause may be either:
- * <ul>
- * <li> a term, indicating all the documents that contain this term; or
- * <li> a nested query, enclosed in parentheses.  Note that this may be used
- * with a <code>+</code>/<code>-</code> prefix to require any of a set of
- * terms.
- * </ul>
- *
- * Thus, in BNF, the query grammar is:
- * <pre>
- *   Query  ::= ( Clause )*
- *   Clause ::= ["+", "-"] [&lt;TERM&gt; ":"] ( &lt;TERM&gt; | "(" Query ")" )
- * </pre>
- *
- * <p>
- * Examples of appropriately formatted queries can be found in the <a
- * href="../../../../../../queryparsersyntax.html">query syntax
- * documentation</a>.
- * </p>
- *
- * <p>
- * In {@link TermRangeQuery}s, QueryParser tries to detect date values, e.g.
- * <tt>date:[6/1/2005 TO 6/4/2005]</tt> produces a range query that searches
- * for "date" fields between 2005-06-01 and 2005-06-04. Note that the format
- * of the accepted input depends on {@link #setLocale(Locale) the locale}.
- * A {@link org.apache.lucene.document.DateTools.Resolution} has to be set,
- * if you want to use {@link DateTools} for date conversion.
- * </p>
- * <p>
- * The date resolution that shall be used for RangeQueries can be set
- * using {@link #setDateResolution(DateTools.Resolution)}
- * or {@link #setDateResolution(String, DateTools.Resolution)}. The former
- * sets the default date resolution for all fields, whereas the latter can
- * be used to set field specific date resolutions. Field specific date
- * resolutions take, if set, precedence over the default date resolution.
- * </p>
- * <p>
- * If you don't use {@link DateTools} in your index, you can create your own
- * query parser that inherits QueryParser and overwrites
- * {@link #getRangeQuery(String, String, String, boolean, boolean)} to
- * use a different method for date conversion.
- * </p>
- *
- * <p>Note that QueryParser is <em>not</em> thread-safe.</p> 
- * 
- * <p><b>NOTE</b>: there is a new QueryParser in contrib, which matches
- * the same syntax as this class, but is more modular,
- * enabling substantial customization to how a query is created.
- *
- * <a name="version"/>
- * <p><b>NOTE</b>: You must specify the required {@link Version}
- * compatibility when creating QueryParser:
- * <ul>
- *    <li> As of 3.1, {@link #setAutoGeneratePhraseQueries} is false by
- *         default.
- * </ul>
- */
-public class QueryParser extends QueryParserBase {
-  /** The default operator for parsing queries.
-   * Use {@link QueryParserBase#setDefaultOperator} to change it.
-   */
-  static public enum Operator { OR, AND }
-  
-  /** Create a query parser.
-   *  @param matchVersion  Lucene version to match. See <a href="#version">above</a>.
-   *  @param f  the default field for query terms.
-   *  @param a   used to find terms in the query text.
-   */
-   public QueryParser(Version matchVersion, String f, Analyzer a) {
-    this(new FastCharStream(new StringReader("")));
-    init(matchVersion, f, a);
-  }
-}
-
-PARSER_END(QueryParser)
-
-/* ***************** */
-/* Token Definitions */
-/* ***************** */
-
-<*> TOKEN : {
-  <#_NUM_CHAR:   ["0"-"9"] >
-// every character that follows a backslash is considered as an escaped character
-| <#_ESCAPED_CHAR: "\\" ~[] >
-| <#_TERM_START_CHAR: ( ~[ " ", "\t", "\n", "\r", "\u3000", "+", "-", "!", "(", ")", ":", "^",
-                           "[", "]", "\"", "{", "}", "~", "*", "?", "\\" ]
-                       | <_ESCAPED_CHAR> ) >
-| <#_TERM_CHAR: ( <_TERM_START_CHAR> | <_ESCAPED_CHAR> | "-" | "+" ) >
-| <#_WHITESPACE: ( " " | "\t" | "\n" | "\r" | "\u3000") >
-| <#_QUOTED_CHAR: ( ~[ "\"", "\\" ] | <_ESCAPED_CHAR> ) >
-}
-
-<DEFAULT, Range> SKIP : {
-  < <_WHITESPACE>>
-}
-
-<DEFAULT> TOKEN : {
-  <AND:       ("AND" | "&&") >
-| <OR:        ("OR" | "||") >
-| <NOT:       ("NOT" | "!") >
-| <PLUS:      "+" >
-| <MINUS:     "-" >
-| <BAREOPER:    ("+"|"-"|"!") <_WHITESPACE> >
-| <LPAREN:    "(" >
-| <RPAREN:    ")" >
-| <COLON:     ":" >
-| <STAR:      "*" >
-| <CARAT:     "^" > : Boost
-| <QUOTED:     "\"" (<_QUOTED_CHAR>)* "\"">
-| <TERM:      <_TERM_START_CHAR> (<_TERM_CHAR>)*  >
-| <FUZZY_SLOP:     "~" ( (<_NUM_CHAR>)+ ( "." (<_NUM_CHAR>)+ )? )? >
-| <PREFIXTERM:  ("*") | ( <_TERM_START_CHAR> (<_TERM_CHAR>)* "*" ) >
-| <WILDTERM:  (<_TERM_START_CHAR> | [ "*", "?" ]) (<_TERM_CHAR> | ( [ "*", "?" ] ))* >
-| <REGEXPTERM: "/" (~[ "/" ] | "\\/" )* "/" >
-| <RANGEIN_START: "[" > : Range
-| <RANGEEX_START: "{" > : Range
-}
-
-<Boost> TOKEN : {
-<NUMBER:    (<_NUM_CHAR>)+ ( "." (<_NUM_CHAR>)+ )? > : DEFAULT
-}
-
-<Range> TOKEN : {
-<RANGE_TO: "TO">
-| <RANGEIN_END: "]"> : DEFAULT
-| <RANGEEX_END: "}"> : DEFAULT
-| <RANGE_QUOTED: "\"" (~["\""] | "\\\"")+ "\"">
-| <RANGE_GOOP: (~[ " ", "]", "}" ])+ >
-}
-
-// *   Query  ::= ( Clause )*
-// *   Clause ::= ["+", "-"] [<TERM> ":"] ( <TERM> | "(" Query ")" )
-
-int Conjunction() : {
-  int ret = CONJ_NONE;
-}
-{
-  [
-    <AND> { ret = CONJ_AND; }
-    | <OR>  { ret = CONJ_OR; }
-  ]
-  { return ret; }
-}
-
-int Modifiers() : {
-  int ret = MOD_NONE;
-}
-{
-  [
-     <PLUS> { ret = MOD_REQ; }
-     | <MINUS> { ret = MOD_NOT; }
-     | <NOT> { ret = MOD_NOT; }
-  ]
-  { return ret; }
-}
-
-// This makes sure that there is no garbage after the query string
-Query TopLevelQuery(String field) : 
-{
-	Query q;
-}
-{
-	q=Query(field) <EOF>
-	{
-		return q;
-	}
-}
-
-Query Query(String field) :
-{
-  List<BooleanClause> clauses = new ArrayList<BooleanClause>();
-  Query q, firstQuery=null;
-  int conj, mods;
-}
-{
-  mods=Modifiers() q=Clause(field)
-  {
-    addClause(clauses, CONJ_NONE, mods, q);
-    if (mods == MOD_NONE)
-        firstQuery=q;
-  }
-  (
-    conj=Conjunction() mods=Modifiers() q=Clause(field)
-    { addClause(clauses, conj, mods, q); }
-  )*
-    {
-      if (clauses.size() == 1 && firstQuery != null)
-        return firstQuery;
-      else {
-  return getBooleanQuery(clauses);
-      }
-    }
-}
-
-Query Clause(String field) : {
-  Query q;
-  Token fieldToken=null, boost=null;
-}
-{
-  [
-    LOOKAHEAD(2)
-    (
-    fieldToken=<TERM> <COLON> {field=discardEscapeChar(fieldToken.image);}
-    | <STAR> <COLON> {field="*";}
-    )
-  ]
-
-  (
-   q=Term(field)
-   | <LPAREN> q=Query(field) <RPAREN> (<CARAT> boost=<NUMBER>)?
-
-  )
-    {  return handleBoost(q, boost); }
-}
-
-
-Query Term(String field) : {
-  Token term, boost=null, fuzzySlop=null, goop1, goop2;
-  boolean prefix = false;
-  boolean wildcard = false;
-  boolean fuzzy = false;
-  boolean regexp = false;
-  boolean startInc=false;
-  boolean endInc=false;
-  Query q;
-}
-{
-  (
-     (
-       term=<TERM>
-       | term=<STAR> { wildcard=true; }
-       | term=<PREFIXTERM> { prefix=true; }
-       | term=<WILDTERM> { wildcard=true; }
-       | term=<REGEXPTERM> { regexp=true; }
-       | term=<NUMBER>
-       | term=<BAREOPER> { term.image = term.image.substring(0,1); }
-     )
-     [ fuzzySlop=<FUZZY_SLOP> { fuzzy=true; } ]
-     [ <CARAT> boost=<NUMBER> [ fuzzySlop=<FUZZY_SLOP> { fuzzy=true; } ] ]
-     {
-       q = handleBareTokenQuery(field, term, fuzzySlop, prefix, wildcard, fuzzy, regexp);
-     }
-     | ( ( <RANGEIN_START> {startInc=true;} | <RANGEEX_START> )
-         ( goop1=<RANGE_GOOP>|goop1=<RANGE_QUOTED> )
-         [ <RANGE_TO> ]
-         ( goop2=<RANGE_GOOP>|goop2=<RANGE_QUOTED> )
-         ( <RANGEIN_END> {endInc=true;} | <RANGEEX_END>))
-       [ <CARAT> boost=<NUMBER> ]
-        {
-          boolean startOpen=false;
-          boolean endOpen=false;
-          if (goop1.kind == RANGE_QUOTED) {
-            goop1.image = goop1.image.substring(1, goop1.image.length()-1);
-          } else if ("*".equals(goop1.image)) {
-            startOpen=true;
-          }
-          if (goop2.kind == RANGE_QUOTED) {
-            goop2.image = goop2.image.substring(1, goop2.image.length()-1);
-          } else if ("*".equals(goop2.image)) {
-            endOpen=true;
-          }
-          q = getRangeQuery(field, startOpen ? null : discardEscapeChar(goop1.image), endOpen ? null : discardEscapeChar(goop2.image), startInc, endInc);
-        }
-     | term=<QUOTED>
-       [ fuzzySlop=<FUZZY_SLOP> ]
-       [ <CARAT> boost=<NUMBER> ]
-       { q = handleQuotedTerm(field, term, fuzzySlop); }
-  )
-  { return handleBoost(q, boost); }
-}
diff --git a/lucene/src/java/org/apache/lucene/queryParser/QueryParserBase.java b/lucene/src/java/org/apache/lucene/queryParser/QueryParserBase.java
deleted file mode 100644
index 58c77fd..0000000
--- a/lucene/src/java/org/apache/lucene/queryParser/QueryParserBase.java
+++ /dev/null
@@ -1,1194 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.lucene.queryParser;
-
-import java.io.IOException;
-import java.io.StringReader;
-import java.text.Collator;
-import java.text.DateFormat;
-import java.util.*;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.CachingTokenFilter;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;
-import org.apache.lucene.document.DateTools;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.queryParser.QueryParser.Operator;
-import org.apache.lucene.search.*;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Version;
-
-/** This class is overridden by QueryParser in QueryParser.jj
- * and acts to separate the majority of the Java code from the .jj grammar file. 
- */
-public abstract class QueryParserBase {
-
-  /** Do not catch this exception in your code, it means you are using methods that you should no longer use. */
-  public static class MethodRemovedUseAnother extends Throwable {}
-
-  static final int CONJ_NONE   = 0;
-  static final int CONJ_AND    = 1;
-  static final int CONJ_OR     = 2;
-
-  static final int MOD_NONE    = 0;
-  static final int MOD_NOT     = 10;
-  static final int MOD_REQ     = 11;
-
-  // make it possible to call setDefaultOperator() without accessing
-  // the nested class:
-  /** Alternative form of QueryParser.Operator.AND */
-  public static final Operator AND_OPERATOR = Operator.AND;
-  /** Alternative form of QueryParser.Operator.OR */
-  public static final Operator OR_OPERATOR = Operator.OR;
-
-  /** The actual operator that parser uses to combine query terms */
-  Operator operator = OR_OPERATOR;
-
-  boolean lowercaseExpandedTerms = true;
-  MultiTermQuery.RewriteMethod multiTermRewriteMethod = MultiTermQuery.CONSTANT_SCORE_AUTO_REWRITE_DEFAULT;
-  boolean allowLeadingWildcard = false;
-  boolean enablePositionIncrements = true;
-
-  Analyzer analyzer;
-  String field;
-  int phraseSlop = 0;
-  float fuzzyMinSim = FuzzyQuery.defaultMinSimilarity;
-  int fuzzyPrefixLength = FuzzyQuery.defaultPrefixLength;
-  Locale locale = Locale.getDefault();
-
-  // the default date resolution
-  DateTools.Resolution dateResolution = null;
-  // maps field names to date resolutions
-  Map<String,DateTools.Resolution> fieldToDateResolution = null;
-
-  //Whether or not to analyze range terms when constructing RangeQuerys
-  // (For example, analyzing terms into collation keys for locale-sensitive RangeQuery)
-  boolean analyzeRangeTerms = false;
-
-  boolean autoGeneratePhraseQueries;
-
-  // So the generated QueryParser(CharStream) won't error out
-  protected QueryParserBase() {
-  }
-
-  /** Initializes a query parser.  Called by the QueryParser constructor
-   *  @param matchVersion  Lucene version to match. See <a href="#version">above</a>.
-   *  @param f  the default field for query terms.
-   *  @param a   used to find terms in the query text.
-   */
-  public void init(Version matchVersion, String f, Analyzer a) {
-    analyzer = a;
-    field = f;
-    if (matchVersion.onOrAfter(Version.LUCENE_31)) {
-      setAutoGeneratePhraseQueries(false);
-    } else {
-      setAutoGeneratePhraseQueries(true);
-    }
-  }
-
-  // the generated parser will create these in QueryParser
-  public abstract void ReInit(CharStream stream);
-  public abstract Query TopLevelQuery(String field) throws ParseException;
-
-
-  /** Parses a query string, returning a {@link org.apache.lucene.search.Query}.
-   *  @param query  the query string to be parsed.
-   *  @throws ParseException if the parsing fails
-   */
-  public Query parse(String query) throws ParseException {
-    ReInit(new FastCharStream(new StringReader(query)));
-    try {
-      // TopLevelQuery is a Query followed by the end-of-input (EOF)
-      Query res = TopLevelQuery(field);
-      return res!=null ? res : newBooleanQuery(false);
-    }
-    catch (ParseException tme) {
-      // rethrow to include the original query:
-      ParseException e = new ParseException("Cannot parse '" +query+ "': " + tme.getMessage());
-      e.initCause(tme);
-      throw e;
-    }
-    catch (TokenMgrError tme) {
-      ParseException e = new ParseException("Cannot parse '" +query+ "': " + tme.getMessage());
-      e.initCause(tme);
-      throw e;
-    }
-    catch (BooleanQuery.TooManyClauses tmc) {
-      ParseException e = new ParseException("Cannot parse '" +query+ "': too many boolean clauses");
-      e.initCause(tmc);
-      throw e;
-    }
-  }
-
-
-   /**
-   * @return Returns the analyzer.
-   */
-  public Analyzer getAnalyzer() {
-    return analyzer;
-  }
-
-  /**
-   * @return Returns the default field.
-   */
-  public String getField() {
-    return field;
-  }
-
-  /**
-   * @see #setAutoGeneratePhraseQueries(boolean)
-   */
-  public final boolean getAutoGeneratePhraseQueries() {
-    return autoGeneratePhraseQueries;
-  }
-
-  /**
-   * Set to true if phrase queries will be automatically generated
-   * when the analyzer returns more than one term from whitespace
-   * delimited text.
-   * NOTE: this behavior may not be suitable for all languages.
-   * <p>
-   * Set to false if phrase queries should only be generated when
-   * surrounded by double quotes.
-   */
-  public final void setAutoGeneratePhraseQueries(boolean value) {
-    this.autoGeneratePhraseQueries = value;
-  }
-
-   /**
-   * Get the minimal similarity for fuzzy queries.
-   */
-  public float getFuzzyMinSim() {
-      return fuzzyMinSim;
-  }
-
-  /**
-   * Set the minimum similarity for fuzzy queries.
-   * Default is 2f.
-   */
-  public void setFuzzyMinSim(float fuzzyMinSim) {
-      this.fuzzyMinSim = fuzzyMinSim;
-  }
-
-   /**
-   * Get the prefix length for fuzzy queries.
-   * @return Returns the fuzzyPrefixLength.
-   */
-  public int getFuzzyPrefixLength() {
-    return fuzzyPrefixLength;
-  }
-
-  /**
-   * Set the prefix length for fuzzy queries. Default is 0.
-   * @param fuzzyPrefixLength The fuzzyPrefixLength to set.
-   */
-  public void setFuzzyPrefixLength(int fuzzyPrefixLength) {
-    this.fuzzyPrefixLength = fuzzyPrefixLength;
-  }
-
-  /**
-   * Sets the default slop for phrases.  If zero, then exact phrase matches
-   * are required.  Default value is zero.
-   */
-  public void setPhraseSlop(int phraseSlop) {
-    this.phraseSlop = phraseSlop;
-  }
-
-  /**
-   * Gets the default slop for phrases.
-   */
-  public int getPhraseSlop() {
-    return phraseSlop;
-  }
-
-
-  /**
-   * Set to <code>true</code> to allow leading wildcard characters.
-   * <p>
-   * When set, <code>*</code> or <code>?</code> are allowed as
-   * the first character of a PrefixQuery and WildcardQuery.
-   * Note that this can produce very slow
-   * queries on big indexes.
-   * <p>
-   * Default: false.
-   */
-  public void setAllowLeadingWildcard(boolean allowLeadingWildcard) {
-    this.allowLeadingWildcard = allowLeadingWildcard;
-  }
-
-  /**
-   * @see #setAllowLeadingWildcard(boolean)
-   */
-  public boolean getAllowLeadingWildcard() {
-    return allowLeadingWildcard;
-  }
-
-  /**
-   * Set to <code>true</code> to enable position increments in result query.
-   * <p>
-   * When set, result phrase and multi-phrase queries will
-   * be aware of position increments.
-   * Useful when e.g. a StopFilter increases the position increment of
-   * the token that follows an omitted token.
-   * <p>
-   * Default: true.
-   */
-  public void setEnablePositionIncrements(boolean enable) {
-    this.enablePositionIncrements = enable;
-  }
-
-  /**
-   * @see #setEnablePositionIncrements(boolean)
-   */
-  public boolean getEnablePositionIncrements() {
-    return enablePositionIncrements;
-  }
-
-  /**
-   * Sets the boolean operator of the QueryParser.
-   * In default mode (<code>OR_OPERATOR</code>) terms without any modifiers
-   * are considered optional: for example <code>capital of Hungary</code> is equal to
-   * <code>capital OR of OR Hungary</code>.<br/>
-   * In <code>AND_OPERATOR</code> mode terms are considered to be in conjunction: the
-   * above mentioned query is parsed as <code>capital AND of AND Hungary</code>
-   */
-  public void setDefaultOperator(Operator op) {
-    this.operator = op;
-  }
-
-
-  /**
-   * Gets implicit operator setting, which will be either AND_OPERATOR
-   * or OR_OPERATOR.
-   */
-  public Operator getDefaultOperator() {
-    return operator;
-  }
-
-
-  /**
-   * Whether terms of wildcard, prefix, fuzzy and range queries are to be automatically
-   * lower-cased or not.  Default is <code>true</code>.
-   */
-  public void setLowercaseExpandedTerms(boolean lowercaseExpandedTerms) {
-    this.lowercaseExpandedTerms = lowercaseExpandedTerms;
-  }
-
-
-  /**
-   * @see #setLowercaseExpandedTerms(boolean)
-   */
-  public boolean getLowercaseExpandedTerms() {
-    return lowercaseExpandedTerms;
-  }
-
-  /**
-   * By default QueryParser uses {@link org.apache.lucene.search.MultiTermQuery#CONSTANT_SCORE_AUTO_REWRITE_DEFAULT}
-   * when creating a PrefixQuery, WildcardQuery or RangeQuery. This implementation is generally preferable because it
-   * a) Runs faster b) Does not have the scarcity of terms unduly influence score
-   * c) avoids any "TooManyBooleanClauses" exception.
-   * However, if your application really needs to use the
-   * old-fashioned BooleanQuery expansion rewriting and the above
-   * points are not relevant then use this to change
-   * the rewrite method.
-   */
-  public void setMultiTermRewriteMethod(MultiTermQuery.RewriteMethod method) {
-    multiTermRewriteMethod = method;
-  }
-
-
-  /**
-   * @see #setMultiTermRewriteMethod
-   */
-  public MultiTermQuery.RewriteMethod getMultiTermRewriteMethod() {
-    return multiTermRewriteMethod;
-  }
-
-  /**
-   * Set locale used by date range parsing.
-   */
-  public void setLocale(Locale locale) {
-    this.locale = locale;
-  }
-
-  /**
-   * Returns current locale, allowing access by subclasses.
-   */
-  public Locale getLocale() {
-    return locale;
-  }
-
-  /**
-   * Sets the default date resolution used by RangeQueries for fields for which no
-   * specific date resolutions has been set. Field specific resolutions can be set
-   * with {@link #setDateResolution(String, org.apache.lucene.document.DateTools.Resolution)}.
-   *
-   * @param dateResolution the default date resolution to set
-   */
-  public void setDateResolution(DateTools.Resolution dateResolution) {
-    this.dateResolution = dateResolution;
-  }
-
-  /**
-   * Sets the date resolution used by RangeQueries for a specific field.
-   *
-   * @param fieldName field for which the date resolution is to be set
-   * @param dateResolution date resolution to set
-   */
-  public void setDateResolution(String fieldName, DateTools.Resolution dateResolution) {
-    if (fieldName == null) {
-      throw new IllegalArgumentException("Field cannot be null.");
-    }
-
-    if (fieldToDateResolution == null) {
-      // lazily initialize HashMap
-      fieldToDateResolution = new HashMap<String,DateTools.Resolution>();
-    }
-
-    fieldToDateResolution.put(fieldName, dateResolution);
-  }
-
-  /**
-   * Returns the date resolution that is used by RangeQueries for the given field.
-   * Returns null, if no default or field specific date resolution has been set
-   * for the given field.
-   *
-   */
-  public DateTools.Resolution getDateResolution(String fieldName) {
-    if (fieldName == null) {
-      throw new IllegalArgumentException("Field cannot be null.");
-    }
-
-    if (fieldToDateResolution == null) {
-      // no field specific date resolutions set; return default date resolution instead
-      return this.dateResolution;
-    }
-
-    DateTools.Resolution resolution = fieldToDateResolution.get(fieldName);
-    if (resolution == null) {
-      // no date resolutions set for the given field; return default date resolution instead
-      resolution = this.dateResolution;
-    }
-
-    return resolution;
-  }
-
-  /**
-   * Set whether or not to analyze range terms when constructing RangeQuerys.
-   * For example, setting this to true can enable analyzing terms into 
-   * collation keys for locale-sensitive RangeQuery.
-   * 
-   * @param analyzeRangeTerms whether or not terms should be analyzed for RangeQuerys
-   */
-  public void setAnalyzeRangeTerms(boolean analyzeRangeTerms) {
-    this.analyzeRangeTerms = analyzeRangeTerms;
-  }
-
-  /**
-   * @return whether or not to analyze range terms when constructing RangeQuerys.
-   */
-  public boolean getAnalyzeRangeTerms() {
-    return analyzeRangeTerms;
-  }
-
-  protected void addClause(List<BooleanClause> clauses, int conj, int mods, Query q) {
-    boolean required, prohibited;
-
-    // If this term is introduced by AND, make the preceding term required,
-    // unless it's already prohibited
-    if (clauses.size() > 0 && conj == CONJ_AND) {
-      BooleanClause c = clauses.get(clauses.size()-1);
-      if (!c.isProhibited())
-        c.setOccur(BooleanClause.Occur.MUST);
-    }
-
-    if (clauses.size() > 0 && operator == AND_OPERATOR && conj == CONJ_OR) {
-      // If this term is introduced by OR, make the preceding term optional,
-      // unless it's prohibited (that means we leave -a OR b but +a OR b-->a OR b)
-      // notice if the input is a OR b, first term is parsed as required; without
-      // this modification a OR b would parsed as +a OR b
-      BooleanClause c = clauses.get(clauses.size()-1);
-      if (!c.isProhibited())
-        c.setOccur(BooleanClause.Occur.SHOULD);
-    }
-
-    // We might have been passed a null query; the term might have been
-    // filtered away by the analyzer.
-    if (q == null)
-      return;
-
-    if (operator == OR_OPERATOR) {
-      // We set REQUIRED if we're introduced by AND or +; PROHIBITED if
-      // introduced by NOT or -; make sure not to set both.
-      prohibited = (mods == MOD_NOT);
-      required = (mods == MOD_REQ);
-      if (conj == CONJ_AND && !prohibited) {
-        required = true;
-      }
-    } else {
-      // We set PROHIBITED if we're introduced by NOT or -; We set REQUIRED
-      // if not PROHIBITED and not introduced by OR
-      prohibited = (mods == MOD_NOT);
-      required   = (!prohibited && conj != CONJ_OR);
-    }
-    if (required && !prohibited)
-      clauses.add(newBooleanClause(q, BooleanClause.Occur.MUST));
-    else if (!required && !prohibited)
-      clauses.add(newBooleanClause(q, BooleanClause.Occur.SHOULD));
-    else if (!required && prohibited)
-      clauses.add(newBooleanClause(q, BooleanClause.Occur.MUST_NOT));
-    else
-      throw new RuntimeException("Clause cannot be both required and prohibited");
-  }
-
-  /**
-   * @exception org.apache.lucene.queryParser.ParseException throw in overridden method to disallow
-   */
-  protected Query getFieldQuery(String field, String queryText, boolean quoted) throws ParseException {
-    return newFieldQuery(analyzer, field, queryText, quoted);
-  }
-  
-  /**
-   * @exception org.apache.lucene.queryParser.ParseException throw in overridden method to disallow
-   */
-  protected Query newFieldQuery(Analyzer analyzer, String field, String queryText, boolean quoted)  throws ParseException {
-    // Use the analyzer to get all the tokens, and then build a TermQuery,
-    // PhraseQuery, or nothing based on the term count
-
-    TokenStream source;
-    try {
-      source = analyzer.reusableTokenStream(field, new StringReader(queryText));
-      source.reset();
-    } catch (IOException e) {
-      source = analyzer.tokenStream(field, new StringReader(queryText));
-    }
-    CachingTokenFilter buffer = new CachingTokenFilter(source);
-    TermToBytesRefAttribute termAtt = null;
-    PositionIncrementAttribute posIncrAtt = null;
-    int numTokens = 0;
-
-    boolean success = false;
-    try {
-      buffer.reset();
-      success = true;
-    } catch (IOException e) {
-      // success==false if we hit an exception
-    }
-    if (success) {
-      if (buffer.hasAttribute(TermToBytesRefAttribute.class)) {
-        termAtt = buffer.getAttribute(TermToBytesRefAttribute.class);
-      }
-      if (buffer.hasAttribute(PositionIncrementAttribute.class)) {
-        posIncrAtt = buffer.getAttribute(PositionIncrementAttribute.class);
-      }
-    }
-
-    int positionCount = 0;
-    boolean severalTokensAtSamePosition = false;
-
-    boolean hasMoreTokens = false;
-    if (termAtt != null) {
-      try {
-        hasMoreTokens = buffer.incrementToken();
-        while (hasMoreTokens) {
-          numTokens++;
-          int positionIncrement = (posIncrAtt != null) ? posIncrAtt.getPositionIncrement() : 1;
-          if (positionIncrement != 0) {
-            positionCount += positionIncrement;
-          } else {
-            severalTokensAtSamePosition = true;
-          }
-          hasMoreTokens = buffer.incrementToken();
-        }
-      } catch (IOException e) {
-        // ignore
-      }
-    }
-    try {
-      // rewind the buffer stream
-      buffer.reset();
-
-      // close original stream - all tokens buffered
-      source.close();
-    }
-    catch (IOException e) {
-      // ignore
-    }
-
-    BytesRef bytes = termAtt == null ? null : termAtt.getBytesRef();
-
-    if (numTokens == 0)
-      return null;
-    else if (numTokens == 1) {
-      try {
-        boolean hasNext = buffer.incrementToken();
-        assert hasNext == true;
-        termAtt.fillBytesRef();
-      } catch (IOException e) {
-        // safe to ignore, because we know the number of tokens
-      }
-      return newTermQuery(new Term(field, new BytesRef(bytes)));
-    } else {
-      if (severalTokensAtSamePosition || (!quoted && !autoGeneratePhraseQueries)) {
-        if (positionCount == 1 || (!quoted && !autoGeneratePhraseQueries)) {
-          // no phrase query:
-          BooleanQuery q = newBooleanQuery(positionCount == 1);
-
-          BooleanClause.Occur occur = positionCount > 1 && operator == AND_OPERATOR ?
-            BooleanClause.Occur.MUST : BooleanClause.Occur.SHOULD;
-
-          for (int i = 0; i < numTokens; i++) {
-            try {
-              boolean hasNext = buffer.incrementToken();
-              assert hasNext == true;
-              termAtt.fillBytesRef();
-            } catch (IOException e) {
-              // safe to ignore, because we know the number of tokens
-            }
-            Query currentQuery = newTermQuery(
-                new Term(field, new BytesRef(bytes)));
-            q.add(currentQuery, occur);
-          }
-          return q;
-        }
-        else {
-          // phrase query:
-          MultiPhraseQuery mpq = newMultiPhraseQuery();
-          mpq.setSlop(phraseSlop);
-          List<Term> multiTerms = new ArrayList<Term>();
-          int position = -1;
-          for (int i = 0; i < numTokens; i++) {
-            int positionIncrement = 1;
-            try {
-              boolean hasNext = buffer.incrementToken();
-              assert hasNext == true;
-              termAtt.fillBytesRef();
-              if (posIncrAtt != null) {
-                positionIncrement = posIncrAtt.getPositionIncrement();
-              }
-            } catch (IOException e) {
-              // safe to ignore, because we know the number of tokens
-            }
-
-            if (positionIncrement > 0 && multiTerms.size() > 0) {
-              if (enablePositionIncrements) {
-                mpq.add(multiTerms.toArray(new Term[0]),position);
-              } else {
-                mpq.add(multiTerms.toArray(new Term[0]));
-              }
-              multiTerms.clear();
-            }
-            position += positionIncrement;
-            multiTerms.add(new Term(field, new BytesRef(bytes)));
-          }
-          if (enablePositionIncrements) {
-            mpq.add(multiTerms.toArray(new Term[0]),position);
-          } else {
-            mpq.add(multiTerms.toArray(new Term[0]));
-          }
-          return mpq;
-        }
-      }
-      else {
-        PhraseQuery pq = newPhraseQuery();
-        pq.setSlop(phraseSlop);
-        int position = -1;
-
-        for (int i = 0; i < numTokens; i++) {
-          int positionIncrement = 1;
-
-          try {
-            boolean hasNext = buffer.incrementToken();
-            assert hasNext == true;
-            termAtt.fillBytesRef();
-            if (posIncrAtt != null) {
-              positionIncrement = posIncrAtt.getPositionIncrement();
-            }
-          } catch (IOException e) {
-            // safe to ignore, because we know the number of tokens
-          }
-
-          if (enablePositionIncrements) {
-            position += positionIncrement;
-            pq.add(new Term(field, new BytesRef(bytes)),position);
-          } else {
-            pq.add(new Term(field, new BytesRef(bytes)));
-          }
-        }
-        return pq;
-      }
-    }
-  }
-
-
-
-  /**
-   * Base implementation delegates to {@link #getFieldQuery(String,String,boolean)}.
-   * This method may be overridden, for example, to return
-   * a SpanNearQuery instead of a PhraseQuery.
-   *
-   * @exception org.apache.lucene.queryParser.ParseException throw in overridden method to disallow
-   */
-  protected Query getFieldQuery(String field, String queryText, int slop)
-        throws ParseException {
-    Query query = getFieldQuery(field, queryText, true);
-
-    if (query instanceof PhraseQuery) {
-      ((PhraseQuery) query).setSlop(slop);
-    }
-    if (query instanceof MultiPhraseQuery) {
-      ((MultiPhraseQuery) query).setSlop(slop);
-    }
-
-    return query;
-  }
-
-  /**
-   *
-   * @exception org.apache.lucene.queryParser.ParseException
-   */
-  protected Query getRangeQuery(String field,
-                                String part1,
-                                String part2,
-                                boolean startInclusive,
-                                boolean endInclusive) throws ParseException
-  {
-    if (lowercaseExpandedTerms) {
-      part1 = part1==null ? null : part1.toLowerCase();
-      part2 = part2==null ? null : part2.toLowerCase();
-    }
-
-
-    DateFormat df = DateFormat.getDateInstance(DateFormat.SHORT, locale);
-    df.setLenient(true);
-    DateTools.Resolution resolution = getDateResolution(field);
-    
-    try {
-      part1 = DateTools.dateToString(df.parse(part1), resolution);
-    } catch (Exception e) { }
-
-    try {
-      Date d2 = df.parse(part2);
-      if (endInclusive) {
-        // The user can only specify the date, not the time, so make sure
-        // the time is set to the latest possible time of that date to really
-        // include all documents:
-        Calendar cal = Calendar.getInstance(locale);
-        cal.setTime(d2);
-        cal.set(Calendar.HOUR_OF_DAY, 23);
-        cal.set(Calendar.MINUTE, 59);
-        cal.set(Calendar.SECOND, 59);
-        cal.set(Calendar.MILLISECOND, 999);
-        d2 = cal.getTime();
-      }
-      part2 = DateTools.dateToString(d2, resolution);
-    } catch (Exception e) { }
-
-    return newRangeQuery(field, part1, part2, startInclusive, endInclusive);
-  }
-
- /**
-  * Builds a new BooleanQuery instance
-  * @param disableCoord disable coord
-  * @return new BooleanQuery instance
-  */
-  protected BooleanQuery newBooleanQuery(boolean disableCoord) {
-    return new BooleanQuery(disableCoord);
-  }
-
- /**
-  * Builds a new BooleanClause instance
-  * @param q sub query
-  * @param occur how this clause should occur when matching documents
-  * @return new BooleanClause instance
-  */
-  protected BooleanClause newBooleanClause(Query q, BooleanClause.Occur occur) {
-    return new BooleanClause(q, occur);
-  }
-
-  /**
-   * Builds a new TermQuery instance
-   * @param term term
-   * @return new TermQuery instance
-   */
-  protected Query newTermQuery(Term term){
-    return new TermQuery(term);
-  }
-
-  /**
-   * Builds a new PhraseQuery instance
-   * @return new PhraseQuery instance
-   */
-  protected PhraseQuery newPhraseQuery(){
-    return new PhraseQuery();
-  }
-
-  /**
-   * Builds a new MultiPhraseQuery instance
-   * @return new MultiPhraseQuery instance
-   */
-  protected MultiPhraseQuery newMultiPhraseQuery(){
-    return new MultiPhraseQuery();
-  }
-
-  /**
-   * Builds a new PrefixQuery instance
-   * @param prefix Prefix term
-   * @return new PrefixQuery instance
-   */
-  protected Query newPrefixQuery(Term prefix){
-    PrefixQuery query = new PrefixQuery(prefix);
-    query.setRewriteMethod(multiTermRewriteMethod);
-    return query;
-  }
-
-  /**
-   * Builds a new RegexpQuery instance
-   * @param regexp Regexp term
-   * @return new RegexpQuery instance
-   */
-  protected Query newRegexpQuery(Term regexp) {
-    RegexpQuery query = new RegexpQuery(regexp);
-    query.setRewriteMethod(multiTermRewriteMethod);
-    return query;
-  }
-
-  /**
-   * Builds a new FuzzyQuery instance
-   * @param term Term
-   * @param minimumSimilarity minimum similarity
-   * @param prefixLength prefix length
-   * @return new FuzzyQuery Instance
-   */
-  protected Query newFuzzyQuery(Term term, float minimumSimilarity, int prefixLength) {
-    // FuzzyQuery doesn't yet allow constant score rewrite
-    return new FuzzyQuery(term,minimumSimilarity,prefixLength);
-  }
-
-  private BytesRef analyzeRangePart(String field, String part) {
-    TokenStream source;
-      
-    try {
-      source = analyzer.reusableTokenStream(field, new StringReader(part));
-      source.reset();
-    } catch (IOException e) {
-      source = analyzer.tokenStream(field, new StringReader(part));
-    }
-      
-    TermToBytesRefAttribute termAtt = source.getAttribute(TermToBytesRefAttribute.class);
-    BytesRef bytes = termAtt.getBytesRef();
-
-    try {
-      if (!source.incrementToken())
-        throw new IllegalArgumentException("analyzer returned no terms for range part: " + part);
-      termAtt.fillBytesRef();
-      if (source.incrementToken())
-        throw new IllegalArgumentException("analyzer returned too many terms for range part: " + part);
-    } catch (IOException e) {
-      throw new RuntimeException("error analyzing range part: " + part, e);
-    }
-      
-    try {
-      source.end();
-      source.close();
-    } catch (IOException ignored) {}
-    
-    return new BytesRef(bytes);
-  }
-
-  /**
-   * Builds a new TermRangeQuery instance
-   * @param field Field
-   * @param part1 min
-   * @param part2 max
-   * @param startInclusive true if the start of the range is inclusive
-   * @param endInclusive true if the end of the range is inclusive
-   * @return new TermRangeQuery instance
-   */
-  protected Query newRangeQuery(String field, String part1, String part2, boolean startInclusive, boolean endInclusive) {
-    final BytesRef start;
-    final BytesRef end;
-     
-    if (part1 == null) {
-      start = null;
-    } else {
-      start = analyzeRangeTerms ? analyzeRangePart(field, part1) : new BytesRef(part1);
-    }
-     
-    if (part2 == null) {
-      end = null;
-    } else {
-      end = analyzeRangeTerms ? analyzeRangePart(field, part2) : new BytesRef(part2);
-    }
-      
-    final TermRangeQuery query = new TermRangeQuery(field, start, end, startInclusive, endInclusive);
-
-    query.setRewriteMethod(multiTermRewriteMethod);
-    return query;
-  }
-
-  /**
-   * Builds a new MatchAllDocsQuery instance
-   * @return new MatchAllDocsQuery instance
-   */
-  protected Query newMatchAllDocsQuery() {
-    return new MatchAllDocsQuery();
-  }
-
-  /**
-   * Builds a new WildcardQuery instance
-   * @param t wildcard term
-   * @return new WildcardQuery instance
-   */
-  protected Query newWildcardQuery(Term t) {
-    WildcardQuery query = new WildcardQuery(t);
-    query.setRewriteMethod(multiTermRewriteMethod);
-    return query;
-  }
-
-  /**
-   * Factory method for generating query, given a set of clauses.
-   * By default creates a boolean query composed of clauses passed in.
-   *
-   * Can be overridden by extending classes, to modify query being
-   * returned.
-   *
-   * @param clauses List that contains {@link org.apache.lucene.search.BooleanClause} instances
-   *    to join.
-   *
-   * @return Resulting {@link org.apache.lucene.search.Query} object.
-   * @exception org.apache.lucene.queryParser.ParseException throw in overridden method to disallow
-   */
-  protected Query getBooleanQuery(List<BooleanClause> clauses) throws ParseException {
-    return getBooleanQuery(clauses, false);
-  }
-
-  /**
-   * Factory method for generating query, given a set of clauses.
-   * By default creates a boolean query composed of clauses passed in.
-   *
-   * Can be overridden by extending classes, to modify query being
-   * returned.
-   *
-   * @param clauses List that contains {@link org.apache.lucene.search.BooleanClause} instances
-   *    to join.
-   * @param disableCoord true if coord scoring should be disabled.
-   *
-   * @return Resulting {@link org.apache.lucene.search.Query} object.
-   * @exception org.apache.lucene.queryParser.ParseException throw in overridden method to disallow
-   */
-  protected Query getBooleanQuery(List<BooleanClause> clauses, boolean disableCoord)
-    throws ParseException
-  {
-    if (clauses.size()==0) {
-      return null; // all clause words were filtered away by the analyzer.
-    }
-    BooleanQuery query = newBooleanQuery(disableCoord);
-    for(final BooleanClause clause: clauses) {
-      query.add(clause);
-    }
-    return query;
-  }
-
-  /**
-   * Factory method for generating a query. Called when parser
-   * parses an input term token that contains one or more wildcard
-   * characters (? and *), but is not a prefix term token (one
-   * that has just a single * character at the end)
-   *<p>
-   * Depending on settings, prefix term may be lower-cased
-   * automatically. It will not go through the default Analyzer,
-   * however, since normal Analyzers are unlikely to work properly
-   * with wildcard templates.
-   *<p>
-   * Can be overridden by extending classes, to provide custom handling for
-   * wildcard queries, which may be necessary due to missing analyzer calls.
-   *
-   * @param field Name of the field query will use.
-   * @param termStr Term token that contains one or more wild card
-   *   characters (? or *), but is not simple prefix term
-   *
-   * @return Resulting {@link org.apache.lucene.search.Query} built for the term
-   * @exception org.apache.lucene.queryParser.ParseException throw in overridden method to disallow
-   */
-  protected Query getWildcardQuery(String field, String termStr) throws ParseException
-  {
-    if ("*".equals(field)) {
-      if ("*".equals(termStr)) return newMatchAllDocsQuery();
-    }
-    if (!allowLeadingWildcard && (termStr.startsWith("*") || termStr.startsWith("?")))
-      throw new ParseException("'*' or '?' not allowed as first character in WildcardQuery");
-    if (lowercaseExpandedTerms) {
-      termStr = termStr.toLowerCase();
-    }
-    Term t = new Term(field, termStr);
-    return newWildcardQuery(t);
-  }
-
-  /**
-   * Factory method for generating a query. Called when parser
-   * parses an input term token that contains a regular expression
-   * query.
-   *<p>
-   * Depending on settings, pattern term may be lower-cased
-   * automatically. It will not go through the default Analyzer,
-   * however, since normal Analyzers are unlikely to work properly
-   * with regular expression templates.
-   *<p>
-   * Can be overridden by extending classes, to provide custom handling for
-   * regular expression queries, which may be necessary due to missing analyzer
-   * calls.
-   *
-   * @param field Name of the field query will use.
-   * @param termStr Term token that contains a regular expression
-   *
-   * @return Resulting {@link org.apache.lucene.search.Query} built for the term
-   * @exception org.apache.lucene.queryParser.ParseException throw in overridden method to disallow
-   */
-  protected Query getRegexpQuery(String field, String termStr) throws ParseException
-  {
-    if (lowercaseExpandedTerms) {
-      termStr = termStr.toLowerCase();
-    }
-    Term t = new Term(field, termStr);
-    return newRegexpQuery(t);
-  }
-
-  /**
-   * Factory method for generating a query (similar to
-   * {@link #getWildcardQuery}). Called when parser parses an input term
-   * token that uses prefix notation; that is, contains a single '*' wildcard
-   * character as its last character. Since this is a special case
-   * of generic wildcard term, and such a query can be optimized easily,
-   * this usually results in a different query object.
-   *<p>
-   * Depending on settings, a prefix term may be lower-cased
-   * automatically. It will not go through the default Analyzer,
-   * however, since normal Analyzers are unlikely to work properly
-   * with wildcard templates.
-   *<p>
-   * Can be overridden by extending classes, to provide custom handling for
-   * wild card queries, which may be necessary due to missing analyzer calls.
-   *
-   * @param field Name of the field query will use.
-   * @param termStr Term token to use for building term for the query
-   *    (<b>without</b> trailing '*' character!)
-   *
-   * @return Resulting {@link org.apache.lucene.search.Query} built for the term
-   * @exception org.apache.lucene.queryParser.ParseException throw in overridden method to disallow
-   */
-  protected Query getPrefixQuery(String field, String termStr) throws ParseException
-  {
-    if (!allowLeadingWildcard && termStr.startsWith("*"))
-      throw new ParseException("'*' not allowed as first character in PrefixQuery");
-    if (lowercaseExpandedTerms) {
-      termStr = termStr.toLowerCase();
-    }
-    Term t = new Term(field, termStr);
-    return newPrefixQuery(t);
-  }
-
-   /**
-   * Factory method for generating a query (similar to
-   * {@link #getWildcardQuery}). Called when parser parses
-   * an input term token that has the fuzzy suffix (~) appended.
-   *
-   * @param field Name of the field query will use.
-   * @param termStr Term token to use for building term for the query
-   *
-   * @return Resulting {@link org.apache.lucene.search.Query} built for the term
-   * @exception org.apache.lucene.queryParser.ParseException throw in overridden method to disallow
-   */
-  protected Query getFuzzyQuery(String field, String termStr, float minSimilarity) throws ParseException
-  {
-    if (lowercaseExpandedTerms) {
-      termStr = termStr.toLowerCase();
-    }
-    Term t = new Term(field, termStr);
-    return newFuzzyQuery(t, minSimilarity, fuzzyPrefixLength);
-  }
-
-
-   // extracted from the .jj grammar
-  Query handleBareTokenQuery(String qfield, Token term, Token fuzzySlop, boolean prefix, boolean wildcard, boolean fuzzy, boolean regexp) throws ParseException {
-    Query q;
-
-    String termImage=discardEscapeChar(term.image);
-    if (wildcard) {
-      q = getWildcardQuery(qfield, term.image);
-    } else if (prefix) {
-      q = getPrefixQuery(qfield,
-          discardEscapeChar(term.image.substring
-              (0, term.image.length()-1)));
-    } else if (regexp) {
-      q = getRegexpQuery(qfield, term.image.substring(1, term.image.length()-1));
-    } else if (fuzzy) {
-      float fms = fuzzyMinSim;
-      try {
-        fms = Float.valueOf(fuzzySlop.image.substring(1)).floatValue();
-      } catch (Exception ignored) { }
-      if(fms < 0.0f){
-        throw new ParseException("Minimum similarity for a FuzzyQuery has to be between 0.0f and 1.0f !");
-      } else if (fms >= 1.0f && fms != (int) fms) {
-        throw new ParseException("Fractional edit distances are not allowed!");
-      }
-      q = getFuzzyQuery(qfield, termImage, fms);
-    } else {
-      q = getFieldQuery(qfield, termImage, false);
-    }
-    return q;
-  }
-
-  // extracted from the .jj grammar
-  Query handleQuotedTerm(String qfield, Token term, Token fuzzySlop) throws ParseException {
-    int s = phraseSlop;  // default
-    if (fuzzySlop != null) {
-      try {
-        s = Float.valueOf(fuzzySlop.image.substring(1)).intValue();
-      }
-      catch (Exception ignored) { }
-    }
-    return getFieldQuery(qfield, discardEscapeChar(term.image.substring(1, term.image.length()-1)), s);
-  }
-
-  // extracted from the .jj grammar
-  Query handleBoost(Query q, Token boost) throws ParseException {
-    if (boost != null) {
-      float f = (float) 1.0;
-      try {
-        f = Float.valueOf(boost.image).floatValue();
-      }
-      catch (Exception ignored) {
-    /* Should this be handled somehow? (defaults to "no boost", if
-     * boost number is invalid)
-     */
-      }
-
-      // avoid boosting null queries, such as those caused by stop words
-      if (q != null) {
-        q.setBoost(f);
-      }
-    }
-    return q;
-  }
-
-
-
-  /**
-   * Returns a String where the escape char has been
-   * removed, or kept only once if there was a double escape.
-   *
-   * Supports escaped unicode characters, e. g. translates
-   * <code>\\u0041</code> to <code>A</code>.
-   *
-   */
-  String discardEscapeChar(String input) throws ParseException {
-    // Create char array to hold unescaped char sequence
-    char[] output = new char[input.length()];
-
-    // The length of the output can be less than the input
-    // due to discarded escape chars. This variable holds
-    // the actual length of the output
-    int length = 0;
-
-    // We remember whether the last processed character was
-    // an escape character
-    boolean lastCharWasEscapeChar = false;
-
-    // The multiplier the current unicode digit must be multiplied with.
-    // E. g. the first digit must be multiplied with 16^3, the second with 16^2...
-    int codePointMultiplier = 0;
-
-    // Used to calculate the codepoint of the escaped unicode character
-    int codePoint = 0;
-
-    for (int i = 0; i < input.length(); i++) {
-      char curChar = input.charAt(i);
-      if (codePointMultiplier > 0) {
-        codePoint += hexToInt(curChar) * codePointMultiplier;
-        codePointMultiplier >>>= 4;
-        if (codePointMultiplier == 0) {
-          output[length++] = (char)codePoint;
-          codePoint = 0;
-        }
-      } else if (lastCharWasEscapeChar) {
-        if (curChar == 'u') {
-          // found an escaped unicode character
-          codePointMultiplier = 16 * 16 * 16;
-        } else {
-          // this character was escaped
-          output[length] = curChar;
-          length++;
-        }
-        lastCharWasEscapeChar = false;
-      } else {
-        if (curChar == '\\') {
-          lastCharWasEscapeChar = true;
-        } else {
-          output[length] = curChar;
-          length++;
-        }
-      }
-    }
-
-    if (codePointMultiplier > 0) {
-      throw new ParseException("Truncated unicode escape sequence.");
-    }
-
-    if (lastCharWasEscapeChar) {
-      throw new ParseException("Term can not end with escape character.");
-    }
-
-    return new String(output, 0, length);
-  }
-
-  /** Returns the numeric value of the hexadecimal character */
-  static final int hexToInt(char c) throws ParseException {
-    if ('0' <= c && c <= '9') {
-      return c - '0';
-    } else if ('a' <= c && c <= 'f'){
-      return c - 'a' + 10;
-    } else if ('A' <= c && c <= 'F') {
-      return c - 'A' + 10;
-    } else {
-      throw new ParseException("None-hex character in unicode escape sequence: " + c);
-    }
-  }
-
-  /**
-   * Returns a String where those characters that QueryParser
-   * expects to be escaped are escaped by a preceding <code>\</code>.
-   */
-  public static String escape(String s) {
-    StringBuilder sb = new StringBuilder();
-    for (int i = 0; i < s.length(); i++) {
-      char c = s.charAt(i);
-      // These characters are part of the query syntax and must be escaped
-      if (c == '\\' || c == '+' || c == '-' || c == '!' || c == '(' || c == ')' || c == ':'
-        || c == '^' || c == '[' || c == ']' || c == '\"' || c == '{' || c == '}' || c == '~'
-        || c == '*' || c == '?' || c == '|' || c == '&') {
-        sb.append('\\');
-      }
-      sb.append(c);
-    }
-    return sb.toString();
-  }
-
-}
diff --git a/lucene/src/java/org/apache/lucene/queryParser/QueryParserConstants.java b/lucene/src/java/org/apache/lucene/queryParser/QueryParserConstants.java
deleted file mode 100644
index 9e9295e..0000000
--- a/lucene/src/java/org/apache/lucene/queryParser/QueryParserConstants.java
+++ /dev/null
@@ -1,120 +0,0 @@
-/* Generated By:JavaCC: Do not edit this line. QueryParserConstants.java */
-package org.apache.lucene.queryParser;
-
-
-/**
- * Token literal values and constants.
- * Generated by org.javacc.parser.OtherFilesGen#start()
- */
-public interface QueryParserConstants {
-
-  /** End of File. */
-  int EOF = 0;
-  /** RegularExpression Id. */
-  int _NUM_CHAR = 1;
-  /** RegularExpression Id. */
-  int _ESCAPED_CHAR = 2;
-  /** RegularExpression Id. */
-  int _TERM_START_CHAR = 3;
-  /** RegularExpression Id. */
-  int _TERM_CHAR = 4;
-  /** RegularExpression Id. */
-  int _WHITESPACE = 5;
-  /** RegularExpression Id. */
-  int _QUOTED_CHAR = 6;
-  /** RegularExpression Id. */
-  int AND = 8;
-  /** RegularExpression Id. */
-  int OR = 9;
-  /** RegularExpression Id. */
-  int NOT = 10;
-  /** RegularExpression Id. */
-  int PLUS = 11;
-  /** RegularExpression Id. */
-  int MINUS = 12;
-  /** RegularExpression Id. */
-  int BAREOPER = 13;
-  /** RegularExpression Id. */
-  int LPAREN = 14;
-  /** RegularExpression Id. */
-  int RPAREN = 15;
-  /** RegularExpression Id. */
-  int COLON = 16;
-  /** RegularExpression Id. */
-  int STAR = 17;
-  /** RegularExpression Id. */
-  int CARAT = 18;
-  /** RegularExpression Id. */
-  int QUOTED = 19;
-  /** RegularExpression Id. */
-  int TERM = 20;
-  /** RegularExpression Id. */
-  int FUZZY_SLOP = 21;
-  /** RegularExpression Id. */
-  int PREFIXTERM = 22;
-  /** RegularExpression Id. */
-  int WILDTERM = 23;
-  /** RegularExpression Id. */
-  int REGEXPTERM = 24;
-  /** RegularExpression Id. */
-  int RANGEIN_START = 25;
-  /** RegularExpression Id. */
-  int RANGEEX_START = 26;
-  /** RegularExpression Id. */
-  int NUMBER = 27;
-  /** RegularExpression Id. */
-  int RANGE_TO = 28;
-  /** RegularExpression Id. */
-  int RANGEIN_END = 29;
-  /** RegularExpression Id. */
-  int RANGEEX_END = 30;
-  /** RegularExpression Id. */
-  int RANGE_QUOTED = 31;
-  /** RegularExpression Id. */
-  int RANGE_GOOP = 32;
-
-  /** Lexical state. */
-  int Boost = 0;
-  /** Lexical state. */
-  int Range = 1;
-  /** Lexical state. */
-  int DEFAULT = 2;
-
-  /** Literal token values. */
-  String[] tokenImage = {
-    "<EOF>",
-    "<_NUM_CHAR>",
-    "<_ESCAPED_CHAR>",
-    "<_TERM_START_CHAR>",
-    "<_TERM_CHAR>",
-    "<_WHITESPACE>",
-    "<_QUOTED_CHAR>",
-    "<token of kind 7>",
-    "<AND>",
-    "<OR>",
-    "<NOT>",
-    "\"+\"",
-    "\"-\"",
-    "<BAREOPER>",
-    "\"(\"",
-    "\")\"",
-    "\":\"",
-    "\"*\"",
-    "\"^\"",
-    "<QUOTED>",
-    "<TERM>",
-    "<FUZZY_SLOP>",
-    "<PREFIXTERM>",
-    "<WILDTERM>",
-    "<REGEXPTERM>",
-    "\"[\"",
-    "\"{\"",
-    "<NUMBER>",
-    "\"TO\"",
-    "\"]\"",
-    "\"}\"",
-    "<RANGE_QUOTED>",
-    "<RANGE_GOOP>",
-  };
-
-}
diff --git a/lucene/src/java/org/apache/lucene/queryParser/QueryParserTokenManager.java b/lucene/src/java/org/apache/lucene/queryParser/QueryParserTokenManager.java
deleted file mode 100644
index fc5fe57..0000000
--- a/lucene/src/java/org/apache/lucene/queryParser/QueryParserTokenManager.java
+++ /dev/null
@@ -1,1079 +0,0 @@
-/* Generated By:JavaCC: Do not edit this line. QueryParserTokenManager.java */
-package org.apache.lucene.queryParser;
-import java.io.StringReader;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Locale;
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.document.DateTools;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermRangeQuery;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.util.Version;
-
-/** Token Manager. */
-public class QueryParserTokenManager implements QueryParserConstants
-{
-
-  /** Debug output. */
-  public  java.io.PrintStream debugStream = System.out;
-  /** Set debug output. */
-  public  void setDebugStream(java.io.PrintStream ds) { debugStream = ds; }
-private final int jjStopStringLiteralDfa_2(int pos, long active0)
-{
-   switch (pos)
-   {
-      default :
-         return -1;
-   }
-}
-private final int jjStartNfa_2(int pos, long active0)
-{
-   return jjMoveNfa_2(jjStopStringLiteralDfa_2(pos, active0), pos + 1);
-}
-private int jjStopAtPos(int pos, int kind)
-{
-   jjmatchedKind = kind;
-   jjmatchedPos = pos;
-   return pos + 1;
-}
-private int jjMoveStringLiteralDfa0_2()
-{
-   switch(curChar)
-   {
-      case 40:
-         return jjStopAtPos(0, 14);
-      case 41:
-         return jjStopAtPos(0, 15);
-      case 42:
-         return jjStartNfaWithStates_2(0, 17, 43);
-      case 43:
-         return jjStartNfaWithStates_2(0, 11, 15);
-      case 45:
-         return jjStartNfaWithStates_2(0, 12, 15);
-      case 58:
-         return jjStopAtPos(0, 16);
-      case 91:
-         return jjStopAtPos(0, 25);
-      case 94:
-         return jjStopAtPos(0, 18);
-      case 123:
-         return jjStopAtPos(0, 26);
-      default :
-         return jjMoveNfa_2(0, 0);
-   }
-}
-private int jjStartNfaWithStates_2(int pos, int kind, int state)
-{
-   jjmatchedKind = kind;
-   jjmatchedPos = pos;
-   try { curChar = input_stream.readChar(); }
-   catch(java.io.IOException e) { return pos + 1; }
-   return jjMoveNfa_2(state, pos + 1);
-}
-static final long[] jjbitVec0 = {
-   0x1L, 0x0L, 0x0L, 0x0L
-};
-static final long[] jjbitVec1 = {
-   0xfffffffffffffffeL, 0xffffffffffffffffL, 0xffffffffffffffffL, 0xffffffffffffffffL
-};
-static final long[] jjbitVec3 = {
-   0x0L, 0x0L, 0xffffffffffffffffL, 0xffffffffffffffffL
-};
-static final long[] jjbitVec4 = {
-   0xfffefffffffffffeL, 0xffffffffffffffffL, 0xffffffffffffffffL, 0xffffffffffffffffL
-};
-private int jjMoveNfa_2(int startState, int curPos)
-{
-   int startsAt = 0;
-   jjnewStateCnt = 43;
-   int i = 1;
-   jjstateSet[0] = startState;
-   int kind = 0x7fffffff;
-   for (;;)
-   {
-      if (++jjround == 0x7fffffff)
-         ReInitRounds();
-      if (curChar < 64)
-      {
-         long l = 1L << curChar;
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-                  if ((0xfbffd4f8ffffd9ffL & l) != 0L)
-                  {
-                     if (kind > 23)
-                        kind = 23;
-                     jjCheckNAddTwoStates(27, 28);
-                  }
-                  else if ((0x100002600L & l) != 0L)
-                  {
-                     if (kind > 7)
-                        kind = 7;
-                  }
-                  else if ((0x280200000000L & l) != 0L)
-                     jjstateSet[jjnewStateCnt++] = 15;
-                  else if (curChar == 34)
-                     jjCheckNAddStates(0, 2);
-                  if ((0x7bffd0f8ffffd9ffL & l) != 0L)
-                  {
-                     if (kind > 20)
-                        kind = 20;
-                     jjCheckNAddStates(3, 7);
-                  }
-                  else if (curChar == 42)
-                  {
-                     if (kind > 22)
-                        kind = 22;
-                  }
-                  else if (curChar == 33)
-                  {
-                     if (kind > 10)
-                        kind = 10;
-                  }
-                  if (curChar == 47)
-                     jjCheckNAddStates(8, 10);
-                  else if (curChar == 38)
-                     jjstateSet[jjnewStateCnt++] = 4;
-                  break;
-               case 43:
-               case 27:
-                  if ((0xfbfffcf8ffffd9ffL & l) == 0L)
-                     break;
-                  if (kind > 23)
-                     kind = 23;
-                  jjCheckNAddTwoStates(27, 28);
-                  break;
-               case 4:
-                  if (curChar == 38 && kind > 8)
-                     kind = 8;
-                  break;
-               case 5:
-                  if (curChar == 38)
-                     jjstateSet[jjnewStateCnt++] = 4;
-                  break;
-               case 13:
-                  if (curChar == 33 && kind > 10)
-                     kind = 10;
-                  break;
-               case 14:
-                  if ((0x280200000000L & l) != 0L)
-                     jjstateSet[jjnewStateCnt++] = 15;
-                  break;
-               case 15:
-                  if ((0x100002600L & l) != 0L && kind > 13)
-                     kind = 13;
-                  break;
-               case 16:
-                  if (curChar == 34)
-                     jjCheckNAddStates(0, 2);
-                  break;
-               case 17:
-                  if ((0xfffffffbffffffffL & l) != 0L)
-                     jjCheckNAddStates(0, 2);
-                  break;
-               case 19:
-                  jjCheckNAddStates(0, 2);
-                  break;
-               case 20:
-                  if (curChar == 34 && kind > 19)
-                     kind = 19;
-                  break;
-               case 22:
-                  if ((0x3ff000000000000L & l) == 0L)
-                     break;
-                  if (kind > 21)
-                     kind = 21;
-                  jjAddStates(11, 12);
-                  break;
-               case 23:
-                  if (curChar == 46)
-                     jjCheckNAdd(24);
-                  break;
-               case 24:
-                  if ((0x3ff000000000000L & l) == 0L)
-                     break;
-                  if (kind > 21)
-                     kind = 21;
-                  jjCheckNAdd(24);
-                  break;
-               case 25:
-                  if (curChar == 42 && kind > 22)
-                     kind = 22;
-                  break;
-               case 26:
-                  if ((0xfbffd4f8ffffd9ffL & l) == 0L)
-                     break;
-                  if (kind > 23)
-                     kind = 23;
-                  jjCheckNAddTwoStates(27, 28);
-                  break;
-               case 29:
-                  if (kind > 23)
-                     kind = 23;
-                  jjCheckNAddTwoStates(27, 28);
-                  break;
-               case 30:
-               case 32:
-                  if (curChar == 47)
-                     jjCheckNAddStates(8, 10);
-                  break;
-               case 31:
-                  if ((0xffff7fffffffffffL & l) != 0L)
-                     jjCheckNAddStates(8, 10);
-                  break;
-               case 34:
-                  if (curChar == 47 && kind > 24)
-                     kind = 24;
-                  break;
-               case 35:
-                  if ((0x7bffd0f8ffffd9ffL & l) == 0L)
-                     break;
-                  if (kind > 20)
-                     kind = 20;
-                  jjCheckNAddStates(3, 7);
-                  break;
-               case 36:
-                  if ((0x7bfff8f8ffffd9ffL & l) == 0L)
-                     break;
-                  if (kind > 20)
-                     kind = 20;
-                  jjCheckNAddTwoStates(36, 37);
-                  break;
-               case 38:
-                  if (kind > 20)
-                     kind = 20;
-                  jjCheckNAddTwoStates(36, 37);
-                  break;
-               case 39:
-                  if ((0x7bfff8f8ffffd9ffL & l) != 0L)
-                     jjCheckNAddStates(13, 15);
-                  break;
-               case 41:
-                  jjCheckNAddStates(13, 15);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else if (curChar < 128)
-      {
-         long l = 1L << (curChar & 077);
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-                  if ((0x97ffffff87ffffffL & l) != 0L)
-                  {
-                     if (kind > 20)
-                        kind = 20;
-                     jjCheckNAddStates(3, 7);
-                  }
-                  else if (curChar == 92)
-                     jjCheckNAddStates(16, 18);
-                  else if (curChar == 126)
-                  {
-                     if (kind > 21)
-                        kind = 21;
-                     jjstateSet[jjnewStateCnt++] = 22;
-                  }
-                  if ((0x97ffffff87ffffffL & l) != 0L)
-                  {
-                     if (kind > 23)
-                        kind = 23;
-                     jjCheckNAddTwoStates(27, 28);
-                  }
-                  if (curChar == 78)
-                     jjstateSet[jjnewStateCnt++] = 11;
-                  else if (curChar == 124)
-                     jjstateSet[jjnewStateCnt++] = 8;
-                  else if (curChar == 79)
-                     jjstateSet[jjnewStateCnt++] = 6;
-                  else if (curChar == 65)
-                     jjstateSet[jjnewStateCnt++] = 2;
-                  break;
-               case 43:
-                  if ((0x97ffffff87ffffffL & l) != 0L)
-                  {
-                     if (kind > 23)
-                        kind = 23;
-                     jjCheckNAddTwoStates(27, 28);
-                  }
-                  else if (curChar == 92)
-                     jjCheckNAddTwoStates(29, 29);
-                  break;
-               case 1:
-                  if (curChar == 68 && kind > 8)
-                     kind = 8;
-                  break;
-               case 2:
-                  if (curChar == 78)
-                     jjstateSet[jjnewStateCnt++] = 1;
-                  break;
-               case 3:
-                  if (curChar == 65)
-                     jjstateSet[jjnewStateCnt++] = 2;
-                  break;
-               case 6:
-                  if (curChar == 82 && kind > 9)
-                     kind = 9;
-                  break;
-               case 7:
-                  if (curChar == 79)
-                     jjstateSet[jjnewStateCnt++] = 6;
-                  break;
-               case 8:
-                  if (curChar == 124 && kind > 9)
-                     kind = 9;
-                  break;
-               case 9:
-                  if (curChar == 124)
-                     jjstateSet[jjnewStateCnt++] = 8;
-                  break;
-               case 10:
-                  if (curChar == 84 && kind > 10)
-                     kind = 10;
-                  break;
-               case 11:
-                  if (curChar == 79)
-                     jjstateSet[jjnewStateCnt++] = 10;
-                  break;
-               case 12:
-                  if (curChar == 78)
-                     jjstateSet[jjnewStateCnt++] = 11;
-                  break;
-               case 17:
-                  if ((0xffffffffefffffffL & l) != 0L)
-                     jjCheckNAddStates(0, 2);
-                  break;
-               case 18:
-                  if (curChar == 92)
-                     jjstateSet[jjnewStateCnt++] = 19;
-                  break;
-               case 19:
-                  jjCheckNAddStates(0, 2);
-                  break;
-               case 21:
-                  if (curChar != 126)
-                     break;
-                  if (kind > 21)
-                     kind = 21;
-                  jjstateSet[jjnewStateCnt++] = 22;
-                  break;
-               case 26:
-                  if ((0x97ffffff87ffffffL & l) == 0L)
-                     break;
-                  if (kind > 23)
-                     kind = 23;
-                  jjCheckNAddTwoStates(27, 28);
-                  break;
-               case 27:
-                  if ((0x97ffffff87ffffffL & l) == 0L)
-                     break;
-                  if (kind > 23)
-                     kind = 23;
-                  jjCheckNAddTwoStates(27, 28);
-                  break;
-               case 28:
-                  if (curChar == 92)
-                     jjCheckNAddTwoStates(29, 29);
-                  break;
-               case 29:
-                  if (kind > 23)
-                     kind = 23;
-                  jjCheckNAddTwoStates(27, 28);
-                  break;
-               case 31:
-                  jjAddStates(8, 10);
-                  break;
-               case 33:
-                  if (curChar == 92)
-                     jjstateSet[jjnewStateCnt++] = 32;
-                  break;
-               case 35:
-                  if ((0x97ffffff87ffffffL & l) == 0L)
-                     break;
-                  if (kind > 20)
-                     kind = 20;
-                  jjCheckNAddStates(3, 7);
-                  break;
-               case 36:
-                  if ((0x97ffffff87ffffffL & l) == 0L)
-                     break;
-                  if (kind > 20)
-                     kind = 20;
-                  jjCheckNAddTwoStates(36, 37);
-                  break;
-               case 37:
-                  if (curChar == 92)
-                     jjCheckNAddTwoStates(38, 38);
-                  break;
-               case 38:
-                  if (kind > 20)
-                     kind = 20;
-                  jjCheckNAddTwoStates(36, 37);
-                  break;
-               case 39:
-                  if ((0x97ffffff87ffffffL & l) != 0L)
-                     jjCheckNAddStates(13, 15);
-                  break;
-               case 40:
-                  if (curChar == 92)
-                     jjCheckNAddTwoStates(41, 41);
-                  break;
-               case 41:
-                  jjCheckNAddStates(13, 15);
-                  break;
-               case 42:
-                  if (curChar == 92)
-                     jjCheckNAddStates(16, 18);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else
-      {
-         int hiByte = (int)(curChar >> 8);
-         int i1 = hiByte >> 6;
-         long l1 = 1L << (hiByte & 077);
-         int i2 = (curChar & 0xff) >> 6;
-         long l2 = 1L << (curChar & 077);
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-                  if (jjCanMove_0(hiByte, i1, i2, l1, l2))
-                  {
-                     if (kind > 7)
-                        kind = 7;
-                  }
-                  if (jjCanMove_2(hiByte, i1, i2, l1, l2))
-                  {
-                     if (kind > 23)
-                        kind = 23;
-                     jjCheckNAddTwoStates(27, 28);
-                  }
-                  if (jjCanMove_2(hiByte, i1, i2, l1, l2))
-                  {
-                     if (kind > 20)
-                        kind = 20;
-                     jjCheckNAddStates(3, 7);
-                  }
-                  break;
-               case 43:
-               case 27:
-                  if (!jjCanMove_2(hiByte, i1, i2, l1, l2))
-                     break;
-                  if (kind > 23)
-                     kind = 23;
-                  jjCheckNAddTwoStates(27, 28);
-                  break;
-               case 15:
-                  if (jjCanMove_0(hiByte, i1, i2, l1, l2) && kind > 13)
-                     kind = 13;
-                  break;
-               case 17:
-               case 19:
-                  if (jjCanMove_1(hiByte, i1, i2, l1, l2))
-                     jjCheckNAddStates(0, 2);
-                  break;
-               case 26:
-                  if (!jjCanMove_2(hiByte, i1, i2, l1, l2))
-                     break;
-                  if (kind > 23)
-                     kind = 23;
-                  jjCheckNAddTwoStates(27, 28);
-                  break;
-               case 29:
-                  if (!jjCanMove_1(hiByte, i1, i2, l1, l2))
-                     break;
-                  if (kind > 23)
-                     kind = 23;
-                  jjCheckNAddTwoStates(27, 28);
-                  break;
-               case 31:
-                  if (jjCanMove_1(hiByte, i1, i2, l1, l2))
-                     jjAddStates(8, 10);
-                  break;
-               case 35:
-                  if (!jjCanMove_2(hiByte, i1, i2, l1, l2))
-                     break;
-                  if (kind > 20)
-                     kind = 20;
-                  jjCheckNAddStates(3, 7);
-                  break;
-               case 36:
-                  if (!jjCanMove_2(hiByte, i1, i2, l1, l2))
-                     break;
-                  if (kind > 20)
-                     kind = 20;
-                  jjCheckNAddTwoStates(36, 37);
-                  break;
-               case 38:
-                  if (!jjCanMove_1(hiByte, i1, i2, l1, l2))
-                     break;
-                  if (kind > 20)
-                     kind = 20;
-                  jjCheckNAddTwoStates(36, 37);
-                  break;
-               case 39:
-                  if (jjCanMove_2(hiByte, i1, i2, l1, l2))
-                     jjCheckNAddStates(13, 15);
-                  break;
-               case 41:
-                  if (jjCanMove_1(hiByte, i1, i2, l1, l2))
-                     jjCheckNAddStates(13, 15);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      if (kind != 0x7fffffff)
-      {
-         jjmatchedKind = kind;
-         jjmatchedPos = curPos;
-         kind = 0x7fffffff;
-      }
-      ++curPos;
-      if ((i = jjnewStateCnt) == (startsAt = 43 - (jjnewStateCnt = startsAt)))
-         return curPos;
-      try { curChar = input_stream.readChar(); }
-      catch(java.io.IOException e) { return curPos; }
-   }
-}
-private int jjMoveStringLiteralDfa0_0()
-{
-   return jjMoveNfa_0(0, 0);
-}
-private int jjMoveNfa_0(int startState, int curPos)
-{
-   int startsAt = 0;
-   jjnewStateCnt = 3;
-   int i = 1;
-   jjstateSet[0] = startState;
-   int kind = 0x7fffffff;
-   for (;;)
-   {
-      if (++jjround == 0x7fffffff)
-         ReInitRounds();
-      if (curChar < 64)
-      {
-         long l = 1L << curChar;
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-                  if ((0x3ff000000000000L & l) == 0L)
-                     break;
-                  if (kind > 27)
-                     kind = 27;
-                  jjAddStates(19, 20);
-                  break;
-               case 1:
-                  if (curChar == 46)
-                     jjCheckNAdd(2);
-                  break;
-               case 2:
-                  if ((0x3ff000000000000L & l) == 0L)
-                     break;
-                  if (kind > 27)
-                     kind = 27;
-                  jjCheckNAdd(2);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else if (curChar < 128)
-      {
-         long l = 1L << (curChar & 077);
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else
-      {
-         int hiByte = (int)(curChar >> 8);
-         int i1 = hiByte >> 6;
-         long l1 = 1L << (hiByte & 077);
-         int i2 = (curChar & 0xff) >> 6;
-         long l2 = 1L << (curChar & 077);
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      if (kind != 0x7fffffff)
-      {
-         jjmatchedKind = kind;
-         jjmatchedPos = curPos;
-         kind = 0x7fffffff;
-      }
-      ++curPos;
-      if ((i = jjnewStateCnt) == (startsAt = 3 - (jjnewStateCnt = startsAt)))
-         return curPos;
-      try { curChar = input_stream.readChar(); }
-      catch(java.io.IOException e) { return curPos; }
-   }
-}
-private final int jjStopStringLiteralDfa_1(int pos, long active0)
-{
-   switch (pos)
-   {
-      case 0:
-         if ((active0 & 0x10000000L) != 0L)
-         {
-            jjmatchedKind = 32;
-            return 6;
-         }
-         return -1;
-      default :
-         return -1;
-   }
-}
-private final int jjStartNfa_1(int pos, long active0)
-{
-   return jjMoveNfa_1(jjStopStringLiteralDfa_1(pos, active0), pos + 1);
-}
-private int jjMoveStringLiteralDfa0_1()
-{
-   switch(curChar)
-   {
-      case 84:
-         return jjMoveStringLiteralDfa1_1(0x10000000L);
-      case 93:
-         return jjStopAtPos(0, 29);
-      case 125:
-         return jjStopAtPos(0, 30);
-      default :
-         return jjMoveNfa_1(0, 0);
-   }
-}
-private int jjMoveStringLiteralDfa1_1(long active0)
-{
-   try { curChar = input_stream.readChar(); }
-   catch(java.io.IOException e) {
-      jjStopStringLiteralDfa_1(0, active0);
-      return 1;
-   }
-   switch(curChar)
-   {
-      case 79:
-         if ((active0 & 0x10000000L) != 0L)
-            return jjStartNfaWithStates_1(1, 28, 6);
-         break;
-      default :
-         break;
-   }
-   return jjStartNfa_1(0, active0);
-}
-private int jjStartNfaWithStates_1(int pos, int kind, int state)
-{
-   jjmatchedKind = kind;
-   jjmatchedPos = pos;
-   try { curChar = input_stream.readChar(); }
-   catch(java.io.IOException e) { return pos + 1; }
-   return jjMoveNfa_1(state, pos + 1);
-}
-private int jjMoveNfa_1(int startState, int curPos)
-{
-   int startsAt = 0;
-   jjnewStateCnt = 7;
-   int i = 1;
-   jjstateSet[0] = startState;
-   int kind = 0x7fffffff;
-   for (;;)
-   {
-      if (++jjround == 0x7fffffff)
-         ReInitRounds();
-      if (curChar < 64)
-      {
-         long l = 1L << curChar;
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-                  if ((0xfffffffeffffffffL & l) != 0L)
-                  {
-                     if (kind > 32)
-                        kind = 32;
-                     jjCheckNAdd(6);
-                  }
-                  if ((0x100002600L & l) != 0L)
-                  {
-                     if (kind > 7)
-                        kind = 7;
-                  }
-                  else if (curChar == 34)
-                     jjCheckNAddTwoStates(2, 4);
-                  break;
-               case 1:
-                  if (curChar == 34)
-                     jjCheckNAddTwoStates(2, 4);
-                  break;
-               case 2:
-                  if ((0xfffffffbffffffffL & l) != 0L)
-                     jjCheckNAddStates(21, 23);
-                  break;
-               case 3:
-                  if (curChar == 34)
-                     jjCheckNAddStates(21, 23);
-                  break;
-               case 5:
-                  if (curChar == 34 && kind > 31)
-                     kind = 31;
-                  break;
-               case 6:
-                  if ((0xfffffffeffffffffL & l) == 0L)
-                     break;
-                  if (kind > 32)
-                     kind = 32;
-                  jjCheckNAdd(6);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else if (curChar < 128)
-      {
-         long l = 1L << (curChar & 077);
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-               case 6:
-                  if ((0xdfffffffdfffffffL & l) == 0L)
-                     break;
-                  if (kind > 32)
-                     kind = 32;
-                  jjCheckNAdd(6);
-                  break;
-               case 2:
-                  jjAddStates(21, 23);
-                  break;
-               case 4:
-                  if (curChar == 92)
-                     jjstateSet[jjnewStateCnt++] = 3;
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else
-      {
-         int hiByte = (int)(curChar >> 8);
-         int i1 = hiByte >> 6;
-         long l1 = 1L << (hiByte & 077);
-         int i2 = (curChar & 0xff) >> 6;
-         long l2 = 1L << (curChar & 077);
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-                  if (jjCanMove_0(hiByte, i1, i2, l1, l2))
-                  {
-                     if (kind > 7)
-                        kind = 7;
-                  }
-                  if (jjCanMove_1(hiByte, i1, i2, l1, l2))
-                  {
-                     if (kind > 32)
-                        kind = 32;
-                     jjCheckNAdd(6);
-                  }
-                  break;
-               case 2:
-                  if (jjCanMove_1(hiByte, i1, i2, l1, l2))
-                     jjAddStates(21, 23);
-                  break;
-               case 6:
-                  if (!jjCanMove_1(hiByte, i1, i2, l1, l2))
-                     break;
-                  if (kind > 32)
-                     kind = 32;
-                  jjCheckNAdd(6);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      if (kind != 0x7fffffff)
-      {
-         jjmatchedKind = kind;
-         jjmatchedPos = curPos;
-         kind = 0x7fffffff;
-      }
-      ++curPos;
-      if ((i = jjnewStateCnt) == (startsAt = 7 - (jjnewStateCnt = startsAt)))
-         return curPos;
-      try { curChar = input_stream.readChar(); }
-      catch(java.io.IOException e) { return curPos; }
-   }
-}
-static final int[] jjnextStates = {
-   17, 18, 20, 36, 39, 25, 40, 37, 31, 33, 34, 22, 23, 39, 25, 40, 
-   38, 41, 29, 0, 1, 2, 4, 5, 
-};
-private static final boolean jjCanMove_0(int hiByte, int i1, int i2, long l1, long l2)
-{
-   switch(hiByte)
-   {
-      case 48:
-         return ((jjbitVec0[i2] & l2) != 0L);
-      default :
-         return false;
-   }
-}
-private static final boolean jjCanMove_1(int hiByte, int i1, int i2, long l1, long l2)
-{
-   switch(hiByte)
-   {
-      case 0:
-         return ((jjbitVec3[i2] & l2) != 0L);
-      default :
-         if ((jjbitVec1[i1] & l1) != 0L)
-            return true;
-         return false;
-   }
-}
-private static final boolean jjCanMove_2(int hiByte, int i1, int i2, long l1, long l2)
-{
-   switch(hiByte)
-   {
-      case 0:
-         return ((jjbitVec3[i2] & l2) != 0L);
-      case 48:
-         return ((jjbitVec1[i2] & l2) != 0L);
-      default :
-         if ((jjbitVec4[i1] & l1) != 0L)
-            return true;
-         return false;
-   }
-}
-
-/** Token literal values. */
-public static final String[] jjstrLiteralImages = {
-"", null, null, null, null, null, null, null, null, null, null, "\53", "\55", 
-null, "\50", "\51", "\72", "\52", "\136", null, null, null, null, null, null, 
-"\133", "\173", null, "\124\117", "\135", "\175", null, null, };
-
-/** Lexer state names. */
-public static final String[] lexStateNames = {
-   "Boost",
-   "Range",
-   "DEFAULT",
-};
-
-/** Lex State array. */
-public static final int[] jjnewLexState = {
-   -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 
-   1, 1, 2, -1, 2, 2, -1, -1, 
-};
-static final long[] jjtoToken = {
-   0x1ffffff01L, 
-};
-static final long[] jjtoSkip = {
-   0x80L, 
-};
-protected CharStream input_stream;
-private final int[] jjrounds = new int[43];
-private final int[] jjstateSet = new int[86];
-protected char curChar;
-/** Constructor. */
-public QueryParserTokenManager(CharStream stream){
-   input_stream = stream;
-}
-
-/** Constructor. */
-public QueryParserTokenManager(CharStream stream, int lexState){
-   this(stream);
-   SwitchTo(lexState);
-}
-
-/** Reinitialise parser. */
-public void ReInit(CharStream stream)
-{
-   jjmatchedPos = jjnewStateCnt = 0;
-   curLexState = defaultLexState;
-   input_stream = stream;
-   ReInitRounds();
-}
-private void ReInitRounds()
-{
-   int i;
-   jjround = 0x80000001;
-   for (i = 43; i-- > 0;)
-      jjrounds[i] = 0x80000000;
-}
-
-/** Reinitialise parser. */
-public void ReInit(CharStream stream, int lexState)
-{
-   ReInit(stream);
-   SwitchTo(lexState);
-}
-
-/** Switch to specified lex state. */
-public void SwitchTo(int lexState)
-{
-   if (lexState >= 3 || lexState < 0)
-      throw new TokenMgrError("Error: Ignoring invalid lexical state : " + lexState + ". State unchanged.", TokenMgrError.INVALID_LEXICAL_STATE);
-   else
-      curLexState = lexState;
-}
-
-protected Token jjFillToken()
-{
-   final Token t;
-   final String curTokenImage;
-   final int beginLine;
-   final int endLine;
-   final int beginColumn;
-   final int endColumn;
-   String im = jjstrLiteralImages[jjmatchedKind];
-   curTokenImage = (im == null) ? input_stream.GetImage() : im;
-   beginLine = input_stream.getBeginLine();
-   beginColumn = input_stream.getBeginColumn();
-   endLine = input_stream.getEndLine();
-   endColumn = input_stream.getEndColumn();
-   t = Token.newToken(jjmatchedKind, curTokenImage);
-
-   t.beginLine = beginLine;
-   t.endLine = endLine;
-   t.beginColumn = beginColumn;
-   t.endColumn = endColumn;
-
-   return t;
-}
-
-int curLexState = 2;
-int defaultLexState = 2;
-int jjnewStateCnt;
-int jjround;
-int jjmatchedPos;
-int jjmatchedKind;
-
-/** Get the next Token. */
-public Token getNextToken() 
-{
-  Token matchedToken;
-  int curPos = 0;
-
-  EOFLoop :
-  for (;;)
-  {
-   try
-   {
-      curChar = input_stream.BeginToken();
-   }
-   catch(java.io.IOException e)
-   {
-      jjmatchedKind = 0;
-      matchedToken = jjFillToken();
-      return matchedToken;
-   }
-
-   switch(curLexState)
-   {
-     case 0:
-       jjmatchedKind = 0x7fffffff;
-       jjmatchedPos = 0;
-       curPos = jjMoveStringLiteralDfa0_0();
-       break;
-     case 1:
-       jjmatchedKind = 0x7fffffff;
-       jjmatchedPos = 0;
-       curPos = jjMoveStringLiteralDfa0_1();
-       break;
-     case 2:
-       jjmatchedKind = 0x7fffffff;
-       jjmatchedPos = 0;
-       curPos = jjMoveStringLiteralDfa0_2();
-       break;
-   }
-     if (jjmatchedKind != 0x7fffffff)
-     {
-        if (jjmatchedPos + 1 < curPos)
-           input_stream.backup(curPos - jjmatchedPos - 1);
-        if ((jjtoToken[jjmatchedKind >> 6] & (1L << (jjmatchedKind & 077))) != 0L)
-        {
-           matchedToken = jjFillToken();
-       if (jjnewLexState[jjmatchedKind] != -1)
-         curLexState = jjnewLexState[jjmatchedKind];
-           return matchedToken;
-        }
-        else
-        {
-         if (jjnewLexState[jjmatchedKind] != -1)
-           curLexState = jjnewLexState[jjmatchedKind];
-           continue EOFLoop;
-        }
-     }
-     int error_line = input_stream.getEndLine();
-     int error_column = input_stream.getEndColumn();
-     String error_after = null;
-     boolean EOFSeen = false;
-     try { input_stream.readChar(); input_stream.backup(1); }
-     catch (java.io.IOException e1) {
-        EOFSeen = true;
-        error_after = curPos <= 1 ? "" : input_stream.GetImage();
-        if (curChar == '\n' || curChar == '\r') {
-           error_line++;
-           error_column = 0;
-        }
-        else
-           error_column++;
-     }
-     if (!EOFSeen) {
-        input_stream.backup(1);
-        error_after = curPos <= 1 ? "" : input_stream.GetImage();
-     }
-     throw new TokenMgrError(EOFSeen, curLexState, error_line, error_column, error_after, curChar, TokenMgrError.LEXICAL_ERROR);
-  }
-}
-
-private void jjCheckNAdd(int state)
-{
-   if (jjrounds[state] != jjround)
-   {
-      jjstateSet[jjnewStateCnt++] = state;
-      jjrounds[state] = jjround;
-   }
-}
-private void jjAddStates(int start, int end)
-{
-   do {
-      jjstateSet[jjnewStateCnt++] = jjnextStates[start];
-   } while (start++ != end);
-}
-private void jjCheckNAddTwoStates(int state1, int state2)
-{
-   jjCheckNAdd(state1);
-   jjCheckNAdd(state2);
-}
-
-private void jjCheckNAddStates(int start, int end)
-{
-   do {
-      jjCheckNAdd(jjnextStates[start]);
-   } while (start++ != end);
-}
-
-}
diff --git a/lucene/src/java/org/apache/lucene/queryParser/Token.java b/lucene/src/java/org/apache/lucene/queryParser/Token.java
deleted file mode 100644
index 2c665d6..0000000
--- a/lucene/src/java/org/apache/lucene/queryParser/Token.java
+++ /dev/null
@@ -1,124 +0,0 @@
-/* Generated By:JavaCC: Do not edit this line. Token.java Version 4.1 */
-/* JavaCCOptions:TOKEN_EXTENDS=,KEEP_LINE_COL=null */
-package org.apache.lucene.queryParser;
-
-/**
- * Describes the input token stream.
- */
-
-public class Token {
-
-  /**
-   * An integer that describes the kind of this token.  This numbering
-   * system is determined by JavaCCParser, and a table of these numbers is
-   * stored in the file ...Constants.java.
-   */
-  public int kind;
-
-  /** The line number of the first character of this Token. */
-  public int beginLine;
-  /** The column number of the first character of this Token. */
-  public int beginColumn;
-  /** The line number of the last character of this Token. */
-  public int endLine;
-  /** The column number of the last character of this Token. */
-  public int endColumn;
-
-  /**
-   * The string image of the token.
-   */
-  public String image;
-
-  /**
-   * A reference to the next regular (non-special) token from the input
-   * stream.  If this is the last token from the input stream, or if the
-   * token manager has not read tokens beyond this one, this field is
-   * set to null.  This is true only if this token is also a regular
-   * token.  Otherwise, see below for a description of the contents of
-   * this field.
-   */
-  public Token next;
-
-  /**
-   * This field is used to access special tokens that occur prior to this
-   * token, but after the immediately preceding regular (non-special) token.
-   * If there are no such special tokens, this field is set to null.
-   * When there are more than one such special token, this field refers
-   * to the last of these special tokens, which in turn refers to the next
-   * previous special token through its specialToken field, and so on
-   * until the first special token (whose specialToken field is null).
-   * The next fields of special tokens refer to other special tokens that
-   * immediately follow it (without an intervening regular token).  If there
-   * is no such token, this field is null.
-   */
-  public Token specialToken;
-
-  /**
-   * An optional attribute value of the Token.
-   * Tokens which are not used as syntactic sugar will often contain
-   * meaningful values that will be used later on by the compiler or
-   * interpreter. This attribute value is often different from the image.
-   * Any subclass of Token that actually wants to return a non-null value can
-   * override this method as appropriate.
-   */
-  public Object getValue() {
-    return null;
-  }
-
-  /**
-   * No-argument constructor
-   */
-  public Token() {}
-
-  /**
-   * Constructs a new token for the specified Image.
-   */
-  public Token(int kind)
-  {
-     this(kind, null);
-  }
-
-  /**
-   * Constructs a new token for the specified Image and Kind.
-   */
-  public Token(int kind, String image)
-  {
-     this.kind = kind;
-     this.image = image;
-  }
-
-  /**
-   * Returns the image.
-   */
-  public String toString()
-  {
-     return image;
-  }
-
-  /**
-   * Returns a new Token object, by default. However, if you want, you
-   * can create and return subclass objects based on the value of ofKind.
-   * Simply add the cases to the switch for all those special cases.
-   * For example, if you have a subclass of Token called IDToken that
-   * you want to create if ofKind is ID, simply add something like :
-   *
-   *    case MyParserConstants.ID : return new IDToken(ofKind, image);
-   *
-   * to the following switch statement. Then you can cast matchedToken
-   * variable to the appropriate type and use sit in your lexical actions.
-   */
-  public static Token newToken(int ofKind, String image)
-  {
-     switch(ofKind)
-     {
-       default : return new Token(ofKind, image);
-     }
-  }
-
-  public static Token newToken(int ofKind)
-  {
-     return newToken(ofKind, null);
-  }
-
-}
-/* JavaCC - OriginalChecksum=c147cc166a7cf8812c7c39bc8c5eb868 (do not edit this line) */
diff --git a/lucene/src/java/org/apache/lucene/queryParser/TokenMgrError.java b/lucene/src/java/org/apache/lucene/queryParser/TokenMgrError.java
deleted file mode 100644
index b4ffd42..0000000
--- a/lucene/src/java/org/apache/lucene/queryParser/TokenMgrError.java
+++ /dev/null
@@ -1,141 +0,0 @@
-/* Generated By:JavaCC: Do not edit this line. TokenMgrError.java Version 4.1 */
-/* JavaCCOptions: */
-package org.apache.lucene.queryParser;
-
-/** Token Manager Error. */
-@SuppressWarnings("serial")
-public class TokenMgrError extends Error
-{
-
-   /*
-    * Ordinals for various reasons why an Error of this type can be thrown.
-    */
-
-   /**
-    * Lexical error occurred.
-    */
-   static final int LEXICAL_ERROR = 0;
-
-   /**
-    * An attempt was made to create a second instance of a static token manager.
-    */
-   static final int STATIC_LEXER_ERROR = 1;
-
-   /**
-    * Tried to change to an invalid lexical state.
-    */
-   static final int INVALID_LEXICAL_STATE = 2;
-
-   /**
-    * Detected (and bailed out of) an infinite loop in the token manager.
-    */
-   static final int LOOP_DETECTED = 3;
-
-   /**
-    * Indicates the reason why the exception is thrown. It will have
-    * one of the above 4 values.
-    */
-   int errorCode;
-
-   /**
-    * Replaces unprintable characters by their escaped (or unicode escaped)
-    * equivalents in the given string
-    */
-   protected static final String addEscapes(String str) {
-      StringBuffer retval = new StringBuffer();
-      char ch;
-      for (int i = 0; i < str.length(); i++) {
-        switch (str.charAt(i))
-        {
-           case 0 :
-              continue;
-           case '\b':
-              retval.append("\\b");
-              continue;
-           case '\t':
-              retval.append("\\t");
-              continue;
-           case '\n':
-              retval.append("\\n");
-              continue;
-           case '\f':
-              retval.append("\\f");
-              continue;
-           case '\r':
-              retval.append("\\r");
-              continue;
-           case '\"':
-              retval.append("\\\"");
-              continue;
-           case '\'':
-              retval.append("\\\'");
-              continue;
-           case '\\':
-              retval.append("\\\\");
-              continue;
-           default:
-              if ((ch = str.charAt(i)) < 0x20 || ch > 0x7e) {
-                 String s = "0000" + Integer.toString(ch, 16);
-                 retval.append("\\u" + s.substring(s.length() - 4, s.length()));
-              } else {
-                 retval.append(ch);
-              }
-              continue;
-        }
-      }
-      return retval.toString();
-   }
-
-   /**
-    * Returns a detailed message for the Error when it is thrown by the
-    * token manager to indicate a lexical error.
-    * Parameters :
-    *    EOFSeen     : indicates if EOF caused the lexical error
-    *    curLexState : lexical state in which this error occurred
-    *    errorLine   : line number when the error occurred
-    *    errorColumn : column number when the error occurred
-    *    errorAfter  : prefix that was seen before this error occurred
-    *    curchar     : the offending character
-    * Note: You can customize the lexical error message by modifying this method.
-    */
-   protected static String LexicalError(boolean EOFSeen, int lexState, int errorLine, int errorColumn, String errorAfter, char curChar) {
-      return("Lexical error at line " +
-           errorLine + ", column " +
-           errorColumn + ".  Encountered: " +
-           (EOFSeen ? "<EOF> " : ("\"" + addEscapes(String.valueOf(curChar)) + "\"") + " (" + (int)curChar + "), ") +
-           "after : \"" + addEscapes(errorAfter) + "\"");
-   }
-
-   /**
-    * You can also modify the body of this method to customize your error messages.
-    * For example, cases like LOOP_DETECTED and INVALID_LEXICAL_STATE are not
-    * of end-users concern, so you can return something like :
-    *
-    *     "Internal Error : Please file a bug report .... "
-    *
-    * from this method for such cases in the release version of your parser.
-    */
-   public String getMessage() {
-      return super.getMessage();
-   }
-
-   /*
-    * Constructors of various flavors follow.
-    */
-
-   /** No arg constructor. */
-   public TokenMgrError() {
-   }
-
-   /** Constructor with message and reason. */
-   public TokenMgrError(String message, int reason) {
-      super(message);
-      errorCode = reason;
-   }
-
-   /** Full Constructor. */
-   public TokenMgrError(boolean EOFSeen, int lexState, int errorLine, int errorColumn, String errorAfter, char curChar, int reason) {
-      this(LexicalError(EOFSeen, lexState, errorLine, errorColumn, errorAfter, curChar), reason);
-   }
-}
-/* JavaCC - OriginalChecksum=1c94e13236c7e0121e49427992341ee3 (do not edit this line) */
diff --git a/lucene/src/java/org/apache/lucene/queryParser/package.html b/lucene/src/java/org/apache/lucene/queryParser/package.html
deleted file mode 100644
index d4017aa..0000000
--- a/lucene/src/java/org/apache/lucene/queryParser/package.html
+++ /dev/null
@@ -1,35 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-
-A simple query parser implemented with JavaCC.
-<p>Note that JavaCC defines lots of public classes, methods and fields
-that do not need to be public.&nbsp; These clutter the documentation.&nbsp;
-Sorry.
-<p>Note that because JavaCC defines a class named <tt>Token</tt>, <tt>org.apache.lucene.analysis.Token</tt>
-must always be fully qualified in source code in this package.
-
-<p><b>NOTE</b>: contrib/queryparser has an alternative queryparser that matches the syntax of this one, but is more modular,
-enabling substantial customization to how a query is created.
-
-</body>
-</html>
diff --git a/lucene/src/java/org/apache/lucene/search/MultiTermQuery.java b/lucene/src/java/org/apache/lucene/search/MultiTermQuery.java
index e8e7020..aba1781 100644
--- a/lucene/src/java/org/apache/lucene/search/MultiTermQuery.java
+++ b/lucene/src/java/org/apache/lucene/search/MultiTermQuery.java
@@ -23,7 +23,6 @@ import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.queryParser.QueryParser;
 import org.apache.lucene.util.AttributeSource;
 import org.apache.lucene.util.TermContext;
 
@@ -56,7 +55,7 @@ import org.apache.lucene.util.TermContext;
  * a priority queue to only collect competitive terms
  * and not hit this limitation.
  *
- * Note that {@link QueryParser} produces
+ * Note that org.apache.lucene.queryparser.classic.QueryParser produces
  * MultiTermQueries using {@link
  * #CONSTANT_SCORE_AUTO_REWRITE_DEFAULT} by default.
  */
diff --git a/lucene/src/java/org/apache/lucene/search/Query.java b/lucene/src/java/org/apache/lucene/search/Query.java
index d4a8d43..0bf05ec 100644
--- a/lucene/src/java/org/apache/lucene/search/Query.java
+++ b/lucene/src/java/org/apache/lucene/search/Query.java
@@ -39,10 +39,6 @@ import org.apache.lucene.index.Term;
     <li> {@link NumericRangeQuery}
     <li> {@link org.apache.lucene.search.spans.SpanQuery}
     </ul>
-    <p>A parser for queries is contained in:
-    <ul>
-    <li>{@link org.apache.lucene.queryParser.QueryParser QueryParser}
-    </ul>
 */
 public abstract class Query implements Cloneable {
   private float boost = 1.0f;                     // query boost factor
@@ -61,17 +57,6 @@ public abstract class Query implements Cloneable {
 
   /** Prints a query to a string, with <code>field</code> assumed to be the 
    * default field and omitted.
-   * <p>The representation used is one that is supposed to be readable
-   * by {@link org.apache.lucene.queryParser.QueryParser QueryParser}. However,
-   * there are the following limitations:
-   * <ul>
-   *  <li>If the query was created by the parser, the printed
-   *  representation may not be exactly what was parsed. For example,
-   *  characters that need to be escaped will be represented without
-   *  the required backslash.</li>
-   * <li>Some of the more complicated queries (e.g. span queries)
-   *  don't have a representation that can be parsed by QueryParser.</li>
-   * </ul>
    */
   public abstract String toString(String field);
 
diff --git a/lucene/src/test/org/apache/lucene/queryParser/TestMultiAnalyzer.java b/lucene/src/test/org/apache/lucene/queryParser/TestMultiAnalyzer.java
deleted file mode 100644
index 56246fb..0000000
--- a/lucene/src/test/org/apache/lucene/queryParser/TestMultiAnalyzer.java
+++ /dev/null
@@ -1,273 +0,0 @@
-package org.apache.lucene.queryParser;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Reader;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.analysis.TokenFilter;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.util.Version;
-
-/**
- * Test QueryParser's ability to deal with Analyzers that return more
- * than one token per position or that return tokens with a position
- * increment &gt; 1.
- *
- */
-public class TestMultiAnalyzer extends BaseTokenStreamTestCase {
-
-  private static int multiToken = 0;
-
-  public void testMultiAnalyzer() throws ParseException {
-    
-    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "", new MultiAnalyzer());
-
-    // trivial, no multiple tokens:
-    assertEquals("foo", qp.parse("foo").toString());
-    assertEquals("foo", qp.parse("\"foo\"").toString());
-    assertEquals("foo foobar", qp.parse("foo foobar").toString());
-    assertEquals("\"foo foobar\"", qp.parse("\"foo foobar\"").toString());
-    assertEquals("\"foo foobar blah\"", qp.parse("\"foo foobar blah\"").toString());
-
-    // two tokens at the same position:
-    assertEquals("(multi multi2) foo", qp.parse("multi foo").toString());
-    assertEquals("foo (multi multi2)", qp.parse("foo multi").toString());
-    assertEquals("(multi multi2) (multi multi2)", qp.parse("multi multi").toString());
-    assertEquals("+(foo (multi multi2)) +(bar (multi multi2))",
-        qp.parse("+(foo multi) +(bar multi)").toString());
-    assertEquals("+(foo (multi multi2)) field:\"bar (multi multi2)\"",
-        qp.parse("+(foo multi) field:\"bar multi\"").toString());
-
-    // phrases:
-    assertEquals("\"(multi multi2) foo\"", qp.parse("\"multi foo\"").toString());
-    assertEquals("\"foo (multi multi2)\"", qp.parse("\"foo multi\"").toString());
-    assertEquals("\"foo (multi multi2) foobar (multi multi2)\"",
-        qp.parse("\"foo multi foobar multi\"").toString());
-
-    // fields:
-    assertEquals("(field:multi field:multi2) field:foo", qp.parse("field:multi field:foo").toString());
-    assertEquals("field:\"(multi multi2) foo\"", qp.parse("field:\"multi foo\"").toString());
-
-    // three tokens at one position:
-    assertEquals("triplemulti multi3 multi2", qp.parse("triplemulti").toString());
-    assertEquals("foo (triplemulti multi3 multi2) foobar",
-        qp.parse("foo triplemulti foobar").toString());
-
-    // phrase with non-default slop:
-    assertEquals("\"(multi multi2) foo\"~10", qp.parse("\"multi foo\"~10").toString());
-
-    // phrase with non-default boost:
-    assertEquals("\"(multi multi2) foo\"^2.0", qp.parse("\"multi foo\"^2").toString());
-
-    // phrase after changing default slop
-    qp.setPhraseSlop(99);
-    assertEquals("\"(multi multi2) foo\"~99 bar",
-                 qp.parse("\"multi foo\" bar").toString());
-    assertEquals("\"(multi multi2) foo\"~99 \"foo bar\"~2",
-                 qp.parse("\"multi foo\" \"foo bar\"~2").toString());
-    qp.setPhraseSlop(0);
-
-    // non-default operator:
-    qp.setDefaultOperator(QueryParser.AND_OPERATOR);
-    assertEquals("+(multi multi2) +foo", qp.parse("multi foo").toString());
-
-  }
-    
-  public void testMultiAnalyzerWithSubclassOfQueryParser() throws ParseException {
-
-    DumbQueryParser qp = new DumbQueryParser("", new MultiAnalyzer());
-    qp.setPhraseSlop(99); // modified default slop
-
-    // direct call to (super's) getFieldQuery to demonstrate differnce
-    // between phrase and multiphrase with modified default slop
-    assertEquals("\"foo bar\"~99",
-                 qp.getSuperFieldQuery("","foo bar", true).toString());
-    assertEquals("\"(multi multi2) bar\"~99",
-                 qp.getSuperFieldQuery("","multi bar", true).toString());
-
-    
-    // ask sublcass to parse phrase with modified default slop
-    assertEquals("\"(multi multi2) foo\"~99 bar",
-                 qp.parse("\"multi foo\" bar").toString());
-    
-  }
-    
-  public void testPosIncrementAnalyzer() throws ParseException {
-    QueryParser qp = new QueryParser(Version.LUCENE_40, "", new PosIncrementAnalyzer());
-    assertEquals("quick brown", qp.parse("the quick brown").toString());
-    assertEquals("quick brown fox", qp.parse("the quick brown fox").toString());
-  }
-  
-  /**
-   * Expands "multi" to "multi" and "multi2", both at the same position,
-   * and expands "triplemulti" to "triplemulti", "multi3", and "multi2".  
-   */
-  private class MultiAnalyzer extends Analyzer {
-
-    public MultiAnalyzer() {
-    }
-
-    @Override
-    public TokenStream tokenStream(String fieldName, Reader reader) {
-      TokenStream result = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);
-      result = new TestFilter(result);
-      return result;
-    }
-  }
-
-  private final class TestFilter extends TokenFilter {
-    
-    private String prevType;
-    private int prevStartOffset;
-    private int prevEndOffset;
-    
-    CharTermAttribute termAtt;
-    PositionIncrementAttribute posIncrAtt;
-    OffsetAttribute offsetAtt;
-    TypeAttribute typeAtt;
-    
-    public TestFilter(TokenStream in) {
-      super(in);
-      termAtt = addAttribute(CharTermAttribute.class);
-      posIncrAtt = addAttribute(PositionIncrementAttribute.class);
-      offsetAtt = addAttribute(OffsetAttribute.class);
-      typeAtt = addAttribute(TypeAttribute.class);
-    }
-
-    @Override
-    public final boolean incrementToken() throws java.io.IOException {
-      if (multiToken > 0) {
-        termAtt.setEmpty().append("multi"+(multiToken+1));
-        offsetAtt.setOffset(prevStartOffset, prevEndOffset);
-        typeAtt.setType(prevType);
-        posIncrAtt.setPositionIncrement(0);
-        multiToken--;
-        return true;
-      } else {
-        boolean next = input.incrementToken();
-        if (next == false) {
-          return false;
-        }
-        prevType = typeAtt.type();
-        prevStartOffset = offsetAtt.startOffset();
-        prevEndOffset = offsetAtt.endOffset();
-        String text = termAtt.toString();
-        if (text.equals("triplemulti")) {
-          multiToken = 2;
-          return true;
-        } else if (text.equals("multi")) {
-          multiToken = 1;
-          return true;
-        } else {
-          return true;
-        }
-      }
-    }
-  }
-
-  /**
-   * Analyzes "the quick brown" as: quick(incr=2) brown(incr=1).
-   * Does not work correctly for input other than "the quick brown ...".
-   */
-  private class PosIncrementAnalyzer extends Analyzer {
-
-    public PosIncrementAnalyzer() {
-    }
-
-    @Override
-    public TokenStream tokenStream(String fieldName, Reader reader) {
-      TokenStream result = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);
-      result = new TestPosIncrementFilter(result);
-      return result;
-    }
-  }
-
-  private final class TestPosIncrementFilter extends TokenFilter {
-    
-    CharTermAttribute termAtt;
-    PositionIncrementAttribute posIncrAtt;
-    
-    public TestPosIncrementFilter(TokenStream in) {
-      super(in);
-      termAtt = addAttribute(CharTermAttribute.class);
-      posIncrAtt = addAttribute(PositionIncrementAttribute.class);
-    }
-
-    @Override
-    public final boolean incrementToken () throws java.io.IOException {
-      while(input.incrementToken()) {
-        if (termAtt.toString().equals("the")) {
-          // stopword, do nothing
-        } else if (termAtt.toString().equals("quick")) {
-          posIncrAtt.setPositionIncrement(2);
-          return true;
-        } else {
-          posIncrAtt.setPositionIncrement(1);
-          return true;
-        }
-      }
-      return false;
-    }
-  }
-
-    /** a very simple subclass of QueryParser */
-    private final static class DumbQueryParser extends QueryParser {
-        
-        public DumbQueryParser(String f, Analyzer a) {
-            super(TEST_VERSION_CURRENT, f, a);
-        }
-
-        /** expose super's version */
-        public Query getSuperFieldQuery(String f, String t, boolean quoted) 
-            throws ParseException {
-            return super.getFieldQuery(f,t,quoted);
-        }
-        /** wrap super's version */
-        @Override
-        protected Query getFieldQuery(String f, String t, boolean quoted)
-            throws ParseException {
-            return new DumbQueryWrapper(getSuperFieldQuery(f,t,quoted));
-        }
-    }
-    
-    /**
-     * A very simple wrapper to prevent instanceof checks but uses
-     * the toString of the query it wraps.
-     */
-    private final static class DumbQueryWrapper extends Query {
-
-        private Query q;
-        public DumbQueryWrapper(Query q) {
-            super();
-            this.q = q;
-        }
-        @Override
-        public String toString(String f) {
-            return q.toString(f);
-        }
-    }
-    
-}
diff --git a/lucene/src/test/org/apache/lucene/queryParser/TestMultiFieldQueryParser.java b/lucene/src/test/org/apache/lucene/queryParser/TestMultiFieldQueryParser.java
deleted file mode 100644
index 4611aca..0000000
--- a/lucene/src/test/org/apache/lucene/queryParser/TestMultiFieldQueryParser.java
+++ /dev/null
@@ -1,328 +0,0 @@
-package org.apache.lucene.queryParser;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.Reader;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.ScoreDoc;
-import org.apache.lucene.search.BooleanClause.Occur;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.LuceneTestCase;
-
-/**
- * Tests QueryParser.
- */
-public class TestMultiFieldQueryParser extends LuceneTestCase {
-
-  /** test stop words parsing for both the non static form, and for the 
-   * corresponding static form (qtxt, fields[]). */
-  public void testStopwordsParsing() throws Exception {
-    assertStopQueryEquals("one", "b:one t:one");  
-    assertStopQueryEquals("one stop", "b:one t:one");  
-    assertStopQueryEquals("one (stop)", "b:one t:one");  
-    assertStopQueryEquals("one ((stop))", "b:one t:one");  
-    assertStopQueryEquals("stop", "");  
-    assertStopQueryEquals("(stop)", "");  
-    assertStopQueryEquals("((stop))", "");  
-  }
-
-  // verify parsing of query using a stopping analyzer  
-  private void assertStopQueryEquals (String qtxt, String expectedRes) throws Exception {
-    String[] fields = {"b", "t"};
-    Occur occur[] = {Occur.SHOULD, Occur.SHOULD};
-    TestQueryParser.QPTestAnalyzer a = new TestQueryParser.QPTestAnalyzer();
-    MultiFieldQueryParser mfqp = new MultiFieldQueryParser(TEST_VERSION_CURRENT, fields, a);
-    
-    Query q = mfqp.parse(qtxt);
-    assertEquals(expectedRes, q.toString());
-    
-    q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, qtxt, fields, occur, a);
-    assertEquals(expectedRes, q.toString());
-  }
-  
-  public void testSimple() throws Exception {
-    String[] fields = {"b", "t"};
-    MultiFieldQueryParser mfqp = new MultiFieldQueryParser(TEST_VERSION_CURRENT, fields, new MockAnalyzer(random));
-    
-    Query q = mfqp.parse("one");
-    assertEquals("b:one t:one", q.toString());
-    
-    q = mfqp.parse("one two");
-    assertEquals("(b:one t:one) (b:two t:two)", q.toString());
-    
-    q = mfqp.parse("+one +two");
-    assertEquals("+(b:one t:one) +(b:two t:two)", q.toString());
-
-    q = mfqp.parse("+one -two -three");
-    assertEquals("+(b:one t:one) -(b:two t:two) -(b:three t:three)", q.toString());
-    
-    q = mfqp.parse("one^2 two");
-    assertEquals("((b:one t:one)^2.0) (b:two t:two)", q.toString());
-
-    q = mfqp.parse("one~ two");
-    assertEquals("(b:one~2.0 t:one~2.0) (b:two t:two)", q.toString());
-
-    q = mfqp.parse("one~0.8 two^2");
-    assertEquals("(b:one~0.8 t:one~0.8) ((b:two t:two)^2.0)", q.toString());
-
-    q = mfqp.parse("one* two*");
-    assertEquals("(b:one* t:one*) (b:two* t:two*)", q.toString());
-
-    q = mfqp.parse("[a TO c] two");
-    assertEquals("(b:[a TO c] t:[a TO c]) (b:two t:two)", q.toString());
-
-    q = mfqp.parse("w?ldcard");
-    assertEquals("b:w?ldcard t:w?ldcard", q.toString());
-
-    q = mfqp.parse("\"foo bar\"");
-    assertEquals("b:\"foo bar\" t:\"foo bar\"", q.toString());
-
-    q = mfqp.parse("\"aa bb cc\" \"dd ee\"");
-    assertEquals("(b:\"aa bb cc\" t:\"aa bb cc\") (b:\"dd ee\" t:\"dd ee\")", q.toString());
-
-    q = mfqp.parse("\"foo bar\"~4");
-    assertEquals("b:\"foo bar\"~4 t:\"foo bar\"~4", q.toString());
-
-    // LUCENE-1213: MultiFieldQueryParser was ignoring slop when phrase had a field.
-    q = mfqp.parse("b:\"foo bar\"~4"); 
-    assertEquals("b:\"foo bar\"~4", q.toString());
-
-    // make sure that terms which have a field are not touched:
-    q = mfqp.parse("one f:two");
-    assertEquals("(b:one t:one) f:two", q.toString());
-
-    // AND mode:
-    mfqp.setDefaultOperator(QueryParser.AND_OPERATOR);
-    q = mfqp.parse("one two");
-    assertEquals("+(b:one t:one) +(b:two t:two)", q.toString());
-    q = mfqp.parse("\"aa bb cc\" \"dd ee\"");
-    assertEquals("+(b:\"aa bb cc\" t:\"aa bb cc\") +(b:\"dd ee\" t:\"dd ee\")", q.toString());
-
-  }
-  
-  public void testBoostsSimple() throws Exception {
-      Map<String,Float> boosts = new HashMap<String,Float>();
-      boosts.put("b", Float.valueOf(5));
-      boosts.put("t", Float.valueOf(10));
-      String[] fields = {"b", "t"};
-      MultiFieldQueryParser mfqp = new MultiFieldQueryParser(TEST_VERSION_CURRENT, fields, new MockAnalyzer(random), boosts);
-      
-      
-      //Check for simple
-      Query q = mfqp.parse("one");
-      assertEquals("b:one^5.0 t:one^10.0", q.toString());
-      
-      //Check for AND
-      q = mfqp.parse("one AND two");
-      assertEquals("+(b:one^5.0 t:one^10.0) +(b:two^5.0 t:two^10.0)", q.toString());
-      
-      //Check for OR
-      q = mfqp.parse("one OR two");
-      assertEquals("(b:one^5.0 t:one^10.0) (b:two^5.0 t:two^10.0)", q.toString());
-      
-      //Check for AND and a field
-      q = mfqp.parse("one AND two AND foo:test");
-      assertEquals("+(b:one^5.0 t:one^10.0) +(b:two^5.0 t:two^10.0) +foo:test", q.toString());
-      
-      q = mfqp.parse("one^3 AND two^4");
-      assertEquals("+((b:one^5.0 t:one^10.0)^3.0) +((b:two^5.0 t:two^10.0)^4.0)", q.toString());
-  }
-
-  public void testStaticMethod1() throws ParseException {
-    String[] fields = {"b", "t"};
-    String[] queries = {"one", "two"};
-    Query q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, queries, fields, new MockAnalyzer(random));
-    assertEquals("b:one t:two", q.toString());
-
-    String[] queries2 = {"+one", "+two"};
-    q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, queries2, fields, new MockAnalyzer(random));
-    assertEquals("(+b:one) (+t:two)", q.toString());
-
-    String[] queries3 = {"one", "+two"};
-    q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, queries3, fields, new MockAnalyzer(random));
-    assertEquals("b:one (+t:two)", q.toString());
-
-    String[] queries4 = {"one +more", "+two"};
-    q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, queries4, fields, new MockAnalyzer(random));
-    assertEquals("(b:one +b:more) (+t:two)", q.toString());
-
-    String[] queries5 = {"blah"};
-    try {
-      q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, queries5, fields, new MockAnalyzer(random));
-      fail();
-    } catch(IllegalArgumentException e) {
-      // expected exception, array length differs
-    }
-    
-    // check also with stop words for this static form (qtxts[], fields[]).
-    TestQueryParser.QPTestAnalyzer stopA = new TestQueryParser.QPTestAnalyzer();
-    
-    String[] queries6 = {"((+stop))", "+((stop))"};
-    q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, queries6, fields, stopA);
-    assertEquals("", q.toString());
-    
-    String[] queries7 = {"one ((+stop)) +more", "+((stop)) +two"};
-    q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, queries7, fields, stopA);
-    assertEquals("(b:one +b:more) (+t:two)", q.toString());
-
-  }
-
-  public void testStaticMethod2() throws ParseException {
-    String[] fields = {"b", "t"};
-    BooleanClause.Occur[] flags = {BooleanClause.Occur.MUST, BooleanClause.Occur.MUST_NOT};
-    Query q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, "one", fields, flags, new MockAnalyzer(random));
-    assertEquals("+b:one -t:one", q.toString());
-
-    q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, "one two", fields, flags, new MockAnalyzer(random));
-    assertEquals("+(b:one b:two) -(t:one t:two)", q.toString());
-
-    try {
-      BooleanClause.Occur[] flags2 = {BooleanClause.Occur.MUST};
-      q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, "blah", fields, flags2, new MockAnalyzer(random));
-      fail();
-    } catch(IllegalArgumentException e) {
-      // expected exception, array length differs
-    }
-  }
-
-  public void testStaticMethod2Old() throws ParseException {
-    String[] fields = {"b", "t"};
-    //int[] flags = {MultiFieldQueryParser.REQUIRED_FIELD, MultiFieldQueryParser.PROHIBITED_FIELD};
-      BooleanClause.Occur[] flags = {BooleanClause.Occur.MUST, BooleanClause.Occur.MUST_NOT};
-
-    Query q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, "one", fields, flags, new MockAnalyzer(random));//, fields, flags, new MockAnalyzer(random));
-    assertEquals("+b:one -t:one", q.toString());
-
-    q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, "one two", fields, flags, new MockAnalyzer(random));
-    assertEquals("+(b:one b:two) -(t:one t:two)", q.toString());
-
-    try {
-      BooleanClause.Occur[] flags2 = {BooleanClause.Occur.MUST};
-      q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, "blah", fields, flags2, new MockAnalyzer(random));
-      fail();
-    } catch(IllegalArgumentException e) {
-      // expected exception, array length differs
-    }
-  }
-
-  public void testStaticMethod3() throws ParseException {
-    String[] queries = {"one", "two", "three"};
-    String[] fields = {"f1", "f2", "f3"};
-    BooleanClause.Occur[] flags = {BooleanClause.Occur.MUST,
-        BooleanClause.Occur.MUST_NOT, BooleanClause.Occur.SHOULD};
-    Query q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, queries, fields, flags, new MockAnalyzer(random));
-    assertEquals("+f1:one -f2:two f3:three", q.toString());
-
-    try {
-      BooleanClause.Occur[] flags2 = {BooleanClause.Occur.MUST};
-      q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, queries, fields, flags2, new MockAnalyzer(random));
-      fail();
-    } catch(IllegalArgumentException e) {
-      // expected exception, array length differs
-    }
-  }
-
-  public void testStaticMethod3Old() throws ParseException {
-    String[] queries = {"one", "two"};
-    String[] fields = {"b", "t"};
-      BooleanClause.Occur[] flags = {BooleanClause.Occur.MUST, BooleanClause.Occur.MUST_NOT};
-    Query q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, queries, fields, flags, new MockAnalyzer(random));
-    assertEquals("+b:one -t:two", q.toString());
-
-    try {
-      BooleanClause.Occur[] flags2 = {BooleanClause.Occur.MUST};
-      q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, queries, fields, flags2, new MockAnalyzer(random));
-      fail();
-    } catch(IllegalArgumentException e) {
-      // expected exception, array length differs
-    }
-  }
-
-  public void testAnalyzerReturningNull() throws ParseException {
-    String[] fields = new String[] { "f1", "f2", "f3" };
-    MultiFieldQueryParser parser = new MultiFieldQueryParser(TEST_VERSION_CURRENT, fields, new AnalyzerReturningNull());
-    Query q = parser.parse("bla AND blo");
-    assertEquals("+(f2:bla f3:bla) +(f2:blo f3:blo)", q.toString());
-    // the following queries are not affected as their terms are not analyzed anyway:
-    q = parser.parse("bla*");
-    assertEquals("f1:bla* f2:bla* f3:bla*", q.toString());
-    q = parser.parse("bla~");
-    assertEquals("f1:bla~2.0 f2:bla~2.0 f3:bla~2.0", q.toString());
-    q = parser.parse("[a TO c]");
-    assertEquals("f1:[a TO c] f2:[a TO c] f3:[a TO c]", q.toString());
-  }
-
-  public void testStopWordSearching() throws Exception {
-    Analyzer analyzer = new MockAnalyzer(random);
-    Directory ramDir = newDirectory();
-    IndexWriter iw =  new IndexWriter(ramDir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));
-    Document doc = new Document();
-    doc.add(newField("body", "blah the footest blah", Field.Store.NO, Field.Index.ANALYZED));
-    iw.addDocument(doc);
-    iw.close();
-    
-    MultiFieldQueryParser mfqp = 
-      new MultiFieldQueryParser(TEST_VERSION_CURRENT, new String[] {"body"}, analyzer);
-    mfqp.setDefaultOperator(QueryParser.Operator.AND);
-    Query q = mfqp.parse("the footest");
-    IndexSearcher is = new IndexSearcher(ramDir, true);
-    ScoreDoc[] hits = is.search(q, null, 1000).scoreDocs;
-    assertEquals(1, hits.length);
-    is.close();
-    ramDir.close();
-  }
-  
-  /**
-   * Return empty tokens for field "f1".
-   */
-  private static class AnalyzerReturningNull extends Analyzer {
-    MockAnalyzer stdAnalyzer = new MockAnalyzer(random);
-
-    public AnalyzerReturningNull() {
-    }
-
-    @Override
-    public TokenStream tokenStream(String fieldName, Reader reader) {
-      if ("f1".equals(fieldName)) {
-        return new EmptyTokenStream();
-      } else {
-        return stdAnalyzer.tokenStream(fieldName, reader);
-      }
-    }
-
-    private static class EmptyTokenStream extends TokenStream {
-      @Override
-      public boolean incrementToken() throws IOException {
-        return false;
-      }
-    }
-  }
-
-}
diff --git a/lucene/src/test/org/apache/lucene/queryParser/TestMultiPhraseQueryParsing.java b/lucene/src/test/org/apache/lucene/queryParser/TestMultiPhraseQueryParsing.java
deleted file mode 100644
index 12bdbab..0000000
--- a/lucene/src/test/org/apache/lucene/queryParser/TestMultiPhraseQueryParsing.java
+++ /dev/null
@@ -1,105 +0,0 @@
-package org.apache.lucene.queryParser;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.MultiPhraseQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.util.LuceneTestCase;
-
-import java.io.IOException;
-import java.io.Reader;
-
-public class TestMultiPhraseQueryParsing extends LuceneTestCase {
-
-  private static class TokenAndPos {
-      public final String token;
-      public final int pos;
-      public TokenAndPos(String token, int pos) {
-        this.token = token;
-        this.pos = pos;
-      }
-    }
-
-  private static class CannedAnalyzer extends Analyzer {
-    private final TokenAndPos[] tokens;
-
-    public CannedAnalyzer(TokenAndPos[] tokens) {
-      this.tokens = tokens;
-    }
-
-    @Override
-    public TokenStream tokenStream(String fieldName, Reader reader) {
-      return new CannedTokenizer(tokens);
-    }
-  }
-
-  private static class CannedTokenizer extends Tokenizer {
-    private final TokenAndPos[] tokens;
-    private int upto = 0;
-    private int lastPos = 0;
-    private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-    private final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);
-
-    public CannedTokenizer(TokenAndPos[] tokens) {
-      this.tokens = tokens;
-    }
-
-    @Override
-    public final boolean incrementToken() throws IOException {
-      clearAttributes();
-      if (upto < tokens.length) {
-        final TokenAndPos token = tokens[upto++];
-        termAtt.setEmpty();
-        termAtt.append(token.token);
-        posIncrAtt.setPositionIncrement(token.pos - lastPos);
-        lastPos = token.pos;
-        return true;
-      } else {
-        return false;
-      }
-    }
-  }
-
-  public void testMultiPhraseQueryParsing() throws Exception {
-    TokenAndPos[] INCR_0_QUERY_TOKENS_AND = new TokenAndPos[]{
-        new TokenAndPos("a", 0),
-        new TokenAndPos("1", 0),
-        new TokenAndPos("b", 1),
-        new TokenAndPos("1", 1),
-        new TokenAndPos("c", 2)
-    };
-
-    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", new CannedAnalyzer(INCR_0_QUERY_TOKENS_AND));
-    Query q = qp.parse("\"this text is acually ignored\"");
-    assertTrue("wrong query type!", q instanceof MultiPhraseQuery);
-
-    MultiPhraseQuery multiPhraseQuery = new MultiPhraseQuery();
-    multiPhraseQuery.add(new Term[]{ new Term("field", "a"), new Term("field", "1") }, -1);
-    multiPhraseQuery.add(new Term[]{ new Term("field", "b"), new Term("field", "1") }, 0);
-    multiPhraseQuery.add(new Term[]{ new Term("field", "c") }, 1);
-
-    assertEquals(multiPhraseQuery, q);
-  }
-
-}
diff --git a/lucene/src/test/org/apache/lucene/queryParser/TestQueryParser.java b/lucene/src/test/org/apache/lucene/queryParser/TestQueryParser.java
deleted file mode 100644
index 715177c..0000000
--- a/lucene/src/test/org/apache/lucene/queryParser/TestQueryParser.java
+++ /dev/null
@@ -1,1333 +0,0 @@
-package org.apache.lucene.queryParser;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.Reader;
-import java.text.DateFormat;
-import java.util.Calendar;
-import java.util.Date;
-import java.util.GregorianCalendar;
-import java.util.Locale;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.analysis.MockTokenFilter;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.analysis.TokenFilter;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.document.DateTools;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.search.*;
-import org.apache.lucene.search.BooleanClause.Occur;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.automaton.BasicAutomata;
-import org.apache.lucene.util.automaton.CharacterRunAutomaton;
-import org.apache.lucene.util.automaton.RegExp;
-
-/**
- * Tests QueryParser.
- */
-public class TestQueryParser extends LuceneTestCase {
-  
-  public static Analyzer qpAnalyzer = new QPTestAnalyzer();
-
-  public static final class QPTestFilter extends TokenFilter {
-    CharTermAttribute termAtt;
-    OffsetAttribute offsetAtt;
-        
-    /**
-     * Filter which discards the token 'stop' and which expands the
-     * token 'phrase' into 'phrase1 phrase2'
-     */
-    public QPTestFilter(TokenStream in) {
-      super(in);
-      termAtt = addAttribute(CharTermAttribute.class);
-      offsetAtt = addAttribute(OffsetAttribute.class);
-    }
-
-    boolean inPhrase = false;
-    int savedStart = 0, savedEnd = 0;
-
-    @Override
-    public boolean incrementToken() throws IOException {
-      if (inPhrase) {
-        inPhrase = false;
-        clearAttributes();
-        termAtt.append("phrase2");
-        offsetAtt.setOffset(savedStart, savedEnd);
-        return true;
-      } else
-        while (input.incrementToken()) {
-          if (termAtt.toString().equals("phrase")) {
-            inPhrase = true;
-            savedStart = offsetAtt.startOffset();
-            savedEnd = offsetAtt.endOffset();
-            termAtt.setEmpty().append("phrase1");
-            offsetAtt.setOffset(savedStart, savedEnd);
-            return true;
-          } else if (!termAtt.toString().equals("stop"))
-            return true;
-        }
-      return false;
-    }
-  }
-
-  
-  public static final class QPTestAnalyzer extends Analyzer {
-
-    /** Filters MockTokenizer with StopFilter. */
-    @Override
-    public final TokenStream tokenStream(String fieldName, Reader reader) {
-      return new QPTestFilter(new MockTokenizer(reader, MockTokenizer.SIMPLE, true));
-    }
-  }
-
-  public static class QPTestParser extends QueryParser {
-    public QPTestParser(String f, Analyzer a) {
-      super(TEST_VERSION_CURRENT, f, a);
-    }
-
-    @Override
-    protected Query getFuzzyQuery(String field, String termStr, float minSimilarity) throws ParseException {
-      throw new ParseException("Fuzzy queries not allowed");
-    }
-
-    @Override
-    protected Query getWildcardQuery(String field, String termStr) throws ParseException {
-      throw new ParseException("Wildcard queries not allowed");
-    }
-  }
-
-  private int originalMaxClauses;
-
-  @Override
-  public void setUp() throws Exception {
-    super.setUp();
-    originalMaxClauses = BooleanQuery.getMaxClauseCount();
-  }
-
-  public QueryParser getParser(Analyzer a) throws Exception {
-    if (a == null)
-      a = new MockAnalyzer(random, MockTokenizer.SIMPLE, true);
-    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", a);
-    qp.setDefaultOperator(QueryParser.OR_OPERATOR);
-    return qp;
-  }
-
-  public Query getQuery(String query, Analyzer a) throws Exception {
-    return getParser(a).parse(query);
-  }
-
-  public void assertQueryEquals(String query, Analyzer a, String result)
-    throws Exception {
-    Query q = getQuery(query, a);
-    String s = q.toString("field");
-    if (!s.equals(result)) {
-      fail("Query /" + query + "/ yielded /" + s
-           + "/, expecting /" + result + "/");
-    }
-  }
-
-  public void assertQueryEquals(QueryParser qp, String field, String query, String result) 
-    throws Exception {
-    Query q = qp.parse(query);
-    String s = q.toString(field);
-    if (!s.equals(result)) {
-      fail("Query /" + query + "/ yielded /" + s
-           + "/, expecting /" + result + "/");
-    }
-  }
-  
-  public void assertEscapedQueryEquals(String query, Analyzer a, String result)
-    throws Exception {
-    String escapedQuery = QueryParser.escape(query);
-    if (!escapedQuery.equals(result)) {
-      fail("Query /" + query + "/ yielded /" + escapedQuery
-          + "/, expecting /" + result + "/");
-    }
-  }
-
-  public void assertWildcardQueryEquals(String query, boolean lowercase, String result, boolean allowLeadingWildcard)
-    throws Exception {
-    QueryParser qp = getParser(null);
-    qp.setLowercaseExpandedTerms(lowercase);
-    qp.setAllowLeadingWildcard(allowLeadingWildcard);
-    Query q = qp.parse(query);
-    String s = q.toString("field");
-    if (!s.equals(result)) {
-      fail("WildcardQuery /" + query + "/ yielded /" + s
-           + "/, expecting /" + result + "/");
-    }
-  }
-
-  public void assertWildcardQueryEquals(String query, boolean lowercase, String result)
-    throws Exception {
-    assertWildcardQueryEquals(query, lowercase, result, false);
-  }
-
-  public void assertWildcardQueryEquals(String query, String result) throws Exception {
-    QueryParser qp = getParser(null);
-    Query q = qp.parse(query);
-    String s = q.toString("field");
-    if (!s.equals(result)) {
-      fail("WildcardQuery /" + query + "/ yielded /" + s + "/, expecting /"
-          + result + "/");
-    }
-  }
-
-  public Query getQueryDOA(String query, Analyzer a)
-    throws Exception {
-    if (a == null)
-      a = new MockAnalyzer(random, MockTokenizer.SIMPLE, true);
-    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", a);
-    qp.setDefaultOperator(QueryParser.AND_OPERATOR);
-    return qp.parse(query);
-  }
-
-  public void assertQueryEqualsDOA(String query, Analyzer a, String result)
-    throws Exception {
-    Query q = getQueryDOA(query, a);
-    String s = q.toString("field");
-    if (!s.equals(result)) {
-      fail("Query /" + query + "/ yielded /" + s
-           + "/, expecting /" + result + "/");
-    }
-  }
-
-  public void testCJK() throws Exception {
-	 // Test Ideographic Space - As wide as a CJK character cell (fullwidth)
-	 // used google to translate the word "term" to japanese -> ??
-	 assertQueryEquals("term\u3000term\u3000term", null, "term\u0020term\u0020term");
-	 assertQueryEquals("??\u3000??\u3000??", null, "??\u0020??\u0020??");
-  }
-
-  //individual CJK chars as terms, like StandardAnalyzer
-  private class SimpleCJKTokenizer extends Tokenizer {
-    private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-
-    public SimpleCJKTokenizer(Reader input) {
-      super(input);
-    }
-
-    @Override
-    public boolean incrementToken() throws IOException {
-      int ch = input.read();
-      if (ch < 0)
-        return false;
-      clearAttributes();
-      termAtt.setEmpty().append((char) ch);
-      return true;
-    }
-  }
-
-  private class SimpleCJKAnalyzer extends Analyzer {
-    @Override
-    public TokenStream tokenStream(String fieldName, Reader reader) {
-      return new SimpleCJKTokenizer(reader);
-    }
-  }
-
-  public void testCJKTerm() throws Exception {
-    // individual CJK chars as terms
-    SimpleCJKAnalyzer analyzer = new SimpleCJKAnalyzer(); 
-    
-    BooleanQuery expected = new BooleanQuery();
-    expected.add(new TermQuery(new Term("field", "?")), BooleanClause.Occur.SHOULD);
-    expected.add(new TermQuery(new Term("field", "??")), BooleanClause.Occur.SHOULD);
-    
-    assertEquals(expected, getQuery("??", analyzer));
-  }
-  
-  public void testCJKBoostedTerm() throws Exception {
-    // individual CJK chars as terms
-    SimpleCJKAnalyzer analyzer = new SimpleCJKAnalyzer();
-    
-    BooleanQuery expected = new BooleanQuery();
-    expected.setBoost(0.5f);
-    expected.add(new TermQuery(new Term("field", "?")), BooleanClause.Occur.SHOULD);
-    expected.add(new TermQuery(new Term("field", "??")), BooleanClause.Occur.SHOULD);
-    
-    assertEquals(expected, getQuery("??^0.5", analyzer));
-  }
-  
-  public void testCJKPhrase() throws Exception {
-    // individual CJK chars as terms
-    SimpleCJKAnalyzer analyzer = new SimpleCJKAnalyzer();
-    
-    PhraseQuery expected = new PhraseQuery();
-    expected.add(new Term("field", "?"));
-    expected.add(new Term("field", "??"));
-    
-    assertEquals(expected, getQuery("\"??\"", analyzer));
-  }
-  
-  public void testCJKBoostedPhrase() throws Exception {
-    // individual CJK chars as terms
-    SimpleCJKAnalyzer analyzer = new SimpleCJKAnalyzer();
-    
-    PhraseQuery expected = new PhraseQuery();
-    expected.setBoost(0.5f);
-    expected.add(new Term("field", "?"));
-    expected.add(new Term("field", "??"));
-    
-    assertEquals(expected, getQuery("\"??\"^0.5", analyzer));
-  }
-  
-  public void testCJKSloppyPhrase() throws Exception {
-    // individual CJK chars as terms
-    SimpleCJKAnalyzer analyzer = new SimpleCJKAnalyzer();
-    
-    PhraseQuery expected = new PhraseQuery();
-    expected.setSlop(3);
-    expected.add(new Term("field", "?"));
-    expected.add(new Term("field", "??"));
-    
-    assertEquals(expected, getQuery("\"??\"~3", analyzer));
-  }
-  
-  public void testAutoGeneratePhraseQueriesOn() throws Exception {
-    // individual CJK chars as terms
-    SimpleCJKAnalyzer analyzer = new SimpleCJKAnalyzer(); 
-  
-    PhraseQuery expected = new PhraseQuery();
-    expected.add(new Term("field", "?"));
-    expected.add(new Term("field", "??"));
-    QueryParser parser = new QueryParser(TEST_VERSION_CURRENT, "field", analyzer);
-    parser.setAutoGeneratePhraseQueries(true);
-    assertEquals(expected, parser.parse("??"));
-  }
-
-  public void testSimple() throws Exception {
-    assertQueryEquals("term term term", null, "term term term");
-    assertQueryEquals("trm term term", new MockAnalyzer(random), "trm term term");
-    assertQueryEquals("mlaut", new MockAnalyzer(random), "mlaut");
-
-    // FIXME: enhance MockAnalyzer to be able to support this
-    // it must no longer extend CharTokenizer
-    //assertQueryEquals("\"\"", new KeywordAnalyzer(), "");
-    //assertQueryEquals("foo:\"\"", new KeywordAnalyzer(), "foo:");
-
-    assertQueryEquals("a AND b", null, "+a +b");
-    assertQueryEquals("(a AND b)", null, "+a +b");
-    assertQueryEquals("c OR (a AND b)", null, "c (+a +b)");
-    assertQueryEquals("a AND NOT b", null, "+a -b");
-    assertQueryEquals("a AND -b", null, "+a -b");
-    assertQueryEquals("a AND !b", null, "+a -b");
-    assertQueryEquals("a && b", null, "+a +b");
-//    assertQueryEquals("a && ! b", null, "+a -b");
-
-    assertQueryEquals("a OR b", null, "a b");
-    assertQueryEquals("a || b", null, "a b");
-    assertQueryEquals("a OR !b", null, "a -b");
-//    assertQueryEquals("a OR ! b", null, "a -b");
-    assertQueryEquals("a OR -b", null, "a -b");
-
-    // +,-,! should be directly adjacent to operand (i.e. not separated by whitespace) to be treated as an operator
-    Analyzer a = new Analyzer() {
-      @Override
-      public TokenStream tokenStream(String fieldName, Reader reader) {
-        return new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);
-      }
-    };
-    assertQueryEquals("a - b", a, "a - b");
-    assertQueryEquals("a + b", a, "a + b");
-    assertQueryEquals("a ! b", a, "a ! b");
-
-    assertQueryEquals("+term -term term", null, "+term -term term");
-    assertQueryEquals("foo:term AND field:anotherTerm", null,
-                      "+foo:term +anotherterm");
-    assertQueryEquals("term AND \"phrase phrase\"", null,
-                      "+term +\"phrase phrase\"");
-    assertQueryEquals("\"hello there\"", null, "\"hello there\"");
-    assertTrue(getQuery("a AND b", null) instanceof BooleanQuery);
-    assertTrue(getQuery("hello", null) instanceof TermQuery);
-    assertTrue(getQuery("\"hello there\"", null) instanceof PhraseQuery);
-
-    assertQueryEquals("germ term^2.0", null, "germ term^2.0");
-    assertQueryEquals("(term)^2.0", null, "term^2.0");
-    assertQueryEquals("(germ term)^2.0", null, "(germ term)^2.0");
-    assertQueryEquals("term^2.0", null, "term^2.0");
-    assertQueryEquals("term^2", null, "term^2.0");
-    assertQueryEquals("\"germ term\"^2.0", null, "\"germ term\"^2.0");
-    assertQueryEquals("\"term germ\"^2", null, "\"term germ\"^2.0");
-
-    assertQueryEquals("(foo OR bar) AND (baz OR boo)", null,
-                      "+(foo bar) +(baz boo)");
-    assertQueryEquals("((a OR b) AND NOT c) OR d", null,
-                      "(+(a b) -c) d");
-    assertQueryEquals("+(apple \"steve jobs\") -(foo bar baz)", null,
-                      "+(apple \"steve jobs\") -(foo bar baz)");
-    assertQueryEquals("+title:(dog OR cat) -author:\"bob dole\"", null,
-                      "+(title:dog title:cat) -author:\"bob dole\"");
-    
-    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", new MockAnalyzer(random));
-    // make sure OR is the default:
-    assertEquals(QueryParser.OR_OPERATOR, qp.getDefaultOperator());
-    qp.setDefaultOperator(QueryParser.AND_OPERATOR);
-    assertEquals(QueryParser.AND_OPERATOR, qp.getDefaultOperator());
-    qp.setDefaultOperator(QueryParser.OR_OPERATOR);
-    assertEquals(QueryParser.OR_OPERATOR, qp.getDefaultOperator());
-  }
-
-  public void testPunct() throws Exception {
-    Analyzer a = new MockAnalyzer(random, MockTokenizer.WHITESPACE, false);
-    assertQueryEquals("a&b", a, "a&b");
-    assertQueryEquals("a&&b", a, "a&&b");
-    assertQueryEquals(".NET", a, ".NET");
-  }
-
-  public void testSlop() throws Exception {
-    assertQueryEquals("\"term germ\"~2", null, "\"term germ\"~2");
-    assertQueryEquals("\"term germ\"~2 flork", null, "\"term germ\"~2 flork");
-    assertQueryEquals("\"term\"~2", null, "term");
-    assertQueryEquals("\" \"~2 germ", null, "germ");
-    assertQueryEquals("\"term germ\"~2^2", null, "\"term germ\"~2^2.0");
-  }
-
-  public void testNumber() throws Exception {
-// The numbers go away because SimpleAnalzyer ignores them
-    assertQueryEquals("3", null, "");
-    assertQueryEquals("term 1.0 1 2", null, "term");
-    assertQueryEquals("term term1 term2", null, "term term term");
-
-    Analyzer a = new MockAnalyzer(random, MockTokenizer.WHITESPACE, true);
-    assertQueryEquals("3", a, "3");
-    assertQueryEquals("term 1.0 1 2", a, "term 1.0 1 2");
-    assertQueryEquals("term term1 term2", a, "term term1 term2");
-  }
-
-  public void testWildcard() throws Exception {
-    assertQueryEquals("term*", null, "term*");
-    assertQueryEquals("term*^2", null, "term*^2.0");
-    assertQueryEquals("term~", null, "term~2.0");
-    assertQueryEquals("term~0.7", null, "term~0.7");
-    assertQueryEquals("term~^3", null, "term~2.0^3.0");
-    assertQueryEquals("term^3~", null, "term~2.0^3.0");
-    assertQueryEquals("term*germ", null, "term*germ");
-    assertQueryEquals("term*germ^3", null, "term*germ^3.0");
-
-    assertTrue(getQuery("term*", null) instanceof PrefixQuery);
-    assertTrue(getQuery("term*^2", null) instanceof PrefixQuery);
-    assertTrue(getQuery("term~", null) instanceof FuzzyQuery);
-    assertTrue(getQuery("term~0.7", null) instanceof FuzzyQuery);
-    FuzzyQuery fq = (FuzzyQuery)getQuery("term~0.7", null);
-    assertEquals(0.7f, fq.getMinSimilarity(), 0.1f);
-    assertEquals(FuzzyQuery.defaultPrefixLength, fq.getPrefixLength());
-    fq = (FuzzyQuery)getQuery("term~", null);
-    assertEquals(2.0f, fq.getMinSimilarity(), 0.1f);
-    assertEquals(FuzzyQuery.defaultPrefixLength, fq.getPrefixLength());
-    
-    assertParseException("term~1.1"); // value > 1, throws exception
-
-    assertTrue(getQuery("term*germ", null) instanceof WildcardQuery);
-
-/* Tests to see that wild card terms are (or are not) properly
-	 * lower-cased with propery parser configuration
-	 */
-// First prefix queries:
-    // by default, convert to lowercase:
-    assertWildcardQueryEquals("Term*", true, "term*");
-    // explicitly set lowercase:
-    assertWildcardQueryEquals("term*", true, "term*");
-    assertWildcardQueryEquals("Term*", true, "term*");
-    assertWildcardQueryEquals("TERM*", true, "term*");
-    // explicitly disable lowercase conversion:
-    assertWildcardQueryEquals("term*", false, "term*");
-    assertWildcardQueryEquals("Term*", false, "Term*");
-    assertWildcardQueryEquals("TERM*", false, "TERM*");
-// Then 'full' wildcard queries:
-    // by default, convert to lowercase:
-    assertWildcardQueryEquals("Te?m", "te?m");
-    // explicitly set lowercase:
-    assertWildcardQueryEquals("te?m", true, "te?m");
-    assertWildcardQueryEquals("Te?m", true, "te?m");
-    assertWildcardQueryEquals("TE?M", true, "te?m");
-    assertWildcardQueryEquals("Te?m*gerM", true, "te?m*germ");
-    // explicitly disable lowercase conversion:
-    assertWildcardQueryEquals("te?m", false, "te?m");
-    assertWildcardQueryEquals("Te?m", false, "Te?m");
-    assertWildcardQueryEquals("TE?M", false, "TE?M");
-    assertWildcardQueryEquals("Te?m*gerM", false, "Te?m*gerM");
-//  Fuzzy queries:
-    assertWildcardQueryEquals("Term~", "term~2.0");
-    assertWildcardQueryEquals("Term~", true, "term~2.0");
-    assertWildcardQueryEquals("Term~", false, "Term~2.0");
-//  Range queries:
-    assertWildcardQueryEquals("[A TO C]", "[a TO c]");
-    assertWildcardQueryEquals("[A TO C]", true, "[a TO c]");
-    assertWildcardQueryEquals("[A TO C]", false, "[A TO C]");
-    // Test suffix queries: first disallow
-    try {
-      assertWildcardQueryEquals("*Term", true, "*term");
-      fail();
-    } catch(ParseException pe) {
-      // expected exception
-    }
-    try {
-      assertWildcardQueryEquals("?Term", true, "?term");
-      fail();
-    } catch(ParseException pe) {
-      // expected exception
-    }
-    // Test suffix queries: then allow
-    assertWildcardQueryEquals("*Term", true, "*term", true);
-    assertWildcardQueryEquals("?Term", true, "?term", true);
-  }
-  
-  public void testLeadingWildcardType() throws Exception {
-    QueryParser qp = getParser(null);
-    qp.setAllowLeadingWildcard(true);
-    assertEquals(WildcardQuery.class, qp.parse("t*erm*").getClass());
-    assertEquals(WildcardQuery.class, qp.parse("?term*").getClass());
-    assertEquals(WildcardQuery.class, qp.parse("*term*").getClass());
-  }
-
-  public void testQPA() throws Exception {
-    assertQueryEquals("term term^3.0 term", qpAnalyzer, "term term^3.0 term");
-    assertQueryEquals("term stop^3.0 term", qpAnalyzer, "term term");
-    
-    assertQueryEquals("term term term", qpAnalyzer, "term term term");
-    assertQueryEquals("term +stop term", qpAnalyzer, "term term");
-    assertQueryEquals("term -stop term", qpAnalyzer, "term term");
-
-    assertQueryEquals("drop AND (stop) AND roll", qpAnalyzer, "+drop +roll");
-    assertQueryEquals("term +(stop) term", qpAnalyzer, "term term");
-    assertQueryEquals("term -(stop) term", qpAnalyzer, "term term");
-    
-    assertQueryEquals("drop AND stop AND roll", qpAnalyzer, "+drop +roll");
-    assertQueryEquals("term phrase term", qpAnalyzer,
-                      "term (phrase1 phrase2) term");
-    assertQueryEquals("term AND NOT phrase term", qpAnalyzer,
-                      "+term -(phrase1 phrase2) term");
-    assertQueryEquals("stop^3", qpAnalyzer, "");
-    assertQueryEquals("stop", qpAnalyzer, "");
-    assertQueryEquals("(stop)^3", qpAnalyzer, "");
-    assertQueryEquals("((stop))^3", qpAnalyzer, "");
-    assertQueryEquals("(stop^3)", qpAnalyzer, "");
-    assertQueryEquals("((stop)^3)", qpAnalyzer, "");
-    assertQueryEquals("(stop)", qpAnalyzer, "");
-    assertQueryEquals("((stop))", qpAnalyzer, "");
-    assertTrue(getQuery("term term term", qpAnalyzer) instanceof BooleanQuery);
-    assertTrue(getQuery("term +stop", qpAnalyzer) instanceof TermQuery);
-  }
-
-  public void testRange() throws Exception {
-    assertQueryEquals("[ a TO z]", null, "[a TO z]");
-    assertQueryEquals("[ a TO z}", null, "[a TO z}");
-    assertQueryEquals("{ a TO z]", null, "{a TO z]"); 
-
-     assertEquals(MultiTermQuery.CONSTANT_SCORE_AUTO_REWRITE_DEFAULT, ((TermRangeQuery)getQuery("[ a TO z]", null)).getRewriteMethod());
-
-    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", new MockAnalyzer(random, MockTokenizer.SIMPLE, true));
-    qp.setMultiTermRewriteMethod(MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE);
-    assertEquals(MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE,((TermRangeQuery)qp.parse("[ a TO z]")).getRewriteMethod());
-    
-    assertQueryEquals("[ a TO z ]", null, "[a TO z]");
-    assertQueryEquals("{ a TO z}", null, "{a TO z}");
-    assertQueryEquals("{ a TO z }", null, "{a TO z}");
-    assertQueryEquals("{ a TO z }^2.0", null, "{a TO z}^2.0");
-    assertQueryEquals("[ a TO z] OR bar", null, "[a TO z] bar");
-    assertQueryEquals("[ a TO z] AND bar", null, "+[a TO z] +bar");
-    assertQueryEquals("( bar blar { a TO z}) ", null, "bar blar {a TO z}");
-    assertQueryEquals("gack ( bar blar { a TO z}) ", null, "gack (bar blar {a TO z})");
-
-    assertQueryEquals("[* TO Z]",null,"[* TO z]");
-    assertQueryEquals("[A TO *]",null,"[a TO *]");
-    assertQueryEquals("[* TO *]",null,"[* TO *]");
-    assertQueryEquals("[\\* TO \"*\"]",null,"[\\* TO \\*]");
- }
-    
-  private String escapeDateString(String s) {
-    if (s.indexOf(" ") > -1) {
-      return "\"" + s + "\"";
-    } else {
-      return s;
-    }
-  }
-  
-  /** for testing DateTools support */
-  private String getDate(String s, DateTools.Resolution resolution) throws Exception {
-    DateFormat df = DateFormat.getDateInstance(DateFormat.SHORT);
-    return getDate(df.parse(s), resolution);      
-  }
-  
-  /** for testing DateTools support */
-  private String getDate(Date d, DateTools.Resolution resolution) throws Exception {
-     return DateTools.dateToString(d, resolution);
-  }
-  
-  private String getLocalizedDate(int year, int month, int day) {
-    DateFormat df = DateFormat.getDateInstance(DateFormat.SHORT);
-    Calendar calendar = new GregorianCalendar();
-    calendar.clear();
-    calendar.set(year, month, day);
-    calendar.set(Calendar.HOUR_OF_DAY, 23);
-    calendar.set(Calendar.MINUTE, 59);
-    calendar.set(Calendar.SECOND, 59);
-    calendar.set(Calendar.MILLISECOND, 999);
-    return df.format(calendar.getTime());
-  }
-
-  public void testDateRange() throws Exception {
-    String startDate = getLocalizedDate(2002, 1, 1);
-    String endDate = getLocalizedDate(2002, 1, 4);
-    Calendar endDateExpected = new GregorianCalendar();
-    endDateExpected.clear();
-    endDateExpected.set(2002, 1, 4, 23, 59, 59);
-    endDateExpected.set(Calendar.MILLISECOND, 999);
-    final String defaultField = "default";
-    final String monthField = "month";
-    final String hourField = "hour";
-    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", new MockAnalyzer(random, MockTokenizer.SIMPLE, true));
-    
-    // set a field specific date resolution
-    qp.setDateResolution(monthField, DateTools.Resolution.MONTH);
-    
-    // set default date resolution to MILLISECOND
-    qp.setDateResolution(DateTools.Resolution.MILLISECOND);
-    
-    // set second field specific date resolution    
-    qp.setDateResolution(hourField, DateTools.Resolution.HOUR);
-
-    // for this field no field specific date resolution has been set,
-    // so verify if the default resolution is used
-    assertDateRangeQueryEquals(qp, defaultField, startDate, endDate, 
-            endDateExpected.getTime(), DateTools.Resolution.MILLISECOND);
-
-    // verify if field specific date resolutions are used for these two fields
-    assertDateRangeQueryEquals(qp, monthField, startDate, endDate, 
-            endDateExpected.getTime(), DateTools.Resolution.MONTH);
-
-    assertDateRangeQueryEquals(qp, hourField, startDate, endDate, 
-            endDateExpected.getTime(), DateTools.Resolution.HOUR);  
-  }
-  
-  public void assertDateRangeQueryEquals(QueryParser qp, String field, String startDate, String endDate, 
-                                         Date endDateInclusive, DateTools.Resolution resolution) throws Exception {
-    assertQueryEquals(qp, field, field + ":[" + escapeDateString(startDate) + " TO " + escapeDateString(endDate) + "]",
-               "[" + getDate(startDate, resolution) + " TO " + getDate(endDateInclusive, resolution) + "]");
-    assertQueryEquals(qp, field, field + ":{" + escapeDateString(startDate) + " TO " + escapeDateString(endDate) + "}",
-               "{" + getDate(startDate, resolution) + " TO " + getDate(endDate, resolution) + "}");
-  }
-
-  public void testEscaped() throws Exception {
-    Analyzer a = new MockAnalyzer(random, MockTokenizer.WHITESPACE, false);
-    
-    /*assertQueryEquals("\\[brackets", a, "\\[brackets");
-    assertQueryEquals("\\[brackets", null, "brackets");
-    assertQueryEquals("\\\\", a, "\\\\");
-    assertQueryEquals("\\+blah", a, "\\+blah");
-    assertQueryEquals("\\(blah", a, "\\(blah");
-
-    assertQueryEquals("\\-blah", a, "\\-blah");
-    assertQueryEquals("\\!blah", a, "\\!blah");
-    assertQueryEquals("\\{blah", a, "\\{blah");
-    assertQueryEquals("\\}blah", a, "\\}blah");
-    assertQueryEquals("\\:blah", a, "\\:blah");
-    assertQueryEquals("\\^blah", a, "\\^blah");
-    assertQueryEquals("\\[blah", a, "\\[blah");
-    assertQueryEquals("\\]blah", a, "\\]blah");
-    assertQueryEquals("\\\"blah", a, "\\\"blah");
-    assertQueryEquals("\\(blah", a, "\\(blah");
-    assertQueryEquals("\\)blah", a, "\\)blah");
-    assertQueryEquals("\\~blah", a, "\\~blah");
-    assertQueryEquals("\\*blah", a, "\\*blah");
-    assertQueryEquals("\\?blah", a, "\\?blah");
-    //assertQueryEquals("foo \\&\\& bar", a, "foo \\&\\& bar");
-    //assertQueryEquals("foo \\|| bar", a, "foo \\|| bar");
-    //assertQueryEquals("foo \\AND bar", a, "foo \\AND bar");*/
-
-    assertQueryEquals("\\a", a, "a");
-    
-    assertQueryEquals("a\\-b:c", a, "a-b:c");
-    assertQueryEquals("a\\+b:c", a, "a+b:c");
-    assertQueryEquals("a\\:b:c", a, "a:b:c");
-    assertQueryEquals("a\\\\b:c", a, "a\\b:c");
-
-    assertQueryEquals("a:b\\-c", a, "a:b-c");
-    assertQueryEquals("a:b\\+c", a, "a:b+c");
-    assertQueryEquals("a:b\\:c", a, "a:b:c");
-    assertQueryEquals("a:b\\\\c", a, "a:b\\c");
-
-    assertQueryEquals("a:b\\-c*", a, "a:b-c*");
-    assertQueryEquals("a:b\\+c*", a, "a:b+c*");
-    assertQueryEquals("a:b\\:c*", a, "a:b:c*");
-
-    assertQueryEquals("a:b\\\\c*", a, "a:b\\c*");
-
-    assertQueryEquals("a:b\\-?c", a, "a:b\\-?c");
-    assertQueryEquals("a:b\\+?c", a, "a:b\\+?c");
-    assertQueryEquals("a:b\\:?c", a, "a:b\\:?c");
-
-    assertQueryEquals("a:b\\\\?c", a, "a:b\\\\?c");
-
-    assertQueryEquals("a:b\\-c~", a, "a:b-c~2.0");
-    assertQueryEquals("a:b\\+c~", a, "a:b+c~2.0");
-    assertQueryEquals("a:b\\:c~", a, "a:b:c~2.0");
-    assertQueryEquals("a:b\\\\c~", a, "a:b\\c~2.0");
-
-    assertQueryEquals("[ a\\- TO a\\+ ]", null, "[a- TO a+]");
-    assertQueryEquals("[ a\\: TO a\\~ ]", null, "[a: TO a~]");
-    assertQueryEquals("[ a\\\\ TO a\\* ]", null, "[a\\ TO a*]");
-
-    assertQueryEquals("[\"c\\:\\\\temp\\\\\\~foo0.txt\" TO \"c\\:\\\\temp\\\\\\~foo9.txt\"]", a, 
-                      "[c:\\temp\\~foo0.txt TO c:\\temp\\~foo9.txt]");
-    
-    assertQueryEquals("a\\\\\\+b", a, "a\\+b");
-    
-    assertQueryEquals("a \\\"b c\\\" d", a, "a \"b c\" d");
-    assertQueryEquals("\"a \\\"b c\\\" d\"", a, "\"a \"b c\" d\"");
-    assertQueryEquals("\"a \\+b c d\"", a, "\"a +b c d\"");
-    
-    assertQueryEquals("c\\:\\\\temp\\\\\\~foo.txt", a, "c:\\temp\\~foo.txt");
-    
-    assertParseException("XY\\"); // there must be a character after the escape char
-    
-    // test unicode escaping
-    assertQueryEquals("a\\u0062c", a, "abc");
-    assertQueryEquals("XY\\u005a", a, "XYZ");
-    assertQueryEquals("XY\\u005A", a, "XYZ");
-    assertQueryEquals("\"a \\\\\\u0028\\u0062\\\" c\"", a, "\"a \\(b\" c\"");
-    
-    assertParseException("XY\\u005G");  // test non-hex character in escaped unicode sequence
-    assertParseException("XY\\u005");   // test incomplete escaped unicode sequence
-    
-    // Tests bug LUCENE-800
-    assertQueryEquals("(item:\\\\ item:ABCD\\\\)", a, "item:\\ item:ABCD\\");
-    assertParseException("(item:\\\\ item:ABCD\\\\))"); // unmatched closing paranthesis 
-    assertQueryEquals("\\*", a, "*");
-    assertQueryEquals("\\\\", a, "\\");  // escaped backslash
-    
-    assertParseException("\\"); // a backslash must always be escaped
-    
-    // LUCENE-1189
-    assertQueryEquals("(\"a\\\\\") or (\"b\")", a ,"a\\ or b");
-  }
-
-  public void testQueryStringEscaping() throws Exception {
-    Analyzer a = new MockAnalyzer(random, MockTokenizer.WHITESPACE, false);
-
-    assertEscapedQueryEquals("a-b:c", a, "a\\-b\\:c");
-    assertEscapedQueryEquals("a+b:c", a, "a\\+b\\:c");
-    assertEscapedQueryEquals("a:b:c", a, "a\\:b\\:c");
-    assertEscapedQueryEquals("a\\b:c", a, "a\\\\b\\:c");
-
-    assertEscapedQueryEquals("a:b-c", a, "a\\:b\\-c");
-    assertEscapedQueryEquals("a:b+c", a, "a\\:b\\+c");
-    assertEscapedQueryEquals("a:b:c", a, "a\\:b\\:c");
-    assertEscapedQueryEquals("a:b\\c", a, "a\\:b\\\\c");
-
-    assertEscapedQueryEquals("a:b-c*", a, "a\\:b\\-c\\*");
-    assertEscapedQueryEquals("a:b+c*", a, "a\\:b\\+c\\*");
-    assertEscapedQueryEquals("a:b:c*", a, "a\\:b\\:c\\*");
-
-    assertEscapedQueryEquals("a:b\\\\c*", a, "a\\:b\\\\\\\\c\\*");
-
-    assertEscapedQueryEquals("a:b-?c", a, "a\\:b\\-\\?c");
-    assertEscapedQueryEquals("a:b+?c", a, "a\\:b\\+\\?c");
-    assertEscapedQueryEquals("a:b:?c", a, "a\\:b\\:\\?c");
-
-    assertEscapedQueryEquals("a:b?c", a, "a\\:b\\?c");
-
-    assertEscapedQueryEquals("a:b-c~", a, "a\\:b\\-c\\~");
-    assertEscapedQueryEquals("a:b+c~", a, "a\\:b\\+c\\~");
-    assertEscapedQueryEquals("a:b:c~", a, "a\\:b\\:c\\~");
-    assertEscapedQueryEquals("a:b\\c~", a, "a\\:b\\\\c\\~");
-
-    assertEscapedQueryEquals("[ a - TO a+ ]", null, "\\[ a \\- TO a\\+ \\]");
-    assertEscapedQueryEquals("[ a : TO a~ ]", null, "\\[ a \\: TO a\\~ \\]");
-    assertEscapedQueryEquals("[ a\\ TO a* ]", null, "\\[ a\\\\ TO a\\* \\]");
-    
-    // LUCENE-881
-    assertEscapedQueryEquals("|| abc ||", a, "\\|\\| abc \\|\\|");
-    assertEscapedQueryEquals("&& abc &&", a, "\\&\\& abc \\&\\&");
-  }
-  
-  public void testTabNewlineCarriageReturn()
-    throws Exception {
-    assertQueryEqualsDOA("+weltbank +worlbank", null,
-      "+weltbank +worlbank");
-
-    assertQueryEqualsDOA("+weltbank\n+worlbank", null,
-      "+weltbank +worlbank");
-    assertQueryEqualsDOA("weltbank \n+worlbank", null,
-      "+weltbank +worlbank");
-    assertQueryEqualsDOA("weltbank \n +worlbank", null,
-      "+weltbank +worlbank");
-
-    assertQueryEqualsDOA("+weltbank\r+worlbank", null,
-      "+weltbank +worlbank");
-    assertQueryEqualsDOA("weltbank \r+worlbank", null,
-      "+weltbank +worlbank");
-    assertQueryEqualsDOA("weltbank \r +worlbank", null,
-      "+weltbank +worlbank");
-
-    assertQueryEqualsDOA("+weltbank\r\n+worlbank", null,
-      "+weltbank +worlbank");
-    assertQueryEqualsDOA("weltbank \r\n+worlbank", null,
-      "+weltbank +worlbank");
-    assertQueryEqualsDOA("weltbank \r\n +worlbank", null,
-      "+weltbank +worlbank");
-    assertQueryEqualsDOA("weltbank \r \n +worlbank", null,
-      "+weltbank +worlbank");
-
-    assertQueryEqualsDOA("+weltbank\t+worlbank", null,
-      "+weltbank +worlbank");
-    assertQueryEqualsDOA("weltbank \t+worlbank", null,
-      "+weltbank +worlbank");
-    assertQueryEqualsDOA("weltbank \t +worlbank", null,
-      "+weltbank +worlbank");
-  }
-
-  public void testSimpleDAO()
-    throws Exception {
-    assertQueryEqualsDOA("term term term", null, "+term +term +term");
-    assertQueryEqualsDOA("term +term term", null, "+term +term +term");
-    assertQueryEqualsDOA("term term +term", null, "+term +term +term");
-    assertQueryEqualsDOA("term +term +term", null, "+term +term +term");
-    assertQueryEqualsDOA("-term term term", null, "-term +term +term");
-  }
-
-  public void testBoost()
-    throws Exception {
-    CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeString("on"));
-    Analyzer oneStopAnalyzer = new MockAnalyzer(random, MockTokenizer.SIMPLE, true, stopWords, true);
-    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", oneStopAnalyzer);
-    Query q = qp.parse("on^1.0");
-    assertNotNull(q);
-    q = qp.parse("\"hello\"^2.0");
-    assertNotNull(q);
-    assertEquals(q.getBoost(), (float) 2.0, (float) 0.5);
-    q = qp.parse("hello^2.0");
-    assertNotNull(q);
-    assertEquals(q.getBoost(), (float) 2.0, (float) 0.5);
-    q = qp.parse("\"on\"^1.0");
-    assertNotNull(q);
-
-    QueryParser qp2 = new QueryParser(TEST_VERSION_CURRENT, "field", new MockAnalyzer(random, MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET, true));
-    q = qp2.parse("the^3");
-    // "the" is a stop word so the result is an empty query:
-    assertNotNull(q);
-    assertEquals("", q.toString());
-    assertEquals(1.0f, q.getBoost(), 0.01f);
-  }
-
-  public void assertParseException(String queryString) throws Exception {
-    try {
-      getQuery(queryString, null);
-    } catch (ParseException expected) {
-      return;
-    }
-    fail("ParseException expected, not thrown");
-  }
-       
-  public void testException() throws Exception {
-    assertParseException("\"some phrase");
-    assertParseException("(foo bar");
-    assertParseException("foo bar))");
-    assertParseException("field:term:with:colon some more terms");
-    assertParseException("(sub query)^5.0^2.0 plus more");
-    assertParseException("secret AND illegal) AND access:confidential");
-  }
-  
-
-  public void testCustomQueryParserWildcard() {
-    try {
-      new QPTestParser("contents", new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)).parse("a?t");
-      fail("Wildcard queries should not be allowed");
-    } catch (ParseException expected) {
-      // expected exception
-    }
-  }
-
-  public void testCustomQueryParserFuzzy() throws Exception {
-    try {
-      new QPTestParser("contents", new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)).parse("xunit~");
-      fail("Fuzzy queries should not be allowed");
-    } catch (ParseException expected) {
-      // expected exception
-    }
-  }
-
-  public void testBooleanQuery() throws Exception {
-    BooleanQuery.setMaxClauseCount(2);
-    try {
-      QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", new MockAnalyzer(random, MockTokenizer.WHITESPACE, false));
-      qp.parse("one two three");
-      fail("ParseException expected due to too many boolean clauses");
-    } catch (ParseException expected) {
-      // too many boolean clauses, so ParseException is expected
-    }
-  }
-
-  /**
-   * This test differs from TestPrecedenceQueryParser
-   */
-  public void testPrecedence() throws Exception {
-    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", new MockAnalyzer(random, MockTokenizer.WHITESPACE, false));
-    Query query1 = qp.parse("A AND B OR C AND D");
-    Query query2 = qp.parse("+A +B +C +D");
-    assertEquals(query1, query2);
-  }
-
-// Todo: convert this from DateField to DateUtil
-//  public void testLocalDateFormat() throws IOException, ParseException {
-//    Directory ramDir = newDirectory();
-//    IndexWriter iw = new IndexWriter(ramDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));
-//    addDateDoc("a", 2005, 12, 2, 10, 15, 33, iw);
-//    addDateDoc("b", 2005, 12, 4, 22, 15, 00, iw);
-//    iw.close();
-//    IndexSearcher is = new IndexSearcher(ramDir, true);
-//    assertHits(1, "[12/1/2005 TO 12/3/2005]", is);
-//    assertHits(2, "[12/1/2005 TO 12/4/2005]", is);
-//    assertHits(1, "[12/3/2005 TO 12/4/2005]", is);
-//    assertHits(1, "{12/1/2005 TO 12/3/2005}", is);
-//    assertHits(1, "{12/1/2005 TO 12/4/2005}", is);
-//    assertHits(0, "{12/3/2005 TO 12/4/2005}", is);
-//    is.close();
-//    ramDir.close();
-//  }
-//
-//  private void addDateDoc(String content, int year, int month,
-//                          int day, int hour, int minute, int second, IndexWriter iw) throws IOException {
-//    Document d = new Document();
-//    d.add(newField("f", content, Field.Store.YES, Field.Index.ANALYZED));
-//    Calendar cal = Calendar.getInstance(Locale.ENGLISH);
-//    cal.set(year, month - 1, day, hour, minute, second);
-//    d.add(newField("date", DateField.dateToString(cal.getTime()), Field.Store.YES, Field.Index.NOT_ANALYZED));
-//    iw.addDocument(d);
-//  }
-
-  public void testStarParsing() throws Exception {
-    final int[] type = new int[1];
-    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)) {
-      @Override
-      protected Query getWildcardQuery(String field, String termStr) throws ParseException {
-        // override error checking of superclass
-        type[0]=1;
-        return new TermQuery(new Term(field,termStr));
-      }
-      @Override
-      protected Query getPrefixQuery(String field, String termStr) throws ParseException {
-        // override error checking of superclass
-        type[0]=2;        
-        return new TermQuery(new Term(field,termStr));
-      }
-
-      @Override
-      protected Query getFieldQuery(String field, String queryText, boolean quoted) throws ParseException {
-        type[0]=3;
-        return super.getFieldQuery(field, queryText, quoted);
-      }
-    };
-
-    TermQuery tq;
-
-    tq = (TermQuery)qp.parse("foo:zoo*");
-    assertEquals("zoo",tq.getTerm().text());
-    assertEquals(2,type[0]);
-
-    tq = (TermQuery)qp.parse("foo:zoo*^2");
-    assertEquals("zoo",tq.getTerm().text());
-    assertEquals(2,type[0]);
-    assertEquals(tq.getBoost(),2,0);
-
-    tq = (TermQuery)qp.parse("foo:*");
-    assertEquals("*",tq.getTerm().text());
-    assertEquals(1,type[0]);  // could be a valid prefix query in the future too
-
-    tq = (TermQuery)qp.parse("foo:*^2");
-    assertEquals("*",tq.getTerm().text());
-    assertEquals(1,type[0]);
-    assertEquals(tq.getBoost(),2,0);    
-
-    tq = (TermQuery)qp.parse("*:foo");
-    assertEquals("*",tq.getTerm().field());
-    assertEquals("foo",tq.getTerm().text());
-    assertEquals(3,type[0]);
-
-    tq = (TermQuery)qp.parse("*:*");
-    assertEquals("*",tq.getTerm().field());
-    assertEquals("*",tq.getTerm().text());
-    assertEquals(1,type[0]);  // could be handled as a prefix query in the future
-
-     tq = (TermQuery)qp.parse("(*:*)");
-    assertEquals("*",tq.getTerm().field());
-    assertEquals("*",tq.getTerm().text());
-    assertEquals(1,type[0]);
-
-  }
-
-  public void testEscapedWildcard() throws Exception {
-    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", new MockAnalyzer(random, MockTokenizer.WHITESPACE, false));
-    WildcardQuery q = new WildcardQuery(new Term("field", "foo\\?ba?r"));
-    assertEquals(q, qp.parse("foo\\?ba?r"));
-  }
-  
-  public void testRegexps() throws Exception {
-    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", new MockAnalyzer(random, MockTokenizer.WHITESPACE, false));
-    RegexpQuery q = new RegexpQuery(new Term("field", "[a-z][123]"));
-    assertEquals(q, qp.parse("/[a-z][123]/"));
-    qp.setLowercaseExpandedTerms(true);
-    assertEquals(q, qp.parse("/[A-Z][123]/"));
-    q.setBoost(0.5f);
-    assertEquals(q, qp.parse("/[A-Z][123]/^0.5"));
-    qp.setMultiTermRewriteMethod(MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE);
-    q.setRewriteMethod(MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE);
-    assertTrue(qp.parse("/[A-Z][123]/^0.5") instanceof RegexpQuery);
-    assertEquals(MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE, ((RegexpQuery)qp.parse("/[A-Z][123]/^0.5")).getRewriteMethod());
-    assertEquals(q, qp.parse("/[A-Z][123]/^0.5"));
-    qp.setMultiTermRewriteMethod(MultiTermQuery.CONSTANT_SCORE_AUTO_REWRITE_DEFAULT);
-    
-    Query escaped = new RegexpQuery(new Term("field", "[a-z]\\/[123]"));
-    assertEquals(escaped, qp.parse("/[a-z]\\/[123]/"));
-    Query escaped2 = new RegexpQuery(new Term("field", "[a-z]\\*[123]"));
-    assertEquals(escaped2, qp.parse("/[a-z]\\*[123]/"));
-    
-    BooleanQuery complex = new BooleanQuery();
-    complex.add(new RegexpQuery(new Term("field", "[a-z]\\/[123]")), Occur.MUST);
-    complex.add(new TermQuery(new Term("path", "/etc/init.d/")), Occur.MUST);
-    complex.add(new TermQuery(new Term("field", "/etc/init[.]d/lucene/")), Occur.SHOULD);
-    assertEquals(complex, qp.parse("/[a-z]\\/[123]/ AND path:/etc/init.d/ OR /etc\\/init\\[.\\]d/lucene/ "));
-  }
-  
-  public void testStopwords() throws Exception {
-    CharacterRunAutomaton stopSet = new CharacterRunAutomaton(new RegExp("the|foo").toAutomaton());
-    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "a", new MockAnalyzer(random, MockTokenizer.SIMPLE, true, stopSet, true));
-    Query result = qp.parse("a:the OR a:foo");
-    assertNotNull("result is null and it shouldn't be", result);
-    assertTrue("result is not a BooleanQuery", result instanceof BooleanQuery);
-    assertTrue(((BooleanQuery) result).clauses().size() + " does not equal: " + 0, ((BooleanQuery) result).clauses().size() == 0);
-    result = qp.parse("a:woo OR a:the");
-    assertNotNull("result is null and it shouldn't be", result);
-    assertTrue("result is not a TermQuery", result instanceof TermQuery);
-    result = qp.parse("(fieldX:xxxxx OR fieldy:xxxxxxxx)^2 AND (fieldx:the OR fieldy:foo)");
-    assertNotNull("result is null and it shouldn't be", result);
-    assertTrue("result is not a BooleanQuery", result instanceof BooleanQuery);
-    if (VERBOSE) System.out.println("Result: " + result);
-    assertTrue(((BooleanQuery) result).clauses().size() + " does not equal: " + 2, ((BooleanQuery) result).clauses().size() == 2);
-  }
-
-  public void testPositionIncrement() throws Exception {
-    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "a", new MockAnalyzer(random, MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET, true));
-    qp.setEnablePositionIncrements(true);
-    String qtxt = "\"the words in poisitions pos02578 are stopped in this phrasequery\"";
-    //               0         2                      5           7  8
-    int expectedPositions[] = {1,3,4,6,9};
-    PhraseQuery pq = (PhraseQuery) qp.parse(qtxt);
-    //System.out.println("Query text: "+qtxt);
-    //System.out.println("Result: "+pq);
-    Term t[] = pq.getTerms();
-    int pos[] = pq.getPositions();
-    for (int i = 0; i < t.length; i++) {
-      //System.out.println(i+". "+t[i]+"  pos: "+pos[i]);
-      assertEquals("term "+i+" = "+t[i]+" has wrong term-position!",expectedPositions[i],pos[i]);
-    }
-  }
-
-  public void testMatchAllDocs() throws Exception {
-    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", new MockAnalyzer(random, MockTokenizer.WHITESPACE, false));
-    assertEquals(new MatchAllDocsQuery(), qp.parse("*:*"));
-    assertEquals(new MatchAllDocsQuery(), qp.parse("(*:*)"));
-    BooleanQuery bq = (BooleanQuery)qp.parse("+*:* -*:*");
-    assertTrue(bq.getClauses()[0].getQuery() instanceof MatchAllDocsQuery);
-    assertTrue(bq.getClauses()[1].getQuery() instanceof MatchAllDocsQuery);
-  }
-  
-  private void assertHits(int expected, String query, IndexSearcher is) throws ParseException, IOException {
-    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "date", new MockAnalyzer(random, MockTokenizer.WHITESPACE, false));
-    qp.setLocale(Locale.ENGLISH);
-    Query q = qp.parse(query);
-    ScoreDoc[] hits = is.search(q, null, 1000).scoreDocs;
-    assertEquals(expected, hits.length);
-  }
-
-  @Override
-  public void tearDown() throws Exception {
-    BooleanQuery.setMaxClauseCount(originalMaxClauses);
-    super.tearDown();
-  }
-
-  // LUCENE-2002: make sure defaults for StandardAnalyzer's
-  // enableStopPositionIncr & QueryParser's enablePosIncr
-  // "match"
-  public void testPositionIncrements() throws Exception {
-    Directory dir = newDirectory();
-    Analyzer a = new MockAnalyzer(random, MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET, true);
-    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, a));
-    Document doc = new Document();
-    doc.add(newField("f", "the wizard of ozzy", Field.Store.NO, Field.Index.ANALYZED));
-    w.addDocument(doc);
-    IndexReader r = IndexReader.open(w, true);
-    w.close();
-    IndexSearcher s = newSearcher(r);
-    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "f", a);
-    Query q = qp.parse("\"wizard of ozzy\"");
-    assertEquals(1, s.search(q, 1).totalHits);
-    s.close();
-    r.close();
-    dir.close();
-  }
-
-  // LUCENE-2002: when we run javacc to regen QueryParser,
-  // we also run a replaceregexp step to fix 2 of the public
-  // ctors (change them to protected):
-  //
-  //   protected QueryParser(CharStream stream)
-  //
-  //   protected QueryParser(QueryParserTokenManager tm)
-  //
-  // This test is here as a safety, in case that ant step
-  // doesn't work for some reason.
-  public void testProtectedCtors() throws Exception {
-    try {
-      QueryParser.class.getConstructor(new Class[] {CharStream.class});
-      fail("please switch public QueryParser(CharStream) to be protected");
-    } catch (NoSuchMethodException nsme) {
-      // expected
-    }
-    try {
-      QueryParser.class.getConstructor(new Class[] {QueryParserTokenManager.class});
-      fail("please switch public QueryParser(QueryParserTokenManager) to be protected");
-    } catch (NoSuchMethodException nsme) {
-      // expected
-    }
-  }
-  
-  /**
-   * adds synonym of "dog" for "dogs".
-   */
-  private class MockSynonymFilter extends TokenFilter {
-    CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-    PositionIncrementAttribute posIncAtt = addAttribute(PositionIncrementAttribute.class);
-    boolean addSynonym = false;
-    
-    public MockSynonymFilter(TokenStream input) {
-      super(input);
-    }
-
-    @Override
-    public final boolean incrementToken() throws IOException {
-      if (addSynonym) { // inject our synonym
-        clearAttributes();
-        termAtt.setEmpty().append("dog");
-        posIncAtt.setPositionIncrement(0);
-        addSynonym = false;
-        return true;
-      }
-      
-      if (input.incrementToken()) {
-        addSynonym = termAtt.toString().equals("dogs");
-        return true;
-      } else {
-        return false;
-      }
-    } 
-  }
-  
-  /** whitespace+lowercase analyzer with synonyms */
-  private class Analyzer1 extends Analyzer {
-    @Override
-    public TokenStream tokenStream(String fieldName, Reader reader) {
-      return new MockSynonymFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));
-    }
-  }
-  
-  /** whitespace+lowercase analyzer without synonyms */
-  private class Analyzer2 extends Analyzer {
-    @Override
-    public TokenStream tokenStream(String fieldName, Reader reader) {
-      return new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);
-    }
-  }
-  
-  /** query parser that doesn't expand synonyms when users use double quotes */
-  private class SmartQueryParser extends QueryParser {
-    Analyzer morePrecise = new Analyzer2();
-    
-    public SmartQueryParser() {
-      super(TEST_VERSION_CURRENT, "field", new Analyzer1());
-    }
-
-    @Override
-    protected Query getFieldQuery(String field, String queryText, boolean quoted)
-        throws ParseException {
-      if (quoted)
-        return newFieldQuery(morePrecise, field, queryText, quoted);
-      else
-        return super.getFieldQuery(field, queryText, quoted);
-    }
-  }
-  
-  public void testNewFieldQuery() throws Exception {
-    /** ordinary behavior, synonyms form uncoordinated boolean query */
-    QueryParser dumb = new QueryParser(TEST_VERSION_CURRENT, "field", new Analyzer1());
-    BooleanQuery expanded = new BooleanQuery(true);
-    expanded.add(new TermQuery(new Term("field", "dogs")), BooleanClause.Occur.SHOULD);
-    expanded.add(new TermQuery(new Term("field", "dog")), BooleanClause.Occur.SHOULD);
-    assertEquals(expanded, dumb.parse("\"dogs\""));
-    /** even with the phrase operator the behavior is the same */
-    assertEquals(expanded, dumb.parse("dogs"));
-    
-    /** custom behavior, the synonyms are expanded, unless you use quote operator */
-    QueryParser smart = new SmartQueryParser();
-    assertEquals(expanded, smart.parse("dogs"));
-    
-    Query unexpanded = new TermQuery(new Term("field", "dogs"));
-    assertEquals(unexpanded, smart.parse("\"dogs\""));
-  }
-  
-  /**
-   * Mock collation analyzer: indexes terms as "collated" + term
-   */
-  private class MockCollationFilter extends TokenFilter {
-    private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-
-    protected MockCollationFilter(TokenStream input) {
-      super(input);
-    }
-
-    @Override
-    public boolean incrementToken() throws IOException {
-      if (input.incrementToken()) {
-        String term = termAtt.toString();
-        termAtt.setEmpty().append("collated").append(term);
-        return true;
-      } else {
-        return false;
-      }
-    }
-    
-  }
-  private class MockCollationAnalyzer extends Analyzer {
-    @Override
-    public TokenStream tokenStream(String fieldName, Reader reader) {
-      return new MockCollationFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));
-    }
-  }
-  
-  public void testCollatedRange() throws Exception {
-    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", new MockCollationAnalyzer());
-    qp.setAnalyzeRangeTerms(true);
-    Query expected = TermRangeQuery.newStringRange("field", "collatedabc", "collateddef", true, true);
-    Query actual = qp.parse("[abc TO def]");
-    assertEquals(expected, actual);
-  }
-
-  public void testDistanceAsEditsParsing() throws Exception {
-    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", new MockAnalyzer(random));
-    FuzzyQuery q = (FuzzyQuery) qp.parse("foobar~2");
-    assertEquals(2f, q.getMinSimilarity(), 0.0001f);
-  }
-
-  public void testPhraseQueryToString() throws ParseException {
-    Analyzer analyzer = new MockAnalyzer(random, MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET, true);
-    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", analyzer);
-    qp.setEnablePositionIncrements(true);
-    PhraseQuery q = (PhraseQuery)qp.parse("\"this hi this is a test is\"");
-    assertEquals("field:\"? hi ? ? ? test\"", q.toString());
-  }
-
-  public void testParseWildcardAndPhraseQueries() throws ParseException {
-    String field = "content";
-    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, field, new MockAnalyzer(random));
-    qp.setAllowLeadingWildcard(true);
-
-    String prefixQueries[][] = {
-        {"a*", "ab*", "abc*",},
-        {"h*", "hi*", "hij*", "\\\\7*"},
-        {"o*", "op*", "opq*", "\\\\\\\\*"},
-    };
-
-    String wildcardQueries[][] = {
-        {"*a*", "*ab*", "*abc**", "ab*e*", "*g?", "*f?1", "abc**"},
-        {"*h*", "*hi*", "*hij**", "hi*k*", "*n?", "*m?1", "hij**"},
-        {"*o*", "*op*", "*opq**", "op*q*", "*u?", "*t?1", "opq**"},
-    };
-
-     // test queries that must be prefix queries
-    for (int i = 0; i < prefixQueries.length; i++) {
-      for (int j = 0; j < prefixQueries[i].length; j++) {
-        String queryString = prefixQueries[i][j];
-        Query q = qp.parse(queryString);
-        assertEquals(PrefixQuery.class, q.getClass());
-      }
-    }
-
-    // test queries that must be wildcard queries
-    for (int i = 0; i < wildcardQueries.length; i++) {
-      for (int j = 0; j < wildcardQueries[i].length; j++) {
-        String qtxt = wildcardQueries[i][j];
-        Query q = qp.parse(qtxt);
-        assertEquals(WildcardQuery.class, q.getClass());
-      }
-    }
-  }
-
-  public void testPhraseQueryPositionIncrements() throws Exception {
-    CharacterRunAutomaton stopStopList =
-    new CharacterRunAutomaton(new RegExp("[sS][tT][oO][pP]").toAutomaton());
-
-    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field",
-        new MockAnalyzer(random, MockTokenizer.WHITESPACE, false, stopStopList, false));
-
-    PhraseQuery phraseQuery = new PhraseQuery();
-    phraseQuery.add(new Term("field", "1"));
-    phraseQuery.add(new Term("field", "2"));
-
-    assertEquals(phraseQuery, qp.parse("\"1 2\""));
-    assertEquals(phraseQuery, qp.parse("\"1 stop 2\""));
-
-    qp.setEnablePositionIncrements(true);
-    assertEquals(phraseQuery, qp.parse("\"1 stop 2\""));
-
-    qp.setEnablePositionIncrements(false);
-    assertEquals(phraseQuery, qp.parse("\"1 stop 2\""));
-
-    qp = new QueryParser(TEST_VERSION_CURRENT, "field",
-                         new MockAnalyzer(random, MockTokenizer.WHITESPACE, false, stopStopList, true));
-    qp.setEnablePositionIncrements(true);
-
-    phraseQuery = new PhraseQuery();
-    phraseQuery.add(new Term("field", "1"));
-    phraseQuery.add(new Term("field", "2"), 2);
-    assertEquals(phraseQuery, qp.parse("\"1 stop 2\""));
-  }
-
-  public void testMatchAllQueryParsing() throws Exception {
-    // test simple parsing of MatchAllDocsQuery
-    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "key", new MockAnalyzer(random));
-    assertEquals(new MatchAllDocsQuery(), qp.parse(new MatchAllDocsQuery().toString()));
-
-    // test parsing with non-default boost
-    MatchAllDocsQuery query = new MatchAllDocsQuery();
-    query.setBoost(2.3f);
-    assertEquals(query, qp.parse(query.toString()));
-  }
-  
-}
diff --git a/lucene/src/test/org/apache/lucene/search/TestMultiPhraseQuery.java b/lucene/src/test/org/apache/lucene/search/TestMultiPhraseQuery.java
index c434b1d..b5ef9fd 100644
--- a/lucene/src/test/org/apache/lucene/search/TestMultiPhraseQuery.java
+++ b/lucene/src/test/org/apache/lucene/search/TestMultiPhraseQuery.java
@@ -23,7 +23,6 @@ import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.MultiFields;
-import org.apache.lucene.queryParser.ParseException;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.TermContext;
@@ -465,7 +464,7 @@ public class TestMultiPhraseQuery extends LuceneTestCase {
    * using query parser, MPQ will be created, and will not be strict about having all query terms 
    * in each position - one of each position is sufficient (OR logic)
    */
-  public void testZeroPosIncrSloppyParsedAnd() throws IOException, ParseException {
+  public void testZeroPosIncrSloppyParsedAnd() throws IOException {
     MultiPhraseQuery q = new MultiPhraseQuery();
     q.add(new Term[]{ new Term("field", "a"), new Term("field", "1") }, -1);
     q.add(new Term[]{ new Term("field", "b"), new Term("field", "1") }, 0);
@@ -509,7 +508,7 @@ public class TestMultiPhraseQuery extends LuceneTestCase {
   /**
    * PQ AND Mode - Manually creating a phrase query
    */
-  public void testZeroPosIncrSloppyPqAnd() throws IOException, ParseException {
+  public void testZeroPosIncrSloppyPqAnd() throws IOException {
     final PhraseQuery pq = new PhraseQuery();
     for (TokenAndPos tap : INCR_0_QUERY_TOKENS_AND) {
       pq.add(new Term("field",tap.token), tap.pos);
@@ -524,7 +523,7 @@ public class TestMultiPhraseQuery extends LuceneTestCase {
   /**
    * MPQ AND Mode - Manually creating a multiple phrase query
    */
-  public void testZeroPosIncrSloppyMpqAnd() throws IOException, ParseException {
+  public void testZeroPosIncrSloppyMpqAnd() throws IOException {
     final MultiPhraseQuery mpq = new MultiPhraseQuery();
     for (TokenAndPos tap : INCR_0_QUERY_TOKENS_AND) {
       mpq.add(new Term[]{new Term("field",tap.token)}, tap.pos); //AND logic
@@ -539,7 +538,7 @@ public class TestMultiPhraseQuery extends LuceneTestCase {
   /**
    * MPQ Combined AND OR Mode - Manually creating a multiple phrase query
    */
-  public void testZeroPosIncrSloppyMpqAndOrMatch() throws IOException, ParseException {
+  public void testZeroPosIncrSloppyMpqAndOrMatch() throws IOException {
     final MultiPhraseQuery mpq = new MultiPhraseQuery();
     for (TokenAndPos tap[] : INCR_0_QUERY_TOKENS_AND_OR_MATCH) {
       Term[] terms = tapTerms(tap);
@@ -556,7 +555,7 @@ public class TestMultiPhraseQuery extends LuceneTestCase {
   /**
    * MPQ Combined AND OR Mode - Manually creating a multiple phrase query - with no match
    */
-  public void testZeroPosIncrSloppyMpqAndOrNoMatch() throws IOException, ParseException {
+  public void testZeroPosIncrSloppyMpqAndOrNoMatch() throws IOException {
     final MultiPhraseQuery mpq = new MultiPhraseQuery();
     for (TokenAndPos tap[] : INCR_0_QUERY_TOKENS_AND_OR_NO_MATCHN) {
       Term[] terms = tapTerms(tap);
diff --git a/lucene/src/test/org/apache/lucene/search/TestSort.java b/lucene/src/test/org/apache/lucene/search/TestSort.java
index 9e355d2..2c076e9 100644
--- a/lucene/src/test/org/apache/lucene/search/TestSort.java
+++ b/lucene/src/test/org/apache/lucene/search/TestSort.java
@@ -38,7 +38,6 @@ import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.codecs.CodecProvider;
 import org.apache.lucene.index.values.ValueType;
-import org.apache.lucene.queryParser.ParseException;
 import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.search.FieldValueHitQueue.Entry;
 import org.apache.lucene.search.cache.ByteValuesCreator;
@@ -365,7 +364,7 @@ public class TestSort extends LuceneTestCase {
   /**
    * Test String sorting: small queue to many matches, multi field sort, reverse sort
    */
-  public void testStringSort() throws IOException, ParseException {
+  public void testStringSort() throws IOException {
     ScoreDoc[] result = null;
     IndexSearcher searcher = getFullStrings();
     sort.setSort(
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/core/StopFilter.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/core/StopFilter.java
index 45b847a..c36dcbe 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/core/StopFilter.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/core/StopFilter.java
@@ -26,7 +26,6 @@ import org.apache.lucene.analysis.util.FilteringTokenFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.util.CharArraySet;
-import org.apache.lucene.queryParser.QueryParser;
 import org.apache.lucene.util.Version;
 
 /**
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/util/FilteringTokenFilter.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/util/FilteringTokenFilter.java
index aa5d41f..f810c28 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/util/FilteringTokenFilter.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/util/FilteringTokenFilter.java
@@ -22,7 +22,6 @@ import java.io.IOException;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.queryParser.QueryParser; // for javadoc
 
 /**
  * Abstract base class for TokenFilters that may remove tokens.
@@ -87,7 +86,7 @@ public abstract class FilteringTokenFilter extends TokenFilter {
    * token is incremented.
    *
    * <p> <b>NOTE</b>: be sure to also
-   * set {@link QueryParser#setEnablePositionIncrements} if
+   * set org.apache.lucene.queryparser.classic.QueryParser#setEnablePositionIncrements if
    * you use QueryParser to create queries.
    */
   public void setEnablePositionIncrements(boolean enable) {
diff --git a/modules/benchmark/build.xml b/modules/benchmark/build.xml
index e49137a..300024e 100644
--- a/modules/benchmark/build.xml
+++ b/modules/benchmark/build.xml
@@ -41,6 +41,8 @@
       property="analyzers-common.uptodate" classpath.property="analyzers-common.jar"/>
     <contrib-uptodate name="memory" property="memory.uptodate" classpath.property="memory.jar"/>
 
+    <module-uptodate name="queryparser" property="queryparser.uptodate" classpath.property="queryparser.jar"/>
+
     <target name="check-files">
         <available file="temp/news20.tar.gz" property="news20.exists"/>
 
@@ -160,6 +162,7 @@
       <pathelement path="${memory.jar}"/>
       <pathelement path="${highlighter.jar}"/>
       <pathelement path="${analyzers-common.jar}"/>
+      <pathelement path="${queryparser.jar}"/>
       <path refid="base.classpath"/>
     	<fileset dir="lib">
     		<include name="**/*.jar"/>
@@ -268,8 +271,13 @@
          <fileset dir="${common.dir}/contrib/memory" includes="build.xml"/>
       </subant>
     </target>
+    <target name="compile-queryparser" unless="queryparser.uptodate">
+      <subant target="default">
+         <fileset dir="${common.dir}/../modules/queryparser" includes="build.xml"/>
+      </subant>
+    </target>
 
-    <target name="init" depends="contrib-build.init,compile-memory,compile-highlighter,compile-analyzers-common"/>
+    <target name="init" depends="contrib-build.init,compile-memory,compile-highlighter,compile-analyzers-common,compile-queryparser"/>
   
     <target name="clean-javacc">
       <fileset dir="src/java/org/apache/lucene/benchmark/byTask/feeds/demohtml" includes="*.java">
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/EnwikiQueryMaker.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/EnwikiQueryMaker.java
index 106c6c3..4253f5b 100644
--- a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/EnwikiQueryMaker.java
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/EnwikiQueryMaker.java
@@ -24,7 +24,7 @@ import java.util.List;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.queryparser.classic.QueryParser;
 import org.apache.lucene.search.MultiTermQuery;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.WildcardQuery;
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/FileBasedQueryMaker.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/FileBasedQueryMaker.java
index dbfc731..ac17182 100644
--- a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/FileBasedQueryMaker.java
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/FileBasedQueryMaker.java
@@ -1,8 +1,8 @@
 package org.apache.lucene.benchmark.byTask.feeds;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.queryParser.QueryParser;
-import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryparser.classic.ParseException;
+import org.apache.lucene.queryparser.classic.QueryParser;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.benchmark.byTask.tasks.NewAnalyzerTask;
 import org.apache.lucene.util.Version;
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/LongToEnglishQueryMaker.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/LongToEnglishQueryMaker.java
index fdee288..d97cde5 100644
--- a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/LongToEnglishQueryMaker.java
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/LongToEnglishQueryMaker.java
@@ -21,7 +21,7 @@ import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.benchmark.byTask.tasks.NewAnalyzerTask;
 import org.apache.lucene.benchmark.byTask.utils.Config;
-import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.queryparser.classic.QueryParser;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.util.English;
 import org.apache.lucene.util.Version;
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/ReutersQueryMaker.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/ReutersQueryMaker.java
index c12ed07..9dcc0af 100644
--- a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/ReutersQueryMaker.java
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/ReutersQueryMaker.java
@@ -19,7 +19,7 @@ package org.apache.lucene.benchmark.byTask.feeds;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.queryparser.classic.QueryParser;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.WildcardQuery;
 import org.apache.lucene.search.spans.SpanFirstQuery;
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleQueryMaker.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleQueryMaker.java
index c550f33..115b5c4 100644
--- a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleQueryMaker.java
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleQueryMaker.java
@@ -19,7 +19,7 @@ package org.apache.lucene.benchmark.byTask.feeds;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.queryparser.classic.QueryParser;
 import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.Query;
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityQueryParser.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityQueryParser.java
index 66bd275..6a0f22d 100755
--- a/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityQueryParser.java
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityQueryParser.java
@@ -16,7 +16,7 @@
  */
 package org.apache.lucene.benchmark.quality;
 
-import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.lucene.search.Query;
 
 /**
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/SimpleQQParser.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/SimpleQQParser.java
index 765bdc6..d7d8d58 100755
--- a/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/SimpleQQParser.java
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/SimpleQQParser.java
@@ -19,8 +19,8 @@ package org.apache.lucene.benchmark.quality.utils;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.benchmark.quality.QualityQuery;
 import org.apache.lucene.benchmark.quality.QualityQueryParser;
-import org.apache.lucene.queryParser.ParseException;
-import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.queryparser.classic.ParseException;
+import org.apache.lucene.queryparser.classic.QueryParser;
 import org.apache.lucene.search.BooleanClause;
 import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.Query;
diff --git a/modules/build.xml b/modules/build.xml
index 8c7c337..62db70a 100644
--- a/modules/build.xml
+++ b/modules/build.xml
@@ -28,6 +28,7 @@
         <fileset dir="grouping" includes="build.xml" />
         <fileset dir="join" includes="build.xml" />
         <fileset dir="queries" includes="build.xml" />
+        <fileset dir="queryparser" includes="build.xml" />
         <fileset dir="suggest" includes="build.xml" />
       </subant>
     </sequential>
@@ -42,6 +43,7 @@
         <fileset dir="grouping" includes="build.xml" />
         <fileset dir="join" includes="build.xml" />
         <fileset dir="queries" includes="build.xml" />
+        <fileset dir="queryparser" includes="build.xml" />
         <fileset dir="suggest" includes="build.xml" />
       </subant>
     </sequential>
@@ -56,6 +58,7 @@
         <fileset dir="grouping" includes="build.xml" />
         <fileset dir="join" includes="build.xml" />
         <fileset dir="queries" includes="build.xml" />
+        <fileset dir="queryparser" includes="build.xml" />
         <fileset dir="suggest" includes="build.xml" />
       </subant>
     </sequential>
@@ -70,6 +73,7 @@
         <fileset dir="grouping" includes="build.xml" />
         <fileset dir="join" includes="build.xml" />
         <fileset dir="queries" includes="build.xml" />
+        <fileset dir="queryparser" includes="build.xml" />
         <fileset dir="suggest" includes="build.xml" />
       </subant>
     </sequential>
@@ -85,6 +89,7 @@
         <fileset dir="grouping" includes="build.xml" />
         <fileset dir="join" includes="build.xml" />
         <fileset dir="queries" includes="build.xml" />
+        <fileset dir="queryparser" includes="build.xml" />
         <fileset dir="suggest" includes="build.xml" />
       </subant>
     </sequential>
@@ -98,6 +103,7 @@
         <fileset dir="grouping" includes="build.xml" />
         <fileset dir="join" includes="build.xml" />
         <fileset dir="queries" includes="build.xml" />
+        <fileset dir="queryparser" includes="build.xml" />
         <fileset dir="suggest" includes="build.xml" />
       </subant>
     </sequential>
@@ -113,6 +119,7 @@
         <fileset dir="grouping" includes="build.xml" />
         <fileset dir="join" includes="build.xml" />
         <fileset dir="queries" includes="build.xml" />
+        <fileset dir="queryparser" includes="build.xml" />
         <fileset dir="suggest" includes="build.xml" />
       </subant>
     </sequential>
diff --git a/modules/queryparser/LICENSE.txt b/modules/queryparser/LICENSE.txt
new file mode 100644
index 0000000..d645695
--- /dev/null
+++ b/modules/queryparser/LICENSE.txt
@@ -0,0 +1,202 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/modules/queryparser/NOTICE.txt b/modules/queryparser/NOTICE.txt
new file mode 100644
index 0000000..1743ea8
--- /dev/null
+++ b/modules/queryparser/NOTICE.txt
@@ -0,0 +1,5 @@
+Apache Lucene QueryParsers
+Copyright 2011 The Apache Software Foundation
+
+This product includes software developed by
+The Apache Software Foundation (http://www.apache.org/).
diff --git a/modules/queryparser/build.xml b/modules/queryparser/build.xml
new file mode 100644
index 0000000..7bdec27
--- /dev/null
+++ b/modules/queryparser/build.xml
@@ -0,0 +1,59 @@
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one or more
+  contributor license agreements.  See the NOTICE file distributed with
+  this work for additional information regarding copyright ownership.
+  The ASF licenses this file to You under the Apache License, Version 2.0
+  (the "License"); you may not use this file except in compliance with
+  the License.  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+  -->
+
+<project name="queryparser" default="default">
+  <description>
+    Lucene QueryParsers
+  </description>
+
+  <property name="build.dir" location="build/"/>
+  <property name="dist.dir" location="dist/"/>
+  <property name="maven.dist.dir" location="../dist/maven"/>
+
+  <import file="../../lucene/contrib/contrib-build.xml"/>
+
+  <target name="dist-maven" depends="jar-core,javadocs,contrib-build.dist-maven"/>
+
+  <target name="clean-javacc">
+    <delete>
+      <fileset dir="src/java/org/apache/lucene/queryparser/classic" includes="*.java">
+        <containsregexp expression="Generated.*By.*JavaCC"/>
+      </fileset>
+    </delete>
+  </target>
+
+  <target name="javacc" depends="init,javacc-check,clean-javacc,javacc-QueryParser"/>
+
+  <target name="javacc-QueryParser" depends="init,javacc-check" if="javacc.present">
+    <sequential>
+      <invoke-javacc target="src/java/org/apache/lucene/queryparser/classic/QueryParser.jj"
+                     outputDir="src/java/org/apache/lucene/queryparser/classic"/>
+
+      <!-- Change the incorrect public ctors for QueryParser to be protected instead -->
+      <replaceregexp file="src/java/org/apache/lucene/queryparser/classic/QueryParser.java"
+		     byline="true"
+		     match="public QueryParser\(CharStream "
+		     replace="protected QueryParser(CharStream "/>
+      <replaceregexp file="src/java/org/apache/lucene/queryparser/classic/QueryParser.java"
+		     byline="true"
+		     match="public QueryParser\(QueryParserTokenManager "
+		     replace="protected QueryParser(QueryParserTokenManager "/>
+
+    </sequential>
+  </target>
+
+</project>
diff --git a/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/CharStream.java b/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/CharStream.java
new file mode 100644
index 0000000..d5b8002
--- /dev/null
+++ b/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/CharStream.java
@@ -0,0 +1,112 @@
+/* Generated By:JavaCC: Do not edit this line. CharStream.java Version 4.1 */
+/* JavaCCOptions:STATIC=false */
+package org.apache.lucene.queryparser.classic;
+
+/**
+ * This interface describes a character stream that maintains line and
+ * column number positions of the characters.  It also has the capability
+ * to backup the stream to some extent.  An implementation of this
+ * interface is used in the TokenManager implementation generated by
+ * JavaCCParser.
+ *
+ * All the methods except backup can be implemented in any fashion. backup
+ * needs to be implemented correctly for the correct operation of the lexer.
+ * Rest of the methods are all used to get information like line number,
+ * column number and the String that constitutes a token and are not used
+ * by the lexer. Hence their implementation won't affect the generated lexer's
+ * operation.
+ */
+
+public interface CharStream {
+
+  /**
+   * Returns the next character from the selected input.  The method
+   * of selecting the input is the responsibility of the class
+   * implementing this interface.  Can throw any java.io.IOException.
+   */
+  char readChar() throws java.io.IOException;
+
+  /**
+   * Returns the column position of the character last read.
+   * @deprecated
+   * @see #getEndColumn
+   */
+  int getColumn();
+
+  /**
+   * Returns the line number of the character last read.
+   * @deprecated
+   * @see #getEndLine
+   */
+  int getLine();
+
+  /**
+   * Returns the column number of the last character for current token (being
+   * matched after the last call to BeginTOken).
+   */
+  int getEndColumn();
+
+  /**
+   * Returns the line number of the last character for current token (being
+   * matched after the last call to BeginTOken).
+   */
+  int getEndLine();
+
+  /**
+   * Returns the column number of the first character for current token (being
+   * matched after the last call to BeginTOken).
+   */
+  int getBeginColumn();
+
+  /**
+   * Returns the line number of the first character for current token (being
+   * matched after the last call to BeginTOken).
+   */
+  int getBeginLine();
+
+  /**
+   * Backs up the input stream by amount steps. Lexer calls this method if it
+   * had already read some characters, but could not use them to match a
+   * (longer) token. So, they will be used again as the prefix of the next
+   * token and it is the implemetation's responsibility to do this right.
+   */
+  void backup(int amount);
+
+  /**
+   * Returns the next character that marks the beginning of the next token.
+   * All characters must remain in the buffer between two successive calls
+   * to this method to implement backup correctly.
+   */
+  char BeginToken() throws java.io.IOException;
+
+  /**
+   * Returns a string made up of characters from the marked token beginning
+   * to the current buffer position. Implementations have the choice of returning
+   * anything that they want to. For example, for efficiency, one might decide
+   * to just return null, which is a valid implementation.
+   */
+  String GetImage();
+
+  /**
+   * Returns an array of characters that make up the suffix of length 'len' for
+   * the currently matched token. This is used to build up the matched string
+   * for use in actions in the case of MORE. A simple and inefficient
+   * implementation of this is as follows :
+   *
+   *   {
+   *      String t = GetImage();
+   *      return t.substring(t.length() - len, t.length()).toCharArray();
+   *   }
+   */
+  char[] GetSuffix(int len);
+
+  /**
+   * The lexer calls this function to indicate that it is done with the stream
+   * and hence implementations can free any resources held by this class.
+   * Again, the body of this function can be just empty and it will not
+   * affect the lexer's operation.
+   */
+  void Done();
+
+}
+/* JavaCC - OriginalChecksum=0790771f0d47abfd976f028fa2364b0f (do not edit this line) */
diff --git a/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/FastCharStream.java b/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/FastCharStream.java
new file mode 100644
index 0000000..3960cc4
--- /dev/null
+++ b/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/FastCharStream.java
@@ -0,0 +1,124 @@
+// FastCharStream.java
+package org.apache.lucene.queryparser.classic;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ *  
+ */
+
+import java.io.*;
+
+/** An efficient implementation of JavaCC's CharStream interface.  <p>Note that
+ * this does not do line-number counting, but instead keeps track of the
+ * character position of the token in the input, as required by Lucene's {@link
+ * org.apache.lucene.analysis.Token} API. 
+ * */
+public final class FastCharStream implements CharStream {
+  char[] buffer = null;
+
+  int bufferLength = 0;				  // end of valid chars
+  int bufferPosition = 0;			  // next char to read
+
+  int tokenStart = 0;				  // offset in buffer
+  int bufferStart = 0;				  // position in file of buffer
+
+  Reader input;					  // source of chars
+
+  /** Constructs from a Reader. */
+  public FastCharStream(Reader r) {
+    input = r;
+  }
+
+  public final char readChar() throws IOException {
+    if (bufferPosition >= bufferLength)
+      refill();
+    return buffer[bufferPosition++];
+  }
+
+  private final void refill() throws IOException {
+    int newPosition = bufferLength - tokenStart;
+
+    if (tokenStart == 0) {			  // token won't fit in buffer
+      if (buffer == null) {			  // first time: alloc buffer
+	buffer = new char[2048];
+      } else if (bufferLength == buffer.length) { // grow buffer
+	char[] newBuffer = new char[buffer.length*2];
+	System.arraycopy(buffer, 0, newBuffer, 0, bufferLength);
+	buffer = newBuffer;
+      }
+    } else {					  // shift token to front
+      System.arraycopy(buffer, tokenStart, buffer, 0, newPosition);
+    }
+
+    bufferLength = newPosition;			  // update state
+    bufferPosition = newPosition;
+    bufferStart += tokenStart;
+    tokenStart = 0;
+
+    int charsRead =				  // fill space in buffer
+      input.read(buffer, newPosition, buffer.length-newPosition);
+    if (charsRead == -1)
+      throw new IOException("read past eof");
+    else
+      bufferLength += charsRead;
+  }
+
+  public final char BeginToken() throws IOException {
+    tokenStart = bufferPosition;
+    return readChar();
+  }
+
+  public final void backup(int amount) {
+    bufferPosition -= amount;
+  }
+
+  public final String GetImage() {
+    return new String(buffer, tokenStart, bufferPosition - tokenStart);
+  }
+
+  public final char[] GetSuffix(int len) {
+    char[] value = new char[len];
+    System.arraycopy(buffer, bufferPosition - len, value, 0, len);
+    return value;
+  }
+
+  public final void Done() {
+    try {
+      input.close();
+    } catch (IOException e) {
+      System.err.println("Caught: " + e + "; ignoring.");
+    }
+  }
+
+  public final int getColumn() {
+    return bufferStart + bufferPosition;
+  }
+  public final int getLine() {
+    return 1;
+  }
+  public final int getEndColumn() {
+    return bufferStart + bufferPosition;
+  }
+  public final int getEndLine() {
+    return 1;
+  }
+  public final int getBeginColumn() {
+    return bufferStart + tokenStart;
+  }
+  public final int getBeginLine() {
+    return 1;
+  }
+}
diff --git a/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/MultiFieldQueryParser.java b/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/MultiFieldQueryParser.java
new file mode 100644
index 0000000..b55f99d
--- /dev/null
+++ b/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/MultiFieldQueryParser.java
@@ -0,0 +1,349 @@
+package org.apache.lucene.queryparser.classic;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.MultiPhraseQuery;
+import org.apache.lucene.search.PhraseQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.util.Version;
+
+/**
+ * A QueryParser which constructs queries to search multiple fields.
+ *
+ */
+public class MultiFieldQueryParser extends QueryParser
+{
+  protected String[] fields;
+  protected Map<String,Float> boosts;
+
+  /**
+   * Creates a MultiFieldQueryParser. 
+   * Allows passing of a map with term to Boost, and the boost to apply to each term.
+   *
+   * <p>It will, when parse(String query)
+   * is called, construct a query like this (assuming the query consists of
+   * two terms and you specify the two fields <code>title</code> and <code>body</code>):</p>
+   * 
+   * <code>
+   * (title:term1 body:term1) (title:term2 body:term2)
+   * </code>
+   *
+   * <p>When setDefaultOperator(AND_OPERATOR) is set, the result will be:</p>
+   *  
+   * <code>
+   * +(title:term1 body:term1) +(title:term2 body:term2)
+   * </code>
+   * 
+   * <p>When you pass a boost (title=>5 body=>10) you can get </p>
+   * 
+   * <code>
+   * +(title:term1^5.0 body:term1^10.0) +(title:term2^5.0 body:term2^10.0)
+   * </code>
+   *
+   * <p>In other words, all the query's terms must appear, but it doesn't matter in
+   * what fields they appear.</p>
+   */
+  public MultiFieldQueryParser(Version matchVersion, String[] fields, Analyzer analyzer, Map<String,Float> boosts) {
+    this(matchVersion, fields, analyzer);
+    this.boosts = boosts;
+  }
+  
+  /**
+   * Creates a MultiFieldQueryParser.
+   *
+   * <p>It will, when parse(String query)
+   * is called, construct a query like this (assuming the query consists of
+   * two terms and you specify the two fields <code>title</code> and <code>body</code>):</p>
+   * 
+   * <code>
+   * (title:term1 body:term1) (title:term2 body:term2)
+   * </code>
+   *
+   * <p>When setDefaultOperator(AND_OPERATOR) is set, the result will be:</p>
+   *  
+   * <code>
+   * +(title:term1 body:term1) +(title:term2 body:term2)
+   * </code>
+   * 
+   * <p>In other words, all the query's terms must appear, but it doesn't matter in
+   * what fields they appear.</p>
+   */
+  public MultiFieldQueryParser(Version matchVersion, String[] fields, Analyzer analyzer) {
+    super(matchVersion, null, analyzer);
+    this.fields = fields;
+  }
+  
+  @Override
+  protected Query getFieldQuery(String field, String queryText, int slop) throws ParseException {
+    if (field == null) {
+      List<BooleanClause> clauses = new ArrayList<BooleanClause>();
+      for (int i = 0; i < fields.length; i++) {
+        Query q = super.getFieldQuery(fields[i], queryText, true);
+        if (q != null) {
+          //If the user passes a map of boosts
+          if (boosts != null) {
+            //Get the boost from the map and apply them
+            Float boost = boosts.get(fields[i]);
+            if (boost != null) {
+              q.setBoost(boost.floatValue());
+            }
+          }
+          applySlop(q,slop);
+          clauses.add(new BooleanClause(q, BooleanClause.Occur.SHOULD));
+        }
+      }
+      if (clauses.size() == 0)  // happens for stopwords
+        return null;
+      return getBooleanQuery(clauses, true);
+    }
+    Query q = super.getFieldQuery(field, queryText, true);
+    applySlop(q,slop);
+    return q;
+  }
+
+  private void applySlop(Query q, int slop) {
+    if (q instanceof PhraseQuery) {
+      ((PhraseQuery) q).setSlop(slop);
+    } else if (q instanceof MultiPhraseQuery) {
+      ((MultiPhraseQuery) q).setSlop(slop);
+    }
+  }
+  
+
+  @Override
+  protected Query getFieldQuery(String field, String queryText, boolean quoted) throws ParseException {
+    if (field == null) {
+      List<BooleanClause> clauses = new ArrayList<BooleanClause>();
+      for (int i = 0; i < fields.length; i++) {
+        Query q = super.getFieldQuery(fields[i], queryText, quoted);
+        if (q != null) {
+          //If the user passes a map of boosts
+          if (boosts != null) {
+            //Get the boost from the map and apply them
+            Float boost = boosts.get(fields[i]);
+            if (boost != null) {
+              q.setBoost(boost.floatValue());
+            }
+          }
+          clauses.add(new BooleanClause(q, BooleanClause.Occur.SHOULD));
+        }
+      }
+      if (clauses.size() == 0)  // happens for stopwords
+        return null;
+      return getBooleanQuery(clauses, true);
+    }
+    Query q = super.getFieldQuery(field, queryText, quoted);
+    return q;
+  }
+
+
+  @Override
+  protected Query getFuzzyQuery(String field, String termStr, float minSimilarity) throws ParseException
+  {
+    if (field == null) {
+      List<BooleanClause> clauses = new ArrayList<BooleanClause>();
+      for (int i = 0; i < fields.length; i++) {
+        clauses.add(new BooleanClause(getFuzzyQuery(fields[i], termStr, minSimilarity),
+            BooleanClause.Occur.SHOULD));
+      }
+      return getBooleanQuery(clauses, true);
+    }
+    return super.getFuzzyQuery(field, termStr, minSimilarity);
+  }
+
+  @Override
+  protected Query getPrefixQuery(String field, String termStr) throws ParseException
+  {
+    if (field == null) {
+      List<BooleanClause> clauses = new ArrayList<BooleanClause>();
+      for (int i = 0; i < fields.length; i++) {
+        clauses.add(new BooleanClause(getPrefixQuery(fields[i], termStr),
+            BooleanClause.Occur.SHOULD));
+      }
+      return getBooleanQuery(clauses, true);
+    }
+    return super.getPrefixQuery(field, termStr);
+  }
+
+  @Override
+  protected Query getWildcardQuery(String field, String termStr) throws ParseException {
+    if (field == null) {
+      List<BooleanClause> clauses = new ArrayList<BooleanClause>();
+      for (int i = 0; i < fields.length; i++) {
+        clauses.add(new BooleanClause(getWildcardQuery(fields[i], termStr),
+            BooleanClause.Occur.SHOULD));
+      }
+      return getBooleanQuery(clauses, true);
+    }
+    return super.getWildcardQuery(field, termStr);
+  }
+
+ 
+  @Override
+  protected Query getRangeQuery(String field, String part1, String part2, boolean startInclusive, boolean endInclusive) throws ParseException {
+    if (field == null) {
+      List<BooleanClause> clauses = new ArrayList<BooleanClause>();
+      for (int i = 0; i < fields.length; i++) {
+        clauses.add(new BooleanClause(getRangeQuery(fields[i], part1, part2, startInclusive, endInclusive),
+            BooleanClause.Occur.SHOULD));
+      }
+      return getBooleanQuery(clauses, true);
+    }
+    return super.getRangeQuery(field, part1, part2, startInclusive, endInclusive);
+  }
+
+  /**
+   * Parses a query which searches on the fields specified.
+   * <p>
+   * If x fields are specified, this effectively constructs:
+   * <pre>
+   * <code>
+   * (field1:query1) (field2:query2) (field3:query3)...(fieldx:queryx)
+   * </code>
+   * </pre>
+   * @param matchVersion Lucene version to match; this is passed through to QueryParser.
+   * @param queries Queries strings to parse
+   * @param fields Fields to search on
+   * @param analyzer Analyzer to use
+   * @throws ParseException if query parsing fails
+   * @throws IllegalArgumentException if the length of the queries array differs
+   *  from the length of the fields array
+   */
+  public static Query parse(Version matchVersion, String[] queries, String[] fields,
+      Analyzer analyzer) throws ParseException
+  {
+    if (queries.length != fields.length)
+      throw new IllegalArgumentException("queries.length != fields.length");
+    BooleanQuery bQuery = new BooleanQuery();
+    for (int i = 0; i < fields.length; i++)
+    {
+      QueryParser qp = new QueryParser(matchVersion, fields[i], analyzer);
+      Query q = qp.parse(queries[i]);
+      if (q!=null && // q never null, just being defensive
+          (!(q instanceof BooleanQuery) || ((BooleanQuery)q).getClauses().length>0)) {
+        bQuery.add(q, BooleanClause.Occur.SHOULD);
+      }
+    }
+    return bQuery;
+  }
+
+  /**
+   * Parses a query, searching on the fields specified.
+   * Use this if you need to specify certain fields as required,
+   * and others as prohibited.
+   * <p><pre>
+   * Usage:
+   * <code>
+   * String[] fields = {"filename", "contents", "description"};
+   * BooleanClause.Occur[] flags = {BooleanClause.Occur.SHOULD,
+   *                BooleanClause.Occur.MUST,
+   *                BooleanClause.Occur.MUST_NOT};
+   * MultiFieldQueryParser.parse("query", fields, flags, analyzer);
+   * </code>
+   * </pre>
+   *<p>
+   * The code above would construct a query:
+   * <pre>
+   * <code>
+   * (filename:query) +(contents:query) -(description:query)
+   * </code>
+   * </pre>
+   *
+   * @param matchVersion Lucene version to match; this is passed through to QueryParser.
+   * @param query Query string to parse
+   * @param fields Fields to search on
+   * @param flags Flags describing the fields
+   * @param analyzer Analyzer to use
+   * @throws ParseException if query parsing fails
+   * @throws IllegalArgumentException if the length of the fields array differs
+   *  from the length of the flags array
+   */
+  public static Query parse(Version matchVersion, String query, String[] fields,
+      BooleanClause.Occur[] flags, Analyzer analyzer) throws ParseException {
+    if (fields.length != flags.length)
+      throw new IllegalArgumentException("fields.length != flags.length");
+    BooleanQuery bQuery = new BooleanQuery();
+    for (int i = 0; i < fields.length; i++) {
+      QueryParser qp = new QueryParser(matchVersion, fields[i], analyzer);
+      Query q = qp.parse(query);
+      if (q!=null && // q never null, just being defensive 
+          (!(q instanceof BooleanQuery) || ((BooleanQuery)q).getClauses().length>0)) {
+        bQuery.add(q, flags[i]);
+      }
+    }
+    return bQuery;
+  }
+
+  /**
+   * Parses a query, searching on the fields specified.
+   * Use this if you need to specify certain fields as required,
+   * and others as prohibited.
+   * <p><pre>
+   * Usage:
+   * <code>
+   * String[] query = {"query1", "query2", "query3"};
+   * String[] fields = {"filename", "contents", "description"};
+   * BooleanClause.Occur[] flags = {BooleanClause.Occur.SHOULD,
+   *                BooleanClause.Occur.MUST,
+   *                BooleanClause.Occur.MUST_NOT};
+   * MultiFieldQueryParser.parse(query, fields, flags, analyzer);
+   * </code>
+   * </pre>
+   *<p>
+   * The code above would construct a query:
+   * <pre>
+   * <code>
+   * (filename:query1) +(contents:query2) -(description:query3)
+   * </code>
+   * </pre>
+   *
+   * @param matchVersion Lucene version to match; this is passed through to QueryParser.
+   * @param queries Queries string to parse
+   * @param fields Fields to search on
+   * @param flags Flags describing the fields
+   * @param analyzer Analyzer to use
+   * @throws ParseException if query parsing fails
+   * @throws IllegalArgumentException if the length of the queries, fields,
+   *  and flags array differ
+   */
+  public static Query parse(Version matchVersion, String[] queries, String[] fields, BooleanClause.Occur[] flags,
+      Analyzer analyzer) throws ParseException
+  {
+    if (!(queries.length == fields.length && queries.length == flags.length))
+      throw new IllegalArgumentException("queries, fields, and flags array have have different length");
+    BooleanQuery bQuery = new BooleanQuery();
+    for (int i = 0; i < fields.length; i++)
+    {
+      QueryParser qp = new QueryParser(matchVersion, fields[i], analyzer);
+      Query q = qp.parse(queries[i]);
+      if (q!=null && // q never null, just being defensive
+          (!(q instanceof BooleanQuery) || ((BooleanQuery)q).getClauses().length>0)) {
+        bQuery.add(q, flags[i]);
+      }
+    }
+    return bQuery;
+  }
+
+}
diff --git a/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/ParseException.java b/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/ParseException.java
new file mode 100644
index 0000000..f638ae1
--- /dev/null
+++ b/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/ParseException.java
@@ -0,0 +1,198 @@
+/* Generated By:JavaCC: Do not edit this line. ParseException.java Version 4.1 */
+/* JavaCCOptions:KEEP_LINE_COL=null */
+package org.apache.lucene.queryparser.classic;
+
+/**
+ * This exception is thrown when parse errors are encountered.
+ * You can explicitly create objects of this exception type by
+ * calling the method generateParseException in the generated
+ * parser.
+ *
+ * You can modify this class to customize your error reporting
+ * mechanisms so long as you retain the public fields.
+ */
+public class ParseException extends Exception {
+
+  /**
+   * This constructor is used by the method "generateParseException"
+   * in the generated parser.  Calling this constructor generates
+   * a new object of this type with the fields "currentToken",
+   * "expectedTokenSequences", and "tokenImage" set.  The boolean
+   * flag "specialConstructor" is also set to true to indicate that
+   * this constructor was used to create this object.
+   * This constructor calls its super class with the empty string
+   * to force the "toString" method of parent class "Throwable" to
+   * print the error message in the form:
+   *     ParseException: <result of getMessage>
+   */
+  public ParseException(Token currentTokenVal,
+                        int[][] expectedTokenSequencesVal,
+                        String[] tokenImageVal
+                       )
+  {
+    super("");
+    specialConstructor = true;
+    currentToken = currentTokenVal;
+    expectedTokenSequences = expectedTokenSequencesVal;
+    tokenImage = tokenImageVal;
+  }
+
+  /**
+   * The following constructors are for use by you for whatever
+   * purpose you can think of.  Constructing the exception in this
+   * manner makes the exception behave in the normal way - i.e., as
+   * documented in the class "Throwable".  The fields "errorToken",
+   * "expectedTokenSequences", and "tokenImage" do not contain
+   * relevant information.  The JavaCC generated code does not use
+   * these constructors.
+   */
+
+  public ParseException() {
+    super();
+    specialConstructor = false;
+  }
+
+  /** Constructor with message. */
+  public ParseException(String message) {
+    super(message);
+    specialConstructor = false;
+  }
+
+  /**
+   * This variable determines which constructor was used to create
+   * this object and thereby affects the semantics of the
+   * "getMessage" method (see below).
+   */
+  protected boolean specialConstructor;
+
+  /**
+   * This is the last token that has been consumed successfully.  If
+   * this object has been created due to a parse error, the token
+   * followng this token will (therefore) be the first error token.
+   */
+  public Token currentToken;
+
+  /**
+   * Each entry in this array is an array of integers.  Each array
+   * of integers represents a sequence of tokens (by their ordinal
+   * values) that is expected at this point of the parse.
+   */
+  public int[][] expectedTokenSequences;
+
+  /**
+   * This is a reference to the "tokenImage" array of the generated
+   * parser within which the parse error occurred.  This array is
+   * defined in the generated ...Constants interface.
+   */
+  public String[] tokenImage;
+
+  /**
+   * This method has the standard behavior when this object has been
+   * created using the standard constructors.  Otherwise, it uses
+   * "currentToken" and "expectedTokenSequences" to generate a parse
+   * error message and returns it.  If this object has been created
+   * due to a parse error, and you do not catch it (it gets thrown
+   * from the parser), then this method is called during the printing
+   * of the final stack trace, and hence the correct error message
+   * gets displayed.
+   */
+  public String getMessage() {
+    if (!specialConstructor) {
+      return super.getMessage();
+    }
+    StringBuffer expected = new StringBuffer();
+    int maxSize = 0;
+    for (int i = 0; i < expectedTokenSequences.length; i++) {
+      if (maxSize < expectedTokenSequences[i].length) {
+        maxSize = expectedTokenSequences[i].length;
+      }
+      for (int j = 0; j < expectedTokenSequences[i].length; j++) {
+        expected.append(tokenImage[expectedTokenSequences[i][j]]).append(' ');
+      }
+      if (expectedTokenSequences[i][expectedTokenSequences[i].length - 1] != 0) {
+        expected.append("...");
+      }
+      expected.append(eol).append("    ");
+    }
+    String retval = "Encountered \"";
+    Token tok = currentToken.next;
+    for (int i = 0; i < maxSize; i++) {
+      if (i != 0) retval += " ";
+      if (tok.kind == 0) {
+        retval += tokenImage[0];
+        break;
+      }
+      retval += " " + tokenImage[tok.kind];
+      retval += " \"";
+      retval += add_escapes(tok.image);
+      retval += " \"";
+      tok = tok.next;
+    }
+    retval += "\" at line " + currentToken.next.beginLine + ", column " + currentToken.next.beginColumn;
+    retval += "." + eol;
+    if (expectedTokenSequences.length == 1) {
+      retval += "Was expecting:" + eol + "    ";
+    } else {
+      retval += "Was expecting one of:" + eol + "    ";
+    }
+    retval += expected.toString();
+    return retval;
+  }
+
+  /**
+   * The end of line string for this machine.
+   */
+  protected String eol = System.getProperty("line.separator", "\n");
+
+  /**
+   * Used to convert raw characters to their escaped version
+   * when these raw version cannot be used as part of an ASCII
+   * string literal.
+   */
+  protected String add_escapes(String str) {
+      StringBuffer retval = new StringBuffer();
+      char ch;
+      for (int i = 0; i < str.length(); i++) {
+        switch (str.charAt(i))
+        {
+           case 0 :
+              continue;
+           case '\b':
+              retval.append("\\b");
+              continue;
+           case '\t':
+              retval.append("\\t");
+              continue;
+           case '\n':
+              retval.append("\\n");
+              continue;
+           case '\f':
+              retval.append("\\f");
+              continue;
+           case '\r':
+              retval.append("\\r");
+              continue;
+           case '\"':
+              retval.append("\\\"");
+              continue;
+           case '\'':
+              retval.append("\\\'");
+              continue;
+           case '\\':
+              retval.append("\\\\");
+              continue;
+           default:
+              if ((ch = str.charAt(i)) < 0x20 || ch > 0x7e) {
+                 String s = "0000" + Integer.toString(ch, 16);
+                 retval.append("\\u" + s.substring(s.length() - 4, s.length()));
+              } else {
+                 retval.append(ch);
+              }
+              continue;
+        }
+      }
+      return retval.toString();
+   }
+
+}
+/* JavaCC - OriginalChecksum=f669ffb14d5be55de6298772ac9befeb (do not edit this line) */
diff --git a/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParser.java b/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParser.java
new file mode 100644
index 0000000..0079595
--- /dev/null
+++ b/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParser.java
@@ -0,0 +1,754 @@
+/* Generated By:JavaCC: Do not edit this line. QueryParser.java */
+package org.apache.lucene.queryparser.classic;
+
+import java.io.StringReader;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Locale;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.document.DateTools;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermRangeQuery;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.util.Version;
+
+/**
+ * This class is generated by JavaCC.  The most important method is
+ * {@link #parse(String)}.
+ *
+ * The syntax for query strings is as follows:
+ * A Query is a series of clauses.
+ * A clause may be prefixed by:
+ * <ul>
+ * <li> a plus (<code>+</code>) or a minus (<code>-</code>) sign, indicating
+ * that the clause is required or prohibited respectively; or
+ * <li> a term followed by a colon, indicating the field to be searched.
+ * This enables one to construct queries which search multiple fields.
+ * </ul>
+ *
+ * A clause may be either:
+ * <ul>
+ * <li> a term, indicating all the documents that contain this term; or
+ * <li> a nested query, enclosed in parentheses.  Note that this may be used
+ * with a <code>+</code>/<code>-</code> prefix to require any of a set of
+ * terms.
+ * </ul>
+ *
+ * Thus, in BNF, the query grammar is:
+ * <pre>
+ *   Query  ::= ( Clause )*
+ *   Clause ::= ["+", "-"] [&lt;TERM&gt; ":"] ( &lt;TERM&gt; | "(" Query ")" )
+ * </pre>
+ *
+ * <p>
+ * Examples of appropriately formatted queries can be found in the <a
+ * href="../../../../../../queryparsersyntax.html">query syntax
+ * documentation</a>.
+ * </p>
+ *
+ * <p>
+ * In {@link TermRangeQuery}s, QueryParser tries to detect date values, e.g.
+ * <tt>date:[6/1/2005 TO 6/4/2005]</tt> produces a range query that searches
+ * for "date" fields between 2005-06-01 and 2005-06-04. Note that the format
+ * of the accepted input depends on {@link #setLocale(Locale) the locale}.
+ * A {@link org.apache.lucene.document.DateTools.Resolution} has to be set,
+ * if you want to use {@link DateTools} for date conversion.
+ * </p>
+ * <p>
+ * The date resolution that shall be used for RangeQueries can be set
+ * using {@link #setDateResolution(DateTools.Resolution)}
+ * or {@link #setDateResolution(String, DateTools.Resolution)}. The former
+ * sets the default date resolution for all fields, whereas the latter can
+ * be used to set field specific date resolutions. Field specific date
+ * resolutions take, if set, precedence over the default date resolution.
+ * </p>
+ * <p>
+ * If you don't use {@link DateTools} in your index, you can create your own
+ * query parser that inherits QueryParser and overwrites
+ * {@link #getRangeQuery(String, String, String, boolean, boolean)} to
+ * use a different method for date conversion.
+ * </p>
+ *
+ * <p>Note that QueryParser is <em>not</em> thread-safe.</p> 
+ * 
+ * <p><b>NOTE</b>: there is a new QueryParser in contrib, which matches
+ * the same syntax as this class, but is more modular,
+ * enabling substantial customization to how a query is created.
+ *
+ * <a name="version"/>
+ * <p><b>NOTE</b>: You must specify the required {@link Version}
+ * compatibility when creating QueryParser:
+ * <ul>
+ *    <li> As of 3.1, {@link #setAutoGeneratePhraseQueries} is false by
+ *         default.
+ * </ul>
+ */
+public class QueryParser extends QueryParserBase implements QueryParserConstants {
+  /** The default operator for parsing queries.
+   * Use {@link QueryParserBase#setDefaultOperator} to change it.
+   */
+  static public enum Operator { OR, AND }
+
+  /** Create a query parser.
+   *  @param matchVersion  Lucene version to match. See <a href="#version">above</a>.
+   *  @param f  the default field for query terms.
+   *  @param a   used to find terms in the query text.
+   */
+   public QueryParser(Version matchVersion, String f, Analyzer a) {
+    this(new FastCharStream(new StringReader("")));
+    init(matchVersion, f, a);
+  }
+
+// *   Query  ::= ( Clause )*
+// *   Clause ::= ["+", "-"] [<TERM> ":"] ( <TERM> | "(" Query ")" )
+  final public int Conjunction() throws ParseException {
+  int ret = CONJ_NONE;
+    switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+    case AND:
+    case OR:
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case AND:
+        jj_consume_token(AND);
+            ret = CONJ_AND;
+        break;
+      case OR:
+        jj_consume_token(OR);
+              ret = CONJ_OR;
+        break;
+      default:
+        jj_la1[0] = jj_gen;
+        jj_consume_token(-1);
+        throw new ParseException();
+      }
+      break;
+    default:
+      jj_la1[1] = jj_gen;
+      ;
+    }
+    {if (true) return ret;}
+    throw new Error("Missing return statement in function");
+  }
+
+  final public int Modifiers() throws ParseException {
+  int ret = MOD_NONE;
+    switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+    case NOT:
+    case PLUS:
+    case MINUS:
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case PLUS:
+        jj_consume_token(PLUS);
+              ret = MOD_REQ;
+        break;
+      case MINUS:
+        jj_consume_token(MINUS);
+                 ret = MOD_NOT;
+        break;
+      case NOT:
+        jj_consume_token(NOT);
+               ret = MOD_NOT;
+        break;
+      default:
+        jj_la1[2] = jj_gen;
+        jj_consume_token(-1);
+        throw new ParseException();
+      }
+      break;
+    default:
+      jj_la1[3] = jj_gen;
+      ;
+    }
+    {if (true) return ret;}
+    throw new Error("Missing return statement in function");
+  }
+
+// This makes sure that there is no garbage after the query string
+  final public Query TopLevelQuery(String field) throws ParseException {
+        Query q;
+    q = Query(field);
+    jj_consume_token(0);
+                {if (true) return q;}
+    throw new Error("Missing return statement in function");
+  }
+
+  final public Query Query(String field) throws ParseException {
+  List<BooleanClause> clauses = new ArrayList<BooleanClause>();
+  Query q, firstQuery=null;
+  int conj, mods;
+    mods = Modifiers();
+    q = Clause(field);
+    addClause(clauses, CONJ_NONE, mods, q);
+    if (mods == MOD_NONE)
+        firstQuery=q;
+    label_1:
+    while (true) {
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case AND:
+      case OR:
+      case NOT:
+      case PLUS:
+      case MINUS:
+      case BAREOPER:
+      case LPAREN:
+      case STAR:
+      case QUOTED:
+      case TERM:
+      case PREFIXTERM:
+      case WILDTERM:
+      case REGEXPTERM:
+      case RANGEIN_START:
+      case RANGEEX_START:
+      case NUMBER:
+        ;
+        break;
+      default:
+        jj_la1[4] = jj_gen;
+        break label_1;
+      }
+      conj = Conjunction();
+      mods = Modifiers();
+      q = Clause(field);
+      addClause(clauses, conj, mods, q);
+    }
+      if (clauses.size() == 1 && firstQuery != null)
+        {if (true) return firstQuery;}
+      else {
+  {if (true) return getBooleanQuery(clauses);}
+      }
+    throw new Error("Missing return statement in function");
+  }
+
+  final public Query Clause(String field) throws ParseException {
+  Query q;
+  Token fieldToken=null, boost=null;
+    if (jj_2_1(2)) {
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case TERM:
+        fieldToken = jj_consume_token(TERM);
+        jj_consume_token(COLON);
+                               field=discardEscapeChar(fieldToken.image);
+        break;
+      case STAR:
+        jj_consume_token(STAR);
+        jj_consume_token(COLON);
+                      field="*";
+        break;
+      default:
+        jj_la1[5] = jj_gen;
+        jj_consume_token(-1);
+        throw new ParseException();
+      }
+    } else {
+      ;
+    }
+    switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+    case BAREOPER:
+    case STAR:
+    case QUOTED:
+    case TERM:
+    case PREFIXTERM:
+    case WILDTERM:
+    case REGEXPTERM:
+    case RANGEIN_START:
+    case RANGEEX_START:
+    case NUMBER:
+      q = Term(field);
+      break;
+    case LPAREN:
+      jj_consume_token(LPAREN);
+      q = Query(field);
+      jj_consume_token(RPAREN);
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case CARAT:
+        jj_consume_token(CARAT);
+        boost = jj_consume_token(NUMBER);
+        break;
+      default:
+        jj_la1[6] = jj_gen;
+        ;
+      }
+      break;
+    default:
+      jj_la1[7] = jj_gen;
+      jj_consume_token(-1);
+      throw new ParseException();
+    }
+       {if (true) return handleBoost(q, boost);}
+    throw new Error("Missing return statement in function");
+  }
+
+  final public Query Term(String field) throws ParseException {
+  Token term, boost=null, fuzzySlop=null, goop1, goop2;
+  boolean prefix = false;
+  boolean wildcard = false;
+  boolean fuzzy = false;
+  boolean regexp = false;
+  boolean startInc=false;
+  boolean endInc=false;
+  Query q;
+    switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+    case BAREOPER:
+    case STAR:
+    case TERM:
+    case PREFIXTERM:
+    case WILDTERM:
+    case REGEXPTERM:
+    case NUMBER:
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case TERM:
+        term = jj_consume_token(TERM);
+        break;
+      case STAR:
+        term = jj_consume_token(STAR);
+                       wildcard=true;
+        break;
+      case PREFIXTERM:
+        term = jj_consume_token(PREFIXTERM);
+                             prefix=true;
+        break;
+      case WILDTERM:
+        term = jj_consume_token(WILDTERM);
+                           wildcard=true;
+        break;
+      case REGEXPTERM:
+        term = jj_consume_token(REGEXPTERM);
+                             regexp=true;
+        break;
+      case NUMBER:
+        term = jj_consume_token(NUMBER);
+        break;
+      case BAREOPER:
+        term = jj_consume_token(BAREOPER);
+                           term.image = term.image.substring(0,1);
+        break;
+      default:
+        jj_la1[8] = jj_gen;
+        jj_consume_token(-1);
+        throw new ParseException();
+      }
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case FUZZY_SLOP:
+        fuzzySlop = jj_consume_token(FUZZY_SLOP);
+                                fuzzy=true;
+        break;
+      default:
+        jj_la1[9] = jj_gen;
+        ;
+      }
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case CARAT:
+        jj_consume_token(CARAT);
+        boost = jj_consume_token(NUMBER);
+        switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+        case FUZZY_SLOP:
+          fuzzySlop = jj_consume_token(FUZZY_SLOP);
+                                                         fuzzy=true;
+          break;
+        default:
+          jj_la1[10] = jj_gen;
+          ;
+        }
+        break;
+      default:
+        jj_la1[11] = jj_gen;
+        ;
+      }
+       q = handleBareTokenQuery(field, term, fuzzySlop, prefix, wildcard, fuzzy, regexp);
+      break;
+    case RANGEIN_START:
+    case RANGEEX_START:
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case RANGEIN_START:
+        jj_consume_token(RANGEIN_START);
+                            startInc=true;
+        break;
+      case RANGEEX_START:
+        jj_consume_token(RANGEEX_START);
+        break;
+      default:
+        jj_la1[12] = jj_gen;
+        jj_consume_token(-1);
+        throw new ParseException();
+      }
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case RANGE_GOOP:
+        goop1 = jj_consume_token(RANGE_GOOP);
+        break;
+      case RANGE_QUOTED:
+        goop1 = jj_consume_token(RANGE_QUOTED);
+        break;
+      default:
+        jj_la1[13] = jj_gen;
+        jj_consume_token(-1);
+        throw new ParseException();
+      }
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case RANGE_TO:
+        jj_consume_token(RANGE_TO);
+        break;
+      default:
+        jj_la1[14] = jj_gen;
+        ;
+      }
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case RANGE_GOOP:
+        goop2 = jj_consume_token(RANGE_GOOP);
+        break;
+      case RANGE_QUOTED:
+        goop2 = jj_consume_token(RANGE_QUOTED);
+        break;
+      default:
+        jj_la1[15] = jj_gen;
+        jj_consume_token(-1);
+        throw new ParseException();
+      }
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case RANGEIN_END:
+        jj_consume_token(RANGEIN_END);
+                          endInc=true;
+        break;
+      case RANGEEX_END:
+        jj_consume_token(RANGEEX_END);
+        break;
+      default:
+        jj_la1[16] = jj_gen;
+        jj_consume_token(-1);
+        throw new ParseException();
+      }
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case CARAT:
+        jj_consume_token(CARAT);
+        boost = jj_consume_token(NUMBER);
+        break;
+      default:
+        jj_la1[17] = jj_gen;
+        ;
+      }
+          boolean startOpen=false;
+          boolean endOpen=false;
+          if (goop1.kind == RANGE_QUOTED) {
+            goop1.image = goop1.image.substring(1, goop1.image.length()-1);
+          } else if ("*".equals(goop1.image)) {
+            startOpen=true;
+          }
+          if (goop2.kind == RANGE_QUOTED) {
+            goop2.image = goop2.image.substring(1, goop2.image.length()-1);
+          } else if ("*".equals(goop2.image)) {
+            endOpen=true;
+          }
+          q = getRangeQuery(field, startOpen ? null : discardEscapeChar(goop1.image), endOpen ? null : discardEscapeChar(goop2.image), startInc, endInc);
+      break;
+    case QUOTED:
+      term = jj_consume_token(QUOTED);
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case FUZZY_SLOP:
+        fuzzySlop = jj_consume_token(FUZZY_SLOP);
+        break;
+      default:
+        jj_la1[18] = jj_gen;
+        ;
+      }
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case CARAT:
+        jj_consume_token(CARAT);
+        boost = jj_consume_token(NUMBER);
+        break;
+      default:
+        jj_la1[19] = jj_gen;
+        ;
+      }
+         q = handleQuotedTerm(field, term, fuzzySlop);
+      break;
+    default:
+      jj_la1[20] = jj_gen;
+      jj_consume_token(-1);
+      throw new ParseException();
+    }
+    {if (true) return handleBoost(q, boost);}
+    throw new Error("Missing return statement in function");
+  }
+
+  private boolean jj_2_1(int xla) {
+    jj_la = xla; jj_lastpos = jj_scanpos = token;
+    try { return !jj_3_1(); }
+    catch(LookaheadSuccess ls) { return true; }
+    finally { jj_save(0, xla); }
+  }
+
+  private boolean jj_3R_2() {
+    if (jj_scan_token(TERM)) return true;
+    if (jj_scan_token(COLON)) return true;
+    return false;
+  }
+
+  private boolean jj_3_1() {
+    Token xsp;
+    xsp = jj_scanpos;
+    if (jj_3R_2()) {
+    jj_scanpos = xsp;
+    if (jj_3R_3()) return true;
+    }
+    return false;
+  }
+
+  private boolean jj_3R_3() {
+    if (jj_scan_token(STAR)) return true;
+    if (jj_scan_token(COLON)) return true;
+    return false;
+  }
+
+  /** Generated Token Manager. */
+  public QueryParserTokenManager token_source;
+  /** Current token. */
+  public Token token;
+  /** Next token. */
+  public Token jj_nt;
+  private int jj_ntk;
+  private Token jj_scanpos, jj_lastpos;
+  private int jj_la;
+  private int jj_gen;
+  final private int[] jj_la1 = new int[21];
+  static private int[] jj_la1_0;
+  static private int[] jj_la1_1;
+  static {
+      jj_la1_init_0();
+      jj_la1_init_1();
+   }
+   private static void jj_la1_init_0() {
+      jj_la1_0 = new int[] {0x300,0x300,0x1c00,0x1c00,0xfda7f00,0x120000,0x40000,0xfda6000,0x9d22000,0x200000,0x200000,0x40000,0x6000000,0x80000000,0x10000000,0x80000000,0x60000000,0x40000,0x200000,0x40000,0xfda2000,};
+   }
+   private static void jj_la1_init_1() {
+      jj_la1_1 = new int[] {0x0,0x0,0x0,0x0,0x0,0x0,0x0,0x0,0x0,0x0,0x0,0x0,0x0,0x1,0x0,0x1,0x0,0x0,0x0,0x0,0x0,};
+   }
+  final private JJCalls[] jj_2_rtns = new JJCalls[1];
+  private boolean jj_rescan = false;
+  private int jj_gc = 0;
+
+  /** Constructor with user supplied CharStream. */
+  protected QueryParser(CharStream stream) {
+    token_source = new QueryParserTokenManager(stream);
+    token = new Token();
+    jj_ntk = -1;
+    jj_gen = 0;
+    for (int i = 0; i < 21; i++) jj_la1[i] = -1;
+    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
+  }
+
+  /** Reinitialise. */
+  public void ReInit(CharStream stream) {
+    token_source.ReInit(stream);
+    token = new Token();
+    jj_ntk = -1;
+    jj_gen = 0;
+    for (int i = 0; i < 21; i++) jj_la1[i] = -1;
+    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
+  }
+
+  /** Constructor with generated Token Manager. */
+  protected QueryParser(QueryParserTokenManager tm) {
+    token_source = tm;
+    token = new Token();
+    jj_ntk = -1;
+    jj_gen = 0;
+    for (int i = 0; i < 21; i++) jj_la1[i] = -1;
+    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
+  }
+
+  /** Reinitialise. */
+  public void ReInit(QueryParserTokenManager tm) {
+    token_source = tm;
+    token = new Token();
+    jj_ntk = -1;
+    jj_gen = 0;
+    for (int i = 0; i < 21; i++) jj_la1[i] = -1;
+    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
+  }
+
+  private Token jj_consume_token(int kind) throws ParseException {
+    Token oldToken;
+    if ((oldToken = token).next != null) token = token.next;
+    else token = token.next = token_source.getNextToken();
+    jj_ntk = -1;
+    if (token.kind == kind) {
+      jj_gen++;
+      if (++jj_gc > 100) {
+        jj_gc = 0;
+        for (int i = 0; i < jj_2_rtns.length; i++) {
+          JJCalls c = jj_2_rtns[i];
+          while (c != null) {
+            if (c.gen < jj_gen) c.first = null;
+            c = c.next;
+          }
+        }
+      }
+      return token;
+    }
+    token = oldToken;
+    jj_kind = kind;
+    throw generateParseException();
+  }
+
+  static private final class LookaheadSuccess extends java.lang.Error { }
+  final private LookaheadSuccess jj_ls = new LookaheadSuccess();
+  private boolean jj_scan_token(int kind) {
+    if (jj_scanpos == jj_lastpos) {
+      jj_la--;
+      if (jj_scanpos.next == null) {
+        jj_lastpos = jj_scanpos = jj_scanpos.next = token_source.getNextToken();
+      } else {
+        jj_lastpos = jj_scanpos = jj_scanpos.next;
+      }
+    } else {
+      jj_scanpos = jj_scanpos.next;
+    }
+    if (jj_rescan) {
+      int i = 0; Token tok = token;
+      while (tok != null && tok != jj_scanpos) { i++; tok = tok.next; }
+      if (tok != null) jj_add_error_token(kind, i);
+    }
+    if (jj_scanpos.kind != kind) return true;
+    if (jj_la == 0 && jj_scanpos == jj_lastpos) throw jj_ls;
+    return false;
+  }
+
+
+/** Get the next Token. */
+  final public Token getNextToken() {
+    if (token.next != null) token = token.next;
+    else token = token.next = token_source.getNextToken();
+    jj_ntk = -1;
+    jj_gen++;
+    return token;
+  }
+
+/** Get the specific Token. */
+  final public Token getToken(int index) {
+    Token t = token;
+    for (int i = 0; i < index; i++) {
+      if (t.next != null) t = t.next;
+      else t = t.next = token_source.getNextToken();
+    }
+    return t;
+  }
+
+  private int jj_ntk() {
+    if ((jj_nt=token.next) == null)
+      return (jj_ntk = (token.next=token_source.getNextToken()).kind);
+    else
+      return (jj_ntk = jj_nt.kind);
+  }
+
+  private java.util.List jj_expentries = new java.util.ArrayList();
+  private int[] jj_expentry;
+  private int jj_kind = -1;
+  private int[] jj_lasttokens = new int[100];
+  private int jj_endpos;
+
+  private void jj_add_error_token(int kind, int pos) {
+    if (pos >= 100) return;
+    if (pos == jj_endpos + 1) {
+      jj_lasttokens[jj_endpos++] = kind;
+    } else if (jj_endpos != 0) {
+      jj_expentry = new int[jj_endpos];
+      for (int i = 0; i < jj_endpos; i++) {
+        jj_expentry[i] = jj_lasttokens[i];
+      }
+      jj_entries_loop: for (java.util.Iterator it = jj_expentries.iterator(); it.hasNext();) {
+        int[] oldentry = (int[])(it.next());
+        if (oldentry.length == jj_expentry.length) {
+          for (int i = 0; i < jj_expentry.length; i++) {
+            if (oldentry[i] != jj_expentry[i]) {
+              continue jj_entries_loop;
+            }
+          }
+          jj_expentries.add(jj_expentry);
+          break jj_entries_loop;
+        }
+      }
+      if (pos != 0) jj_lasttokens[(jj_endpos = pos) - 1] = kind;
+    }
+  }
+
+  /** Generate ParseException. */
+  public ParseException generateParseException() {
+    jj_expentries.clear();
+    boolean[] la1tokens = new boolean[33];
+    if (jj_kind >= 0) {
+      la1tokens[jj_kind] = true;
+      jj_kind = -1;
+    }
+    for (int i = 0; i < 21; i++) {
+      if (jj_la1[i] == jj_gen) {
+        for (int j = 0; j < 32; j++) {
+          if ((jj_la1_0[i] & (1<<j)) != 0) {
+            la1tokens[j] = true;
+          }
+          if ((jj_la1_1[i] & (1<<j)) != 0) {
+            la1tokens[32+j] = true;
+          }
+        }
+      }
+    }
+    for (int i = 0; i < 33; i++) {
+      if (la1tokens[i]) {
+        jj_expentry = new int[1];
+        jj_expentry[0] = i;
+        jj_expentries.add(jj_expentry);
+      }
+    }
+    jj_endpos = 0;
+    jj_rescan_token();
+    jj_add_error_token(0, 0);
+    int[][] exptokseq = new int[jj_expentries.size()][];
+    for (int i = 0; i < jj_expentries.size(); i++) {
+      exptokseq[i] = (int[])jj_expentries.get(i);
+    }
+    return new ParseException(token, exptokseq, tokenImage);
+  }
+
+  /** Enable tracing. */
+  final public void enable_tracing() {
+  }
+
+  /** Disable tracing. */
+  final public void disable_tracing() {
+  }
+
+  private void jj_rescan_token() {
+    jj_rescan = true;
+    for (int i = 0; i < 1; i++) {
+    try {
+      JJCalls p = jj_2_rtns[i];
+      do {
+        if (p.gen > jj_gen) {
+          jj_la = p.arg; jj_lastpos = jj_scanpos = p.first;
+          switch (i) {
+            case 0: jj_3_1(); break;
+          }
+        }
+        p = p.next;
+      } while (p != null);
+      } catch(LookaheadSuccess ls) { }
+    }
+    jj_rescan = false;
+  }
+
+  private void jj_save(int index, int xla) {
+    JJCalls p = jj_2_rtns[index];
+    while (p.gen > jj_gen) {
+      if (p.next == null) { p = p.next = new JJCalls(); break; }
+      p = p.next;
+    }
+    p.gen = jj_gen + xla - jj_la; p.first = token; p.arg = xla;
+  }
+
+  static final class JJCalls {
+    int gen;
+    Token first;
+    int arg;
+    JJCalls next;
+  }
+
+}
diff --git a/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParser.jj b/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParser.jj
new file mode 100644
index 0000000..26f15c8
--- /dev/null
+++ b/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParser.jj
@@ -0,0 +1,324 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+options {
+  STATIC=false;
+  JAVA_UNICODE_ESCAPE=true;
+  USER_CHAR_STREAM=true;
+}
+
+PARSER_BEGIN(QueryParser)
+
+package org.apache.lucene.queryparser.classic;
+
+import java.io.StringReader;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Locale;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.document.DateTools;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermRangeQuery;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.util.Version;
+
+/**
+ * This class is generated by JavaCC.  The most important method is
+ * {@link #parse(String)}.
+ *
+ * The syntax for query strings is as follows:
+ * A Query is a series of clauses.
+ * A clause may be prefixed by:
+ * <ul>
+ * <li> a plus (<code>+</code>) or a minus (<code>-</code>) sign, indicating
+ * that the clause is required or prohibited respectively; or
+ * <li> a term followed by a colon, indicating the field to be searched.
+ * This enables one to construct queries which search multiple fields.
+ * </ul>
+ *
+ * A clause may be either:
+ * <ul>
+ * <li> a term, indicating all the documents that contain this term; or
+ * <li> a nested query, enclosed in parentheses.  Note that this may be used
+ * with a <code>+</code>/<code>-</code> prefix to require any of a set of
+ * terms.
+ * </ul>
+ *
+ * Thus, in BNF, the query grammar is:
+ * <pre>
+ *   Query  ::= ( Clause )*
+ *   Clause ::= ["+", "-"] [&lt;TERM&gt; ":"] ( &lt;TERM&gt; | "(" Query ")" )
+ * </pre>
+ *
+ * <p>
+ * Examples of appropriately formatted queries can be found in the <a
+ * href="../../../../../../queryparsersyntax.html">query syntax
+ * documentation</a>.
+ * </p>
+ *
+ * <p>
+ * In {@link TermRangeQuery}s, QueryParser tries to detect date values, e.g.
+ * <tt>date:[6/1/2005 TO 6/4/2005]</tt> produces a range query that searches
+ * for "date" fields between 2005-06-01 and 2005-06-04. Note that the format
+ * of the accepted input depends on {@link #setLocale(Locale) the locale}.
+ * A {@link org.apache.lucene.document.DateTools.Resolution} has to be set,
+ * if you want to use {@link DateTools} for date conversion.
+ * </p>
+ * <p>
+ * The date resolution that shall be used for RangeQueries can be set
+ * using {@link #setDateResolution(DateTools.Resolution)}
+ * or {@link #setDateResolution(String, DateTools.Resolution)}. The former
+ * sets the default date resolution for all fields, whereas the latter can
+ * be used to set field specific date resolutions. Field specific date
+ * resolutions take, if set, precedence over the default date resolution.
+ * </p>
+ * <p>
+ * If you don't use {@link DateTools} in your index, you can create your own
+ * query parser that inherits QueryParser and overwrites
+ * {@link #getRangeQuery(String, String, String, boolean, boolean)} to
+ * use a different method for date conversion.
+ * </p>
+ *
+ * <p>Note that QueryParser is <em>not</em> thread-safe.</p> 
+ * 
+ * <p><b>NOTE</b>: there is a new QueryParser in contrib, which matches
+ * the same syntax as this class, but is more modular,
+ * enabling substantial customization to how a query is created.
+ *
+ * <a name="version"/>
+ * <p><b>NOTE</b>: You must specify the required {@link Version}
+ * compatibility when creating QueryParser:
+ * <ul>
+ *    <li> As of 3.1, {@link #setAutoGeneratePhraseQueries} is false by
+ *         default.
+ * </ul>
+ */
+public class QueryParser extends QueryParserBase {
+  /** The default operator for parsing queries.
+   * Use {@link QueryParserBase#setDefaultOperator} to change it.
+   */
+  static public enum Operator { OR, AND }
+  
+  /** Create a query parser.
+   *  @param matchVersion  Lucene version to match. See <a href="#version">above</a>.
+   *  @param f  the default field for query terms.
+   *  @param a   used to find terms in the query text.
+   */
+   public QueryParser(Version matchVersion, String f, Analyzer a) {
+    this(new FastCharStream(new StringReader("")));
+    init(matchVersion, f, a);
+  }
+}
+
+PARSER_END(QueryParser)
+
+/* ***************** */
+/* Token Definitions */
+/* ***************** */
+
+<*> TOKEN : {
+  <#_NUM_CHAR:   ["0"-"9"] >
+// every character that follows a backslash is considered as an escaped character
+| <#_ESCAPED_CHAR: "\\" ~[] >
+| <#_TERM_START_CHAR: ( ~[ " ", "\t", "\n", "\r", "\u3000", "+", "-", "!", "(", ")", ":", "^",
+                           "[", "]", "\"", "{", "}", "~", "*", "?", "\\" ]
+                       | <_ESCAPED_CHAR> ) >
+| <#_TERM_CHAR: ( <_TERM_START_CHAR> | <_ESCAPED_CHAR> | "-" | "+" ) >
+| <#_WHITESPACE: ( " " | "\t" | "\n" | "\r" | "\u3000") >
+| <#_QUOTED_CHAR: ( ~[ "\"", "\\" ] | <_ESCAPED_CHAR> ) >
+}
+
+<DEFAULT, Range> SKIP : {
+  < <_WHITESPACE>>
+}
+
+<DEFAULT> TOKEN : {
+  <AND:       ("AND" | "&&") >
+| <OR:        ("OR" | "||") >
+| <NOT:       ("NOT" | "!") >
+| <PLUS:      "+" >
+| <MINUS:     "-" >
+| <BAREOPER:    ("+"|"-"|"!") <_WHITESPACE> >
+| <LPAREN:    "(" >
+| <RPAREN:    ")" >
+| <COLON:     ":" >
+| <STAR:      "*" >
+| <CARAT:     "^" > : Boost
+| <QUOTED:     "\"" (<_QUOTED_CHAR>)* "\"">
+| <TERM:      <_TERM_START_CHAR> (<_TERM_CHAR>)*  >
+| <FUZZY_SLOP:     "~" ( (<_NUM_CHAR>)+ ( "." (<_NUM_CHAR>)+ )? )? >
+| <PREFIXTERM:  ("*") | ( <_TERM_START_CHAR> (<_TERM_CHAR>)* "*" ) >
+| <WILDTERM:  (<_TERM_START_CHAR> | [ "*", "?" ]) (<_TERM_CHAR> | ( [ "*", "?" ] ))* >
+| <REGEXPTERM: "/" (~[ "/" ] | "\\/" )* "/" >
+| <RANGEIN_START: "[" > : Range
+| <RANGEEX_START: "{" > : Range
+}
+
+<Boost> TOKEN : {
+<NUMBER:    (<_NUM_CHAR>)+ ( "." (<_NUM_CHAR>)+ )? > : DEFAULT
+}
+
+<Range> TOKEN : {
+<RANGE_TO: "TO">
+| <RANGEIN_END: "]"> : DEFAULT
+| <RANGEEX_END: "}"> : DEFAULT
+| <RANGE_QUOTED: "\"" (~["\""] | "\\\"")+ "\"">
+| <RANGE_GOOP: (~[ " ", "]", "}" ])+ >
+}
+
+// *   Query  ::= ( Clause )*
+// *   Clause ::= ["+", "-"] [<TERM> ":"] ( <TERM> | "(" Query ")" )
+
+int Conjunction() : {
+  int ret = CONJ_NONE;
+}
+{
+  [
+    <AND> { ret = CONJ_AND; }
+    | <OR>  { ret = CONJ_OR; }
+  ]
+  { return ret; }
+}
+
+int Modifiers() : {
+  int ret = MOD_NONE;
+}
+{
+  [
+     <PLUS> { ret = MOD_REQ; }
+     | <MINUS> { ret = MOD_NOT; }
+     | <NOT> { ret = MOD_NOT; }
+  ]
+  { return ret; }
+}
+
+// This makes sure that there is no garbage after the query string
+Query TopLevelQuery(String field) : 
+{
+	Query q;
+}
+{
+	q=Query(field) <EOF>
+	{
+		return q;
+	}
+}
+
+Query Query(String field) :
+{
+  List<BooleanClause> clauses = new ArrayList<BooleanClause>();
+  Query q, firstQuery=null;
+  int conj, mods;
+}
+{
+  mods=Modifiers() q=Clause(field)
+  {
+    addClause(clauses, CONJ_NONE, mods, q);
+    if (mods == MOD_NONE)
+        firstQuery=q;
+  }
+  (
+    conj=Conjunction() mods=Modifiers() q=Clause(field)
+    { addClause(clauses, conj, mods, q); }
+  )*
+    {
+      if (clauses.size() == 1 && firstQuery != null)
+        return firstQuery;
+      else {
+  return getBooleanQuery(clauses);
+      }
+    }
+}
+
+Query Clause(String field) : {
+  Query q;
+  Token fieldToken=null, boost=null;
+}
+{
+  [
+    LOOKAHEAD(2)
+    (
+    fieldToken=<TERM> <COLON> {field=discardEscapeChar(fieldToken.image);}
+    | <STAR> <COLON> {field="*";}
+    )
+  ]
+
+  (
+   q=Term(field)
+   | <LPAREN> q=Query(field) <RPAREN> (<CARAT> boost=<NUMBER>)?
+
+  )
+    {  return handleBoost(q, boost); }
+}
+
+
+Query Term(String field) : {
+  Token term, boost=null, fuzzySlop=null, goop1, goop2;
+  boolean prefix = false;
+  boolean wildcard = false;
+  boolean fuzzy = false;
+  boolean regexp = false;
+  boolean startInc=false;
+  boolean endInc=false;
+  Query q;
+}
+{
+  (
+     (
+       term=<TERM>
+       | term=<STAR> { wildcard=true; }
+       | term=<PREFIXTERM> { prefix=true; }
+       | term=<WILDTERM> { wildcard=true; }
+       | term=<REGEXPTERM> { regexp=true; }
+       | term=<NUMBER>
+       | term=<BAREOPER> { term.image = term.image.substring(0,1); }
+     )
+     [ fuzzySlop=<FUZZY_SLOP> { fuzzy=true; } ]
+     [ <CARAT> boost=<NUMBER> [ fuzzySlop=<FUZZY_SLOP> { fuzzy=true; } ] ]
+     {
+       q = handleBareTokenQuery(field, term, fuzzySlop, prefix, wildcard, fuzzy, regexp);
+     }
+     | ( ( <RANGEIN_START> {startInc=true;} | <RANGEEX_START> )
+         ( goop1=<RANGE_GOOP>|goop1=<RANGE_QUOTED> )
+         [ <RANGE_TO> ]
+         ( goop2=<RANGE_GOOP>|goop2=<RANGE_QUOTED> )
+         ( <RANGEIN_END> {endInc=true;} | <RANGEEX_END>))
+       [ <CARAT> boost=<NUMBER> ]
+        {
+          boolean startOpen=false;
+          boolean endOpen=false;
+          if (goop1.kind == RANGE_QUOTED) {
+            goop1.image = goop1.image.substring(1, goop1.image.length()-1);
+          } else if ("*".equals(goop1.image)) {
+            startOpen=true;
+          }
+          if (goop2.kind == RANGE_QUOTED) {
+            goop2.image = goop2.image.substring(1, goop2.image.length()-1);
+          } else if ("*".equals(goop2.image)) {
+            endOpen=true;
+          }
+          q = getRangeQuery(field, startOpen ? null : discardEscapeChar(goop1.image), endOpen ? null : discardEscapeChar(goop2.image), startInc, endInc);
+        }
+     | term=<QUOTED>
+       [ fuzzySlop=<FUZZY_SLOP> ]
+       [ <CARAT> boost=<NUMBER> ]
+       { q = handleQuotedTerm(field, term, fuzzySlop); }
+  )
+  { return handleBoost(q, boost); }
+}
diff --git a/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserBase.java b/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserBase.java
new file mode 100644
index 0000000..9abddc0
--- /dev/null
+++ b/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserBase.java
@@ -0,0 +1,1194 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.queryparser.classic;
+
+import java.io.IOException;
+import java.io.StringReader;
+import java.text.Collator;
+import java.text.DateFormat;
+import java.util.*;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.CachingTokenFilter;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;
+import org.apache.lucene.document.DateTools;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.queryparser.classic.QueryParser.Operator;
+import org.apache.lucene.search.*;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.Version;
+
+/** This class is overridden by QueryParser in QueryParser.jj
+ * and acts to separate the majority of the Java code from the .jj grammar file. 
+ */
+public abstract class QueryParserBase {
+
+  /** Do not catch this exception in your code, it means you are using methods that you should no longer use. */
+  public static class MethodRemovedUseAnother extends Throwable {}
+
+  static final int CONJ_NONE   = 0;
+  static final int CONJ_AND    = 1;
+  static final int CONJ_OR     = 2;
+
+  static final int MOD_NONE    = 0;
+  static final int MOD_NOT     = 10;
+  static final int MOD_REQ     = 11;
+
+  // make it possible to call setDefaultOperator() without accessing
+  // the nested class:
+  /** Alternative form of QueryParser.Operator.AND */
+  public static final Operator AND_OPERATOR = Operator.AND;
+  /** Alternative form of QueryParser.Operator.OR */
+  public static final Operator OR_OPERATOR = Operator.OR;
+
+  /** The actual operator that parser uses to combine query terms */
+  Operator operator = OR_OPERATOR;
+
+  boolean lowercaseExpandedTerms = true;
+  MultiTermQuery.RewriteMethod multiTermRewriteMethod = MultiTermQuery.CONSTANT_SCORE_AUTO_REWRITE_DEFAULT;
+  boolean allowLeadingWildcard = false;
+  boolean enablePositionIncrements = true;
+
+  Analyzer analyzer;
+  String field;
+  int phraseSlop = 0;
+  float fuzzyMinSim = FuzzyQuery.defaultMinSimilarity;
+  int fuzzyPrefixLength = FuzzyQuery.defaultPrefixLength;
+  Locale locale = Locale.getDefault();
+
+  // the default date resolution
+  DateTools.Resolution dateResolution = null;
+  // maps field names to date resolutions
+  Map<String,DateTools.Resolution> fieldToDateResolution = null;
+
+  //Whether or not to analyze range terms when constructing RangeQuerys
+  // (For example, analyzing terms into collation keys for locale-sensitive RangeQuery)
+  boolean analyzeRangeTerms = false;
+
+  boolean autoGeneratePhraseQueries;
+
+  // So the generated QueryParser(CharStream) won't error out
+  protected QueryParserBase() {
+  }
+
+  /** Initializes a query parser.  Called by the QueryParser constructor
+   *  @param matchVersion  Lucene version to match. See <a href="#version">above</a>.
+   *  @param f  the default field for query terms.
+   *  @param a   used to find terms in the query text.
+   */
+  public void init(Version matchVersion, String f, Analyzer a) {
+    analyzer = a;
+    field = f;
+    if (matchVersion.onOrAfter(Version.LUCENE_31)) {
+      setAutoGeneratePhraseQueries(false);
+    } else {
+      setAutoGeneratePhraseQueries(true);
+    }
+  }
+
+  // the generated parser will create these in QueryParser
+  public abstract void ReInit(CharStream stream);
+  public abstract Query TopLevelQuery(String field) throws ParseException;
+
+
+  /** Parses a query string, returning a {@link org.apache.lucene.search.Query}.
+   *  @param query  the query string to be parsed.
+   *  @throws ParseException if the parsing fails
+   */
+  public Query parse(String query) throws ParseException {
+    ReInit(new FastCharStream(new StringReader(query)));
+    try {
+      // TopLevelQuery is a Query followed by the end-of-input (EOF)
+      Query res = TopLevelQuery(field);
+      return res!=null ? res : newBooleanQuery(false);
+    }
+    catch (ParseException tme) {
+      // rethrow to include the original query:
+      ParseException e = new ParseException("Cannot parse '" +query+ "': " + tme.getMessage());
+      e.initCause(tme);
+      throw e;
+    }
+    catch (TokenMgrError tme) {
+      ParseException e = new ParseException("Cannot parse '" +query+ "': " + tme.getMessage());
+      e.initCause(tme);
+      throw e;
+    }
+    catch (BooleanQuery.TooManyClauses tmc) {
+      ParseException e = new ParseException("Cannot parse '" +query+ "': too many boolean clauses");
+      e.initCause(tmc);
+      throw e;
+    }
+  }
+
+
+   /**
+   * @return Returns the analyzer.
+   */
+  public Analyzer getAnalyzer() {
+    return analyzer;
+  }
+
+  /**
+   * @return Returns the default field.
+   */
+  public String getField() {
+    return field;
+  }
+
+  /**
+   * @see #setAutoGeneratePhraseQueries(boolean)
+   */
+  public final boolean getAutoGeneratePhraseQueries() {
+    return autoGeneratePhraseQueries;
+  }
+
+  /**
+   * Set to true if phrase queries will be automatically generated
+   * when the analyzer returns more than one term from whitespace
+   * delimited text.
+   * NOTE: this behavior may not be suitable for all languages.
+   * <p>
+   * Set to false if phrase queries should only be generated when
+   * surrounded by double quotes.
+   */
+  public final void setAutoGeneratePhraseQueries(boolean value) {
+    this.autoGeneratePhraseQueries = value;
+  }
+
+   /**
+   * Get the minimal similarity for fuzzy queries.
+   */
+  public float getFuzzyMinSim() {
+      return fuzzyMinSim;
+  }
+
+  /**
+   * Set the minimum similarity for fuzzy queries.
+   * Default is 2f.
+   */
+  public void setFuzzyMinSim(float fuzzyMinSim) {
+      this.fuzzyMinSim = fuzzyMinSim;
+  }
+
+   /**
+   * Get the prefix length for fuzzy queries.
+   * @return Returns the fuzzyPrefixLength.
+   */
+  public int getFuzzyPrefixLength() {
+    return fuzzyPrefixLength;
+  }
+
+  /**
+   * Set the prefix length for fuzzy queries. Default is 0.
+   * @param fuzzyPrefixLength The fuzzyPrefixLength to set.
+   */
+  public void setFuzzyPrefixLength(int fuzzyPrefixLength) {
+    this.fuzzyPrefixLength = fuzzyPrefixLength;
+  }
+
+  /**
+   * Sets the default slop for phrases.  If zero, then exact phrase matches
+   * are required.  Default value is zero.
+   */
+  public void setPhraseSlop(int phraseSlop) {
+    this.phraseSlop = phraseSlop;
+  }
+
+  /**
+   * Gets the default slop for phrases.
+   */
+  public int getPhraseSlop() {
+    return phraseSlop;
+  }
+
+
+  /**
+   * Set to <code>true</code> to allow leading wildcard characters.
+   * <p>
+   * When set, <code>*</code> or <code>?</code> are allowed as
+   * the first character of a PrefixQuery and WildcardQuery.
+   * Note that this can produce very slow
+   * queries on big indexes.
+   * <p>
+   * Default: false.
+   */
+  public void setAllowLeadingWildcard(boolean allowLeadingWildcard) {
+    this.allowLeadingWildcard = allowLeadingWildcard;
+  }
+
+  /**
+   * @see #setAllowLeadingWildcard(boolean)
+   */
+  public boolean getAllowLeadingWildcard() {
+    return allowLeadingWildcard;
+  }
+
+  /**
+   * Set to <code>true</code> to enable position increments in result query.
+   * <p>
+   * When set, result phrase and multi-phrase queries will
+   * be aware of position increments.
+   * Useful when e.g. a StopFilter increases the position increment of
+   * the token that follows an omitted token.
+   * <p>
+   * Default: true.
+   */
+  public void setEnablePositionIncrements(boolean enable) {
+    this.enablePositionIncrements = enable;
+  }
+
+  /**
+   * @see #setEnablePositionIncrements(boolean)
+   */
+  public boolean getEnablePositionIncrements() {
+    return enablePositionIncrements;
+  }
+
+  /**
+   * Sets the boolean operator of the QueryParser.
+   * In default mode (<code>OR_OPERATOR</code>) terms without any modifiers
+   * are considered optional: for example <code>capital of Hungary</code> is equal to
+   * <code>capital OR of OR Hungary</code>.<br/>
+   * In <code>AND_OPERATOR</code> mode terms are considered to be in conjunction: the
+   * above mentioned query is parsed as <code>capital AND of AND Hungary</code>
+   */
+  public void setDefaultOperator(Operator op) {
+    this.operator = op;
+  }
+
+
+  /**
+   * Gets implicit operator setting, which will be either AND_OPERATOR
+   * or OR_OPERATOR.
+   */
+  public Operator getDefaultOperator() {
+    return operator;
+  }
+
+
+  /**
+   * Whether terms of wildcard, prefix, fuzzy and range queries are to be automatically
+   * lower-cased or not.  Default is <code>true</code>.
+   */
+  public void setLowercaseExpandedTerms(boolean lowercaseExpandedTerms) {
+    this.lowercaseExpandedTerms = lowercaseExpandedTerms;
+  }
+
+
+  /**
+   * @see #setLowercaseExpandedTerms(boolean)
+   */
+  public boolean getLowercaseExpandedTerms() {
+    return lowercaseExpandedTerms;
+  }
+
+  /**
+   * By default QueryParser uses {@link org.apache.lucene.search.MultiTermQuery#CONSTANT_SCORE_AUTO_REWRITE_DEFAULT}
+   * when creating a PrefixQuery, WildcardQuery or RangeQuery. This implementation is generally preferable because it
+   * a) Runs faster b) Does not have the scarcity of terms unduly influence score
+   * c) avoids any "TooManyBooleanClauses" exception.
+   * However, if your application really needs to use the
+   * old-fashioned BooleanQuery expansion rewriting and the above
+   * points are not relevant then use this to change
+   * the rewrite method.
+   */
+  public void setMultiTermRewriteMethod(MultiTermQuery.RewriteMethod method) {
+    multiTermRewriteMethod = method;
+  }
+
+
+  /**
+   * @see #setMultiTermRewriteMethod
+   */
+  public MultiTermQuery.RewriteMethod getMultiTermRewriteMethod() {
+    return multiTermRewriteMethod;
+  }
+
+  /**
+   * Set locale used by date range parsing.
+   */
+  public void setLocale(Locale locale) {
+    this.locale = locale;
+  }
+
+  /**
+   * Returns current locale, allowing access by subclasses.
+   */
+  public Locale getLocale() {
+    return locale;
+  }
+
+  /**
+   * Sets the default date resolution used by RangeQueries for fields for which no
+   * specific date resolutions has been set. Field specific resolutions can be set
+   * with {@link #setDateResolution(String, org.apache.lucene.document.DateTools.Resolution)}.
+   *
+   * @param dateResolution the default date resolution to set
+   */
+  public void setDateResolution(DateTools.Resolution dateResolution) {
+    this.dateResolution = dateResolution;
+  }
+
+  /**
+   * Sets the date resolution used by RangeQueries for a specific field.
+   *
+   * @param fieldName field for which the date resolution is to be set
+   * @param dateResolution date resolution to set
+   */
+  public void setDateResolution(String fieldName, DateTools.Resolution dateResolution) {
+    if (fieldName == null) {
+      throw new IllegalArgumentException("Field cannot be null.");
+    }
+
+    if (fieldToDateResolution == null) {
+      // lazily initialize HashMap
+      fieldToDateResolution = new HashMap<String,DateTools.Resolution>();
+    }
+
+    fieldToDateResolution.put(fieldName, dateResolution);
+  }
+
+  /**
+   * Returns the date resolution that is used by RangeQueries for the given field.
+   * Returns null, if no default or field specific date resolution has been set
+   * for the given field.
+   *
+   */
+  public DateTools.Resolution getDateResolution(String fieldName) {
+    if (fieldName == null) {
+      throw new IllegalArgumentException("Field cannot be null.");
+    }
+
+    if (fieldToDateResolution == null) {
+      // no field specific date resolutions set; return default date resolution instead
+      return this.dateResolution;
+    }
+
+    DateTools.Resolution resolution = fieldToDateResolution.get(fieldName);
+    if (resolution == null) {
+      // no date resolutions set for the given field; return default date resolution instead
+      resolution = this.dateResolution;
+    }
+
+    return resolution;
+  }
+
+  /**
+   * Set whether or not to analyze range terms when constructing RangeQuerys.
+   * For example, setting this to true can enable analyzing terms into 
+   * collation keys for locale-sensitive RangeQuery.
+   * 
+   * @param analyzeRangeTerms whether or not terms should be analyzed for RangeQuerys
+   */
+  public void setAnalyzeRangeTerms(boolean analyzeRangeTerms) {
+    this.analyzeRangeTerms = analyzeRangeTerms;
+  }
+
+  /**
+   * @return whether or not to analyze range terms when constructing RangeQuerys.
+   */
+  public boolean getAnalyzeRangeTerms() {
+    return analyzeRangeTerms;
+  }
+
+  protected void addClause(List<BooleanClause> clauses, int conj, int mods, Query q) {
+    boolean required, prohibited;
+
+    // If this term is introduced by AND, make the preceding term required,
+    // unless it's already prohibited
+    if (clauses.size() > 0 && conj == CONJ_AND) {
+      BooleanClause c = clauses.get(clauses.size()-1);
+      if (!c.isProhibited())
+        c.setOccur(BooleanClause.Occur.MUST);
+    }
+
+    if (clauses.size() > 0 && operator == AND_OPERATOR && conj == CONJ_OR) {
+      // If this term is introduced by OR, make the preceding term optional,
+      // unless it's prohibited (that means we leave -a OR b but +a OR b-->a OR b)
+      // notice if the input is a OR b, first term is parsed as required; without
+      // this modification a OR b would parsed as +a OR b
+      BooleanClause c = clauses.get(clauses.size()-1);
+      if (!c.isProhibited())
+        c.setOccur(BooleanClause.Occur.SHOULD);
+    }
+
+    // We might have been passed a null query; the term might have been
+    // filtered away by the analyzer.
+    if (q == null)
+      return;
+
+    if (operator == OR_OPERATOR) {
+      // We set REQUIRED if we're introduced by AND or +; PROHIBITED if
+      // introduced by NOT or -; make sure not to set both.
+      prohibited = (mods == MOD_NOT);
+      required = (mods == MOD_REQ);
+      if (conj == CONJ_AND && !prohibited) {
+        required = true;
+      }
+    } else {
+      // We set PROHIBITED if we're introduced by NOT or -; We set REQUIRED
+      // if not PROHIBITED and not introduced by OR
+      prohibited = (mods == MOD_NOT);
+      required   = (!prohibited && conj != CONJ_OR);
+    }
+    if (required && !prohibited)
+      clauses.add(newBooleanClause(q, BooleanClause.Occur.MUST));
+    else if (!required && !prohibited)
+      clauses.add(newBooleanClause(q, BooleanClause.Occur.SHOULD));
+    else if (!required && prohibited)
+      clauses.add(newBooleanClause(q, BooleanClause.Occur.MUST_NOT));
+    else
+      throw new RuntimeException("Clause cannot be both required and prohibited");
+  }
+
+  /**
+   * @exception org.apache.lucene.queryparser.classic.ParseException throw in overridden method to disallow
+   */
+  protected Query getFieldQuery(String field, String queryText, boolean quoted) throws ParseException {
+    return newFieldQuery(analyzer, field, queryText, quoted);
+  }
+  
+  /**
+   * @exception org.apache.lucene.queryparser.classic.ParseException throw in overridden method to disallow
+   */
+  protected Query newFieldQuery(Analyzer analyzer, String field, String queryText, boolean quoted)  throws ParseException {
+    // Use the analyzer to get all the tokens, and then build a TermQuery,
+    // PhraseQuery, or nothing based on the term count
+
+    TokenStream source;
+    try {
+      source = analyzer.reusableTokenStream(field, new StringReader(queryText));
+      source.reset();
+    } catch (IOException e) {
+      source = analyzer.tokenStream(field, new StringReader(queryText));
+    }
+    CachingTokenFilter buffer = new CachingTokenFilter(source);
+    TermToBytesRefAttribute termAtt = null;
+    PositionIncrementAttribute posIncrAtt = null;
+    int numTokens = 0;
+
+    boolean success = false;
+    try {
+      buffer.reset();
+      success = true;
+    } catch (IOException e) {
+      // success==false if we hit an exception
+    }
+    if (success) {
+      if (buffer.hasAttribute(TermToBytesRefAttribute.class)) {
+        termAtt = buffer.getAttribute(TermToBytesRefAttribute.class);
+      }
+      if (buffer.hasAttribute(PositionIncrementAttribute.class)) {
+        posIncrAtt = buffer.getAttribute(PositionIncrementAttribute.class);
+      }
+    }
+
+    int positionCount = 0;
+    boolean severalTokensAtSamePosition = false;
+
+    boolean hasMoreTokens = false;
+    if (termAtt != null) {
+      try {
+        hasMoreTokens = buffer.incrementToken();
+        while (hasMoreTokens) {
+          numTokens++;
+          int positionIncrement = (posIncrAtt != null) ? posIncrAtt.getPositionIncrement() : 1;
+          if (positionIncrement != 0) {
+            positionCount += positionIncrement;
+          } else {
+            severalTokensAtSamePosition = true;
+          }
+          hasMoreTokens = buffer.incrementToken();
+        }
+      } catch (IOException e) {
+        // ignore
+      }
+    }
+    try {
+      // rewind the buffer stream
+      buffer.reset();
+
+      // close original stream - all tokens buffered
+      source.close();
+    }
+    catch (IOException e) {
+      // ignore
+    }
+
+    BytesRef bytes = termAtt == null ? null : termAtt.getBytesRef();
+
+    if (numTokens == 0)
+      return null;
+    else if (numTokens == 1) {
+      try {
+        boolean hasNext = buffer.incrementToken();
+        assert hasNext == true;
+        termAtt.fillBytesRef();
+      } catch (IOException e) {
+        // safe to ignore, because we know the number of tokens
+      }
+      return newTermQuery(new Term(field, new BytesRef(bytes)));
+    } else {
+      if (severalTokensAtSamePosition || (!quoted && !autoGeneratePhraseQueries)) {
+        if (positionCount == 1 || (!quoted && !autoGeneratePhraseQueries)) {
+          // no phrase query:
+          BooleanQuery q = newBooleanQuery(positionCount == 1);
+
+          BooleanClause.Occur occur = positionCount > 1 && operator == AND_OPERATOR ?
+            BooleanClause.Occur.MUST : BooleanClause.Occur.SHOULD;
+
+          for (int i = 0; i < numTokens; i++) {
+            try {
+              boolean hasNext = buffer.incrementToken();
+              assert hasNext == true;
+              termAtt.fillBytesRef();
+            } catch (IOException e) {
+              // safe to ignore, because we know the number of tokens
+            }
+            Query currentQuery = newTermQuery(
+                new Term(field, new BytesRef(bytes)));
+            q.add(currentQuery, occur);
+          }
+          return q;
+        }
+        else {
+          // phrase query:
+          MultiPhraseQuery mpq = newMultiPhraseQuery();
+          mpq.setSlop(phraseSlop);
+          List<Term> multiTerms = new ArrayList<Term>();
+          int position = -1;
+          for (int i = 0; i < numTokens; i++) {
+            int positionIncrement = 1;
+            try {
+              boolean hasNext = buffer.incrementToken();
+              assert hasNext == true;
+              termAtt.fillBytesRef();
+              if (posIncrAtt != null) {
+                positionIncrement = posIncrAtt.getPositionIncrement();
+              }
+            } catch (IOException e) {
+              // safe to ignore, because we know the number of tokens
+            }
+
+            if (positionIncrement > 0 && multiTerms.size() > 0) {
+              if (enablePositionIncrements) {
+                mpq.add(multiTerms.toArray(new Term[0]),position);
+              } else {
+                mpq.add(multiTerms.toArray(new Term[0]));
+              }
+              multiTerms.clear();
+            }
+            position += positionIncrement;
+            multiTerms.add(new Term(field, new BytesRef(bytes)));
+          }
+          if (enablePositionIncrements) {
+            mpq.add(multiTerms.toArray(new Term[0]),position);
+          } else {
+            mpq.add(multiTerms.toArray(new Term[0]));
+          }
+          return mpq;
+        }
+      }
+      else {
+        PhraseQuery pq = newPhraseQuery();
+        pq.setSlop(phraseSlop);
+        int position = -1;
+
+        for (int i = 0; i < numTokens; i++) {
+          int positionIncrement = 1;
+
+          try {
+            boolean hasNext = buffer.incrementToken();
+            assert hasNext == true;
+            termAtt.fillBytesRef();
+            if (posIncrAtt != null) {
+              positionIncrement = posIncrAtt.getPositionIncrement();
+            }
+          } catch (IOException e) {
+            // safe to ignore, because we know the number of tokens
+          }
+
+          if (enablePositionIncrements) {
+            position += positionIncrement;
+            pq.add(new Term(field, new BytesRef(bytes)),position);
+          } else {
+            pq.add(new Term(field, new BytesRef(bytes)));
+          }
+        }
+        return pq;
+      }
+    }
+  }
+
+
+
+  /**
+   * Base implementation delegates to {@link #getFieldQuery(String,String,boolean)}.
+   * This method may be overridden, for example, to return
+   * a SpanNearQuery instead of a PhraseQuery.
+   *
+   * @exception org.apache.lucene.queryparser.classic.ParseException throw in overridden method to disallow
+   */
+  protected Query getFieldQuery(String field, String queryText, int slop)
+        throws ParseException {
+    Query query = getFieldQuery(field, queryText, true);
+
+    if (query instanceof PhraseQuery) {
+      ((PhraseQuery) query).setSlop(slop);
+    }
+    if (query instanceof MultiPhraseQuery) {
+      ((MultiPhraseQuery) query).setSlop(slop);
+    }
+
+    return query;
+  }
+
+  /**
+   *
+   * @exception org.apache.lucene.queryparser.classic.ParseException
+   */
+  protected Query getRangeQuery(String field,
+                                String part1,
+                                String part2,
+                                boolean startInclusive,
+                                boolean endInclusive) throws ParseException
+  {
+    if (lowercaseExpandedTerms) {
+      part1 = part1==null ? null : part1.toLowerCase();
+      part2 = part2==null ? null : part2.toLowerCase();
+    }
+
+
+    DateFormat df = DateFormat.getDateInstance(DateFormat.SHORT, locale);
+    df.setLenient(true);
+    DateTools.Resolution resolution = getDateResolution(field);
+    
+    try {
+      part1 = DateTools.dateToString(df.parse(part1), resolution);
+    } catch (Exception e) { }
+
+    try {
+      Date d2 = df.parse(part2);
+      if (endInclusive) {
+        // The user can only specify the date, not the time, so make sure
+        // the time is set to the latest possible time of that date to really
+        // include all documents:
+        Calendar cal = Calendar.getInstance(locale);
+        cal.setTime(d2);
+        cal.set(Calendar.HOUR_OF_DAY, 23);
+        cal.set(Calendar.MINUTE, 59);
+        cal.set(Calendar.SECOND, 59);
+        cal.set(Calendar.MILLISECOND, 999);
+        d2 = cal.getTime();
+      }
+      part2 = DateTools.dateToString(d2, resolution);
+    } catch (Exception e) { }
+
+    return newRangeQuery(field, part1, part2, startInclusive, endInclusive);
+  }
+
+ /**
+  * Builds a new BooleanQuery instance
+  * @param disableCoord disable coord
+  * @return new BooleanQuery instance
+  */
+  protected BooleanQuery newBooleanQuery(boolean disableCoord) {
+    return new BooleanQuery(disableCoord);
+  }
+
+ /**
+  * Builds a new BooleanClause instance
+  * @param q sub query
+  * @param occur how this clause should occur when matching documents
+  * @return new BooleanClause instance
+  */
+  protected BooleanClause newBooleanClause(Query q, BooleanClause.Occur occur) {
+    return new BooleanClause(q, occur);
+  }
+
+  /**
+   * Builds a new TermQuery instance
+   * @param term term
+   * @return new TermQuery instance
+   */
+  protected Query newTermQuery(Term term){
+    return new TermQuery(term);
+  }
+
+  /**
+   * Builds a new PhraseQuery instance
+   * @return new PhraseQuery instance
+   */
+  protected PhraseQuery newPhraseQuery(){
+    return new PhraseQuery();
+  }
+
+  /**
+   * Builds a new MultiPhraseQuery instance
+   * @return new MultiPhraseQuery instance
+   */
+  protected MultiPhraseQuery newMultiPhraseQuery(){
+    return new MultiPhraseQuery();
+  }
+
+  /**
+   * Builds a new PrefixQuery instance
+   * @param prefix Prefix term
+   * @return new PrefixQuery instance
+   */
+  protected Query newPrefixQuery(Term prefix){
+    PrefixQuery query = new PrefixQuery(prefix);
+    query.setRewriteMethod(multiTermRewriteMethod);
+    return query;
+  }
+
+  /**
+   * Builds a new RegexpQuery instance
+   * @param regexp Regexp term
+   * @return new RegexpQuery instance
+   */
+  protected Query newRegexpQuery(Term regexp) {
+    RegexpQuery query = new RegexpQuery(regexp);
+    query.setRewriteMethod(multiTermRewriteMethod);
+    return query;
+  }
+
+  /**
+   * Builds a new FuzzyQuery instance
+   * @param term Term
+   * @param minimumSimilarity minimum similarity
+   * @param prefixLength prefix length
+   * @return new FuzzyQuery Instance
+   */
+  protected Query newFuzzyQuery(Term term, float minimumSimilarity, int prefixLength) {
+    // FuzzyQuery doesn't yet allow constant score rewrite
+    return new FuzzyQuery(term,minimumSimilarity,prefixLength);
+  }
+
+  private BytesRef analyzeRangePart(String field, String part) {
+    TokenStream source;
+      
+    try {
+      source = analyzer.reusableTokenStream(field, new StringReader(part));
+      source.reset();
+    } catch (IOException e) {
+      source = analyzer.tokenStream(field, new StringReader(part));
+    }
+      
+    TermToBytesRefAttribute termAtt = source.getAttribute(TermToBytesRefAttribute.class);
+    BytesRef bytes = termAtt.getBytesRef();
+
+    try {
+      if (!source.incrementToken())
+        throw new IllegalArgumentException("analyzer returned no terms for range part: " + part);
+      termAtt.fillBytesRef();
+      if (source.incrementToken())
+        throw new IllegalArgumentException("analyzer returned too many terms for range part: " + part);
+    } catch (IOException e) {
+      throw new RuntimeException("error analyzing range part: " + part, e);
+    }
+      
+    try {
+      source.end();
+      source.close();
+    } catch (IOException ignored) {}
+    
+    return new BytesRef(bytes);
+  }
+
+  /**
+   * Builds a new TermRangeQuery instance
+   * @param field Field
+   * @param part1 min
+   * @param part2 max
+   * @param startInclusive true if the start of the range is inclusive
+   * @param endInclusive true if the end of the range is inclusive
+   * @return new TermRangeQuery instance
+   */
+  protected Query newRangeQuery(String field, String part1, String part2, boolean startInclusive, boolean endInclusive) {
+    final BytesRef start;
+    final BytesRef end;
+     
+    if (part1 == null) {
+      start = null;
+    } else {
+      start = analyzeRangeTerms ? analyzeRangePart(field, part1) : new BytesRef(part1);
+    }
+     
+    if (part2 == null) {
+      end = null;
+    } else {
+      end = analyzeRangeTerms ? analyzeRangePart(field, part2) : new BytesRef(part2);
+    }
+      
+    final TermRangeQuery query = new TermRangeQuery(field, start, end, startInclusive, endInclusive);
+
+    query.setRewriteMethod(multiTermRewriteMethod);
+    return query;
+  }
+
+  /**
+   * Builds a new MatchAllDocsQuery instance
+   * @return new MatchAllDocsQuery instance
+   */
+  protected Query newMatchAllDocsQuery() {
+    return new MatchAllDocsQuery();
+  }
+
+  /**
+   * Builds a new WildcardQuery instance
+   * @param t wildcard term
+   * @return new WildcardQuery instance
+   */
+  protected Query newWildcardQuery(Term t) {
+    WildcardQuery query = new WildcardQuery(t);
+    query.setRewriteMethod(multiTermRewriteMethod);
+    return query;
+  }
+
+  /**
+   * Factory method for generating query, given a set of clauses.
+   * By default creates a boolean query composed of clauses passed in.
+   *
+   * Can be overridden by extending classes, to modify query being
+   * returned.
+   *
+   * @param clauses List that contains {@link org.apache.lucene.search.BooleanClause} instances
+   *    to join.
+   *
+   * @return Resulting {@link org.apache.lucene.search.Query} object.
+   * @exception org.apache.lucene.queryparser.classic.ParseException throw in overridden method to disallow
+   */
+  protected Query getBooleanQuery(List<BooleanClause> clauses) throws ParseException {
+    return getBooleanQuery(clauses, false);
+  }
+
+  /**
+   * Factory method for generating query, given a set of clauses.
+   * By default creates a boolean query composed of clauses passed in.
+   *
+   * Can be overridden by extending classes, to modify query being
+   * returned.
+   *
+   * @param clauses List that contains {@link org.apache.lucene.search.BooleanClause} instances
+   *    to join.
+   * @param disableCoord true if coord scoring should be disabled.
+   *
+   * @return Resulting {@link org.apache.lucene.search.Query} object.
+   * @exception org.apache.lucene.queryparser.classic.ParseException throw in overridden method to disallow
+   */
+  protected Query getBooleanQuery(List<BooleanClause> clauses, boolean disableCoord)
+    throws ParseException
+  {
+    if (clauses.size()==0) {
+      return null; // all clause words were filtered away by the analyzer.
+    }
+    BooleanQuery query = newBooleanQuery(disableCoord);
+    for(final BooleanClause clause: clauses) {
+      query.add(clause);
+    }
+    return query;
+  }
+
+  /**
+   * Factory method for generating a query. Called when parser
+   * parses an input term token that contains one or more wildcard
+   * characters (? and *), but is not a prefix term token (one
+   * that has just a single * character at the end)
+   *<p>
+   * Depending on settings, prefix term may be lower-cased
+   * automatically. It will not go through the default Analyzer,
+   * however, since normal Analyzers are unlikely to work properly
+   * with wildcard templates.
+   *<p>
+   * Can be overridden by extending classes, to provide custom handling for
+   * wildcard queries, which may be necessary due to missing analyzer calls.
+   *
+   * @param field Name of the field query will use.
+   * @param termStr Term token that contains one or more wild card
+   *   characters (? or *), but is not simple prefix term
+   *
+   * @return Resulting {@link org.apache.lucene.search.Query} built for the term
+   * @exception org.apache.lucene.queryparser.classic.ParseException throw in overridden method to disallow
+   */
+  protected Query getWildcardQuery(String field, String termStr) throws ParseException
+  {
+    if ("*".equals(field)) {
+      if ("*".equals(termStr)) return newMatchAllDocsQuery();
+    }
+    if (!allowLeadingWildcard && (termStr.startsWith("*") || termStr.startsWith("?")))
+      throw new ParseException("'*' or '?' not allowed as first character in WildcardQuery");
+    if (lowercaseExpandedTerms) {
+      termStr = termStr.toLowerCase();
+    }
+    Term t = new Term(field, termStr);
+    return newWildcardQuery(t);
+  }
+
+  /**
+   * Factory method for generating a query. Called when parser
+   * parses an input term token that contains a regular expression
+   * query.
+   *<p>
+   * Depending on settings, pattern term may be lower-cased
+   * automatically. It will not go through the default Analyzer,
+   * however, since normal Analyzers are unlikely to work properly
+   * with regular expression templates.
+   *<p>
+   * Can be overridden by extending classes, to provide custom handling for
+   * regular expression queries, which may be necessary due to missing analyzer
+   * calls.
+   *
+   * @param field Name of the field query will use.
+   * @param termStr Term token that contains a regular expression
+   *
+   * @return Resulting {@link org.apache.lucene.search.Query} built for the term
+   * @exception org.apache.lucene.queryparser.classic.ParseException throw in overridden method to disallow
+   */
+  protected Query getRegexpQuery(String field, String termStr) throws ParseException
+  {
+    if (lowercaseExpandedTerms) {
+      termStr = termStr.toLowerCase();
+    }
+    Term t = new Term(field, termStr);
+    return newRegexpQuery(t);
+  }
+
+  /**
+   * Factory method for generating a query (similar to
+   * {@link #getWildcardQuery}). Called when parser parses an input term
+   * token that uses prefix notation; that is, contains a single '*' wildcard
+   * character as its last character. Since this is a special case
+   * of generic wildcard term, and such a query can be optimized easily,
+   * this usually results in a different query object.
+   *<p>
+   * Depending on settings, a prefix term may be lower-cased
+   * automatically. It will not go through the default Analyzer,
+   * however, since normal Analyzers are unlikely to work properly
+   * with wildcard templates.
+   *<p>
+   * Can be overridden by extending classes, to provide custom handling for
+   * wild card queries, which may be necessary due to missing analyzer calls.
+   *
+   * @param field Name of the field query will use.
+   * @param termStr Term token to use for building term for the query
+   *    (<b>without</b> trailing '*' character!)
+   *
+   * @return Resulting {@link org.apache.lucene.search.Query} built for the term
+   * @exception org.apache.lucene.queryparser.classic.ParseException throw in overridden method to disallow
+   */
+  protected Query getPrefixQuery(String field, String termStr) throws ParseException
+  {
+    if (!allowLeadingWildcard && termStr.startsWith("*"))
+      throw new ParseException("'*' not allowed as first character in PrefixQuery");
+    if (lowercaseExpandedTerms) {
+      termStr = termStr.toLowerCase();
+    }
+    Term t = new Term(field, termStr);
+    return newPrefixQuery(t);
+  }
+
+   /**
+   * Factory method for generating a query (similar to
+   * {@link #getWildcardQuery}). Called when parser parses
+   * an input term token that has the fuzzy suffix (~) appended.
+   *
+   * @param field Name of the field query will use.
+   * @param termStr Term token to use for building term for the query
+   *
+   * @return Resulting {@link org.apache.lucene.search.Query} built for the term
+   * @exception org.apache.lucene.queryparser.classic.ParseException throw in overridden method to disallow
+   */
+  protected Query getFuzzyQuery(String field, String termStr, float minSimilarity) throws ParseException
+  {
+    if (lowercaseExpandedTerms) {
+      termStr = termStr.toLowerCase();
+    }
+    Term t = new Term(field, termStr);
+    return newFuzzyQuery(t, minSimilarity, fuzzyPrefixLength);
+  }
+
+
+   // extracted from the .jj grammar
+  Query handleBareTokenQuery(String qfield, Token term, Token fuzzySlop, boolean prefix, boolean wildcard, boolean fuzzy, boolean regexp) throws ParseException {
+    Query q;
+
+    String termImage=discardEscapeChar(term.image);
+    if (wildcard) {
+      q = getWildcardQuery(qfield, term.image);
+    } else if (prefix) {
+      q = getPrefixQuery(qfield,
+          discardEscapeChar(term.image.substring
+              (0, term.image.length()-1)));
+    } else if (regexp) {
+      q = getRegexpQuery(qfield, term.image.substring(1, term.image.length()-1));
+    } else if (fuzzy) {
+      float fms = fuzzyMinSim;
+      try {
+        fms = Float.valueOf(fuzzySlop.image.substring(1)).floatValue();
+      } catch (Exception ignored) { }
+      if(fms < 0.0f){
+        throw new ParseException("Minimum similarity for a FuzzyQuery has to be between 0.0f and 1.0f !");
+      } else if (fms >= 1.0f && fms != (int) fms) {
+        throw new ParseException("Fractional edit distances are not allowed!");
+      }
+      q = getFuzzyQuery(qfield, termImage, fms);
+    } else {
+      q = getFieldQuery(qfield, termImage, false);
+    }
+    return q;
+  }
+
+  // extracted from the .jj grammar
+  Query handleQuotedTerm(String qfield, Token term, Token fuzzySlop) throws ParseException {
+    int s = phraseSlop;  // default
+    if (fuzzySlop != null) {
+      try {
+        s = Float.valueOf(fuzzySlop.image.substring(1)).intValue();
+      }
+      catch (Exception ignored) { }
+    }
+    return getFieldQuery(qfield, discardEscapeChar(term.image.substring(1, term.image.length()-1)), s);
+  }
+
+  // extracted from the .jj grammar
+  Query handleBoost(Query q, Token boost) throws ParseException {
+    if (boost != null) {
+      float f = (float) 1.0;
+      try {
+        f = Float.valueOf(boost.image).floatValue();
+      }
+      catch (Exception ignored) {
+    /* Should this be handled somehow? (defaults to "no boost", if
+     * boost number is invalid)
+     */
+      }
+
+      // avoid boosting null queries, such as those caused by stop words
+      if (q != null) {
+        q.setBoost(f);
+      }
+    }
+    return q;
+  }
+
+
+
+  /**
+   * Returns a String where the escape char has been
+   * removed, or kept only once if there was a double escape.
+   *
+   * Supports escaped unicode characters, e. g. translates
+   * <code>\\u0041</code> to <code>A</code>.
+   *
+   */
+  String discardEscapeChar(String input) throws ParseException {
+    // Create char array to hold unescaped char sequence
+    char[] output = new char[input.length()];
+
+    // The length of the output can be less than the input
+    // due to discarded escape chars. This variable holds
+    // the actual length of the output
+    int length = 0;
+
+    // We remember whether the last processed character was
+    // an escape character
+    boolean lastCharWasEscapeChar = false;
+
+    // The multiplier the current unicode digit must be multiplied with.
+    // E. g. the first digit must be multiplied with 16^3, the second with 16^2...
+    int codePointMultiplier = 0;
+
+    // Used to calculate the codepoint of the escaped unicode character
+    int codePoint = 0;
+
+    for (int i = 0; i < input.length(); i++) {
+      char curChar = input.charAt(i);
+      if (codePointMultiplier > 0) {
+        codePoint += hexToInt(curChar) * codePointMultiplier;
+        codePointMultiplier >>>= 4;
+        if (codePointMultiplier == 0) {
+          output[length++] = (char)codePoint;
+          codePoint = 0;
+        }
+      } else if (lastCharWasEscapeChar) {
+        if (curChar == 'u') {
+          // found an escaped unicode character
+          codePointMultiplier = 16 * 16 * 16;
+        } else {
+          // this character was escaped
+          output[length] = curChar;
+          length++;
+        }
+        lastCharWasEscapeChar = false;
+      } else {
+        if (curChar == '\\') {
+          lastCharWasEscapeChar = true;
+        } else {
+          output[length] = curChar;
+          length++;
+        }
+      }
+    }
+
+    if (codePointMultiplier > 0) {
+      throw new ParseException("Truncated unicode escape sequence.");
+    }
+
+    if (lastCharWasEscapeChar) {
+      throw new ParseException("Term can not end with escape character.");
+    }
+
+    return new String(output, 0, length);
+  }
+
+  /** Returns the numeric value of the hexadecimal character */
+  static final int hexToInt(char c) throws ParseException {
+    if ('0' <= c && c <= '9') {
+      return c - '0';
+    } else if ('a' <= c && c <= 'f'){
+      return c - 'a' + 10;
+    } else if ('A' <= c && c <= 'F') {
+      return c - 'A' + 10;
+    } else {
+      throw new ParseException("None-hex character in unicode escape sequence: " + c);
+    }
+  }
+
+  /**
+   * Returns a String where those characters that QueryParser
+   * expects to be escaped are escaped by a preceding <code>\</code>.
+   */
+  public static String escape(String s) {
+    StringBuilder sb = new StringBuilder();
+    for (int i = 0; i < s.length(); i++) {
+      char c = s.charAt(i);
+      // These characters are part of the query syntax and must be escaped
+      if (c == '\\' || c == '+' || c == '-' || c == '!' || c == '(' || c == ')' || c == ':'
+        || c == '^' || c == '[' || c == ']' || c == '\"' || c == '{' || c == '}' || c == '~'
+        || c == '*' || c == '?' || c == '|' || c == '&') {
+        sb.append('\\');
+      }
+      sb.append(c);
+    }
+    return sb.toString();
+  }
+
+}
diff --git a/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserConstants.java b/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserConstants.java
new file mode 100644
index 0000000..02a5b5f
--- /dev/null
+++ b/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserConstants.java
@@ -0,0 +1,120 @@
+/* Generated By:JavaCC: Do not edit this line. QueryParserConstants.java */
+package org.apache.lucene.queryparser.classic;
+
+
+/**
+ * Token literal values and constants.
+ * Generated by org.javacc.parser.OtherFilesGen#start()
+ */
+public interface QueryParserConstants {
+
+  /** End of File. */
+  int EOF = 0;
+  /** RegularExpression Id. */
+  int _NUM_CHAR = 1;
+  /** RegularExpression Id. */
+  int _ESCAPED_CHAR = 2;
+  /** RegularExpression Id. */
+  int _TERM_START_CHAR = 3;
+  /** RegularExpression Id. */
+  int _TERM_CHAR = 4;
+  /** RegularExpression Id. */
+  int _WHITESPACE = 5;
+  /** RegularExpression Id. */
+  int _QUOTED_CHAR = 6;
+  /** RegularExpression Id. */
+  int AND = 8;
+  /** RegularExpression Id. */
+  int OR = 9;
+  /** RegularExpression Id. */
+  int NOT = 10;
+  /** RegularExpression Id. */
+  int PLUS = 11;
+  /** RegularExpression Id. */
+  int MINUS = 12;
+  /** RegularExpression Id. */
+  int BAREOPER = 13;
+  /** RegularExpression Id. */
+  int LPAREN = 14;
+  /** RegularExpression Id. */
+  int RPAREN = 15;
+  /** RegularExpression Id. */
+  int COLON = 16;
+  /** RegularExpression Id. */
+  int STAR = 17;
+  /** RegularExpression Id. */
+  int CARAT = 18;
+  /** RegularExpression Id. */
+  int QUOTED = 19;
+  /** RegularExpression Id. */
+  int TERM = 20;
+  /** RegularExpression Id. */
+  int FUZZY_SLOP = 21;
+  /** RegularExpression Id. */
+  int PREFIXTERM = 22;
+  /** RegularExpression Id. */
+  int WILDTERM = 23;
+  /** RegularExpression Id. */
+  int REGEXPTERM = 24;
+  /** RegularExpression Id. */
+  int RANGEIN_START = 25;
+  /** RegularExpression Id. */
+  int RANGEEX_START = 26;
+  /** RegularExpression Id. */
+  int NUMBER = 27;
+  /** RegularExpression Id. */
+  int RANGE_TO = 28;
+  /** RegularExpression Id. */
+  int RANGEIN_END = 29;
+  /** RegularExpression Id. */
+  int RANGEEX_END = 30;
+  /** RegularExpression Id. */
+  int RANGE_QUOTED = 31;
+  /** RegularExpression Id. */
+  int RANGE_GOOP = 32;
+
+  /** Lexical state. */
+  int Boost = 0;
+  /** Lexical state. */
+  int Range = 1;
+  /** Lexical state. */
+  int DEFAULT = 2;
+
+  /** Literal token values. */
+  String[] tokenImage = {
+    "<EOF>",
+    "<_NUM_CHAR>",
+    "<_ESCAPED_CHAR>",
+    "<_TERM_START_CHAR>",
+    "<_TERM_CHAR>",
+    "<_WHITESPACE>",
+    "<_QUOTED_CHAR>",
+    "<token of kind 7>",
+    "<AND>",
+    "<OR>",
+    "<NOT>",
+    "\"+\"",
+    "\"-\"",
+    "<BAREOPER>",
+    "\"(\"",
+    "\")\"",
+    "\":\"",
+    "\"*\"",
+    "\"^\"",
+    "<QUOTED>",
+    "<TERM>",
+    "<FUZZY_SLOP>",
+    "<PREFIXTERM>",
+    "<WILDTERM>",
+    "<REGEXPTERM>",
+    "\"[\"",
+    "\"{\"",
+    "<NUMBER>",
+    "\"TO\"",
+    "\"]\"",
+    "\"}\"",
+    "<RANGE_QUOTED>",
+    "<RANGE_GOOP>",
+  };
+
+}
diff --git a/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserTokenManager.java b/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserTokenManager.java
new file mode 100644
index 0000000..ba27851
--- /dev/null
+++ b/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserTokenManager.java
@@ -0,0 +1,1079 @@
+/* Generated By:JavaCC: Do not edit this line. QueryParserTokenManager.java */
+package org.apache.lucene.queryparser.classic;
+import java.io.StringReader;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Locale;
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.document.DateTools;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermRangeQuery;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.util.Version;
+
+/** Token Manager. */
+public class QueryParserTokenManager implements QueryParserConstants
+{
+
+  /** Debug output. */
+  public  java.io.PrintStream debugStream = System.out;
+  /** Set debug output. */
+  public  void setDebugStream(java.io.PrintStream ds) { debugStream = ds; }
+private final int jjStopStringLiteralDfa_2(int pos, long active0)
+{
+   switch (pos)
+   {
+      default :
+         return -1;
+   }
+}
+private final int jjStartNfa_2(int pos, long active0)
+{
+   return jjMoveNfa_2(jjStopStringLiteralDfa_2(pos, active0), pos + 1);
+}
+private int jjStopAtPos(int pos, int kind)
+{
+   jjmatchedKind = kind;
+   jjmatchedPos = pos;
+   return pos + 1;
+}
+private int jjMoveStringLiteralDfa0_2()
+{
+   switch(curChar)
+   {
+      case 40:
+         return jjStopAtPos(0, 14);
+      case 41:
+         return jjStopAtPos(0, 15);
+      case 42:
+         return jjStartNfaWithStates_2(0, 17, 43);
+      case 43:
+         return jjStartNfaWithStates_2(0, 11, 15);
+      case 45:
+         return jjStartNfaWithStates_2(0, 12, 15);
+      case 58:
+         return jjStopAtPos(0, 16);
+      case 91:
+         return jjStopAtPos(0, 25);
+      case 94:
+         return jjStopAtPos(0, 18);
+      case 123:
+         return jjStopAtPos(0, 26);
+      default :
+         return jjMoveNfa_2(0, 0);
+   }
+}
+private int jjStartNfaWithStates_2(int pos, int kind, int state)
+{
+   jjmatchedKind = kind;
+   jjmatchedPos = pos;
+   try { curChar = input_stream.readChar(); }
+   catch(java.io.IOException e) { return pos + 1; }
+   return jjMoveNfa_2(state, pos + 1);
+}
+static final long[] jjbitVec0 = {
+   0x1L, 0x0L, 0x0L, 0x0L
+};
+static final long[] jjbitVec1 = {
+   0xfffffffffffffffeL, 0xffffffffffffffffL, 0xffffffffffffffffL, 0xffffffffffffffffL
+};
+static final long[] jjbitVec3 = {
+   0x0L, 0x0L, 0xffffffffffffffffL, 0xffffffffffffffffL
+};
+static final long[] jjbitVec4 = {
+   0xfffefffffffffffeL, 0xffffffffffffffffL, 0xffffffffffffffffL, 0xffffffffffffffffL
+};
+private int jjMoveNfa_2(int startState, int curPos)
+{
+   int startsAt = 0;
+   jjnewStateCnt = 43;
+   int i = 1;
+   jjstateSet[0] = startState;
+   int kind = 0x7fffffff;
+   for (;;)
+   {
+      if (++jjround == 0x7fffffff)
+         ReInitRounds();
+      if (curChar < 64)
+      {
+         long l = 1L << curChar;
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+                  if ((0xfbffd4f8ffffd9ffL & l) != 0L)
+                  {
+                     if (kind > 23)
+                        kind = 23;
+                     jjCheckNAddTwoStates(27, 28);
+                  }
+                  else if ((0x100002600L & l) != 0L)
+                  {
+                     if (kind > 7)
+                        kind = 7;
+                  }
+                  else if ((0x280200000000L & l) != 0L)
+                     jjstateSet[jjnewStateCnt++] = 15;
+                  else if (curChar == 34)
+                     jjCheckNAddStates(0, 2);
+                  if ((0x7bffd0f8ffffd9ffL & l) != 0L)
+                  {
+                     if (kind > 20)
+                        kind = 20;
+                     jjCheckNAddStates(3, 7);
+                  }
+                  else if (curChar == 42)
+                  {
+                     if (kind > 22)
+                        kind = 22;
+                  }
+                  else if (curChar == 33)
+                  {
+                     if (kind > 10)
+                        kind = 10;
+                  }
+                  if (curChar == 47)
+                     jjCheckNAddStates(8, 10);
+                  else if (curChar == 38)
+                     jjstateSet[jjnewStateCnt++] = 4;
+                  break;
+               case 43:
+               case 27:
+                  if ((0xfbfffcf8ffffd9ffL & l) == 0L)
+                     break;
+                  if (kind > 23)
+                     kind = 23;
+                  jjCheckNAddTwoStates(27, 28);
+                  break;
+               case 4:
+                  if (curChar == 38 && kind > 8)
+                     kind = 8;
+                  break;
+               case 5:
+                  if (curChar == 38)
+                     jjstateSet[jjnewStateCnt++] = 4;
+                  break;
+               case 13:
+                  if (curChar == 33 && kind > 10)
+                     kind = 10;
+                  break;
+               case 14:
+                  if ((0x280200000000L & l) != 0L)
+                     jjstateSet[jjnewStateCnt++] = 15;
+                  break;
+               case 15:
+                  if ((0x100002600L & l) != 0L && kind > 13)
+                     kind = 13;
+                  break;
+               case 16:
+                  if (curChar == 34)
+                     jjCheckNAddStates(0, 2);
+                  break;
+               case 17:
+                  if ((0xfffffffbffffffffL & l) != 0L)
+                     jjCheckNAddStates(0, 2);
+                  break;
+               case 19:
+                  jjCheckNAddStates(0, 2);
+                  break;
+               case 20:
+                  if (curChar == 34 && kind > 19)
+                     kind = 19;
+                  break;
+               case 22:
+                  if ((0x3ff000000000000L & l) == 0L)
+                     break;
+                  if (kind > 21)
+                     kind = 21;
+                  jjAddStates(11, 12);
+                  break;
+               case 23:
+                  if (curChar == 46)
+                     jjCheckNAdd(24);
+                  break;
+               case 24:
+                  if ((0x3ff000000000000L & l) == 0L)
+                     break;
+                  if (kind > 21)
+                     kind = 21;
+                  jjCheckNAdd(24);
+                  break;
+               case 25:
+                  if (curChar == 42 && kind > 22)
+                     kind = 22;
+                  break;
+               case 26:
+                  if ((0xfbffd4f8ffffd9ffL & l) == 0L)
+                     break;
+                  if (kind > 23)
+                     kind = 23;
+                  jjCheckNAddTwoStates(27, 28);
+                  break;
+               case 29:
+                  if (kind > 23)
+                     kind = 23;
+                  jjCheckNAddTwoStates(27, 28);
+                  break;
+               case 30:
+               case 32:
+                  if (curChar == 47)
+                     jjCheckNAddStates(8, 10);
+                  break;
+               case 31:
+                  if ((0xffff7fffffffffffL & l) != 0L)
+                     jjCheckNAddStates(8, 10);
+                  break;
+               case 34:
+                  if (curChar == 47 && kind > 24)
+                     kind = 24;
+                  break;
+               case 35:
+                  if ((0x7bffd0f8ffffd9ffL & l) == 0L)
+                     break;
+                  if (kind > 20)
+                     kind = 20;
+                  jjCheckNAddStates(3, 7);
+                  break;
+               case 36:
+                  if ((0x7bfff8f8ffffd9ffL & l) == 0L)
+                     break;
+                  if (kind > 20)
+                     kind = 20;
+                  jjCheckNAddTwoStates(36, 37);
+                  break;
+               case 38:
+                  if (kind > 20)
+                     kind = 20;
+                  jjCheckNAddTwoStates(36, 37);
+                  break;
+               case 39:
+                  if ((0x7bfff8f8ffffd9ffL & l) != 0L)
+                     jjCheckNAddStates(13, 15);
+                  break;
+               case 41:
+                  jjCheckNAddStates(13, 15);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else if (curChar < 128)
+      {
+         long l = 1L << (curChar & 077);
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+                  if ((0x97ffffff87ffffffL & l) != 0L)
+                  {
+                     if (kind > 20)
+                        kind = 20;
+                     jjCheckNAddStates(3, 7);
+                  }
+                  else if (curChar == 92)
+                     jjCheckNAddStates(16, 18);
+                  else if (curChar == 126)
+                  {
+                     if (kind > 21)
+                        kind = 21;
+                     jjstateSet[jjnewStateCnt++] = 22;
+                  }
+                  if ((0x97ffffff87ffffffL & l) != 0L)
+                  {
+                     if (kind > 23)
+                        kind = 23;
+                     jjCheckNAddTwoStates(27, 28);
+                  }
+                  if (curChar == 78)
+                     jjstateSet[jjnewStateCnt++] = 11;
+                  else if (curChar == 124)
+                     jjstateSet[jjnewStateCnt++] = 8;
+                  else if (curChar == 79)
+                     jjstateSet[jjnewStateCnt++] = 6;
+                  else if (curChar == 65)
+                     jjstateSet[jjnewStateCnt++] = 2;
+                  break;
+               case 43:
+                  if ((0x97ffffff87ffffffL & l) != 0L)
+                  {
+                     if (kind > 23)
+                        kind = 23;
+                     jjCheckNAddTwoStates(27, 28);
+                  }
+                  else if (curChar == 92)
+                     jjCheckNAddTwoStates(29, 29);
+                  break;
+               case 1:
+                  if (curChar == 68 && kind > 8)
+                     kind = 8;
+                  break;
+               case 2:
+                  if (curChar == 78)
+                     jjstateSet[jjnewStateCnt++] = 1;
+                  break;
+               case 3:
+                  if (curChar == 65)
+                     jjstateSet[jjnewStateCnt++] = 2;
+                  break;
+               case 6:
+                  if (curChar == 82 && kind > 9)
+                     kind = 9;
+                  break;
+               case 7:
+                  if (curChar == 79)
+                     jjstateSet[jjnewStateCnt++] = 6;
+                  break;
+               case 8:
+                  if (curChar == 124 && kind > 9)
+                     kind = 9;
+                  break;
+               case 9:
+                  if (curChar == 124)
+                     jjstateSet[jjnewStateCnt++] = 8;
+                  break;
+               case 10:
+                  if (curChar == 84 && kind > 10)
+                     kind = 10;
+                  break;
+               case 11:
+                  if (curChar == 79)
+                     jjstateSet[jjnewStateCnt++] = 10;
+                  break;
+               case 12:
+                  if (curChar == 78)
+                     jjstateSet[jjnewStateCnt++] = 11;
+                  break;
+               case 17:
+                  if ((0xffffffffefffffffL & l) != 0L)
+                     jjCheckNAddStates(0, 2);
+                  break;
+               case 18:
+                  if (curChar == 92)
+                     jjstateSet[jjnewStateCnt++] = 19;
+                  break;
+               case 19:
+                  jjCheckNAddStates(0, 2);
+                  break;
+               case 21:
+                  if (curChar != 126)
+                     break;
+                  if (kind > 21)
+                     kind = 21;
+                  jjstateSet[jjnewStateCnt++] = 22;
+                  break;
+               case 26:
+                  if ((0x97ffffff87ffffffL & l) == 0L)
+                     break;
+                  if (kind > 23)
+                     kind = 23;
+                  jjCheckNAddTwoStates(27, 28);
+                  break;
+               case 27:
+                  if ((0x97ffffff87ffffffL & l) == 0L)
+                     break;
+                  if (kind > 23)
+                     kind = 23;
+                  jjCheckNAddTwoStates(27, 28);
+                  break;
+               case 28:
+                  if (curChar == 92)
+                     jjCheckNAddTwoStates(29, 29);
+                  break;
+               case 29:
+                  if (kind > 23)
+                     kind = 23;
+                  jjCheckNAddTwoStates(27, 28);
+                  break;
+               case 31:
+                  jjAddStates(8, 10);
+                  break;
+               case 33:
+                  if (curChar == 92)
+                     jjstateSet[jjnewStateCnt++] = 32;
+                  break;
+               case 35:
+                  if ((0x97ffffff87ffffffL & l) == 0L)
+                     break;
+                  if (kind > 20)
+                     kind = 20;
+                  jjCheckNAddStates(3, 7);
+                  break;
+               case 36:
+                  if ((0x97ffffff87ffffffL & l) == 0L)
+                     break;
+                  if (kind > 20)
+                     kind = 20;
+                  jjCheckNAddTwoStates(36, 37);
+                  break;
+               case 37:
+                  if (curChar == 92)
+                     jjCheckNAddTwoStates(38, 38);
+                  break;
+               case 38:
+                  if (kind > 20)
+                     kind = 20;
+                  jjCheckNAddTwoStates(36, 37);
+                  break;
+               case 39:
+                  if ((0x97ffffff87ffffffL & l) != 0L)
+                     jjCheckNAddStates(13, 15);
+                  break;
+               case 40:
+                  if (curChar == 92)
+                     jjCheckNAddTwoStates(41, 41);
+                  break;
+               case 41:
+                  jjCheckNAddStates(13, 15);
+                  break;
+               case 42:
+                  if (curChar == 92)
+                     jjCheckNAddStates(16, 18);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else
+      {
+         int hiByte = (int)(curChar >> 8);
+         int i1 = hiByte >> 6;
+         long l1 = 1L << (hiByte & 077);
+         int i2 = (curChar & 0xff) >> 6;
+         long l2 = 1L << (curChar & 077);
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+                  if (jjCanMove_0(hiByte, i1, i2, l1, l2))
+                  {
+                     if (kind > 7)
+                        kind = 7;
+                  }
+                  if (jjCanMove_2(hiByte, i1, i2, l1, l2))
+                  {
+                     if (kind > 23)
+                        kind = 23;
+                     jjCheckNAddTwoStates(27, 28);
+                  }
+                  if (jjCanMove_2(hiByte, i1, i2, l1, l2))
+                  {
+                     if (kind > 20)
+                        kind = 20;
+                     jjCheckNAddStates(3, 7);
+                  }
+                  break;
+               case 43:
+               case 27:
+                  if (!jjCanMove_2(hiByte, i1, i2, l1, l2))
+                     break;
+                  if (kind > 23)
+                     kind = 23;
+                  jjCheckNAddTwoStates(27, 28);
+                  break;
+               case 15:
+                  if (jjCanMove_0(hiByte, i1, i2, l1, l2) && kind > 13)
+                     kind = 13;
+                  break;
+               case 17:
+               case 19:
+                  if (jjCanMove_1(hiByte, i1, i2, l1, l2))
+                     jjCheckNAddStates(0, 2);
+                  break;
+               case 26:
+                  if (!jjCanMove_2(hiByte, i1, i2, l1, l2))
+                     break;
+                  if (kind > 23)
+                     kind = 23;
+                  jjCheckNAddTwoStates(27, 28);
+                  break;
+               case 29:
+                  if (!jjCanMove_1(hiByte, i1, i2, l1, l2))
+                     break;
+                  if (kind > 23)
+                     kind = 23;
+                  jjCheckNAddTwoStates(27, 28);
+                  break;
+               case 31:
+                  if (jjCanMove_1(hiByte, i1, i2, l1, l2))
+                     jjAddStates(8, 10);
+                  break;
+               case 35:
+                  if (!jjCanMove_2(hiByte, i1, i2, l1, l2))
+                     break;
+                  if (kind > 20)
+                     kind = 20;
+                  jjCheckNAddStates(3, 7);
+                  break;
+               case 36:
+                  if (!jjCanMove_2(hiByte, i1, i2, l1, l2))
+                     break;
+                  if (kind > 20)
+                     kind = 20;
+                  jjCheckNAddTwoStates(36, 37);
+                  break;
+               case 38:
+                  if (!jjCanMove_1(hiByte, i1, i2, l1, l2))
+                     break;
+                  if (kind > 20)
+                     kind = 20;
+                  jjCheckNAddTwoStates(36, 37);
+                  break;
+               case 39:
+                  if (jjCanMove_2(hiByte, i1, i2, l1, l2))
+                     jjCheckNAddStates(13, 15);
+                  break;
+               case 41:
+                  if (jjCanMove_1(hiByte, i1, i2, l1, l2))
+                     jjCheckNAddStates(13, 15);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      if (kind != 0x7fffffff)
+      {
+         jjmatchedKind = kind;
+         jjmatchedPos = curPos;
+         kind = 0x7fffffff;
+      }
+      ++curPos;
+      if ((i = jjnewStateCnt) == (startsAt = 43 - (jjnewStateCnt = startsAt)))
+         return curPos;
+      try { curChar = input_stream.readChar(); }
+      catch(java.io.IOException e) { return curPos; }
+   }
+}
+private int jjMoveStringLiteralDfa0_0()
+{
+   return jjMoveNfa_0(0, 0);
+}
+private int jjMoveNfa_0(int startState, int curPos)
+{
+   int startsAt = 0;
+   jjnewStateCnt = 3;
+   int i = 1;
+   jjstateSet[0] = startState;
+   int kind = 0x7fffffff;
+   for (;;)
+   {
+      if (++jjround == 0x7fffffff)
+         ReInitRounds();
+      if (curChar < 64)
+      {
+         long l = 1L << curChar;
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+                  if ((0x3ff000000000000L & l) == 0L)
+                     break;
+                  if (kind > 27)
+                     kind = 27;
+                  jjAddStates(19, 20);
+                  break;
+               case 1:
+                  if (curChar == 46)
+                     jjCheckNAdd(2);
+                  break;
+               case 2:
+                  if ((0x3ff000000000000L & l) == 0L)
+                     break;
+                  if (kind > 27)
+                     kind = 27;
+                  jjCheckNAdd(2);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else if (curChar < 128)
+      {
+         long l = 1L << (curChar & 077);
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else
+      {
+         int hiByte = (int)(curChar >> 8);
+         int i1 = hiByte >> 6;
+         long l1 = 1L << (hiByte & 077);
+         int i2 = (curChar & 0xff) >> 6;
+         long l2 = 1L << (curChar & 077);
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      if (kind != 0x7fffffff)
+      {
+         jjmatchedKind = kind;
+         jjmatchedPos = curPos;
+         kind = 0x7fffffff;
+      }
+      ++curPos;
+      if ((i = jjnewStateCnt) == (startsAt = 3 - (jjnewStateCnt = startsAt)))
+         return curPos;
+      try { curChar = input_stream.readChar(); }
+      catch(java.io.IOException e) { return curPos; }
+   }
+}
+private final int jjStopStringLiteralDfa_1(int pos, long active0)
+{
+   switch (pos)
+   {
+      case 0:
+         if ((active0 & 0x10000000L) != 0L)
+         {
+            jjmatchedKind = 32;
+            return 6;
+         }
+         return -1;
+      default :
+         return -1;
+   }
+}
+private final int jjStartNfa_1(int pos, long active0)
+{
+   return jjMoveNfa_1(jjStopStringLiteralDfa_1(pos, active0), pos + 1);
+}
+private int jjMoveStringLiteralDfa0_1()
+{
+   switch(curChar)
+   {
+      case 84:
+         return jjMoveStringLiteralDfa1_1(0x10000000L);
+      case 93:
+         return jjStopAtPos(0, 29);
+      case 125:
+         return jjStopAtPos(0, 30);
+      default :
+         return jjMoveNfa_1(0, 0);
+   }
+}
+private int jjMoveStringLiteralDfa1_1(long active0)
+{
+   try { curChar = input_stream.readChar(); }
+   catch(java.io.IOException e) {
+      jjStopStringLiteralDfa_1(0, active0);
+      return 1;
+   }
+   switch(curChar)
+   {
+      case 79:
+         if ((active0 & 0x10000000L) != 0L)
+            return jjStartNfaWithStates_1(1, 28, 6);
+         break;
+      default :
+         break;
+   }
+   return jjStartNfa_1(0, active0);
+}
+private int jjStartNfaWithStates_1(int pos, int kind, int state)
+{
+   jjmatchedKind = kind;
+   jjmatchedPos = pos;
+   try { curChar = input_stream.readChar(); }
+   catch(java.io.IOException e) { return pos + 1; }
+   return jjMoveNfa_1(state, pos + 1);
+}
+private int jjMoveNfa_1(int startState, int curPos)
+{
+   int startsAt = 0;
+   jjnewStateCnt = 7;
+   int i = 1;
+   jjstateSet[0] = startState;
+   int kind = 0x7fffffff;
+   for (;;)
+   {
+      if (++jjround == 0x7fffffff)
+         ReInitRounds();
+      if (curChar < 64)
+      {
+         long l = 1L << curChar;
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+                  if ((0xfffffffeffffffffL & l) != 0L)
+                  {
+                     if (kind > 32)
+                        kind = 32;
+                     jjCheckNAdd(6);
+                  }
+                  if ((0x100002600L & l) != 0L)
+                  {
+                     if (kind > 7)
+                        kind = 7;
+                  }
+                  else if (curChar == 34)
+                     jjCheckNAddTwoStates(2, 4);
+                  break;
+               case 1:
+                  if (curChar == 34)
+                     jjCheckNAddTwoStates(2, 4);
+                  break;
+               case 2:
+                  if ((0xfffffffbffffffffL & l) != 0L)
+                     jjCheckNAddStates(21, 23);
+                  break;
+               case 3:
+                  if (curChar == 34)
+                     jjCheckNAddStates(21, 23);
+                  break;
+               case 5:
+                  if (curChar == 34 && kind > 31)
+                     kind = 31;
+                  break;
+               case 6:
+                  if ((0xfffffffeffffffffL & l) == 0L)
+                     break;
+                  if (kind > 32)
+                     kind = 32;
+                  jjCheckNAdd(6);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else if (curChar < 128)
+      {
+         long l = 1L << (curChar & 077);
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+               case 6:
+                  if ((0xdfffffffdfffffffL & l) == 0L)
+                     break;
+                  if (kind > 32)
+                     kind = 32;
+                  jjCheckNAdd(6);
+                  break;
+               case 2:
+                  jjAddStates(21, 23);
+                  break;
+               case 4:
+                  if (curChar == 92)
+                     jjstateSet[jjnewStateCnt++] = 3;
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else
+      {
+         int hiByte = (int)(curChar >> 8);
+         int i1 = hiByte >> 6;
+         long l1 = 1L << (hiByte & 077);
+         int i2 = (curChar & 0xff) >> 6;
+         long l2 = 1L << (curChar & 077);
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+                  if (jjCanMove_0(hiByte, i1, i2, l1, l2))
+                  {
+                     if (kind > 7)
+                        kind = 7;
+                  }
+                  if (jjCanMove_1(hiByte, i1, i2, l1, l2))
+                  {
+                     if (kind > 32)
+                        kind = 32;
+                     jjCheckNAdd(6);
+                  }
+                  break;
+               case 2:
+                  if (jjCanMove_1(hiByte, i1, i2, l1, l2))
+                     jjAddStates(21, 23);
+                  break;
+               case 6:
+                  if (!jjCanMove_1(hiByte, i1, i2, l1, l2))
+                     break;
+                  if (kind > 32)
+                     kind = 32;
+                  jjCheckNAdd(6);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      if (kind != 0x7fffffff)
+      {
+         jjmatchedKind = kind;
+         jjmatchedPos = curPos;
+         kind = 0x7fffffff;
+      }
+      ++curPos;
+      if ((i = jjnewStateCnt) == (startsAt = 7 - (jjnewStateCnt = startsAt)))
+         return curPos;
+      try { curChar = input_stream.readChar(); }
+      catch(java.io.IOException e) { return curPos; }
+   }
+}
+static final int[] jjnextStates = {
+   17, 18, 20, 36, 39, 25, 40, 37, 31, 33, 34, 22, 23, 39, 25, 40, 
+   38, 41, 29, 0, 1, 2, 4, 5, 
+};
+private static final boolean jjCanMove_0(int hiByte, int i1, int i2, long l1, long l2)
+{
+   switch(hiByte)
+   {
+      case 48:
+         return ((jjbitVec0[i2] & l2) != 0L);
+      default :
+         return false;
+   }
+}
+private static final boolean jjCanMove_1(int hiByte, int i1, int i2, long l1, long l2)
+{
+   switch(hiByte)
+   {
+      case 0:
+         return ((jjbitVec3[i2] & l2) != 0L);
+      default :
+         if ((jjbitVec1[i1] & l1) != 0L)
+            return true;
+         return false;
+   }
+}
+private static final boolean jjCanMove_2(int hiByte, int i1, int i2, long l1, long l2)
+{
+   switch(hiByte)
+   {
+      case 0:
+         return ((jjbitVec3[i2] & l2) != 0L);
+      case 48:
+         return ((jjbitVec1[i2] & l2) != 0L);
+      default :
+         if ((jjbitVec4[i1] & l1) != 0L)
+            return true;
+         return false;
+   }
+}
+
+/** Token literal values. */
+public static final String[] jjstrLiteralImages = {
+"", null, null, null, null, null, null, null, null, null, null, "\53", "\55", 
+null, "\50", "\51", "\72", "\52", "\136", null, null, null, null, null, null, 
+"\133", "\173", null, "\124\117", "\135", "\175", null, null, };
+
+/** Lexer state names. */
+public static final String[] lexStateNames = {
+   "Boost",
+   "Range",
+   "DEFAULT",
+};
+
+/** Lex State array. */
+public static final int[] jjnewLexState = {
+   -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 
+   1, 1, 2, -1, 2, 2, -1, -1, 
+};
+static final long[] jjtoToken = {
+   0x1ffffff01L, 
+};
+static final long[] jjtoSkip = {
+   0x80L, 
+};
+protected CharStream input_stream;
+private final int[] jjrounds = new int[43];
+private final int[] jjstateSet = new int[86];
+protected char curChar;
+/** Constructor. */
+public QueryParserTokenManager(CharStream stream){
+   input_stream = stream;
+}
+
+/** Constructor. */
+public QueryParserTokenManager(CharStream stream, int lexState){
+   this(stream);
+   SwitchTo(lexState);
+}
+
+/** Reinitialise parser. */
+public void ReInit(CharStream stream)
+{
+   jjmatchedPos = jjnewStateCnt = 0;
+   curLexState = defaultLexState;
+   input_stream = stream;
+   ReInitRounds();
+}
+private void ReInitRounds()
+{
+   int i;
+   jjround = 0x80000001;
+   for (i = 43; i-- > 0;)
+      jjrounds[i] = 0x80000000;
+}
+
+/** Reinitialise parser. */
+public void ReInit(CharStream stream, int lexState)
+{
+   ReInit(stream);
+   SwitchTo(lexState);
+}
+
+/** Switch to specified lex state. */
+public void SwitchTo(int lexState)
+{
+   if (lexState >= 3 || lexState < 0)
+      throw new TokenMgrError("Error: Ignoring invalid lexical state : " + lexState + ". State unchanged.", TokenMgrError.INVALID_LEXICAL_STATE);
+   else
+      curLexState = lexState;
+}
+
+protected Token jjFillToken()
+{
+   final Token t;
+   final String curTokenImage;
+   final int beginLine;
+   final int endLine;
+   final int beginColumn;
+   final int endColumn;
+   String im = jjstrLiteralImages[jjmatchedKind];
+   curTokenImage = (im == null) ? input_stream.GetImage() : im;
+   beginLine = input_stream.getBeginLine();
+   beginColumn = input_stream.getBeginColumn();
+   endLine = input_stream.getEndLine();
+   endColumn = input_stream.getEndColumn();
+   t = Token.newToken(jjmatchedKind, curTokenImage);
+
+   t.beginLine = beginLine;
+   t.endLine = endLine;
+   t.beginColumn = beginColumn;
+   t.endColumn = endColumn;
+
+   return t;
+}
+
+int curLexState = 2;
+int defaultLexState = 2;
+int jjnewStateCnt;
+int jjround;
+int jjmatchedPos;
+int jjmatchedKind;
+
+/** Get the next Token. */
+public Token getNextToken() 
+{
+  Token matchedToken;
+  int curPos = 0;
+
+  EOFLoop :
+  for (;;)
+  {
+   try
+   {
+      curChar = input_stream.BeginToken();
+   }
+   catch(java.io.IOException e)
+   {
+      jjmatchedKind = 0;
+      matchedToken = jjFillToken();
+      return matchedToken;
+   }
+
+   switch(curLexState)
+   {
+     case 0:
+       jjmatchedKind = 0x7fffffff;
+       jjmatchedPos = 0;
+       curPos = jjMoveStringLiteralDfa0_0();
+       break;
+     case 1:
+       jjmatchedKind = 0x7fffffff;
+       jjmatchedPos = 0;
+       curPos = jjMoveStringLiteralDfa0_1();
+       break;
+     case 2:
+       jjmatchedKind = 0x7fffffff;
+       jjmatchedPos = 0;
+       curPos = jjMoveStringLiteralDfa0_2();
+       break;
+   }
+     if (jjmatchedKind != 0x7fffffff)
+     {
+        if (jjmatchedPos + 1 < curPos)
+           input_stream.backup(curPos - jjmatchedPos - 1);
+        if ((jjtoToken[jjmatchedKind >> 6] & (1L << (jjmatchedKind & 077))) != 0L)
+        {
+           matchedToken = jjFillToken();
+       if (jjnewLexState[jjmatchedKind] != -1)
+         curLexState = jjnewLexState[jjmatchedKind];
+           return matchedToken;
+        }
+        else
+        {
+         if (jjnewLexState[jjmatchedKind] != -1)
+           curLexState = jjnewLexState[jjmatchedKind];
+           continue EOFLoop;
+        }
+     }
+     int error_line = input_stream.getEndLine();
+     int error_column = input_stream.getEndColumn();
+     String error_after = null;
+     boolean EOFSeen = false;
+     try { input_stream.readChar(); input_stream.backup(1); }
+     catch (java.io.IOException e1) {
+        EOFSeen = true;
+        error_after = curPos <= 1 ? "" : input_stream.GetImage();
+        if (curChar == '\n' || curChar == '\r') {
+           error_line++;
+           error_column = 0;
+        }
+        else
+           error_column++;
+     }
+     if (!EOFSeen) {
+        input_stream.backup(1);
+        error_after = curPos <= 1 ? "" : input_stream.GetImage();
+     }
+     throw new TokenMgrError(EOFSeen, curLexState, error_line, error_column, error_after, curChar, TokenMgrError.LEXICAL_ERROR);
+  }
+}
+
+private void jjCheckNAdd(int state)
+{
+   if (jjrounds[state] != jjround)
+   {
+      jjstateSet[jjnewStateCnt++] = state;
+      jjrounds[state] = jjround;
+   }
+}
+private void jjAddStates(int start, int end)
+{
+   do {
+      jjstateSet[jjnewStateCnt++] = jjnextStates[start];
+   } while (start++ != end);
+}
+private void jjCheckNAddTwoStates(int state1, int state2)
+{
+   jjCheckNAdd(state1);
+   jjCheckNAdd(state2);
+}
+
+private void jjCheckNAddStates(int start, int end)
+{
+   do {
+      jjCheckNAdd(jjnextStates[start]);
+   } while (start++ != end);
+}
+
+}
diff --git a/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/Token.java b/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/Token.java
new file mode 100644
index 0000000..2b47564
--- /dev/null
+++ b/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/Token.java
@@ -0,0 +1,124 @@
+/* Generated By:JavaCC: Do not edit this line. Token.java Version 4.1 */
+/* JavaCCOptions:TOKEN_EXTENDS=,KEEP_LINE_COL=null */
+package org.apache.lucene.queryparser.classic;
+
+/**
+ * Describes the input token stream.
+ */
+
+public class Token {
+
+  /**
+   * An integer that describes the kind of this token.  This numbering
+   * system is determined by JavaCCParser, and a table of these numbers is
+   * stored in the file ...Constants.java.
+   */
+  public int kind;
+
+  /** The line number of the first character of this Token. */
+  public int beginLine;
+  /** The column number of the first character of this Token. */
+  public int beginColumn;
+  /** The line number of the last character of this Token. */
+  public int endLine;
+  /** The column number of the last character of this Token. */
+  public int endColumn;
+
+  /**
+   * The string image of the token.
+   */
+  public String image;
+
+  /**
+   * A reference to the next regular (non-special) token from the input
+   * stream.  If this is the last token from the input stream, or if the
+   * token manager has not read tokens beyond this one, this field is
+   * set to null.  This is true only if this token is also a regular
+   * token.  Otherwise, see below for a description of the contents of
+   * this field.
+   */
+  public Token next;
+
+  /**
+   * This field is used to access special tokens that occur prior to this
+   * token, but after the immediately preceding regular (non-special) token.
+   * If there are no such special tokens, this field is set to null.
+   * When there are more than one such special token, this field refers
+   * to the last of these special tokens, which in turn refers to the next
+   * previous special token through its specialToken field, and so on
+   * until the first special token (whose specialToken field is null).
+   * The next fields of special tokens refer to other special tokens that
+   * immediately follow it (without an intervening regular token).  If there
+   * is no such token, this field is null.
+   */
+  public Token specialToken;
+
+  /**
+   * An optional attribute value of the Token.
+   * Tokens which are not used as syntactic sugar will often contain
+   * meaningful values that will be used later on by the compiler or
+   * interpreter. This attribute value is often different from the image.
+   * Any subclass of Token that actually wants to return a non-null value can
+   * override this method as appropriate.
+   */
+  public Object getValue() {
+    return null;
+  }
+
+  /**
+   * No-argument constructor
+   */
+  public Token() {}
+
+  /**
+   * Constructs a new token for the specified Image.
+   */
+  public Token(int kind)
+  {
+     this(kind, null);
+  }
+
+  /**
+   * Constructs a new token for the specified Image and Kind.
+   */
+  public Token(int kind, String image)
+  {
+     this.kind = kind;
+     this.image = image;
+  }
+
+  /**
+   * Returns the image.
+   */
+  public String toString()
+  {
+     return image;
+  }
+
+  /**
+   * Returns a new Token object, by default. However, if you want, you
+   * can create and return subclass objects based on the value of ofKind.
+   * Simply add the cases to the switch for all those special cases.
+   * For example, if you have a subclass of Token called IDToken that
+   * you want to create if ofKind is ID, simply add something like :
+   *
+   *    case MyParserConstants.ID : return new IDToken(ofKind, image);
+   *
+   * to the following switch statement. Then you can cast matchedToken
+   * variable to the appropriate type and use sit in your lexical actions.
+   */
+  public static Token newToken(int ofKind, String image)
+  {
+     switch(ofKind)
+     {
+       default : return new Token(ofKind, image);
+     }
+  }
+
+  public static Token newToken(int ofKind)
+  {
+     return newToken(ofKind, null);
+  }
+
+}
+/* JavaCC - OriginalChecksum=9f74ef8b727ef4e5dafb84a45b3584c9 (do not edit this line) */
diff --git a/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/TokenMgrError.java b/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/TokenMgrError.java
new file mode 100644
index 0000000..7dad747
--- /dev/null
+++ b/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/TokenMgrError.java
@@ -0,0 +1,141 @@
+/* Generated By:JavaCC: Do not edit this line. TokenMgrError.java Version 4.1 */
+/* JavaCCOptions: */
+package org.apache.lucene.queryparser.classic;
+
+/** Token Manager Error. */
+@SuppressWarnings("serial")
+public class TokenMgrError extends Error
+{
+
+   /*
+    * Ordinals for various reasons why an Error of this type can be thrown.
+    */
+
+   /**
+    * Lexical error occurred.
+    */
+   static final int LEXICAL_ERROR = 0;
+
+   /**
+    * An attempt was made to create a second instance of a static token manager.
+    */
+   static final int STATIC_LEXER_ERROR = 1;
+
+   /**
+    * Tried to change to an invalid lexical state.
+    */
+   static final int INVALID_LEXICAL_STATE = 2;
+
+   /**
+    * Detected (and bailed out of) an infinite loop in the token manager.
+    */
+   static final int LOOP_DETECTED = 3;
+
+   /**
+    * Indicates the reason why the exception is thrown. It will have
+    * one of the above 4 values.
+    */
+   int errorCode;
+
+   /**
+    * Replaces unprintable characters by their escaped (or unicode escaped)
+    * equivalents in the given string
+    */
+   protected static final String addEscapes(String str) {
+      StringBuffer retval = new StringBuffer();
+      char ch;
+      for (int i = 0; i < str.length(); i++) {
+        switch (str.charAt(i))
+        {
+           case 0 :
+              continue;
+           case '\b':
+              retval.append("\\b");
+              continue;
+           case '\t':
+              retval.append("\\t");
+              continue;
+           case '\n':
+              retval.append("\\n");
+              continue;
+           case '\f':
+              retval.append("\\f");
+              continue;
+           case '\r':
+              retval.append("\\r");
+              continue;
+           case '\"':
+              retval.append("\\\"");
+              continue;
+           case '\'':
+              retval.append("\\\'");
+              continue;
+           case '\\':
+              retval.append("\\\\");
+              continue;
+           default:
+              if ((ch = str.charAt(i)) < 0x20 || ch > 0x7e) {
+                 String s = "0000" + Integer.toString(ch, 16);
+                 retval.append("\\u" + s.substring(s.length() - 4, s.length()));
+              } else {
+                 retval.append(ch);
+              }
+              continue;
+        }
+      }
+      return retval.toString();
+   }
+
+   /**
+    * Returns a detailed message for the Error when it is thrown by the
+    * token manager to indicate a lexical error.
+    * Parameters :
+    *    EOFSeen     : indicates if EOF caused the lexical error
+    *    curLexState : lexical state in which this error occurred
+    *    errorLine   : line number when the error occurred
+    *    errorColumn : column number when the error occurred
+    *    errorAfter  : prefix that was seen before this error occurred
+    *    curchar     : the offending character
+    * Note: You can customize the lexical error message by modifying this method.
+    */
+   protected static String LexicalError(boolean EOFSeen, int lexState, int errorLine, int errorColumn, String errorAfter, char curChar) {
+      return("Lexical error at line " +
+           errorLine + ", column " +
+           errorColumn + ".  Encountered: " +
+           (EOFSeen ? "<EOF> " : ("\"" + addEscapes(String.valueOf(curChar)) + "\"") + " (" + (int)curChar + "), ") +
+           "after : \"" + addEscapes(errorAfter) + "\"");
+   }
+
+   /**
+    * You can also modify the body of this method to customize your error messages.
+    * For example, cases like LOOP_DETECTED and INVALID_LEXICAL_STATE are not
+    * of end-users concern, so you can return something like :
+    *
+    *     "Internal Error : Please file a bug report .... "
+    *
+    * from this method for such cases in the release version of your parser.
+    */
+   public String getMessage() {
+      return super.getMessage();
+   }
+
+   /*
+    * Constructors of various flavors follow.
+    */
+
+   /** No arg constructor. */
+   public TokenMgrError() {
+   }
+
+   /** Constructor with message and reason. */
+   public TokenMgrError(String message, int reason) {
+      super(message);
+      errorCode = reason;
+   }
+
+   /** Full Constructor. */
+   public TokenMgrError(boolean EOFSeen, int lexState, int errorLine, int errorColumn, String errorAfter, char curChar, int reason) {
+      this(LexicalError(EOFSeen, lexState, errorLine, errorColumn, errorAfter, curChar), reason);
+   }
+}
+/* JavaCC - OriginalChecksum=b55ad725f5fbc672fa115f498926930c (do not edit this line) */
diff --git a/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/package.html b/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/package.html
new file mode 100644
index 0000000..d4017aa
--- /dev/null
+++ b/modules/queryparser/src/java/org/apache/lucene/queryparser/classic/package.html
@@ -0,0 +1,35 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
+</head>
+<body>
+
+A simple query parser implemented with JavaCC.
+<p>Note that JavaCC defines lots of public classes, methods and fields
+that do not need to be public.&nbsp; These clutter the documentation.&nbsp;
+Sorry.
+<p>Note that because JavaCC defines a class named <tt>Token</tt>, <tt>org.apache.lucene.analysis.Token</tt>
+must always be fully qualified in source code in this package.
+
+<p><b>NOTE</b>: contrib/queryparser has an alternative queryparser that matches the syntax of this one, but is more modular,
+enabling substantial customization to how a query is created.
+
+</body>
+</html>
diff --git a/modules/queryparser/src/java/overview.html b/modules/queryparser/src/java/overview.html
new file mode 100644
index 0000000..40c7086
--- /dev/null
+++ b/modules/queryparser/src/java/overview.html
@@ -0,0 +1,26 @@
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one or more
+  contributor license agreements.  See the NOTICE file distributed with
+  this work for additional information regarding copyright ownership.
+  The ASF licenses this file to You under the Apache License, Version 2.0
+  (the "License"); you may not use this file except in compliance with
+  the License.  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+  -->
+<html>
+  <head>
+    <title>
+      QueryParsers
+    </title>
+  </head>
+  <body>
+  QueryParsers
+  </body>
+</html>
diff --git a/modules/queryparser/src/test/org/apache/lucene/queryparser/classic/TestMultiAnalyzer.java b/modules/queryparser/src/test/org/apache/lucene/queryparser/classic/TestMultiAnalyzer.java
new file mode 100644
index 0000000..468fae5
--- /dev/null
+++ b/modules/queryparser/src/test/org/apache/lucene/queryparser/classic/TestMultiAnalyzer.java
@@ -0,0 +1,273 @@
+package org.apache.lucene.queryparser.classic;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Reader;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.analysis.TokenFilter;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.util.Version;
+
+/**
+ * Test QueryParser's ability to deal with Analyzers that return more
+ * than one token per position or that return tokens with a position
+ * increment &gt; 1.
+ *
+ */
+public class TestMultiAnalyzer extends BaseTokenStreamTestCase {
+
+  private static int multiToken = 0;
+
+  public void testMultiAnalyzer() throws ParseException {
+    
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "", new MultiAnalyzer());
+
+    // trivial, no multiple tokens:
+    assertEquals("foo", qp.parse("foo").toString());
+    assertEquals("foo", qp.parse("\"foo\"").toString());
+    assertEquals("foo foobar", qp.parse("foo foobar").toString());
+    assertEquals("\"foo foobar\"", qp.parse("\"foo foobar\"").toString());
+    assertEquals("\"foo foobar blah\"", qp.parse("\"foo foobar blah\"").toString());
+
+    // two tokens at the same position:
+    assertEquals("(multi multi2) foo", qp.parse("multi foo").toString());
+    assertEquals("foo (multi multi2)", qp.parse("foo multi").toString());
+    assertEquals("(multi multi2) (multi multi2)", qp.parse("multi multi").toString());
+    assertEquals("+(foo (multi multi2)) +(bar (multi multi2))",
+        qp.parse("+(foo multi) +(bar multi)").toString());
+    assertEquals("+(foo (multi multi2)) field:\"bar (multi multi2)\"",
+        qp.parse("+(foo multi) field:\"bar multi\"").toString());
+
+    // phrases:
+    assertEquals("\"(multi multi2) foo\"", qp.parse("\"multi foo\"").toString());
+    assertEquals("\"foo (multi multi2)\"", qp.parse("\"foo multi\"").toString());
+    assertEquals("\"foo (multi multi2) foobar (multi multi2)\"",
+        qp.parse("\"foo multi foobar multi\"").toString());
+
+    // fields:
+    assertEquals("(field:multi field:multi2) field:foo", qp.parse("field:multi field:foo").toString());
+    assertEquals("field:\"(multi multi2) foo\"", qp.parse("field:\"multi foo\"").toString());
+
+    // three tokens at one position:
+    assertEquals("triplemulti multi3 multi2", qp.parse("triplemulti").toString());
+    assertEquals("foo (triplemulti multi3 multi2) foobar",
+        qp.parse("foo triplemulti foobar").toString());
+
+    // phrase with non-default slop:
+    assertEquals("\"(multi multi2) foo\"~10", qp.parse("\"multi foo\"~10").toString());
+
+    // phrase with non-default boost:
+    assertEquals("\"(multi multi2) foo\"^2.0", qp.parse("\"multi foo\"^2").toString());
+
+    // phrase after changing default slop
+    qp.setPhraseSlop(99);
+    assertEquals("\"(multi multi2) foo\"~99 bar",
+                 qp.parse("\"multi foo\" bar").toString());
+    assertEquals("\"(multi multi2) foo\"~99 \"foo bar\"~2",
+                 qp.parse("\"multi foo\" \"foo bar\"~2").toString());
+    qp.setPhraseSlop(0);
+
+    // non-default operator:
+    qp.setDefaultOperator(QueryParser.AND_OPERATOR);
+    assertEquals("+(multi multi2) +foo", qp.parse("multi foo").toString());
+
+  }
+    
+  public void testMultiAnalyzerWithSubclassOfQueryParser() throws ParseException {
+
+    DumbQueryParser qp = new DumbQueryParser("", new MultiAnalyzer());
+    qp.setPhraseSlop(99); // modified default slop
+
+    // direct call to (super's) getFieldQuery to demonstrate differnce
+    // between phrase and multiphrase with modified default slop
+    assertEquals("\"foo bar\"~99",
+                 qp.getSuperFieldQuery("","foo bar", true).toString());
+    assertEquals("\"(multi multi2) bar\"~99",
+                 qp.getSuperFieldQuery("","multi bar", true).toString());
+
+    
+    // ask sublcass to parse phrase with modified default slop
+    assertEquals("\"(multi multi2) foo\"~99 bar",
+                 qp.parse("\"multi foo\" bar").toString());
+    
+  }
+    
+  public void testPosIncrementAnalyzer() throws ParseException {
+    QueryParser qp = new QueryParser(Version.LUCENE_40, "", new PosIncrementAnalyzer());
+    assertEquals("quick brown", qp.parse("the quick brown").toString());
+    assertEquals("quick brown fox", qp.parse("the quick brown fox").toString());
+  }
+  
+  /**
+   * Expands "multi" to "multi" and "multi2", both at the same position,
+   * and expands "triplemulti" to "triplemulti", "multi3", and "multi2".  
+   */
+  private class MultiAnalyzer extends Analyzer {
+
+    public MultiAnalyzer() {
+    }
+
+    @Override
+    public TokenStream tokenStream(String fieldName, Reader reader) {
+      TokenStream result = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);
+      result = new TestFilter(result);
+      return result;
+    }
+  }
+
+  private final class TestFilter extends TokenFilter {
+    
+    private String prevType;
+    private int prevStartOffset;
+    private int prevEndOffset;
+    
+    CharTermAttribute termAtt;
+    PositionIncrementAttribute posIncrAtt;
+    OffsetAttribute offsetAtt;
+    TypeAttribute typeAtt;
+    
+    public TestFilter(TokenStream in) {
+      super(in);
+      termAtt = addAttribute(CharTermAttribute.class);
+      posIncrAtt = addAttribute(PositionIncrementAttribute.class);
+      offsetAtt = addAttribute(OffsetAttribute.class);
+      typeAtt = addAttribute(TypeAttribute.class);
+    }
+
+    @Override
+    public final boolean incrementToken() throws java.io.IOException {
+      if (multiToken > 0) {
+        termAtt.setEmpty().append("multi"+(multiToken+1));
+        offsetAtt.setOffset(prevStartOffset, prevEndOffset);
+        typeAtt.setType(prevType);
+        posIncrAtt.setPositionIncrement(0);
+        multiToken--;
+        return true;
+      } else {
+        boolean next = input.incrementToken();
+        if (next == false) {
+          return false;
+        }
+        prevType = typeAtt.type();
+        prevStartOffset = offsetAtt.startOffset();
+        prevEndOffset = offsetAtt.endOffset();
+        String text = termAtt.toString();
+        if (text.equals("triplemulti")) {
+          multiToken = 2;
+          return true;
+        } else if (text.equals("multi")) {
+          multiToken = 1;
+          return true;
+        } else {
+          return true;
+        }
+      }
+    }
+  }
+
+  /**
+   * Analyzes "the quick brown" as: quick(incr=2) brown(incr=1).
+   * Does not work correctly for input other than "the quick brown ...".
+   */
+  private class PosIncrementAnalyzer extends Analyzer {
+
+    public PosIncrementAnalyzer() {
+    }
+
+    @Override
+    public TokenStream tokenStream(String fieldName, Reader reader) {
+      TokenStream result = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);
+      result = new TestPosIncrementFilter(result);
+      return result;
+    }
+  }
+
+  private final class TestPosIncrementFilter extends TokenFilter {
+    
+    CharTermAttribute termAtt;
+    PositionIncrementAttribute posIncrAtt;
+    
+    public TestPosIncrementFilter(TokenStream in) {
+      super(in);
+      termAtt = addAttribute(CharTermAttribute.class);
+      posIncrAtt = addAttribute(PositionIncrementAttribute.class);
+    }
+
+    @Override
+    public final boolean incrementToken () throws java.io.IOException {
+      while(input.incrementToken()) {
+        if (termAtt.toString().equals("the")) {
+          // stopword, do nothing
+        } else if (termAtt.toString().equals("quick")) {
+          posIncrAtt.setPositionIncrement(2);
+          return true;
+        } else {
+          posIncrAtt.setPositionIncrement(1);
+          return true;
+        }
+      }
+      return false;
+    }
+  }
+
+    /** a very simple subclass of QueryParser */
+    private final static class DumbQueryParser extends QueryParser {
+        
+        public DumbQueryParser(String f, Analyzer a) {
+            super(TEST_VERSION_CURRENT, f, a);
+        }
+
+        /** expose super's version */
+        public Query getSuperFieldQuery(String f, String t, boolean quoted) 
+            throws ParseException {
+            return super.getFieldQuery(f,t,quoted);
+        }
+        /** wrap super's version */
+        @Override
+        protected Query getFieldQuery(String f, String t, boolean quoted)
+            throws ParseException {
+            return new DumbQueryWrapper(getSuperFieldQuery(f,t,quoted));
+        }
+    }
+    
+    /**
+     * A very simple wrapper to prevent instanceof checks but uses
+     * the toString of the query it wraps.
+     */
+    private final static class DumbQueryWrapper extends Query {
+
+        private Query q;
+        public DumbQueryWrapper(Query q) {
+            super();
+            this.q = q;
+        }
+        @Override
+        public String toString(String f) {
+            return q.toString(f);
+        }
+    }
+    
+}
diff --git a/modules/queryparser/src/test/org/apache/lucene/queryparser/classic/TestMultiFieldQueryParser.java b/modules/queryparser/src/test/org/apache/lucene/queryparser/classic/TestMultiFieldQueryParser.java
new file mode 100644
index 0000000..a1fd549
--- /dev/null
+++ b/modules/queryparser/src/test/org/apache/lucene/queryparser/classic/TestMultiFieldQueryParser.java
@@ -0,0 +1,328 @@
+package org.apache.lucene.queryparser.classic;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.io.Reader;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.BooleanClause.Occur;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.LuceneTestCase;
+
+/**
+ * Tests QueryParser.
+ */
+public class TestMultiFieldQueryParser extends LuceneTestCase {
+
+  /** test stop words parsing for both the non static form, and for the 
+   * corresponding static form (qtxt, fields[]). */
+  public void testStopwordsParsing() throws Exception {
+    assertStopQueryEquals("one", "b:one t:one");  
+    assertStopQueryEquals("one stop", "b:one t:one");  
+    assertStopQueryEquals("one (stop)", "b:one t:one");  
+    assertStopQueryEquals("one ((stop))", "b:one t:one");  
+    assertStopQueryEquals("stop", "");  
+    assertStopQueryEquals("(stop)", "");  
+    assertStopQueryEquals("((stop))", "");  
+  }
+
+  // verify parsing of query using a stopping analyzer  
+  private void assertStopQueryEquals (String qtxt, String expectedRes) throws Exception {
+    String[] fields = {"b", "t"};
+    Occur occur[] = {Occur.SHOULD, Occur.SHOULD};
+    TestQueryParser.QPTestAnalyzer a = new TestQueryParser.QPTestAnalyzer();
+    MultiFieldQueryParser mfqp = new MultiFieldQueryParser(TEST_VERSION_CURRENT, fields, a);
+    
+    Query q = mfqp.parse(qtxt);
+    assertEquals(expectedRes, q.toString());
+    
+    q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, qtxt, fields, occur, a);
+    assertEquals(expectedRes, q.toString());
+  }
+  
+  public void testSimple() throws Exception {
+    String[] fields = {"b", "t"};
+    MultiFieldQueryParser mfqp = new MultiFieldQueryParser(TEST_VERSION_CURRENT, fields, new MockAnalyzer(random));
+    
+    Query q = mfqp.parse("one");
+    assertEquals("b:one t:one", q.toString());
+    
+    q = mfqp.parse("one two");
+    assertEquals("(b:one t:one) (b:two t:two)", q.toString());
+    
+    q = mfqp.parse("+one +two");
+    assertEquals("+(b:one t:one) +(b:two t:two)", q.toString());
+
+    q = mfqp.parse("+one -two -three");
+    assertEquals("+(b:one t:one) -(b:two t:two) -(b:three t:three)", q.toString());
+    
+    q = mfqp.parse("one^2 two");
+    assertEquals("((b:one t:one)^2.0) (b:two t:two)", q.toString());
+
+    q = mfqp.parse("one~ two");
+    assertEquals("(b:one~2.0 t:one~2.0) (b:two t:two)", q.toString());
+
+    q = mfqp.parse("one~0.8 two^2");
+    assertEquals("(b:one~0.8 t:one~0.8) ((b:two t:two)^2.0)", q.toString());
+
+    q = mfqp.parse("one* two*");
+    assertEquals("(b:one* t:one*) (b:two* t:two*)", q.toString());
+
+    q = mfqp.parse("[a TO c] two");
+    assertEquals("(b:[a TO c] t:[a TO c]) (b:two t:two)", q.toString());
+
+    q = mfqp.parse("w?ldcard");
+    assertEquals("b:w?ldcard t:w?ldcard", q.toString());
+
+    q = mfqp.parse("\"foo bar\"");
+    assertEquals("b:\"foo bar\" t:\"foo bar\"", q.toString());
+
+    q = mfqp.parse("\"aa bb cc\" \"dd ee\"");
+    assertEquals("(b:\"aa bb cc\" t:\"aa bb cc\") (b:\"dd ee\" t:\"dd ee\")", q.toString());
+
+    q = mfqp.parse("\"foo bar\"~4");
+    assertEquals("b:\"foo bar\"~4 t:\"foo bar\"~4", q.toString());
+
+    // LUCENE-1213: MultiFieldQueryParser was ignoring slop when phrase had a field.
+    q = mfqp.parse("b:\"foo bar\"~4"); 
+    assertEquals("b:\"foo bar\"~4", q.toString());
+
+    // make sure that terms which have a field are not touched:
+    q = mfqp.parse("one f:two");
+    assertEquals("(b:one t:one) f:two", q.toString());
+
+    // AND mode:
+    mfqp.setDefaultOperator(QueryParser.AND_OPERATOR);
+    q = mfqp.parse("one two");
+    assertEquals("+(b:one t:one) +(b:two t:two)", q.toString());
+    q = mfqp.parse("\"aa bb cc\" \"dd ee\"");
+    assertEquals("+(b:\"aa bb cc\" t:\"aa bb cc\") +(b:\"dd ee\" t:\"dd ee\")", q.toString());
+
+  }
+  
+  public void testBoostsSimple() throws Exception {
+      Map<String,Float> boosts = new HashMap<String,Float>();
+      boosts.put("b", Float.valueOf(5));
+      boosts.put("t", Float.valueOf(10));
+      String[] fields = {"b", "t"};
+      MultiFieldQueryParser mfqp = new MultiFieldQueryParser(TEST_VERSION_CURRENT, fields, new MockAnalyzer(random), boosts);
+      
+      
+      //Check for simple
+      Query q = mfqp.parse("one");
+      assertEquals("b:one^5.0 t:one^10.0", q.toString());
+      
+      //Check for AND
+      q = mfqp.parse("one AND two");
+      assertEquals("+(b:one^5.0 t:one^10.0) +(b:two^5.0 t:two^10.0)", q.toString());
+      
+      //Check for OR
+      q = mfqp.parse("one OR two");
+      assertEquals("(b:one^5.0 t:one^10.0) (b:two^5.0 t:two^10.0)", q.toString());
+      
+      //Check for AND and a field
+      q = mfqp.parse("one AND two AND foo:test");
+      assertEquals("+(b:one^5.0 t:one^10.0) +(b:two^5.0 t:two^10.0) +foo:test", q.toString());
+      
+      q = mfqp.parse("one^3 AND two^4");
+      assertEquals("+((b:one^5.0 t:one^10.0)^3.0) +((b:two^5.0 t:two^10.0)^4.0)", q.toString());
+  }
+
+  public void testStaticMethod1() throws ParseException {
+    String[] fields = {"b", "t"};
+    String[] queries = {"one", "two"};
+    Query q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, queries, fields, new MockAnalyzer(random));
+    assertEquals("b:one t:two", q.toString());
+
+    String[] queries2 = {"+one", "+two"};
+    q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, queries2, fields, new MockAnalyzer(random));
+    assertEquals("(+b:one) (+t:two)", q.toString());
+
+    String[] queries3 = {"one", "+two"};
+    q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, queries3, fields, new MockAnalyzer(random));
+    assertEquals("b:one (+t:two)", q.toString());
+
+    String[] queries4 = {"one +more", "+two"};
+    q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, queries4, fields, new MockAnalyzer(random));
+    assertEquals("(b:one +b:more) (+t:two)", q.toString());
+
+    String[] queries5 = {"blah"};
+    try {
+      q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, queries5, fields, new MockAnalyzer(random));
+      fail();
+    } catch(IllegalArgumentException e) {
+      // expected exception, array length differs
+    }
+    
+    // check also with stop words for this static form (qtxts[], fields[]).
+    TestQueryParser.QPTestAnalyzer stopA = new TestQueryParser.QPTestAnalyzer();
+    
+    String[] queries6 = {"((+stop))", "+((stop))"};
+    q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, queries6, fields, stopA);
+    assertEquals("", q.toString());
+    
+    String[] queries7 = {"one ((+stop)) +more", "+((stop)) +two"};
+    q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, queries7, fields, stopA);
+    assertEquals("(b:one +b:more) (+t:two)", q.toString());
+
+  }
+
+  public void testStaticMethod2() throws ParseException {
+    String[] fields = {"b", "t"};
+    BooleanClause.Occur[] flags = {BooleanClause.Occur.MUST, BooleanClause.Occur.MUST_NOT};
+    Query q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, "one", fields, flags, new MockAnalyzer(random));
+    assertEquals("+b:one -t:one", q.toString());
+
+    q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, "one two", fields, flags, new MockAnalyzer(random));
+    assertEquals("+(b:one b:two) -(t:one t:two)", q.toString());
+
+    try {
+      BooleanClause.Occur[] flags2 = {BooleanClause.Occur.MUST};
+      q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, "blah", fields, flags2, new MockAnalyzer(random));
+      fail();
+    } catch(IllegalArgumentException e) {
+      // expected exception, array length differs
+    }
+  }
+
+  public void testStaticMethod2Old() throws ParseException {
+    String[] fields = {"b", "t"};
+    //int[] flags = {MultiFieldQueryParser.REQUIRED_FIELD, MultiFieldQueryParser.PROHIBITED_FIELD};
+      BooleanClause.Occur[] flags = {BooleanClause.Occur.MUST, BooleanClause.Occur.MUST_NOT};
+
+    Query q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, "one", fields, flags, new MockAnalyzer(random));//, fields, flags, new MockAnalyzer(random));
+    assertEquals("+b:one -t:one", q.toString());
+
+    q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, "one two", fields, flags, new MockAnalyzer(random));
+    assertEquals("+(b:one b:two) -(t:one t:two)", q.toString());
+
+    try {
+      BooleanClause.Occur[] flags2 = {BooleanClause.Occur.MUST};
+      q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, "blah", fields, flags2, new MockAnalyzer(random));
+      fail();
+    } catch(IllegalArgumentException e) {
+      // expected exception, array length differs
+    }
+  }
+
+  public void testStaticMethod3() throws ParseException {
+    String[] queries = {"one", "two", "three"};
+    String[] fields = {"f1", "f2", "f3"};
+    BooleanClause.Occur[] flags = {BooleanClause.Occur.MUST,
+        BooleanClause.Occur.MUST_NOT, BooleanClause.Occur.SHOULD};
+    Query q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, queries, fields, flags, new MockAnalyzer(random));
+    assertEquals("+f1:one -f2:two f3:three", q.toString());
+
+    try {
+      BooleanClause.Occur[] flags2 = {BooleanClause.Occur.MUST};
+      q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, queries, fields, flags2, new MockAnalyzer(random));
+      fail();
+    } catch(IllegalArgumentException e) {
+      // expected exception, array length differs
+    }
+  }
+
+  public void testStaticMethod3Old() throws ParseException {
+    String[] queries = {"one", "two"};
+    String[] fields = {"b", "t"};
+      BooleanClause.Occur[] flags = {BooleanClause.Occur.MUST, BooleanClause.Occur.MUST_NOT};
+    Query q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, queries, fields, flags, new MockAnalyzer(random));
+    assertEquals("+b:one -t:two", q.toString());
+
+    try {
+      BooleanClause.Occur[] flags2 = {BooleanClause.Occur.MUST};
+      q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, queries, fields, flags2, new MockAnalyzer(random));
+      fail();
+    } catch(IllegalArgumentException e) {
+      // expected exception, array length differs
+    }
+  }
+
+  public void testAnalyzerReturningNull() throws ParseException {
+    String[] fields = new String[] { "f1", "f2", "f3" };
+    MultiFieldQueryParser parser = new MultiFieldQueryParser(TEST_VERSION_CURRENT, fields, new AnalyzerReturningNull());
+    Query q = parser.parse("bla AND blo");
+    assertEquals("+(f2:bla f3:bla) +(f2:blo f3:blo)", q.toString());
+    // the following queries are not affected as their terms are not analyzed anyway:
+    q = parser.parse("bla*");
+    assertEquals("f1:bla* f2:bla* f3:bla*", q.toString());
+    q = parser.parse("bla~");
+    assertEquals("f1:bla~2.0 f2:bla~2.0 f3:bla~2.0", q.toString());
+    q = parser.parse("[a TO c]");
+    assertEquals("f1:[a TO c] f2:[a TO c] f3:[a TO c]", q.toString());
+  }
+
+  public void testStopWordSearching() throws Exception {
+    Analyzer analyzer = new MockAnalyzer(random);
+    Directory ramDir = newDirectory();
+    IndexWriter iw =  new IndexWriter(ramDir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));
+    Document doc = new Document();
+    doc.add(newField("body", "blah the footest blah", Field.Store.NO, Field.Index.ANALYZED));
+    iw.addDocument(doc);
+    iw.close();
+    
+    MultiFieldQueryParser mfqp = 
+      new MultiFieldQueryParser(TEST_VERSION_CURRENT, new String[] {"body"}, analyzer);
+    mfqp.setDefaultOperator(QueryParser.Operator.AND);
+    Query q = mfqp.parse("the footest");
+    IndexSearcher is = new IndexSearcher(ramDir, true);
+    ScoreDoc[] hits = is.search(q, null, 1000).scoreDocs;
+    assertEquals(1, hits.length);
+    is.close();
+    ramDir.close();
+  }
+  
+  /**
+   * Return empty tokens for field "f1".
+   */
+  private static class AnalyzerReturningNull extends Analyzer {
+    MockAnalyzer stdAnalyzer = new MockAnalyzer(random);
+
+    public AnalyzerReturningNull() {
+    }
+
+    @Override
+    public TokenStream tokenStream(String fieldName, Reader reader) {
+      if ("f1".equals(fieldName)) {
+        return new EmptyTokenStream();
+      } else {
+        return stdAnalyzer.tokenStream(fieldName, reader);
+      }
+    }
+
+    private static class EmptyTokenStream extends TokenStream {
+      @Override
+      public boolean incrementToken() throws IOException {
+        return false;
+      }
+    }
+  }
+
+}
diff --git a/modules/queryparser/src/test/org/apache/lucene/queryparser/classic/TestMultiPhraseQueryParsing.java b/modules/queryparser/src/test/org/apache/lucene/queryparser/classic/TestMultiPhraseQueryParsing.java
new file mode 100644
index 0000000..e6b9a15
--- /dev/null
+++ b/modules/queryparser/src/test/org/apache/lucene/queryparser/classic/TestMultiPhraseQueryParsing.java
@@ -0,0 +1,105 @@
+package org.apache.lucene.queryparser.classic;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.MultiPhraseQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.util.LuceneTestCase;
+
+import java.io.IOException;
+import java.io.Reader;
+
+public class TestMultiPhraseQueryParsing extends LuceneTestCase {
+
+  private static class TokenAndPos {
+      public final String token;
+      public final int pos;
+      public TokenAndPos(String token, int pos) {
+        this.token = token;
+        this.pos = pos;
+      }
+    }
+
+  private static class CannedAnalyzer extends Analyzer {
+    private final TokenAndPos[] tokens;
+
+    public CannedAnalyzer(TokenAndPos[] tokens) {
+      this.tokens = tokens;
+    }
+
+    @Override
+    public TokenStream tokenStream(String fieldName, Reader reader) {
+      return new CannedTokenizer(tokens);
+    }
+  }
+
+  private static class CannedTokenizer extends Tokenizer {
+    private final TokenAndPos[] tokens;
+    private int upto = 0;
+    private int lastPos = 0;
+    private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
+    private final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);
+
+    public CannedTokenizer(TokenAndPos[] tokens) {
+      this.tokens = tokens;
+    }
+
+    @Override
+    public final boolean incrementToken() throws IOException {
+      clearAttributes();
+      if (upto < tokens.length) {
+        final TokenAndPos token = tokens[upto++];
+        termAtt.setEmpty();
+        termAtt.append(token.token);
+        posIncrAtt.setPositionIncrement(token.pos - lastPos);
+        lastPos = token.pos;
+        return true;
+      } else {
+        return false;
+      }
+    }
+  }
+
+  public void testMultiPhraseQueryParsing() throws Exception {
+    TokenAndPos[] INCR_0_QUERY_TOKENS_AND = new TokenAndPos[]{
+        new TokenAndPos("a", 0),
+        new TokenAndPos("1", 0),
+        new TokenAndPos("b", 1),
+        new TokenAndPos("1", 1),
+        new TokenAndPos("c", 2)
+    };
+
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", new CannedAnalyzer(INCR_0_QUERY_TOKENS_AND));
+    Query q = qp.parse("\"this text is acually ignored\"");
+    assertTrue("wrong query type!", q instanceof MultiPhraseQuery);
+
+    MultiPhraseQuery multiPhraseQuery = new MultiPhraseQuery();
+    multiPhraseQuery.add(new Term[]{ new Term("field", "a"), new Term("field", "1") }, -1);
+    multiPhraseQuery.add(new Term[]{ new Term("field", "b"), new Term("field", "1") }, 0);
+    multiPhraseQuery.add(new Term[]{ new Term("field", "c") }, 1);
+
+    assertEquals(multiPhraseQuery, q);
+  }
+
+}
diff --git a/modules/queryparser/src/test/org/apache/lucene/queryparser/classic/TestQueryParser.java b/modules/queryparser/src/test/org/apache/lucene/queryparser/classic/TestQueryParser.java
new file mode 100644
index 0000000..d2c7513
--- /dev/null
+++ b/modules/queryparser/src/test/org/apache/lucene/queryparser/classic/TestQueryParser.java
@@ -0,0 +1,1333 @@
+package org.apache.lucene.queryparser.classic;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.io.Reader;
+import java.text.DateFormat;
+import java.util.Calendar;
+import java.util.Date;
+import java.util.GregorianCalendar;
+import java.util.Locale;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.analysis.MockTokenFilter;
+import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.analysis.TokenFilter;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.document.DateTools;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.search.*;
+import org.apache.lucene.search.BooleanClause.Occur;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.automaton.BasicAutomata;
+import org.apache.lucene.util.automaton.CharacterRunAutomaton;
+import org.apache.lucene.util.automaton.RegExp;
+
+/**
+ * Tests QueryParser.
+ */
+public class TestQueryParser extends LuceneTestCase {
+  
+  public static Analyzer qpAnalyzer = new QPTestAnalyzer();
+
+  public static final class QPTestFilter extends TokenFilter {
+    CharTermAttribute termAtt;
+    OffsetAttribute offsetAtt;
+        
+    /**
+     * Filter which discards the token 'stop' and which expands the
+     * token 'phrase' into 'phrase1 phrase2'
+     */
+    public QPTestFilter(TokenStream in) {
+      super(in);
+      termAtt = addAttribute(CharTermAttribute.class);
+      offsetAtt = addAttribute(OffsetAttribute.class);
+    }
+
+    boolean inPhrase = false;
+    int savedStart = 0, savedEnd = 0;
+
+    @Override
+    public boolean incrementToken() throws IOException {
+      if (inPhrase) {
+        inPhrase = false;
+        clearAttributes();
+        termAtt.append("phrase2");
+        offsetAtt.setOffset(savedStart, savedEnd);
+        return true;
+      } else
+        while (input.incrementToken()) {
+          if (termAtt.toString().equals("phrase")) {
+            inPhrase = true;
+            savedStart = offsetAtt.startOffset();
+            savedEnd = offsetAtt.endOffset();
+            termAtt.setEmpty().append("phrase1");
+            offsetAtt.setOffset(savedStart, savedEnd);
+            return true;
+          } else if (!termAtt.toString().equals("stop"))
+            return true;
+        }
+      return false;
+    }
+  }
+
+  
+  public static final class QPTestAnalyzer extends Analyzer {
+
+    /** Filters MockTokenizer with StopFilter. */
+    @Override
+    public final TokenStream tokenStream(String fieldName, Reader reader) {
+      return new QPTestFilter(new MockTokenizer(reader, MockTokenizer.SIMPLE, true));
+    }
+  }
+
+  public static class QPTestParser extends QueryParser {
+    public QPTestParser(String f, Analyzer a) {
+      super(TEST_VERSION_CURRENT, f, a);
+    }
+
+    @Override
+    protected Query getFuzzyQuery(String field, String termStr, float minSimilarity) throws ParseException {
+      throw new ParseException("Fuzzy queries not allowed");
+    }
+
+    @Override
+    protected Query getWildcardQuery(String field, String termStr) throws ParseException {
+      throw new ParseException("Wildcard queries not allowed");
+    }
+  }
+
+  private int originalMaxClauses;
+
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    originalMaxClauses = BooleanQuery.getMaxClauseCount();
+  }
+
+  public QueryParser getParser(Analyzer a) throws Exception {
+    if (a == null)
+      a = new MockAnalyzer(random, MockTokenizer.SIMPLE, true);
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", a);
+    qp.setDefaultOperator(QueryParser.OR_OPERATOR);
+    return qp;
+  }
+
+  public Query getQuery(String query, Analyzer a) throws Exception {
+    return getParser(a).parse(query);
+  }
+
+  public void assertQueryEquals(String query, Analyzer a, String result)
+    throws Exception {
+    Query q = getQuery(query, a);
+    String s = q.toString("field");
+    if (!s.equals(result)) {
+      fail("Query /" + query + "/ yielded /" + s
+           + "/, expecting /" + result + "/");
+    }
+  }
+
+  public void assertQueryEquals(QueryParser qp, String field, String query, String result) 
+    throws Exception {
+    Query q = qp.parse(query);
+    String s = q.toString(field);
+    if (!s.equals(result)) {
+      fail("Query /" + query + "/ yielded /" + s
+           + "/, expecting /" + result + "/");
+    }
+  }
+  
+  public void assertEscapedQueryEquals(String query, Analyzer a, String result)
+    throws Exception {
+    String escapedQuery = QueryParser.escape(query);
+    if (!escapedQuery.equals(result)) {
+      fail("Query /" + query + "/ yielded /" + escapedQuery
+          + "/, expecting /" + result + "/");
+    }
+  }
+
+  public void assertWildcardQueryEquals(String query, boolean lowercase, String result, boolean allowLeadingWildcard)
+    throws Exception {
+    QueryParser qp = getParser(null);
+    qp.setLowercaseExpandedTerms(lowercase);
+    qp.setAllowLeadingWildcard(allowLeadingWildcard);
+    Query q = qp.parse(query);
+    String s = q.toString("field");
+    if (!s.equals(result)) {
+      fail("WildcardQuery /" + query + "/ yielded /" + s
+           + "/, expecting /" + result + "/");
+    }
+  }
+
+  public void assertWildcardQueryEquals(String query, boolean lowercase, String result)
+    throws Exception {
+    assertWildcardQueryEquals(query, lowercase, result, false);
+  }
+
+  public void assertWildcardQueryEquals(String query, String result) throws Exception {
+    QueryParser qp = getParser(null);
+    Query q = qp.parse(query);
+    String s = q.toString("field");
+    if (!s.equals(result)) {
+      fail("WildcardQuery /" + query + "/ yielded /" + s + "/, expecting /"
+          + result + "/");
+    }
+  }
+
+  public Query getQueryDOA(String query, Analyzer a)
+    throws Exception {
+    if (a == null)
+      a = new MockAnalyzer(random, MockTokenizer.SIMPLE, true);
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", a);
+    qp.setDefaultOperator(QueryParser.AND_OPERATOR);
+    return qp.parse(query);
+  }
+
+  public void assertQueryEqualsDOA(String query, Analyzer a, String result)
+    throws Exception {
+    Query q = getQueryDOA(query, a);
+    String s = q.toString("field");
+    if (!s.equals(result)) {
+      fail("Query /" + query + "/ yielded /" + s
+           + "/, expecting /" + result + "/");
+    }
+  }
+
+  public void testCJK() throws Exception {
+	 // Test Ideographic Space - As wide as a CJK character cell (fullwidth)
+	 // used google to translate the word "term" to japanese -> ??
+	 assertQueryEquals("term\u3000term\u3000term", null, "term\u0020term\u0020term");
+	 assertQueryEquals("??\u3000??\u3000??", null, "??\u0020??\u0020??");
+  }
+
+  //individual CJK chars as terms, like StandardAnalyzer
+  private class SimpleCJKTokenizer extends Tokenizer {
+    private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
+
+    public SimpleCJKTokenizer(Reader input) {
+      super(input);
+    }
+
+    @Override
+    public boolean incrementToken() throws IOException {
+      int ch = input.read();
+      if (ch < 0)
+        return false;
+      clearAttributes();
+      termAtt.setEmpty().append((char) ch);
+      return true;
+    }
+  }
+
+  private class SimpleCJKAnalyzer extends Analyzer {
+    @Override
+    public TokenStream tokenStream(String fieldName, Reader reader) {
+      return new SimpleCJKTokenizer(reader);
+    }
+  }
+
+  public void testCJKTerm() throws Exception {
+    // individual CJK chars as terms
+    SimpleCJKAnalyzer analyzer = new SimpleCJKAnalyzer(); 
+    
+    BooleanQuery expected = new BooleanQuery();
+    expected.add(new TermQuery(new Term("field", "?")), BooleanClause.Occur.SHOULD);
+    expected.add(new TermQuery(new Term("field", "??")), BooleanClause.Occur.SHOULD);
+    
+    assertEquals(expected, getQuery("??", analyzer));
+  }
+  
+  public void testCJKBoostedTerm() throws Exception {
+    // individual CJK chars as terms
+    SimpleCJKAnalyzer analyzer = new SimpleCJKAnalyzer();
+    
+    BooleanQuery expected = new BooleanQuery();
+    expected.setBoost(0.5f);
+    expected.add(new TermQuery(new Term("field", "?")), BooleanClause.Occur.SHOULD);
+    expected.add(new TermQuery(new Term("field", "??")), BooleanClause.Occur.SHOULD);
+    
+    assertEquals(expected, getQuery("??^0.5", analyzer));
+  }
+  
+  public void testCJKPhrase() throws Exception {
+    // individual CJK chars as terms
+    SimpleCJKAnalyzer analyzer = new SimpleCJKAnalyzer();
+    
+    PhraseQuery expected = new PhraseQuery();
+    expected.add(new Term("field", "?"));
+    expected.add(new Term("field", "??"));
+    
+    assertEquals(expected, getQuery("\"??\"", analyzer));
+  }
+  
+  public void testCJKBoostedPhrase() throws Exception {
+    // individual CJK chars as terms
+    SimpleCJKAnalyzer analyzer = new SimpleCJKAnalyzer();
+    
+    PhraseQuery expected = new PhraseQuery();
+    expected.setBoost(0.5f);
+    expected.add(new Term("field", "?"));
+    expected.add(new Term("field", "??"));
+    
+    assertEquals(expected, getQuery("\"??\"^0.5", analyzer));
+  }
+  
+  public void testCJKSloppyPhrase() throws Exception {
+    // individual CJK chars as terms
+    SimpleCJKAnalyzer analyzer = new SimpleCJKAnalyzer();
+    
+    PhraseQuery expected = new PhraseQuery();
+    expected.setSlop(3);
+    expected.add(new Term("field", "?"));
+    expected.add(new Term("field", "??"));
+    
+    assertEquals(expected, getQuery("\"??\"~3", analyzer));
+  }
+  
+  public void testAutoGeneratePhraseQueriesOn() throws Exception {
+    // individual CJK chars as terms
+    SimpleCJKAnalyzer analyzer = new SimpleCJKAnalyzer(); 
+  
+    PhraseQuery expected = new PhraseQuery();
+    expected.add(new Term("field", "?"));
+    expected.add(new Term("field", "??"));
+    QueryParser parser = new QueryParser(TEST_VERSION_CURRENT, "field", analyzer);
+    parser.setAutoGeneratePhraseQueries(true);
+    assertEquals(expected, parser.parse("??"));
+  }
+
+  public void testSimple() throws Exception {
+    assertQueryEquals("term term term", null, "term term term");
+    assertQueryEquals("trm term term", new MockAnalyzer(random), "trm term term");
+    assertQueryEquals("mlaut", new MockAnalyzer(random), "mlaut");
+
+    // FIXME: enhance MockAnalyzer to be able to support this
+    // it must no longer extend CharTokenizer
+    //assertQueryEquals("\"\"", new KeywordAnalyzer(), "");
+    //assertQueryEquals("foo:\"\"", new KeywordAnalyzer(), "foo:");
+
+    assertQueryEquals("a AND b", null, "+a +b");
+    assertQueryEquals("(a AND b)", null, "+a +b");
+    assertQueryEquals("c OR (a AND b)", null, "c (+a +b)");
+    assertQueryEquals("a AND NOT b", null, "+a -b");
+    assertQueryEquals("a AND -b", null, "+a -b");
+    assertQueryEquals("a AND !b", null, "+a -b");
+    assertQueryEquals("a && b", null, "+a +b");
+//    assertQueryEquals("a && ! b", null, "+a -b");
+
+    assertQueryEquals("a OR b", null, "a b");
+    assertQueryEquals("a || b", null, "a b");
+    assertQueryEquals("a OR !b", null, "a -b");
+//    assertQueryEquals("a OR ! b", null, "a -b");
+    assertQueryEquals("a OR -b", null, "a -b");
+
+    // +,-,! should be directly adjacent to operand (i.e. not separated by whitespace) to be treated as an operator
+    Analyzer a = new Analyzer() {
+      @Override
+      public TokenStream tokenStream(String fieldName, Reader reader) {
+        return new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);
+      }
+    };
+    assertQueryEquals("a - b", a, "a - b");
+    assertQueryEquals("a + b", a, "a + b");
+    assertQueryEquals("a ! b", a, "a ! b");
+
+    assertQueryEquals("+term -term term", null, "+term -term term");
+    assertQueryEquals("foo:term AND field:anotherTerm", null,
+                      "+foo:term +anotherterm");
+    assertQueryEquals("term AND \"phrase phrase\"", null,
+                      "+term +\"phrase phrase\"");
+    assertQueryEquals("\"hello there\"", null, "\"hello there\"");
+    assertTrue(getQuery("a AND b", null) instanceof BooleanQuery);
+    assertTrue(getQuery("hello", null) instanceof TermQuery);
+    assertTrue(getQuery("\"hello there\"", null) instanceof PhraseQuery);
+
+    assertQueryEquals("germ term^2.0", null, "germ term^2.0");
+    assertQueryEquals("(term)^2.0", null, "term^2.0");
+    assertQueryEquals("(germ term)^2.0", null, "(germ term)^2.0");
+    assertQueryEquals("term^2.0", null, "term^2.0");
+    assertQueryEquals("term^2", null, "term^2.0");
+    assertQueryEquals("\"germ term\"^2.0", null, "\"germ term\"^2.0");
+    assertQueryEquals("\"term germ\"^2", null, "\"term germ\"^2.0");
+
+    assertQueryEquals("(foo OR bar) AND (baz OR boo)", null,
+                      "+(foo bar) +(baz boo)");
+    assertQueryEquals("((a OR b) AND NOT c) OR d", null,
+                      "(+(a b) -c) d");
+    assertQueryEquals("+(apple \"steve jobs\") -(foo bar baz)", null,
+                      "+(apple \"steve jobs\") -(foo bar baz)");
+    assertQueryEquals("+title:(dog OR cat) -author:\"bob dole\"", null,
+                      "+(title:dog title:cat) -author:\"bob dole\"");
+    
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", new MockAnalyzer(random));
+    // make sure OR is the default:
+    assertEquals(QueryParser.OR_OPERATOR, qp.getDefaultOperator());
+    qp.setDefaultOperator(QueryParser.AND_OPERATOR);
+    assertEquals(QueryParser.AND_OPERATOR, qp.getDefaultOperator());
+    qp.setDefaultOperator(QueryParser.OR_OPERATOR);
+    assertEquals(QueryParser.OR_OPERATOR, qp.getDefaultOperator());
+  }
+
+  public void testPunct() throws Exception {
+    Analyzer a = new MockAnalyzer(random, MockTokenizer.WHITESPACE, false);
+    assertQueryEquals("a&b", a, "a&b");
+    assertQueryEquals("a&&b", a, "a&&b");
+    assertQueryEquals(".NET", a, ".NET");
+  }
+
+  public void testSlop() throws Exception {
+    assertQueryEquals("\"term germ\"~2", null, "\"term germ\"~2");
+    assertQueryEquals("\"term germ\"~2 flork", null, "\"term germ\"~2 flork");
+    assertQueryEquals("\"term\"~2", null, "term");
+    assertQueryEquals("\" \"~2 germ", null, "germ");
+    assertQueryEquals("\"term germ\"~2^2", null, "\"term germ\"~2^2.0");
+  }
+
+  public void testNumber() throws Exception {
+// The numbers go away because SimpleAnalzyer ignores them
+    assertQueryEquals("3", null, "");
+    assertQueryEquals("term 1.0 1 2", null, "term");
+    assertQueryEquals("term term1 term2", null, "term term term");
+
+    Analyzer a = new MockAnalyzer(random, MockTokenizer.WHITESPACE, true);
+    assertQueryEquals("3", a, "3");
+    assertQueryEquals("term 1.0 1 2", a, "term 1.0 1 2");
+    assertQueryEquals("term term1 term2", a, "term term1 term2");
+  }
+
+  public void testWildcard() throws Exception {
+    assertQueryEquals("term*", null, "term*");
+    assertQueryEquals("term*^2", null, "term*^2.0");
+    assertQueryEquals("term~", null, "term~2.0");
+    assertQueryEquals("term~0.7", null, "term~0.7");
+    assertQueryEquals("term~^3", null, "term~2.0^3.0");
+    assertQueryEquals("term^3~", null, "term~2.0^3.0");
+    assertQueryEquals("term*germ", null, "term*germ");
+    assertQueryEquals("term*germ^3", null, "term*germ^3.0");
+
+    assertTrue(getQuery("term*", null) instanceof PrefixQuery);
+    assertTrue(getQuery("term*^2", null) instanceof PrefixQuery);
+    assertTrue(getQuery("term~", null) instanceof FuzzyQuery);
+    assertTrue(getQuery("term~0.7", null) instanceof FuzzyQuery);
+    FuzzyQuery fq = (FuzzyQuery)getQuery("term~0.7", null);
+    assertEquals(0.7f, fq.getMinSimilarity(), 0.1f);
+    assertEquals(FuzzyQuery.defaultPrefixLength, fq.getPrefixLength());
+    fq = (FuzzyQuery)getQuery("term~", null);
+    assertEquals(2.0f, fq.getMinSimilarity(), 0.1f);
+    assertEquals(FuzzyQuery.defaultPrefixLength, fq.getPrefixLength());
+    
+    assertParseException("term~1.1"); // value > 1, throws exception
+
+    assertTrue(getQuery("term*germ", null) instanceof WildcardQuery);
+
+/* Tests to see that wild card terms are (or are not) properly
+	 * lower-cased with propery parser configuration
+	 */
+// First prefix queries:
+    // by default, convert to lowercase:
+    assertWildcardQueryEquals("Term*", true, "term*");
+    // explicitly set lowercase:
+    assertWildcardQueryEquals("term*", true, "term*");
+    assertWildcardQueryEquals("Term*", true, "term*");
+    assertWildcardQueryEquals("TERM*", true, "term*");
+    // explicitly disable lowercase conversion:
+    assertWildcardQueryEquals("term*", false, "term*");
+    assertWildcardQueryEquals("Term*", false, "Term*");
+    assertWildcardQueryEquals("TERM*", false, "TERM*");
+// Then 'full' wildcard queries:
+    // by default, convert to lowercase:
+    assertWildcardQueryEquals("Te?m", "te?m");
+    // explicitly set lowercase:
+    assertWildcardQueryEquals("te?m", true, "te?m");
+    assertWildcardQueryEquals("Te?m", true, "te?m");
+    assertWildcardQueryEquals("TE?M", true, "te?m");
+    assertWildcardQueryEquals("Te?m*gerM", true, "te?m*germ");
+    // explicitly disable lowercase conversion:
+    assertWildcardQueryEquals("te?m", false, "te?m");
+    assertWildcardQueryEquals("Te?m", false, "Te?m");
+    assertWildcardQueryEquals("TE?M", false, "TE?M");
+    assertWildcardQueryEquals("Te?m*gerM", false, "Te?m*gerM");
+//  Fuzzy queries:
+    assertWildcardQueryEquals("Term~", "term~2.0");
+    assertWildcardQueryEquals("Term~", true, "term~2.0");
+    assertWildcardQueryEquals("Term~", false, "Term~2.0");
+//  Range queries:
+    assertWildcardQueryEquals("[A TO C]", "[a TO c]");
+    assertWildcardQueryEquals("[A TO C]", true, "[a TO c]");
+    assertWildcardQueryEquals("[A TO C]", false, "[A TO C]");
+    // Test suffix queries: first disallow
+    try {
+      assertWildcardQueryEquals("*Term", true, "*term");
+      fail();
+    } catch(ParseException pe) {
+      // expected exception
+    }
+    try {
+      assertWildcardQueryEquals("?Term", true, "?term");
+      fail();
+    } catch(ParseException pe) {
+      // expected exception
+    }
+    // Test suffix queries: then allow
+    assertWildcardQueryEquals("*Term", true, "*term", true);
+    assertWildcardQueryEquals("?Term", true, "?term", true);
+  }
+  
+  public void testLeadingWildcardType() throws Exception {
+    QueryParser qp = getParser(null);
+    qp.setAllowLeadingWildcard(true);
+    assertEquals(WildcardQuery.class, qp.parse("t*erm*").getClass());
+    assertEquals(WildcardQuery.class, qp.parse("?term*").getClass());
+    assertEquals(WildcardQuery.class, qp.parse("*term*").getClass());
+  }
+
+  public void testQPA() throws Exception {
+    assertQueryEquals("term term^3.0 term", qpAnalyzer, "term term^3.0 term");
+    assertQueryEquals("term stop^3.0 term", qpAnalyzer, "term term");
+    
+    assertQueryEquals("term term term", qpAnalyzer, "term term term");
+    assertQueryEquals("term +stop term", qpAnalyzer, "term term");
+    assertQueryEquals("term -stop term", qpAnalyzer, "term term");
+
+    assertQueryEquals("drop AND (stop) AND roll", qpAnalyzer, "+drop +roll");
+    assertQueryEquals("term +(stop) term", qpAnalyzer, "term term");
+    assertQueryEquals("term -(stop) term", qpAnalyzer, "term term");
+    
+    assertQueryEquals("drop AND stop AND roll", qpAnalyzer, "+drop +roll");
+    assertQueryEquals("term phrase term", qpAnalyzer,
+                      "term (phrase1 phrase2) term");
+    assertQueryEquals("term AND NOT phrase term", qpAnalyzer,
+                      "+term -(phrase1 phrase2) term");
+    assertQueryEquals("stop^3", qpAnalyzer, "");
+    assertQueryEquals("stop", qpAnalyzer, "");
+    assertQueryEquals("(stop)^3", qpAnalyzer, "");
+    assertQueryEquals("((stop))^3", qpAnalyzer, "");
+    assertQueryEquals("(stop^3)", qpAnalyzer, "");
+    assertQueryEquals("((stop)^3)", qpAnalyzer, "");
+    assertQueryEquals("(stop)", qpAnalyzer, "");
+    assertQueryEquals("((stop))", qpAnalyzer, "");
+    assertTrue(getQuery("term term term", qpAnalyzer) instanceof BooleanQuery);
+    assertTrue(getQuery("term +stop", qpAnalyzer) instanceof TermQuery);
+  }
+
+  public void testRange() throws Exception {
+    assertQueryEquals("[ a TO z]", null, "[a TO z]");
+    assertQueryEquals("[ a TO z}", null, "[a TO z}");
+    assertQueryEquals("{ a TO z]", null, "{a TO z]"); 
+
+     assertEquals(MultiTermQuery.CONSTANT_SCORE_AUTO_REWRITE_DEFAULT, ((TermRangeQuery)getQuery("[ a TO z]", null)).getRewriteMethod());
+
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", new MockAnalyzer(random, MockTokenizer.SIMPLE, true));
+    qp.setMultiTermRewriteMethod(MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE);
+    assertEquals(MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE,((TermRangeQuery)qp.parse("[ a TO z]")).getRewriteMethod());
+    
+    assertQueryEquals("[ a TO z ]", null, "[a TO z]");
+    assertQueryEquals("{ a TO z}", null, "{a TO z}");
+    assertQueryEquals("{ a TO z }", null, "{a TO z}");
+    assertQueryEquals("{ a TO z }^2.0", null, "{a TO z}^2.0");
+    assertQueryEquals("[ a TO z] OR bar", null, "[a TO z] bar");
+    assertQueryEquals("[ a TO z] AND bar", null, "+[a TO z] +bar");
+    assertQueryEquals("( bar blar { a TO z}) ", null, "bar blar {a TO z}");
+    assertQueryEquals("gack ( bar blar { a TO z}) ", null, "gack (bar blar {a TO z})");
+
+    assertQueryEquals("[* TO Z]",null,"[* TO z]");
+    assertQueryEquals("[A TO *]",null,"[a TO *]");
+    assertQueryEquals("[* TO *]",null,"[* TO *]");
+    assertQueryEquals("[\\* TO \"*\"]",null,"[\\* TO \\*]");
+ }
+    
+  private String escapeDateString(String s) {
+    if (s.indexOf(" ") > -1) {
+      return "\"" + s + "\"";
+    } else {
+      return s;
+    }
+  }
+  
+  /** for testing DateTools support */
+  private String getDate(String s, DateTools.Resolution resolution) throws Exception {
+    DateFormat df = DateFormat.getDateInstance(DateFormat.SHORT);
+    return getDate(df.parse(s), resolution);      
+  }
+  
+  /** for testing DateTools support */
+  private String getDate(Date d, DateTools.Resolution resolution) throws Exception {
+     return DateTools.dateToString(d, resolution);
+  }
+  
+  private String getLocalizedDate(int year, int month, int day) {
+    DateFormat df = DateFormat.getDateInstance(DateFormat.SHORT);
+    Calendar calendar = new GregorianCalendar();
+    calendar.clear();
+    calendar.set(year, month, day);
+    calendar.set(Calendar.HOUR_OF_DAY, 23);
+    calendar.set(Calendar.MINUTE, 59);
+    calendar.set(Calendar.SECOND, 59);
+    calendar.set(Calendar.MILLISECOND, 999);
+    return df.format(calendar.getTime());
+  }
+
+  public void testDateRange() throws Exception {
+    String startDate = getLocalizedDate(2002, 1, 1);
+    String endDate = getLocalizedDate(2002, 1, 4);
+    Calendar endDateExpected = new GregorianCalendar();
+    endDateExpected.clear();
+    endDateExpected.set(2002, 1, 4, 23, 59, 59);
+    endDateExpected.set(Calendar.MILLISECOND, 999);
+    final String defaultField = "default";
+    final String monthField = "month";
+    final String hourField = "hour";
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", new MockAnalyzer(random, MockTokenizer.SIMPLE, true));
+    
+    // set a field specific date resolution
+    qp.setDateResolution(monthField, DateTools.Resolution.MONTH);
+    
+    // set default date resolution to MILLISECOND
+    qp.setDateResolution(DateTools.Resolution.MILLISECOND);
+    
+    // set second field specific date resolution    
+    qp.setDateResolution(hourField, DateTools.Resolution.HOUR);
+
+    // for this field no field specific date resolution has been set,
+    // so verify if the default resolution is used
+    assertDateRangeQueryEquals(qp, defaultField, startDate, endDate, 
+            endDateExpected.getTime(), DateTools.Resolution.MILLISECOND);
+
+    // verify if field specific date resolutions are used for these two fields
+    assertDateRangeQueryEquals(qp, monthField, startDate, endDate, 
+            endDateExpected.getTime(), DateTools.Resolution.MONTH);
+
+    assertDateRangeQueryEquals(qp, hourField, startDate, endDate, 
+            endDateExpected.getTime(), DateTools.Resolution.HOUR);  
+  }
+  
+  public void assertDateRangeQueryEquals(QueryParser qp, String field, String startDate, String endDate, 
+                                         Date endDateInclusive, DateTools.Resolution resolution) throws Exception {
+    assertQueryEquals(qp, field, field + ":[" + escapeDateString(startDate) + " TO " + escapeDateString(endDate) + "]",
+               "[" + getDate(startDate, resolution) + " TO " + getDate(endDateInclusive, resolution) + "]");
+    assertQueryEquals(qp, field, field + ":{" + escapeDateString(startDate) + " TO " + escapeDateString(endDate) + "}",
+               "{" + getDate(startDate, resolution) + " TO " + getDate(endDate, resolution) + "}");
+  }
+
+  public void testEscaped() throws Exception {
+    Analyzer a = new MockAnalyzer(random, MockTokenizer.WHITESPACE, false);
+    
+    /*assertQueryEquals("\\[brackets", a, "\\[brackets");
+    assertQueryEquals("\\[brackets", null, "brackets");
+    assertQueryEquals("\\\\", a, "\\\\");
+    assertQueryEquals("\\+blah", a, "\\+blah");
+    assertQueryEquals("\\(blah", a, "\\(blah");
+
+    assertQueryEquals("\\-blah", a, "\\-blah");
+    assertQueryEquals("\\!blah", a, "\\!blah");
+    assertQueryEquals("\\{blah", a, "\\{blah");
+    assertQueryEquals("\\}blah", a, "\\}blah");
+    assertQueryEquals("\\:blah", a, "\\:blah");
+    assertQueryEquals("\\^blah", a, "\\^blah");
+    assertQueryEquals("\\[blah", a, "\\[blah");
+    assertQueryEquals("\\]blah", a, "\\]blah");
+    assertQueryEquals("\\\"blah", a, "\\\"blah");
+    assertQueryEquals("\\(blah", a, "\\(blah");
+    assertQueryEquals("\\)blah", a, "\\)blah");
+    assertQueryEquals("\\~blah", a, "\\~blah");
+    assertQueryEquals("\\*blah", a, "\\*blah");
+    assertQueryEquals("\\?blah", a, "\\?blah");
+    //assertQueryEquals("foo \\&\\& bar", a, "foo \\&\\& bar");
+    //assertQueryEquals("foo \\|| bar", a, "foo \\|| bar");
+    //assertQueryEquals("foo \\AND bar", a, "foo \\AND bar");*/
+
+    assertQueryEquals("\\a", a, "a");
+    
+    assertQueryEquals("a\\-b:c", a, "a-b:c");
+    assertQueryEquals("a\\+b:c", a, "a+b:c");
+    assertQueryEquals("a\\:b:c", a, "a:b:c");
+    assertQueryEquals("a\\\\b:c", a, "a\\b:c");
+
+    assertQueryEquals("a:b\\-c", a, "a:b-c");
+    assertQueryEquals("a:b\\+c", a, "a:b+c");
+    assertQueryEquals("a:b\\:c", a, "a:b:c");
+    assertQueryEquals("a:b\\\\c", a, "a:b\\c");
+
+    assertQueryEquals("a:b\\-c*", a, "a:b-c*");
+    assertQueryEquals("a:b\\+c*", a, "a:b+c*");
+    assertQueryEquals("a:b\\:c*", a, "a:b:c*");
+
+    assertQueryEquals("a:b\\\\c*", a, "a:b\\c*");
+
+    assertQueryEquals("a:b\\-?c", a, "a:b\\-?c");
+    assertQueryEquals("a:b\\+?c", a, "a:b\\+?c");
+    assertQueryEquals("a:b\\:?c", a, "a:b\\:?c");
+
+    assertQueryEquals("a:b\\\\?c", a, "a:b\\\\?c");
+
+    assertQueryEquals("a:b\\-c~", a, "a:b-c~2.0");
+    assertQueryEquals("a:b\\+c~", a, "a:b+c~2.0");
+    assertQueryEquals("a:b\\:c~", a, "a:b:c~2.0");
+    assertQueryEquals("a:b\\\\c~", a, "a:b\\c~2.0");
+
+    assertQueryEquals("[ a\\- TO a\\+ ]", null, "[a- TO a+]");
+    assertQueryEquals("[ a\\: TO a\\~ ]", null, "[a: TO a~]");
+    assertQueryEquals("[ a\\\\ TO a\\* ]", null, "[a\\ TO a*]");
+
+    assertQueryEquals("[\"c\\:\\\\temp\\\\\\~foo0.txt\" TO \"c\\:\\\\temp\\\\\\~foo9.txt\"]", a, 
+                      "[c:\\temp\\~foo0.txt TO c:\\temp\\~foo9.txt]");
+    
+    assertQueryEquals("a\\\\\\+b", a, "a\\+b");
+    
+    assertQueryEquals("a \\\"b c\\\" d", a, "a \"b c\" d");
+    assertQueryEquals("\"a \\\"b c\\\" d\"", a, "\"a \"b c\" d\"");
+    assertQueryEquals("\"a \\+b c d\"", a, "\"a +b c d\"");
+    
+    assertQueryEquals("c\\:\\\\temp\\\\\\~foo.txt", a, "c:\\temp\\~foo.txt");
+    
+    assertParseException("XY\\"); // there must be a character after the escape char
+    
+    // test unicode escaping
+    assertQueryEquals("a\\u0062c", a, "abc");
+    assertQueryEquals("XY\\u005a", a, "XYZ");
+    assertQueryEquals("XY\\u005A", a, "XYZ");
+    assertQueryEquals("\"a \\\\\\u0028\\u0062\\\" c\"", a, "\"a \\(b\" c\"");
+    
+    assertParseException("XY\\u005G");  // test non-hex character in escaped unicode sequence
+    assertParseException("XY\\u005");   // test incomplete escaped unicode sequence
+    
+    // Tests bug LUCENE-800
+    assertQueryEquals("(item:\\\\ item:ABCD\\\\)", a, "item:\\ item:ABCD\\");
+    assertParseException("(item:\\\\ item:ABCD\\\\))"); // unmatched closing paranthesis 
+    assertQueryEquals("\\*", a, "*");
+    assertQueryEquals("\\\\", a, "\\");  // escaped backslash
+    
+    assertParseException("\\"); // a backslash must always be escaped
+    
+    // LUCENE-1189
+    assertQueryEquals("(\"a\\\\\") or (\"b\")", a ,"a\\ or b");
+  }
+
+  public void testQueryStringEscaping() throws Exception {
+    Analyzer a = new MockAnalyzer(random, MockTokenizer.WHITESPACE, false);
+
+    assertEscapedQueryEquals("a-b:c", a, "a\\-b\\:c");
+    assertEscapedQueryEquals("a+b:c", a, "a\\+b\\:c");
+    assertEscapedQueryEquals("a:b:c", a, "a\\:b\\:c");
+    assertEscapedQueryEquals("a\\b:c", a, "a\\\\b\\:c");
+
+    assertEscapedQueryEquals("a:b-c", a, "a\\:b\\-c");
+    assertEscapedQueryEquals("a:b+c", a, "a\\:b\\+c");
+    assertEscapedQueryEquals("a:b:c", a, "a\\:b\\:c");
+    assertEscapedQueryEquals("a:b\\c", a, "a\\:b\\\\c");
+
+    assertEscapedQueryEquals("a:b-c*", a, "a\\:b\\-c\\*");
+    assertEscapedQueryEquals("a:b+c*", a, "a\\:b\\+c\\*");
+    assertEscapedQueryEquals("a:b:c*", a, "a\\:b\\:c\\*");
+
+    assertEscapedQueryEquals("a:b\\\\c*", a, "a\\:b\\\\\\\\c\\*");
+
+    assertEscapedQueryEquals("a:b-?c", a, "a\\:b\\-\\?c");
+    assertEscapedQueryEquals("a:b+?c", a, "a\\:b\\+\\?c");
+    assertEscapedQueryEquals("a:b:?c", a, "a\\:b\\:\\?c");
+
+    assertEscapedQueryEquals("a:b?c", a, "a\\:b\\?c");
+
+    assertEscapedQueryEquals("a:b-c~", a, "a\\:b\\-c\\~");
+    assertEscapedQueryEquals("a:b+c~", a, "a\\:b\\+c\\~");
+    assertEscapedQueryEquals("a:b:c~", a, "a\\:b\\:c\\~");
+    assertEscapedQueryEquals("a:b\\c~", a, "a\\:b\\\\c\\~");
+
+    assertEscapedQueryEquals("[ a - TO a+ ]", null, "\\[ a \\- TO a\\+ \\]");
+    assertEscapedQueryEquals("[ a : TO a~ ]", null, "\\[ a \\: TO a\\~ \\]");
+    assertEscapedQueryEquals("[ a\\ TO a* ]", null, "\\[ a\\\\ TO a\\* \\]");
+    
+    // LUCENE-881
+    assertEscapedQueryEquals("|| abc ||", a, "\\|\\| abc \\|\\|");
+    assertEscapedQueryEquals("&& abc &&", a, "\\&\\& abc \\&\\&");
+  }
+  
+  public void testTabNewlineCarriageReturn()
+    throws Exception {
+    assertQueryEqualsDOA("+weltbank +worlbank", null,
+      "+weltbank +worlbank");
+
+    assertQueryEqualsDOA("+weltbank\n+worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \n+worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \n +worlbank", null,
+      "+weltbank +worlbank");
+
+    assertQueryEqualsDOA("+weltbank\r+worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \r+worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \r +worlbank", null,
+      "+weltbank +worlbank");
+
+    assertQueryEqualsDOA("+weltbank\r\n+worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \r\n+worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \r\n +worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \r \n +worlbank", null,
+      "+weltbank +worlbank");
+
+    assertQueryEqualsDOA("+weltbank\t+worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \t+worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \t +worlbank", null,
+      "+weltbank +worlbank");
+  }
+
+  public void testSimpleDAO()
+    throws Exception {
+    assertQueryEqualsDOA("term term term", null, "+term +term +term");
+    assertQueryEqualsDOA("term +term term", null, "+term +term +term");
+    assertQueryEqualsDOA("term term +term", null, "+term +term +term");
+    assertQueryEqualsDOA("term +term +term", null, "+term +term +term");
+    assertQueryEqualsDOA("-term term term", null, "-term +term +term");
+  }
+
+  public void testBoost()
+    throws Exception {
+    CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeString("on"));
+    Analyzer oneStopAnalyzer = new MockAnalyzer(random, MockTokenizer.SIMPLE, true, stopWords, true);
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", oneStopAnalyzer);
+    Query q = qp.parse("on^1.0");
+    assertNotNull(q);
+    q = qp.parse("\"hello\"^2.0");
+    assertNotNull(q);
+    assertEquals(q.getBoost(), (float) 2.0, (float) 0.5);
+    q = qp.parse("hello^2.0");
+    assertNotNull(q);
+    assertEquals(q.getBoost(), (float) 2.0, (float) 0.5);
+    q = qp.parse("\"on\"^1.0");
+    assertNotNull(q);
+
+    QueryParser qp2 = new QueryParser(TEST_VERSION_CURRENT, "field", new MockAnalyzer(random, MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET, true));
+    q = qp2.parse("the^3");
+    // "the" is a stop word so the result is an empty query:
+    assertNotNull(q);
+    assertEquals("", q.toString());
+    assertEquals(1.0f, q.getBoost(), 0.01f);
+  }
+
+  public void assertParseException(String queryString) throws Exception {
+    try {
+      getQuery(queryString, null);
+    } catch (ParseException expected) {
+      return;
+    }
+    fail("ParseException expected, not thrown");
+  }
+       
+  public void testException() throws Exception {
+    assertParseException("\"some phrase");
+    assertParseException("(foo bar");
+    assertParseException("foo bar))");
+    assertParseException("field:term:with:colon some more terms");
+    assertParseException("(sub query)^5.0^2.0 plus more");
+    assertParseException("secret AND illegal) AND access:confidential");
+  }
+  
+
+  public void testCustomQueryParserWildcard() {
+    try {
+      new QPTestParser("contents", new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)).parse("a?t");
+      fail("Wildcard queries should not be allowed");
+    } catch (ParseException expected) {
+      // expected exception
+    }
+  }
+
+  public void testCustomQueryParserFuzzy() throws Exception {
+    try {
+      new QPTestParser("contents", new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)).parse("xunit~");
+      fail("Fuzzy queries should not be allowed");
+    } catch (ParseException expected) {
+      // expected exception
+    }
+  }
+
+  public void testBooleanQuery() throws Exception {
+    BooleanQuery.setMaxClauseCount(2);
+    try {
+      QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", new MockAnalyzer(random, MockTokenizer.WHITESPACE, false));
+      qp.parse("one two three");
+      fail("ParseException expected due to too many boolean clauses");
+    } catch (ParseException expected) {
+      // too many boolean clauses, so ParseException is expected
+    }
+  }
+
+  /**
+   * This test differs from TestPrecedenceQueryParser
+   */
+  public void testPrecedence() throws Exception {
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", new MockAnalyzer(random, MockTokenizer.WHITESPACE, false));
+    Query query1 = qp.parse("A AND B OR C AND D");
+    Query query2 = qp.parse("+A +B +C +D");
+    assertEquals(query1, query2);
+  }
+
+// Todo: convert this from DateField to DateUtil
+//  public void testLocalDateFormat() throws IOException, ParseException {
+//    Directory ramDir = newDirectory();
+//    IndexWriter iw = new IndexWriter(ramDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));
+//    addDateDoc("a", 2005, 12, 2, 10, 15, 33, iw);
+//    addDateDoc("b", 2005, 12, 4, 22, 15, 00, iw);
+//    iw.close();
+//    IndexSearcher is = new IndexSearcher(ramDir, true);
+//    assertHits(1, "[12/1/2005 TO 12/3/2005]", is);
+//    assertHits(2, "[12/1/2005 TO 12/4/2005]", is);
+//    assertHits(1, "[12/3/2005 TO 12/4/2005]", is);
+//    assertHits(1, "{12/1/2005 TO 12/3/2005}", is);
+//    assertHits(1, "{12/1/2005 TO 12/4/2005}", is);
+//    assertHits(0, "{12/3/2005 TO 12/4/2005}", is);
+//    is.close();
+//    ramDir.close();
+//  }
+//
+//  private void addDateDoc(String content, int year, int month,
+//                          int day, int hour, int minute, int second, IndexWriter iw) throws IOException {
+//    Document d = new Document();
+//    d.add(newField("f", content, Field.Store.YES, Field.Index.ANALYZED));
+//    Calendar cal = Calendar.getInstance(Locale.ENGLISH);
+//    cal.set(year, month - 1, day, hour, minute, second);
+//    d.add(newField("date", DateField.dateToString(cal.getTime()), Field.Store.YES, Field.Index.NOT_ANALYZED));
+//    iw.addDocument(d);
+//  }
+
+  public void testStarParsing() throws Exception {
+    final int[] type = new int[1];
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)) {
+      @Override
+      protected Query getWildcardQuery(String field, String termStr) throws ParseException {
+        // override error checking of superclass
+        type[0]=1;
+        return new TermQuery(new Term(field,termStr));
+      }
+      @Override
+      protected Query getPrefixQuery(String field, String termStr) throws ParseException {
+        // override error checking of superclass
+        type[0]=2;        
+        return new TermQuery(new Term(field,termStr));
+      }
+
+      @Override
+      protected Query getFieldQuery(String field, String queryText, boolean quoted) throws ParseException {
+        type[0]=3;
+        return super.getFieldQuery(field, queryText, quoted);
+      }
+    };
+
+    TermQuery tq;
+
+    tq = (TermQuery)qp.parse("foo:zoo*");
+    assertEquals("zoo",tq.getTerm().text());
+    assertEquals(2,type[0]);
+
+    tq = (TermQuery)qp.parse("foo:zoo*^2");
+    assertEquals("zoo",tq.getTerm().text());
+    assertEquals(2,type[0]);
+    assertEquals(tq.getBoost(),2,0);
+
+    tq = (TermQuery)qp.parse("foo:*");
+    assertEquals("*",tq.getTerm().text());
+    assertEquals(1,type[0]);  // could be a valid prefix query in the future too
+
+    tq = (TermQuery)qp.parse("foo:*^2");
+    assertEquals("*",tq.getTerm().text());
+    assertEquals(1,type[0]);
+    assertEquals(tq.getBoost(),2,0);    
+
+    tq = (TermQuery)qp.parse("*:foo");
+    assertEquals("*",tq.getTerm().field());
+    assertEquals("foo",tq.getTerm().text());
+    assertEquals(3,type[0]);
+
+    tq = (TermQuery)qp.parse("*:*");
+    assertEquals("*",tq.getTerm().field());
+    assertEquals("*",tq.getTerm().text());
+    assertEquals(1,type[0]);  // could be handled as a prefix query in the future
+
+     tq = (TermQuery)qp.parse("(*:*)");
+    assertEquals("*",tq.getTerm().field());
+    assertEquals("*",tq.getTerm().text());
+    assertEquals(1,type[0]);
+
+  }
+
+  public void testEscapedWildcard() throws Exception {
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", new MockAnalyzer(random, MockTokenizer.WHITESPACE, false));
+    WildcardQuery q = new WildcardQuery(new Term("field", "foo\\?ba?r"));
+    assertEquals(q, qp.parse("foo\\?ba?r"));
+  }
+  
+  public void testRegexps() throws Exception {
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", new MockAnalyzer(random, MockTokenizer.WHITESPACE, false));
+    RegexpQuery q = new RegexpQuery(new Term("field", "[a-z][123]"));
+    assertEquals(q, qp.parse("/[a-z][123]/"));
+    qp.setLowercaseExpandedTerms(true);
+    assertEquals(q, qp.parse("/[A-Z][123]/"));
+    q.setBoost(0.5f);
+    assertEquals(q, qp.parse("/[A-Z][123]/^0.5"));
+    qp.setMultiTermRewriteMethod(MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE);
+    q.setRewriteMethod(MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE);
+    assertTrue(qp.parse("/[A-Z][123]/^0.5") instanceof RegexpQuery);
+    assertEquals(MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE, ((RegexpQuery)qp.parse("/[A-Z][123]/^0.5")).getRewriteMethod());
+    assertEquals(q, qp.parse("/[A-Z][123]/^0.5"));
+    qp.setMultiTermRewriteMethod(MultiTermQuery.CONSTANT_SCORE_AUTO_REWRITE_DEFAULT);
+    
+    Query escaped = new RegexpQuery(new Term("field", "[a-z]\\/[123]"));
+    assertEquals(escaped, qp.parse("/[a-z]\\/[123]/"));
+    Query escaped2 = new RegexpQuery(new Term("field", "[a-z]\\*[123]"));
+    assertEquals(escaped2, qp.parse("/[a-z]\\*[123]/"));
+    
+    BooleanQuery complex = new BooleanQuery();
+    complex.add(new RegexpQuery(new Term("field", "[a-z]\\/[123]")), Occur.MUST);
+    complex.add(new TermQuery(new Term("path", "/etc/init.d/")), Occur.MUST);
+    complex.add(new TermQuery(new Term("field", "/etc/init[.]d/lucene/")), Occur.SHOULD);
+    assertEquals(complex, qp.parse("/[a-z]\\/[123]/ AND path:/etc/init.d/ OR /etc\\/init\\[.\\]d/lucene/ "));
+  }
+  
+  public void testStopwords() throws Exception {
+    CharacterRunAutomaton stopSet = new CharacterRunAutomaton(new RegExp("the|foo").toAutomaton());
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "a", new MockAnalyzer(random, MockTokenizer.SIMPLE, true, stopSet, true));
+    Query result = qp.parse("a:the OR a:foo");
+    assertNotNull("result is null and it shouldn't be", result);
+    assertTrue("result is not a BooleanQuery", result instanceof BooleanQuery);
+    assertTrue(((BooleanQuery) result).clauses().size() + " does not equal: " + 0, ((BooleanQuery) result).clauses().size() == 0);
+    result = qp.parse("a:woo OR a:the");
+    assertNotNull("result is null and it shouldn't be", result);
+    assertTrue("result is not a TermQuery", result instanceof TermQuery);
+    result = qp.parse("(fieldX:xxxxx OR fieldy:xxxxxxxx)^2 AND (fieldx:the OR fieldy:foo)");
+    assertNotNull("result is null and it shouldn't be", result);
+    assertTrue("result is not a BooleanQuery", result instanceof BooleanQuery);
+    if (VERBOSE) System.out.println("Result: " + result);
+    assertTrue(((BooleanQuery) result).clauses().size() + " does not equal: " + 2, ((BooleanQuery) result).clauses().size() == 2);
+  }
+
+  public void testPositionIncrement() throws Exception {
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "a", new MockAnalyzer(random, MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET, true));
+    qp.setEnablePositionIncrements(true);
+    String qtxt = "\"the words in poisitions pos02578 are stopped in this phrasequery\"";
+    //               0         2                      5           7  8
+    int expectedPositions[] = {1,3,4,6,9};
+    PhraseQuery pq = (PhraseQuery) qp.parse(qtxt);
+    //System.out.println("Query text: "+qtxt);
+    //System.out.println("Result: "+pq);
+    Term t[] = pq.getTerms();
+    int pos[] = pq.getPositions();
+    for (int i = 0; i < t.length; i++) {
+      //System.out.println(i+". "+t[i]+"  pos: "+pos[i]);
+      assertEquals("term "+i+" = "+t[i]+" has wrong term-position!",expectedPositions[i],pos[i]);
+    }
+  }
+
+  public void testMatchAllDocs() throws Exception {
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", new MockAnalyzer(random, MockTokenizer.WHITESPACE, false));
+    assertEquals(new MatchAllDocsQuery(), qp.parse("*:*"));
+    assertEquals(new MatchAllDocsQuery(), qp.parse("(*:*)"));
+    BooleanQuery bq = (BooleanQuery)qp.parse("+*:* -*:*");
+    assertTrue(bq.getClauses()[0].getQuery() instanceof MatchAllDocsQuery);
+    assertTrue(bq.getClauses()[1].getQuery() instanceof MatchAllDocsQuery);
+  }
+  
+  private void assertHits(int expected, String query, IndexSearcher is) throws ParseException, IOException {
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "date", new MockAnalyzer(random, MockTokenizer.WHITESPACE, false));
+    qp.setLocale(Locale.ENGLISH);
+    Query q = qp.parse(query);
+    ScoreDoc[] hits = is.search(q, null, 1000).scoreDocs;
+    assertEquals(expected, hits.length);
+  }
+
+  @Override
+  public void tearDown() throws Exception {
+    BooleanQuery.setMaxClauseCount(originalMaxClauses);
+    super.tearDown();
+  }
+
+  // LUCENE-2002: make sure defaults for StandardAnalyzer's
+  // enableStopPositionIncr & QueryParser's enablePosIncr
+  // "match"
+  public void testPositionIncrements() throws Exception {
+    Directory dir = newDirectory();
+    Analyzer a = new MockAnalyzer(random, MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET, true);
+    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, a));
+    Document doc = new Document();
+    doc.add(newField("f", "the wizard of ozzy", Field.Store.NO, Field.Index.ANALYZED));
+    w.addDocument(doc);
+    IndexReader r = IndexReader.open(w, true);
+    w.close();
+    IndexSearcher s = newSearcher(r);
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "f", a);
+    Query q = qp.parse("\"wizard of ozzy\"");
+    assertEquals(1, s.search(q, 1).totalHits);
+    s.close();
+    r.close();
+    dir.close();
+  }
+
+  // LUCENE-2002: when we run javacc to regen QueryParser,
+  // we also run a replaceregexp step to fix 2 of the public
+  // ctors (change them to protected):
+  //
+  //   protected QueryParser(CharStream stream)
+  //
+  //   protected QueryParser(QueryParserTokenManager tm)
+  //
+  // This test is here as a safety, in case that ant step
+  // doesn't work for some reason.
+  public void testProtectedCtors() throws Exception {
+    try {
+      QueryParser.class.getConstructor(new Class[] {CharStream.class});
+      fail("please switch public QueryParser(CharStream) to be protected");
+    } catch (NoSuchMethodException nsme) {
+      // expected
+    }
+    try {
+      QueryParser.class.getConstructor(new Class[] {QueryParserTokenManager.class});
+      fail("please switch public QueryParser(QueryParserTokenManager) to be protected");
+    } catch (NoSuchMethodException nsme) {
+      // expected
+    }
+  }
+  
+  /**
+   * adds synonym of "dog" for "dogs".
+   */
+  private class MockSynonymFilter extends TokenFilter {
+    CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
+    PositionIncrementAttribute posIncAtt = addAttribute(PositionIncrementAttribute.class);
+    boolean addSynonym = false;
+    
+    public MockSynonymFilter(TokenStream input) {
+      super(input);
+    }
+
+    @Override
+    public final boolean incrementToken() throws IOException {
+      if (addSynonym) { // inject our synonym
+        clearAttributes();
+        termAtt.setEmpty().append("dog");
+        posIncAtt.setPositionIncrement(0);
+        addSynonym = false;
+        return true;
+      }
+      
+      if (input.incrementToken()) {
+        addSynonym = termAtt.toString().equals("dogs");
+        return true;
+      } else {
+        return false;
+      }
+    } 
+  }
+  
+  /** whitespace+lowercase analyzer with synonyms */
+  private class Analyzer1 extends Analyzer {
+    @Override
+    public TokenStream tokenStream(String fieldName, Reader reader) {
+      return new MockSynonymFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));
+    }
+  }
+  
+  /** whitespace+lowercase analyzer without synonyms */
+  private class Analyzer2 extends Analyzer {
+    @Override
+    public TokenStream tokenStream(String fieldName, Reader reader) {
+      return new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);
+    }
+  }
+  
+  /** query parser that doesn't expand synonyms when users use double quotes */
+  private class SmartQueryParser extends QueryParser {
+    Analyzer morePrecise = new Analyzer2();
+    
+    public SmartQueryParser() {
+      super(TEST_VERSION_CURRENT, "field", new Analyzer1());
+    }
+
+    @Override
+    protected Query getFieldQuery(String field, String queryText, boolean quoted)
+        throws ParseException {
+      if (quoted)
+        return newFieldQuery(morePrecise, field, queryText, quoted);
+      else
+        return super.getFieldQuery(field, queryText, quoted);
+    }
+  }
+  
+  public void testNewFieldQuery() throws Exception {
+    /** ordinary behavior, synonyms form uncoordinated boolean query */
+    QueryParser dumb = new QueryParser(TEST_VERSION_CURRENT, "field", new Analyzer1());
+    BooleanQuery expanded = new BooleanQuery(true);
+    expanded.add(new TermQuery(new Term("field", "dogs")), BooleanClause.Occur.SHOULD);
+    expanded.add(new TermQuery(new Term("field", "dog")), BooleanClause.Occur.SHOULD);
+    assertEquals(expanded, dumb.parse("\"dogs\""));
+    /** even with the phrase operator the behavior is the same */
+    assertEquals(expanded, dumb.parse("dogs"));
+    
+    /** custom behavior, the synonyms are expanded, unless you use quote operator */
+    QueryParser smart = new SmartQueryParser();
+    assertEquals(expanded, smart.parse("dogs"));
+    
+    Query unexpanded = new TermQuery(new Term("field", "dogs"));
+    assertEquals(unexpanded, smart.parse("\"dogs\""));
+  }
+  
+  /**
+   * Mock collation analyzer: indexes terms as "collated" + term
+   */
+  private class MockCollationFilter extends TokenFilter {
+    private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
+
+    protected MockCollationFilter(TokenStream input) {
+      super(input);
+    }
+
+    @Override
+    public boolean incrementToken() throws IOException {
+      if (input.incrementToken()) {
+        String term = termAtt.toString();
+        termAtt.setEmpty().append("collated").append(term);
+        return true;
+      } else {
+        return false;
+      }
+    }
+    
+  }
+  private class MockCollationAnalyzer extends Analyzer {
+    @Override
+    public TokenStream tokenStream(String fieldName, Reader reader) {
+      return new MockCollationFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));
+    }
+  }
+  
+  public void testCollatedRange() throws Exception {
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", new MockCollationAnalyzer());
+    qp.setAnalyzeRangeTerms(true);
+    Query expected = TermRangeQuery.newStringRange("field", "collatedabc", "collateddef", true, true);
+    Query actual = qp.parse("[abc TO def]");
+    assertEquals(expected, actual);
+  }
+
+  public void testDistanceAsEditsParsing() throws Exception {
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", new MockAnalyzer(random));
+    FuzzyQuery q = (FuzzyQuery) qp.parse("foobar~2");
+    assertEquals(2f, q.getMinSimilarity(), 0.0001f);
+  }
+
+  public void testPhraseQueryToString() throws ParseException {
+    Analyzer analyzer = new MockAnalyzer(random, MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET, true);
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", analyzer);
+    qp.setEnablePositionIncrements(true);
+    PhraseQuery q = (PhraseQuery)qp.parse("\"this hi this is a test is\"");
+    assertEquals("field:\"? hi ? ? ? test\"", q.toString());
+  }
+
+  public void testParseWildcardAndPhraseQueries() throws ParseException {
+    String field = "content";
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, field, new MockAnalyzer(random));
+    qp.setAllowLeadingWildcard(true);
+
+    String prefixQueries[][] = {
+        {"a*", "ab*", "abc*",},
+        {"h*", "hi*", "hij*", "\\\\7*"},
+        {"o*", "op*", "opq*", "\\\\\\\\*"},
+    };
+
+    String wildcardQueries[][] = {
+        {"*a*", "*ab*", "*abc**", "ab*e*", "*g?", "*f?1", "abc**"},
+        {"*h*", "*hi*", "*hij**", "hi*k*", "*n?", "*m?1", "hij**"},
+        {"*o*", "*op*", "*opq**", "op*q*", "*u?", "*t?1", "opq**"},
+    };
+
+     // test queries that must be prefix queries
+    for (int i = 0; i < prefixQueries.length; i++) {
+      for (int j = 0; j < prefixQueries[i].length; j++) {
+        String queryString = prefixQueries[i][j];
+        Query q = qp.parse(queryString);
+        assertEquals(PrefixQuery.class, q.getClass());
+      }
+    }
+
+    // test queries that must be wildcard queries
+    for (int i = 0; i < wildcardQueries.length; i++) {
+      for (int j = 0; j < wildcardQueries[i].length; j++) {
+        String qtxt = wildcardQueries[i][j];
+        Query q = qp.parse(qtxt);
+        assertEquals(WildcardQuery.class, q.getClass());
+      }
+    }
+  }
+
+  public void testPhraseQueryPositionIncrements() throws Exception {
+    CharacterRunAutomaton stopStopList =
+    new CharacterRunAutomaton(new RegExp("[sS][tT][oO][pP]").toAutomaton());
+
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field",
+        new MockAnalyzer(random, MockTokenizer.WHITESPACE, false, stopStopList, false));
+
+    PhraseQuery phraseQuery = new PhraseQuery();
+    phraseQuery.add(new Term("field", "1"));
+    phraseQuery.add(new Term("field", "2"));
+
+    assertEquals(phraseQuery, qp.parse("\"1 2\""));
+    assertEquals(phraseQuery, qp.parse("\"1 stop 2\""));
+
+    qp.setEnablePositionIncrements(true);
+    assertEquals(phraseQuery, qp.parse("\"1 stop 2\""));
+
+    qp.setEnablePositionIncrements(false);
+    assertEquals(phraseQuery, qp.parse("\"1 stop 2\""));
+
+    qp = new QueryParser(TEST_VERSION_CURRENT, "field",
+                         new MockAnalyzer(random, MockTokenizer.WHITESPACE, false, stopStopList, true));
+    qp.setEnablePositionIncrements(true);
+
+    phraseQuery = new PhraseQuery();
+    phraseQuery.add(new Term("field", "1"));
+    phraseQuery.add(new Term("field", "2"), 2);
+    assertEquals(phraseQuery, qp.parse("\"1 stop 2\""));
+  }
+
+  public void testMatchAllQueryParsing() throws Exception {
+    // test simple parsing of MatchAllDocsQuery
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "key", new MockAnalyzer(random));
+    assertEquals(new MatchAllDocsQuery(), qp.parse(new MatchAllDocsQuery().toString()));
+
+    // test parsing with non-default boost
+    MatchAllDocsQuery query = new MatchAllDocsQuery();
+    query.setBoost(2.3f);
+    assertEquals(query, qp.parse(query.toString()));
+  }
+  
+}
diff --git a/solr/common-build.xml b/solr/common-build.xml
index a6e400f..5478079 100644
--- a/solr/common-build.xml
+++ b/solr/common-build.xml
@@ -89,6 +89,8 @@
         property="grouping.uptodate" classpath.property="grouping.jar"/>
   <module-uptodate name="queries" jarfile="${common.dir}/../modules/queries/build/lucene-queries-${version}.jar"
         property="queries.uptodate" classpath.property="queries.jar"/>
+  <module-uptodate name="queryparser" jarfile="${common.dir}/../modules/queryparser/build/lucene-queryparser-${version}.jar"
+        property="queryparser.uptodate" classpath.property="queryparser.jar"/>
   <contrib-uptodate name="highlighter" property="highlighter.uptodate" classpath.property="highlighter.jar"/>
   <contrib-uptodate name="memory" property="memory.uptodate" classpath.property="memory.jar"/>
   <contrib-uptodate name="misc" property="misc.uptodate" classpath.property="misc.jar"/>
@@ -120,6 +122,11 @@
       <propertyset refid="uptodate.and.compiled.properties"/>
     </ant>
   </target>
+  <target name="compile-queryparser" unless="queryparser.uptodate">
+  	<ant dir="${common.dir}/../modules/queryparser" target="default" inheritAll="false">
+      <propertyset refid="uptodate.and.compiled.properties"/>
+    </ant>
+  </target>
   <target name="compile-highlighter" unless="highlighter.uptodate">
   	<ant dir="${common.dir}/contrib/highlighter" target="default" inheritAll="false">
       <propertyset refid="uptodate.and.compiled.properties"/>
@@ -163,6 +170,7 @@
   	<pathelement path="${suggest.jar}"/>
     <pathelement path="${grouping.jar}"/>
     <pathelement path="${queries.jar}"/>
+    <pathelement path="${queryparser.jar}"/>
     <pathelement location="${common-solr.dir}/build/solr-solrj/classes/java"/>
     <pathelement location="${common-solr.dir}/build/solr-core/classes/java"/>
     <path refid="base.classpath"/>
@@ -245,7 +253,7 @@
   <target name="prep-lucene-jars"
           depends="compile-analyzers-common, compile-analyzers-phonetic, compile-suggest,
                    compile-highlighter, compile-memory, compile-misc, compile-queries-contrib,
-                   compile-spatial, compile-grouping, compile-queries">
+                   compile-spatial, compile-grouping, compile-queries, compile-queryparser">
     <ant dir="${common.dir}" target="default" inheritall="false">
       <propertyset refid="uptodate.and.compiled.properties"/>
     </ant>
@@ -260,6 +268,7 @@
       <fileset file="${grouping.jar}" />
       <fileset file="${common-module.jar}" />
       <fileset file="${queries.jar}" />
+      <fileset file="${queryparser.jar}" />
       <fileset file="${highlighter.jar}" />
       <fileset file="${memory.jar}" />
       <fileset file="${misc.jar}" />
diff --git a/solr/core/src/java/org/apache/solr/handler/MoreLikeThisHandler.java b/solr/core/src/java/org/apache/solr/handler/MoreLikeThisHandler.java
index 216c263..507f3b9 100644
--- a/solr/core/src/java/org/apache/solr/handler/MoreLikeThisHandler.java
+++ b/solr/core/src/java/org/apache/solr/handler/MoreLikeThisHandler.java
@@ -32,7 +32,7 @@ import java.util.regex.Pattern;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.lucene.search.*;
 import org.apache.lucene.search.similar.MoreLikeThis;
 import org.apache.solr.common.SolrException;
diff --git a/solr/core/src/java/org/apache/solr/handler/RequestHandlerBase.java b/solr/core/src/java/org/apache/solr/handler/RequestHandlerBase.java
index 3fe4e02..c5df537 100644
--- a/solr/core/src/java/org/apache/solr/handler/RequestHandlerBase.java
+++ b/solr/core/src/java/org/apache/solr/handler/RequestHandlerBase.java
@@ -17,6 +17,7 @@
 
 package org.apache.solr.handler;
 
+import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.solr.common.SolrException;
 import org.apache.solr.common.params.SolrParams;
 import org.apache.solr.common.util.NamedList;
@@ -27,7 +28,6 @@ import org.apache.solr.request.SolrQueryRequest;
 import org.apache.solr.request.SolrRequestHandler;
 import org.apache.solr.response.SolrQueryResponse;
 import org.apache.solr.util.SolrPluginUtils;
-import org.apache.lucene.queryParser.ParseException;
 
 import java.net.URL;
 
diff --git a/solr/core/src/java/org/apache/solr/handler/component/FacetComponent.java b/solr/core/src/java/org/apache/solr/handler/component/FacetComponent.java
index a398967..ffe51a8 100644
--- a/solr/core/src/java/org/apache/solr/handler/component/FacetComponent.java
+++ b/solr/core/src/java/org/apache/solr/handler/component/FacetComponent.java
@@ -17,7 +17,7 @@
 
 package org.apache.solr.handler.component;
 
-import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.lucene.util.OpenBitSet;
 import org.apache.solr.common.SolrException;
 import org.apache.solr.common.params.CommonParams;
diff --git a/solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java b/solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java
index 91b884c..84cd42d 100644
--- a/solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java
+++ b/solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java
@@ -21,7 +21,7 @@ import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader.ReaderContext;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.lucene.search.*;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CharsRef;
diff --git a/solr/core/src/java/org/apache/solr/handler/component/SearchHandler.java b/solr/core/src/java/org/apache/solr/handler/component/SearchHandler.java
index 9991152..de6349d 100644
--- a/solr/core/src/java/org/apache/solr/handler/component/SearchHandler.java
+++ b/solr/core/src/java/org/apache/solr/handler/component/SearchHandler.java
@@ -17,7 +17,7 @@
 
 package org.apache.solr.handler.component;
 
-import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.solr.common.SolrException;
 import org.apache.solr.common.params.CommonParams;
 import org.apache.solr.common.params.ModifiableSolrParams;
diff --git a/solr/core/src/java/org/apache/solr/request/SimpleFacets.java b/solr/core/src/java/org/apache/solr/request/SimpleFacets.java
index 1593d92..7fa02ee 100644
--- a/solr/core/src/java/org/apache/solr/request/SimpleFacets.java
+++ b/solr/core/src/java/org/apache/solr/request/SimpleFacets.java
@@ -18,7 +18,7 @@
 package org.apache.solr.request;
 
 import org.apache.lucene.index.*;
-import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.lucene.search.*;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CharsRef;
diff --git a/solr/core/src/java/org/apache/solr/schema/IndexSchema.java b/solr/core/src/java/org/apache/solr/schema/IndexSchema.java
index 4fd106d..ad501ce 100644
--- a/solr/core/src/java/org/apache/solr/schema/IndexSchema.java
+++ b/solr/core/src/java/org/apache/solr/schema/IndexSchema.java
@@ -24,7 +24,6 @@ import org.apache.lucene.search.DefaultSimilarity;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Similarity;
 import org.apache.lucene.search.SimilarityProvider;
-import org.apache.lucene.queryParser.QueryParser;
 import org.apache.lucene.util.Version;
 import org.apache.solr.common.ResourceLoader;
 import org.apache.solr.common.SolrException;
diff --git a/solr/core/src/java/org/apache/solr/search/BoostQParserPlugin.java b/solr/core/src/java/org/apache/solr/search/BoostQParserPlugin.java
index 1c194c8..5acd6b1 100755
--- a/solr/core/src/java/org/apache/solr/search/BoostQParserPlugin.java
+++ b/solr/core/src/java/org/apache/solr/search/BoostQParserPlugin.java
@@ -20,7 +20,7 @@ import org.apache.lucene.queries.function.BoostedQuery;
 import org.apache.lucene.queries.function.FunctionQuery;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.valuesource.QueryValueSource;
-import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.lucene.search.Query;
 import org.apache.solr.common.params.SolrParams;
 import org.apache.solr.common.util.NamedList;
diff --git a/solr/core/src/java/org/apache/solr/search/DisMaxQParser.java b/solr/core/src/java/org/apache/solr/search/DisMaxQParser.java
index 965fe99..e64fa18 100644
--- a/solr/core/src/java/org/apache/solr/search/DisMaxQParser.java
+++ b/solr/core/src/java/org/apache/solr/search/DisMaxQParser.java
@@ -16,8 +16,8 @@
  */
 package org.apache.solr.search;
 
-import org.apache.lucene.queryParser.ParseException;
-import org.apache.lucene.queryParser.QueryParser.Operator;
+import org.apache.lucene.queryparser.classic.ParseException;
+import org.apache.lucene.queryparser.classic.QueryParser;
 import org.apache.lucene.search.BooleanClause;
 import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.Query;
@@ -59,10 +59,10 @@ public class DisMaxQParser extends QParser {
    */
   public static String parseMinShouldMatch(final IndexSchema schema, 
                                            final SolrParams params) {
-    Operator op = QueryParsing.getQueryParserDefaultOperator
+    QueryParser.Operator op = QueryParsing.getQueryParserDefaultOperator
       (schema, params.get(QueryParsing.OP));
     return params.get(DisMaxParams.MM, 
-                      op.equals(Operator.AND) ? "100%" : "0%");
+                      op.equals(QueryParser.Operator.AND) ? "100%" : "0%");
   }
 
   public DisMaxQParser(String qstr, SolrParams localParams, SolrParams params, SolrQueryRequest req) {
diff --git a/solr/core/src/java/org/apache/solr/search/ExtendedDismaxQParserPlugin.java b/solr/core/src/java/org/apache/solr/search/ExtendedDismaxQParserPlugin.java
index 11139ab..28650ae 100755
--- a/solr/core/src/java/org/apache/solr/search/ExtendedDismaxQParserPlugin.java
+++ b/solr/core/src/java/org/apache/solr/search/ExtendedDismaxQParserPlugin.java
@@ -27,8 +27,8 @@ import org.apache.lucene.queries.function.FunctionQuery;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.valuesource.ProductFloatFunction;
 import org.apache.lucene.queries.function.valuesource.QueryValueSource;
-import org.apache.lucene.queryParser.ParseException;
-import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.queryparser.classic.ParseException;
+import org.apache.lucene.queryparser.classic.QueryParser;
 import org.apache.lucene.search.*;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
diff --git a/solr/core/src/java/org/apache/solr/search/FieldQParserPlugin.java b/solr/core/src/java/org/apache/solr/search/FieldQParserPlugin.java
index 499086f..4f01081 100644
--- a/solr/core/src/java/org/apache/solr/search/FieldQParserPlugin.java
+++ b/solr/core/src/java/org/apache/solr/search/FieldQParserPlugin.java
@@ -16,7 +16,7 @@
  */
 package org.apache.solr.search;
 
-import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.lucene.search.*;
 import org.apache.solr.common.params.SolrParams;
 import org.apache.solr.common.util.NamedList;
diff --git a/solr/core/src/java/org/apache/solr/search/FunctionQParser.java b/solr/core/src/java/org/apache/solr/search/FunctionQParser.java
index d7ef606..58a766b 100755
--- a/solr/core/src/java/org/apache/solr/search/FunctionQParser.java
+++ b/solr/core/src/java/org/apache/solr/search/FunctionQParser.java
@@ -19,7 +19,7 @@ package org.apache.solr.search;
 import org.apache.lucene.queries.function.FunctionQuery;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.valuesource.*;
-import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.lucene.search.Query;
 import org.apache.solr.common.params.SolrParams;
 import org.apache.solr.request.SolrQueryRequest;
diff --git a/solr/core/src/java/org/apache/solr/search/FunctionRangeQParserPlugin.java b/solr/core/src/java/org/apache/solr/search/FunctionRangeQParserPlugin.java
index 11be096..9abe44b 100755
--- a/solr/core/src/java/org/apache/solr/search/FunctionRangeQParserPlugin.java
+++ b/solr/core/src/java/org/apache/solr/search/FunctionRangeQParserPlugin.java
@@ -22,7 +22,7 @@ import org.apache.lucene.queries.function.FunctionQuery;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.ValueSourceScorer;
 import org.apache.lucene.queries.function.valuesource.QueryValueSource;
-import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.lucene.search.*;
 import org.apache.solr.common.params.SolrParams;
 import org.apache.solr.common.util.NamedList;
diff --git a/solr/core/src/java/org/apache/solr/search/Grouping.java b/solr/core/src/java/org/apache/solr/search/Grouping.java
index 4ee4c80..8fff90b 100755
--- a/solr/core/src/java/org/apache/solr/search/Grouping.java
+++ b/solr/core/src/java/org/apache/solr/search/Grouping.java
@@ -24,7 +24,7 @@ import org.apache.lucene.queries.function.DocValues;
 import org.apache.lucene.queries.function.FunctionQuery;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.valuesource.QueryValueSource;
-import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.lucene.search.*;
 import org.apache.lucene.search.grouping.*;
 import org.apache.lucene.util.BytesRef;
diff --git a/solr/core/src/java/org/apache/solr/search/JoinQParserPlugin.java b/solr/core/src/java/org/apache/solr/search/JoinQParserPlugin.java
index 61da1b1..ca71ceb 100644
--- a/solr/core/src/java/org/apache/solr/search/JoinQParserPlugin.java
+++ b/solr/core/src/java/org/apache/solr/search/JoinQParserPlugin.java
@@ -17,7 +17,7 @@
 package org.apache.solr.search;
 
 import org.apache.lucene.index.*;
-import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.lucene.search.*;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
diff --git a/solr/core/src/java/org/apache/solr/search/LuceneQParserPlugin.java b/solr/core/src/java/org/apache/solr/search/LuceneQParserPlugin.java
index b9d61e9..793af1c 100755
--- a/solr/core/src/java/org/apache/solr/search/LuceneQParserPlugin.java
+++ b/solr/core/src/java/org/apache/solr/search/LuceneQParserPlugin.java
@@ -16,7 +16,7 @@
  */
 package org.apache.solr.search;
 
-import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.Sort;
 import org.apache.solr.common.SolrException;
diff --git a/solr/core/src/java/org/apache/solr/search/NestedQParserPlugin.java b/solr/core/src/java/org/apache/solr/search/NestedQParserPlugin.java
index cb011ec..ad3bf5c 100755
--- a/solr/core/src/java/org/apache/solr/search/NestedQParserPlugin.java
+++ b/solr/core/src/java/org/apache/solr/search/NestedQParserPlugin.java
@@ -17,7 +17,7 @@
 package org.apache.solr.search;
 
 import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.lucene.search.Query;
 import org.apache.solr.common.params.SolrParams;
 import org.apache.solr.common.util.NamedList;
diff --git a/solr/core/src/java/org/apache/solr/search/PrefixQParserPlugin.java b/solr/core/src/java/org/apache/solr/search/PrefixQParserPlugin.java
index 8e1858c..6258209 100755
--- a/solr/core/src/java/org/apache/solr/search/PrefixQParserPlugin.java
+++ b/solr/core/src/java/org/apache/solr/search/PrefixQParserPlugin.java
@@ -17,7 +17,7 @@
 package org.apache.solr.search;
 
 import org.apache.lucene.index.Term;
-import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.lucene.search.PrefixQuery;
 import org.apache.lucene.search.Query;
 import org.apache.solr.common.params.SolrParams;
diff --git a/solr/core/src/java/org/apache/solr/search/QParser.java b/solr/core/src/java/org/apache/solr/search/QParser.java
index 5050464..a8b7cf8 100755
--- a/solr/core/src/java/org/apache/solr/search/QParser.java
+++ b/solr/core/src/java/org/apache/solr/search/QParser.java
@@ -16,7 +16,7 @@
  */
 package org.apache.solr.search;
 
-import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.Sort;
 import org.apache.solr.common.params.CommonParams;
diff --git a/solr/core/src/java/org/apache/solr/search/QueryParsing.java b/solr/core/src/java/org/apache/solr/search/QueryParsing.java
index 4ccfd0e..44f9adf 100644
--- a/solr/core/src/java/org/apache/solr/search/QueryParsing.java
+++ b/solr/core/src/java/org/apache/solr/search/QueryParsing.java
@@ -20,8 +20,8 @@ package org.apache.solr.search;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.queries.function.FunctionQuery;
 import org.apache.lucene.queries.function.valuesource.QueryValueSource;
-import org.apache.lucene.queryParser.ParseException;
-import org.apache.lucene.queryParser.QueryParser.Operator;
+import org.apache.lucene.queryparser.classic.ParseException;
+import org.apache.lucene.queryparser.classic.QueryParser;
 import org.apache.lucene.search.BooleanClause;
 import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.ConstantScoreQuery;
@@ -79,11 +79,11 @@ public class QueryParsing {
    * @see IndexSchema#getQueryParserDefaultOperator()
    * @see #OP
    */
-  public static Operator getQueryParserDefaultOperator(final IndexSchema sch, 
+  public static QueryParser.Operator getQueryParserDefaultOperator(final IndexSchema sch,
                                                        final String override) {
     String val = override;
     if (null == val) val = sch.getQueryParserDefaultOperator();
-    return "AND".equals(val) ? Operator.AND : Operator.OR;
+    return "AND".equals(val) ? QueryParser.Operator.AND : QueryParser.Operator.OR;
   }
 
 
diff --git a/solr/core/src/java/org/apache/solr/search/RawQParserPlugin.java b/solr/core/src/java/org/apache/solr/search/RawQParserPlugin.java
index cbbea89..7c7d894 100644
--- a/solr/core/src/java/org/apache/solr/search/RawQParserPlugin.java
+++ b/solr/core/src/java/org/apache/solr/search/RawQParserPlugin.java
@@ -17,7 +17,7 @@
 package org.apache.solr.search;
 
 import org.apache.lucene.index.Term;
-import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.TermQuery;
 import org.apache.solr.common.params.SolrParams;
diff --git a/solr/core/src/java/org/apache/solr/search/ReturnFields.java b/solr/core/src/java/org/apache/solr/search/ReturnFields.java
index 03aaa4b..eb63a90 100644
--- a/solr/core/src/java/org/apache/solr/search/ReturnFields.java
+++ b/solr/core/src/java/org/apache/solr/search/ReturnFields.java
@@ -22,7 +22,7 @@ import org.apache.commons.io.FilenameUtils;
 import org.apache.lucene.queries.function.FunctionQuery;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.valuesource.QueryValueSource;
-import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.lucene.search.Query;
 import org.apache.solr.common.SolrException;
 import org.apache.solr.common.params.CommonParams;
diff --git a/solr/core/src/java/org/apache/solr/search/SolrQueryParser.java b/solr/core/src/java/org/apache/solr/search/SolrQueryParser.java
index 80db331..94c98bc 100644
--- a/solr/core/src/java/org/apache/solr/search/SolrQueryParser.java
+++ b/solr/core/src/java/org/apache/solr/search/SolrQueryParser.java
@@ -22,8 +22,8 @@ import java.util.Map;
 import java.util.Map.Entry;
 
 import org.apache.lucene.index.Term;
-import org.apache.lucene.queryParser.ParseException;
-import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.queryparser.classic.ParseException;
+import org.apache.lucene.queryparser.classic.QueryParser;
 import org.apache.lucene.search.*;
 import org.apache.lucene.util.ToStringUtils;
 import org.apache.lucene.util.Version;
diff --git a/solr/core/src/java/org/apache/solr/search/SpatialFilterQParser.java b/solr/core/src/java/org/apache/solr/search/SpatialFilterQParser.java
index c97a4dc..05b749e 100644
--- a/solr/core/src/java/org/apache/solr/search/SpatialFilterQParser.java
+++ b/solr/core/src/java/org/apache/solr/search/SpatialFilterQParser.java
@@ -17,7 +17,7 @@ package org.apache.solr.search;
  */
 
 
-import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.spatial.geometry.DistanceUnits;
 import org.apache.lucene.spatial.DistanceUtils;
diff --git a/solr/core/src/java/org/apache/solr/search/TermQParserPlugin.java b/solr/core/src/java/org/apache/solr/search/TermQParserPlugin.java
index 39ce3d5..4670a38 100644
--- a/solr/core/src/java/org/apache/solr/search/TermQParserPlugin.java
+++ b/solr/core/src/java/org/apache/solr/search/TermQParserPlugin.java
@@ -17,7 +17,7 @@
 package org.apache.solr.search;
 
 import org.apache.lucene.index.Term;
-import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.util.BytesRef;
diff --git a/solr/core/src/java/org/apache/solr/search/ValueSourceParser.java b/solr/core/src/java/org/apache/solr/search/ValueSourceParser.java
index 8999ac8..a749764 100755
--- a/solr/core/src/java/org/apache/solr/search/ValueSourceParser.java
+++ b/solr/core/src/java/org/apache/solr/search/ValueSourceParser.java
@@ -25,7 +25,7 @@ import org.apache.lucene.queries.function.docvalues.BoolDocValues;
 import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
 import org.apache.lucene.queries.function.docvalues.LongDocValues;
 import org.apache.lucene.queries.function.valuesource.*;
-import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.TermQuery;
diff --git a/solr/core/src/java/org/apache/solr/search/function/distance/HaversineConstFunction.java b/solr/core/src/java/org/apache/solr/search/function/distance/HaversineConstFunction.java
index dba15dd..b2aa275 100755
--- a/solr/core/src/java/org/apache/solr/search/function/distance/HaversineConstFunction.java
+++ b/solr/core/src/java/org/apache/solr/search/function/distance/HaversineConstFunction.java
@@ -24,7 +24,7 @@ import org.apache.lucene.queries.function.valuesource.ConstNumberSource;
 import org.apache.lucene.queries.function.valuesource.DoubleConstValueSource;
 import org.apache.lucene.queries.function.valuesource.MultiValueSource;
 import org.apache.lucene.queries.function.valuesource.VectorValueSource;
-import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.spatial.DistanceUtils;
 import org.apache.lucene.spatial.tier.InvalidGeoException;
diff --git a/solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java b/solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java
index afe4558..d2efaef 100644
--- a/solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java
+++ b/solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java
@@ -23,7 +23,7 @@ package org.apache.solr.update;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.lucene.search.BooleanClause;
 import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.Query;
diff --git a/solr/core/src/java/org/apache/solr/util/SolrPluginUtils.java b/solr/core/src/java/org/apache/solr/util/SolrPluginUtils.java
index 80b5892..9cd4ac3 100644
--- a/solr/core/src/java/org/apache/solr/util/SolrPluginUtils.java
+++ b/solr/core/src/java/org/apache/solr/util/SolrPluginUtils.java
@@ -18,8 +18,8 @@
 package org.apache.solr.util;
 
 import org.apache.lucene.document.Document;
-import org.apache.lucene.queryParser.ParseException;
-import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.queryparser.classic.ParseException;
+import org.apache.lucene.queryparser.classic.QueryParser;
 import org.apache.lucene.search.*;
 import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.solr.common.SolrDocument;
diff --git a/solr/core/src/test/org/apache/solr/core/DummyValueSourceParser.java b/solr/core/src/test/org/apache/solr/core/DummyValueSourceParser.java
index 4e714dd..5939e47 100644
--- a/solr/core/src/test/org/apache/solr/core/DummyValueSourceParser.java
+++ b/solr/core/src/test/org/apache/solr/core/DummyValueSourceParser.java
@@ -19,7 +19,7 @@ package org.apache.solr.core;
 import org.apache.lucene.queries.function.DocValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.valuesource.SimpleFloatFunction;
-import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.solr.common.util.NamedList;
 import org.apache.solr.search.FunctionQParser;
 import org.apache.solr.search.ValueSourceParser;
diff --git a/solr/core/src/test/org/apache/solr/core/TestArbitraryIndexDir.java b/solr/core/src/test/org/apache/solr/core/TestArbitraryIndexDir.java
index 104c5ed..12590a7 100644
--- a/solr/core/src/test/org/apache/solr/core/TestArbitraryIndexDir.java
+++ b/solr/core/src/test/org/apache/solr/core/TestArbitraryIndexDir.java
@@ -28,7 +28,7 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.Version;
 import org.apache.solr.common.SolrException;
@@ -77,7 +77,7 @@ public class TestArbitraryIndexDir extends AbstractSolrTestCase{
   }
 
   @Test
-  public void testLoadNewIndexDir() throws IOException, ParserConfigurationException, SAXException, ParseException{
+  public void testLoadNewIndexDir() throws IOException, ParserConfigurationException, SAXException, ParseException {
     //add a doc in original index dir
     assertU(adoc("id", String.valueOf(1),
         "name", "name"+String.valueOf(1)));
diff --git a/solr/core/src/test/org/apache/solr/search/FooQParserPlugin.java b/solr/core/src/test/org/apache/solr/search/FooQParserPlugin.java
index b58006f..36fe30b 100755
--- a/solr/core/src/test/org/apache/solr/search/FooQParserPlugin.java
+++ b/solr/core/src/test/org/apache/solr/search/FooQParserPlugin.java
@@ -17,12 +17,12 @@
 
 package org.apache.solr.search;
 
+import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.solr.common.params.SolrParams;
 import org.apache.solr.common.util.NamedList;
 import org.apache.solr.request.SolrQueryRequest;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.queryParser.ParseException;
 import org.apache.lucene.index.Term;
 
 
diff --git a/solr/core/src/test/org/apache/solr/search/function/NvlValueSourceParser.java b/solr/core/src/test/org/apache/solr/search/function/NvlValueSourceParser.java
index 726c548..d17d1b5 100755
--- a/solr/core/src/test/org/apache/solr/search/function/NvlValueSourceParser.java
+++ b/solr/core/src/test/org/apache/solr/search/function/NvlValueSourceParser.java
@@ -20,7 +20,7 @@ package org.apache.solr.search.function;
 import org.apache.lucene.queries.function.DocValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.valuesource.SimpleFloatFunction;
-import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.solr.common.util.NamedList;
 import org.apache.solr.search.FunctionQParser;
 import org.apache.solr.search.ValueSourceParser;

