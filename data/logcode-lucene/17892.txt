GitDiffStart: 71b59ca5667e93f21470370dca9dbec21340d527 | Tue May 25 20:16:44 2010 +0000
diff --git a/lucene/build.xml b/lucene/build.xml
index 5377724..a9d301c 100644
--- a/lucene/build.xml
+++ b/lucene/build.xml
@@ -23,21 +23,11 @@
 
   <import file="common-build.xml"/>
 
-  <property name="build.demo.template" value="src/demo/demo-build.template"/> 
-
-  <property name="demo.name" value="lucene-demos-${version}"/>
-  <property name="demo.war.name" value="luceneweb"/>
-
   <!-- Build classpath -->
   <path id="classpath">
     <pathelement location="${build.dir}/classes/java"/>
   </path>
 
-  <path id="demo.classpath">
-    <path refid="classpath"/>
-    <pathelement location="${build.dir}/classes/demo"/>
-  </path>
-  
   <path id="test.classpath">
   	<path refid="classpath"/>
     <path refid="junit-path"/>
@@ -57,10 +47,10 @@
               excludes="contrib/db/*/lib/,contrib/*/ext-libs/,src/site/build/,contrib/benchmark/temp/,contrib/benchmark/work/"
   />
   <patternset id="binary.build.dist.patterns"
-              includes="${final.name}.jar,${demo.war.name}.war,${demo.name}.jar,docs/,contrib/*/*.jar,contrib/*/*.war, contrib/*/*/*.jar"
+              includes="${final.name}.jar,docs/,contrib/*/*.jar,contrib/*/*.war, contrib/*/*/*.jar"
   />
   <patternset id="binary.root.dist.patterns"
-              includes="src/demo/,src/jsp/,docs/,*.txt,contrib/*/README*,**/CHANGES.txt,lib/servlet-api-*.jar"
+              includes="docs/,*.txt,contrib/*/README*,**/CHANGES.txt"
               excludes="${build.demo.template}"
   />
 
@@ -177,70 +167,7 @@ The source distribution does not contain sources of the previous Lucene Java ver
   <!--                                                                    -->
   <!-- ================================================================== -->
 
-  <target name="jar-demo" depends="compile-demo">
-  	<sequential>
-  	  <build-manifest title="Lucene Search Engine: demos"/>
-      <jar
-        destfile="${build.dir}/${demo.name}.jar"
-        basedir="${build.dir}/classes/demo"
-        excludes="**/*.java"
-      	manifest="${manifest.file}">
-        <metainf dir="${common.dir}">
-          <include name="LICENSE.txt"/>
-          <include name="NOTICE.txt"/>
-        </metainf>
-      </jar>
-  	</sequential>
-  </target>
-
-  <target name="jar-demo-src" depends="compile-demo">
-  	<sequential>
-  	  <build-manifest title="Lucene Search Engine: demos"/>
-      <jar
-        destfile="${build.dir}/${demo.name}-src.jar"
-        basedir="src/demo"
-      	manifest="${manifest.file}">
-        <metainf dir="${common.dir}">
-          <include name="LICENSE.txt"/>
-          <include name="NOTICE.txt"/>
-        </metainf>
-      </jar>
-  	</sequential>
-  </target>
-
-  <target name="war-demo" depends="jar-core,jar-demo">
-    <sequential>
-      <build-manifest title="Lucene Search Engine: demos"/>
-  	  <war destfile="${build.dir}/${demo.war.name}.war"
-           webxml="src/jsp/WEB-INF/web.xml"
-      	   manifest="${manifest.file}">
-        <fileset dir="src/jsp" excludes="WEB-INF/web.xml"/>
-        <lib dir="${build.dir}" includes="${demo.name}.jar"/>
-        <lib dir="${build.dir}" includes="${final.name}.jar"/>
-        <metainf dir="${common.dir}">
-          <include name="LICENSE.txt"/>
-          <include name="NOTICE.txt"/>
-        </metainf>
-      </war>
-    </sequential>
-  </target>
-
   <target name="compile-core" depends="jflex-notice, javacc-notice, common.compile-core"/>
-  
-  <!-- ================================================================== -->
-  <!-- B U I L D  D E M O                                                 -->
-  <!-- ================================================================== -->
-  <!--                                                                    -->
-  <!-- ================================================================== -->
-  <target name="compile-demo" depends="compile-core">
-    <mkdir dir="${build.dir}/classes/demo"/>
-
-    <compile
-      srcdir="src/demo"
-      destdir="${build.dir}/classes/demo">
-      <classpath refid="demo.classpath"/>
-    </compile>
-  </target>
 
   <!-- ================================================================== -->
   <!-- D O C U M E N T A T I O N                                          -->
@@ -252,7 +179,7 @@ The source distribution does not contain sources of the previous Lucene Java ver
   </target>
 
   <target name="javadocs" description="Generate javadoc" 
-          depends="javadocs-all, javadocs-core, javadocs-demo, javadocs-contrib">
+          depends="javadocs-all, javadocs-core, javadocs-contrib">
     <echo file="${javadoc.dir}/index.html" append="false">
 <![CDATA[<html><head><title>${Name} ${version} Javadoc Index</title></head>
 <body>
@@ -266,7 +193,6 @@ The source distribution does not contain sources of the previous Lucene Java ver
     <contrib-crawl target="javadocs-index.html" failonerror="false"/>
     <echo file="${javadoc.dir}/index.html" append="true"><![CDATA[
   </ul>
-  <li><a href="demo/index.html">Demo</a></li>
 </ul></body>]]></echo>
   </target>
 	
@@ -285,27 +211,12 @@ The source distribution does not contain sources of the previous Lucene Java ver
     </sequential>
   </target>
 
-  <target name="javadocs-demo" description="Generate javadoc for demo classes">
-  	<sequential>
-      <mkdir dir="${javadoc.dir}/demo"/>
-      <invoke-javadoc
-        destdir="${javadoc.dir}/demo"
-      	title="${Name} ${version} demo API">
-        <sources>
-          <packageset dir="src/demo"/>
-          <link href=""/>
-        </sources>
-      </invoke-javadoc>
-      <jarify basedir="${javadoc.dir}/demo" destfile="${build.dir}/${demo.name}-javadoc.jar"/>
-    </sequential>
-  </target>
-	
   <target name="javadocs-contrib" description="Generate javadoc for contrib classes">
     <contrib-crawl target="javadocs"
                    failonerror="false"/>
   </target>
   	
-  <target name="javadocs-all" description="Generate javadoc for core, demo and contrib classes" depends="build-contrib">
+  <target name="javadocs-all" description="Generate javadoc for core and contrib classes" depends="build-contrib">
   	<sequential>
       <mkdir dir="${javadoc.dir}/all"/>
       <invoke-javadoc
@@ -314,8 +225,6 @@ The source distribution does not contain sources of the previous Lucene Java ver
           <!-- TODO: find a dynamic way to do include multiple source roots -->
           <packageset dir="src/java"/>
 
-          <packageset dir="src/demo"/>
-
           <!-- please keep this list up to date, and in alpha order...   -->
         
           <!-- ie: `find contrib/* -path \*src/java | sort` -->
@@ -348,11 +257,10 @@ The source distribution does not contain sources of the previous Lucene Java ver
           <!-- packages are not being matched by any of these rules   -->
   
           <group title="Core" packages="org.apache.*:org.apache.lucene.analysis:org.apache.lucene.analysis.standard*:org.apache.lucene.analysis.tokenattributes*"/>
-  
-          <group title="Demo" packages="org.apache.lucene.demo*"/>
-  
+    
           <group title="contrib: Ant" packages="org.apache.lucene.ant*"/>
           <group title="contrib: Benchmark" packages="org.apache.lucene.benchmark*"/>
+          <group title="contrib: Demo" packages="org.apache.lucene.demo*"/>
           <group title="contrib: ICU" packages="org.apache.lucene.collation*"/>
           <group title="contrib: DB" packages="org.apache.lucene.store.db*:org.apache.lucene.store.je*:com.sleepycat*"/>
           <group title="contrib: Highlighter" packages="org.apache.lucene.search.highlight:*org.apache.lucene.search.vectorhighlight*"/>
@@ -379,7 +287,7 @@ The source distribution does not contain sources of the previous Lucene Java ver
   <!-- ================================================================== -->
   <!--                                                                    -->
   <!-- ================================================================== -->
-  <target name="package" depends="jar-core, javadocs, war-demo, build-contrib, init-dist, changes-to-html">
+  <target name="package" depends="jar-core, javadocs, build-contrib, init-dist, changes-to-html">
      <copy file="${build.demo.template}" tofile="${build.dir}/build-demo.xml">
         <filterset begintoken="@PLACEHOLDER_" endtoken="@"> 
 	  <filter token="version" value="${version}"/>
@@ -518,7 +426,7 @@ The source distribution does not contain sources of the previous Lucene Java ver
 
   <target name="dist-all" depends="dist, dist-src"/>
 
-  <target name="generate-maven-artifacts" depends="maven.ant.tasks-check, package, jar-src, jar-demo-src, javadocs">
+  <target name="generate-maven-artifacts" depends="maven.ant.tasks-check, package, jar-src, javadocs">
     <sequential>
       <m2-deploy pom.xml="lucene-parent-pom.xml.template"/>
       <m2-deploy pom.xml="lucene-core-pom.xml.template">
@@ -529,16 +437,7 @@ The source distribution does not contain sources of the previous Lucene Java ver
                   classifier="javadoc"/>
         </artifact-attachments>
       </m2-deploy>
-      
-      <m2-deploy pom.xml="lucene-demos-pom.xml.template">
-        <artifact-attachments>
-          <attach file="${build.dir}/${demo.name}-src.jar"
-                  classifier="sources"/>
-          <attach file="${build.dir}/${demo.name}-javadoc.jar"
-                  classifier="javadoc"/>
-        </artifact-attachments>
-      </m2-deploy>
-      
+           
       <m2-deploy pom.xml="lucene-contrib-pom.xml.template"/>
       <contrib-crawl target="dist-maven"/>
     </sequential>
@@ -604,13 +503,10 @@ The source distribution does not contain sources of the previous Lucene Java ver
       <fileset dir="contrib/queryparser/src/java/org/apache/lucene/queryParser/standard/parser" includes="*.java">
         <containsregexp expression="Generated.*By.*JavaCC"/>
       </fileset>
-      <fileset dir="src/demo/org/apache/lucene/demo/html" includes="*.java">
-        <containsregexp expression="Generated.*By.*JavaCC"/>
-      </fileset>
     </delete>
   </target>
 
-  <target name="javacc" depends="init,javacc-check,clean-javacc,javacc-QueryParser,javacc-HTMLParser,javacc-contrib-queryparser"/>
+  <target name="javacc" depends="init,javacc-check,clean-javacc,javacc-QueryParser,javacc-contrib-queryparser"/>
 
   <target name="javacc-QueryParser" depends="init,javacc-check" if="javacc.present">
     <sequential>
@@ -629,12 +525,6 @@ The source distribution does not contain sources of the previous Lucene Java ver
 
     </sequential>
   </target>	
-  
-  <target name="javacc-HTMLParser" depends="init,javacc-check" if="javacc.present">
-    <invoke-javacc target="src/demo/org/apache/lucene/demo/html/HTMLParser.jj"
-                   outputDir="src/demo/org/apache/lucene/demo/html"
-    />
-  </target>
 	
   <target name="javacc-contrib-queryparser" depends="init,javacc-check" if="javacc.present">
     <ant target="javacc"
@@ -642,33 +532,6 @@ The source distribution does not contain sources of the previous Lucene Java ver
       antfile="build.xml" 
     />
   </target>
-  
-  <!-- ================================================================== -->
-  <!-- Build the JFlex files into the source tree                         -->
-  <!-- ================================================================== -->
-
-  <target name="jflex" depends="jflex-check, clean-jflex,jflex-StandardAnalyzer" />
-
-  <target name="jflex-StandardAnalyzer" depends="init,jflex-check" if="jflex.present">
-    <taskdef classname="jflex.anttask.JFlexTask" name="jflex">
-			<classpath refid="jflex.classpath"/>
-    </taskdef>
-
-    <jflex file="src/java/org/apache/lucene/analysis/standard/StandardTokenizerImplOrig.jflex"
-           outdir="src/java/org/apache/lucene/analysis/standard"
-           nobak="on" />
-    <jflex file="src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl31.jflex"
-           outdir="src/java/org/apache/lucene/analysis/standard"
-           nobak="on" />
-  </target>
-
-  <target name="clean-jflex">
-    <delete>
-      <fileset dir="src/java/org/apache/lucene/analysis/standard" includes="*.java">
-        <containsregexp expression="generated.*by.*JFlex"/>
-      </fileset>
-    </delete>
-  </target>
 
   <macrodef name="createLevAutomaton">
   	<attribute name="n"/>
diff --git a/lucene/contrib/CHANGES.txt b/lucene/contrib/CHANGES.txt
index 88c68df..0d8cd00 100644
--- a/lucene/contrib/CHANGES.txt
+++ b/lucene/contrib/CHANGES.txt
@@ -2,6 +2,11 @@ Lucene contrib change Log
 
 ======================= Trunk (not yet released) =======================
   
+Build
+
+ * LUCENE-2413: Moved the demo out of lucene core and into contrib/demo.
+   (Robert Muir)
+
 ======================= Lucene 3.x (not yet released) =======================
 
 Changes in backwards compatibility policy
diff --git a/lucene/contrib/ant/build.xml b/lucene/contrib/ant/build.xml
index cda5e56..529fb01 100644
--- a/lucene/contrib/ant/build.xml
+++ b/lucene/contrib/ant/build.xml
@@ -34,4 +34,21 @@
   />
 
   <import file="../contrib-build.xml"/>
+	
+  <module-uptodate name="analysis/common" jarfile="${common.dir}/../modules/analysis/build/common/lucene-analyzers-common-${version}.jar"
+      property="analyzers-common.uptodate" classpath.property="analyzers-common.jar"/>
+
+  <path id="classpath">
+	 <pathelement path="${analyzers-common.jar}"/>
+	 <path refid="base.classpath"/>
+  </path>
+
+  <target name="compile-core" depends="compile-analyzers-common, common.compile-core" />
+
+  <target name="compile-analyzers-common" unless="analyzers-common.uptodate">
+    <subant target="default">
+      <fileset dir="${common.dir}/../modules/analysis/common" includes="build.xml"/>
+    </subant>
+  </target>
+
 </project>
diff --git a/lucene/contrib/ant/src/java/org/apache/lucene/ant/IndexTask.java b/lucene/contrib/ant/src/java/org/apache/lucene/ant/IndexTask.java
index 2b0240b..aa0356b 100644
--- a/lucene/contrib/ant/src/java/org/apache/lucene/ant/IndexTask.java
+++ b/lucene/contrib/ant/src/java/org/apache/lucene/ant/IndexTask.java
@@ -30,9 +30,9 @@ import java.util.Vector;
 import java.lang.reflect.Constructor;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.SimpleAnalyzer;
-import org.apache.lucene.analysis.StopAnalyzer;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.core.SimpleAnalyzer;
+import org.apache.lucene.analysis.core.StopAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.document.DateTools;
 import org.apache.lucene.document.Document;
diff --git a/lucene/contrib/ant/src/test/org/apache/lucene/ant/IndexTaskTest.java b/lucene/contrib/ant/src/test/org/apache/lucene/ant/IndexTaskTest.java
index b58ed30..e22177c 100644
--- a/lucene/contrib/ant/src/test/org/apache/lucene/ant/IndexTaskTest.java
+++ b/lucene/contrib/ant/src/test/org/apache/lucene/ant/IndexTaskTest.java
@@ -21,7 +21,7 @@ import java.io.File;
 import java.io.IOException;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.StopAnalyzer;
+import org.apache.lucene.analysis.core.StopAnalyzer;
 import org.apache.lucene.queryParser.QueryParser;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
diff --git a/lucene/contrib/benchmark/build.xml b/lucene/contrib/benchmark/build.xml
index 171412e..ae5b00a 100644
--- a/lucene/contrib/benchmark/build.xml
+++ b/lucene/contrib/benchmark/build.xml
@@ -18,6 +18,7 @@
     <module-uptodate name="analysis/common" jarfile="${common.dir}/../modules/analysis/build/common/lucene-analyzers-common-${version}.jar"
       property="analyzers-common.uptodate" classpath.property="analyzers-common.jar"/>
     <contrib-uptodate name="memory" property="memory.uptodate" classpath.property="memory.jar"/>
+    <contrib-uptodate name="demo" property="demo.uptodate" classpath.property="demo.jar"/>
 
     <target name="check-files">
         <available file="temp/news20.tar.gz" property="news20.exists"/>
@@ -139,8 +140,8 @@
       <pathelement path="${memory.jar}"/>
       <pathelement path="${highlighter.jar}"/>
       <pathelement path="${analyzers-common.jar}"/>
+      <pathelement path="${demo.jar}"/>
       <path refid="base.classpath"/>
-      <pathelement path="${common.dir}/build/classes/demo"/>
     	<fileset dir="lib">
     		<include name="**/*.jar"/>
     	</fileset>
@@ -228,9 +229,9 @@
       <echo>Benchmark output in JIRA table format is in file: ${shingle.jira.output.file}</echo>
     </target>
 
-    <target name="compile-demo">
-      <subant target="compile-demo">
-         <fileset dir="${common.dir}" includes="build.xml"/>
+    <target name="compile-demo" unless="demo.uptodate">
+      <subant target="default">
+         <fileset dir="${common.dir}/contrib/demo" includes="build.xml"/>
       </subant>
     </target>
     <target name="compile-highlighter" unless="highlighter.uptodate">
diff --git a/lucene/contrib/demo/build.xml b/lucene/contrib/demo/build.xml
new file mode 100644
index 0000000..1b1fb7b
--- /dev/null
+++ b/lucene/contrib/demo/build.xml
@@ -0,0 +1,78 @@
+<?xml version="1.0"?>
+
+<!--
+    Licensed to the Apache Software Foundation (ASF) under one or more
+    contributor license agreements.  See the NOTICE file distributed with
+    this work for additional information regarding copyright ownership.
+    The ASF licenses this file to You under the Apache License, Version 2.0
+    the "License"); you may not use this file except in compliance with
+    the License.  You may obtain a copy of the License at
+ 
+        http://www.apache.org/licenses/LICENSE-2.0
+ 
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an "AS IS" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
+ -->
+
+<project name="demo" default="default">
+
+  <description>
+    Lucene Demo
+  </description>
+
+  <property name="build.demo.template" value="src/java/demo-build.template"/> 
+
+  <property name="demo.name" value="lucene-demos-${version}"/>
+  <property name="demo.war.name" value="luceneweb"/>
+
+  <import file="../contrib-build.xml"/>
+
+  <module-uptodate name="analysis/common" jarfile="${common.dir}/../modules/analysis/build/common/lucene-analyzers-common-${version}.jar"
+      property="analyzers-common.uptodate" classpath.property="analyzers-common.jar"/>
+
+  <path id="classpath">
+	 <pathelement path="${analyzers-common.jar}"/>
+	 <path refid="base.classpath"/>
+  </path>
+
+  <target name="compile-core" depends="compile-analyzers-common, common.compile-core" />
+
+  <target name="compile-analyzers-common" unless="analyzers-common.uptodate">
+    <subant target="default">
+      <fileset dir="${common.dir}/../modules/analysis/common" includes="build.xml"/>
+    </subant>
+  </target>
+
+  <target name="war-demo" depends="jar-core">
+    <sequential>
+      <build-manifest title="Lucene Search Engine: demos"/>
+  	  <war destfile="${build.dir}/${demo.war.name}.war"
+           webxml="src/jsp/WEB-INF/web.xml"
+      	   manifest="${manifest.file}">
+        <fileset dir="src/jsp" excludes="WEB-INF/web.xml"/>
+        <lib dir="${build.dir}/../.." includes="lucene-core-${version}.jar"/>
+  	  	<lib dir="${common.dir}/../modules/analysis/build/common" includes="lucene-analyzers-common-${version}.jar"/>
+        <lib dir="${build.dir}" includes="${final.name}.jar"/>
+        <metainf dir="${common.dir}">
+          <include name="LICENSE.txt"/>
+          <include name="NOTICE.txt"/>
+        </metainf>
+      </war>
+    </sequential>
+  </target>
+	
+  <target name="clean-javacc">
+    <fileset dir="src/demo/org/apache/lucene/demo/html" includes="*.java">
+      <containsregexp expression="Generated.*By.*JavaCC"/>
+    </fileset>
+  </target>
+	
+  <target name="javacc" depends="init,javacc-check" if="javacc.present">
+    <invoke-javacc target="src/demo/org/apache/lucene/demo/html/HTMLParser.jj"
+                     outputDir="src/demo/org/apache/lucene/demo/html"
+    />
+  </target>
+</project>
diff --git a/lucene/contrib/demo/lib/servlet-api-2.4.jar b/lucene/contrib/demo/lib/servlet-api-2.4.jar
new file mode 100644
index 0000000..9e7f1e8
--- /dev/null
+++ b/lucene/contrib/demo/lib/servlet-api-2.4.jar
@@ -0,0 +1,2 @@
+AnyObjectId[018d6effad3823d0ea59f1b58ab154fc2652f418] was removed in git history.
+Apache SVN contains full history.
\ No newline at end of file
diff --git a/lucene/contrib/demo/src/java/demo-build.template b/lucene/contrib/demo/src/java/demo-build.template
new file mode 100644
index 0000000..d673b21
--- /dev/null
+++ b/lucene/contrib/demo/src/java/demo-build.template
@@ -0,0 +1,253 @@
+<?xml version="1.0"?>
+
+<!--
+    Licensed to the Apache Software Foundation (ASF) under one or more
+    contributor license agreements.  See the NOTICE file distributed with
+    this work for additional information regarding copyright ownership.
+    The ASF licenses this file to You under the Apache License, Version 2.0
+    the "License"); you may not use this file except in compliance with
+    the License.  You may obtain a copy of the License at
+ 
+        http://www.apache.org/licenses/LICENSE-2.0
+ 
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an "AS IS" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
+ -->
+
+<project name="lucene-demo" default="compile-demo" basedir=".">
+  <dirname file="${ant.file.common}" property="common.dir"/>
+
+  <property name="version" value="@PLACEHOLDER_version@"/>
+  <property name="javac.source" value="@PLACEHOLDER_javac.source@"/>
+  <property name="javac.target" value="@PLACEHOLDER_javac.target@"/>
+	
+  <property name="build.dir" location="build"/>
+	
+	
+  <property name="core.name" value="lucene-core-${version}"/>
+  <property name="demo.name" value="lucene-demos-${version}"/>
+  <property name="demo.war.name" value="luceneweb"/>
+
+  <property name="manifest.file" location="${build.dir}/MANIFEST.MF"/>
+
+  <!-- Build classpath -->
+  <path id="classpath">
+    <pathelement location="${common.dir}/${core.name}.jar"/>
+  </path>
+
+  <path id="demo.classpath">
+    <path refid="classpath"/>
+    <pathelement location="${build.dir}/classes/demo"/>
+  </path>
+	
+  <available
+    property="jar.core.present"
+	type="file"
+	file="${common.dir}/${core.name}.jar"
+  />
+
+  <target name="jar.core-check">
+    <fail unless="jar.core.present">
+	  ##################################################################
+	  ${common.dir}/${core.name}.jar not found.
+	  ##################################################################
+	</fail>
+  </target>
+
+  <!-- ================================================================== -->
+  <!-- J A R                                                              -->
+  <!-- ================================================================== -->
+  <!--                                                                    -->
+  <!-- ================================================================== -->
+
+  <target name="jar-demo" depends="compile-demo"
+	description="Build demo jar file">
+	<sequential>
+  	  <build-manifest/>
+	  <jar
+        destfile="${demo.name}.jar"
+        basedir="${build.dir}/classes/demo"
+        excludes="**/*.java"
+        manifest="${manifest.file}">
+        <metainf dir="${common.dir}">
+          <include name="LICENSE.txt"/>
+          <include name="NOTICE.txt"/>
+        </metainf>
+      </jar>
+    </sequential>
+  </target>
+
+  <target name="war-demo" depends="jar-demo"	
+	description="Build demo war file">
+	<sequential>
+  	  <build-manifest/>
+      <war destfile="${demo.war.name}.war"
+           webxml="src/jsp/WEB-INF/web.xml"
+           manifest="${manifest.file}">
+        <fileset dir="src/jsp" excludes="WEB-INF/web.xml"/>
+        <lib dir="." includes="${demo.name}.jar"/>
+        <lib dir="." includes="${core.name}.jar"/>
+        <metainf dir="${common.dir}">
+          <include name="LICENSE.txt"/>
+          <include name="NOTICE.txt"/>
+        </metainf>
+      </war>
+    </sequential>
+  </target>
+
+  <!-- ================================================================== -->
+  <!-- B U I L D  D E M O                                                 -->
+  <!-- ================================================================== -->
+  <!--                                                                    -->
+  <!-- ================================================================== -->
+  <target name="compile-demo" depends="jar.core-check"
+	description="Compile demo classes">
+    <mkdir dir="${build.dir}/classes/demo"/>
+
+    <compile
+      srcdir="src/demo"
+      destdir="${build.dir}/classes/demo">
+      <classpath refid="demo.classpath"/>
+    </compile>
+  </target>
+	
+  <target name="clean"
+    description="Removes contents of build directory">
+    <delete dir="${build.dir}"/>
+    <delete dir="${common.dir}/demo-text-dir"/>
+    <delete dir="${common.dir}/demo-html-dir"/>
+  </target>
+
+  <!-- ================================================================== -->
+  <!-- R U N  T E X T  I N D E X I N G  D E M O                           -->
+  <!-- ================================================================== -->
+  <!--                                                                    -->
+  <!-- ================================================================== -->
+  <target name="demo-index-text" depends="jar-demo"
+	description="Run text indexing demo (index the sources of the demo).">
+    <echo>----- (1) Prepare dir ----- </echo>
+    <echo>cd ${common.dir} </echo>
+    <echo>rmdir demo-text-dir </echo>
+    <delete dir="${common.dir}/demo-text-dir"/>
+    <echo>mkdir demo-text-dir </echo>
+    <mkdir dir="${common.dir}/demo-text-dir"/>
+    <echo>cd demo-text-dir </echo>
+    <echo>----- (2) Index the files located under ${common.dir}/src ----- </echo>
+    <invoke-java class="IndexFiles" params="${common.dir}/src/demo" paramsDisplay="../src/demo" type="text"/>
+  </target>
+
+  <!-- ================================================================== -->
+  <!-- R U N  T E X T  S E A R C H  D E M O                               -->
+  <!-- ================================================================== -->
+  <!--                                                                    -->
+  <!-- ================================================================== -->
+  <target name="demo-search-text" depends="jar-demo"
+	description="Run interactive search demo.">
+    <echo>----- Interactive search ----- </echo>
+    <echo>cd demo-text-dir </echo>
+    <invoke-java class="SearchFiles" params="-index index" paramsDisplay="-index index" type="text"/>
+  </target>
+
+
+  <!-- ================================================================== -->
+  <!-- R U N  H T M L  I N D E X I N G  D E M O                           -->
+  <!-- ================================================================== -->
+  <!--                                                                    -->
+  <!-- ================================================================== -->
+  <target name="demo-index-html" depends="jar-demo"
+	description="Run html indexing demo (index the javadocs).">
+    <echo>----- (1) Prepare dir ----- </echo>
+    <echo>cd ${common.dir} </echo>
+    <echo>rmdir demo-html-dir </echo>
+    <delete dir="${common.dir}/demo-html-dir"/>
+    <echo>mkdir demo-html-dir </echo>
+    <mkdir dir="${common.dir}/demo-html-dir"/>
+    <echo>cd demo-html-dir </echo>
+    <echo>----- (2) Index the files located under ${common.dir}/src ----- </echo>
+    <invoke-java class="IndexFiles" params="${common.dir}/docs/api" paramsDisplay="../docs/api" type="html"/>
+  </target>
+
+  <!-- ================================================================== -->
+  <!-- R U N  H T M L  S E A R C H  D E M O                               -->
+  <!-- ================================================================== -->
+  <!--                                                                    -->
+  <!-- ================================================================== -->
+  <target name="demo-search-html" depends="jar-demo"
+	description="Run interactive search demo.">
+    <echo>----- Interactive search ----- </echo>
+    <echo>cd demo-html-dir </echo>
+    <invoke-java class="SearchFiles" params="-index index" paramsDisplay="-index index" type="html"/>
+  </target>
+
+
+  <!--+
+      | M A C R O S
+      +-->
+      
+  <macrodef name="build-manifest" description="Builds a manifest file">
+  	<sequential>
+      <manifest file="${manifest.file}">
+        <attribute name="Specification-Title" value="Lucene Search Engine: demos"/>
+		<!-- spec version must match "digit+{.digit+}*" -->
+		<attribute name="Specification-Version" value="${version}"/>
+		<attribute name="Specification-Vendor"
+		           value="The Apache Software Foundation"/>
+		<attribute name="Implementation-Title" value="org.apache.lucene"/>
+		<!-- impl version can be any string -->
+		<attribute name="Implementation-Version"
+		           value="${version}"/>
+		<attribute name="Implementation-Vendor"
+		           value="The Apache Software Foundation"/>
+		<attribute name="X-Compile-Source-JDK" 
+		           value="${javac.source}"/>
+		<attribute name="X-Compile-Target-JDK" 
+		           value="${javac.target}"/>
+	  </manifest>
+  	</sequential>
+  </macrodef>
+      
+  <macrodef name="compile">
+    <attribute name="srcdir"/>
+    <attribute name="destdir"/>
+    <element name="nested" implicit="yes" optional="yes"/>
+
+    <sequential>
+      <mkdir dir="@{destdir}"/>
+      <javac
+        srcdir="@{srcdir}"
+        destdir="@{destdir}"
+        deprecation="off"
+        debug="on"
+        source="${javac.source}"
+        target="${javac.target}">
+        <nested/>
+      </javac>
+    </sequential>
+  </macrodef>
+
+  <macrodef name="invoke-java">
+    <attribute name="class"/>
+    <attribute name="params"/>
+    <attribute name="paramsDisplay"/>
+    <attribute name="type"/>
+    <sequential>
+      <echo>java -classpath "../${core.name}.jar;../${demo.name}.jar" org.apache.lucene.demo.@{class} @{paramsDisplay} </echo>
+      <java classname="org.apache.lucene.demo.@{class}"
+            dir="${common.dir}/demo-@{type}-dir"
+            fork="true"
+            failonerror="true"
+            maxmemory="128m"
+      >
+        <arg value="@{params}"/>
+        <classpath>
+           <pathelement location="${common.dir}/${core.name}.jar"/>
+           <pathelement location="${common.dir}/${demo.name}.jar"/>
+        </classpath>
+      </java>
+    </sequential>
+  </macrodef>
+
+</project>
diff --git a/lucene/contrib/demo/src/java/org/apache/lucene/demo/DeleteFiles.java b/lucene/contrib/demo/src/java/org/apache/lucene/demo/DeleteFiles.java
new file mode 100644
index 0000000..a5588eb
--- /dev/null
+++ b/lucene/contrib/demo/src/java/org/apache/lucene/demo/DeleteFiles.java
@@ -0,0 +1,66 @@
+package org.apache.lucene.demo;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.File;
+
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.FSDirectory;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.Term;
+//import org.apache.lucene.index.Term;
+
+
+/** Deletes documents from an index that do not contain a term. */
+public class DeleteFiles {
+  
+  private DeleteFiles() {}                         // singleton
+
+  /** Deletes documents from an index that do not contain a term. */
+  public static void main(String[] args) {
+    String usage = "java org.apache.lucene.demo.DeleteFiles <unique_term>";
+    if (args.length == 0) {
+      System.err.println("Usage: " + usage);
+      System.exit(1);
+    }
+    try {
+      Directory directory = FSDirectory.open(new File("index"));
+      IndexReader reader = IndexReader.open(directory, false); // we don't want read-only because we are about to delete
+
+      Term term = new Term("path", args[0]);
+      int deleted = reader.deleteDocuments(term);
+
+      System.out.println("deleted " + deleted +
+ 			 " documents containing " + term);
+
+      // one can also delete documents by their internal id:
+      /*
+      for (int i = 0; i < reader.maxDoc(); i++) {
+        System.out.println("Deleting document with id " + i);
+        reader.delete(i);
+      }*/
+
+      reader.close();
+      directory.close();
+
+    } catch (Exception e) {
+      System.out.println(" caught a " + e.getClass() +
+			 "\n with message: " + e.getMessage());
+    }
+  }
+}
diff --git a/lucene/contrib/demo/src/java/org/apache/lucene/demo/FileDocument.java b/lucene/contrib/demo/src/java/org/apache/lucene/demo/FileDocument.java
new file mode 100644
index 0000000..ac634cd
--- /dev/null
+++ b/lucene/contrib/demo/src/java/org/apache/lucene/demo/FileDocument.java
@@ -0,0 +1,71 @@
+package org.apache.lucene.demo;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.File;
+import java.io.FileReader;
+
+import org.apache.lucene.document.DateTools;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+
+/** A utility for making Lucene Documents from a File. */
+
+public class FileDocument {
+  /** Makes a document for a File.
+    <p>
+    The document has three fields:
+    <ul>
+    <li><code>path</code>--containing the pathname of the file, as a stored,
+    untokenized field;
+    <li><code>modified</code>--containing the last modified date of the file as
+    a field as created by <a
+    href="lucene.document.DateTools.html">DateTools</a>; and
+    <li><code>contents</code>--containing the full contents of the file, as a
+    Reader field;
+    */
+  public static Document Document(File f)
+       throws java.io.FileNotFoundException {
+	 
+    // make a new, empty document
+    Document doc = new Document();
+
+    // Add the path of the file as a field named "path".  Use a field that is 
+    // indexed (i.e. searchable), but don't tokenize the field into words.
+    doc.add(new Field("path", f.getPath(), Field.Store.YES, Field.Index.NOT_ANALYZED));
+
+    // Add the last modified date of the file a field named "modified".  Use 
+    // a field that is indexed (i.e. searchable), but don't tokenize the field
+    // into words.
+    doc.add(new Field("modified",
+        DateTools.timeToString(f.lastModified(), DateTools.Resolution.MINUTE),
+        Field.Store.YES, Field.Index.NOT_ANALYZED));
+
+    // Add the contents of the file to a field named "contents".  Specify a Reader,
+    // so that the text of the file is tokenized and indexed, but not stored.
+    // Note that FileReader expects the file to be in the system's default encoding.
+    // If that's not the case searching for special characters will fail.
+    doc.add(new Field("contents", new FileReader(f)));
+
+    // return the document
+    return doc;
+  }
+
+  private FileDocument() {}
+}
+    
diff --git a/lucene/contrib/demo/src/java/org/apache/lucene/demo/HTMLDocument.java b/lucene/contrib/demo/src/java/org/apache/lucene/demo/HTMLDocument.java
new file mode 100644
index 0000000..aa38195
--- /dev/null
+++ b/lucene/contrib/demo/src/java/org/apache/lucene/demo/HTMLDocument.java
@@ -0,0 +1,86 @@
+package org.apache.lucene.demo;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.*;
+import org.apache.lucene.document.*;
+import org.apache.lucene.demo.html.HTMLParser;
+
+/** A utility for making Lucene Documents for HTML documents. */
+
+public class HTMLDocument {
+  static char dirSep = System.getProperty("file.separator").charAt(0);
+
+  public static String uid(File f) {
+    // Append path and date into a string in such a way that lexicographic
+    // sorting gives the same results as a walk of the file hierarchy.  Thus
+    // null (\u0000) is used both to separate directory components and to
+    // separate the path from the date.
+    return f.getPath().replace(dirSep, '\u0000') +
+      "\u0000" +
+      DateTools.timeToString(f.lastModified(), DateTools.Resolution.SECOND);
+  }
+
+  public static String uid2url(String uid) {
+    String url = uid.replace('\u0000', '/');	  // replace nulls with slashes
+    return url.substring(0, url.lastIndexOf('/')); // remove date from end
+  }
+
+  public static Document Document(File f)
+       throws IOException, InterruptedException  {
+    // make a new, empty document
+    Document doc = new Document();
+
+    // Add the url as a field named "path".  Use a field that is 
+    // indexed (i.e. searchable), but don't tokenize the field into words.
+    doc.add(new Field("path", f.getPath().replace(dirSep, '/'), Field.Store.YES,
+        Field.Index.NOT_ANALYZED));
+
+    // Add the last modified date of the file a field named "modified".  
+    // Use a field that is indexed (i.e. searchable), but don't tokenize
+    // the field into words.
+    doc.add(new Field("modified",
+        DateTools.timeToString(f.lastModified(), DateTools.Resolution.MINUTE),
+        Field.Store.YES, Field.Index.NOT_ANALYZED));
+
+    // Add the uid as a field, so that index can be incrementally maintained.
+    // This field is not stored with document, it is indexed, but it is not
+    // tokenized prior to indexing.
+    doc.add(new Field("uid", uid(f), Field.Store.NO, Field.Index.NOT_ANALYZED));
+
+    FileInputStream fis = new FileInputStream(f);
+    HTMLParser parser = new HTMLParser(fis);
+      
+    // Add the tag-stripped contents as a Reader-valued Text field so it will
+    // get tokenized and indexed.
+    doc.add(new Field("contents", parser.getReader()));
+
+    // Add the summary as a field that is stored and returned with
+    // hit documents for display.
+    doc.add(new Field("summary", parser.getSummary(), Field.Store.YES, Field.Index.NO));
+
+    // Add the title as a field that it can be searched and that is stored.
+    doc.add(new Field("title", parser.getTitle(), Field.Store.YES, Field.Index.ANALYZED));
+
+    // return the document
+    return doc;
+  }
+
+  private HTMLDocument() {}
+}
+    
diff --git a/lucene/contrib/demo/src/java/org/apache/lucene/demo/IndexFiles.java b/lucene/contrib/demo/src/java/org/apache/lucene/demo/IndexFiles.java
new file mode 100644
index 0000000..7652f6c
--- /dev/null
+++ b/lucene/contrib/demo/src/java/org/apache/lucene/demo/IndexFiles.java
@@ -0,0 +1,103 @@
+package org.apache.lucene.demo;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.IndexWriterConfig.OpenMode;
+import org.apache.lucene.store.FSDirectory;
+import org.apache.lucene.util.Version;
+
+import java.io.File;
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.util.Date;
+
+/** Index all text files under a directory. */
+public class IndexFiles {
+  
+  private IndexFiles() {}
+
+  static final File INDEX_DIR = new File("index");
+  
+  /** Index all text files under a directory. */
+  public static void main(String[] args) {
+    String usage = "java org.apache.lucene.demo.IndexFiles <root_directory>";
+    if (args.length == 0) {
+      System.err.println("Usage: " + usage);
+      System.exit(1);
+    }
+
+    if (INDEX_DIR.exists()) {
+      System.out.println("Cannot save index to '" +INDEX_DIR+ "' directory, please delete it first");
+      System.exit(1);
+    }
+    
+    final File docDir = new File(args[0]);
+    if (!docDir.exists() || !docDir.canRead()) {
+      System.out.println("Document directory '" +docDir.getAbsolutePath()+ "' does not exist or is not readable, please check the path");
+      System.exit(1);
+    }
+    
+    Date start = new Date();
+    try {
+      IndexWriter writer = new IndexWriter(FSDirectory.open(INDEX_DIR),
+          new IndexWriterConfig(Version.LUCENE_CURRENT, new StandardAnalyzer(
+              Version.LUCENE_CURRENT)).setOpenMode(OpenMode.CREATE));
+      System.out.println("Indexing to directory '" +INDEX_DIR+ "'...");
+      indexDocs(writer, docDir);
+      System.out.println("Optimizing...");
+      writer.optimize();
+      writer.close();
+
+      Date end = new Date();
+      System.out.println(end.getTime() - start.getTime() + " total milliseconds");
+
+    } catch (IOException e) {
+      System.out.println(" caught a " + e.getClass() +
+       "\n with message: " + e.getMessage());
+    }
+  }
+
+  static void indexDocs(IndexWriter writer, File file)
+    throws IOException {
+    // do not try to index files that cannot be read
+    if (file.canRead()) {
+      if (file.isDirectory()) {
+        String[] files = file.list();
+        // an IO error could occur
+        if (files != null) {
+          for (int i = 0; i < files.length; i++) {
+            indexDocs(writer, new File(file, files[i]));
+          }
+        }
+      } else {
+        System.out.println("adding " + file);
+        try {
+          writer.addDocument(FileDocument.Document(file));
+        }
+        // at least on windows, some temporary files raise this exception with an "access denied" message
+        // checking if the file can be read doesn't help
+        catch (FileNotFoundException fnfe) {
+        }
+      }
+    }
+  }
+  
+}
diff --git a/lucene/contrib/demo/src/java/org/apache/lucene/demo/IndexHTML.java b/lucene/contrib/demo/src/java/org/apache/lucene/demo/IndexHTML.java
new file mode 100644
index 0000000..8e5543d
--- /dev/null
+++ b/lucene/contrib/demo/src/java/org/apache/lucene/demo/IndexHTML.java
@@ -0,0 +1,172 @@
+package org.apache.lucene.demo;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.TermEnum;
+import org.apache.lucene.index.IndexWriterConfig.OpenMode;
+import org.apache.lucene.store.FSDirectory;
+import org.apache.lucene.util.Version;
+
+import java.io.File;
+import java.util.Date;
+import java.util.Arrays;
+
+/** Indexer for HTML files. */
+public class IndexHTML {
+  private IndexHTML() {}
+
+  private static boolean deleting = false;	  // true during deletion pass
+  private static IndexReader reader;		  // existing index
+  private static IndexWriter writer;		  // new index being built
+  private static TermEnum uidIter;		  // document id iterator
+
+  /** Indexer for HTML files.*/
+  public static void main(String[] argv) {
+    try {
+      File index = new File("index");
+      boolean create = false;
+      File root = null;
+
+      String usage = "IndexHTML [-create] [-index <index>] <root_directory>";
+
+      if (argv.length == 0) {
+        System.err.println("Usage: " + usage);
+        return;
+      }
+
+      for (int i = 0; i < argv.length; i++) {
+        if (argv[i].equals("-index")) {		  // parse -index option
+          index = new File(argv[++i]);
+        } else if (argv[i].equals("-create")) {	  // parse -create option
+          create = true;
+        } else if (i != argv.length-1) {
+          System.err.println("Usage: " + usage);
+          return;
+        } else
+          root = new File(argv[i]);
+      }
+      
+      if(root == null) {
+        System.err.println("Specify directory to index");
+        System.err.println("Usage: " + usage);
+        return;
+      }
+
+      Date start = new Date();
+
+      if (!create) {				  // delete stale docs
+        deleting = true;
+        indexDocs(root, index, create);
+      }
+      writer = new IndexWriter(FSDirectory.open(index), new IndexWriterConfig(
+          Version.LUCENE_CURRENT, new StandardAnalyzer(Version.LUCENE_CURRENT))
+          .setMaxFieldLength(1000000).setOpenMode(
+              create ? OpenMode.CREATE : OpenMode.CREATE_OR_APPEND));
+      indexDocs(root, index, create);		  // add new docs
+
+      System.out.println("Optimizing index...");
+      writer.optimize();
+      writer.close();
+
+      Date end = new Date();
+
+      System.out.print(end.getTime() - start.getTime());
+      System.out.println(" total milliseconds");
+
+    } catch (Exception e) {
+      e.printStackTrace();
+    }
+  }
+
+  /* Walk directory hierarchy in uid order, while keeping uid iterator from
+  /* existing index in sync.  Mismatches indicate one of: (a) old documents to
+  /* be deleted; (b) unchanged documents, to be left alone; or (c) new
+  /* documents, to be indexed.
+   */
+
+  private static void indexDocs(File file, File index, boolean create)
+       throws Exception {
+    if (!create) {				  // incrementally update
+
+      reader = IndexReader.open(FSDirectory.open(index), false);		  // open existing index
+      uidIter = reader.terms(new Term("uid", "")); // init uid iterator
+
+      indexDocs(file);
+
+      if (deleting) {				  // delete rest of stale docs
+        while (uidIter.term() != null && uidIter.term().field() == "uid") {
+          System.out.println("deleting " +
+              HTMLDocument.uid2url(uidIter.term().text()));
+          reader.deleteDocuments(uidIter.term());
+          uidIter.next();
+        }
+        deleting = false;
+      }
+
+      uidIter.close();				  // close uid iterator
+      reader.close();				  // close existing index
+
+    } else					  // don't have exisiting
+      indexDocs(file);
+  }
+
+  private static void indexDocs(File file) throws Exception {
+    if (file.isDirectory()) {			  // if a directory
+      String[] files = file.list();		  // list its files
+      Arrays.sort(files);			  // sort the files
+      for (int i = 0; i < files.length; i++)	  // recursively index them
+        indexDocs(new File(file, files[i]));
+
+    } else if (file.getPath().endsWith(".html") || // index .html files
+      file.getPath().endsWith(".htm") || // index .htm files
+      file.getPath().endsWith(".txt")) { // index .txt files
+
+      if (uidIter != null) {
+        String uid = HTMLDocument.uid(file);	  // construct uid for doc
+
+        while (uidIter.term() != null && uidIter.term().field() == "uid" &&
+            uidIter.term().text().compareTo(uid) < 0) {
+          if (deleting) {			  // delete stale docs
+            System.out.println("deleting " +
+                HTMLDocument.uid2url(uidIter.term().text()));
+            reader.deleteDocuments(uidIter.term());
+          }
+          uidIter.next();
+        }
+        if (uidIter.term() != null && uidIter.term().field() == "uid" &&
+            uidIter.term().text().compareTo(uid) == 0) {
+          uidIter.next();			  // keep matching docs
+        } else if (!deleting) {			  // add new docs
+          Document doc = HTMLDocument.Document(file);
+          System.out.println("adding " + doc.get("path"));
+          writer.addDocument(doc);
+        }
+      } else {					  // creating a new index
+        Document doc = HTMLDocument.Document(file);
+        System.out.println("adding " + doc.get("path"));
+        writer.addDocument(doc);		  // add docs unconditionally
+      }
+    }
+  }
+}
diff --git a/lucene/contrib/demo/src/java/org/apache/lucene/demo/SearchFiles.java b/lucene/contrib/demo/src/java/org/apache/lucene/demo/SearchFiles.java
new file mode 100644
index 0000000..65629fe
--- /dev/null
+++ b/lucene/contrib/demo/src/java/org/apache/lucene/demo/SearchFiles.java
@@ -0,0 +1,313 @@
+package org.apache.lucene.demo;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.FileReader;
+import java.io.IOException;
+import java.io.InputStreamReader;
+import java.util.Date;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.index.FilterIndexReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.search.Collector;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.Scorer;
+import org.apache.lucene.search.Searcher;
+import org.apache.lucene.search.TopScoreDocCollector;
+import org.apache.lucene.store.FSDirectory;
+import org.apache.lucene.util.Version;
+
+/** Simple command-line based search demo. */
+public class SearchFiles {
+
+  /** Use the norms from one field for all fields.  Norms are read into memory,
+   * using a byte of memory per document per searched field.  This can cause
+   * search of large collections with a large number of fields to run out of
+   * memory.  If all of the fields contain only a single token, then the norms
+   * are all identical, then single norm vector may be shared. */
+  private static class OneNormsReader extends FilterIndexReader {
+    private String field;
+
+    public OneNormsReader(IndexReader in, String field) {
+      super(in);
+      this.field = field;
+    }
+
+    @Override
+    public byte[] norms(String field) throws IOException {
+      return in.norms(this.field);
+    }
+  }
+
+  private SearchFiles() {}
+
+  /** Simple command-line based search demo. */
+  public static void main(String[] args) throws Exception {
+    String usage =
+      "Usage:\tjava org.apache.lucene.demo.SearchFiles [-index dir] [-field f] [-repeat n] [-queries file] [-raw] [-norms field] [-paging hitsPerPage]";
+    usage += "\n\tSpecify 'false' for hitsPerPage to use streaming instead of paging search.";
+    if (args.length > 0 && ("-h".equals(args[0]) || "-help".equals(args[0]))) {
+      System.out.println(usage);
+      System.exit(0);
+    }
+
+    String index = "index";
+    String field = "contents";
+    String queries = null;
+    int repeat = 0;
+    boolean raw = false;
+    String normsField = null;
+    boolean paging = true;
+    int hitsPerPage = 10;
+    
+    for (int i = 0; i < args.length; i++) {
+      if ("-index".equals(args[i])) {
+        index = args[i+1];
+        i++;
+      } else if ("-field".equals(args[i])) {
+        field = args[i+1];
+        i++;
+      } else if ("-queries".equals(args[i])) {
+        queries = args[i+1];
+        i++;
+      } else if ("-repeat".equals(args[i])) {
+        repeat = Integer.parseInt(args[i+1]);
+        i++;
+      } else if ("-raw".equals(args[i])) {
+        raw = true;
+      } else if ("-norms".equals(args[i])) {
+        normsField = args[i+1];
+        i++;
+      } else if ("-paging".equals(args[i])) {
+        if (args[i+1].equals("false")) {
+          paging = false;
+        } else {
+          hitsPerPage = Integer.parseInt(args[i+1]);
+          if (hitsPerPage == 0) {
+            paging = false;
+          }
+        }
+        i++;
+      }
+    }
+    
+    IndexReader reader = IndexReader.open(FSDirectory.open(new File(index)), true); // only searching, so read-only=true
+
+    if (normsField != null)
+      reader = new OneNormsReader(reader, normsField);
+
+    Searcher searcher = new IndexSearcher(reader);
+    Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_CURRENT);
+
+    BufferedReader in = null;
+    if (queries != null) {
+      in = new BufferedReader(new FileReader(queries));
+    } else {
+      in = new BufferedReader(new InputStreamReader(System.in, "UTF-8"));
+    }
+    QueryParser parser = new QueryParser(Version.LUCENE_CURRENT, field, analyzer);
+    while (true) {
+      if (queries == null)                        // prompt the user
+        System.out.println("Enter query: ");
+
+      String line = in.readLine();
+
+      if (line == null || line.length() == -1)
+        break;
+
+      line = line.trim();
+      if (line.length() == 0)
+        break;
+      
+      Query query = parser.parse(line);
+      System.out.println("Searching for: " + query.toString(field));
+
+            
+      if (repeat > 0) {                           // repeat & time as benchmark
+        Date start = new Date();
+        for (int i = 0; i < repeat; i++) {
+          searcher.search(query, null, 100);
+        }
+        Date end = new Date();
+        System.out.println("Time: "+(end.getTime()-start.getTime())+"ms");
+      }
+
+      if (paging) {
+        doPagingSearch(in, searcher, query, hitsPerPage, raw, queries == null);
+      } else {
+        doStreamingSearch(searcher, query);
+      }
+    }
+    reader.close();
+  }
+  
+  /**
+   * This method uses a custom HitCollector implementation which simply prints out
+   * the docId and score of every matching document. 
+   * 
+   *  This simulates the streaming search use case, where all hits are supposed to
+   *  be processed, regardless of their relevance.
+   */
+  public static void doStreamingSearch(final Searcher searcher, Query query) throws IOException {
+    Collector streamingHitCollector = new Collector() {
+      private Scorer scorer;
+      private int docBase;
+      
+      // simply print docId and score of every matching document
+      @Override
+      public void collect(int doc) throws IOException {
+        System.out.println("doc=" + doc + docBase + " score=" + scorer.score());
+      }
+
+      @Override
+      public boolean acceptsDocsOutOfOrder() {
+        return true;
+      }
+
+      @Override
+      public void setNextReader(IndexReader reader, int docBase)
+          throws IOException {
+        this.docBase = docBase;
+      }
+
+      @Override
+      public void setScorer(Scorer scorer) throws IOException {
+        this.scorer = scorer;
+      }
+      
+    };
+    
+    searcher.search(query, streamingHitCollector);
+  }
+
+  /**
+   * This demonstrates a typical paging search scenario, where the search engine presents 
+   * pages of size n to the user. The user can then go to the next page if interested in
+   * the next hits.
+   * 
+   * When the query is executed for the first time, then only enough results are collected
+   * to fill 5 result pages. If the user wants to page beyond this limit, then the query
+   * is executed another time and all hits are collected.
+   * 
+   */
+  public static void doPagingSearch(BufferedReader in, Searcher searcher, Query query, 
+                                     int hitsPerPage, boolean raw, boolean interactive) throws IOException {
+ 
+    // Collect enough docs to show 5 pages
+    TopScoreDocCollector collector = TopScoreDocCollector.create(
+        5 * hitsPerPage, false);
+    searcher.search(query, collector);
+    ScoreDoc[] hits = collector.topDocs().scoreDocs;
+    
+    int numTotalHits = collector.getTotalHits();
+    System.out.println(numTotalHits + " total matching documents");
+
+    int start = 0;
+    int end = Math.min(numTotalHits, hitsPerPage);
+        
+    while (true) {
+      if (end > hits.length) {
+        System.out.println("Only results 1 - " + hits.length +" of " + numTotalHits + " total matching documents collected.");
+        System.out.println("Collect more (y/n) ?");
+        String line = in.readLine();
+        if (line.length() == 0 || line.charAt(0) == 'n') {
+          break;
+        }
+
+        collector = TopScoreDocCollector.create(numTotalHits, false);
+        searcher.search(query, collector);
+        hits = collector.topDocs().scoreDocs;
+      }
+      
+      end = Math.min(hits.length, start + hitsPerPage);
+      
+      for (int i = start; i < end; i++) {
+        if (raw) {                              // output raw format
+          System.out.println("doc="+hits[i].doc+" score="+hits[i].score);
+          continue;
+        }
+
+        Document doc = searcher.doc(hits[i].doc);
+        String path = doc.get("path");
+        if (path != null) {
+          System.out.println((i+1) + ". " + path);
+          String title = doc.get("title");
+          if (title != null) {
+            System.out.println("   Title: " + doc.get("title"));
+          }
+        } else {
+          System.out.println((i+1) + ". " + "No path for this document");
+        }
+                  
+      }
+
+      if (!interactive) {
+        break;
+      }
+
+      if (numTotalHits >= end) {
+        boolean quit = false;
+        while (true) {
+          System.out.print("Press ");
+          if (start - hitsPerPage >= 0) {
+            System.out.print("(p)revious page, ");  
+          }
+          if (start + hitsPerPage < numTotalHits) {
+            System.out.print("(n)ext page, ");
+          }
+          System.out.println("(q)uit or enter number to jump to a page.");
+          
+          String line = in.readLine();
+          if (line.length() == 0 || line.charAt(0)=='q') {
+            quit = true;
+            break;
+          }
+          if (line.charAt(0) == 'p') {
+            start = Math.max(0, start - hitsPerPage);
+            break;
+          } else if (line.charAt(0) == 'n') {
+            if (start + hitsPerPage < numTotalHits) {
+              start+=hitsPerPage;
+            }
+            break;
+          } else {
+            int page = Integer.parseInt(line);
+            if ((page - 1) * hitsPerPage < numTotalHits) {
+              start = (page - 1) * hitsPerPage;
+              break;
+            } else {
+              System.out.println("No such page");
+            }
+          }
+        }
+        if (quit) break;
+        end = Math.min(numTotalHits, start + hitsPerPage);
+      }
+      
+    }
+
+  }
+}
diff --git a/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/Entities.java b/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/Entities.java
new file mode 100644
index 0000000..5c5a4f2
--- /dev/null
+++ b/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/Entities.java
@@ -0,0 +1,327 @@
+package org.apache.lucene.demo.html;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.HashMap;
+import java.util.Map;
+
+public class Entities {
+  static final Map<String,String> decoder = new HashMap<String,String>(300);
+  static final String[]  encoder = new String[0x100];
+
+  static final String decode(String entity) {
+    if (entity.charAt(entity.length()-1) == ';')  // remove trailing semicolon
+      entity = entity.substring(0, entity.length()-1);
+    if (entity.charAt(1) == '#') {
+      int start = 2;
+      int radix = 10;
+      if (entity.charAt(2) == 'X' || entity.charAt(2) == 'x') {
+	start++;
+	radix = 16;
+      }
+      Character c =
+	new Character((char)Integer.parseInt(entity.substring(start), radix));
+      return c.toString();
+    } else {
+      String s = decoder.get(entity);
+      if (s != null)
+	return s;
+      else return "";
+    }
+  }
+
+  public static final String encode(String s) {
+    int length = s.length();
+    StringBuffer buffer = new StringBuffer(length * 2);
+    for (int i = 0; i < length; i++) {
+      int j = s.charAt(i);
+      if (j < 0x100 && encoder[j] != null) {
+	buffer.append(encoder[j]);		  // have a named encoding
+	buffer.append(';');
+      } else if (j < 0x80) {
+	buffer.append((char) j);			  // use ASCII value
+      } else {
+	buffer.append("&#");			  // use numeric encoding
+	buffer.append(j).append(';');
+      }
+    }
+    return buffer.toString();
+  }
+
+  static final void add(String entity, int value) {
+    decoder.put(entity, (new Character((char)value)).toString());
+    if (value < 0x100)
+      encoder[value] = entity;
+  }
+
+  static {
+    add("&nbsp",   160);
+    add("&iexcl",  161);
+    add("&cent",   162);
+    add("&pound",  163);
+    add("&curren", 164);
+    add("&yen",    165);
+    add("&brvbar", 166);
+    add("&sect",   167);
+    add("&uml",    168);
+    add("&copy",   169);
+    add("&ordf",   170);
+    add("&laquo",  171);
+    add("&not",    172);
+    add("&shy",    173);
+    add("&reg",    174);
+    add("&macr",   175);
+    add("&deg",    176);
+    add("&plusmn", 177);
+    add("&sup2",   178);
+    add("&sup3",   179);
+    add("&acute",  180);
+    add("&micro",  181);
+    add("&para",   182);
+    add("&middot", 183);
+    add("&cedil",  184);
+    add("&sup1",   185);
+    add("&ordm",   186);
+    add("&raquo",  187);
+    add("&frac14", 188);
+    add("&frac12", 189);
+    add("&frac34", 190);
+    add("&iquest", 191);
+    add("&Agrave", 192);
+    add("&Aacute", 193);
+    add("&Acirc",  194);
+    add("&Atilde", 195);
+    add("&Auml",   196);
+    add("&Aring",  197);
+    add("&AElig",  198);
+    add("&Ccedil", 199);
+    add("&Egrave", 200);
+    add("&Eacute", 201);
+    add("&Ecirc",  202);
+    add("&Euml",   203);
+    add("&Igrave", 204);
+    add("&Iacute", 205);
+    add("&Icirc",  206);
+    add("&Iuml",   207);
+    add("&ETH",    208);
+    add("&Ntilde", 209);
+    add("&Ograve", 210);
+    add("&Oacute", 211);
+    add("&Ocirc",  212);
+    add("&Otilde", 213);
+    add("&Ouml",   214);
+    add("&times",  215);
+    add("&Oslash", 216);
+    add("&Ugrave", 217);
+    add("&Uacute", 218);
+    add("&Ucirc",  219);
+    add("&Uuml",   220);
+    add("&Yacute", 221);
+    add("&THORN",  222);
+    add("&szlig",  223);
+    add("&agrave", 224);
+    add("&aacute", 225);
+    add("&acirc",  226);
+    add("&atilde", 227);
+    add("&auml",   228);
+    add("&aring",  229);
+    add("&aelig",  230);
+    add("&ccedil", 231);
+    add("&egrave", 232);
+    add("&eacute", 233);
+    add("&ecirc",  234);
+    add("&euml",   235);
+    add("&igrave", 236);
+    add("&iacute", 237);
+    add("&icirc",  238);
+    add("&iuml",   239);
+    add("&eth",    240);
+    add("&ntilde", 241);
+    add("&ograve", 242);
+    add("&oacute", 243);
+    add("&ocirc",  244);
+    add("&otilde", 245);
+    add("&ouml",   246);
+    add("&divide", 247);
+    add("&oslash", 248);
+    add("&ugrave", 249);
+    add("&uacute", 250);
+    add("&ucirc",  251);
+    add("&uuml",   252);
+    add("&yacute", 253);
+    add("&thorn",  254);
+    add("&yuml",   255);
+    add("&fnof",   402);
+    add("&Alpha",  913);
+    add("&Beta",   914);
+    add("&Gamma",  915);
+    add("&Delta",  916);
+    add("&Epsilon",917);
+    add("&Zeta",   918);
+    add("&Eta",    919);
+    add("&Theta",  920);
+    add("&Iota",   921);
+    add("&Kappa",  922);
+    add("&Lambda", 923);
+    add("&Mu",     924);
+    add("&Nu",     925);
+    add("&Xi",     926);
+    add("&Omicron",927);
+    add("&Pi",     928);
+    add("&Rho",    929);
+    add("&Sigma",  931);
+    add("&Tau",    932);
+    add("&Upsilon",933);
+    add("&Phi",    934);
+    add("&Chi",    935);
+    add("&Psi",    936);
+    add("&Omega",  937);
+    add("&alpha",  945);
+    add("&beta",   946);
+    add("&gamma",  947);
+    add("&delta",  948);
+    add("&epsilon",949);
+    add("&zeta",   950);
+    add("&eta",    951);
+    add("&theta",  952);
+    add("&iota",   953);
+    add("&kappa",  954);
+    add("&lambda", 955);
+    add("&mu",     956);
+    add("&nu",     957);
+    add("&xi",     958);
+    add("&omicron",959);
+    add("&pi",     960);
+    add("&rho",    961);
+    add("&sigmaf", 962);
+    add("&sigma",  963);
+    add("&tau",    964);
+    add("&upsilon",965);
+    add("&phi",    966);
+    add("&chi",    967);
+    add("&psi",    968);
+    add("&omega",  969);
+    add("&thetasym",977);
+    add("&upsih",  978);
+    add("&piv",    982);
+    add("&bull",   8226);
+    add("&hellip", 8230);
+    add("&prime",  8242);
+    add("&Prime",  8243);
+    add("&oline",  8254);
+    add("&frasl",  8260);
+    add("&weierp", 8472);
+    add("&image",  8465);
+    add("&real",   8476);
+    add("&trade",  8482);
+    add("&alefsym",8501);
+    add("&larr",   8592);
+    add("&uarr",   8593);
+    add("&rarr",   8594);
+    add("&darr",   8595);
+    add("&harr",   8596);
+    add("&crarr",  8629);
+    add("&lArr",   8656);
+    add("&uArr",   8657);
+    add("&rArr",   8658);
+    add("&dArr",   8659);
+    add("&hArr",   8660);
+    add("&forall", 8704);
+    add("&part",   8706);
+    add("&exist",  8707);
+    add("&empty",  8709);
+    add("&nabla",  8711);
+    add("&isin",   8712);
+    add("&notin",  8713);
+    add("&ni",     8715);
+    add("&prod",   8719);
+    add("&sum",    8721);
+    add("&minus",  8722);
+    add("&lowast", 8727);
+    add("&radic",  8730);
+    add("&prop",   8733);
+    add("&infin",  8734);
+    add("&ang",    8736);
+    add("&and",    8743);
+    add("&or",     8744);
+    add("&cap",    8745);
+    add("&cup",    8746);
+    add("&int",    8747);
+    add("&there4", 8756);
+    add("&sim",    8764);
+    add("&cong",   8773);
+    add("&asymp",  8776);
+    add("&ne",     8800);
+    add("&equiv",  8801);
+    add("&le",     8804);
+    add("&ge",     8805);
+    add("&sub",    8834);
+    add("&sup",    8835);
+    add("&nsub",   8836);
+    add("&sube",   8838);
+    add("&supe",   8839);
+    add("&oplus",  8853);
+    add("&otimes", 8855);
+    add("&perp",   8869);
+    add("&sdot",   8901);
+    add("&lceil",  8968);
+    add("&rceil",  8969);
+    add("&lfloor", 8970);
+    add("&rfloor", 8971);
+    add("&lang",   9001);
+    add("&rang",   9002);
+    add("&loz",    9674);
+    add("&spades", 9824);
+    add("&clubs",  9827);
+    add("&hearts", 9829);
+    add("&diams",  9830);
+    add("&quot",   34);
+    add("&amp",    38);
+    add("&lt",     60);
+    add("&gt",     62);
+    add("&OElig",  338);
+    add("&oelig",  339);
+    add("&Scaron", 352);
+    add("&scaron", 353);
+    add("&Yuml",   376);
+    add("&circ",   710);
+    add("&tilde",  732);
+    add("&ensp",   8194);
+    add("&emsp",   8195);
+    add("&thinsp", 8201);
+    add("&zwnj",   8204);
+    add("&zwj",    8205);
+    add("&lrm",    8206);
+    add("&rlm",    8207);
+    add("&ndash",  8211);
+    add("&mdash",  8212);
+    add("&lsquo",  8216);
+    add("&rsquo",  8217);
+    add("&sbquo",  8218);
+    add("&ldquo",  8220);
+    add("&rdquo",  8221);
+    add("&bdquo",  8222);
+    add("&dagger", 8224);
+    add("&Dagger", 8225);
+    add("&permil", 8240);
+    add("&lsaquo", 8249);
+    add("&rsaquo", 8250);
+    add("&euro",   8364);
+
+  }
+}
diff --git a/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/HTMLParser.java b/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/HTMLParser.java
new file mode 100644
index 0000000..4a89440
--- /dev/null
+++ b/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/HTMLParser.java
@@ -0,0 +1,755 @@
+/* Generated By:JavaCC: Do not edit this line. HTMLParser.java */
+package org.apache.lucene.demo.html;
+
+import java.io.*;
+import java.util.Properties;
+
+public class HTMLParser implements HTMLParserConstants {
+  public static int SUMMARY_LENGTH = 200;
+
+  StringBuffer title = new StringBuffer(SUMMARY_LENGTH);
+  StringBuffer summary = new StringBuffer(SUMMARY_LENGTH * 2);
+  Properties metaTags=new Properties();
+  String currentMetaTag=null;
+  String currentMetaContent=null;
+  int length = 0;
+  boolean titleComplete = false;
+  boolean inTitle = false;
+  boolean inMetaTag = false;
+  boolean inStyle = false;
+  boolean afterTag = false;
+  boolean afterSpace = false;
+  String eol = System.getProperty("line.separator");
+  Reader pipeIn = null;
+  Writer pipeOut;
+  private MyPipedInputStream pipeInStream = null;
+  private PipedOutputStream pipeOutStream = null;
+
+  private class MyPipedInputStream extends PipedInputStream{
+
+    public MyPipedInputStream(){
+      super();
+    }
+
+    public MyPipedInputStream(PipedOutputStream src) throws IOException{
+      super(src);
+    }
+
+    public boolean full() throws IOException{
+      return this.available() >= PipedInputStream.PIPE_SIZE;
+    }
+  }
+
+  /**
+   * @deprecated Use HTMLParser(FileInputStream) instead
+   */
+  @Deprecated
+  public HTMLParser(File file) throws FileNotFoundException {
+    this(new FileInputStream(file));
+  }
+
+  public String getTitle() throws IOException, InterruptedException {
+    if (pipeIn == null)
+      getReader();                                // spawn parsing thread
+    while (true) {
+      synchronized(this) {
+        if (titleComplete || pipeInStream.full())
+          break;
+        wait(10);
+      }
+    }
+    return title.toString().trim();
+  }
+
+  public Properties getMetaTags() throws IOException,
+InterruptedException {
+    if (pipeIn == null)
+      getReader();                                // spawn parsing thread
+    while (true) {
+      synchronized(this) {
+        if (titleComplete || pipeInStream.full())
+          break;
+        wait(10);
+      }
+    }
+    return metaTags;
+  }
+
+
+  public String getSummary() throws IOException, InterruptedException {
+    if (pipeIn == null)
+      getReader();                                // spawn parsing thread
+    while (true) {
+      synchronized(this) {
+        if (summary.length() >= SUMMARY_LENGTH || pipeInStream.full())
+          break;
+        wait(10);
+      }
+    }
+    if (summary.length() > SUMMARY_LENGTH)
+      summary.setLength(SUMMARY_LENGTH);
+
+    String sum = summary.toString().trim();
+    String tit = getTitle();
+    if (sum.startsWith(tit) || sum.equals(""))
+      return tit;
+    else
+      return sum;
+  }
+
+  public Reader getReader() throws IOException {
+    if (pipeIn == null) {
+      pipeInStream = new MyPipedInputStream();
+      pipeOutStream = new PipedOutputStream(pipeInStream);
+      pipeIn = new InputStreamReader(pipeInStream, "UTF-16BE");
+      pipeOut = new OutputStreamWriter(pipeOutStream, "UTF-16BE");
+
+      Thread thread = new ParserThread(this);
+      thread.start();                             // start parsing
+    }
+
+    return pipeIn;
+  }
+
+  void addToSummary(String text) {
+    if (summary.length() < SUMMARY_LENGTH) {
+      summary.append(text);
+      if (summary.length() >= SUMMARY_LENGTH) {
+        synchronized(this) {
+          notifyAll();
+        }
+      }
+    }
+  }
+
+  void addText(String text) throws IOException {
+    if (inStyle)
+      return;
+    if (inTitle)
+      title.append(text);
+    else {
+      addToSummary(text);
+      if (!titleComplete && !(title.length() == 0)) {  // finished title
+        synchronized(this) {
+          titleComplete = true;                   // tell waiting threads
+          notifyAll();
+        }
+      }
+    }
+
+    length += text.length();
+    pipeOut.write(text);
+
+    afterSpace = false;
+  }
+
+  void addMetaTag() {
+      metaTags.setProperty(currentMetaTag, currentMetaContent);
+      currentMetaTag = null;
+      currentMetaContent = null;
+      return;
+  }
+
+  void addSpace() throws IOException {
+    if (!afterSpace) {
+      if (inTitle)
+        title.append(" ");
+      else
+        addToSummary(" ");
+
+      String space = afterTag ? eol : " ";
+      length += space.length();
+      pipeOut.write(space);
+      afterSpace = true;
+    }
+  }
+
+  final public void HTMLDocument() throws ParseException, IOException {
+  Token t;
+    label_1:
+    while (true) {
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case ScriptStart:
+      case TagName:
+      case DeclName:
+      case Comment1:
+      case Comment2:
+      case Word:
+      case Entity:
+      case Space:
+      case Punct:
+        ;
+        break;
+      default:
+        jj_la1[0] = jj_gen;
+        break label_1;
+      }
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case TagName:
+        Tag();
+                      afterTag = true;
+        break;
+      case DeclName:
+        t = Decl();
+                      afterTag = true;
+        break;
+      case Comment1:
+      case Comment2:
+        CommentTag();
+                      afterTag = true;
+        break;
+      case ScriptStart:
+        ScriptTag();
+                     afterTag = true;
+        break;
+      case Word:
+        t = jj_consume_token(Word);
+                      addText(t.image); afterTag = false;
+        break;
+      case Entity:
+        t = jj_consume_token(Entity);
+                      addText(Entities.decode(t.image)); afterTag = false;
+        break;
+      case Punct:
+        t = jj_consume_token(Punct);
+                      addText(t.image); afterTag = false;
+        break;
+      case Space:
+        jj_consume_token(Space);
+                      addSpace(); afterTag = false;
+        break;
+      default:
+        jj_la1[1] = jj_gen;
+        jj_consume_token(-1);
+        throw new ParseException();
+      }
+    }
+    jj_consume_token(0);
+  }
+
+  final public void Tag() throws ParseException, IOException {
+  Token t1, t2;
+  boolean inImg = false;
+    t1 = jj_consume_token(TagName);
+   String tagName = t1.image.toLowerCase();
+   if(Tags.WS_ELEMS.contains(tagName) ) {
+      addSpace();
+    }
+    inTitle = tagName.equalsIgnoreCase("<title"); // keep track if in <TITLE>
+    inMetaTag = tagName.equalsIgnoreCase("<META"); // keep track if in <META>
+    inStyle = tagName.equalsIgnoreCase("<STYLE"); // keep track if in <STYLE>
+    inImg = tagName.equalsIgnoreCase("<img");     // keep track if in <IMG>
+
+    label_2:
+    while (true) {
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case ArgName:
+        ;
+        break;
+      default:
+        jj_la1[2] = jj_gen;
+        break label_2;
+      }
+      t1 = jj_consume_token(ArgName);
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case ArgEquals:
+        jj_consume_token(ArgEquals);
+        switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+        case ArgValue:
+        case ArgQuote1:
+        case ArgQuote2:
+          t2 = ArgValue();
+       if (inImg && t1.image.equalsIgnoreCase("alt") && t2 != null)
+         addText("[" + t2.image + "]");
+
+        if(inMetaTag &&
+                        (  t1.image.equalsIgnoreCase("name") ||
+                           t1.image.equalsIgnoreCase("HTTP-EQUIV")
+                        )
+           && t2 != null)
+        {
+                currentMetaTag=t2.image.toLowerCase();
+                if(currentMetaTag != null && currentMetaContent != null) {
+                addMetaTag();
+                }
+        }
+        if(inMetaTag && t1.image.equalsIgnoreCase("content") && t2 !=
+null)
+        {
+                currentMetaContent=t2.image.toLowerCase();
+                if(currentMetaTag != null && currentMetaContent != null) {
+                addMetaTag();
+                }
+        }
+          break;
+        default:
+          jj_la1[3] = jj_gen;
+          ;
+        }
+        break;
+      default:
+        jj_la1[4] = jj_gen;
+        ;
+      }
+    }
+    jj_consume_token(TagEnd);
+  }
+
+  final public Token ArgValue() throws ParseException {
+  Token t = null;
+    switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+    case ArgValue:
+      t = jj_consume_token(ArgValue);
+                                              {if (true) return t;}
+      break;
+    default:
+      jj_la1[5] = jj_gen;
+      if (jj_2_1(2)) {
+        jj_consume_token(ArgQuote1);
+        jj_consume_token(CloseQuote1);
+                                              {if (true) return t;}
+      } else {
+        switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+        case ArgQuote1:
+          jj_consume_token(ArgQuote1);
+          t = jj_consume_token(Quote1Text);
+          jj_consume_token(CloseQuote1);
+                                              {if (true) return t;}
+          break;
+        default:
+          jj_la1[6] = jj_gen;
+          if (jj_2_2(2)) {
+            jj_consume_token(ArgQuote2);
+            jj_consume_token(CloseQuote2);
+                                              {if (true) return t;}
+          } else {
+            switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+            case ArgQuote2:
+              jj_consume_token(ArgQuote2);
+              t = jj_consume_token(Quote2Text);
+              jj_consume_token(CloseQuote2);
+                                              {if (true) return t;}
+              break;
+            default:
+              jj_la1[7] = jj_gen;
+              jj_consume_token(-1);
+              throw new ParseException();
+            }
+          }
+        }
+      }
+    }
+    throw new Error("Missing return statement in function");
+  }
+
+  final public Token Decl() throws ParseException {
+  Token t;
+    t = jj_consume_token(DeclName);
+    label_3:
+    while (true) {
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case ArgName:
+      case ArgEquals:
+      case ArgValue:
+      case ArgQuote1:
+      case ArgQuote2:
+        ;
+        break;
+      default:
+        jj_la1[8] = jj_gen;
+        break label_3;
+      }
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case ArgName:
+        jj_consume_token(ArgName);
+        break;
+      case ArgValue:
+      case ArgQuote1:
+      case ArgQuote2:
+        ArgValue();
+        break;
+      case ArgEquals:
+        jj_consume_token(ArgEquals);
+        break;
+      default:
+        jj_la1[9] = jj_gen;
+        jj_consume_token(-1);
+        throw new ParseException();
+      }
+    }
+    jj_consume_token(TagEnd);
+    {if (true) return t;}
+    throw new Error("Missing return statement in function");
+  }
+
+  final public void CommentTag() throws ParseException {
+    switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+    case Comment1:
+      jj_consume_token(Comment1);
+      label_4:
+      while (true) {
+        switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+        case CommentText1:
+          ;
+          break;
+        default:
+          jj_la1[10] = jj_gen;
+          break label_4;
+        }
+        jj_consume_token(CommentText1);
+      }
+      jj_consume_token(CommentEnd1);
+      break;
+    case Comment2:
+      jj_consume_token(Comment2);
+      label_5:
+      while (true) {
+        switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+        case CommentText2:
+          ;
+          break;
+        default:
+          jj_la1[11] = jj_gen;
+          break label_5;
+        }
+        jj_consume_token(CommentText2);
+      }
+      jj_consume_token(CommentEnd2);
+      break;
+    default:
+      jj_la1[12] = jj_gen;
+      jj_consume_token(-1);
+      throw new ParseException();
+    }
+  }
+
+  final public void ScriptTag() throws ParseException {
+    jj_consume_token(ScriptStart);
+    label_6:
+    while (true) {
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case ScriptText:
+        ;
+        break;
+      default:
+        jj_la1[13] = jj_gen;
+        break label_6;
+      }
+      jj_consume_token(ScriptText);
+    }
+    jj_consume_token(ScriptEnd);
+  }
+
+  private boolean jj_2_1(int xla) {
+    jj_la = xla; jj_lastpos = jj_scanpos = token;
+    try { return !jj_3_1(); }
+    catch(LookaheadSuccess ls) { return true; }
+    finally { jj_save(0, xla); }
+  }
+
+  private boolean jj_2_2(int xla) {
+    jj_la = xla; jj_lastpos = jj_scanpos = token;
+    try { return !jj_3_2(); }
+    catch(LookaheadSuccess ls) { return true; }
+    finally { jj_save(1, xla); }
+  }
+
+  private boolean jj_3_1() {
+    if (jj_scan_token(ArgQuote1)) return true;
+    if (jj_scan_token(CloseQuote1)) return true;
+    return false;
+  }
+
+  private boolean jj_3_2() {
+    if (jj_scan_token(ArgQuote2)) return true;
+    if (jj_scan_token(CloseQuote2)) return true;
+    return false;
+  }
+
+  /** Generated Token Manager. */
+  public HTMLParserTokenManager token_source;
+  SimpleCharStream jj_input_stream;
+  /** Current token. */
+  public Token token;
+  /** Next token. */
+  public Token jj_nt;
+  private int jj_ntk;
+  private Token jj_scanpos, jj_lastpos;
+  private int jj_la;
+  private int jj_gen;
+  final private int[] jj_la1 = new int[14];
+  static private int[] jj_la1_0;
+  static {
+      jj_la1_init_0();
+   }
+   private static void jj_la1_init_0() {
+      jj_la1_0 = new int[] {0x2c7e,0x2c7e,0x10000,0x380000,0x20000,0x80000,0x100000,0x200000,0x3b0000,0x3b0000,0x8000000,0x20000000,0x30,0x4000,};
+   }
+  final private JJCalls[] jj_2_rtns = new JJCalls[2];
+  private boolean jj_rescan = false;
+  private int jj_gc = 0;
+
+  /** Constructor with InputStream. */
+  public HTMLParser(java.io.InputStream stream) {
+     this(stream, null);
+  }
+  /** Constructor with InputStream and supplied encoding */
+  public HTMLParser(java.io.InputStream stream, String encoding) {
+    try { jj_input_stream = new SimpleCharStream(stream, encoding, 1, 1); } catch(java.io.UnsupportedEncodingException e) { throw new RuntimeException(e); }
+    token_source = new HTMLParserTokenManager(jj_input_stream);
+    token = new Token();
+    jj_ntk = -1;
+    jj_gen = 0;
+    for (int i = 0; i < 14; i++) jj_la1[i] = -1;
+    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
+  }
+
+  /** Reinitialise. */
+  public void ReInit(java.io.InputStream stream) {
+     ReInit(stream, null);
+  }
+  /** Reinitialise. */
+  public void ReInit(java.io.InputStream stream, String encoding) {
+    try { jj_input_stream.ReInit(stream, encoding, 1, 1); } catch(java.io.UnsupportedEncodingException e) { throw new RuntimeException(e); }
+    token_source.ReInit(jj_input_stream);
+    token = new Token();
+    jj_ntk = -1;
+    jj_gen = 0;
+    for (int i = 0; i < 14; i++) jj_la1[i] = -1;
+    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
+  }
+
+  /** Constructor. */
+  public HTMLParser(java.io.Reader stream) {
+    jj_input_stream = new SimpleCharStream(stream, 1, 1);
+    token_source = new HTMLParserTokenManager(jj_input_stream);
+    token = new Token();
+    jj_ntk = -1;
+    jj_gen = 0;
+    for (int i = 0; i < 14; i++) jj_la1[i] = -1;
+    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
+  }
+
+  /** Reinitialise. */
+  public void ReInit(java.io.Reader stream) {
+    jj_input_stream.ReInit(stream, 1, 1);
+    token_source.ReInit(jj_input_stream);
+    token = new Token();
+    jj_ntk = -1;
+    jj_gen = 0;
+    for (int i = 0; i < 14; i++) jj_la1[i] = -1;
+    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
+  }
+
+  /** Constructor with generated Token Manager. */
+  public HTMLParser(HTMLParserTokenManager tm) {
+    token_source = tm;
+    token = new Token();
+    jj_ntk = -1;
+    jj_gen = 0;
+    for (int i = 0; i < 14; i++) jj_la1[i] = -1;
+    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
+  }
+
+  /** Reinitialise. */
+  public void ReInit(HTMLParserTokenManager tm) {
+    token_source = tm;
+    token = new Token();
+    jj_ntk = -1;
+    jj_gen = 0;
+    for (int i = 0; i < 14; i++) jj_la1[i] = -1;
+    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
+  }
+
+  private Token jj_consume_token(int kind) throws ParseException {
+    Token oldToken;
+    if ((oldToken = token).next != null) token = token.next;
+    else token = token.next = token_source.getNextToken();
+    jj_ntk = -1;
+    if (token.kind == kind) {
+      jj_gen++;
+      if (++jj_gc > 100) {
+        jj_gc = 0;
+        for (int i = 0; i < jj_2_rtns.length; i++) {
+          JJCalls c = jj_2_rtns[i];
+          while (c != null) {
+            if (c.gen < jj_gen) c.first = null;
+            c = c.next;
+          }
+        }
+      }
+      return token;
+    }
+    token = oldToken;
+    jj_kind = kind;
+    throw generateParseException();
+  }
+
+  static private final class LookaheadSuccess extends java.lang.Error { }
+  final private LookaheadSuccess jj_ls = new LookaheadSuccess();
+  private boolean jj_scan_token(int kind) {
+    if (jj_scanpos == jj_lastpos) {
+      jj_la--;
+      if (jj_scanpos.next == null) {
+        jj_lastpos = jj_scanpos = jj_scanpos.next = token_source.getNextToken();
+      } else {
+        jj_lastpos = jj_scanpos = jj_scanpos.next;
+      }
+    } else {
+      jj_scanpos = jj_scanpos.next;
+    }
+    if (jj_rescan) {
+      int i = 0; Token tok = token;
+      while (tok != null && tok != jj_scanpos) { i++; tok = tok.next; }
+      if (tok != null) jj_add_error_token(kind, i);
+    }
+    if (jj_scanpos.kind != kind) return true;
+    if (jj_la == 0 && jj_scanpos == jj_lastpos) throw jj_ls;
+    return false;
+  }
+
+
+/** Get the next Token. */
+  final public Token getNextToken() {
+    if (token.next != null) token = token.next;
+    else token = token.next = token_source.getNextToken();
+    jj_ntk = -1;
+    jj_gen++;
+    return token;
+  }
+
+/** Get the specific Token. */
+  final public Token getToken(int index) {
+    Token t = token;
+    for (int i = 0; i < index; i++) {
+      if (t.next != null) t = t.next;
+      else t = t.next = token_source.getNextToken();
+    }
+    return t;
+  }
+
+  private int jj_ntk() {
+    if ((jj_nt=token.next) == null)
+      return (jj_ntk = (token.next=token_source.getNextToken()).kind);
+    else
+      return (jj_ntk = jj_nt.kind);
+  }
+
+  private java.util.List<int[]> jj_expentries = new java.util.ArrayList<int[]>();
+  private int[] jj_expentry;
+  private int jj_kind = -1;
+  private int[] jj_lasttokens = new int[100];
+  private int jj_endpos;
+
+  private void jj_add_error_token(int kind, int pos) {
+    if (pos >= 100) return;
+    if (pos == jj_endpos + 1) {
+      jj_lasttokens[jj_endpos++] = kind;
+    } else if (jj_endpos != 0) {
+      jj_expentry = new int[jj_endpos];
+      for (int i = 0; i < jj_endpos; i++) {
+        jj_expentry[i] = jj_lasttokens[i];
+      }
+      jj_entries_loop: for (java.util.Iterator it = jj_expentries.iterator(); it.hasNext();) {
+        int[] oldentry = (int[])(it.next());
+        if (oldentry.length == jj_expentry.length) {
+          for (int i = 0; i < jj_expentry.length; i++) {
+            if (oldentry[i] != jj_expentry[i]) {
+              continue jj_entries_loop;
+            }
+          }
+          jj_expentries.add(jj_expentry);
+          break jj_entries_loop;
+        }
+      }
+      if (pos != 0) jj_lasttokens[(jj_endpos = pos) - 1] = kind;
+    }
+  }
+
+  /** Generate ParseException. */
+  public ParseException generateParseException() {
+    jj_expentries.clear();
+    boolean[] la1tokens = new boolean[31];
+    if (jj_kind >= 0) {
+      la1tokens[jj_kind] = true;
+      jj_kind = -1;
+    }
+    for (int i = 0; i < 14; i++) {
+      if (jj_la1[i] == jj_gen) {
+        for (int j = 0; j < 32; j++) {
+          if ((jj_la1_0[i] & (1<<j)) != 0) {
+            la1tokens[j] = true;
+          }
+        }
+      }
+    }
+    for (int i = 0; i < 31; i++) {
+      if (la1tokens[i]) {
+        jj_expentry = new int[1];
+        jj_expentry[0] = i;
+        jj_expentries.add(jj_expentry);
+      }
+    }
+    jj_endpos = 0;
+    jj_rescan_token();
+    jj_add_error_token(0, 0);
+    int[][] exptokseq = new int[jj_expentries.size()][];
+    for (int i = 0; i < jj_expentries.size(); i++) {
+      exptokseq[i] = jj_expentries.get(i);
+    }
+    return new ParseException(token, exptokseq, tokenImage);
+  }
+
+  /** Enable tracing. */
+  final public void enable_tracing() {
+  }
+
+  /** Disable tracing. */
+  final public void disable_tracing() {
+  }
+
+  private void jj_rescan_token() {
+    jj_rescan = true;
+    for (int i = 0; i < 2; i++) {
+    try {
+      JJCalls p = jj_2_rtns[i];
+      do {
+        if (p.gen > jj_gen) {
+          jj_la = p.arg; jj_lastpos = jj_scanpos = p.first;
+          switch (i) {
+            case 0: jj_3_1(); break;
+            case 1: jj_3_2(); break;
+          }
+        }
+        p = p.next;
+      } while (p != null);
+      } catch(LookaheadSuccess ls) { }
+    }
+    jj_rescan = false;
+  }
+
+  private void jj_save(int index, int xla) {
+    JJCalls p = jj_2_rtns[index];
+    while (p.gen > jj_gen) {
+      if (p.next == null) { p = p.next = new JJCalls(); break; }
+      p = p.next;
+    }
+    p.gen = jj_gen + xla - jj_la; p.first = token; p.arg = xla;
+  }
+
+  static final class JJCalls {
+    int gen;
+    Token first;
+    int arg;
+    JJCalls next;
+  }
+
+//    void handleException(Exception e) {
+//      System.out.println(e.toString());  // print the error message
+//      System.out.println("Skipping...");
+//      Token t;
+//      do {
+//        t = getNextToken();
+//      } while (t.kind != TagEnd);
+//    }
+}
diff --git a/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/HTMLParser.jj b/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/HTMLParser.jj
new file mode 100644
index 0000000..6a3578c
--- /dev/null
+++ b/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/HTMLParser.jj
@@ -0,0 +1,393 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// HTMLParser.jj
+
+options {
+  STATIC = false;
+  OPTIMIZE_TOKEN_MANAGER = true;
+  //DEBUG_LOOKAHEAD = true;
+  //DEBUG_TOKEN_MANAGER = true;
+}
+
+PARSER_BEGIN(HTMLParser)
+
+package org.apache.lucene.demo.html;
+
+import java.io.*;
+import java.util.Properties;
+
+public class HTMLParser {
+  public static int SUMMARY_LENGTH = 200;
+
+  StringBuffer title = new StringBuffer(SUMMARY_LENGTH);
+  StringBuffer summary = new StringBuffer(SUMMARY_LENGTH * 2);
+  Properties metaTags=new Properties();
+  String currentMetaTag=null;
+  String currentMetaContent=null;
+  int length = 0;
+  boolean titleComplete = false;
+  boolean inTitle = false;
+  boolean inMetaTag = false;
+  boolean inStyle = false;
+  boolean afterTag = false;
+  boolean afterSpace = false;
+  String eol = System.getProperty("line.separator");
+  Reader pipeIn = null;
+  Writer pipeOut;
+  private MyPipedInputStream pipeInStream = null;
+  private PipedOutputStream pipeOutStream = null;
+  
+  private class MyPipedInputStream extends PipedInputStream{
+    
+    public MyPipedInputStream(){
+      super();
+    }
+    
+    public MyPipedInputStream(PipedOutputStream src) throws IOException{
+      super(src);
+    }
+    
+    public boolean full() throws IOException{
+      return this.available() >= PipedInputStream.PIPE_SIZE;
+    }
+  }
+
+  /**
+   * @deprecated Use HTMLParser(FileInputStream) instead
+   */
+  @Deprecated
+  public HTMLParser(File file) throws FileNotFoundException {
+    this(new FileInputStream(file));
+  }
+
+  public String getTitle() throws IOException, InterruptedException {
+    if (pipeIn == null)
+      getReader();				  // spawn parsing thread
+    while (true) {
+      synchronized(this) {
+	if (titleComplete || pipeInStream.full())
+	  break;
+	wait(10);
+      }
+    }
+    return title.toString().trim();
+  }
+
+  public Properties getMetaTags() throws IOException,
+InterruptedException {
+    if (pipeIn == null)
+      getReader();				  // spawn parsing thread
+    while (true) {
+      synchronized(this) {
+	if (titleComplete || pipeInStream.full())
+	  break;
+	wait(10);
+      }
+    }
+    return metaTags;
+  }
+
+
+  public String getSummary() throws IOException, InterruptedException {
+    if (pipeIn == null)
+      getReader();				  // spawn parsing thread
+    while (true) {
+      synchronized(this) {
+	if (summary.length() >= SUMMARY_LENGTH || pipeInStream.full())
+	  break;
+	wait(10);
+      }
+    }
+    if (summary.length() > SUMMARY_LENGTH)
+      summary.setLength(SUMMARY_LENGTH);
+
+    String sum = summary.toString().trim();
+    String tit = getTitle();
+    if (sum.startsWith(tit) || sum.equals(""))
+      return tit;
+    else
+      return sum;
+  }
+
+  public Reader getReader() throws IOException {
+    if (pipeIn == null) {
+      pipeInStream = new MyPipedInputStream();
+      pipeOutStream = new PipedOutputStream(pipeInStream);
+      pipeIn = new InputStreamReader(pipeInStream, "UTF-16BE");
+      pipeOut = new OutputStreamWriter(pipeOutStream, "UTF-16BE");
+
+      Thread thread = new ParserThread(this);
+      thread.start();				  // start parsing
+    }
+
+    return pipeIn;
+  }
+
+  void addToSummary(String text) {
+    if (summary.length() < SUMMARY_LENGTH) {
+      summary.append(text);
+      if (summary.length() >= SUMMARY_LENGTH) {
+	synchronized(this) {
+	  notifyAll();
+	}
+      }
+    }
+  }
+
+  void addText(String text) throws IOException {
+    if (inStyle)
+      return;
+    if (inTitle)
+      title.append(text);
+    else {
+      addToSummary(text);
+      if (!titleComplete && !(title.length() == 0)) {  // finished title
+	synchronized(this) {
+	  titleComplete = true;			  // tell waiting threads
+	  notifyAll();
+	}
+      }
+    }
+
+    length += text.length();
+    pipeOut.write(text);
+
+    afterSpace = false;
+  }
+  
+  void addMetaTag() {
+      metaTags.setProperty(currentMetaTag, currentMetaContent);
+      currentMetaTag = null;
+      currentMetaContent = null;
+      return;
+  }
+
+  void addSpace() throws IOException {
+    if (!afterSpace) {
+      if (inTitle)
+	title.append(" ");
+      else
+	addToSummary(" ");
+
+      String space = afterTag ? eol : " ";
+      length += space.length();
+      pipeOut.write(space);
+      afterSpace = true;
+    }
+  }
+
+//    void handleException(Exception e) {
+//      System.out.println(e.toString());  // print the error message
+//      System.out.println("Skipping...");
+//      Token t;
+//      do {
+//        t = getNextToken();
+//      } while (t.kind != TagEnd);
+//    }
+}
+
+PARSER_END(HTMLParser)
+
+
+void HTMLDocument() throws IOException :
+{
+  Token t;
+}
+{
+//  try {
+    ( Tag()         { afterTag = true; }
+    | t=Decl()      { afterTag = true; }
+    | CommentTag()  { afterTag = true; }
+    | ScriptTag()  { afterTag = true; }
+    | t=<Word>      { addText(t.image); afterTag = false; }
+    | t=<Entity>    { addText(Entities.decode(t.image)); afterTag = false; }
+    | t=<Punct>     { addText(t.image); afterTag = false; }
+    | <Space>       { addSpace(); afterTag = false; }
+    )* <EOF>
+//  } catch (ParseException e) {
+//    handleException(e);
+//  }
+}
+
+void Tag() throws IOException :
+{
+  Token t1, t2;
+  boolean inImg = false;
+}
+{
+  t1=<TagName> {
+   String tagName = t1.image.toLowerCase();
+   if(Tags.WS_ELEMS.contains(tagName) ) {
+      addSpace();
+    }
+    inTitle = tagName.equalsIgnoreCase("<title"); // keep track if in <TITLE>
+    inMetaTag = tagName.equalsIgnoreCase("<META"); // keep track if in <META>
+    inStyle = tagName.equalsIgnoreCase("<STYLE"); // keep track if in <STYLE>
+    inImg = tagName.equalsIgnoreCase("<img");	  // keep track if in <IMG>
+  }
+  (t1=<ArgName>
+   (<ArgEquals>
+    (t2=ArgValue()				  // save ALT text in IMG tag
+     {
+       if (inImg && t1.image.equalsIgnoreCase("alt") && t2 != null)
+         addText("[" + t2.image + "]");
+
+    	if(inMetaTag &&
+			(  t1.image.equalsIgnoreCase("name") ||
+			   t1.image.equalsIgnoreCase("HTTP-EQUIV")
+			)
+	   && t2 != null)
+	{
+		currentMetaTag=t2.image.toLowerCase();
+		if(currentMetaTag != null && currentMetaContent != null) {
+        	addMetaTag();
+		}
+	}
+    	if(inMetaTag && t1.image.equalsIgnoreCase("content") && t2 !=
+null)
+	{
+		currentMetaContent=t2.image.toLowerCase();
+		if(currentMetaTag != null && currentMetaContent != null) {
+        	addMetaTag();
+		}
+	}
+     }
+    )?
+   )?
+  )*
+  <TagEnd>
+}
+
+Token ArgValue() :
+{
+  Token t = null;
+}
+{
+  t=<ArgValue>                              { return t; }
+| LOOKAHEAD(2)
+  <ArgQuote1> <CloseQuote1>                 { return t; }
+| <ArgQuote1> t=<Quote1Text> <CloseQuote1>  { return t; }
+| LOOKAHEAD(2)
+  <ArgQuote2> <CloseQuote2>                 { return t; }
+| <ArgQuote2> t=<Quote2Text> <CloseQuote2>  { return t; }
+}
+
+
+Token Decl() :
+{
+  Token t;
+}
+{
+  t=<DeclName> ( <ArgName> | ArgValue() | <ArgEquals> )* <TagEnd>
+  { return t; }
+}
+
+
+void CommentTag() :
+{}
+{
+  (<Comment1> ( <CommentText1> )* <CommentEnd1>)
+ |
+  (<Comment2> ( <CommentText2> )* <CommentEnd2>)
+}
+
+void ScriptTag() :
+{}
+{
+  <ScriptStart> ( <ScriptText> )* <ScriptEnd>
+}
+
+
+TOKEN :
+{
+  < ScriptStart: "<script" > : WithinScript
+| < TagName:  "<" ("/")? ["A"-"Z","a"-"z"] (<ArgName>)? > : WithinTag
+| < DeclName: "<"  "!"   ["A"-"Z","a"-"z"] (<ArgName>)? > : WithinTag
+
+| < Comment1:  "<!--" > : WithinComment1
+| < Comment2:  "<!" >   : WithinComment2
+
+| < Word:     ( <LET> | <LET> (["+","/"])+ | <NUM> ["\""] |
+                <LET> ["-","'"] <LET> | ("$")? <NUM> [",","."] <NUM> )+ >
+| < #LET:     ["A"-"Z","a"-"z","0"-"9"] >
+| < #NUM:     ["0"-"9"] >
+| < #HEX:     ["0"-"9","A"-"F","a"-"f"] >
+
+| < Entity:   ( "&" (["A"-"Z","a"-"z"])+ (";")? | "&" "#" (<NUM>)+ (";")? | "&" "#" ["X","x"] (<HEX>)+ (";")? ) >
+
+| < Space:    (<SP>)+ >
+| < #SP:      [" ","\t","\r","\n"] >
+
+| < Punct:    ~[] > // Keep this last.  It is a catch-all.
+}
+
+<WithinScript> TOKEN:
+{
+  < ScriptText:  (~["<",">"])+ | "<" | ">" >
+| < ScriptEnd: "</script" (~["<",">"])* ">" > : DEFAULT
+}
+
+<WithinTag> TOKEN:
+{
+  < ArgName:   (~[" ","\t","\r","\n","=",">","'","\""])
+               (~[" ","\t","\r","\n","=",">"])* >
+| < ArgEquals: "=" >  : AfterEquals
+| < TagEnd:    ">" | "=>" >  : DEFAULT
+}
+
+<AfterEquals> TOKEN:
+{
+  < ArgValue:  (~[" ","\t","\r","\n","=",">","'","\""])
+	       (~[" ","\t","\r","\n",">"])* > : WithinTag
+}
+
+<WithinTag, AfterEquals> TOKEN:
+{
+  < ArgQuote1: "'"  > : WithinQuote1
+| < ArgQuote2: "\"" > : WithinQuote2
+}
+
+<WithinTag, AfterEquals> SKIP:
+{
+  < <Space> >
+}
+
+<WithinQuote1> TOKEN:
+{
+  < Quote1Text:  (~["'"])+ >
+| < CloseQuote1: <ArgQuote1> > : WithinTag
+}
+
+<WithinQuote2> TOKEN:
+{
+  < Quote2Text:  (~["\""])+ >
+| < CloseQuote2: <ArgQuote2> > : WithinTag
+}
+
+
+<WithinComment1> TOKEN :
+{
+  < CommentText1:  (~["-"])+ | "-" >
+| < CommentEnd1:   "-->" > : DEFAULT
+}
+
+<WithinComment2> TOKEN :
+{
+  < CommentText2:  (~[">"])+ >
+| < CommentEnd2:   ">" > : DEFAULT
+}
diff --git a/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/HTMLParserConstants.java b/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/HTMLParserConstants.java
new file mode 100644
index 0000000..bf11b45
--- /dev/null
+++ b/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/HTMLParserConstants.java
@@ -0,0 +1,124 @@
+/* Generated By:JavaCC: Do not edit this line. HTMLParserConstants.java */
+package org.apache.lucene.demo.html;
+
+
+/**
+ * Token literal values and constants.
+ * Generated by org.javacc.parser.OtherFilesGen#start()
+ */
+public interface HTMLParserConstants {
+
+  /** End of File. */
+  int EOF = 0;
+  /** RegularExpression Id. */
+  int ScriptStart = 1;
+  /** RegularExpression Id. */
+  int TagName = 2;
+  /** RegularExpression Id. */
+  int DeclName = 3;
+  /** RegularExpression Id. */
+  int Comment1 = 4;
+  /** RegularExpression Id. */
+  int Comment2 = 5;
+  /** RegularExpression Id. */
+  int Word = 6;
+  /** RegularExpression Id. */
+  int LET = 7;
+  /** RegularExpression Id. */
+  int NUM = 8;
+  /** RegularExpression Id. */
+  int HEX = 9;
+  /** RegularExpression Id. */
+  int Entity = 10;
+  /** RegularExpression Id. */
+  int Space = 11;
+  /** RegularExpression Id. */
+  int SP = 12;
+  /** RegularExpression Id. */
+  int Punct = 13;
+  /** RegularExpression Id. */
+  int ScriptText = 14;
+  /** RegularExpression Id. */
+  int ScriptEnd = 15;
+  /** RegularExpression Id. */
+  int ArgName = 16;
+  /** RegularExpression Id. */
+  int ArgEquals = 17;
+  /** RegularExpression Id. */
+  int TagEnd = 18;
+  /** RegularExpression Id. */
+  int ArgValue = 19;
+  /** RegularExpression Id. */
+  int ArgQuote1 = 20;
+  /** RegularExpression Id. */
+  int ArgQuote2 = 21;
+  /** RegularExpression Id. */
+  int Quote1Text = 23;
+  /** RegularExpression Id. */
+  int CloseQuote1 = 24;
+  /** RegularExpression Id. */
+  int Quote2Text = 25;
+  /** RegularExpression Id. */
+  int CloseQuote2 = 26;
+  /** RegularExpression Id. */
+  int CommentText1 = 27;
+  /** RegularExpression Id. */
+  int CommentEnd1 = 28;
+  /** RegularExpression Id. */
+  int CommentText2 = 29;
+  /** RegularExpression Id. */
+  int CommentEnd2 = 30;
+
+  /** Lexical state. */
+  int DEFAULT = 0;
+  /** Lexical state. */
+  int WithinScript = 1;
+  /** Lexical state. */
+  int WithinTag = 2;
+  /** Lexical state. */
+  int AfterEquals = 3;
+  /** Lexical state. */
+  int WithinQuote1 = 4;
+  /** Lexical state. */
+  int WithinQuote2 = 5;
+  /** Lexical state. */
+  int WithinComment1 = 6;
+  /** Lexical state. */
+  int WithinComment2 = 7;
+
+  /** Literal token values. */
+  String[] tokenImage = {
+    "<EOF>",
+    "\"<script\"",
+    "<TagName>",
+    "<DeclName>",
+    "\"<!--\"",
+    "\"<!\"",
+    "<Word>",
+    "<LET>",
+    "<NUM>",
+    "<HEX>",
+    "<Entity>",
+    "<Space>",
+    "<SP>",
+    "<Punct>",
+    "<ScriptText>",
+    "<ScriptEnd>",
+    "<ArgName>",
+    "\"=\"",
+    "<TagEnd>",
+    "<ArgValue>",
+    "\"\\\'\"",
+    "\"\\\"\"",
+    "<token of kind 22>",
+    "<Quote1Text>",
+    "<CloseQuote1>",
+    "<Quote2Text>",
+    "<CloseQuote2>",
+    "<CommentText1>",
+    "\"-->\"",
+    "<CommentText2>",
+    "\">\"",
+  };
+
+}
diff --git a/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/HTMLParserTokenManager.java b/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/HTMLParserTokenManager.java
new file mode 100644
index 0000000..d6064d5
--- /dev/null
+++ b/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/HTMLParserTokenManager.java
@@ -0,0 +1,1619 @@
+/* Generated By:JavaCC: Do not edit this line. HTMLParserTokenManager.java */
+package org.apache.lucene.demo.html;
+import java.io.*;
+import java.util.Properties;
+
+/** Token Manager. */
+public class HTMLParserTokenManager implements HTMLParserConstants
+{
+
+  /** Debug output. */
+  public  java.io.PrintStream debugStream = System.out;
+  /** Set debug output. */
+  public  void setDebugStream(java.io.PrintStream ds) { debugStream = ds; }
+private final int jjStopStringLiteralDfa_0(int pos, long active0)
+{
+   switch (pos)
+   {
+      case 0:
+         if ((active0 & 0x32L) != 0L)
+            return 20;
+         return -1;
+      case 1:
+         if ((active0 & 0x2L) != 0L)
+         {
+            if (jjmatchedPos != 1)
+            {
+               jjmatchedKind = 2;
+               jjmatchedPos = 1;
+            }
+            return 22;
+         }
+         if ((active0 & 0x30L) != 0L)
+            return 25;
+         return -1;
+      case 2:
+         if ((active0 & 0x2L) != 0L)
+         {
+            jjmatchedKind = 2;
+            jjmatchedPos = 2;
+            return 23;
+         }
+         return -1;
+      case 3:
+         if ((active0 & 0x2L) != 0L)
+         {
+            jjmatchedKind = 2;
+            jjmatchedPos = 3;
+            return 23;
+         }
+         return -1;
+      case 4:
+         if ((active0 & 0x2L) != 0L)
+         {
+            jjmatchedKind = 2;
+            jjmatchedPos = 4;
+            return 23;
+         }
+         return -1;
+      case 5:
+         if ((active0 & 0x2L) != 0L)
+         {
+            jjmatchedKind = 2;
+            jjmatchedPos = 5;
+            return 23;
+         }
+         return -1;
+      default :
+         return -1;
+   }
+}
+private final int jjStartNfa_0(int pos, long active0)
+{
+   return jjMoveNfa_0(jjStopStringLiteralDfa_0(pos, active0), pos + 1);
+}
+private int jjStopAtPos(int pos, int kind)
+{
+   jjmatchedKind = kind;
+   jjmatchedPos = pos;
+   return pos + 1;
+}
+private int jjMoveStringLiteralDfa0_0()
+{
+   switch(curChar)
+   {
+      case 60:
+         return jjMoveStringLiteralDfa1_0(0x32L);
+      default :
+         return jjMoveNfa_0(11, 0);
+   }
+}
+private int jjMoveStringLiteralDfa1_0(long active0)
+{
+   try { curChar = input_stream.readChar(); }
+   catch(java.io.IOException e) {
+      jjStopStringLiteralDfa_0(0, active0);
+      return 1;
+   }
+   switch(curChar)
+   {
+      case 33:
+         if ((active0 & 0x20L) != 0L)
+         {
+            jjmatchedKind = 5;
+            jjmatchedPos = 1;
+         }
+         return jjMoveStringLiteralDfa2_0(active0, 0x10L);
+      case 115:
+         return jjMoveStringLiteralDfa2_0(active0, 0x2L);
+      default :
+         break;
+   }
+   return jjStartNfa_0(0, active0);
+}
+private int jjMoveStringLiteralDfa2_0(long old0, long active0)
+{
+   if (((active0 &= old0)) == 0L)
+      return jjStartNfa_0(0, old0);
+   try { curChar = input_stream.readChar(); }
+   catch(java.io.IOException e) {
+      jjStopStringLiteralDfa_0(1, active0);
+      return 2;
+   }
+   switch(curChar)
+   {
+      case 45:
+         return jjMoveStringLiteralDfa3_0(active0, 0x10L);
+      case 99:
+         return jjMoveStringLiteralDfa3_0(active0, 0x2L);
+      default :
+         break;
+   }
+   return jjStartNfa_0(1, active0);
+}
+private int jjMoveStringLiteralDfa3_0(long old0, long active0)
+{
+   if (((active0 &= old0)) == 0L)
+      return jjStartNfa_0(1, old0);
+   try { curChar = input_stream.readChar(); }
+   catch(java.io.IOException e) {
+      jjStopStringLiteralDfa_0(2, active0);
+      return 3;
+   }
+   switch(curChar)
+   {
+      case 45:
+         if ((active0 & 0x10L) != 0L)
+            return jjStopAtPos(3, 4);
+         break;
+      case 114:
+         return jjMoveStringLiteralDfa4_0(active0, 0x2L);
+      default :
+         break;
+   }
+   return jjStartNfa_0(2, active0);
+}
+private int jjMoveStringLiteralDfa4_0(long old0, long active0)
+{
+   if (((active0 &= old0)) == 0L)
+      return jjStartNfa_0(2, old0);
+   try { curChar = input_stream.readChar(); }
+   catch(java.io.IOException e) {
+      jjStopStringLiteralDfa_0(3, active0);
+      return 4;
+   }
+   switch(curChar)
+   {
+      case 105:
+         return jjMoveStringLiteralDfa5_0(active0, 0x2L);
+      default :
+         break;
+   }
+   return jjStartNfa_0(3, active0);
+}
+private int jjMoveStringLiteralDfa5_0(long old0, long active0)
+{
+   if (((active0 &= old0)) == 0L)
+      return jjStartNfa_0(3, old0);
+   try { curChar = input_stream.readChar(); }
+   catch(java.io.IOException e) {
+      jjStopStringLiteralDfa_0(4, active0);
+      return 5;
+   }
+   switch(curChar)
+   {
+      case 112:
+         return jjMoveStringLiteralDfa6_0(active0, 0x2L);
+      default :
+         break;
+   }
+   return jjStartNfa_0(4, active0);
+}
+private int jjMoveStringLiteralDfa6_0(long old0, long active0)
+{
+   if (((active0 &= old0)) == 0L)
+      return jjStartNfa_0(4, old0);
+   try { curChar = input_stream.readChar(); }
+   catch(java.io.IOException e) {
+      jjStopStringLiteralDfa_0(5, active0);
+      return 6;
+   }
+   switch(curChar)
+   {
+      case 116:
+         if ((active0 & 0x2L) != 0L)
+            return jjStartNfaWithStates_0(6, 1, 23);
+         break;
+      default :
+         break;
+   }
+   return jjStartNfa_0(5, active0);
+}
+private int jjStartNfaWithStates_0(int pos, int kind, int state)
+{
+   jjmatchedKind = kind;
+   jjmatchedPos = pos;
+   try { curChar = input_stream.readChar(); }
+   catch(java.io.IOException e) { return pos + 1; }
+   return jjMoveNfa_0(state, pos + 1);
+}
+static final long[] jjbitVec0 = {
+   0x0L, 0x0L, 0xffffffffffffffffL, 0xffffffffffffffffL
+};
+private int jjMoveNfa_0(int startState, int curPos)
+{
+   int startsAt = 0;
+   jjnewStateCnt = 28;
+   int i = 1;
+   jjstateSet[0] = startState;
+   int kind = 0x7fffffff;
+   for (;;)
+   {
+      if (++jjround == 0x7fffffff)
+         ReInitRounds();
+      if (curChar < 64)
+      {
+         long l = 1L << curChar;
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 20:
+                  if (curChar == 33)
+                     jjstateSet[jjnewStateCnt++] = 25;
+                  else if (curChar == 47)
+                     jjCheckNAdd(21);
+                  break;
+               case 11:
+                  if ((0x3ff000000000000L & l) != 0L)
+                     jjCheckNAddTwoStates(7, 2);
+                  else if ((0x100002600L & l) != 0L)
+                  {
+                     if (kind > 11)
+                        kind = 11;
+                     jjCheckNAdd(10);
+                  }
+                  else if (curChar == 60)
+                     jjCheckNAddStates(0, 2);
+                  else if (curChar == 38)
+                     jjAddStates(3, 5);
+                  else if (curChar == 36)
+                     jjstateSet[jjnewStateCnt++] = 1;
+                  if ((0x3ff000000000000L & l) != 0L)
+                  {
+                     if (kind > 6)
+                        kind = 6;
+                     jjCheckNAddStates(6, 10);
+                  }
+                  break;
+               case 0:
+                  if (curChar == 36)
+                     jjstateSet[jjnewStateCnt++] = 1;
+                  break;
+               case 1:
+                  if ((0x3ff000000000000L & l) != 0L)
+                     jjCheckNAdd(2);
+                  break;
+               case 2:
+                  if ((0x500000000000L & l) != 0L)
+                     jjstateSet[jjnewStateCnt++] = 3;
+                  break;
+               case 3:
+               case 9:
+                  if ((0x3ff000000000000L & l) == 0L)
+                     break;
+                  if (kind > 6)
+                     kind = 6;
+                  jjCheckNAddStates(11, 13);
+                  break;
+               case 4:
+                  if ((0x3ff000000000000L & l) == 0L)
+                     break;
+                  if (kind > 6)
+                     kind = 6;
+                  jjCheckNAddStates(6, 10);
+                  break;
+               case 5:
+                  if ((0x880000000000L & l) == 0L)
+                     break;
+                  if (kind > 6)
+                     kind = 6;
+                  jjCheckNAddStates(14, 17);
+                  break;
+               case 6:
+                  if ((0x3ff000000000000L & l) != 0L)
+                     jjCheckNAddTwoStates(7, 2);
+                  break;
+               case 7:
+                  if (curChar != 34)
+                     break;
+                  if (kind > 6)
+                     kind = 6;
+                  jjCheckNAddStates(11, 13);
+                  break;
+               case 8:
+                  if ((0x208000000000L & l) != 0L)
+                     jjstateSet[jjnewStateCnt++] = 9;
+                  break;
+               case 10:
+                  if ((0x100002600L & l) == 0L)
+                     break;
+                  kind = 11;
+                  jjCheckNAdd(10);
+                  break;
+               case 13:
+                  if (curChar == 59 && kind > 10)
+                     kind = 10;
+                  break;
+               case 14:
+                  if (curChar == 35)
+                     jjCheckNAdd(15);
+                  break;
+               case 15:
+                  if ((0x3ff000000000000L & l) == 0L)
+                     break;
+                  if (kind > 10)
+                     kind = 10;
+                  jjCheckNAddTwoStates(15, 13);
+                  break;
+               case 16:
+                  if (curChar == 35)
+                     jjstateSet[jjnewStateCnt++] = 17;
+                  break;
+               case 18:
+                  if ((0x3ff000000000000L & l) == 0L)
+                     break;
+                  if (kind > 10)
+                     kind = 10;
+                  jjCheckNAddTwoStates(18, 13);
+                  break;
+               case 19:
+                  if (curChar == 60)
+                     jjCheckNAddStates(0, 2);
+                  break;
+               case 22:
+                  if ((0x9fffff7affffd9ffL & l) == 0L)
+                     break;
+                  if (kind > 2)
+                     kind = 2;
+                  jjCheckNAdd(23);
+                  break;
+               case 23:
+                  if ((0x9ffffffeffffd9ffL & l) == 0L)
+                     break;
+                  if (kind > 2)
+                     kind = 2;
+                  jjCheckNAdd(23);
+                  break;
+               case 24:
+                  if (curChar == 33)
+                     jjstateSet[jjnewStateCnt++] = 25;
+                  break;
+               case 26:
+                  if ((0x9fffff7affffd9ffL & l) == 0L)
+                     break;
+                  if (kind > 3)
+                     kind = 3;
+                  jjCheckNAdd(27);
+                  break;
+               case 27:
+                  if ((0x9ffffffeffffd9ffL & l) == 0L)
+                     break;
+                  if (kind > 3)
+                     kind = 3;
+                  jjCheckNAdd(27);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else if (curChar < 128)
+      {
+         long l = 1L << (curChar & 077);
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 20:
+               case 21:
+                  if ((0x7fffffe07fffffeL & l) == 0L)
+                     break;
+                  if (kind > 2)
+                     kind = 2;
+                  jjstateSet[jjnewStateCnt++] = 22;
+                  break;
+               case 11:
+               case 4:
+                  if ((0x7fffffe07fffffeL & l) == 0L)
+                     break;
+                  if (kind > 6)
+                     kind = 6;
+                  jjCheckNAddStates(6, 10);
+                  break;
+               case 9:
+                  if ((0x7fffffe07fffffeL & l) == 0L)
+                     break;
+                  if (kind > 6)
+                     kind = 6;
+                  jjCheckNAddStates(11, 13);
+                  break;
+               case 12:
+                  if ((0x7fffffe07fffffeL & l) == 0L)
+                     break;
+                  if (kind > 10)
+                     kind = 10;
+                  jjCheckNAddTwoStates(12, 13);
+                  break;
+               case 17:
+                  if ((0x100000001000000L & l) != 0L)
+                     jjCheckNAdd(18);
+                  break;
+               case 18:
+                  if ((0x7e0000007eL & l) == 0L)
+                     break;
+                  if (kind > 10)
+                     kind = 10;
+                  jjCheckNAddTwoStates(18, 13);
+                  break;
+               case 22:
+               case 23:
+                  if (kind > 2)
+                     kind = 2;
+                  jjCheckNAdd(23);
+                  break;
+               case 25:
+                  if ((0x7fffffe07fffffeL & l) == 0L)
+                     break;
+                  if (kind > 3)
+                     kind = 3;
+                  jjstateSet[jjnewStateCnt++] = 26;
+                  break;
+               case 26:
+               case 27:
+                  if (kind > 3)
+                     kind = 3;
+                  jjCheckNAdd(27);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else
+      {
+         int i2 = (curChar & 0xff) >> 6;
+         long l2 = 1L << (curChar & 077);
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 22:
+               case 23:
+                  if ((jjbitVec0[i2] & l2) == 0L)
+                     break;
+                  if (kind > 2)
+                     kind = 2;
+                  jjCheckNAdd(23);
+                  break;
+               case 26:
+               case 27:
+                  if ((jjbitVec0[i2] & l2) == 0L)
+                     break;
+                  if (kind > 3)
+                     kind = 3;
+                  jjCheckNAdd(27);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      if (kind != 0x7fffffff)
+      {
+         jjmatchedKind = kind;
+         jjmatchedPos = curPos;
+         kind = 0x7fffffff;
+      }
+      ++curPos;
+      if ((i = jjnewStateCnt) == (startsAt = 28 - (jjnewStateCnt = startsAt)))
+         return curPos;
+      try { curChar = input_stream.readChar(); }
+      catch(java.io.IOException e) { return curPos; }
+   }
+}
+private int jjMoveStringLiteralDfa0_5()
+{
+   return jjMoveNfa_5(1, 0);
+}
+private int jjMoveNfa_5(int startState, int curPos)
+{
+   int startsAt = 0;
+   jjnewStateCnt = 2;
+   int i = 1;
+   jjstateSet[0] = startState;
+   int kind = 0x7fffffff;
+   for (;;)
+   {
+      if (++jjround == 0x7fffffff)
+         ReInitRounds();
+      if (curChar < 64)
+      {
+         long l = 1L << curChar;
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 1:
+                  if ((0xfffffffbffffffffL & l) != 0L)
+                  {
+                     if (kind > 25)
+                        kind = 25;
+                     jjCheckNAdd(0);
+                  }
+                  else if (curChar == 34)
+                  {
+                     if (kind > 26)
+                        kind = 26;
+                  }
+                  break;
+               case 0:
+                  if ((0xfffffffbffffffffL & l) == 0L)
+                     break;
+                  kind = 25;
+                  jjCheckNAdd(0);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else if (curChar < 128)
+      {
+         long l = 1L << (curChar & 077);
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 1:
+               case 0:
+                  kind = 25;
+                  jjCheckNAdd(0);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else
+      {
+         int i2 = (curChar & 0xff) >> 6;
+         long l2 = 1L << (curChar & 077);
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 1:
+               case 0:
+                  if ((jjbitVec0[i2] & l2) == 0L)
+                     break;
+                  if (kind > 25)
+                     kind = 25;
+                  jjCheckNAdd(0);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      if (kind != 0x7fffffff)
+      {
+         jjmatchedKind = kind;
+         jjmatchedPos = curPos;
+         kind = 0x7fffffff;
+      }
+      ++curPos;
+      if ((i = jjnewStateCnt) == (startsAt = 2 - (jjnewStateCnt = startsAt)))
+         return curPos;
+      try { curChar = input_stream.readChar(); }
+      catch(java.io.IOException e) { return curPos; }
+   }
+}
+private final int jjStopStringLiteralDfa_7(int pos, long active0)
+{
+   switch (pos)
+   {
+      default :
+         return -1;
+   }
+}
+private final int jjStartNfa_7(int pos, long active0)
+{
+   return jjMoveNfa_7(jjStopStringLiteralDfa_7(pos, active0), pos + 1);
+}
+private int jjMoveStringLiteralDfa0_7()
+{
+   switch(curChar)
+   {
+      case 62:
+         return jjStopAtPos(0, 30);
+      default :
+         return jjMoveNfa_7(0, 0);
+   }
+}
+private int jjMoveNfa_7(int startState, int curPos)
+{
+   int startsAt = 0;
+   jjnewStateCnt = 1;
+   int i = 1;
+   jjstateSet[0] = startState;
+   int kind = 0x7fffffff;
+   for (;;)
+   {
+      if (++jjround == 0x7fffffff)
+         ReInitRounds();
+      if (curChar < 64)
+      {
+         long l = 1L << curChar;
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+                  if ((0xbfffffffffffffffL & l) == 0L)
+                     break;
+                  kind = 29;
+                  jjstateSet[jjnewStateCnt++] = 0;
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else if (curChar < 128)
+      {
+         long l = 1L << (curChar & 077);
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+                  kind = 29;
+                  jjstateSet[jjnewStateCnt++] = 0;
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else
+      {
+         int i2 = (curChar & 0xff) >> 6;
+         long l2 = 1L << (curChar & 077);
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+                  if ((jjbitVec0[i2] & l2) == 0L)
+                     break;
+                  if (kind > 29)
+                     kind = 29;
+                  jjstateSet[jjnewStateCnt++] = 0;
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      if (kind != 0x7fffffff)
+      {
+         jjmatchedKind = kind;
+         jjmatchedPos = curPos;
+         kind = 0x7fffffff;
+      }
+      ++curPos;
+      if ((i = jjnewStateCnt) == (startsAt = 1 - (jjnewStateCnt = startsAt)))
+         return curPos;
+      try { curChar = input_stream.readChar(); }
+      catch(java.io.IOException e) { return curPos; }
+   }
+}
+private int jjMoveStringLiteralDfa0_4()
+{
+   return jjMoveNfa_4(1, 0);
+}
+private int jjMoveNfa_4(int startState, int curPos)
+{
+   int startsAt = 0;
+   jjnewStateCnt = 2;
+   int i = 1;
+   jjstateSet[0] = startState;
+   int kind = 0x7fffffff;
+   for (;;)
+   {
+      if (++jjround == 0x7fffffff)
+         ReInitRounds();
+      if (curChar < 64)
+      {
+         long l = 1L << curChar;
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 1:
+                  if ((0xffffff7fffffffffL & l) != 0L)
+                  {
+                     if (kind > 23)
+                        kind = 23;
+                     jjCheckNAdd(0);
+                  }
+                  else if (curChar == 39)
+                  {
+                     if (kind > 24)
+                        kind = 24;
+                  }
+                  break;
+               case 0:
+                  if ((0xffffff7fffffffffL & l) == 0L)
+                     break;
+                  kind = 23;
+                  jjCheckNAdd(0);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else if (curChar < 128)
+      {
+         long l = 1L << (curChar & 077);
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 1:
+               case 0:
+                  kind = 23;
+                  jjCheckNAdd(0);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else
+      {
+         int i2 = (curChar & 0xff) >> 6;
+         long l2 = 1L << (curChar & 077);
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 1:
+               case 0:
+                  if ((jjbitVec0[i2] & l2) == 0L)
+                     break;
+                  if (kind > 23)
+                     kind = 23;
+                  jjCheckNAdd(0);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      if (kind != 0x7fffffff)
+      {
+         jjmatchedKind = kind;
+         jjmatchedPos = curPos;
+         kind = 0x7fffffff;
+      }
+      ++curPos;
+      if ((i = jjnewStateCnt) == (startsAt = 2 - (jjnewStateCnt = startsAt)))
+         return curPos;
+      try { curChar = input_stream.readChar(); }
+      catch(java.io.IOException e) { return curPos; }
+   }
+}
+private final int jjStopStringLiteralDfa_3(int pos, long active0)
+{
+   switch (pos)
+   {
+      default :
+         return -1;
+   }
+}
+private final int jjStartNfa_3(int pos, long active0)
+{
+   return jjMoveNfa_3(jjStopStringLiteralDfa_3(pos, active0), pos + 1);
+}
+private int jjMoveStringLiteralDfa0_3()
+{
+   switch(curChar)
+   {
+      case 34:
+         return jjStopAtPos(0, 21);
+      case 39:
+         return jjStopAtPos(0, 20);
+      default :
+         return jjMoveNfa_3(0, 0);
+   }
+}
+private int jjMoveNfa_3(int startState, int curPos)
+{
+   int startsAt = 0;
+   jjnewStateCnt = 3;
+   int i = 1;
+   jjstateSet[0] = startState;
+   int kind = 0x7fffffff;
+   for (;;)
+   {
+      if (++jjround == 0x7fffffff)
+         ReInitRounds();
+      if (curChar < 64)
+      {
+         long l = 1L << curChar;
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+                  if ((0x9fffff7affffd9ffL & l) != 0L)
+                  {
+                     if (kind > 19)
+                        kind = 19;
+                     jjCheckNAdd(1);
+                  }
+                  else if ((0x100002600L & l) != 0L)
+                  {
+                     if (kind > 22)
+                        kind = 22;
+                     jjCheckNAdd(2);
+                  }
+                  break;
+               case 1:
+                  if ((0xbffffffeffffd9ffL & l) == 0L)
+                     break;
+                  if (kind > 19)
+                     kind = 19;
+                  jjCheckNAdd(1);
+                  break;
+               case 2:
+                  if ((0x100002600L & l) == 0L)
+                     break;
+                  kind = 22;
+                  jjCheckNAdd(2);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else if (curChar < 128)
+      {
+         long l = 1L << (curChar & 077);
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+               case 1:
+                  if (kind > 19)
+                     kind = 19;
+                  jjCheckNAdd(1);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else
+      {
+         int i2 = (curChar & 0xff) >> 6;
+         long l2 = 1L << (curChar & 077);
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+               case 1:
+                  if ((jjbitVec0[i2] & l2) == 0L)
+                     break;
+                  if (kind > 19)
+                     kind = 19;
+                  jjCheckNAdd(1);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      if (kind != 0x7fffffff)
+      {
+         jjmatchedKind = kind;
+         jjmatchedPos = curPos;
+         kind = 0x7fffffff;
+      }
+      ++curPos;
+      if ((i = jjnewStateCnt) == (startsAt = 3 - (jjnewStateCnt = startsAt)))
+         return curPos;
+      try { curChar = input_stream.readChar(); }
+      catch(java.io.IOException e) { return curPos; }
+   }
+}
+private final int jjStopStringLiteralDfa_6(int pos, long active0)
+{
+   switch (pos)
+   {
+      case 0:
+         if ((active0 & 0x10000000L) != 0L)
+         {
+            jjmatchedKind = 27;
+            return -1;
+         }
+         return -1;
+      case 1:
+         if ((active0 & 0x10000000L) != 0L)
+         {
+            if (jjmatchedPos == 0)
+            {
+               jjmatchedKind = 27;
+               jjmatchedPos = 0;
+            }
+            return -1;
+         }
+         return -1;
+      default :
+         return -1;
+   }
+}
+private final int jjStartNfa_6(int pos, long active0)
+{
+   return jjMoveNfa_6(jjStopStringLiteralDfa_6(pos, active0), pos + 1);
+}
+private int jjMoveStringLiteralDfa0_6()
+{
+   switch(curChar)
+   {
+      case 45:
+         return jjMoveStringLiteralDfa1_6(0x10000000L);
+      default :
+         return jjMoveNfa_6(1, 0);
+   }
+}
+private int jjMoveStringLiteralDfa1_6(long active0)
+{
+   try { curChar = input_stream.readChar(); }
+   catch(java.io.IOException e) {
+      jjStopStringLiteralDfa_6(0, active0);
+      return 1;
+   }
+   switch(curChar)
+   {
+      case 45:
+         return jjMoveStringLiteralDfa2_6(active0, 0x10000000L);
+      default :
+         break;
+   }
+   return jjStartNfa_6(0, active0);
+}
+private int jjMoveStringLiteralDfa2_6(long old0, long active0)
+{
+   if (((active0 &= old0)) == 0L)
+      return jjStartNfa_6(0, old0);
+   try { curChar = input_stream.readChar(); }
+   catch(java.io.IOException e) {
+      jjStopStringLiteralDfa_6(1, active0);
+      return 2;
+   }
+   switch(curChar)
+   {
+      case 62:
+         if ((active0 & 0x10000000L) != 0L)
+            return jjStopAtPos(2, 28);
+         break;
+      default :
+         break;
+   }
+   return jjStartNfa_6(1, active0);
+}
+private int jjMoveNfa_6(int startState, int curPos)
+{
+   int startsAt = 0;
+   jjnewStateCnt = 2;
+   int i = 1;
+   jjstateSet[0] = startState;
+   int kind = 0x7fffffff;
+   for (;;)
+   {
+      if (++jjround == 0x7fffffff)
+         ReInitRounds();
+      if (curChar < 64)
+      {
+         long l = 1L << curChar;
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 1:
+                  if ((0xffffdfffffffffffL & l) != 0L)
+                  {
+                     if (kind > 27)
+                        kind = 27;
+                     jjCheckNAdd(0);
+                  }
+                  else if (curChar == 45)
+                  {
+                     if (kind > 27)
+                        kind = 27;
+                  }
+                  break;
+               case 0:
+                  if ((0xffffdfffffffffffL & l) == 0L)
+                     break;
+                  kind = 27;
+                  jjCheckNAdd(0);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else if (curChar < 128)
+      {
+         long l = 1L << (curChar & 077);
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 1:
+               case 0:
+                  kind = 27;
+                  jjCheckNAdd(0);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else
+      {
+         int i2 = (curChar & 0xff) >> 6;
+         long l2 = 1L << (curChar & 077);
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 1:
+               case 0:
+                  if ((jjbitVec0[i2] & l2) == 0L)
+                     break;
+                  if (kind > 27)
+                     kind = 27;
+                  jjCheckNAdd(0);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      if (kind != 0x7fffffff)
+      {
+         jjmatchedKind = kind;
+         jjmatchedPos = curPos;
+         kind = 0x7fffffff;
+      }
+      ++curPos;
+      if ((i = jjnewStateCnt) == (startsAt = 2 - (jjnewStateCnt = startsAt)))
+         return curPos;
+      try { curChar = input_stream.readChar(); }
+      catch(java.io.IOException e) { return curPos; }
+   }
+}
+private int jjMoveStringLiteralDfa0_1()
+{
+   return jjMoveNfa_1(1, 0);
+}
+private int jjMoveNfa_1(int startState, int curPos)
+{
+   int startsAt = 0;
+   jjnewStateCnt = 12;
+   int i = 1;
+   jjstateSet[0] = startState;
+   int kind = 0x7fffffff;
+   for (;;)
+   {
+      if (++jjround == 0x7fffffff)
+         ReInitRounds();
+      if (curChar < 64)
+      {
+         long l = 1L << curChar;
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 1:
+                  if ((0xafffffffffffffffL & l) != 0L)
+                  {
+                     if (kind > 14)
+                        kind = 14;
+                     jjCheckNAdd(0);
+                  }
+                  else if ((0x5000000000000000L & l) != 0L)
+                  {
+                     if (kind > 14)
+                        kind = 14;
+                  }
+                  if (curChar == 60)
+                     jjstateSet[jjnewStateCnt++] = 10;
+                  break;
+               case 0:
+                  if ((0xafffffffffffffffL & l) == 0L)
+                     break;
+                  if (kind > 14)
+                     kind = 14;
+                  jjCheckNAdd(0);
+                  break;
+               case 3:
+                  if ((0xafffffffffffffffL & l) != 0L)
+                     jjAddStates(18, 19);
+                  break;
+               case 4:
+                  if (curChar == 62 && kind > 15)
+                     kind = 15;
+                  break;
+               case 10:
+                  if (curChar == 47)
+                     jjstateSet[jjnewStateCnt++] = 9;
+                  break;
+               case 11:
+                  if (curChar == 60)
+                     jjstateSet[jjnewStateCnt++] = 10;
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else if (curChar < 128)
+      {
+         long l = 1L << (curChar & 077);
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 1:
+               case 0:
+                  if (kind > 14)
+                     kind = 14;
+                  jjCheckNAdd(0);
+                  break;
+               case 2:
+                  if (curChar == 116)
+                     jjCheckNAddTwoStates(3, 4);
+                  break;
+               case 3:
+                  jjCheckNAddTwoStates(3, 4);
+                  break;
+               case 5:
+                  if (curChar == 112)
+                     jjstateSet[jjnewStateCnt++] = 2;
+                  break;
+               case 6:
+                  if (curChar == 105)
+                     jjstateSet[jjnewStateCnt++] = 5;
+                  break;
+               case 7:
+                  if (curChar == 114)
+                     jjstateSet[jjnewStateCnt++] = 6;
+                  break;
+               case 8:
+                  if (curChar == 99)
+                     jjstateSet[jjnewStateCnt++] = 7;
+                  break;
+               case 9:
+                  if (curChar == 115)
+                     jjstateSet[jjnewStateCnt++] = 8;
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else
+      {
+         int i2 = (curChar & 0xff) >> 6;
+         long l2 = 1L << (curChar & 077);
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 1:
+               case 0:
+                  if ((jjbitVec0[i2] & l2) == 0L)
+                     break;
+                  if (kind > 14)
+                     kind = 14;
+                  jjCheckNAdd(0);
+                  break;
+               case 3:
+                  if ((jjbitVec0[i2] & l2) != 0L)
+                     jjAddStates(18, 19);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      if (kind != 0x7fffffff)
+      {
+         jjmatchedKind = kind;
+         jjmatchedPos = curPos;
+         kind = 0x7fffffff;
+      }
+      ++curPos;
+      if ((i = jjnewStateCnt) == (startsAt = 12 - (jjnewStateCnt = startsAt)))
+         return curPos;
+      try { curChar = input_stream.readChar(); }
+      catch(java.io.IOException e) { return curPos; }
+   }
+}
+private final int jjStopStringLiteralDfa_2(int pos, long active0)
+{
+   switch (pos)
+   {
+      default :
+         return -1;
+   }
+}
+private final int jjStartNfa_2(int pos, long active0)
+{
+   return jjMoveNfa_2(jjStopStringLiteralDfa_2(pos, active0), pos + 1);
+}
+private int jjMoveStringLiteralDfa0_2()
+{
+   switch(curChar)
+   {
+      case 34:
+         return jjStopAtPos(0, 21);
+      case 39:
+         return jjStopAtPos(0, 20);
+      case 61:
+         return jjStartNfaWithStates_2(0, 17, 3);
+      default :
+         return jjMoveNfa_2(0, 0);
+   }
+}
+private int jjStartNfaWithStates_2(int pos, int kind, int state)
+{
+   jjmatchedKind = kind;
+   jjmatchedPos = pos;
+   try { curChar = input_stream.readChar(); }
+   catch(java.io.IOException e) { return pos + 1; }
+   return jjMoveNfa_2(state, pos + 1);
+}
+private int jjMoveNfa_2(int startState, int curPos)
+{
+   int startsAt = 0;
+   jjnewStateCnt = 6;
+   int i = 1;
+   jjstateSet[0] = startState;
+   int kind = 0x7fffffff;
+   for (;;)
+   {
+      if (++jjround == 0x7fffffff)
+         ReInitRounds();
+      if (curChar < 64)
+      {
+         long l = 1L << curChar;
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+                  if ((0x9fffff7affffd9ffL & l) != 0L)
+                  {
+                     if (kind > 16)
+                        kind = 16;
+                     jjCheckNAdd(1);
+                  }
+                  else if ((0x100002600L & l) != 0L)
+                  {
+                     if (kind > 22)
+                        kind = 22;
+                     jjCheckNAdd(5);
+                  }
+                  else if (curChar == 61)
+                     jjstateSet[jjnewStateCnt++] = 3;
+                  else if (curChar == 62)
+                  {
+                     if (kind > 18)
+                        kind = 18;
+                  }
+                  break;
+               case 1:
+                  if ((0x9ffffffeffffd9ffL & l) == 0L)
+                     break;
+                  if (kind > 16)
+                     kind = 16;
+                  jjCheckNAdd(1);
+                  break;
+               case 2:
+               case 3:
+                  if (curChar == 62 && kind > 18)
+                     kind = 18;
+                  break;
+               case 4:
+                  if (curChar == 61)
+                     jjstateSet[jjnewStateCnt++] = 3;
+                  break;
+               case 5:
+                  if ((0x100002600L & l) == 0L)
+                     break;
+                  kind = 22;
+                  jjCheckNAdd(5);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else if (curChar < 128)
+      {
+         long l = 1L << (curChar & 077);
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+               case 1:
+                  if (kind > 16)
+                     kind = 16;
+                  jjCheckNAdd(1);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else
+      {
+         int i2 = (curChar & 0xff) >> 6;
+         long l2 = 1L << (curChar & 077);
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+               case 1:
+                  if ((jjbitVec0[i2] & l2) == 0L)
+                     break;
+                  if (kind > 16)
+                     kind = 16;
+                  jjCheckNAdd(1);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      if (kind != 0x7fffffff)
+      {
+         jjmatchedKind = kind;
+         jjmatchedPos = curPos;
+         kind = 0x7fffffff;
+      }
+      ++curPos;
+      if ((i = jjnewStateCnt) == (startsAt = 6 - (jjnewStateCnt = startsAt)))
+         return curPos;
+      try { curChar = input_stream.readChar(); }
+      catch(java.io.IOException e) { return curPos; }
+   }
+}
+static final int[] jjnextStates = {
+   20, 21, 24, 12, 14, 16, 5, 8, 0, 4, 6, 0, 4, 6, 5, 0, 
+   4, 6, 3, 4, 
+};
+
+/** Token literal values. */
+public static final String[] jjstrLiteralImages = {
+"", "\74\163\143\162\151\160\164", null, null, "\74\41\55\55", "\74\41", null, 
+null, null, null, null, null, null, null, null, null, null, "\75", null, null, 
+"\47", "\42", null, null, null, null, null, null, "\55\55\76", null, "\76", };
+
+/** Lexer state names. */
+public static final String[] lexStateNames = {
+   "DEFAULT",
+   "WithinScript",
+   "WithinTag",
+   "AfterEquals",
+   "WithinQuote1",
+   "WithinQuote2",
+   "WithinComment1",
+   "WithinComment2",
+};
+
+/** Lex State array. */
+public static final int[] jjnewLexState = {
+   -1, 1, 2, 2, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 3, 0, 2, 4, 5, -1, -1, 2, 
+   -1, 2, -1, 0, -1, 0, 
+};
+static final long[] jjtoToken = {
+   0x7fbfec7fL, 
+};
+static final long[] jjtoSkip = {
+   0x400000L, 
+};
+protected SimpleCharStream input_stream;
+private final int[] jjrounds = new int[28];
+private final int[] jjstateSet = new int[56];
+protected char curChar;
+/** Constructor. */
+public HTMLParserTokenManager(SimpleCharStream stream){
+   if (SimpleCharStream.staticFlag)
+      throw new Error("ERROR: Cannot use a static CharStream class with a non-static lexical analyzer.");
+   input_stream = stream;
+}
+
+/** Constructor. */
+public HTMLParserTokenManager(SimpleCharStream stream, int lexState){
+   this(stream);
+   SwitchTo(lexState);
+}
+
+/** Reinitialise parser. */
+public void ReInit(SimpleCharStream stream)
+{
+   jjmatchedPos = jjnewStateCnt = 0;
+   curLexState = defaultLexState;
+   input_stream = stream;
+   ReInitRounds();
+}
+private void ReInitRounds()
+{
+   int i;
+   jjround = 0x80000001;
+   for (i = 28; i-- > 0;)
+      jjrounds[i] = 0x80000000;
+}
+
+/** Reinitialise parser. */
+public void ReInit(SimpleCharStream stream, int lexState)
+{
+   ReInit(stream);
+   SwitchTo(lexState);
+}
+
+/** Switch to specified lex state. */
+public void SwitchTo(int lexState)
+{
+   if (lexState >= 8 || lexState < 0)
+      throw new TokenMgrError("Error: Ignoring invalid lexical state : " + lexState + ". State unchanged.", TokenMgrError.INVALID_LEXICAL_STATE);
+   else
+      curLexState = lexState;
+}
+
+protected Token jjFillToken()
+{
+   final Token t;
+   final String curTokenImage;
+   final int beginLine;
+   final int endLine;
+   final int beginColumn;
+   final int endColumn;
+   String im = jjstrLiteralImages[jjmatchedKind];
+   curTokenImage = (im == null) ? input_stream.GetImage() : im;
+   beginLine = input_stream.getBeginLine();
+   beginColumn = input_stream.getBeginColumn();
+   endLine = input_stream.getEndLine();
+   endColumn = input_stream.getEndColumn();
+   t = Token.newToken(jjmatchedKind, curTokenImage);
+
+   t.beginLine = beginLine;
+   t.endLine = endLine;
+   t.beginColumn = beginColumn;
+   t.endColumn = endColumn;
+
+   return t;
+}
+
+int curLexState = 0;
+int defaultLexState = 0;
+int jjnewStateCnt;
+int jjround;
+int jjmatchedPos;
+int jjmatchedKind;
+
+/** Get the next Token. */
+public Token getNextToken() 
+{
+  Token matchedToken;
+  int curPos = 0;
+
+  EOFLoop :
+  for (;;)
+  {
+   try
+   {
+      curChar = input_stream.BeginToken();
+   }
+   catch(java.io.IOException e)
+   {
+      jjmatchedKind = 0;
+      matchedToken = jjFillToken();
+      return matchedToken;
+   }
+
+   switch(curLexState)
+   {
+     case 0:
+       jjmatchedKind = 0x7fffffff;
+       jjmatchedPos = 0;
+       curPos = jjMoveStringLiteralDfa0_0();
+       if (jjmatchedPos == 0 && jjmatchedKind > 13)
+       {
+          jjmatchedKind = 13;
+       }
+       break;
+     case 1:
+       jjmatchedKind = 0x7fffffff;
+       jjmatchedPos = 0;
+       curPos = jjMoveStringLiteralDfa0_1();
+       break;
+     case 2:
+       jjmatchedKind = 0x7fffffff;
+       jjmatchedPos = 0;
+       curPos = jjMoveStringLiteralDfa0_2();
+       break;
+     case 3:
+       jjmatchedKind = 0x7fffffff;
+       jjmatchedPos = 0;
+       curPos = jjMoveStringLiteralDfa0_3();
+       break;
+     case 4:
+       jjmatchedKind = 0x7fffffff;
+       jjmatchedPos = 0;
+       curPos = jjMoveStringLiteralDfa0_4();
+       break;
+     case 5:
+       jjmatchedKind = 0x7fffffff;
+       jjmatchedPos = 0;
+       curPos = jjMoveStringLiteralDfa0_5();
+       break;
+     case 6:
+       jjmatchedKind = 0x7fffffff;
+       jjmatchedPos = 0;
+       curPos = jjMoveStringLiteralDfa0_6();
+       break;
+     case 7:
+       jjmatchedKind = 0x7fffffff;
+       jjmatchedPos = 0;
+       curPos = jjMoveStringLiteralDfa0_7();
+       break;
+   }
+     if (jjmatchedKind != 0x7fffffff)
+     {
+        if (jjmatchedPos + 1 < curPos)
+           input_stream.backup(curPos - jjmatchedPos - 1);
+        if ((jjtoToken[jjmatchedKind >> 6] & (1L << (jjmatchedKind & 077))) != 0L)
+        {
+           matchedToken = jjFillToken();
+       if (jjnewLexState[jjmatchedKind] != -1)
+         curLexState = jjnewLexState[jjmatchedKind];
+           return matchedToken;
+        }
+        else
+        {
+         if (jjnewLexState[jjmatchedKind] != -1)
+           curLexState = jjnewLexState[jjmatchedKind];
+           continue EOFLoop;
+        }
+     }
+     int error_line = input_stream.getEndLine();
+     int error_column = input_stream.getEndColumn();
+     String error_after = null;
+     boolean EOFSeen = false;
+     try { input_stream.readChar(); input_stream.backup(1); }
+     catch (java.io.IOException e1) {
+        EOFSeen = true;
+        error_after = curPos <= 1 ? "" : input_stream.GetImage();
+        if (curChar == '\n' || curChar == '\r') {
+           error_line++;
+           error_column = 0;
+        }
+        else
+           error_column++;
+     }
+     if (!EOFSeen) {
+        input_stream.backup(1);
+        error_after = curPos <= 1 ? "" : input_stream.GetImage();
+     }
+     throw new TokenMgrError(EOFSeen, curLexState, error_line, error_column, error_after, curChar, TokenMgrError.LEXICAL_ERROR);
+  }
+}
+
+private void jjCheckNAdd(int state)
+{
+   if (jjrounds[state] != jjround)
+   {
+      jjstateSet[jjnewStateCnt++] = state;
+      jjrounds[state] = jjround;
+   }
+}
+private void jjAddStates(int start, int end)
+{
+   do {
+      jjstateSet[jjnewStateCnt++] = jjnextStates[start];
+   } while (start++ != end);
+}
+private void jjCheckNAddTwoStates(int state1, int state2)
+{
+   jjCheckNAdd(state1);
+   jjCheckNAdd(state2);
+}
+
+private void jjCheckNAddStates(int start, int end)
+{
+   do {
+      jjCheckNAdd(jjnextStates[start]);
+   } while (start++ != end);
+}
+
+}
diff --git a/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/ParseException.java b/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/ParseException.java
new file mode 100644
index 0000000..0441bf8
--- /dev/null
+++ b/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/ParseException.java
@@ -0,0 +1,198 @@
+/* Generated By:JavaCC: Do not edit this line. ParseException.java Version 4.1 */
+/* JavaCCOptions:KEEP_LINE_COL=null */
+package org.apache.lucene.demo.html;
+
+/**
+ * This exception is thrown when parse errors are encountered.
+ * You can explicitly create objects of this exception type by
+ * calling the method generateParseException in the generated
+ * parser.
+ *
+ * You can modify this class to customize your error reporting
+ * mechanisms so long as you retain the public fields.
+ */
+public class ParseException extends Exception {
+
+  /**
+   * This constructor is used by the method "generateParseException"
+   * in the generated parser.  Calling this constructor generates
+   * a new object of this type with the fields "currentToken",
+   * "expectedTokenSequences", and "tokenImage" set.  The boolean
+   * flag "specialConstructor" is also set to true to indicate that
+   * this constructor was used to create this object.
+   * This constructor calls its super class with the empty string
+   * to force the "toString" method of parent class "Throwable" to
+   * print the error message in the form:
+   *     ParseException: <result of getMessage>
+   */
+  public ParseException(Token currentTokenVal,
+                        int[][] expectedTokenSequencesVal,
+                        String[] tokenImageVal
+                       )
+  {
+    super("");
+    specialConstructor = true;
+    currentToken = currentTokenVal;
+    expectedTokenSequences = expectedTokenSequencesVal;
+    tokenImage = tokenImageVal;
+  }
+
+  /**
+   * The following constructors are for use by you for whatever
+   * purpose you can think of.  Constructing the exception in this
+   * manner makes the exception behave in the normal way - i.e., as
+   * documented in the class "Throwable".  The fields "errorToken",
+   * "expectedTokenSequences", and "tokenImage" do not contain
+   * relevant information.  The JavaCC generated code does not use
+   * these constructors.
+   */
+
+  public ParseException() {
+    super();
+    specialConstructor = false;
+  }
+
+  /** Constructor with message. */
+  public ParseException(String message) {
+    super(message);
+    specialConstructor = false;
+  }
+
+  /**
+   * This variable determines which constructor was used to create
+   * this object and thereby affects the semantics of the
+   * "getMessage" method (see below).
+   */
+  protected boolean specialConstructor;
+
+  /**
+   * This is the last token that has been consumed successfully.  If
+   * this object has been created due to a parse error, the token
+   * followng this token will (therefore) be the first error token.
+   */
+  public Token currentToken;
+
+  /**
+   * Each entry in this array is an array of integers.  Each array
+   * of integers represents a sequence of tokens (by their ordinal
+   * values) that is expected at this point of the parse.
+   */
+  public int[][] expectedTokenSequences;
+
+  /**
+   * This is a reference to the "tokenImage" array of the generated
+   * parser within which the parse error occurred.  This array is
+   * defined in the generated ...Constants interface.
+   */
+  public String[] tokenImage;
+
+  /**
+   * This method has the standard behavior when this object has been
+   * created using the standard constructors.  Otherwise, it uses
+   * "currentToken" and "expectedTokenSequences" to generate a parse
+   * error message and returns it.  If this object has been created
+   * due to a parse error, and you do not catch it (it gets thrown
+   * from the parser), then this method is called during the printing
+   * of the final stack trace, and hence the correct error message
+   * gets displayed.
+   */
+  public String getMessage() {
+    if (!specialConstructor) {
+      return super.getMessage();
+    }
+    StringBuffer expected = new StringBuffer();
+    int maxSize = 0;
+    for (int i = 0; i < expectedTokenSequences.length; i++) {
+      if (maxSize < expectedTokenSequences[i].length) {
+        maxSize = expectedTokenSequences[i].length;
+      }
+      for (int j = 0; j < expectedTokenSequences[i].length; j++) {
+        expected.append(tokenImage[expectedTokenSequences[i][j]]).append(' ');
+      }
+      if (expectedTokenSequences[i][expectedTokenSequences[i].length - 1] != 0) {
+        expected.append("...");
+      }
+      expected.append(eol).append("    ");
+    }
+    String retval = "Encountered \"";
+    Token tok = currentToken.next;
+    for (int i = 0; i < maxSize; i++) {
+      if (i != 0) retval += " ";
+      if (tok.kind == 0) {
+        retval += tokenImage[0];
+        break;
+      }
+      retval += " " + tokenImage[tok.kind];
+      retval += " \"";
+      retval += add_escapes(tok.image);
+      retval += " \"";
+      tok = tok.next;
+    }
+    retval += "\" at line " + currentToken.next.beginLine + ", column " + currentToken.next.beginColumn;
+    retval += "." + eol;
+    if (expectedTokenSequences.length == 1) {
+      retval += "Was expecting:" + eol + "    ";
+    } else {
+      retval += "Was expecting one of:" + eol + "    ";
+    }
+    retval += expected.toString();
+    return retval;
+  }
+
+  /**
+   * The end of line string for this machine.
+   */
+  protected String eol = System.getProperty("line.separator", "\n");
+
+  /**
+   * Used to convert raw characters to their escaped version
+   * when these raw version cannot be used as part of an ASCII
+   * string literal.
+   */
+  protected String add_escapes(String str) {
+      StringBuffer retval = new StringBuffer();
+      char ch;
+      for (int i = 0; i < str.length(); i++) {
+        switch (str.charAt(i))
+        {
+           case 0 :
+              continue;
+           case '\b':
+              retval.append("\\b");
+              continue;
+           case '\t':
+              retval.append("\\t");
+              continue;
+           case '\n':
+              retval.append("\\n");
+              continue;
+           case '\f':
+              retval.append("\\f");
+              continue;
+           case '\r':
+              retval.append("\\r");
+              continue;
+           case '\"':
+              retval.append("\\\"");
+              continue;
+           case '\'':
+              retval.append("\\\'");
+              continue;
+           case '\\':
+              retval.append("\\\\");
+              continue;
+           default:
+              if ((ch = str.charAt(i)) < 0x20 || ch > 0x7e) {
+                 String s = "0000" + Integer.toString(ch, 16);
+                 retval.append("\\u" + s.substring(s.length() - 4, s.length()));
+              } else {
+                 retval.append(ch);
+              }
+              continue;
+        }
+      }
+      return retval.toString();
+   }
+
+}
+/* JavaCC - OriginalChecksum=63b2008c66e199b79536447c26bee2ab (do not edit this line) */
diff --git a/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/ParserThread.java b/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/ParserThread.java
new file mode 100644
index 0000000..88fa41a
--- /dev/null
+++ b/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/ParserThread.java
@@ -0,0 +1,50 @@
+package org.apache.lucene.demo.html;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.*;
+
+class ParserThread extends Thread {
+  HTMLParser parser;
+
+  ParserThread(HTMLParser p) {
+    parser = p;
+  }
+
+  @Override
+  public void run() {				  // convert pipeOut to pipeIn
+    try {
+      try {					  // parse document to pipeOut
+        parser.HTMLDocument();
+      } catch (ParseException e) {
+        System.out.println("Parse Aborted: " + e.getMessage());
+      } catch (TokenMgrError e) {
+        System.out.println("Parse Aborted: " + e.getMessage());
+      } finally {
+        parser.pipeOut.close();
+        synchronized (parser) {
+	      parser.summary.setLength(HTMLParser.SUMMARY_LENGTH);
+	      parser.titleComplete = true;
+	      parser.notifyAll();
+	    }
+      }
+    } catch (IOException e) {
+	  e.printStackTrace();
+    }
+  }
+}
diff --git a/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/SimpleCharStream.java b/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/SimpleCharStream.java
new file mode 100644
index 0000000..fedf92d
--- /dev/null
+++ b/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/SimpleCharStream.java
@@ -0,0 +1,472 @@
+/* Generated By:JavaCC: Do not edit this line. SimpleCharStream.java Version 4.1 */
+/* JavaCCOptions:STATIC=false */
+package org.apache.lucene.demo.html;
+
+/**
+ * An implementation of interface CharStream, where the stream is assumed to
+ * contain only ASCII characters (without unicode processing).
+ */
+
+public class SimpleCharStream
+{
+/** Whether parser is static. */
+  public static final boolean staticFlag = false;
+  int bufsize;
+  int available;
+  int tokenBegin;
+/** Position in buffer. */
+  public int bufpos = -1;
+  protected int bufline[];
+  protected int bufcolumn[];
+
+  protected int column = 0;
+  protected int line = 1;
+
+  protected boolean prevCharIsCR = false;
+  protected boolean prevCharIsLF = false;
+
+  protected java.io.Reader inputStream;
+
+  protected char[] buffer;
+  protected int maxNextCharInd = 0;
+  protected int inBuf = 0;
+  protected int tabSize = 8;
+
+  protected void setTabSize(int i) { tabSize = i; }
+  protected int getTabSize(int i) { return tabSize; }
+
+
+  protected void ExpandBuff(boolean wrapAround)
+  {
+     char[] newbuffer = new char[bufsize + 2048];
+     int newbufline[] = new int[bufsize + 2048];
+     int newbufcolumn[] = new int[bufsize + 2048];
+
+     try
+     {
+        if (wrapAround)
+        {
+           System.arraycopy(buffer, tokenBegin, newbuffer, 0, bufsize - tokenBegin);
+           System.arraycopy(buffer, 0, newbuffer,
+                                             bufsize - tokenBegin, bufpos);
+           buffer = newbuffer;
+
+           System.arraycopy(bufline, tokenBegin, newbufline, 0, bufsize - tokenBegin);
+           System.arraycopy(bufline, 0, newbufline, bufsize - tokenBegin, bufpos);
+           bufline = newbufline;
+
+           System.arraycopy(bufcolumn, tokenBegin, newbufcolumn, 0, bufsize - tokenBegin);
+           System.arraycopy(bufcolumn, 0, newbufcolumn, bufsize - tokenBegin, bufpos);
+           bufcolumn = newbufcolumn;
+
+           maxNextCharInd = (bufpos += (bufsize - tokenBegin));
+        }
+        else
+        {
+           System.arraycopy(buffer, tokenBegin, newbuffer, 0, bufsize - tokenBegin);
+           buffer = newbuffer;
+
+           System.arraycopy(bufline, tokenBegin, newbufline, 0, bufsize - tokenBegin);
+           bufline = newbufline;
+
+           System.arraycopy(bufcolumn, tokenBegin, newbufcolumn, 0, bufsize - tokenBegin);
+           bufcolumn = newbufcolumn;
+
+           maxNextCharInd = (bufpos -= tokenBegin);
+        }
+     }
+     catch (Throwable t)
+     {
+        throw new Error(t.getMessage());
+     }
+
+
+     bufsize += 2048;
+     available = bufsize;
+     tokenBegin = 0;
+  }
+
+  protected void FillBuff() throws java.io.IOException
+  {
+     if (maxNextCharInd == available)
+     {
+        if (available == bufsize)
+        {
+           if (tokenBegin > 2048)
+           {
+              bufpos = maxNextCharInd = 0;
+              available = tokenBegin;
+           }
+           else if (tokenBegin < 0)
+              bufpos = maxNextCharInd = 0;
+           else
+              ExpandBuff(false);
+        }
+        else if (available > tokenBegin)
+           available = bufsize;
+        else if ((tokenBegin - available) < 2048)
+           ExpandBuff(true);
+        else
+           available = tokenBegin;
+     }
+
+     int i;
+     try {
+        if ((i = inputStream.read(buffer, maxNextCharInd,
+                                    available - maxNextCharInd)) == -1)
+        {
+           inputStream.close();
+           throw new java.io.IOException();
+        }
+        else
+           maxNextCharInd += i;
+        return;
+     }
+     catch(java.io.IOException e) {
+        --bufpos;
+        backup(0);
+        if (tokenBegin == -1)
+           tokenBegin = bufpos;
+        throw e;
+     }
+  }
+
+/** Start. */
+  public char BeginToken() throws java.io.IOException
+  {
+     tokenBegin = -1;
+     char c = readChar();
+     tokenBegin = bufpos;
+
+     return c;
+  }
+
+  protected void UpdateLineColumn(char c)
+  {
+     column++;
+
+     if (prevCharIsLF)
+     {
+        prevCharIsLF = false;
+        line += (column = 1);
+     }
+     else if (prevCharIsCR)
+     {
+        prevCharIsCR = false;
+        if (c == '\n')
+        {
+           prevCharIsLF = true;
+        }
+        else
+           line += (column = 1);
+     }
+
+     switch (c)
+     {
+        case '\r' :
+           prevCharIsCR = true;
+           break;
+        case '\n' :
+           prevCharIsLF = true;
+           break;
+        case '\t' :
+           column--;
+           column += (tabSize - (column % tabSize));
+           break;
+        default :
+           break;
+     }
+
+     bufline[bufpos] = line;
+     bufcolumn[bufpos] = column;
+  }
+
+/** Read a character. */
+  public char readChar() throws java.io.IOException
+  {
+     if (inBuf > 0)
+     {
+        --inBuf;
+
+        if (++bufpos == bufsize)
+           bufpos = 0;
+
+        return buffer[bufpos];
+     }
+
+     if (++bufpos >= maxNextCharInd)
+        FillBuff();
+
+     char c = buffer[bufpos];
+
+     UpdateLineColumn(c);
+     return c;
+  }
+
+  /**
+   * @deprecated
+   * @see #getEndColumn
+   */
+
+  public int getColumn() {
+     return bufcolumn[bufpos];
+  }
+
+  /**
+   * @deprecated
+   * @see #getEndLine
+   */
+
+  public int getLine() {
+     return bufline[bufpos];
+  }
+
+  /** Get token end column number. */
+  public int getEndColumn() {
+     return bufcolumn[bufpos];
+  }
+
+  /** Get token end line number. */
+  public int getEndLine() {
+     return bufline[bufpos];
+  }
+
+  /** Get token beginning column number. */
+  public int getBeginColumn() {
+     return bufcolumn[tokenBegin];
+  }
+
+  /** Get token beginning line number. */
+  public int getBeginLine() {
+     return bufline[tokenBegin];
+  }
+
+/** Backup a number of characters. */
+  public void backup(int amount) {
+
+    inBuf += amount;
+    if ((bufpos -= amount) < 0)
+       bufpos += bufsize;
+  }
+
+  /** Constructor. */
+  public SimpleCharStream(java.io.Reader dstream, int startline,
+  int startcolumn, int buffersize)
+  {
+    inputStream = dstream;
+    line = startline;
+    column = startcolumn - 1;
+
+    available = bufsize = buffersize;
+    buffer = new char[buffersize];
+    bufline = new int[buffersize];
+    bufcolumn = new int[buffersize];
+  }
+
+  /** Constructor. */
+  public SimpleCharStream(java.io.Reader dstream, int startline,
+                          int startcolumn)
+  {
+     this(dstream, startline, startcolumn, 4096);
+  }
+
+  /** Constructor. */
+  public SimpleCharStream(java.io.Reader dstream)
+  {
+     this(dstream, 1, 1, 4096);
+  }
+
+  /** Reinitialise. */
+  public void ReInit(java.io.Reader dstream, int startline,
+  int startcolumn, int buffersize)
+  {
+    inputStream = dstream;
+    line = startline;
+    column = startcolumn - 1;
+
+    if (buffer == null || buffersize != buffer.length)
+    {
+      available = bufsize = buffersize;
+      buffer = new char[buffersize];
+      bufline = new int[buffersize];
+      bufcolumn = new int[buffersize];
+    }
+    prevCharIsLF = prevCharIsCR = false;
+    tokenBegin = inBuf = maxNextCharInd = 0;
+    bufpos = -1;
+  }
+
+  /** Reinitialise. */
+  public void ReInit(java.io.Reader dstream, int startline,
+                     int startcolumn)
+  {
+     ReInit(dstream, startline, startcolumn, 4096);
+  }
+
+  /** Reinitialise. */
+  public void ReInit(java.io.Reader dstream)
+  {
+     ReInit(dstream, 1, 1, 4096);
+  }
+  /** Constructor. */
+  public SimpleCharStream(java.io.InputStream dstream, String encoding, int startline,
+  int startcolumn, int buffersize) throws java.io.UnsupportedEncodingException
+  {
+     this(encoding == null ? new java.io.InputStreamReader(dstream) : new java.io.InputStreamReader(dstream, encoding), startline, startcolumn, buffersize);
+  }
+
+  /** Constructor. */
+  public SimpleCharStream(java.io.InputStream dstream, int startline,
+  int startcolumn, int buffersize)
+  {
+     this(new java.io.InputStreamReader(dstream), startline, startcolumn, buffersize);
+  }
+
+  /** Constructor. */
+  public SimpleCharStream(java.io.InputStream dstream, String encoding, int startline,
+                          int startcolumn) throws java.io.UnsupportedEncodingException
+  {
+     this(dstream, encoding, startline, startcolumn, 4096);
+  }
+
+  /** Constructor. */
+  public SimpleCharStream(java.io.InputStream dstream, int startline,
+                          int startcolumn)
+  {
+     this(dstream, startline, startcolumn, 4096);
+  }
+
+  /** Constructor. */
+  public SimpleCharStream(java.io.InputStream dstream, String encoding) throws java.io.UnsupportedEncodingException
+  {
+     this(dstream, encoding, 1, 1, 4096);
+  }
+
+  /** Constructor. */
+  public SimpleCharStream(java.io.InputStream dstream)
+  {
+     this(dstream, 1, 1, 4096);
+  }
+
+  /** Reinitialise. */
+  public void ReInit(java.io.InputStream dstream, String encoding, int startline,
+                          int startcolumn, int buffersize) throws java.io.UnsupportedEncodingException
+  {
+     ReInit(encoding == null ? new java.io.InputStreamReader(dstream) : new java.io.InputStreamReader(dstream, encoding), startline, startcolumn, buffersize);
+  }
+
+  /** Reinitialise. */
+  public void ReInit(java.io.InputStream dstream, int startline,
+                          int startcolumn, int buffersize)
+  {
+     ReInit(new java.io.InputStreamReader(dstream), startline, startcolumn, buffersize);
+  }
+
+  /** Reinitialise. */
+  public void ReInit(java.io.InputStream dstream, String encoding) throws java.io.UnsupportedEncodingException
+  {
+     ReInit(dstream, encoding, 1, 1, 4096);
+  }
+
+  /** Reinitialise. */
+  public void ReInit(java.io.InputStream dstream)
+  {
+     ReInit(dstream, 1, 1, 4096);
+  }
+  /** Reinitialise. */
+  public void ReInit(java.io.InputStream dstream, String encoding, int startline,
+                     int startcolumn) throws java.io.UnsupportedEncodingException
+  {
+     ReInit(dstream, encoding, startline, startcolumn, 4096);
+  }
+  /** Reinitialise. */
+  public void ReInit(java.io.InputStream dstream, int startline,
+                     int startcolumn)
+  {
+     ReInit(dstream, startline, startcolumn, 4096);
+  }
+  /** Get token literal value. */
+  public String GetImage()
+  {
+     if (bufpos >= tokenBegin)
+        return new String(buffer, tokenBegin, bufpos - tokenBegin + 1);
+     else
+        return new String(buffer, tokenBegin, bufsize - tokenBegin) +
+                              new String(buffer, 0, bufpos + 1);
+  }
+
+  /** Get the suffix. */
+  public char[] GetSuffix(int len)
+  {
+     char[] ret = new char[len];
+
+     if ((bufpos + 1) >= len)
+        System.arraycopy(buffer, bufpos - len + 1, ret, 0, len);
+     else
+     {
+        System.arraycopy(buffer, bufsize - (len - bufpos - 1), ret, 0,
+                                                          len - bufpos - 1);
+        System.arraycopy(buffer, 0, ret, len - bufpos - 1, bufpos + 1);
+     }
+
+     return ret;
+  }
+
+  /** Reset buffer when finished. */
+  public void Done()
+  {
+     buffer = null;
+     bufline = null;
+     bufcolumn = null;
+  }
+
+  /**
+   * Method to adjust line and column numbers for the start of a token.
+   */
+  public void adjustBeginLineColumn(int newLine, int newCol)
+  {
+     int start = tokenBegin;
+     int len;
+
+     if (bufpos >= tokenBegin)
+     {
+        len = bufpos - tokenBegin + inBuf + 1;
+     }
+     else
+     {
+        len = bufsize - tokenBegin + bufpos + 1 + inBuf;
+     }
+
+     int i = 0, j = 0, k = 0;
+     int nextColDiff = 0, columnDiff = 0;
+
+     while (i < len &&
+            bufline[j = start % bufsize] == bufline[k = ++start % bufsize])
+     {
+        bufline[j] = newLine;
+        nextColDiff = columnDiff + bufcolumn[k] - bufcolumn[j];
+        bufcolumn[j] = newCol + columnDiff;
+        columnDiff = nextColDiff;
+        i++;
+     }
+
+     if (i < len)
+     {
+        bufline[j] = newLine++;
+        bufcolumn[j] = newCol + columnDiff;
+
+        while (i++ < len)
+        {
+           if (bufline[j = start % bufsize] != bufline[++start % bufsize])
+              bufline[j] = newLine++;
+           else
+              bufline[j] = newLine;
+        }
+     }
+
+     line = bufline[j];
+     column = bufcolumn[j];
+  }
+
+}
+/* JavaCC - OriginalChecksum=7393ed4ac2709e2de22d164f9db78b65 (do not edit this line) */
diff --git a/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/Tags.java b/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/Tags.java
new file mode 100644
index 0000000..a9215f2
--- /dev/null
+++ b/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/Tags.java
@@ -0,0 +1,64 @@
+package org.apache.lucene.demo.html;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Collections;
+import java.util.HashSet;
+import java.util.Set;
+
+
+public final class Tags {
+
+  /**
+   * contains all tags for which whitespaces have to be inserted for proper tokenization
+   */
+  public static final Set<String> WS_ELEMS = Collections.synchronizedSet(new HashSet<String>());
+
+  static{
+    WS_ELEMS.add("<hr");
+    WS_ELEMS.add("<hr/");  // note that "<hr />" does not need to be listed explicitly
+    WS_ELEMS.add("<br");
+    WS_ELEMS.add("<br/");
+    WS_ELEMS.add("<p");
+    WS_ELEMS.add("</p");
+    WS_ELEMS.add("<div");
+    WS_ELEMS.add("</div");
+    WS_ELEMS.add("<td");
+    WS_ELEMS.add("</td");
+    WS_ELEMS.add("<li");
+    WS_ELEMS.add("</li");
+    WS_ELEMS.add("<q");
+    WS_ELEMS.add("</q");
+    WS_ELEMS.add("<blockquote");
+    WS_ELEMS.add("</blockquote");
+    WS_ELEMS.add("<dt");
+    WS_ELEMS.add("</dt");
+    WS_ELEMS.add("<h1");
+    WS_ELEMS.add("</h1");
+    WS_ELEMS.add("<h2");
+    WS_ELEMS.add("</h2");
+    WS_ELEMS.add("<h3");
+    WS_ELEMS.add("</h3");
+    WS_ELEMS.add("<h4");
+    WS_ELEMS.add("</h4");
+    WS_ELEMS.add("<h5");
+    WS_ELEMS.add("</h5");
+    WS_ELEMS.add("<h6");
+    WS_ELEMS.add("</h6");
+  }
+}
diff --git a/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/Test.java b/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/Test.java
new file mode 100644
index 0000000..224ae5e
--- /dev/null
+++ b/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/Test.java
@@ -0,0 +1,51 @@
+package org.apache.lucene.demo.html;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.*;
+
+class Test {
+  public static void main(String[] argv) throws IOException, InterruptedException {
+    if ("-dir".equals(argv[0])) {
+      String[] files = new File(argv[1]).list();
+      java.util.Arrays.sort(files);
+      for (int i = 0; i < files.length; i++) {
+	System.err.println(files[i]);
+	File file = new File(argv[1], files[i]);
+	parse(file);
+      }
+    } else
+      parse(new File(argv[0]));
+  }
+
+  public static void parse(File file) throws IOException, InterruptedException {
+    FileInputStream fis = null;
+    try {
+      fis = new FileInputStream(file);
+      HTMLParser parser = new HTMLParser(fis);
+      System.out.println("Title: " + Entities.encode(parser.getTitle()));
+      System.out.println("Summary: " + Entities.encode(parser.getSummary()));
+      System.out.println("Content:");
+      LineNumberReader reader = new LineNumberReader(parser.getReader());
+      for (String l = reader.readLine(); l != null; l = reader.readLine())
+        System.out.println(l);
+    } finally {
+      if (fis != null) fis.close();
+    }
+  }
+}
diff --git a/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/Token.java b/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/Token.java
new file mode 100644
index 0000000..c38a7bf
--- /dev/null
+++ b/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/Token.java
@@ -0,0 +1,124 @@
+/* Generated By:JavaCC: Do not edit this line. Token.java Version 4.1 */
+/* JavaCCOptions:TOKEN_EXTENDS=,KEEP_LINE_COL=null */
+package org.apache.lucene.demo.html;
+
+/**
+ * Describes the input token stream.
+ */
+
+public class Token {
+
+  /**
+   * An integer that describes the kind of this token.  This numbering
+   * system is determined by JavaCCParser, and a table of these numbers is
+   * stored in the file ...Constants.java.
+   */
+  public int kind;
+
+  /** The line number of the first character of this Token. */
+  public int beginLine;
+  /** The column number of the first character of this Token. */
+  public int beginColumn;
+  /** The line number of the last character of this Token. */
+  public int endLine;
+  /** The column number of the last character of this Token. */
+  public int endColumn;
+
+  /**
+   * The string image of the token.
+   */
+  public String image;
+
+  /**
+   * A reference to the next regular (non-special) token from the input
+   * stream.  If this is the last token from the input stream, or if the
+   * token manager has not read tokens beyond this one, this field is
+   * set to null.  This is true only if this token is also a regular
+   * token.  Otherwise, see below for a description of the contents of
+   * this field.
+   */
+  public Token next;
+
+  /**
+   * This field is used to access special tokens that occur prior to this
+   * token, but after the immediately preceding regular (non-special) token.
+   * If there are no such special tokens, this field is set to null.
+   * When there are more than one such special token, this field refers
+   * to the last of these special tokens, which in turn refers to the next
+   * previous special token through its specialToken field, and so on
+   * until the first special token (whose specialToken field is null).
+   * The next fields of special tokens refer to other special tokens that
+   * immediately follow it (without an intervening regular token).  If there
+   * is no such token, this field is null.
+   */
+  public Token specialToken;
+
+  /**
+   * An optional attribute value of the Token.
+   * Tokens which are not used as syntactic sugar will often contain
+   * meaningful values that will be used later on by the compiler or
+   * interpreter. This attribute value is often different from the image.
+   * Any subclass of Token that actually wants to return a non-null value can
+   * override this method as appropriate.
+   */
+  public Object getValue() {
+    return null;
+  }
+
+  /**
+   * No-argument constructor
+   */
+  public Token() {}
+
+  /**
+   * Constructs a new token for the specified Image.
+   */
+  public Token(int kind)
+  {
+     this(kind, null);
+  }
+
+  /**
+   * Constructs a new token for the specified Image and Kind.
+   */
+  public Token(int kind, String image)
+  {
+     this.kind = kind;
+     this.image = image;
+  }
+
+  /**
+   * Returns the image.
+   */
+  public String toString()
+  {
+     return image;
+  }
+
+  /**
+   * Returns a new Token object, by default. However, if you want, you
+   * can create and return subclass objects based on the value of ofKind.
+   * Simply add the cases to the switch for all those special cases.
+   * For example, if you have a subclass of Token called IDToken that
+   * you want to create if ofKind is ID, simply add something like :
+   *
+   *    case MyParserConstants.ID : return new IDToken(ofKind, image);
+   *
+   * to the following switch statement. Then you can cast matchedToken
+   * variable to the appropriate type and use sit in your lexical actions.
+   */
+  public static Token newToken(int ofKind, String image)
+  {
+     switch(ofKind)
+     {
+       default : return new Token(ofKind, image);
+     }
+  }
+
+  public static Token newToken(int ofKind)
+  {
+     return newToken(ofKind, null);
+  }
+
+}
+/* JavaCC - OriginalChecksum=7bf8bdbb1c45bccd8162cdd48316d5e0 (do not edit this line) */
diff --git a/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/TokenMgrError.java b/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/TokenMgrError.java
new file mode 100644
index 0000000..836d1f7
--- /dev/null
+++ b/lucene/contrib/demo/src/java/org/apache/lucene/demo/html/TokenMgrError.java
@@ -0,0 +1,141 @@
+/* Generated By:JavaCC: Do not edit this line. TokenMgrError.java Version 4.1 */
+/* JavaCCOptions: */
+package org.apache.lucene.demo.html;
+
+/** Token Manager Error. */
+@SuppressWarnings("serial")
+public class TokenMgrError extends Error
+{
+
+   /*
+    * Ordinals for various reasons why an Error of this type can be thrown.
+    */
+
+   /**
+    * Lexical error occurred.
+    */
+   static final int LEXICAL_ERROR = 0;
+
+   /**
+    * An attempt was made to create a second instance of a static token manager.
+    */
+   static final int STATIC_LEXER_ERROR = 1;
+
+   /**
+    * Tried to change to an invalid lexical state.
+    */
+   static final int INVALID_LEXICAL_STATE = 2;
+
+   /**
+    * Detected (and bailed out of) an infinite loop in the token manager.
+    */
+   static final int LOOP_DETECTED = 3;
+
+   /**
+    * Indicates the reason why the exception is thrown. It will have
+    * one of the above 4 values.
+    */
+   int errorCode;
+
+   /**
+    * Replaces unprintable characters by their escaped (or unicode escaped)
+    * equivalents in the given string
+    */
+   protected static final String addEscapes(String str) {
+      StringBuffer retval = new StringBuffer();
+      char ch;
+      for (int i = 0; i < str.length(); i++) {
+        switch (str.charAt(i))
+        {
+           case 0 :
+              continue;
+           case '\b':
+              retval.append("\\b");
+              continue;
+           case '\t':
+              retval.append("\\t");
+              continue;
+           case '\n':
+              retval.append("\\n");
+              continue;
+           case '\f':
+              retval.append("\\f");
+              continue;
+           case '\r':
+              retval.append("\\r");
+              continue;
+           case '\"':
+              retval.append("\\\"");
+              continue;
+           case '\'':
+              retval.append("\\\'");
+              continue;
+           case '\\':
+              retval.append("\\\\");
+              continue;
+           default:
+              if ((ch = str.charAt(i)) < 0x20 || ch > 0x7e) {
+                 String s = "0000" + Integer.toString(ch, 16);
+                 retval.append("\\u" + s.substring(s.length() - 4, s.length()));
+              } else {
+                 retval.append(ch);
+              }
+              continue;
+        }
+      }
+      return retval.toString();
+   }
+
+   /**
+    * Returns a detailed message for the Error when it is thrown by the
+    * token manager to indicate a lexical error.
+    * Parameters :
+    *    EOFSeen     : indicates if EOF caused the lexical error
+    *    curLexState : lexical state in which this error occurred
+    *    errorLine   : line number when the error occurred
+    *    errorColumn : column number when the error occurred
+    *    errorAfter  : prefix that was seen before this error occurred
+    *    curchar     : the offending character
+    * Note: You can customize the lexical error message by modifying this method.
+    */
+   protected static String LexicalError(boolean EOFSeen, int lexState, int errorLine, int errorColumn, String errorAfter, char curChar) {
+      return("Lexical error at line " +
+           errorLine + ", column " +
+           errorColumn + ".  Encountered: " +
+           (EOFSeen ? "<EOF> " : ("\"" + addEscapes(String.valueOf(curChar)) + "\"") + " (" + (int)curChar + "), ") +
+           "after : \"" + addEscapes(errorAfter) + "\"");
+   }
+
+   /**
+    * You can also modify the body of this method to customize your error messages.
+    * For example, cases like LOOP_DETECTED and INVALID_LEXICAL_STATE are not
+    * of end-users concern, so you can return something like :
+    *
+    *     "Internal Error : Please file a bug report .... "
+    *
+    * from this method for such cases in the release version of your parser.
+    */
+   public String getMessage() {
+      return super.getMessage();
+   }
+
+   /*
+    * Constructors of various flavors follow.
+    */
+
+   /** No arg constructor. */
+   public TokenMgrError() {
+   }
+
+   /** Constructor with message and reason. */
+   public TokenMgrError(String message, int reason) {
+      super(message);
+      errorCode = reason;
+   }
+
+   /** Full Constructor. */
+   public TokenMgrError(boolean EOFSeen, int lexState, int errorLine, int errorColumn, String errorAfter, char curChar, int reason) {
+      this(LexicalError(EOFSeen, lexState, errorLine, errorColumn, errorAfter, curChar), reason);
+   }
+}
+/* JavaCC - OriginalChecksum=5ffb7e46d5ae93d8d59e6f4ae7eb36d1 (do not edit this line) */
diff --git a/lucene/contrib/demo/src/jsp/README.txt b/lucene/contrib/demo/src/jsp/README.txt
new file mode 100644
index 0000000..31ae063
--- /dev/null
+++ b/lucene/contrib/demo/src/jsp/README.txt
@@ -0,0 +1,8 @@
+To build the Apache Lucene web app demo just run 
+"ant war-demo" from the Apache Lucene Installation
+directory (follow the master instructions in 
+BUILD.txt).  If you have questions please post 
+them to the Apache Lucene mailing lists.  To 
+actually figure this out you really need to 
+read the Lucene "Getting Started" guide provided
+with the doc build ("ant docs").
diff --git a/lucene/contrib/demo/src/jsp/WEB-INF/web.xml b/lucene/contrib/demo/src/jsp/WEB-INF/web.xml
new file mode 100755
index 0000000..d6740d2
--- /dev/null
+++ b/lucene/contrib/demo/src/jsp/WEB-INF/web.xml
@@ -0,0 +1,10 @@
+<?xml version="1.0" encoding="ISO-8859-1"?>
+
+<!DOCTYPE web-app
+    PUBLIC "-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN"
+    "http://java.sun.com/dtd/web-app_2_3.dtd">
+
+<web-app>
+
+
+</web-app>
diff --git a/lucene/contrib/demo/src/jsp/configuration.jsp b/lucene/contrib/demo/src/jsp/configuration.jsp
new file mode 100644
index 0000000..eb0bcfe
--- /dev/null
+++ b/lucene/contrib/demo/src/jsp/configuration.jsp
@@ -0,0 +1,22 @@
+<!--
+    Licensed to the Apache Software Foundation (ASF) under one or more
+    contributor license agreements.  See the NOTICE file distributed with
+    this work for additional information regarding copyright ownership.
+    The ASF licenses this file to You under the Apache License, Version 2.0
+    the "License"); you may not use this file except in compliance with
+    the License.  You may obtain a copy of the License at
+ 
+        http://www.apache.org/licenses/LICENSE-2.0
+ 
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an "AS IS" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
+ -->
+<%
+String appTitle = "Apache Lucene Example - Intranet Server Search Application";
+/* make sure you point the below string to the index you created with IndexHTML */
+String indexLocation = "/opt/lucene/index";
+String appfooter = "Apache Lucene Template WebApp 1.0";
+%>
diff --git a/lucene/contrib/demo/src/jsp/footer.jsp b/lucene/contrib/demo/src/jsp/footer.jsp
new file mode 100644
index 0000000..44127a0
--- /dev/null
+++ b/lucene/contrib/demo/src/jsp/footer.jsp
@@ -0,0 +1,21 @@
+<!--
+    Licensed to the Apache Software Foundation (ASF) under one or more
+    contributor license agreements.  See the NOTICE file distributed with
+    this work for additional information regarding copyright ownership.
+    The ASF licenses this file to You under the Apache License, Version 2.0
+    the "License"); you may not use this file except in compliance with
+    the License.  You may obtain a copy of the License at
+ 
+        http://www.apache.org/licenses/LICENSE-2.0
+ 
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an "AS IS" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
+ -->
+<p align="center">
+	<%=appfooter%>
+</p>
+</body>
+</html>
diff --git a/lucene/contrib/demo/src/jsp/header.jsp b/lucene/contrib/demo/src/jsp/header.jsp
new file mode 100644
index 0000000..3806d7a
--- /dev/null
+++ b/lucene/contrib/demo/src/jsp/header.jsp
@@ -0,0 +1,26 @@
+<!--
+    Licensed to the Apache Software Foundation (ASF) under one or more
+    contributor license agreements.  See the NOTICE file distributed with
+    this work for additional information regarding copyright ownership.
+    The ASF licenses this file to You under the Apache License, Version 2.0
+    the "License"); you may not use this file except in compliance with
+    the License.  You may obtain a copy of the License at
+ 
+        http://www.apache.org/licenses/LICENSE-2.0
+ 
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an "AS IS" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
+ -->
+<%@include file="configuration.jsp"%>
+<html>
+<head>
+	<title><%=appTitle%></title>
+</head>
+<body>
+
+<p align="center">
+Welcome to the Lucene Template application. (This is the header)
+</p>
diff --git a/lucene/contrib/demo/src/jsp/index.jsp b/lucene/contrib/demo/src/jsp/index.jsp
new file mode 100755
index 0000000..5e63721
--- /dev/null
+++ b/lucene/contrib/demo/src/jsp/index.jsp
@@ -0,0 +1,29 @@
+<!--
+    Licensed to the Apache Software Foundation (ASF) under one or more
+    contributor license agreements.  See the NOTICE file distributed with
+    this work for additional information regarding copyright ownership.
+    The ASF licenses this file to You under the Apache License, Version 2.0
+    the "License"); you may not use this file except in compliance with
+    the License.  You may obtain a copy of the License at
+ 
+        http://www.apache.org/licenses/LICENSE-2.0
+ 
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an "AS IS" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
+ -->
+<%@include file="header.jsp"%>
+<center> 
+	<form name="search" action="results.jsp" method="get">
+		<p>
+			<input name="query" size="44"/>&nbsp;Search Criteria
+		</p>
+		<p>
+			<input name="maxresults" size="4" value="100"/>&nbsp;Results Per Page&nbsp;
+			<input type="submit" value="Search"/>
+		</p>
+        </form>
+</center>
+<%@include file="footer.jsp"%>
diff --git a/lucene/contrib/demo/src/jsp/results.jsp b/lucene/contrib/demo/src/jsp/results.jsp
new file mode 100755
index 0000000..90cc020
--- /dev/null
+++ b/lucene/contrib/demo/src/jsp/results.jsp
@@ -0,0 +1,179 @@
+<!--
+    Licensed to the Apache Software Foundation (ASF) under one or more
+    contributor license agreements.  See the NOTICE file distributed with
+    this work for additional information regarding copyright ownership.
+    The ASF licenses this file to You under the Apache License, Version 2.0
+    the "License"); you may not use this file except in compliance with
+    the License.  You may obtain a copy of the License at
+ 
+        http://www.apache.org/licenses/LICENSE-2.0
+ 
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an "AS IS" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
+ -->
+<%@ page import = "  javax.servlet.*, javax.servlet.http.*, java.io.*, org.apache.lucene.analysis.*, org.apache.lucene.analysis.standard.StandardAnalyzer, org.apache.lucene.document.*, org.apache.lucene.index.*, org.apache.lucene.store.*, org.apache.lucene.search.*, org.apache.lucene.queryParser.*, org.apache.lucene.demo.*, org.apache.lucene.demo.html.Entities, java.net.URLEncoder, org.apache.lucene.util.Version" %>
+
+<%
+/*
+
+        This jsp page is deliberatly written in the horrible java directly embedded 
+        in the page style for an easy and concise demonstration of Lucene.
+        Due note...if you write pages that look like this...sooner or later
+        you'll have a maintenance nightmare.  If you use jsps...use taglibs
+        and beans!  That being said, this should be acceptable for a small
+        page demonstrating how one uses Lucene in a web app. 
+
+        This is also deliberately overcommented. ;-)
+
+*/
+%>
+<%!
+public String escapeHTML(String s) {
+  s = s.replaceAll("&", "&amp;");
+  s = s.replaceAll("<", "&lt;");
+  s = s.replaceAll(">", "&gt;");
+  s = s.replaceAll("\"", "&quot;");
+  s = s.replaceAll("'", "&apos;");
+  return s;
+}
+%>
+<%@include file="header.jsp"%>
+<%
+        boolean error = false;                  //used to control flow for error messages
+        String indexName = indexLocation;       //local copy of the configuration variable
+        IndexSearcher searcher = null;          //the searcher used to open/search the index
+        Query query = null;                     //the Query created by the QueryParser
+        TopDocs hits = null;                       //the search results
+        int startindex = 0;                     //the first index displayed on this page
+        int maxpage    = 50;                    //the maximum items displayed on this page
+        String queryString = null;              //the query entered in the previous page
+        String startVal    = null;              //string version of startindex
+        String maxresults  = null;              //string version of maxpage
+        int thispage = 0;                       //used for the for/next either maxpage or
+                                                //hits.totalHits - startindex - whichever is
+                                                //less
+
+        try {
+          IndexReader reader = IndexReader.open(FSDirectory.open(new File(indexName)), true); // only searching, so read-only=true
+          searcher = new IndexSearcher(reader);         //create an indexSearcher for our page
+                                                        //NOTE: this operation is slow for large
+                                                        //indices (much slower than the search itself)
+                                                        //so you might want to keep an IndexSearcher 
+                                                        //open
+                                                        
+        } catch (Exception e) {                         //any error that happens is probably due
+                                                        //to a permission problem or non-existant
+                                                        //or otherwise corrupt index
+%>
+                <p>ERROR opening the Index - contact sysadmin!</p>
+                <p>Error message: <%=escapeHTML(e.getMessage())%></p>   
+<%                error = true;                                  //don't do anything up to the footer
+        }
+%>
+<%
+       if (error == false) {                                           //did we open the index?
+                queryString = request.getParameter("query");           //get the search criteria
+                startVal    = request.getParameter("startat");         //get the start index
+                maxresults  = request.getParameter("maxresults");      //get max results per page
+                try {
+                        maxpage    = Integer.parseInt(maxresults);    //parse the max results first
+                        startindex = Integer.parseInt(startVal);      //then the start index  
+                } catch (Exception e) { } //we don't care if something happens we'll just start at 0
+                                          //or end at 50
+
+                
+
+                if (queryString == null)
+                        throw new ServletException("no query "+       //if you don't have a query then
+                                                   "specified");      //you probably played on the 
+                                                                      //query string so you get the 
+                                                                      //treatment
+
+                Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_CURRENT);           //construct our usual analyzer
+                try {
+                        QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, "contents", analyzer);
+                        query = qp.parse(queryString); //parse the 
+                } catch (ParseException e) {                          //query and construct the Query
+                                                                      //object
+                                                                      //if it's just "operator error"
+                                                                      //send them a nice error HTML
+                                                                      
+%>
+                        <p>Error while parsing query: <%=escapeHTML(e.getMessage())%></p>
+<%
+                        error = true;                                 //don't bother with the rest of
+                                                                      //the page
+                }
+        }
+%>
+<%
+        if (error == false && searcher != null) {                     // if we've had no errors
+                                                                      // searcher != null was to handle
+                                                                      // a weird compilation bug 
+                thispage = maxpage;                                   // default last element to maxpage
+                hits = searcher.search(query, maxpage + startindex);  // run the query 
+                if (hits.totalHits == 0) {                             // if we got no results tell the user
+%>
+                <p> I'm sorry I couldn't find what you were looking for. </p>
+<%
+                error = true;                                        // don't bother with the rest of the
+                                                                     // page
+                }
+        }
+
+        if (error == false && searcher != null) {                   
+%>
+                <table>
+                <tr>
+                        <td>Document</td>
+                        <td>Summary</td>
+                </tr>
+<%
+                if ((startindex + maxpage) > hits.totalHits) {
+                        thispage = hits.totalHits - startindex;      // set the max index to maxpage or last
+                }                                                   // actual search result whichever is less
+
+                for (int i = startindex; i < (thispage + startindex); i++) {  // for each element
+%>
+                <tr>
+<%
+                        Document doc = searcher.doc(hits.scoreDocs[i].doc);                    //get the next document 
+                        String doctitle = doc.get("title");            //get its title
+                        String url = doc.get("path");                  //get its path field
+                        if (url != null && url.startsWith("../webapps/")) { // strip off ../webapps prefix if present
+                                url = url.substring(10);
+                        }
+                        if ((doctitle == null) || doctitle.equals("")) //use the path if it has no title
+                                doctitle = url;
+                                                                       //then output!
+%>
+                        <td><a href="<%=url%>"><%=doctitle%></a></td>
+                        <td><%=doc.get("summary")%></td>
+                </tr>
+<%
+                }
+%>
+<%                if ( (startindex + maxpage) < hits.totalHits) {   //if there are more results...display 
+                                                                   //the more link
+
+                        String moreurl="results.jsp?query=" + 
+                                       URLEncoder.encode(queryString) +  //construct the "more" link
+                                       "&amp;maxresults=" + maxpage + 
+                                       "&amp;startat=" + (startindex + maxpage);
+%>
+                <tr>
+                        <td></td><td><a href="<%=moreurl%>">More Results>></a></td>
+                </tr>
+<%
+                }
+%>
+                </table>
+
+<%       }                                            //then include our footer.
+         if (searcher != null)
+                searcher.close();
+%>
+<%@include file="footer.jsp"%>        
diff --git a/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldTermStack.java b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldTermStack.java
index 86ca670..1282d66 100644
--- a/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldTermStack.java
+++ b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldTermStack.java
@@ -21,24 +21,10 @@ import java.util.Collections;
 import java.util.LinkedList;
 import java.util.Set;
 
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.Field.Index;
-import org.apache.lucene.document.Field.Store;
-import org.apache.lucene.document.Field.TermVector;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.TermFreqVector;
 import org.apache.lucene.index.TermPositionVector;
 import org.apache.lucene.index.TermVectorOffsetInfo;
-import org.apache.lucene.queryParser.QueryParser;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.util.Version;
 
 /**
  * <code>FieldTermStack</code> is a stack that keeps query terms in the specified field
@@ -49,24 +35,24 @@ public class FieldTermStack {
   private final String fieldName;
   LinkedList<TermInfo> termList = new LinkedList<TermInfo>();
   
-  public static void main( String[] args ) throws Exception {
-    Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_CURRENT);
-    QueryParser parser = new QueryParser(Version.LUCENE_CURRENT,  "f", analyzer );
-    Query query = parser.parse( "a x:b" );
-    FieldQuery fieldQuery = new FieldQuery( query, true, false );
+  //public static void main( String[] args ) throws Exception {
+  //  Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_CURRENT);
+  //  QueryParser parser = new QueryParser(Version.LUCENE_CURRENT,  "f", analyzer );
+  //  Query query = parser.parse( "a x:b" );
+  //  FieldQuery fieldQuery = new FieldQuery( query, true, false );
     
-    Directory dir = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(Version.LUCENE_CURRENT, analyzer));
-    Document doc = new Document();
-    doc.add( new Field( "f", "a a a b b c a b b c d e f", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS ) );
-    doc.add( new Field( "f", "b a b a f", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS ) );
-    writer.addDocument( doc );
-    writer.close();
+  //  Directory dir = new RAMDirectory();
+  //  IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(Version.LUCENE_CURRENT, analyzer));
+  //  Document doc = new Document();
+  //  doc.add( new Field( "f", "a a a b b c a b b c d e f", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS ) );
+  //  doc.add( new Field( "f", "b a b a f", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS ) );
+  //  writer.addDocument( doc );
+  //  writer.close();
     
-    IndexReader reader = IndexReader.open( dir, true );
-    new FieldTermStack( reader, 0, "f", fieldQuery );
-    reader.close();
-  }
+  //  IndexReader reader = IndexReader.open( dir, true );
+  //  new FieldTermStack( reader, 0, "f", fieldQuery );
+  //  reader.close();
+  //}
 
   /**
    * a constructor.
diff --git a/lucene/contrib/lucli/build.xml b/lucene/contrib/lucli/build.xml
index 61837de..167a8d1 100644
--- a/lucene/contrib/lucli/build.xml
+++ b/lucene/contrib/lucli/build.xml
@@ -38,6 +38,22 @@
 
   <import file="../contrib-build.xml"/>
 
+  <module-uptodate name="analysis/common" jarfile="${common.dir}/../modules/analysis/build/common/lucene-analyzers-common-${version}.jar"
+      property="analyzers-common.uptodate" classpath.property="analyzers-common.jar"/>
+
+  <path id="classpath">
+	 <pathelement path="${analyzers-common.jar}"/>
+	 <path refid="base.classpath"/>
+  </path>
+
+  <target name="compile-core" depends="compile-analyzers-common, common.compile-core" />
+
+  <target name="compile-analyzers-common" unless="analyzers-common.uptodate">
+    <subant target="default">
+      <fileset dir="${common.dir}/../modules/analysis/common" includes="build.xml"/>
+    </subant>
+  </target>
+	
   <target name="jar" depends="compile" description="Create JAR">
     <jarify>
       <manifest-attributes>
diff --git a/lucene/contrib/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java b/lucene/contrib/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java
index cd8137c..82fa020 100644
--- a/lucene/contrib/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java
+++ b/lucene/contrib/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java
@@ -27,13 +27,9 @@ import java.util.Set;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.KeywordAnalyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenFilter;
 import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.analysis.SimpleAnalyzer;
-import org.apache.lucene.analysis.StopAnalyzer;
-import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexWriter;
diff --git a/lucene/contrib/misc/build.xml b/lucene/contrib/misc/build.xml
index ad8749c..732c6ec 100644
--- a/lucene/contrib/misc/build.xml
+++ b/lucene/contrib/misc/build.xml
@@ -27,4 +27,19 @@
 
   <import file="../contrib-build.xml"/>
 
+  <module-uptodate name="analysis/common" jarfile="${common.dir}/../modules/analysis/build/common/lucene-analyzers-common-${version}.jar"
+      property="analyzers-common.uptodate" classpath.property="analyzers-common.jar"/>
+
+  <path id="classpath">
+	 <pathelement path="${analyzers-common.jar}"/>
+	 <path refid="base.classpath"/>
+  </path>
+
+  <target name="compile-core" depends="compile-analyzers-common, common.compile-core" />
+
+  <target name="compile-analyzers-common" unless="analyzers-common.uptodate">
+    <subant target="default">
+      <fileset dir="${common.dir}/../modules/analysis/common" includes="build.xml"/>
+    </subant>
+  </target>
 </project>
diff --git a/lucene/contrib/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter.java b/lucene/contrib/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter.java
index 3092d42..bd2b130 100644
--- a/lucene/contrib/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter.java
+++ b/lucene/contrib/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter.java
@@ -21,7 +21,7 @@ import java.io.File;
 import java.io.IOException;
 import java.util.ArrayList;
 
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.FSDirectory;
diff --git a/lucene/contrib/misc/src/java/org/apache/lucene/misc/IndexMergeTool.java b/lucene/contrib/misc/src/java/org/apache/lucene/misc/IndexMergeTool.java
index 1ed7c93..bbda292 100644
--- a/lucene/contrib/misc/src/java/org/apache/lucene/misc/IndexMergeTool.java
+++ b/lucene/contrib/misc/src/java/org/apache/lucene/misc/IndexMergeTool.java
@@ -16,7 +16,7 @@ package org.apache.lucene.misc;
   * limitations under the License.
   */
 
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
diff --git a/lucene/contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis.java b/lucene/contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis.java
index 2560cd2..f7970f8 100644
--- a/lucene/contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis.java
+++ b/lucene/contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis.java
@@ -32,7 +32,6 @@ import java.util.Set;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.index.IndexReader;
@@ -49,7 +48,6 @@ import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.search.TopDocs;
 import org.apache.lucene.store.FSDirectory;
 import org.apache.lucene.util.PriorityQueue;
-import org.apache.lucene.util.Version;
 
 
 /**
@@ -158,13 +156,6 @@ public final class MoreLikeThis {
 	 */
     public static final int DEFAULT_MAX_NUM_TOKENS_PARSED=5000;
        
-
-	/**
-     * Default analyzer to parse source doc with.
-	 * @see #getAnalyzer
-     */
-    public static final Analyzer DEFAULT_ANALYZER = new StandardAnalyzer(Version.LUCENE_CURRENT);
-
     /**
      * Ignore terms with less than this frequency in the source doc.
 	 * @see #getMinTermFreq
@@ -240,7 +231,7 @@ public final class MoreLikeThis {
     /**
      * Analyzer that will be used to parse the doc.
      */
-    private Analyzer analyzer = DEFAULT_ANALYZER;
+    private Analyzer analyzer = null;
 
     /**
      * Ignore words less frequent that this.
@@ -343,10 +334,9 @@ public final class MoreLikeThis {
 
   /**
      * Returns an analyzer that will be used to parse source doc with. The default analyzer
-     * is the {@link #DEFAULT_ANALYZER}.
+     * is not set.
      *
      * @return the analyzer that will be used to parse source doc with.
-	 * @see #DEFAULT_ANALYZER
      */
     public Analyzer getAnalyzer() {
         return analyzer;
@@ -887,6 +877,10 @@ public final class MoreLikeThis {
 	private void addTermFrequencies(Reader r, Map<String,Int> termFreqMap, String fieldName)
 		throws IOException
 	{
+	  if (analyzer == null) {
+	    throw new UnsupportedOperationException("To use MoreLikeThis without " +
+	    		"term vectors, you must provide an Analyzer");
+	  }
 		   TokenStream ts = analyzer.tokenStream(fieldName, r);
 			int tokenCount=0;
 			// for every token
diff --git a/lucene/contrib/queries/src/test/org/apache/lucene/search/BooleanFilterTest.java b/lucene/contrib/queries/src/test/org/apache/lucene/search/BooleanFilterTest.java
index e40a7aa..2e768c1 100644
--- a/lucene/contrib/queries/src/test/org/apache/lucene/search/BooleanFilterTest.java
+++ b/lucene/contrib/queries/src/test/org/apache/lucene/search/BooleanFilterTest.java
@@ -21,7 +21,6 @@ import java.io.IOException;
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexReader;
diff --git a/lucene/contrib/queries/src/test/org/apache/lucene/search/similar/TestMoreLikeThis.java b/lucene/contrib/queries/src/test/org/apache/lucene/search/similar/TestMoreLikeThis.java
index 9140aaa..25e6c32 100644
--- a/lucene/contrib/queries/src/test/org/apache/lucene/search/similar/TestMoreLikeThis.java
+++ b/lucene/contrib/queries/src/test/org/apache/lucene/search/similar/TestMoreLikeThis.java
@@ -24,6 +24,7 @@ import java.util.List;
 import java.util.Map;
 
 import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexReader;
@@ -76,6 +77,7 @@ public class TestMoreLikeThis extends LuceneTestCase {
 
 	MoreLikeThis mlt = new MoreLikeThis(
 		reader);
+	mlt.setAnalyzer(new MockAnalyzer(MockTokenizer.WHITESPACE, false));
 	mlt.setMinDocFreq(1);
 	mlt.setMinTermFreq(1);
 	mlt.setMinWordLen(1);
@@ -110,6 +112,7 @@ public class TestMoreLikeThis extends LuceneTestCase {
     private Map<String,Float> getOriginalValues() throws IOException {
 	Map<String,Float> originalValues = new HashMap<String,Float>();
 	MoreLikeThis mlt = new MoreLikeThis(reader);
+	mlt.setAnalyzer(new MockAnalyzer(MockTokenizer.WHITESPACE, false));
 	mlt.setMinDocFreq(1);
 	mlt.setMinTermFreq(1);
 	mlt.setMinWordLen(1);
diff --git a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.java b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.java
index f8043bc..b76ddf0 100644
--- a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.java
+++ b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.java
@@ -577,22 +577,6 @@ public class PrecedenceQueryParser implements PrecedenceQueryParserConstants {
     return sb.toString();
   }
 
-  /**
-   * Command line tool to test QueryParser, using {@link org.apache.lucene.analysis.SimpleAnalyzer}.
-   * Usage:<br>
-   * <code>java org.apache.lucene.queryParser.QueryParser &lt;input&gt;</code>
-   */
-  public static void main(String[] args) throws Exception {
-    if (args.length == 0) {
-      System.out.println("Usage: java org.apache.lucene.queryParser.QueryParser <input>");
-      System.exit(0);
-    }
-    PrecedenceQueryParser qp = new PrecedenceQueryParser("field",
-                           new org.apache.lucene.analysis.SimpleAnalyzer());
-    Query q = qp.parse(args[0]);
-    System.out.println(q.toString("field"));
-  }
-
 // *   Query  ::= ( Clause )*
 // *   Clause ::= ["+", "-"] [<TERM> ":"] ( <TERM> | "(" Query ")" )
   final public int Conjunction() throws ParseException {
@@ -1290,4 +1274,19 @@ public class PrecedenceQueryParser implements PrecedenceQueryParserConstants {
     JJCalls next;
   }
 
+  /**
+   * Command line tool to test QueryParser, using {@link org.apache.lucene.analysis.SimpleAnalyzer}.
+   * Usage:<br>
+   * <code>java org.apache.lucene.queryParser.QueryParser &lt;input&gt;</code>
+   */
+//  public static void main(String[] args) throws Exception {
+//    if (args.length == 0) {
+//      System.out.println("Usage: java org.apache.lucene.queryParser.QueryParser <input>");
+//      System.exit(0);
+//    }
+//    PrecedenceQueryParser qp = new PrecedenceQueryParser("field",
+//                           new org.apache.lucene.analysis.SimpleAnalyzer());
+//    Query q = qp.parse(args[0]);
+//    System.out.println(q.toString("field"));
+//  }
 }
diff --git a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.jj b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.jj
index 9794e13..9cd2124 100644
--- a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.jj
+++ b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.jj
@@ -606,16 +606,16 @@ public class PrecedenceQueryParser {
    * Usage:<br>
    * <code>java org.apache.lucene.queryParser.QueryParser &lt;input&gt;</code>
    */
-  public static void main(String[] args) throws Exception {
-    if (args.length == 0) {
-      System.out.println("Usage: java org.apache.lucene.queryParser.QueryParser <input>");
-      System.exit(0);
-    }
-    PrecedenceQueryParser qp = new PrecedenceQueryParser("field",
-                           new org.apache.lucene.analysis.SimpleAnalyzer());
-    Query q = qp.parse(args[0]);
-    System.out.println(q.toString("field"));
-  }
+//  public static void main(String[] args) throws Exception {
+//    if (args.length == 0) {
+//      System.out.println("Usage: java org.apache.lucene.queryParser.QueryParser <input>");
+//      System.exit(0);
+//    }
+//    PrecedenceQueryParser qp = new PrecedenceQueryParser("field",
+//                           new org.apache.lucene.analysis.SimpleAnalyzer());
+//    Query q = qp.parse(args[0]);
+//    System.out.println(q.toString("field"));
+//  }
 }
 
 PARSER_END(PrecedenceQueryParser)
diff --git a/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestMultiAnalyzerQPHelper.java b/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestMultiAnalyzerQPHelper.java
index e53e3c1..e98cc6f 100644
--- a/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestMultiAnalyzerQPHelper.java
+++ b/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestMultiAnalyzerQPHelper.java
@@ -20,11 +20,9 @@ package org.apache.lucene.queryParser.standard;
 import java.io.Reader;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.LowerCaseFilter;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.standard.StandardTokenizer;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
diff --git a/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestQPHelper.java b/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestQPHelper.java
index 27b3dfa..addbca2 100644
--- a/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestQPHelper.java
+++ b/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestQPHelper.java
@@ -32,7 +32,6 @@ import java.util.Locale;
 import java.util.Map;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.KeywordAnalyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenFilter;
 import org.apache.lucene.analysis.MockTokenizer;
@@ -341,8 +340,9 @@ public class TestQPHelper extends LocalizedTestCase {
         "t?m term term");
     assertQueryEquals("?laut", new MockAnalyzer(MockTokenizer.WHITESPACE, false), "?laut");
 
-    assertQueryEquals("\"\"", new KeywordAnalyzer(), "");
-    assertQueryEquals("foo:\"\"", new KeywordAnalyzer(), "foo:");
+    // FIXME: change MockAnalyzer to not extend CharTokenizer for this test
+    //assertQueryEquals("\"\"", new KeywordAnalyzer(), "");
+    //assertQueryEquals("foo:\"\"", new KeywordAnalyzer(), "foo:");
 
     assertQueryEquals("a AND b", null, "+a +b");
     assertQueryEquals("(a AND b)", null, "+a +b");
diff --git a/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestQueryParserWrapper.java b/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestQueryParserWrapper.java
index b08c306..b3a28db 100644
--- a/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestQueryParserWrapper.java
+++ b/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestQueryParserWrapper.java
@@ -30,7 +30,6 @@ import java.util.List;
 import java.util.Locale;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.KeywordAnalyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenFilter;
 import org.apache.lucene.analysis.MockTokenizer;
@@ -333,8 +332,9 @@ public class TestQueryParserWrapper extends LocalizedTestCase {
         "t?m term term");
     assertQueryEquals("?laut", new MockAnalyzer(MockTokenizer.WHITESPACE, false), "?laut");
 
-    assertQueryEquals("\"\"", new KeywordAnalyzer(), "");
-    assertQueryEquals("foo:\"\"", new KeywordAnalyzer(), "foo:");
+    //FIXME: Change MockAnalyzer to not extend CharTokenizer for this test
+    //assertQueryEquals("\"\"", new KeywordAnalyzer(), "");
+    //assertQueryEquals("foo:\"\"", new KeywordAnalyzer(), "foo:");
 
     assertQueryEquals("a AND b", null, "+a +b");
     assertQueryEquals("(a AND b)", null, "+a +b");
diff --git a/lucene/contrib/spellchecker/build.xml b/lucene/contrib/spellchecker/build.xml
index d89be94..3d92680 100755
--- a/lucene/contrib/spellchecker/build.xml
+++ b/lucene/contrib/spellchecker/build.xml
@@ -24,4 +24,20 @@
   </description>
 
   <import file="../contrib-build.xml"/>
+
+  <module-uptodate name="analysis/common" jarfile="${common.dir}/../modules/analysis/build/common/lucene-analyzers-common-${version}.jar"
+      property="analyzers-common.uptodate" classpath.property="analyzers-common.jar"/>
+
+  <path id="classpath">
+	 <pathelement path="${analyzers-common.jar}"/>
+	 <path refid="base.classpath"/>
+  </path>
+
+  <target name="compile-core" depends="compile-analyzers-common, common.compile-core" />
+
+  <target name="compile-analyzers-common" unless="analyzers-common.uptodate">
+    <subant target="default">
+      <fileset dir="${common.dir}/../modules/analysis/common" includes="build.xml"/>
+    </subant>
+  </target>
 </project>
diff --git a/lucene/contrib/spellchecker/src/java/org/apache/lucene/search/spell/SpellChecker.java b/lucene/contrib/spellchecker/src/java/org/apache/lucene/search/spell/SpellChecker.java
index 36836d4..c35acd0 100755
--- a/lucene/contrib/spellchecker/src/java/org/apache/lucene/search/spell/SpellChecker.java
+++ b/lucene/contrib/spellchecker/src/java/org/apache/lucene/search/spell/SpellChecker.java
@@ -20,7 +20,7 @@ package org.apache.lucene.search.spell;
 import java.io.IOException;
 import java.util.Iterator;
 
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexReader;
diff --git a/lucene/contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestLuceneDictionary.java b/lucene/contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestLuceneDictionary.java
index 340cfcb..7c68947 100644
--- a/lucene/contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestLuceneDictionary.java
+++ b/lucene/contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestLuceneDictionary.java
@@ -22,7 +22,7 @@ import java.util.Iterator;
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexReader;
diff --git a/lucene/contrib/swing/build.xml b/lucene/contrib/swing/build.xml
index 170978a..3dcecc8 100644
--- a/lucene/contrib/swing/build.xml
+++ b/lucene/contrib/swing/build.xml
@@ -25,6 +25,22 @@
 
   <import file="../contrib-build.xml"/>
 
+  <module-uptodate name="analysis/common" jarfile="${common.dir}/../modules/analysis/build/common/lucene-analyzers-common-${version}.jar"
+      property="analyzers-common.uptodate" classpath.property="analyzers-common.jar"/>
+
+  <path id="classpath">
+	 <pathelement path="${analyzers-common.jar}"/>
+	 <path refid="base.classpath"/>
+  </path>
+
+  <target name="compile-core" depends="compile-analyzers-common, common.compile-core" />
+
+  <target name="compile-analyzers-common" unless="analyzers-common.uptodate">
+    <subant target="default">
+      <fileset dir="${common.dir}/../modules/analysis/common" includes="build.xml"/>
+    </subant>
+  </target>
+
   <target name="list-demo" depends="compile">
     <java classname="org.apache.lucene.swing.models.ListSearcherSimulator"
           fork="yes" spawn="yes"
diff --git a/lucene/contrib/swing/src/java/org/apache/lucene/swing/models/ListSearcher.java b/lucene/contrib/swing/src/java/org/apache/lucene/swing/models/ListSearcher.java
index ed2fc71..611b063 100644
--- a/lucene/contrib/swing/src/java/org/apache/lucene/swing/models/ListSearcher.java
+++ b/lucene/contrib/swing/src/java/org/apache/lucene/swing/models/ListSearcher.java
@@ -25,7 +25,7 @@ import javax.swing.event.ListDataEvent;
 import javax.swing.event.ListDataListener;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.Fieldable;
diff --git a/lucene/contrib/swing/src/java/org/apache/lucene/swing/models/TableSearcher.java b/lucene/contrib/swing/src/java/org/apache/lucene/swing/models/TableSearcher.java
index 7d6b6bc..d01d985 100644
--- a/lucene/contrib/swing/src/java/org/apache/lucene/swing/models/TableSearcher.java
+++ b/lucene/contrib/swing/src/java/org/apache/lucene/swing/models/TableSearcher.java
@@ -24,7 +24,7 @@ import javax.swing.table.AbstractTableModel;
 import javax.swing.table.TableModel;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.Fieldable;
diff --git a/lucene/contrib/wordnet/build.xml b/lucene/contrib/wordnet/build.xml
index 269c9e0..9785e3e 100644
--- a/lucene/contrib/wordnet/build.xml
+++ b/lucene/contrib/wordnet/build.xml
@@ -30,6 +30,22 @@
 
   <import file="../contrib-build.xml"/>
 
+  <module-uptodate name="analysis/common" jarfile="${common.dir}/../modules/analysis/build/common/lucene-analyzers-common-${version}.jar"
+      property="analyzers-common.uptodate" classpath.property="analyzers-common.jar"/>
+
+  <path id="classpath">
+	 <pathelement path="${analyzers-common.jar}"/>
+	 <path refid="base.classpath"/>
+  </path>
+
+  <target name="compile-core" depends="compile-analyzers-common, common.compile-core" />
+
+  <target name="compile-analyzers-common" unless="analyzers-common.uptodate">
+    <subant target="default">
+      <fileset dir="${common.dir}/../modules/analysis/common" includes="build.xml"/>
+    </subant>
+  </target>
+	
   <target name="index" depends="compile" description="Build WordNet index">
     <fail if="synindex.exists">
       Index already exists - must remove first.
diff --git a/lucene/contrib/wordnet/src/test/org/apache/lucene/wordnet/TestSynonymTokenFilter.java b/lucene/contrib/wordnet/src/test/org/apache/lucene/wordnet/TestSynonymTokenFilter.java
index d723a31..847c952 100644
--- a/lucene/contrib/wordnet/src/test/org/apache/lucene/wordnet/TestSynonymTokenFilter.java
+++ b/lucene/contrib/wordnet/src/test/org/apache/lucene/wordnet/TestSynonymTokenFilter.java
@@ -23,12 +23,12 @@ import java.io.IOException;
 import java.io.Reader;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.LowerCaseFilter;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 public class TestSynonymTokenFilter extends BaseTokenStreamTestCase {
   final String testFile = "testSynonyms.txt";
diff --git a/lucene/lib/servlet-api-2.4.jar b/lucene/lib/servlet-api-2.4.jar
deleted file mode 100644
index 9e7f1e8..0000000
--- a/lucene/lib/servlet-api-2.4.jar
+++ /dev/null
@@ -1,2 +0,0 @@
-AnyObjectId[018d6effad3823d0ea59f1b58ab154fc2652f418] was removed in git history.
-Apache SVN contains full history.
\ No newline at end of file
diff --git a/lucene/src/demo/demo-build.template b/lucene/src/demo/demo-build.template
deleted file mode 100644
index d673b21..0000000
--- a/lucene/src/demo/demo-build.template
+++ /dev/null
@@ -1,253 +0,0 @@
-<?xml version="1.0"?>
-
-<!--
-    Licensed to the Apache Software Foundation (ASF) under one or more
-    contributor license agreements.  See the NOTICE file distributed with
-    this work for additional information regarding copyright ownership.
-    The ASF licenses this file to You under the Apache License, Version 2.0
-    the "License"); you may not use this file except in compliance with
-    the License.  You may obtain a copy of the License at
- 
-        http://www.apache.org/licenses/LICENSE-2.0
- 
-    Unless required by applicable law or agreed to in writing, software
-    distributed under the License is distributed on an "AS IS" BASIS,
-    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-    See the License for the specific language governing permissions and
-    limitations under the License.
- -->
-
-<project name="lucene-demo" default="compile-demo" basedir=".">
-  <dirname file="${ant.file.common}" property="common.dir"/>
-
-  <property name="version" value="@PLACEHOLDER_version@"/>
-  <property name="javac.source" value="@PLACEHOLDER_javac.source@"/>
-  <property name="javac.target" value="@PLACEHOLDER_javac.target@"/>
-	
-  <property name="build.dir" location="build"/>
-	
-	
-  <property name="core.name" value="lucene-core-${version}"/>
-  <property name="demo.name" value="lucene-demos-${version}"/>
-  <property name="demo.war.name" value="luceneweb"/>
-
-  <property name="manifest.file" location="${build.dir}/MANIFEST.MF"/>
-
-  <!-- Build classpath -->
-  <path id="classpath">
-    <pathelement location="${common.dir}/${core.name}.jar"/>
-  </path>
-
-  <path id="demo.classpath">
-    <path refid="classpath"/>
-    <pathelement location="${build.dir}/classes/demo"/>
-  </path>
-	
-  <available
-    property="jar.core.present"
-	type="file"
-	file="${common.dir}/${core.name}.jar"
-  />
-
-  <target name="jar.core-check">
-    <fail unless="jar.core.present">
-	  ##################################################################
-	  ${common.dir}/${core.name}.jar not found.
-	  ##################################################################
-	</fail>
-  </target>
-
-  <!-- ================================================================== -->
-  <!-- J A R                                                              -->
-  <!-- ================================================================== -->
-  <!--                                                                    -->
-  <!-- ================================================================== -->
-
-  <target name="jar-demo" depends="compile-demo"
-	description="Build demo jar file">
-	<sequential>
-  	  <build-manifest/>
-	  <jar
-        destfile="${demo.name}.jar"
-        basedir="${build.dir}/classes/demo"
-        excludes="**/*.java"
-        manifest="${manifest.file}">
-        <metainf dir="${common.dir}">
-          <include name="LICENSE.txt"/>
-          <include name="NOTICE.txt"/>
-        </metainf>
-      </jar>
-    </sequential>
-  </target>
-
-  <target name="war-demo" depends="jar-demo"	
-	description="Build demo war file">
-	<sequential>
-  	  <build-manifest/>
-      <war destfile="${demo.war.name}.war"
-           webxml="src/jsp/WEB-INF/web.xml"
-           manifest="${manifest.file}">
-        <fileset dir="src/jsp" excludes="WEB-INF/web.xml"/>
-        <lib dir="." includes="${demo.name}.jar"/>
-        <lib dir="." includes="${core.name}.jar"/>
-        <metainf dir="${common.dir}">
-          <include name="LICENSE.txt"/>
-          <include name="NOTICE.txt"/>
-        </metainf>
-      </war>
-    </sequential>
-  </target>
-
-  <!-- ================================================================== -->
-  <!-- B U I L D  D E M O                                                 -->
-  <!-- ================================================================== -->
-  <!--                                                                    -->
-  <!-- ================================================================== -->
-  <target name="compile-demo" depends="jar.core-check"
-	description="Compile demo classes">
-    <mkdir dir="${build.dir}/classes/demo"/>
-
-    <compile
-      srcdir="src/demo"
-      destdir="${build.dir}/classes/demo">
-      <classpath refid="demo.classpath"/>
-    </compile>
-  </target>
-	
-  <target name="clean"
-    description="Removes contents of build directory">
-    <delete dir="${build.dir}"/>
-    <delete dir="${common.dir}/demo-text-dir"/>
-    <delete dir="${common.dir}/demo-html-dir"/>
-  </target>
-
-  <!-- ================================================================== -->
-  <!-- R U N  T E X T  I N D E X I N G  D E M O                           -->
-  <!-- ================================================================== -->
-  <!--                                                                    -->
-  <!-- ================================================================== -->
-  <target name="demo-index-text" depends="jar-demo"
-	description="Run text indexing demo (index the sources of the demo).">
-    <echo>----- (1) Prepare dir ----- </echo>
-    <echo>cd ${common.dir} </echo>
-    <echo>rmdir demo-text-dir </echo>
-    <delete dir="${common.dir}/demo-text-dir"/>
-    <echo>mkdir demo-text-dir </echo>
-    <mkdir dir="${common.dir}/demo-text-dir"/>
-    <echo>cd demo-text-dir </echo>
-    <echo>----- (2) Index the files located under ${common.dir}/src ----- </echo>
-    <invoke-java class="IndexFiles" params="${common.dir}/src/demo" paramsDisplay="../src/demo" type="text"/>
-  </target>
-
-  <!-- ================================================================== -->
-  <!-- R U N  T E X T  S E A R C H  D E M O                               -->
-  <!-- ================================================================== -->
-  <!--                                                                    -->
-  <!-- ================================================================== -->
-  <target name="demo-search-text" depends="jar-demo"
-	description="Run interactive search demo.">
-    <echo>----- Interactive search ----- </echo>
-    <echo>cd demo-text-dir </echo>
-    <invoke-java class="SearchFiles" params="-index index" paramsDisplay="-index index" type="text"/>
-  </target>
-
-
-  <!-- ================================================================== -->
-  <!-- R U N  H T M L  I N D E X I N G  D E M O                           -->
-  <!-- ================================================================== -->
-  <!--                                                                    -->
-  <!-- ================================================================== -->
-  <target name="demo-index-html" depends="jar-demo"
-	description="Run html indexing demo (index the javadocs).">
-    <echo>----- (1) Prepare dir ----- </echo>
-    <echo>cd ${common.dir} </echo>
-    <echo>rmdir demo-html-dir </echo>
-    <delete dir="${common.dir}/demo-html-dir"/>
-    <echo>mkdir demo-html-dir </echo>
-    <mkdir dir="${common.dir}/demo-html-dir"/>
-    <echo>cd demo-html-dir </echo>
-    <echo>----- (2) Index the files located under ${common.dir}/src ----- </echo>
-    <invoke-java class="IndexFiles" params="${common.dir}/docs/api" paramsDisplay="../docs/api" type="html"/>
-  </target>
-
-  <!-- ================================================================== -->
-  <!-- R U N  H T M L  S E A R C H  D E M O                               -->
-  <!-- ================================================================== -->
-  <!--                                                                    -->
-  <!-- ================================================================== -->
-  <target name="demo-search-html" depends="jar-demo"
-	description="Run interactive search demo.">
-    <echo>----- Interactive search ----- </echo>
-    <echo>cd demo-html-dir </echo>
-    <invoke-java class="SearchFiles" params="-index index" paramsDisplay="-index index" type="html"/>
-  </target>
-
-
-  <!--+
-      | M A C R O S
-      +-->
-      
-  <macrodef name="build-manifest" description="Builds a manifest file">
-  	<sequential>
-      <manifest file="${manifest.file}">
-        <attribute name="Specification-Title" value="Lucene Search Engine: demos"/>
-		<!-- spec version must match "digit+{.digit+}*" -->
-		<attribute name="Specification-Version" value="${version}"/>
-		<attribute name="Specification-Vendor"
-		           value="The Apache Software Foundation"/>
-		<attribute name="Implementation-Title" value="org.apache.lucene"/>
-		<!-- impl version can be any string -->
-		<attribute name="Implementation-Version"
-		           value="${version}"/>
-		<attribute name="Implementation-Vendor"
-		           value="The Apache Software Foundation"/>
-		<attribute name="X-Compile-Source-JDK" 
-		           value="${javac.source}"/>
-		<attribute name="X-Compile-Target-JDK" 
-		           value="${javac.target}"/>
-	  </manifest>
-  	</sequential>
-  </macrodef>
-      
-  <macrodef name="compile">
-    <attribute name="srcdir"/>
-    <attribute name="destdir"/>
-    <element name="nested" implicit="yes" optional="yes"/>
-
-    <sequential>
-      <mkdir dir="@{destdir}"/>
-      <javac
-        srcdir="@{srcdir}"
-        destdir="@{destdir}"
-        deprecation="off"
-        debug="on"
-        source="${javac.source}"
-        target="${javac.target}">
-        <nested/>
-      </javac>
-    </sequential>
-  </macrodef>
-
-  <macrodef name="invoke-java">
-    <attribute name="class"/>
-    <attribute name="params"/>
-    <attribute name="paramsDisplay"/>
-    <attribute name="type"/>
-    <sequential>
-      <echo>java -classpath "../${core.name}.jar;../${demo.name}.jar" org.apache.lucene.demo.@{class} @{paramsDisplay} </echo>
-      <java classname="org.apache.lucene.demo.@{class}"
-            dir="${common.dir}/demo-@{type}-dir"
-            fork="true"
-            failonerror="true"
-            maxmemory="128m"
-      >
-        <arg value="@{params}"/>
-        <classpath>
-           <pathelement location="${common.dir}/${core.name}.jar"/>
-           <pathelement location="${common.dir}/${demo.name}.jar"/>
-        </classpath>
-      </java>
-    </sequential>
-  </macrodef>
-
-</project>
diff --git a/lucene/src/demo/org/apache/lucene/demo/DeleteFiles.java b/lucene/src/demo/org/apache/lucene/demo/DeleteFiles.java
deleted file mode 100644
index a5588eb..0000000
--- a/lucene/src/demo/org/apache/lucene/demo/DeleteFiles.java
+++ /dev/null
@@ -1,66 +0,0 @@
-package org.apache.lucene.demo;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.File;
-
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.FSDirectory;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.Term;
-//import org.apache.lucene.index.Term;
-
-
-/** Deletes documents from an index that do not contain a term. */
-public class DeleteFiles {
-  
-  private DeleteFiles() {}                         // singleton
-
-  /** Deletes documents from an index that do not contain a term. */
-  public static void main(String[] args) {
-    String usage = "java org.apache.lucene.demo.DeleteFiles <unique_term>";
-    if (args.length == 0) {
-      System.err.println("Usage: " + usage);
-      System.exit(1);
-    }
-    try {
-      Directory directory = FSDirectory.open(new File("index"));
-      IndexReader reader = IndexReader.open(directory, false); // we don't want read-only because we are about to delete
-
-      Term term = new Term("path", args[0]);
-      int deleted = reader.deleteDocuments(term);
-
-      System.out.println("deleted " + deleted +
- 			 " documents containing " + term);
-
-      // one can also delete documents by their internal id:
-      /*
-      for (int i = 0; i < reader.maxDoc(); i++) {
-        System.out.println("Deleting document with id " + i);
-        reader.delete(i);
-      }*/
-
-      reader.close();
-      directory.close();
-
-    } catch (Exception e) {
-      System.out.println(" caught a " + e.getClass() +
-			 "\n with message: " + e.getMessage());
-    }
-  }
-}
diff --git a/lucene/src/demo/org/apache/lucene/demo/FileDocument.java b/lucene/src/demo/org/apache/lucene/demo/FileDocument.java
deleted file mode 100644
index ac634cd..0000000
--- a/lucene/src/demo/org/apache/lucene/demo/FileDocument.java
+++ /dev/null
@@ -1,71 +0,0 @@
-package org.apache.lucene.demo;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.File;
-import java.io.FileReader;
-
-import org.apache.lucene.document.DateTools;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-
-/** A utility for making Lucene Documents from a File. */
-
-public class FileDocument {
-  /** Makes a document for a File.
-    <p>
-    The document has three fields:
-    <ul>
-    <li><code>path</code>--containing the pathname of the file, as a stored,
-    untokenized field;
-    <li><code>modified</code>--containing the last modified date of the file as
-    a field as created by <a
-    href="lucene.document.DateTools.html">DateTools</a>; and
-    <li><code>contents</code>--containing the full contents of the file, as a
-    Reader field;
-    */
-  public static Document Document(File f)
-       throws java.io.FileNotFoundException {
-	 
-    // make a new, empty document
-    Document doc = new Document();
-
-    // Add the path of the file as a field named "path".  Use a field that is 
-    // indexed (i.e. searchable), but don't tokenize the field into words.
-    doc.add(new Field("path", f.getPath(), Field.Store.YES, Field.Index.NOT_ANALYZED));
-
-    // Add the last modified date of the file a field named "modified".  Use 
-    // a field that is indexed (i.e. searchable), but don't tokenize the field
-    // into words.
-    doc.add(new Field("modified",
-        DateTools.timeToString(f.lastModified(), DateTools.Resolution.MINUTE),
-        Field.Store.YES, Field.Index.NOT_ANALYZED));
-
-    // Add the contents of the file to a field named "contents".  Specify a Reader,
-    // so that the text of the file is tokenized and indexed, but not stored.
-    // Note that FileReader expects the file to be in the system's default encoding.
-    // If that's not the case searching for special characters will fail.
-    doc.add(new Field("contents", new FileReader(f)));
-
-    // return the document
-    return doc;
-  }
-
-  private FileDocument() {}
-}
-    
diff --git a/lucene/src/demo/org/apache/lucene/demo/HTMLDocument.java b/lucene/src/demo/org/apache/lucene/demo/HTMLDocument.java
deleted file mode 100644
index aa38195..0000000
--- a/lucene/src/demo/org/apache/lucene/demo/HTMLDocument.java
+++ /dev/null
@@ -1,86 +0,0 @@
-package org.apache.lucene.demo;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.*;
-import org.apache.lucene.document.*;
-import org.apache.lucene.demo.html.HTMLParser;
-
-/** A utility for making Lucene Documents for HTML documents. */
-
-public class HTMLDocument {
-  static char dirSep = System.getProperty("file.separator").charAt(0);
-
-  public static String uid(File f) {
-    // Append path and date into a string in such a way that lexicographic
-    // sorting gives the same results as a walk of the file hierarchy.  Thus
-    // null (\u0000) is used both to separate directory components and to
-    // separate the path from the date.
-    return f.getPath().replace(dirSep, '\u0000') +
-      "\u0000" +
-      DateTools.timeToString(f.lastModified(), DateTools.Resolution.SECOND);
-  }
-
-  public static String uid2url(String uid) {
-    String url = uid.replace('\u0000', '/');	  // replace nulls with slashes
-    return url.substring(0, url.lastIndexOf('/')); // remove date from end
-  }
-
-  public static Document Document(File f)
-       throws IOException, InterruptedException  {
-    // make a new, empty document
-    Document doc = new Document();
-
-    // Add the url as a field named "path".  Use a field that is 
-    // indexed (i.e. searchable), but don't tokenize the field into words.
-    doc.add(new Field("path", f.getPath().replace(dirSep, '/'), Field.Store.YES,
-        Field.Index.NOT_ANALYZED));
-
-    // Add the last modified date of the file a field named "modified".  
-    // Use a field that is indexed (i.e. searchable), but don't tokenize
-    // the field into words.
-    doc.add(new Field("modified",
-        DateTools.timeToString(f.lastModified(), DateTools.Resolution.MINUTE),
-        Field.Store.YES, Field.Index.NOT_ANALYZED));
-
-    // Add the uid as a field, so that index can be incrementally maintained.
-    // This field is not stored with document, it is indexed, but it is not
-    // tokenized prior to indexing.
-    doc.add(new Field("uid", uid(f), Field.Store.NO, Field.Index.NOT_ANALYZED));
-
-    FileInputStream fis = new FileInputStream(f);
-    HTMLParser parser = new HTMLParser(fis);
-      
-    // Add the tag-stripped contents as a Reader-valued Text field so it will
-    // get tokenized and indexed.
-    doc.add(new Field("contents", parser.getReader()));
-
-    // Add the summary as a field that is stored and returned with
-    // hit documents for display.
-    doc.add(new Field("summary", parser.getSummary(), Field.Store.YES, Field.Index.NO));
-
-    // Add the title as a field that it can be searched and that is stored.
-    doc.add(new Field("title", parser.getTitle(), Field.Store.YES, Field.Index.ANALYZED));
-
-    // return the document
-    return doc;
-  }
-
-  private HTMLDocument() {}
-}
-    
diff --git a/lucene/src/demo/org/apache/lucene/demo/IndexFiles.java b/lucene/src/demo/org/apache/lucene/demo/IndexFiles.java
deleted file mode 100644
index 7652f6c..0000000
--- a/lucene/src/demo/org/apache/lucene/demo/IndexFiles.java
+++ /dev/null
@@ -1,103 +0,0 @@
-package org.apache.lucene.demo;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.standard.StandardAnalyzer;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import org.apache.lucene.store.FSDirectory;
-import org.apache.lucene.util.Version;
-
-import java.io.File;
-import java.io.FileNotFoundException;
-import java.io.IOException;
-import java.util.Date;
-
-/** Index all text files under a directory. */
-public class IndexFiles {
-  
-  private IndexFiles() {}
-
-  static final File INDEX_DIR = new File("index");
-  
-  /** Index all text files under a directory. */
-  public static void main(String[] args) {
-    String usage = "java org.apache.lucene.demo.IndexFiles <root_directory>";
-    if (args.length == 0) {
-      System.err.println("Usage: " + usage);
-      System.exit(1);
-    }
-
-    if (INDEX_DIR.exists()) {
-      System.out.println("Cannot save index to '" +INDEX_DIR+ "' directory, please delete it first");
-      System.exit(1);
-    }
-    
-    final File docDir = new File(args[0]);
-    if (!docDir.exists() || !docDir.canRead()) {
-      System.out.println("Document directory '" +docDir.getAbsolutePath()+ "' does not exist or is not readable, please check the path");
-      System.exit(1);
-    }
-    
-    Date start = new Date();
-    try {
-      IndexWriter writer = new IndexWriter(FSDirectory.open(INDEX_DIR),
-          new IndexWriterConfig(Version.LUCENE_CURRENT, new StandardAnalyzer(
-              Version.LUCENE_CURRENT)).setOpenMode(OpenMode.CREATE));
-      System.out.println("Indexing to directory '" +INDEX_DIR+ "'...");
-      indexDocs(writer, docDir);
-      System.out.println("Optimizing...");
-      writer.optimize();
-      writer.close();
-
-      Date end = new Date();
-      System.out.println(end.getTime() - start.getTime() + " total milliseconds");
-
-    } catch (IOException e) {
-      System.out.println(" caught a " + e.getClass() +
-       "\n with message: " + e.getMessage());
-    }
-  }
-
-  static void indexDocs(IndexWriter writer, File file)
-    throws IOException {
-    // do not try to index files that cannot be read
-    if (file.canRead()) {
-      if (file.isDirectory()) {
-        String[] files = file.list();
-        // an IO error could occur
-        if (files != null) {
-          for (int i = 0; i < files.length; i++) {
-            indexDocs(writer, new File(file, files[i]));
-          }
-        }
-      } else {
-        System.out.println("adding " + file);
-        try {
-          writer.addDocument(FileDocument.Document(file));
-        }
-        // at least on windows, some temporary files raise this exception with an "access denied" message
-        // checking if the file can be read doesn't help
-        catch (FileNotFoundException fnfe) {
-        }
-      }
-    }
-  }
-  
-}
diff --git a/lucene/src/demo/org/apache/lucene/demo/IndexHTML.java b/lucene/src/demo/org/apache/lucene/demo/IndexHTML.java
deleted file mode 100644
index 8e5543d..0000000
--- a/lucene/src/demo/org/apache/lucene/demo/IndexHTML.java
+++ /dev/null
@@ -1,172 +0,0 @@
-package org.apache.lucene.demo;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.standard.StandardAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.TermEnum;
-import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import org.apache.lucene.store.FSDirectory;
-import org.apache.lucene.util.Version;
-
-import java.io.File;
-import java.util.Date;
-import java.util.Arrays;
-
-/** Indexer for HTML files. */
-public class IndexHTML {
-  private IndexHTML() {}
-
-  private static boolean deleting = false;	  // true during deletion pass
-  private static IndexReader reader;		  // existing index
-  private static IndexWriter writer;		  // new index being built
-  private static TermEnum uidIter;		  // document id iterator
-
-  /** Indexer for HTML files.*/
-  public static void main(String[] argv) {
-    try {
-      File index = new File("index");
-      boolean create = false;
-      File root = null;
-
-      String usage = "IndexHTML [-create] [-index <index>] <root_directory>";
-
-      if (argv.length == 0) {
-        System.err.println("Usage: " + usage);
-        return;
-      }
-
-      for (int i = 0; i < argv.length; i++) {
-        if (argv[i].equals("-index")) {		  // parse -index option
-          index = new File(argv[++i]);
-        } else if (argv[i].equals("-create")) {	  // parse -create option
-          create = true;
-        } else if (i != argv.length-1) {
-          System.err.println("Usage: " + usage);
-          return;
-        } else
-          root = new File(argv[i]);
-      }
-      
-      if(root == null) {
-        System.err.println("Specify directory to index");
-        System.err.println("Usage: " + usage);
-        return;
-      }
-
-      Date start = new Date();
-
-      if (!create) {				  // delete stale docs
-        deleting = true;
-        indexDocs(root, index, create);
-      }
-      writer = new IndexWriter(FSDirectory.open(index), new IndexWriterConfig(
-          Version.LUCENE_CURRENT, new StandardAnalyzer(Version.LUCENE_CURRENT))
-          .setMaxFieldLength(1000000).setOpenMode(
-              create ? OpenMode.CREATE : OpenMode.CREATE_OR_APPEND));
-      indexDocs(root, index, create);		  // add new docs
-
-      System.out.println("Optimizing index...");
-      writer.optimize();
-      writer.close();
-
-      Date end = new Date();
-
-      System.out.print(end.getTime() - start.getTime());
-      System.out.println(" total milliseconds");
-
-    } catch (Exception e) {
-      e.printStackTrace();
-    }
-  }
-
-  /* Walk directory hierarchy in uid order, while keeping uid iterator from
-  /* existing index in sync.  Mismatches indicate one of: (a) old documents to
-  /* be deleted; (b) unchanged documents, to be left alone; or (c) new
-  /* documents, to be indexed.
-   */
-
-  private static void indexDocs(File file, File index, boolean create)
-       throws Exception {
-    if (!create) {				  // incrementally update
-
-      reader = IndexReader.open(FSDirectory.open(index), false);		  // open existing index
-      uidIter = reader.terms(new Term("uid", "")); // init uid iterator
-
-      indexDocs(file);
-
-      if (deleting) {				  // delete rest of stale docs
-        while (uidIter.term() != null && uidIter.term().field() == "uid") {
-          System.out.println("deleting " +
-              HTMLDocument.uid2url(uidIter.term().text()));
-          reader.deleteDocuments(uidIter.term());
-          uidIter.next();
-        }
-        deleting = false;
-      }
-
-      uidIter.close();				  // close uid iterator
-      reader.close();				  // close existing index
-
-    } else					  // don't have exisiting
-      indexDocs(file);
-  }
-
-  private static void indexDocs(File file) throws Exception {
-    if (file.isDirectory()) {			  // if a directory
-      String[] files = file.list();		  // list its files
-      Arrays.sort(files);			  // sort the files
-      for (int i = 0; i < files.length; i++)	  // recursively index them
-        indexDocs(new File(file, files[i]));
-
-    } else if (file.getPath().endsWith(".html") || // index .html files
-      file.getPath().endsWith(".htm") || // index .htm files
-      file.getPath().endsWith(".txt")) { // index .txt files
-
-      if (uidIter != null) {
-        String uid = HTMLDocument.uid(file);	  // construct uid for doc
-
-        while (uidIter.term() != null && uidIter.term().field() == "uid" &&
-            uidIter.term().text().compareTo(uid) < 0) {
-          if (deleting) {			  // delete stale docs
-            System.out.println("deleting " +
-                HTMLDocument.uid2url(uidIter.term().text()));
-            reader.deleteDocuments(uidIter.term());
-          }
-          uidIter.next();
-        }
-        if (uidIter.term() != null && uidIter.term().field() == "uid" &&
-            uidIter.term().text().compareTo(uid) == 0) {
-          uidIter.next();			  // keep matching docs
-        } else if (!deleting) {			  // add new docs
-          Document doc = HTMLDocument.Document(file);
-          System.out.println("adding " + doc.get("path"));
-          writer.addDocument(doc);
-        }
-      } else {					  // creating a new index
-        Document doc = HTMLDocument.Document(file);
-        System.out.println("adding " + doc.get("path"));
-        writer.addDocument(doc);		  // add docs unconditionally
-      }
-    }
-  }
-}
diff --git a/lucene/src/demo/org/apache/lucene/demo/SearchFiles.java b/lucene/src/demo/org/apache/lucene/demo/SearchFiles.java
deleted file mode 100644
index 65629fe..0000000
--- a/lucene/src/demo/org/apache/lucene/demo/SearchFiles.java
+++ /dev/null
@@ -1,313 +0,0 @@
-package org.apache.lucene.demo;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.BufferedReader;
-import java.io.File;
-import java.io.FileReader;
-import java.io.IOException;
-import java.io.InputStreamReader;
-import java.util.Date;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.standard.StandardAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.index.FilterIndexReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.queryParser.QueryParser;
-import org.apache.lucene.search.Collector;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.ScoreDoc;
-import org.apache.lucene.search.Scorer;
-import org.apache.lucene.search.Searcher;
-import org.apache.lucene.search.TopScoreDocCollector;
-import org.apache.lucene.store.FSDirectory;
-import org.apache.lucene.util.Version;
-
-/** Simple command-line based search demo. */
-public class SearchFiles {
-
-  /** Use the norms from one field for all fields.  Norms are read into memory,
-   * using a byte of memory per document per searched field.  This can cause
-   * search of large collections with a large number of fields to run out of
-   * memory.  If all of the fields contain only a single token, then the norms
-   * are all identical, then single norm vector may be shared. */
-  private static class OneNormsReader extends FilterIndexReader {
-    private String field;
-
-    public OneNormsReader(IndexReader in, String field) {
-      super(in);
-      this.field = field;
-    }
-
-    @Override
-    public byte[] norms(String field) throws IOException {
-      return in.norms(this.field);
-    }
-  }
-
-  private SearchFiles() {}
-
-  /** Simple command-line based search demo. */
-  public static void main(String[] args) throws Exception {
-    String usage =
-      "Usage:\tjava org.apache.lucene.demo.SearchFiles [-index dir] [-field f] [-repeat n] [-queries file] [-raw] [-norms field] [-paging hitsPerPage]";
-    usage += "\n\tSpecify 'false' for hitsPerPage to use streaming instead of paging search.";
-    if (args.length > 0 && ("-h".equals(args[0]) || "-help".equals(args[0]))) {
-      System.out.println(usage);
-      System.exit(0);
-    }
-
-    String index = "index";
-    String field = "contents";
-    String queries = null;
-    int repeat = 0;
-    boolean raw = false;
-    String normsField = null;
-    boolean paging = true;
-    int hitsPerPage = 10;
-    
-    for (int i = 0; i < args.length; i++) {
-      if ("-index".equals(args[i])) {
-        index = args[i+1];
-        i++;
-      } else if ("-field".equals(args[i])) {
-        field = args[i+1];
-        i++;
-      } else if ("-queries".equals(args[i])) {
-        queries = args[i+1];
-        i++;
-      } else if ("-repeat".equals(args[i])) {
-        repeat = Integer.parseInt(args[i+1]);
-        i++;
-      } else if ("-raw".equals(args[i])) {
-        raw = true;
-      } else if ("-norms".equals(args[i])) {
-        normsField = args[i+1];
-        i++;
-      } else if ("-paging".equals(args[i])) {
-        if (args[i+1].equals("false")) {
-          paging = false;
-        } else {
-          hitsPerPage = Integer.parseInt(args[i+1]);
-          if (hitsPerPage == 0) {
-            paging = false;
-          }
-        }
-        i++;
-      }
-    }
-    
-    IndexReader reader = IndexReader.open(FSDirectory.open(new File(index)), true); // only searching, so read-only=true
-
-    if (normsField != null)
-      reader = new OneNormsReader(reader, normsField);
-
-    Searcher searcher = new IndexSearcher(reader);
-    Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_CURRENT);
-
-    BufferedReader in = null;
-    if (queries != null) {
-      in = new BufferedReader(new FileReader(queries));
-    } else {
-      in = new BufferedReader(new InputStreamReader(System.in, "UTF-8"));
-    }
-    QueryParser parser = new QueryParser(Version.LUCENE_CURRENT, field, analyzer);
-    while (true) {
-      if (queries == null)                        // prompt the user
-        System.out.println("Enter query: ");
-
-      String line = in.readLine();
-
-      if (line == null || line.length() == -1)
-        break;
-
-      line = line.trim();
-      if (line.length() == 0)
-        break;
-      
-      Query query = parser.parse(line);
-      System.out.println("Searching for: " + query.toString(field));
-
-            
-      if (repeat > 0) {                           // repeat & time as benchmark
-        Date start = new Date();
-        for (int i = 0; i < repeat; i++) {
-          searcher.search(query, null, 100);
-        }
-        Date end = new Date();
-        System.out.println("Time: "+(end.getTime()-start.getTime())+"ms");
-      }
-
-      if (paging) {
-        doPagingSearch(in, searcher, query, hitsPerPage, raw, queries == null);
-      } else {
-        doStreamingSearch(searcher, query);
-      }
-    }
-    reader.close();
-  }
-  
-  /**
-   * This method uses a custom HitCollector implementation which simply prints out
-   * the docId and score of every matching document. 
-   * 
-   *  This simulates the streaming search use case, where all hits are supposed to
-   *  be processed, regardless of their relevance.
-   */
-  public static void doStreamingSearch(final Searcher searcher, Query query) throws IOException {
-    Collector streamingHitCollector = new Collector() {
-      private Scorer scorer;
-      private int docBase;
-      
-      // simply print docId and score of every matching document
-      @Override
-      public void collect(int doc) throws IOException {
-        System.out.println("doc=" + doc + docBase + " score=" + scorer.score());
-      }
-
-      @Override
-      public boolean acceptsDocsOutOfOrder() {
-        return true;
-      }
-
-      @Override
-      public void setNextReader(IndexReader reader, int docBase)
-          throws IOException {
-        this.docBase = docBase;
-      }
-
-      @Override
-      public void setScorer(Scorer scorer) throws IOException {
-        this.scorer = scorer;
-      }
-      
-    };
-    
-    searcher.search(query, streamingHitCollector);
-  }
-
-  /**
-   * This demonstrates a typical paging search scenario, where the search engine presents 
-   * pages of size n to the user. The user can then go to the next page if interested in
-   * the next hits.
-   * 
-   * When the query is executed for the first time, then only enough results are collected
-   * to fill 5 result pages. If the user wants to page beyond this limit, then the query
-   * is executed another time and all hits are collected.
-   * 
-   */
-  public static void doPagingSearch(BufferedReader in, Searcher searcher, Query query, 
-                                     int hitsPerPage, boolean raw, boolean interactive) throws IOException {
- 
-    // Collect enough docs to show 5 pages
-    TopScoreDocCollector collector = TopScoreDocCollector.create(
-        5 * hitsPerPage, false);
-    searcher.search(query, collector);
-    ScoreDoc[] hits = collector.topDocs().scoreDocs;
-    
-    int numTotalHits = collector.getTotalHits();
-    System.out.println(numTotalHits + " total matching documents");
-
-    int start = 0;
-    int end = Math.min(numTotalHits, hitsPerPage);
-        
-    while (true) {
-      if (end > hits.length) {
-        System.out.println("Only results 1 - " + hits.length +" of " + numTotalHits + " total matching documents collected.");
-        System.out.println("Collect more (y/n) ?");
-        String line = in.readLine();
-        if (line.length() == 0 || line.charAt(0) == 'n') {
-          break;
-        }
-
-        collector = TopScoreDocCollector.create(numTotalHits, false);
-        searcher.search(query, collector);
-        hits = collector.topDocs().scoreDocs;
-      }
-      
-      end = Math.min(hits.length, start + hitsPerPage);
-      
-      for (int i = start; i < end; i++) {
-        if (raw) {                              // output raw format
-          System.out.println("doc="+hits[i].doc+" score="+hits[i].score);
-          continue;
-        }
-
-        Document doc = searcher.doc(hits[i].doc);
-        String path = doc.get("path");
-        if (path != null) {
-          System.out.println((i+1) + ". " + path);
-          String title = doc.get("title");
-          if (title != null) {
-            System.out.println("   Title: " + doc.get("title"));
-          }
-        } else {
-          System.out.println((i+1) + ". " + "No path for this document");
-        }
-                  
-      }
-
-      if (!interactive) {
-        break;
-      }
-
-      if (numTotalHits >= end) {
-        boolean quit = false;
-        while (true) {
-          System.out.print("Press ");
-          if (start - hitsPerPage >= 0) {
-            System.out.print("(p)revious page, ");  
-          }
-          if (start + hitsPerPage < numTotalHits) {
-            System.out.print("(n)ext page, ");
-          }
-          System.out.println("(q)uit or enter number to jump to a page.");
-          
-          String line = in.readLine();
-          if (line.length() == 0 || line.charAt(0)=='q') {
-            quit = true;
-            break;
-          }
-          if (line.charAt(0) == 'p') {
-            start = Math.max(0, start - hitsPerPage);
-            break;
-          } else if (line.charAt(0) == 'n') {
-            if (start + hitsPerPage < numTotalHits) {
-              start+=hitsPerPage;
-            }
-            break;
-          } else {
-            int page = Integer.parseInt(line);
-            if ((page - 1) * hitsPerPage < numTotalHits) {
-              start = (page - 1) * hitsPerPage;
-              break;
-            } else {
-              System.out.println("No such page");
-            }
-          }
-        }
-        if (quit) break;
-        end = Math.min(numTotalHits, start + hitsPerPage);
-      }
-      
-    }
-
-  }
-}
diff --git a/lucene/src/demo/org/apache/lucene/demo/html/Entities.java b/lucene/src/demo/org/apache/lucene/demo/html/Entities.java
deleted file mode 100644
index 5c5a4f2..0000000
--- a/lucene/src/demo/org/apache/lucene/demo/html/Entities.java
+++ /dev/null
@@ -1,327 +0,0 @@
-package org.apache.lucene.demo.html;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.HashMap;
-import java.util.Map;
-
-public class Entities {
-  static final Map<String,String> decoder = new HashMap<String,String>(300);
-  static final String[]  encoder = new String[0x100];
-
-  static final String decode(String entity) {
-    if (entity.charAt(entity.length()-1) == ';')  // remove trailing semicolon
-      entity = entity.substring(0, entity.length()-1);
-    if (entity.charAt(1) == '#') {
-      int start = 2;
-      int radix = 10;
-      if (entity.charAt(2) == 'X' || entity.charAt(2) == 'x') {
-	start++;
-	radix = 16;
-      }
-      Character c =
-	new Character((char)Integer.parseInt(entity.substring(start), radix));
-      return c.toString();
-    } else {
-      String s = decoder.get(entity);
-      if (s != null)
-	return s;
-      else return "";
-    }
-  }
-
-  public static final String encode(String s) {
-    int length = s.length();
-    StringBuffer buffer = new StringBuffer(length * 2);
-    for (int i = 0; i < length; i++) {
-      int j = s.charAt(i);
-      if (j < 0x100 && encoder[j] != null) {
-	buffer.append(encoder[j]);		  // have a named encoding
-	buffer.append(';');
-      } else if (j < 0x80) {
-	buffer.append((char) j);			  // use ASCII value
-      } else {
-	buffer.append("&#");			  // use numeric encoding
-	buffer.append(j).append(';');
-      }
-    }
-    return buffer.toString();
-  }
-
-  static final void add(String entity, int value) {
-    decoder.put(entity, (new Character((char)value)).toString());
-    if (value < 0x100)
-      encoder[value] = entity;
-  }
-
-  static {
-    add("&nbsp",   160);
-    add("&iexcl",  161);
-    add("&cent",   162);
-    add("&pound",  163);
-    add("&curren", 164);
-    add("&yen",    165);
-    add("&brvbar", 166);
-    add("&sect",   167);
-    add("&uml",    168);
-    add("&copy",   169);
-    add("&ordf",   170);
-    add("&laquo",  171);
-    add("&not",    172);
-    add("&shy",    173);
-    add("&reg",    174);
-    add("&macr",   175);
-    add("&deg",    176);
-    add("&plusmn", 177);
-    add("&sup2",   178);
-    add("&sup3",   179);
-    add("&acute",  180);
-    add("&micro",  181);
-    add("&para",   182);
-    add("&middot", 183);
-    add("&cedil",  184);
-    add("&sup1",   185);
-    add("&ordm",   186);
-    add("&raquo",  187);
-    add("&frac14", 188);
-    add("&frac12", 189);
-    add("&frac34", 190);
-    add("&iquest", 191);
-    add("&Agrave", 192);
-    add("&Aacute", 193);
-    add("&Acirc",  194);
-    add("&Atilde", 195);
-    add("&Auml",   196);
-    add("&Aring",  197);
-    add("&AElig",  198);
-    add("&Ccedil", 199);
-    add("&Egrave", 200);
-    add("&Eacute", 201);
-    add("&Ecirc",  202);
-    add("&Euml",   203);
-    add("&Igrave", 204);
-    add("&Iacute", 205);
-    add("&Icirc",  206);
-    add("&Iuml",   207);
-    add("&ETH",    208);
-    add("&Ntilde", 209);
-    add("&Ograve", 210);
-    add("&Oacute", 211);
-    add("&Ocirc",  212);
-    add("&Otilde", 213);
-    add("&Ouml",   214);
-    add("&times",  215);
-    add("&Oslash", 216);
-    add("&Ugrave", 217);
-    add("&Uacute", 218);
-    add("&Ucirc",  219);
-    add("&Uuml",   220);
-    add("&Yacute", 221);
-    add("&THORN",  222);
-    add("&szlig",  223);
-    add("&agrave", 224);
-    add("&aacute", 225);
-    add("&acirc",  226);
-    add("&atilde", 227);
-    add("&auml",   228);
-    add("&aring",  229);
-    add("&aelig",  230);
-    add("&ccedil", 231);
-    add("&egrave", 232);
-    add("&eacute", 233);
-    add("&ecirc",  234);
-    add("&euml",   235);
-    add("&igrave", 236);
-    add("&iacute", 237);
-    add("&icirc",  238);
-    add("&iuml",   239);
-    add("&eth",    240);
-    add("&ntilde", 241);
-    add("&ograve", 242);
-    add("&oacute", 243);
-    add("&ocirc",  244);
-    add("&otilde", 245);
-    add("&ouml",   246);
-    add("&divide", 247);
-    add("&oslash", 248);
-    add("&ugrave", 249);
-    add("&uacute", 250);
-    add("&ucirc",  251);
-    add("&uuml",   252);
-    add("&yacute", 253);
-    add("&thorn",  254);
-    add("&yuml",   255);
-    add("&fnof",   402);
-    add("&Alpha",  913);
-    add("&Beta",   914);
-    add("&Gamma",  915);
-    add("&Delta",  916);
-    add("&Epsilon",917);
-    add("&Zeta",   918);
-    add("&Eta",    919);
-    add("&Theta",  920);
-    add("&Iota",   921);
-    add("&Kappa",  922);
-    add("&Lambda", 923);
-    add("&Mu",     924);
-    add("&Nu",     925);
-    add("&Xi",     926);
-    add("&Omicron",927);
-    add("&Pi",     928);
-    add("&Rho",    929);
-    add("&Sigma",  931);
-    add("&Tau",    932);
-    add("&Upsilon",933);
-    add("&Phi",    934);
-    add("&Chi",    935);
-    add("&Psi",    936);
-    add("&Omega",  937);
-    add("&alpha",  945);
-    add("&beta",   946);
-    add("&gamma",  947);
-    add("&delta",  948);
-    add("&epsilon",949);
-    add("&zeta",   950);
-    add("&eta",    951);
-    add("&theta",  952);
-    add("&iota",   953);
-    add("&kappa",  954);
-    add("&lambda", 955);
-    add("&mu",     956);
-    add("&nu",     957);
-    add("&xi",     958);
-    add("&omicron",959);
-    add("&pi",     960);
-    add("&rho",    961);
-    add("&sigmaf", 962);
-    add("&sigma",  963);
-    add("&tau",    964);
-    add("&upsilon",965);
-    add("&phi",    966);
-    add("&chi",    967);
-    add("&psi",    968);
-    add("&omega",  969);
-    add("&thetasym",977);
-    add("&upsih",  978);
-    add("&piv",    982);
-    add("&bull",   8226);
-    add("&hellip", 8230);
-    add("&prime",  8242);
-    add("&Prime",  8243);
-    add("&oline",  8254);
-    add("&frasl",  8260);
-    add("&weierp", 8472);
-    add("&image",  8465);
-    add("&real",   8476);
-    add("&trade",  8482);
-    add("&alefsym",8501);
-    add("&larr",   8592);
-    add("&uarr",   8593);
-    add("&rarr",   8594);
-    add("&darr",   8595);
-    add("&harr",   8596);
-    add("&crarr",  8629);
-    add("&lArr",   8656);
-    add("&uArr",   8657);
-    add("&rArr",   8658);
-    add("&dArr",   8659);
-    add("&hArr",   8660);
-    add("&forall", 8704);
-    add("&part",   8706);
-    add("&exist",  8707);
-    add("&empty",  8709);
-    add("&nabla",  8711);
-    add("&isin",   8712);
-    add("&notin",  8713);
-    add("&ni",     8715);
-    add("&prod",   8719);
-    add("&sum",    8721);
-    add("&minus",  8722);
-    add("&lowast", 8727);
-    add("&radic",  8730);
-    add("&prop",   8733);
-    add("&infin",  8734);
-    add("&ang",    8736);
-    add("&and",    8743);
-    add("&or",     8744);
-    add("&cap",    8745);
-    add("&cup",    8746);
-    add("&int",    8747);
-    add("&there4", 8756);
-    add("&sim",    8764);
-    add("&cong",   8773);
-    add("&asymp",  8776);
-    add("&ne",     8800);
-    add("&equiv",  8801);
-    add("&le",     8804);
-    add("&ge",     8805);
-    add("&sub",    8834);
-    add("&sup",    8835);
-    add("&nsub",   8836);
-    add("&sube",   8838);
-    add("&supe",   8839);
-    add("&oplus",  8853);
-    add("&otimes", 8855);
-    add("&perp",   8869);
-    add("&sdot",   8901);
-    add("&lceil",  8968);
-    add("&rceil",  8969);
-    add("&lfloor", 8970);
-    add("&rfloor", 8971);
-    add("&lang",   9001);
-    add("&rang",   9002);
-    add("&loz",    9674);
-    add("&spades", 9824);
-    add("&clubs",  9827);
-    add("&hearts", 9829);
-    add("&diams",  9830);
-    add("&quot",   34);
-    add("&amp",    38);
-    add("&lt",     60);
-    add("&gt",     62);
-    add("&OElig",  338);
-    add("&oelig",  339);
-    add("&Scaron", 352);
-    add("&scaron", 353);
-    add("&Yuml",   376);
-    add("&circ",   710);
-    add("&tilde",  732);
-    add("&ensp",   8194);
-    add("&emsp",   8195);
-    add("&thinsp", 8201);
-    add("&zwnj",   8204);
-    add("&zwj",    8205);
-    add("&lrm",    8206);
-    add("&rlm",    8207);
-    add("&ndash",  8211);
-    add("&mdash",  8212);
-    add("&lsquo",  8216);
-    add("&rsquo",  8217);
-    add("&sbquo",  8218);
-    add("&ldquo",  8220);
-    add("&rdquo",  8221);
-    add("&bdquo",  8222);
-    add("&dagger", 8224);
-    add("&Dagger", 8225);
-    add("&permil", 8240);
-    add("&lsaquo", 8249);
-    add("&rsaquo", 8250);
-    add("&euro",   8364);
-
-  }
-}
diff --git a/lucene/src/demo/org/apache/lucene/demo/html/HTMLParser.java b/lucene/src/demo/org/apache/lucene/demo/html/HTMLParser.java
deleted file mode 100644
index 4a89440..0000000
--- a/lucene/src/demo/org/apache/lucene/demo/html/HTMLParser.java
+++ /dev/null
@@ -1,755 +0,0 @@
-/* Generated By:JavaCC: Do not edit this line. HTMLParser.java */
-package org.apache.lucene.demo.html;
-
-import java.io.*;
-import java.util.Properties;
-
-public class HTMLParser implements HTMLParserConstants {
-  public static int SUMMARY_LENGTH = 200;
-
-  StringBuffer title = new StringBuffer(SUMMARY_LENGTH);
-  StringBuffer summary = new StringBuffer(SUMMARY_LENGTH * 2);
-  Properties metaTags=new Properties();
-  String currentMetaTag=null;
-  String currentMetaContent=null;
-  int length = 0;
-  boolean titleComplete = false;
-  boolean inTitle = false;
-  boolean inMetaTag = false;
-  boolean inStyle = false;
-  boolean afterTag = false;
-  boolean afterSpace = false;
-  String eol = System.getProperty("line.separator");
-  Reader pipeIn = null;
-  Writer pipeOut;
-  private MyPipedInputStream pipeInStream = null;
-  private PipedOutputStream pipeOutStream = null;
-
-  private class MyPipedInputStream extends PipedInputStream{
-
-    public MyPipedInputStream(){
-      super();
-    }
-
-    public MyPipedInputStream(PipedOutputStream src) throws IOException{
-      super(src);
-    }
-
-    public boolean full() throws IOException{
-      return this.available() >= PipedInputStream.PIPE_SIZE;
-    }
-  }
-
-  /**
-   * @deprecated Use HTMLParser(FileInputStream) instead
-   */
-  @Deprecated
-  public HTMLParser(File file) throws FileNotFoundException {
-    this(new FileInputStream(file));
-  }
-
-  public String getTitle() throws IOException, InterruptedException {
-    if (pipeIn == null)
-      getReader();                                // spawn parsing thread
-    while (true) {
-      synchronized(this) {
-        if (titleComplete || pipeInStream.full())
-          break;
-        wait(10);
-      }
-    }
-    return title.toString().trim();
-  }
-
-  public Properties getMetaTags() throws IOException,
-InterruptedException {
-    if (pipeIn == null)
-      getReader();                                // spawn parsing thread
-    while (true) {
-      synchronized(this) {
-        if (titleComplete || pipeInStream.full())
-          break;
-        wait(10);
-      }
-    }
-    return metaTags;
-  }
-
-
-  public String getSummary() throws IOException, InterruptedException {
-    if (pipeIn == null)
-      getReader();                                // spawn parsing thread
-    while (true) {
-      synchronized(this) {
-        if (summary.length() >= SUMMARY_LENGTH || pipeInStream.full())
-          break;
-        wait(10);
-      }
-    }
-    if (summary.length() > SUMMARY_LENGTH)
-      summary.setLength(SUMMARY_LENGTH);
-
-    String sum = summary.toString().trim();
-    String tit = getTitle();
-    if (sum.startsWith(tit) || sum.equals(""))
-      return tit;
-    else
-      return sum;
-  }
-
-  public Reader getReader() throws IOException {
-    if (pipeIn == null) {
-      pipeInStream = new MyPipedInputStream();
-      pipeOutStream = new PipedOutputStream(pipeInStream);
-      pipeIn = new InputStreamReader(pipeInStream, "UTF-16BE");
-      pipeOut = new OutputStreamWriter(pipeOutStream, "UTF-16BE");
-
-      Thread thread = new ParserThread(this);
-      thread.start();                             // start parsing
-    }
-
-    return pipeIn;
-  }
-
-  void addToSummary(String text) {
-    if (summary.length() < SUMMARY_LENGTH) {
-      summary.append(text);
-      if (summary.length() >= SUMMARY_LENGTH) {
-        synchronized(this) {
-          notifyAll();
-        }
-      }
-    }
-  }
-
-  void addText(String text) throws IOException {
-    if (inStyle)
-      return;
-    if (inTitle)
-      title.append(text);
-    else {
-      addToSummary(text);
-      if (!titleComplete && !(title.length() == 0)) {  // finished title
-        synchronized(this) {
-          titleComplete = true;                   // tell waiting threads
-          notifyAll();
-        }
-      }
-    }
-
-    length += text.length();
-    pipeOut.write(text);
-
-    afterSpace = false;
-  }
-
-  void addMetaTag() {
-      metaTags.setProperty(currentMetaTag, currentMetaContent);
-      currentMetaTag = null;
-      currentMetaContent = null;
-      return;
-  }
-
-  void addSpace() throws IOException {
-    if (!afterSpace) {
-      if (inTitle)
-        title.append(" ");
-      else
-        addToSummary(" ");
-
-      String space = afterTag ? eol : " ";
-      length += space.length();
-      pipeOut.write(space);
-      afterSpace = true;
-    }
-  }
-
-  final public void HTMLDocument() throws ParseException, IOException {
-  Token t;
-    label_1:
-    while (true) {
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case ScriptStart:
-      case TagName:
-      case DeclName:
-      case Comment1:
-      case Comment2:
-      case Word:
-      case Entity:
-      case Space:
-      case Punct:
-        ;
-        break;
-      default:
-        jj_la1[0] = jj_gen;
-        break label_1;
-      }
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case TagName:
-        Tag();
-                      afterTag = true;
-        break;
-      case DeclName:
-        t = Decl();
-                      afterTag = true;
-        break;
-      case Comment1:
-      case Comment2:
-        CommentTag();
-                      afterTag = true;
-        break;
-      case ScriptStart:
-        ScriptTag();
-                     afterTag = true;
-        break;
-      case Word:
-        t = jj_consume_token(Word);
-                      addText(t.image); afterTag = false;
-        break;
-      case Entity:
-        t = jj_consume_token(Entity);
-                      addText(Entities.decode(t.image)); afterTag = false;
-        break;
-      case Punct:
-        t = jj_consume_token(Punct);
-                      addText(t.image); afterTag = false;
-        break;
-      case Space:
-        jj_consume_token(Space);
-                      addSpace(); afterTag = false;
-        break;
-      default:
-        jj_la1[1] = jj_gen;
-        jj_consume_token(-1);
-        throw new ParseException();
-      }
-    }
-    jj_consume_token(0);
-  }
-
-  final public void Tag() throws ParseException, IOException {
-  Token t1, t2;
-  boolean inImg = false;
-    t1 = jj_consume_token(TagName);
-   String tagName = t1.image.toLowerCase();
-   if(Tags.WS_ELEMS.contains(tagName) ) {
-      addSpace();
-    }
-    inTitle = tagName.equalsIgnoreCase("<title"); // keep track if in <TITLE>
-    inMetaTag = tagName.equalsIgnoreCase("<META"); // keep track if in <META>
-    inStyle = tagName.equalsIgnoreCase("<STYLE"); // keep track if in <STYLE>
-    inImg = tagName.equalsIgnoreCase("<img");     // keep track if in <IMG>
-
-    label_2:
-    while (true) {
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case ArgName:
-        ;
-        break;
-      default:
-        jj_la1[2] = jj_gen;
-        break label_2;
-      }
-      t1 = jj_consume_token(ArgName);
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case ArgEquals:
-        jj_consume_token(ArgEquals);
-        switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-        case ArgValue:
-        case ArgQuote1:
-        case ArgQuote2:
-          t2 = ArgValue();
-       if (inImg && t1.image.equalsIgnoreCase("alt") && t2 != null)
-         addText("[" + t2.image + "]");
-
-        if(inMetaTag &&
-                        (  t1.image.equalsIgnoreCase("name") ||
-                           t1.image.equalsIgnoreCase("HTTP-EQUIV")
-                        )
-           && t2 != null)
-        {
-                currentMetaTag=t2.image.toLowerCase();
-                if(currentMetaTag != null && currentMetaContent != null) {
-                addMetaTag();
-                }
-        }
-        if(inMetaTag && t1.image.equalsIgnoreCase("content") && t2 !=
-null)
-        {
-                currentMetaContent=t2.image.toLowerCase();
-                if(currentMetaTag != null && currentMetaContent != null) {
-                addMetaTag();
-                }
-        }
-          break;
-        default:
-          jj_la1[3] = jj_gen;
-          ;
-        }
-        break;
-      default:
-        jj_la1[4] = jj_gen;
-        ;
-      }
-    }
-    jj_consume_token(TagEnd);
-  }
-
-  final public Token ArgValue() throws ParseException {
-  Token t = null;
-    switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-    case ArgValue:
-      t = jj_consume_token(ArgValue);
-                                              {if (true) return t;}
-      break;
-    default:
-      jj_la1[5] = jj_gen;
-      if (jj_2_1(2)) {
-        jj_consume_token(ArgQuote1);
-        jj_consume_token(CloseQuote1);
-                                              {if (true) return t;}
-      } else {
-        switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-        case ArgQuote1:
-          jj_consume_token(ArgQuote1);
-          t = jj_consume_token(Quote1Text);
-          jj_consume_token(CloseQuote1);
-                                              {if (true) return t;}
-          break;
-        default:
-          jj_la1[6] = jj_gen;
-          if (jj_2_2(2)) {
-            jj_consume_token(ArgQuote2);
-            jj_consume_token(CloseQuote2);
-                                              {if (true) return t;}
-          } else {
-            switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-            case ArgQuote2:
-              jj_consume_token(ArgQuote2);
-              t = jj_consume_token(Quote2Text);
-              jj_consume_token(CloseQuote2);
-                                              {if (true) return t;}
-              break;
-            default:
-              jj_la1[7] = jj_gen;
-              jj_consume_token(-1);
-              throw new ParseException();
-            }
-          }
-        }
-      }
-    }
-    throw new Error("Missing return statement in function");
-  }
-
-  final public Token Decl() throws ParseException {
-  Token t;
-    t = jj_consume_token(DeclName);
-    label_3:
-    while (true) {
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case ArgName:
-      case ArgEquals:
-      case ArgValue:
-      case ArgQuote1:
-      case ArgQuote2:
-        ;
-        break;
-      default:
-        jj_la1[8] = jj_gen;
-        break label_3;
-      }
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case ArgName:
-        jj_consume_token(ArgName);
-        break;
-      case ArgValue:
-      case ArgQuote1:
-      case ArgQuote2:
-        ArgValue();
-        break;
-      case ArgEquals:
-        jj_consume_token(ArgEquals);
-        break;
-      default:
-        jj_la1[9] = jj_gen;
-        jj_consume_token(-1);
-        throw new ParseException();
-      }
-    }
-    jj_consume_token(TagEnd);
-    {if (true) return t;}
-    throw new Error("Missing return statement in function");
-  }
-
-  final public void CommentTag() throws ParseException {
-    switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-    case Comment1:
-      jj_consume_token(Comment1);
-      label_4:
-      while (true) {
-        switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-        case CommentText1:
-          ;
-          break;
-        default:
-          jj_la1[10] = jj_gen;
-          break label_4;
-        }
-        jj_consume_token(CommentText1);
-      }
-      jj_consume_token(CommentEnd1);
-      break;
-    case Comment2:
-      jj_consume_token(Comment2);
-      label_5:
-      while (true) {
-        switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-        case CommentText2:
-          ;
-          break;
-        default:
-          jj_la1[11] = jj_gen;
-          break label_5;
-        }
-        jj_consume_token(CommentText2);
-      }
-      jj_consume_token(CommentEnd2);
-      break;
-    default:
-      jj_la1[12] = jj_gen;
-      jj_consume_token(-1);
-      throw new ParseException();
-    }
-  }
-
-  final public void ScriptTag() throws ParseException {
-    jj_consume_token(ScriptStart);
-    label_6:
-    while (true) {
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case ScriptText:
-        ;
-        break;
-      default:
-        jj_la1[13] = jj_gen;
-        break label_6;
-      }
-      jj_consume_token(ScriptText);
-    }
-    jj_consume_token(ScriptEnd);
-  }
-
-  private boolean jj_2_1(int xla) {
-    jj_la = xla; jj_lastpos = jj_scanpos = token;
-    try { return !jj_3_1(); }
-    catch(LookaheadSuccess ls) { return true; }
-    finally { jj_save(0, xla); }
-  }
-
-  private boolean jj_2_2(int xla) {
-    jj_la = xla; jj_lastpos = jj_scanpos = token;
-    try { return !jj_3_2(); }
-    catch(LookaheadSuccess ls) { return true; }
-    finally { jj_save(1, xla); }
-  }
-
-  private boolean jj_3_1() {
-    if (jj_scan_token(ArgQuote1)) return true;
-    if (jj_scan_token(CloseQuote1)) return true;
-    return false;
-  }
-
-  private boolean jj_3_2() {
-    if (jj_scan_token(ArgQuote2)) return true;
-    if (jj_scan_token(CloseQuote2)) return true;
-    return false;
-  }
-
-  /** Generated Token Manager. */
-  public HTMLParserTokenManager token_source;
-  SimpleCharStream jj_input_stream;
-  /** Current token. */
-  public Token token;
-  /** Next token. */
-  public Token jj_nt;
-  private int jj_ntk;
-  private Token jj_scanpos, jj_lastpos;
-  private int jj_la;
-  private int jj_gen;
-  final private int[] jj_la1 = new int[14];
-  static private int[] jj_la1_0;
-  static {
-      jj_la1_init_0();
-   }
-   private static void jj_la1_init_0() {
-      jj_la1_0 = new int[] {0x2c7e,0x2c7e,0x10000,0x380000,0x20000,0x80000,0x100000,0x200000,0x3b0000,0x3b0000,0x8000000,0x20000000,0x30,0x4000,};
-   }
-  final private JJCalls[] jj_2_rtns = new JJCalls[2];
-  private boolean jj_rescan = false;
-  private int jj_gc = 0;
-
-  /** Constructor with InputStream. */
-  public HTMLParser(java.io.InputStream stream) {
-     this(stream, null);
-  }
-  /** Constructor with InputStream and supplied encoding */
-  public HTMLParser(java.io.InputStream stream, String encoding) {
-    try { jj_input_stream = new SimpleCharStream(stream, encoding, 1, 1); } catch(java.io.UnsupportedEncodingException e) { throw new RuntimeException(e); }
-    token_source = new HTMLParserTokenManager(jj_input_stream);
-    token = new Token();
-    jj_ntk = -1;
-    jj_gen = 0;
-    for (int i = 0; i < 14; i++) jj_la1[i] = -1;
-    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
-  }
-
-  /** Reinitialise. */
-  public void ReInit(java.io.InputStream stream) {
-     ReInit(stream, null);
-  }
-  /** Reinitialise. */
-  public void ReInit(java.io.InputStream stream, String encoding) {
-    try { jj_input_stream.ReInit(stream, encoding, 1, 1); } catch(java.io.UnsupportedEncodingException e) { throw new RuntimeException(e); }
-    token_source.ReInit(jj_input_stream);
-    token = new Token();
-    jj_ntk = -1;
-    jj_gen = 0;
-    for (int i = 0; i < 14; i++) jj_la1[i] = -1;
-    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
-  }
-
-  /** Constructor. */
-  public HTMLParser(java.io.Reader stream) {
-    jj_input_stream = new SimpleCharStream(stream, 1, 1);
-    token_source = new HTMLParserTokenManager(jj_input_stream);
-    token = new Token();
-    jj_ntk = -1;
-    jj_gen = 0;
-    for (int i = 0; i < 14; i++) jj_la1[i] = -1;
-    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
-  }
-
-  /** Reinitialise. */
-  public void ReInit(java.io.Reader stream) {
-    jj_input_stream.ReInit(stream, 1, 1);
-    token_source.ReInit(jj_input_stream);
-    token = new Token();
-    jj_ntk = -1;
-    jj_gen = 0;
-    for (int i = 0; i < 14; i++) jj_la1[i] = -1;
-    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
-  }
-
-  /** Constructor with generated Token Manager. */
-  public HTMLParser(HTMLParserTokenManager tm) {
-    token_source = tm;
-    token = new Token();
-    jj_ntk = -1;
-    jj_gen = 0;
-    for (int i = 0; i < 14; i++) jj_la1[i] = -1;
-    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
-  }
-
-  /** Reinitialise. */
-  public void ReInit(HTMLParserTokenManager tm) {
-    token_source = tm;
-    token = new Token();
-    jj_ntk = -1;
-    jj_gen = 0;
-    for (int i = 0; i < 14; i++) jj_la1[i] = -1;
-    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
-  }
-
-  private Token jj_consume_token(int kind) throws ParseException {
-    Token oldToken;
-    if ((oldToken = token).next != null) token = token.next;
-    else token = token.next = token_source.getNextToken();
-    jj_ntk = -1;
-    if (token.kind == kind) {
-      jj_gen++;
-      if (++jj_gc > 100) {
-        jj_gc = 0;
-        for (int i = 0; i < jj_2_rtns.length; i++) {
-          JJCalls c = jj_2_rtns[i];
-          while (c != null) {
-            if (c.gen < jj_gen) c.first = null;
-            c = c.next;
-          }
-        }
-      }
-      return token;
-    }
-    token = oldToken;
-    jj_kind = kind;
-    throw generateParseException();
-  }
-
-  static private final class LookaheadSuccess extends java.lang.Error { }
-  final private LookaheadSuccess jj_ls = new LookaheadSuccess();
-  private boolean jj_scan_token(int kind) {
-    if (jj_scanpos == jj_lastpos) {
-      jj_la--;
-      if (jj_scanpos.next == null) {
-        jj_lastpos = jj_scanpos = jj_scanpos.next = token_source.getNextToken();
-      } else {
-        jj_lastpos = jj_scanpos = jj_scanpos.next;
-      }
-    } else {
-      jj_scanpos = jj_scanpos.next;
-    }
-    if (jj_rescan) {
-      int i = 0; Token tok = token;
-      while (tok != null && tok != jj_scanpos) { i++; tok = tok.next; }
-      if (tok != null) jj_add_error_token(kind, i);
-    }
-    if (jj_scanpos.kind != kind) return true;
-    if (jj_la == 0 && jj_scanpos == jj_lastpos) throw jj_ls;
-    return false;
-  }
-
-
-/** Get the next Token. */
-  final public Token getNextToken() {
-    if (token.next != null) token = token.next;
-    else token = token.next = token_source.getNextToken();
-    jj_ntk = -1;
-    jj_gen++;
-    return token;
-  }
-
-/** Get the specific Token. */
-  final public Token getToken(int index) {
-    Token t = token;
-    for (int i = 0; i < index; i++) {
-      if (t.next != null) t = t.next;
-      else t = t.next = token_source.getNextToken();
-    }
-    return t;
-  }
-
-  private int jj_ntk() {
-    if ((jj_nt=token.next) == null)
-      return (jj_ntk = (token.next=token_source.getNextToken()).kind);
-    else
-      return (jj_ntk = jj_nt.kind);
-  }
-
-  private java.util.List<int[]> jj_expentries = new java.util.ArrayList<int[]>();
-  private int[] jj_expentry;
-  private int jj_kind = -1;
-  private int[] jj_lasttokens = new int[100];
-  private int jj_endpos;
-
-  private void jj_add_error_token(int kind, int pos) {
-    if (pos >= 100) return;
-    if (pos == jj_endpos + 1) {
-      jj_lasttokens[jj_endpos++] = kind;
-    } else if (jj_endpos != 0) {
-      jj_expentry = new int[jj_endpos];
-      for (int i = 0; i < jj_endpos; i++) {
-        jj_expentry[i] = jj_lasttokens[i];
-      }
-      jj_entries_loop: for (java.util.Iterator it = jj_expentries.iterator(); it.hasNext();) {
-        int[] oldentry = (int[])(it.next());
-        if (oldentry.length == jj_expentry.length) {
-          for (int i = 0; i < jj_expentry.length; i++) {
-            if (oldentry[i] != jj_expentry[i]) {
-              continue jj_entries_loop;
-            }
-          }
-          jj_expentries.add(jj_expentry);
-          break jj_entries_loop;
-        }
-      }
-      if (pos != 0) jj_lasttokens[(jj_endpos = pos) - 1] = kind;
-    }
-  }
-
-  /** Generate ParseException. */
-  public ParseException generateParseException() {
-    jj_expentries.clear();
-    boolean[] la1tokens = new boolean[31];
-    if (jj_kind >= 0) {
-      la1tokens[jj_kind] = true;
-      jj_kind = -1;
-    }
-    for (int i = 0; i < 14; i++) {
-      if (jj_la1[i] == jj_gen) {
-        for (int j = 0; j < 32; j++) {
-          if ((jj_la1_0[i] & (1<<j)) != 0) {
-            la1tokens[j] = true;
-          }
-        }
-      }
-    }
-    for (int i = 0; i < 31; i++) {
-      if (la1tokens[i]) {
-        jj_expentry = new int[1];
-        jj_expentry[0] = i;
-        jj_expentries.add(jj_expentry);
-      }
-    }
-    jj_endpos = 0;
-    jj_rescan_token();
-    jj_add_error_token(0, 0);
-    int[][] exptokseq = new int[jj_expentries.size()][];
-    for (int i = 0; i < jj_expentries.size(); i++) {
-      exptokseq[i] = jj_expentries.get(i);
-    }
-    return new ParseException(token, exptokseq, tokenImage);
-  }
-
-  /** Enable tracing. */
-  final public void enable_tracing() {
-  }
-
-  /** Disable tracing. */
-  final public void disable_tracing() {
-  }
-
-  private void jj_rescan_token() {
-    jj_rescan = true;
-    for (int i = 0; i < 2; i++) {
-    try {
-      JJCalls p = jj_2_rtns[i];
-      do {
-        if (p.gen > jj_gen) {
-          jj_la = p.arg; jj_lastpos = jj_scanpos = p.first;
-          switch (i) {
-            case 0: jj_3_1(); break;
-            case 1: jj_3_2(); break;
-          }
-        }
-        p = p.next;
-      } while (p != null);
-      } catch(LookaheadSuccess ls) { }
-    }
-    jj_rescan = false;
-  }
-
-  private void jj_save(int index, int xla) {
-    JJCalls p = jj_2_rtns[index];
-    while (p.gen > jj_gen) {
-      if (p.next == null) { p = p.next = new JJCalls(); break; }
-      p = p.next;
-    }
-    p.gen = jj_gen + xla - jj_la; p.first = token; p.arg = xla;
-  }
-
-  static final class JJCalls {
-    int gen;
-    Token first;
-    int arg;
-    JJCalls next;
-  }
-
-//    void handleException(Exception e) {
-//      System.out.println(e.toString());  // print the error message
-//      System.out.println("Skipping...");
-//      Token t;
-//      do {
-//        t = getNextToken();
-//      } while (t.kind != TagEnd);
-//    }
-}
diff --git a/lucene/src/demo/org/apache/lucene/demo/html/HTMLParser.jj b/lucene/src/demo/org/apache/lucene/demo/html/HTMLParser.jj
deleted file mode 100644
index 6a3578c..0000000
--- a/lucene/src/demo/org/apache/lucene/demo/html/HTMLParser.jj
+++ /dev/null
@@ -1,393 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-// HTMLParser.jj
-
-options {
-  STATIC = false;
-  OPTIMIZE_TOKEN_MANAGER = true;
-  //DEBUG_LOOKAHEAD = true;
-  //DEBUG_TOKEN_MANAGER = true;
-}
-
-PARSER_BEGIN(HTMLParser)
-
-package org.apache.lucene.demo.html;
-
-import java.io.*;
-import java.util.Properties;
-
-public class HTMLParser {
-  public static int SUMMARY_LENGTH = 200;
-
-  StringBuffer title = new StringBuffer(SUMMARY_LENGTH);
-  StringBuffer summary = new StringBuffer(SUMMARY_LENGTH * 2);
-  Properties metaTags=new Properties();
-  String currentMetaTag=null;
-  String currentMetaContent=null;
-  int length = 0;
-  boolean titleComplete = false;
-  boolean inTitle = false;
-  boolean inMetaTag = false;
-  boolean inStyle = false;
-  boolean afterTag = false;
-  boolean afterSpace = false;
-  String eol = System.getProperty("line.separator");
-  Reader pipeIn = null;
-  Writer pipeOut;
-  private MyPipedInputStream pipeInStream = null;
-  private PipedOutputStream pipeOutStream = null;
-  
-  private class MyPipedInputStream extends PipedInputStream{
-    
-    public MyPipedInputStream(){
-      super();
-    }
-    
-    public MyPipedInputStream(PipedOutputStream src) throws IOException{
-      super(src);
-    }
-    
-    public boolean full() throws IOException{
-      return this.available() >= PipedInputStream.PIPE_SIZE;
-    }
-  }
-
-  /**
-   * @deprecated Use HTMLParser(FileInputStream) instead
-   */
-  @Deprecated
-  public HTMLParser(File file) throws FileNotFoundException {
-    this(new FileInputStream(file));
-  }
-
-  public String getTitle() throws IOException, InterruptedException {
-    if (pipeIn == null)
-      getReader();				  // spawn parsing thread
-    while (true) {
-      synchronized(this) {
-	if (titleComplete || pipeInStream.full())
-	  break;
-	wait(10);
-      }
-    }
-    return title.toString().trim();
-  }
-
-  public Properties getMetaTags() throws IOException,
-InterruptedException {
-    if (pipeIn == null)
-      getReader();				  // spawn parsing thread
-    while (true) {
-      synchronized(this) {
-	if (titleComplete || pipeInStream.full())
-	  break;
-	wait(10);
-      }
-    }
-    return metaTags;
-  }
-
-
-  public String getSummary() throws IOException, InterruptedException {
-    if (pipeIn == null)
-      getReader();				  // spawn parsing thread
-    while (true) {
-      synchronized(this) {
-	if (summary.length() >= SUMMARY_LENGTH || pipeInStream.full())
-	  break;
-	wait(10);
-      }
-    }
-    if (summary.length() > SUMMARY_LENGTH)
-      summary.setLength(SUMMARY_LENGTH);
-
-    String sum = summary.toString().trim();
-    String tit = getTitle();
-    if (sum.startsWith(tit) || sum.equals(""))
-      return tit;
-    else
-      return sum;
-  }
-
-  public Reader getReader() throws IOException {
-    if (pipeIn == null) {
-      pipeInStream = new MyPipedInputStream();
-      pipeOutStream = new PipedOutputStream(pipeInStream);
-      pipeIn = new InputStreamReader(pipeInStream, "UTF-16BE");
-      pipeOut = new OutputStreamWriter(pipeOutStream, "UTF-16BE");
-
-      Thread thread = new ParserThread(this);
-      thread.start();				  // start parsing
-    }
-
-    return pipeIn;
-  }
-
-  void addToSummary(String text) {
-    if (summary.length() < SUMMARY_LENGTH) {
-      summary.append(text);
-      if (summary.length() >= SUMMARY_LENGTH) {
-	synchronized(this) {
-	  notifyAll();
-	}
-      }
-    }
-  }
-
-  void addText(String text) throws IOException {
-    if (inStyle)
-      return;
-    if (inTitle)
-      title.append(text);
-    else {
-      addToSummary(text);
-      if (!titleComplete && !(title.length() == 0)) {  // finished title
-	synchronized(this) {
-	  titleComplete = true;			  // tell waiting threads
-	  notifyAll();
-	}
-      }
-    }
-
-    length += text.length();
-    pipeOut.write(text);
-
-    afterSpace = false;
-  }
-  
-  void addMetaTag() {
-      metaTags.setProperty(currentMetaTag, currentMetaContent);
-      currentMetaTag = null;
-      currentMetaContent = null;
-      return;
-  }
-
-  void addSpace() throws IOException {
-    if (!afterSpace) {
-      if (inTitle)
-	title.append(" ");
-      else
-	addToSummary(" ");
-
-      String space = afterTag ? eol : " ";
-      length += space.length();
-      pipeOut.write(space);
-      afterSpace = true;
-    }
-  }
-
-//    void handleException(Exception e) {
-//      System.out.println(e.toString());  // print the error message
-//      System.out.println("Skipping...");
-//      Token t;
-//      do {
-//        t = getNextToken();
-//      } while (t.kind != TagEnd);
-//    }
-}
-
-PARSER_END(HTMLParser)
-
-
-void HTMLDocument() throws IOException :
-{
-  Token t;
-}
-{
-//  try {
-    ( Tag()         { afterTag = true; }
-    | t=Decl()      { afterTag = true; }
-    | CommentTag()  { afterTag = true; }
-    | ScriptTag()  { afterTag = true; }
-    | t=<Word>      { addText(t.image); afterTag = false; }
-    | t=<Entity>    { addText(Entities.decode(t.image)); afterTag = false; }
-    | t=<Punct>     { addText(t.image); afterTag = false; }
-    | <Space>       { addSpace(); afterTag = false; }
-    )* <EOF>
-//  } catch (ParseException e) {
-//    handleException(e);
-//  }
-}
-
-void Tag() throws IOException :
-{
-  Token t1, t2;
-  boolean inImg = false;
-}
-{
-  t1=<TagName> {
-   String tagName = t1.image.toLowerCase();
-   if(Tags.WS_ELEMS.contains(tagName) ) {
-      addSpace();
-    }
-    inTitle = tagName.equalsIgnoreCase("<title"); // keep track if in <TITLE>
-    inMetaTag = tagName.equalsIgnoreCase("<META"); // keep track if in <META>
-    inStyle = tagName.equalsIgnoreCase("<STYLE"); // keep track if in <STYLE>
-    inImg = tagName.equalsIgnoreCase("<img");	  // keep track if in <IMG>
-  }
-  (t1=<ArgName>
-   (<ArgEquals>
-    (t2=ArgValue()				  // save ALT text in IMG tag
-     {
-       if (inImg && t1.image.equalsIgnoreCase("alt") && t2 != null)
-         addText("[" + t2.image + "]");
-
-    	if(inMetaTag &&
-			(  t1.image.equalsIgnoreCase("name") ||
-			   t1.image.equalsIgnoreCase("HTTP-EQUIV")
-			)
-	   && t2 != null)
-	{
-		currentMetaTag=t2.image.toLowerCase();
-		if(currentMetaTag != null && currentMetaContent != null) {
-        	addMetaTag();
-		}
-	}
-    	if(inMetaTag && t1.image.equalsIgnoreCase("content") && t2 !=
-null)
-	{
-		currentMetaContent=t2.image.toLowerCase();
-		if(currentMetaTag != null && currentMetaContent != null) {
-        	addMetaTag();
-		}
-	}
-     }
-    )?
-   )?
-  )*
-  <TagEnd>
-}
-
-Token ArgValue() :
-{
-  Token t = null;
-}
-{
-  t=<ArgValue>                              { return t; }
-| LOOKAHEAD(2)
-  <ArgQuote1> <CloseQuote1>                 { return t; }
-| <ArgQuote1> t=<Quote1Text> <CloseQuote1>  { return t; }
-| LOOKAHEAD(2)
-  <ArgQuote2> <CloseQuote2>                 { return t; }
-| <ArgQuote2> t=<Quote2Text> <CloseQuote2>  { return t; }
-}
-
-
-Token Decl() :
-{
-  Token t;
-}
-{
-  t=<DeclName> ( <ArgName> | ArgValue() | <ArgEquals> )* <TagEnd>
-  { return t; }
-}
-
-
-void CommentTag() :
-{}
-{
-  (<Comment1> ( <CommentText1> )* <CommentEnd1>)
- |
-  (<Comment2> ( <CommentText2> )* <CommentEnd2>)
-}
-
-void ScriptTag() :
-{}
-{
-  <ScriptStart> ( <ScriptText> )* <ScriptEnd>
-}
-
-
-TOKEN :
-{
-  < ScriptStart: "<script" > : WithinScript
-| < TagName:  "<" ("/")? ["A"-"Z","a"-"z"] (<ArgName>)? > : WithinTag
-| < DeclName: "<"  "!"   ["A"-"Z","a"-"z"] (<ArgName>)? > : WithinTag
-
-| < Comment1:  "<!--" > : WithinComment1
-| < Comment2:  "<!" >   : WithinComment2
-
-| < Word:     ( <LET> | <LET> (["+","/"])+ | <NUM> ["\""] |
-                <LET> ["-","'"] <LET> | ("$")? <NUM> [",","."] <NUM> )+ >
-| < #LET:     ["A"-"Z","a"-"z","0"-"9"] >
-| < #NUM:     ["0"-"9"] >
-| < #HEX:     ["0"-"9","A"-"F","a"-"f"] >
-
-| < Entity:   ( "&" (["A"-"Z","a"-"z"])+ (";")? | "&" "#" (<NUM>)+ (";")? | "&" "#" ["X","x"] (<HEX>)+ (";")? ) >
-
-| < Space:    (<SP>)+ >
-| < #SP:      [" ","\t","\r","\n"] >
-
-| < Punct:    ~[] > // Keep this last.  It is a catch-all.
-}
-
-<WithinScript> TOKEN:
-{
-  < ScriptText:  (~["<",">"])+ | "<" | ">" >
-| < ScriptEnd: "</script" (~["<",">"])* ">" > : DEFAULT
-}
-
-<WithinTag> TOKEN:
-{
-  < ArgName:   (~[" ","\t","\r","\n","=",">","'","\""])
-               (~[" ","\t","\r","\n","=",">"])* >
-| < ArgEquals: "=" >  : AfterEquals
-| < TagEnd:    ">" | "=>" >  : DEFAULT
-}
-
-<AfterEquals> TOKEN:
-{
-  < ArgValue:  (~[" ","\t","\r","\n","=",">","'","\""])
-	       (~[" ","\t","\r","\n",">"])* > : WithinTag
-}
-
-<WithinTag, AfterEquals> TOKEN:
-{
-  < ArgQuote1: "'"  > : WithinQuote1
-| < ArgQuote2: "\"" > : WithinQuote2
-}
-
-<WithinTag, AfterEquals> SKIP:
-{
-  < <Space> >
-}
-
-<WithinQuote1> TOKEN:
-{
-  < Quote1Text:  (~["'"])+ >
-| < CloseQuote1: <ArgQuote1> > : WithinTag
-}
-
-<WithinQuote2> TOKEN:
-{
-  < Quote2Text:  (~["\""])+ >
-| < CloseQuote2: <ArgQuote2> > : WithinTag
-}
-
-
-<WithinComment1> TOKEN :
-{
-  < CommentText1:  (~["-"])+ | "-" >
-| < CommentEnd1:   "-->" > : DEFAULT
-}
-
-<WithinComment2> TOKEN :
-{
-  < CommentText2:  (~[">"])+ >
-| < CommentEnd2:   ">" > : DEFAULT
-}
diff --git a/lucene/src/demo/org/apache/lucene/demo/html/HTMLParserConstants.java b/lucene/src/demo/org/apache/lucene/demo/html/HTMLParserConstants.java
deleted file mode 100644
index bf11b45..0000000
--- a/lucene/src/demo/org/apache/lucene/demo/html/HTMLParserConstants.java
+++ /dev/null
@@ -1,124 +0,0 @@
-/* Generated By:JavaCC: Do not edit this line. HTMLParserConstants.java */
-package org.apache.lucene.demo.html;
-
-
-/**
- * Token literal values and constants.
- * Generated by org.javacc.parser.OtherFilesGen#start()
- */
-public interface HTMLParserConstants {
-
-  /** End of File. */
-  int EOF = 0;
-  /** RegularExpression Id. */
-  int ScriptStart = 1;
-  /** RegularExpression Id. */
-  int TagName = 2;
-  /** RegularExpression Id. */
-  int DeclName = 3;
-  /** RegularExpression Id. */
-  int Comment1 = 4;
-  /** RegularExpression Id. */
-  int Comment2 = 5;
-  /** RegularExpression Id. */
-  int Word = 6;
-  /** RegularExpression Id. */
-  int LET = 7;
-  /** RegularExpression Id. */
-  int NUM = 8;
-  /** RegularExpression Id. */
-  int HEX = 9;
-  /** RegularExpression Id. */
-  int Entity = 10;
-  /** RegularExpression Id. */
-  int Space = 11;
-  /** RegularExpression Id. */
-  int SP = 12;
-  /** RegularExpression Id. */
-  int Punct = 13;
-  /** RegularExpression Id. */
-  int ScriptText = 14;
-  /** RegularExpression Id. */
-  int ScriptEnd = 15;
-  /** RegularExpression Id. */
-  int ArgName = 16;
-  /** RegularExpression Id. */
-  int ArgEquals = 17;
-  /** RegularExpression Id. */
-  int TagEnd = 18;
-  /** RegularExpression Id. */
-  int ArgValue = 19;
-  /** RegularExpression Id. */
-  int ArgQuote1 = 20;
-  /** RegularExpression Id. */
-  int ArgQuote2 = 21;
-  /** RegularExpression Id. */
-  int Quote1Text = 23;
-  /** RegularExpression Id. */
-  int CloseQuote1 = 24;
-  /** RegularExpression Id. */
-  int Quote2Text = 25;
-  /** RegularExpression Id. */
-  int CloseQuote2 = 26;
-  /** RegularExpression Id. */
-  int CommentText1 = 27;
-  /** RegularExpression Id. */
-  int CommentEnd1 = 28;
-  /** RegularExpression Id. */
-  int CommentText2 = 29;
-  /** RegularExpression Id. */
-  int CommentEnd2 = 30;
-
-  /** Lexical state. */
-  int DEFAULT = 0;
-  /** Lexical state. */
-  int WithinScript = 1;
-  /** Lexical state. */
-  int WithinTag = 2;
-  /** Lexical state. */
-  int AfterEquals = 3;
-  /** Lexical state. */
-  int WithinQuote1 = 4;
-  /** Lexical state. */
-  int WithinQuote2 = 5;
-  /** Lexical state. */
-  int WithinComment1 = 6;
-  /** Lexical state. */
-  int WithinComment2 = 7;
-
-  /** Literal token values. */
-  String[] tokenImage = {
-    "<EOF>",
-    "\"<script\"",
-    "<TagName>",
-    "<DeclName>",
-    "\"<!--\"",
-    "\"<!\"",
-    "<Word>",
-    "<LET>",
-    "<NUM>",
-    "<HEX>",
-    "<Entity>",
-    "<Space>",
-    "<SP>",
-    "<Punct>",
-    "<ScriptText>",
-    "<ScriptEnd>",
-    "<ArgName>",
-    "\"=\"",
-    "<TagEnd>",
-    "<ArgValue>",
-    "\"\\\'\"",
-    "\"\\\"\"",
-    "<token of kind 22>",
-    "<Quote1Text>",
-    "<CloseQuote1>",
-    "<Quote2Text>",
-    "<CloseQuote2>",
-    "<CommentText1>",
-    "\"-->\"",
-    "<CommentText2>",
-    "\">\"",
-  };
-
-}
diff --git a/lucene/src/demo/org/apache/lucene/demo/html/HTMLParserTokenManager.java b/lucene/src/demo/org/apache/lucene/demo/html/HTMLParserTokenManager.java
deleted file mode 100644
index d6064d5..0000000
--- a/lucene/src/demo/org/apache/lucene/demo/html/HTMLParserTokenManager.java
+++ /dev/null
@@ -1,1619 +0,0 @@
-/* Generated By:JavaCC: Do not edit this line. HTMLParserTokenManager.java */
-package org.apache.lucene.demo.html;
-import java.io.*;
-import java.util.Properties;
-
-/** Token Manager. */
-public class HTMLParserTokenManager implements HTMLParserConstants
-{
-
-  /** Debug output. */
-  public  java.io.PrintStream debugStream = System.out;
-  /** Set debug output. */
-  public  void setDebugStream(java.io.PrintStream ds) { debugStream = ds; }
-private final int jjStopStringLiteralDfa_0(int pos, long active0)
-{
-   switch (pos)
-   {
-      case 0:
-         if ((active0 & 0x32L) != 0L)
-            return 20;
-         return -1;
-      case 1:
-         if ((active0 & 0x2L) != 0L)
-         {
-            if (jjmatchedPos != 1)
-            {
-               jjmatchedKind = 2;
-               jjmatchedPos = 1;
-            }
-            return 22;
-         }
-         if ((active0 & 0x30L) != 0L)
-            return 25;
-         return -1;
-      case 2:
-         if ((active0 & 0x2L) != 0L)
-         {
-            jjmatchedKind = 2;
-            jjmatchedPos = 2;
-            return 23;
-         }
-         return -1;
-      case 3:
-         if ((active0 & 0x2L) != 0L)
-         {
-            jjmatchedKind = 2;
-            jjmatchedPos = 3;
-            return 23;
-         }
-         return -1;
-      case 4:
-         if ((active0 & 0x2L) != 0L)
-         {
-            jjmatchedKind = 2;
-            jjmatchedPos = 4;
-            return 23;
-         }
-         return -1;
-      case 5:
-         if ((active0 & 0x2L) != 0L)
-         {
-            jjmatchedKind = 2;
-            jjmatchedPos = 5;
-            return 23;
-         }
-         return -1;
-      default :
-         return -1;
-   }
-}
-private final int jjStartNfa_0(int pos, long active0)
-{
-   return jjMoveNfa_0(jjStopStringLiteralDfa_0(pos, active0), pos + 1);
-}
-private int jjStopAtPos(int pos, int kind)
-{
-   jjmatchedKind = kind;
-   jjmatchedPos = pos;
-   return pos + 1;
-}
-private int jjMoveStringLiteralDfa0_0()
-{
-   switch(curChar)
-   {
-      case 60:
-         return jjMoveStringLiteralDfa1_0(0x32L);
-      default :
-         return jjMoveNfa_0(11, 0);
-   }
-}
-private int jjMoveStringLiteralDfa1_0(long active0)
-{
-   try { curChar = input_stream.readChar(); }
-   catch(java.io.IOException e) {
-      jjStopStringLiteralDfa_0(0, active0);
-      return 1;
-   }
-   switch(curChar)
-   {
-      case 33:
-         if ((active0 & 0x20L) != 0L)
-         {
-            jjmatchedKind = 5;
-            jjmatchedPos = 1;
-         }
-         return jjMoveStringLiteralDfa2_0(active0, 0x10L);
-      case 115:
-         return jjMoveStringLiteralDfa2_0(active0, 0x2L);
-      default :
-         break;
-   }
-   return jjStartNfa_0(0, active0);
-}
-private int jjMoveStringLiteralDfa2_0(long old0, long active0)
-{
-   if (((active0 &= old0)) == 0L)
-      return jjStartNfa_0(0, old0);
-   try { curChar = input_stream.readChar(); }
-   catch(java.io.IOException e) {
-      jjStopStringLiteralDfa_0(1, active0);
-      return 2;
-   }
-   switch(curChar)
-   {
-      case 45:
-         return jjMoveStringLiteralDfa3_0(active0, 0x10L);
-      case 99:
-         return jjMoveStringLiteralDfa3_0(active0, 0x2L);
-      default :
-         break;
-   }
-   return jjStartNfa_0(1, active0);
-}
-private int jjMoveStringLiteralDfa3_0(long old0, long active0)
-{
-   if (((active0 &= old0)) == 0L)
-      return jjStartNfa_0(1, old0);
-   try { curChar = input_stream.readChar(); }
-   catch(java.io.IOException e) {
-      jjStopStringLiteralDfa_0(2, active0);
-      return 3;
-   }
-   switch(curChar)
-   {
-      case 45:
-         if ((active0 & 0x10L) != 0L)
-            return jjStopAtPos(3, 4);
-         break;
-      case 114:
-         return jjMoveStringLiteralDfa4_0(active0, 0x2L);
-      default :
-         break;
-   }
-   return jjStartNfa_0(2, active0);
-}
-private int jjMoveStringLiteralDfa4_0(long old0, long active0)
-{
-   if (((active0 &= old0)) == 0L)
-      return jjStartNfa_0(2, old0);
-   try { curChar = input_stream.readChar(); }
-   catch(java.io.IOException e) {
-      jjStopStringLiteralDfa_0(3, active0);
-      return 4;
-   }
-   switch(curChar)
-   {
-      case 105:
-         return jjMoveStringLiteralDfa5_0(active0, 0x2L);
-      default :
-         break;
-   }
-   return jjStartNfa_0(3, active0);
-}
-private int jjMoveStringLiteralDfa5_0(long old0, long active0)
-{
-   if (((active0 &= old0)) == 0L)
-      return jjStartNfa_0(3, old0);
-   try { curChar = input_stream.readChar(); }
-   catch(java.io.IOException e) {
-      jjStopStringLiteralDfa_0(4, active0);
-      return 5;
-   }
-   switch(curChar)
-   {
-      case 112:
-         return jjMoveStringLiteralDfa6_0(active0, 0x2L);
-      default :
-         break;
-   }
-   return jjStartNfa_0(4, active0);
-}
-private int jjMoveStringLiteralDfa6_0(long old0, long active0)
-{
-   if (((active0 &= old0)) == 0L)
-      return jjStartNfa_0(4, old0);
-   try { curChar = input_stream.readChar(); }
-   catch(java.io.IOException e) {
-      jjStopStringLiteralDfa_0(5, active0);
-      return 6;
-   }
-   switch(curChar)
-   {
-      case 116:
-         if ((active0 & 0x2L) != 0L)
-            return jjStartNfaWithStates_0(6, 1, 23);
-         break;
-      default :
-         break;
-   }
-   return jjStartNfa_0(5, active0);
-}
-private int jjStartNfaWithStates_0(int pos, int kind, int state)
-{
-   jjmatchedKind = kind;
-   jjmatchedPos = pos;
-   try { curChar = input_stream.readChar(); }
-   catch(java.io.IOException e) { return pos + 1; }
-   return jjMoveNfa_0(state, pos + 1);
-}
-static final long[] jjbitVec0 = {
-   0x0L, 0x0L, 0xffffffffffffffffL, 0xffffffffffffffffL
-};
-private int jjMoveNfa_0(int startState, int curPos)
-{
-   int startsAt = 0;
-   jjnewStateCnt = 28;
-   int i = 1;
-   jjstateSet[0] = startState;
-   int kind = 0x7fffffff;
-   for (;;)
-   {
-      if (++jjround == 0x7fffffff)
-         ReInitRounds();
-      if (curChar < 64)
-      {
-         long l = 1L << curChar;
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 20:
-                  if (curChar == 33)
-                     jjstateSet[jjnewStateCnt++] = 25;
-                  else if (curChar == 47)
-                     jjCheckNAdd(21);
-                  break;
-               case 11:
-                  if ((0x3ff000000000000L & l) != 0L)
-                     jjCheckNAddTwoStates(7, 2);
-                  else if ((0x100002600L & l) != 0L)
-                  {
-                     if (kind > 11)
-                        kind = 11;
-                     jjCheckNAdd(10);
-                  }
-                  else if (curChar == 60)
-                     jjCheckNAddStates(0, 2);
-                  else if (curChar == 38)
-                     jjAddStates(3, 5);
-                  else if (curChar == 36)
-                     jjstateSet[jjnewStateCnt++] = 1;
-                  if ((0x3ff000000000000L & l) != 0L)
-                  {
-                     if (kind > 6)
-                        kind = 6;
-                     jjCheckNAddStates(6, 10);
-                  }
-                  break;
-               case 0:
-                  if (curChar == 36)
-                     jjstateSet[jjnewStateCnt++] = 1;
-                  break;
-               case 1:
-                  if ((0x3ff000000000000L & l) != 0L)
-                     jjCheckNAdd(2);
-                  break;
-               case 2:
-                  if ((0x500000000000L & l) != 0L)
-                     jjstateSet[jjnewStateCnt++] = 3;
-                  break;
-               case 3:
-               case 9:
-                  if ((0x3ff000000000000L & l) == 0L)
-                     break;
-                  if (kind > 6)
-                     kind = 6;
-                  jjCheckNAddStates(11, 13);
-                  break;
-               case 4:
-                  if ((0x3ff000000000000L & l) == 0L)
-                     break;
-                  if (kind > 6)
-                     kind = 6;
-                  jjCheckNAddStates(6, 10);
-                  break;
-               case 5:
-                  if ((0x880000000000L & l) == 0L)
-                     break;
-                  if (kind > 6)
-                     kind = 6;
-                  jjCheckNAddStates(14, 17);
-                  break;
-               case 6:
-                  if ((0x3ff000000000000L & l) != 0L)
-                     jjCheckNAddTwoStates(7, 2);
-                  break;
-               case 7:
-                  if (curChar != 34)
-                     break;
-                  if (kind > 6)
-                     kind = 6;
-                  jjCheckNAddStates(11, 13);
-                  break;
-               case 8:
-                  if ((0x208000000000L & l) != 0L)
-                     jjstateSet[jjnewStateCnt++] = 9;
-                  break;
-               case 10:
-                  if ((0x100002600L & l) == 0L)
-                     break;
-                  kind = 11;
-                  jjCheckNAdd(10);
-                  break;
-               case 13:
-                  if (curChar == 59 && kind > 10)
-                     kind = 10;
-                  break;
-               case 14:
-                  if (curChar == 35)
-                     jjCheckNAdd(15);
-                  break;
-               case 15:
-                  if ((0x3ff000000000000L & l) == 0L)
-                     break;
-                  if (kind > 10)
-                     kind = 10;
-                  jjCheckNAddTwoStates(15, 13);
-                  break;
-               case 16:
-                  if (curChar == 35)
-                     jjstateSet[jjnewStateCnt++] = 17;
-                  break;
-               case 18:
-                  if ((0x3ff000000000000L & l) == 0L)
-                     break;
-                  if (kind > 10)
-                     kind = 10;
-                  jjCheckNAddTwoStates(18, 13);
-                  break;
-               case 19:
-                  if (curChar == 60)
-                     jjCheckNAddStates(0, 2);
-                  break;
-               case 22:
-                  if ((0x9fffff7affffd9ffL & l) == 0L)
-                     break;
-                  if (kind > 2)
-                     kind = 2;
-                  jjCheckNAdd(23);
-                  break;
-               case 23:
-                  if ((0x9ffffffeffffd9ffL & l) == 0L)
-                     break;
-                  if (kind > 2)
-                     kind = 2;
-                  jjCheckNAdd(23);
-                  break;
-               case 24:
-                  if (curChar == 33)
-                     jjstateSet[jjnewStateCnt++] = 25;
-                  break;
-               case 26:
-                  if ((0x9fffff7affffd9ffL & l) == 0L)
-                     break;
-                  if (kind > 3)
-                     kind = 3;
-                  jjCheckNAdd(27);
-                  break;
-               case 27:
-                  if ((0x9ffffffeffffd9ffL & l) == 0L)
-                     break;
-                  if (kind > 3)
-                     kind = 3;
-                  jjCheckNAdd(27);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else if (curChar < 128)
-      {
-         long l = 1L << (curChar & 077);
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 20:
-               case 21:
-                  if ((0x7fffffe07fffffeL & l) == 0L)
-                     break;
-                  if (kind > 2)
-                     kind = 2;
-                  jjstateSet[jjnewStateCnt++] = 22;
-                  break;
-               case 11:
-               case 4:
-                  if ((0x7fffffe07fffffeL & l) == 0L)
-                     break;
-                  if (kind > 6)
-                     kind = 6;
-                  jjCheckNAddStates(6, 10);
-                  break;
-               case 9:
-                  if ((0x7fffffe07fffffeL & l) == 0L)
-                     break;
-                  if (kind > 6)
-                     kind = 6;
-                  jjCheckNAddStates(11, 13);
-                  break;
-               case 12:
-                  if ((0x7fffffe07fffffeL & l) == 0L)
-                     break;
-                  if (kind > 10)
-                     kind = 10;
-                  jjCheckNAddTwoStates(12, 13);
-                  break;
-               case 17:
-                  if ((0x100000001000000L & l) != 0L)
-                     jjCheckNAdd(18);
-                  break;
-               case 18:
-                  if ((0x7e0000007eL & l) == 0L)
-                     break;
-                  if (kind > 10)
-                     kind = 10;
-                  jjCheckNAddTwoStates(18, 13);
-                  break;
-               case 22:
-               case 23:
-                  if (kind > 2)
-                     kind = 2;
-                  jjCheckNAdd(23);
-                  break;
-               case 25:
-                  if ((0x7fffffe07fffffeL & l) == 0L)
-                     break;
-                  if (kind > 3)
-                     kind = 3;
-                  jjstateSet[jjnewStateCnt++] = 26;
-                  break;
-               case 26:
-               case 27:
-                  if (kind > 3)
-                     kind = 3;
-                  jjCheckNAdd(27);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else
-      {
-         int i2 = (curChar & 0xff) >> 6;
-         long l2 = 1L << (curChar & 077);
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 22:
-               case 23:
-                  if ((jjbitVec0[i2] & l2) == 0L)
-                     break;
-                  if (kind > 2)
-                     kind = 2;
-                  jjCheckNAdd(23);
-                  break;
-               case 26:
-               case 27:
-                  if ((jjbitVec0[i2] & l2) == 0L)
-                     break;
-                  if (kind > 3)
-                     kind = 3;
-                  jjCheckNAdd(27);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      if (kind != 0x7fffffff)
-      {
-         jjmatchedKind = kind;
-         jjmatchedPos = curPos;
-         kind = 0x7fffffff;
-      }
-      ++curPos;
-      if ((i = jjnewStateCnt) == (startsAt = 28 - (jjnewStateCnt = startsAt)))
-         return curPos;
-      try { curChar = input_stream.readChar(); }
-      catch(java.io.IOException e) { return curPos; }
-   }
-}
-private int jjMoveStringLiteralDfa0_5()
-{
-   return jjMoveNfa_5(1, 0);
-}
-private int jjMoveNfa_5(int startState, int curPos)
-{
-   int startsAt = 0;
-   jjnewStateCnt = 2;
-   int i = 1;
-   jjstateSet[0] = startState;
-   int kind = 0x7fffffff;
-   for (;;)
-   {
-      if (++jjround == 0x7fffffff)
-         ReInitRounds();
-      if (curChar < 64)
-      {
-         long l = 1L << curChar;
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 1:
-                  if ((0xfffffffbffffffffL & l) != 0L)
-                  {
-                     if (kind > 25)
-                        kind = 25;
-                     jjCheckNAdd(0);
-                  }
-                  else if (curChar == 34)
-                  {
-                     if (kind > 26)
-                        kind = 26;
-                  }
-                  break;
-               case 0:
-                  if ((0xfffffffbffffffffL & l) == 0L)
-                     break;
-                  kind = 25;
-                  jjCheckNAdd(0);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else if (curChar < 128)
-      {
-         long l = 1L << (curChar & 077);
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 1:
-               case 0:
-                  kind = 25;
-                  jjCheckNAdd(0);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else
-      {
-         int i2 = (curChar & 0xff) >> 6;
-         long l2 = 1L << (curChar & 077);
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 1:
-               case 0:
-                  if ((jjbitVec0[i2] & l2) == 0L)
-                     break;
-                  if (kind > 25)
-                     kind = 25;
-                  jjCheckNAdd(0);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      if (kind != 0x7fffffff)
-      {
-         jjmatchedKind = kind;
-         jjmatchedPos = curPos;
-         kind = 0x7fffffff;
-      }
-      ++curPos;
-      if ((i = jjnewStateCnt) == (startsAt = 2 - (jjnewStateCnt = startsAt)))
-         return curPos;
-      try { curChar = input_stream.readChar(); }
-      catch(java.io.IOException e) { return curPos; }
-   }
-}
-private final int jjStopStringLiteralDfa_7(int pos, long active0)
-{
-   switch (pos)
-   {
-      default :
-         return -1;
-   }
-}
-private final int jjStartNfa_7(int pos, long active0)
-{
-   return jjMoveNfa_7(jjStopStringLiteralDfa_7(pos, active0), pos + 1);
-}
-private int jjMoveStringLiteralDfa0_7()
-{
-   switch(curChar)
-   {
-      case 62:
-         return jjStopAtPos(0, 30);
-      default :
-         return jjMoveNfa_7(0, 0);
-   }
-}
-private int jjMoveNfa_7(int startState, int curPos)
-{
-   int startsAt = 0;
-   jjnewStateCnt = 1;
-   int i = 1;
-   jjstateSet[0] = startState;
-   int kind = 0x7fffffff;
-   for (;;)
-   {
-      if (++jjround == 0x7fffffff)
-         ReInitRounds();
-      if (curChar < 64)
-      {
-         long l = 1L << curChar;
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-                  if ((0xbfffffffffffffffL & l) == 0L)
-                     break;
-                  kind = 29;
-                  jjstateSet[jjnewStateCnt++] = 0;
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else if (curChar < 128)
-      {
-         long l = 1L << (curChar & 077);
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-                  kind = 29;
-                  jjstateSet[jjnewStateCnt++] = 0;
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else
-      {
-         int i2 = (curChar & 0xff) >> 6;
-         long l2 = 1L << (curChar & 077);
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-                  if ((jjbitVec0[i2] & l2) == 0L)
-                     break;
-                  if (kind > 29)
-                     kind = 29;
-                  jjstateSet[jjnewStateCnt++] = 0;
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      if (kind != 0x7fffffff)
-      {
-         jjmatchedKind = kind;
-         jjmatchedPos = curPos;
-         kind = 0x7fffffff;
-      }
-      ++curPos;
-      if ((i = jjnewStateCnt) == (startsAt = 1 - (jjnewStateCnt = startsAt)))
-         return curPos;
-      try { curChar = input_stream.readChar(); }
-      catch(java.io.IOException e) { return curPos; }
-   }
-}
-private int jjMoveStringLiteralDfa0_4()
-{
-   return jjMoveNfa_4(1, 0);
-}
-private int jjMoveNfa_4(int startState, int curPos)
-{
-   int startsAt = 0;
-   jjnewStateCnt = 2;
-   int i = 1;
-   jjstateSet[0] = startState;
-   int kind = 0x7fffffff;
-   for (;;)
-   {
-      if (++jjround == 0x7fffffff)
-         ReInitRounds();
-      if (curChar < 64)
-      {
-         long l = 1L << curChar;
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 1:
-                  if ((0xffffff7fffffffffL & l) != 0L)
-                  {
-                     if (kind > 23)
-                        kind = 23;
-                     jjCheckNAdd(0);
-                  }
-                  else if (curChar == 39)
-                  {
-                     if (kind > 24)
-                        kind = 24;
-                  }
-                  break;
-               case 0:
-                  if ((0xffffff7fffffffffL & l) == 0L)
-                     break;
-                  kind = 23;
-                  jjCheckNAdd(0);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else if (curChar < 128)
-      {
-         long l = 1L << (curChar & 077);
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 1:
-               case 0:
-                  kind = 23;
-                  jjCheckNAdd(0);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else
-      {
-         int i2 = (curChar & 0xff) >> 6;
-         long l2 = 1L << (curChar & 077);
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 1:
-               case 0:
-                  if ((jjbitVec0[i2] & l2) == 0L)
-                     break;
-                  if (kind > 23)
-                     kind = 23;
-                  jjCheckNAdd(0);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      if (kind != 0x7fffffff)
-      {
-         jjmatchedKind = kind;
-         jjmatchedPos = curPos;
-         kind = 0x7fffffff;
-      }
-      ++curPos;
-      if ((i = jjnewStateCnt) == (startsAt = 2 - (jjnewStateCnt = startsAt)))
-         return curPos;
-      try { curChar = input_stream.readChar(); }
-      catch(java.io.IOException e) { return curPos; }
-   }
-}
-private final int jjStopStringLiteralDfa_3(int pos, long active0)
-{
-   switch (pos)
-   {
-      default :
-         return -1;
-   }
-}
-private final int jjStartNfa_3(int pos, long active0)
-{
-   return jjMoveNfa_3(jjStopStringLiteralDfa_3(pos, active0), pos + 1);
-}
-private int jjMoveStringLiteralDfa0_3()
-{
-   switch(curChar)
-   {
-      case 34:
-         return jjStopAtPos(0, 21);
-      case 39:
-         return jjStopAtPos(0, 20);
-      default :
-         return jjMoveNfa_3(0, 0);
-   }
-}
-private int jjMoveNfa_3(int startState, int curPos)
-{
-   int startsAt = 0;
-   jjnewStateCnt = 3;
-   int i = 1;
-   jjstateSet[0] = startState;
-   int kind = 0x7fffffff;
-   for (;;)
-   {
-      if (++jjround == 0x7fffffff)
-         ReInitRounds();
-      if (curChar < 64)
-      {
-         long l = 1L << curChar;
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-                  if ((0x9fffff7affffd9ffL & l) != 0L)
-                  {
-                     if (kind > 19)
-                        kind = 19;
-                     jjCheckNAdd(1);
-                  }
-                  else if ((0x100002600L & l) != 0L)
-                  {
-                     if (kind > 22)
-                        kind = 22;
-                     jjCheckNAdd(2);
-                  }
-                  break;
-               case 1:
-                  if ((0xbffffffeffffd9ffL & l) == 0L)
-                     break;
-                  if (kind > 19)
-                     kind = 19;
-                  jjCheckNAdd(1);
-                  break;
-               case 2:
-                  if ((0x100002600L & l) == 0L)
-                     break;
-                  kind = 22;
-                  jjCheckNAdd(2);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else if (curChar < 128)
-      {
-         long l = 1L << (curChar & 077);
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-               case 1:
-                  if (kind > 19)
-                     kind = 19;
-                  jjCheckNAdd(1);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else
-      {
-         int i2 = (curChar & 0xff) >> 6;
-         long l2 = 1L << (curChar & 077);
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-               case 1:
-                  if ((jjbitVec0[i2] & l2) == 0L)
-                     break;
-                  if (kind > 19)
-                     kind = 19;
-                  jjCheckNAdd(1);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      if (kind != 0x7fffffff)
-      {
-         jjmatchedKind = kind;
-         jjmatchedPos = curPos;
-         kind = 0x7fffffff;
-      }
-      ++curPos;
-      if ((i = jjnewStateCnt) == (startsAt = 3 - (jjnewStateCnt = startsAt)))
-         return curPos;
-      try { curChar = input_stream.readChar(); }
-      catch(java.io.IOException e) { return curPos; }
-   }
-}
-private final int jjStopStringLiteralDfa_6(int pos, long active0)
-{
-   switch (pos)
-   {
-      case 0:
-         if ((active0 & 0x10000000L) != 0L)
-         {
-            jjmatchedKind = 27;
-            return -1;
-         }
-         return -1;
-      case 1:
-         if ((active0 & 0x10000000L) != 0L)
-         {
-            if (jjmatchedPos == 0)
-            {
-               jjmatchedKind = 27;
-               jjmatchedPos = 0;
-            }
-            return -1;
-         }
-         return -1;
-      default :
-         return -1;
-   }
-}
-private final int jjStartNfa_6(int pos, long active0)
-{
-   return jjMoveNfa_6(jjStopStringLiteralDfa_6(pos, active0), pos + 1);
-}
-private int jjMoveStringLiteralDfa0_6()
-{
-   switch(curChar)
-   {
-      case 45:
-         return jjMoveStringLiteralDfa1_6(0x10000000L);
-      default :
-         return jjMoveNfa_6(1, 0);
-   }
-}
-private int jjMoveStringLiteralDfa1_6(long active0)
-{
-   try { curChar = input_stream.readChar(); }
-   catch(java.io.IOException e) {
-      jjStopStringLiteralDfa_6(0, active0);
-      return 1;
-   }
-   switch(curChar)
-   {
-      case 45:
-         return jjMoveStringLiteralDfa2_6(active0, 0x10000000L);
-      default :
-         break;
-   }
-   return jjStartNfa_6(0, active0);
-}
-private int jjMoveStringLiteralDfa2_6(long old0, long active0)
-{
-   if (((active0 &= old0)) == 0L)
-      return jjStartNfa_6(0, old0);
-   try { curChar = input_stream.readChar(); }
-   catch(java.io.IOException e) {
-      jjStopStringLiteralDfa_6(1, active0);
-      return 2;
-   }
-   switch(curChar)
-   {
-      case 62:
-         if ((active0 & 0x10000000L) != 0L)
-            return jjStopAtPos(2, 28);
-         break;
-      default :
-         break;
-   }
-   return jjStartNfa_6(1, active0);
-}
-private int jjMoveNfa_6(int startState, int curPos)
-{
-   int startsAt = 0;
-   jjnewStateCnt = 2;
-   int i = 1;
-   jjstateSet[0] = startState;
-   int kind = 0x7fffffff;
-   for (;;)
-   {
-      if (++jjround == 0x7fffffff)
-         ReInitRounds();
-      if (curChar < 64)
-      {
-         long l = 1L << curChar;
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 1:
-                  if ((0xffffdfffffffffffL & l) != 0L)
-                  {
-                     if (kind > 27)
-                        kind = 27;
-                     jjCheckNAdd(0);
-                  }
-                  else if (curChar == 45)
-                  {
-                     if (kind > 27)
-                        kind = 27;
-                  }
-                  break;
-               case 0:
-                  if ((0xffffdfffffffffffL & l) == 0L)
-                     break;
-                  kind = 27;
-                  jjCheckNAdd(0);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else if (curChar < 128)
-      {
-         long l = 1L << (curChar & 077);
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 1:
-               case 0:
-                  kind = 27;
-                  jjCheckNAdd(0);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else
-      {
-         int i2 = (curChar & 0xff) >> 6;
-         long l2 = 1L << (curChar & 077);
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 1:
-               case 0:
-                  if ((jjbitVec0[i2] & l2) == 0L)
-                     break;
-                  if (kind > 27)
-                     kind = 27;
-                  jjCheckNAdd(0);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      if (kind != 0x7fffffff)
-      {
-         jjmatchedKind = kind;
-         jjmatchedPos = curPos;
-         kind = 0x7fffffff;
-      }
-      ++curPos;
-      if ((i = jjnewStateCnt) == (startsAt = 2 - (jjnewStateCnt = startsAt)))
-         return curPos;
-      try { curChar = input_stream.readChar(); }
-      catch(java.io.IOException e) { return curPos; }
-   }
-}
-private int jjMoveStringLiteralDfa0_1()
-{
-   return jjMoveNfa_1(1, 0);
-}
-private int jjMoveNfa_1(int startState, int curPos)
-{
-   int startsAt = 0;
-   jjnewStateCnt = 12;
-   int i = 1;
-   jjstateSet[0] = startState;
-   int kind = 0x7fffffff;
-   for (;;)
-   {
-      if (++jjround == 0x7fffffff)
-         ReInitRounds();
-      if (curChar < 64)
-      {
-         long l = 1L << curChar;
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 1:
-                  if ((0xafffffffffffffffL & l) != 0L)
-                  {
-                     if (kind > 14)
-                        kind = 14;
-                     jjCheckNAdd(0);
-                  }
-                  else if ((0x5000000000000000L & l) != 0L)
-                  {
-                     if (kind > 14)
-                        kind = 14;
-                  }
-                  if (curChar == 60)
-                     jjstateSet[jjnewStateCnt++] = 10;
-                  break;
-               case 0:
-                  if ((0xafffffffffffffffL & l) == 0L)
-                     break;
-                  if (kind > 14)
-                     kind = 14;
-                  jjCheckNAdd(0);
-                  break;
-               case 3:
-                  if ((0xafffffffffffffffL & l) != 0L)
-                     jjAddStates(18, 19);
-                  break;
-               case 4:
-                  if (curChar == 62 && kind > 15)
-                     kind = 15;
-                  break;
-               case 10:
-                  if (curChar == 47)
-                     jjstateSet[jjnewStateCnt++] = 9;
-                  break;
-               case 11:
-                  if (curChar == 60)
-                     jjstateSet[jjnewStateCnt++] = 10;
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else if (curChar < 128)
-      {
-         long l = 1L << (curChar & 077);
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 1:
-               case 0:
-                  if (kind > 14)
-                     kind = 14;
-                  jjCheckNAdd(0);
-                  break;
-               case 2:
-                  if (curChar == 116)
-                     jjCheckNAddTwoStates(3, 4);
-                  break;
-               case 3:
-                  jjCheckNAddTwoStates(3, 4);
-                  break;
-               case 5:
-                  if (curChar == 112)
-                     jjstateSet[jjnewStateCnt++] = 2;
-                  break;
-               case 6:
-                  if (curChar == 105)
-                     jjstateSet[jjnewStateCnt++] = 5;
-                  break;
-               case 7:
-                  if (curChar == 114)
-                     jjstateSet[jjnewStateCnt++] = 6;
-                  break;
-               case 8:
-                  if (curChar == 99)
-                     jjstateSet[jjnewStateCnt++] = 7;
-                  break;
-               case 9:
-                  if (curChar == 115)
-                     jjstateSet[jjnewStateCnt++] = 8;
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else
-      {
-         int i2 = (curChar & 0xff) >> 6;
-         long l2 = 1L << (curChar & 077);
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 1:
-               case 0:
-                  if ((jjbitVec0[i2] & l2) == 0L)
-                     break;
-                  if (kind > 14)
-                     kind = 14;
-                  jjCheckNAdd(0);
-                  break;
-               case 3:
-                  if ((jjbitVec0[i2] & l2) != 0L)
-                     jjAddStates(18, 19);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      if (kind != 0x7fffffff)
-      {
-         jjmatchedKind = kind;
-         jjmatchedPos = curPos;
-         kind = 0x7fffffff;
-      }
-      ++curPos;
-      if ((i = jjnewStateCnt) == (startsAt = 12 - (jjnewStateCnt = startsAt)))
-         return curPos;
-      try { curChar = input_stream.readChar(); }
-      catch(java.io.IOException e) { return curPos; }
-   }
-}
-private final int jjStopStringLiteralDfa_2(int pos, long active0)
-{
-   switch (pos)
-   {
-      default :
-         return -1;
-   }
-}
-private final int jjStartNfa_2(int pos, long active0)
-{
-   return jjMoveNfa_2(jjStopStringLiteralDfa_2(pos, active0), pos + 1);
-}
-private int jjMoveStringLiteralDfa0_2()
-{
-   switch(curChar)
-   {
-      case 34:
-         return jjStopAtPos(0, 21);
-      case 39:
-         return jjStopAtPos(0, 20);
-      case 61:
-         return jjStartNfaWithStates_2(0, 17, 3);
-      default :
-         return jjMoveNfa_2(0, 0);
-   }
-}
-private int jjStartNfaWithStates_2(int pos, int kind, int state)
-{
-   jjmatchedKind = kind;
-   jjmatchedPos = pos;
-   try { curChar = input_stream.readChar(); }
-   catch(java.io.IOException e) { return pos + 1; }
-   return jjMoveNfa_2(state, pos + 1);
-}
-private int jjMoveNfa_2(int startState, int curPos)
-{
-   int startsAt = 0;
-   jjnewStateCnt = 6;
-   int i = 1;
-   jjstateSet[0] = startState;
-   int kind = 0x7fffffff;
-   for (;;)
-   {
-      if (++jjround == 0x7fffffff)
-         ReInitRounds();
-      if (curChar < 64)
-      {
-         long l = 1L << curChar;
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-                  if ((0x9fffff7affffd9ffL & l) != 0L)
-                  {
-                     if (kind > 16)
-                        kind = 16;
-                     jjCheckNAdd(1);
-                  }
-                  else if ((0x100002600L & l) != 0L)
-                  {
-                     if (kind > 22)
-                        kind = 22;
-                     jjCheckNAdd(5);
-                  }
-                  else if (curChar == 61)
-                     jjstateSet[jjnewStateCnt++] = 3;
-                  else if (curChar == 62)
-                  {
-                     if (kind > 18)
-                        kind = 18;
-                  }
-                  break;
-               case 1:
-                  if ((0x9ffffffeffffd9ffL & l) == 0L)
-                     break;
-                  if (kind > 16)
-                     kind = 16;
-                  jjCheckNAdd(1);
-                  break;
-               case 2:
-               case 3:
-                  if (curChar == 62 && kind > 18)
-                     kind = 18;
-                  break;
-               case 4:
-                  if (curChar == 61)
-                     jjstateSet[jjnewStateCnt++] = 3;
-                  break;
-               case 5:
-                  if ((0x100002600L & l) == 0L)
-                     break;
-                  kind = 22;
-                  jjCheckNAdd(5);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else if (curChar < 128)
-      {
-         long l = 1L << (curChar & 077);
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-               case 1:
-                  if (kind > 16)
-                     kind = 16;
-                  jjCheckNAdd(1);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else
-      {
-         int i2 = (curChar & 0xff) >> 6;
-         long l2 = 1L << (curChar & 077);
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-               case 1:
-                  if ((jjbitVec0[i2] & l2) == 0L)
-                     break;
-                  if (kind > 16)
-                     kind = 16;
-                  jjCheckNAdd(1);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      if (kind != 0x7fffffff)
-      {
-         jjmatchedKind = kind;
-         jjmatchedPos = curPos;
-         kind = 0x7fffffff;
-      }
-      ++curPos;
-      if ((i = jjnewStateCnt) == (startsAt = 6 - (jjnewStateCnt = startsAt)))
-         return curPos;
-      try { curChar = input_stream.readChar(); }
-      catch(java.io.IOException e) { return curPos; }
-   }
-}
-static final int[] jjnextStates = {
-   20, 21, 24, 12, 14, 16, 5, 8, 0, 4, 6, 0, 4, 6, 5, 0, 
-   4, 6, 3, 4, 
-};
-
-/** Token literal values. */
-public static final String[] jjstrLiteralImages = {
-"", "\74\163\143\162\151\160\164", null, null, "\74\41\55\55", "\74\41", null, 
-null, null, null, null, null, null, null, null, null, null, "\75", null, null, 
-"\47", "\42", null, null, null, null, null, null, "\55\55\76", null, "\76", };
-
-/** Lexer state names. */
-public static final String[] lexStateNames = {
-   "DEFAULT",
-   "WithinScript",
-   "WithinTag",
-   "AfterEquals",
-   "WithinQuote1",
-   "WithinQuote2",
-   "WithinComment1",
-   "WithinComment2",
-};
-
-/** Lex State array. */
-public static final int[] jjnewLexState = {
-   -1, 1, 2, 2, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 3, 0, 2, 4, 5, -1, -1, 2, 
-   -1, 2, -1, 0, -1, 0, 
-};
-static final long[] jjtoToken = {
-   0x7fbfec7fL, 
-};
-static final long[] jjtoSkip = {
-   0x400000L, 
-};
-protected SimpleCharStream input_stream;
-private final int[] jjrounds = new int[28];
-private final int[] jjstateSet = new int[56];
-protected char curChar;
-/** Constructor. */
-public HTMLParserTokenManager(SimpleCharStream stream){
-   if (SimpleCharStream.staticFlag)
-      throw new Error("ERROR: Cannot use a static CharStream class with a non-static lexical analyzer.");
-   input_stream = stream;
-}
-
-/** Constructor. */
-public HTMLParserTokenManager(SimpleCharStream stream, int lexState){
-   this(stream);
-   SwitchTo(lexState);
-}
-
-/** Reinitialise parser. */
-public void ReInit(SimpleCharStream stream)
-{
-   jjmatchedPos = jjnewStateCnt = 0;
-   curLexState = defaultLexState;
-   input_stream = stream;
-   ReInitRounds();
-}
-private void ReInitRounds()
-{
-   int i;
-   jjround = 0x80000001;
-   for (i = 28; i-- > 0;)
-      jjrounds[i] = 0x80000000;
-}
-
-/** Reinitialise parser. */
-public void ReInit(SimpleCharStream stream, int lexState)
-{
-   ReInit(stream);
-   SwitchTo(lexState);
-}
-
-/** Switch to specified lex state. */
-public void SwitchTo(int lexState)
-{
-   if (lexState >= 8 || lexState < 0)
-      throw new TokenMgrError("Error: Ignoring invalid lexical state : " + lexState + ". State unchanged.", TokenMgrError.INVALID_LEXICAL_STATE);
-   else
-      curLexState = lexState;
-}
-
-protected Token jjFillToken()
-{
-   final Token t;
-   final String curTokenImage;
-   final int beginLine;
-   final int endLine;
-   final int beginColumn;
-   final int endColumn;
-   String im = jjstrLiteralImages[jjmatchedKind];
-   curTokenImage = (im == null) ? input_stream.GetImage() : im;
-   beginLine = input_stream.getBeginLine();
-   beginColumn = input_stream.getBeginColumn();
-   endLine = input_stream.getEndLine();
-   endColumn = input_stream.getEndColumn();
-   t = Token.newToken(jjmatchedKind, curTokenImage);
-
-   t.beginLine = beginLine;
-   t.endLine = endLine;
-   t.beginColumn = beginColumn;
-   t.endColumn = endColumn;
-
-   return t;
-}
-
-int curLexState = 0;
-int defaultLexState = 0;
-int jjnewStateCnt;
-int jjround;
-int jjmatchedPos;
-int jjmatchedKind;
-
-/** Get the next Token. */
-public Token getNextToken() 
-{
-  Token matchedToken;
-  int curPos = 0;
-
-  EOFLoop :
-  for (;;)
-  {
-   try
-   {
-      curChar = input_stream.BeginToken();
-   }
-   catch(java.io.IOException e)
-   {
-      jjmatchedKind = 0;
-      matchedToken = jjFillToken();
-      return matchedToken;
-   }
-
-   switch(curLexState)
-   {
-     case 0:
-       jjmatchedKind = 0x7fffffff;
-       jjmatchedPos = 0;
-       curPos = jjMoveStringLiteralDfa0_0();
-       if (jjmatchedPos == 0 && jjmatchedKind > 13)
-       {
-          jjmatchedKind = 13;
-       }
-       break;
-     case 1:
-       jjmatchedKind = 0x7fffffff;
-       jjmatchedPos = 0;
-       curPos = jjMoveStringLiteralDfa0_1();
-       break;
-     case 2:
-       jjmatchedKind = 0x7fffffff;
-       jjmatchedPos = 0;
-       curPos = jjMoveStringLiteralDfa0_2();
-       break;
-     case 3:
-       jjmatchedKind = 0x7fffffff;
-       jjmatchedPos = 0;
-       curPos = jjMoveStringLiteralDfa0_3();
-       break;
-     case 4:
-       jjmatchedKind = 0x7fffffff;
-       jjmatchedPos = 0;
-       curPos = jjMoveStringLiteralDfa0_4();
-       break;
-     case 5:
-       jjmatchedKind = 0x7fffffff;
-       jjmatchedPos = 0;
-       curPos = jjMoveStringLiteralDfa0_5();
-       break;
-     case 6:
-       jjmatchedKind = 0x7fffffff;
-       jjmatchedPos = 0;
-       curPos = jjMoveStringLiteralDfa0_6();
-       break;
-     case 7:
-       jjmatchedKind = 0x7fffffff;
-       jjmatchedPos = 0;
-       curPos = jjMoveStringLiteralDfa0_7();
-       break;
-   }
-     if (jjmatchedKind != 0x7fffffff)
-     {
-        if (jjmatchedPos + 1 < curPos)
-           input_stream.backup(curPos - jjmatchedPos - 1);
-        if ((jjtoToken[jjmatchedKind >> 6] & (1L << (jjmatchedKind & 077))) != 0L)
-        {
-           matchedToken = jjFillToken();
-       if (jjnewLexState[jjmatchedKind] != -1)
-         curLexState = jjnewLexState[jjmatchedKind];
-           return matchedToken;
-        }
-        else
-        {
-         if (jjnewLexState[jjmatchedKind] != -1)
-           curLexState = jjnewLexState[jjmatchedKind];
-           continue EOFLoop;
-        }
-     }
-     int error_line = input_stream.getEndLine();
-     int error_column = input_stream.getEndColumn();
-     String error_after = null;
-     boolean EOFSeen = false;
-     try { input_stream.readChar(); input_stream.backup(1); }
-     catch (java.io.IOException e1) {
-        EOFSeen = true;
-        error_after = curPos <= 1 ? "" : input_stream.GetImage();
-        if (curChar == '\n' || curChar == '\r') {
-           error_line++;
-           error_column = 0;
-        }
-        else
-           error_column++;
-     }
-     if (!EOFSeen) {
-        input_stream.backup(1);
-        error_after = curPos <= 1 ? "" : input_stream.GetImage();
-     }
-     throw new TokenMgrError(EOFSeen, curLexState, error_line, error_column, error_after, curChar, TokenMgrError.LEXICAL_ERROR);
-  }
-}
-
-private void jjCheckNAdd(int state)
-{
-   if (jjrounds[state] != jjround)
-   {
-      jjstateSet[jjnewStateCnt++] = state;
-      jjrounds[state] = jjround;
-   }
-}
-private void jjAddStates(int start, int end)
-{
-   do {
-      jjstateSet[jjnewStateCnt++] = jjnextStates[start];
-   } while (start++ != end);
-}
-private void jjCheckNAddTwoStates(int state1, int state2)
-{
-   jjCheckNAdd(state1);
-   jjCheckNAdd(state2);
-}
-
-private void jjCheckNAddStates(int start, int end)
-{
-   do {
-      jjCheckNAdd(jjnextStates[start]);
-   } while (start++ != end);
-}
-
-}
diff --git a/lucene/src/demo/org/apache/lucene/demo/html/ParseException.java b/lucene/src/demo/org/apache/lucene/demo/html/ParseException.java
deleted file mode 100644
index 0441bf8..0000000
--- a/lucene/src/demo/org/apache/lucene/demo/html/ParseException.java
+++ /dev/null
@@ -1,198 +0,0 @@
-/* Generated By:JavaCC: Do not edit this line. ParseException.java Version 4.1 */
-/* JavaCCOptions:KEEP_LINE_COL=null */
-package org.apache.lucene.demo.html;
-
-/**
- * This exception is thrown when parse errors are encountered.
- * You can explicitly create objects of this exception type by
- * calling the method generateParseException in the generated
- * parser.
- *
- * You can modify this class to customize your error reporting
- * mechanisms so long as you retain the public fields.
- */
-public class ParseException extends Exception {
-
-  /**
-   * This constructor is used by the method "generateParseException"
-   * in the generated parser.  Calling this constructor generates
-   * a new object of this type with the fields "currentToken",
-   * "expectedTokenSequences", and "tokenImage" set.  The boolean
-   * flag "specialConstructor" is also set to true to indicate that
-   * this constructor was used to create this object.
-   * This constructor calls its super class with the empty string
-   * to force the "toString" method of parent class "Throwable" to
-   * print the error message in the form:
-   *     ParseException: <result of getMessage>
-   */
-  public ParseException(Token currentTokenVal,
-                        int[][] expectedTokenSequencesVal,
-                        String[] tokenImageVal
-                       )
-  {
-    super("");
-    specialConstructor = true;
-    currentToken = currentTokenVal;
-    expectedTokenSequences = expectedTokenSequencesVal;
-    tokenImage = tokenImageVal;
-  }
-
-  /**
-   * The following constructors are for use by you for whatever
-   * purpose you can think of.  Constructing the exception in this
-   * manner makes the exception behave in the normal way - i.e., as
-   * documented in the class "Throwable".  The fields "errorToken",
-   * "expectedTokenSequences", and "tokenImage" do not contain
-   * relevant information.  The JavaCC generated code does not use
-   * these constructors.
-   */
-
-  public ParseException() {
-    super();
-    specialConstructor = false;
-  }
-
-  /** Constructor with message. */
-  public ParseException(String message) {
-    super(message);
-    specialConstructor = false;
-  }
-
-  /**
-   * This variable determines which constructor was used to create
-   * this object and thereby affects the semantics of the
-   * "getMessage" method (see below).
-   */
-  protected boolean specialConstructor;
-
-  /**
-   * This is the last token that has been consumed successfully.  If
-   * this object has been created due to a parse error, the token
-   * followng this token will (therefore) be the first error token.
-   */
-  public Token currentToken;
-
-  /**
-   * Each entry in this array is an array of integers.  Each array
-   * of integers represents a sequence of tokens (by their ordinal
-   * values) that is expected at this point of the parse.
-   */
-  public int[][] expectedTokenSequences;
-
-  /**
-   * This is a reference to the "tokenImage" array of the generated
-   * parser within which the parse error occurred.  This array is
-   * defined in the generated ...Constants interface.
-   */
-  public String[] tokenImage;
-
-  /**
-   * This method has the standard behavior when this object has been
-   * created using the standard constructors.  Otherwise, it uses
-   * "currentToken" and "expectedTokenSequences" to generate a parse
-   * error message and returns it.  If this object has been created
-   * due to a parse error, and you do not catch it (it gets thrown
-   * from the parser), then this method is called during the printing
-   * of the final stack trace, and hence the correct error message
-   * gets displayed.
-   */
-  public String getMessage() {
-    if (!specialConstructor) {
-      return super.getMessage();
-    }
-    StringBuffer expected = new StringBuffer();
-    int maxSize = 0;
-    for (int i = 0; i < expectedTokenSequences.length; i++) {
-      if (maxSize < expectedTokenSequences[i].length) {
-        maxSize = expectedTokenSequences[i].length;
-      }
-      for (int j = 0; j < expectedTokenSequences[i].length; j++) {
-        expected.append(tokenImage[expectedTokenSequences[i][j]]).append(' ');
-      }
-      if (expectedTokenSequences[i][expectedTokenSequences[i].length - 1] != 0) {
-        expected.append("...");
-      }
-      expected.append(eol).append("    ");
-    }
-    String retval = "Encountered \"";
-    Token tok = currentToken.next;
-    for (int i = 0; i < maxSize; i++) {
-      if (i != 0) retval += " ";
-      if (tok.kind == 0) {
-        retval += tokenImage[0];
-        break;
-      }
-      retval += " " + tokenImage[tok.kind];
-      retval += " \"";
-      retval += add_escapes(tok.image);
-      retval += " \"";
-      tok = tok.next;
-    }
-    retval += "\" at line " + currentToken.next.beginLine + ", column " + currentToken.next.beginColumn;
-    retval += "." + eol;
-    if (expectedTokenSequences.length == 1) {
-      retval += "Was expecting:" + eol + "    ";
-    } else {
-      retval += "Was expecting one of:" + eol + "    ";
-    }
-    retval += expected.toString();
-    return retval;
-  }
-
-  /**
-   * The end of line string for this machine.
-   */
-  protected String eol = System.getProperty("line.separator", "\n");
-
-  /**
-   * Used to convert raw characters to their escaped version
-   * when these raw version cannot be used as part of an ASCII
-   * string literal.
-   */
-  protected String add_escapes(String str) {
-      StringBuffer retval = new StringBuffer();
-      char ch;
-      for (int i = 0; i < str.length(); i++) {
-        switch (str.charAt(i))
-        {
-           case 0 :
-              continue;
-           case '\b':
-              retval.append("\\b");
-              continue;
-           case '\t':
-              retval.append("\\t");
-              continue;
-           case '\n':
-              retval.append("\\n");
-              continue;
-           case '\f':
-              retval.append("\\f");
-              continue;
-           case '\r':
-              retval.append("\\r");
-              continue;
-           case '\"':
-              retval.append("\\\"");
-              continue;
-           case '\'':
-              retval.append("\\\'");
-              continue;
-           case '\\':
-              retval.append("\\\\");
-              continue;
-           default:
-              if ((ch = str.charAt(i)) < 0x20 || ch > 0x7e) {
-                 String s = "0000" + Integer.toString(ch, 16);
-                 retval.append("\\u" + s.substring(s.length() - 4, s.length()));
-              } else {
-                 retval.append(ch);
-              }
-              continue;
-        }
-      }
-      return retval.toString();
-   }
-
-}
-/* JavaCC - OriginalChecksum=63b2008c66e199b79536447c26bee2ab (do not edit this line) */
diff --git a/lucene/src/demo/org/apache/lucene/demo/html/ParserThread.java b/lucene/src/demo/org/apache/lucene/demo/html/ParserThread.java
deleted file mode 100644
index 88fa41a..0000000
--- a/lucene/src/demo/org/apache/lucene/demo/html/ParserThread.java
+++ /dev/null
@@ -1,50 +0,0 @@
-package org.apache.lucene.demo.html;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.*;
-
-class ParserThread extends Thread {
-  HTMLParser parser;
-
-  ParserThread(HTMLParser p) {
-    parser = p;
-  }
-
-  @Override
-  public void run() {				  // convert pipeOut to pipeIn
-    try {
-      try {					  // parse document to pipeOut
-        parser.HTMLDocument();
-      } catch (ParseException e) {
-        System.out.println("Parse Aborted: " + e.getMessage());
-      } catch (TokenMgrError e) {
-        System.out.println("Parse Aborted: " + e.getMessage());
-      } finally {
-        parser.pipeOut.close();
-        synchronized (parser) {
-	      parser.summary.setLength(HTMLParser.SUMMARY_LENGTH);
-	      parser.titleComplete = true;
-	      parser.notifyAll();
-	    }
-      }
-    } catch (IOException e) {
-	  e.printStackTrace();
-    }
-  }
-}
diff --git a/lucene/src/demo/org/apache/lucene/demo/html/SimpleCharStream.java b/lucene/src/demo/org/apache/lucene/demo/html/SimpleCharStream.java
deleted file mode 100644
index fedf92d..0000000
--- a/lucene/src/demo/org/apache/lucene/demo/html/SimpleCharStream.java
+++ /dev/null
@@ -1,472 +0,0 @@
-/* Generated By:JavaCC: Do not edit this line. SimpleCharStream.java Version 4.1 */
-/* JavaCCOptions:STATIC=false */
-package org.apache.lucene.demo.html;
-
-/**
- * An implementation of interface CharStream, where the stream is assumed to
- * contain only ASCII characters (without unicode processing).
- */
-
-public class SimpleCharStream
-{
-/** Whether parser is static. */
-  public static final boolean staticFlag = false;
-  int bufsize;
-  int available;
-  int tokenBegin;
-/** Position in buffer. */
-  public int bufpos = -1;
-  protected int bufline[];
-  protected int bufcolumn[];
-
-  protected int column = 0;
-  protected int line = 1;
-
-  protected boolean prevCharIsCR = false;
-  protected boolean prevCharIsLF = false;
-
-  protected java.io.Reader inputStream;
-
-  protected char[] buffer;
-  protected int maxNextCharInd = 0;
-  protected int inBuf = 0;
-  protected int tabSize = 8;
-
-  protected void setTabSize(int i) { tabSize = i; }
-  protected int getTabSize(int i) { return tabSize; }
-
-
-  protected void ExpandBuff(boolean wrapAround)
-  {
-     char[] newbuffer = new char[bufsize + 2048];
-     int newbufline[] = new int[bufsize + 2048];
-     int newbufcolumn[] = new int[bufsize + 2048];
-
-     try
-     {
-        if (wrapAround)
-        {
-           System.arraycopy(buffer, tokenBegin, newbuffer, 0, bufsize - tokenBegin);
-           System.arraycopy(buffer, 0, newbuffer,
-                                             bufsize - tokenBegin, bufpos);
-           buffer = newbuffer;
-
-           System.arraycopy(bufline, tokenBegin, newbufline, 0, bufsize - tokenBegin);
-           System.arraycopy(bufline, 0, newbufline, bufsize - tokenBegin, bufpos);
-           bufline = newbufline;
-
-           System.arraycopy(bufcolumn, tokenBegin, newbufcolumn, 0, bufsize - tokenBegin);
-           System.arraycopy(bufcolumn, 0, newbufcolumn, bufsize - tokenBegin, bufpos);
-           bufcolumn = newbufcolumn;
-
-           maxNextCharInd = (bufpos += (bufsize - tokenBegin));
-        }
-        else
-        {
-           System.arraycopy(buffer, tokenBegin, newbuffer, 0, bufsize - tokenBegin);
-           buffer = newbuffer;
-
-           System.arraycopy(bufline, tokenBegin, newbufline, 0, bufsize - tokenBegin);
-           bufline = newbufline;
-
-           System.arraycopy(bufcolumn, tokenBegin, newbufcolumn, 0, bufsize - tokenBegin);
-           bufcolumn = newbufcolumn;
-
-           maxNextCharInd = (bufpos -= tokenBegin);
-        }
-     }
-     catch (Throwable t)
-     {
-        throw new Error(t.getMessage());
-     }
-
-
-     bufsize += 2048;
-     available = bufsize;
-     tokenBegin = 0;
-  }
-
-  protected void FillBuff() throws java.io.IOException
-  {
-     if (maxNextCharInd == available)
-     {
-        if (available == bufsize)
-        {
-           if (tokenBegin > 2048)
-           {
-              bufpos = maxNextCharInd = 0;
-              available = tokenBegin;
-           }
-           else if (tokenBegin < 0)
-              bufpos = maxNextCharInd = 0;
-           else
-              ExpandBuff(false);
-        }
-        else if (available > tokenBegin)
-           available = bufsize;
-        else if ((tokenBegin - available) < 2048)
-           ExpandBuff(true);
-        else
-           available = tokenBegin;
-     }
-
-     int i;
-     try {
-        if ((i = inputStream.read(buffer, maxNextCharInd,
-                                    available - maxNextCharInd)) == -1)
-        {
-           inputStream.close();
-           throw new java.io.IOException();
-        }
-        else
-           maxNextCharInd += i;
-        return;
-     }
-     catch(java.io.IOException e) {
-        --bufpos;
-        backup(0);
-        if (tokenBegin == -1)
-           tokenBegin = bufpos;
-        throw e;
-     }
-  }
-
-/** Start. */
-  public char BeginToken() throws java.io.IOException
-  {
-     tokenBegin = -1;
-     char c = readChar();
-     tokenBegin = bufpos;
-
-     return c;
-  }
-
-  protected void UpdateLineColumn(char c)
-  {
-     column++;
-
-     if (prevCharIsLF)
-     {
-        prevCharIsLF = false;
-        line += (column = 1);
-     }
-     else if (prevCharIsCR)
-     {
-        prevCharIsCR = false;
-        if (c == '\n')
-        {
-           prevCharIsLF = true;
-        }
-        else
-           line += (column = 1);
-     }
-
-     switch (c)
-     {
-        case '\r' :
-           prevCharIsCR = true;
-           break;
-        case '\n' :
-           prevCharIsLF = true;
-           break;
-        case '\t' :
-           column--;
-           column += (tabSize - (column % tabSize));
-           break;
-        default :
-           break;
-     }
-
-     bufline[bufpos] = line;
-     bufcolumn[bufpos] = column;
-  }
-
-/** Read a character. */
-  public char readChar() throws java.io.IOException
-  {
-     if (inBuf > 0)
-     {
-        --inBuf;
-
-        if (++bufpos == bufsize)
-           bufpos = 0;
-
-        return buffer[bufpos];
-     }
-
-     if (++bufpos >= maxNextCharInd)
-        FillBuff();
-
-     char c = buffer[bufpos];
-
-     UpdateLineColumn(c);
-     return c;
-  }
-
-  /**
-   * @deprecated
-   * @see #getEndColumn
-   */
-
-  public int getColumn() {
-     return bufcolumn[bufpos];
-  }
-
-  /**
-   * @deprecated
-   * @see #getEndLine
-   */
-
-  public int getLine() {
-     return bufline[bufpos];
-  }
-
-  /** Get token end column number. */
-  public int getEndColumn() {
-     return bufcolumn[bufpos];
-  }
-
-  /** Get token end line number. */
-  public int getEndLine() {
-     return bufline[bufpos];
-  }
-
-  /** Get token beginning column number. */
-  public int getBeginColumn() {
-     return bufcolumn[tokenBegin];
-  }
-
-  /** Get token beginning line number. */
-  public int getBeginLine() {
-     return bufline[tokenBegin];
-  }
-
-/** Backup a number of characters. */
-  public void backup(int amount) {
-
-    inBuf += amount;
-    if ((bufpos -= amount) < 0)
-       bufpos += bufsize;
-  }
-
-  /** Constructor. */
-  public SimpleCharStream(java.io.Reader dstream, int startline,
-  int startcolumn, int buffersize)
-  {
-    inputStream = dstream;
-    line = startline;
-    column = startcolumn - 1;
-
-    available = bufsize = buffersize;
-    buffer = new char[buffersize];
-    bufline = new int[buffersize];
-    bufcolumn = new int[buffersize];
-  }
-
-  /** Constructor. */
-  public SimpleCharStream(java.io.Reader dstream, int startline,
-                          int startcolumn)
-  {
-     this(dstream, startline, startcolumn, 4096);
-  }
-
-  /** Constructor. */
-  public SimpleCharStream(java.io.Reader dstream)
-  {
-     this(dstream, 1, 1, 4096);
-  }
-
-  /** Reinitialise. */
-  public void ReInit(java.io.Reader dstream, int startline,
-  int startcolumn, int buffersize)
-  {
-    inputStream = dstream;
-    line = startline;
-    column = startcolumn - 1;
-
-    if (buffer == null || buffersize != buffer.length)
-    {
-      available = bufsize = buffersize;
-      buffer = new char[buffersize];
-      bufline = new int[buffersize];
-      bufcolumn = new int[buffersize];
-    }
-    prevCharIsLF = prevCharIsCR = false;
-    tokenBegin = inBuf = maxNextCharInd = 0;
-    bufpos = -1;
-  }
-
-  /** Reinitialise. */
-  public void ReInit(java.io.Reader dstream, int startline,
-                     int startcolumn)
-  {
-     ReInit(dstream, startline, startcolumn, 4096);
-  }
-
-  /** Reinitialise. */
-  public void ReInit(java.io.Reader dstream)
-  {
-     ReInit(dstream, 1, 1, 4096);
-  }
-  /** Constructor. */
-  public SimpleCharStream(java.io.InputStream dstream, String encoding, int startline,
-  int startcolumn, int buffersize) throws java.io.UnsupportedEncodingException
-  {
-     this(encoding == null ? new java.io.InputStreamReader(dstream) : new java.io.InputStreamReader(dstream, encoding), startline, startcolumn, buffersize);
-  }
-
-  /** Constructor. */
-  public SimpleCharStream(java.io.InputStream dstream, int startline,
-  int startcolumn, int buffersize)
-  {
-     this(new java.io.InputStreamReader(dstream), startline, startcolumn, buffersize);
-  }
-
-  /** Constructor. */
-  public SimpleCharStream(java.io.InputStream dstream, String encoding, int startline,
-                          int startcolumn) throws java.io.UnsupportedEncodingException
-  {
-     this(dstream, encoding, startline, startcolumn, 4096);
-  }
-
-  /** Constructor. */
-  public SimpleCharStream(java.io.InputStream dstream, int startline,
-                          int startcolumn)
-  {
-     this(dstream, startline, startcolumn, 4096);
-  }
-
-  /** Constructor. */
-  public SimpleCharStream(java.io.InputStream dstream, String encoding) throws java.io.UnsupportedEncodingException
-  {
-     this(dstream, encoding, 1, 1, 4096);
-  }
-
-  /** Constructor. */
-  public SimpleCharStream(java.io.InputStream dstream)
-  {
-     this(dstream, 1, 1, 4096);
-  }
-
-  /** Reinitialise. */
-  public void ReInit(java.io.InputStream dstream, String encoding, int startline,
-                          int startcolumn, int buffersize) throws java.io.UnsupportedEncodingException
-  {
-     ReInit(encoding == null ? new java.io.InputStreamReader(dstream) : new java.io.InputStreamReader(dstream, encoding), startline, startcolumn, buffersize);
-  }
-
-  /** Reinitialise. */
-  public void ReInit(java.io.InputStream dstream, int startline,
-                          int startcolumn, int buffersize)
-  {
-     ReInit(new java.io.InputStreamReader(dstream), startline, startcolumn, buffersize);
-  }
-
-  /** Reinitialise. */
-  public void ReInit(java.io.InputStream dstream, String encoding) throws java.io.UnsupportedEncodingException
-  {
-     ReInit(dstream, encoding, 1, 1, 4096);
-  }
-
-  /** Reinitialise. */
-  public void ReInit(java.io.InputStream dstream)
-  {
-     ReInit(dstream, 1, 1, 4096);
-  }
-  /** Reinitialise. */
-  public void ReInit(java.io.InputStream dstream, String encoding, int startline,
-                     int startcolumn) throws java.io.UnsupportedEncodingException
-  {
-     ReInit(dstream, encoding, startline, startcolumn, 4096);
-  }
-  /** Reinitialise. */
-  public void ReInit(java.io.InputStream dstream, int startline,
-                     int startcolumn)
-  {
-     ReInit(dstream, startline, startcolumn, 4096);
-  }
-  /** Get token literal value. */
-  public String GetImage()
-  {
-     if (bufpos >= tokenBegin)
-        return new String(buffer, tokenBegin, bufpos - tokenBegin + 1);
-     else
-        return new String(buffer, tokenBegin, bufsize - tokenBegin) +
-                              new String(buffer, 0, bufpos + 1);
-  }
-
-  /** Get the suffix. */
-  public char[] GetSuffix(int len)
-  {
-     char[] ret = new char[len];
-
-     if ((bufpos + 1) >= len)
-        System.arraycopy(buffer, bufpos - len + 1, ret, 0, len);
-     else
-     {
-        System.arraycopy(buffer, bufsize - (len - bufpos - 1), ret, 0,
-                                                          len - bufpos - 1);
-        System.arraycopy(buffer, 0, ret, len - bufpos - 1, bufpos + 1);
-     }
-
-     return ret;
-  }
-
-  /** Reset buffer when finished. */
-  public void Done()
-  {
-     buffer = null;
-     bufline = null;
-     bufcolumn = null;
-  }
-
-  /**
-   * Method to adjust line and column numbers for the start of a token.
-   */
-  public void adjustBeginLineColumn(int newLine, int newCol)
-  {
-     int start = tokenBegin;
-     int len;
-
-     if (bufpos >= tokenBegin)
-     {
-        len = bufpos - tokenBegin + inBuf + 1;
-     }
-     else
-     {
-        len = bufsize - tokenBegin + bufpos + 1 + inBuf;
-     }
-
-     int i = 0, j = 0, k = 0;
-     int nextColDiff = 0, columnDiff = 0;
-
-     while (i < len &&
-            bufline[j = start % bufsize] == bufline[k = ++start % bufsize])
-     {
-        bufline[j] = newLine;
-        nextColDiff = columnDiff + bufcolumn[k] - bufcolumn[j];
-        bufcolumn[j] = newCol + columnDiff;
-        columnDiff = nextColDiff;
-        i++;
-     }
-
-     if (i < len)
-     {
-        bufline[j] = newLine++;
-        bufcolumn[j] = newCol + columnDiff;
-
-        while (i++ < len)
-        {
-           if (bufline[j = start % bufsize] != bufline[++start % bufsize])
-              bufline[j] = newLine++;
-           else
-              bufline[j] = newLine;
-        }
-     }
-
-     line = bufline[j];
-     column = bufcolumn[j];
-  }
-
-}
-/* JavaCC - OriginalChecksum=7393ed4ac2709e2de22d164f9db78b65 (do not edit this line) */
diff --git a/lucene/src/demo/org/apache/lucene/demo/html/Tags.java b/lucene/src/demo/org/apache/lucene/demo/html/Tags.java
deleted file mode 100644
index a9215f2..0000000
--- a/lucene/src/demo/org/apache/lucene/demo/html/Tags.java
+++ /dev/null
@@ -1,64 +0,0 @@
-package org.apache.lucene.demo.html;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Collections;
-import java.util.HashSet;
-import java.util.Set;
-
-
-public final class Tags {
-
-  /**
-   * contains all tags for which whitespaces have to be inserted for proper tokenization
-   */
-  public static final Set<String> WS_ELEMS = Collections.synchronizedSet(new HashSet<String>());
-
-  static{
-    WS_ELEMS.add("<hr");
-    WS_ELEMS.add("<hr/");  // note that "<hr />" does not need to be listed explicitly
-    WS_ELEMS.add("<br");
-    WS_ELEMS.add("<br/");
-    WS_ELEMS.add("<p");
-    WS_ELEMS.add("</p");
-    WS_ELEMS.add("<div");
-    WS_ELEMS.add("</div");
-    WS_ELEMS.add("<td");
-    WS_ELEMS.add("</td");
-    WS_ELEMS.add("<li");
-    WS_ELEMS.add("</li");
-    WS_ELEMS.add("<q");
-    WS_ELEMS.add("</q");
-    WS_ELEMS.add("<blockquote");
-    WS_ELEMS.add("</blockquote");
-    WS_ELEMS.add("<dt");
-    WS_ELEMS.add("</dt");
-    WS_ELEMS.add("<h1");
-    WS_ELEMS.add("</h1");
-    WS_ELEMS.add("<h2");
-    WS_ELEMS.add("</h2");
-    WS_ELEMS.add("<h3");
-    WS_ELEMS.add("</h3");
-    WS_ELEMS.add("<h4");
-    WS_ELEMS.add("</h4");
-    WS_ELEMS.add("<h5");
-    WS_ELEMS.add("</h5");
-    WS_ELEMS.add("<h6");
-    WS_ELEMS.add("</h6");
-  }
-}
diff --git a/lucene/src/demo/org/apache/lucene/demo/html/Test.java b/lucene/src/demo/org/apache/lucene/demo/html/Test.java
deleted file mode 100644
index 224ae5e..0000000
--- a/lucene/src/demo/org/apache/lucene/demo/html/Test.java
+++ /dev/null
@@ -1,51 +0,0 @@
-package org.apache.lucene.demo.html;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.*;
-
-class Test {
-  public static void main(String[] argv) throws IOException, InterruptedException {
-    if ("-dir".equals(argv[0])) {
-      String[] files = new File(argv[1]).list();
-      java.util.Arrays.sort(files);
-      for (int i = 0; i < files.length; i++) {
-	System.err.println(files[i]);
-	File file = new File(argv[1], files[i]);
-	parse(file);
-      }
-    } else
-      parse(new File(argv[0]));
-  }
-
-  public static void parse(File file) throws IOException, InterruptedException {
-    FileInputStream fis = null;
-    try {
-      fis = new FileInputStream(file);
-      HTMLParser parser = new HTMLParser(fis);
-      System.out.println("Title: " + Entities.encode(parser.getTitle()));
-      System.out.println("Summary: " + Entities.encode(parser.getSummary()));
-      System.out.println("Content:");
-      LineNumberReader reader = new LineNumberReader(parser.getReader());
-      for (String l = reader.readLine(); l != null; l = reader.readLine())
-        System.out.println(l);
-    } finally {
-      if (fis != null) fis.close();
-    }
-  }
-}
diff --git a/lucene/src/demo/org/apache/lucene/demo/html/Token.java b/lucene/src/demo/org/apache/lucene/demo/html/Token.java
deleted file mode 100644
index c38a7bf..0000000
--- a/lucene/src/demo/org/apache/lucene/demo/html/Token.java
+++ /dev/null
@@ -1,124 +0,0 @@
-/* Generated By:JavaCC: Do not edit this line. Token.java Version 4.1 */
-/* JavaCCOptions:TOKEN_EXTENDS=,KEEP_LINE_COL=null */
-package org.apache.lucene.demo.html;
-
-/**
- * Describes the input token stream.
- */
-
-public class Token {
-
-  /**
-   * An integer that describes the kind of this token.  This numbering
-   * system is determined by JavaCCParser, and a table of these numbers is
-   * stored in the file ...Constants.java.
-   */
-  public int kind;
-
-  /** The line number of the first character of this Token. */
-  public int beginLine;
-  /** The column number of the first character of this Token. */
-  public int beginColumn;
-  /** The line number of the last character of this Token. */
-  public int endLine;
-  /** The column number of the last character of this Token. */
-  public int endColumn;
-
-  /**
-   * The string image of the token.
-   */
-  public String image;
-
-  /**
-   * A reference to the next regular (non-special) token from the input
-   * stream.  If this is the last token from the input stream, or if the
-   * token manager has not read tokens beyond this one, this field is
-   * set to null.  This is true only if this token is also a regular
-   * token.  Otherwise, see below for a description of the contents of
-   * this field.
-   */
-  public Token next;
-
-  /**
-   * This field is used to access special tokens that occur prior to this
-   * token, but after the immediately preceding regular (non-special) token.
-   * If there are no such special tokens, this field is set to null.
-   * When there are more than one such special token, this field refers
-   * to the last of these special tokens, which in turn refers to the next
-   * previous special token through its specialToken field, and so on
-   * until the first special token (whose specialToken field is null).
-   * The next fields of special tokens refer to other special tokens that
-   * immediately follow it (without an intervening regular token).  If there
-   * is no such token, this field is null.
-   */
-  public Token specialToken;
-
-  /**
-   * An optional attribute value of the Token.
-   * Tokens which are not used as syntactic sugar will often contain
-   * meaningful values that will be used later on by the compiler or
-   * interpreter. This attribute value is often different from the image.
-   * Any subclass of Token that actually wants to return a non-null value can
-   * override this method as appropriate.
-   */
-  public Object getValue() {
-    return null;
-  }
-
-  /**
-   * No-argument constructor
-   */
-  public Token() {}
-
-  /**
-   * Constructs a new token for the specified Image.
-   */
-  public Token(int kind)
-  {
-     this(kind, null);
-  }
-
-  /**
-   * Constructs a new token for the specified Image and Kind.
-   */
-  public Token(int kind, String image)
-  {
-     this.kind = kind;
-     this.image = image;
-  }
-
-  /**
-   * Returns the image.
-   */
-  public String toString()
-  {
-     return image;
-  }
-
-  /**
-   * Returns a new Token object, by default. However, if you want, you
-   * can create and return subclass objects based on the value of ofKind.
-   * Simply add the cases to the switch for all those special cases.
-   * For example, if you have a subclass of Token called IDToken that
-   * you want to create if ofKind is ID, simply add something like :
-   *
-   *    case MyParserConstants.ID : return new IDToken(ofKind, image);
-   *
-   * to the following switch statement. Then you can cast matchedToken
-   * variable to the appropriate type and use sit in your lexical actions.
-   */
-  public static Token newToken(int ofKind, String image)
-  {
-     switch(ofKind)
-     {
-       default : return new Token(ofKind, image);
-     }
-  }
-
-  public static Token newToken(int ofKind)
-  {
-     return newToken(ofKind, null);
-  }
-
-}
-/* JavaCC - OriginalChecksum=7bf8bdbb1c45bccd8162cdd48316d5e0 (do not edit this line) */
diff --git a/lucene/src/demo/org/apache/lucene/demo/html/TokenMgrError.java b/lucene/src/demo/org/apache/lucene/demo/html/TokenMgrError.java
deleted file mode 100644
index 836d1f7..0000000
--- a/lucene/src/demo/org/apache/lucene/demo/html/TokenMgrError.java
+++ /dev/null
@@ -1,141 +0,0 @@
-/* Generated By:JavaCC: Do not edit this line. TokenMgrError.java Version 4.1 */
-/* JavaCCOptions: */
-package org.apache.lucene.demo.html;
-
-/** Token Manager Error. */
-@SuppressWarnings("serial")
-public class TokenMgrError extends Error
-{
-
-   /*
-    * Ordinals for various reasons why an Error of this type can be thrown.
-    */
-
-   /**
-    * Lexical error occurred.
-    */
-   static final int LEXICAL_ERROR = 0;
-
-   /**
-    * An attempt was made to create a second instance of a static token manager.
-    */
-   static final int STATIC_LEXER_ERROR = 1;
-
-   /**
-    * Tried to change to an invalid lexical state.
-    */
-   static final int INVALID_LEXICAL_STATE = 2;
-
-   /**
-    * Detected (and bailed out of) an infinite loop in the token manager.
-    */
-   static final int LOOP_DETECTED = 3;
-
-   /**
-    * Indicates the reason why the exception is thrown. It will have
-    * one of the above 4 values.
-    */
-   int errorCode;
-
-   /**
-    * Replaces unprintable characters by their escaped (or unicode escaped)
-    * equivalents in the given string
-    */
-   protected static final String addEscapes(String str) {
-      StringBuffer retval = new StringBuffer();
-      char ch;
-      for (int i = 0; i < str.length(); i++) {
-        switch (str.charAt(i))
-        {
-           case 0 :
-              continue;
-           case '\b':
-              retval.append("\\b");
-              continue;
-           case '\t':
-              retval.append("\\t");
-              continue;
-           case '\n':
-              retval.append("\\n");
-              continue;
-           case '\f':
-              retval.append("\\f");
-              continue;
-           case '\r':
-              retval.append("\\r");
-              continue;
-           case '\"':
-              retval.append("\\\"");
-              continue;
-           case '\'':
-              retval.append("\\\'");
-              continue;
-           case '\\':
-              retval.append("\\\\");
-              continue;
-           default:
-              if ((ch = str.charAt(i)) < 0x20 || ch > 0x7e) {
-                 String s = "0000" + Integer.toString(ch, 16);
-                 retval.append("\\u" + s.substring(s.length() - 4, s.length()));
-              } else {
-                 retval.append(ch);
-              }
-              continue;
-        }
-      }
-      return retval.toString();
-   }
-
-   /**
-    * Returns a detailed message for the Error when it is thrown by the
-    * token manager to indicate a lexical error.
-    * Parameters :
-    *    EOFSeen     : indicates if EOF caused the lexical error
-    *    curLexState : lexical state in which this error occurred
-    *    errorLine   : line number when the error occurred
-    *    errorColumn : column number when the error occurred
-    *    errorAfter  : prefix that was seen before this error occurred
-    *    curchar     : the offending character
-    * Note: You can customize the lexical error message by modifying this method.
-    */
-   protected static String LexicalError(boolean EOFSeen, int lexState, int errorLine, int errorColumn, String errorAfter, char curChar) {
-      return("Lexical error at line " +
-           errorLine + ", column " +
-           errorColumn + ".  Encountered: " +
-           (EOFSeen ? "<EOF> " : ("\"" + addEscapes(String.valueOf(curChar)) + "\"") + " (" + (int)curChar + "), ") +
-           "after : \"" + addEscapes(errorAfter) + "\"");
-   }
-
-   /**
-    * You can also modify the body of this method to customize your error messages.
-    * For example, cases like LOOP_DETECTED and INVALID_LEXICAL_STATE are not
-    * of end-users concern, so you can return something like :
-    *
-    *     "Internal Error : Please file a bug report .... "
-    *
-    * from this method for such cases in the release version of your parser.
-    */
-   public String getMessage() {
-      return super.getMessage();
-   }
-
-   /*
-    * Constructors of various flavors follow.
-    */
-
-   /** No arg constructor. */
-   public TokenMgrError() {
-   }
-
-   /** Constructor with message and reason. */
-   public TokenMgrError(String message, int reason) {
-      super(message);
-      errorCode = reason;
-   }
-
-   /** Full Constructor. */
-   public TokenMgrError(boolean EOFSeen, int lexState, int errorLine, int errorColumn, String errorAfter, char curChar, int reason) {
-      this(LexicalError(EOFSeen, lexState, errorLine, errorColumn, errorAfter, curChar), reason);
-   }
-}
-/* JavaCC - OriginalChecksum=5ffb7e46d5ae93d8d59e6f4ae7eb36d1 (do not edit this line) */
diff --git a/lucene/src/java/org/apache/lucene/analysis/KeywordAnalyzer.java b/lucene/src/java/org/apache/lucene/analysis/KeywordAnalyzer.java
deleted file mode 100644
index 74d0f4c..0000000
--- a/lucene/src/java/org/apache/lucene/analysis/KeywordAnalyzer.java
+++ /dev/null
@@ -1,34 +0,0 @@
-package org.apache.lucene.analysis;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Reader;
-
-/**
- * "Tokenizes" the entire stream as a single token. This is useful
- * for data like zip codes, ids, and some product names.
- */
-public final class KeywordAnalyzer extends ReusableAnalyzerBase {
-  public KeywordAnalyzer() {
-  }
-
-  @Override
-  protected TokenStreamComponents createComponents(final String fieldName, final Reader reader) {
-    return new TokenStreamComponents(new KeywordTokenizer(reader));
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/analysis/KeywordTokenizer.java b/lucene/src/java/org/apache/lucene/analysis/KeywordTokenizer.java
deleted file mode 100644
index 8b818be..0000000
--- a/lucene/src/java/org/apache/lucene/analysis/KeywordTokenizer.java
+++ /dev/null
@@ -1,91 +0,0 @@
-package org.apache.lucene.analysis;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.Reader;
-
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.util.AttributeSource;
-
-/**
- * Emits the entire input as a single token.
- */
-public final class KeywordTokenizer extends Tokenizer {
-  
-  private static final int DEFAULT_BUFFER_SIZE = 256;
-
-  private boolean done = false;
-  private int finalOffset;
-  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-  private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
-  
-  public KeywordTokenizer(Reader input) {
-    this(input, DEFAULT_BUFFER_SIZE);
-  }
-
-  public KeywordTokenizer(Reader input, int bufferSize) {
-    super(input);
-    termAtt.resizeBuffer(bufferSize);
-  }
-
-  public KeywordTokenizer(AttributeSource source, Reader input, int bufferSize) {
-    super(source, input);
-    termAtt.resizeBuffer(bufferSize);
-  }
-
-  public KeywordTokenizer(AttributeFactory factory, Reader input, int bufferSize) {
-    super(factory, input);
-    termAtt.resizeBuffer(bufferSize);
-  }
-  
-  @Override
-  public final boolean incrementToken() throws IOException {
-    if (!done) {
-      clearAttributes();
-      done = true;
-      int upto = 0;
-      char[] buffer = termAtt.buffer();
-      while (true) {
-        final int length = input.read(buffer, upto, buffer.length-upto);
-        if (length == -1) break;
-        upto += length;
-        if (upto == buffer.length)
-          buffer = termAtt.resizeBuffer(1+buffer.length);
-      }
-      termAtt.setLength(upto);
-      finalOffset = correctOffset(upto);
-      offsetAtt.setOffset(correctOffset(0), finalOffset);
-      return true;
-    }
-    return false;
-  }
-  
-  @Override
-  public final void end() {
-    // set final offset 
-    offsetAtt.setOffset(finalOffset, finalOffset);
-  }
-
-  @Override
-  public void reset(Reader input) throws IOException {
-    super.reset(input);
-    this.done = false;
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/analysis/LetterTokenizer.java b/lucene/src/java/org/apache/lucene/analysis/LetterTokenizer.java
deleted file mode 100644
index 8a79415..0000000
--- a/lucene/src/java/org/apache/lucene/analysis/LetterTokenizer.java
+++ /dev/null
@@ -1,125 +0,0 @@
-package org.apache.lucene.analysis;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Reader;
-
-import org.apache.lucene.util.AttributeSource;
-import org.apache.lucene.util.Version;
-
-/**
- * A LetterTokenizer is a tokenizer that divides text at non-letters. That's to
- * say, it defines tokens as maximal strings of adjacent letters, as defined by
- * java.lang.Character.isLetter() predicate.
- * <p>
- * Note: this does a decent job for most European languages, but does a terrible
- * job for some Asian languages, where words are not separated by spaces.
- * </p>
- * <p>
- * <a name="version"/>
- * You must specify the required {@link Version} compatibility when creating
- * {@link LetterTokenizer}:
- * <ul>
- * <li>As of 3.1, {@link CharTokenizer} uses an int based API to normalize and
- * detect token characters. See {@link CharTokenizer#isTokenChar(int)} and
- * {@link CharTokenizer#normalize(int)} for details.</li>
- * </ul>
- * </p>
- */
-
-public class LetterTokenizer extends CharTokenizer {
-  
-  /**
-   * Construct a new LetterTokenizer.
-   * 
-   * @param matchVersion
-   *          Lucene version to match See {@link <a href="#version">above</a>}
-   * @param in
-   *          the input to split up into tokens
-   */
-  public LetterTokenizer(Version matchVersion, Reader in) {
-    super(matchVersion, in);
-  }
-  
-  /**
-   * Construct a new LetterTokenizer using a given {@link AttributeSource}.
-   * 
-   * @param matchVersion
-   *          Lucene version to match See {@link <a href="#version">above</a>}
-   * @param source
-   *          the attribute source to use for this {@link Tokenizer}
-   * @param in
-   *          the input to split up into tokens
-   */
-  public LetterTokenizer(Version matchVersion, AttributeSource source, Reader in) {
-    super(matchVersion, source, in);
-  }
-  
-  /**
-   * Construct a new LetterTokenizer using a given
-   * {@link org.apache.lucene.util.AttributeSource.AttributeFactory}.
-   * 
-   * @param matchVersion
-   *          Lucene version to match See {@link <a href="#version">above</a>}
-   * @param factory
-   *          the attribute factory to use for this {@link Tokenizer}
-   * @param in
-   *          the input to split up into tokens
-   */
-  public LetterTokenizer(Version matchVersion, AttributeFactory factory, Reader in) {
-    super(matchVersion, factory, in);
-  }
-  
-  /**
-   * Construct a new LetterTokenizer.
-   * 
-   * @deprecated use {@link #LetterTokenizer(Version, Reader)} instead. This
-   *             will be removed in Lucene 4.0.
-   */
-  public LetterTokenizer(Reader in) {
-    super(Version.LUCENE_30, in);
-  }
-  
-  /**
-   * Construct a new LetterTokenizer using a given {@link AttributeSource}. 
-   * @deprecated
-   * use {@link #LetterTokenizer(Version, AttributeSource, Reader)} instead.
-   * This will be removed in Lucene 4.0.
-   */
-  public LetterTokenizer(AttributeSource source, Reader in) {
-    super(Version.LUCENE_30, source, in);
-  }
-  
-  /**
-   * Construct a new LetterTokenizer using a given
-   * {@link org.apache.lucene.util.AttributeSource.AttributeFactory}.
-   * 
-   * @deprecated use {@link #LetterTokenizer(Version, AttributeSource.AttributeFactory, Reader)}
-   *             instead. This will be removed in Lucene 4.0.
-   */
-  public LetterTokenizer(AttributeFactory factory, Reader in) {
-    super(Version.LUCENE_30, factory, in);
-  }
-  
-  /** Collects only characters which satisfy
-   * {@link Character#isLetter(int)}.*/
-  @Override
-  protected boolean isTokenChar(int c) {
-    return Character.isLetter(c);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/analysis/LowerCaseFilter.java b/lucene/src/java/org/apache/lucene/analysis/LowerCaseFilter.java
deleted file mode 100644
index 7a4d769..0000000
--- a/lucene/src/java/org/apache/lucene/analysis/LowerCaseFilter.java
+++ /dev/null
@@ -1,72 +0,0 @@
-package org.apache.lucene.analysis;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.util.CharacterUtils;
-import org.apache.lucene.util.Version;
-
-/**
- * Normalizes token text to lower case.
- * <a name="version"/>
- * <p>You must specify the required {@link Version}
- * compatibility when creating LowerCaseFilter:
- * <ul>
- *   <li> As of 3.1, supplementary characters are properly lowercased.
- * </ul>
- */
-public final class LowerCaseFilter extends TokenFilter {
-  private final CharacterUtils charUtils;
-  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-  
-  /**
-   * Create a new LowerCaseFilter, that normalizes token text to lower case.
-   * 
-   * @param matchVersion See <a href="#version">above</a>
-   * @param in TokenStream to filter
-   */
-  public LowerCaseFilter(Version matchVersion, TokenStream in) {
-    super(in);
-    charUtils = CharacterUtils.getInstance(matchVersion);
-  }
-  
-  /**
-   * @deprecated Use {@link #LowerCaseFilter(Version, TokenStream)} instead.
-   */
-  @Deprecated
-  public LowerCaseFilter(TokenStream in) {
-    this(Version.LUCENE_30, in);
-  }
-
-  @Override
-  public final boolean incrementToken() throws IOException {
-    if (input.incrementToken()) {
-      final char[] buffer = termAtt.buffer();
-      final int length = termAtt.length();
-      for (int i = 0; i < length;) {
-       i += Character.toChars(
-               Character.toLowerCase(
-                   charUtils.codePointAt(buffer, i)), buffer, i);
-      }
-      return true;
-    } else
-      return false;
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/analysis/LowerCaseTokenizer.java b/lucene/src/java/org/apache/lucene/analysis/LowerCaseTokenizer.java
deleted file mode 100644
index ee102f0..0000000
--- a/lucene/src/java/org/apache/lucene/analysis/LowerCaseTokenizer.java
+++ /dev/null
@@ -1,128 +0,0 @@
-package org.apache.lucene.analysis;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Reader;
-
-import org.apache.lucene.util.AttributeSource;
-import org.apache.lucene.util.Version;
-
-/**
- * LowerCaseTokenizer performs the function of LetterTokenizer
- * and LowerCaseFilter together.  It divides text at non-letters and converts
- * them to lower case.  While it is functionally equivalent to the combination
- * of LetterTokenizer and LowerCaseFilter, there is a performance advantage
- * to doing the two tasks at once, hence this (redundant) implementation.
- * <P>
- * Note: this does a decent job for most European languages, but does a terrible
- * job for some Asian languages, where words are not separated by spaces.
- * </p>
- * <p>
- * <a name="version"/>
- * You must specify the required {@link Version} compatibility when creating
- * {@link LowerCaseTokenizer}:
- * <ul>
- * <li>As of 3.1, {@link CharTokenizer} uses an int based API to normalize and
- * detect token characters. See {@link CharTokenizer#isTokenChar(int)} and
- * {@link CharTokenizer#normalize(int)} for details.</li>
- * </ul>
- * </p>
- */
-public final class LowerCaseTokenizer extends LetterTokenizer {
-  
-  /**
-   * Construct a new LowerCaseTokenizer.
-   * 
-   * @param matchVersion
-   *          Lucene version to match See {@link <a href="#version">above</a>}
-   * 
-   * @param in
-   *          the input to split up into tokens
-   */
-  public LowerCaseTokenizer(Version matchVersion, Reader in) {
-    super(matchVersion, in);
-  }
-
-  /** 
-   * Construct a new LowerCaseTokenizer using a given {@link AttributeSource}.
-   *
-   * @param matchVersion
-   *          Lucene version to match See {@link <a href="#version">above</a>}
-   * @param source
-   *          the attribute source to use for this {@link Tokenizer}
-   * @param in
-   *          the input to split up into tokens
-   */
-  public LowerCaseTokenizer(Version matchVersion, AttributeSource source, Reader in) {
-    super(matchVersion, source, in);
-  }
-
-  /**
-   * Construct a new LowerCaseTokenizer using a given
-   * {@link org.apache.lucene.util.AttributeSource.AttributeFactory}.
-   *
-   * @param matchVersion
-   *          Lucene version to match See {@link <a href="#version">above</a>}
-   * @param factory
-   *          the attribute factory to use for this {@link Tokenizer}
-   * @param in
-   *          the input to split up into tokens
-   */
-  public LowerCaseTokenizer(Version matchVersion, AttributeFactory factory, Reader in) {
-    super(matchVersion, factory, in);
-  }
-  
-  /**
-   * Construct a new LowerCaseTokenizer.
-   * 
-   * @deprecated use {@link #LowerCaseTokenizer(Reader)} instead. This will be
-   *             removed in Lucene 4.0.
-   */
-  @Deprecated
-  public LowerCaseTokenizer(Reader in) {
-    super(Version.LUCENE_30, in);
-  }
-
-  /**
-   * Construct a new LowerCaseTokenizer using a given {@link AttributeSource}.
-   * 
-   * @deprecated use {@link #LowerCaseTokenizer(AttributeSource, Reader)}
-   *             instead. This will be removed in Lucene 4.0.
-   */
-  public LowerCaseTokenizer(AttributeSource source, Reader in) {
-    super(Version.LUCENE_30, source, in);
-  }
-
-  /**
-   * Construct a new LowerCaseTokenizer using a given
-   * {@link org.apache.lucene.util.AttributeSource.AttributeFactory}.
-   * 
-   * @deprecated use {@link #LowerCaseTokenizer(AttributeSource.AttributeFactory, Reader)}
-   *             instead. This will be removed in Lucene 4.0.
-   */
-  public LowerCaseTokenizer(AttributeFactory factory, Reader in) {
-    super(Version.LUCENE_30, factory, in);
-  }
-  
-  /** Converts char to lower case
-   * {@link Character#toLowerCase(int)}.*/
-  @Override
-  protected int normalize(int c) {
-    return Character.toLowerCase(c);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/analysis/ReusableAnalyzerBase.java b/lucene/src/java/org/apache/lucene/analysis/ReusableAnalyzerBase.java
deleted file mode 100644
index 2c3986a..0000000
--- a/lucene/src/java/org/apache/lucene/analysis/ReusableAnalyzerBase.java
+++ /dev/null
@@ -1,163 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.lucene.analysis;
-
-import java.io.IOException;
-import java.io.Reader;
-
-/**
- * An convenience subclass of Analyzer that makes it easy to implement
- * {@link TokenStream} reuse.
- * <p>
- * ReusableAnalyzerBase is a simplification of Analyzer that supports easy reuse
- * for the most common use-cases. Analyzers such as
- * {@link PerFieldAnalyzerWrapper} that behave differently depending upon the
- * field name need to subclass Analyzer directly instead.
- * </p>
- * <p>
- * To prevent consistency problems, this class does not allow subclasses to
- * extend {@link #reusableTokenStream(String, Reader)} or
- * {@link #tokenStream(String, Reader)} directly. Instead, subclasses must
- * implement {@link #createComponents(String, Reader)}.
- * </p>
- */
-public abstract class ReusableAnalyzerBase extends Analyzer {
-
-  /**
-   * Creates a new {@link TokenStreamComponents} instance for this analyzer.
-   * 
-   * @param fieldName
-   *          the name of the fields content passed to the
-   *          {@link TokenStreamComponents} sink as a reader
-   * @param aReader
-   *          the reader passed to the {@link Tokenizer} constructor
-   * @return the {@link TokenStreamComponents} for this analyzer.
-   */
-  protected abstract TokenStreamComponents createComponents(String fieldName,
-      Reader aReader);
-
-  /**
-   * This method uses {@link #createComponents(String, Reader)} to obtain an
-   * instance of {@link TokenStreamComponents}. It returns the sink of the
-   * components and stores the components internally. Subsequent calls to this
-   * method will reuse the previously stored components if and only if the
-   * {@link TokenStreamComponents#reset(Reader)} method returned
-   * <code>true</code>. Otherwise a new instance of
-   * {@link TokenStreamComponents} is created.
-   * 
-   * @param fieldName the name of the field the created TokenStream is used for
-   * @param reader the reader the streams source reads from
-   */
-  @Override
-  public final TokenStream reusableTokenStream(final String fieldName,
-      final Reader reader) throws IOException {
-    TokenStreamComponents streamChain = (TokenStreamComponents)
-    getPreviousTokenStream();
-    if (streamChain == null || !streamChain.reset(reader)) {
-      streamChain = createComponents(fieldName, reader);
-      setPreviousTokenStream(streamChain);
-    }
-    return streamChain.getTokenStream();
-  }
-
-  /**
-   * This method uses {@link #createComponents(String, Reader)} to obtain an
-   * instance of {@link TokenStreamComponents} and returns the sink of the
-   * components. Each calls to this method will create a new instance of
-   * {@link TokenStreamComponents}. Created {@link TokenStream} instances are 
-   * never reused.
-   * 
-   * @param fieldName the name of the field the created TokenStream is used for
-   * @param reader the reader the streams source reads from
-   */
-  @Override
-  public final TokenStream tokenStream(final String fieldName,
-      final Reader reader) {
-    return createComponents(fieldName, reader).getTokenStream();
-  }
-  
-  /**
-   * This class encapsulates the outer components of a token stream. It provides
-   * access to the source ({@link Tokenizer}) and the outer end (sink), an
-   * instance of {@link TokenFilter} which also serves as the
-   * {@link TokenStream} returned by
-   * {@link Analyzer#tokenStream(String, Reader)} and
-   * {@link Analyzer#reusableTokenStream(String, Reader)}.
-   */
-  public static class TokenStreamComponents {
-    protected final Tokenizer source;
-    protected final TokenStream sink;
-
-    /**
-     * Creates a new {@link TokenStreamComponents} instance.
-     * 
-     * @param source
-     *          the analyzer's tokenizer
-     * @param result
-     *          the analyzer's resulting token stream
-     */
-    public TokenStreamComponents(final Tokenizer source,
-        final TokenStream result) {
-      this.source = source;
-      this.sink = result;
-    }
-    
-    /**
-     * Creates a new {@link TokenStreamComponents} instance.
-     * 
-     * @param source
-     *          the analyzer's tokenizer
-     */
-    public TokenStreamComponents(final Tokenizer source) {
-      this.source = source;
-      this.sink = source;
-    }
-
-    /**
-     * Resets the encapsulated components with the given reader. This method by
-     * default returns <code>true</code> indicating that the components have
-     * been reset successfully. Subclasses of {@link ReusableAnalyzerBase} might use
-     * their own {@link TokenStreamComponents} returning <code>false</code> if
-     * the components cannot be reset.
-     * 
-     * @param reader
-     *          a reader to reset the source component
-     * @return <code>true</code> if the components were reset, otherwise
-     *         <code>false</code>
-     * @throws IOException
-     *           if the component's reset method throws an {@link IOException}
-     */
-    protected boolean reset(final Reader reader) throws IOException {
-      source.reset(reader);
-      if(sink != source)
-        sink.reset(); // only reset if the sink reference is different from source
-      return true;
-    }
-
-    /**
-     * Returns the sink {@link TokenStream}
-     * 
-     * @return the sink {@link TokenStream}
-     */
-    protected TokenStream getTokenStream() {
-      return sink;
-    }
-
-  }
-
-}
diff --git a/lucene/src/java/org/apache/lucene/analysis/SimpleAnalyzer.java b/lucene/src/java/org/apache/lucene/analysis/SimpleAnalyzer.java
deleted file mode 100644
index 590c18e..0000000
--- a/lucene/src/java/org/apache/lucene/analysis/SimpleAnalyzer.java
+++ /dev/null
@@ -1,60 +0,0 @@
-package org.apache.lucene.analysis;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Reader;
-
-import org.apache.lucene.util.Version;
-
-/** An {@link Analyzer} that filters {@link LetterTokenizer} 
- *  with {@link LowerCaseFilter} 
- * <p>
- * <a name="version">You must specify the required {@link Version} compatibility
- * when creating {@link CharTokenizer}:
- * <ul>
- * <li>As of 3.1, {@link LowerCaseTokenizer} uses an int based API to normalize and
- * detect token codepoints. See {@link CharTokenizer#isTokenChar(int)} and
- * {@link CharTokenizer#normalize(int)} for details.</li>
- * </ul>
- * <p>
- **/
-public final class SimpleAnalyzer extends ReusableAnalyzerBase {
-
-  private final Version matchVersion;
-  
-  /**
-   * Creates a new {@link SimpleAnalyzer}
-   * @param matchVersion Lucene version to match See {@link <a href="#version">above</a>}
-   */
-  public SimpleAnalyzer(Version matchVersion) {
-    this.matchVersion = matchVersion;
-  }
-  
-  /**
-   * Creates a new {@link SimpleAnalyzer}
-   * @deprecated use {@link #SimpleAnalyzer(Version)} instead 
-   */
-  @Deprecated  public SimpleAnalyzer() {
-    this(Version.LUCENE_30);
-  }
-  @Override
-  protected TokenStreamComponents createComponents(final String fieldName,
-      final Reader reader) {
-    return new TokenStreamComponents(new LowerCaseTokenizer(matchVersion, reader));
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/analysis/StopAnalyzer.java b/lucene/src/java/org/apache/lucene/analysis/StopAnalyzer.java
deleted file mode 100644
index 387281a..0000000
--- a/lucene/src/java/org/apache/lucene/analysis/StopAnalyzer.java
+++ /dev/null
@@ -1,109 +0,0 @@
-package org.apache.lucene.analysis;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.File;
-import java.io.IOException;
-import java.io.Reader;
-import java.util.Arrays;
-import java.util.Set;
-import java.util.List;
-
-import org.apache.lucene.util.Version;
-
-/** Filters {@link LetterTokenizer} with {@link LowerCaseFilter} and {@link StopFilter}.
- *
- * <a name="version"/>
- * <p>You must specify the required {@link Version}
- * compatibility when creating StopAnalyzer:
- * <ul>
- *    <li> As of 3.1, StopFilter correctly handles Unicode 4.0
- *         supplementary characters in stopwords
- *   <li> As of 2.9, position increments are preserved
- * </ul>
-*/
-
-public final class StopAnalyzer extends StopwordAnalyzerBase {
-  
-  /** An unmodifiable set containing some common English words that are not usually useful
-  for searching.*/
-  public static final Set<?> ENGLISH_STOP_WORDS_SET;
-  
-  static {
-    final List<String> stopWords = Arrays.asList(
-      "a", "an", "and", "are", "as", "at", "be", "but", "by",
-      "for", "if", "in", "into", "is", "it",
-      "no", "not", "of", "on", "or", "such",
-      "that", "the", "their", "then", "there", "these",
-      "they", "this", "to", "was", "will", "with"
-    );
-    final CharArraySet stopSet = new CharArraySet(Version.LUCENE_CURRENT, 
-        stopWords.size(), false);
-    stopSet.addAll(stopWords);  
-    ENGLISH_STOP_WORDS_SET = CharArraySet.unmodifiableSet(stopSet); 
-  }
-  
-  /** Builds an analyzer which removes words in
-   *  {@link #ENGLISH_STOP_WORDS_SET}.
-   * @param matchVersion See <a href="#version">above</a>
-   */
-  public StopAnalyzer(Version matchVersion) {
-    this(matchVersion, ENGLISH_STOP_WORDS_SET);
-  }
-
-  /** Builds an analyzer with the stop words from the given set.
-   * @param matchVersion See <a href="#version">above</a>
-   * @param stopWords Set of stop words */
-  public StopAnalyzer(Version matchVersion, Set<?> stopWords) {
-    super(matchVersion, stopWords);
-  }
-
-  /** Builds an analyzer with the stop words from the given file.
-   * @see WordlistLoader#getWordSet(File)
-   * @param matchVersion See <a href="#version">above</a>
-   * @param stopwordsFile File to load stop words from */
-  public StopAnalyzer(Version matchVersion, File stopwordsFile) throws IOException {
-    this(matchVersion, WordlistLoader.getWordSet(stopwordsFile));
-  }
-
-  /** Builds an analyzer with the stop words from the given reader.
-   * @see WordlistLoader#getWordSet(Reader)
-   * @param matchVersion See <a href="#version">above</a>
-   * @param stopwords Reader to load stop words from */
-  public StopAnalyzer(Version matchVersion, Reader stopwords) throws IOException {
-    this(matchVersion, WordlistLoader.getWordSet(stopwords));
-  }
-
-  /**
-   * Creates
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
-   * used to tokenize all the text in the provided {@link Reader}.
-   * 
-   * @return {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
-   *         built from a {@link LowerCaseTokenizer} filtered with
-   *         {@link StopFilter}
-   */
-  @Override
-  protected TokenStreamComponents createComponents(String fieldName,
-      Reader reader) {
-    final Tokenizer source = new LowerCaseTokenizer(matchVersion, reader);
-    return new TokenStreamComponents(source, new StopFilter(matchVersion,
-          source, stopwords));
-  }
-}
-
diff --git a/lucene/src/java/org/apache/lucene/analysis/StopFilter.java b/lucene/src/java/org/apache/lucene/analysis/StopFilter.java
deleted file mode 100644
index 18d1a8a..0000000
--- a/lucene/src/java/org/apache/lucene/analysis/StopFilter.java
+++ /dev/null
@@ -1,309 +0,0 @@
-package org.apache.lucene.analysis;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.Set;
-import java.util.List;
-
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.queryParser.QueryParser; // for javadoc
-import org.apache.lucene.util.Version;
-
-/**
- * Removes stop words from a token stream.
- * 
- * <a name="version"/>
- * <p>You must specify the required {@link Version}
- * compatibility when creating StopFilter:
- * <ul>
- *   <li> As of 3.1, StopFilter correctly handles Unicode 4.0
- *         supplementary characters in stopwords and position
- *         increments are preserved
- * </ul>
- */
-public final class StopFilter extends TokenFilter {
-
-  private final CharArraySet stopWords;
-  private boolean enablePositionIncrements = false;
-
-  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-  private final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);
-
-  /**
-   * Construct a token stream filtering the given input.
-   * If <code>stopWords</code> is an instance of {@link CharArraySet} (true if
-   * <code>makeStopSet()</code> was used to construct the set) it will be directly used
-   * and <code>ignoreCase</code> will be ignored since <code>CharArraySet</code>
-   * directly controls case sensitivity.
-   * <p/>
-   * If <code>stopWords</code> is not an instance of {@link CharArraySet},
-   * a new CharArraySet will be constructed and <code>ignoreCase</code> will be
-   * used to specify the case sensitivity of that set.
-   *
-   * @param enablePositionIncrements true if token positions should record the removed stop words
-   * @param input Input TokenStream
-   * @param stopWords A Set of Strings or char[] or any other toString()-able set representing the stopwords
-   * @param ignoreCase if true, all words are lower cased first
-   * @deprecated use {@link #StopFilter(Version, TokenStream, Set, boolean)} instead
-   */
-  @Deprecated
-  public StopFilter(boolean enablePositionIncrements, TokenStream input, Set<?> stopWords, boolean ignoreCase)
-  {
-    this(Version.LUCENE_30, enablePositionIncrements, input, stopWords, ignoreCase);
-  }
-  
-  /**
-   * Construct a token stream filtering the given input. If
-   * <code>stopWords</code> is an instance of {@link CharArraySet} (true if
-   * <code>makeStopSet()</code> was used to construct the set) it will be
-   * directly used and <code>ignoreCase</code> will be ignored since
-   * <code>CharArraySet</code> directly controls case sensitivity.
-   * <p/>
-   * If <code>stopWords</code> is not an instance of {@link CharArraySet}, a new
-   * CharArraySet will be constructed and <code>ignoreCase</code> will be used
-   * to specify the case sensitivity of that set.
-   * 
-   * @param matchVersion
-   *          Lucene version to enable correct Unicode 4.0 behavior in the stop
-   *          set if Version > 3.0. See <a href="#version">above</a> for details.
-   * @param input
-   *          Input TokenStream
-   * @param stopWords
-   *          A Set of Strings or char[] or any other toString()-able set
-   *          representing the stopwords
-   * @param ignoreCase
-   *          if true, all words are lower cased first
-   */
-  public StopFilter(Version matchVersion, TokenStream input, Set<?> stopWords, boolean ignoreCase)
-  {
-   this(matchVersion, matchVersion.onOrAfter(Version.LUCENE_29), input, stopWords, ignoreCase);
-  }
-  
-  /*
-   * convenience ctor to enable deprecated ctors to set posInc explicitly
-   */
-  private StopFilter(Version matchVersion, boolean enablePositionIncrements, TokenStream input, Set<?> stopWords, boolean ignoreCase){
-    super(input);
-    this.stopWords = stopWords instanceof CharArraySet ? (CharArraySet)stopWords : new CharArraySet(matchVersion, stopWords, ignoreCase);  
-    this.enablePositionIncrements = enablePositionIncrements;
-  }
-
-  /**
-   * Constructs a filter which removes words from the input
-   * TokenStream that are named in the Set.
-   *
-   * @param enablePositionIncrements true if token positions should record the removed stop words
-   * @param in Input stream
-   * @param stopWords A Set of Strings or char[] or any other toString()-able set representing the stopwords
-   * @see #makeStopSet(Version, java.lang.String[])
-   * @deprecated use {@link #StopFilter(Version, TokenStream, Set)} instead
-   */
-  @Deprecated
-  public StopFilter(boolean enablePositionIncrements, TokenStream in, Set<?> stopWords) {
-    this(Version.LUCENE_CURRENT, enablePositionIncrements, in, stopWords, false);
-  }
-  
-  /**
-   * Constructs a filter which removes words from the input TokenStream that are
-   * named in the Set.
-   * 
-   * @param matchVersion
-   *          Lucene version to enable correct Unicode 4.0 behavior in the stop
-   *          set if Version > 3.0.  See <a href="#version">above</a> for details.
-   * @param in
-   *          Input stream
-   * @param stopWords
-   *          A Set of Strings or char[] or any other toString()-able set
-   *          representing the stopwords
-   * @see #makeStopSet(Version, java.lang.String[])
-   */
-  public StopFilter(Version matchVersion, TokenStream in, Set<?> stopWords) {
-    this(matchVersion, in, stopWords, false);
-  }
-
-  /**
-   * Builds a Set from an array of stop words,
-   * appropriate for passing into the StopFilter constructor.
-   * This permits this stopWords construction to be cached once when
-   * an Analyzer is constructed.
-   * 
-   * @see #makeStopSet(Version, java.lang.String[], boolean) passing false to ignoreCase
-   * @deprecated use {@link #makeStopSet(Version, String...)} instead
-   */
-  @Deprecated
-  public static final Set<Object> makeStopSet(String... stopWords) {
-    return makeStopSet(Version.LUCENE_30, stopWords, false);
-  }
-
-  /**
-   * Builds a Set from an array of stop words,
-   * appropriate for passing into the StopFilter constructor.
-   * This permits this stopWords construction to be cached once when
-   * an Analyzer is constructed.
-   * 
-   * @param matchVersion Lucene version to enable correct Unicode 4.0 behavior in the returned set if Version > 3.0
-   * @param stopWords An array of stopwords
-   * @see #makeStopSet(Version, java.lang.String[], boolean) passing false to ignoreCase
-   */
-  public static final Set<Object> makeStopSet(Version matchVersion, String... stopWords) {
-    return makeStopSet(matchVersion, stopWords, false);
-  }
-  
-  /**
-   * Builds a Set from an array of stop words,
-   * appropriate for passing into the StopFilter constructor.
-   * This permits this stopWords construction to be cached once when
-   * an Analyzer is constructed.
-   * @param stopWords A List of Strings or char[] or any other toString()-able list representing the stopwords
-   * @return A Set ({@link CharArraySet}) containing the words
-   * @see #makeStopSet(Version, java.lang.String[], boolean) passing false to ignoreCase
-   * @deprecated use {@link #makeStopSet(Version, List)} instead
-   */
-  @Deprecated
-  public static final Set<Object> makeStopSet(List<?> stopWords) {
-    return makeStopSet(Version.LUCENE_30, stopWords, false);
-  }
-
-  /**
-   * Builds a Set from an array of stop words,
-   * appropriate for passing into the StopFilter constructor.
-   * This permits this stopWords construction to be cached once when
-   * an Analyzer is constructed.
-   * 
-   * @param matchVersion Lucene version to enable correct Unicode 4.0 behavior in the returned set if Version > 3.0
-   * @param stopWords A List of Strings or char[] or any other toString()-able list representing the stopwords
-   * @return A Set ({@link CharArraySet}) containing the words
-   * @see #makeStopSet(Version, java.lang.String[], boolean) passing false to ignoreCase
-   */
-  public static final Set<Object> makeStopSet(Version matchVersion, List<?> stopWords) {
-    return makeStopSet(matchVersion, stopWords, false);
-  }
-    
-  /**
-   * Creates a stopword set from the given stopword array.
-   * @param stopWords An array of stopwords
-   * @param ignoreCase If true, all words are lower cased first.  
-   * @return a Set containing the words
-   * @deprecated use {@link #makeStopSet(Version, String[], boolean)} instead;
-   */  
-  @Deprecated
-  public static final Set<Object> makeStopSet(String[] stopWords, boolean ignoreCase) {
-    return makeStopSet(Version.LUCENE_30, stopWords, ignoreCase);
-  }
-  /**
-   * Creates a stopword set from the given stopword array.
-   * 
-   * @param matchVersion Lucene version to enable correct Unicode 4.0 behavior in the returned set if Version > 3.0
-   * @param stopWords An array of stopwords
-   * @param ignoreCase If true, all words are lower cased first.  
-   * @return a Set containing the words
-   */    
-  public static final Set<Object> makeStopSet(Version matchVersion, String[] stopWords, boolean ignoreCase) {
-    CharArraySet stopSet = new CharArraySet(matchVersion, stopWords.length, ignoreCase);
-    stopSet.addAll(Arrays.asList(stopWords));
-    return stopSet;
-  }
-  
-  /**
-   * Creates a stopword set from the given stopword list.
-   * @param stopWords A List of Strings or char[] or any other toString()-able list representing the stopwords
-   * @param ignoreCase if true, all words are lower cased first
-   * @return A Set ({@link CharArraySet}) containing the words
-   * @deprecated use {@link #makeStopSet(Version, List, boolean)} instead
-   */
-  @Deprecated
-  public static final Set<Object> makeStopSet(List<?> stopWords, boolean ignoreCase){
-    return makeStopSet(Version.LUCENE_30, stopWords, ignoreCase);
-  }
-
-  /**
-   * Creates a stopword set from the given stopword list.
-   * @param matchVersion Lucene version to enable correct Unicode 4.0 behavior in the returned set if Version > 3.0
-   * @param stopWords A List of Strings or char[] or any other toString()-able list representing the stopwords
-   * @param ignoreCase if true, all words are lower cased first
-   * @return A Set ({@link CharArraySet}) containing the words
-   */
-  public static final Set<Object> makeStopSet(Version matchVersion, List<?> stopWords, boolean ignoreCase){
-    CharArraySet stopSet = new CharArraySet(matchVersion, stopWords.size(), ignoreCase);
-    stopSet.addAll(stopWords);
-    return stopSet;
-  }
-  
-  /**
-   * Returns the next input Token whose term() is not a stop word.
-   */
-  @Override
-  public final boolean incrementToken() throws IOException {
-    // return the first non-stop word found
-    int skippedPositions = 0;
-    while (input.incrementToken()) {
-      if (!stopWords.contains(termAtt.buffer(), 0, termAtt.length())) {
-        if (enablePositionIncrements) {
-          posIncrAtt.setPositionIncrement(posIncrAtt.getPositionIncrement() + skippedPositions);
-        }
-        return true;
-      }
-      skippedPositions += posIncrAtt.getPositionIncrement();
-    }
-    // reached EOS -- return false
-    return false;
-  }
-
-  /**
-   * Returns version-dependent default for
-   * enablePositionIncrements.  Analyzers that embed
-   * StopFilter use this method when creating the
-   * StopFilter.  Prior to 2.9, this returns false.  On 2.9
-   * or later, it returns true.
-   * @deprecated use {@link #StopFilter(Version, TokenStream, Set)} instead
-   */
-  @Deprecated
-  public static boolean getEnablePositionIncrementsVersionDefault(Version matchVersion) {
-    return matchVersion.onOrAfter(Version.LUCENE_29);
-  }
-
-  /**
-   * @see #setEnablePositionIncrements(boolean)
-   */
-  public boolean getEnablePositionIncrements() {
-    return enablePositionIncrements;
-  }
-
-  /**
-   * If <code>true</code>, this StopFilter will preserve
-   * positions of the incoming tokens (ie, accumulate and
-   * set position increments of the removed stop tokens).
-   * Generally, <code>true</code> is best as it does not
-   * lose information (positions of the original tokens)
-   * during indexing.
-   * 
-   * <p> When set, when a token is stopped
-   * (omitted), the position increment of the following
-   * token is incremented.
-   *
-   * <p> <b>NOTE</b>: be sure to also
-   * set {@link QueryParser#setEnablePositionIncrements} if
-   * you use QueryParser to create queries.
-   */
-  public void setEnablePositionIncrements(boolean enable) {
-    this.enablePositionIncrements = enable;
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/analysis/StopwordAnalyzerBase.java b/lucene/src/java/org/apache/lucene/analysis/StopwordAnalyzerBase.java
deleted file mode 100644
index 4e68212..0000000
--- a/lucene/src/java/org/apache/lucene/analysis/StopwordAnalyzerBase.java
+++ /dev/null
@@ -1,105 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.lucene.analysis;
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.analysis.CharArraySet;
-import org.apache.lucene.analysis.ReusableAnalyzerBase;
-import org.apache.lucene.analysis.WordlistLoader;
-import org.apache.lucene.util.Version;
-
-/**
- * Base class for Analyzers that need to make use of stopword sets. 
- * 
- */
-public abstract class StopwordAnalyzerBase extends ReusableAnalyzerBase {
-
-  /**
-   * An immutable stopword set
-   */
-  protected final CharArraySet stopwords;
-
-  protected final Version matchVersion;
-
-  /**
-   * Returns the analyzer's stopword set or an empty set if the analyzer has no
-   * stopwords
-   * 
-   * @return the analyzer's stopword set or an empty set if the analyzer has no
-   *         stopwords
-   */
-  public Set<?> getStopwordSet() {
-    return stopwords;
-  }
-
-  /**
-   * Creates a new instance initialized with the given stopword set
-   * 
-   * @param version
-   *          the Lucene version for cross version compatibility
-   * @param stopwords
-   *          the analyzer's stopword set
-   */
-  protected StopwordAnalyzerBase(final Version version, final Set<?> stopwords) {
-    matchVersion = version;
-    // analyzers should use char array set for stopwords!
-    this.stopwords = stopwords == null ? CharArraySet.EMPTY_SET : CharArraySet
-        .unmodifiableSet(CharArraySet.copy(version, stopwords));
-  }
-
-  /**
-   * Creates a new Analyzer with an empty stopword set
-   * 
-   * @param version
-   *          the Lucene version for cross version compatibility
-   */
-  protected StopwordAnalyzerBase(final Version version) {
-    this(version, null);
-  }
-
-  /**
-   * Creates a CharArraySet from a file resource associated with a class. (See
-   * {@link Class#getResourceAsStream(String)}).
-   * 
-   * @param ignoreCase
-   *          <code>true</code> if the set should ignore the case of the
-   *          stopwords, otherwise <code>false</code>
-   * @param aClass
-   *          a class that is associated with the given stopwordResource
-   * @param resource
-   *          name of the resource file associated with the given class
-   * @param comment
-   *          comment string to ignore in the stopword file
-   * @return a CharArraySet containing the distinct stopwords from the given
-   *         file
-   * @throws IOException
-   *           if loading the stopwords throws an {@link IOException}
-   */
-  protected static CharArraySet loadStopwordSet(final boolean ignoreCase,
-      final Class<? extends ReusableAnalyzerBase> aClass, final String resource,
-      final String comment) throws IOException {
-    final Set<String> wordSet = WordlistLoader.getWordSet(aClass, resource,
-        comment);
-    final CharArraySet set = new CharArraySet(Version.LUCENE_31, wordSet.size(), ignoreCase);
-    set.addAll(wordSet);
-    return set;
-  }
-
-}
diff --git a/lucene/src/java/org/apache/lucene/analysis/TokenStream.java b/lucene/src/java/org/apache/lucene/analysis/TokenStream.java
index e432132..6fb7e8c 100644
--- a/lucene/src/java/org/apache/lucene/analysis/TokenStream.java
+++ b/lucene/src/java/org/apache/lucene/analysis/TokenStream.java
@@ -156,7 +156,7 @@ public abstract class TokenStream extends AttributeSource implements Closeable {
    * This method can be used to perform any end-of-stream operations, such as
    * setting the final offset of a stream. The final offset of a stream might
    * differ from the offset of the last token eg in case one or more whitespaces
-   * followed after the last token, but a {@link WhitespaceTokenizer} was used.
+   * followed after the last token, but a WhitespaceTokenizer was used.
    * 
    * @throws IOException
    */
diff --git a/lucene/src/java/org/apache/lucene/analysis/WhitespaceAnalyzer.java b/lucene/src/java/org/apache/lucene/analysis/WhitespaceAnalyzer.java
deleted file mode 100644
index 011749c..0000000
--- a/lucene/src/java/org/apache/lucene/analysis/WhitespaceAnalyzer.java
+++ /dev/null
@@ -1,62 +0,0 @@
-package org.apache.lucene.analysis;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Reader;
-
-import org.apache.lucene.util.Version;
-
-/**
- * An Analyzer that uses {@link WhitespaceTokenizer}.
- * <p>
- * <a name="version">You must specify the required {@link Version} compatibility
- * when creating {@link CharTokenizer}:
- * <ul>
- * <li>As of 3.1, {@link WhitespaceTokenizer} uses an int based API to normalize and
- * detect token codepoints. See {@link CharTokenizer#isTokenChar(int)} and
- * {@link CharTokenizer#normalize(int)} for details.</li>
- * </ul>
- * <p>
- **/
-public final class WhitespaceAnalyzer extends ReusableAnalyzerBase {
-  
-  private final Version matchVersion;
-  
-  /**
-   * Creates a new {@link WhitespaceAnalyzer}
-   * @param matchVersion Lucene version to match See {@link <a href="#version">above</a>}
-   */
-  public WhitespaceAnalyzer(Version matchVersion) {
-    this.matchVersion = matchVersion;
-  }
-  
-  /**
-   * Creates a new {@link WhitespaceAnalyzer}
-   * @deprecated use {@link #WhitespaceAnalyzer(Version)} instead 
-   */
-  @Deprecated
-  public WhitespaceAnalyzer() {
-    this(Version.LUCENE_30);
-  }
-  
-  @Override
-  protected TokenStreamComponents createComponents(final String fieldName,
-      final Reader reader) {
-    return new TokenStreamComponents(new WhitespaceTokenizer(matchVersion, reader));
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/analysis/WhitespaceTokenizer.java b/lucene/src/java/org/apache/lucene/analysis/WhitespaceTokenizer.java
deleted file mode 100644
index 819a547..0000000
--- a/lucene/src/java/org/apache/lucene/analysis/WhitespaceTokenizer.java
+++ /dev/null
@@ -1,121 +0,0 @@
-package org.apache.lucene.analysis;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Reader;
-
-import org.apache.lucene.util.AttributeSource;
-import org.apache.lucene.util.Version;
-
-/**
- * A WhitespaceTokenizer is a tokenizer that divides text at whitespace.
- * Adjacent sequences of non-Whitespace characters form tokens. <a
- * name="version"/>
- * <p>
- * You must specify the required {@link Version} compatibility when creating
- * {@link WhitespaceTokenizer}:
- * <ul>
- * <li>As of 3.1, {@link CharTokenizer} uses an int based API to normalize and
- * detect token characters. See {@link CharTokenizer#isTokenChar(int)} and
- * {@link CharTokenizer#normalize(int)} for details.</li>
- * </ul>
- */
-public final class WhitespaceTokenizer extends CharTokenizer {
-  
-  /**
-   * Construct a new WhitespaceTokenizer. * @param matchVersion Lucene version
-   * to match See {@link <a href="#version">above</a>}
-   * 
-   * @param in
-   *          the input to split up into tokens
-   */
-  public WhitespaceTokenizer(Version matchVersion, Reader in) {
-    super(matchVersion, in);
-  }
-
-  /**
-   * Construct a new WhitespaceTokenizer using a given {@link AttributeSource}.
-   * 
-   * @param matchVersion
-   *          Lucene version to match See {@link <a href="#version">above</a>}
-   * @param source
-   *          the attribute source to use for this {@link Tokenizer}
-   * @param in
-   *          the input to split up into tokens
-   */
-  public WhitespaceTokenizer(Version matchVersion, AttributeSource source, Reader in) {
-    super(matchVersion, source, in);
-  }
-
-  /**
-   * Construct a new WhitespaceTokenizer using a given
-   * {@link org.apache.lucene.util.AttributeSource.AttributeFactory}.
-   *
-   * @param
-   *          matchVersion Lucene version to match See
-   *          {@link <a href="#version">above</a>}
-   * @param factory
-   *          the attribute factory to use for this {@link Tokenizer}
-   * @param in
-   *          the input to split up into tokens
-   */
-  public WhitespaceTokenizer(Version matchVersion, AttributeFactory factory, Reader in) {
-    super(matchVersion, factory, in);
-  }
-  
-  /**
-   * Construct a new WhitespaceTokenizer.
-   * 
-   * @deprecated use {@link #WhitespaceTokenizer(Version, Reader)} instead. This will
-   *             be removed in Lucene 4.0.
-   */
-  @Deprecated
-  public WhitespaceTokenizer(Reader in) {
-    super(in);
-  }
-
-  /**
-   * Construct a new WhitespaceTokenizer using a given {@link AttributeSource}.
-   * 
-   * @deprecated use {@link #WhitespaceTokenizer(Version, AttributeSource, Reader)}
-   *             instead. This will be removed in Lucene 4.0.
-   */
-  @Deprecated
-  public WhitespaceTokenizer(AttributeSource source, Reader in) {
-    super(source, in);
-  }
-
-  /**
-   * Construct a new WhitespaceTokenizer using a given
-   * {@link org.apache.lucene.util.AttributeSource.AttributeFactory}.
-   * 
-   * @deprecated use {@link #WhitespaceTokenizer(Version, AttributeSource.AttributeFactory, Reader)}
-   *             instead. This will be removed in Lucene 4.0.
-   */
-  @Deprecated
-  public WhitespaceTokenizer(AttributeFactory factory, Reader in) {
-    super(factory, in);
-  }
-  
-  /** Collects only characters which do not satisfy
-   * {@link Character#isWhitespace(int)}.*/
-  @Override
-  protected boolean isTokenChar(int c) {
-    return !Character.isWhitespace(c);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/analysis/WordlistLoader.java b/lucene/src/java/org/apache/lucene/analysis/WordlistLoader.java
deleted file mode 100644
index ac8a224..0000000
--- a/lucene/src/java/org/apache/lucene/analysis/WordlistLoader.java
+++ /dev/null
@@ -1,284 +0,0 @@
-package org.apache.lucene.analysis;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.BufferedReader;
-import java.io.File;
-import java.io.FileReader;
-import java.io.IOException;
-import java.io.InputStreamReader;
-import java.io.Reader;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Set;
-
-/**
- * Loader for text files that represent a list of stopwords.
- */
-public class WordlistLoader {
- 
-  /**
-   * Loads a text file associated with a given class (See
-   * {@link Class#getResourceAsStream(String)}) and adds every line as an entry
-   * to a {@link Set} (omitting leading and trailing whitespace). Every line of
-   * the file should contain only one word. The words need to be in lower-case if
-   * you make use of an Analyzer which uses LowerCaseFilter (like
-   * StandardAnalyzer).
-   * 
-   * @param aClass
-   *          a class that is associated with the given stopwordResource
-   * @param stopwordResource
-   *          name of the resource file associated with the given class
-   * @return a {@link Set} with the file's words
-   */
-  public static Set<String> getWordSet(Class<?> aClass, String stopwordResource)
-      throws IOException {
-    final Reader reader = new BufferedReader(new InputStreamReader(aClass
-        .getResourceAsStream(stopwordResource), "UTF-8"));
-    try {
-      return getWordSet(reader);
-    } finally {
-      reader.close();
-    }
-  }
-  
-  /**
-   * Loads a text file associated with a given class (See
-   * {@link Class#getResourceAsStream(String)}) and adds every line as an entry
-   * to a {@link Set} (omitting leading and trailing whitespace). Every line of
-   * the file should contain only one word. The words need to be in lower-case if
-   * you make use of an Analyzer which uses LowerCaseFilter (like
-   * StandardAnalyzer).
-   * 
-   * @param aClass
-   *          a class that is associated with the given stopwordResource
-   * @param stopwordResource
-   *          name of the resource file associated with the given class
-   * @param comment
-   *          the comment string to ignore
-   * @return a {@link Set} with the file's words
-   */
-  public static Set<String> getWordSet(Class<?> aClass,
-      String stopwordResource, String comment) throws IOException {
-    final Reader reader = new BufferedReader(new InputStreamReader(aClass
-        .getResourceAsStream(stopwordResource), "UTF-8"));
-    try {
-      return getWordSet(reader, comment);
-    } finally {
-      reader.close();
-    }
-  }
-  
-  /**
-   * Loads a text file and adds every line as an entry to a HashSet (omitting
-   * leading and trailing whitespace). Every line of the file should contain only
-   * one word. The words need to be in lowercase if you make use of an
-   * Analyzer which uses LowerCaseFilter (like StandardAnalyzer).
-   *
-   * @param wordfile File containing the wordlist
-   * @return A HashSet with the file's words
-   */
-  public static HashSet<String> getWordSet(File wordfile) throws IOException {
-    FileReader reader = null;
-    try {
-      reader = new FileReader(wordfile);
-      return getWordSet(reader);
-    }
-    finally {
-      if (reader != null)
-        reader.close();
-    }
-  }
-
-  /**
-   * Loads a text file and adds every non-comment line as an entry to a HashSet (omitting
-   * leading and trailing whitespace). Every line of the file should contain only
-   * one word. The words need to be in lowercase if you make use of an
-   * Analyzer which uses LowerCaseFilter (like StandardAnalyzer).
-   *
-   * @param wordfile File containing the wordlist
-   * @param comment The comment string to ignore
-   * @return A HashSet with the file's words
-   */
-  public static HashSet<String> getWordSet(File wordfile, String comment) throws IOException {
-    FileReader reader = null;
-    try {
-      reader = new FileReader(wordfile);
-      return getWordSet(reader, comment);
-    }
-    finally {
-      if (reader != null)
-        reader.close();
-    }
-  }
-
-
-  /**
-   * Reads lines from a Reader and adds every line as an entry to a HashSet (omitting
-   * leading and trailing whitespace). Every line of the Reader should contain only
-   * one word. The words need to be in lowercase if you make use of an
-   * Analyzer which uses LowerCaseFilter (like StandardAnalyzer).
-   *
-   * @param reader Reader containing the wordlist
-   * @return A HashSet with the reader's words
-   */
-  public static HashSet<String> getWordSet(Reader reader) throws IOException {
-    final HashSet<String> result = new HashSet<String>();
-    BufferedReader br = null;
-    try {
-      if (reader instanceof BufferedReader) {
-        br = (BufferedReader) reader;
-      } else {
-        br = new BufferedReader(reader);
-      }
-      String word = null;
-      while ((word = br.readLine()) != null) {
-        result.add(word.trim());
-      }
-    }
-    finally {
-      if (br != null)
-        br.close();
-    }
-    return result;
-  }
-
-  /**
-   * Reads lines from a Reader and adds every non-comment line as an entry to a HashSet (omitting
-   * leading and trailing whitespace). Every line of the Reader should contain only
-   * one word. The words need to be in lowercase if you make use of an
-   * Analyzer which uses LowerCaseFilter (like StandardAnalyzer).
-   *
-   * @param reader Reader containing the wordlist
-   * @param comment The string representing a comment.
-   * @return A HashSet with the reader's words
-   */
-  public static HashSet<String> getWordSet(Reader reader, String comment) throws IOException {
-    final HashSet<String> result = new HashSet<String>();
-    BufferedReader br = null;
-    try {
-      if (reader instanceof BufferedReader) {
-        br = (BufferedReader) reader;
-      } else {
-        br = new BufferedReader(reader);
-      }
-      String word = null;
-      while ((word = br.readLine()) != null) {
-        if (word.startsWith(comment) == false){
-          result.add(word.trim());
-        }
-      }
-    }
-    finally {
-      if (br != null)
-        br.close();
-    }
-    return result;
-  }
-
-  /**
-   * Loads a text file in Snowball format associated with a given class (See
-   * {@link Class#getResourceAsStream(String)}) and adds all words as entries to
-   * a {@link Set}. The words need to be in lower-case if you make use of an
-   * Analyzer which uses LowerCaseFilter (like StandardAnalyzer).
-   * 
-   * @param aClass a class that is associated with the given stopwordResource
-   * @param stopwordResource name of the resource file associated with the given
-   *          class
-   * @return a {@link Set} with the file's words
-   * @see #getSnowballWordSet(Reader)
-   */
-  public static Set<String> getSnowballWordSet(Class<?> aClass,
-      String stopwordResource) throws IOException {
-    final Reader reader = new BufferedReader(new InputStreamReader(aClass
-        .getResourceAsStream(stopwordResource), "UTF-8"));
-    try {
-      return getSnowballWordSet(reader);
-    } finally {
-      reader.close();
-    }
-  }
-  
-  /**
-   * Reads stopwords from a stopword list in Snowball format.
-   * <p>
-   * The snowball format is the following:
-   * <ul>
-   * <li>Lines may contain multiple words separated by whitespace.
-   * <li>The comment character is the vertical line (&#124;).
-   * <li>Lines may contain trailing comments.
-   * </ul>
-   * </p>
-   * 
-   * @param reader Reader containing a Snowball stopword list
-   * @return A Set with the reader's words
-   */
-  public static Set<String> getSnowballWordSet(Reader reader)
-      throws IOException {
-    final Set<String> result = new HashSet<String>();
-    BufferedReader br = null;
-    try {
-      if (reader instanceof BufferedReader) {
-        br = (BufferedReader) reader;
-      } else {
-        br = new BufferedReader(reader);
-      }
-      String line = null;
-      while ((line = br.readLine()) != null) {
-        int comment = line.indexOf('|');
-        if (comment >= 0) line = line.substring(0, comment);
-        String words[] = line.split("\\s+");
-        for (int i = 0; i < words.length; i++)
-          if (words[i].length() > 0) result.add(words[i]);
-      }
-    } finally {
-      if (br != null) br.close();
-    }
-    return result;
-  }
-
-
-  /**
-   * Reads a stem dictionary. Each line contains:
-   * <pre>word<b>\t</b>stem</pre>
-   * (i.e. two tab separated words)
-   *
-   * @return stem dictionary that overrules the stemming algorithm
-   * @throws IOException 
-   */
-  public static HashMap<String, String> getStemDict(File wordstemfile) throws IOException {
-    if (wordstemfile == null)
-      throw new NullPointerException("wordstemfile may not be null");
-    final HashMap<String, String> result = new HashMap<String,String>();
-    BufferedReader br = null;
-    
-    try {
-      br = new BufferedReader(new FileReader(wordstemfile));
-      String line;
-      while ((line = br.readLine()) != null) {
-        String[] wordstem = line.split("\t", 2);
-        result.put(wordstem[0], wordstem[1]);
-      }
-    } finally {
-      if(br != null)
-        br.close();
-    }
-    return result;
-  }
-
-}
diff --git a/lucene/src/java/org/apache/lucene/analysis/standard/READ_BEFORE_REGENERATING.txt b/lucene/src/java/org/apache/lucene/analysis/standard/READ_BEFORE_REGENERATING.txt
deleted file mode 100644
index c01c036..0000000
--- a/lucene/src/java/org/apache/lucene/analysis/standard/READ_BEFORE_REGENERATING.txt
+++ /dev/null
@@ -1,21 +0,0 @@
-/*
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-*/
-
-
-WARNING: if you change StandardTokenizerImpl*.jflex and need to regenerate
-      the tokenizer, only use the trunk version of JFlex 1.5 (with a minimum
-      SVN revision 591) at the moment!
diff --git a/lucene/src/java/org/apache/lucene/analysis/standard/StandardAnalyzer.java b/lucene/src/java/org/apache/lucene/analysis/standard/StandardAnalyzer.java
deleted file mode 100644
index a09ce1f..0000000
--- a/lucene/src/java/org/apache/lucene/analysis/standard/StandardAnalyzer.java
+++ /dev/null
@@ -1,131 +0,0 @@
-package org.apache.lucene.analysis.standard;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.*;
-import org.apache.lucene.util.Version;
-
-import java.io.File;
-import java.io.IOException;
-import java.io.Reader;
-import java.util.Set;
-
-/**
- * Filters {@link StandardTokenizer} with {@link StandardFilter}, {@link
- * LowerCaseFilter} and {@link StopFilter}, using a list of
- * English stop words.
- *
- * <a name="version"/>
- * <p>You must specify the required {@link Version}
- * compatibility when creating StandardAnalyzer:
- * <ul>
- *   <li> As of 3.1, StopFilter correctly handles Unicode 4.0
- *         supplementary characters in stopwords
- *   <li> As of 2.9, StopFilter preserves position
- *        increments
- *   <li> As of 2.4, Tokens incorrectly identified as acronyms
- *        are corrected (see <a href="https://issues.apache.org/jira/browse/LUCENE-1068">LUCENE-1068</a>)
- * </ul>
- */
-public final class StandardAnalyzer extends StopwordAnalyzerBase {
-
-  /** Default maximum allowed token length */
-  public static final int DEFAULT_MAX_TOKEN_LENGTH = 255;
-
-  private int maxTokenLength = DEFAULT_MAX_TOKEN_LENGTH;
-
-  /**
-   * Specifies whether deprecated acronyms should be replaced with HOST type.
-   * See {@linkplain "https://issues.apache.org/jira/browse/LUCENE-1068"}
-   */
-  private final boolean replaceInvalidAcronym;
-
-  /** An unmodifiable set containing some common English words that are usually not
-  useful for searching. */
-  public static final Set<?> STOP_WORDS_SET = StopAnalyzer.ENGLISH_STOP_WORDS_SET; 
-
-  /** Builds an analyzer with the given stop words.
-   * @param matchVersion Lucene version to match See {@link
-   * <a href="#version">above</a>}
-   * @param stopWords stop words */
-  public StandardAnalyzer(Version matchVersion, Set<?> stopWords) {
-    super(matchVersion, stopWords);
-    replaceInvalidAcronym = matchVersion.onOrAfter(Version.LUCENE_24);
-  }
-
-  /** Builds an analyzer with the default stop words ({@link
-   * #STOP_WORDS_SET}).
-   * @param matchVersion Lucene version to match See {@link
-   * <a href="#version">above</a>}
-   */
-  public StandardAnalyzer(Version matchVersion) {
-    this(matchVersion, STOP_WORDS_SET);
-  }
-
-  /** Builds an analyzer with the stop words from the given file.
-   * @see WordlistLoader#getWordSet(File)
-   * @param matchVersion Lucene version to match See {@link
-   * <a href="#version">above</a>}
-   * @param stopwords File to read stop words from */
-  public StandardAnalyzer(Version matchVersion, File stopwords) throws IOException {
-    this(matchVersion, WordlistLoader.getWordSet(stopwords));
-  }
-
-  /** Builds an analyzer with the stop words from the given reader.
-   * @see WordlistLoader#getWordSet(Reader)
-   * @param matchVersion Lucene version to match See {@link
-   * <a href="#version">above</a>}
-   * @param stopwords Reader to read stop words from */
-  public StandardAnalyzer(Version matchVersion, Reader stopwords) throws IOException {
-    this(matchVersion, WordlistLoader.getWordSet(stopwords));
-  }
-
-  /**
-   * Set maximum allowed token length.  If a token is seen
-   * that exceeds this length then it is discarded.  This
-   * setting only takes effect the next time tokenStream or
-   * reusableTokenStream is called.
-   */
-  public void setMaxTokenLength(int length) {
-    maxTokenLength = length;
-  }
-    
-  /**
-   * @see #setMaxTokenLength
-   */
-  public int getMaxTokenLength() {
-    return maxTokenLength;
-  }
-
-  @Override
-  protected TokenStreamComponents createComponents(final String fieldName, final Reader reader) {
-    final StandardTokenizer src = new StandardTokenizer(matchVersion, reader);
-    src.setMaxTokenLength(maxTokenLength);
-    src.setReplaceInvalidAcronym(replaceInvalidAcronym);
-    TokenStream tok = new StandardFilter(src);
-    tok = new LowerCaseFilter(matchVersion, tok);
-    tok = new StopFilter(matchVersion, tok, stopwords);
-    return new TokenStreamComponents(src, tok) {
-      @Override
-      protected boolean reset(final Reader reader) throws IOException {
-        src.setMaxTokenLength(StandardAnalyzer.this.maxTokenLength);
-        return super.reset(reader);
-      }
-    };
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/analysis/standard/StandardFilter.java b/lucene/src/java/org/apache/lucene/analysis/standard/StandardFilter.java
deleted file mode 100644
index b6394e5..0000000
--- a/lucene/src/java/org/apache/lucene/analysis/standard/StandardFilter.java
+++ /dev/null
@@ -1,73 +0,0 @@
-package org.apache.lucene.analysis.standard;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.TokenFilter;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
-
-/** Normalizes tokens extracted with {@link StandardTokenizer}. */
-
-public final class StandardFilter extends TokenFilter {
-
-  /** Construct filtering <i>in</i>. */
-  public StandardFilter(TokenStream in) {
-    super(in);
-  }
-
-  private static final String APOSTROPHE_TYPE = StandardTokenizer.TOKEN_TYPES[StandardTokenizer.APOSTROPHE];
-  private static final String ACRONYM_TYPE = StandardTokenizer.TOKEN_TYPES[StandardTokenizer.ACRONYM];
-
-  // this filters uses attribute type
-  private final TypeAttribute typeAtt = addAttribute(TypeAttribute.class);
-  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-  
-  /** Returns the next token in the stream, or null at EOS.
-   * <p>Removes <tt>'s</tt> from the end of words.
-   * <p>Removes dots from acronyms.
-   */
-  @Override
-  public final boolean incrementToken() throws java.io.IOException {
-    if (!input.incrementToken()) {
-      return false;
-    }
-
-    final char[] buffer = termAtt.buffer();
-    final int bufferLength = termAtt.length();
-    final String type = typeAtt.type();
-
-    if (type == APOSTROPHE_TYPE &&      // remove 's
-        bufferLength >= 2 &&
-        buffer[bufferLength-2] == '\'' &&
-        (buffer[bufferLength-1] == 's' || buffer[bufferLength-1] == 'S')) {
-      // Strip last 2 characters off
-      termAtt.setLength(bufferLength - 2);
-    } else if (type == ACRONYM_TYPE) {      // remove dots
-      int upto = 0;
-      for(int i=0;i<bufferLength;i++) {
-        char c = buffer[i];
-        if (c != '.')
-          buffer[upto++] = c;
-      }
-      termAtt.setLength(upto);
-    }
-
-    return true;
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/analysis/standard/StandardTokenizer.java b/lucene/src/java/org/apache/lucene/analysis/standard/StandardTokenizer.java
deleted file mode 100644
index e790171..0000000
--- a/lucene/src/java/org/apache/lucene/analysis/standard/StandardTokenizer.java
+++ /dev/null
@@ -1,230 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.lucene.analysis.standard;
-
-import java.io.IOException;
-import java.io.Reader;
-
-import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
-import org.apache.lucene.util.AttributeSource;
-import org.apache.lucene.util.Version;
-
-/** A grammar-based tokenizer constructed with JFlex
- *
- * <p> This should be a good tokenizer for most European-language documents:
- *
- * <ul>
- *   <li>Splits words at punctuation characters, removing punctuation. However, a 
- *     dot that's not followed by whitespace is considered part of a token.
- *   <li>Splits words at hyphens, unless there's a number in the token, in which case
- *     the whole token is interpreted as a product number and is not split.
- *   <li>Recognizes email addresses and internet hostnames as one token.
- * </ul>
- *
- * <p>Many applications have specific tokenizer needs.  If this tokenizer does
- * not suit your application, please consider copying this source code
- * directory to your project and maintaining your own grammar-based tokenizer.
- *
- * <a name="version"/>
- * <p>You must specify the required {@link Version}
- * compatibility when creating StandardAnalyzer:
- * <ul>
- *   <li> As of 2.4, Tokens incorrectly identified as acronyms
- *        are corrected (see <a href="https://issues.apache.org/jira/browse/LUCENE-1068">LUCENE-1608</a>
- * </ul>
- */
-
-public final class StandardTokenizer extends Tokenizer {
-  /** A private instance of the JFlex-constructed scanner */
-  private StandardTokenizerInterface scanner;
-
-  public static final int ALPHANUM          = 0;
-  public static final int APOSTROPHE        = 1;
-  public static final int ACRONYM           = 2;
-  public static final int COMPANY           = 3;
-  public static final int EMAIL             = 4;
-  public static final int HOST              = 5;
-  public static final int NUM               = 6;
-  public static final int CJ                = 7;
-
-  /**
-   * @deprecated this solves a bug where HOSTs that end with '.' are identified
-   *             as ACRONYMs.
-   */
-  @Deprecated
-  public static final int ACRONYM_DEP       = 8;
-
-  /** String token types that correspond to token type int constants */
-  public static final String [] TOKEN_TYPES = new String [] {
-    "<ALPHANUM>",
-    "<APOSTROPHE>",
-    "<ACRONYM>",
-    "<COMPANY>",
-    "<EMAIL>",
-    "<HOST>",
-    "<NUM>",
-    "<CJ>",
-    "<ACRONYM_DEP>"
-  };
-
-  private boolean replaceInvalidAcronym;
-    
-  private int maxTokenLength = StandardAnalyzer.DEFAULT_MAX_TOKEN_LENGTH;
-
-  /** Set the max allowed token length.  Any token longer
-   *  than this is skipped. */
-  public void setMaxTokenLength(int length) {
-    this.maxTokenLength = length;
-  }
-
-  /** @see #setMaxTokenLength */
-  public int getMaxTokenLength() {
-    return maxTokenLength;
-  }
-
-  /**
-   * Creates a new instance of the {@link org.apache.lucene.analysis.standard.StandardTokenizer}.  Attaches
-   * the <code>input</code> to the newly created JFlex scanner.
-   *
-   * @param input The input reader
-   *
-   * See http://issues.apache.org/jira/browse/LUCENE-1068
-   */
-  public StandardTokenizer(Version matchVersion, Reader input) {
-    super();
-    init(input, matchVersion);
-  }
-
-  /**
-   * Creates a new StandardTokenizer with a given {@link AttributeSource}. 
-   */
-  public StandardTokenizer(Version matchVersion, AttributeSource source, Reader input) {
-    super(source);
-    init(input, matchVersion);
-  }
-
-  /**
-   * Creates a new StandardTokenizer with a given {@link org.apache.lucene.util.AttributeSource.AttributeFactory} 
-   */
-  public StandardTokenizer(Version matchVersion, AttributeFactory factory, Reader input) {
-    super(factory);
-    init(input, matchVersion);
-  }
-
-  private final void init(Reader input, Version matchVersion) {
-    this.scanner = matchVersion.onOrAfter(Version.LUCENE_31) ?
-      new StandardTokenizerImpl31(input) : new StandardTokenizerImplOrig(input);
-    if (matchVersion.onOrAfter(Version.LUCENE_24)) {
-      replaceInvalidAcronym = true;
-    } else {
-      replaceInvalidAcronym = false;
-    }
-    this.input = input;    
-  }
-
-  // this tokenizer generates three attributes:
-  // term offset, positionIncrement and type
-  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-  private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
-  private final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);
-  private final TypeAttribute typeAtt = addAttribute(TypeAttribute.class);
-
-  /*
-   * (non-Javadoc)
-   *
-   * @see org.apache.lucene.analysis.TokenStream#next()
-   */
-  @Override
-  public final boolean incrementToken() throws IOException {
-    clearAttributes();
-    int posIncr = 1;
-
-    while(true) {
-      int tokenType = scanner.getNextToken();
-
-      if (tokenType == StandardTokenizerInterface.YYEOF) {
-        return false;
-      }
-
-      if (scanner.yylength() <= maxTokenLength) {
-        posIncrAtt.setPositionIncrement(posIncr);
-        scanner.getText(termAtt);
-        final int start = scanner.yychar();
-        offsetAtt.setOffset(correctOffset(start), correctOffset(start+termAtt.length()));
-        // This 'if' should be removed in the next release. For now, it converts
-        // invalid acronyms to HOST. When removed, only the 'else' part should
-        // remain.
-        if (tokenType == StandardTokenizer.ACRONYM_DEP) {
-          if (replaceInvalidAcronym) {
-            typeAtt.setType(StandardTokenizer.TOKEN_TYPES[StandardTokenizer.HOST]);
-            termAtt.setLength(termAtt.length() - 1); // remove extra '.'
-          } else {
-            typeAtt.setType(StandardTokenizer.TOKEN_TYPES[StandardTokenizer.ACRONYM]);
-          }
-        } else {
-          typeAtt.setType(StandardTokenizer.TOKEN_TYPES[tokenType]);
-        }
-        return true;
-      } else
-        // When we skip a too-long term, we still increment the
-        // position increment
-        posIncr++;
-    }
-  }
-  
-  @Override
-  public final void end() {
-    // set final offset
-    int finalOffset = correctOffset(scanner.yychar() + scanner.yylength());
-    offsetAtt.setOffset(finalOffset, finalOffset);
-  }
-
-  @Override
-  public void reset(Reader reader) throws IOException {
-    super.reset(reader);
-    scanner.yyreset(reader);
-  }
-
-  /**
-   * Prior to https://issues.apache.org/jira/browse/LUCENE-1068, StandardTokenizer mischaracterized as acronyms tokens like www.abc.com
-   * when they should have been labeled as hosts instead.
-   * @return true if StandardTokenizer now returns these tokens as Hosts, otherwise false
-   *
-   * @deprecated Remove in 3.X and make true the only valid value
-   */
-  @Deprecated
-  public boolean isReplaceInvalidAcronym() {
-    return replaceInvalidAcronym;
-  }
-
-  /**
-   *
-   * @param replaceInvalidAcronym Set to true to replace mischaracterized acronyms as HOST.
-   * @deprecated Remove in 3.X and make true the only valid value
-   *
-   * See https://issues.apache.org/jira/browse/LUCENE-1068
-   */
-  @Deprecated
-  public void setReplaceInvalidAcronym(boolean replaceInvalidAcronym) {
-    this.replaceInvalidAcronym = replaceInvalidAcronym;
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl31.java b/lucene/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl31.java
deleted file mode 100644
index 49837da..0000000
--- a/lucene/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl31.java
+++ /dev/null
@@ -1,740 +0,0 @@
-/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 17.05.10 14:50 */
-
-package org.apache.lucene.analysis.standard;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/*
-
-WARNING: if you change StandardTokenizerImpl*.jflex and need to regenerate
-      the tokenizer, only use the trunk version of JFlex 1.5 at the moment!
-
-*/
-
-import java.io.Reader;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-
-
-/**
- * This class is a scanner generated by 
- * <a href="http://www.jflex.de/">JFlex</a> 1.5.0-SNAPSHOT
- * on 17.05.10 14:50 from the specification file
- * <tt>C:/Users/Uwe Schindler/Projects/lucene/newtrunk/lucene/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl31.jflex</tt>
- */
-class StandardTokenizerImpl31 implements StandardTokenizerInterface {
-
-  /** This character denotes the end of file */
-  public static final int YYEOF = -1;
-
-  /** initial size of the lookahead buffer */
-  private static final int ZZ_BUFFERSIZE = 16384;
-
-  /** lexical states */
-  public static final int YYINITIAL = 0;
-
-  /**
-   * ZZ_LEXSTATE[l] is the state in the DFA for the lexical state l
-   * ZZ_LEXSTATE[l+1] is the state in the DFA for the lexical state l
-   *                  at the beginning of a line
-   * l is of the form l = 2*k, k a non negative integer
-   */
-  private static final int ZZ_LEXSTATE[] = { 
-     0, 0
-  };
-
-  /** 
-   * Translates characters to character classes
-   */
-  private static final String ZZ_CMAP_PACKED = 
-    "\11\0\1\0\1\15\1\0\1\0\1\14\22\0\1\0\5\0\1\5"+
-    "\1\3\4\0\1\11\1\7\1\4\1\11\12\2\6\0\1\6\32\12"+
-    "\4\0\1\10\1\0\32\12\57\0\1\12\12\0\1\12\4\0\1\12"+
-    "\5\0\27\12\1\0\37\12\1\0\u013f\12\31\0\162\12\4\0\14\12"+
-    "\16\0\5\12\11\0\1\12\213\0\1\12\13\0\1\12\1\0\3\12"+
-    "\1\0\1\12\1\0\24\12\1\0\54\12\1\0\46\12\1\0\5\12"+
-    "\4\0\202\12\10\0\105\12\1\0\46\12\2\0\2\12\6\0\20\12"+
-    "\41\0\46\12\2\0\1\12\7\0\47\12\110\0\33\12\5\0\3\12"+
-    "\56\0\32\12\5\0\13\12\25\0\12\2\4\0\2\12\1\0\143\12"+
-    "\1\0\1\12\17\0\2\12\7\0\2\12\12\2\3\12\2\0\1\12"+
-    "\20\0\1\12\1\0\36\12\35\0\3\12\60\0\46\12\13\0\1\12"+
-    "\u0152\0\66\12\3\0\1\12\22\0\1\12\7\0\12\12\4\0\12\2"+
-    "\25\0\10\12\2\0\2\12\2\0\26\12\1\0\7\12\1\0\1\12"+
-    "\3\0\4\12\3\0\1\12\36\0\2\12\1\0\3\12\4\0\12\2"+
-    "\2\12\23\0\6\12\4\0\2\12\2\0\26\12\1\0\7\12\1\0"+
-    "\2\12\1\0\2\12\1\0\2\12\37\0\4\12\1\0\1\12\7\0"+
-    "\12\2\2\0\3\12\20\0\11\12\1\0\3\12\1\0\26\12\1\0"+
-    "\7\12\1\0\2\12\1\0\5\12\3\0\1\12\22\0\1\12\17\0"+
-    "\2\12\4\0\12\2\25\0\10\12\2\0\2\12\2\0\26\12\1\0"+
-    "\7\12\1\0\2\12\1\0\5\12\3\0\1\12\36\0\2\12\1\0"+
-    "\3\12\4\0\12\2\1\0\1\12\21\0\1\12\1\0\6\12\3\0"+
-    "\3\12\1\0\4\12\3\0\2\12\1\0\1\12\1\0\2\12\3\0"+
-    "\2\12\3\0\3\12\3\0\10\12\1\0\3\12\55\0\11\2\25\0"+
-    "\10\12\1\0\3\12\1\0\27\12\1\0\12\12\1\0\5\12\46\0"+
-    "\2\12\4\0\12\2\25\0\10\12\1\0\3\12\1\0\27\12\1\0"+
-    "\12\12\1\0\5\12\3\0\1\12\40\0\1\12\1\0\2\12\4\0"+
-    "\12\2\25\0\10\12\1\0\3\12\1\0\27\12\1\0\20\12\46\0"+
-    "\2\12\4\0\12\2\25\0\22\12\3\0\30\12\1\0\11\12\1\0"+
-    "\1\12\2\0\7\12\71\0\1\1\60\12\1\1\2\12\14\1\7\12"+
-    "\11\1\12\2\47\0\2\12\1\0\1\12\2\0\2\12\1\0\1\12"+
-    "\2\0\1\12\6\0\4\12\1\0\7\12\1\0\3\12\1\0\1\12"+
-    "\1\0\1\12\2\0\2\12\1\0\4\12\1\0\2\12\11\0\1\12"+
-    "\2\0\5\12\1\0\1\12\11\0\12\2\2\0\2\12\42\0\1\12"+
-    "\37\0\12\2\26\0\10\12\1\0\42\12\35\0\4\12\164\0\42\12"+
-    "\1\0\5\12\1\0\2\12\25\0\12\2\6\0\6\12\112\0\46\12"+
-    "\12\0\51\12\7\0\132\12\5\0\104\12\5\0\122\12\6\0\7\12"+
-    "\1\0\77\12\1\0\1\12\1\0\4\12\2\0\7\12\1\0\1\12"+
-    "\1\0\4\12\2\0\47\12\1\0\1\12\1\0\4\12\2\0\37\12"+
-    "\1\0\1\12\1\0\4\12\2\0\7\12\1\0\1\12\1\0\4\12"+
-    "\2\0\7\12\1\0\7\12\1\0\27\12\1\0\37\12\1\0\1\12"+
-    "\1\0\4\12\2\0\7\12\1\0\47\12\1\0\23\12\16\0\11\2"+
-    "\56\0\125\12\14\0\u026c\12\2\0\10\12\12\0\32\12\5\0\113\12"+
-    "\25\0\15\12\1\0\4\12\16\0\22\12\16\0\22\12\16\0\15\12"+
-    "\1\0\3\12\17\0\64\12\43\0\1\12\4\0\1\12\3\0\12\2"+
-    "\46\0\12\2\6\0\130\12\10\0\51\12\127\0\35\12\51\0\12\2"+
-    "\36\12\2\0\5\12\u038b\0\154\12\224\0\234\12\4\0\132\12\6\0"+
-    "\26\12\2\0\6\12\2\0\46\12\2\0\6\12\2\0\10\12\1\0"+
-    "\1\12\1\0\1\12\1\0\1\12\1\0\37\12\2\0\65\12\1\0"+
-    "\7\12\1\0\1\12\3\0\3\12\1\0\7\12\3\0\4\12\2\0"+
-    "\6\12\4\0\15\12\5\0\3\12\1\0\7\12\164\0\1\12\15\0"+
-    "\1\12\202\0\1\12\4\0\1\12\2\0\12\12\1\0\1\12\3\0"+
-    "\5\12\6\0\1\12\1\0\1\12\1\0\1\12\1\0\4\12\1\0"+
-    "\3\12\1\0\7\12\3\0\3\12\5\0\5\12\u0ebb\0\2\12\52\0"+
-    "\5\12\5\0\2\12\3\0\1\13\126\13\6\13\3\13\1\13\132\13"+
-    "\1\13\4\13\5\13\50\13\3\13\1\0\136\12\21\0\30\12\70\0"+
-    "\20\13\u0100\0\200\13\200\0\u19b6\13\12\13\100\0\u51a6\13\132\13\u048d\12"+
-    "\u0773\0\u2ba4\12\u215c\0\u012e\13\2\13\73\13\225\13\7\12\14\0\5\12"+
-    "\5\0\1\12\1\0\12\12\1\0\15\12\1\0\5\12\1\0\1\12"+
-    "\1\0\2\12\1\0\2\12\1\0\154\12\41\0\u016b\12\22\0\100\12"+
-    "\2\0\66\12\50\0\14\12\164\0\5\12\1\0\207\12\23\0\12\2"+
-    "\7\0\32\12\6\0\32\12\12\0\1\13\72\13\37\12\3\0\6\12"+
-    "\2\0\6\12\2\0\6\12\2\0\3\12\43\0";
-
-  /** 
-   * Translates characters to character classes
-   */
-  private static final char [] ZZ_CMAP = zzUnpackCMap(ZZ_CMAP_PACKED);
-
-  /** 
-   * Translates DFA states to action switch labels.
-   */
-  private static final int [] ZZ_ACTION = zzUnpackAction();
-
-  private static final String ZZ_ACTION_PACKED_0 =
-    "\1\0\1\1\3\2\1\3\1\1\13\0\1\2\3\4"+
-    "\2\0\1\5\1\0\1\5\3\4\6\5\1\6\1\4"+
-    "\2\7\1\10\1\0\1\10\3\0\2\10\1\11\1\12"+
-    "\1\4";
-
-  private static int [] zzUnpackAction() {
-    int [] result = new int[51];
-    int offset = 0;
-    offset = zzUnpackAction(ZZ_ACTION_PACKED_0, offset, result);
-    return result;
-  }
-
-  private static int zzUnpackAction(String packed, int offset, int [] result) {
-    int i = 0;       /* index in packed string  */
-    int j = offset;  /* index in unpacked array */
-    int l = packed.length();
-    while (i < l) {
-      int count = packed.charAt(i++);
-      int value = packed.charAt(i++);
-      do result[j++] = value; while (--count > 0);
-    }
-    return j;
-  }
-
-
-  /** 
-   * Translates a state to a row index in the transition table
-   */
-  private static final int [] ZZ_ROWMAP = zzUnpackRowMap();
-
-  private static final String ZZ_ROWMAP_PACKED_0 =
-    "\0\0\0\16\0\34\0\52\0\70\0\16\0\106\0\124"+
-    "\0\142\0\160\0\176\0\214\0\232\0\250\0\266\0\304"+
-    "\0\322\0\340\0\356\0\374\0\u010a\0\u0118\0\u0126\0\u0134"+
-    "\0\u0142\0\u0150\0\u015e\0\u016c\0\u017a\0\u0188\0\u0196\0\u01a4"+
-    "\0\u01b2\0\u01c0\0\u01ce\0\u01dc\0\u01ea\0\u01f8\0\322\0\u0206"+
-    "\0\u0214\0\u0222\0\u0230\0\u023e\0\u024c\0\u025a\0\124\0\214"+
-    "\0\u0268\0\u0276\0\u0284";
-
-  private static int [] zzUnpackRowMap() {
-    int [] result = new int[51];
-    int offset = 0;
-    offset = zzUnpackRowMap(ZZ_ROWMAP_PACKED_0, offset, result);
-    return result;
-  }
-
-  private static int zzUnpackRowMap(String packed, int offset, int [] result) {
-    int i = 0;  /* index in packed string  */
-    int j = offset;  /* index in unpacked array */
-    int l = packed.length();
-    while (i < l) {
-      int high = packed.charAt(i++) << 16;
-      result[j++] = high | packed.charAt(i++);
-    }
-    return j;
-  }
-
-  /** 
-   * The transition table of the DFA
-   */
-  private static final int [] ZZ_TRANS = zzUnpackTrans();
-
-  private static final String ZZ_TRANS_PACKED_0 =
-    "\1\2\1\3\1\4\7\2\1\5\1\6\1\7\1\2"+
-    "\17\0\2\3\1\0\1\10\1\0\1\11\2\12\1\13"+
-    "\1\3\4\0\1\3\1\4\1\0\1\14\1\0\1\11"+
-    "\2\15\1\16\1\4\4\0\1\3\1\4\1\17\1\20"+
-    "\1\21\1\22\2\12\1\13\1\23\20\0\1\2\1\0"+
-    "\1\24\1\25\7\0\1\26\4\0\2\27\7\0\1\27"+
-    "\4\0\1\30\1\31\7\0\1\32\5\0\1\33\7\0"+
-    "\1\13\4\0\1\34\1\35\7\0\1\36\4\0\1\37"+
-    "\1\40\7\0\1\41\4\0\1\42\1\43\7\0\1\44"+
-    "\15\0\1\45\4\0\1\24\1\25\7\0\1\46\15\0"+
-    "\1\47\4\0\2\27\7\0\1\50\4\0\1\3\1\4"+
-    "\1\17\1\10\1\21\1\22\2\12\1\13\1\23\4\0"+
-    "\2\24\1\0\1\51\1\0\1\11\2\52\1\0\1\24"+
-    "\4\0\1\24\1\25\1\0\1\53\1\0\1\11\2\54"+
-    "\1\55\1\25\4\0\1\24\1\25\1\0\1\51\1\0"+
-    "\1\11\2\52\1\0\1\26\4\0\2\27\1\0\1\56"+
-    "\2\0\1\56\2\0\1\27\4\0\2\30\1\0\1\52"+
-    "\1\0\1\11\2\52\1\0\1\30\4\0\1\30\1\31"+
-    "\1\0\1\54\1\0\1\11\2\54\1\55\1\31\4\0"+
-    "\1\30\1\31\1\0\1\52\1\0\1\11\2\52\1\0"+
-    "\1\32\5\0\1\33\1\0\1\55\2\0\3\55\1\33"+
-    "\4\0\2\34\1\0\1\57\1\0\1\11\2\12\1\13"+
-    "\1\34\4\0\1\34\1\35\1\0\1\60\1\0\1\11"+
-    "\2\15\1\16\1\35\4\0\1\34\1\35\1\0\1\57"+
-    "\1\0\1\11\2\12\1\13\1\36\4\0\2\37\1\0"+
-    "\1\12\1\0\1\11\2\12\1\13\1\37\4\0\1\37"+
-    "\1\40\1\0\1\15\1\0\1\11\2\15\1\16\1\40"+
-    "\4\0\1\37\1\40\1\0\1\12\1\0\1\11\2\12"+
-    "\1\13\1\41\4\0\2\42\1\0\1\13\2\0\3\13"+
-    "\1\42\4\0\1\42\1\43\1\0\1\16\2\0\3\16"+
-    "\1\43\4\0\1\42\1\43\1\0\1\13\2\0\3\13"+
-    "\1\44\6\0\1\17\6\0\1\45\4\0\1\24\1\25"+
-    "\1\0\1\61\1\0\1\11\2\52\1\0\1\26\4\0"+
-    "\2\27\1\0\1\56\2\0\1\56\2\0\1\50\4\0"+
-    "\2\24\7\0\1\24\4\0\2\30\7\0\1\30\4\0"+
-    "\2\34\7\0\1\34\4\0\2\37\7\0\1\37\4\0"+
-    "\2\42\7\0\1\42\4\0\2\62\7\0\1\62\4\0"+
-    "\2\24\7\0\1\63\4\0\2\62\1\0\1\56\2\0"+
-    "\1\56\2\0\1\62\4\0\2\24\1\0\1\61\1\0"+
-    "\1\11\2\52\1\0\1\24\3\0";
-
-  private static int [] zzUnpackTrans() {
-    int [] result = new int[658];
-    int offset = 0;
-    offset = zzUnpackTrans(ZZ_TRANS_PACKED_0, offset, result);
-    return result;
-  }
-
-  private static int zzUnpackTrans(String packed, int offset, int [] result) {
-    int i = 0;       /* index in packed string  */
-    int j = offset;  /* index in unpacked array */
-    int l = packed.length();
-    while (i < l) {
-      int count = packed.charAt(i++);
-      int value = packed.charAt(i++);
-      value--;
-      do result[j++] = value; while (--count > 0);
-    }
-    return j;
-  }
-
-
-  /* error codes */
-  private static final int ZZ_UNKNOWN_ERROR = 0;
-  private static final int ZZ_NO_MATCH = 1;
-  private static final int ZZ_PUSHBACK_2BIG = 2;
-
-  /* error messages for the codes above */
-  private static final String ZZ_ERROR_MSG[] = {
-    "Unkown internal scanner error",
-    "Error: could not match input",
-    "Error: pushback value was too large"
-  };
-
-  /**
-   * ZZ_ATTRIBUTE[aState] contains the attributes of state <code>aState</code>
-   */
-  private static final int [] ZZ_ATTRIBUTE = zzUnpackAttribute();
-
-  private static final String ZZ_ATTRIBUTE_PACKED_0 =
-    "\1\0\1\11\3\1\1\11\1\1\13\0\4\1\2\0"+
-    "\1\1\1\0\17\1\1\0\1\1\3\0\5\1";
-
-  private static int [] zzUnpackAttribute() {
-    int [] result = new int[51];
-    int offset = 0;
-    offset = zzUnpackAttribute(ZZ_ATTRIBUTE_PACKED_0, offset, result);
-    return result;
-  }
-
-  private static int zzUnpackAttribute(String packed, int offset, int [] result) {
-    int i = 0;       /* index in packed string  */
-    int j = offset;  /* index in unpacked array */
-    int l = packed.length();
-    while (i < l) {
-      int count = packed.charAt(i++);
-      int value = packed.charAt(i++);
-      do result[j++] = value; while (--count > 0);
-    }
-    return j;
-  }
-
-  /** the input device */
-  private java.io.Reader zzReader;
-
-  /** the current state of the DFA */
-  private int zzState;
-
-  /** the current lexical state */
-  private int zzLexicalState = YYINITIAL;
-
-  /** this buffer contains the current text to be matched and is
-      the source of the yytext() string */
-  private char zzBuffer[] = new char[ZZ_BUFFERSIZE];
-
-  /** the textposition at the last accepting state */
-  private int zzMarkedPos;
-
-  /** the current text position in the buffer */
-  private int zzCurrentPos;
-
-  /** startRead marks the beginning of the yytext() string in the buffer */
-  private int zzStartRead;
-
-  /** endRead marks the last character in the buffer, that has been read
-      from input */
-  private int zzEndRead;
-
-  /** number of newlines encountered up to the start of the matched text */
-  private int yyline;
-
-  /** the number of characters up to the start of the matched text */
-  private int yychar;
-
-  /**
-   * the number of characters from the last newline up to the start of the 
-   * matched text
-   */
-  private int yycolumn;
-
-  /** 
-   * zzAtBOL == true <=> the scanner is currently at the beginning of a line
-   */
-  private boolean zzAtBOL = true;
-
-  /** zzAtEOF == true <=> the scanner is at the EOF */
-  private boolean zzAtEOF;
-
-  /** denotes if the user-EOF-code has already been executed */
-  private boolean zzEOFDone;
-
-  /* user code: */
-
-public static final int ALPHANUM          = StandardTokenizer.ALPHANUM;
-public static final int APOSTROPHE        = StandardTokenizer.APOSTROPHE;
-public static final int ACRONYM           = StandardTokenizer.ACRONYM;
-public static final int COMPANY           = StandardTokenizer.COMPANY;
-public static final int EMAIL             = StandardTokenizer.EMAIL;
-public static final int HOST              = StandardTokenizer.HOST;
-public static final int NUM               = StandardTokenizer.NUM;
-public static final int CJ                = StandardTokenizer.CJ;
-/**
- * @deprecated this solves a bug where HOSTs that end with '.' are identified
- *             as ACRONYMs.
- */
-public static final int ACRONYM_DEP       = StandardTokenizer.ACRONYM_DEP;
-
-public static final String [] TOKEN_TYPES = StandardTokenizer.TOKEN_TYPES;
-
-public final int yychar()
-{
-    return yychar;
-}
-
-/**
- * Fills CharTermAttribute with the current token text.
- */
-public final void getText(CharTermAttribute t) {
-  t.copyBuffer(zzBuffer, zzStartRead, zzMarkedPos-zzStartRead);
-}
-
-
-
-  /**
-   * Creates a new scanner
-   * There is also a java.io.InputStream version of this constructor.
-   *
-   * @param   in  the java.io.Reader to read input from.
-   */
-  StandardTokenizerImpl31(java.io.Reader in) {
-    this.zzReader = in;
-  }
-
-  /**
-   * Creates a new scanner.
-   * There is also java.io.Reader version of this constructor.
-   *
-   * @param   in  the java.io.Inputstream to read input from.
-   */
-  StandardTokenizerImpl31(java.io.InputStream in) {
-    this(new java.io.InputStreamReader(in));
-  }
-
-  /** 
-   * Unpacks the compressed character translation table.
-   *
-   * @param packed   the packed character translation table
-   * @return         the unpacked character translation table
-   */
-  private static char [] zzUnpackCMap(String packed) {
-    char [] map = new char[0x10000];
-    int i = 0;  /* index in packed string  */
-    int j = 0;  /* index in unpacked array */
-    while (i < 1234) {
-      int  count = packed.charAt(i++);
-      char value = packed.charAt(i++);
-      do map[j++] = value; while (--count > 0);
-    }
-    return map;
-  }
-
-
-  /**
-   * Refills the input buffer.
-   *
-   * @return      <code>false</code>, iff there was new input.
-   * 
-   * @exception   java.io.IOException  if any I/O-Error occurs
-   */
-  private boolean zzRefill() throws java.io.IOException {
-
-    /* first: make room (if you can) */
-    if (zzStartRead > 0) {
-      System.arraycopy(zzBuffer, zzStartRead,
-                       zzBuffer, 0,
-                       zzEndRead-zzStartRead);
-
-      /* translate stored positions */
-      zzEndRead-= zzStartRead;
-      zzCurrentPos-= zzStartRead;
-      zzMarkedPos-= zzStartRead;
-      zzStartRead = 0;
-    }
-
-    /* is the buffer big enough? */
-    if (zzCurrentPos >= zzBuffer.length) {
-      /* if not: blow it up */
-      char newBuffer[] = new char[zzCurrentPos*2];
-      System.arraycopy(zzBuffer, 0, newBuffer, 0, zzBuffer.length);
-      zzBuffer = newBuffer;
-    }
-
-    /* finally: fill the buffer with new input */
-    int numRead = zzReader.read(zzBuffer, zzEndRead,
-                                            zzBuffer.length-zzEndRead);
-
-    if (numRead > 0) {
-      zzEndRead+= numRead;
-      return false;
-    }
-    // unlikely but not impossible: read 0 characters, but not at end of stream    
-    if (numRead == 0) {
-      int c = zzReader.read();
-      if (c == -1) {
-        return true;
-      } else {
-        zzBuffer[zzEndRead++] = (char) c;
-        return false;
-      }     
-    }
-
-	// numRead < 0
-    return true;
-  }
-
-    
-  /**
-   * Closes the input stream.
-   */
-  public final void yyclose() throws java.io.IOException {
-    zzAtEOF = true;            /* indicate end of file */
-    zzEndRead = zzStartRead;  /* invalidate buffer    */
-
-    if (zzReader != null)
-      zzReader.close();
-  }
-
-
-  /**
-   * Resets the scanner to read from a new input stream.
-   * Does not close the old reader.
-   *
-   * All internal variables are reset, the old input stream 
-   * <b>cannot</b> be reused (internal buffer is discarded and lost).
-   * Lexical state is set to <tt>ZZ_INITIAL</tt>.
-   *
-   * Internal scan buffer is resized down to its initial length, if it has grown.
-   *
-   * @param reader   the new input stream 
-   */
-  public final void yyreset(java.io.Reader reader) {
-    zzReader = reader;
-    zzAtBOL  = true;
-    zzAtEOF  = false;
-    zzEOFDone = false;
-    zzEndRead = zzStartRead = 0;
-    zzCurrentPos = zzMarkedPos = 0;
-    yyline = yychar = yycolumn = 0;
-    zzLexicalState = YYINITIAL;
-    if (zzBuffer.length > ZZ_BUFFERSIZE)
-      zzBuffer = new char[ZZ_BUFFERSIZE];
-  }
-
-
-  /**
-   * Returns the current lexical state.
-   */
-  public final int yystate() {
-    return zzLexicalState;
-  }
-
-
-  /**
-   * Enters a new lexical state
-   *
-   * @param newState the new lexical state
-   */
-  public final void yybegin(int newState) {
-    zzLexicalState = newState;
-  }
-
-
-  /**
-   * Returns the text matched by the current regular expression.
-   */
-  public final String yytext() {
-    return new String( zzBuffer, zzStartRead, zzMarkedPos-zzStartRead );
-  }
-
-
-  /**
-   * Returns the character at position <tt>pos</tt> from the 
-   * matched text. 
-   * 
-   * It is equivalent to yytext().charAt(pos), but faster
-   *
-   * @param pos the position of the character to fetch. 
-   *            A value from 0 to yylength()-1.
-   *
-   * @return the character at position pos
-   */
-  public final char yycharat(int pos) {
-    return zzBuffer[zzStartRead+pos];
-  }
-
-
-  /**
-   * Returns the length of the matched text region.
-   */
-  public final int yylength() {
-    return zzMarkedPos-zzStartRead;
-  }
-
-
-  /**
-   * Reports an error that occured while scanning.
-   *
-   * In a wellformed scanner (no or only correct usage of 
-   * yypushback(int) and a match-all fallback rule) this method 
-   * will only be called with things that "Can't Possibly Happen".
-   * If this method is called, something is seriously wrong
-   * (e.g. a JFlex bug producing a faulty scanner etc.).
-   *
-   * Usual syntax/scanner level error handling should be done
-   * in error fallback rules.
-   *
-   * @param   errorCode  the code of the errormessage to display
-   */
-  private void zzScanError(int errorCode) {
-    String message;
-    try {
-      message = ZZ_ERROR_MSG[errorCode];
-    }
-    catch (ArrayIndexOutOfBoundsException e) {
-      message = ZZ_ERROR_MSG[ZZ_UNKNOWN_ERROR];
-    }
-
-    throw new Error(message);
-  } 
-
-
-  /**
-   * Pushes the specified amount of characters back into the input stream.
-   *
-   * They will be read again by then next call of the scanning method
-   *
-   * @param number  the number of characters to be read again.
-   *                This number must not be greater than yylength()!
-   */
-  public void yypushback(int number)  {
-    if ( number > yylength() )
-      zzScanError(ZZ_PUSHBACK_2BIG);
-
-    zzMarkedPos -= number;
-  }
-
-
-  /**
-   * Resumes scanning until the next regular expression is matched,
-   * the end of input is encountered or an I/O-Error occurs.
-   *
-   * @return      the next token
-   * @exception   java.io.IOException  if any I/O-Error occurs
-   */
-  public int getNextToken() throws java.io.IOException {
-    int zzInput;
-    int zzAction;
-
-    // cached fields:
-    int zzCurrentPosL;
-    int zzMarkedPosL;
-    int zzEndReadL = zzEndRead;
-    char [] zzBufferL = zzBuffer;
-    char [] zzCMapL = ZZ_CMAP;
-
-    int [] zzTransL = ZZ_TRANS;
-    int [] zzRowMapL = ZZ_ROWMAP;
-    int [] zzAttrL = ZZ_ATTRIBUTE;
-
-    while (true) {
-      zzMarkedPosL = zzMarkedPos;
-
-      yychar+= zzMarkedPosL-zzStartRead;
-
-      zzAction = -1;
-
-      zzCurrentPosL = zzCurrentPos = zzStartRead = zzMarkedPosL;
-  
-      zzState = ZZ_LEXSTATE[zzLexicalState];
-
-
-      zzForAction: {
-        while (true) {
-    
-          if (zzCurrentPosL < zzEndReadL)
-            zzInput = zzBufferL[zzCurrentPosL++];
-          else if (zzAtEOF) {
-            zzInput = YYEOF;
-            break zzForAction;
-          }
-          else {
-            // store back cached positions
-            zzCurrentPos  = zzCurrentPosL;
-            zzMarkedPos   = zzMarkedPosL;
-            boolean eof = zzRefill();
-            // get translated positions and possibly new buffer
-            zzCurrentPosL  = zzCurrentPos;
-            zzMarkedPosL   = zzMarkedPos;
-            zzBufferL      = zzBuffer;
-            zzEndReadL     = zzEndRead;
-            if (eof) {
-              zzInput = YYEOF;
-              break zzForAction;
-            }
-            else {
-              zzInput = zzBufferL[zzCurrentPosL++];
-            }
-          }
-          int zzNext = zzTransL[ zzRowMapL[zzState] + zzCMapL[zzInput] ];
-          if (zzNext == -1) break zzForAction;
-          zzState = zzNext;
-
-          int zzAttributes = zzAttrL[zzState];
-          if ( (zzAttributes & 1) == 1 ) {
-            zzAction = zzState;
-            zzMarkedPosL = zzCurrentPosL;
-            if ( (zzAttributes & 8) == 8 ) break zzForAction;
-          }
-
-        }
-      }
-
-      // store back cached position
-      zzMarkedPos = zzMarkedPosL;
-
-      switch (zzAction < 0 ? zzAction : ZZ_ACTION[zzAction]) {
-        case 5: 
-          { return NUM;
-          }
-        case 11: break;
-        case 9: 
-          { return ACRONYM;
-          }
-        case 12: break;
-        case 7: 
-          { return COMPANY;
-          }
-        case 13: break;
-        case 10: 
-          { return EMAIL;
-          }
-        case 14: break;
-        case 1: 
-          { /* ignore */
-          }
-        case 15: break;
-        case 6: 
-          { return APOSTROPHE;
-          }
-        case 16: break;
-        case 3: 
-          { return CJ;
-          }
-        case 17: break;
-        case 8: 
-          { return ACRONYM_DEP;
-          }
-        case 18: break;
-        case 2: 
-          { return ALPHANUM;
-          }
-        case 19: break;
-        case 4: 
-          { return HOST;
-          }
-        case 20: break;
-        default: 
-          if (zzInput == YYEOF && zzStartRead == zzCurrentPos) {
-            zzAtEOF = true;
-            return YYEOF;
-          } 
-          else {
-            zzScanError(ZZ_NO_MATCH);
-          }
-      }
-    }
-  }
-
-
-}
diff --git a/lucene/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl31.jflex b/lucene/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl31.jflex
deleted file mode 100644
index c2fbc59..0000000
--- a/lucene/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl31.jflex
+++ /dev/null
@@ -1,134 +0,0 @@
-package org.apache.lucene.analysis.standard;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/*
-
-WARNING: if you change StandardTokenizerImpl*.jflex and need to regenerate
-      the tokenizer, only use the trunk version of JFlex 1.5 at the moment!
-
-*/
-
-import java.io.Reader;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-
-%%
-
-%class StandardTokenizerImpl31
-%implements StandardTokenizerInterface
-%unicode 4.0
-%integer
-%function getNextToken
-%pack
-%char
-
-%{
-
-public static final int ALPHANUM          = StandardTokenizer.ALPHANUM;
-public static final int APOSTROPHE        = StandardTokenizer.APOSTROPHE;
-public static final int ACRONYM           = StandardTokenizer.ACRONYM;
-public static final int COMPANY           = StandardTokenizer.COMPANY;
-public static final int EMAIL             = StandardTokenizer.EMAIL;
-public static final int HOST              = StandardTokenizer.HOST;
-public static final int NUM               = StandardTokenizer.NUM;
-public static final int CJ                = StandardTokenizer.CJ;
-/**
- * @deprecated this solves a bug where HOSTs that end with '.' are identified
- *             as ACRONYMs.
- */
-public static final int ACRONYM_DEP       = StandardTokenizer.ACRONYM_DEP;
-
-public static final String [] TOKEN_TYPES = StandardTokenizer.TOKEN_TYPES;
-
-public final int yychar()
-{
-    return yychar;
-}
-
-/**
- * Fills CharTermAttribute with the current token text.
- */
-public final void getText(CharTermAttribute t) {
-  t.copyBuffer(zzBuffer, zzStartRead, zzMarkedPos-zzStartRead);
-}
-
-%}
-
-THAI       = [\u0E00-\u0E59]
-
-// basic word: a sequence of digits & letters (includes Thai to enable ThaiAnalyzer to function)
-ALPHANUM   = ({LETTER}|{THAI}|[:digit:])+
-
-// internal apostrophes: O'Reilly, you're, O'Reilly's
-// use a post-filter to remove possessives
-APOSTROPHE =  {ALPHA} ("'" {ALPHA})+
-
-// acronyms: U.S.A., I.B.M., etc.
-// use a post-filter to remove dots
-ACRONYM    =  {LETTER} "." ({LETTER} ".")+
-
-ACRONYM_DEP	= {ALPHANUM} "." ({ALPHANUM} ".")+
-
-// company names like AT&T and Excite@Home.
-COMPANY    =  {ALPHA} ("&"|"@") {ALPHA}
-
-// email addresses
-EMAIL      =  {ALPHANUM} (("."|"-"|"_") {ALPHANUM})* "@" {ALPHANUM} (("."|"-") {ALPHANUM})+
-
-// hostname
-HOST       =  {ALPHANUM} ((".") {ALPHANUM})+
-
-// floating point, serial, model numbers, ip addresses, etc.
-// every other segment must have at least one digit
-NUM        = ({ALPHANUM} {P} {HAS_DIGIT}
-           | {HAS_DIGIT} {P} {ALPHANUM}
-           | {ALPHANUM} ({P} {HAS_DIGIT} {P} {ALPHANUM})+
-           | {HAS_DIGIT} ({P} {ALPHANUM} {P} {HAS_DIGIT})+
-           | {ALPHANUM} {P} {HAS_DIGIT} ({P} {ALPHANUM} {P} {HAS_DIGIT})+
-           | {HAS_DIGIT} {P} {ALPHANUM} ({P} {HAS_DIGIT} {P} {ALPHANUM})+)
-
-// punctuation
-P	         = ("_"|"-"|"/"|"."|",")
-
-// at least one digit
-HAS_DIGIT  = ({LETTER}|[:digit:])* [:digit:] ({LETTER}|[:digit:])*
-
-ALPHA      = ({LETTER})+
-
-// From the JFlex manual: "the expression that matches everything of <a> not matched by <b> is !(!<a>|<b>)"
-LETTER     = !(![:letter:]|{CJ})
-
-// Chinese and Japanese (but NOT Korean, which is included in [:letter:])
-CJ         = [\u3100-\u312f\u3040-\u309F\u30A0-\u30FF\u31F0-\u31FF\u3300-\u337f\u3400-\u4dbf\u4e00-\u9fff\uf900-\ufaff\uff65-\uff9f]
-
-WHITESPACE = \r\n | [ \r\n\t\f]
-
-%%
-
-{ALPHANUM}                                                     { return ALPHANUM; }
-{APOSTROPHE}                                                   { return APOSTROPHE; }
-{ACRONYM}                                                      { return ACRONYM; }
-{COMPANY}                                                      { return COMPANY; }
-{EMAIL}                                                        { return EMAIL; }
-{HOST}                                                         { return HOST; }
-{NUM}                                                          { return NUM; }
-{CJ}                                                           { return CJ; }
-{ACRONYM_DEP}                                                  { return ACRONYM_DEP; }
-
-/** Ignore the rest */
-. | {WHITESPACE}                                               { /* ignore */ }
diff --git a/lucene/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImplOrig.java b/lucene/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImplOrig.java
deleted file mode 100644
index 29bb994..0000000
--- a/lucene/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImplOrig.java
+++ /dev/null
@@ -1,736 +0,0 @@
-/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 17.05.10 14:50 */
-
-package org.apache.lucene.analysis.standard;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/*
-
-WARNING: if you change StandardTokenizerImpl*.jflex and need to regenerate
-      the tokenizer, only use the trunk version of JFlex 1.5 at the moment!
-
-*/
-
-import java.io.Reader;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-
-
-/**
- * This class is a scanner generated by 
- * <a href="http://www.jflex.de/">JFlex</a> 1.5.0-SNAPSHOT
- * on 17.05.10 14:50 from the specification file
- * <tt>C:/Users/Uwe Schindler/Projects/lucene/newtrunk/lucene/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImplOrig.jflex</tt>
- */
-class StandardTokenizerImplOrig implements StandardTokenizerInterface {
-
-  /** This character denotes the end of file */
-  public static final int YYEOF = -1;
-
-  /** initial size of the lookahead buffer */
-  private static final int ZZ_BUFFERSIZE = 16384;
-
-  /** lexical states */
-  public static final int YYINITIAL = 0;
-
-  /**
-   * ZZ_LEXSTATE[l] is the state in the DFA for the lexical state l
-   * ZZ_LEXSTATE[l+1] is the state in the DFA for the lexical state l
-   *                  at the beginning of a line
-   * l is of the form l = 2*k, k a non negative integer
-   */
-  private static final int ZZ_LEXSTATE[] = { 
-     0, 0
-  };
-
-  /** 
-   * Translates characters to character classes
-   */
-  private static final String ZZ_CMAP_PACKED = 
-    "\11\0\1\0\1\15\1\0\1\0\1\14\22\0\1\0\5\0\1\5"+
-    "\1\3\4\0\1\11\1\7\1\4\1\11\12\2\6\0\1\6\32\12"+
-    "\4\0\1\10\1\0\32\12\57\0\1\12\12\0\1\12\4\0\1\12"+
-    "\5\0\27\12\1\0\37\12\1\0\u0128\12\2\0\22\12\34\0\136\12"+
-    "\2\0\11\12\2\0\7\12\16\0\2\12\16\0\5\12\11\0\1\12"+
-    "\213\0\1\12\13\0\1\12\1\0\3\12\1\0\1\12\1\0\24\12"+
-    "\1\0\54\12\1\0\10\12\2\0\32\12\14\0\202\12\12\0\71\12"+
-    "\2\0\2\12\2\0\2\12\3\0\46\12\2\0\2\12\67\0\46\12"+
-    "\2\0\1\12\7\0\47\12\110\0\33\12\5\0\3\12\56\0\32\12"+
-    "\5\0\13\12\25\0\12\2\7\0\143\12\1\0\1\12\17\0\2\12"+
-    "\11\0\12\2\3\12\23\0\1\12\1\0\33\12\123\0\46\12\u015f\0"+
-    "\65\12\3\0\1\12\22\0\1\12\7\0\12\12\4\0\12\2\25\0"+
-    "\10\12\2\0\2\12\2\0\26\12\1\0\7\12\1\0\1\12\3\0"+
-    "\4\12\42\0\2\12\1\0\3\12\4\0\12\2\2\12\23\0\6\12"+
-    "\4\0\2\12\2\0\26\12\1\0\7\12\1\0\2\12\1\0\2\12"+
-    "\1\0\2\12\37\0\4\12\1\0\1\12\7\0\12\2\2\0\3\12"+
-    "\20\0\7\12\1\0\1\12\1\0\3\12\1\0\26\12\1\0\7\12"+
-    "\1\0\2\12\1\0\5\12\3\0\1\12\22\0\1\12\17\0\1\12"+
-    "\5\0\12\2\25\0\10\12\2\0\2\12\2\0\26\12\1\0\7\12"+
-    "\1\0\2\12\2\0\4\12\3\0\1\12\36\0\2\12\1\0\3\12"+
-    "\4\0\12\2\25\0\6\12\3\0\3\12\1\0\4\12\3\0\2\12"+
-    "\1\0\1\12\1\0\2\12\3\0\2\12\3\0\3\12\3\0\10\12"+
-    "\1\0\3\12\55\0\11\2\25\0\10\12\1\0\3\12\1\0\27\12"+
-    "\1\0\12\12\1\0\5\12\46\0\2\12\4\0\12\2\25\0\10\12"+
-    "\1\0\3\12\1\0\27\12\1\0\12\12\1\0\5\12\44\0\1\12"+
-    "\1\0\2\12\4\0\12\2\25\0\10\12\1\0\3\12\1\0\27\12"+
-    "\1\0\20\12\46\0\2\12\4\0\12\2\25\0\22\12\3\0\30\12"+
-    "\1\0\11\12\1\0\1\12\2\0\7\12\71\0\1\1\60\12\1\1"+
-    "\2\12\14\1\7\12\11\1\12\2\47\0\2\12\1\0\1\12\2\0"+
-    "\2\12\1\0\1\12\2\0\1\12\6\0\4\12\1\0\7\12\1\0"+
-    "\3\12\1\0\1\12\1\0\1\12\2\0\2\12\1\0\4\12\1\0"+
-    "\2\12\11\0\1\12\2\0\5\12\1\0\1\12\11\0\12\2\2\0"+
-    "\2\12\42\0\1\12\37\0\12\2\26\0\10\12\1\0\42\12\35\0"+
-    "\4\12\164\0\42\12\1\0\5\12\1\0\2\12\25\0\12\2\6\0"+
-    "\6\12\112\0\46\12\12\0\47\12\11\0\132\12\5\0\104\12\5\0"+
-    "\122\12\6\0\7\12\1\0\77\12\1\0\1\12\1\0\4\12\2\0"+
-    "\7\12\1\0\1\12\1\0\4\12\2\0\47\12\1\0\1\12\1\0"+
-    "\4\12\2\0\37\12\1\0\1\12\1\0\4\12\2\0\7\12\1\0"+
-    "\1\12\1\0\4\12\2\0\7\12\1\0\7\12\1\0\27\12\1\0"+
-    "\37\12\1\0\1\12\1\0\4\12\2\0\7\12\1\0\47\12\1\0"+
-    "\23\12\16\0\11\2\56\0\125\12\14\0\u026c\12\2\0\10\12\12\0"+
-    "\32\12\5\0\113\12\225\0\64\12\54\0\12\2\46\0\12\2\6\0"+
-    "\130\12\10\0\51\12\u0557\0\234\12\4\0\132\12\6\0\26\12\2\0"+
-    "\6\12\2\0\46\12\2\0\6\12\2\0\10\12\1\0\1\12\1\0"+
-    "\1\12\1\0\1\12\1\0\37\12\2\0\65\12\1\0\7\12\1\0"+
-    "\1\12\3\0\3\12\1\0\7\12\3\0\4\12\2\0\6\12\4\0"+
-    "\15\12\5\0\3\12\1\0\7\12\202\0\1\12\202\0\1\12\4\0"+
-    "\1\12\2\0\12\12\1\0\1\12\3\0\5\12\6\0\1\12\1\0"+
-    "\1\12\1\0\1\12\1\0\4\12\1\0\3\12\1\0\7\12\u0ecb\0"+
-    "\2\12\52\0\5\12\12\0\1\13\124\13\10\13\2\13\2\13\132\13"+
-    "\1\13\3\13\6\13\50\13\3\13\1\0\136\12\21\0\30\12\70\0"+
-    "\20\13\u0100\0\200\13\200\0\u19b6\13\12\13\100\0\u51a6\13\132\13\u048d\12"+
-    "\u0773\0\u2ba4\12\u215c\0\u012e\13\322\13\7\12\14\0\5\12\5\0\1\12"+
-    "\1\0\12\12\1\0\15\12\1\0\5\12\1\0\1\12\1\0\2\12"+
-    "\1\0\2\12\1\0\154\12\41\0\u016b\12\22\0\100\12\2\0\66\12"+
-    "\50\0\14\12\164\0\3\12\1\0\1\12\1\0\207\12\23\0\12\2"+
-    "\7\0\32\12\6\0\32\12\12\0\1\13\72\13\37\12\3\0\6\12"+
-    "\2\0\6\12\2\0\6\12\2\0\3\12\43\0";
-
-  /** 
-   * Translates characters to character classes
-   */
-  private static final char [] ZZ_CMAP = zzUnpackCMap(ZZ_CMAP_PACKED);
-
-  /** 
-   * Translates DFA states to action switch labels.
-   */
-  private static final int [] ZZ_ACTION = zzUnpackAction();
-
-  private static final String ZZ_ACTION_PACKED_0 =
-    "\1\0\1\1\3\2\1\3\1\1\13\0\1\2\3\4"+
-    "\2\0\1\5\1\0\1\5\3\4\6\5\1\6\1\4"+
-    "\2\7\1\10\1\0\1\10\3\0\2\10\1\11\1\12"+
-    "\1\4";
-
-  private static int [] zzUnpackAction() {
-    int [] result = new int[51];
-    int offset = 0;
-    offset = zzUnpackAction(ZZ_ACTION_PACKED_0, offset, result);
-    return result;
-  }
-
-  private static int zzUnpackAction(String packed, int offset, int [] result) {
-    int i = 0;       /* index in packed string  */
-    int j = offset;  /* index in unpacked array */
-    int l = packed.length();
-    while (i < l) {
-      int count = packed.charAt(i++);
-      int value = packed.charAt(i++);
-      do result[j++] = value; while (--count > 0);
-    }
-    return j;
-  }
-
-
-  /** 
-   * Translates a state to a row index in the transition table
-   */
-  private static final int [] ZZ_ROWMAP = zzUnpackRowMap();
-
-  private static final String ZZ_ROWMAP_PACKED_0 =
-    "\0\0\0\16\0\34\0\52\0\70\0\16\0\106\0\124"+
-    "\0\142\0\160\0\176\0\214\0\232\0\250\0\266\0\304"+
-    "\0\322\0\340\0\356\0\374\0\u010a\0\u0118\0\u0126\0\u0134"+
-    "\0\u0142\0\u0150\0\u015e\0\u016c\0\u017a\0\u0188\0\u0196\0\u01a4"+
-    "\0\u01b2\0\u01c0\0\u01ce\0\u01dc\0\u01ea\0\u01f8\0\322\0\u0206"+
-    "\0\u0214\0\u0222\0\u0230\0\u023e\0\u024c\0\u025a\0\124\0\214"+
-    "\0\u0268\0\u0276\0\u0284";
-
-  private static int [] zzUnpackRowMap() {
-    int [] result = new int[51];
-    int offset = 0;
-    offset = zzUnpackRowMap(ZZ_ROWMAP_PACKED_0, offset, result);
-    return result;
-  }
-
-  private static int zzUnpackRowMap(String packed, int offset, int [] result) {
-    int i = 0;  /* index in packed string  */
-    int j = offset;  /* index in unpacked array */
-    int l = packed.length();
-    while (i < l) {
-      int high = packed.charAt(i++) << 16;
-      result[j++] = high | packed.charAt(i++);
-    }
-    return j;
-  }
-
-  /** 
-   * The transition table of the DFA
-   */
-  private static final int [] ZZ_TRANS = zzUnpackTrans();
-
-  private static final String ZZ_TRANS_PACKED_0 =
-    "\1\2\1\3\1\4\7\2\1\5\1\6\1\7\1\2"+
-    "\17\0\2\3\1\0\1\10\1\0\1\11\2\12\1\13"+
-    "\1\3\4\0\1\3\1\4\1\0\1\14\1\0\1\11"+
-    "\2\15\1\16\1\4\4\0\1\3\1\4\1\17\1\20"+
-    "\1\21\1\22\2\12\1\13\1\23\20\0\1\2\1\0"+
-    "\1\24\1\25\7\0\1\26\4\0\2\27\7\0\1\27"+
-    "\4\0\1\30\1\31\7\0\1\32\5\0\1\33\7\0"+
-    "\1\13\4\0\1\34\1\35\7\0\1\36\4\0\1\37"+
-    "\1\40\7\0\1\41\4\0\1\42\1\43\7\0\1\44"+
-    "\15\0\1\45\4\0\1\24\1\25\7\0\1\46\15\0"+
-    "\1\47\4\0\2\27\7\0\1\50\4\0\1\3\1\4"+
-    "\1\17\1\10\1\21\1\22\2\12\1\13\1\23\4\0"+
-    "\2\24\1\0\1\51\1\0\1\11\2\52\1\0\1\24"+
-    "\4\0\1\24\1\25\1\0\1\53\1\0\1\11\2\54"+
-    "\1\55\1\25\4\0\1\24\1\25\1\0\1\51\1\0"+
-    "\1\11\2\52\1\0\1\26\4\0\2\27\1\0\1\56"+
-    "\2\0\1\56\2\0\1\27\4\0\2\30\1\0\1\52"+
-    "\1\0\1\11\2\52\1\0\1\30\4\0\1\30\1\31"+
-    "\1\0\1\54\1\0\1\11\2\54\1\55\1\31\4\0"+
-    "\1\30\1\31\1\0\1\52\1\0\1\11\2\52\1\0"+
-    "\1\32\5\0\1\33\1\0\1\55\2\0\3\55\1\33"+
-    "\4\0\2\34\1\0\1\57\1\0\1\11\2\12\1\13"+
-    "\1\34\4\0\1\34\1\35\1\0\1\60\1\0\1\11"+
-    "\2\15\1\16\1\35\4\0\1\34\1\35\1\0\1\57"+
-    "\1\0\1\11\2\12\1\13\1\36\4\0\2\37\1\0"+
-    "\1\12\1\0\1\11\2\12\1\13\1\37\4\0\1\37"+
-    "\1\40\1\0\1\15\1\0\1\11\2\15\1\16\1\40"+
-    "\4\0\1\37\1\40\1\0\1\12\1\0\1\11\2\12"+
-    "\1\13\1\41\4\0\2\42\1\0\1\13\2\0\3\13"+
-    "\1\42\4\0\1\42\1\43\1\0\1\16\2\0\3\16"+
-    "\1\43\4\0\1\42\1\43\1\0\1\13\2\0\3\13"+
-    "\1\44\6\0\1\17\6\0\1\45\4\0\1\24\1\25"+
-    "\1\0\1\61\1\0\1\11\2\52\1\0\1\26\4\0"+
-    "\2\27\1\0\1\56\2\0\1\56\2\0\1\50\4\0"+
-    "\2\24\7\0\1\24\4\0\2\30\7\0\1\30\4\0"+
-    "\2\34\7\0\1\34\4\0\2\37\7\0\1\37\4\0"+
-    "\2\42\7\0\1\42\4\0\2\62\7\0\1\62\4\0"+
-    "\2\24\7\0\1\63\4\0\2\62\1\0\1\56\2\0"+
-    "\1\56\2\0\1\62\4\0\2\24\1\0\1\61\1\0"+
-    "\1\11\2\52\1\0\1\24\3\0";
-
-  private static int [] zzUnpackTrans() {
-    int [] result = new int[658];
-    int offset = 0;
-    offset = zzUnpackTrans(ZZ_TRANS_PACKED_0, offset, result);
-    return result;
-  }
-
-  private static int zzUnpackTrans(String packed, int offset, int [] result) {
-    int i = 0;       /* index in packed string  */
-    int j = offset;  /* index in unpacked array */
-    int l = packed.length();
-    while (i < l) {
-      int count = packed.charAt(i++);
-      int value = packed.charAt(i++);
-      value--;
-      do result[j++] = value; while (--count > 0);
-    }
-    return j;
-  }
-
-
-  /* error codes */
-  private static final int ZZ_UNKNOWN_ERROR = 0;
-  private static final int ZZ_NO_MATCH = 1;
-  private static final int ZZ_PUSHBACK_2BIG = 2;
-
-  /* error messages for the codes above */
-  private static final String ZZ_ERROR_MSG[] = {
-    "Unkown internal scanner error",
-    "Error: could not match input",
-    "Error: pushback value was too large"
-  };
-
-  /**
-   * ZZ_ATTRIBUTE[aState] contains the attributes of state <code>aState</code>
-   */
-  private static final int [] ZZ_ATTRIBUTE = zzUnpackAttribute();
-
-  private static final String ZZ_ATTRIBUTE_PACKED_0 =
-    "\1\0\1\11\3\1\1\11\1\1\13\0\4\1\2\0"+
-    "\1\1\1\0\17\1\1\0\1\1\3\0\5\1";
-
-  private static int [] zzUnpackAttribute() {
-    int [] result = new int[51];
-    int offset = 0;
-    offset = zzUnpackAttribute(ZZ_ATTRIBUTE_PACKED_0, offset, result);
-    return result;
-  }
-
-  private static int zzUnpackAttribute(String packed, int offset, int [] result) {
-    int i = 0;       /* index in packed string  */
-    int j = offset;  /* index in unpacked array */
-    int l = packed.length();
-    while (i < l) {
-      int count = packed.charAt(i++);
-      int value = packed.charAt(i++);
-      do result[j++] = value; while (--count > 0);
-    }
-    return j;
-  }
-
-  /** the input device */
-  private java.io.Reader zzReader;
-
-  /** the current state of the DFA */
-  private int zzState;
-
-  /** the current lexical state */
-  private int zzLexicalState = YYINITIAL;
-
-  /** this buffer contains the current text to be matched and is
-      the source of the yytext() string */
-  private char zzBuffer[] = new char[ZZ_BUFFERSIZE];
-
-  /** the textposition at the last accepting state */
-  private int zzMarkedPos;
-
-  /** the current text position in the buffer */
-  private int zzCurrentPos;
-
-  /** startRead marks the beginning of the yytext() string in the buffer */
-  private int zzStartRead;
-
-  /** endRead marks the last character in the buffer, that has been read
-      from input */
-  private int zzEndRead;
-
-  /** number of newlines encountered up to the start of the matched text */
-  private int yyline;
-
-  /** the number of characters up to the start of the matched text */
-  private int yychar;
-
-  /**
-   * the number of characters from the last newline up to the start of the 
-   * matched text
-   */
-  private int yycolumn;
-
-  /** 
-   * zzAtBOL == true <=> the scanner is currently at the beginning of a line
-   */
-  private boolean zzAtBOL = true;
-
-  /** zzAtEOF == true <=> the scanner is at the EOF */
-  private boolean zzAtEOF;
-
-  /** denotes if the user-EOF-code has already been executed */
-  private boolean zzEOFDone;
-
-  /* user code: */
-
-public static final int ALPHANUM          = StandardTokenizer.ALPHANUM;
-public static final int APOSTROPHE        = StandardTokenizer.APOSTROPHE;
-public static final int ACRONYM           = StandardTokenizer.ACRONYM;
-public static final int COMPANY           = StandardTokenizer.COMPANY;
-public static final int EMAIL             = StandardTokenizer.EMAIL;
-public static final int HOST              = StandardTokenizer.HOST;
-public static final int NUM               = StandardTokenizer.NUM;
-public static final int CJ                = StandardTokenizer.CJ;
-/**
- * @deprecated this solves a bug where HOSTs that end with '.' are identified
- *             as ACRONYMs.
- */
-public static final int ACRONYM_DEP       = StandardTokenizer.ACRONYM_DEP;
-
-public static final String [] TOKEN_TYPES = StandardTokenizer.TOKEN_TYPES;
-
-public final int yychar()
-{
-    return yychar;
-}
-
-/**
- * Fills CharTermAttribute with the current token text.
- */
-public final void getText(CharTermAttribute t) {
-  t.copyBuffer(zzBuffer, zzStartRead, zzMarkedPos-zzStartRead);
-}
-
-
-
-  /**
-   * Creates a new scanner
-   * There is also a java.io.InputStream version of this constructor.
-   *
-   * @param   in  the java.io.Reader to read input from.
-   */
-  StandardTokenizerImplOrig(java.io.Reader in) {
-    this.zzReader = in;
-  }
-
-  /**
-   * Creates a new scanner.
-   * There is also java.io.Reader version of this constructor.
-   *
-   * @param   in  the java.io.Inputstream to read input from.
-   */
-  StandardTokenizerImplOrig(java.io.InputStream in) {
-    this(new java.io.InputStreamReader(in));
-  }
-
-  /** 
-   * Unpacks the compressed character translation table.
-   *
-   * @param packed   the packed character translation table
-   * @return         the unpacked character translation table
-   */
-  private static char [] zzUnpackCMap(String packed) {
-    char [] map = new char[0x10000];
-    int i = 0;  /* index in packed string  */
-    int j = 0;  /* index in unpacked array */
-    while (i < 1154) {
-      int  count = packed.charAt(i++);
-      char value = packed.charAt(i++);
-      do map[j++] = value; while (--count > 0);
-    }
-    return map;
-  }
-
-
-  /**
-   * Refills the input buffer.
-   *
-   * @return      <code>false</code>, iff there was new input.
-   * 
-   * @exception   java.io.IOException  if any I/O-Error occurs
-   */
-  private boolean zzRefill() throws java.io.IOException {
-
-    /* first: make room (if you can) */
-    if (zzStartRead > 0) {
-      System.arraycopy(zzBuffer, zzStartRead,
-                       zzBuffer, 0,
-                       zzEndRead-zzStartRead);
-
-      /* translate stored positions */
-      zzEndRead-= zzStartRead;
-      zzCurrentPos-= zzStartRead;
-      zzMarkedPos-= zzStartRead;
-      zzStartRead = 0;
-    }
-
-    /* is the buffer big enough? */
-    if (zzCurrentPos >= zzBuffer.length) {
-      /* if not: blow it up */
-      char newBuffer[] = new char[zzCurrentPos*2];
-      System.arraycopy(zzBuffer, 0, newBuffer, 0, zzBuffer.length);
-      zzBuffer = newBuffer;
-    }
-
-    /* finally: fill the buffer with new input */
-    int numRead = zzReader.read(zzBuffer, zzEndRead,
-                                            zzBuffer.length-zzEndRead);
-
-    if (numRead > 0) {
-      zzEndRead+= numRead;
-      return false;
-    }
-    // unlikely but not impossible: read 0 characters, but not at end of stream    
-    if (numRead == 0) {
-      int c = zzReader.read();
-      if (c == -1) {
-        return true;
-      } else {
-        zzBuffer[zzEndRead++] = (char) c;
-        return false;
-      }     
-    }
-
-	// numRead < 0
-    return true;
-  }
-
-    
-  /**
-   * Closes the input stream.
-   */
-  public final void yyclose() throws java.io.IOException {
-    zzAtEOF = true;            /* indicate end of file */
-    zzEndRead = zzStartRead;  /* invalidate buffer    */
-
-    if (zzReader != null)
-      zzReader.close();
-  }
-
-
-  /**
-   * Resets the scanner to read from a new input stream.
-   * Does not close the old reader.
-   *
-   * All internal variables are reset, the old input stream 
-   * <b>cannot</b> be reused (internal buffer is discarded and lost).
-   * Lexical state is set to <tt>ZZ_INITIAL</tt>.
-   *
-   * Internal scan buffer is resized down to its initial length, if it has grown.
-   *
-   * @param reader   the new input stream 
-   */
-  public final void yyreset(java.io.Reader reader) {
-    zzReader = reader;
-    zzAtBOL  = true;
-    zzAtEOF  = false;
-    zzEOFDone = false;
-    zzEndRead = zzStartRead = 0;
-    zzCurrentPos = zzMarkedPos = 0;
-    yyline = yychar = yycolumn = 0;
-    zzLexicalState = YYINITIAL;
-    if (zzBuffer.length > ZZ_BUFFERSIZE)
-      zzBuffer = new char[ZZ_BUFFERSIZE];
-  }
-
-
-  /**
-   * Returns the current lexical state.
-   */
-  public final int yystate() {
-    return zzLexicalState;
-  }
-
-
-  /**
-   * Enters a new lexical state
-   *
-   * @param newState the new lexical state
-   */
-  public final void yybegin(int newState) {
-    zzLexicalState = newState;
-  }
-
-
-  /**
-   * Returns the text matched by the current regular expression.
-   */
-  public final String yytext() {
-    return new String( zzBuffer, zzStartRead, zzMarkedPos-zzStartRead );
-  }
-
-
-  /**
-   * Returns the character at position <tt>pos</tt> from the 
-   * matched text. 
-   * 
-   * It is equivalent to yytext().charAt(pos), but faster
-   *
-   * @param pos the position of the character to fetch. 
-   *            A value from 0 to yylength()-1.
-   *
-   * @return the character at position pos
-   */
-  public final char yycharat(int pos) {
-    return zzBuffer[zzStartRead+pos];
-  }
-
-
-  /**
-   * Returns the length of the matched text region.
-   */
-  public final int yylength() {
-    return zzMarkedPos-zzStartRead;
-  }
-
-
-  /**
-   * Reports an error that occured while scanning.
-   *
-   * In a wellformed scanner (no or only correct usage of 
-   * yypushback(int) and a match-all fallback rule) this method 
-   * will only be called with things that "Can't Possibly Happen".
-   * If this method is called, something is seriously wrong
-   * (e.g. a JFlex bug producing a faulty scanner etc.).
-   *
-   * Usual syntax/scanner level error handling should be done
-   * in error fallback rules.
-   *
-   * @param   errorCode  the code of the errormessage to display
-   */
-  private void zzScanError(int errorCode) {
-    String message;
-    try {
-      message = ZZ_ERROR_MSG[errorCode];
-    }
-    catch (ArrayIndexOutOfBoundsException e) {
-      message = ZZ_ERROR_MSG[ZZ_UNKNOWN_ERROR];
-    }
-
-    throw new Error(message);
-  } 
-
-
-  /**
-   * Pushes the specified amount of characters back into the input stream.
-   *
-   * They will be read again by then next call of the scanning method
-   *
-   * @param number  the number of characters to be read again.
-   *                This number must not be greater than yylength()!
-   */
-  public void yypushback(int number)  {
-    if ( number > yylength() )
-      zzScanError(ZZ_PUSHBACK_2BIG);
-
-    zzMarkedPos -= number;
-  }
-
-
-  /**
-   * Resumes scanning until the next regular expression is matched,
-   * the end of input is encountered or an I/O-Error occurs.
-   *
-   * @return      the next token
-   * @exception   java.io.IOException  if any I/O-Error occurs
-   */
-  public int getNextToken() throws java.io.IOException {
-    int zzInput;
-    int zzAction;
-
-    // cached fields:
-    int zzCurrentPosL;
-    int zzMarkedPosL;
-    int zzEndReadL = zzEndRead;
-    char [] zzBufferL = zzBuffer;
-    char [] zzCMapL = ZZ_CMAP;
-
-    int [] zzTransL = ZZ_TRANS;
-    int [] zzRowMapL = ZZ_ROWMAP;
-    int [] zzAttrL = ZZ_ATTRIBUTE;
-
-    while (true) {
-      zzMarkedPosL = zzMarkedPos;
-
-      yychar+= zzMarkedPosL-zzStartRead;
-
-      zzAction = -1;
-
-      zzCurrentPosL = zzCurrentPos = zzStartRead = zzMarkedPosL;
-  
-      zzState = ZZ_LEXSTATE[zzLexicalState];
-
-
-      zzForAction: {
-        while (true) {
-    
-          if (zzCurrentPosL < zzEndReadL)
-            zzInput = zzBufferL[zzCurrentPosL++];
-          else if (zzAtEOF) {
-            zzInput = YYEOF;
-            break zzForAction;
-          }
-          else {
-            // store back cached positions
-            zzCurrentPos  = zzCurrentPosL;
-            zzMarkedPos   = zzMarkedPosL;
-            boolean eof = zzRefill();
-            // get translated positions and possibly new buffer
-            zzCurrentPosL  = zzCurrentPos;
-            zzMarkedPosL   = zzMarkedPos;
-            zzBufferL      = zzBuffer;
-            zzEndReadL     = zzEndRead;
-            if (eof) {
-              zzInput = YYEOF;
-              break zzForAction;
-            }
-            else {
-              zzInput = zzBufferL[zzCurrentPosL++];
-            }
-          }
-          int zzNext = zzTransL[ zzRowMapL[zzState] + zzCMapL[zzInput] ];
-          if (zzNext == -1) break zzForAction;
-          zzState = zzNext;
-
-          int zzAttributes = zzAttrL[zzState];
-          if ( (zzAttributes & 1) == 1 ) {
-            zzAction = zzState;
-            zzMarkedPosL = zzCurrentPosL;
-            if ( (zzAttributes & 8) == 8 ) break zzForAction;
-          }
-
-        }
-      }
-
-      // store back cached position
-      zzMarkedPos = zzMarkedPosL;
-
-      switch (zzAction < 0 ? zzAction : ZZ_ACTION[zzAction]) {
-        case 5: 
-          { return NUM;
-          }
-        case 11: break;
-        case 9: 
-          { return ACRONYM;
-          }
-        case 12: break;
-        case 7: 
-          { return COMPANY;
-          }
-        case 13: break;
-        case 10: 
-          { return EMAIL;
-          }
-        case 14: break;
-        case 1: 
-          { /* ignore */
-          }
-        case 15: break;
-        case 6: 
-          { return APOSTROPHE;
-          }
-        case 16: break;
-        case 3: 
-          { return CJ;
-          }
-        case 17: break;
-        case 8: 
-          { return ACRONYM_DEP;
-          }
-        case 18: break;
-        case 2: 
-          { return ALPHANUM;
-          }
-        case 19: break;
-        case 4: 
-          { return HOST;
-          }
-        case 20: break;
-        default: 
-          if (zzInput == YYEOF && zzStartRead == zzCurrentPos) {
-            zzAtEOF = true;
-            return YYEOF;
-          } 
-          else {
-            zzScanError(ZZ_NO_MATCH);
-          }
-      }
-    }
-  }
-
-
-}
diff --git a/lucene/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImplOrig.jflex b/lucene/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImplOrig.jflex
deleted file mode 100644
index 93aca3a..0000000
--- a/lucene/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImplOrig.jflex
+++ /dev/null
@@ -1,134 +0,0 @@
-package org.apache.lucene.analysis.standard;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/*
-
-WARNING: if you change StandardTokenizerImpl*.jflex and need to regenerate
-      the tokenizer, only use the trunk version of JFlex 1.5 at the moment!
-
-*/
-
-import java.io.Reader;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-
-%%
-
-%class StandardTokenizerImplOrig
-%implements StandardTokenizerInterface
-%unicode 3.0
-%integer
-%function getNextToken
-%pack
-%char
-
-%{
-
-public static final int ALPHANUM          = StandardTokenizer.ALPHANUM;
-public static final int APOSTROPHE        = StandardTokenizer.APOSTROPHE;
-public static final int ACRONYM           = StandardTokenizer.ACRONYM;
-public static final int COMPANY           = StandardTokenizer.COMPANY;
-public static final int EMAIL             = StandardTokenizer.EMAIL;
-public static final int HOST              = StandardTokenizer.HOST;
-public static final int NUM               = StandardTokenizer.NUM;
-public static final int CJ                = StandardTokenizer.CJ;
-/**
- * @deprecated this solves a bug where HOSTs that end with '.' are identified
- *             as ACRONYMs.
- */
-public static final int ACRONYM_DEP       = StandardTokenizer.ACRONYM_DEP;
-
-public static final String [] TOKEN_TYPES = StandardTokenizer.TOKEN_TYPES;
-
-public final int yychar()
-{
-    return yychar;
-}
-
-/**
- * Fills CharTermAttribute with the current token text.
- */
-public final void getText(CharTermAttribute t) {
-  t.copyBuffer(zzBuffer, zzStartRead, zzMarkedPos-zzStartRead);
-}
-
-%}
-
-THAI       = [\u0E00-\u0E59]
-
-// basic word: a sequence of digits & letters (includes Thai to enable ThaiAnalyzer to function)
-ALPHANUM   = ({LETTER}|{THAI}|[:digit:])+
-
-// internal apostrophes: O'Reilly, you're, O'Reilly's
-// use a post-filter to remove possessives
-APOSTROPHE =  {ALPHA} ("'" {ALPHA})+
-
-// acronyms: U.S.A., I.B.M., etc.
-// use a post-filter to remove dots
-ACRONYM    =  {LETTER} "." ({LETTER} ".")+
-
-ACRONYM_DEP	= {ALPHANUM} "." ({ALPHANUM} ".")+
-
-// company names like AT&T and Excite@Home.
-COMPANY    =  {ALPHA} ("&"|"@") {ALPHA}
-
-// email addresses
-EMAIL      =  {ALPHANUM} (("."|"-"|"_") {ALPHANUM})* "@" {ALPHANUM} (("."|"-") {ALPHANUM})+
-
-// hostname
-HOST       =  {ALPHANUM} ((".") {ALPHANUM})+
-
-// floating point, serial, model numbers, ip addresses, etc.
-// every other segment must have at least one digit
-NUM        = ({ALPHANUM} {P} {HAS_DIGIT}
-           | {HAS_DIGIT} {P} {ALPHANUM}
-           | {ALPHANUM} ({P} {HAS_DIGIT} {P} {ALPHANUM})+
-           | {HAS_DIGIT} ({P} {ALPHANUM} {P} {HAS_DIGIT})+
-           | {ALPHANUM} {P} {HAS_DIGIT} ({P} {ALPHANUM} {P} {HAS_DIGIT})+
-           | {HAS_DIGIT} {P} {ALPHANUM} ({P} {HAS_DIGIT} {P} {ALPHANUM})+)
-
-// punctuation
-P	         = ("_"|"-"|"/"|"."|",")
-
-// at least one digit
-HAS_DIGIT  = ({LETTER}|[:digit:])* [:digit:] ({LETTER}|[:digit:])*
-
-ALPHA      = ({LETTER})+
-
-// From the JFlex manual: "the expression that matches everything of <a> not matched by <b> is !(!<a>|<b>)"
-LETTER     = !(![:letter:]|{CJ})
-
-// Chinese and Japanese (but NOT Korean, which is included in [:letter:])
-CJ         = [\u3100-\u312f\u3040-\u309F\u30A0-\u30FF\u31F0-\u31FF\u3300-\u337f\u3400-\u4dbf\u4e00-\u9fff\uf900-\ufaff\uff65-\uff9f]
-
-WHITESPACE = \r\n | [ \r\n\t\f]
-
-%%
-
-{ALPHANUM}                                                     { return ALPHANUM; }
-{APOSTROPHE}                                                   { return APOSTROPHE; }
-{ACRONYM}                                                      { return ACRONYM; }
-{COMPANY}                                                      { return COMPANY; }
-{EMAIL}                                                        { return EMAIL; }
-{HOST}                                                         { return HOST; }
-{NUM}                                                          { return NUM; }
-{CJ}                                                           { return CJ; }
-{ACRONYM_DEP}                                                  { return ACRONYM_DEP; }
-
-/** Ignore the rest */
-. | {WHITESPACE}                                               { /* ignore */ }
diff --git a/lucene/src/java/org/apache/lucene/analysis/standard/StandardTokenizerInterface.java b/lucene/src/java/org/apache/lucene/analysis/standard/StandardTokenizerInterface.java
deleted file mode 100644
index f78cee3..0000000
--- a/lucene/src/java/org/apache/lucene/analysis/standard/StandardTokenizerInterface.java
+++ /dev/null
@@ -1,66 +0,0 @@
-package org.apache.lucene.analysis.standard;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-
-import java.io.Reader;
-import java.io.IOException;
-
-interface StandardTokenizerInterface {
-
-  /** This character denotes the end of file */
-  public static final int YYEOF = -1;
-
-  /**
-   * Copies the matched text into the CharTermAttribute
-   */
-  void getText(CharTermAttribute t);
-
-  /**
-   * Returns the current position.
-   */
-  int yychar();
-
-  /**
-   * Resets the scanner to read from a new input stream.
-   * Does not close the old reader.
-   *
-   * All internal variables are reset, the old input stream 
-   * <b>cannot</b> be reused (internal buffer is discarded and lost).
-   * Lexical state is set to <tt>ZZ_INITIAL</tt>.
-   *
-   * @param reader   the new input stream 
-   */
-  void yyreset(Reader reader);
-
-  /**
-   * Returns the length of the matched text region.
-   */
-  int yylength();
-
-  /**
-   * Resumes scanning until the next regular expression is matched,
-   * the end of input is encountered or an I/O-Error occurs.
-   *
-   * @return      the next token, {@link #YYEOF} on end of stream
-   * @exception   IOException  if any I/O-Error occurs
-   */
-  int getNextToken() throws IOException;
-
-}
diff --git a/lucene/src/java/org/apache/lucene/analysis/standard/package.html b/lucene/src/java/org/apache/lucene/analysis/standard/package.html
deleted file mode 100644
index 6035d9b..0000000
--- a/lucene/src/java/org/apache/lucene/analysis/standard/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-A fast grammar-based tokenizer constructed with JFlex.
-</body>
-</html>
diff --git a/lucene/src/java/org/apache/lucene/collation/CollationKeyAnalyzer.java b/lucene/src/java/org/apache/lucene/collation/CollationKeyAnalyzer.java
deleted file mode 100644
index 509b748..0000000
--- a/lucene/src/java/org/apache/lucene/collation/CollationKeyAnalyzer.java
+++ /dev/null
@@ -1,113 +0,0 @@
-package org.apache.lucene.collation;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.KeywordTokenizer;
-import org.apache.lucene.analysis.Tokenizer;
-
-import java.text.Collator;
-import java.io.Reader;
-import java.io.IOException;
-
-/**
- * <p>
- *   Filters {@link KeywordTokenizer} with {@link CollationKeyFilter}.
- * </p>
- * <p>
- *   Converts the token into its {@link java.text.CollationKey}, and then
- *   encodes the CollationKey with 
- *   {@link org.apache.lucene.util.IndexableBinaryStringTools}, to allow 
- *   it to be stored as an index term.
- * </p>
- * <p>
- *   <strong>WARNING:</strong> Make sure you use exactly the same Collator at
- *   index and query time -- CollationKeys are only comparable when produced by
- *   the same Collator.  Since {@link java.text.RuleBasedCollator}s are not
- *   independently versioned, it is unsafe to search against stored
- *   CollationKeys unless the following are exactly the same (best practice is
- *   to store this information with the index and check that they remain the
- *   same at query time):
- * </p>
- * <ol>
- *   <li>JVM vendor</li>
- *   <li>JVM version, including patch version</li>
- *   <li>
- *     The language (and country and variant, if specified) of the Locale
- *     used when constructing the collator via
- *     {@link Collator#getInstance(java.util.Locale)}.
- *   </li>
- *   <li>
- *     The collation strength used - see {@link Collator#setStrength(int)}
- *   </li>
- * </ol> 
- * <p>
- *   The <code>ICUCollationKeyAnalyzer</code> in the icu package of Lucene's
- *   contrib area uses ICU4J's Collator, which makes its
- *   its version available, thus allowing collation to be versioned
- *   independently from the JVM.  ICUCollationKeyAnalyzer is also significantly
- *   faster and generates significantly shorter keys than CollationKeyAnalyzer.
- *   See <a href="http://site.icu-project.org/charts/collation-icu4j-sun"
- *   >http://site.icu-project.org/charts/collation-icu4j-sun</a> for key
- *   generation timing and key length comparisons between ICU4J and
- *   java.text.Collator over several languages.
- * </p>
- * <p>
- *   CollationKeys generated by java.text.Collators are not compatible
- *   with those those generated by ICU Collators.  Specifically, if you use 
- *   CollationKeyAnalyzer to generate index terms, do not use
- *   ICUCollationKeyAnalyzer on the query side, or vice versa.
- * </p>
- */
-public final class CollationKeyAnalyzer extends Analyzer {
-  private Collator collator;
-
-  public CollationKeyAnalyzer(Collator collator) {
-    this.collator = collator;
-  }
-
-  @Override
-  public TokenStream tokenStream(String fieldName, Reader reader) {
-    TokenStream result = new KeywordTokenizer(reader);
-    result = new CollationKeyFilter(result, collator);
-    return result;
-  }
-  
-  private class SavedStreams {
-    Tokenizer source;
-    TokenStream result;
-  }
-  
-  @Override
-  public TokenStream reusableTokenStream(String fieldName, Reader reader) 
-    throws IOException {
-    
-    SavedStreams streams = (SavedStreams)getPreviousTokenStream();
-    if (streams == null) {
-      streams = new SavedStreams();
-      streams.source = new KeywordTokenizer(reader);
-      streams.result = new CollationKeyFilter(streams.source, collator);
-      setPreviousTokenStream(streams);
-    } else {
-      streams.source.reset(reader);
-    }
-    return streams.result;
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/collation/CollationKeyFilter.java b/lucene/src/java/org/apache/lucene/collation/CollationKeyFilter.java
deleted file mode 100644
index d5bcc81..0000000
--- a/lucene/src/java/org/apache/lucene/collation/CollationKeyFilter.java
+++ /dev/null
@@ -1,103 +0,0 @@
-package org.apache.lucene.collation;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-import org.apache.lucene.analysis.TokenFilter;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.util.IndexableBinaryStringTools;
-
-import java.io.IOException;
-import java.text.Collator;
-
-
-/**
- * <p>
- *   Converts each token into its {@link java.text.CollationKey}, and then
- *   encodes the CollationKey with {@link IndexableBinaryStringTools}, to allow 
- *   it to be stored as an index term.
- * </p>
- * <p>
- *   <strong>WARNING:</strong> Make sure you use exactly the same Collator at
- *   index and query time -- CollationKeys are only comparable when produced by
- *   the same Collator.  Since {@link java.text.RuleBasedCollator}s are not
- *   independently versioned, it is unsafe to search against stored
- *   CollationKeys unless the following are exactly the same (best practice is
- *   to store this information with the index and check that they remain the
- *   same at query time):
- * </p>
- * <ol>
- *   <li>JVM vendor</li>
- *   <li>JVM version, including patch version</li>
- *   <li>
- *     The language (and country and variant, if specified) of the Locale
- *     used when constructing the collator via
- *     {@link Collator#getInstance(java.util.Locale)}.
- *   </li>
- *   <li>
- *     The collation strength used - see {@link Collator#setStrength(int)}
- *   </li>
- * </ol> 
- * <p>
- *   The <code>ICUCollationKeyFilter</code> in the icu package of Lucene's
- *   contrib area uses ICU4J's Collator, which makes its
- *   version available, thus allowing collation to be versioned independently
- *   from the JVM.  ICUCollationKeyFilter is also significantly faster and
- *   generates significantly shorter keys than CollationKeyFilter.  See
- *   <a href="http://site.icu-project.org/charts/collation-icu4j-sun"
- *   >http://site.icu-project.org/charts/collation-icu4j-sun</a> for key
- *   generation timing and key length comparisons between ICU4J and
- *   java.text.Collator over several languages.
- * </p>
- * <p>
- *   CollationKeys generated by java.text.Collators are not compatible
- *   with those those generated by ICU Collators.  Specifically, if you use 
- *   CollationKeyFilter to generate index terms, do not use
- *   ICUCollationKeyFilter on the query side, or vice versa.
- * </p>
- */
-public final class CollationKeyFilter extends TokenFilter {
-  private final Collator collator;
-  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-
-  /**
-   * @param input Source token stream
-   * @param collator CollationKey generator
-   */
-  public CollationKeyFilter(TokenStream input, Collator collator) {
-    super(input);
-    this.collator = collator;
-  }
-
-  @Override
-  public boolean incrementToken() throws IOException {
-    if (input.incrementToken()) {
-      byte[] collationKey = collator.getCollationKey(termAtt.toString()).toByteArray();
-      int encodedLength = IndexableBinaryStringTools.getEncodedLength(
-          collationKey, 0, collationKey.length);
-      termAtt.resizeBuffer(encodedLength);
-      termAtt.setLength(encodedLength);
-      IndexableBinaryStringTools.encode(collationKey, 0, collationKey.length,
-          termAtt.buffer(), 0, encodedLength);
-      return true;
-    } else {
-      return false;
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/collation/package.html b/lucene/src/java/org/apache/lucene/collation/package.html
deleted file mode 100644
index b0c6f80..0000000
--- a/lucene/src/java/org/apache/lucene/collation/package.html
+++ /dev/null
@@ -1,176 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-  <title>Lucene Collation Package</title>
-</head>
-<body>
-<p>
-  <code>CollationKeyFilter</code>
-  converts each token into its binary <code>CollationKey</code> using the 
-  provided <code>Collator</code>, and then encode the <code>CollationKey</code>
-  as a String using
-  {@link org.apache.lucene.util.IndexableBinaryStringTools}, to allow it to be 
-  stored as an index term.
-</p>
-
-<h2>Use Cases</h2>
-
-<ul>
-  <li>
-    Efficient sorting of terms in languages that use non-Unicode character 
-    orderings.  (Lucene Sort using a Locale can be very slow.) 
-  </li>
-  <li>
-    Efficient range queries over fields that contain terms in languages that 
-    use non-Unicode character orderings.  (Range queries using a Locale can be
-    very slow.)
-  </li>
-  <li>
-    Effective Locale-specific normalization (case differences, diacritics, etc.).
-    ({@link org.apache.lucene.analysis.LowerCaseFilter} and 
-    {@link org.apache.lucene.analysis.ASCIIFoldingFilter} provide these services
-    in a generic way that doesn't take into account locale-specific needs.)
-  </li>
-</ul>
-
-<h2>Example Usages</h2>
-
-<h3>Farsi Range Queries</h3>
-<code><pre>
-  // "fa" Locale is not supported by Sun JDK 1.4 or 1.5
-  Collator collator = Collator.getInstance(new Locale("ar"));
-  CollationKeyAnalyzer analyzer = new CollationKeyAnalyzer(collator);
-  RAMDirectory ramDir = new RAMDirectory();
-  IndexWriter writer = new IndexWriter
-    (ramDir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);
-  Document doc = new Document();
-  doc.add(new Field("content", "\u0633\u0627\u0628", 
-                    Field.Store.YES, Field.Index.ANALYZED));
-  writer.addDocument(doc);
-  writer.close();
-  IndexSearcher is = new IndexSearcher(ramDir, true);
-
-  // The AnalyzingQueryParser in Lucene's contrib allows terms in range queries
-  // to be passed through an analyzer - Lucene's standard QueryParser does not
-  // allow this.
-  AnalyzingQueryParser aqp = new AnalyzingQueryParser("content", analyzer);
-  aqp.setLowercaseExpandedTerms(false);
-  
-  // Unicode order would include U+0633 in [ U+062F - U+0698 ], but Farsi
-  // orders the U+0698 character before the U+0633 character, so the single
-  // indexed Term above should NOT be returned by a ConstantScoreRangeQuery
-  // with a Farsi Collator (or an Arabic one for the case when Farsi is not
-  // supported).
-  ScoreDoc[] result
-    = is.search(aqp.parse("[ \u062F TO \u0698 ]"), null, 1000).scoreDocs;
-  assertEquals("The index Term should not be included.", 0, result.length);
-</pre></code>
-
-<h3>Danish Sorting</h3>
-<code><pre>
-  Analyzer analyzer 
-    = new CollationKeyAnalyzer(Collator.getInstance(new Locale("da", "dk")));
-  RAMDirectory indexStore = new RAMDirectory();
-  IndexWriter writer = new IndexWriter 
-    (indexStore, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);
-  String[] tracer = new String[] { "A", "B", "C", "D", "E" };
-  String[] data = new String[] { "HAT", "HUT", "H\u00C5T", "H\u00D8T", "HOT" };
-  String[] sortedTracerOrder = new String[] { "A", "E", "B", "D", "C" };
-  for (int i = 0 ; i < data.length ; ++i) {
-    Document doc = new Document();
-    doc.add(new Field("tracer", tracer[i], Field.Store.YES, Field.Index.NO));
-    doc.add(new Field("contents", data[i], Field.Store.NO, Field.Index.ANALYZED));
-    writer.addDocument(doc);
-  }
-  writer.close();
-  Searcher searcher = new IndexSearcher(indexStore, true);
-  Sort sort = new Sort();
-  sort.setSort(new SortField("contents", SortField.STRING));
-  Query query = new MatchAllDocsQuery();
-  ScoreDoc[] result = searcher.search(query, null, 1000, sort).scoreDocs;
-  for (int i = 0 ; i < result.length ; ++i) {
-    Document doc = searcher.doc(result[i].doc);
-    assertEquals(sortedTracerOrder[i], doc.getValues("tracer")[0]);
-  }
-</pre></code>
-
-<h3>Turkish Case Normalization</h3>
-<code><pre>
-  Collator collator = Collator.getInstance(new Locale("tr", "TR"));
-  collator.setStrength(Collator.PRIMARY);
-  Analyzer analyzer = new CollationKeyAnalyzer(collator);
-  RAMDirectory ramDir = new RAMDirectory();
-  IndexWriter writer = new IndexWriter
-    (ramDir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);
-  Document doc = new Document();
-  doc.add(new Field("contents", "DIGY", Field.Store.NO, Field.Index.ANALYZED));
-  writer.addDocument(doc);
-  writer.close();
-  IndexSearcher is = new IndexSearcher(ramDir, true);
-  QueryParser parser = new QueryParser("contents", analyzer);
-  Query query = parser.parse("d\u0131gy");   // U+0131: dotless i
-  ScoreDoc[] result = is.search(query, null, 1000).scoreDocs;
-  assertEquals("The index Term should be included.", 1, result.length);
-</pre></code>
-
-<h2>Caveats and Comparisons</h2>
-<p>
-  <strong>WARNING:</strong> Make sure you use exactly the same 
-  <code>Collator</code> at index and query time -- <code>CollationKey</code>s
-  are only comparable when produced by
-  the same <code>Collator</code>.  Since {@link java.text.RuleBasedCollator}s
-  are not independently versioned, it is unsafe to search against stored
-  <code>CollationKey</code>s unless the following are exactly the same (best 
-  practice is to store this information with the index and check that they
-  remain the same at query time):
-</p>
-<ol>
-  <li>JVM vendor</li>
-  <li>JVM version, including patch version</li>
-  <li>
-    The language (and country and variant, if specified) of the Locale
-    used when constructing the collator via
-    {@link java.text.Collator#getInstance(java.util.Locale)}.
-  </li>
-  <li>
-    The collation strength used - see {@link java.text.Collator#setStrength(int)}
-  </li>
-</ol> 
-<p>
-  <code>ICUCollationKeyFilter</code>, available in the icu package in Lucene's contrib area,
-  uses ICU4J's <code>Collator</code>, which 
-  makes its version available, thus allowing collation to be versioned
-  independently from the JVM.  <code>ICUCollationKeyFilter</code> is also 
-  significantly faster and generates significantly shorter keys than 
-  <code>CollationKeyFilter</code>.  See
-  <a href="http://site.icu-project.org/charts/collation-icu4j-sun"
-    >http://site.icu-project.org/charts/collation-icu4j-sun</a> for key
-  generation timing and key length comparisons between ICU4J and
-  <code>java.text.Collator</code> over several languages.
-</p>
-<p>
-  <code>CollationKey</code>s generated by <code>java.text.Collator</code>s are 
-  not compatible with those those generated by ICU Collators.  Specifically, if
-  you use <code>CollationKeyFilter</code> to generate index terms, do not use
-  <code>ICUCollationKeyFilter</code> on the query side, or vice versa.
-</p>
-<pre>
-</pre>
-</body>
-</html>
diff --git a/lucene/src/java/org/apache/lucene/queryParser/QueryParser.java b/lucene/src/java/org/apache/lucene/queryParser/QueryParser.java
index 238833c..9fed418 100644
--- a/lucene/src/java/org/apache/lucene/queryParser/QueryParser.java
+++ b/lucene/src/java/org/apache/lucene/queryParser/QueryParser.java
@@ -1082,22 +1082,6 @@ public class QueryParser implements QueryParserConstants {
     return sb.toString();
   }
 
-  /**
-   * Command line tool to test QueryParser, using {@link org.apache.lucene.analysis.SimpleAnalyzer}.
-   * Usage:<br>
-   * <code>java org.apache.lucene.queryParser.QueryParser &lt;input&gt;</code>
-   */
-  public static void main(String[] args) throws Exception {
-    if (args.length == 0) {
-      System.out.println("Usage: java org.apache.lucene.queryParser.QueryParser <input>");
-      System.exit(0);
-    }
-    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, "field",
-                           new org.apache.lucene.analysis.SimpleAnalyzer());
-    Query q = qp.parse(args[0]);
-    System.out.println(q.toString("field"));
-  }
-
 // *   Query  ::= ( Clause )*
 // *   Clause ::= ["+", "-"] [<TERM> ":"] ( <TERM> | "(" Query ")" )
   final public int Conjunction() throws ParseException {
@@ -1802,4 +1786,19 @@ public class QueryParser implements QueryParserConstants {
     JJCalls next;
   }
 
+  /**
+   * Command line tool to test QueryParser, using {@link org.apache.lucene.analysis.SimpleAnalyzer}.
+   * Usage:<br>
+   * <code>java org.apache.lucene.queryParser.QueryParser &lt;input&gt;</code>
+   */
+//  public static void main(String[] args) throws Exception {
+//    if (args.length == 0) {
+//      System.out.println("Usage: java org.apache.lucene.queryParser.QueryParser <input>");
+//      System.exit(0);
+//    }
+//    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, "field",
+//                           new org.apache.lucene.analysis.SimpleAnalyzer());
+//    Query q = qp.parse(args[0]);
+//    System.out.println(q.toString("field"));
+//  }
 }
diff --git a/lucene/src/java/org/apache/lucene/queryParser/QueryParser.jj b/lucene/src/java/org/apache/lucene/queryParser/QueryParser.jj
index 1784114..fa4eed3 100644
--- a/lucene/src/java/org/apache/lucene/queryParser/QueryParser.jj
+++ b/lucene/src/java/org/apache/lucene/queryParser/QueryParser.jj
@@ -1111,16 +1111,16 @@ public class QueryParser {
    * Usage:<br>
    * <code>java org.apache.lucene.queryParser.QueryParser &lt;input&gt;</code>
    */
-  public static void main(String[] args) throws Exception {
-    if (args.length == 0) {
-      System.out.println("Usage: java org.apache.lucene.queryParser.QueryParser <input>");
-      System.exit(0);
-    }
-    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, "field",
-                           new org.apache.lucene.analysis.SimpleAnalyzer());
-    Query q = qp.parse(args[0]);
-    System.out.println(q.toString("field"));
-  }
+//  public static void main(String[] args) throws Exception {
+//    if (args.length == 0) {
+//      System.out.println("Usage: java org.apache.lucene.queryParser.QueryParser <input>");
+//      System.exit(0);
+//    }
+//    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, "field",
+//                           new org.apache.lucene.analysis.SimpleAnalyzer());
+//    Query q = qp.parse(args[0]);
+//    System.out.println(q.toString("field"));
+//  }
 }
 
 PARSER_END(QueryParser)
diff --git a/lucene/src/jsp/README.txt b/lucene/src/jsp/README.txt
deleted file mode 100644
index 31ae063..0000000
--- a/lucene/src/jsp/README.txt
+++ /dev/null
@@ -1,8 +0,0 @@
-To build the Apache Lucene web app demo just run 
-"ant war-demo" from the Apache Lucene Installation
-directory (follow the master instructions in 
-BUILD.txt).  If you have questions please post 
-them to the Apache Lucene mailing lists.  To 
-actually figure this out you really need to 
-read the Lucene "Getting Started" guide provided
-with the doc build ("ant docs").
diff --git a/lucene/src/jsp/WEB-INF/web.xml b/lucene/src/jsp/WEB-INF/web.xml
deleted file mode 100755
index d6740d2..0000000
--- a/lucene/src/jsp/WEB-INF/web.xml
+++ /dev/null
@@ -1,10 +0,0 @@
-<?xml version="1.0" encoding="ISO-8859-1"?>
-
-<!DOCTYPE web-app
-    PUBLIC "-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN"
-    "http://java.sun.com/dtd/web-app_2_3.dtd">
-
-<web-app>
-
-
-</web-app>
diff --git a/lucene/src/jsp/configuration.jsp b/lucene/src/jsp/configuration.jsp
deleted file mode 100644
index eb0bcfe..0000000
--- a/lucene/src/jsp/configuration.jsp
+++ /dev/null
@@ -1,22 +0,0 @@
-<!--
-    Licensed to the Apache Software Foundation (ASF) under one or more
-    contributor license agreements.  See the NOTICE file distributed with
-    this work for additional information regarding copyright ownership.
-    The ASF licenses this file to You under the Apache License, Version 2.0
-    the "License"); you may not use this file except in compliance with
-    the License.  You may obtain a copy of the License at
- 
-        http://www.apache.org/licenses/LICENSE-2.0
- 
-    Unless required by applicable law or agreed to in writing, software
-    distributed under the License is distributed on an "AS IS" BASIS,
-    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-    See the License for the specific language governing permissions and
-    limitations under the License.
- -->
-<%
-String appTitle = "Apache Lucene Example - Intranet Server Search Application";
-/* make sure you point the below string to the index you created with IndexHTML */
-String indexLocation = "/opt/lucene/index";
-String appfooter = "Apache Lucene Template WebApp 1.0";
-%>
diff --git a/lucene/src/jsp/footer.jsp b/lucene/src/jsp/footer.jsp
deleted file mode 100644
index 44127a0..0000000
--- a/lucene/src/jsp/footer.jsp
+++ /dev/null
@@ -1,21 +0,0 @@
-<!--
-    Licensed to the Apache Software Foundation (ASF) under one or more
-    contributor license agreements.  See the NOTICE file distributed with
-    this work for additional information regarding copyright ownership.
-    The ASF licenses this file to You under the Apache License, Version 2.0
-    the "License"); you may not use this file except in compliance with
-    the License.  You may obtain a copy of the License at
- 
-        http://www.apache.org/licenses/LICENSE-2.0
- 
-    Unless required by applicable law or agreed to in writing, software
-    distributed under the License is distributed on an "AS IS" BASIS,
-    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-    See the License for the specific language governing permissions and
-    limitations under the License.
- -->
-<p align="center">
-	<%=appfooter%>
-</p>
-</body>
-</html>
diff --git a/lucene/src/jsp/header.jsp b/lucene/src/jsp/header.jsp
deleted file mode 100644
index 3806d7a..0000000
--- a/lucene/src/jsp/header.jsp
+++ /dev/null
@@ -1,26 +0,0 @@
-<!--
-    Licensed to the Apache Software Foundation (ASF) under one or more
-    contributor license agreements.  See the NOTICE file distributed with
-    this work for additional information regarding copyright ownership.
-    The ASF licenses this file to You under the Apache License, Version 2.0
-    the "License"); you may not use this file except in compliance with
-    the License.  You may obtain a copy of the License at
- 
-        http://www.apache.org/licenses/LICENSE-2.0
- 
-    Unless required by applicable law or agreed to in writing, software
-    distributed under the License is distributed on an "AS IS" BASIS,
-    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-    See the License for the specific language governing permissions and
-    limitations under the License.
- -->
-<%@include file="configuration.jsp"%>
-<html>
-<head>
-	<title><%=appTitle%></title>
-</head>
-<body>
-
-<p align="center">
-Welcome to the Lucene Template application. (This is the header)
-</p>
diff --git a/lucene/src/jsp/index.jsp b/lucene/src/jsp/index.jsp
deleted file mode 100755
index 5e63721..0000000
--- a/lucene/src/jsp/index.jsp
+++ /dev/null
@@ -1,29 +0,0 @@
-<!--
-    Licensed to the Apache Software Foundation (ASF) under one or more
-    contributor license agreements.  See the NOTICE file distributed with
-    this work for additional information regarding copyright ownership.
-    The ASF licenses this file to You under the Apache License, Version 2.0
-    the "License"); you may not use this file except in compliance with
-    the License.  You may obtain a copy of the License at
- 
-        http://www.apache.org/licenses/LICENSE-2.0
- 
-    Unless required by applicable law or agreed to in writing, software
-    distributed under the License is distributed on an "AS IS" BASIS,
-    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-    See the License for the specific language governing permissions and
-    limitations under the License.
- -->
-<%@include file="header.jsp"%>
-<center> 
-	<form name="search" action="results.jsp" method="get">
-		<p>
-			<input name="query" size="44"/>&nbsp;Search Criteria
-		</p>
-		<p>
-			<input name="maxresults" size="4" value="100"/>&nbsp;Results Per Page&nbsp;
-			<input type="submit" value="Search"/>
-		</p>
-        </form>
-</center>
-<%@include file="footer.jsp"%>
diff --git a/lucene/src/jsp/results.jsp b/lucene/src/jsp/results.jsp
deleted file mode 100755
index 90cc020..0000000
--- a/lucene/src/jsp/results.jsp
+++ /dev/null
@@ -1,179 +0,0 @@
-<!--
-    Licensed to the Apache Software Foundation (ASF) under one or more
-    contributor license agreements.  See the NOTICE file distributed with
-    this work for additional information regarding copyright ownership.
-    The ASF licenses this file to You under the Apache License, Version 2.0
-    the "License"); you may not use this file except in compliance with
-    the License.  You may obtain a copy of the License at
- 
-        http://www.apache.org/licenses/LICENSE-2.0
- 
-    Unless required by applicable law or agreed to in writing, software
-    distributed under the License is distributed on an "AS IS" BASIS,
-    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-    See the License for the specific language governing permissions and
-    limitations under the License.
- -->
-<%@ page import = "  javax.servlet.*, javax.servlet.http.*, java.io.*, org.apache.lucene.analysis.*, org.apache.lucene.analysis.standard.StandardAnalyzer, org.apache.lucene.document.*, org.apache.lucene.index.*, org.apache.lucene.store.*, org.apache.lucene.search.*, org.apache.lucene.queryParser.*, org.apache.lucene.demo.*, org.apache.lucene.demo.html.Entities, java.net.URLEncoder, org.apache.lucene.util.Version" %>
-
-<%
-/*
-
-        This jsp page is deliberatly written in the horrible java directly embedded 
-        in the page style for an easy and concise demonstration of Lucene.
-        Due note...if you write pages that look like this...sooner or later
-        you'll have a maintenance nightmare.  If you use jsps...use taglibs
-        and beans!  That being said, this should be acceptable for a small
-        page demonstrating how one uses Lucene in a web app. 
-
-        This is also deliberately overcommented. ;-)
-
-*/
-%>
-<%!
-public String escapeHTML(String s) {
-  s = s.replaceAll("&", "&amp;");
-  s = s.replaceAll("<", "&lt;");
-  s = s.replaceAll(">", "&gt;");
-  s = s.replaceAll("\"", "&quot;");
-  s = s.replaceAll("'", "&apos;");
-  return s;
-}
-%>
-<%@include file="header.jsp"%>
-<%
-        boolean error = false;                  //used to control flow for error messages
-        String indexName = indexLocation;       //local copy of the configuration variable
-        IndexSearcher searcher = null;          //the searcher used to open/search the index
-        Query query = null;                     //the Query created by the QueryParser
-        TopDocs hits = null;                       //the search results
-        int startindex = 0;                     //the first index displayed on this page
-        int maxpage    = 50;                    //the maximum items displayed on this page
-        String queryString = null;              //the query entered in the previous page
-        String startVal    = null;              //string version of startindex
-        String maxresults  = null;              //string version of maxpage
-        int thispage = 0;                       //used for the for/next either maxpage or
-                                                //hits.totalHits - startindex - whichever is
-                                                //less
-
-        try {
-          IndexReader reader = IndexReader.open(FSDirectory.open(new File(indexName)), true); // only searching, so read-only=true
-          searcher = new IndexSearcher(reader);         //create an indexSearcher for our page
-                                                        //NOTE: this operation is slow for large
-                                                        //indices (much slower than the search itself)
-                                                        //so you might want to keep an IndexSearcher 
-                                                        //open
-                                                        
-        } catch (Exception e) {                         //any error that happens is probably due
-                                                        //to a permission problem or non-existant
-                                                        //or otherwise corrupt index
-%>
-                <p>ERROR opening the Index - contact sysadmin!</p>
-                <p>Error message: <%=escapeHTML(e.getMessage())%></p>   
-<%                error = true;                                  //don't do anything up to the footer
-        }
-%>
-<%
-       if (error == false) {                                           //did we open the index?
-                queryString = request.getParameter("query");           //get the search criteria
-                startVal    = request.getParameter("startat");         //get the start index
-                maxresults  = request.getParameter("maxresults");      //get max results per page
-                try {
-                        maxpage    = Integer.parseInt(maxresults);    //parse the max results first
-                        startindex = Integer.parseInt(startVal);      //then the start index  
-                } catch (Exception e) { } //we don't care if something happens we'll just start at 0
-                                          //or end at 50
-
-                
-
-                if (queryString == null)
-                        throw new ServletException("no query "+       //if you don't have a query then
-                                                   "specified");      //you probably played on the 
-                                                                      //query string so you get the 
-                                                                      //treatment
-
-                Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_CURRENT);           //construct our usual analyzer
-                try {
-                        QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, "contents", analyzer);
-                        query = qp.parse(queryString); //parse the 
-                } catch (ParseException e) {                          //query and construct the Query
-                                                                      //object
-                                                                      //if it's just "operator error"
-                                                                      //send them a nice error HTML
-                                                                      
-%>
-                        <p>Error while parsing query: <%=escapeHTML(e.getMessage())%></p>
-<%
-                        error = true;                                 //don't bother with the rest of
-                                                                      //the page
-                }
-        }
-%>
-<%
-        if (error == false && searcher != null) {                     // if we've had no errors
-                                                                      // searcher != null was to handle
-                                                                      // a weird compilation bug 
-                thispage = maxpage;                                   // default last element to maxpage
-                hits = searcher.search(query, maxpage + startindex);  // run the query 
-                if (hits.totalHits == 0) {                             // if we got no results tell the user
-%>
-                <p> I'm sorry I couldn't find what you were looking for. </p>
-<%
-                error = true;                                        // don't bother with the rest of the
-                                                                     // page
-                }
-        }
-
-        if (error == false && searcher != null) {                   
-%>
-                <table>
-                <tr>
-                        <td>Document</td>
-                        <td>Summary</td>
-                </tr>
-<%
-                if ((startindex + maxpage) > hits.totalHits) {
-                        thispage = hits.totalHits - startindex;      // set the max index to maxpage or last
-                }                                                   // actual search result whichever is less
-
-                for (int i = startindex; i < (thispage + startindex); i++) {  // for each element
-%>
-                <tr>
-<%
-                        Document doc = searcher.doc(hits.scoreDocs[i].doc);                    //get the next document 
-                        String doctitle = doc.get("title");            //get its title
-                        String url = doc.get("path");                  //get its path field
-                        if (url != null && url.startsWith("../webapps/")) { // strip off ../webapps prefix if present
-                                url = url.substring(10);
-                        }
-                        if ((doctitle == null) || doctitle.equals("")) //use the path if it has no title
-                                doctitle = url;
-                                                                       //then output!
-%>
-                        <td><a href="<%=url%>"><%=doctitle%></a></td>
-                        <td><%=doc.get("summary")%></td>
-                </tr>
-<%
-                }
-%>
-<%                if ( (startindex + maxpage) < hits.totalHits) {   //if there are more results...display 
-                                                                   //the more link
-
-                        String moreurl="results.jsp?query=" + 
-                                       URLEncoder.encode(queryString) +  //construct the "more" link
-                                       "&amp;maxresults=" + maxpage + 
-                                       "&amp;startat=" + (startindex + maxpage);
-%>
-                <tr>
-                        <td></td><td><a href="<%=moreurl%>">More Results>></a></td>
-                </tr>
-<%
-                }
-%>
-                </table>
-
-<%       }                                            //then include our footer.
-         if (searcher != null)
-                searcher.close();
-%>
-<%@include file="footer.jsp"%>        
diff --git a/lucene/src/test/org/apache/lucene/analysis/MockTokenizer.java b/lucene/src/test/org/apache/lucene/analysis/MockTokenizer.java
index 0472b00..5396920 100644
--- a/lucene/src/test/org/apache/lucene/analysis/MockTokenizer.java
+++ b/lucene/src/test/org/apache/lucene/analysis/MockTokenizer.java
@@ -21,6 +21,7 @@ import java.io.IOException;
 import java.io.Reader;
 
 import org.apache.lucene.util.Version;
+import org.apache.lucene.util.AttributeSource.AttributeFactory;
 import org.apache.lucene.util.automaton.CharacterRunAutomaton;
 import org.apache.lucene.util.automaton.RegExp;
 
@@ -45,6 +46,13 @@ public class MockTokenizer extends CharTokenizer {
   private final boolean lowerCase;
   private int state;
 
+  public MockTokenizer(AttributeFactory factory, Reader input, CharacterRunAutomaton runAutomaton, boolean lowerCase) {
+    super(Version.LUCENE_CURRENT, factory, input);
+    this.runAutomaton = runAutomaton;
+    this.lowerCase = lowerCase;
+    this.state = runAutomaton.getInitialState();
+  }
+
   public MockTokenizer(Reader input, CharacterRunAutomaton runAutomaton, boolean lowerCase) {
     super(Version.LUCENE_CURRENT, input);
     this.runAutomaton = runAutomaton;
diff --git a/lucene/src/test/org/apache/lucene/analysis/TestAnalyzers.java b/lucene/src/test/org/apache/lucene/analysis/TestAnalyzers.java
deleted file mode 100644
index e78832c..0000000
--- a/lucene/src/test/org/apache/lucene/analysis/TestAnalyzers.java
+++ /dev/null
@@ -1,237 +0,0 @@
-package org.apache.lucene.analysis;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.StringReader;
-import java.io.Reader;
-
-import org.apache.lucene.analysis.standard.StandardTokenizer;
-import org.apache.lucene.analysis.standard.StandardAnalyzer;
-import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.index.Payload;
-
-public class TestAnalyzers extends BaseTokenStreamTestCase {
-
-   public TestAnalyzers(String name) {
-      super(name);
-   }
-
-  public void testSimple() throws Exception {
-    Analyzer a = new SimpleAnalyzer(TEST_VERSION_CURRENT);
-    assertAnalyzesTo(a, "foo bar FOO BAR", 
-                     new String[] { "foo", "bar", "foo", "bar" });
-    assertAnalyzesTo(a, "foo      bar .  FOO <> BAR", 
-                     new String[] { "foo", "bar", "foo", "bar" });
-    assertAnalyzesTo(a, "foo.bar.FOO.BAR", 
-                     new String[] { "foo", "bar", "foo", "bar" });
-    assertAnalyzesTo(a, "U.S.A.", 
-                     new String[] { "u", "s", "a" });
-    assertAnalyzesTo(a, "C++", 
-                     new String[] { "c" });
-    assertAnalyzesTo(a, "B2B", 
-                     new String[] { "b", "b" });
-    assertAnalyzesTo(a, "2B", 
-                     new String[] { "b" });
-    assertAnalyzesTo(a, "\"QUOTED\" word", 
-                     new String[] { "quoted", "word" });
-  }
-
-  public void testNull() throws Exception {
-    Analyzer a = new WhitespaceAnalyzer(TEST_VERSION_CURRENT);
-    assertAnalyzesTo(a, "foo bar FOO BAR", 
-                     new String[] { "foo", "bar", "FOO", "BAR" });
-    assertAnalyzesTo(a, "foo      bar .  FOO <> BAR", 
-                     new String[] { "foo", "bar", ".", "FOO", "<>", "BAR" });
-    assertAnalyzesTo(a, "foo.bar.FOO.BAR", 
-                     new String[] { "foo.bar.FOO.BAR" });
-    assertAnalyzesTo(a, "U.S.A.", 
-                     new String[] { "U.S.A." });
-    assertAnalyzesTo(a, "C++", 
-                     new String[] { "C++" });
-    assertAnalyzesTo(a, "B2B", 
-                     new String[] { "B2B" });
-    assertAnalyzesTo(a, "2B", 
-                     new String[] { "2B" });
-    assertAnalyzesTo(a, "\"QUOTED\" word", 
-                     new String[] { "\"QUOTED\"", "word" });
-  }
-
-  public void testStop() throws Exception {
-    Analyzer a = new StopAnalyzer(TEST_VERSION_CURRENT);
-    assertAnalyzesTo(a, "foo bar FOO BAR", 
-                     new String[] { "foo", "bar", "foo", "bar" });
-    assertAnalyzesTo(a, "foo a bar such FOO THESE BAR", 
-                     new String[] { "foo", "bar", "foo", "bar" });
-  }
-
-  void verifyPayload(TokenStream ts) throws IOException {
-    PayloadAttribute payloadAtt = ts.getAttribute(PayloadAttribute.class);
-    for(byte b=1;;b++) {
-      boolean hasNext = ts.incrementToken();
-      if (!hasNext) break;
-      // System.out.println("id="+System.identityHashCode(nextToken) + " " + t);
-      // System.out.println("payload=" + (int)nextToken.getPayload().toByteArray()[0]);
-      assertEquals(b, payloadAtt.getPayload().toByteArray()[0]);
-    }
-  }
-
-  // Make sure old style next() calls result in a new copy of payloads
-  public void testPayloadCopy() throws IOException {
-    String s = "how now brown cow";
-    TokenStream ts;
-    ts = new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(s));
-    ts = new PayloadSetter(ts);
-    verifyPayload(ts);
-
-    ts = new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(s));
-    ts = new PayloadSetter(ts);
-    verifyPayload(ts);
-  }
-
-  // LUCENE-1150: Just a compile time test, to ensure the
-  // StandardAnalyzer constants remain publicly accessible
-  @SuppressWarnings("unused")
-  public void _testStandardConstants() {
-    int x = StandardTokenizer.ALPHANUM;
-    x = StandardTokenizer.APOSTROPHE;
-    x = StandardTokenizer.ACRONYM;
-    x = StandardTokenizer.COMPANY;
-    x = StandardTokenizer.EMAIL;
-    x = StandardTokenizer.HOST;
-    x = StandardTokenizer.NUM;
-    x = StandardTokenizer.CJ;
-    String[] y = StandardTokenizer.TOKEN_TYPES;
-  }
-
-  private static class LowerCaseWhitespaceAnalyzer extends Analyzer {
-
-    @Override
-    public TokenStream tokenStream(String fieldName, Reader reader) {
-      return new LowerCaseFilter(TEST_VERSION_CURRENT,
-          new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader));
-    }
-    
-  }
-  
-  /**
-   * @deprecated remove this when lucene 3.0 "broken unicode 4" support
-   * is no longer needed.
-   */
-  @Deprecated
-  private static class LowerCaseWhitespaceAnalyzerBWComp extends Analyzer {
-
-    @Override
-    public TokenStream tokenStream(String fieldName, Reader reader) {
-      return new LowerCaseFilter(new WhitespaceTokenizer(reader));
-    }
-    
-  }
-  
-  /**
-   * Test that LowercaseFilter handles entire unicode range correctly
-   */
-  public void testLowerCaseFilter() throws IOException {
-    Analyzer a = new LowerCaseWhitespaceAnalyzer();
-    // BMP
-    assertAnalyzesTo(a, "AbaCaDabA", new String[] { "abacadaba" });
-    // supplementary
-    assertAnalyzesTo(a, "\ud801\udc16\ud801\udc16\ud801\udc16\ud801\udc16",
-        new String[] {"\ud801\udc3e\ud801\udc3e\ud801\udc3e\ud801\udc3e"});
-    assertAnalyzesTo(a, "AbaCa\ud801\udc16DabA", 
-        new String[] { "abaca\ud801\udc3edaba" });
-    // unpaired lead surrogate
-    assertAnalyzesTo(a, "AbaC\uD801AdaBa", 
-        new String [] { "abac\uD801adaba" });
-    // unpaired trail surrogate
-    assertAnalyzesTo(a, "AbaC\uDC16AdaBa", 
-        new String [] { "abac\uDC16adaba" });
-  }
-  
-  /**
-   * Test that LowercaseFilter handles the lowercasing correctly if the term
-   * buffer has a trailing surrogate character leftover and the current term in
-   * the buffer ends with a corresponding leading surrogate.
-   */
-  public void testLowerCaseFilterLowSurrogateLeftover() throws IOException {
-    // test if the limit of the termbuffer is correctly used with supplementary
-    // chars
-    WhitespaceTokenizer tokenizer = new WhitespaceTokenizer(TEST_VERSION_CURRENT, 
-        new StringReader("BogustermBogusterm\udc16"));
-    LowerCaseFilter filter = new LowerCaseFilter(TEST_VERSION_CURRENT,
-        tokenizer);
-    assertTokenStreamContents(filter, new String[] {"bogustermbogusterm\udc16"});
-    filter.reset();
-    String highSurEndingUpper = "BogustermBoguster\ud801";
-    String highSurEndingLower = "bogustermboguster\ud801";
-    tokenizer.reset(new StringReader(highSurEndingUpper));
-    assertTokenStreamContents(filter, new String[] {highSurEndingLower});
-    assertTrue(filter.hasAttribute(CharTermAttribute.class));
-    char[] termBuffer = filter.getAttribute(CharTermAttribute.class).buffer();
-    int length = highSurEndingLower.length();
-    assertEquals('\ud801', termBuffer[length - 1]);
-    assertEquals('\udc3e', termBuffer[length]);
-    
-  }
-  
-  /**
-   * Test that LowercaseFilter only works on BMP for back compat,
-   * depending upon version
-   * @deprecated remove this test when lucene 3.0 "broken unicode 4" support
-   * is no longer needed.
-   */
-  @Deprecated
-  public void testLowerCaseFilterBWComp() throws IOException {
-    Analyzer a = new LowerCaseWhitespaceAnalyzerBWComp();
-    // BMP
-    assertAnalyzesTo(a, "AbaCaDabA", new String[] { "abacadaba" });
-    // supplementary, no-op
-    assertAnalyzesTo(a, "\ud801\udc16\ud801\udc16\ud801\udc16\ud801\udc16",
-        new String[] {"\ud801\udc16\ud801\udc16\ud801\udc16\ud801\udc16"});
-    assertAnalyzesTo(a, "AbaCa\ud801\udc16DabA",
-        new String[] { "abaca\ud801\udc16daba" });
-    // unpaired lead surrogate
-    assertAnalyzesTo(a, "AbaC\uD801AdaBa", 
-        new String [] { "abac\uD801adaba" });
-    // unpaired trail surrogate
-    assertAnalyzesTo(a, "AbaC\uDC16AdaBa", 
-        new String [] { "abac\uDC16adaba" });
-  }
-  
-}
-
-final class PayloadSetter extends TokenFilter {
-  PayloadAttribute payloadAtt;
-  public  PayloadSetter(TokenStream input) {
-    super(input);
-    payloadAtt = addAttribute(PayloadAttribute.class);
-  }
-
-  byte[] data = new byte[1];
-  Payload p = new Payload(data,0,1);
-
-  @Override
-  public boolean incrementToken() throws IOException {
-    boolean hasNext = input.incrementToken();
-    if (!hasNext) return false;
-    payloadAtt.setPayload(p);  // reuse the payload / byte[]
-    data[0]++;
-    return true;
-  }
-}
\ No newline at end of file
diff --git a/lucene/src/test/org/apache/lucene/analysis/TestCharTokenizers.java b/lucene/src/test/org/apache/lucene/analysis/TestCharTokenizers.java
index 26f1737..77c2883 100644
--- a/lucene/src/test/org/apache/lucene/analysis/TestCharTokenizers.java
+++ b/lucene/src/test/org/apache/lucene/analysis/TestCharTokenizers.java
@@ -46,8 +46,7 @@ public class TestCharTokenizers extends BaseTokenStreamTestCase {
     }
     // internal buffer size is 1024 make sure we have a surrogate pair right at the border
     builder.insert(1023, "\ud801\udc1c");
-    LowerCaseTokenizer tokenizer = new LowerCaseTokenizer(
-        TEST_VERSION_CURRENT, new StringReader(builder.toString()));
+    MockTokenizer tokenizer = new MockTokenizer(new StringReader(builder.toString()), MockTokenizer.SIMPLE, true);
     assertTokenStreamContents(tokenizer, builder.toString().toLowerCase().split(" "));
   }
   
@@ -64,8 +63,7 @@ public class TestCharTokenizers extends BaseTokenStreamTestCase {
         builder.append("a");
       }
       builder.append("\ud801\udc1cabc");
-      LowerCaseTokenizer tokenizer = new LowerCaseTokenizer(
-          TEST_VERSION_CURRENT, new StringReader(builder.toString()));
+      MockTokenizer tokenizer = new MockTokenizer(new StringReader(builder.toString()), MockTokenizer.SIMPLE, true);
       assertTokenStreamContents(tokenizer, new String[] {builder.toString().toLowerCase()});
     }
   }
@@ -79,8 +77,7 @@ public class TestCharTokenizers extends BaseTokenStreamTestCase {
     for (int i = 0; i < 255; i++) {
       builder.append("A");
     }
-    LowerCaseTokenizer tokenizer = new LowerCaseTokenizer(
-        TEST_VERSION_CURRENT, new StringReader(builder.toString() + builder.toString()));
+    MockTokenizer tokenizer = new MockTokenizer(new StringReader(builder.toString() + builder.toString()), MockTokenizer.SIMPLE, true);
     assertTokenStreamContents(tokenizer, new String[] {builder.toString().toLowerCase(), builder.toString().toLowerCase()});
   }
   
@@ -94,42 +91,10 @@ public class TestCharTokenizers extends BaseTokenStreamTestCase {
       builder.append("A");
     }
     builder.append("\ud801\udc1c");
-    LowerCaseTokenizer tokenizer = new LowerCaseTokenizer(
-        TEST_VERSION_CURRENT, new StringReader(builder.toString() + builder.toString()));
+    MockTokenizer tokenizer = new MockTokenizer(new StringReader(builder.toString() + builder.toString()), MockTokenizer.SIMPLE, true);
     assertTokenStreamContents(tokenizer, new String[] {builder.toString().toLowerCase(), builder.toString().toLowerCase()});
   }
 
-  public void testLowerCaseTokenizer() throws IOException {
-    StringReader reader = new StringReader("Tokenizer \ud801\udc1ctest");
-    LowerCaseTokenizer tokenizer = new LowerCaseTokenizer(TEST_VERSION_CURRENT,
-        reader);
-    assertTokenStreamContents(tokenizer, new String[] { "tokenizer",
-        "\ud801\udc44test" });
-  }
-
-  public void testLowerCaseTokenizerBWCompat() throws IOException {
-    StringReader reader = new StringReader("Tokenizer \ud801\udc1ctest");
-    LowerCaseTokenizer tokenizer = new LowerCaseTokenizer(Version.LUCENE_30,
-        reader);
-    assertTokenStreamContents(tokenizer, new String[] { "tokenizer", "test" });
-  }
-
-  public void testWhitespaceTokenizer() throws IOException {
-    StringReader reader = new StringReader("Tokenizer \ud801\udc1ctest");
-    WhitespaceTokenizer tokenizer = new WhitespaceTokenizer(TEST_VERSION_CURRENT,
-        reader);
-    assertTokenStreamContents(tokenizer, new String[] { "Tokenizer",
-        "\ud801\udc1ctest" });
-  }
-
-  public void testWhitespaceTokenizerBWCompat() throws IOException {
-    StringReader reader = new StringReader("Tokenizer \ud801\udc1ctest");
-    WhitespaceTokenizer tokenizer = new WhitespaceTokenizer(Version.LUCENE_30,
-        reader);
-    assertTokenStreamContents(tokenizer, new String[] { "Tokenizer",
-        "\ud801\udc1ctest" });
-  }
-
   public void testIsTokenCharCharInSubclass() {
     new TestingCharTokenizer(Version.LUCENE_30, new StringReader(""));
     try {
diff --git a/lucene/src/test/org/apache/lucene/analysis/TestKeywordAnalyzer.java b/lucene/src/test/org/apache/lucene/analysis/TestKeywordAnalyzer.java
deleted file mode 100644
index 712e917..0000000
--- a/lucene/src/test/org/apache/lucene/analysis/TestKeywordAnalyzer.java
+++ /dev/null
@@ -1,100 +0,0 @@
-package org.apache.lucene.analysis;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.StringReader;
-
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.TermDocs;
-import org.apache.lucene.queryParser.QueryParser;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.ScoreDoc;
-import org.apache.lucene.store.RAMDirectory;
-
-public class TestKeywordAnalyzer extends BaseTokenStreamTestCase {
-  
-  private RAMDirectory directory;
-  private IndexSearcher searcher;
-
-  @Override
-  protected void setUp() throws Exception {
-    super.setUp();
-    directory = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(
-        TEST_VERSION_CURRENT, new SimpleAnalyzer(
-        TEST_VERSION_CURRENT)));
-
-    Document doc = new Document();
-    doc.add(new Field("partnum", "Q36", Field.Store.YES, Field.Index.NOT_ANALYZED));
-    doc.add(new Field("description", "Illidium Space Modulator", Field.Store.YES, Field.Index.ANALYZED));
-    writer.addDocument(doc);
-
-    writer.close();
-
-    searcher = new IndexSearcher(directory, true);
-  }
-
-  /*
-  public void testPerFieldAnalyzer() throws Exception {
-    PerFieldAnalyzerWrapper analyzer = new PerFieldAnalyzerWrapper(new SimpleAnalyzer(TEST_VERSION_CURRENT));
-    analyzer.addAnalyzer("partnum", new KeywordAnalyzer());
-
-    QueryParser queryParser = new QueryParser(TEST_VERSION_CURRENT, "description", analyzer);
-    Query query = queryParser.parse("partnum:Q36 AND SPACE");
-
-    ScoreDoc[] hits = searcher.search(query, null, 1000).scoreDocs;
-    assertEquals("Q36 kept as-is",
-              "+partnum:Q36 +space", query.toString("description"));
-    assertEquals("doc found!", 1, hits.length);
-  }
-  */
-
-  public void testMutipleDocument() throws Exception {
-    RAMDirectory dir = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new KeywordAnalyzer()));
-    Document doc = new Document();
-    doc.add(new Field("partnum", "Q36", Field.Store.YES, Field.Index.ANALYZED));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new Field("partnum", "Q37", Field.Store.YES, Field.Index.ANALYZED));
-    writer.addDocument(doc);
-    writer.close();
-
-    IndexReader reader = IndexReader.open(dir, true);
-    TermDocs td = reader.termDocs(new Term("partnum", "Q36"));
-    assertTrue(td.next());
-    td = reader.termDocs(new Term("partnum", "Q37"));
-    assertTrue(td.next());
-  }
-
-  // LUCENE-1441
-  public void testOffsets() throws Exception {
-    TokenStream stream = new KeywordAnalyzer().tokenStream("field", new StringReader("abcd"));
-    OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);
-    assertTrue(stream.incrementToken());
-    assertEquals(0, offsetAtt.startOffset());
-    assertEquals(4, offsetAtt.endOffset());
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/analysis/TestStandardAnalyzer.java b/lucene/src/test/org/apache/lucene/analysis/TestStandardAnalyzer.java
deleted file mode 100644
index 40c8239..0000000
--- a/lucene/src/test/org/apache/lucene/analysis/TestStandardAnalyzer.java
+++ /dev/null
@@ -1,235 +0,0 @@
-package org.apache.lucene.analysis;
-
-import org.apache.lucene.analysis.standard.StandardAnalyzer;
-
-import org.apache.lucene.util.Version;
-
-
-/**
- * Copyright 2004 The Apache Software Foundation
- * <p/>
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class TestStandardAnalyzer extends BaseTokenStreamTestCase {
-
-  private Analyzer a = new StandardAnalyzer(TEST_VERSION_CURRENT);
-
-  public void testMaxTermLength() throws Exception {
-    StandardAnalyzer sa = new StandardAnalyzer(TEST_VERSION_CURRENT);
-    sa.setMaxTokenLength(5);
-    assertAnalyzesTo(sa, "ab cd toolong xy z", new String[]{"ab", "cd", "xy", "z"});
-  }
-
-  public void testMaxTermLength2() throws Exception {
-    StandardAnalyzer sa = new StandardAnalyzer(TEST_VERSION_CURRENT);
-    assertAnalyzesTo(sa, "ab cd toolong xy z", new String[]{"ab", "cd", "toolong", "xy", "z"});
-    sa.setMaxTokenLength(5);
-    
-    assertAnalyzesTo(sa, "ab cd toolong xy z", new String[]{"ab", "cd", "xy", "z"}, new int[]{1, 1, 2, 1});
-  }
-
-  public void testMaxTermLength3() throws Exception {
-    char[] chars = new char[255];
-    for(int i=0;i<255;i++)
-      chars[i] = 'a';
-    String longTerm = new String(chars, 0, 255);
-    
-    assertAnalyzesTo(a, "ab cd " + longTerm + " xy z", new String[]{"ab", "cd", longTerm, "xy", "z"});
-    assertAnalyzesTo(a, "ab cd " + longTerm + "a xy z", new String[]{"ab", "cd", "xy", "z"});
-  }
-
-  public void testAlphanumeric() throws Exception {
-    // alphanumeric tokens
-    assertAnalyzesTo(a, "B2B", new String[]{"b2b"});
-    assertAnalyzesTo(a, "2B", new String[]{"2b"});
-  }
-
-  public void testUnderscores() throws Exception {
-    // underscores are delimiters, but not in email addresses (below)
-    assertAnalyzesTo(a, "word_having_underscore", new String[]{"word", "having", "underscore"});
-    assertAnalyzesTo(a, "word_with_underscore_and_stopwords", new String[]{"word", "underscore", "stopwords"});
-  }
-
-  public void testDelimiters() throws Exception {
-    // other delimiters: "-", "/", ","
-    assertAnalyzesTo(a, "some-dashed-phrase", new String[]{"some", "dashed", "phrase"});
-    assertAnalyzesTo(a, "dogs,chase,cats", new String[]{"dogs", "chase", "cats"});
-    assertAnalyzesTo(a, "ac/dc", new String[]{"ac", "dc"});
-  }
-
-  public void testApostrophes() throws Exception {
-    // internal apostrophes: O'Reilly, you're, O'Reilly's
-    // possessives are actually removed by StardardFilter, not the tokenizer
-    assertAnalyzesTo(a, "O'Reilly", new String[]{"o'reilly"});
-    assertAnalyzesTo(a, "you're", new String[]{"you're"});
-    assertAnalyzesTo(a, "she's", new String[]{"she"});
-    assertAnalyzesTo(a, "Jim's", new String[]{"jim"});
-    assertAnalyzesTo(a, "don't", new String[]{"don't"});
-    assertAnalyzesTo(a, "O'Reilly's", new String[]{"o'reilly"});
-  }
-
-  public void testTSADash() throws Exception {
-    // t and s had been stopwords in Lucene <= 2.0, which made it impossible
-    // to correctly search for these terms:
-    assertAnalyzesTo(a, "s-class", new String[]{"s", "class"});
-    assertAnalyzesTo(a, "t-com", new String[]{"t", "com"});
-    // 'a' is still a stopword:
-    assertAnalyzesTo(a, "a-class", new String[]{"class"});
-  }
-
-  public void testCompanyNames() throws Exception {
-    // company names
-    assertAnalyzesTo(a, "AT&T", new String[]{"at&t"});
-    assertAnalyzesTo(a, "Excite@Home", new String[]{"excite@home"});
-  }
-
-  public void testLucene1140() throws Exception {
-    try {
-      StandardAnalyzer analyzer = new StandardAnalyzer(TEST_VERSION_CURRENT);
-      assertAnalyzesTo(analyzer, "www.nutch.org.", new String[]{ "www.nutch.org" }, new String[] { "<HOST>" });
-    } catch (NullPointerException e) {
-      fail("Should not throw an NPE and it did");
-    }
-
-  }
-
-  public void testDomainNames() throws Exception {
-    // Current lucene should not show the bug
-    StandardAnalyzer a2 = new StandardAnalyzer(TEST_VERSION_CURRENT);
-
-    // domain names
-    assertAnalyzesTo(a2, "www.nutch.org", new String[]{"www.nutch.org"});
-    //Notice the trailing .  See https://issues.apache.org/jira/browse/LUCENE-1068.
-    // the following should be recognized as HOST:
-    assertAnalyzesTo(a2, "www.nutch.org.", new String[]{ "www.nutch.org" }, new String[] { "<HOST>" });
-
-    // 2.3 should show the bug
-    a2 = new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_23);
-    assertAnalyzesTo(a2, "www.nutch.org.", new String[]{ "wwwnutchorg" }, new String[] { "<ACRONYM>" });
-
-    // 2.4 should not show the bug
-    a2 = new StandardAnalyzer(Version.LUCENE_24);
-    assertAnalyzesTo(a2, "www.nutch.org.", new String[]{ "www.nutch.org" }, new String[] { "<HOST>" });
-  }
-
-  public void testEMailAddresses() throws Exception {
-    // email addresses, possibly with underscores, periods, etc
-    assertAnalyzesTo(a, "test@example.com", new String[]{"test@example.com"});
-    assertAnalyzesTo(a, "first.lastname@example.com", new String[]{"first.lastname@example.com"});
-    assertAnalyzesTo(a, "first_lastname@example.com", new String[]{"first_lastname@example.com"});
-  }
-
-  public void testNumeric() throws Exception {
-    // floating point, serial, model numbers, ip addresses, etc.
-    // every other segment must have at least one digit
-    assertAnalyzesTo(a, "21.35", new String[]{"21.35"});
-    assertAnalyzesTo(a, "R2D2 C3PO", new String[]{"r2d2", "c3po"});
-    assertAnalyzesTo(a, "216.239.63.104", new String[]{"216.239.63.104"});
-    assertAnalyzesTo(a, "1-2-3", new String[]{"1-2-3"});
-    assertAnalyzesTo(a, "a1-b2-c3", new String[]{"a1-b2-c3"});
-    assertAnalyzesTo(a, "a1-b-c3", new String[]{"a1-b-c3"});
-  }
-
-  public void testTextWithNumbers() throws Exception {
-    // numbers
-    assertAnalyzesTo(a, "David has 5000 bones", new String[]{"david", "has", "5000", "bones"});
-  }
-
-  public void testVariousText() throws Exception {
-    // various
-    assertAnalyzesTo(a, "C embedded developers wanted", new String[]{"c", "embedded", "developers", "wanted"});
-    assertAnalyzesTo(a, "foo bar FOO BAR", new String[]{"foo", "bar", "foo", "bar"});
-    assertAnalyzesTo(a, "foo      bar .  FOO <> BAR", new String[]{"foo", "bar", "foo", "bar"});
-    assertAnalyzesTo(a, "\"QUOTED\" word", new String[]{"quoted", "word"});
-  }
-
-  public void testAcronyms() throws Exception {
-    // acronyms have their dots stripped
-    assertAnalyzesTo(a, "U.S.A.", new String[]{"usa"});
-  }
-
-  public void testCPlusPlusHash() throws Exception {
-    // It would be nice to change the grammar in StandardTokenizer.jj to make "C#" and "C++" end up as tokens.
-    assertAnalyzesTo(a, "C++", new String[]{"c"});
-    assertAnalyzesTo(a, "C#", new String[]{"c"});
-  }
-
-  public void testKorean() throws Exception {
-    // Korean words
-    assertAnalyzesTo(a, "???????? ????????", new String[]{"????????", "????????"});
-  }
-
-  // Compliance with the "old" JavaCC-based analyzer, see:
-  // https://issues.apache.org/jira/browse/LUCENE-966#action_12516752
-
-  public void testComplianceFileName() throws Exception {
-    assertAnalyzesTo(a, "2004.jpg",
-            new String[]{"2004.jpg"},
-            new String[]{"<HOST>"});
-  }
-
-  public void testComplianceNumericIncorrect() throws Exception {
-    assertAnalyzesTo(a, "62.46",
-            new String[]{"62.46"},
-            new String[]{"<HOST>"});
-  }
-
-  public void testComplianceNumericLong() throws Exception {
-    assertAnalyzesTo(a, "978-0-94045043-1",
-            new String[]{"978-0-94045043-1"},
-            new String[]{"<NUM>"});
-  }
-
-  public void testComplianceNumericFile() throws Exception {
-    assertAnalyzesTo(
-            a,
-            "78academyawards/rules/rule02.html",
-            new String[]{"78academyawards/rules/rule02.html"},
-            new String[]{"<NUM>"});
-  }
-
-  public void testComplianceNumericWithUnderscores() throws Exception {
-    assertAnalyzesTo(
-            a,
-            "2006-03-11t082958z_01_ban130523_rtridst_0_ozabs",
-            new String[]{"2006-03-11t082958z_01_ban130523_rtridst_0_ozabs"},
-            new String[]{"<NUM>"});
-  }
-
-  public void testComplianceNumericWithDash() throws Exception {
-    assertAnalyzesTo(a, "mid-20th", new String[]{"mid-20th"},
-            new String[]{"<NUM>"});
-  }
-
-  public void testComplianceManyTokens() throws Exception {
-    assertAnalyzesTo(
-            a,
-            "/money.cnn.com/magazines/fortune/fortune_archive/2007/03/19/8402357/index.htm "
-                    + "safari-0-sheikh-zayed-grand-mosque.jpg",
-            new String[]{"money.cnn.com", "magazines", "fortune",
-                    "fortune", "archive/2007/03/19/8402357", "index.htm",
-                    "safari-0-sheikh", "zayed", "grand", "mosque.jpg"},
-            new String[]{"<HOST>", "<ALPHANUM>", "<ALPHANUM>",
-                    "<ALPHANUM>", "<NUM>", "<HOST>", "<NUM>", "<ALPHANUM>",
-                    "<ALPHANUM>", "<HOST>"});
-  }
-
-  public void testJava14BWCompatibility() throws Exception {
-    StandardAnalyzer sa = new StandardAnalyzer(Version.LUCENE_30);
-    assertAnalyzesTo(sa, "test\u02C6test", new String[] { "test", "test" });
-    sa = new StandardAnalyzer(Version.LUCENE_31);
-    assertAnalyzesTo(sa, "test\u02C6test", new String[] { "test\u02C6test" });
-  }
-
-}
diff --git a/lucene/src/test/org/apache/lucene/analysis/TestStopAnalyzer.java b/lucene/src/test/org/apache/lucene/analysis/TestStopAnalyzer.java
deleted file mode 100644
index 4eb35df..0000000
--- a/lucene/src/test/org/apache/lucene/analysis/TestStopAnalyzer.java
+++ /dev/null
@@ -1,101 +0,0 @@
-package org.apache.lucene.analysis;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.util.Version;
-
-import java.io.StringReader;
-import java.io.IOException;
-import java.util.Iterator;
-import java.util.Set;
-import java.util.HashSet;
-
-public class TestStopAnalyzer extends BaseTokenStreamTestCase {
-  
-  private StopAnalyzer stop = new StopAnalyzer(TEST_VERSION_CURRENT);
-  private Set<Object> inValidTokens = new HashSet<Object>();
-  
-  public TestStopAnalyzer(String s) {
-    super(s);
-  }
-
-  @Override
-  protected void setUp() throws Exception {
-    super.setUp();
-    
-    Iterator<?> it = StopAnalyzer.ENGLISH_STOP_WORDS_SET.iterator();
-    while(it.hasNext()) {
-      inValidTokens.add(it.next());
-    }
-  }
-
-  public void testDefaults() throws IOException {
-    assertTrue(stop != null);
-    StringReader reader = new StringReader("This is a test of the english stop analyzer");
-    TokenStream stream = stop.tokenStream("test", reader);
-    assertTrue(stream != null);
-    CharTermAttribute termAtt = stream.getAttribute(CharTermAttribute.class);
-    
-    while (stream.incrementToken()) {
-      assertFalse(inValidTokens.contains(termAtt.toString()));
-    }
-  }
-
-  public void testStopList() throws IOException {
-    Set<Object> stopWordsSet = new HashSet<Object>();
-    stopWordsSet.add("good");
-    stopWordsSet.add("test");
-    stopWordsSet.add("analyzer");
-    StopAnalyzer newStop = new StopAnalyzer(Version.LUCENE_24, stopWordsSet);
-    StringReader reader = new StringReader("This is a good test of the english stop analyzer");
-    TokenStream stream = newStop.tokenStream("test", reader);
-    assertNotNull(stream);
-    CharTermAttribute termAtt = stream.getAttribute(CharTermAttribute.class);
-    PositionIncrementAttribute posIncrAtt = stream.addAttribute(PositionIncrementAttribute.class);
-    
-    while (stream.incrementToken()) {
-      String text = termAtt.toString();
-      assertFalse(stopWordsSet.contains(text));
-      assertEquals(1,posIncrAtt.getPositionIncrement()); // in 2.4 stop tokenizer does not apply increments.
-    }
-  }
-
-  public void testStopListPositions() throws IOException {
-    Set<Object> stopWordsSet = new HashSet<Object>();
-    stopWordsSet.add("good");
-    stopWordsSet.add("test");
-    stopWordsSet.add("analyzer");
-    StopAnalyzer newStop = new StopAnalyzer(TEST_VERSION_CURRENT, stopWordsSet);
-    StringReader reader = new StringReader("This is a good test of the english stop analyzer with positions");
-    int expectedIncr[] =                  { 1,   1, 1,          3, 1,  1,      1,            2,   1};
-    TokenStream stream = newStop.tokenStream("test", reader);
-    assertNotNull(stream);
-    int i = 0;
-    CharTermAttribute termAtt = stream.getAttribute(CharTermAttribute.class);
-    PositionIncrementAttribute posIncrAtt = stream.addAttribute(PositionIncrementAttribute.class);
-
-    while (stream.incrementToken()) {
-      String text = termAtt.toString();
-      assertFalse(stopWordsSet.contains(text));
-      assertEquals(expectedIncr[i++],posIncrAtt.getPositionIncrement());
-    }
-  }
-
-}
diff --git a/lucene/src/test/org/apache/lucene/analysis/TestStopFilter.java b/lucene/src/test/org/apache/lucene/analysis/TestStopFilter.java
deleted file mode 100644
index ec989a5..0000000
--- a/lucene/src/test/org/apache/lucene/analysis/TestStopFilter.java
+++ /dev/null
@@ -1,138 +0,0 @@
-package org.apache.lucene.analysis;
-
-/**
- * Copyright 2005 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.util.English;
-import org.apache.lucene.util.Version;
-
-import java.io.IOException;
-import java.io.StringReader;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Set;
-import java.util.HashSet;
-
-
-public class TestStopFilter extends BaseTokenStreamTestCase {
-  
-  // other StopFilter functionality is already tested by TestStopAnalyzer
-
-  public void testExactCase() throws IOException {
-    StringReader reader = new StringReader("Now is The Time");
-    Set<String> stopWords = new HashSet<String>(Arrays.asList("is", "the", "Time"));
-    TokenStream stream = new StopFilter(TEST_VERSION_CURRENT, new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader), stopWords, false);
-    final CharTermAttribute termAtt = stream.getAttribute(CharTermAttribute.class);
-    assertTrue(stream.incrementToken());
-    assertEquals("Now", termAtt.toString());
-    assertTrue(stream.incrementToken());
-    assertEquals("The", termAtt.toString());
-    assertFalse(stream.incrementToken());
-  }
-
-  public void testIgnoreCase() throws IOException {
-    StringReader reader = new StringReader("Now is The Time");
-    Set<Object> stopWords = new HashSet<Object>(Arrays.asList( "is", "the", "Time" ));
-    TokenStream stream = new StopFilter(TEST_VERSION_CURRENT, new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader), stopWords, true);
-    final CharTermAttribute termAtt = stream.getAttribute(CharTermAttribute.class);
-    assertTrue(stream.incrementToken());
-    assertEquals("Now", termAtt.toString());
-    assertFalse(stream.incrementToken());
-  }
-
-  public void testStopFilt() throws IOException {
-    StringReader reader = new StringReader("Now is The Time");
-    String[] stopWords = new String[] { "is", "the", "Time" };
-    Set<Object> stopSet = StopFilter.makeStopSet(TEST_VERSION_CURRENT, stopWords);
-    TokenStream stream = new StopFilter(TEST_VERSION_CURRENT, new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader), stopSet);
-    final CharTermAttribute termAtt = stream.getAttribute(CharTermAttribute.class);
-    assertTrue(stream.incrementToken());
-    assertEquals("Now", termAtt.toString());
-    assertTrue(stream.incrementToken());
-    assertEquals("The", termAtt.toString());
-    assertFalse(stream.incrementToken());
-  }
-
-  /**
-   * Test Position increments applied by StopFilter with and without enabling this option.
-   */
-  public void testStopPositons() throws IOException {
-    StringBuilder sb = new StringBuilder();
-    ArrayList<String> a = new ArrayList<String>();
-    for (int i=0; i<20; i++) {
-      String w = English.intToEnglish(i).trim();
-      sb.append(w).append(" ");
-      if (i%3 != 0) a.add(w);
-    }
-    log(sb.toString());
-    String stopWords[] = a.toArray(new String[0]);
-    for (int i=0; i<a.size(); i++) log("Stop: "+stopWords[i]);
-    Set<Object> stopSet = StopFilter.makeStopSet(TEST_VERSION_CURRENT, stopWords);
-    // with increments
-    StringReader reader = new StringReader(sb.toString());
-    StopFilter stpf = new StopFilter(Version.LUCENE_24, new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader), stopSet);
-    doTestStopPositons(stpf,true);
-    // without increments
-    reader = new StringReader(sb.toString());
-    stpf = new StopFilter(TEST_VERSION_CURRENT, new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader), stopSet);
-    doTestStopPositons(stpf,false);
-    // with increments, concatenating two stop filters
-    ArrayList<String> a0 = new ArrayList<String>();
-    ArrayList<String> a1 = new ArrayList<String>();
-    for (int i=0; i<a.size(); i++) {
-      if (i%2==0) { 
-        a0.add(a.get(i));
-      } else {
-        a1.add(a.get(i));
-      }
-    }
-    String stopWords0[] =  a0.toArray(new String[0]);
-    for (int i=0; i<a0.size(); i++) log("Stop0: "+stopWords0[i]);
-    String stopWords1[] =  a1.toArray(new String[0]);
-    for (int i=0; i<a1.size(); i++) log("Stop1: "+stopWords1[i]);
-    Set<Object> stopSet0 = StopFilter.makeStopSet(TEST_VERSION_CURRENT, stopWords0);
-    Set<Object> stopSet1 = StopFilter.makeStopSet(TEST_VERSION_CURRENT, stopWords1);
-    reader = new StringReader(sb.toString());
-    StopFilter stpf0 = new StopFilter(TEST_VERSION_CURRENT, new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader), stopSet0); // first part of the set
-    stpf0.setEnablePositionIncrements(true);
-    StopFilter stpf01 = new StopFilter(TEST_VERSION_CURRENT, stpf0, stopSet1); // two stop filters concatenated!
-    doTestStopPositons(stpf01,true);
-  }
-  
-  private void doTestStopPositons(StopFilter stpf, boolean enableIcrements) throws IOException {
-    log("---> test with enable-increments-"+(enableIcrements?"enabled":"disabled"));
-    stpf.setEnablePositionIncrements(enableIcrements);
-    CharTermAttribute termAtt = stpf.getAttribute(CharTermAttribute.class);
-    PositionIncrementAttribute posIncrAtt = stpf.getAttribute(PositionIncrementAttribute.class);
-    for (int i=0; i<20; i+=3) {
-      assertTrue(stpf.incrementToken());
-      log("Token "+i+": "+stpf);
-      String w = English.intToEnglish(i).trim();
-      assertEquals("expecting token "+i+" to be "+w,w,termAtt.toString());
-      assertEquals("all but first token must have position increment of 3",enableIcrements?(i==0?1:3):1,posIncrAtt.getPositionIncrement());
-    }
-    assertFalse(stpf.incrementToken());
-  }
-  
-  // print debug info depending on VERBOSE
-  private static void log(String s) {
-    if (VERBOSE) {
-      System.out.println(s);
-    }
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/analysis/TestToken.java b/lucene/src/test/org/apache/lucene/analysis/TestToken.java
index be5f611..552259d 100644
--- a/lucene/src/test/org/apache/lucene/analysis/TestToken.java
+++ b/lucene/src/test/org/apache/lucene/analysis/TestToken.java
@@ -239,7 +239,7 @@ public class TestToken extends LuceneTestCase {
   }
 
   public void testTokenAttributeFactory() throws Exception {
-    TokenStream ts = new WhitespaceTokenizer(Token.TOKEN_ATTRIBUTE_FACTORY, new StringReader("foo bar"));
+    TokenStream ts = new MockTokenizer(Token.TOKEN_ATTRIBUTE_FACTORY, new StringReader("foo bar"), MockTokenizer.WHITESPACE, false);
     
     assertTrue("TypeAttribute is not implemented by SenselessAttributeImpl",
       ts.addAttribute(SenselessAttribute.class) instanceof SenselessAttributeImpl);
diff --git a/lucene/src/test/org/apache/lucene/collation/CollationTestBase.java b/lucene/src/test/org/apache/lucene/collation/CollationTestBase.java
deleted file mode 100644
index 342b509..0000000
--- a/lucene/src/test/org/apache/lucene/collation/CollationTestBase.java
+++ /dev/null
@@ -1,248 +0,0 @@
-package org.apache.lucene.collation;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.ScoreDoc;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermRangeFilter;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.TermRangeQuery;
-import org.apache.lucene.search.Searcher;
-import org.apache.lucene.search.Sort;
-import org.apache.lucene.search.SortField;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.util.IndexableBinaryStringTools;
-import org.apache.lucene.util.LuceneTestCase;
-
-import java.io.StringReader;
-import java.io.IOException;
-
-public abstract class CollationTestBase extends LuceneTestCase {
-
-  protected String firstRangeBeginningOriginal = "\u062F";
-  protected String firstRangeEndOriginal = "\u0698";
-  
-  protected String secondRangeBeginningOriginal = "\u0633";
-  protected String secondRangeEndOriginal = "\u0638";
-  
-  /**
-   * Convenience method to perform the same function as CollationKeyFilter.
-   *  
-   * @param keyBits the result from 
-   *  collator.getCollationKey(original).toByteArray()
-   * @return The encoded collation key for the original String
-   */
-  protected String encodeCollationKey(byte[] keyBits) {
-    // Ensure that the backing char[] array is large enough to hold the encoded
-    // Binary String
-    int encodedLength = IndexableBinaryStringTools.getEncodedLength(keyBits, 0, keyBits.length);
-    char[] encodedBegArray = new char[encodedLength];
-    IndexableBinaryStringTools.encode(keyBits, 0, keyBits.length, encodedBegArray, 0, encodedLength);
-    return new String(encodedBegArray);
-  }
-    
-  public void testFarsiRangeFilterCollating(Analyzer analyzer, String firstBeg, 
-                                            String firstEnd, String secondBeg,
-                                            String secondEnd) throws Exception {
-    RAMDirectory ramDir = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(ramDir, new IndexWriterConfig(
-        TEST_VERSION_CURRENT, analyzer));
-    Document doc = new Document();
-    doc.add(new Field("content", "\u0633\u0627\u0628", 
-                      Field.Store.YES, Field.Index.ANALYZED));
-    doc.add(new Field("body", "body",
-                      Field.Store.YES, Field.Index.NOT_ANALYZED));
-    writer.addDocument(doc);
-    writer.close();
-    IndexSearcher searcher = new IndexSearcher(ramDir, true);
-    Query query = new TermQuery(new Term("body","body"));
-
-    // Unicode order would include U+0633 in [ U+062F - U+0698 ], but Farsi
-    // orders the U+0698 character before the U+0633 character, so the single
-    // index Term below should NOT be returned by a TermRangeFilter with a Farsi
-    // Collator (or an Arabic one for the case when Farsi searcher not
-    // supported).
-    ScoreDoc[] result = searcher.search
-      (query, new TermRangeFilter("content", firstBeg, firstEnd, true, true), 1).scoreDocs;
-    assertEquals("The index Term should not be included.", 0, result.length);
-
-    result = searcher.search
-      (query, new TermRangeFilter("content", secondBeg, secondEnd, true, true), 1).scoreDocs;
-    assertEquals("The index Term should be included.", 1, result.length);
-
-    searcher.close();
-  }
- 
-  public void testFarsiRangeQueryCollating(Analyzer analyzer, String firstBeg, 
-                                            String firstEnd, String secondBeg,
-                                            String secondEnd) throws Exception {
-    RAMDirectory ramDir = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(ramDir, new IndexWriterConfig(
-        TEST_VERSION_CURRENT, analyzer));
-    Document doc = new Document();
-
-    // Unicode order would include U+0633 in [ U+062F - U+0698 ], but Farsi
-    // orders the U+0698 character before the U+0633 character, so the single
-    // index Term below should NOT be returned by a TermRangeQuery with a Farsi
-    // Collator (or an Arabic one for the case when Farsi is not supported).
-    doc.add(new Field("content", "\u0633\u0627\u0628", 
-                      Field.Store.YES, Field.Index.ANALYZED));
-    writer.addDocument(doc);
-    writer.close();
-    IndexSearcher searcher = new IndexSearcher(ramDir, true);
-
-    Query query = new TermRangeQuery("content", firstBeg, firstEnd, true, true);
-    ScoreDoc[] hits = searcher.search(query, null, 1000).scoreDocs;
-    assertEquals("The index Term should not be included.", 0, hits.length);
-
-    query = new TermRangeQuery("content", secondBeg, secondEnd, true, true);
-    hits = searcher.search(query, null, 1000).scoreDocs;
-    assertEquals("The index Term should be included.", 1, hits.length);
-    searcher.close();
-  }
-
-  public void testFarsiTermRangeQuery(Analyzer analyzer, String firstBeg,
-      String firstEnd, String secondBeg, String secondEnd) throws Exception {
-
-    RAMDirectory farsiIndex = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(farsiIndex, new IndexWriterConfig(
-        TEST_VERSION_CURRENT, analyzer));
-    Document doc = new Document();
-    doc.add(new Field("content", "\u0633\u0627\u0628", 
-                      Field.Store.YES, Field.Index.ANALYZED));
-    doc.add(new Field("body", "body",
-                      Field.Store.YES, Field.Index.NOT_ANALYZED));
-    writer.addDocument(doc);
-    writer.close();
-
-    IndexReader reader = IndexReader.open(farsiIndex, true);
-    IndexSearcher search = new IndexSearcher(reader);
-        
-    // Unicode order would include U+0633 in [ U+062F - U+0698 ], but Farsi
-    // orders the U+0698 character before the U+0633 character, so the single
-    // index Term below should NOT be returned by a TermRangeQuery
-    // with a Farsi Collator (or an Arabic one for the case when Farsi is 
-    // not supported).
-    Query csrq 
-      = new TermRangeQuery("content", firstBeg, firstEnd, true, true);
-    ScoreDoc[] result = search.search(csrq, null, 1000).scoreDocs;
-    assertEquals("The index Term should not be included.", 0, result.length);
-
-    csrq = new TermRangeQuery
-      ("content", secondBeg, secondEnd, true, true);
-    result = search.search(csrq, null, 1000).scoreDocs;
-    assertEquals("The index Term should be included.", 1, result.length);
-    search.close();
-  }
-  
-  // Test using various international locales with accented characters (which
-  // sort differently depending on locale)
-  //
-  // Copied (and slightly modified) from 
-  // org.apache.lucene.search.TestSort.testInternationalSort()
-  //  
-  public void testCollationKeySort(Analyzer usAnalyzer,
-                                   Analyzer franceAnalyzer,
-                                   Analyzer swedenAnalyzer,
-                                   Analyzer denmarkAnalyzer,
-                                   String usResult) throws Exception {
-    RAMDirectory indexStore = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(
-        TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));
-
-    // document data:
-    // the tracer field is used to determine which document was hit
-    String[][] sortData = new String[][] {
-      // tracer contents US                 France             Sweden (sv_SE)     Denmark (da_DK)
-      {  "A",   "x",     "p\u00EAche",      "p\u00EAche",      "p\u00EAche",      "p\u00EAche"      },
-      {  "B",   "y",     "HAT",             "HAT",             "HAT",             "HAT"             },
-      {  "C",   "x",     "p\u00E9ch\u00E9", "p\u00E9ch\u00E9", "p\u00E9ch\u00E9", "p\u00E9ch\u00E9" },
-      {  "D",   "y",     "HUT",             "HUT",             "HUT",             "HUT"             },
-      {  "E",   "x",     "peach",           "peach",           "peach",           "peach"           },
-      {  "F",   "y",     "H\u00C5T",        "H\u00C5T",        "H\u00C5T",        "H\u00C5T"        },
-      {  "G",   "x",     "sin",             "sin",             "sin",             "sin"             },
-      {  "H",   "y",     "H\u00D8T",        "H\u00D8T",        "H\u00D8T",        "H\u00D8T"        },
-      {  "I",   "x",     "s\u00EDn",        "s\u00EDn",        "s\u00EDn",        "s\u00EDn"        },
-      {  "J",   "y",     "HOT",             "HOT",             "HOT",             "HOT"             },
-    };
-
-    for (int i = 0 ; i < sortData.length ; ++i) {
-      Document doc = new Document();
-      doc.add(new Field("tracer", sortData[i][0], 
-                        Field.Store.YES, Field.Index.NO));
-      doc.add(new Field("contents", sortData[i][1], 
-                        Field.Store.NO, Field.Index.ANALYZED));
-      if (sortData[i][2] != null) 
-        doc.add(new Field("US", usAnalyzer.reusableTokenStream("US", new StringReader(sortData[i][2]))));
-      if (sortData[i][3] != null) 
-        doc.add(new Field("France", franceAnalyzer.reusableTokenStream("France", new StringReader(sortData[i][3]))));
-      if (sortData[i][4] != null)
-        doc.add(new Field("Sweden", swedenAnalyzer.reusableTokenStream("Sweden", new StringReader(sortData[i][4]))));
-      if (sortData[i][5] != null) 
-        doc.add(new Field("Denmark", denmarkAnalyzer.reusableTokenStream("Denmark", new StringReader(sortData[i][5]))));
-      writer.addDocument(doc);
-    }
-    writer.optimize();
-    writer.close();
-    Searcher searcher = new IndexSearcher(indexStore, true);
-
-    Sort sort = new Sort();
-    Query queryX = new TermQuery(new Term ("contents", "x"));
-    Query queryY = new TermQuery(new Term ("contents", "y"));
-    
-    sort.setSort(new SortField("US", SortField.STRING));
-    assertMatches(searcher, queryY, sort, usResult);
-
-    sort.setSort(new SortField("France", SortField.STRING));
-    assertMatches(searcher, queryX, sort, "EACGI");
-
-    sort.setSort(new SortField("Sweden", SortField.STRING));
-    assertMatches(searcher, queryY, sort, "BJDFH");
-
-    sort.setSort(new SortField("Denmark", SortField.STRING));
-    assertMatches(searcher, queryY, sort, "BJDHF");
-  }
-    
-  // Make sure the documents returned by the search match the expected list
-  // Copied from TestSort.java
-  private void assertMatches(Searcher searcher, Query query, Sort sort, 
-                             String expectedResult) throws IOException {
-    ScoreDoc[] result = searcher.search(query, null, 1000, sort).scoreDocs;
-    StringBuilder buff = new StringBuilder(10);
-    int n = result.length;
-    for (int i = 0 ; i < n ; ++i) {
-      Document doc = searcher.doc(result[i].doc);
-      String[] v = doc.getValues("tracer");
-      for (int j = 0 ; j < v.length ; ++j) {
-        buff.append(v[j]);
-      }
-    }
-    assertEquals(expectedResult, buff.toString());
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/collation/TestCollationKeyAnalyzer.java b/lucene/src/test/org/apache/lucene/collation/TestCollationKeyAnalyzer.java
deleted file mode 100644
index 06c6d07..0000000
--- a/lucene/src/test/org/apache/lucene/collation/TestCollationKeyAnalyzer.java
+++ /dev/null
@@ -1,77 +0,0 @@
-package org.apache.lucene.collation;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-import org.apache.lucene.analysis.Analyzer;
-
-import java.text.Collator;
-import java.util.Locale;
-
-
-public class TestCollationKeyAnalyzer extends CollationTestBase {
-
-  // Neither Java 1.4.2 nor 1.5.0 has Farsi Locale collation available in
-  // RuleBasedCollator.  However, the Arabic Locale seems to order the Farsi
-  // characters properly.
-  private Collator collator = Collator.getInstance(new Locale("ar"));
-  private Analyzer analyzer = new CollationKeyAnalyzer(collator);
-
-  private String firstRangeBeginning = encodeCollationKey
-    (collator.getCollationKey(firstRangeBeginningOriginal).toByteArray());
-  private String firstRangeEnd = encodeCollationKey
-    (collator.getCollationKey(firstRangeEndOriginal).toByteArray());
-  private String secondRangeBeginning = encodeCollationKey
-    (collator.getCollationKey(secondRangeBeginningOriginal).toByteArray());
-  private String secondRangeEnd = encodeCollationKey
-    (collator.getCollationKey(secondRangeEndOriginal).toByteArray());
-  
-  public void testFarsiRangeFilterCollating() throws Exception {
-    testFarsiRangeFilterCollating
-      (analyzer, firstRangeBeginning, firstRangeEnd, 
-       secondRangeBeginning, secondRangeEnd);
-  }
- 
-  public void testFarsiRangeQueryCollating() throws Exception {
-    testFarsiRangeQueryCollating
-      (analyzer, firstRangeBeginning, firstRangeEnd, 
-       secondRangeBeginning, secondRangeEnd);
-  }
-
-  public void testFarsiTermRangeQuery() throws Exception {
-    testFarsiTermRangeQuery
-      (analyzer, firstRangeBeginning, firstRangeEnd, 
-       secondRangeBeginning, secondRangeEnd);
-  }
-  
-  public void testCollationKeySort() throws Exception {
-    Analyzer usAnalyzer 
-      = new CollationKeyAnalyzer(Collator.getInstance(Locale.US));
-    Analyzer franceAnalyzer 
-      = new CollationKeyAnalyzer(Collator.getInstance(Locale.FRANCE));
-    Analyzer swedenAnalyzer 
-      = new CollationKeyAnalyzer(Collator.getInstance(new Locale("sv", "se")));
-    Analyzer denmarkAnalyzer 
-      = new CollationKeyAnalyzer(Collator.getInstance(new Locale("da", "dk")));
-    
-    // The ICU Collator and java.text.Collator implementations differ in their
-    // orderings - "BFJDH" is the ordering for java.text.Collator for Locale.US.
-    testCollationKeySort
-      (usAnalyzer, franceAnalyzer, swedenAnalyzer, denmarkAnalyzer, "BFJDH");
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/collation/TestCollationKeyFilter.java b/lucene/src/test/org/apache/lucene/collation/TestCollationKeyFilter.java
deleted file mode 100644
index 533242b..0000000
--- a/lucene/src/test/org/apache/lucene/collation/TestCollationKeyFilter.java
+++ /dev/null
@@ -1,95 +0,0 @@
-package org.apache.lucene.collation;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.KeywordTokenizer;
-
-import java.text.Collator;
-import java.util.Locale;
-import java.io.Reader;
-
-
-public class TestCollationKeyFilter extends CollationTestBase {
-
-  // Neither Java 1.4.2 nor 1.5.0 has Farsi Locale collation available in
-  // RuleBasedCollator.  However, the Arabic Locale seems to order the Farsi
-  // characters properly.
-  private Collator collator = Collator.getInstance(new Locale("ar"));
-  private Analyzer analyzer = new TestAnalyzer(collator);
-
-  private String firstRangeBeginning = encodeCollationKey
-    (collator.getCollationKey(firstRangeBeginningOriginal).toByteArray());
-  private String firstRangeEnd = encodeCollationKey
-    (collator.getCollationKey(firstRangeEndOriginal).toByteArray());
-  private String secondRangeBeginning = encodeCollationKey
-    (collator.getCollationKey(secondRangeBeginningOriginal).toByteArray());
-  private String secondRangeEnd = encodeCollationKey
-    (collator.getCollationKey(secondRangeEndOriginal).toByteArray());
-
-  
-  public final class TestAnalyzer extends Analyzer {
-    private Collator _collator;
-
-    TestAnalyzer(Collator collator) {
-      _collator = collator;
-    }
-
-    @Override
-    public TokenStream tokenStream(String fieldName, Reader reader) {
-      TokenStream result = new KeywordTokenizer(reader);
-      result = new CollationKeyFilter(result, _collator);
-      return result;
-    }
-  }
-
-  public void testFarsiRangeFilterCollating() throws Exception {
-    testFarsiRangeFilterCollating
-      (analyzer, firstRangeBeginning, firstRangeEnd, 
-       secondRangeBeginning, secondRangeEnd);
-  }
- 
-  public void testFarsiRangeQueryCollating() throws Exception {
-    testFarsiRangeQueryCollating
-      (analyzer, firstRangeBeginning, firstRangeEnd, 
-       secondRangeBeginning, secondRangeEnd);
-  }
-
-  public void testFarsiTermRangeQuery() throws Exception {
-    testFarsiTermRangeQuery
-      (analyzer, firstRangeBeginning, firstRangeEnd, 
-       secondRangeBeginning, secondRangeEnd);
-  }
-  
-  public void testCollationKeySort() throws Exception {
-    Analyzer usAnalyzer = new TestAnalyzer(Collator.getInstance(Locale.US));
-    Analyzer franceAnalyzer 
-      = new TestAnalyzer(Collator.getInstance(Locale.FRANCE));
-    Analyzer swedenAnalyzer 
-      = new TestAnalyzer(Collator.getInstance(new Locale("sv", "se")));
-    Analyzer denmarkAnalyzer 
-      = new TestAnalyzer(Collator.getInstance(new Locale("da", "dk")));
-    
-    // The ICU Collator and java.text.Collator implementations differ in their
-    // orderings - "BFJDH" is the ordering for java.text.Collator for Locale.US.
-    testCollationKeySort
-      (usAnalyzer, franceAnalyzer, swedenAnalyzer, denmarkAnalyzer, "BFJDH");
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/index/TestDocumentWriter.java b/lucene/src/test/org/apache/lucene/index/TestDocumentWriter.java
index 01a1fa1..ad292b2 100644
--- a/lucene/src/test/org/apache/lucene/index/TestDocumentWriter.java
+++ b/lucene/src/test/org/apache/lucene/index/TestDocumentWriter.java
@@ -25,8 +25,6 @@ import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
diff --git a/lucene/src/test/org/apache/lucene/index/TestIndexWriter.java b/lucene/src/test/org/apache/lucene/index/TestIndexWriter.java
index 7d1ddfb..b9dfcd3 100644
--- a/lucene/src/test/org/apache/lucene/index/TestIndexWriter.java
+++ b/lucene/src/test/org/apache/lucene/index/TestIndexWriter.java
@@ -42,7 +42,6 @@ import org.apache.lucene.analysis.MockTokenFilter;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.document.Document;
@@ -538,67 +537,6 @@ public class TestIndexWriter extends LuceneTestCase {
       }
     }
 
-    /**
-     * Make sure we skip wicked long terms.
-    */
-    public void testWickedLongTerm() throws IOException {
-      RAMDirectory dir = new RAMDirectory();
-      IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(
-        TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT)));
-
-      char[] chars = new char[DocumentsWriter.MAX_TERM_LENGTH_UTF8];
-      Arrays.fill(chars, 'x');
-      Document doc = new Document();
-      final String bigTerm = new String(chars);
-
-      // This produces a too-long term:
-      String contents = "abc xyz x" + bigTerm + " another term";
-      doc.add(new Field("content", contents, Field.Store.NO, Field.Index.ANALYZED));
-      writer.addDocument(doc);
-
-      // Make sure we can add another normal document
-      doc = new Document();
-      doc.add(new Field("content", "abc bbb ccc", Field.Store.NO, Field.Index.ANALYZED));
-      writer.addDocument(doc);
-      writer.close();
-
-      IndexReader reader = IndexReader.open(dir, true);
-
-      // Make sure all terms < max size were indexed
-      assertEquals(2, reader.docFreq(new Term("content", "abc")));
-      assertEquals(1, reader.docFreq(new Term("content", "bbb")));
-      assertEquals(1, reader.docFreq(new Term("content", "term")));
-      assertEquals(1, reader.docFreq(new Term("content", "another")));
-
-      // Make sure position is still incremented when
-      // massive term is skipped:
-      TermPositions tps = reader.termPositions(new Term("content", "another"));
-      assertTrue(tps.next());
-      assertEquals(1, tps.freq());
-      assertEquals(3, tps.nextPosition());
-
-      // Make sure the doc that has the massive term is in
-      // the index:
-      assertEquals("document with wicked long term should is not in the index!", 2, reader.numDocs());
-
-      reader.close();
-
-      // Make sure we can add a document with exactly the
-      // maximum length term, and search on that term:
-      doc = new Document();
-      doc.add(new Field("content", bigTerm, Field.Store.NO, Field.Index.ANALYZED));
-      StandardAnalyzer sa = new StandardAnalyzer(TEST_VERSION_CURRENT);
-      sa.setMaxTokenLength(100000);
-      writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));
-      writer.addDocument(doc);
-      writer.close();
-      reader = IndexReader.open(dir, true);
-      assertEquals(1, reader.docFreq(new Term("content", bigTerm)));
-      reader.close();
-
-      dir.close();
-    }
-
     public void testOptimizeMaxNumSegments() throws IOException {
 
       MockRAMDirectory dir = new MockRAMDirectory();
diff --git a/lucene/src/test/org/apache/lucene/index/TestPayloads.java b/lucene/src/test/org/apache/lucene/index/TestPayloads.java
index ece9a76..3f97234 100644
--- a/lucene/src/test/org/apache/lucene/index/TestPayloads.java
+++ b/lucene/src/test/org/apache/lucene/index/TestPayloads.java
@@ -32,7 +32,6 @@ import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.document.Document;
diff --git a/lucene/src/test/org/apache/lucene/index/TestWordlistLoader.java b/lucene/src/test/org/apache/lucene/index/TestWordlistLoader.java
deleted file mode 100644
index 0730523..0000000
--- a/lucene/src/test/org/apache/lucene/index/TestWordlistLoader.java
+++ /dev/null
@@ -1,81 +0,0 @@
-package org.apache.lucene.index;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.BufferedReader;
-import java.io.IOException;
-import java.io.StringReader;
-import java.util.HashSet;
-import java.util.Set;
-
-import org.apache.lucene.util.LuceneTestCase;
-
-import org.apache.lucene.analysis.WordlistLoader;
-
-public class TestWordlistLoader extends LuceneTestCase {
-
-  public void testWordlistLoading() throws IOException {
-    String s = "ONE\n  two \nthree";
-    HashSet<String> wordSet1 = WordlistLoader.getWordSet(new StringReader(s));
-    checkSet(wordSet1);
-    HashSet<String> wordSet2 = WordlistLoader.getWordSet(new BufferedReader(new StringReader(s)));
-    checkSet(wordSet2);
-  }
-
-  public void testComments() throws Exception {
-    String s = "ONE\n  two \nthree\n#comment";
-    HashSet<String> wordSet1 = WordlistLoader.getWordSet(new StringReader(s), "#");
-    checkSet(wordSet1);
-    assertFalse(wordSet1.contains("#comment"));
-    assertFalse(wordSet1.contains("comment"));
-  }
-
-
-  private void checkSet(HashSet<String> wordset) {
-    assertEquals(3, wordset.size());
-    assertTrue(wordset.contains("ONE"));		// case is not modified
-    assertTrue(wordset.contains("two"));		// surrounding whitespace is removed
-    assertTrue(wordset.contains("three"));
-    assertFalse(wordset.contains("four"));
-  }
-
-  /**
-   * Test stopwords in snowball format
-   */
-  public void testSnowballListLoading() throws IOException {
-    String s = 
-      "|comment\n" + // commented line
-      " |comment\n" + // commented line with leading whitespace
-      "\n" + // blank line
-      "  \t\n" + // line with only whitespace
-      " |comment | comment\n" + // commented line with comment
-      "ONE\n" + // stopword, in uppercase
-      "   two   \n" + // stopword with leading/trailing space
-      " three   four five \n" + // multiple stopwords
-      "six seven | comment\n"; //multiple stopwords + comment
-    Set<String> wordset = WordlistLoader.getSnowballWordSet(new StringReader(s));
-    assertEquals(7, wordset.size());
-    assertTrue(wordset.contains("ONE"));
-    assertTrue(wordset.contains("two"));
-    assertTrue(wordset.contains("three"));
-    assertTrue(wordset.contains("four"));
-    assertTrue(wordset.contains("five"));
-    assertTrue(wordset.contains("six"));
-    assertTrue(wordset.contains("seven"));
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/queryParser/TestQueryParser.java b/lucene/src/test/org/apache/lucene/queryParser/TestQueryParser.java
index 511645c..3691b87 100644
--- a/lucene/src/test/org/apache/lucene/queryParser/TestQueryParser.java
+++ b/lucene/src/test/org/apache/lucene/queryParser/TestQueryParser.java
@@ -29,7 +29,6 @@ import java.util.HashSet;
 import java.util.Locale;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.KeywordAnalyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenFilter;
 import org.apache.lucene.analysis.MockTokenizer;
@@ -255,8 +254,10 @@ public class TestQueryParser extends LocalizedTestCase {
     assertQueryEquals("trm term term", new MockAnalyzer(), "trm term term");
     assertQueryEquals("mlaut", new MockAnalyzer(), "mlaut");
 
-    assertQueryEquals("\"\"", new KeywordAnalyzer(), "");
-    assertQueryEquals("foo:\"\"", new KeywordAnalyzer(), "foo:");
+    // FIXME: enhance MockAnalyzer to be able to support this
+    // it must no longer extend CharTokenizer
+    //assertQueryEquals("\"\"", new KeywordAnalyzer(), "");
+    //assertQueryEquals("foo:\"\"", new KeywordAnalyzer(), "foo:");
 
     assertQueryEquals("a AND b", null, "+a +b");
     assertQueryEquals("(a AND b)", null, "+a +b");
diff --git a/lucene/src/test/org/apache/lucene/search/TestMultiTermConstantScore.java b/lucene/src/test/org/apache/lucene/search/TestMultiTermConstantScore.java
index 474dd06..d76dc71 100644
--- a/lucene/src/test/org/apache/lucene/search/TestMultiTermConstantScore.java
+++ b/lucene/src/test/org/apache/lucene/search/TestMultiTermConstantScore.java
@@ -19,8 +19,6 @@ package org.apache.lucene.search;
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.analysis.SimpleAnalyzer;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexReader;
diff --git a/lucene/src/test/org/apache/lucene/search/TestPositionIncrement.java b/lucene/src/test/org/apache/lucene/search/TestPositionIncrement.java
index 48a6dec..c372487 100644
--- a/lucene/src/test/org/apache/lucene/search/TestPositionIncrement.java
+++ b/lucene/src/test/org/apache/lucene/search/TestPositionIncrement.java
@@ -21,19 +21,15 @@ import java.io.Reader;
 import java.io.IOException;
 import java.io.StringReader;
 import java.util.Collection;
-import java.util.Collections;
-import java.util.Iterator;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.analysis.StopFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.CharArraySet;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexWriter;
@@ -44,7 +40,6 @@ import org.apache.lucene.index.TermPositions;
 import org.apache.lucene.queryParser.QueryParser;
 import org.apache.lucene.store.MockRAMDirectory;
 import org.apache.lucene.store.Directory;
-import org.apache.lucene.analysis.LowerCaseTokenizer;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.index.Payload;
 import org.apache.lucene.search.payloads.PayloadSpanUtil;
@@ -52,9 +47,7 @@ import org.apache.lucene.search.spans.SpanNearQuery;
 import org.apache.lucene.search.spans.SpanQuery;
 import org.apache.lucene.search.spans.SpanTermQuery;
 import org.apache.lucene.search.spans.Spans;
-import org.apache.lucene.util.Version;
 import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.automaton.BasicAutomata;
 import org.apache.lucene.util.automaton.CharacterRunAutomaton;
 import org.apache.lucene.util.automaton.RegExp;
 
diff --git a/lucene/src/test/org/apache/lucene/search/TestRegexpRandom2.java b/lucene/src/test/org/apache/lucene/search/TestRegexpRandom2.java
index fcb4d2a..e7a6064 100644
--- a/lucene/src/test/org/apache/lucene/search/TestRegexpRandom2.java
+++ b/lucene/src/test/org/apache/lucene/search/TestRegexpRandom2.java
@@ -20,7 +20,8 @@ package org.apache.lucene.search;
 import java.io.IOException;
 import java.util.Random;
 
-import org.apache.lucene.analysis.KeywordAnalyzer;
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexReader;
@@ -50,7 +51,7 @@ public class TestRegexpRandom2 extends LuceneTestCase {
     super.setUp();
     random = newRandom();
     RAMDirectory dir = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new KeywordAnalyzer(),
+    IndexWriter writer = new IndexWriter(dir, new MockAnalyzer(MockTokenizer.KEYWORD, false),
         IndexWriter.MaxFieldLength.UNLIMITED);
     
     Document doc = new Document();
diff --git a/lucene/src/test/org/apache/lucene/search/TestSloppyPhraseQuery.java b/lucene/src/test/org/apache/lucene/search/TestSloppyPhraseQuery.java
index bd755d4..f928eb8 100755
--- a/lucene/src/test/org/apache/lucene/search/TestSloppyPhraseQuery.java
+++ b/lucene/src/test/org/apache/lucene/search/TestSloppyPhraseQuery.java
@@ -20,7 +20,6 @@ package org.apache.lucene.search;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexWriter;
diff --git a/lucene/src/test/org/apache/lucene/search/TestTermVectors.java b/lucene/src/test/org/apache/lucene/search/TestTermVectors.java
index e1feda7..39fd3de 100644
--- a/lucene/src/test/org/apache/lucene/search/TestTermVectors.java
+++ b/lucene/src/test/org/apache/lucene/search/TestTermVectors.java
@@ -20,7 +20,6 @@ package org.apache.lucene.search;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.analysis.SimpleAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.*;
diff --git a/modules/analysis/CHANGES.txt b/modules/analysis/CHANGES.txt
index 8e1b0db..ec107a2 100644
--- a/modules/analysis/CHANGES.txt
+++ b/modules/analysis/CHANGES.txt
@@ -34,6 +34,16 @@ New Features
    (... in progress)
 
   * LUCENE-2413: Consolidated all Lucene analyzers into common.
+    - o.a.l.analysis.KeywordAnalyzer -> o.a.l.analysis.core.KeywordAnalyzer
+    - o.a.l.analysis.KeywordTokenizer -> o.a.l.analysis.core.KeywordTokenizer
+    - o.a.l.analysis.LetterTokenizer -> o.a.l.analysis.core.LetterTokenizer
+    - o.a.l.analysis.LowerCaseFilter -> o.a.l.analysis.core.LowerCaseFilter
+    - o.a.l.analysis.LowerCaseTokenizer -> o.a.l.analysis.core.LowerCaseTokenizer
+    - o.a.l.analysis.SimpleAnalyzer -> o.a.l.analysis.core.SimpleAnalyzer
+    - o.a.l.analysis.StopAnalyzer -> o.a.l.analysis.core.StopAnalyzer
+    - o.a.l.analysis.StopFilter -> o.a.l.analysis.core.StopFilter
+    - o.a.l.analysis.WhitespaceAnalyzer -> o.a.l.analysis.core.WhitespaceAnalyzer
+    - o.a.l.analysis.WhitespaceTokenizer -> o.a.l.analysis.core.WhitespaceTokenizer
     - o.a.l.analysis.PorterStemFilter -> o.a.l.analysis.en.PorterStemFilter
     - o.a.l.analysis.ASCIIFoldingFilter -> o.a.l.analysis.miscellaneous.ASCIIFoldingFilter
     - o.a.l.analysis.ISOLatin1AccentFilter -> o.a.l.analysis.miscellaneous.ISOLatin1AccentFilter
@@ -44,6 +54,9 @@ New Features
     - o.a.l.analysis.BaseCharFilter -> o.a.l.analysis.charfilter.BaseCharFilter
     - o.a.l.analysis.MappingCharFilter -> o.a.l.analysis.charfilter.MappingCharFilter
     - o.a.l.analysis.NormalizeCharMap -> o.a.l.analysis.charfilter.NormalizeCharMap
+    - o.a.l.analysis.ReusableAnalyzerBase -> o.a.l.analysis.util.ReusableAnalyzerBase
+    - o.a.l.analysis.StopwordAnalyzerBase -> o.a.l.analysis.util.StopwordAnalyzerBase
+    - o.a.l.analysis.WordListLoader -> o.a.l.analysis.util.WordListLoader
     ... (in progress)
 
 Build
diff --git a/modules/analysis/common/build.xml b/modules/analysis/common/build.xml
index 5344c71..596c04c 100644
--- a/modules/analysis/common/build.xml
+++ b/modules/analysis/common/build.xml
@@ -38,7 +38,7 @@
 
   <target name="compile-core" depends="jflex-notice, common.compile-core"/>
 
-  <target name="jflex" depends="jflex-check,clean-jflex,jflex-wiki-tokenizer"/>
+  <target name="jflex" depends="jflex-check,clean-jflex,jflex-StandardAnalyzer,jflex-wiki-tokenizer"/>
 
   <target name="jflex-wiki-tokenizer" depends="init,jflex-check" if="jflex.present">
     <taskdef classname="jflex.anttask.JFlexTask" name="jflex">
@@ -49,11 +49,27 @@
            nobak="on"/>
   </target>
 
+  <target name="jflex-StandardAnalyzer" depends="init,jflex-check" if="jflex.present">
+    <taskdef classname="jflex.anttask.JFlexTask" name="jflex">
+			<classpath refid="jflex.classpath"/>
+    </taskdef>
+
+    <jflex file="src/java/org/apache/lucene/analysis/standard/StandardTokenizerImplOrig.jflex"
+           outdir="src/java/org/apache/lucene/analysis/standard"
+           nobak="on" />
+    <jflex file="src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl31.jflex"
+           outdir="src/java/org/apache/lucene/analysis/standard"
+           nobak="on" />
+  </target>
+
   <target name="clean-jflex">
     <delete>
       <fileset dir="src/java/org/apache/lucene/analysis/wikipedia" includes="*.java">
         <containsregexp expression="generated.*by.*JFlex"/>
       </fileset>
+      <fileset dir="src/java/org/apache/lucene/analysis/standard" includes="*.java">
+    	<containsregexp expression="generated.*by.*JFlex"/>
+      </fileset>
     </delete>
   </target>
 </project>
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/ar/ArabicAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/ar/ArabicAnalyzer.java
index ec89f64..a0d1f59 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/ar/ArabicAnalyzer.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/ar/ArabicAnalyzer.java
@@ -24,14 +24,14 @@ import java.util.Hashtable;
 import java.util.Set;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.LowerCaseFilter;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.util.Version;
 
 /**
@@ -163,10 +163,10 @@ public final class ArabicAnalyzer extends StopwordAnalyzerBase {
 
   /**
    * Creates
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * used to tokenize all the text in the provided {@link Reader}.
    * 
-   * @return {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * @return {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from an {@link ArabicLetterTokenizer} filtered with
    *         {@link LowerCaseFilter}, {@link StopFilter},
    *         {@link ArabicNormalizationFilter}, {@link KeywordMarkerFilter}
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/ar/ArabicLetterTokenizer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/ar/ArabicLetterTokenizer.java
index 5286eac..09ccecd 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/ar/ArabicLetterTokenizer.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/ar/ArabicLetterTokenizer.java
@@ -19,7 +19,7 @@ package org.apache.lucene.analysis.ar;
 import java.io.Reader;
 
 import org.apache.lucene.analysis.CharTokenizer;
-import org.apache.lucene.analysis.LetterTokenizer;
+import org.apache.lucene.analysis.core.LetterTokenizer;
 import org.apache.lucene.util.AttributeSource;
 import org.apache.lucene.util.Version;
 
@@ -120,7 +120,7 @@ public class ArabicLetterTokenizer extends LetterTokenizer {
   
   /** 
    * Allows for Letter category or NonspacingMark category
-   * @see org.apache.lucene.analysis.LetterTokenizer#isTokenChar(int)
+   * @see org.apache.lucene.analysis.core.LetterTokenizer#isTokenChar(int)
    */
   @Override
   protected boolean isTokenChar(int c) {
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/bg/BulgarianAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/bg/BulgarianAnalyzer.java
index 99aa34f..a1f8a47 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/bg/BulgarianAnalyzer.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/bg/BulgarianAnalyzer.java
@@ -23,16 +23,16 @@ import java.io.Reader;
 import java.util.Set;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.LowerCaseFilter;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 
 /**
@@ -119,11 +119,11 @@ public final class BulgarianAnalyzer extends StopwordAnalyzerBase {
 
   /**
    * Creates a
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * which tokenizes all the text in the provided {@link Reader}.
    * 
    * @return A
-   *         {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   *         {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from an {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
    *         , {@link KeywordMarkerFilter} if a stem exclusion set is
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/br/BrazilianAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/br/BrazilianAnalyzer.java
index a24e431..54d0c30 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/br/BrazilianAnalyzer.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/br/BrazilianAnalyzer.java
@@ -28,16 +28,16 @@ import java.util.Set;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
-import org.apache.lucene.analysis.LowerCaseFilter;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 
 /**
@@ -193,10 +193,10 @@ public final class BrazilianAnalyzer extends StopwordAnalyzerBase {
 
   /**
    * Creates
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * used to tokenize all the text in the provided {@link Reader}.
    * 
-   * @return {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * @return {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from a {@link StandardTokenizer} filtered with
    *         {@link LowerCaseFilter}, {@link StandardFilter}, {@link StopFilter}
    *         , and {@link BrazilianStemFilter}.
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKAnalyzer.java
index 54104c5..1bb5885 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKAnalyzer.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKAnalyzer.java
@@ -19,9 +19,9 @@ package org.apache.lucene.analysis.cjk;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.core.StopFilter;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
 import org.apache.lucene.util.Version;
 
 import java.io.Reader;
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/cn/ChineseAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/cn/ChineseAnalyzer.java
index d1f9387..c04c349 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/cn/ChineseAnalyzer.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/cn/ChineseAnalyzer.java
@@ -19,8 +19,8 @@ package org.apache.lucene.analysis.cn;
 
 import java.io.Reader;
 
-import org.apache.lucene.analysis.ReusableAnalyzerBase;
 import org.apache.lucene.analysis.standard.StandardAnalyzer; // javadoc @link
+import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.Tokenizer;
 
@@ -35,10 +35,10 @@ public final class ChineseAnalyzer extends ReusableAnalyzerBase {
 
   /**
    * Creates
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * used to tokenize all the text in the provided {@link Reader}.
    * 
-   * @return {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * @return {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from a {@link ChineseTokenizer} filtered with
    *         {@link ChineseFilter}
    */
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/cn/ChineseFilter.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/cn/ChineseFilter.java
index 5ed043b..87a38b1 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/cn/ChineseFilter.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/cn/ChineseFilter.java
@@ -23,7 +23,7 @@ import java.util.Arrays;
 import org.apache.lucene.analysis.CharArraySet;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.StopFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.util.Version;
 
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/core/KeywordAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/core/KeywordAnalyzer.java
new file mode 100644
index 0000000..a253cb9
--- /dev/null
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/core/KeywordAnalyzer.java
@@ -0,0 +1,36 @@
+package org.apache.lucene.analysis.core;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Reader;
+
+import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
+
+/**
+ * "Tokenizes" the entire stream as a single token. This is useful
+ * for data like zip codes, ids, and some product names.
+ */
+public final class KeywordAnalyzer extends ReusableAnalyzerBase {
+  public KeywordAnalyzer() {
+  }
+
+  @Override
+  protected TokenStreamComponents createComponents(final String fieldName, final Reader reader) {
+    return new TokenStreamComponents(new KeywordTokenizer(reader));
+  }
+}
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/core/KeywordTokenizer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/core/KeywordTokenizer.java
new file mode 100644
index 0000000..a4ac6f0
--- /dev/null
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/core/KeywordTokenizer.java
@@ -0,0 +1,92 @@
+package org.apache.lucene.analysis.core;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.io.Reader;
+
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.util.AttributeSource;
+
+/**
+ * Emits the entire input as a single token.
+ */
+public final class KeywordTokenizer extends Tokenizer {
+  
+  private static final int DEFAULT_BUFFER_SIZE = 256;
+
+  private boolean done = false;
+  private int finalOffset;
+  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
+  private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
+  
+  public KeywordTokenizer(Reader input) {
+    this(input, DEFAULT_BUFFER_SIZE);
+  }
+
+  public KeywordTokenizer(Reader input, int bufferSize) {
+    super(input);
+    termAtt.resizeBuffer(bufferSize);
+  }
+
+  public KeywordTokenizer(AttributeSource source, Reader input, int bufferSize) {
+    super(source, input);
+    termAtt.resizeBuffer(bufferSize);
+  }
+
+  public KeywordTokenizer(AttributeFactory factory, Reader input, int bufferSize) {
+    super(factory, input);
+    termAtt.resizeBuffer(bufferSize);
+  }
+  
+  @Override
+  public final boolean incrementToken() throws IOException {
+    if (!done) {
+      clearAttributes();
+      done = true;
+      int upto = 0;
+      char[] buffer = termAtt.buffer();
+      while (true) {
+        final int length = input.read(buffer, upto, buffer.length-upto);
+        if (length == -1) break;
+        upto += length;
+        if (upto == buffer.length)
+          buffer = termAtt.resizeBuffer(1+buffer.length);
+      }
+      termAtt.setLength(upto);
+      finalOffset = correctOffset(upto);
+      offsetAtt.setOffset(correctOffset(0), finalOffset);
+      return true;
+    }
+    return false;
+  }
+  
+  @Override
+  public final void end() {
+    // set final offset 
+    offsetAtt.setOffset(finalOffset, finalOffset);
+  }
+
+  @Override
+  public void reset(Reader input) throws IOException {
+    super.reset(input);
+    this.done = false;
+  }
+}
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/core/LetterTokenizer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/core/LetterTokenizer.java
new file mode 100644
index 0000000..471f471
--- /dev/null
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/core/LetterTokenizer.java
@@ -0,0 +1,127 @@
+package org.apache.lucene.analysis.core;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Reader;
+
+import org.apache.lucene.analysis.CharTokenizer;
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.util.AttributeSource;
+import org.apache.lucene.util.Version;
+
+/**
+ * A LetterTokenizer is a tokenizer that divides text at non-letters. That's to
+ * say, it defines tokens as maximal strings of adjacent letters, as defined by
+ * java.lang.Character.isLetter() predicate.
+ * <p>
+ * Note: this does a decent job for most European languages, but does a terrible
+ * job for some Asian languages, where words are not separated by spaces.
+ * </p>
+ * <p>
+ * <a name="version"/>
+ * You must specify the required {@link Version} compatibility when creating
+ * {@link LetterTokenizer}:
+ * <ul>
+ * <li>As of 3.1, {@link CharTokenizer} uses an int based API to normalize and
+ * detect token characters. See {@link CharTokenizer#isTokenChar(int)} and
+ * {@link CharTokenizer#normalize(int)} for details.</li>
+ * </ul>
+ * </p>
+ */
+
+public class LetterTokenizer extends CharTokenizer {
+  
+  /**
+   * Construct a new LetterTokenizer.
+   * 
+   * @param matchVersion
+   *          Lucene version to match See {@link <a href="#version">above</a>}
+   * @param in
+   *          the input to split up into tokens
+   */
+  public LetterTokenizer(Version matchVersion, Reader in) {
+    super(matchVersion, in);
+  }
+  
+  /**
+   * Construct a new LetterTokenizer using a given {@link AttributeSource}.
+   * 
+   * @param matchVersion
+   *          Lucene version to match See {@link <a href="#version">above</a>}
+   * @param source
+   *          the attribute source to use for this {@link Tokenizer}
+   * @param in
+   *          the input to split up into tokens
+   */
+  public LetterTokenizer(Version matchVersion, AttributeSource source, Reader in) {
+    super(matchVersion, source, in);
+  }
+  
+  /**
+   * Construct a new LetterTokenizer using a given
+   * {@link org.apache.lucene.util.AttributeSource.AttributeFactory}.
+   * 
+   * @param matchVersion
+   *          Lucene version to match See {@link <a href="#version">above</a>}
+   * @param factory
+   *          the attribute factory to use for this {@link Tokenizer}
+   * @param in
+   *          the input to split up into tokens
+   */
+  public LetterTokenizer(Version matchVersion, AttributeFactory factory, Reader in) {
+    super(matchVersion, factory, in);
+  }
+  
+  /**
+   * Construct a new LetterTokenizer.
+   * 
+   * @deprecated use {@link #LetterTokenizer(Version, Reader)} instead. This
+   *             will be removed in Lucene 4.0.
+   */
+  public LetterTokenizer(Reader in) {
+    super(Version.LUCENE_30, in);
+  }
+  
+  /**
+   * Construct a new LetterTokenizer using a given {@link AttributeSource}. 
+   * @deprecated
+   * use {@link #LetterTokenizer(Version, AttributeSource, Reader)} instead.
+   * This will be removed in Lucene 4.0.
+   */
+  public LetterTokenizer(AttributeSource source, Reader in) {
+    super(Version.LUCENE_30, source, in);
+  }
+  
+  /**
+   * Construct a new LetterTokenizer using a given
+   * {@link org.apache.lucene.util.AttributeSource.AttributeFactory}.
+   * 
+   * @deprecated use {@link #LetterTokenizer(Version, AttributeSource.AttributeFactory, Reader)}
+   *             instead. This will be removed in Lucene 4.0.
+   */
+  public LetterTokenizer(AttributeFactory factory, Reader in) {
+    super(Version.LUCENE_30, factory, in);
+  }
+  
+  /** Collects only characters which satisfy
+   * {@link Character#isLetter(int)}.*/
+  @Override
+  protected boolean isTokenChar(int c) {
+    return Character.isLetter(c);
+  }
+}
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/core/LowerCaseFilter.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/core/LowerCaseFilter.java
new file mode 100644
index 0000000..e44e388
--- /dev/null
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/core/LowerCaseFilter.java
@@ -0,0 +1,74 @@
+package org.apache.lucene.analysis.core;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.analysis.TokenFilter;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.util.CharacterUtils;
+import org.apache.lucene.util.Version;
+
+/**
+ * Normalizes token text to lower case.
+ * <a name="version"/>
+ * <p>You must specify the required {@link Version}
+ * compatibility when creating LowerCaseFilter:
+ * <ul>
+ *   <li> As of 3.1, supplementary characters are properly lowercased.
+ * </ul>
+ */
+public final class LowerCaseFilter extends TokenFilter {
+  private final CharacterUtils charUtils;
+  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
+  
+  /**
+   * Create a new LowerCaseFilter, that normalizes token text to lower case.
+   * 
+   * @param matchVersion See <a href="#version">above</a>
+   * @param in TokenStream to filter
+   */
+  public LowerCaseFilter(Version matchVersion, TokenStream in) {
+    super(in);
+    charUtils = CharacterUtils.getInstance(matchVersion);
+  }
+  
+  /**
+   * @deprecated Use {@link #LowerCaseFilter(Version, TokenStream)} instead.
+   */
+  @Deprecated
+  public LowerCaseFilter(TokenStream in) {
+    this(Version.LUCENE_30, in);
+  }
+
+  @Override
+  public final boolean incrementToken() throws IOException {
+    if (input.incrementToken()) {
+      final char[] buffer = termAtt.buffer();
+      final int length = termAtt.length();
+      for (int i = 0; i < length;) {
+       i += Character.toChars(
+               Character.toLowerCase(
+                   charUtils.codePointAt(buffer, i)), buffer, i);
+      }
+      return true;
+    } else
+      return false;
+  }
+}
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/core/LowerCaseTokenizer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/core/LowerCaseTokenizer.java
new file mode 100644
index 0000000..b3e1072
--- /dev/null
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/core/LowerCaseTokenizer.java
@@ -0,0 +1,130 @@
+package org.apache.lucene.analysis.core;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Reader;
+
+import org.apache.lucene.analysis.CharTokenizer;
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.util.AttributeSource;
+import org.apache.lucene.util.Version;
+
+/**
+ * LowerCaseTokenizer performs the function of LetterTokenizer
+ * and LowerCaseFilter together.  It divides text at non-letters and converts
+ * them to lower case.  While it is functionally equivalent to the combination
+ * of LetterTokenizer and LowerCaseFilter, there is a performance advantage
+ * to doing the two tasks at once, hence this (redundant) implementation.
+ * <P>
+ * Note: this does a decent job for most European languages, but does a terrible
+ * job for some Asian languages, where words are not separated by spaces.
+ * </p>
+ * <p>
+ * <a name="version"/>
+ * You must specify the required {@link Version} compatibility when creating
+ * {@link LowerCaseTokenizer}:
+ * <ul>
+ * <li>As of 3.1, {@link CharTokenizer} uses an int based API to normalize and
+ * detect token characters. See {@link CharTokenizer#isTokenChar(int)} and
+ * {@link CharTokenizer#normalize(int)} for details.</li>
+ * </ul>
+ * </p>
+ */
+public final class LowerCaseTokenizer extends LetterTokenizer {
+  
+  /**
+   * Construct a new LowerCaseTokenizer.
+   * 
+   * @param matchVersion
+   *          Lucene version to match See {@link <a href="#version">above</a>}
+   * 
+   * @param in
+   *          the input to split up into tokens
+   */
+  public LowerCaseTokenizer(Version matchVersion, Reader in) {
+    super(matchVersion, in);
+  }
+
+  /** 
+   * Construct a new LowerCaseTokenizer using a given {@link AttributeSource}.
+   *
+   * @param matchVersion
+   *          Lucene version to match See {@link <a href="#version">above</a>}
+   * @param source
+   *          the attribute source to use for this {@link Tokenizer}
+   * @param in
+   *          the input to split up into tokens
+   */
+  public LowerCaseTokenizer(Version matchVersion, AttributeSource source, Reader in) {
+    super(matchVersion, source, in);
+  }
+
+  /**
+   * Construct a new LowerCaseTokenizer using a given
+   * {@link org.apache.lucene.util.AttributeSource.AttributeFactory}.
+   *
+   * @param matchVersion
+   *          Lucene version to match See {@link <a href="#version">above</a>}
+   * @param factory
+   *          the attribute factory to use for this {@link Tokenizer}
+   * @param in
+   *          the input to split up into tokens
+   */
+  public LowerCaseTokenizer(Version matchVersion, AttributeFactory factory, Reader in) {
+    super(matchVersion, factory, in);
+  }
+  
+  /**
+   * Construct a new LowerCaseTokenizer.
+   * 
+   * @deprecated use {@link #LowerCaseTokenizer(Reader)} instead. This will be
+   *             removed in Lucene 4.0.
+   */
+  @Deprecated
+  public LowerCaseTokenizer(Reader in) {
+    super(Version.LUCENE_30, in);
+  }
+
+  /**
+   * Construct a new LowerCaseTokenizer using a given {@link AttributeSource}.
+   * 
+   * @deprecated use {@link #LowerCaseTokenizer(AttributeSource, Reader)}
+   *             instead. This will be removed in Lucene 4.0.
+   */
+  public LowerCaseTokenizer(AttributeSource source, Reader in) {
+    super(Version.LUCENE_30, source, in);
+  }
+
+  /**
+   * Construct a new LowerCaseTokenizer using a given
+   * {@link org.apache.lucene.util.AttributeSource.AttributeFactory}.
+   * 
+   * @deprecated use {@link #LowerCaseTokenizer(AttributeSource.AttributeFactory, Reader)}
+   *             instead. This will be removed in Lucene 4.0.
+   */
+  public LowerCaseTokenizer(AttributeFactory factory, Reader in) {
+    super(Version.LUCENE_30, factory, in);
+  }
+  
+  /** Converts char to lower case
+   * {@link Character#toLowerCase(int)}.*/
+  @Override
+  protected int normalize(int c) {
+    return Character.toLowerCase(c);
+  }
+}
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/core/SimpleAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/core/SimpleAnalyzer.java
new file mode 100644
index 0000000..64e2c6c
--- /dev/null
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/core/SimpleAnalyzer.java
@@ -0,0 +1,64 @@
+package org.apache.lucene.analysis.core;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Reader;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.CharTokenizer;
+import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
+import org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents;
+import org.apache.lucene.util.Version;
+
+/** An {@link Analyzer} that filters {@link LetterTokenizer} 
+ *  with {@link LowerCaseFilter} 
+ * <p>
+ * <a name="version">You must specify the required {@link Version} compatibility
+ * when creating {@link CharTokenizer}:
+ * <ul>
+ * <li>As of 3.1, {@link LowerCaseTokenizer} uses an int based API to normalize and
+ * detect token codepoints. See {@link CharTokenizer#isTokenChar(int)} and
+ * {@link CharTokenizer#normalize(int)} for details.</li>
+ * </ul>
+ * <p>
+ **/
+public final class SimpleAnalyzer extends ReusableAnalyzerBase {
+
+  private final Version matchVersion;
+  
+  /**
+   * Creates a new {@link SimpleAnalyzer}
+   * @param matchVersion Lucene version to match See {@link <a href="#version">above</a>}
+   */
+  public SimpleAnalyzer(Version matchVersion) {
+    this.matchVersion = matchVersion;
+  }
+  
+  /**
+   * Creates a new {@link SimpleAnalyzer}
+   * @deprecated use {@link #SimpleAnalyzer(Version)} instead 
+   */
+  @Deprecated  public SimpleAnalyzer() {
+    this(Version.LUCENE_30);
+  }
+  @Override
+  protected TokenStreamComponents createComponents(final String fieldName,
+      final Reader reader) {
+    return new TokenStreamComponents(new LowerCaseTokenizer(matchVersion, reader));
+  }
+}
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/core/StopAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/core/StopAnalyzer.java
new file mode 100644
index 0000000..224a1ab
--- /dev/null
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/core/StopAnalyzer.java
@@ -0,0 +1,115 @@
+package org.apache.lucene.analysis.core;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.File;
+import java.io.IOException;
+import java.io.Reader;
+import java.util.Arrays;
+import java.util.Set;
+import java.util.List;
+
+import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
+import org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents;
+import org.apache.lucene.util.Version;
+
+/** Filters {@link LetterTokenizer} with {@link LowerCaseFilter} and {@link StopFilter}.
+ *
+ * <a name="version"/>
+ * <p>You must specify the required {@link Version}
+ * compatibility when creating StopAnalyzer:
+ * <ul>
+ *    <li> As of 3.1, StopFilter correctly handles Unicode 4.0
+ *         supplementary characters in stopwords
+ *   <li> As of 2.9, position increments are preserved
+ * </ul>
+*/
+
+public final class StopAnalyzer extends StopwordAnalyzerBase {
+  
+  /** An unmodifiable set containing some common English words that are not usually useful
+  for searching.*/
+  public static final Set<?> ENGLISH_STOP_WORDS_SET;
+  
+  static {
+    final List<String> stopWords = Arrays.asList(
+      "a", "an", "and", "are", "as", "at", "be", "but", "by",
+      "for", "if", "in", "into", "is", "it",
+      "no", "not", "of", "on", "or", "such",
+      "that", "the", "their", "then", "there", "these",
+      "they", "this", "to", "was", "will", "with"
+    );
+    final CharArraySet stopSet = new CharArraySet(Version.LUCENE_CURRENT, 
+        stopWords.size(), false);
+    stopSet.addAll(stopWords);  
+    ENGLISH_STOP_WORDS_SET = CharArraySet.unmodifiableSet(stopSet); 
+  }
+  
+  /** Builds an analyzer which removes words in
+   *  {@link #ENGLISH_STOP_WORDS_SET}.
+   * @param matchVersion See <a href="#version">above</a>
+   */
+  public StopAnalyzer(Version matchVersion) {
+    this(matchVersion, ENGLISH_STOP_WORDS_SET);
+  }
+
+  /** Builds an analyzer with the stop words from the given set.
+   * @param matchVersion See <a href="#version">above</a>
+   * @param stopWords Set of stop words */
+  public StopAnalyzer(Version matchVersion, Set<?> stopWords) {
+    super(matchVersion, stopWords);
+  }
+
+  /** Builds an analyzer with the stop words from the given file.
+   * @see WordlistLoader#getWordSet(File)
+   * @param matchVersion See <a href="#version">above</a>
+   * @param stopwordsFile File to load stop words from */
+  public StopAnalyzer(Version matchVersion, File stopwordsFile) throws IOException {
+    this(matchVersion, WordlistLoader.getWordSet(stopwordsFile));
+  }
+
+  /** Builds an analyzer with the stop words from the given reader.
+   * @see WordlistLoader#getWordSet(Reader)
+   * @param matchVersion See <a href="#version">above</a>
+   * @param stopwords Reader to load stop words from */
+  public StopAnalyzer(Version matchVersion, Reader stopwords) throws IOException {
+    this(matchVersion, WordlistLoader.getWordSet(stopwords));
+  }
+
+  /**
+   * Creates
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
+   * used to tokenize all the text in the provided {@link Reader}.
+   * 
+   * @return {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
+   *         built from a {@link LowerCaseTokenizer} filtered with
+   *         {@link StopFilter}
+   */
+  @Override
+  protected TokenStreamComponents createComponents(String fieldName,
+      Reader reader) {
+    final Tokenizer source = new LowerCaseTokenizer(matchVersion, reader);
+    return new TokenStreamComponents(source, new StopFilter(matchVersion,
+          source, stopwords));
+  }
+}
+
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/core/StopFilter.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/core/StopFilter.java
new file mode 100644
index 0000000..6f2968a
--- /dev/null
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/core/StopFilter.java
@@ -0,0 +1,312 @@
+package org.apache.lucene.analysis.core;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.Set;
+import java.util.List;
+
+import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.TokenFilter;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.queryParser.QueryParser; // for javadoc
+import org.apache.lucene.util.Version;
+
+/**
+ * Removes stop words from a token stream.
+ * 
+ * <a name="version"/>
+ * <p>You must specify the required {@link Version}
+ * compatibility when creating StopFilter:
+ * <ul>
+ *   <li> As of 3.1, StopFilter correctly handles Unicode 4.0
+ *         supplementary characters in stopwords and position
+ *         increments are preserved
+ * </ul>
+ */
+public final class StopFilter extends TokenFilter {
+
+  private final CharArraySet stopWords;
+  private boolean enablePositionIncrements = false;
+
+  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
+  private final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);
+
+  /**
+   * Construct a token stream filtering the given input.
+   * If <code>stopWords</code> is an instance of {@link CharArraySet} (true if
+   * <code>makeStopSet()</code> was used to construct the set) it will be directly used
+   * and <code>ignoreCase</code> will be ignored since <code>CharArraySet</code>
+   * directly controls case sensitivity.
+   * <p/>
+   * If <code>stopWords</code> is not an instance of {@link CharArraySet},
+   * a new CharArraySet will be constructed and <code>ignoreCase</code> will be
+   * used to specify the case sensitivity of that set.
+   *
+   * @param enablePositionIncrements true if token positions should record the removed stop words
+   * @param input Input TokenStream
+   * @param stopWords A Set of Strings or char[] or any other toString()-able set representing the stopwords
+   * @param ignoreCase if true, all words are lower cased first
+   * @deprecated use {@link #StopFilter(Version, TokenStream, Set, boolean)} instead
+   */
+  @Deprecated
+  public StopFilter(boolean enablePositionIncrements, TokenStream input, Set<?> stopWords, boolean ignoreCase)
+  {
+    this(Version.LUCENE_30, enablePositionIncrements, input, stopWords, ignoreCase);
+  }
+  
+  /**
+   * Construct a token stream filtering the given input. If
+   * <code>stopWords</code> is an instance of {@link CharArraySet} (true if
+   * <code>makeStopSet()</code> was used to construct the set) it will be
+   * directly used and <code>ignoreCase</code> will be ignored since
+   * <code>CharArraySet</code> directly controls case sensitivity.
+   * <p/>
+   * If <code>stopWords</code> is not an instance of {@link CharArraySet}, a new
+   * CharArraySet will be constructed and <code>ignoreCase</code> will be used
+   * to specify the case sensitivity of that set.
+   * 
+   * @param matchVersion
+   *          Lucene version to enable correct Unicode 4.0 behavior in the stop
+   *          set if Version > 3.0. See <a href="#version">above</a> for details.
+   * @param input
+   *          Input TokenStream
+   * @param stopWords
+   *          A Set of Strings or char[] or any other toString()-able set
+   *          representing the stopwords
+   * @param ignoreCase
+   *          if true, all words are lower cased first
+   */
+  public StopFilter(Version matchVersion, TokenStream input, Set<?> stopWords, boolean ignoreCase)
+  {
+   this(matchVersion, matchVersion.onOrAfter(Version.LUCENE_29), input, stopWords, ignoreCase);
+  }
+  
+  /*
+   * convenience ctor to enable deprecated ctors to set posInc explicitly
+   */
+  private StopFilter(Version matchVersion, boolean enablePositionIncrements, TokenStream input, Set<?> stopWords, boolean ignoreCase){
+    super(input);
+    this.stopWords = stopWords instanceof CharArraySet ? (CharArraySet)stopWords : new CharArraySet(matchVersion, stopWords, ignoreCase);  
+    this.enablePositionIncrements = enablePositionIncrements;
+  }
+
+  /**
+   * Constructs a filter which removes words from the input
+   * TokenStream that are named in the Set.
+   *
+   * @param enablePositionIncrements true if token positions should record the removed stop words
+   * @param in Input stream
+   * @param stopWords A Set of Strings or char[] or any other toString()-able set representing the stopwords
+   * @see #makeStopSet(Version, java.lang.String[])
+   * @deprecated use {@link #StopFilter(Version, TokenStream, Set)} instead
+   */
+  @Deprecated
+  public StopFilter(boolean enablePositionIncrements, TokenStream in, Set<?> stopWords) {
+    this(Version.LUCENE_CURRENT, enablePositionIncrements, in, stopWords, false);
+  }
+  
+  /**
+   * Constructs a filter which removes words from the input TokenStream that are
+   * named in the Set.
+   * 
+   * @param matchVersion
+   *          Lucene version to enable correct Unicode 4.0 behavior in the stop
+   *          set if Version > 3.0.  See <a href="#version">above</a> for details.
+   * @param in
+   *          Input stream
+   * @param stopWords
+   *          A Set of Strings or char[] or any other toString()-able set
+   *          representing the stopwords
+   * @see #makeStopSet(Version, java.lang.String[])
+   */
+  public StopFilter(Version matchVersion, TokenStream in, Set<?> stopWords) {
+    this(matchVersion, in, stopWords, false);
+  }
+
+  /**
+   * Builds a Set from an array of stop words,
+   * appropriate for passing into the StopFilter constructor.
+   * This permits this stopWords construction to be cached once when
+   * an Analyzer is constructed.
+   * 
+   * @see #makeStopSet(Version, java.lang.String[], boolean) passing false to ignoreCase
+   * @deprecated use {@link #makeStopSet(Version, String...)} instead
+   */
+  @Deprecated
+  public static final Set<Object> makeStopSet(String... stopWords) {
+    return makeStopSet(Version.LUCENE_30, stopWords, false);
+  }
+
+  /**
+   * Builds a Set from an array of stop words,
+   * appropriate for passing into the StopFilter constructor.
+   * This permits this stopWords construction to be cached once when
+   * an Analyzer is constructed.
+   * 
+   * @param matchVersion Lucene version to enable correct Unicode 4.0 behavior in the returned set if Version > 3.0
+   * @param stopWords An array of stopwords
+   * @see #makeStopSet(Version, java.lang.String[], boolean) passing false to ignoreCase
+   */
+  public static final Set<Object> makeStopSet(Version matchVersion, String... stopWords) {
+    return makeStopSet(matchVersion, stopWords, false);
+  }
+  
+  /**
+   * Builds a Set from an array of stop words,
+   * appropriate for passing into the StopFilter constructor.
+   * This permits this stopWords construction to be cached once when
+   * an Analyzer is constructed.
+   * @param stopWords A List of Strings or char[] or any other toString()-able list representing the stopwords
+   * @return A Set ({@link CharArraySet}) containing the words
+   * @see #makeStopSet(Version, java.lang.String[], boolean) passing false to ignoreCase
+   * @deprecated use {@link #makeStopSet(Version, List)} instead
+   */
+  @Deprecated
+  public static final Set<Object> makeStopSet(List<?> stopWords) {
+    return makeStopSet(Version.LUCENE_30, stopWords, false);
+  }
+
+  /**
+   * Builds a Set from an array of stop words,
+   * appropriate for passing into the StopFilter constructor.
+   * This permits this stopWords construction to be cached once when
+   * an Analyzer is constructed.
+   * 
+   * @param matchVersion Lucene version to enable correct Unicode 4.0 behavior in the returned set if Version > 3.0
+   * @param stopWords A List of Strings or char[] or any other toString()-able list representing the stopwords
+   * @return A Set ({@link CharArraySet}) containing the words
+   * @see #makeStopSet(Version, java.lang.String[], boolean) passing false to ignoreCase
+   */
+  public static final Set<Object> makeStopSet(Version matchVersion, List<?> stopWords) {
+    return makeStopSet(matchVersion, stopWords, false);
+  }
+    
+  /**
+   * Creates a stopword set from the given stopword array.
+   * @param stopWords An array of stopwords
+   * @param ignoreCase If true, all words are lower cased first.  
+   * @return a Set containing the words
+   * @deprecated use {@link #makeStopSet(Version, String[], boolean)} instead;
+   */  
+  @Deprecated
+  public static final Set<Object> makeStopSet(String[] stopWords, boolean ignoreCase) {
+    return makeStopSet(Version.LUCENE_30, stopWords, ignoreCase);
+  }
+  /**
+   * Creates a stopword set from the given stopword array.
+   * 
+   * @param matchVersion Lucene version to enable correct Unicode 4.0 behavior in the returned set if Version > 3.0
+   * @param stopWords An array of stopwords
+   * @param ignoreCase If true, all words are lower cased first.  
+   * @return a Set containing the words
+   */    
+  public static final Set<Object> makeStopSet(Version matchVersion, String[] stopWords, boolean ignoreCase) {
+    CharArraySet stopSet = new CharArraySet(matchVersion, stopWords.length, ignoreCase);
+    stopSet.addAll(Arrays.asList(stopWords));
+    return stopSet;
+  }
+  
+  /**
+   * Creates a stopword set from the given stopword list.
+   * @param stopWords A List of Strings or char[] or any other toString()-able list representing the stopwords
+   * @param ignoreCase if true, all words are lower cased first
+   * @return A Set ({@link CharArraySet}) containing the words
+   * @deprecated use {@link #makeStopSet(Version, List, boolean)} instead
+   */
+  @Deprecated
+  public static final Set<Object> makeStopSet(List<?> stopWords, boolean ignoreCase){
+    return makeStopSet(Version.LUCENE_30, stopWords, ignoreCase);
+  }
+
+  /**
+   * Creates a stopword set from the given stopword list.
+   * @param matchVersion Lucene version to enable correct Unicode 4.0 behavior in the returned set if Version > 3.0
+   * @param stopWords A List of Strings or char[] or any other toString()-able list representing the stopwords
+   * @param ignoreCase if true, all words are lower cased first
+   * @return A Set ({@link CharArraySet}) containing the words
+   */
+  public static final Set<Object> makeStopSet(Version matchVersion, List<?> stopWords, boolean ignoreCase){
+    CharArraySet stopSet = new CharArraySet(matchVersion, stopWords.size(), ignoreCase);
+    stopSet.addAll(stopWords);
+    return stopSet;
+  }
+  
+  /**
+   * Returns the next input Token whose term() is not a stop word.
+   */
+  @Override
+  public final boolean incrementToken() throws IOException {
+    // return the first non-stop word found
+    int skippedPositions = 0;
+    while (input.incrementToken()) {
+      if (!stopWords.contains(termAtt.buffer(), 0, termAtt.length())) {
+        if (enablePositionIncrements) {
+          posIncrAtt.setPositionIncrement(posIncrAtt.getPositionIncrement() + skippedPositions);
+        }
+        return true;
+      }
+      skippedPositions += posIncrAtt.getPositionIncrement();
+    }
+    // reached EOS -- return false
+    return false;
+  }
+
+  /**
+   * Returns version-dependent default for
+   * enablePositionIncrements.  Analyzers that embed
+   * StopFilter use this method when creating the
+   * StopFilter.  Prior to 2.9, this returns false.  On 2.9
+   * or later, it returns true.
+   * @deprecated use {@link #StopFilter(Version, TokenStream, Set)} instead
+   */
+  @Deprecated
+  public static boolean getEnablePositionIncrementsVersionDefault(Version matchVersion) {
+    return matchVersion.onOrAfter(Version.LUCENE_29);
+  }
+
+  /**
+   * @see #setEnablePositionIncrements(boolean)
+   */
+  public boolean getEnablePositionIncrements() {
+    return enablePositionIncrements;
+  }
+
+  /**
+   * If <code>true</code>, this StopFilter will preserve
+   * positions of the incoming tokens (ie, accumulate and
+   * set position increments of the removed stop tokens).
+   * Generally, <code>true</code> is best as it does not
+   * lose information (positions of the original tokens)
+   * during indexing.
+   * 
+   * <p> When set, when a token is stopped
+   * (omitted), the position increment of the following
+   * token is incremented.
+   *
+   * <p> <b>NOTE</b>: be sure to also
+   * set {@link QueryParser#setEnablePositionIncrements} if
+   * you use QueryParser to create queries.
+   */
+  public void setEnablePositionIncrements(boolean enable) {
+    this.enablePositionIncrements = enable;
+  }
+}
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/core/WhitespaceAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/core/WhitespaceAnalyzer.java
new file mode 100644
index 0000000..300a021
--- /dev/null
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/core/WhitespaceAnalyzer.java
@@ -0,0 +1,65 @@
+package org.apache.lucene.analysis.core;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Reader;
+
+import org.apache.lucene.analysis.CharTokenizer;
+import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
+import org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents;
+import org.apache.lucene.util.Version;
+
+/**
+ * An Analyzer that uses {@link WhitespaceTokenizer}.
+ * <p>
+ * <a name="version">You must specify the required {@link Version} compatibility
+ * when creating {@link CharTokenizer}:
+ * <ul>
+ * <li>As of 3.1, {@link WhitespaceTokenizer} uses an int based API to normalize and
+ * detect token codepoints. See {@link CharTokenizer#isTokenChar(int)} and
+ * {@link CharTokenizer#normalize(int)} for details.</li>
+ * </ul>
+ * <p>
+ **/
+public final class WhitespaceAnalyzer extends ReusableAnalyzerBase {
+  
+  private final Version matchVersion;
+  
+  /**
+   * Creates a new {@link WhitespaceAnalyzer}
+   * @param matchVersion Lucene version to match See {@link <a href="#version">above</a>}
+   */
+  public WhitespaceAnalyzer(Version matchVersion) {
+    this.matchVersion = matchVersion;
+  }
+  
+  /**
+   * Creates a new {@link WhitespaceAnalyzer}
+   * @deprecated use {@link #WhitespaceAnalyzer(Version)} instead 
+   */
+  @Deprecated
+  public WhitespaceAnalyzer() {
+    this(Version.LUCENE_30);
+  }
+  
+  @Override
+  protected TokenStreamComponents createComponents(final String fieldName,
+      final Reader reader) {
+    return new TokenStreamComponents(new WhitespaceTokenizer(matchVersion, reader));
+  }
+}
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/core/WhitespaceTokenizer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/core/WhitespaceTokenizer.java
new file mode 100644
index 0000000..d3d6b5e
--- /dev/null
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/core/WhitespaceTokenizer.java
@@ -0,0 +1,123 @@
+package org.apache.lucene.analysis.core;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Reader;
+
+import org.apache.lucene.analysis.CharTokenizer;
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.util.AttributeSource;
+import org.apache.lucene.util.Version;
+
+/**
+ * A WhitespaceTokenizer is a tokenizer that divides text at whitespace.
+ * Adjacent sequences of non-Whitespace characters form tokens. <a
+ * name="version"/>
+ * <p>
+ * You must specify the required {@link Version} compatibility when creating
+ * {@link WhitespaceTokenizer}:
+ * <ul>
+ * <li>As of 3.1, {@link CharTokenizer} uses an int based API to normalize and
+ * detect token characters. See {@link CharTokenizer#isTokenChar(int)} and
+ * {@link CharTokenizer#normalize(int)} for details.</li>
+ * </ul>
+ */
+public final class WhitespaceTokenizer extends CharTokenizer {
+  
+  /**
+   * Construct a new WhitespaceTokenizer. * @param matchVersion Lucene version
+   * to match See {@link <a href="#version">above</a>}
+   * 
+   * @param in
+   *          the input to split up into tokens
+   */
+  public WhitespaceTokenizer(Version matchVersion, Reader in) {
+    super(matchVersion, in);
+  }
+
+  /**
+   * Construct a new WhitespaceTokenizer using a given {@link AttributeSource}.
+   * 
+   * @param matchVersion
+   *          Lucene version to match See {@link <a href="#version">above</a>}
+   * @param source
+   *          the attribute source to use for this {@link Tokenizer}
+   * @param in
+   *          the input to split up into tokens
+   */
+  public WhitespaceTokenizer(Version matchVersion, AttributeSource source, Reader in) {
+    super(matchVersion, source, in);
+  }
+
+  /**
+   * Construct a new WhitespaceTokenizer using a given
+   * {@link org.apache.lucene.util.AttributeSource.AttributeFactory}.
+   *
+   * @param
+   *          matchVersion Lucene version to match See
+   *          {@link <a href="#version">above</a>}
+   * @param factory
+   *          the attribute factory to use for this {@link Tokenizer}
+   * @param in
+   *          the input to split up into tokens
+   */
+  public WhitespaceTokenizer(Version matchVersion, AttributeFactory factory, Reader in) {
+    super(matchVersion, factory, in);
+  }
+  
+  /**
+   * Construct a new WhitespaceTokenizer.
+   * 
+   * @deprecated use {@link #WhitespaceTokenizer(Version, Reader)} instead. This will
+   *             be removed in Lucene 4.0.
+   */
+  @Deprecated
+  public WhitespaceTokenizer(Reader in) {
+    super(in);
+  }
+
+  /**
+   * Construct a new WhitespaceTokenizer using a given {@link AttributeSource}.
+   * 
+   * @deprecated use {@link #WhitespaceTokenizer(Version, AttributeSource, Reader)}
+   *             instead. This will be removed in Lucene 4.0.
+   */
+  @Deprecated
+  public WhitespaceTokenizer(AttributeSource source, Reader in) {
+    super(source, in);
+  }
+
+  /**
+   * Construct a new WhitespaceTokenizer using a given
+   * {@link org.apache.lucene.util.AttributeSource.AttributeFactory}.
+   * 
+   * @deprecated use {@link #WhitespaceTokenizer(Version, AttributeSource.AttributeFactory, Reader)}
+   *             instead. This will be removed in Lucene 4.0.
+   */
+  @Deprecated
+  public WhitespaceTokenizer(AttributeFactory factory, Reader in) {
+    super(factory, in);
+  }
+  
+  /** Collects only characters which do not satisfy
+   * {@link Character#isWhitespace(int)}.*/
+  @Override
+  protected boolean isTokenChar(int c) {
+    return !Character.isWhitespace(c);
+  }
+}
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/cz/CzechAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/cz/CzechAnalyzer.java
index 7204493..3437ec7 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/cz/CzechAnalyzer.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/cz/CzechAnalyzer.java
@@ -17,17 +17,17 @@ package org.apache.lucene.analysis.cz;
  * limitations under the License.
  */
 
-import org.apache.lucene.analysis.ReusableAnalyzerBase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
-import org.apache.lucene.analysis.StopFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 
 import java.io.*;
@@ -218,10 +218,10 @@ public final class CzechAnalyzer extends ReusableAnalyzerBase {
 
   /**
    * Creates
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * used to tokenize all the text in the provided {@link Reader}.
    * 
-   * @return {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * @return {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from a {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
    *         , and {@link CzechStemFilter} (only if version is >= LUCENE_31). If
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/da/DanishAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/da/DanishAnalyzer.java
index 70990ad..b0a7cab 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/da/DanishAnalyzer.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/da/DanishAnalyzer.java
@@ -23,16 +23,16 @@ import java.util.Set;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.snowball.SnowballFilter;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 import org.tartarus.snowball.ext.DanishStemmer;
 
@@ -106,11 +106,11 @@ public final class DanishAnalyzer extends StopwordAnalyzerBase {
 
   /**
    * Creates a
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * which tokenizes all the text in the provided {@link Reader}.
    * 
    * @return A
-   *         {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   *         {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from an {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
    *         , {@link KeywordMarkerFilter} if a stem exclusion set is
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/de/GermanAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/de/GermanAnalyzer.java
index 27696c8..3f23b29 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/de/GermanAnalyzer.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/de/GermanAnalyzer.java
@@ -28,17 +28,17 @@ import java.util.Set;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
-import org.apache.lucene.analysis.LowerCaseFilter;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.snowball.SnowballFilter;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 import org.tartarus.snowball.ext.German2Stemmer;
 
@@ -224,10 +224,10 @@ public final class GermanAnalyzer extends StopwordAnalyzerBase {
 
   /**
    * Creates
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * used to tokenize all the text in the provided {@link Reader}.
    * 
-   * @return {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * @return {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from a {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
    *         , {@link KeywordMarkerFilter} if a stem exclusion set is
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/el/GreekAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/el/GreekAnalyzer.java
index 4dcf341..b2a4c0b 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/el/GreekAnalyzer.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/el/GreekAnalyzer.java
@@ -17,13 +17,13 @@ package org.apache.lucene.analysis.el;
  */
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;  // for javadoc
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
 import org.apache.lucene.util.Version;
 
 import java.io.IOException;
@@ -121,10 +121,10 @@ public final class GreekAnalyzer extends StopwordAnalyzerBase {
   
   /**
    * Creates
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * used to tokenize all the text in the provided {@link Reader}.
    * 
-   * @return {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * @return {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from a {@link StandardTokenizer} filtered with
    *         {@link GreekLowerCaseFilter}, {@link StandardFilter},
    *         {@link StopFilter}, and {@link GreekStemFilter}
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/en/EnglishAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/en/EnglishAnalyzer.java
index b326816..75d19ed 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/en/EnglishAnalyzer.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/en/EnglishAnalyzer.java
@@ -22,15 +22,15 @@ import java.util.Set;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
 import org.apache.lucene.util.Version;
 
 /**
@@ -89,11 +89,11 @@ public final class EnglishAnalyzer extends StopwordAnalyzerBase {
 
   /**
    * Creates a
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * which tokenizes all the text in the provided {@link Reader}.
    * 
    * @return A
-   *         {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   *         {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from an {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
    *         , {@link KeywordMarkerFilter} if a stem exclusion set is
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/es/SpanishAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/es/SpanishAnalyzer.java
index e4bbd45..275ad43 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/es/SpanishAnalyzer.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/es/SpanishAnalyzer.java
@@ -23,16 +23,16 @@ import java.util.Set;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.snowball.SnowballFilter;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 import org.tartarus.snowball.ext.SpanishStemmer;
 
@@ -106,11 +106,11 @@ public final class SpanishAnalyzer extends StopwordAnalyzerBase {
 
   /**
    * Creates a
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * which tokenizes all the text in the provided {@link Reader}.
    * 
    * @return A
-   *         {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   *         {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from an {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
    *         , {@link KeywordMarkerFilter} if a stem exclusion set is
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/fa/PersianAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/fa/PersianAnalyzer.java
index 62f1795..88eea5b 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/fa/PersianAnalyzer.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/fa/PersianAnalyzer.java
@@ -24,14 +24,14 @@ import java.util.Hashtable;
 import java.util.Set;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.LowerCaseFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.ar.ArabicLetterTokenizer;
 import org.apache.lucene.analysis.ar.ArabicNormalizationFilter;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 
 /**
@@ -136,10 +136,10 @@ public final class PersianAnalyzer extends StopwordAnalyzerBase {
 
   /**
    * Creates
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * used to tokenize all the text in the provided {@link Reader}.
    * 
-   * @return {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * @return {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from a {@link ArabicLetterTokenizer} filtered with
    *         {@link LowerCaseFilter}, {@link ArabicNormalizationFilter},
    *         {@link PersianNormalizationFilter} and Persian Stop words
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/fi/FinnishAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/fi/FinnishAnalyzer.java
index 9589677..49f7f07 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/fi/FinnishAnalyzer.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/fi/FinnishAnalyzer.java
@@ -23,16 +23,16 @@ import java.util.Set;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.snowball.SnowballFilter;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 import org.tartarus.snowball.ext.FinnishStemmer;
 
@@ -106,11 +106,11 @@ public final class FinnishAnalyzer extends StopwordAnalyzerBase {
 
   /**
    * Creates a
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * which tokenizes all the text in the provided {@link Reader}.
    * 
    * @return A
-   *         {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   *         {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from an {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
    *         , {@link KeywordMarkerFilter} if a stem exclusion set is
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/fr/FrenchAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/fr/FrenchAnalyzer.java
index f68c5c2..96df139 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/fr/FrenchAnalyzer.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/fr/FrenchAnalyzer.java
@@ -19,17 +19,17 @@ package org.apache.lucene.analysis.fr;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
-import org.apache.lucene.analysis.LowerCaseFilter;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.snowball.SnowballFilter;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;  // for javadoc
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 
 import java.io.File;
@@ -225,10 +225,10 @@ public final class FrenchAnalyzer extends StopwordAnalyzerBase {
 
   /**
    * Creates
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * used to tokenize all the text in the provided {@link Reader}.
    * 
-   * @return {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * @return {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from a {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link ElisionFilter},
    *         {@link LowerCaseFilter}, {@link StopFilter},
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/hi/HindiAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/hi/HindiAnalyzer.java
index 2da7ed8..e2c3209 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/hi/HindiAnalyzer.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/hi/HindiAnalyzer.java
@@ -21,13 +21,13 @@ import java.io.IOException;
 import java.io.Reader;
 import java.util.Set;
 
-import org.apache.lucene.analysis.LowerCaseFilter;
 import org.apache.lucene.analysis.CharArraySet;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.in.IndicNormalizationFilter;
 import org.apache.lucene.analysis.in.IndicTokenizer;
 import org.apache.lucene.util.Version;
@@ -106,10 +106,10 @@ public final class HindiAnalyzer extends StopwordAnalyzerBase {
 
   /**
    * Creates
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * used to tokenize all the text in the provided {@link Reader}.
    * 
-   * @return {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * @return {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from a {@link IndicTokenizer} filtered with
    *         {@link LowerCaseFilter}, {@link IndicNormalizationFilter},
    *         {@link HindiNormalizationFilter}, {@link KeywordMarkerFilter}
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/hu/HungarianAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/hu/HungarianAnalyzer.java
index 10c8f73..a51d433 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/hu/HungarianAnalyzer.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/hu/HungarianAnalyzer.java
@@ -23,16 +23,16 @@ import java.util.Set;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.snowball.SnowballFilter;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 import org.tartarus.snowball.ext.HungarianStemmer;
 
@@ -106,11 +106,11 @@ public final class HungarianAnalyzer extends StopwordAnalyzerBase {
 
   /**
    * Creates a
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * which tokenizes all the text in the provided {@link Reader}.
    * 
    * @return A
-   *         {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   *         {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from an {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
    *         , {@link KeywordMarkerFilter} if a stem exclusion set is
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/id/IndonesianAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/id/IndonesianAnalyzer.java
index 6d0b783..f7ff58e 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/id/IndonesianAnalyzer.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/id/IndonesianAnalyzer.java
@@ -22,14 +22,14 @@ import java.io.Reader;
 import java.util.Set;
 
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
 import org.apache.lucene.util.Version;
 
 /**
@@ -106,10 +106,10 @@ public final class IndonesianAnalyzer extends StopwordAnalyzerBase {
 
   /**
    * Creates
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * used to tokenize all the text in the provided {@link Reader}.
    * 
-   * @return {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * @return {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from an {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter},
    *         {@link StopFilter}, {@link KeywordMarkerFilter}
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/it/ItalianAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/it/ItalianAnalyzer.java
index 2bb51f9..cec09c2 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/it/ItalianAnalyzer.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/it/ItalianAnalyzer.java
@@ -23,16 +23,16 @@ import java.util.Set;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.snowball.SnowballFilter;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 import org.tartarus.snowball.ext.ItalianStemmer;
 
@@ -106,11 +106,11 @@ public final class ItalianAnalyzer extends StopwordAnalyzerBase {
 
   /**
    * Creates a
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * which tokenizes all the text in the provided {@link Reader}.
    * 
    * @return A
-   *         {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   *         {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from an {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
    *         , {@link KeywordMarkerFilter} if a stem exclusion set is
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/PatternAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/PatternAnalyzer.java
index ad5fa2d..0995396 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/PatternAnalyzer.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/PatternAnalyzer.java
@@ -28,9 +28,9 @@ import java.util.regex.Pattern;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
-import org.apache.lucene.analysis.StopAnalyzer;
-import org.apache.lucene.analysis.StopFilter;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.core.StopAnalyzer;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.util.Version;
@@ -40,10 +40,10 @@ import org.apache.lucene.util.Version;
  * {@link java.io.Reader}, that can flexibly separate text into terms via a regular expression {@link Pattern}
  * (with behaviour identical to {@link String#split(String)}),
  * and that combines the functionality of
- * {@link org.apache.lucene.analysis.LetterTokenizer},
- * {@link org.apache.lucene.analysis.LowerCaseTokenizer},
- * {@link org.apache.lucene.analysis.WhitespaceTokenizer},
- * {@link org.apache.lucene.analysis.StopFilter} into a single efficient
+ * {@link org.apache.lucene.analysis.core.LetterTokenizer},
+ * {@link org.apache.lucene.analysis.core.LowerCaseTokenizer},
+ * {@link org.apache.lucene.analysis.core.WhitespaceTokenizer},
+ * {@link org.apache.lucene.analysis.core.StopFilter} into a single efficient
  * multi-purpose class.
  * <p>
  * If you are unsure how exactly a regular expression should look like, consider 
@@ -157,7 +157,7 @@ public final class PatternAnalyzer extends Analyzer {
    *            given stop set (after previously having applied toLowerCase()
    *            if applicable). For example, created via
    *            {@link StopFilter#makeStopSet(Version, String[])}and/or
-   *            {@link org.apache.lucene.analysis.WordlistLoader}as in
+   *            {@link org.apache.lucene.analysis.util.WordlistLoader}as in
    *            <code>WordlistLoader.getWordSet(new File("samples/fulltext/stopwords.txt")</code>
    *            or <a href="http://www.unine.ch/info/clef/">other stop words
    *            lists </a>.
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/nl/DutchAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/nl/DutchAnalyzer.java
index 65db885..8224c6e 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/nl/DutchAnalyzer.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/nl/DutchAnalyzer.java
@@ -19,18 +19,18 @@ package org.apache.lucene.analysis.nl;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
-import org.apache.lucene.analysis.ReusableAnalyzerBase;
-import org.apache.lucene.analysis.StopFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.miscellaneous.StemmerOverrideFilter;
 import org.apache.lucene.analysis.snowball.SnowballFilter;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;  // for javadoc
+import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 
 import java.io.File;
@@ -171,7 +171,7 @@ public final class DutchAnalyzer extends ReusableAnalyzerBase {
   public DutchAnalyzer(Version matchVersion, File stopwords) {
     // this is completely broken!
     try {
-      stoptable = org.apache.lucene.analysis.WordlistLoader.getWordSet(stopwords);
+      stoptable = org.apache.lucene.analysis.util.WordlistLoader.getWordSet(stopwords);
     } catch (IOException e) {
       // TODO: throw IOException
       throw new RuntimeException(e);
@@ -208,7 +208,7 @@ public final class DutchAnalyzer extends ReusableAnalyzerBase {
   @Deprecated
   public void setStemExclusionTable(File exclusionlist) {
     try {
-      excltable = org.apache.lucene.analysis.WordlistLoader.getWordSet(exclusionlist);
+      excltable = org.apache.lucene.analysis.util.WordlistLoader.getWordSet(exclusionlist);
       setPreviousTokenStream(null); // force a new stemmer to be created
     } catch (IOException e) {
       // TODO: throw IOException
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/no/NorwegianAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/no/NorwegianAnalyzer.java
index fcf3042..1ad3111 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/no/NorwegianAnalyzer.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/no/NorwegianAnalyzer.java
@@ -23,16 +23,16 @@ import java.util.Set;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.snowball.SnowballFilter;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 import org.tartarus.snowball.ext.NorwegianStemmer;
 
@@ -106,11 +106,11 @@ public final class NorwegianAnalyzer extends StopwordAnalyzerBase {
 
   /**
    * Creates a
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * which tokenizes all the text in the provided {@link Reader}.
    * 
    * @return A
-   *         {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   *         {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from an {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
    *         , {@link KeywordMarkerFilter} if a stem exclusion set is
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/pt/PortugueseAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/pt/PortugueseAnalyzer.java
index 4a521c8..7bd7761 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/pt/PortugueseAnalyzer.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/pt/PortugueseAnalyzer.java
@@ -23,16 +23,16 @@ import java.util.Set;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.snowball.SnowballFilter;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 import org.tartarus.snowball.ext.PortugueseStemmer;
 
@@ -106,11 +106,11 @@ public final class PortugueseAnalyzer extends StopwordAnalyzerBase {
 
   /**
    * Creates a
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * which tokenizes all the text in the provided {@link Reader}.
    * 
    * @return A
-   *         {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   *         {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from an {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
    *         , {@link KeywordMarkerFilter} if a stem exclusion set is
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer.java
index 3e45463..41720e6 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer.java
@@ -21,7 +21,7 @@ import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermEnum;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.StopFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.util.StringHelper;
 import org.apache.lucene.util.Version;
 
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/ro/RomanianAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/ro/RomanianAnalyzer.java
index e065525..a06d222 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/ro/RomanianAnalyzer.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/ro/RomanianAnalyzer.java
@@ -23,15 +23,15 @@ import java.util.Set;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.snowball.SnowballFilter;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
 import org.apache.lucene.util.Version;
 import org.tartarus.snowball.ext.RomanianStemmer;
 
@@ -110,11 +110,11 @@ public final class RomanianAnalyzer extends StopwordAnalyzerBase {
 
   /**
    * Creates a
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * which tokenizes all the text in the provided {@link Reader}.
    * 
    * @return A
-   *         {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   *         {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from an {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
    *         , {@link KeywordMarkerFilter} if a stem exclusion set is
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianAnalyzer.java
index 7d59b85..1b94cdb 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianAnalyzer.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianAnalyzer.java
@@ -25,16 +25,16 @@ import java.util.Set;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
-import org.apache.lucene.analysis.LowerCaseFilter;
 import org.apache.lucene.analysis.snowball.SnowballFilter;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.util.Version;
 
 /**
@@ -161,10 +161,10 @@ public final class RussianAnalyzer extends StopwordAnalyzerBase
 
   /**
    * Creates
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * used to tokenize all the text in the provided {@link Reader}.
    * 
-   * @return {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * @return {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from a {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
    *         , {@link KeywordMarkerFilter} if a stem exclusion set is
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianLetterTokenizer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianLetterTokenizer.java
index 967c8eb..1a244e4 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianLetterTokenizer.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianLetterTokenizer.java
@@ -20,7 +20,7 @@ package org.apache.lucene.analysis.ru;
 import java.io.Reader;
 import org.apache.lucene.analysis.CharTokenizer;
 import org.apache.lucene.analysis.Tokenizer; // for javadocs
-import org.apache.lucene.analysis.LetterTokenizer; // for javadocs
+import org.apache.lucene.analysis.core.LetterTokenizer;
 import org.apache.lucene.analysis.standard.StandardTokenizer; // for javadocs
 import org.apache.lucene.util.AttributeSource;
 import org.apache.lucene.util.Version;
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianLowerCaseFilter.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianLowerCaseFilter.java
index 5beec03..6b96e16 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianLowerCaseFilter.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianLowerCaseFilter.java
@@ -19,9 +19,9 @@ package org.apache.lucene.analysis.ru;
 
 import java.io.IOException;
 
-import org.apache.lucene.analysis.LowerCaseFilter; // for javadoc
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
 /**
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianStemFilter.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianStemFilter.java
index 3cdb5d2..11655a8 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianStemFilter.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianStemFilter.java
@@ -17,8 +17,8 @@ package org.apache.lucene.analysis.ru;
  * limitations under the License.
  */
 
+import org.apache.lucene.analysis.core.LowerCaseFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter; // for javadoc
-import org.apache.lucene.analysis.LowerCaseFilter; // for javadoc
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.KeywordAttribute;
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/snowball/SnowballAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/snowball/SnowballAnalyzer.java
index a718472..ed0306b 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/snowball/SnowballAnalyzer.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/snowball/SnowballAnalyzer.java
@@ -18,6 +18,8 @@ package org.apache.lucene.analysis.snowball;
  */
 
 import org.apache.lucene.analysis.*;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.standard.*;
 import org.apache.lucene.analysis.tr.TurkishLowerCaseFilter;
 import org.apache.lucene.util.Version;
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/snowball/SnowballFilter.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/snowball/SnowballFilter.java
index 52a49b5..f2e8894 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/snowball/SnowballFilter.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/snowball/SnowballFilter.java
@@ -21,10 +21,10 @@ import java.io.IOException;
 
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
 import org.apache.lucene.analysis.tokenattributes.KeywordAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.analysis.tr.TurkishLowerCaseFilter; // javadoc @link
-import org.apache.lucene.analysis.LowerCaseFilter; // javadoc @link
 import org.tartarus.snowball.SnowballProgram;
 
 /**
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/READ_BEFORE_REGENERATING.txt b/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/READ_BEFORE_REGENERATING.txt
new file mode 100644
index 0000000..c01c036
--- /dev/null
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/READ_BEFORE_REGENERATING.txt
@@ -0,0 +1,21 @@
+/*
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+*/
+
+
+WARNING: if you change StandardTokenizerImpl*.jflex and need to regenerate
+      the tokenizer, only use the trunk version of JFlex 1.5 (with a minimum
+      SVN revision 591) at the moment!
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardAnalyzer.java
new file mode 100644
index 0000000..93e8ab8
--- /dev/null
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardAnalyzer.java
@@ -0,0 +1,136 @@
+package org.apache.lucene.analysis.standard;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.analysis.*;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopAnalyzer;
+import org.apache.lucene.analysis.core.StopFilter;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
+import org.apache.lucene.util.Version;
+
+import java.io.File;
+import java.io.IOException;
+import java.io.Reader;
+import java.util.Set;
+
+/**
+ * Filters {@link StandardTokenizer} with {@link StandardFilter}, {@link
+ * LowerCaseFilter} and {@link StopFilter}, using a list of
+ * English stop words.
+ *
+ * <a name="version"/>
+ * <p>You must specify the required {@link Version}
+ * compatibility when creating StandardAnalyzer:
+ * <ul>
+ *   <li> As of 3.1, StopFilter correctly handles Unicode 4.0
+ *         supplementary characters in stopwords
+ *   <li> As of 2.9, StopFilter preserves position
+ *        increments
+ *   <li> As of 2.4, Tokens incorrectly identified as acronyms
+ *        are corrected (see <a href="https://issues.apache.org/jira/browse/LUCENE-1068">LUCENE-1068</a>)
+ * </ul>
+ */
+public final class StandardAnalyzer extends StopwordAnalyzerBase {
+
+  /** Default maximum allowed token length */
+  public static final int DEFAULT_MAX_TOKEN_LENGTH = 255;
+
+  private int maxTokenLength = DEFAULT_MAX_TOKEN_LENGTH;
+
+  /**
+   * Specifies whether deprecated acronyms should be replaced with HOST type.
+   * See {@linkplain "https://issues.apache.org/jira/browse/LUCENE-1068"}
+   */
+  private final boolean replaceInvalidAcronym;
+
+  /** An unmodifiable set containing some common English words that are usually not
+  useful for searching. */
+  public static final Set<?> STOP_WORDS_SET = StopAnalyzer.ENGLISH_STOP_WORDS_SET; 
+
+  /** Builds an analyzer with the given stop words.
+   * @param matchVersion Lucene version to match See {@link
+   * <a href="#version">above</a>}
+   * @param stopWords stop words */
+  public StandardAnalyzer(Version matchVersion, Set<?> stopWords) {
+    super(matchVersion, stopWords);
+    replaceInvalidAcronym = matchVersion.onOrAfter(Version.LUCENE_24);
+  }
+
+  /** Builds an analyzer with the default stop words ({@link
+   * #STOP_WORDS_SET}).
+   * @param matchVersion Lucene version to match See {@link
+   * <a href="#version">above</a>}
+   */
+  public StandardAnalyzer(Version matchVersion) {
+    this(matchVersion, STOP_WORDS_SET);
+  }
+
+  /** Builds an analyzer with the stop words from the given file.
+   * @see WordlistLoader#getWordSet(File)
+   * @param matchVersion Lucene version to match See {@link
+   * <a href="#version">above</a>}
+   * @param stopwords File to read stop words from */
+  public StandardAnalyzer(Version matchVersion, File stopwords) throws IOException {
+    this(matchVersion, WordlistLoader.getWordSet(stopwords));
+  }
+
+  /** Builds an analyzer with the stop words from the given reader.
+   * @see WordlistLoader#getWordSet(Reader)
+   * @param matchVersion Lucene version to match See {@link
+   * <a href="#version">above</a>}
+   * @param stopwords Reader to read stop words from */
+  public StandardAnalyzer(Version matchVersion, Reader stopwords) throws IOException {
+    this(matchVersion, WordlistLoader.getWordSet(stopwords));
+  }
+
+  /**
+   * Set maximum allowed token length.  If a token is seen
+   * that exceeds this length then it is discarded.  This
+   * setting only takes effect the next time tokenStream or
+   * reusableTokenStream is called.
+   */
+  public void setMaxTokenLength(int length) {
+    maxTokenLength = length;
+  }
+    
+  /**
+   * @see #setMaxTokenLength
+   */
+  public int getMaxTokenLength() {
+    return maxTokenLength;
+  }
+
+  @Override
+  protected TokenStreamComponents createComponents(final String fieldName, final Reader reader) {
+    final StandardTokenizer src = new StandardTokenizer(matchVersion, reader);
+    src.setMaxTokenLength(maxTokenLength);
+    src.setReplaceInvalidAcronym(replaceInvalidAcronym);
+    TokenStream tok = new StandardFilter(src);
+    tok = new LowerCaseFilter(matchVersion, tok);
+    tok = new StopFilter(matchVersion, tok, stopwords);
+    return new TokenStreamComponents(src, tok) {
+      @Override
+      protected boolean reset(final Reader reader) throws IOException {
+        src.setMaxTokenLength(StandardAnalyzer.this.maxTokenLength);
+        return super.reset(reader);
+      }
+    };
+  }
+}
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardFilter.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardFilter.java
new file mode 100644
index 0000000..b6394e5
--- /dev/null
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardFilter.java
@@ -0,0 +1,73 @@
+package org.apache.lucene.analysis.standard;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.analysis.TokenFilter;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
+
+/** Normalizes tokens extracted with {@link StandardTokenizer}. */
+
+public final class StandardFilter extends TokenFilter {
+
+  /** Construct filtering <i>in</i>. */
+  public StandardFilter(TokenStream in) {
+    super(in);
+  }
+
+  private static final String APOSTROPHE_TYPE = StandardTokenizer.TOKEN_TYPES[StandardTokenizer.APOSTROPHE];
+  private static final String ACRONYM_TYPE = StandardTokenizer.TOKEN_TYPES[StandardTokenizer.ACRONYM];
+
+  // this filters uses attribute type
+  private final TypeAttribute typeAtt = addAttribute(TypeAttribute.class);
+  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
+  
+  /** Returns the next token in the stream, or null at EOS.
+   * <p>Removes <tt>'s</tt> from the end of words.
+   * <p>Removes dots from acronyms.
+   */
+  @Override
+  public final boolean incrementToken() throws java.io.IOException {
+    if (!input.incrementToken()) {
+      return false;
+    }
+
+    final char[] buffer = termAtt.buffer();
+    final int bufferLength = termAtt.length();
+    final String type = typeAtt.type();
+
+    if (type == APOSTROPHE_TYPE &&      // remove 's
+        bufferLength >= 2 &&
+        buffer[bufferLength-2] == '\'' &&
+        (buffer[bufferLength-1] == 's' || buffer[bufferLength-1] == 'S')) {
+      // Strip last 2 characters off
+      termAtt.setLength(bufferLength - 2);
+    } else if (type == ACRONYM_TYPE) {      // remove dots
+      int upto = 0;
+      for(int i=0;i<bufferLength;i++) {
+        char c = buffer[i];
+        if (c != '.')
+          buffer[upto++] = c;
+      }
+      termAtt.setLength(upto);
+    }
+
+    return true;
+  }
+}
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizer.java
new file mode 100644
index 0000000..e790171
--- /dev/null
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizer.java
@@ -0,0 +1,230 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.analysis.standard;
+
+import java.io.IOException;
+import java.io.Reader;
+
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
+import org.apache.lucene.util.AttributeSource;
+import org.apache.lucene.util.Version;
+
+/** A grammar-based tokenizer constructed with JFlex
+ *
+ * <p> This should be a good tokenizer for most European-language documents:
+ *
+ * <ul>
+ *   <li>Splits words at punctuation characters, removing punctuation. However, a 
+ *     dot that's not followed by whitespace is considered part of a token.
+ *   <li>Splits words at hyphens, unless there's a number in the token, in which case
+ *     the whole token is interpreted as a product number and is not split.
+ *   <li>Recognizes email addresses and internet hostnames as one token.
+ * </ul>
+ *
+ * <p>Many applications have specific tokenizer needs.  If this tokenizer does
+ * not suit your application, please consider copying this source code
+ * directory to your project and maintaining your own grammar-based tokenizer.
+ *
+ * <a name="version"/>
+ * <p>You must specify the required {@link Version}
+ * compatibility when creating StandardAnalyzer:
+ * <ul>
+ *   <li> As of 2.4, Tokens incorrectly identified as acronyms
+ *        are corrected (see <a href="https://issues.apache.org/jira/browse/LUCENE-1068">LUCENE-1608</a>
+ * </ul>
+ */
+
+public final class StandardTokenizer extends Tokenizer {
+  /** A private instance of the JFlex-constructed scanner */
+  private StandardTokenizerInterface scanner;
+
+  public static final int ALPHANUM          = 0;
+  public static final int APOSTROPHE        = 1;
+  public static final int ACRONYM           = 2;
+  public static final int COMPANY           = 3;
+  public static final int EMAIL             = 4;
+  public static final int HOST              = 5;
+  public static final int NUM               = 6;
+  public static final int CJ                = 7;
+
+  /**
+   * @deprecated this solves a bug where HOSTs that end with '.' are identified
+   *             as ACRONYMs.
+   */
+  @Deprecated
+  public static final int ACRONYM_DEP       = 8;
+
+  /** String token types that correspond to token type int constants */
+  public static final String [] TOKEN_TYPES = new String [] {
+    "<ALPHANUM>",
+    "<APOSTROPHE>",
+    "<ACRONYM>",
+    "<COMPANY>",
+    "<EMAIL>",
+    "<HOST>",
+    "<NUM>",
+    "<CJ>",
+    "<ACRONYM_DEP>"
+  };
+
+  private boolean replaceInvalidAcronym;
+    
+  private int maxTokenLength = StandardAnalyzer.DEFAULT_MAX_TOKEN_LENGTH;
+
+  /** Set the max allowed token length.  Any token longer
+   *  than this is skipped. */
+  public void setMaxTokenLength(int length) {
+    this.maxTokenLength = length;
+  }
+
+  /** @see #setMaxTokenLength */
+  public int getMaxTokenLength() {
+    return maxTokenLength;
+  }
+
+  /**
+   * Creates a new instance of the {@link org.apache.lucene.analysis.standard.StandardTokenizer}.  Attaches
+   * the <code>input</code> to the newly created JFlex scanner.
+   *
+   * @param input The input reader
+   *
+   * See http://issues.apache.org/jira/browse/LUCENE-1068
+   */
+  public StandardTokenizer(Version matchVersion, Reader input) {
+    super();
+    init(input, matchVersion);
+  }
+
+  /**
+   * Creates a new StandardTokenizer with a given {@link AttributeSource}. 
+   */
+  public StandardTokenizer(Version matchVersion, AttributeSource source, Reader input) {
+    super(source);
+    init(input, matchVersion);
+  }
+
+  /**
+   * Creates a new StandardTokenizer with a given {@link org.apache.lucene.util.AttributeSource.AttributeFactory} 
+   */
+  public StandardTokenizer(Version matchVersion, AttributeFactory factory, Reader input) {
+    super(factory);
+    init(input, matchVersion);
+  }
+
+  private final void init(Reader input, Version matchVersion) {
+    this.scanner = matchVersion.onOrAfter(Version.LUCENE_31) ?
+      new StandardTokenizerImpl31(input) : new StandardTokenizerImplOrig(input);
+    if (matchVersion.onOrAfter(Version.LUCENE_24)) {
+      replaceInvalidAcronym = true;
+    } else {
+      replaceInvalidAcronym = false;
+    }
+    this.input = input;    
+  }
+
+  // this tokenizer generates three attributes:
+  // term offset, positionIncrement and type
+  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
+  private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
+  private final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);
+  private final TypeAttribute typeAtt = addAttribute(TypeAttribute.class);
+
+  /*
+   * (non-Javadoc)
+   *
+   * @see org.apache.lucene.analysis.TokenStream#next()
+   */
+  @Override
+  public final boolean incrementToken() throws IOException {
+    clearAttributes();
+    int posIncr = 1;
+
+    while(true) {
+      int tokenType = scanner.getNextToken();
+
+      if (tokenType == StandardTokenizerInterface.YYEOF) {
+        return false;
+      }
+
+      if (scanner.yylength() <= maxTokenLength) {
+        posIncrAtt.setPositionIncrement(posIncr);
+        scanner.getText(termAtt);
+        final int start = scanner.yychar();
+        offsetAtt.setOffset(correctOffset(start), correctOffset(start+termAtt.length()));
+        // This 'if' should be removed in the next release. For now, it converts
+        // invalid acronyms to HOST. When removed, only the 'else' part should
+        // remain.
+        if (tokenType == StandardTokenizer.ACRONYM_DEP) {
+          if (replaceInvalidAcronym) {
+            typeAtt.setType(StandardTokenizer.TOKEN_TYPES[StandardTokenizer.HOST]);
+            termAtt.setLength(termAtt.length() - 1); // remove extra '.'
+          } else {
+            typeAtt.setType(StandardTokenizer.TOKEN_TYPES[StandardTokenizer.ACRONYM]);
+          }
+        } else {
+          typeAtt.setType(StandardTokenizer.TOKEN_TYPES[tokenType]);
+        }
+        return true;
+      } else
+        // When we skip a too-long term, we still increment the
+        // position increment
+        posIncr++;
+    }
+  }
+  
+  @Override
+  public final void end() {
+    // set final offset
+    int finalOffset = correctOffset(scanner.yychar() + scanner.yylength());
+    offsetAtt.setOffset(finalOffset, finalOffset);
+  }
+
+  @Override
+  public void reset(Reader reader) throws IOException {
+    super.reset(reader);
+    scanner.yyreset(reader);
+  }
+
+  /**
+   * Prior to https://issues.apache.org/jira/browse/LUCENE-1068, StandardTokenizer mischaracterized as acronyms tokens like www.abc.com
+   * when they should have been labeled as hosts instead.
+   * @return true if StandardTokenizer now returns these tokens as Hosts, otherwise false
+   *
+   * @deprecated Remove in 3.X and make true the only valid value
+   */
+  @Deprecated
+  public boolean isReplaceInvalidAcronym() {
+    return replaceInvalidAcronym;
+  }
+
+  /**
+   *
+   * @param replaceInvalidAcronym Set to true to replace mischaracterized acronyms as HOST.
+   * @deprecated Remove in 3.X and make true the only valid value
+   *
+   * See https://issues.apache.org/jira/browse/LUCENE-1068
+   */
+  @Deprecated
+  public void setReplaceInvalidAcronym(boolean replaceInvalidAcronym) {
+    this.replaceInvalidAcronym = replaceInvalidAcronym;
+  }
+}
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl31.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl31.java
new file mode 100644
index 0000000..49837da
--- /dev/null
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl31.java
@@ -0,0 +1,740 @@
+/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 17.05.10 14:50 */
+
+package org.apache.lucene.analysis.standard;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/*
+
+WARNING: if you change StandardTokenizerImpl*.jflex and need to regenerate
+      the tokenizer, only use the trunk version of JFlex 1.5 at the moment!
+
+*/
+
+import java.io.Reader;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+
+
+/**
+ * This class is a scanner generated by 
+ * <a href="http://www.jflex.de/">JFlex</a> 1.5.0-SNAPSHOT
+ * on 17.05.10 14:50 from the specification file
+ * <tt>C:/Users/Uwe Schindler/Projects/lucene/newtrunk/lucene/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl31.jflex</tt>
+ */
+class StandardTokenizerImpl31 implements StandardTokenizerInterface {
+
+  /** This character denotes the end of file */
+  public static final int YYEOF = -1;
+
+  /** initial size of the lookahead buffer */
+  private static final int ZZ_BUFFERSIZE = 16384;
+
+  /** lexical states */
+  public static final int YYINITIAL = 0;
+
+  /**
+   * ZZ_LEXSTATE[l] is the state in the DFA for the lexical state l
+   * ZZ_LEXSTATE[l+1] is the state in the DFA for the lexical state l
+   *                  at the beginning of a line
+   * l is of the form l = 2*k, k a non negative integer
+   */
+  private static final int ZZ_LEXSTATE[] = { 
+     0, 0
+  };
+
+  /** 
+   * Translates characters to character classes
+   */
+  private static final String ZZ_CMAP_PACKED = 
+    "\11\0\1\0\1\15\1\0\1\0\1\14\22\0\1\0\5\0\1\5"+
+    "\1\3\4\0\1\11\1\7\1\4\1\11\12\2\6\0\1\6\32\12"+
+    "\4\0\1\10\1\0\32\12\57\0\1\12\12\0\1\12\4\0\1\12"+
+    "\5\0\27\12\1\0\37\12\1\0\u013f\12\31\0\162\12\4\0\14\12"+
+    "\16\0\5\12\11\0\1\12\213\0\1\12\13\0\1\12\1\0\3\12"+
+    "\1\0\1\12\1\0\24\12\1\0\54\12\1\0\46\12\1\0\5\12"+
+    "\4\0\202\12\10\0\105\12\1\0\46\12\2\0\2\12\6\0\20\12"+
+    "\41\0\46\12\2\0\1\12\7\0\47\12\110\0\33\12\5\0\3\12"+
+    "\56\0\32\12\5\0\13\12\25\0\12\2\4\0\2\12\1\0\143\12"+
+    "\1\0\1\12\17\0\2\12\7\0\2\12\12\2\3\12\2\0\1\12"+
+    "\20\0\1\12\1\0\36\12\35\0\3\12\60\0\46\12\13\0\1\12"+
+    "\u0152\0\66\12\3\0\1\12\22\0\1\12\7\0\12\12\4\0\12\2"+
+    "\25\0\10\12\2\0\2\12\2\0\26\12\1\0\7\12\1\0\1\12"+
+    "\3\0\4\12\3\0\1\12\36\0\2\12\1\0\3\12\4\0\12\2"+
+    "\2\12\23\0\6\12\4\0\2\12\2\0\26\12\1\0\7\12\1\0"+
+    "\2\12\1\0\2\12\1\0\2\12\37\0\4\12\1\0\1\12\7\0"+
+    "\12\2\2\0\3\12\20\0\11\12\1\0\3\12\1\0\26\12\1\0"+
+    "\7\12\1\0\2\12\1\0\5\12\3\0\1\12\22\0\1\12\17\0"+
+    "\2\12\4\0\12\2\25\0\10\12\2\0\2\12\2\0\26\12\1\0"+
+    "\7\12\1\0\2\12\1\0\5\12\3\0\1\12\36\0\2\12\1\0"+
+    "\3\12\4\0\12\2\1\0\1\12\21\0\1\12\1\0\6\12\3\0"+
+    "\3\12\1\0\4\12\3\0\2\12\1\0\1\12\1\0\2\12\3\0"+
+    "\2\12\3\0\3\12\3\0\10\12\1\0\3\12\55\0\11\2\25\0"+
+    "\10\12\1\0\3\12\1\0\27\12\1\0\12\12\1\0\5\12\46\0"+
+    "\2\12\4\0\12\2\25\0\10\12\1\0\3\12\1\0\27\12\1\0"+
+    "\12\12\1\0\5\12\3\0\1\12\40\0\1\12\1\0\2\12\4\0"+
+    "\12\2\25\0\10\12\1\0\3\12\1\0\27\12\1\0\20\12\46\0"+
+    "\2\12\4\0\12\2\25\0\22\12\3\0\30\12\1\0\11\12\1\0"+
+    "\1\12\2\0\7\12\71\0\1\1\60\12\1\1\2\12\14\1\7\12"+
+    "\11\1\12\2\47\0\2\12\1\0\1\12\2\0\2\12\1\0\1\12"+
+    "\2\0\1\12\6\0\4\12\1\0\7\12\1\0\3\12\1\0\1\12"+
+    "\1\0\1\12\2\0\2\12\1\0\4\12\1\0\2\12\11\0\1\12"+
+    "\2\0\5\12\1\0\1\12\11\0\12\2\2\0\2\12\42\0\1\12"+
+    "\37\0\12\2\26\0\10\12\1\0\42\12\35\0\4\12\164\0\42\12"+
+    "\1\0\5\12\1\0\2\12\25\0\12\2\6\0\6\12\112\0\46\12"+
+    "\12\0\51\12\7\0\132\12\5\0\104\12\5\0\122\12\6\0\7\12"+
+    "\1\0\77\12\1\0\1\12\1\0\4\12\2\0\7\12\1\0\1\12"+
+    "\1\0\4\12\2\0\47\12\1\0\1\12\1\0\4\12\2\0\37\12"+
+    "\1\0\1\12\1\0\4\12\2\0\7\12\1\0\1\12\1\0\4\12"+
+    "\2\0\7\12\1\0\7\12\1\0\27\12\1\0\37\12\1\0\1\12"+
+    "\1\0\4\12\2\0\7\12\1\0\47\12\1\0\23\12\16\0\11\2"+
+    "\56\0\125\12\14\0\u026c\12\2\0\10\12\12\0\32\12\5\0\113\12"+
+    "\25\0\15\12\1\0\4\12\16\0\22\12\16\0\22\12\16\0\15\12"+
+    "\1\0\3\12\17\0\64\12\43\0\1\12\4\0\1\12\3\0\12\2"+
+    "\46\0\12\2\6\0\130\12\10\0\51\12\127\0\35\12\51\0\12\2"+
+    "\36\12\2\0\5\12\u038b\0\154\12\224\0\234\12\4\0\132\12\6\0"+
+    "\26\12\2\0\6\12\2\0\46\12\2\0\6\12\2\0\10\12\1\0"+
+    "\1\12\1\0\1\12\1\0\1\12\1\0\37\12\2\0\65\12\1\0"+
+    "\7\12\1\0\1\12\3\0\3\12\1\0\7\12\3\0\4\12\2\0"+
+    "\6\12\4\0\15\12\5\0\3\12\1\0\7\12\164\0\1\12\15\0"+
+    "\1\12\202\0\1\12\4\0\1\12\2\0\12\12\1\0\1\12\3\0"+
+    "\5\12\6\0\1\12\1\0\1\12\1\0\1\12\1\0\4\12\1\0"+
+    "\3\12\1\0\7\12\3\0\3\12\5\0\5\12\u0ebb\0\2\12\52\0"+
+    "\5\12\5\0\2\12\3\0\1\13\126\13\6\13\3\13\1\13\132\13"+
+    "\1\13\4\13\5\13\50\13\3\13\1\0\136\12\21\0\30\12\70\0"+
+    "\20\13\u0100\0\200\13\200\0\u19b6\13\12\13\100\0\u51a6\13\132\13\u048d\12"+
+    "\u0773\0\u2ba4\12\u215c\0\u012e\13\2\13\73\13\225\13\7\12\14\0\5\12"+
+    "\5\0\1\12\1\0\12\12\1\0\15\12\1\0\5\12\1\0\1\12"+
+    "\1\0\2\12\1\0\2\12\1\0\154\12\41\0\u016b\12\22\0\100\12"+
+    "\2\0\66\12\50\0\14\12\164\0\5\12\1\0\207\12\23\0\12\2"+
+    "\7\0\32\12\6\0\32\12\12\0\1\13\72\13\37\12\3\0\6\12"+
+    "\2\0\6\12\2\0\6\12\2\0\3\12\43\0";
+
+  /** 
+   * Translates characters to character classes
+   */
+  private static final char [] ZZ_CMAP = zzUnpackCMap(ZZ_CMAP_PACKED);
+
+  /** 
+   * Translates DFA states to action switch labels.
+   */
+  private static final int [] ZZ_ACTION = zzUnpackAction();
+
+  private static final String ZZ_ACTION_PACKED_0 =
+    "\1\0\1\1\3\2\1\3\1\1\13\0\1\2\3\4"+
+    "\2\0\1\5\1\0\1\5\3\4\6\5\1\6\1\4"+
+    "\2\7\1\10\1\0\1\10\3\0\2\10\1\11\1\12"+
+    "\1\4";
+
+  private static int [] zzUnpackAction() {
+    int [] result = new int[51];
+    int offset = 0;
+    offset = zzUnpackAction(ZZ_ACTION_PACKED_0, offset, result);
+    return result;
+  }
+
+  private static int zzUnpackAction(String packed, int offset, int [] result) {
+    int i = 0;       /* index in packed string  */
+    int j = offset;  /* index in unpacked array */
+    int l = packed.length();
+    while (i < l) {
+      int count = packed.charAt(i++);
+      int value = packed.charAt(i++);
+      do result[j++] = value; while (--count > 0);
+    }
+    return j;
+  }
+
+
+  /** 
+   * Translates a state to a row index in the transition table
+   */
+  private static final int [] ZZ_ROWMAP = zzUnpackRowMap();
+
+  private static final String ZZ_ROWMAP_PACKED_0 =
+    "\0\0\0\16\0\34\0\52\0\70\0\16\0\106\0\124"+
+    "\0\142\0\160\0\176\0\214\0\232\0\250\0\266\0\304"+
+    "\0\322\0\340\0\356\0\374\0\u010a\0\u0118\0\u0126\0\u0134"+
+    "\0\u0142\0\u0150\0\u015e\0\u016c\0\u017a\0\u0188\0\u0196\0\u01a4"+
+    "\0\u01b2\0\u01c0\0\u01ce\0\u01dc\0\u01ea\0\u01f8\0\322\0\u0206"+
+    "\0\u0214\0\u0222\0\u0230\0\u023e\0\u024c\0\u025a\0\124\0\214"+
+    "\0\u0268\0\u0276\0\u0284";
+
+  private static int [] zzUnpackRowMap() {
+    int [] result = new int[51];
+    int offset = 0;
+    offset = zzUnpackRowMap(ZZ_ROWMAP_PACKED_0, offset, result);
+    return result;
+  }
+
+  private static int zzUnpackRowMap(String packed, int offset, int [] result) {
+    int i = 0;  /* index in packed string  */
+    int j = offset;  /* index in unpacked array */
+    int l = packed.length();
+    while (i < l) {
+      int high = packed.charAt(i++) << 16;
+      result[j++] = high | packed.charAt(i++);
+    }
+    return j;
+  }
+
+  /** 
+   * The transition table of the DFA
+   */
+  private static final int [] ZZ_TRANS = zzUnpackTrans();
+
+  private static final String ZZ_TRANS_PACKED_0 =
+    "\1\2\1\3\1\4\7\2\1\5\1\6\1\7\1\2"+
+    "\17\0\2\3\1\0\1\10\1\0\1\11\2\12\1\13"+
+    "\1\3\4\0\1\3\1\4\1\0\1\14\1\0\1\11"+
+    "\2\15\1\16\1\4\4\0\1\3\1\4\1\17\1\20"+
+    "\1\21\1\22\2\12\1\13\1\23\20\0\1\2\1\0"+
+    "\1\24\1\25\7\0\1\26\4\0\2\27\7\0\1\27"+
+    "\4\0\1\30\1\31\7\0\1\32\5\0\1\33\7\0"+
+    "\1\13\4\0\1\34\1\35\7\0\1\36\4\0\1\37"+
+    "\1\40\7\0\1\41\4\0\1\42\1\43\7\0\1\44"+
+    "\15\0\1\45\4\0\1\24\1\25\7\0\1\46\15\0"+
+    "\1\47\4\0\2\27\7\0\1\50\4\0\1\3\1\4"+
+    "\1\17\1\10\1\21\1\22\2\12\1\13\1\23\4\0"+
+    "\2\24\1\0\1\51\1\0\1\11\2\52\1\0\1\24"+
+    "\4\0\1\24\1\25\1\0\1\53\1\0\1\11\2\54"+
+    "\1\55\1\25\4\0\1\24\1\25\1\0\1\51\1\0"+
+    "\1\11\2\52\1\0\1\26\4\0\2\27\1\0\1\56"+
+    "\2\0\1\56\2\0\1\27\4\0\2\30\1\0\1\52"+
+    "\1\0\1\11\2\52\1\0\1\30\4\0\1\30\1\31"+
+    "\1\0\1\54\1\0\1\11\2\54\1\55\1\31\4\0"+
+    "\1\30\1\31\1\0\1\52\1\0\1\11\2\52\1\0"+
+    "\1\32\5\0\1\33\1\0\1\55\2\0\3\55\1\33"+
+    "\4\0\2\34\1\0\1\57\1\0\1\11\2\12\1\13"+
+    "\1\34\4\0\1\34\1\35\1\0\1\60\1\0\1\11"+
+    "\2\15\1\16\1\35\4\0\1\34\1\35\1\0\1\57"+
+    "\1\0\1\11\2\12\1\13\1\36\4\0\2\37\1\0"+
+    "\1\12\1\0\1\11\2\12\1\13\1\37\4\0\1\37"+
+    "\1\40\1\0\1\15\1\0\1\11\2\15\1\16\1\40"+
+    "\4\0\1\37\1\40\1\0\1\12\1\0\1\11\2\12"+
+    "\1\13\1\41\4\0\2\42\1\0\1\13\2\0\3\13"+
+    "\1\42\4\0\1\42\1\43\1\0\1\16\2\0\3\16"+
+    "\1\43\4\0\1\42\1\43\1\0\1\13\2\0\3\13"+
+    "\1\44\6\0\1\17\6\0\1\45\4\0\1\24\1\25"+
+    "\1\0\1\61\1\0\1\11\2\52\1\0\1\26\4\0"+
+    "\2\27\1\0\1\56\2\0\1\56\2\0\1\50\4\0"+
+    "\2\24\7\0\1\24\4\0\2\30\7\0\1\30\4\0"+
+    "\2\34\7\0\1\34\4\0\2\37\7\0\1\37\4\0"+
+    "\2\42\7\0\1\42\4\0\2\62\7\0\1\62\4\0"+
+    "\2\24\7\0\1\63\4\0\2\62\1\0\1\56\2\0"+
+    "\1\56\2\0\1\62\4\0\2\24\1\0\1\61\1\0"+
+    "\1\11\2\52\1\0\1\24\3\0";
+
+  private static int [] zzUnpackTrans() {
+    int [] result = new int[658];
+    int offset = 0;
+    offset = zzUnpackTrans(ZZ_TRANS_PACKED_0, offset, result);
+    return result;
+  }
+
+  private static int zzUnpackTrans(String packed, int offset, int [] result) {
+    int i = 0;       /* index in packed string  */
+    int j = offset;  /* index in unpacked array */
+    int l = packed.length();
+    while (i < l) {
+      int count = packed.charAt(i++);
+      int value = packed.charAt(i++);
+      value--;
+      do result[j++] = value; while (--count > 0);
+    }
+    return j;
+  }
+
+
+  /* error codes */
+  private static final int ZZ_UNKNOWN_ERROR = 0;
+  private static final int ZZ_NO_MATCH = 1;
+  private static final int ZZ_PUSHBACK_2BIG = 2;
+
+  /* error messages for the codes above */
+  private static final String ZZ_ERROR_MSG[] = {
+    "Unkown internal scanner error",
+    "Error: could not match input",
+    "Error: pushback value was too large"
+  };
+
+  /**
+   * ZZ_ATTRIBUTE[aState] contains the attributes of state <code>aState</code>
+   */
+  private static final int [] ZZ_ATTRIBUTE = zzUnpackAttribute();
+
+  private static final String ZZ_ATTRIBUTE_PACKED_0 =
+    "\1\0\1\11\3\1\1\11\1\1\13\0\4\1\2\0"+
+    "\1\1\1\0\17\1\1\0\1\1\3\0\5\1";
+
+  private static int [] zzUnpackAttribute() {
+    int [] result = new int[51];
+    int offset = 0;
+    offset = zzUnpackAttribute(ZZ_ATTRIBUTE_PACKED_0, offset, result);
+    return result;
+  }
+
+  private static int zzUnpackAttribute(String packed, int offset, int [] result) {
+    int i = 0;       /* index in packed string  */
+    int j = offset;  /* index in unpacked array */
+    int l = packed.length();
+    while (i < l) {
+      int count = packed.charAt(i++);
+      int value = packed.charAt(i++);
+      do result[j++] = value; while (--count > 0);
+    }
+    return j;
+  }
+
+  /** the input device */
+  private java.io.Reader zzReader;
+
+  /** the current state of the DFA */
+  private int zzState;
+
+  /** the current lexical state */
+  private int zzLexicalState = YYINITIAL;
+
+  /** this buffer contains the current text to be matched and is
+      the source of the yytext() string */
+  private char zzBuffer[] = new char[ZZ_BUFFERSIZE];
+
+  /** the textposition at the last accepting state */
+  private int zzMarkedPos;
+
+  /** the current text position in the buffer */
+  private int zzCurrentPos;
+
+  /** startRead marks the beginning of the yytext() string in the buffer */
+  private int zzStartRead;
+
+  /** endRead marks the last character in the buffer, that has been read
+      from input */
+  private int zzEndRead;
+
+  /** number of newlines encountered up to the start of the matched text */
+  private int yyline;
+
+  /** the number of characters up to the start of the matched text */
+  private int yychar;
+
+  /**
+   * the number of characters from the last newline up to the start of the 
+   * matched text
+   */
+  private int yycolumn;
+
+  /** 
+   * zzAtBOL == true <=> the scanner is currently at the beginning of a line
+   */
+  private boolean zzAtBOL = true;
+
+  /** zzAtEOF == true <=> the scanner is at the EOF */
+  private boolean zzAtEOF;
+
+  /** denotes if the user-EOF-code has already been executed */
+  private boolean zzEOFDone;
+
+  /* user code: */
+
+public static final int ALPHANUM          = StandardTokenizer.ALPHANUM;
+public static final int APOSTROPHE        = StandardTokenizer.APOSTROPHE;
+public static final int ACRONYM           = StandardTokenizer.ACRONYM;
+public static final int COMPANY           = StandardTokenizer.COMPANY;
+public static final int EMAIL             = StandardTokenizer.EMAIL;
+public static final int HOST              = StandardTokenizer.HOST;
+public static final int NUM               = StandardTokenizer.NUM;
+public static final int CJ                = StandardTokenizer.CJ;
+/**
+ * @deprecated this solves a bug where HOSTs that end with '.' are identified
+ *             as ACRONYMs.
+ */
+public static final int ACRONYM_DEP       = StandardTokenizer.ACRONYM_DEP;
+
+public static final String [] TOKEN_TYPES = StandardTokenizer.TOKEN_TYPES;
+
+public final int yychar()
+{
+    return yychar;
+}
+
+/**
+ * Fills CharTermAttribute with the current token text.
+ */
+public final void getText(CharTermAttribute t) {
+  t.copyBuffer(zzBuffer, zzStartRead, zzMarkedPos-zzStartRead);
+}
+
+
+
+  /**
+   * Creates a new scanner
+   * There is also a java.io.InputStream version of this constructor.
+   *
+   * @param   in  the java.io.Reader to read input from.
+   */
+  StandardTokenizerImpl31(java.io.Reader in) {
+    this.zzReader = in;
+  }
+
+  /**
+   * Creates a new scanner.
+   * There is also java.io.Reader version of this constructor.
+   *
+   * @param   in  the java.io.Inputstream to read input from.
+   */
+  StandardTokenizerImpl31(java.io.InputStream in) {
+    this(new java.io.InputStreamReader(in));
+  }
+
+  /** 
+   * Unpacks the compressed character translation table.
+   *
+   * @param packed   the packed character translation table
+   * @return         the unpacked character translation table
+   */
+  private static char [] zzUnpackCMap(String packed) {
+    char [] map = new char[0x10000];
+    int i = 0;  /* index in packed string  */
+    int j = 0;  /* index in unpacked array */
+    while (i < 1234) {
+      int  count = packed.charAt(i++);
+      char value = packed.charAt(i++);
+      do map[j++] = value; while (--count > 0);
+    }
+    return map;
+  }
+
+
+  /**
+   * Refills the input buffer.
+   *
+   * @return      <code>false</code>, iff there was new input.
+   * 
+   * @exception   java.io.IOException  if any I/O-Error occurs
+   */
+  private boolean zzRefill() throws java.io.IOException {
+
+    /* first: make room (if you can) */
+    if (zzStartRead > 0) {
+      System.arraycopy(zzBuffer, zzStartRead,
+                       zzBuffer, 0,
+                       zzEndRead-zzStartRead);
+
+      /* translate stored positions */
+      zzEndRead-= zzStartRead;
+      zzCurrentPos-= zzStartRead;
+      zzMarkedPos-= zzStartRead;
+      zzStartRead = 0;
+    }
+
+    /* is the buffer big enough? */
+    if (zzCurrentPos >= zzBuffer.length) {
+      /* if not: blow it up */
+      char newBuffer[] = new char[zzCurrentPos*2];
+      System.arraycopy(zzBuffer, 0, newBuffer, 0, zzBuffer.length);
+      zzBuffer = newBuffer;
+    }
+
+    /* finally: fill the buffer with new input */
+    int numRead = zzReader.read(zzBuffer, zzEndRead,
+                                            zzBuffer.length-zzEndRead);
+
+    if (numRead > 0) {
+      zzEndRead+= numRead;
+      return false;
+    }
+    // unlikely but not impossible: read 0 characters, but not at end of stream    
+    if (numRead == 0) {
+      int c = zzReader.read();
+      if (c == -1) {
+        return true;
+      } else {
+        zzBuffer[zzEndRead++] = (char) c;
+        return false;
+      }     
+    }
+
+	// numRead < 0
+    return true;
+  }
+
+    
+  /**
+   * Closes the input stream.
+   */
+  public final void yyclose() throws java.io.IOException {
+    zzAtEOF = true;            /* indicate end of file */
+    zzEndRead = zzStartRead;  /* invalidate buffer    */
+
+    if (zzReader != null)
+      zzReader.close();
+  }
+
+
+  /**
+   * Resets the scanner to read from a new input stream.
+   * Does not close the old reader.
+   *
+   * All internal variables are reset, the old input stream 
+   * <b>cannot</b> be reused (internal buffer is discarded and lost).
+   * Lexical state is set to <tt>ZZ_INITIAL</tt>.
+   *
+   * Internal scan buffer is resized down to its initial length, if it has grown.
+   *
+   * @param reader   the new input stream 
+   */
+  public final void yyreset(java.io.Reader reader) {
+    zzReader = reader;
+    zzAtBOL  = true;
+    zzAtEOF  = false;
+    zzEOFDone = false;
+    zzEndRead = zzStartRead = 0;
+    zzCurrentPos = zzMarkedPos = 0;
+    yyline = yychar = yycolumn = 0;
+    zzLexicalState = YYINITIAL;
+    if (zzBuffer.length > ZZ_BUFFERSIZE)
+      zzBuffer = new char[ZZ_BUFFERSIZE];
+  }
+
+
+  /**
+   * Returns the current lexical state.
+   */
+  public final int yystate() {
+    return zzLexicalState;
+  }
+
+
+  /**
+   * Enters a new lexical state
+   *
+   * @param newState the new lexical state
+   */
+  public final void yybegin(int newState) {
+    zzLexicalState = newState;
+  }
+
+
+  /**
+   * Returns the text matched by the current regular expression.
+   */
+  public final String yytext() {
+    return new String( zzBuffer, zzStartRead, zzMarkedPos-zzStartRead );
+  }
+
+
+  /**
+   * Returns the character at position <tt>pos</tt> from the 
+   * matched text. 
+   * 
+   * It is equivalent to yytext().charAt(pos), but faster
+   *
+   * @param pos the position of the character to fetch. 
+   *            A value from 0 to yylength()-1.
+   *
+   * @return the character at position pos
+   */
+  public final char yycharat(int pos) {
+    return zzBuffer[zzStartRead+pos];
+  }
+
+
+  /**
+   * Returns the length of the matched text region.
+   */
+  public final int yylength() {
+    return zzMarkedPos-zzStartRead;
+  }
+
+
+  /**
+   * Reports an error that occured while scanning.
+   *
+   * In a wellformed scanner (no or only correct usage of 
+   * yypushback(int) and a match-all fallback rule) this method 
+   * will only be called with things that "Can't Possibly Happen".
+   * If this method is called, something is seriously wrong
+   * (e.g. a JFlex bug producing a faulty scanner etc.).
+   *
+   * Usual syntax/scanner level error handling should be done
+   * in error fallback rules.
+   *
+   * @param   errorCode  the code of the errormessage to display
+   */
+  private void zzScanError(int errorCode) {
+    String message;
+    try {
+      message = ZZ_ERROR_MSG[errorCode];
+    }
+    catch (ArrayIndexOutOfBoundsException e) {
+      message = ZZ_ERROR_MSG[ZZ_UNKNOWN_ERROR];
+    }
+
+    throw new Error(message);
+  } 
+
+
+  /**
+   * Pushes the specified amount of characters back into the input stream.
+   *
+   * They will be read again by then next call of the scanning method
+   *
+   * @param number  the number of characters to be read again.
+   *                This number must not be greater than yylength()!
+   */
+  public void yypushback(int number)  {
+    if ( number > yylength() )
+      zzScanError(ZZ_PUSHBACK_2BIG);
+
+    zzMarkedPos -= number;
+  }
+
+
+  /**
+   * Resumes scanning until the next regular expression is matched,
+   * the end of input is encountered or an I/O-Error occurs.
+   *
+   * @return      the next token
+   * @exception   java.io.IOException  if any I/O-Error occurs
+   */
+  public int getNextToken() throws java.io.IOException {
+    int zzInput;
+    int zzAction;
+
+    // cached fields:
+    int zzCurrentPosL;
+    int zzMarkedPosL;
+    int zzEndReadL = zzEndRead;
+    char [] zzBufferL = zzBuffer;
+    char [] zzCMapL = ZZ_CMAP;
+
+    int [] zzTransL = ZZ_TRANS;
+    int [] zzRowMapL = ZZ_ROWMAP;
+    int [] zzAttrL = ZZ_ATTRIBUTE;
+
+    while (true) {
+      zzMarkedPosL = zzMarkedPos;
+
+      yychar+= zzMarkedPosL-zzStartRead;
+
+      zzAction = -1;
+
+      zzCurrentPosL = zzCurrentPos = zzStartRead = zzMarkedPosL;
+  
+      zzState = ZZ_LEXSTATE[zzLexicalState];
+
+
+      zzForAction: {
+        while (true) {
+    
+          if (zzCurrentPosL < zzEndReadL)
+            zzInput = zzBufferL[zzCurrentPosL++];
+          else if (zzAtEOF) {
+            zzInput = YYEOF;
+            break zzForAction;
+          }
+          else {
+            // store back cached positions
+            zzCurrentPos  = zzCurrentPosL;
+            zzMarkedPos   = zzMarkedPosL;
+            boolean eof = zzRefill();
+            // get translated positions and possibly new buffer
+            zzCurrentPosL  = zzCurrentPos;
+            zzMarkedPosL   = zzMarkedPos;
+            zzBufferL      = zzBuffer;
+            zzEndReadL     = zzEndRead;
+            if (eof) {
+              zzInput = YYEOF;
+              break zzForAction;
+            }
+            else {
+              zzInput = zzBufferL[zzCurrentPosL++];
+            }
+          }
+          int zzNext = zzTransL[ zzRowMapL[zzState] + zzCMapL[zzInput] ];
+          if (zzNext == -1) break zzForAction;
+          zzState = zzNext;
+
+          int zzAttributes = zzAttrL[zzState];
+          if ( (zzAttributes & 1) == 1 ) {
+            zzAction = zzState;
+            zzMarkedPosL = zzCurrentPosL;
+            if ( (zzAttributes & 8) == 8 ) break zzForAction;
+          }
+
+        }
+      }
+
+      // store back cached position
+      zzMarkedPos = zzMarkedPosL;
+
+      switch (zzAction < 0 ? zzAction : ZZ_ACTION[zzAction]) {
+        case 5: 
+          { return NUM;
+          }
+        case 11: break;
+        case 9: 
+          { return ACRONYM;
+          }
+        case 12: break;
+        case 7: 
+          { return COMPANY;
+          }
+        case 13: break;
+        case 10: 
+          { return EMAIL;
+          }
+        case 14: break;
+        case 1: 
+          { /* ignore */
+          }
+        case 15: break;
+        case 6: 
+          { return APOSTROPHE;
+          }
+        case 16: break;
+        case 3: 
+          { return CJ;
+          }
+        case 17: break;
+        case 8: 
+          { return ACRONYM_DEP;
+          }
+        case 18: break;
+        case 2: 
+          { return ALPHANUM;
+          }
+        case 19: break;
+        case 4: 
+          { return HOST;
+          }
+        case 20: break;
+        default: 
+          if (zzInput == YYEOF && zzStartRead == zzCurrentPos) {
+            zzAtEOF = true;
+            return YYEOF;
+          } 
+          else {
+            zzScanError(ZZ_NO_MATCH);
+          }
+      }
+    }
+  }
+
+
+}
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl31.jflex b/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl31.jflex
new file mode 100644
index 0000000..c2fbc59
--- /dev/null
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl31.jflex
@@ -0,0 +1,134 @@
+package org.apache.lucene.analysis.standard;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/*
+
+WARNING: if you change StandardTokenizerImpl*.jflex and need to regenerate
+      the tokenizer, only use the trunk version of JFlex 1.5 at the moment!
+
+*/
+
+import java.io.Reader;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+
+%%
+
+%class StandardTokenizerImpl31
+%implements StandardTokenizerInterface
+%unicode 4.0
+%integer
+%function getNextToken
+%pack
+%char
+
+%{
+
+public static final int ALPHANUM          = StandardTokenizer.ALPHANUM;
+public static final int APOSTROPHE        = StandardTokenizer.APOSTROPHE;
+public static final int ACRONYM           = StandardTokenizer.ACRONYM;
+public static final int COMPANY           = StandardTokenizer.COMPANY;
+public static final int EMAIL             = StandardTokenizer.EMAIL;
+public static final int HOST              = StandardTokenizer.HOST;
+public static final int NUM               = StandardTokenizer.NUM;
+public static final int CJ                = StandardTokenizer.CJ;
+/**
+ * @deprecated this solves a bug where HOSTs that end with '.' are identified
+ *             as ACRONYMs.
+ */
+public static final int ACRONYM_DEP       = StandardTokenizer.ACRONYM_DEP;
+
+public static final String [] TOKEN_TYPES = StandardTokenizer.TOKEN_TYPES;
+
+public final int yychar()
+{
+    return yychar;
+}
+
+/**
+ * Fills CharTermAttribute with the current token text.
+ */
+public final void getText(CharTermAttribute t) {
+  t.copyBuffer(zzBuffer, zzStartRead, zzMarkedPos-zzStartRead);
+}
+
+%}
+
+THAI       = [\u0E00-\u0E59]
+
+// basic word: a sequence of digits & letters (includes Thai to enable ThaiAnalyzer to function)
+ALPHANUM   = ({LETTER}|{THAI}|[:digit:])+
+
+// internal apostrophes: O'Reilly, you're, O'Reilly's
+// use a post-filter to remove possessives
+APOSTROPHE =  {ALPHA} ("'" {ALPHA})+
+
+// acronyms: U.S.A., I.B.M., etc.
+// use a post-filter to remove dots
+ACRONYM    =  {LETTER} "." ({LETTER} ".")+
+
+ACRONYM_DEP	= {ALPHANUM} "." ({ALPHANUM} ".")+
+
+// company names like AT&T and Excite@Home.
+COMPANY    =  {ALPHA} ("&"|"@") {ALPHA}
+
+// email addresses
+EMAIL      =  {ALPHANUM} (("."|"-"|"_") {ALPHANUM})* "@" {ALPHANUM} (("."|"-") {ALPHANUM})+
+
+// hostname
+HOST       =  {ALPHANUM} ((".") {ALPHANUM})+
+
+// floating point, serial, model numbers, ip addresses, etc.
+// every other segment must have at least one digit
+NUM        = ({ALPHANUM} {P} {HAS_DIGIT}
+           | {HAS_DIGIT} {P} {ALPHANUM}
+           | {ALPHANUM} ({P} {HAS_DIGIT} {P} {ALPHANUM})+
+           | {HAS_DIGIT} ({P} {ALPHANUM} {P} {HAS_DIGIT})+
+           | {ALPHANUM} {P} {HAS_DIGIT} ({P} {ALPHANUM} {P} {HAS_DIGIT})+
+           | {HAS_DIGIT} {P} {ALPHANUM} ({P} {HAS_DIGIT} {P} {ALPHANUM})+)
+
+// punctuation
+P	         = ("_"|"-"|"/"|"."|",")
+
+// at least one digit
+HAS_DIGIT  = ({LETTER}|[:digit:])* [:digit:] ({LETTER}|[:digit:])*
+
+ALPHA      = ({LETTER})+
+
+// From the JFlex manual: "the expression that matches everything of <a> not matched by <b> is !(!<a>|<b>)"
+LETTER     = !(![:letter:]|{CJ})
+
+// Chinese and Japanese (but NOT Korean, which is included in [:letter:])
+CJ         = [\u3100-\u312f\u3040-\u309F\u30A0-\u30FF\u31F0-\u31FF\u3300-\u337f\u3400-\u4dbf\u4e00-\u9fff\uf900-\ufaff\uff65-\uff9f]
+
+WHITESPACE = \r\n | [ \r\n\t\f]
+
+%%
+
+{ALPHANUM}                                                     { return ALPHANUM; }
+{APOSTROPHE}                                                   { return APOSTROPHE; }
+{ACRONYM}                                                      { return ACRONYM; }
+{COMPANY}                                                      { return COMPANY; }
+{EMAIL}                                                        { return EMAIL; }
+{HOST}                                                         { return HOST; }
+{NUM}                                                          { return NUM; }
+{CJ}                                                           { return CJ; }
+{ACRONYM_DEP}                                                  { return ACRONYM_DEP; }
+
+/** Ignore the rest */
+. | {WHITESPACE}                                               { /* ignore */ }
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImplOrig.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImplOrig.java
new file mode 100644
index 0000000..29bb994
--- /dev/null
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImplOrig.java
@@ -0,0 +1,736 @@
+/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 17.05.10 14:50 */
+
+package org.apache.lucene.analysis.standard;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/*
+
+WARNING: if you change StandardTokenizerImpl*.jflex and need to regenerate
+      the tokenizer, only use the trunk version of JFlex 1.5 at the moment!
+
+*/
+
+import java.io.Reader;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+
+
+/**
+ * This class is a scanner generated by 
+ * <a href="http://www.jflex.de/">JFlex</a> 1.5.0-SNAPSHOT
+ * on 17.05.10 14:50 from the specification file
+ * <tt>C:/Users/Uwe Schindler/Projects/lucene/newtrunk/lucene/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImplOrig.jflex</tt>
+ */
+class StandardTokenizerImplOrig implements StandardTokenizerInterface {
+
+  /** This character denotes the end of file */
+  public static final int YYEOF = -1;
+
+  /** initial size of the lookahead buffer */
+  private static final int ZZ_BUFFERSIZE = 16384;
+
+  /** lexical states */
+  public static final int YYINITIAL = 0;
+
+  /**
+   * ZZ_LEXSTATE[l] is the state in the DFA for the lexical state l
+   * ZZ_LEXSTATE[l+1] is the state in the DFA for the lexical state l
+   *                  at the beginning of a line
+   * l is of the form l = 2*k, k a non negative integer
+   */
+  private static final int ZZ_LEXSTATE[] = { 
+     0, 0
+  };
+
+  /** 
+   * Translates characters to character classes
+   */
+  private static final String ZZ_CMAP_PACKED = 
+    "\11\0\1\0\1\15\1\0\1\0\1\14\22\0\1\0\5\0\1\5"+
+    "\1\3\4\0\1\11\1\7\1\4\1\11\12\2\6\0\1\6\32\12"+
+    "\4\0\1\10\1\0\32\12\57\0\1\12\12\0\1\12\4\0\1\12"+
+    "\5\0\27\12\1\0\37\12\1\0\u0128\12\2\0\22\12\34\0\136\12"+
+    "\2\0\11\12\2\0\7\12\16\0\2\12\16\0\5\12\11\0\1\12"+
+    "\213\0\1\12\13\0\1\12\1\0\3\12\1\0\1\12\1\0\24\12"+
+    "\1\0\54\12\1\0\10\12\2\0\32\12\14\0\202\12\12\0\71\12"+
+    "\2\0\2\12\2\0\2\12\3\0\46\12\2\0\2\12\67\0\46\12"+
+    "\2\0\1\12\7\0\47\12\110\0\33\12\5\0\3\12\56\0\32\12"+
+    "\5\0\13\12\25\0\12\2\7\0\143\12\1\0\1\12\17\0\2\12"+
+    "\11\0\12\2\3\12\23\0\1\12\1\0\33\12\123\0\46\12\u015f\0"+
+    "\65\12\3\0\1\12\22\0\1\12\7\0\12\12\4\0\12\2\25\0"+
+    "\10\12\2\0\2\12\2\0\26\12\1\0\7\12\1\0\1\12\3\0"+
+    "\4\12\42\0\2\12\1\0\3\12\4\0\12\2\2\12\23\0\6\12"+
+    "\4\0\2\12\2\0\26\12\1\0\7\12\1\0\2\12\1\0\2\12"+
+    "\1\0\2\12\37\0\4\12\1\0\1\12\7\0\12\2\2\0\3\12"+
+    "\20\0\7\12\1\0\1\12\1\0\3\12\1\0\26\12\1\0\7\12"+
+    "\1\0\2\12\1\0\5\12\3\0\1\12\22\0\1\12\17\0\1\12"+
+    "\5\0\12\2\25\0\10\12\2\0\2\12\2\0\26\12\1\0\7\12"+
+    "\1\0\2\12\2\0\4\12\3\0\1\12\36\0\2\12\1\0\3\12"+
+    "\4\0\12\2\25\0\6\12\3\0\3\12\1\0\4\12\3\0\2\12"+
+    "\1\0\1\12\1\0\2\12\3\0\2\12\3\0\3\12\3\0\10\12"+
+    "\1\0\3\12\55\0\11\2\25\0\10\12\1\0\3\12\1\0\27\12"+
+    "\1\0\12\12\1\0\5\12\46\0\2\12\4\0\12\2\25\0\10\12"+
+    "\1\0\3\12\1\0\27\12\1\0\12\12\1\0\5\12\44\0\1\12"+
+    "\1\0\2\12\4\0\12\2\25\0\10\12\1\0\3\12\1\0\27\12"+
+    "\1\0\20\12\46\0\2\12\4\0\12\2\25\0\22\12\3\0\30\12"+
+    "\1\0\11\12\1\0\1\12\2\0\7\12\71\0\1\1\60\12\1\1"+
+    "\2\12\14\1\7\12\11\1\12\2\47\0\2\12\1\0\1\12\2\0"+
+    "\2\12\1\0\1\12\2\0\1\12\6\0\4\12\1\0\7\12\1\0"+
+    "\3\12\1\0\1\12\1\0\1\12\2\0\2\12\1\0\4\12\1\0"+
+    "\2\12\11\0\1\12\2\0\5\12\1\0\1\12\11\0\12\2\2\0"+
+    "\2\12\42\0\1\12\37\0\12\2\26\0\10\12\1\0\42\12\35\0"+
+    "\4\12\164\0\42\12\1\0\5\12\1\0\2\12\25\0\12\2\6\0"+
+    "\6\12\112\0\46\12\12\0\47\12\11\0\132\12\5\0\104\12\5\0"+
+    "\122\12\6\0\7\12\1\0\77\12\1\0\1\12\1\0\4\12\2\0"+
+    "\7\12\1\0\1\12\1\0\4\12\2\0\47\12\1\0\1\12\1\0"+
+    "\4\12\2\0\37\12\1\0\1\12\1\0\4\12\2\0\7\12\1\0"+
+    "\1\12\1\0\4\12\2\0\7\12\1\0\7\12\1\0\27\12\1\0"+
+    "\37\12\1\0\1\12\1\0\4\12\2\0\7\12\1\0\47\12\1\0"+
+    "\23\12\16\0\11\2\56\0\125\12\14\0\u026c\12\2\0\10\12\12\0"+
+    "\32\12\5\0\113\12\225\0\64\12\54\0\12\2\46\0\12\2\6\0"+
+    "\130\12\10\0\51\12\u0557\0\234\12\4\0\132\12\6\0\26\12\2\0"+
+    "\6\12\2\0\46\12\2\0\6\12\2\0\10\12\1\0\1\12\1\0"+
+    "\1\12\1\0\1\12\1\0\37\12\2\0\65\12\1\0\7\12\1\0"+
+    "\1\12\3\0\3\12\1\0\7\12\3\0\4\12\2\0\6\12\4\0"+
+    "\15\12\5\0\3\12\1\0\7\12\202\0\1\12\202\0\1\12\4\0"+
+    "\1\12\2\0\12\12\1\0\1\12\3\0\5\12\6\0\1\12\1\0"+
+    "\1\12\1\0\1\12\1\0\4\12\1\0\3\12\1\0\7\12\u0ecb\0"+
+    "\2\12\52\0\5\12\12\0\1\13\124\13\10\13\2\13\2\13\132\13"+
+    "\1\13\3\13\6\13\50\13\3\13\1\0\136\12\21\0\30\12\70\0"+
+    "\20\13\u0100\0\200\13\200\0\u19b6\13\12\13\100\0\u51a6\13\132\13\u048d\12"+
+    "\u0773\0\u2ba4\12\u215c\0\u012e\13\322\13\7\12\14\0\5\12\5\0\1\12"+
+    "\1\0\12\12\1\0\15\12\1\0\5\12\1\0\1\12\1\0\2\12"+
+    "\1\0\2\12\1\0\154\12\41\0\u016b\12\22\0\100\12\2\0\66\12"+
+    "\50\0\14\12\164\0\3\12\1\0\1\12\1\0\207\12\23\0\12\2"+
+    "\7\0\32\12\6\0\32\12\12\0\1\13\72\13\37\12\3\0\6\12"+
+    "\2\0\6\12\2\0\6\12\2\0\3\12\43\0";
+
+  /** 
+   * Translates characters to character classes
+   */
+  private static final char [] ZZ_CMAP = zzUnpackCMap(ZZ_CMAP_PACKED);
+
+  /** 
+   * Translates DFA states to action switch labels.
+   */
+  private static final int [] ZZ_ACTION = zzUnpackAction();
+
+  private static final String ZZ_ACTION_PACKED_0 =
+    "\1\0\1\1\3\2\1\3\1\1\13\0\1\2\3\4"+
+    "\2\0\1\5\1\0\1\5\3\4\6\5\1\6\1\4"+
+    "\2\7\1\10\1\0\1\10\3\0\2\10\1\11\1\12"+
+    "\1\4";
+
+  private static int [] zzUnpackAction() {
+    int [] result = new int[51];
+    int offset = 0;
+    offset = zzUnpackAction(ZZ_ACTION_PACKED_0, offset, result);
+    return result;
+  }
+
+  private static int zzUnpackAction(String packed, int offset, int [] result) {
+    int i = 0;       /* index in packed string  */
+    int j = offset;  /* index in unpacked array */
+    int l = packed.length();
+    while (i < l) {
+      int count = packed.charAt(i++);
+      int value = packed.charAt(i++);
+      do result[j++] = value; while (--count > 0);
+    }
+    return j;
+  }
+
+
+  /** 
+   * Translates a state to a row index in the transition table
+   */
+  private static final int [] ZZ_ROWMAP = zzUnpackRowMap();
+
+  private static final String ZZ_ROWMAP_PACKED_0 =
+    "\0\0\0\16\0\34\0\52\0\70\0\16\0\106\0\124"+
+    "\0\142\0\160\0\176\0\214\0\232\0\250\0\266\0\304"+
+    "\0\322\0\340\0\356\0\374\0\u010a\0\u0118\0\u0126\0\u0134"+
+    "\0\u0142\0\u0150\0\u015e\0\u016c\0\u017a\0\u0188\0\u0196\0\u01a4"+
+    "\0\u01b2\0\u01c0\0\u01ce\0\u01dc\0\u01ea\0\u01f8\0\322\0\u0206"+
+    "\0\u0214\0\u0222\0\u0230\0\u023e\0\u024c\0\u025a\0\124\0\214"+
+    "\0\u0268\0\u0276\0\u0284";
+
+  private static int [] zzUnpackRowMap() {
+    int [] result = new int[51];
+    int offset = 0;
+    offset = zzUnpackRowMap(ZZ_ROWMAP_PACKED_0, offset, result);
+    return result;
+  }
+
+  private static int zzUnpackRowMap(String packed, int offset, int [] result) {
+    int i = 0;  /* index in packed string  */
+    int j = offset;  /* index in unpacked array */
+    int l = packed.length();
+    while (i < l) {
+      int high = packed.charAt(i++) << 16;
+      result[j++] = high | packed.charAt(i++);
+    }
+    return j;
+  }
+
+  /** 
+   * The transition table of the DFA
+   */
+  private static final int [] ZZ_TRANS = zzUnpackTrans();
+
+  private static final String ZZ_TRANS_PACKED_0 =
+    "\1\2\1\3\1\4\7\2\1\5\1\6\1\7\1\2"+
+    "\17\0\2\3\1\0\1\10\1\0\1\11\2\12\1\13"+
+    "\1\3\4\0\1\3\1\4\1\0\1\14\1\0\1\11"+
+    "\2\15\1\16\1\4\4\0\1\3\1\4\1\17\1\20"+
+    "\1\21\1\22\2\12\1\13\1\23\20\0\1\2\1\0"+
+    "\1\24\1\25\7\0\1\26\4\0\2\27\7\0\1\27"+
+    "\4\0\1\30\1\31\7\0\1\32\5\0\1\33\7\0"+
+    "\1\13\4\0\1\34\1\35\7\0\1\36\4\0\1\37"+
+    "\1\40\7\0\1\41\4\0\1\42\1\43\7\0\1\44"+
+    "\15\0\1\45\4\0\1\24\1\25\7\0\1\46\15\0"+
+    "\1\47\4\0\2\27\7\0\1\50\4\0\1\3\1\4"+
+    "\1\17\1\10\1\21\1\22\2\12\1\13\1\23\4\0"+
+    "\2\24\1\0\1\51\1\0\1\11\2\52\1\0\1\24"+
+    "\4\0\1\24\1\25\1\0\1\53\1\0\1\11\2\54"+
+    "\1\55\1\25\4\0\1\24\1\25\1\0\1\51\1\0"+
+    "\1\11\2\52\1\0\1\26\4\0\2\27\1\0\1\56"+
+    "\2\0\1\56\2\0\1\27\4\0\2\30\1\0\1\52"+
+    "\1\0\1\11\2\52\1\0\1\30\4\0\1\30\1\31"+
+    "\1\0\1\54\1\0\1\11\2\54\1\55\1\31\4\0"+
+    "\1\30\1\31\1\0\1\52\1\0\1\11\2\52\1\0"+
+    "\1\32\5\0\1\33\1\0\1\55\2\0\3\55\1\33"+
+    "\4\0\2\34\1\0\1\57\1\0\1\11\2\12\1\13"+
+    "\1\34\4\0\1\34\1\35\1\0\1\60\1\0\1\11"+
+    "\2\15\1\16\1\35\4\0\1\34\1\35\1\0\1\57"+
+    "\1\0\1\11\2\12\1\13\1\36\4\0\2\37\1\0"+
+    "\1\12\1\0\1\11\2\12\1\13\1\37\4\0\1\37"+
+    "\1\40\1\0\1\15\1\0\1\11\2\15\1\16\1\40"+
+    "\4\0\1\37\1\40\1\0\1\12\1\0\1\11\2\12"+
+    "\1\13\1\41\4\0\2\42\1\0\1\13\2\0\3\13"+
+    "\1\42\4\0\1\42\1\43\1\0\1\16\2\0\3\16"+
+    "\1\43\4\0\1\42\1\43\1\0\1\13\2\0\3\13"+
+    "\1\44\6\0\1\17\6\0\1\45\4\0\1\24\1\25"+
+    "\1\0\1\61\1\0\1\11\2\52\1\0\1\26\4\0"+
+    "\2\27\1\0\1\56\2\0\1\56\2\0\1\50\4\0"+
+    "\2\24\7\0\1\24\4\0\2\30\7\0\1\30\4\0"+
+    "\2\34\7\0\1\34\4\0\2\37\7\0\1\37\4\0"+
+    "\2\42\7\0\1\42\4\0\2\62\7\0\1\62\4\0"+
+    "\2\24\7\0\1\63\4\0\2\62\1\0\1\56\2\0"+
+    "\1\56\2\0\1\62\4\0\2\24\1\0\1\61\1\0"+
+    "\1\11\2\52\1\0\1\24\3\0";
+
+  private static int [] zzUnpackTrans() {
+    int [] result = new int[658];
+    int offset = 0;
+    offset = zzUnpackTrans(ZZ_TRANS_PACKED_0, offset, result);
+    return result;
+  }
+
+  private static int zzUnpackTrans(String packed, int offset, int [] result) {
+    int i = 0;       /* index in packed string  */
+    int j = offset;  /* index in unpacked array */
+    int l = packed.length();
+    while (i < l) {
+      int count = packed.charAt(i++);
+      int value = packed.charAt(i++);
+      value--;
+      do result[j++] = value; while (--count > 0);
+    }
+    return j;
+  }
+
+
+  /* error codes */
+  private static final int ZZ_UNKNOWN_ERROR = 0;
+  private static final int ZZ_NO_MATCH = 1;
+  private static final int ZZ_PUSHBACK_2BIG = 2;
+
+  /* error messages for the codes above */
+  private static final String ZZ_ERROR_MSG[] = {
+    "Unkown internal scanner error",
+    "Error: could not match input",
+    "Error: pushback value was too large"
+  };
+
+  /**
+   * ZZ_ATTRIBUTE[aState] contains the attributes of state <code>aState</code>
+   */
+  private static final int [] ZZ_ATTRIBUTE = zzUnpackAttribute();
+
+  private static final String ZZ_ATTRIBUTE_PACKED_0 =
+    "\1\0\1\11\3\1\1\11\1\1\13\0\4\1\2\0"+
+    "\1\1\1\0\17\1\1\0\1\1\3\0\5\1";
+
+  private static int [] zzUnpackAttribute() {
+    int [] result = new int[51];
+    int offset = 0;
+    offset = zzUnpackAttribute(ZZ_ATTRIBUTE_PACKED_0, offset, result);
+    return result;
+  }
+
+  private static int zzUnpackAttribute(String packed, int offset, int [] result) {
+    int i = 0;       /* index in packed string  */
+    int j = offset;  /* index in unpacked array */
+    int l = packed.length();
+    while (i < l) {
+      int count = packed.charAt(i++);
+      int value = packed.charAt(i++);
+      do result[j++] = value; while (--count > 0);
+    }
+    return j;
+  }
+
+  /** the input device */
+  private java.io.Reader zzReader;
+
+  /** the current state of the DFA */
+  private int zzState;
+
+  /** the current lexical state */
+  private int zzLexicalState = YYINITIAL;
+
+  /** this buffer contains the current text to be matched and is
+      the source of the yytext() string */
+  private char zzBuffer[] = new char[ZZ_BUFFERSIZE];
+
+  /** the textposition at the last accepting state */
+  private int zzMarkedPos;
+
+  /** the current text position in the buffer */
+  private int zzCurrentPos;
+
+  /** startRead marks the beginning of the yytext() string in the buffer */
+  private int zzStartRead;
+
+  /** endRead marks the last character in the buffer, that has been read
+      from input */
+  private int zzEndRead;
+
+  /** number of newlines encountered up to the start of the matched text */
+  private int yyline;
+
+  /** the number of characters up to the start of the matched text */
+  private int yychar;
+
+  /**
+   * the number of characters from the last newline up to the start of the 
+   * matched text
+   */
+  private int yycolumn;
+
+  /** 
+   * zzAtBOL == true <=> the scanner is currently at the beginning of a line
+   */
+  private boolean zzAtBOL = true;
+
+  /** zzAtEOF == true <=> the scanner is at the EOF */
+  private boolean zzAtEOF;
+
+  /** denotes if the user-EOF-code has already been executed */
+  private boolean zzEOFDone;
+
+  /* user code: */
+
+public static final int ALPHANUM          = StandardTokenizer.ALPHANUM;
+public static final int APOSTROPHE        = StandardTokenizer.APOSTROPHE;
+public static final int ACRONYM           = StandardTokenizer.ACRONYM;
+public static final int COMPANY           = StandardTokenizer.COMPANY;
+public static final int EMAIL             = StandardTokenizer.EMAIL;
+public static final int HOST              = StandardTokenizer.HOST;
+public static final int NUM               = StandardTokenizer.NUM;
+public static final int CJ                = StandardTokenizer.CJ;
+/**
+ * @deprecated this solves a bug where HOSTs that end with '.' are identified
+ *             as ACRONYMs.
+ */
+public static final int ACRONYM_DEP       = StandardTokenizer.ACRONYM_DEP;
+
+public static final String [] TOKEN_TYPES = StandardTokenizer.TOKEN_TYPES;
+
+public final int yychar()
+{
+    return yychar;
+}
+
+/**
+ * Fills CharTermAttribute with the current token text.
+ */
+public final void getText(CharTermAttribute t) {
+  t.copyBuffer(zzBuffer, zzStartRead, zzMarkedPos-zzStartRead);
+}
+
+
+
+  /**
+   * Creates a new scanner
+   * There is also a java.io.InputStream version of this constructor.
+   *
+   * @param   in  the java.io.Reader to read input from.
+   */
+  StandardTokenizerImplOrig(java.io.Reader in) {
+    this.zzReader = in;
+  }
+
+  /**
+   * Creates a new scanner.
+   * There is also java.io.Reader version of this constructor.
+   *
+   * @param   in  the java.io.Inputstream to read input from.
+   */
+  StandardTokenizerImplOrig(java.io.InputStream in) {
+    this(new java.io.InputStreamReader(in));
+  }
+
+  /** 
+   * Unpacks the compressed character translation table.
+   *
+   * @param packed   the packed character translation table
+   * @return         the unpacked character translation table
+   */
+  private static char [] zzUnpackCMap(String packed) {
+    char [] map = new char[0x10000];
+    int i = 0;  /* index in packed string  */
+    int j = 0;  /* index in unpacked array */
+    while (i < 1154) {
+      int  count = packed.charAt(i++);
+      char value = packed.charAt(i++);
+      do map[j++] = value; while (--count > 0);
+    }
+    return map;
+  }
+
+
+  /**
+   * Refills the input buffer.
+   *
+   * @return      <code>false</code>, iff there was new input.
+   * 
+   * @exception   java.io.IOException  if any I/O-Error occurs
+   */
+  private boolean zzRefill() throws java.io.IOException {
+
+    /* first: make room (if you can) */
+    if (zzStartRead > 0) {
+      System.arraycopy(zzBuffer, zzStartRead,
+                       zzBuffer, 0,
+                       zzEndRead-zzStartRead);
+
+      /* translate stored positions */
+      zzEndRead-= zzStartRead;
+      zzCurrentPos-= zzStartRead;
+      zzMarkedPos-= zzStartRead;
+      zzStartRead = 0;
+    }
+
+    /* is the buffer big enough? */
+    if (zzCurrentPos >= zzBuffer.length) {
+      /* if not: blow it up */
+      char newBuffer[] = new char[zzCurrentPos*2];
+      System.arraycopy(zzBuffer, 0, newBuffer, 0, zzBuffer.length);
+      zzBuffer = newBuffer;
+    }
+
+    /* finally: fill the buffer with new input */
+    int numRead = zzReader.read(zzBuffer, zzEndRead,
+                                            zzBuffer.length-zzEndRead);
+
+    if (numRead > 0) {
+      zzEndRead+= numRead;
+      return false;
+    }
+    // unlikely but not impossible: read 0 characters, but not at end of stream    
+    if (numRead == 0) {
+      int c = zzReader.read();
+      if (c == -1) {
+        return true;
+      } else {
+        zzBuffer[zzEndRead++] = (char) c;
+        return false;
+      }     
+    }
+
+	// numRead < 0
+    return true;
+  }
+
+    
+  /**
+   * Closes the input stream.
+   */
+  public final void yyclose() throws java.io.IOException {
+    zzAtEOF = true;            /* indicate end of file */
+    zzEndRead = zzStartRead;  /* invalidate buffer    */
+
+    if (zzReader != null)
+      zzReader.close();
+  }
+
+
+  /**
+   * Resets the scanner to read from a new input stream.
+   * Does not close the old reader.
+   *
+   * All internal variables are reset, the old input stream 
+   * <b>cannot</b> be reused (internal buffer is discarded and lost).
+   * Lexical state is set to <tt>ZZ_INITIAL</tt>.
+   *
+   * Internal scan buffer is resized down to its initial length, if it has grown.
+   *
+   * @param reader   the new input stream 
+   */
+  public final void yyreset(java.io.Reader reader) {
+    zzReader = reader;
+    zzAtBOL  = true;
+    zzAtEOF  = false;
+    zzEOFDone = false;
+    zzEndRead = zzStartRead = 0;
+    zzCurrentPos = zzMarkedPos = 0;
+    yyline = yychar = yycolumn = 0;
+    zzLexicalState = YYINITIAL;
+    if (zzBuffer.length > ZZ_BUFFERSIZE)
+      zzBuffer = new char[ZZ_BUFFERSIZE];
+  }
+
+
+  /**
+   * Returns the current lexical state.
+   */
+  public final int yystate() {
+    return zzLexicalState;
+  }
+
+
+  /**
+   * Enters a new lexical state
+   *
+   * @param newState the new lexical state
+   */
+  public final void yybegin(int newState) {
+    zzLexicalState = newState;
+  }
+
+
+  /**
+   * Returns the text matched by the current regular expression.
+   */
+  public final String yytext() {
+    return new String( zzBuffer, zzStartRead, zzMarkedPos-zzStartRead );
+  }
+
+
+  /**
+   * Returns the character at position <tt>pos</tt> from the 
+   * matched text. 
+   * 
+   * It is equivalent to yytext().charAt(pos), but faster
+   *
+   * @param pos the position of the character to fetch. 
+   *            A value from 0 to yylength()-1.
+   *
+   * @return the character at position pos
+   */
+  public final char yycharat(int pos) {
+    return zzBuffer[zzStartRead+pos];
+  }
+
+
+  /**
+   * Returns the length of the matched text region.
+   */
+  public final int yylength() {
+    return zzMarkedPos-zzStartRead;
+  }
+
+
+  /**
+   * Reports an error that occured while scanning.
+   *
+   * In a wellformed scanner (no or only correct usage of 
+   * yypushback(int) and a match-all fallback rule) this method 
+   * will only be called with things that "Can't Possibly Happen".
+   * If this method is called, something is seriously wrong
+   * (e.g. a JFlex bug producing a faulty scanner etc.).
+   *
+   * Usual syntax/scanner level error handling should be done
+   * in error fallback rules.
+   *
+   * @param   errorCode  the code of the errormessage to display
+   */
+  private void zzScanError(int errorCode) {
+    String message;
+    try {
+      message = ZZ_ERROR_MSG[errorCode];
+    }
+    catch (ArrayIndexOutOfBoundsException e) {
+      message = ZZ_ERROR_MSG[ZZ_UNKNOWN_ERROR];
+    }
+
+    throw new Error(message);
+  } 
+
+
+  /**
+   * Pushes the specified amount of characters back into the input stream.
+   *
+   * They will be read again by then next call of the scanning method
+   *
+   * @param number  the number of characters to be read again.
+   *                This number must not be greater than yylength()!
+   */
+  public void yypushback(int number)  {
+    if ( number > yylength() )
+      zzScanError(ZZ_PUSHBACK_2BIG);
+
+    zzMarkedPos -= number;
+  }
+
+
+  /**
+   * Resumes scanning until the next regular expression is matched,
+   * the end of input is encountered or an I/O-Error occurs.
+   *
+   * @return      the next token
+   * @exception   java.io.IOException  if any I/O-Error occurs
+   */
+  public int getNextToken() throws java.io.IOException {
+    int zzInput;
+    int zzAction;
+
+    // cached fields:
+    int zzCurrentPosL;
+    int zzMarkedPosL;
+    int zzEndReadL = zzEndRead;
+    char [] zzBufferL = zzBuffer;
+    char [] zzCMapL = ZZ_CMAP;
+
+    int [] zzTransL = ZZ_TRANS;
+    int [] zzRowMapL = ZZ_ROWMAP;
+    int [] zzAttrL = ZZ_ATTRIBUTE;
+
+    while (true) {
+      zzMarkedPosL = zzMarkedPos;
+
+      yychar+= zzMarkedPosL-zzStartRead;
+
+      zzAction = -1;
+
+      zzCurrentPosL = zzCurrentPos = zzStartRead = zzMarkedPosL;
+  
+      zzState = ZZ_LEXSTATE[zzLexicalState];
+
+
+      zzForAction: {
+        while (true) {
+    
+          if (zzCurrentPosL < zzEndReadL)
+            zzInput = zzBufferL[zzCurrentPosL++];
+          else if (zzAtEOF) {
+            zzInput = YYEOF;
+            break zzForAction;
+          }
+          else {
+            // store back cached positions
+            zzCurrentPos  = zzCurrentPosL;
+            zzMarkedPos   = zzMarkedPosL;
+            boolean eof = zzRefill();
+            // get translated positions and possibly new buffer
+            zzCurrentPosL  = zzCurrentPos;
+            zzMarkedPosL   = zzMarkedPos;
+            zzBufferL      = zzBuffer;
+            zzEndReadL     = zzEndRead;
+            if (eof) {
+              zzInput = YYEOF;
+              break zzForAction;
+            }
+            else {
+              zzInput = zzBufferL[zzCurrentPosL++];
+            }
+          }
+          int zzNext = zzTransL[ zzRowMapL[zzState] + zzCMapL[zzInput] ];
+          if (zzNext == -1) break zzForAction;
+          zzState = zzNext;
+
+          int zzAttributes = zzAttrL[zzState];
+          if ( (zzAttributes & 1) == 1 ) {
+            zzAction = zzState;
+            zzMarkedPosL = zzCurrentPosL;
+            if ( (zzAttributes & 8) == 8 ) break zzForAction;
+          }
+
+        }
+      }
+
+      // store back cached position
+      zzMarkedPos = zzMarkedPosL;
+
+      switch (zzAction < 0 ? zzAction : ZZ_ACTION[zzAction]) {
+        case 5: 
+          { return NUM;
+          }
+        case 11: break;
+        case 9: 
+          { return ACRONYM;
+          }
+        case 12: break;
+        case 7: 
+          { return COMPANY;
+          }
+        case 13: break;
+        case 10: 
+          { return EMAIL;
+          }
+        case 14: break;
+        case 1: 
+          { /* ignore */
+          }
+        case 15: break;
+        case 6: 
+          { return APOSTROPHE;
+          }
+        case 16: break;
+        case 3: 
+          { return CJ;
+          }
+        case 17: break;
+        case 8: 
+          { return ACRONYM_DEP;
+          }
+        case 18: break;
+        case 2: 
+          { return ALPHANUM;
+          }
+        case 19: break;
+        case 4: 
+          { return HOST;
+          }
+        case 20: break;
+        default: 
+          if (zzInput == YYEOF && zzStartRead == zzCurrentPos) {
+            zzAtEOF = true;
+            return YYEOF;
+          } 
+          else {
+            zzScanError(ZZ_NO_MATCH);
+          }
+      }
+    }
+  }
+
+
+}
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImplOrig.jflex b/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImplOrig.jflex
new file mode 100644
index 0000000..93aca3a
--- /dev/null
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImplOrig.jflex
@@ -0,0 +1,134 @@
+package org.apache.lucene.analysis.standard;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/*
+
+WARNING: if you change StandardTokenizerImpl*.jflex and need to regenerate
+      the tokenizer, only use the trunk version of JFlex 1.5 at the moment!
+
+*/
+
+import java.io.Reader;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+
+%%
+
+%class StandardTokenizerImplOrig
+%implements StandardTokenizerInterface
+%unicode 3.0
+%integer
+%function getNextToken
+%pack
+%char
+
+%{
+
+public static final int ALPHANUM          = StandardTokenizer.ALPHANUM;
+public static final int APOSTROPHE        = StandardTokenizer.APOSTROPHE;
+public static final int ACRONYM           = StandardTokenizer.ACRONYM;
+public static final int COMPANY           = StandardTokenizer.COMPANY;
+public static final int EMAIL             = StandardTokenizer.EMAIL;
+public static final int HOST              = StandardTokenizer.HOST;
+public static final int NUM               = StandardTokenizer.NUM;
+public static final int CJ                = StandardTokenizer.CJ;
+/**
+ * @deprecated this solves a bug where HOSTs that end with '.' are identified
+ *             as ACRONYMs.
+ */
+public static final int ACRONYM_DEP       = StandardTokenizer.ACRONYM_DEP;
+
+public static final String [] TOKEN_TYPES = StandardTokenizer.TOKEN_TYPES;
+
+public final int yychar()
+{
+    return yychar;
+}
+
+/**
+ * Fills CharTermAttribute with the current token text.
+ */
+public final void getText(CharTermAttribute t) {
+  t.copyBuffer(zzBuffer, zzStartRead, zzMarkedPos-zzStartRead);
+}
+
+%}
+
+THAI       = [\u0E00-\u0E59]
+
+// basic word: a sequence of digits & letters (includes Thai to enable ThaiAnalyzer to function)
+ALPHANUM   = ({LETTER}|{THAI}|[:digit:])+
+
+// internal apostrophes: O'Reilly, you're, O'Reilly's
+// use a post-filter to remove possessives
+APOSTROPHE =  {ALPHA} ("'" {ALPHA})+
+
+// acronyms: U.S.A., I.B.M., etc.
+// use a post-filter to remove dots
+ACRONYM    =  {LETTER} "." ({LETTER} ".")+
+
+ACRONYM_DEP	= {ALPHANUM} "." ({ALPHANUM} ".")+
+
+// company names like AT&T and Excite@Home.
+COMPANY    =  {ALPHA} ("&"|"@") {ALPHA}
+
+// email addresses
+EMAIL      =  {ALPHANUM} (("."|"-"|"_") {ALPHANUM})* "@" {ALPHANUM} (("."|"-") {ALPHANUM})+
+
+// hostname
+HOST       =  {ALPHANUM} ((".") {ALPHANUM})+
+
+// floating point, serial, model numbers, ip addresses, etc.
+// every other segment must have at least one digit
+NUM        = ({ALPHANUM} {P} {HAS_DIGIT}
+           | {HAS_DIGIT} {P} {ALPHANUM}
+           | {ALPHANUM} ({P} {HAS_DIGIT} {P} {ALPHANUM})+
+           | {HAS_DIGIT} ({P} {ALPHANUM} {P} {HAS_DIGIT})+
+           | {ALPHANUM} {P} {HAS_DIGIT} ({P} {ALPHANUM} {P} {HAS_DIGIT})+
+           | {HAS_DIGIT} {P} {ALPHANUM} ({P} {HAS_DIGIT} {P} {ALPHANUM})+)
+
+// punctuation
+P	         = ("_"|"-"|"/"|"."|",")
+
+// at least one digit
+HAS_DIGIT  = ({LETTER}|[:digit:])* [:digit:] ({LETTER}|[:digit:])*
+
+ALPHA      = ({LETTER})+
+
+// From the JFlex manual: "the expression that matches everything of <a> not matched by <b> is !(!<a>|<b>)"
+LETTER     = !(![:letter:]|{CJ})
+
+// Chinese and Japanese (but NOT Korean, which is included in [:letter:])
+CJ         = [\u3100-\u312f\u3040-\u309F\u30A0-\u30FF\u31F0-\u31FF\u3300-\u337f\u3400-\u4dbf\u4e00-\u9fff\uf900-\ufaff\uff65-\uff9f]
+
+WHITESPACE = \r\n | [ \r\n\t\f]
+
+%%
+
+{ALPHANUM}                                                     { return ALPHANUM; }
+{APOSTROPHE}                                                   { return APOSTROPHE; }
+{ACRONYM}                                                      { return ACRONYM; }
+{COMPANY}                                                      { return COMPANY; }
+{EMAIL}                                                        { return EMAIL; }
+{HOST}                                                         { return HOST; }
+{NUM}                                                          { return NUM; }
+{CJ}                                                           { return CJ; }
+{ACRONYM_DEP}                                                  { return ACRONYM_DEP; }
+
+/** Ignore the rest */
+. | {WHITESPACE}                                               { /* ignore */ }
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerInterface.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerInterface.java
new file mode 100644
index 0000000..f78cee3
--- /dev/null
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerInterface.java
@@ -0,0 +1,66 @@
+package org.apache.lucene.analysis.standard;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+
+import java.io.Reader;
+import java.io.IOException;
+
+interface StandardTokenizerInterface {
+
+  /** This character denotes the end of file */
+  public static final int YYEOF = -1;
+
+  /**
+   * Copies the matched text into the CharTermAttribute
+   */
+  void getText(CharTermAttribute t);
+
+  /**
+   * Returns the current position.
+   */
+  int yychar();
+
+  /**
+   * Resets the scanner to read from a new input stream.
+   * Does not close the old reader.
+   *
+   * All internal variables are reset, the old input stream 
+   * <b>cannot</b> be reused (internal buffer is discarded and lost).
+   * Lexical state is set to <tt>ZZ_INITIAL</tt>.
+   *
+   * @param reader   the new input stream 
+   */
+  void yyreset(Reader reader);
+
+  /**
+   * Returns the length of the matched text region.
+   */
+  int yylength();
+
+  /**
+   * Resumes scanning until the next regular expression is matched,
+   * the end of input is encountered or an I/O-Error occurs.
+   *
+   * @return      the next token, {@link #YYEOF} on end of stream
+   * @exception   IOException  if any I/O-Error occurs
+   */
+  int getNextToken() throws IOException;
+
+}
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/package.html b/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/package.html
new file mode 100644
index 0000000..6035d9b
--- /dev/null
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/package.html
@@ -0,0 +1,25 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
+</head>
+<body>
+A fast grammar-based tokenizer constructed with JFlex.
+</body>
+</html>
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/sv/SwedishAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/sv/SwedishAnalyzer.java
index 49653c7..dc59097 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/sv/SwedishAnalyzer.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/sv/SwedishAnalyzer.java
@@ -23,16 +23,16 @@ import java.util.Set;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.snowball.SnowballFilter;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 import org.tartarus.snowball.ext.SwedishStemmer;
 
@@ -106,11 +106,11 @@ public final class SwedishAnalyzer extends StopwordAnalyzerBase {
 
   /**
    * Creates a
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * which tokenizes all the text in the provided {@link Reader}.
    * 
    * @return A
-   *         {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   *         {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from an {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
    *         , {@link KeywordMarkerFilter} if a stem exclusion set is
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiAnalyzer.java
index d67ff98..f0eb1d6 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiAnalyzer.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiAnalyzer.java
@@ -18,16 +18,16 @@ package org.apache.lucene.analysis.th;
 
 import java.io.Reader;
 
-import org.apache.lucene.analysis.ReusableAnalyzerBase;
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.StopAnalyzer;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopAnalyzer;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
 import org.apache.lucene.util.Version;
 
 /**
@@ -46,10 +46,10 @@ public final class ThaiAnalyzer extends ReusableAnalyzerBase {
 
   /**
    * Creates
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * used to tokenize all the text in the provided {@link Reader}.
    * 
-   * @return {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * @return {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from a {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link ThaiWordFilter}, and
    *         {@link StopFilter}
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiWordFilter.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiWordFilter.java
index 831bba3..9751c1a 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiWordFilter.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiWordFilter.java
@@ -24,7 +24,7 @@ import java.text.BreakIterator;
 
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.LowerCaseFilter;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/tr/TurkishAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/tr/TurkishAnalyzer.java
index b0d9a47..313c017 100644
--- a/modules/analysis/common/src/java/org/apache/lucene/analysis/tr/TurkishAnalyzer.java
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/tr/TurkishAnalyzer.java
@@ -23,14 +23,14 @@ import java.util.Set;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.snowball.SnowballFilter;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
 import org.apache.lucene.util.Version;
 import org.tartarus.snowball.ext.TurkishStemmer;
 
@@ -109,11 +109,11 @@ public final class TurkishAnalyzer extends StopwordAnalyzerBase {
 
   /**
    * Creates a
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * which tokenizes all the text in the provided {@link Reader}.
    * 
    * @return A
-   *         {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   *         {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from an {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link TurkishLowerCaseFilter},
    *         {@link StopFilter}, {@link KeywordMarkerFilter} if a stem
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/util/ReusableAnalyzerBase.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/util/ReusableAnalyzerBase.java
new file mode 100644
index 0000000..ba0575d
--- /dev/null
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/util/ReusableAnalyzerBase.java
@@ -0,0 +1,168 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.analysis.util;
+
+import java.io.IOException;
+import java.io.Reader;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.TokenFilter;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.Tokenizer;
+
+/**
+ * An convenience subclass of Analyzer that makes it easy to implement
+ * {@link TokenStream} reuse.
+ * <p>
+ * ReusableAnalyzerBase is a simplification of Analyzer that supports easy reuse
+ * for the most common use-cases. Analyzers such as
+ * {@link PerFieldAnalyzerWrapper} that behave differently depending upon the
+ * field name need to subclass Analyzer directly instead.
+ * </p>
+ * <p>
+ * To prevent consistency problems, this class does not allow subclasses to
+ * extend {@link #reusableTokenStream(String, Reader)} or
+ * {@link #tokenStream(String, Reader)} directly. Instead, subclasses must
+ * implement {@link #createComponents(String, Reader)}.
+ * </p>
+ */
+public abstract class ReusableAnalyzerBase extends Analyzer {
+
+  /**
+   * Creates a new {@link TokenStreamComponents} instance for this analyzer.
+   * 
+   * @param fieldName
+   *          the name of the fields content passed to the
+   *          {@link TokenStreamComponents} sink as a reader
+   * @param aReader
+   *          the reader passed to the {@link Tokenizer} constructor
+   * @return the {@link TokenStreamComponents} for this analyzer.
+   */
+  protected abstract TokenStreamComponents createComponents(String fieldName,
+      Reader aReader);
+
+  /**
+   * This method uses {@link #createComponents(String, Reader)} to obtain an
+   * instance of {@link TokenStreamComponents}. It returns the sink of the
+   * components and stores the components internally. Subsequent calls to this
+   * method will reuse the previously stored components if and only if the
+   * {@link TokenStreamComponents#reset(Reader)} method returned
+   * <code>true</code>. Otherwise a new instance of
+   * {@link TokenStreamComponents} is created.
+   * 
+   * @param fieldName the name of the field the created TokenStream is used for
+   * @param reader the reader the streams source reads from
+   */
+  @Override
+  public final TokenStream reusableTokenStream(final String fieldName,
+      final Reader reader) throws IOException {
+    TokenStreamComponents streamChain = (TokenStreamComponents)
+    getPreviousTokenStream();
+    if (streamChain == null || !streamChain.reset(reader)) {
+      streamChain = createComponents(fieldName, reader);
+      setPreviousTokenStream(streamChain);
+    }
+    return streamChain.getTokenStream();
+  }
+
+  /**
+   * This method uses {@link #createComponents(String, Reader)} to obtain an
+   * instance of {@link TokenStreamComponents} and returns the sink of the
+   * components. Each calls to this method will create a new instance of
+   * {@link TokenStreamComponents}. Created {@link TokenStream} instances are 
+   * never reused.
+   * 
+   * @param fieldName the name of the field the created TokenStream is used for
+   * @param reader the reader the streams source reads from
+   */
+  @Override
+  public final TokenStream tokenStream(final String fieldName,
+      final Reader reader) {
+    return createComponents(fieldName, reader).getTokenStream();
+  }
+  
+  /**
+   * This class encapsulates the outer components of a token stream. It provides
+   * access to the source ({@link Tokenizer}) and the outer end (sink), an
+   * instance of {@link TokenFilter} which also serves as the
+   * {@link TokenStream} returned by
+   * {@link Analyzer#tokenStream(String, Reader)} and
+   * {@link Analyzer#reusableTokenStream(String, Reader)}.
+   */
+  public static class TokenStreamComponents {
+    protected final Tokenizer source;
+    protected final TokenStream sink;
+
+    /**
+     * Creates a new {@link TokenStreamComponents} instance.
+     * 
+     * @param source
+     *          the analyzer's tokenizer
+     * @param result
+     *          the analyzer's resulting token stream
+     */
+    public TokenStreamComponents(final Tokenizer source,
+        final TokenStream result) {
+      this.source = source;
+      this.sink = result;
+    }
+    
+    /**
+     * Creates a new {@link TokenStreamComponents} instance.
+     * 
+     * @param source
+     *          the analyzer's tokenizer
+     */
+    public TokenStreamComponents(final Tokenizer source) {
+      this.source = source;
+      this.sink = source;
+    }
+
+    /**
+     * Resets the encapsulated components with the given reader. This method by
+     * default returns <code>true</code> indicating that the components have
+     * been reset successfully. Subclasses of {@link ReusableAnalyzerBase} might use
+     * their own {@link TokenStreamComponents} returning <code>false</code> if
+     * the components cannot be reset.
+     * 
+     * @param reader
+     *          a reader to reset the source component
+     * @return <code>true</code> if the components were reset, otherwise
+     *         <code>false</code>
+     * @throws IOException
+     *           if the component's reset method throws an {@link IOException}
+     */
+    protected boolean reset(final Reader reader) throws IOException {
+      source.reset(reader);
+      if(sink != source)
+        sink.reset(); // only reset if the sink reference is different from source
+      return true;
+    }
+
+    /**
+     * Returns the sink {@link TokenStream}
+     * 
+     * @return the sink {@link TokenStream}
+     */
+    protected TokenStream getTokenStream() {
+      return sink;
+    }
+
+  }
+
+}
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/util/StopwordAnalyzerBase.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/util/StopwordAnalyzerBase.java
new file mode 100644
index 0000000..8ff6886
--- /dev/null
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/util/StopwordAnalyzerBase.java
@@ -0,0 +1,105 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.analysis.util;
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
+import org.apache.lucene.util.Version;
+
+/**
+ * Base class for Analyzers that need to make use of stopword sets. 
+ * 
+ */
+public abstract class StopwordAnalyzerBase extends ReusableAnalyzerBase {
+
+  /**
+   * An immutable stopword set
+   */
+  protected final CharArraySet stopwords;
+
+  protected final Version matchVersion;
+
+  /**
+   * Returns the analyzer's stopword set or an empty set if the analyzer has no
+   * stopwords
+   * 
+   * @return the analyzer's stopword set or an empty set if the analyzer has no
+   *         stopwords
+   */
+  public Set<?> getStopwordSet() {
+    return stopwords;
+  }
+
+  /**
+   * Creates a new instance initialized with the given stopword set
+   * 
+   * @param version
+   *          the Lucene version for cross version compatibility
+   * @param stopwords
+   *          the analyzer's stopword set
+   */
+  protected StopwordAnalyzerBase(final Version version, final Set<?> stopwords) {
+    matchVersion = version;
+    // analyzers should use char array set for stopwords!
+    this.stopwords = stopwords == null ? CharArraySet.EMPTY_SET : CharArraySet
+        .unmodifiableSet(CharArraySet.copy(version, stopwords));
+  }
+
+  /**
+   * Creates a new Analyzer with an empty stopword set
+   * 
+   * @param version
+   *          the Lucene version for cross version compatibility
+   */
+  protected StopwordAnalyzerBase(final Version version) {
+    this(version, null);
+  }
+
+  /**
+   * Creates a CharArraySet from a file resource associated with a class. (See
+   * {@link Class#getResourceAsStream(String)}).
+   * 
+   * @param ignoreCase
+   *          <code>true</code> if the set should ignore the case of the
+   *          stopwords, otherwise <code>false</code>
+   * @param aClass
+   *          a class that is associated with the given stopwordResource
+   * @param resource
+   *          name of the resource file associated with the given class
+   * @param comment
+   *          comment string to ignore in the stopword file
+   * @return a CharArraySet containing the distinct stopwords from the given
+   *         file
+   * @throws IOException
+   *           if loading the stopwords throws an {@link IOException}
+   */
+  protected static CharArraySet loadStopwordSet(final boolean ignoreCase,
+      final Class<? extends ReusableAnalyzerBase> aClass, final String resource,
+      final String comment) throws IOException {
+    final Set<String> wordSet = WordlistLoader.getWordSet(aClass, resource,
+        comment);
+    final CharArraySet set = new CharArraySet(Version.LUCENE_31, wordSet.size(), ignoreCase);
+    set.addAll(wordSet);
+    return set;
+  }
+
+}
diff --git a/modules/analysis/common/src/java/org/apache/lucene/analysis/util/WordlistLoader.java b/modules/analysis/common/src/java/org/apache/lucene/analysis/util/WordlistLoader.java
new file mode 100644
index 0000000..78aa03d
--- /dev/null
+++ b/modules/analysis/common/src/java/org/apache/lucene/analysis/util/WordlistLoader.java
@@ -0,0 +1,284 @@
+package org.apache.lucene.analysis.util;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.FileReader;
+import java.io.IOException;
+import java.io.InputStreamReader;
+import java.io.Reader;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Set;
+
+/**
+ * Loader for text files that represent a list of stopwords.
+ */
+public class WordlistLoader {
+ 
+  /**
+   * Loads a text file associated with a given class (See
+   * {@link Class#getResourceAsStream(String)}) and adds every line as an entry
+   * to a {@link Set} (omitting leading and trailing whitespace). Every line of
+   * the file should contain only one word. The words need to be in lower-case if
+   * you make use of an Analyzer which uses LowerCaseFilter (like
+   * StandardAnalyzer).
+   * 
+   * @param aClass
+   *          a class that is associated with the given stopwordResource
+   * @param stopwordResource
+   *          name of the resource file associated with the given class
+   * @return a {@link Set} with the file's words
+   */
+  public static Set<String> getWordSet(Class<?> aClass, String stopwordResource)
+      throws IOException {
+    final Reader reader = new BufferedReader(new InputStreamReader(aClass
+        .getResourceAsStream(stopwordResource), "UTF-8"));
+    try {
+      return getWordSet(reader);
+    } finally {
+      reader.close();
+    }
+  }
+  
+  /**
+   * Loads a text file associated with a given class (See
+   * {@link Class#getResourceAsStream(String)}) and adds every line as an entry
+   * to a {@link Set} (omitting leading and trailing whitespace). Every line of
+   * the file should contain only one word. The words need to be in lower-case if
+   * you make use of an Analyzer which uses LowerCaseFilter (like
+   * StandardAnalyzer).
+   * 
+   * @param aClass
+   *          a class that is associated with the given stopwordResource
+   * @param stopwordResource
+   *          name of the resource file associated with the given class
+   * @param comment
+   *          the comment string to ignore
+   * @return a {@link Set} with the file's words
+   */
+  public static Set<String> getWordSet(Class<?> aClass,
+      String stopwordResource, String comment) throws IOException {
+    final Reader reader = new BufferedReader(new InputStreamReader(aClass
+        .getResourceAsStream(stopwordResource), "UTF-8"));
+    try {
+      return getWordSet(reader, comment);
+    } finally {
+      reader.close();
+    }
+  }
+  
+  /**
+   * Loads a text file and adds every line as an entry to a HashSet (omitting
+   * leading and trailing whitespace). Every line of the file should contain only
+   * one word. The words need to be in lowercase if you make use of an
+   * Analyzer which uses LowerCaseFilter (like StandardAnalyzer).
+   *
+   * @param wordfile File containing the wordlist
+   * @return A HashSet with the file's words
+   */
+  public static HashSet<String> getWordSet(File wordfile) throws IOException {
+    FileReader reader = null;
+    try {
+      reader = new FileReader(wordfile);
+      return getWordSet(reader);
+    }
+    finally {
+      if (reader != null)
+        reader.close();
+    }
+  }
+
+  /**
+   * Loads a text file and adds every non-comment line as an entry to a HashSet (omitting
+   * leading and trailing whitespace). Every line of the file should contain only
+   * one word. The words need to be in lowercase if you make use of an
+   * Analyzer which uses LowerCaseFilter (like StandardAnalyzer).
+   *
+   * @param wordfile File containing the wordlist
+   * @param comment The comment string to ignore
+   * @return A HashSet with the file's words
+   */
+  public static HashSet<String> getWordSet(File wordfile, String comment) throws IOException {
+    FileReader reader = null;
+    try {
+      reader = new FileReader(wordfile);
+      return getWordSet(reader, comment);
+    }
+    finally {
+      if (reader != null)
+        reader.close();
+    }
+  }
+
+
+  /**
+   * Reads lines from a Reader and adds every line as an entry to a HashSet (omitting
+   * leading and trailing whitespace). Every line of the Reader should contain only
+   * one word. The words need to be in lowercase if you make use of an
+   * Analyzer which uses LowerCaseFilter (like StandardAnalyzer).
+   *
+   * @param reader Reader containing the wordlist
+   * @return A HashSet with the reader's words
+   */
+  public static HashSet<String> getWordSet(Reader reader) throws IOException {
+    final HashSet<String> result = new HashSet<String>();
+    BufferedReader br = null;
+    try {
+      if (reader instanceof BufferedReader) {
+        br = (BufferedReader) reader;
+      } else {
+        br = new BufferedReader(reader);
+      }
+      String word = null;
+      while ((word = br.readLine()) != null) {
+        result.add(word.trim());
+      }
+    }
+    finally {
+      if (br != null)
+        br.close();
+    }
+    return result;
+  }
+
+  /**
+   * Reads lines from a Reader and adds every non-comment line as an entry to a HashSet (omitting
+   * leading and trailing whitespace). Every line of the Reader should contain only
+   * one word. The words need to be in lowercase if you make use of an
+   * Analyzer which uses LowerCaseFilter (like StandardAnalyzer).
+   *
+   * @param reader Reader containing the wordlist
+   * @param comment The string representing a comment.
+   * @return A HashSet with the reader's words
+   */
+  public static HashSet<String> getWordSet(Reader reader, String comment) throws IOException {
+    final HashSet<String> result = new HashSet<String>();
+    BufferedReader br = null;
+    try {
+      if (reader instanceof BufferedReader) {
+        br = (BufferedReader) reader;
+      } else {
+        br = new BufferedReader(reader);
+      }
+      String word = null;
+      while ((word = br.readLine()) != null) {
+        if (word.startsWith(comment) == false){
+          result.add(word.trim());
+        }
+      }
+    }
+    finally {
+      if (br != null)
+        br.close();
+    }
+    return result;
+  }
+
+  /**
+   * Loads a text file in Snowball format associated with a given class (See
+   * {@link Class#getResourceAsStream(String)}) and adds all words as entries to
+   * a {@link Set}. The words need to be in lower-case if you make use of an
+   * Analyzer which uses LowerCaseFilter (like StandardAnalyzer).
+   * 
+   * @param aClass a class that is associated with the given stopwordResource
+   * @param stopwordResource name of the resource file associated with the given
+   *          class
+   * @return a {@link Set} with the file's words
+   * @see #getSnowballWordSet(Reader)
+   */
+  public static Set<String> getSnowballWordSet(Class<?> aClass,
+      String stopwordResource) throws IOException {
+    final Reader reader = new BufferedReader(new InputStreamReader(aClass
+        .getResourceAsStream(stopwordResource), "UTF-8"));
+    try {
+      return getSnowballWordSet(reader);
+    } finally {
+      reader.close();
+    }
+  }
+  
+  /**
+   * Reads stopwords from a stopword list in Snowball format.
+   * <p>
+   * The snowball format is the following:
+   * <ul>
+   * <li>Lines may contain multiple words separated by whitespace.
+   * <li>The comment character is the vertical line (&#124;).
+   * <li>Lines may contain trailing comments.
+   * </ul>
+   * </p>
+   * 
+   * @param reader Reader containing a Snowball stopword list
+   * @return A Set with the reader's words
+   */
+  public static Set<String> getSnowballWordSet(Reader reader)
+      throws IOException {
+    final Set<String> result = new HashSet<String>();
+    BufferedReader br = null;
+    try {
+      if (reader instanceof BufferedReader) {
+        br = (BufferedReader) reader;
+      } else {
+        br = new BufferedReader(reader);
+      }
+      String line = null;
+      while ((line = br.readLine()) != null) {
+        int comment = line.indexOf('|');
+        if (comment >= 0) line = line.substring(0, comment);
+        String words[] = line.split("\\s+");
+        for (int i = 0; i < words.length; i++)
+          if (words[i].length() > 0) result.add(words[i]);
+      }
+    } finally {
+      if (br != null) br.close();
+    }
+    return result;
+  }
+
+
+  /**
+   * Reads a stem dictionary. Each line contains:
+   * <pre>word<b>\t</b>stem</pre>
+   * (i.e. two tab separated words)
+   *
+   * @return stem dictionary that overrules the stemming algorithm
+   * @throws IOException 
+   */
+  public static HashMap<String, String> getStemDict(File wordstemfile) throws IOException {
+    if (wordstemfile == null)
+      throw new NullPointerException("wordstemfile may not be null");
+    final HashMap<String, String> result = new HashMap<String,String>();
+    BufferedReader br = null;
+    
+    try {
+      br = new BufferedReader(new FileReader(wordstemfile));
+      String line;
+      while ((line = br.readLine()) != null) {
+        String[] wordstem = line.split("\t", 2);
+        result.put(wordstem[0], wordstem[1]);
+      }
+    } finally {
+      if(br != null)
+        br.close();
+    }
+    return result;
+  }
+
+}
diff --git a/modules/analysis/common/src/java/org/apache/lucene/collation/CollationKeyAnalyzer.java b/modules/analysis/common/src/java/org/apache/lucene/collation/CollationKeyAnalyzer.java
new file mode 100644
index 0000000..7c59422
--- /dev/null
+++ b/modules/analysis/common/src/java/org/apache/lucene/collation/CollationKeyAnalyzer.java
@@ -0,0 +1,113 @@
+package org.apache.lucene.collation;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
+
+import java.text.Collator;
+import java.io.Reader;
+import java.io.IOException;
+
+/**
+ * <p>
+ *   Filters {@link KeywordTokenizer} with {@link CollationKeyFilter}.
+ * </p>
+ * <p>
+ *   Converts the token into its {@link java.text.CollationKey}, and then
+ *   encodes the CollationKey with 
+ *   {@link org.apache.lucene.util.IndexableBinaryStringTools}, to allow 
+ *   it to be stored as an index term.
+ * </p>
+ * <p>
+ *   <strong>WARNING:</strong> Make sure you use exactly the same Collator at
+ *   index and query time -- CollationKeys are only comparable when produced by
+ *   the same Collator.  Since {@link java.text.RuleBasedCollator}s are not
+ *   independently versioned, it is unsafe to search against stored
+ *   CollationKeys unless the following are exactly the same (best practice is
+ *   to store this information with the index and check that they remain the
+ *   same at query time):
+ * </p>
+ * <ol>
+ *   <li>JVM vendor</li>
+ *   <li>JVM version, including patch version</li>
+ *   <li>
+ *     The language (and country and variant, if specified) of the Locale
+ *     used when constructing the collator via
+ *     {@link Collator#getInstance(java.util.Locale)}.
+ *   </li>
+ *   <li>
+ *     The collation strength used - see {@link Collator#setStrength(int)}
+ *   </li>
+ * </ol> 
+ * <p>
+ *   The <code>ICUCollationKeyAnalyzer</code> in the icu package of Lucene's
+ *   contrib area uses ICU4J's Collator, which makes its
+ *   its version available, thus allowing collation to be versioned
+ *   independently from the JVM.  ICUCollationKeyAnalyzer is also significantly
+ *   faster and generates significantly shorter keys than CollationKeyAnalyzer.
+ *   See <a href="http://site.icu-project.org/charts/collation-icu4j-sun"
+ *   >http://site.icu-project.org/charts/collation-icu4j-sun</a> for key
+ *   generation timing and key length comparisons between ICU4J and
+ *   java.text.Collator over several languages.
+ * </p>
+ * <p>
+ *   CollationKeys generated by java.text.Collators are not compatible
+ *   with those those generated by ICU Collators.  Specifically, if you use 
+ *   CollationKeyAnalyzer to generate index terms, do not use
+ *   ICUCollationKeyAnalyzer on the query side, or vice versa.
+ * </p>
+ */
+public final class CollationKeyAnalyzer extends Analyzer {
+  private Collator collator;
+
+  public CollationKeyAnalyzer(Collator collator) {
+    this.collator = collator;
+  }
+
+  @Override
+  public TokenStream tokenStream(String fieldName, Reader reader) {
+    TokenStream result = new KeywordTokenizer(reader);
+    result = new CollationKeyFilter(result, collator);
+    return result;
+  }
+  
+  private class SavedStreams {
+    Tokenizer source;
+    TokenStream result;
+  }
+  
+  @Override
+  public TokenStream reusableTokenStream(String fieldName, Reader reader) 
+    throws IOException {
+    
+    SavedStreams streams = (SavedStreams)getPreviousTokenStream();
+    if (streams == null) {
+      streams = new SavedStreams();
+      streams.source = new KeywordTokenizer(reader);
+      streams.result = new CollationKeyFilter(streams.source, collator);
+      setPreviousTokenStream(streams);
+    } else {
+      streams.source.reset(reader);
+    }
+    return streams.result;
+  }
+}
diff --git a/modules/analysis/common/src/java/org/apache/lucene/collation/CollationKeyFilter.java b/modules/analysis/common/src/java/org/apache/lucene/collation/CollationKeyFilter.java
new file mode 100644
index 0000000..d5bcc81
--- /dev/null
+++ b/modules/analysis/common/src/java/org/apache/lucene/collation/CollationKeyFilter.java
@@ -0,0 +1,103 @@
+package org.apache.lucene.collation;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+import org.apache.lucene.analysis.TokenFilter;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.util.IndexableBinaryStringTools;
+
+import java.io.IOException;
+import java.text.Collator;
+
+
+/**
+ * <p>
+ *   Converts each token into its {@link java.text.CollationKey}, and then
+ *   encodes the CollationKey with {@link IndexableBinaryStringTools}, to allow 
+ *   it to be stored as an index term.
+ * </p>
+ * <p>
+ *   <strong>WARNING:</strong> Make sure you use exactly the same Collator at
+ *   index and query time -- CollationKeys are only comparable when produced by
+ *   the same Collator.  Since {@link java.text.RuleBasedCollator}s are not
+ *   independently versioned, it is unsafe to search against stored
+ *   CollationKeys unless the following are exactly the same (best practice is
+ *   to store this information with the index and check that they remain the
+ *   same at query time):
+ * </p>
+ * <ol>
+ *   <li>JVM vendor</li>
+ *   <li>JVM version, including patch version</li>
+ *   <li>
+ *     The language (and country and variant, if specified) of the Locale
+ *     used when constructing the collator via
+ *     {@link Collator#getInstance(java.util.Locale)}.
+ *   </li>
+ *   <li>
+ *     The collation strength used - see {@link Collator#setStrength(int)}
+ *   </li>
+ * </ol> 
+ * <p>
+ *   The <code>ICUCollationKeyFilter</code> in the icu package of Lucene's
+ *   contrib area uses ICU4J's Collator, which makes its
+ *   version available, thus allowing collation to be versioned independently
+ *   from the JVM.  ICUCollationKeyFilter is also significantly faster and
+ *   generates significantly shorter keys than CollationKeyFilter.  See
+ *   <a href="http://site.icu-project.org/charts/collation-icu4j-sun"
+ *   >http://site.icu-project.org/charts/collation-icu4j-sun</a> for key
+ *   generation timing and key length comparisons between ICU4J and
+ *   java.text.Collator over several languages.
+ * </p>
+ * <p>
+ *   CollationKeys generated by java.text.Collators are not compatible
+ *   with those those generated by ICU Collators.  Specifically, if you use 
+ *   CollationKeyFilter to generate index terms, do not use
+ *   ICUCollationKeyFilter on the query side, or vice versa.
+ * </p>
+ */
+public final class CollationKeyFilter extends TokenFilter {
+  private final Collator collator;
+  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
+
+  /**
+   * @param input Source token stream
+   * @param collator CollationKey generator
+   */
+  public CollationKeyFilter(TokenStream input, Collator collator) {
+    super(input);
+    this.collator = collator;
+  }
+
+  @Override
+  public boolean incrementToken() throws IOException {
+    if (input.incrementToken()) {
+      byte[] collationKey = collator.getCollationKey(termAtt.toString()).toByteArray();
+      int encodedLength = IndexableBinaryStringTools.getEncodedLength(
+          collationKey, 0, collationKey.length);
+      termAtt.resizeBuffer(encodedLength);
+      termAtt.setLength(encodedLength);
+      IndexableBinaryStringTools.encode(collationKey, 0, collationKey.length,
+          termAtt.buffer(), 0, encodedLength);
+      return true;
+    } else {
+      return false;
+    }
+  }
+}
diff --git a/modules/analysis/common/src/java/org/apache/lucene/collation/package.html b/modules/analysis/common/src/java/org/apache/lucene/collation/package.html
new file mode 100644
index 0000000..b0c6f80
--- /dev/null
+++ b/modules/analysis/common/src/java/org/apache/lucene/collation/package.html
@@ -0,0 +1,176 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+  <title>Lucene Collation Package</title>
+</head>
+<body>
+<p>
+  <code>CollationKeyFilter</code>
+  converts each token into its binary <code>CollationKey</code> using the 
+  provided <code>Collator</code>, and then encode the <code>CollationKey</code>
+  as a String using
+  {@link org.apache.lucene.util.IndexableBinaryStringTools}, to allow it to be 
+  stored as an index term.
+</p>
+
+<h2>Use Cases</h2>
+
+<ul>
+  <li>
+    Efficient sorting of terms in languages that use non-Unicode character 
+    orderings.  (Lucene Sort using a Locale can be very slow.) 
+  </li>
+  <li>
+    Efficient range queries over fields that contain terms in languages that 
+    use non-Unicode character orderings.  (Range queries using a Locale can be
+    very slow.)
+  </li>
+  <li>
+    Effective Locale-specific normalization (case differences, diacritics, etc.).
+    ({@link org.apache.lucene.analysis.LowerCaseFilter} and 
+    {@link org.apache.lucene.analysis.ASCIIFoldingFilter} provide these services
+    in a generic way that doesn't take into account locale-specific needs.)
+  </li>
+</ul>
+
+<h2>Example Usages</h2>
+
+<h3>Farsi Range Queries</h3>
+<code><pre>
+  // "fa" Locale is not supported by Sun JDK 1.4 or 1.5
+  Collator collator = Collator.getInstance(new Locale("ar"));
+  CollationKeyAnalyzer analyzer = new CollationKeyAnalyzer(collator);
+  RAMDirectory ramDir = new RAMDirectory();
+  IndexWriter writer = new IndexWriter
+    (ramDir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);
+  Document doc = new Document();
+  doc.add(new Field("content", "\u0633\u0627\u0628", 
+                    Field.Store.YES, Field.Index.ANALYZED));
+  writer.addDocument(doc);
+  writer.close();
+  IndexSearcher is = new IndexSearcher(ramDir, true);
+
+  // The AnalyzingQueryParser in Lucene's contrib allows terms in range queries
+  // to be passed through an analyzer - Lucene's standard QueryParser does not
+  // allow this.
+  AnalyzingQueryParser aqp = new AnalyzingQueryParser("content", analyzer);
+  aqp.setLowercaseExpandedTerms(false);
+  
+  // Unicode order would include U+0633 in [ U+062F - U+0698 ], but Farsi
+  // orders the U+0698 character before the U+0633 character, so the single
+  // indexed Term above should NOT be returned by a ConstantScoreRangeQuery
+  // with a Farsi Collator (or an Arabic one for the case when Farsi is not
+  // supported).
+  ScoreDoc[] result
+    = is.search(aqp.parse("[ \u062F TO \u0698 ]"), null, 1000).scoreDocs;
+  assertEquals("The index Term should not be included.", 0, result.length);
+</pre></code>
+
+<h3>Danish Sorting</h3>
+<code><pre>
+  Analyzer analyzer 
+    = new CollationKeyAnalyzer(Collator.getInstance(new Locale("da", "dk")));
+  RAMDirectory indexStore = new RAMDirectory();
+  IndexWriter writer = new IndexWriter 
+    (indexStore, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);
+  String[] tracer = new String[] { "A", "B", "C", "D", "E" };
+  String[] data = new String[] { "HAT", "HUT", "H\u00C5T", "H\u00D8T", "HOT" };
+  String[] sortedTracerOrder = new String[] { "A", "E", "B", "D", "C" };
+  for (int i = 0 ; i < data.length ; ++i) {
+    Document doc = new Document();
+    doc.add(new Field("tracer", tracer[i], Field.Store.YES, Field.Index.NO));
+    doc.add(new Field("contents", data[i], Field.Store.NO, Field.Index.ANALYZED));
+    writer.addDocument(doc);
+  }
+  writer.close();
+  Searcher searcher = new IndexSearcher(indexStore, true);
+  Sort sort = new Sort();
+  sort.setSort(new SortField("contents", SortField.STRING));
+  Query query = new MatchAllDocsQuery();
+  ScoreDoc[] result = searcher.search(query, null, 1000, sort).scoreDocs;
+  for (int i = 0 ; i < result.length ; ++i) {
+    Document doc = searcher.doc(result[i].doc);
+    assertEquals(sortedTracerOrder[i], doc.getValues("tracer")[0]);
+  }
+</pre></code>
+
+<h3>Turkish Case Normalization</h3>
+<code><pre>
+  Collator collator = Collator.getInstance(new Locale("tr", "TR"));
+  collator.setStrength(Collator.PRIMARY);
+  Analyzer analyzer = new CollationKeyAnalyzer(collator);
+  RAMDirectory ramDir = new RAMDirectory();
+  IndexWriter writer = new IndexWriter
+    (ramDir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);
+  Document doc = new Document();
+  doc.add(new Field("contents", "DIGY", Field.Store.NO, Field.Index.ANALYZED));
+  writer.addDocument(doc);
+  writer.close();
+  IndexSearcher is = new IndexSearcher(ramDir, true);
+  QueryParser parser = new QueryParser("contents", analyzer);
+  Query query = parser.parse("d\u0131gy");   // U+0131: dotless i
+  ScoreDoc[] result = is.search(query, null, 1000).scoreDocs;
+  assertEquals("The index Term should be included.", 1, result.length);
+</pre></code>
+
+<h2>Caveats and Comparisons</h2>
+<p>
+  <strong>WARNING:</strong> Make sure you use exactly the same 
+  <code>Collator</code> at index and query time -- <code>CollationKey</code>s
+  are only comparable when produced by
+  the same <code>Collator</code>.  Since {@link java.text.RuleBasedCollator}s
+  are not independently versioned, it is unsafe to search against stored
+  <code>CollationKey</code>s unless the following are exactly the same (best 
+  practice is to store this information with the index and check that they
+  remain the same at query time):
+</p>
+<ol>
+  <li>JVM vendor</li>
+  <li>JVM version, including patch version</li>
+  <li>
+    The language (and country and variant, if specified) of the Locale
+    used when constructing the collator via
+    {@link java.text.Collator#getInstance(java.util.Locale)}.
+  </li>
+  <li>
+    The collation strength used - see {@link java.text.Collator#setStrength(int)}
+  </li>
+</ol> 
+<p>
+  <code>ICUCollationKeyFilter</code>, available in the icu package in Lucene's contrib area,
+  uses ICU4J's <code>Collator</code>, which 
+  makes its version available, thus allowing collation to be versioned
+  independently from the JVM.  <code>ICUCollationKeyFilter</code> is also 
+  significantly faster and generates significantly shorter keys than 
+  <code>CollationKeyFilter</code>.  See
+  <a href="http://site.icu-project.org/charts/collation-icu4j-sun"
+    >http://site.icu-project.org/charts/collation-icu4j-sun</a> for key
+  generation timing and key length comparisons between ICU4J and
+  <code>java.text.Collator</code> over several languages.
+</p>
+<p>
+  <code>CollationKey</code>s generated by <code>java.text.Collator</code>s are 
+  not compatible with those those generated by ICU Collators.  Specifically, if
+  you use <code>CollationKeyFilter</code> to generate index terms, do not use
+  <code>ICUCollationKeyFilter</code> on the query side, or vice versa.
+</p>
+<pre>
+</pre>
+</body>
+</html>
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/bg/TestBulgarianStemmer.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/bg/TestBulgarianStemmer.java
index e01bcbd..dda2f2d 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/bg/TestBulgarianStemmer.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/bg/TestBulgarianStemmer.java
@@ -22,8 +22,8 @@ import java.io.StringReader;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.util.Version;
 
 /**
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/br/TestBrazilianStemmer.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/br/TestBrazilianStemmer.java
index 6b4d31c..89cdc10 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/br/TestBrazilianStemmer.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/br/TestBrazilianStemmer.java
@@ -23,8 +23,8 @@ import java.io.StringReader;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseTokenizer;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.LowerCaseTokenizer;
 
 /**
  * Test the Brazilian Stem Filter, which only modifies the term text.
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/charfilter/TestMappingCharFilter.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/charfilter/TestMappingCharFilter.java
index 08d567a..448a5c7 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/charfilter/TestMappingCharFilter.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/charfilter/TestMappingCharFilter.java
@@ -23,7 +23,7 @@ import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.CharReader;
 import org.apache.lucene.analysis.CharStream;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 public class TestMappingCharFilter extends BaseTokenStreamTestCase {
 
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/cn/TestChineseTokenizer.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/cn/TestChineseTokenizer.java
index 7f125ad..0f24353 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/cn/TestChineseTokenizer.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/cn/TestChineseTokenizer.java
@@ -24,7 +24,7 @@ import java.io.StringReader;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.util.Version;
 
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/commongrams/CommonGramsFilterTest.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/commongrams/CommonGramsFilterTest.java
index dc7d0b3..769db62 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/commongrams/CommonGramsFilterTest.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/commongrams/CommonGramsFilterTest.java
@@ -24,7 +24,7 @@ import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 
 /**
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java
index 9fc9f97..28bfbf6 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java
@@ -25,8 +25,8 @@ import java.io.StringReader;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.compound.hyphenation.HyphenationTree;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
 public class TestCompoundWordTokenFilter extends BaseTokenStreamTestCase {
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestAnalyzers.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestAnalyzers.java
new file mode 100644
index 0000000..febda6f
--- /dev/null
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestAnalyzers.java
@@ -0,0 +1,279 @@
+package org.apache.lucene.analysis.core;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.io.StringReader;
+import java.io.Reader;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.core.LowerCaseTokenizer;
+import org.apache.lucene.analysis.TokenFilter;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.SimpleAnalyzer;
+import org.apache.lucene.analysis.core.StopAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
+import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.index.Payload;
+import org.apache.lucene.util.Version;
+
+public class TestAnalyzers extends BaseTokenStreamTestCase {
+
+   public TestAnalyzers(String name) {
+      super(name);
+   }
+
+  public void testSimple() throws Exception {
+    Analyzer a = new SimpleAnalyzer(TEST_VERSION_CURRENT);
+    assertAnalyzesTo(a, "foo bar FOO BAR", 
+                     new String[] { "foo", "bar", "foo", "bar" });
+    assertAnalyzesTo(a, "foo      bar .  FOO <> BAR", 
+                     new String[] { "foo", "bar", "foo", "bar" });
+    assertAnalyzesTo(a, "foo.bar.FOO.BAR", 
+                     new String[] { "foo", "bar", "foo", "bar" });
+    assertAnalyzesTo(a, "U.S.A.", 
+                     new String[] { "u", "s", "a" });
+    assertAnalyzesTo(a, "C++", 
+                     new String[] { "c" });
+    assertAnalyzesTo(a, "B2B", 
+                     new String[] { "b", "b" });
+    assertAnalyzesTo(a, "2B", 
+                     new String[] { "b" });
+    assertAnalyzesTo(a, "\"QUOTED\" word", 
+                     new String[] { "quoted", "word" });
+  }
+
+  public void testNull() throws Exception {
+    Analyzer a = new WhitespaceAnalyzer(TEST_VERSION_CURRENT);
+    assertAnalyzesTo(a, "foo bar FOO BAR", 
+                     new String[] { "foo", "bar", "FOO", "BAR" });
+    assertAnalyzesTo(a, "foo      bar .  FOO <> BAR", 
+                     new String[] { "foo", "bar", ".", "FOO", "<>", "BAR" });
+    assertAnalyzesTo(a, "foo.bar.FOO.BAR", 
+                     new String[] { "foo.bar.FOO.BAR" });
+    assertAnalyzesTo(a, "U.S.A.", 
+                     new String[] { "U.S.A." });
+    assertAnalyzesTo(a, "C++", 
+                     new String[] { "C++" });
+    assertAnalyzesTo(a, "B2B", 
+                     new String[] { "B2B" });
+    assertAnalyzesTo(a, "2B", 
+                     new String[] { "2B" });
+    assertAnalyzesTo(a, "\"QUOTED\" word", 
+                     new String[] { "\"QUOTED\"", "word" });
+  }
+
+  public void testStop() throws Exception {
+    Analyzer a = new StopAnalyzer(TEST_VERSION_CURRENT);
+    assertAnalyzesTo(a, "foo bar FOO BAR", 
+                     new String[] { "foo", "bar", "foo", "bar" });
+    assertAnalyzesTo(a, "foo a bar such FOO THESE BAR", 
+                     new String[] { "foo", "bar", "foo", "bar" });
+  }
+
+  void verifyPayload(TokenStream ts) throws IOException {
+    PayloadAttribute payloadAtt = ts.getAttribute(PayloadAttribute.class);
+    for(byte b=1;;b++) {
+      boolean hasNext = ts.incrementToken();
+      if (!hasNext) break;
+      // System.out.println("id="+System.identityHashCode(nextToken) + " " + t);
+      // System.out.println("payload=" + (int)nextToken.getPayload().toByteArray()[0]);
+      assertEquals(b, payloadAtt.getPayload().toByteArray()[0]);
+    }
+  }
+
+  // Make sure old style next() calls result in a new copy of payloads
+  public void testPayloadCopy() throws IOException {
+    String s = "how now brown cow";
+    TokenStream ts;
+    ts = new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(s));
+    ts = new PayloadSetter(ts);
+    verifyPayload(ts);
+
+    ts = new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(s));
+    ts = new PayloadSetter(ts);
+    verifyPayload(ts);
+  }
+
+  // LUCENE-1150: Just a compile time test, to ensure the
+  // StandardAnalyzer constants remain publicly accessible
+  @SuppressWarnings("unused")
+  public void _testStandardConstants() {
+    int x = StandardTokenizer.ALPHANUM;
+    x = StandardTokenizer.APOSTROPHE;
+    x = StandardTokenizer.ACRONYM;
+    x = StandardTokenizer.COMPANY;
+    x = StandardTokenizer.EMAIL;
+    x = StandardTokenizer.HOST;
+    x = StandardTokenizer.NUM;
+    x = StandardTokenizer.CJ;
+    String[] y = StandardTokenizer.TOKEN_TYPES;
+  }
+
+  private static class LowerCaseWhitespaceAnalyzer extends Analyzer {
+
+    @Override
+    public TokenStream tokenStream(String fieldName, Reader reader) {
+      return new LowerCaseFilter(TEST_VERSION_CURRENT,
+          new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader));
+    }
+    
+  }
+  
+  /**
+   * @deprecated remove this when lucene 3.0 "broken unicode 4" support
+   * is no longer needed.
+   */
+  @Deprecated
+  private static class LowerCaseWhitespaceAnalyzerBWComp extends Analyzer {
+
+    @Override
+    public TokenStream tokenStream(String fieldName, Reader reader) {
+      return new LowerCaseFilter(new WhitespaceTokenizer(reader));
+    }
+    
+  }
+  
+  /**
+   * Test that LowercaseFilter handles entire unicode range correctly
+   */
+  public void testLowerCaseFilter() throws IOException {
+    Analyzer a = new LowerCaseWhitespaceAnalyzer();
+    // BMP
+    assertAnalyzesTo(a, "AbaCaDabA", new String[] { "abacadaba" });
+    // supplementary
+    assertAnalyzesTo(a, "\ud801\udc16\ud801\udc16\ud801\udc16\ud801\udc16",
+        new String[] {"\ud801\udc3e\ud801\udc3e\ud801\udc3e\ud801\udc3e"});
+    assertAnalyzesTo(a, "AbaCa\ud801\udc16DabA", 
+        new String[] { "abaca\ud801\udc3edaba" });
+    // unpaired lead surrogate
+    assertAnalyzesTo(a, "AbaC\uD801AdaBa", 
+        new String [] { "abac\uD801adaba" });
+    // unpaired trail surrogate
+    assertAnalyzesTo(a, "AbaC\uDC16AdaBa", 
+        new String [] { "abac\uDC16adaba" });
+  }
+  
+  /**
+   * Test that LowercaseFilter handles the lowercasing correctly if the term
+   * buffer has a trailing surrogate character leftover and the current term in
+   * the buffer ends with a corresponding leading surrogate.
+   */
+  public void testLowerCaseFilterLowSurrogateLeftover() throws IOException {
+    // test if the limit of the termbuffer is correctly used with supplementary
+    // chars
+    WhitespaceTokenizer tokenizer = new WhitespaceTokenizer(TEST_VERSION_CURRENT, 
+        new StringReader("BogustermBogusterm\udc16"));
+    LowerCaseFilter filter = new LowerCaseFilter(TEST_VERSION_CURRENT,
+        tokenizer);
+    assertTokenStreamContents(filter, new String[] {"bogustermbogusterm\udc16"});
+    filter.reset();
+    String highSurEndingUpper = "BogustermBoguster\ud801";
+    String highSurEndingLower = "bogustermboguster\ud801";
+    tokenizer.reset(new StringReader(highSurEndingUpper));
+    assertTokenStreamContents(filter, new String[] {highSurEndingLower});
+    assertTrue(filter.hasAttribute(CharTermAttribute.class));
+    char[] termBuffer = filter.getAttribute(CharTermAttribute.class).buffer();
+    int length = highSurEndingLower.length();
+    assertEquals('\ud801', termBuffer[length - 1]);
+    assertEquals('\udc3e', termBuffer[length]);
+    
+  }
+  
+  /**
+   * Test that LowercaseFilter only works on BMP for back compat,
+   * depending upon version
+   * @deprecated remove this test when lucene 3.0 "broken unicode 4" support
+   * is no longer needed.
+   */
+  @Deprecated
+  public void testLowerCaseFilterBWComp() throws IOException {
+    Analyzer a = new LowerCaseWhitespaceAnalyzerBWComp();
+    // BMP
+    assertAnalyzesTo(a, "AbaCaDabA", new String[] { "abacadaba" });
+    // supplementary, no-op
+    assertAnalyzesTo(a, "\ud801\udc16\ud801\udc16\ud801\udc16\ud801\udc16",
+        new String[] {"\ud801\udc16\ud801\udc16\ud801\udc16\ud801\udc16"});
+    assertAnalyzesTo(a, "AbaCa\ud801\udc16DabA",
+        new String[] { "abaca\ud801\udc16daba" });
+    // unpaired lead surrogate
+    assertAnalyzesTo(a, "AbaC\uD801AdaBa", 
+        new String [] { "abac\uD801adaba" });
+    // unpaired trail surrogate
+    assertAnalyzesTo(a, "AbaC\uDC16AdaBa", 
+        new String [] { "abac\uDC16adaba" });
+  }
+  
+  public void testLowerCaseTokenizer() throws IOException {
+    StringReader reader = new StringReader("Tokenizer \ud801\udc1ctest");
+    LowerCaseTokenizer tokenizer = new LowerCaseTokenizer(TEST_VERSION_CURRENT,
+        reader);
+    assertTokenStreamContents(tokenizer, new String[] { "tokenizer",
+        "\ud801\udc44test" });
+  }
+
+  @Deprecated
+  public void testLowerCaseTokenizerBWCompat() throws IOException {
+    StringReader reader = new StringReader("Tokenizer \ud801\udc1ctest");
+    LowerCaseTokenizer tokenizer = new LowerCaseTokenizer(Version.LUCENE_30,
+        reader);
+    assertTokenStreamContents(tokenizer, new String[] { "tokenizer", "test" });
+  }
+  
+  public void testWhitespaceTokenizer() throws IOException {
+    StringReader reader = new StringReader("Tokenizer \ud801\udc1ctest");
+    WhitespaceTokenizer tokenizer = new WhitespaceTokenizer(TEST_VERSION_CURRENT,
+        reader);
+    assertTokenStreamContents(tokenizer, new String[] { "Tokenizer",
+        "\ud801\udc1ctest" });
+  }
+
+  @Deprecated
+  public void testWhitespaceTokenizerBWCompat() throws IOException {
+    StringReader reader = new StringReader("Tokenizer \ud801\udc1ctest");
+    WhitespaceTokenizer tokenizer = new WhitespaceTokenizer(Version.LUCENE_30,
+        reader);
+    assertTokenStreamContents(tokenizer, new String[] { "Tokenizer",
+        "\ud801\udc1ctest" });
+  }
+}
+
+final class PayloadSetter extends TokenFilter {
+  PayloadAttribute payloadAtt;
+  public  PayloadSetter(TokenStream input) {
+    super(input);
+    payloadAtt = addAttribute(PayloadAttribute.class);
+  }
+
+  byte[] data = new byte[1];
+  Payload p = new Payload(data,0,1);
+
+  @Override
+  public boolean incrementToken() throws IOException {
+    boolean hasNext = input.incrementToken();
+    if (!hasNext) return false;
+    payloadAtt.setPayload(p);  // reuse the payload / byte[]
+    data[0]++;
+    return true;
+  }
+}
\ No newline at end of file
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestKeywordAnalyzer.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestKeywordAnalyzer.java
new file mode 100644
index 0000000..3baa631
--- /dev/null
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestKeywordAnalyzer.java
@@ -0,0 +1,99 @@
+package org.apache.lucene.analysis.core;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.StringReader;
+
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.core.KeywordAnalyzer;
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.TermDocs;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.store.RAMDirectory;
+
+public class TestKeywordAnalyzer extends BaseTokenStreamTestCase {
+  
+  private RAMDirectory directory;
+  private IndexSearcher searcher;
+
+  @Override
+  protected void setUp() throws Exception {
+    super.setUp();
+    directory = new RAMDirectory();
+    IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(
+        TEST_VERSION_CURRENT, new SimpleAnalyzer(TEST_VERSION_CURRENT)));
+
+    Document doc = new Document();
+    doc.add(new Field("partnum", "Q36", Field.Store.YES, Field.Index.NOT_ANALYZED));
+    doc.add(new Field("description", "Illidium Space Modulator", Field.Store.YES, Field.Index.ANALYZED));
+    writer.addDocument(doc);
+
+    writer.close();
+
+    searcher = new IndexSearcher(directory, true);
+  }
+
+  /*
+  public void testPerFieldAnalyzer() throws Exception {
+    PerFieldAnalyzerWrapper analyzer = new PerFieldAnalyzerWrapper(new SimpleAnalyzer(TEST_VERSION_CURRENT));
+    analyzer.addAnalyzer("partnum", new KeywordAnalyzer());
+
+    QueryParser queryParser = new QueryParser(TEST_VERSION_CURRENT, "description", analyzer);
+    Query query = queryParser.parse("partnum:Q36 AND SPACE");
+
+    ScoreDoc[] hits = searcher.search(query, null, 1000).scoreDocs;
+    assertEquals("Q36 kept as-is",
+              "+partnum:Q36 +space", query.toString("description"));
+    assertEquals("doc found!", 1, hits.length);
+  }
+  */
+
+  public void testMutipleDocument() throws Exception {
+    RAMDirectory dir = new RAMDirectory();
+    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new KeywordAnalyzer()));
+    Document doc = new Document();
+    doc.add(new Field("partnum", "Q36", Field.Store.YES, Field.Index.ANALYZED));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new Field("partnum", "Q37", Field.Store.YES, Field.Index.ANALYZED));
+    writer.addDocument(doc);
+    writer.close();
+
+    IndexReader reader = IndexReader.open(dir, true);
+    TermDocs td = reader.termDocs(new Term("partnum", "Q36"));
+    assertTrue(td.next());
+    td = reader.termDocs(new Term("partnum", "Q37"));
+    assertTrue(td.next());
+  }
+
+  // LUCENE-1441
+  public void testOffsets() throws Exception {
+    TokenStream stream = new KeywordAnalyzer().tokenStream("field", new StringReader("abcd"));
+    OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);
+    assertTrue(stream.incrementToken());
+    assertEquals(0, offsetAtt.startOffset());
+    assertEquals(4, offsetAtt.endOffset());
+  }
+}
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestStandardAnalyzer.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestStandardAnalyzer.java
new file mode 100644
index 0000000..803606f
--- /dev/null
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestStandardAnalyzer.java
@@ -0,0 +1,308 @@
+package org.apache.lucene.analysis.core;
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.TermPositions;
+
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.util.Version;
+
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ * <p/>
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class TestStandardAnalyzer extends BaseTokenStreamTestCase {
+
+  private Analyzer a = new StandardAnalyzer(TEST_VERSION_CURRENT);
+
+  public void testMaxTermLength() throws Exception {
+    StandardAnalyzer sa = new StandardAnalyzer(TEST_VERSION_CURRENT);
+    sa.setMaxTokenLength(5);
+    assertAnalyzesTo(sa, "ab cd toolong xy z", new String[]{"ab", "cd", "xy", "z"});
+  }
+
+  public void testMaxTermLength2() throws Exception {
+    StandardAnalyzer sa = new StandardAnalyzer(TEST_VERSION_CURRENT);
+    assertAnalyzesTo(sa, "ab cd toolong xy z", new String[]{"ab", "cd", "toolong", "xy", "z"});
+    sa.setMaxTokenLength(5);
+    
+    assertAnalyzesTo(sa, "ab cd toolong xy z", new String[]{"ab", "cd", "xy", "z"}, new int[]{1, 1, 2, 1});
+  }
+
+  public void testMaxTermLength3() throws Exception {
+    char[] chars = new char[255];
+    for(int i=0;i<255;i++)
+      chars[i] = 'a';
+    String longTerm = new String(chars, 0, 255);
+    
+    assertAnalyzesTo(a, "ab cd " + longTerm + " xy z", new String[]{"ab", "cd", longTerm, "xy", "z"});
+    assertAnalyzesTo(a, "ab cd " + longTerm + "a xy z", new String[]{"ab", "cd", "xy", "z"});
+  }
+
+  public void testAlphanumeric() throws Exception {
+    // alphanumeric tokens
+    assertAnalyzesTo(a, "B2B", new String[]{"b2b"});
+    assertAnalyzesTo(a, "2B", new String[]{"2b"});
+  }
+
+  public void testUnderscores() throws Exception {
+    // underscores are delimiters, but not in email addresses (below)
+    assertAnalyzesTo(a, "word_having_underscore", new String[]{"word", "having", "underscore"});
+    assertAnalyzesTo(a, "word_with_underscore_and_stopwords", new String[]{"word", "underscore", "stopwords"});
+  }
+
+  public void testDelimiters() throws Exception {
+    // other delimiters: "-", "/", ","
+    assertAnalyzesTo(a, "some-dashed-phrase", new String[]{"some", "dashed", "phrase"});
+    assertAnalyzesTo(a, "dogs,chase,cats", new String[]{"dogs", "chase", "cats"});
+    assertAnalyzesTo(a, "ac/dc", new String[]{"ac", "dc"});
+  }
+
+  public void testApostrophes() throws Exception {
+    // internal apostrophes: O'Reilly, you're, O'Reilly's
+    // possessives are actually removed by StardardFilter, not the tokenizer
+    assertAnalyzesTo(a, "O'Reilly", new String[]{"o'reilly"});
+    assertAnalyzesTo(a, "you're", new String[]{"you're"});
+    assertAnalyzesTo(a, "she's", new String[]{"she"});
+    assertAnalyzesTo(a, "Jim's", new String[]{"jim"});
+    assertAnalyzesTo(a, "don't", new String[]{"don't"});
+    assertAnalyzesTo(a, "O'Reilly's", new String[]{"o'reilly"});
+  }
+
+  public void testTSADash() throws Exception {
+    // t and s had been stopwords in Lucene <= 2.0, which made it impossible
+    // to correctly search for these terms:
+    assertAnalyzesTo(a, "s-class", new String[]{"s", "class"});
+    assertAnalyzesTo(a, "t-com", new String[]{"t", "com"});
+    // 'a' is still a stopword:
+    assertAnalyzesTo(a, "a-class", new String[]{"class"});
+  }
+
+  public void testCompanyNames() throws Exception {
+    // company names
+    assertAnalyzesTo(a, "AT&T", new String[]{"at&t"});
+    assertAnalyzesTo(a, "Excite@Home", new String[]{"excite@home"});
+  }
+
+  public void testLucene1140() throws Exception {
+    try {
+      StandardAnalyzer analyzer = new StandardAnalyzer(TEST_VERSION_CURRENT);
+      assertAnalyzesTo(analyzer, "www.nutch.org.", new String[]{ "www.nutch.org" }, new String[] { "<HOST>" });
+    } catch (NullPointerException e) {
+      fail("Should not throw an NPE and it did");
+    }
+
+  }
+
+  public void testDomainNames() throws Exception {
+    // Current lucene should not show the bug
+    StandardAnalyzer a2 = new StandardAnalyzer(TEST_VERSION_CURRENT);
+
+    // domain names
+    assertAnalyzesTo(a2, "www.nutch.org", new String[]{"www.nutch.org"});
+    //Notice the trailing .  See https://issues.apache.org/jira/browse/LUCENE-1068.
+    // the following should be recognized as HOST:
+    assertAnalyzesTo(a2, "www.nutch.org.", new String[]{ "www.nutch.org" }, new String[] { "<HOST>" });
+
+    // 2.3 should show the bug
+    a2 = new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_23);
+    assertAnalyzesTo(a2, "www.nutch.org.", new String[]{ "wwwnutchorg" }, new String[] { "<ACRONYM>" });
+
+    // 2.4 should not show the bug
+    a2 = new StandardAnalyzer(Version.LUCENE_24);
+    assertAnalyzesTo(a2, "www.nutch.org.", new String[]{ "www.nutch.org" }, new String[] { "<HOST>" });
+  }
+
+  public void testEMailAddresses() throws Exception {
+    // email addresses, possibly with underscores, periods, etc
+    assertAnalyzesTo(a, "test@example.com", new String[]{"test@example.com"});
+    assertAnalyzesTo(a, "first.lastname@example.com", new String[]{"first.lastname@example.com"});
+    assertAnalyzesTo(a, "first_lastname@example.com", new String[]{"first_lastname@example.com"});
+  }
+
+  public void testNumeric() throws Exception {
+    // floating point, serial, model numbers, ip addresses, etc.
+    // every other segment must have at least one digit
+    assertAnalyzesTo(a, "21.35", new String[]{"21.35"});
+    assertAnalyzesTo(a, "R2D2 C3PO", new String[]{"r2d2", "c3po"});
+    assertAnalyzesTo(a, "216.239.63.104", new String[]{"216.239.63.104"});
+    assertAnalyzesTo(a, "1-2-3", new String[]{"1-2-3"});
+    assertAnalyzesTo(a, "a1-b2-c3", new String[]{"a1-b2-c3"});
+    assertAnalyzesTo(a, "a1-b-c3", new String[]{"a1-b-c3"});
+  }
+
+  public void testTextWithNumbers() throws Exception {
+    // numbers
+    assertAnalyzesTo(a, "David has 5000 bones", new String[]{"david", "has", "5000", "bones"});
+  }
+
+  public void testVariousText() throws Exception {
+    // various
+    assertAnalyzesTo(a, "C embedded developers wanted", new String[]{"c", "embedded", "developers", "wanted"});
+    assertAnalyzesTo(a, "foo bar FOO BAR", new String[]{"foo", "bar", "foo", "bar"});
+    assertAnalyzesTo(a, "foo      bar .  FOO <> BAR", new String[]{"foo", "bar", "foo", "bar"});
+    assertAnalyzesTo(a, "\"QUOTED\" word", new String[]{"quoted", "word"});
+  }
+
+  public void testAcronyms() throws Exception {
+    // acronyms have their dots stripped
+    assertAnalyzesTo(a, "U.S.A.", new String[]{"usa"});
+  }
+
+  public void testCPlusPlusHash() throws Exception {
+    // It would be nice to change the grammar in StandardTokenizer.jj to make "C#" and "C++" end up as tokens.
+    assertAnalyzesTo(a, "C++", new String[]{"c"});
+    assertAnalyzesTo(a, "C#", new String[]{"c"});
+  }
+
+  public void testKorean() throws Exception {
+    // Korean words
+    assertAnalyzesTo(a, "???????? ????????", new String[]{"????????", "????????"});
+  }
+
+  // Compliance with the "old" JavaCC-based analyzer, see:
+  // https://issues.apache.org/jira/browse/LUCENE-966#action_12516752
+
+  public void testComplianceFileName() throws Exception {
+    assertAnalyzesTo(a, "2004.jpg",
+            new String[]{"2004.jpg"},
+            new String[]{"<HOST>"});
+  }
+
+  public void testComplianceNumericIncorrect() throws Exception {
+    assertAnalyzesTo(a, "62.46",
+            new String[]{"62.46"},
+            new String[]{"<HOST>"});
+  }
+
+  public void testComplianceNumericLong() throws Exception {
+    assertAnalyzesTo(a, "978-0-94045043-1",
+            new String[]{"978-0-94045043-1"},
+            new String[]{"<NUM>"});
+  }
+
+  public void testComplianceNumericFile() throws Exception {
+    assertAnalyzesTo(
+            a,
+            "78academyawards/rules/rule02.html",
+            new String[]{"78academyawards/rules/rule02.html"},
+            new String[]{"<NUM>"});
+  }
+
+  public void testComplianceNumericWithUnderscores() throws Exception {
+    assertAnalyzesTo(
+            a,
+            "2006-03-11t082958z_01_ban130523_rtridst_0_ozabs",
+            new String[]{"2006-03-11t082958z_01_ban130523_rtridst_0_ozabs"},
+            new String[]{"<NUM>"});
+  }
+
+  public void testComplianceNumericWithDash() throws Exception {
+    assertAnalyzesTo(a, "mid-20th", new String[]{"mid-20th"},
+            new String[]{"<NUM>"});
+  }
+
+  public void testComplianceManyTokens() throws Exception {
+    assertAnalyzesTo(
+            a,
+            "/money.cnn.com/magazines/fortune/fortune_archive/2007/03/19/8402357/index.htm "
+                    + "safari-0-sheikh-zayed-grand-mosque.jpg",
+            new String[]{"money.cnn.com", "magazines", "fortune",
+                    "fortune", "archive/2007/03/19/8402357", "index.htm",
+                    "safari-0-sheikh", "zayed", "grand", "mosque.jpg"},
+            new String[]{"<HOST>", "<ALPHANUM>", "<ALPHANUM>",
+                    "<ALPHANUM>", "<NUM>", "<HOST>", "<NUM>", "<ALPHANUM>",
+                    "<ALPHANUM>", "<HOST>"});
+  }
+
+  public void testJava14BWCompatibility() throws Exception {
+    StandardAnalyzer sa = new StandardAnalyzer(Version.LUCENE_30);
+    assertAnalyzesTo(sa, "test\u02C6test", new String[] { "test", "test" });
+    sa = new StandardAnalyzer(Version.LUCENE_31);
+    assertAnalyzesTo(sa, "test\u02C6test", new String[] { "test\u02C6test" });
+  }
+
+  /**
+   * Make sure we skip wicked long terms.
+  */
+  public void testWickedLongTerm() throws IOException {
+    RAMDirectory dir = new RAMDirectory();
+    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(
+      TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT)));
+
+    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];
+    Arrays.fill(chars, 'x');
+    Document doc = new Document();
+    final String bigTerm = new String(chars);
+
+    // This produces a too-long term:
+    String contents = "abc xyz x" + bigTerm + " another term";
+    doc.add(new Field("content", contents, Field.Store.NO, Field.Index.ANALYZED));
+    writer.addDocument(doc);
+
+    // Make sure we can add another normal document
+    doc = new Document();
+    doc.add(new Field("content", "abc bbb ccc", Field.Store.NO, Field.Index.ANALYZED));
+    writer.addDocument(doc);
+    writer.close();
+
+    IndexReader reader = IndexReader.open(dir, true);
+
+    // Make sure all terms < max size were indexed
+    assertEquals(2, reader.docFreq(new Term("content", "abc")));
+    assertEquals(1, reader.docFreq(new Term("content", "bbb")));
+    assertEquals(1, reader.docFreq(new Term("content", "term")));
+    assertEquals(1, reader.docFreq(new Term("content", "another")));
+
+    // Make sure position is still incremented when
+    // massive term is skipped:
+    TermPositions tps = reader.termPositions(new Term("content", "another"));
+    assertTrue(tps.next());
+    assertEquals(1, tps.freq());
+    assertEquals(3, tps.nextPosition());
+
+    // Make sure the doc that has the massive term is in
+    // the index:
+    assertEquals("document with wicked long term should is not in the index!", 2, reader.numDocs());
+
+    reader.close();
+
+    // Make sure we can add a document with exactly the
+    // maximum length term, and search on that term:
+    doc = new Document();
+    doc.add(new Field("content", bigTerm, Field.Store.NO, Field.Index.ANALYZED));
+    StandardAnalyzer sa = new StandardAnalyzer(TEST_VERSION_CURRENT);
+    sa.setMaxTokenLength(100000);
+    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));
+    writer.addDocument(doc);
+    writer.close();
+    reader = IndexReader.open(dir, true);
+    assertEquals(1, reader.docFreq(new Term("content", bigTerm)));
+    reader.close();
+
+    dir.close();
+  }
+}
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopAnalyzer.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopAnalyzer.java
new file mode 100644
index 0000000..a453d62
--- /dev/null
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopAnalyzer.java
@@ -0,0 +1,104 @@
+package org.apache.lucene.analysis.core;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.core.StopAnalyzer;
+import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.util.Version;
+
+import java.io.StringReader;
+import java.io.IOException;
+import java.util.Iterator;
+import java.util.Set;
+import java.util.HashSet;
+
+public class TestStopAnalyzer extends BaseTokenStreamTestCase {
+  
+  private StopAnalyzer stop = new StopAnalyzer(TEST_VERSION_CURRENT);
+  private Set<Object> inValidTokens = new HashSet<Object>();
+  
+  public TestStopAnalyzer(String s) {
+    super(s);
+  }
+
+  @Override
+  protected void setUp() throws Exception {
+    super.setUp();
+    
+    Iterator<?> it = StopAnalyzer.ENGLISH_STOP_WORDS_SET.iterator();
+    while(it.hasNext()) {
+      inValidTokens.add(it.next());
+    }
+  }
+
+  public void testDefaults() throws IOException {
+    assertTrue(stop != null);
+    StringReader reader = new StringReader("This is a test of the english stop analyzer");
+    TokenStream stream = stop.tokenStream("test", reader);
+    assertTrue(stream != null);
+    CharTermAttribute termAtt = stream.getAttribute(CharTermAttribute.class);
+    
+    while (stream.incrementToken()) {
+      assertFalse(inValidTokens.contains(termAtt.toString()));
+    }
+  }
+
+  public void testStopList() throws IOException {
+    Set<Object> stopWordsSet = new HashSet<Object>();
+    stopWordsSet.add("good");
+    stopWordsSet.add("test");
+    stopWordsSet.add("analyzer");
+    StopAnalyzer newStop = new StopAnalyzer(Version.LUCENE_24, stopWordsSet);
+    StringReader reader = new StringReader("This is a good test of the english stop analyzer");
+    TokenStream stream = newStop.tokenStream("test", reader);
+    assertNotNull(stream);
+    CharTermAttribute termAtt = stream.getAttribute(CharTermAttribute.class);
+    PositionIncrementAttribute posIncrAtt = stream.addAttribute(PositionIncrementAttribute.class);
+    
+    while (stream.incrementToken()) {
+      String text = termAtt.toString();
+      assertFalse(stopWordsSet.contains(text));
+      assertEquals(1,posIncrAtt.getPositionIncrement()); // in 2.4 stop tokenizer does not apply increments.
+    }
+  }
+
+  public void testStopListPositions() throws IOException {
+    Set<Object> stopWordsSet = new HashSet<Object>();
+    stopWordsSet.add("good");
+    stopWordsSet.add("test");
+    stopWordsSet.add("analyzer");
+    StopAnalyzer newStop = new StopAnalyzer(TEST_VERSION_CURRENT, stopWordsSet);
+    StringReader reader = new StringReader("This is a good test of the english stop analyzer with positions");
+    int expectedIncr[] =                  { 1,   1, 1,          3, 1,  1,      1,            2,   1};
+    TokenStream stream = newStop.tokenStream("test", reader);
+    assertNotNull(stream);
+    int i = 0;
+    CharTermAttribute termAtt = stream.getAttribute(CharTermAttribute.class);
+    PositionIncrementAttribute posIncrAtt = stream.addAttribute(PositionIncrementAttribute.class);
+
+    while (stream.incrementToken()) {
+      String text = termAtt.toString();
+      assertFalse(stopWordsSet.contains(text));
+      assertEquals(expectedIncr[i++],posIncrAtt.getPositionIncrement());
+    }
+  }
+
+}
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopFilter.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopFilter.java
new file mode 100644
index 0000000..c17843f
--- /dev/null
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopFilter.java
@@ -0,0 +1,142 @@
+package org.apache.lucene.analysis.core;
+
+/**
+ * Copyright 2005 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.core.StopFilter;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
+import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.util.English;
+import org.apache.lucene.util.Version;
+
+import java.io.IOException;
+import java.io.StringReader;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Set;
+import java.util.HashSet;
+
+
+public class TestStopFilter extends BaseTokenStreamTestCase {
+  
+  // other StopFilter functionality is already tested by TestStopAnalyzer
+
+  public void testExactCase() throws IOException {
+    StringReader reader = new StringReader("Now is The Time");
+    Set<String> stopWords = new HashSet<String>(Arrays.asList("is", "the", "Time"));
+    TokenStream stream = new StopFilter(TEST_VERSION_CURRENT, new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader), stopWords, false);
+    final CharTermAttribute termAtt = stream.getAttribute(CharTermAttribute.class);
+    assertTrue(stream.incrementToken());
+    assertEquals("Now", termAtt.toString());
+    assertTrue(stream.incrementToken());
+    assertEquals("The", termAtt.toString());
+    assertFalse(stream.incrementToken());
+  }
+
+  public void testIgnoreCase() throws IOException {
+    StringReader reader = new StringReader("Now is The Time");
+    Set<Object> stopWords = new HashSet<Object>(Arrays.asList( "is", "the", "Time" ));
+    TokenStream stream = new StopFilter(TEST_VERSION_CURRENT, new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader), stopWords, true);
+    final CharTermAttribute termAtt = stream.getAttribute(CharTermAttribute.class);
+    assertTrue(stream.incrementToken());
+    assertEquals("Now", termAtt.toString());
+    assertFalse(stream.incrementToken());
+  }
+
+  public void testStopFilt() throws IOException {
+    StringReader reader = new StringReader("Now is The Time");
+    String[] stopWords = new String[] { "is", "the", "Time" };
+    Set<Object> stopSet = StopFilter.makeStopSet(TEST_VERSION_CURRENT, stopWords);
+    TokenStream stream = new StopFilter(TEST_VERSION_CURRENT, new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader), stopSet);
+    final CharTermAttribute termAtt = stream.getAttribute(CharTermAttribute.class);
+    assertTrue(stream.incrementToken());
+    assertEquals("Now", termAtt.toString());
+    assertTrue(stream.incrementToken());
+    assertEquals("The", termAtt.toString());
+    assertFalse(stream.incrementToken());
+  }
+
+  /**
+   * Test Position increments applied by StopFilter with and without enabling this option.
+   */
+  public void testStopPositons() throws IOException {
+    StringBuilder sb = new StringBuilder();
+    ArrayList<String> a = new ArrayList<String>();
+    for (int i=0; i<20; i++) {
+      String w = English.intToEnglish(i).trim();
+      sb.append(w).append(" ");
+      if (i%3 != 0) a.add(w);
+    }
+    log(sb.toString());
+    String stopWords[] = a.toArray(new String[0]);
+    for (int i=0; i<a.size(); i++) log("Stop: "+stopWords[i]);
+    Set<Object> stopSet = StopFilter.makeStopSet(TEST_VERSION_CURRENT, stopWords);
+    // with increments
+    StringReader reader = new StringReader(sb.toString());
+    StopFilter stpf = new StopFilter(Version.LUCENE_24, new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader), stopSet);
+    doTestStopPositons(stpf,true);
+    // without increments
+    reader = new StringReader(sb.toString());
+    stpf = new StopFilter(TEST_VERSION_CURRENT, new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader), stopSet);
+    doTestStopPositons(stpf,false);
+    // with increments, concatenating two stop filters
+    ArrayList<String> a0 = new ArrayList<String>();
+    ArrayList<String> a1 = new ArrayList<String>();
+    for (int i=0; i<a.size(); i++) {
+      if (i%2==0) { 
+        a0.add(a.get(i));
+      } else {
+        a1.add(a.get(i));
+      }
+    }
+    String stopWords0[] =  a0.toArray(new String[0]);
+    for (int i=0; i<a0.size(); i++) log("Stop0: "+stopWords0[i]);
+    String stopWords1[] =  a1.toArray(new String[0]);
+    for (int i=0; i<a1.size(); i++) log("Stop1: "+stopWords1[i]);
+    Set<Object> stopSet0 = StopFilter.makeStopSet(TEST_VERSION_CURRENT, stopWords0);
+    Set<Object> stopSet1 = StopFilter.makeStopSet(TEST_VERSION_CURRENT, stopWords1);
+    reader = new StringReader(sb.toString());
+    StopFilter stpf0 = new StopFilter(TEST_VERSION_CURRENT, new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader), stopSet0); // first part of the set
+    stpf0.setEnablePositionIncrements(true);
+    StopFilter stpf01 = new StopFilter(TEST_VERSION_CURRENT, stpf0, stopSet1); // two stop filters concatenated!
+    doTestStopPositons(stpf01,true);
+  }
+  
+  private void doTestStopPositons(StopFilter stpf, boolean enableIcrements) throws IOException {
+    log("---> test with enable-increments-"+(enableIcrements?"enabled":"disabled"));
+    stpf.setEnablePositionIncrements(enableIcrements);
+    CharTermAttribute termAtt = stpf.getAttribute(CharTermAttribute.class);
+    PositionIncrementAttribute posIncrAtt = stpf.getAttribute(PositionIncrementAttribute.class);
+    for (int i=0; i<20; i+=3) {
+      assertTrue(stpf.incrementToken());
+      log("Token "+i+": "+stpf);
+      String w = English.intToEnglish(i).trim();
+      assertEquals("expecting token "+i+" to be "+w,w,termAtt.toString());
+      assertEquals("all but first token must have position increment of 3",enableIcrements?(i==0?1:3):1,posIncrAtt.getPositionIncrement());
+    }
+    assertFalse(stpf.incrementToken());
+  }
+  
+  // print debug info depending on VERBOSE
+  private static void log(String s) {
+    if (VERBOSE) {
+      System.out.println(s);
+    }
+  }
+}
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/cz/TestCzechStemmer.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/cz/TestCzechStemmer.java
index d2e9b52..75bb060 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/cz/TestCzechStemmer.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/cz/TestCzechStemmer.java
@@ -22,8 +22,8 @@ import java.io.StringReader;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
 
 /**
  * Test the Czech Stemmer.
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanAnalyzer.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanAnalyzer.java
index 8b85b27..94574ca 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanAnalyzer.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanAnalyzer.java
@@ -23,8 +23,8 @@ import java.io.StringReader;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseTokenizer;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.LowerCaseTokenizer;
 import org.apache.lucene.util.Version;
 
 public class TestGermanAnalyzer extends BaseTokenStreamTestCase {
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter.java
index 213be5e..0376ff5 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter.java
@@ -24,10 +24,10 @@ import java.io.InputStreamReader;
 import java.io.StringReader;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.KeywordTokenizer;
-import org.apache.lucene.analysis.LowerCaseFilter;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
 
 /**
  * Test the German stemmer. The stemming algorithm is known to work less 
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/en/TestPorterStemFilter.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/en/TestPorterStemFilter.java
index 613f540..96a6122 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/en/TestPorterStemFilter.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/en/TestPorterStemFilter.java
@@ -26,11 +26,11 @@ import java.util.zip.ZipFile;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.KeywordTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
 
 /**
  * Test the PorterStemFilter with Martin Porter's test data.
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/hi/TestHindiNormalizer.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/hi/TestHindiNormalizer.java
index 42c4781..8d5195d 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/hi/TestHindiNormalizer.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/hi/TestHindiNormalizer.java
@@ -23,7 +23,7 @@ import java.io.StringReader;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Test HindiNormalizer
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/hi/TestHindiStemmer.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/hi/TestHindiStemmer.java
index cce0015..06cda99 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/hi/TestHindiStemmer.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/hi/TestHindiStemmer.java
@@ -23,7 +23,7 @@ import java.io.StringReader;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Test HindiStemmer
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/id/TestIndonesianStemmer.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/id/TestIndonesianStemmer.java
index 09c3c94..3d8468d 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/id/TestIndonesianStemmer.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/id/TestIndonesianStemmer.java
@@ -22,9 +22,9 @@ import java.io.Reader;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.KeywordTokenizer;
-import org.apache.lucene.analysis.ReusableAnalyzerBase;
 import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
+import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
 
 /**
  * Tests {@link IndonesianStemmer}
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/in/TestIndicNormalizer.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/in/TestIndicNormalizer.java
index b1ffd9b..1d586d1 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/in/TestIndicNormalizer.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/in/TestIndicNormalizer.java
@@ -23,7 +23,7 @@ import java.io.StringReader;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Test IndicNormalizer
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/PatternAnalyzerTest.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/PatternAnalyzerTest.java
index a346470..a0a59f6 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/PatternAnalyzerTest.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/PatternAnalyzerTest.java
@@ -22,8 +22,8 @@ import java.util.Arrays;
 import java.util.regex.Pattern;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.StopAnalyzer;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.core.StopAnalyzer;
 
 /**
  * Verifies the behavior of PatternAnalyzer.
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestASCIIFoldingFilter.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestASCIIFoldingFilter.java
index d76954f..824a899 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestASCIIFoldingFilter.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestASCIIFoldingFilter.java
@@ -19,7 +19,7 @@ package org.apache.lucene.analysis.miscellaneous;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import java.io.StringReader;
 import java.util.List;
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestHyphenatedWordsFilter.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestHyphenatedWordsFilter.java
index 5545fa8..a51fa22 100755
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestHyphenatedWordsFilter.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestHyphenatedWordsFilter.java
@@ -21,7 +21,7 @@ import java.io.StringReader;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * HyphenatedWordsFilter test
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestISOLatin1AccentFilter.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestISOLatin1AccentFilter.java
index 4225ecf..8092810 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestISOLatin1AccentFilter.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestISOLatin1AccentFilter.java
@@ -19,7 +19,7 @@ package org.apache.lucene.analysis.miscellaneous;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import java.io.StringReader;
 
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestKeepWordFilter.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestKeepWordFilter.java
index bea2a48..e1ebf7d 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestKeepWordFilter.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestKeepWordFilter.java
@@ -23,7 +23,7 @@ import java.util.Set;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /** Test {@link KeepWordFilter} */
 public class TestKeepWordFilter extends BaseTokenStreamTestCase {
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestKeywordMarkerFilter.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestKeywordMarkerFilter.java
index b24112b..9500c40 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestKeywordMarkerFilter.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestKeywordMarkerFilter.java
@@ -10,7 +10,7 @@ import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.CharArraySet;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.KeywordAttribute;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.junit.Test;
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLengthFilter.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLengthFilter.java
index 010110c..de8b731 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLengthFilter.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLengthFilter.java
@@ -18,6 +18,7 @@ package org.apache.lucene.analysis.miscellaneous;
  */
 
 import org.apache.lucene.analysis.*;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import java.io.StringReader;
 
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPerFieldAnalzyerWrapper.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPerFieldAnalzyerWrapper.java
index a06bac8..fa2c51d 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPerFieldAnalzyerWrapper.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPerFieldAnalzyerWrapper.java
@@ -3,6 +3,8 @@ package org.apache.lucene.analysis.miscellaneous;
 import java.io.StringReader;
 
 import org.apache.lucene.analysis.*;
+import org.apache.lucene.analysis.core.SimpleAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 
 /**
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAndSuffixAwareTokenFilter.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAndSuffixAwareTokenFilter.java
index 1fe55e3..a266fff 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAndSuffixAwareTokenFilter.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAndSuffixAwareTokenFilter.java
@@ -19,7 +19,7 @@ package org.apache.lucene.analysis.miscellaneous;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Token;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 import java.io.IOException;
 import java.io.StringReader;
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAwareTokenFilter.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAwareTokenFilter.java
index b10fc73..c7c9ae5 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAwareTokenFilter.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAwareTokenFilter.java
@@ -19,7 +19,7 @@ package org.apache.lucene.analysis.miscellaneous;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Token;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 import java.io.IOException;
 import java.io.StringReader;
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestStemmerOverrideFilter.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestStemmerOverrideFilter.java
index bf9e129..463faf4 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestStemmerOverrideFilter.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestStemmerOverrideFilter.java
@@ -6,7 +6,7 @@ import java.util.HashMap;
 import java.util.Map;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.KeywordTokenizer;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
 import org.apache.lucene.analysis.en.PorterStemFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter.java
index e04a469..e17d693 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter.java
@@ -20,12 +20,12 @@ package org.apache.lucene.analysis.miscellaneous;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.CharArraySet;
-import org.apache.lucene.analysis.KeywordTokenizer;
-import org.apache.lucene.analysis.StopFilter;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Token;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
+import org.apache.lucene.analysis.core.StopFilter;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.miscellaneous.SingleTokenTokenStream;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java
index 346af2a..903e254 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java
@@ -18,8 +18,8 @@ package org.apache.lucene.analysis.ngram;
  */
 
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 import java.io.StringReader;
 
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java
index ed7c170..9443a25 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java
@@ -18,8 +18,8 @@ package org.apache.lucene.analysis.ngram;
  */
 
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 import java.io.StringReader;
 
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternReplaceCharFilter.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternReplaceCharFilter.java
index 071d601..56418b5 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternReplaceCharFilter.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternReplaceCharFilter.java
@@ -25,7 +25,7 @@ import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.CharReader;
 import org.apache.lucene.analysis.CharStream;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Tests {@link PatternReplaceCharFilter}
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternReplaceFilter.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternReplaceFilter.java
index 41e664b..56e7da3 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternReplaceFilter.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternReplaceFilter.java
@@ -19,7 +19,7 @@ package org.apache.lucene.analysis.pattern;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 import java.io.StringReader;
 import java.util.regex.Pattern;
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/payloads/DelimitedPayloadTokenFilterTest.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/payloads/DelimitedPayloadTokenFilterTest.java
index 1e8970b..dc1e53f 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/payloads/DelimitedPayloadTokenFilterTest.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/payloads/DelimitedPayloadTokenFilterTest.java
@@ -17,7 +17,7 @@ package org.apache.lucene.analysis.payloads;
  */
 
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.index.Payload;
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/payloads/NumericPayloadTokenFilterTest.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/payloads/NumericPayloadTokenFilterTest.java
index a0f479e..7cc9a4a 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/payloads/NumericPayloadTokenFilterTest.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/payloads/NumericPayloadTokenFilterTest.java
@@ -19,7 +19,7 @@ package org.apache.lucene.analysis.payloads;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/payloads/TokenOffsetPayloadTokenFilterTest.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/payloads/TokenOffsetPayloadTokenFilterTest.java
index e503395..371e45c 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/payloads/TokenOffsetPayloadTokenFilterTest.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/payloads/TokenOffsetPayloadTokenFilterTest.java
@@ -17,7 +17,7 @@ package org.apache.lucene.analysis.payloads;
  */
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
 import org.apache.lucene.index.Payload;
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/payloads/TypeAsPayloadTokenFilterTest.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/payloads/TypeAsPayloadTokenFilterTest.java
index b07bd72..aacebe8 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/payloads/TypeAsPayloadTokenFilterTest.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/payloads/TypeAsPayloadTokenFilterTest.java
@@ -19,7 +19,7 @@ package org.apache.lucene.analysis.payloads;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzerTest.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzerTest.java
index 2993dcf..7ed432a9 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzerTest.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzerTest.java
@@ -22,10 +22,10 @@ import java.io.StringReader;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.LetterTokenizer;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.LetterTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/reverse/TestReverseStringFilter.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/reverse/TestReverseStringFilter.java
index fe1a319..b55b735 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/reverse/TestReverseStringFilter.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/reverse/TestReverseStringFilter.java
@@ -20,7 +20,7 @@ package org.apache.lucene.analysis.reverse;
 import java.io.StringReader;
 
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.util.Version;
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java
index 6cda813..ba7346c 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java
@@ -22,10 +22,10 @@ import java.io.StringReader;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.LetterTokenizer;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.LetterTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.document.Document;
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest.java
index d5c43f7..34ffcb8 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest.java
@@ -24,7 +24,7 @@ import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.*;
 
 public class ShingleFilterTest extends BaseTokenStreamTestCase {
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/shingle/TestShingleMatrixFilter.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/shingle/TestShingleMatrixFilter.java
index c715df0..363b97d 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/shingle/TestShingleMatrixFilter.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/shingle/TestShingleMatrixFilter.java
@@ -24,6 +24,7 @@ import java.util.Iterator;
 import java.util.LinkedList;
 
 import org.apache.lucene.analysis.*;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.miscellaneous.EmptyTokenStream;
 import org.apache.lucene.analysis.miscellaneous.PrefixAndSuffixAwareTokenFilter;
 import org.apache.lucene.analysis.miscellaneous.SingleTokenTokenStream;
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/sinks/DateRecognizerSinkTokenizerTest.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/sinks/DateRecognizerSinkTokenizerTest.java
index 7479a6d..344fbde 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/sinks/DateRecognizerSinkTokenizerTest.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/sinks/DateRecognizerSinkTokenizerTest.java
@@ -22,7 +22,7 @@ import java.text.SimpleDateFormat;
 import java.util.Locale;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 public class DateRecognizerSinkTokenizerTest extends BaseTokenStreamTestCase {
 
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter.java
index 54adc3b..57e7831 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter.java
@@ -19,11 +19,11 @@ package org.apache.lucene.analysis.sinks;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.CachingTokenFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/sinks/TokenRangeSinkTokenizerTest.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/sinks/TokenRangeSinkTokenizerTest.java
index 8e99368..10d9cc4 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/sinks/TokenRangeSinkTokenizerTest.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/sinks/TokenRangeSinkTokenizerTest.java
@@ -20,7 +20,7 @@ import java.io.IOException;
 import java.io.StringReader;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 public class TokenRangeSinkTokenizerTest extends BaseTokenStreamTestCase {
 
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/sinks/TokenTypeSinkTokenizerTest.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/sinks/TokenTypeSinkTokenizerTest.java
index 6455e73..bb3fe3c 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/sinks/TokenTypeSinkTokenizerTest.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/sinks/TokenTypeSinkTokenizerTest.java
@@ -22,7 +22,7 @@ import java.io.StringReader;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
 
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/snowball/TestSnowballVocab.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/snowball/TestSnowballVocab.java
index fd7a3a1..22790cc 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/snowball/TestSnowballVocab.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/snowball/TestSnowballVocab.java
@@ -25,9 +25,9 @@ import java.io.StringReader;
 import java.util.zip.ZipFile;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.KeywordTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
 
 /**
  * Test the snowball filters against the snowball data tests
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymFilter.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymFilter.java
index 25e23cd..a8cbff5 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymFilter.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymFilter.java
@@ -21,7 +21,7 @@ import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.FlagsAttribute;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/tr/TestTurkishLowerCaseFilter.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/tr/TestTurkishLowerCaseFilter.java
index c2d1b1d..3885c36 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/analysis/tr/TestTurkishLowerCaseFilter.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/tr/TestTurkishLowerCaseFilter.java
@@ -21,7 +21,7 @@ import java.io.StringReader;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Test the Turkish lowercase filter.
diff --git a/modules/analysis/common/src/test/org/apache/lucene/analysis/util/TestWordlistLoader.java b/modules/analysis/common/src/test/org/apache/lucene/analysis/util/TestWordlistLoader.java
new file mode 100644
index 0000000..74356c4
--- /dev/null
+++ b/modules/analysis/common/src/test/org/apache/lucene/analysis/util/TestWordlistLoader.java
@@ -0,0 +1,81 @@
+package org.apache.lucene.analysis.util;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.io.StringReader;
+import java.util.HashSet;
+import java.util.Set;
+
+import org.apache.lucene.util.LuceneTestCase;
+
+import org.apache.lucene.analysis.util.WordlistLoader;
+
+public class TestWordlistLoader extends LuceneTestCase {
+
+  public void testWordlistLoading() throws IOException {
+    String s = "ONE\n  two \nthree";
+    HashSet<String> wordSet1 = WordlistLoader.getWordSet(new StringReader(s));
+    checkSet(wordSet1);
+    HashSet<String> wordSet2 = WordlistLoader.getWordSet(new BufferedReader(new StringReader(s)));
+    checkSet(wordSet2);
+  }
+
+  public void testComments() throws Exception {
+    String s = "ONE\n  two \nthree\n#comment";
+    HashSet<String> wordSet1 = WordlistLoader.getWordSet(new StringReader(s), "#");
+    checkSet(wordSet1);
+    assertFalse(wordSet1.contains("#comment"));
+    assertFalse(wordSet1.contains("comment"));
+  }
+
+
+  private void checkSet(HashSet<String> wordset) {
+    assertEquals(3, wordset.size());
+    assertTrue(wordset.contains("ONE"));		// case is not modified
+    assertTrue(wordset.contains("two"));		// surrounding whitespace is removed
+    assertTrue(wordset.contains("three"));
+    assertFalse(wordset.contains("four"));
+  }
+
+  /**
+   * Test stopwords in snowball format
+   */
+  public void testSnowballListLoading() throws IOException {
+    String s = 
+      "|comment\n" + // commented line
+      " |comment\n" + // commented line with leading whitespace
+      "\n" + // blank line
+      "  \t\n" + // line with only whitespace
+      " |comment | comment\n" + // commented line with comment
+      "ONE\n" + // stopword, in uppercase
+      "   two   \n" + // stopword with leading/trailing space
+      " three   four five \n" + // multiple stopwords
+      "six seven | comment\n"; //multiple stopwords + comment
+    Set<String> wordset = WordlistLoader.getSnowballWordSet(new StringReader(s));
+    assertEquals(7, wordset.size());
+    assertTrue(wordset.contains("ONE"));
+    assertTrue(wordset.contains("two"));
+    assertTrue(wordset.contains("three"));
+    assertTrue(wordset.contains("four"));
+    assertTrue(wordset.contains("five"));
+    assertTrue(wordset.contains("six"));
+    assertTrue(wordset.contains("seven"));
+  }
+}
diff --git a/modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase.java b/modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase.java
new file mode 100644
index 0000000..342b509
--- /dev/null
+++ b/modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase.java
@@ -0,0 +1,248 @@
+package org.apache.lucene.collation;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermRangeFilter;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TermRangeQuery;
+import org.apache.lucene.search.Searcher;
+import org.apache.lucene.search.Sort;
+import org.apache.lucene.search.SortField;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.util.IndexableBinaryStringTools;
+import org.apache.lucene.util.LuceneTestCase;
+
+import java.io.StringReader;
+import java.io.IOException;
+
+public abstract class CollationTestBase extends LuceneTestCase {
+
+  protected String firstRangeBeginningOriginal = "\u062F";
+  protected String firstRangeEndOriginal = "\u0698";
+  
+  protected String secondRangeBeginningOriginal = "\u0633";
+  protected String secondRangeEndOriginal = "\u0638";
+  
+  /**
+   * Convenience method to perform the same function as CollationKeyFilter.
+   *  
+   * @param keyBits the result from 
+   *  collator.getCollationKey(original).toByteArray()
+   * @return The encoded collation key for the original String
+   */
+  protected String encodeCollationKey(byte[] keyBits) {
+    // Ensure that the backing char[] array is large enough to hold the encoded
+    // Binary String
+    int encodedLength = IndexableBinaryStringTools.getEncodedLength(keyBits, 0, keyBits.length);
+    char[] encodedBegArray = new char[encodedLength];
+    IndexableBinaryStringTools.encode(keyBits, 0, keyBits.length, encodedBegArray, 0, encodedLength);
+    return new String(encodedBegArray);
+  }
+    
+  public void testFarsiRangeFilterCollating(Analyzer analyzer, String firstBeg, 
+                                            String firstEnd, String secondBeg,
+                                            String secondEnd) throws Exception {
+    RAMDirectory ramDir = new RAMDirectory();
+    IndexWriter writer = new IndexWriter(ramDir, new IndexWriterConfig(
+        TEST_VERSION_CURRENT, analyzer));
+    Document doc = new Document();
+    doc.add(new Field("content", "\u0633\u0627\u0628", 
+                      Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(new Field("body", "body",
+                      Field.Store.YES, Field.Index.NOT_ANALYZED));
+    writer.addDocument(doc);
+    writer.close();
+    IndexSearcher searcher = new IndexSearcher(ramDir, true);
+    Query query = new TermQuery(new Term("body","body"));
+
+    // Unicode order would include U+0633 in [ U+062F - U+0698 ], but Farsi
+    // orders the U+0698 character before the U+0633 character, so the single
+    // index Term below should NOT be returned by a TermRangeFilter with a Farsi
+    // Collator (or an Arabic one for the case when Farsi searcher not
+    // supported).
+    ScoreDoc[] result = searcher.search
+      (query, new TermRangeFilter("content", firstBeg, firstEnd, true, true), 1).scoreDocs;
+    assertEquals("The index Term should not be included.", 0, result.length);
+
+    result = searcher.search
+      (query, new TermRangeFilter("content", secondBeg, secondEnd, true, true), 1).scoreDocs;
+    assertEquals("The index Term should be included.", 1, result.length);
+
+    searcher.close();
+  }
+ 
+  public void testFarsiRangeQueryCollating(Analyzer analyzer, String firstBeg, 
+                                            String firstEnd, String secondBeg,
+                                            String secondEnd) throws Exception {
+    RAMDirectory ramDir = new RAMDirectory();
+    IndexWriter writer = new IndexWriter(ramDir, new IndexWriterConfig(
+        TEST_VERSION_CURRENT, analyzer));
+    Document doc = new Document();
+
+    // Unicode order would include U+0633 in [ U+062F - U+0698 ], but Farsi
+    // orders the U+0698 character before the U+0633 character, so the single
+    // index Term below should NOT be returned by a TermRangeQuery with a Farsi
+    // Collator (or an Arabic one for the case when Farsi is not supported).
+    doc.add(new Field("content", "\u0633\u0627\u0628", 
+                      Field.Store.YES, Field.Index.ANALYZED));
+    writer.addDocument(doc);
+    writer.close();
+    IndexSearcher searcher = new IndexSearcher(ramDir, true);
+
+    Query query = new TermRangeQuery("content", firstBeg, firstEnd, true, true);
+    ScoreDoc[] hits = searcher.search(query, null, 1000).scoreDocs;
+    assertEquals("The index Term should not be included.", 0, hits.length);
+
+    query = new TermRangeQuery("content", secondBeg, secondEnd, true, true);
+    hits = searcher.search(query, null, 1000).scoreDocs;
+    assertEquals("The index Term should be included.", 1, hits.length);
+    searcher.close();
+  }
+
+  public void testFarsiTermRangeQuery(Analyzer analyzer, String firstBeg,
+      String firstEnd, String secondBeg, String secondEnd) throws Exception {
+
+    RAMDirectory farsiIndex = new RAMDirectory();
+    IndexWriter writer = new IndexWriter(farsiIndex, new IndexWriterConfig(
+        TEST_VERSION_CURRENT, analyzer));
+    Document doc = new Document();
+    doc.add(new Field("content", "\u0633\u0627\u0628", 
+                      Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(new Field("body", "body",
+                      Field.Store.YES, Field.Index.NOT_ANALYZED));
+    writer.addDocument(doc);
+    writer.close();
+
+    IndexReader reader = IndexReader.open(farsiIndex, true);
+    IndexSearcher search = new IndexSearcher(reader);
+        
+    // Unicode order would include U+0633 in [ U+062F - U+0698 ], but Farsi
+    // orders the U+0698 character before the U+0633 character, so the single
+    // index Term below should NOT be returned by a TermRangeQuery
+    // with a Farsi Collator (or an Arabic one for the case when Farsi is 
+    // not supported).
+    Query csrq 
+      = new TermRangeQuery("content", firstBeg, firstEnd, true, true);
+    ScoreDoc[] result = search.search(csrq, null, 1000).scoreDocs;
+    assertEquals("The index Term should not be included.", 0, result.length);
+
+    csrq = new TermRangeQuery
+      ("content", secondBeg, secondEnd, true, true);
+    result = search.search(csrq, null, 1000).scoreDocs;
+    assertEquals("The index Term should be included.", 1, result.length);
+    search.close();
+  }
+  
+  // Test using various international locales with accented characters (which
+  // sort differently depending on locale)
+  //
+  // Copied (and slightly modified) from 
+  // org.apache.lucene.search.TestSort.testInternationalSort()
+  //  
+  public void testCollationKeySort(Analyzer usAnalyzer,
+                                   Analyzer franceAnalyzer,
+                                   Analyzer swedenAnalyzer,
+                                   Analyzer denmarkAnalyzer,
+                                   String usResult) throws Exception {
+    RAMDirectory indexStore = new RAMDirectory();
+    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(
+        TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));
+
+    // document data:
+    // the tracer field is used to determine which document was hit
+    String[][] sortData = new String[][] {
+      // tracer contents US                 France             Sweden (sv_SE)     Denmark (da_DK)
+      {  "A",   "x",     "p\u00EAche",      "p\u00EAche",      "p\u00EAche",      "p\u00EAche"      },
+      {  "B",   "y",     "HAT",             "HAT",             "HAT",             "HAT"             },
+      {  "C",   "x",     "p\u00E9ch\u00E9", "p\u00E9ch\u00E9", "p\u00E9ch\u00E9", "p\u00E9ch\u00E9" },
+      {  "D",   "y",     "HUT",             "HUT",             "HUT",             "HUT"             },
+      {  "E",   "x",     "peach",           "peach",           "peach",           "peach"           },
+      {  "F",   "y",     "H\u00C5T",        "H\u00C5T",        "H\u00C5T",        "H\u00C5T"        },
+      {  "G",   "x",     "sin",             "sin",             "sin",             "sin"             },
+      {  "H",   "y",     "H\u00D8T",        "H\u00D8T",        "H\u00D8T",        "H\u00D8T"        },
+      {  "I",   "x",     "s\u00EDn",        "s\u00EDn",        "s\u00EDn",        "s\u00EDn"        },
+      {  "J",   "y",     "HOT",             "HOT",             "HOT",             "HOT"             },
+    };
+
+    for (int i = 0 ; i < sortData.length ; ++i) {
+      Document doc = new Document();
+      doc.add(new Field("tracer", sortData[i][0], 
+                        Field.Store.YES, Field.Index.NO));
+      doc.add(new Field("contents", sortData[i][1], 
+                        Field.Store.NO, Field.Index.ANALYZED));
+      if (sortData[i][2] != null) 
+        doc.add(new Field("US", usAnalyzer.reusableTokenStream("US", new StringReader(sortData[i][2]))));
+      if (sortData[i][3] != null) 
+        doc.add(new Field("France", franceAnalyzer.reusableTokenStream("France", new StringReader(sortData[i][3]))));
+      if (sortData[i][4] != null)
+        doc.add(new Field("Sweden", swedenAnalyzer.reusableTokenStream("Sweden", new StringReader(sortData[i][4]))));
+      if (sortData[i][5] != null) 
+        doc.add(new Field("Denmark", denmarkAnalyzer.reusableTokenStream("Denmark", new StringReader(sortData[i][5]))));
+      writer.addDocument(doc);
+    }
+    writer.optimize();
+    writer.close();
+    Searcher searcher = new IndexSearcher(indexStore, true);
+
+    Sort sort = new Sort();
+    Query queryX = new TermQuery(new Term ("contents", "x"));
+    Query queryY = new TermQuery(new Term ("contents", "y"));
+    
+    sort.setSort(new SortField("US", SortField.STRING));
+    assertMatches(searcher, queryY, sort, usResult);
+
+    sort.setSort(new SortField("France", SortField.STRING));
+    assertMatches(searcher, queryX, sort, "EACGI");
+
+    sort.setSort(new SortField("Sweden", SortField.STRING));
+    assertMatches(searcher, queryY, sort, "BJDFH");
+
+    sort.setSort(new SortField("Denmark", SortField.STRING));
+    assertMatches(searcher, queryY, sort, "BJDHF");
+  }
+    
+  // Make sure the documents returned by the search match the expected list
+  // Copied from TestSort.java
+  private void assertMatches(Searcher searcher, Query query, Sort sort, 
+                             String expectedResult) throws IOException {
+    ScoreDoc[] result = searcher.search(query, null, 1000, sort).scoreDocs;
+    StringBuilder buff = new StringBuilder(10);
+    int n = result.length;
+    for (int i = 0 ; i < n ; ++i) {
+      Document doc = searcher.doc(result[i].doc);
+      String[] v = doc.getValues("tracer");
+      for (int j = 0 ; j < v.length ; ++j) {
+        buff.append(v[j]);
+      }
+    }
+    assertEquals(expectedResult, buff.toString());
+  }
+}
diff --git a/modules/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyAnalyzer.java b/modules/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyAnalyzer.java
new file mode 100644
index 0000000..06c6d07
--- /dev/null
+++ b/modules/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyAnalyzer.java
@@ -0,0 +1,77 @@
+package org.apache.lucene.collation;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+import org.apache.lucene.analysis.Analyzer;
+
+import java.text.Collator;
+import java.util.Locale;
+
+
+public class TestCollationKeyAnalyzer extends CollationTestBase {
+
+  // Neither Java 1.4.2 nor 1.5.0 has Farsi Locale collation available in
+  // RuleBasedCollator.  However, the Arabic Locale seems to order the Farsi
+  // characters properly.
+  private Collator collator = Collator.getInstance(new Locale("ar"));
+  private Analyzer analyzer = new CollationKeyAnalyzer(collator);
+
+  private String firstRangeBeginning = encodeCollationKey
+    (collator.getCollationKey(firstRangeBeginningOriginal).toByteArray());
+  private String firstRangeEnd = encodeCollationKey
+    (collator.getCollationKey(firstRangeEndOriginal).toByteArray());
+  private String secondRangeBeginning = encodeCollationKey
+    (collator.getCollationKey(secondRangeBeginningOriginal).toByteArray());
+  private String secondRangeEnd = encodeCollationKey
+    (collator.getCollationKey(secondRangeEndOriginal).toByteArray());
+  
+  public void testFarsiRangeFilterCollating() throws Exception {
+    testFarsiRangeFilterCollating
+      (analyzer, firstRangeBeginning, firstRangeEnd, 
+       secondRangeBeginning, secondRangeEnd);
+  }
+ 
+  public void testFarsiRangeQueryCollating() throws Exception {
+    testFarsiRangeQueryCollating
+      (analyzer, firstRangeBeginning, firstRangeEnd, 
+       secondRangeBeginning, secondRangeEnd);
+  }
+
+  public void testFarsiTermRangeQuery() throws Exception {
+    testFarsiTermRangeQuery
+      (analyzer, firstRangeBeginning, firstRangeEnd, 
+       secondRangeBeginning, secondRangeEnd);
+  }
+  
+  public void testCollationKeySort() throws Exception {
+    Analyzer usAnalyzer 
+      = new CollationKeyAnalyzer(Collator.getInstance(Locale.US));
+    Analyzer franceAnalyzer 
+      = new CollationKeyAnalyzer(Collator.getInstance(Locale.FRANCE));
+    Analyzer swedenAnalyzer 
+      = new CollationKeyAnalyzer(Collator.getInstance(new Locale("sv", "se")));
+    Analyzer denmarkAnalyzer 
+      = new CollationKeyAnalyzer(Collator.getInstance(new Locale("da", "dk")));
+    
+    // The ICU Collator and java.text.Collator implementations differ in their
+    // orderings - "BFJDH" is the ordering for java.text.Collator for Locale.US.
+    testCollationKeySort
+      (usAnalyzer, franceAnalyzer, swedenAnalyzer, denmarkAnalyzer, "BFJDH");
+  }
+}
diff --git a/modules/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyFilter.java b/modules/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyFilter.java
new file mode 100644
index 0000000..543c0ef
--- /dev/null
+++ b/modules/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyFilter.java
@@ -0,0 +1,95 @@
+package org.apache.lucene.collation;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
+
+import java.text.Collator;
+import java.util.Locale;
+import java.io.Reader;
+
+
+public class TestCollationKeyFilter extends CollationTestBase {
+
+  // Neither Java 1.4.2 nor 1.5.0 has Farsi Locale collation available in
+  // RuleBasedCollator.  However, the Arabic Locale seems to order the Farsi
+  // characters properly.
+  private Collator collator = Collator.getInstance(new Locale("ar"));
+  private Analyzer analyzer = new TestAnalyzer(collator);
+
+  private String firstRangeBeginning = encodeCollationKey
+    (collator.getCollationKey(firstRangeBeginningOriginal).toByteArray());
+  private String firstRangeEnd = encodeCollationKey
+    (collator.getCollationKey(firstRangeEndOriginal).toByteArray());
+  private String secondRangeBeginning = encodeCollationKey
+    (collator.getCollationKey(secondRangeBeginningOriginal).toByteArray());
+  private String secondRangeEnd = encodeCollationKey
+    (collator.getCollationKey(secondRangeEndOriginal).toByteArray());
+
+  
+  public final class TestAnalyzer extends Analyzer {
+    private Collator _collator;
+
+    TestAnalyzer(Collator collator) {
+      _collator = collator;
+    }
+
+    @Override
+    public TokenStream tokenStream(String fieldName, Reader reader) {
+      TokenStream result = new KeywordTokenizer(reader);
+      result = new CollationKeyFilter(result, _collator);
+      return result;
+    }
+  }
+
+  public void testFarsiRangeFilterCollating() throws Exception {
+    testFarsiRangeFilterCollating
+      (analyzer, firstRangeBeginning, firstRangeEnd, 
+       secondRangeBeginning, secondRangeEnd);
+  }
+ 
+  public void testFarsiRangeQueryCollating() throws Exception {
+    testFarsiRangeQueryCollating
+      (analyzer, firstRangeBeginning, firstRangeEnd, 
+       secondRangeBeginning, secondRangeEnd);
+  }
+
+  public void testFarsiTermRangeQuery() throws Exception {
+    testFarsiTermRangeQuery
+      (analyzer, firstRangeBeginning, firstRangeEnd, 
+       secondRangeBeginning, secondRangeEnd);
+  }
+  
+  public void testCollationKeySort() throws Exception {
+    Analyzer usAnalyzer = new TestAnalyzer(Collator.getInstance(Locale.US));
+    Analyzer franceAnalyzer 
+      = new TestAnalyzer(Collator.getInstance(Locale.FRANCE));
+    Analyzer swedenAnalyzer 
+      = new TestAnalyzer(Collator.getInstance(new Locale("sv", "se")));
+    Analyzer denmarkAnalyzer 
+      = new TestAnalyzer(Collator.getInstance(new Locale("da", "dk")));
+    
+    // The ICU Collator and java.text.Collator implementations differ in their
+    // orderings - "BFJDH" is the ordering for java.text.Collator for Locale.US.
+    testCollationKeySort
+      (usAnalyzer, franceAnalyzer, swedenAnalyzer, denmarkAnalyzer, "BFJDH");
+  }
+}
diff --git a/modules/analysis/icu/build.xml b/modules/analysis/icu/build.xml
index a415ab7..6e0e64d 100644
--- a/modules/analysis/icu/build.xml
+++ b/modules/analysis/icu/build.xml
@@ -38,6 +38,30 @@
 
   <import file="../../../lucene/contrib/contrib-build.xml"/>
 
+  <module-uptodate name="analysis/common" jarfile="../build/common/lucene-analyzers-common-${version}.jar"
+    property="analyzers-common.uptodate" classpath.property="analyzers-common.jar"/>
+
+  <path id="classpath">
+    <pathelement path="${analyzers-common.jar}"/>
+    <path refid="base.classpath"/>
+  </path>
+
+  <path id="test.classpath">
+  	<pathelement path="${analyzers-common.jar}"/>
+    <path refid="classpath"/>
+    <pathelement location="../../../lucene/build/classes/test/"/>
+  	<pathelement location="../build/common/classes/test/"/>
+    <path refid="junit-path"/>
+    <pathelement location="${build.dir}/classes/java"/>
+  </path>
+
+  <target name="compile-core" depends="build-analyzers-common, common.compile-core" />
+
+  <target name="build-analyzers-common" unless="analyzers-common.uptodate">
+    <echo>ICU building dependency ${analyzers-common.jar}</echo>
+    <ant antfile="../common/build.xml" target="default" inheritall="false" dir="../common" />
+  </target>
+
   <property name="gennorm2.src.dir" value="src/data/utr30"/>
   <property name="gennorm2.src.files" 
   	value="nfkc.txt nfkc_cf.txt BasicFoldings.txt DiacriticFolding.txt DingbatFolding.txt HanRadicalFolding.txt NativeDigitFolding.txt"/>
diff --git a/modules/analysis/icu/src/java/org/apache/lucene/collation/ICUCollationKeyAnalyzer.java b/modules/analysis/icu/src/java/org/apache/lucene/collation/ICUCollationKeyAnalyzer.java
index 982428a..4ad530a 100644
--- a/modules/analysis/icu/src/java/org/apache/lucene/collation/ICUCollationKeyAnalyzer.java
+++ b/modules/analysis/icu/src/java/org/apache/lucene/collation/ICUCollationKeyAnalyzer.java
@@ -21,7 +21,7 @@ package org.apache.lucene.collation;
 import com.ibm.icu.text.Collator;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.KeywordTokenizer;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
 import org.apache.lucene.analysis.Tokenizer;
 
 import java.io.Reader;
diff --git a/modules/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUFoldingFilter.java b/modules/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUFoldingFilter.java
index 350f575..a5ae994 100644
--- a/modules/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUFoldingFilter.java
+++ b/modules/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUFoldingFilter.java
@@ -23,7 +23,7 @@ import java.io.Reader;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Tests ICUFoldingFilter
diff --git a/modules/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUNormalizer2Filter.java b/modules/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUNormalizer2Filter.java
index 8b13dac..7966270 100644
--- a/modules/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUNormalizer2Filter.java
+++ b/modules/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUNormalizer2Filter.java
@@ -23,7 +23,7 @@ import java.io.Reader;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 import com.ibm.icu.text.Normalizer2;
 
diff --git a/modules/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUTransformFilter.java b/modules/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUTransformFilter.java
index 52f8be5..29733ce 100644
--- a/modules/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUTransformFilter.java
+++ b/modules/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUTransformFilter.java
@@ -21,7 +21,7 @@ import java.io.IOException;
 import java.io.StringReader;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.KeywordTokenizer;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 
 import com.ibm.icu.text.Transliterator;
diff --git a/modules/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestICUTokenizer.java b/modules/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestICUTokenizer.java
index ca7b178..7a5ea21 100644
--- a/modules/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestICUTokenizer.java
+++ b/modules/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestICUTokenizer.java
@@ -23,7 +23,7 @@ import java.io.StringReader;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.ReusableAnalyzerBase;
+import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.icu.ICUNormalizer2Filter;
diff --git a/modules/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationKeyFilter.java b/modules/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationKeyFilter.java
index b4536fc..30c2f59 100644
--- a/modules/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationKeyFilter.java
+++ b/modules/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationKeyFilter.java
@@ -21,7 +21,7 @@ package org.apache.lucene.collation;
 import com.ibm.icu.text.Collator;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.KeywordTokenizer;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
 
 import java.io.Reader;
 import java.util.Locale;
diff --git a/modules/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SmartChineseAnalyzer.java b/modules/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SmartChineseAnalyzer.java
index 629e401..e422879 100644
--- a/modules/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SmartChineseAnalyzer.java
+++ b/modules/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SmartChineseAnalyzer.java
@@ -26,12 +26,12 @@ import java.util.Set;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.en.PorterStemFilter;
-import org.apache.lucene.analysis.StopFilter;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.cn.smart.SentenceTokenizer;
 import org.apache.lucene.analysis.cn.smart.WordTokenFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.util.Version;
 
 /**
diff --git a/modules/analysis/stempel/src/java/org/apache/lucene/analysis/pl/PolishAnalyzer.java b/modules/analysis/stempel/src/java/org/apache/lucene/analysis/pl/PolishAnalyzer.java
index 5617612..7565edb 100644
--- a/modules/analysis/stempel/src/java/org/apache/lucene/analysis/pl/PolishAnalyzer.java
+++ b/modules/analysis/stempel/src/java/org/apache/lucene/analysis/pl/PolishAnalyzer.java
@@ -26,17 +26,17 @@ import java.util.Set;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
 import org.apache.lucene.analysis.stempel.StempelStemmer;
 import org.apache.lucene.analysis.stempel.StempelFilter;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 import org.egothor.stemmer.Trie;
 
@@ -129,11 +129,11 @@ public final class PolishAnalyzer extends StopwordAnalyzerBase {
 
   /**
    * Creates a
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * which tokenizes all the text in the provided {@link Reader}.
    * 
    * @return A
-   *         {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   *         {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from an {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
    *         , {@link KeywordMarkerFilter} if a stem exclusion set is
diff --git a/solr/contrib/extraction/src/test/resources/solr/conf/schema.xml b/solr/contrib/extraction/src/test/resources/solr/conf/schema.xml
index 68c0d44..de8e7c7 100644
--- a/solr/contrib/extraction/src/test/resources/solr/conf/schema.xml
+++ b/solr/contrib/extraction/src/test/resources/solr/conf/schema.xml
@@ -127,7 +127,7 @@
 
 
     <fieldtype name="nametext" class="solr.TextField">
-      <analyzer class="org.apache.lucene.analysis.WhitespaceAnalyzer"/>
+      <analyzer class="org.apache.lucene.analysis.core.WhitespaceAnalyzer"/>
     </fieldtype>
 
     <fieldtype name="teststop" class="solr.TextField">
diff --git a/solr/src/java/org/apache/solr/analysis/BaseTokenStreamFactory.java b/solr/src/java/org/apache/solr/analysis/BaseTokenStreamFactory.java
index af01e6b..dc3f31e 100644
--- a/solr/src/java/org/apache/solr/analysis/BaseTokenStreamFactory.java
+++ b/solr/src/java/org/apache/solr/analysis/BaseTokenStreamFactory.java
@@ -27,7 +27,7 @@ import java.util.List;
 import java.util.Map;
 
 import org.apache.lucene.analysis.CharArraySet;
-import org.apache.lucene.analysis.StopFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.util.Version;
 
 
diff --git a/solr/src/java/org/apache/solr/analysis/CommonGramsFilterFactory.java b/solr/src/java/org/apache/solr/analysis/CommonGramsFilterFactory.java
index 2116cdd..2e2eeef 100644
--- a/solr/src/java/org/apache/solr/analysis/CommonGramsFilterFactory.java
+++ b/solr/src/java/org/apache/solr/analysis/CommonGramsFilterFactory.java
@@ -20,9 +20,9 @@ import java.io.IOException;
 import java.util.Set;
 
 import org.apache.lucene.analysis.CharArraySet;
-import org.apache.lucene.analysis.StopAnalyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.commongrams.CommonGramsFilter;
+import org.apache.lucene.analysis.core.StopAnalyzer;
 import org.apache.solr.common.ResourceLoader;
 import org.apache.solr.util.plugin.ResourceLoaderAware;
 
diff --git a/solr/src/java/org/apache/solr/analysis/CommonGramsQueryFilterFactory.java b/solr/src/java/org/apache/solr/analysis/CommonGramsQueryFilterFactory.java
index 7b0397f..d0e2b2b 100644
--- a/solr/src/java/org/apache/solr/analysis/CommonGramsQueryFilterFactory.java
+++ b/solr/src/java/org/apache/solr/analysis/CommonGramsQueryFilterFactory.java
@@ -21,10 +21,10 @@ import java.util.Map;
 import java.util.Set;
 
 import org.apache.lucene.analysis.CharArraySet;
-import org.apache.lucene.analysis.StopAnalyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.commongrams.CommonGramsFilter;
 import org.apache.lucene.analysis.commongrams.CommonGramsQueryFilter;
+import org.apache.lucene.analysis.core.StopAnalyzer;
 import org.apache.solr.common.ResourceLoader;
 import org.apache.solr.util.plugin.ResourceLoaderAware;
 
diff --git a/solr/src/java/org/apache/solr/analysis/KeywordTokenizerFactory.java b/solr/src/java/org/apache/solr/analysis/KeywordTokenizerFactory.java
index 1edaf2e..e50aa72 100644
--- a/solr/src/java/org/apache/solr/analysis/KeywordTokenizerFactory.java
+++ b/solr/src/java/org/apache/solr/analysis/KeywordTokenizerFactory.java
@@ -17,7 +17,7 @@
 
 package org.apache.solr.analysis;
 
-import org.apache.lucene.analysis.KeywordTokenizer;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
 
 import java.io.Reader;
 
diff --git a/solr/src/java/org/apache/solr/analysis/LetterTokenizerFactory.java b/solr/src/java/org/apache/solr/analysis/LetterTokenizerFactory.java
index 09f34dc..4362bbd 100644
--- a/solr/src/java/org/apache/solr/analysis/LetterTokenizerFactory.java
+++ b/solr/src/java/org/apache/solr/analysis/LetterTokenizerFactory.java
@@ -17,7 +17,7 @@
 
 package org.apache.solr.analysis;
 
-import org.apache.lucene.analysis.LetterTokenizer;
+import org.apache.lucene.analysis.core.LetterTokenizer;
 
 import java.io.Reader;
 import java.util.Map;
diff --git a/solr/src/java/org/apache/solr/analysis/LowerCaseFilterFactory.java b/solr/src/java/org/apache/solr/analysis/LowerCaseFilterFactory.java
index 67af3ce..7aa82f0 100644
--- a/solr/src/java/org/apache/solr/analysis/LowerCaseFilterFactory.java
+++ b/solr/src/java/org/apache/solr/analysis/LowerCaseFilterFactory.java
@@ -20,7 +20,7 @@ package org.apache.solr.analysis;
 import java.util.Map;
 
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.LowerCaseFilter;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
 
 /**
  * @version $Id$
diff --git a/solr/src/java/org/apache/solr/analysis/LowerCaseTokenizerFactory.java b/solr/src/java/org/apache/solr/analysis/LowerCaseTokenizerFactory.java
index 5c45c8a..7f4c255 100644
--- a/solr/src/java/org/apache/solr/analysis/LowerCaseTokenizerFactory.java
+++ b/solr/src/java/org/apache/solr/analysis/LowerCaseTokenizerFactory.java
@@ -17,7 +17,7 @@
 
 package org.apache.solr.analysis;
 
-import org.apache.lucene.analysis.LowerCaseTokenizer;
+import org.apache.lucene.analysis.core.LowerCaseTokenizer;
 
 import java.io.Reader;
 import java.util.Map;
diff --git a/solr/src/java/org/apache/solr/analysis/RussianLowerCaseFilterFactory.java b/solr/src/java/org/apache/solr/analysis/RussianLowerCaseFilterFactory.java
index d240c36..de6cda7 100644
--- a/solr/src/java/org/apache/solr/analysis/RussianLowerCaseFilterFactory.java
+++ b/solr/src/java/org/apache/solr/analysis/RussianLowerCaseFilterFactory.java
@@ -19,9 +19,9 @@ package org.apache.solr.analysis;
 
 import java.util.Map;
 
-import org.apache.lucene.analysis.LowerCaseFilter;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
 import org.apache.lucene.util.Version;
 import org.apache.solr.common.SolrException;
 import org.apache.solr.common.SolrException.ErrorCode;
diff --git a/solr/src/java/org/apache/solr/analysis/StopFilterFactory.java b/solr/src/java/org/apache/solr/analysis/StopFilterFactory.java
index 563d5aa..ca65b0c 100644
--- a/solr/src/java/org/apache/solr/analysis/StopFilterFactory.java
+++ b/solr/src/java/org/apache/solr/analysis/StopFilterFactory.java
@@ -19,10 +19,10 @@ package org.apache.solr.analysis;
 
 import org.apache.solr.common.ResourceLoader;
 import org.apache.solr.util.plugin.ResourceLoaderAware;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopAnalyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.StopAnalyzer;
+import org.apache.lucene.analysis.core.StopFilter;
 
 import java.util.Map;
 import java.util.Set;
diff --git a/solr/src/java/org/apache/solr/analysis/WhitespaceTokenizerFactory.java b/solr/src/java/org/apache/solr/analysis/WhitespaceTokenizerFactory.java
index c752188..bb8e7cf 100644
--- a/solr/src/java/org/apache/solr/analysis/WhitespaceTokenizerFactory.java
+++ b/solr/src/java/org/apache/solr/analysis/WhitespaceTokenizerFactory.java
@@ -17,7 +17,7 @@
 
 package org.apache.solr.analysis;
 
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 import java.io.Reader;
 import java.util.Map;
diff --git a/solr/src/java/org/apache/solr/handler/component/SpellCheckComponent.java b/solr/src/java/org/apache/solr/handler/component/SpellCheckComponent.java
index 07da525..8d373b2 100644
--- a/solr/src/java/org/apache/solr/handler/component/SpellCheckComponent.java
+++ b/solr/src/java/org/apache/solr/handler/component/SpellCheckComponent.java
@@ -32,7 +32,7 @@ import org.slf4j.LoggerFactory;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.analysis.tokenattributes.FlagsAttribute;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
diff --git a/solr/src/java/org/apache/solr/spelling/AbstractLuceneSpellChecker.java b/solr/src/java/org/apache/solr/spelling/AbstractLuceneSpellChecker.java
index 1da398f..52a4d08 100644
--- a/solr/src/java/org/apache/solr/spelling/AbstractLuceneSpellChecker.java
+++ b/solr/src/java/org/apache/solr/spelling/AbstractLuceneSpellChecker.java
@@ -27,7 +27,7 @@ import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 import org.apache.lucene.analysis.Token;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.spell.Dictionary;
diff --git a/solr/src/test/org/apache/solr/analysis/CommonGramsFilterFactoryTest.java b/solr/src/test/org/apache/solr/analysis/CommonGramsFilterFactoryTest.java
index 6da1129..5d0123a 100644
--- a/solr/src/test/org/apache/solr/analysis/CommonGramsFilterFactoryTest.java
+++ b/solr/src/test/org/apache/solr/analysis/CommonGramsFilterFactoryTest.java
@@ -19,7 +19,7 @@ package org.apache.solr.analysis;
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.solr.common.ResourceLoader;
 import org.apache.solr.core.SolrResourceLoader;
 
diff --git a/solr/src/test/org/apache/solr/analysis/CommonGramsQueryFilterFactoryTest.java b/solr/src/test/org/apache/solr/analysis/CommonGramsQueryFilterFactoryTest.java
index bf09300..29a655d 100644
--- a/solr/src/test/org/apache/solr/analysis/CommonGramsQueryFilterFactoryTest.java
+++ b/solr/src/test/org/apache/solr/analysis/CommonGramsQueryFilterFactoryTest.java
@@ -18,7 +18,7 @@ package org.apache.solr.analysis;
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.solr.common.ResourceLoader;
 import org.apache.solr.core.SolrResourceLoader;
 
diff --git a/solr/src/test/org/apache/solr/analysis/DoubleMetaphoneFilterFactoryTest.java b/solr/src/test/org/apache/solr/analysis/DoubleMetaphoneFilterFactoryTest.java
index 45f8c1f..c61c827 100644
--- a/solr/src/test/org/apache/solr/analysis/DoubleMetaphoneFilterFactoryTest.java
+++ b/solr/src/test/org/apache/solr/analysis/DoubleMetaphoneFilterFactoryTest.java
@@ -21,7 +21,7 @@ import java.util.HashMap;
 import java.util.Map;
 
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 
 public class DoubleMetaphoneFilterFactoryTest extends BaseTokenTestCase {
diff --git a/solr/src/test/org/apache/solr/analysis/DoubleMetaphoneFilterTest.java b/solr/src/test/org/apache/solr/analysis/DoubleMetaphoneFilterTest.java
index f05d24a..35d03b1 100644
--- a/solr/src/test/org/apache/solr/analysis/DoubleMetaphoneFilterTest.java
+++ b/solr/src/test/org/apache/solr/analysis/DoubleMetaphoneFilterTest.java
@@ -19,7 +19,7 @@ package org.apache.solr.analysis;
 import java.io.StringReader;
 
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 public class DoubleMetaphoneFilterTest extends BaseTokenTestCase {
 
diff --git a/solr/src/test/org/apache/solr/analysis/EnglishPorterFilterFactoryTest.java b/solr/src/test/org/apache/solr/analysis/EnglishPorterFilterFactoryTest.java
index ac37331..a0357ac 100644
--- a/solr/src/test/org/apache/solr/analysis/EnglishPorterFilterFactoryTest.java
+++ b/solr/src/test/org/apache/solr/analysis/EnglishPorterFilterFactoryTest.java
@@ -18,7 +18,7 @@ package org.apache.solr.analysis;
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.solr.common.ResourceLoader;
 import org.apache.solr.common.util.StrUtils;
 import org.tartarus.snowball.ext.EnglishStemmer;
diff --git a/solr/src/test/org/apache/solr/analysis/LengthFilterTest.java b/solr/src/test/org/apache/solr/analysis/LengthFilterTest.java
index 300e4b4..66ba3a8 100644
--- a/solr/src/test/org/apache/solr/analysis/LengthFilterTest.java
+++ b/solr/src/test/org/apache/solr/analysis/LengthFilterTest.java
@@ -22,7 +22,7 @@ import java.util.HashMap;
 import java.util.Map;
 
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 public class LengthFilterTest extends BaseTokenTestCase {
 
diff --git a/solr/src/test/org/apache/solr/analysis/SnowballPorterFilterFactoryTest.java b/solr/src/test/org/apache/solr/analysis/SnowballPorterFilterFactoryTest.java
index e786c00..5475d3e 100644
--- a/solr/src/test/org/apache/solr/analysis/SnowballPorterFilterFactoryTest.java
+++ b/solr/src/test/org/apache/solr/analysis/SnowballPorterFilterFactoryTest.java
@@ -18,7 +18,7 @@ package org.apache.solr.analysis;
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.solr.common.ResourceLoader;
 import org.apache.solr.common.util.StrUtils;
 import org.apache.solr.core.SolrResourceLoader;
diff --git a/solr/src/test/org/apache/solr/analysis/TestBrazilianStemFilterFactory.java b/solr/src/test/org/apache/solr/analysis/TestBrazilianStemFilterFactory.java
index bba8893..e090ade 100644
--- a/solr/src/test/org/apache/solr/analysis/TestBrazilianStemFilterFactory.java
+++ b/solr/src/test/org/apache/solr/analysis/TestBrazilianStemFilterFactory.java
@@ -22,7 +22,7 @@ import java.io.StringReader;
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the Brazilian stem filter factory is working.
diff --git a/solr/src/test/org/apache/solr/analysis/TestBufferedTokenStream.java b/solr/src/test/org/apache/solr/analysis/TestBufferedTokenStream.java
index aa0012f..b135204 100644
--- a/solr/src/test/org/apache/solr/analysis/TestBufferedTokenStream.java
+++ b/solr/src/test/org/apache/solr/analysis/TestBufferedTokenStream.java
@@ -20,7 +20,7 @@ package org.apache.solr.analysis;
 import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 
 import java.io.IOException;
diff --git a/solr/src/test/org/apache/solr/analysis/TestBulgarianStemFilterFactory.java b/solr/src/test/org/apache/solr/analysis/TestBulgarianStemFilterFactory.java
index 1a9a558..e2107bf 100644
--- a/solr/src/test/org/apache/solr/analysis/TestBulgarianStemFilterFactory.java
+++ b/solr/src/test/org/apache/solr/analysis/TestBulgarianStemFilterFactory.java
@@ -22,7 +22,7 @@ import java.io.StringReader;
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the Bulgarian stem filter factory is working.
diff --git a/solr/src/test/org/apache/solr/analysis/TestCapitalizationFilter.java b/solr/src/test/org/apache/solr/analysis/TestCapitalizationFilter.java
index 70f24f9..2b1bd10 100644
--- a/solr/src/test/org/apache/solr/analysis/TestCapitalizationFilter.java
+++ b/solr/src/test/org/apache/solr/analysis/TestCapitalizationFilter.java
@@ -21,10 +21,10 @@ import java.io.StringReader;
 import java.util.HashMap;
 import java.util.Map;
 
-import org.apache.lucene.analysis.KeywordTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 
 /**
diff --git a/solr/src/test/org/apache/solr/analysis/TestChineseFilterFactory.java b/solr/src/test/org/apache/solr/analysis/TestChineseFilterFactory.java
index 8c0d9e3..ae41c17 100644
--- a/solr/src/test/org/apache/solr/analysis/TestChineseFilterFactory.java
+++ b/solr/src/test/org/apache/solr/analysis/TestChineseFilterFactory.java
@@ -22,7 +22,7 @@ import java.io.StringReader;
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the Chinese filter factory is working.
diff --git a/solr/src/test/org/apache/solr/analysis/TestCollationKeyFilterFactory.java b/solr/src/test/org/apache/solr/analysis/TestCollationKeyFilterFactory.java
index c508a65..34719d2 100644
--- a/solr/src/test/org/apache/solr/analysis/TestCollationKeyFilterFactory.java
+++ b/solr/src/test/org/apache/solr/analysis/TestCollationKeyFilterFactory.java
@@ -28,8 +28,8 @@ import java.util.List;
 import java.util.Locale;
 import java.util.Map;
 
-import org.apache.lucene.analysis.KeywordTokenizer;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.solr.common.ResourceLoader;
 
diff --git a/solr/src/test/org/apache/solr/analysis/TestCzechStemFilterFactory.java b/solr/src/test/org/apache/solr/analysis/TestCzechStemFilterFactory.java
index 5f94194..929ec82 100644
--- a/solr/src/test/org/apache/solr/analysis/TestCzechStemFilterFactory.java
+++ b/solr/src/test/org/apache/solr/analysis/TestCzechStemFilterFactory.java
@@ -22,7 +22,7 @@ import java.io.StringReader;
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the Czech stem filter factory is working.
diff --git a/solr/src/test/org/apache/solr/analysis/TestDelimitedPayloadTokenFilterFactory.java b/solr/src/test/org/apache/solr/analysis/TestDelimitedPayloadTokenFilterFactory.java
index bc7f4b9..cc4af4f 100644
--- a/solr/src/test/org/apache/solr/analysis/TestDelimitedPayloadTokenFilterFactory.java
+++ b/solr/src/test/org/apache/solr/analysis/TestDelimitedPayloadTokenFilterFactory.java
@@ -22,7 +22,7 @@ import java.util.HashMap;
 import java.util.Map;
 
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.payloads.DelimitedPayloadTokenFilter;
 import org.apache.lucene.analysis.payloads.FloatEncoder;
 import org.apache.lucene.analysis.payloads.PayloadHelper;
diff --git a/solr/src/test/org/apache/solr/analysis/TestDictionaryCompoundWordTokenFilterFactory.java b/solr/src/test/org/apache/solr/analysis/TestDictionaryCompoundWordTokenFilterFactory.java
index 85fbc5c..69dea68 100644
--- a/solr/src/test/org/apache/solr/analysis/TestDictionaryCompoundWordTokenFilterFactory.java
+++ b/solr/src/test/org/apache/solr/analysis/TestDictionaryCompoundWordTokenFilterFactory.java
@@ -24,7 +24,7 @@ import java.util.Map;
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.solr.common.ResourceLoader;
 import org.apache.solr.core.SolrResourceLoader;
 
diff --git a/solr/src/test/org/apache/solr/analysis/TestDutchStemFilterFactory.java b/solr/src/test/org/apache/solr/analysis/TestDutchStemFilterFactory.java
index aee235c..6e3be4d 100644
--- a/solr/src/test/org/apache/solr/analysis/TestDutchStemFilterFactory.java
+++ b/solr/src/test/org/apache/solr/analysis/TestDutchStemFilterFactory.java
@@ -22,7 +22,7 @@ import java.io.StringReader;
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the Dutch stem filter factory is working.
diff --git a/solr/src/test/org/apache/solr/analysis/TestElisionFilterFactory.java b/solr/src/test/org/apache/solr/analysis/TestElisionFilterFactory.java
index feb3749..7399baf 100644
--- a/solr/src/test/org/apache/solr/analysis/TestElisionFilterFactory.java
+++ b/solr/src/test/org/apache/solr/analysis/TestElisionFilterFactory.java
@@ -24,7 +24,7 @@ import java.util.Map;
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.solr.common.ResourceLoader;
 import org.apache.solr.core.SolrResourceLoader;
 
diff --git a/solr/src/test/org/apache/solr/analysis/TestFrenchStemFilterFactory.java b/solr/src/test/org/apache/solr/analysis/TestFrenchStemFilterFactory.java
index 7a32060..d950df1 100644
--- a/solr/src/test/org/apache/solr/analysis/TestFrenchStemFilterFactory.java
+++ b/solr/src/test/org/apache/solr/analysis/TestFrenchStemFilterFactory.java
@@ -22,7 +22,7 @@ import java.io.StringReader;
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the French stem filter factory is working.
diff --git a/solr/src/test/org/apache/solr/analysis/TestGermanStemFilterFactory.java b/solr/src/test/org/apache/solr/analysis/TestGermanStemFilterFactory.java
index 618ab52..ef6cea0 100644
--- a/solr/src/test/org/apache/solr/analysis/TestGermanStemFilterFactory.java
+++ b/solr/src/test/org/apache/solr/analysis/TestGermanStemFilterFactory.java
@@ -22,7 +22,7 @@ import java.io.StringReader;
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the German stem filter factory is working.
diff --git a/solr/src/test/org/apache/solr/analysis/TestGreekLowerCaseFilterFactory.java b/solr/src/test/org/apache/solr/analysis/TestGreekLowerCaseFilterFactory.java
index 929204e..a74d108 100644
--- a/solr/src/test/org/apache/solr/analysis/TestGreekLowerCaseFilterFactory.java
+++ b/solr/src/test/org/apache/solr/analysis/TestGreekLowerCaseFilterFactory.java
@@ -22,7 +22,7 @@ import java.io.StringReader;
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the Greek lowercase filter factory is working.
diff --git a/solr/src/test/org/apache/solr/analysis/TestGreekStemFilterFactory.java b/solr/src/test/org/apache/solr/analysis/TestGreekStemFilterFactory.java
index c2ede0d..b4f70f9 100644
--- a/solr/src/test/org/apache/solr/analysis/TestGreekStemFilterFactory.java
+++ b/solr/src/test/org/apache/solr/analysis/TestGreekStemFilterFactory.java
@@ -5,7 +5,7 @@ import java.io.StringReader;
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.el.GreekLowerCaseFilter;
 
 /**
diff --git a/solr/src/test/org/apache/solr/analysis/TestIndonesianStemFilterFactory.java b/solr/src/test/org/apache/solr/analysis/TestIndonesianStemFilterFactory.java
index 37a35af..e73961f 100644
--- a/solr/src/test/org/apache/solr/analysis/TestIndonesianStemFilterFactory.java
+++ b/solr/src/test/org/apache/solr/analysis/TestIndonesianStemFilterFactory.java
@@ -24,7 +24,7 @@ import java.util.Map;
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the Indonesian stem filter factory is working.
diff --git a/solr/src/test/org/apache/solr/analysis/TestKeywordMarkerFilterFactory.java b/solr/src/test/org/apache/solr/analysis/TestKeywordMarkerFilterFactory.java
index 5638d58..922082c 100644
--- a/solr/src/test/org/apache/solr/analysis/TestKeywordMarkerFilterFactory.java
+++ b/solr/src/test/org/apache/solr/analysis/TestKeywordMarkerFilterFactory.java
@@ -23,10 +23,10 @@ import java.io.StringReader;
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.en.PorterStemFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.solr.common.ResourceLoader;
 import org.apache.solr.core.SolrResourceLoader;
 
diff --git a/solr/src/test/org/apache/solr/analysis/TestMultiWordSynonyms.java b/solr/src/test/org/apache/solr/analysis/TestMultiWordSynonyms.java
index 77f3e5b3..e4f71c5 100644
--- a/solr/src/test/org/apache/solr/analysis/TestMultiWordSynonyms.java
+++ b/solr/src/test/org/apache/solr/analysis/TestMultiWordSynonyms.java
@@ -1,6 +1,6 @@
 package org.apache.solr.analysis;
 
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.synonym.SynonymFilter;
 import org.apache.lucene.analysis.synonym.SynonymMap;
 import org.junit.Test;
diff --git a/solr/src/test/org/apache/solr/analysis/TestNGramFilters.java b/solr/src/test/org/apache/solr/analysis/TestNGramFilters.java
index 0b31ee2..45dbeda 100644
--- a/solr/src/test/org/apache/solr/analysis/TestNGramFilters.java
+++ b/solr/src/test/org/apache/solr/analysis/TestNGramFilters.java
@@ -24,7 +24,7 @@ import java.util.Map;
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the NGram filter factories are working.
diff --git a/solr/src/test/org/apache/solr/analysis/TestPatternReplaceCharFilterFactory.java b/solr/src/test/org/apache/solr/analysis/TestPatternReplaceCharFilterFactory.java
index 983474e..ef60e0d 100644
--- a/solr/src/test/org/apache/solr/analysis/TestPatternReplaceCharFilterFactory.java
+++ b/solr/src/test/org/apache/solr/analysis/TestPatternReplaceCharFilterFactory.java
@@ -25,7 +25,7 @@ import java.util.Map;
 import org.apache.lucene.analysis.CharReader;
 import org.apache.lucene.analysis.CharStream;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure this factory is working
diff --git a/solr/src/test/org/apache/solr/analysis/TestPatternReplaceFilterFactory.java b/solr/src/test/org/apache/solr/analysis/TestPatternReplaceFilterFactory.java
index 485fdd7..878b07d 100644
--- a/solr/src/test/org/apache/solr/analysis/TestPatternReplaceFilterFactory.java
+++ b/solr/src/test/org/apache/solr/analysis/TestPatternReplaceFilterFactory.java
@@ -18,7 +18,7 @@
 package org.apache.solr.analysis;
 
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 import java.io.StringReader;
 import java.util.HashMap;
diff --git a/solr/src/test/org/apache/solr/analysis/TestPersianNormalizationFilterFactory.java b/solr/src/test/org/apache/solr/analysis/TestPersianNormalizationFilterFactory.java
index 8831f36..d7cd1c6 100644
--- a/solr/src/test/org/apache/solr/analysis/TestPersianNormalizationFilterFactory.java
+++ b/solr/src/test/org/apache/solr/analysis/TestPersianNormalizationFilterFactory.java
@@ -22,7 +22,7 @@ import java.io.StringReader;
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the Persian normalization factory is working.
diff --git a/solr/src/test/org/apache/solr/analysis/TestPhoneticFilter.java b/solr/src/test/org/apache/solr/analysis/TestPhoneticFilter.java
index 6f0c62b..c2875be 100644
--- a/solr/src/test/org/apache/solr/analysis/TestPhoneticFilter.java
+++ b/solr/src/test/org/apache/solr/analysis/TestPhoneticFilter.java
@@ -24,7 +24,7 @@ import java.util.Map;
 import org.apache.commons.codec.language.Metaphone;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 
 /**
diff --git a/solr/src/test/org/apache/solr/analysis/TestPorterStemFilterFactory.java b/solr/src/test/org/apache/solr/analysis/TestPorterStemFilterFactory.java
index acfc2a7..58b17d9 100644
--- a/solr/src/test/org/apache/solr/analysis/TestPorterStemFilterFactory.java
+++ b/solr/src/test/org/apache/solr/analysis/TestPorterStemFilterFactory.java
@@ -22,7 +22,7 @@ import java.io.StringReader;
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the Porter stem filter factory is working.
diff --git a/solr/src/test/org/apache/solr/analysis/TestReverseStringFilterFactory.java b/solr/src/test/org/apache/solr/analysis/TestReverseStringFilterFactory.java
index 9672693..8165684 100644
--- a/solr/src/test/org/apache/solr/analysis/TestReverseStringFilterFactory.java
+++ b/solr/src/test/org/apache/solr/analysis/TestReverseStringFilterFactory.java
@@ -22,7 +22,7 @@ import java.io.StringReader;
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the Reverse string filter factory is working.
diff --git a/solr/src/test/org/apache/solr/analysis/TestReversedWildcardFilterFactory.java b/solr/src/test/org/apache/solr/analysis/TestReversedWildcardFilterFactory.java
index a457df9..00ff0cd 100644
--- a/solr/src/test/org/apache/solr/analysis/TestReversedWildcardFilterFactory.java
+++ b/solr/src/test/org/apache/solr/analysis/TestReversedWildcardFilterFactory.java
@@ -25,7 +25,7 @@ import java.util.Map;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.queryParser.ParseException;
 import org.apache.lucene.search.Query;
 import org.apache.solr.SolrTestCaseJ4;
diff --git a/solr/src/test/org/apache/solr/analysis/TestShingleFilterFactory.java b/solr/src/test/org/apache/solr/analysis/TestShingleFilterFactory.java
index 4e8248c..1f30fee 100644
--- a/solr/src/test/org/apache/solr/analysis/TestShingleFilterFactory.java
+++ b/solr/src/test/org/apache/solr/analysis/TestShingleFilterFactory.java
@@ -23,7 +23,7 @@ import java.util.HashMap;
 import java.util.Map;
 
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the Shingle filter factory works.
diff --git a/solr/src/test/org/apache/solr/analysis/TestStandardFactories.java b/solr/src/test/org/apache/solr/analysis/TestStandardFactories.java
index 62c807f5..c3b9a08 100644
--- a/solr/src/test/org/apache/solr/analysis/TestStandardFactories.java
+++ b/solr/src/test/org/apache/solr/analysis/TestStandardFactories.java
@@ -22,7 +22,7 @@ import java.io.StringReader;
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the standard lucene factories are working.
diff --git a/solr/src/test/org/apache/solr/analysis/TestStemmerOverrideFilterFactory.java b/solr/src/test/org/apache/solr/analysis/TestStemmerOverrideFilterFactory.java
index ae2b936..6abd902 100644
--- a/solr/src/test/org/apache/solr/analysis/TestStemmerOverrideFilterFactory.java
+++ b/solr/src/test/org/apache/solr/analysis/TestStemmerOverrideFilterFactory.java
@@ -23,10 +23,10 @@ import java.io.StringReader;
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.en.PorterStemFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.solr.common.ResourceLoader;
 import org.apache.solr.core.SolrResourceLoader;
 
diff --git a/solr/src/test/org/apache/solr/analysis/TestThaiWordFilterFactory.java b/solr/src/test/org/apache/solr/analysis/TestThaiWordFilterFactory.java
index 1420b30..3ae0e21 100644
--- a/solr/src/test/org/apache/solr/analysis/TestThaiWordFilterFactory.java
+++ b/solr/src/test/org/apache/solr/analysis/TestThaiWordFilterFactory.java
@@ -22,7 +22,7 @@ import java.io.StringReader;
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the Thai word filter factory is working.
diff --git a/solr/src/test/org/apache/solr/analysis/TestTrimFilterFactory.java b/solr/src/test/org/apache/solr/analysis/TestTrimFilterFactory.java
index 9e1c684..627eba5 100644
--- a/solr/src/test/org/apache/solr/analysis/TestTrimFilterFactory.java
+++ b/solr/src/test/org/apache/solr/analysis/TestTrimFilterFactory.java
@@ -23,9 +23,9 @@ import java.util.Collection;
 import java.util.HashMap;
 import java.util.Map;
 
-import org.apache.lucene.analysis.KeywordTokenizer;
 import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
 import org.apache.lucene.analysis.tokenattributes.FlagsAttribute;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
diff --git a/solr/src/test/org/apache/solr/analysis/TestTurkishLowerCaseFilterFactory.java b/solr/src/test/org/apache/solr/analysis/TestTurkishLowerCaseFilterFactory.java
index c533a53..e2174ea 100644
--- a/solr/src/test/org/apache/solr/analysis/TestTurkishLowerCaseFilterFactory.java
+++ b/solr/src/test/org/apache/solr/analysis/TestTurkishLowerCaseFilterFactory.java
@@ -22,7 +22,7 @@ import java.io.StringReader;
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the Turkish lowercase filter factory is working.
diff --git a/solr/src/test/org/apache/solr/handler/DocumentAnalysisRequestHandlerTest.java b/solr/src/test/org/apache/solr/handler/DocumentAnalysisRequestHandlerTest.java
index 0c7e080..099accc 100644
--- a/solr/src/test/org/apache/solr/handler/DocumentAnalysisRequestHandlerTest.java
+++ b/solr/src/test/org/apache/solr/handler/DocumentAnalysisRequestHandlerTest.java
@@ -158,7 +158,7 @@ public class DocumentAnalysisRequestHandlerTest extends AnalysisRequestHandlerTe
     NamedList<NamedList<Object>> whitetokResult = documentResult.get("whitetok");
     assertNotNull("an analysis for the 'whitetok' field should be returned", whitetokResult);
     queryResult = whitetokResult.get("query");
-    tokenList = (List<NamedList>) queryResult.get("org.apache.lucene.analysis.WhitespaceTokenizer");
+    tokenList = (List<NamedList>) queryResult.get("org.apache.lucene.analysis.core.WhitespaceTokenizer");
     assertNotNull("Expecting the 'WhitespaceTokenizer' to be applied on the query for the 'whitetok' field", tokenList);
     assertEquals("Query has only one token", 1, tokenList.size());
     assertToken(tokenList.get(0), new TokenInfo("JUMPING", null, "word", 0, 7, 1, null, false));
@@ -182,11 +182,11 @@ public class DocumentAnalysisRequestHandlerTest extends AnalysisRequestHandlerTe
     assertNotNull("Expecting the 'StandardFilter' to be applied on the query for the 'text' field", tokenList);
     assertEquals("Query has only one token", 1, tokenList.size());
     assertToken(tokenList.get(0), new TokenInfo("JUMPING", null, "<ALPHANUM>", 0, 7, 1, null, false));
-    tokenList = (List<NamedList>) queryResult.get("org.apache.lucene.analysis.LowerCaseFilter");
+    tokenList = (List<NamedList>) queryResult.get("org.apache.lucene.analysis.core.LowerCaseFilter");
     assertNotNull("Expecting the 'LowerCaseFilter' to be applied on the query for the 'text' field", tokenList);
     assertEquals("Query has only one token", 1, tokenList.size());
     assertToken(tokenList.get(0), new TokenInfo("jumping", null, "<ALPHANUM>", 0, 7, 1, null, false));
-    tokenList = (List<NamedList>) queryResult.get("org.apache.lucene.analysis.StopFilter");
+    tokenList = (List<NamedList>) queryResult.get("org.apache.lucene.analysis.core.StopFilter");
     assertNotNull("Expecting the 'StopFilter' to be applied on the query for the 'text' field", tokenList);
     assertEquals("Query has only one token", 1, tokenList.size());
     assertToken(tokenList.get(0), new TokenInfo("jumping", null, "<ALPHANUM>", 0, 7, 1, null, false));
@@ -215,7 +215,7 @@ public class DocumentAnalysisRequestHandlerTest extends AnalysisRequestHandlerTe
     assertToken(tokenList.get(3), new TokenInfo("Over", null, "<ALPHANUM>", 15, 19, 4, null, false));
     assertToken(tokenList.get(4), new TokenInfo("The", null, "<ALPHANUM>", 20, 23, 5, null, false));
     assertToken(tokenList.get(5), new TokenInfo("Dogs", null, "<ALPHANUM>", 24, 28, 6, null, false));
-    tokenList = valueResult.get("org.apache.lucene.analysis.LowerCaseFilter");
+    tokenList = valueResult.get("org.apache.lucene.analysis.core.LowerCaseFilter");
     assertNotNull("Expecting the 'LowerCaseFilter' to be applied on the index for the 'text' field", tokenList);
     assertEquals("Expecting 6 tokens", 6, tokenList.size());
     assertToken(tokenList.get(0), new TokenInfo("the", null, "<ALPHANUM>", 0, 3, 1, null, false));
@@ -224,7 +224,7 @@ public class DocumentAnalysisRequestHandlerTest extends AnalysisRequestHandlerTe
     assertToken(tokenList.get(3), new TokenInfo("over", null, "<ALPHANUM>", 15, 19, 4, null, false));
     assertToken(tokenList.get(4), new TokenInfo("the", null, "<ALPHANUM>", 20, 23, 5, null, false));
     assertToken(tokenList.get(5), new TokenInfo("dogs", null, "<ALPHANUM>", 24, 28, 6, null, false));
-    tokenList = valueResult.get("org.apache.lucene.analysis.StopFilter");
+    tokenList = valueResult.get("org.apache.lucene.analysis.core.StopFilter");
     assertNotNull("Expecting the 'StopFilter' to be applied on the index for the 'text' field", tokenList);
     assertEquals("Expecting 4 tokens after stop word removal", 4, tokenList.size());
     assertToken(tokenList.get(0), new TokenInfo("fox", null, "<ALPHANUM>", 4, 7, 1, null, false));
diff --git a/solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java b/solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java
index 99c0da4..cf57032 100644
--- a/solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java
+++ b/solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java
@@ -17,8 +17,8 @@
 
 package org.apache.solr.handler;
 
-import org.apache.lucene.analysis.KeywordTokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.solr.common.params.AnalysisParams;
 import org.apache.solr.common.params.CommonParams;
 import org.apache.solr.common.params.ModifiableSolrParams;
@@ -149,7 +149,7 @@ public class FieldAnalysisRequestHandlerTest extends AnalysisRequestHandlerTestB
     assertToken(tokenList.get(7), new TokenInfo("lazy", null, "<ALPHANUM>", 34, 38, 8, null, false));
     assertToken(tokenList.get(8), new TokenInfo("brown", null, "<ALPHANUM>", 39, 44, 9, null, true));
     assertToken(tokenList.get(9), new TokenInfo("dogs", null, "<ALPHANUM>", 45, 49, 10, null, false));
-    tokenList = indexPart.get("org.apache.lucene.analysis.LowerCaseFilter");
+    tokenList = indexPart.get("org.apache.lucene.analysis.core.LowerCaseFilter");
     assertNotNull("Expcting LowerCaseFilter analysis breakdown", tokenList);
     assertEquals(tokenList.size(), 10);
     assertToken(tokenList.get(0), new TokenInfo("the", null, "<ALPHANUM>", 0, 3, 1, null, false));
@@ -162,7 +162,7 @@ public class FieldAnalysisRequestHandlerTest extends AnalysisRequestHandlerTestB
     assertToken(tokenList.get(7), new TokenInfo("lazy", null, "<ALPHANUM>", 34, 38, 8, null, false));
     assertToken(tokenList.get(8), new TokenInfo("brown", null, "<ALPHANUM>", 39, 44, 9, null, true));
     assertToken(tokenList.get(9), new TokenInfo("dogs", null, "<ALPHANUM>", 45, 49, 10, null, false));
-    tokenList = indexPart.get("org.apache.lucene.analysis.StopFilter");
+    tokenList = indexPart.get("org.apache.lucene.analysis.core.StopFilter");
     assertNotNull("Expcting StopFilter analysis breakdown", tokenList);
     assertEquals(tokenList.size(), 8);
     assertToken(tokenList.get(0), new TokenInfo("quick", null, "<ALPHANUM>", 4, 9, 1, null, false));
@@ -198,12 +198,12 @@ public class FieldAnalysisRequestHandlerTest extends AnalysisRequestHandlerTestB
     assertEquals(2, tokenList.size());
     assertToken(tokenList.get(0), new TokenInfo("fox", null, "<ALPHANUM>", 0, 3, 1, null, false));
     assertToken(tokenList.get(1), new TokenInfo("brown", null, "<ALPHANUM>", 4, 9, 2, null, false));
-    tokenList = queryPart.get("org.apache.lucene.analysis.LowerCaseFilter");
+    tokenList = queryPart.get("org.apache.lucene.analysis.core.LowerCaseFilter");
     assertNotNull("Expcting LowerCaseFilter analysis breakdown", tokenList);
     assertEquals(2, tokenList.size());
     assertToken(tokenList.get(0), new TokenInfo("fox", null, "<ALPHANUM>", 0, 3, 1, null, false));
     assertToken(tokenList.get(1), new TokenInfo("brown", null, "<ALPHANUM>", 4, 9, 2, null, false));
-    tokenList = queryPart.get("org.apache.lucene.analysis.StopFilter");
+    tokenList = queryPart.get("org.apache.lucene.analysis.core.StopFilter");
     assertNotNull("Expcting StopFilter analysis breakdown", tokenList);
     assertEquals(2, tokenList.size());
     assertToken(tokenList.get(0), new TokenInfo("fox", null, "<ALPHANUM>", 0, 3, 1, null, false));
@@ -220,7 +220,7 @@ public class FieldAnalysisRequestHandlerTest extends AnalysisRequestHandlerTestB
     indexPart = nameTextType.get("index");
     assertNotNull("expecting an index token analysis for field type 'nametext'", indexPart);
 
-    tokenList = indexPart.get("org.apache.lucene.analysis.WhitespaceTokenizer");
+    tokenList = indexPart.get("org.apache.lucene.analysis.core.WhitespaceTokenizer");
     assertNotNull("Expcting WhitespaceTokenizer analysis breakdown", tokenList);
     assertEquals(10, tokenList.size());
     assertToken(tokenList.get(0), new TokenInfo("the", null, "word", 0, 3, 1, null, false));
@@ -314,7 +314,7 @@ public class FieldAnalysisRequestHandlerTest extends AnalysisRequestHandlerTestB
     assertEquals("  whtvr  ", indexPart.get("org.apache.lucene.analysis.charfilter.HTMLStripCharFilter"));
     assertEquals("  whatever  ", indexPart.get("org.apache.lucene.analysis.charfilter.MappingCharFilter"));
 
-    List<NamedList> tokenList = (List<NamedList>)indexPart.get("org.apache.lucene.analysis.WhitespaceTokenizer");
+    List<NamedList> tokenList = (List<NamedList>)indexPart.get("org.apache.lucene.analysis.core.WhitespaceTokenizer");
     assertNotNull("Expecting WhitespaceTokenizer analysis breakdown", tokenList);
     assertEquals(tokenList.size(), 1);
     assertToken(tokenList.get(0), new TokenInfo("whatever", null, "word", 12, 20, 1, null, false));
diff --git a/solr/src/test/org/apache/solr/highlight/HighlighterTest.java b/solr/src/test/org/apache/solr/highlight/HighlighterTest.java
index 612a2a3..7df465a 100755
--- a/solr/src/test/org/apache/solr/highlight/HighlighterTest.java
+++ b/solr/src/test/org/apache/solr/highlight/HighlighterTest.java
@@ -20,7 +20,7 @@ package org.apache.solr.highlight;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.solr.SolrTestCaseJ4;
 import org.apache.solr.core.SolrCore;
 import org.apache.solr.request.SolrQueryRequest;
diff --git a/solr/src/test/org/apache/solr/search/TestSort.java b/solr/src/test/org/apache/solr/search/TestSort.java
index aff9ba6..8d60d66 100755
--- a/solr/src/test/org/apache/solr/search/TestSort.java
+++ b/solr/src/test/org/apache/solr/search/TestSort.java
@@ -17,7 +17,7 @@
 
 package org.apache.solr.search;
 
-import org.apache.lucene.analysis.SimpleAnalyzer;
+import org.apache.lucene.analysis.core.SimpleAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexReader;
diff --git a/solr/src/test/org/apache/solr/spelling/IndexBasedSpellCheckerTest.java b/solr/src/test/org/apache/solr/spelling/IndexBasedSpellCheckerTest.java
index ba772ac..dfd5806 100644
--- a/solr/src/test/org/apache/solr/spelling/IndexBasedSpellCheckerTest.java
+++ b/solr/src/test/org/apache/solr/spelling/IndexBasedSpellCheckerTest.java
@@ -19,7 +19,7 @@ package org.apache.solr.spelling;
 import static org.junit.Assert.*;
 
 import org.apache.lucene.analysis.Token;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexReader;
diff --git a/solr/src/test/org/apache/solr/spelling/SimpleQueryConverter.java b/solr/src/test/org/apache/solr/spelling/SimpleQueryConverter.java
index 19bc9c6..3b13cad 100644
--- a/solr/src/test/org/apache/solr/spelling/SimpleQueryConverter.java
+++ b/solr/src/test/org/apache/solr/spelling/SimpleQueryConverter.java
@@ -17,8 +17,8 @@
 package org.apache.solr.spelling;
 
 import org.apache.lucene.analysis.Token;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.tokenattributes.FlagsAttribute;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
diff --git a/solr/src/test/org/apache/solr/spelling/SpellingQueryConverterTest.java b/solr/src/test/org/apache/solr/spelling/SpellingQueryConverterTest.java
index 0bb1d05..a6ba3fd 100644
--- a/solr/src/test/org/apache/solr/spelling/SpellingQueryConverterTest.java
+++ b/solr/src/test/org/apache/solr/spelling/SpellingQueryConverterTest.java
@@ -18,7 +18,7 @@
 package org.apache.solr.spelling;
 
 import org.apache.lucene.analysis.Token;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.solr.common.util.NamedList;
 import static org.junit.Assert.assertTrue;
 import static org.junit.Assert.assertEquals;
diff --git a/solr/src/test/test-files/solr/conf/schema-copyfield-test.xml b/solr/src/test/test-files/solr/conf/schema-copyfield-test.xml
index ce05b3f..2332e8c 100644
--- a/solr/src/test/test-files/solr/conf/schema-copyfield-test.xml
+++ b/solr/src/test/test-files/solr/conf/schema-copyfield-test.xml
@@ -119,7 +119,7 @@
 
 
     <fieldtype name="nametext" class="solr.TextField">
-      <analyzer class="org.apache.lucene.analysis.WhitespaceAnalyzer"/>
+      <analyzer class="org.apache.lucene.analysis.core.WhitespaceAnalyzer"/>
     </fieldtype>
 
     <fieldtype name="teststop" class="solr.TextField">
diff --git a/solr/src/test/test-files/solr/conf/schema-required-fields.xml b/solr/src/test/test-files/solr/conf/schema-required-fields.xml
index 3e7d33d..1535c8a 100644
--- a/solr/src/test/test-files/solr/conf/schema-required-fields.xml
+++ b/solr/src/test/test-files/solr/conf/schema-required-fields.xml
@@ -110,7 +110,7 @@
 
 
     <fieldtype name="nametext" class="solr.TextField">
-      <analyzer class="org.apache.lucene.analysis.WhitespaceAnalyzer"/>
+      <analyzer class="org.apache.lucene.analysis.core.WhitespaceAnalyzer"/>
     </fieldtype>
 
     <fieldtype name="teststop" class="solr.TextField">
diff --git a/solr/src/test/test-files/solr/conf/schema.xml b/solr/src/test/test-files/solr/conf/schema.xml
index 159d1af..ccf09be 100644
--- a/solr/src/test/test-files/solr/conf/schema.xml
+++ b/solr/src/test/test-files/solr/conf/schema.xml
@@ -140,7 +140,7 @@
 
 
     <fieldtype name="nametext" class="solr.TextField">
-      <analyzer class="org.apache.lucene.analysis.WhitespaceAnalyzer"/>
+      <analyzer class="org.apache.lucene.analysis.core.WhitespaceAnalyzer"/>
     </fieldtype>
 
     <fieldtype name="teststop" class="solr.TextField">
diff --git a/solr/src/test/test-files/solr/conf/schema12.xml b/solr/src/test/test-files/solr/conf/schema12.xml
index c2cc72e..bb82b1a 100755
--- a/solr/src/test/test-files/solr/conf/schema12.xml
+++ b/solr/src/test/test-files/solr/conf/schema12.xml
@@ -146,7 +146,7 @@
 
 
     <fieldtype name="nametext" class="solr.TextField">
-      <analyzer class="org.apache.lucene.analysis.WhitespaceAnalyzer"/>
+      <analyzer class="org.apache.lucene.analysis.core.WhitespaceAnalyzer"/>
     </fieldtype>
 
     <fieldtype name="teststop" class="solr.TextField">

