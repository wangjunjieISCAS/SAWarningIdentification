GitDiffStart: b8d51b6ce096c77d1129eda52da73aa963a7032a | Wed Nov 10 16:36:31 2010 +0000
diff --git a/lucene/CHANGES.txt b/lucene/CHANGES.txt
index d242a53..2cf134f 100644
--- a/lucene/CHANGES.txt
+++ b/lucene/CHANGES.txt
@@ -118,7 +118,7 @@ Changes in backwards compatibility policy
   for example).  If your code somehow depends on the old behavior, you will
   need to change it (e.g. using "\\" to escape '\' itself).  
   (Sunil Kamath, Terry Yang via Robert Muir)
- 
+
 Changes in Runtime Behavior
 
 * LUCENE-2650: The behavior of FSDirectory.open has changed. On 64-bit
@@ -289,6 +289,12 @@ New features
   (i.e. \* or "*")  Custom QueryParser sublcasses overriding getRangeQuery()
   will be passed null for any open endpoint. (Adriano Crestani, yonik)
 
+* LUCENE-2742: Add native per-field codec support. CodecProvider lets you now
+  register a codec for each field and which is in turn recorded in the segment
+  and field information. Codecs are maintained on a per-segment basis and be
+  resolved without knowing the actual codec used for writing the segment.
+  (Simon Willnauer)
+
 Optimizations
 
 * LUCENE-2410: ~20% speedup on exact (slop=0) PhraseQuery matching.
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CreateIndexTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CreateIndexTask.java
index 56d5a4c..503f4bc 100644
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CreateIndexTask.java
+++ b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CreateIndexTask.java
@@ -134,6 +134,7 @@ public class CreateIndexTask extends PerfTask {
     final String defaultCodec = config.get("default.codec", null);
     if (defaultCodec != null) {
       CodecProvider.setDefaultCodec(defaultCodec);
+      CodecProvider.getDefault().setDefaultFieldCodec(defaultCodec);
     }
 
     final String mergePolicy = config.get("merge.policy",
diff --git a/lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec.java b/lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec.java
index fab18ed..97a02b0 100644
--- a/lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec.java
+++ b/lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec.java
@@ -21,7 +21,6 @@ import java.io.IOException;
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
 import org.apache.lucene.document.Field.Index;
 import org.apache.lucene.document.Field.Store;
 import org.apache.lucene.document.Field.TermVector;
@@ -32,7 +31,6 @@ import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.LogMergePolicy;
 import org.apache.lucene.index.MultiFields;
-import org.apache.lucene.index.SegmentWriteState;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.index.TermsEnum.SeekStatus;
@@ -54,15 +52,14 @@ public class TestAppendingCodec extends LuceneTestCase {
     Codec appending = new AppendingCodec();
     SegmentInfosWriter infosWriter = new AppendingSegmentInfosWriter();
     SegmentInfosReader infosReader = new AppendingSegmentInfosReader();
-    
-    @Override
-    public Codec lookup(String name) {
-      return appending;
+    public AppendingCodecProvider() {
+      setDefaultFieldCodec(appending.name);
     }
     @Override
-    public Codec getWriter(SegmentWriteState state) {
+    public Codec lookup(String name) {
       return appending;
     }
+   
     @Override
     public SegmentInfosReader getSegmentInfosReader() {
       return infosReader;
diff --git a/lucene/src/java/org/apache/lucene/index/CheckIndex.java b/lucene/src/java/org/apache/lucene/index/CheckIndex.java
index 409649b..4d417de 100644
--- a/lucene/src/java/org/apache/lucene/index/CheckIndex.java
+++ b/lucene/src/java/org/apache/lucene/index/CheckIndex.java
@@ -129,8 +129,8 @@ public class CheckIndex {
       /** Name of the segment. */
       public String name;
 
-      /** Name of codec used to read this segment. */
-      public String codec;
+      /** CodecInfo used to read this segment. */
+      public SegmentCodecs codec;
 
       /** Document count (does not take deletions into account). */
       public int docCount;
@@ -408,7 +408,7 @@ public class CheckIndex {
       SegmentReader reader = null;
 
       try {
-        final String codec = info.getCodec().name;
+        final SegmentCodecs codec = info.getCodecInfo();
         msg("    codec=" + codec);
         segInfoStat.codec = codec;
         msg("    compound=" + info.getUseCompoundFile());
@@ -602,7 +602,7 @@ public class CheckIndex {
         }
         
         final TermsEnum terms = fieldsEnum.terms();
-
+        assert terms != null;
         boolean hasOrd = true;
         final long termCountStart = status.termCount;
 
diff --git a/lucene/src/java/org/apache/lucene/index/DocFieldProcessor.java b/lucene/src/java/org/apache/lucene/index/DocFieldProcessor.java
index c2a586a..980479b 100644
--- a/lucene/src/java/org/apache/lucene/index/DocFieldProcessor.java
+++ b/lucene/src/java/org/apache/lucene/index/DocFieldProcessor.java
@@ -34,11 +34,12 @@ import java.util.HashMap;
 final class DocFieldProcessor extends DocConsumer {
 
   final DocumentsWriter docWriter;
-  final FieldInfos fieldInfos = new FieldInfos();
+  final FieldInfos fieldInfos;
   final DocFieldConsumer consumer;
   final StoredFieldsWriter fieldsWriter;
 
   public DocFieldProcessor(DocumentsWriter docWriter, DocFieldConsumer consumer) {
+    this.fieldInfos = new FieldInfos();
     this.docWriter = docWriter;
     this.consumer = consumer;
     consumer.setFieldInfos(fieldInfos);
diff --git a/lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread.java b/lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread.java
index 8e92236..298b3bd 100644
--- a/lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread.java
+++ b/lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread.java
@@ -196,7 +196,6 @@ final class DocFieldProcessorPerThread extends DocConsumerPerThread {
         FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),
                                       field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),
                                       field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());
-
         fp = new DocFieldProcessorPerField(this, fi);
         fp.next = fieldHash[hashPos];
         fieldHash[hashPos] = fp;
diff --git a/lucene/src/java/org/apache/lucene/index/DocumentsWriter.java b/lucene/src/java/org/apache/lucene/index/DocumentsWriter.java
index 27784c2..31fdaa8 100644
--- a/lucene/src/java/org/apache/lucene/index/DocumentsWriter.java
+++ b/lucene/src/java/org/apache/lucene/index/DocumentsWriter.java
@@ -28,11 +28,9 @@ import java.util.HashSet;
 import java.util.List;
 import java.util.Map.Entry;
 import java.util.concurrent.atomic.AtomicLong;
-import java.util.concurrent.locks.ReentrantLock;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.document.Document;
-import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.Scorer;
@@ -604,14 +602,14 @@ final class DocumentsWriter {
 
   synchronized private void initFlushState(boolean onlyDocStore) {
     initSegmentName(onlyDocStore);
+    final SegmentCodecs info = SegmentCodecs.build(docFieldProcessor.fieldInfos, writer.codecs);
     flushState = new SegmentWriteState(infoStream, directory, segment, docFieldProcessor.fieldInfos,
-                                       docStoreSegment, numDocsInRAM, numDocsInStore, writer.getConfig().getTermIndexInterval(),
-                                       writer.codecs);
+                                       docStoreSegment, numDocsInRAM, numDocsInStore, writer.getConfig().getTermIndexInterval(), info);
   }
 
-  /** Returns the codec used to flush the last segment */
-  Codec getCodec() {
-    return flushState.codec;
+  /** Returns the SegmentCodecs used to flush the last segment */
+  SegmentCodecs getSegmentCodecs() {
+    return flushState.segmentCodecs;
   }
   
   /** Flush all pending docs to a new segment */
@@ -653,7 +651,7 @@ final class DocumentsWriter {
       if (infoStream != null) {
         SegmentInfo si = new SegmentInfo(flushState.segmentName,
             flushState.numDocs, directory, false, -1, flushState.segmentName,
-            false, hasProx(), flushState.codec);
+            false, hasProx(), flushState.segmentCodecs);
         final long newSegmentSize = si.sizeInBytes();
         String message = "  ramUsed=" + nf.format(startNumBytesUsed/1024./1024.) + " MB" +
           " newFlushedSize=" + newSegmentSize +
diff --git a/lucene/src/java/org/apache/lucene/index/FieldInfo.java b/lucene/src/java/org/apache/lucene/index/FieldInfo.java
index e15b568..9526a7b 100644
--- a/lucene/src/java/org/apache/lucene/index/FieldInfo.java
+++ b/lucene/src/java/org/apache/lucene/index/FieldInfo.java
@@ -32,6 +32,7 @@ public final class FieldInfo {
   public boolean omitTermFreqAndPositions;
 
   public boolean storePayloads; // whether this field stores payloads together with term positions
+  int codecId = 0; // set inside SegmentCodecs#build() during segment flush - this is used to identify the codec used to write this field 
 
   FieldInfo(String na, boolean tk, int nu, boolean storeTermVector, 
             boolean storePositionWithTermVector,  boolean storeOffsetWithTermVector, 
diff --git a/lucene/src/java/org/apache/lucene/index/FieldInfos.java b/lucene/src/java/org/apache/lucene/index/FieldInfos.java
index d4eb996..07111ef 100644
--- a/lucene/src/java/org/apache/lucene/index/FieldInfos.java
+++ b/lucene/src/java/org/apache/lucene/index/FieldInfos.java
@@ -38,9 +38,10 @@ public final class FieldInfos {
 
   // First used in 2.9; prior to 2.9 there was no format header
   public static final int FORMAT_START = -2;
+  public static final int FORMAT_PER_FIELD_CODEC = -3;
 
   // whenever you add a new format, make it 1 smaller (negative version logic)!
-  static final int FORMAT_CURRENT = FORMAT_START;
+  static final int FORMAT_CURRENT = FORMAT_PER_FIELD_CODEC;
   
   static final int FORMAT_MINIMUM = FORMAT_START;
   
@@ -56,7 +57,8 @@ public final class FieldInfos {
   private final HashMap<String,FieldInfo> byName = new HashMap<String,FieldInfo>();
   private int format;
 
-  public FieldInfos() { }
+  public FieldInfos() {
+  }
 
   /**
    * Construct a FieldInfos object using the directory and the name of the file
@@ -301,8 +303,8 @@ public final class FieldInfos {
       if (fi.omitNorms) bits |= OMIT_NORMS;
       if (fi.storePayloads) bits |= STORE_PAYLOADS;
       if (fi.omitTermFreqAndPositions) bits |= OMIT_TERM_FREQ_AND_POSITIONS;
-      
       output.writeString(fi.name);
+      output.writeInt(fi.codecId);
       output.writeByte(bits);
     }
   }
@@ -321,6 +323,8 @@ public final class FieldInfos {
 
     for (int i = 0; i < size; i++) {
       String name = StringHelper.intern(input.readString());
+      // if this is a previous format codec 0 will be preflex!
+      final int codecId = format <= FORMAT_PER_FIELD_CODEC? input.readInt():0;
       byte bits = input.readByte();
       boolean isIndexed = (bits & IS_INDEXED) != 0;
       boolean storeTermVector = (bits & STORE_TERMVECTOR) != 0;
@@ -329,8 +333,8 @@ public final class FieldInfos {
       boolean omitNorms = (bits & OMIT_NORMS) != 0;
       boolean storePayloads = (bits & STORE_PAYLOADS) != 0;
       boolean omitTermFreqAndPositions = (bits & OMIT_TERM_FREQ_AND_POSITIONS) != 0;
-      
-      addInternal(name, isIndexed, storeTermVector, storePositionsWithTermVector, storeOffsetWithTermVector, omitNorms, storePayloads, omitTermFreqAndPositions);
+      final FieldInfo addInternal = addInternal(name, isIndexed, storeTermVector, storePositionsWithTermVector, storeOffsetWithTermVector, omitNorms, storePayloads, omitTermFreqAndPositions);
+      addInternal.codecId = codecId;
     }
 
     if (input.getFilePointer() != input.length()) {
diff --git a/lucene/src/java/org/apache/lucene/index/FreqProxTermsWriter.java b/lucene/src/java/org/apache/lucene/index/FreqProxTermsWriter.java
index b7cd1d8..5f2d288 100644
--- a/lucene/src/java/org/apache/lucene/index/FreqProxTermsWriter.java
+++ b/lucene/src/java/org/apache/lucene/index/FreqProxTermsWriter.java
@@ -77,8 +77,7 @@ final class FreqProxTermsWriter extends TermsHashConsumer {
     // Sort by field name
     CollectionUtil.quickSort(allFields);
 
-    // TODO: allow Lucene user to customize this codec:
-    final FieldsConsumer consumer = state.codec.fieldsConsumer(state);
+    final FieldsConsumer consumer = state.segmentCodecs.codec().fieldsConsumer(state);
 
     /*
     Current writer chain:
diff --git a/lucene/src/java/org/apache/lucene/index/IndexWriter.java b/lucene/src/java/org/apache/lucene/index/IndexWriter.java
index 75ba136..b209f1c 100644
--- a/lucene/src/java/org/apache/lucene/index/IndexWriter.java
+++ b/lucene/src/java/org/apache/lucene/index/IndexWriter.java
@@ -623,7 +623,7 @@ public class IndexWriter implements Closeable {
         // TODO: we may want to avoid doing this while
         // synchronized
         // Returns a ref, which we xfer to readerMap:
-        sr = SegmentReader.get(false, info.dir, info, readBufferSize, doOpenStores, termsIndexDivisor, codecs);
+        sr = SegmentReader.get(false, info.dir, info, readBufferSize, doOpenStores, termsIndexDivisor);
 
         if (info.dir == directory) {
           // Only pool if reader is not external
@@ -2997,7 +2997,7 @@ public class IndexWriter implements Closeable {
       SegmentInfo info = null;
       synchronized(this) {
         info = new SegmentInfo(mergedName, docCount, directory, false, -1,
-            null, false, merger.hasProx(), merger.getCodec());
+            null, false, merger.hasProx(), merger.getSegmentCodecs());
         setDiagnostics(info, "addIndexes(IndexReader...)");
         segmentInfos.add(info);
         checkpoint();
@@ -3375,10 +3375,10 @@ public class IndexWriter implements Closeable {
                                      directory, false, docStoreOffset,
                                      docStoreSegment, docStoreIsCompoundFile,
                                      docWriter.hasProx(),    
-                                     docWriter.getCodec());
+                                     docWriter.getSegmentCodecs());
 
         if (infoStream != null) {
-          message("flush codec=" + docWriter.getCodec().name);
+          message("flush codec=" + docWriter.getSegmentCodecs());
         }
         setDiagnostics(newSegment, "flush");
       }
@@ -4068,10 +4068,10 @@ public class IndexWriter implements Closeable {
       mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);
 
       // Record which codec was used to write the segment
-      merge.info.setCodec(merger.getCodec());
+      merge.info.setSegmentCodecs(merger.getSegmentCodecs());
 
       if (infoStream != null) {
-        message("merge codec=" + merger.getCodec().name);
+        message("merge segmentCodecs=" + merger.getSegmentCodecs());
       }
       
       assert mergedDocCount == totDocCount;
diff --git a/lucene/src/java/org/apache/lucene/index/PerFieldCodecWrapper.java b/lucene/src/java/org/apache/lucene/index/PerFieldCodecWrapper.java
new file mode 100644
index 0000000..b052b26
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/index/PerFieldCodecWrapper.java
@@ -0,0 +1,209 @@
+package org.apache.lucene.index;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.Set;
+import java.util.TreeSet;
+
+import org.apache.lucene.index.codecs.Codec;
+import org.apache.lucene.index.codecs.FieldsConsumer;
+import org.apache.lucene.index.codecs.FieldsProducer;
+import org.apache.lucene.index.codecs.TermsConsumer;
+import org.apache.lucene.store.Directory;
+
+/**
+ * Enables native per field codec support. This class selects the codec used to
+ * write a field depending on the provided {@link SegmentCodecs}. For each field
+ * seen it resolves the codec based on the {@link FieldInfo#codecId} which is
+ * only valid during a segment merge. See {@link SegmentCodecs} javadoc for
+ * details.
+ * 
+ * @lucene.internal
+ */
+final class PerFieldCodecWrapper extends Codec {
+  private final SegmentCodecs segmentCodecs;
+
+  PerFieldCodecWrapper(SegmentCodecs segmentCodecs) {
+    name = "PerField";
+    this.segmentCodecs = segmentCodecs;
+  }
+
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state)
+      throws IOException {
+    return new FieldsWriter(state);
+  }
+
+  private class FieldsWriter extends FieldsConsumer {
+    private final ArrayList<FieldsConsumer> consumers = new ArrayList<FieldsConsumer>();
+
+    public FieldsWriter(SegmentWriteState state) throws IOException {
+      assert segmentCodecs == state.segmentCodecs;
+      final Codec[] codecs = segmentCodecs.codecs;
+      for (int i = 0; i < codecs.length; i++) {
+        state.currentCodecId = i; // actual codec should use that to create its
+                                  // files
+        consumers.add(codecs[i].fieldsConsumer(state));
+      }
+    }
+
+    @Override
+    public TermsConsumer addField(FieldInfo field) throws IOException {
+      final FieldsConsumer fields = consumers.get(field.codecId);
+      return fields.addField(field);
+    }
+
+    @Override
+    public void close() throws IOException {
+      Iterator<FieldsConsumer> it = consumers.iterator();
+      IOException err = null;
+      while (it.hasNext()) {
+        try {
+          it.next().close();
+        } catch (IOException ioe) {
+          // keep first IOException we hit but keep
+          // closing the rest
+          if (err == null) {
+            err = ioe;
+          }
+        }
+      }
+      if (err != null) {
+        throw err;
+      }
+    }
+  }
+
+  private class FieldsReader extends FieldsProducer {
+
+    private final Set<String> fields = new TreeSet<String>();
+    private final Map<String, FieldsProducer> codecs = new HashMap<String, FieldsProducer>();
+
+    public FieldsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo si,
+        int readBufferSize, int indexDivisor) throws IOException {
+
+      final int fieldCount = fieldInfos.size();
+      final Map<Codec, FieldsProducer> producers = new HashMap<Codec, FieldsProducer>();
+      for (int i = 0; i < fieldCount; i++) {
+        FieldInfo fi = fieldInfos.fieldInfo(i);
+        if (fi.isIndexed) { // TODO this does not work for non-indexed fields
+          fields.add(fi.name);
+          Codec codec = segmentCodecs.codecs[fi.codecId];
+          if (!producers.containsKey(codec)) {
+            producers.put(codec, codec.fieldsProducer(new SegmentReadState(dir,
+                si, fieldInfos, readBufferSize, indexDivisor)));
+          }
+          codecs.put(fi.name, producers.get(codec));
+        }
+      }
+    }
+
+    private final class FieldsIterator extends FieldsEnum {
+      private final Iterator<String> it;
+      private String current;
+
+      public FieldsIterator() {
+        it = fields.iterator();
+      }
+
+      @Override
+      public String next() {
+        if (it.hasNext()) {
+          current = it.next();
+        } else {
+          current = null;
+        }
+
+        return current;
+      }
+
+      @Override
+      public TermsEnum terms() throws IOException {
+        Terms terms = codecs.get(current).terms(current);
+        if (terms != null) {
+          return terms.iterator();
+        } else {
+          return TermsEnum.EMPTY;
+        }
+      }
+    }
+
+    @Override
+    public FieldsEnum iterator() throws IOException {
+      return new FieldsIterator();
+    }
+
+    @Override
+    public Terms terms(String field) throws IOException {
+      FieldsProducer fields = codecs.get(field);
+      return fields == null ? null : fields.terms(field);
+    }
+
+    @Override
+    public void close() throws IOException {
+      Iterator<FieldsProducer> it = codecs.values().iterator();
+      IOException err = null;
+      while (it.hasNext()) {
+        try {
+          it.next().close();
+        } catch (IOException ioe) {
+          // keep first IOException we hit but keep
+          // closing the rest
+          if (err == null) {
+            err = ioe;
+          }
+        }
+      }
+      if (err != null) {
+        throw err;
+      }
+    }
+
+    @Override
+    public void loadTermsIndex(int indexDivisor) throws IOException {
+      Iterator<FieldsProducer> it = codecs.values().iterator();
+      while (it.hasNext()) {
+        it.next().loadTermsIndex(indexDivisor);
+      }
+    }
+  }
+
+  public FieldsProducer fieldsProducer(SegmentReadState state)
+      throws IOException {
+    return new FieldsReader(state.dir, state.fieldInfos, state.segmentInfo,
+        state.readBufferSize, state.termsIndexDivisor);
+  }
+
+  @Override
+  public void files(Directory dir, SegmentInfo info, Set<String> files)
+      throws IOException {
+    segmentCodecs.files(dir, info, files);
+  }
+
+  @Override
+  public void getExtensions(Set<String> extensions) {
+    for (Codec codec : segmentCodecs.codecs) {
+      codec.getExtensions(extensions);
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/index/SegmentCodecs.java b/lucene/src/java/org/apache/lucene/index/SegmentCodecs.java
new file mode 100644
index 0000000..8ad1efb
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/index/SegmentCodecs.java
@@ -0,0 +1,137 @@
+package org.apache.lucene.index;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashSet;
+import java.util.IdentityHashMap;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.lucene.index.codecs.Codec;
+import org.apache.lucene.index.codecs.CodecProvider;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+
+/**
+ * SegmentCodecs maintains an ordered list of distinct codecs used within a
+ * segment. Within a segment on codec is used to write multiple fields while
+ * each field could be written by a different codec. To enable codecs per field
+ * within a single segment we need to record the distinct codecs and map them to
+ * each field present in the segment. SegmentCodecs is created together with
+ * {@link SegmentWriteState} for each flush and is maintained in the
+ * corresponding {@link SegmentInfo} until it is committed.
+ * <p>
+ * {@link SegmentCodecs#build(FieldInfos, CodecProvider)} should be used to
+ * create a {@link SegmentCodecs} instance during {@link IndexWriter} sessions
+ * which creates the ordering of distinct codecs and assigns the
+ * {@link FieldInfo#codecId} or in other words, the ord of the codec maintained
+ * inside {@link SegmentCodecs}, to the {@link FieldInfo}. This ord is valid
+ * only until the current segment is flushed and {@link FieldInfos} for that
+ * segment are written including the ord for each field. This ord is later used
+ * to get the right codec when the segment is opened in a reader. The
+ * {@link Codec} returned from {@link SegmentCodecs#codec()} in turn uses
+ * {@link SegmentCodecs} internal structure to select and initialize the right
+ * codec for a fields when it is written.
+ * <p>
+ * Once a flush succeeded the {@link SegmentCodecs} is maintained inside the
+ * {@link SegmentInfo} for the flushed segment it was created for.
+ * {@link SegmentInfo} writes the name of each codec in {@link SegmentCodecs}
+ * for each segment and maintains the order. Later if a segment is opened by a
+ * reader this mapping is deserialized and used to create the codec per field.
+ * 
+ * 
+ * @lucene.internal
+ */
+final class SegmentCodecs implements Cloneable {
+  /**
+   * internal structure to map codecs to fields - don't modify this from outside
+   * of this class!
+   */
+  Codec[] codecs;
+  final CodecProvider provider;
+  private final Codec codec = new PerFieldCodecWrapper(this);
+
+  SegmentCodecs(CodecProvider provider, Codec... codecs) {
+    this.provider = provider;
+    this.codecs = codecs;
+  }
+
+  static SegmentCodecs build(FieldInfos infos, CodecProvider provider) {
+    final int size = infos.size();
+    final Map<Codec, Integer> codecRegistry = new IdentityHashMap<Codec, Integer>();
+    final ArrayList<Codec> codecs = new ArrayList<Codec>();
+
+    for (int i = 0; i < size; i++) {
+      final FieldInfo info = infos.fieldInfo(i);
+      if (info.isIndexed) {
+        final Codec fieldCodec = provider.lookup(provider
+            .getFieldCodec(info.name));
+        Integer ord = codecRegistry.get(fieldCodec);
+        if (ord == null) {
+          ord = Integer.valueOf(codecs.size());
+          codecRegistry.put(fieldCodec, ord);
+          codecs.add(fieldCodec);
+        }
+        info.codecId = ord.intValue();
+      }
+    }
+    return new SegmentCodecs(provider, codecs.toArray(Codec.EMPTY));
+
+  }
+
+  Codec codec() {
+    return codec;
+  }
+
+  void write(IndexOutput out) throws IOException {
+    out.writeVInt(codecs.length);
+    for (Codec codec : codecs) {
+      out.writeString(codec.name);
+    }
+  }
+
+  void read(IndexInput in) throws IOException {
+    final int size = in.readVInt();
+    final ArrayList<Codec> list = new ArrayList<Codec>();
+    for (int i = 0; i < size; i++) {
+      final String codecName = in.readString();
+      final Codec lookup = provider.lookup(codecName);
+      list.add(i, lookup);
+    }
+    codecs = list.toArray(Codec.EMPTY);
+  }
+
+  void files(Directory dir, SegmentInfo info, Set<String> files)
+      throws IOException {
+    final Set<Codec> seen = new HashSet<Codec>();
+    final Codec[] codecArray = codecs;
+    for (Codec codec : codecArray) {
+      if (!seen.contains(codec)) {
+        seen.add(codec);
+        codec.files(dir, info, files);
+      }
+    }
+  }
+
+  @Override
+  public String toString() {
+    return "CodecInfo [codecs=" + codecs + ", provider=" + provider + "]";
+  }
+}
\ No newline at end of file
diff --git a/lucene/src/java/org/apache/lucene/index/SegmentInfo.java b/lucene/src/java/org/apache/lucene/index/SegmentInfo.java
index 4686481..d1eeea5 100644
--- a/lucene/src/java/org/apache/lucene/index/SegmentInfo.java
+++ b/lucene/src/java/org/apache/lucene/index/SegmentInfo.java
@@ -80,13 +80,12 @@ public final class SegmentInfo {
 
   private boolean hasProx;                        // True if this segment has any fields with omitTermFreqAndPositions==false
   
-  private Codec codec;
-
+  private SegmentCodecs segmentCodecs;
 
   private Map<String,String> diagnostics;
 
   public SegmentInfo(String name, int docCount, Directory dir, boolean isCompoundFile, int docStoreOffset, 
-                     String docStoreSegment, boolean docStoreIsCompoundFile, boolean hasProx, Codec codec) { 
+                     String docStoreSegment, boolean docStoreIsCompoundFile, boolean hasProx, SegmentCodecs codecInfo) { 
     this.name = name;
     this.docCount = docCount;
     this.dir = dir;
@@ -96,7 +95,7 @@ public final class SegmentInfo {
     this.docStoreSegment = docStoreSegment;
     this.docStoreIsCompoundFile = docStoreIsCompoundFile;
     this.hasProx = hasProx;
-    this.codec = codec;
+    this.segmentCodecs = codecInfo;
     delCount = 0;
     assert docStoreOffset == -1 || docStoreSegment != null: "dso=" + docStoreOffset + " dss=" + docStoreSegment + " docCount=" + docCount;
   }
@@ -120,7 +119,7 @@ public final class SegmentInfo {
     }
     isCompoundFile = src.isCompoundFile;
     delCount = src.delCount;
-    codec = src.codec;
+    segmentCodecs = src.segmentCodecs;
   }
 
   void setDiagnostics(Map<String, String> diagnostics) {
@@ -145,7 +144,6 @@ public final class SegmentInfo {
     this.dir = dir;
     name = input.readString();
     docCount = input.readInt();
-    final String codecName;
     delGen = input.readLong();
     docStoreOffset = input.readInt();
     if (docStoreOffset != -1) {
@@ -177,14 +175,15 @@ public final class SegmentInfo {
     hasProx = input.readByte() == YES;
     
     // System.out.println(Thread.currentThread().getName() + ": si.read hasProx=" + hasProx + " seg=" + name);
-    
-    if (format <= DefaultSegmentInfosWriter.FORMAT_4_0)
-      codecName = input.readString();
-    else
-      codecName = "PreFlex";
-    
+    segmentCodecs = new SegmentCodecs(codecs);
+    if (format <= DefaultSegmentInfosWriter.FORMAT_4_0) {
+      segmentCodecs.read(input);
+    } else {
+      // codec ID on FieldInfo is 0 so it will simply use the first codec available
+      // TODO what todo if preflex is not available in the provider? register it or fail?
+      segmentCodecs.codecs = new Codec[] { codecs.lookup("PreFlex")};
+    }
     diagnostics = input.readStringStringMap();
-    codec = codecs.lookup(codecName);
   }
   
   /** Returns total size in bytes of all of files used by
@@ -230,7 +229,7 @@ public final class SegmentInfo {
 
   @Override
   public Object clone() {
-    SegmentInfo si = new SegmentInfo(name, docCount, dir, isCompoundFile, docStoreOffset, docStoreSegment, docStoreIsCompoundFile, hasProx, codec);
+    SegmentInfo si = new SegmentInfo(name, docCount, dir, isCompoundFile, docStoreOffset, docStoreSegment, docStoreIsCompoundFile, hasProx, segmentCodecs);
     si.isCompoundFile = isCompoundFile;
     si.delGen = delGen;
     si.delCount = delCount;
@@ -242,7 +241,6 @@ public final class SegmentInfo {
     si.docStoreOffset = docStoreOffset;
     si.docStoreSegment = docStoreSegment;
     si.docStoreIsCompoundFile = docStoreIsCompoundFile;
-    si.codec = codec;
     return si;
   }
 
@@ -400,7 +398,7 @@ public final class SegmentInfo {
     output.writeByte((byte) (isCompoundFile ? YES : NO));
     output.writeInt(delCount);
     output.writeByte((byte) (hasProx ? 1:0));
-    output.writeString(codec.name);
+    segmentCodecs.write(output);
     output.writeStringStringMap(diagnostics);
   }
 
@@ -414,16 +412,16 @@ public final class SegmentInfo {
   }
 
   /** Can only be called once. */
-  public void setCodec(Codec codec) {
-    assert this.codec == null;
-    if (codec == null) {
-      throw new IllegalArgumentException("codec must be non-null");
+  public void setSegmentCodecs(SegmentCodecs segmentCodecs) {
+    assert this.segmentCodecs == null;
+    if (segmentCodecs == null) {
+      throw new IllegalArgumentException("segmentCodecs must be non-null");
     }
-    this.codec = codec;
+    this.segmentCodecs = segmentCodecs;
   }
 
-  Codec getCodec() {
-    return codec;
+  SegmentCodecs getCodecInfo() {
+    return segmentCodecs;
   }
 
   private void addIfExists(Set<String> files, String fileName) throws IOException {
@@ -454,7 +452,7 @@ public final class SegmentInfo {
       for(String ext : IndexFileNames.NON_STORE_INDEX_EXTENSIONS) {
         addIfExists(fileSet, IndexFileNames.segmentFileName(name, "", ext));
       }
-      codec.files(dir, this, fileSet);
+      segmentCodecs.files(dir, this, fileSet);
     }
 
     if (docStoreOffset != -1) {
diff --git a/lucene/src/java/org/apache/lucene/index/SegmentInfos.java b/lucene/src/java/org/apache/lucene/index/SegmentInfos.java
index 900e824..f20074f 100644
--- a/lucene/src/java/org/apache/lucene/index/SegmentInfos.java
+++ b/lucene/src/java/org/apache/lucene/index/SegmentInfos.java
@@ -308,7 +308,7 @@ public final class SegmentInfos extends Vector<SegmentInfo> {
     SegmentInfos sis = (SegmentInfos) super.clone();
     for(int i=0;i<sis.size();i++) {
       final SegmentInfo info = sis.info(i);
-      assert info.getCodec() != null;
+      assert info.getCodecInfo() != null;
       sis.set(i, (SegmentInfo) info.clone());
     }
     sis.userData = new HashMap<String,String>(userData);
diff --git a/lucene/src/java/org/apache/lucene/index/SegmentMerger.java b/lucene/src/java/org/apache/lucene/index/SegmentMerger.java
index 70ed9f0..6dcaa28 100644
--- a/lucene/src/java/org/apache/lucene/index/SegmentMerger.java
+++ b/lucene/src/java/org/apache/lucene/index/SegmentMerger.java
@@ -184,7 +184,7 @@ final class SegmentMerger {
         fileSet.add(IndexFileNames.segmentFileName(segment, "", ext));
     }
 
-    codec.files(directory, info, fileSet);
+    segmentWriteState.segmentCodecs.files(directory, info, fileSet);
     
     // Fieldable norm files
     int numFIs = fieldInfos.size();
@@ -278,7 +278,7 @@ final class SegmentMerger {
       final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);
       fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();
     } else {
-      fieldInfos = new FieldInfos();		  // merge field names
+      fieldInfos = new FieldInfos();// merge field names
     }
 
     for (IndexReader reader : readers) {
@@ -304,6 +304,7 @@ final class SegmentMerger {
         fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);
       }
     }
+    final SegmentCodecs codecInfo = SegmentCodecs.build(fieldInfos, this.codecs);
     fieldInfos.write(directory, segment + ".fnm");
 
     int docCount = 0;
@@ -357,8 +358,8 @@ final class SegmentMerger {
       }
     }
 
-    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, null, docCount, 0, termIndexInterval, codecs);
-
+    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, null, docCount, 0, termIndexInterval, codecInfo);
+    
     return docCount;
   }
 
@@ -554,15 +555,15 @@ final class SegmentMerger {
     }
   }
 
-  Codec getCodec() {
-    return codec;
+  SegmentCodecs getSegmentCodecs() {
+    assert segmentWriteState != null;
+    return segmentWriteState.segmentCodecs;
   }
 
   private final void mergeTerms() throws CorruptIndexException, IOException {
 
     // Let CodecProvider decide which codec will be used to write
     // the new segment:
-    codec = codecs.getWriter(segmentWriteState);
     
     int docBase = 0;
 
@@ -644,7 +645,7 @@ final class SegmentMerger {
       }
     }
     starts[mergeState.readerCount] = inputDocBase;
-
+    codec = segmentWriteState.segmentCodecs.codec();
     final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);
 
     // NOTE: this is silly, yet, necessary -- we create a
diff --git a/lucene/src/java/org/apache/lucene/index/SegmentReader.java b/lucene/src/java/org/apache/lucene/index/SegmentReader.java
index 3b71ab6..6adf955 100644
--- a/lucene/src/java/org/apache/lucene/index/SegmentReader.java
+++ b/lucene/src/java/org/apache/lucene/index/SegmentReader.java
@@ -39,7 +39,6 @@ import org.apache.lucene.store.IndexOutput;
 import org.apache.lucene.util.BitVector;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.CloseableThreadLocal;
-import org.apache.lucene.index.codecs.CodecProvider;
 import org.apache.lucene.index.codecs.FieldsProducer;
 import org.apache.lucene.search.FieldCache; // not great (circular); used only to purge FieldCache entry on close
 import org.apache.lucene.util.BytesRef;
@@ -90,7 +89,6 @@ public class SegmentReader extends IndexReader implements Cloneable {
     final FieldInfos fieldInfos;
 
     final FieldsProducer fields;
-    final CodecProvider codecs;
     
     final Directory dir;
     final Directory cfsDir;
@@ -104,17 +102,14 @@ public class SegmentReader extends IndexReader implements Cloneable {
     CompoundFileReader cfsReader;
     CompoundFileReader storeCFSReader;
 
-    CoreReaders(SegmentReader origInstance, Directory dir, SegmentInfo si, int readBufferSize, int termsIndexDivisor, CodecProvider codecs) throws IOException {
+    CoreReaders(SegmentReader origInstance, Directory dir, SegmentInfo si, int readBufferSize, int termsIndexDivisor) throws IOException {
 
       if (termsIndexDivisor == 0) {
         throw new IllegalArgumentException("indexDivisor must be < 0 (don't load terms index) or greater than 0 (got 0)");
       }
 
       segment = si.name;
-      if (codecs == null) {
-        codecs = CodecProvider.getDefault();
-      }
-      this.codecs = codecs;      
+      final SegmentCodecs codecInfo = si.getCodecInfo();
       this.readBufferSize = readBufferSize;
       this.dir = dir;
 
@@ -129,11 +124,11 @@ public class SegmentReader extends IndexReader implements Cloneable {
         cfsDir = dir0;
 
         fieldInfos = new FieldInfos(cfsDir, IndexFileNames.segmentFileName(segment, "", IndexFileNames.FIELD_INFOS_EXTENSION));
-
+        
         this.termsIndexDivisor = termsIndexDivisor;
-
+        
         // Ask codec for its Fields
-        fields = si.getCodec().fieldsProducer(new SegmentReadState(cfsDir, si, fieldInfos, readBufferSize, termsIndexDivisor));
+        fields = codecInfo.codec().fieldsProducer(new SegmentReadState(cfsDir, si, fieldInfos, readBufferSize, termsIndexDivisor));
         assert fields != null;
 
         success = true;
@@ -506,7 +501,7 @@ public class SegmentReader extends IndexReader implements Cloneable {
    * @throws IOException if there is a low-level IO error
    */
   public static SegmentReader get(boolean readOnly, SegmentInfo si, int termInfosIndexDivisor) throws CorruptIndexException, IOException {
-    return get(readOnly, si.dir, si, BufferedIndexInput.BUFFER_SIZE, true, termInfosIndexDivisor, null);
+    return get(readOnly, si.dir, si, BufferedIndexInput.BUFFER_SIZE, true, termInfosIndexDivisor);
   }
 
   /**
@@ -518,12 +513,8 @@ public class SegmentReader extends IndexReader implements Cloneable {
                                   SegmentInfo si,
                                   int readBufferSize,
                                   boolean doOpenStores,
-                                  int termInfosIndexDivisor,
-                                  CodecProvider codecs)
+                                  int termInfosIndexDivisor)
     throws CorruptIndexException, IOException {
-    if (codecs == null)  {
-      codecs = CodecProvider.getDefault();
-    }
     
     SegmentReader instance = new SegmentReader();
     instance.readOnly = readOnly;
@@ -533,7 +524,7 @@ public class SegmentReader extends IndexReader implements Cloneable {
     boolean success = false;
 
     try {
-      instance.core = new CoreReaders(instance, dir, si, readBufferSize, termInfosIndexDivisor, codecs);
+      instance.core = new CoreReaders(instance, dir, si, readBufferSize, termInfosIndexDivisor);
       if (doOpenStores) {
         instance.core.openDocStores(si);
       }
diff --git a/lucene/src/java/org/apache/lucene/index/SegmentWriteState.java b/lucene/src/java/org/apache/lucene/index/SegmentWriteState.java
index a03bb0b..3999049 100644
--- a/lucene/src/java/org/apache/lucene/index/SegmentWriteState.java
+++ b/lucene/src/java/org/apache/lucene/index/SegmentWriteState.java
@@ -17,13 +17,11 @@ package org.apache.lucene.index;
  * limitations under the License.
  */
 
-import java.util.HashSet;
-import java.util.Collection;
 import java.io.PrintStream;
+import java.util.Collection;
+import java.util.HashSet;
 
 import org.apache.lucene.store.Directory;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.CodecProvider;
 
 /**
  * @lucene.experimental
@@ -38,8 +36,8 @@ public class SegmentWriteState {
   public int numDocsInStore;
   public final Collection<String> flushedFiles;
 
-  // Actual codec used
-  final Codec codec;
+  final SegmentCodecs segmentCodecs;
+  public int currentCodecId;
 
   /** Expert: The fraction of terms in the "dictionary" which should be stored
    * in RAM.  Smaller values use more memory, but make searching slightly
@@ -59,11 +57,12 @@ public class SegmentWriteState {
    * slightly smaller indexes, but slower skipping in big posting lists.
    */
   public final int maxSkipLevels = 10;
+  
+
 
   public SegmentWriteState(PrintStream infoStream, Directory directory, String segmentName, FieldInfos fieldInfos,
                            String docStoreSegmentName, int numDocs,
-                           int numDocsInStore, int termIndexInterval,
-                           CodecProvider codecs) {
+                           int numDocsInStore, int termIndexInterval, SegmentCodecs segmentCodecs) {
     this.infoStream = infoStream;
     this.directory = directory;
     this.segmentName = segmentName;
@@ -72,7 +71,7 @@ public class SegmentWriteState {
     this.numDocs = numDocs;
     this.numDocsInStore = numDocsInStore;
     this.termIndexInterval = termIndexInterval;
-    this.codec = codecs.getWriter(this);
+    this.segmentCodecs = segmentCodecs;
     flushedFiles = new HashSet<String>();
   }
 }
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/Codec.java b/lucene/src/java/org/apache/lucene/index/codecs/Codec.java
index bbb03d3..223111a 100644
--- a/lucene/src/java/org/apache/lucene/index/codecs/Codec.java
+++ b/lucene/src/java/org/apache/lucene/index/codecs/Codec.java
@@ -27,7 +27,7 @@ import org.apache.lucene.store.Directory;
 
 /** @lucene.experimental */
 public abstract class Codec {
-
+  public static final Codec[] EMPTY = new Codec[0];
   /** Unique name that's used to retrieve this codec when
    *  reading the index */
   public String name;
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/CodecProvider.java b/lucene/src/java/org/apache/lucene/index/codecs/CodecProvider.java
index 136b7d0..dfbb9d5 100644
--- a/lucene/src/java/org/apache/lucene/index/codecs/CodecProvider.java
+++ b/lucene/src/java/org/apache/lucene/index/codecs/CodecProvider.java
@@ -20,13 +20,13 @@ package org.apache.lucene.index.codecs;
 import java.util.Collection;
 import java.util.HashMap;
 import java.util.HashSet;
+import java.util.Map;
 import java.util.Set;
 
-import org.apache.lucene.index.SegmentWriteState;
 import org.apache.lucene.index.codecs.preflex.PreFlexCodec;
 import org.apache.lucene.index.codecs.pulsing.PulsingCodec;
-import org.apache.lucene.index.codecs.standard.StandardCodec;
 import org.apache.lucene.index.codecs.simpletext.SimpleTextCodec;
+import org.apache.lucene.index.codecs.standard.StandardCodec;
 
 /** Holds a set of codecs, keyed by name.  You subclass
  *  this, instantiate it, and register your codecs, then
@@ -36,10 +36,13 @@ import org.apache.lucene.index.codecs.simpletext.SimpleTextCodec;
  *
  *  @lucene.experimental */
 
-public abstract class CodecProvider {
+public class CodecProvider {
   private SegmentInfosWriter infosWriter = new DefaultSegmentInfosWriter();
   private SegmentInfosReader infosReader = new DefaultSegmentInfosReader();
+  private String defaultFieldCodec = defaultCodec;
+  private final Map<String, String> perFieldMap = new HashMap<String, String>();
 
+  
   private final HashMap<String, Codec> codecs = new HashMap<String, Codec>();
 
   private final Set<String> knownExtensions = new HashSet<String>();
@@ -86,8 +89,6 @@ public abstract class CodecProvider {
     return codec;
   }
 
-  public abstract Codec getWriter(SegmentWriteState state);
-  
   public SegmentInfosWriter getSegmentInfosWriter() {
     return infosWriter;
   }
@@ -110,6 +111,62 @@ public abstract class CodecProvider {
   public synchronized static String getDefaultCodec() {
     return defaultCodec;
   }
+  
+  /**
+   * Sets the {@link Codec} for a given field. Not that setting a fields code is
+   * write-once. If the fields codec is already set this method will throw an
+   * {@link IllegalArgumentException}
+   * 
+   * @param field
+   *          the name of the field
+   * @param codec
+   *          the name of the codec
+   * @throws IllegalArgumentException
+   *           if the codec for the given field is already set
+   * 
+   */
+  public synchronized void setFieldCodec(String field, String codec) {
+    if (perFieldMap.containsKey(field))
+      throw new IllegalArgumentException("codec for field: " + field
+          + " already set to " + perFieldMap.get(field));
+    perFieldMap.put(field, codec);
+  }
+
+  /**
+   * Returns the {@link Codec} name for the given field or the default codec if
+   * not set.
+   * 
+   * @param name
+   *          the fields name
+   * @return the {@link Codec} name for the given field or the default codec if
+   *         not set.
+   */
+  public synchronized String getFieldCodec(String name) {
+    final String codec;
+    if ((codec = perFieldMap.get(name)) == null) {
+      return defaultFieldCodec;
+    }
+    return codec;
+  }
+
+  /**
+   * Returns the default {@link Codec} for this {@link CodecProvider}
+   * 
+   * @return the default {@link Codec} for this {@link CodecProvider}
+   */
+  public synchronized String getDefaultFieldCodec() {
+    return defaultFieldCodec;
+  }
+
+  /**
+   * Sets the default {@link Codec} for this {@link CodecProvider}
+   * 
+   * @param codec
+   *          the codecs name
+   */
+  public synchronized void setDefaultFieldCodec(String codec) {
+    defaultFieldCodec = codec;
+  }
 }
 
 class DefaultCodecProvider extends CodecProvider {
@@ -118,10 +175,6 @@ class DefaultCodecProvider extends CodecProvider {
     register(new PreFlexCodec());
     register(new PulsingCodec(1));
     register(new SimpleTextCodec());
-  }
-
-  @Override
-  public Codec getWriter(SegmentWriteState state) {
-    return lookup(CodecProvider.getDefaultCodec());
+    setDefaultFieldCodec(CodecProvider.getDefaultCodec());
   }
 }
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/PerFieldCodecWrapper.java b/lucene/src/java/org/apache/lucene/index/codecs/PerFieldCodecWrapper.java
deleted file mode 100644
index 8839e8f..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/PerFieldCodecWrapper.java
+++ /dev/null
@@ -1,238 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Map;
-import java.util.HashMap;
-import java.util.Set;
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.IdentityHashMap;
-import java.util.TreeMap;
-import java.util.TreeSet;
-import java.io.IOException;
-
-import org.apache.lucene.index.FieldsEnum;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.store.Directory;
-
-
-/** Simple Codec that dispatches field-specific codecs.
- *  You must ensure every field you index has a Codec, or
- *  the defaultCodec is non null.  Also, the separate
- *  codecs cannot conflict on file names.
- *
- * @lucene.experimental */
-public class PerFieldCodecWrapper extends Codec {
-  private final Map<String,Codec> fields = new IdentityHashMap<String,Codec>();
-  private final Codec defaultCodec;
-
-  public PerFieldCodecWrapper(Codec defaultCodec) {
-    name = "PerField";
-    this.defaultCodec = defaultCodec;
-  }
-
-  public void add(String field, Codec codec) {
-    fields.put(field, codec);
-  }
-
-  public Codec getCodec(String field) {
-    Codec codec = fields.get(field);
-    if (codec != null) {
-      return codec;
-    } else {
-      return defaultCodec;
-    }
-  }
-      
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    return new FieldsWriter(state);
-  }
-
-  private class FieldsWriter extends FieldsConsumer {
-    private final SegmentWriteState state;
-    private final Map<Codec,FieldsConsumer> codecs = new HashMap<Codec,FieldsConsumer>();
-    private final Set<String> fieldsSeen = new TreeSet<String>();
-
-    public FieldsWriter(SegmentWriteState state) {
-      this.state = state;
-    }
-
-    @Override
-    public TermsConsumer addField(FieldInfo field) throws IOException {
-      fieldsSeen.add(field.name);
-      Codec codec = getCodec(field.name);
-
-      FieldsConsumer fields = codecs.get(codec);
-      if (fields == null) {
-        fields = codec.fieldsConsumer(state);
-        codecs.put(codec, fields);
-      }
-      return fields.addField(field);
-    }
-
-    @Override
-    public void close() throws IOException {
-      Iterator<FieldsConsumer> it = codecs.values().iterator();
-      IOException err = null;
-      while(it.hasNext()) {
-        try {
-          it.next().close();
-        } catch (IOException ioe) {
-          // keep first IOException we hit but keep
-          // closing the rest
-          if (err == null) {
-            err = ioe;
-          }
-        }
-      }
-      if (err != null) {
-        throw err;
-      }
-    }
-  }
-
-  private class FieldsReader extends FieldsProducer {
-
-    private final Set<String> fields = new TreeSet<String>();
-    private final Map<Codec,FieldsProducer> codecs = new HashMap<Codec,FieldsProducer>();
-
-    public FieldsReader(Directory dir, FieldInfos fieldInfos,
-                        SegmentInfo si, int readBufferSize,
-                        int indexDivisor) throws IOException {
-
-      final int fieldCount = fieldInfos.size();
-      for(int i=0;i<fieldCount;i++) {
-        FieldInfo fi = fieldInfos.fieldInfo(i);
-        if (fi.isIndexed) {
-          fields.add(fi.name);
-          Codec codec = getCodec(fi.name);
-          if (!codecs.containsKey(codec)) {
-            codecs.put(codec, codec.fieldsProducer(new SegmentReadState(dir, si, fieldInfos, readBufferSize, indexDivisor)));
-          }
-        }
-      }
-    }
-
-    private final class FieldsIterator extends FieldsEnum {
-      private final Iterator<String> it;
-      private String current;
-
-      public FieldsIterator() {
-        it = fields.iterator();
-      }
-
-      @Override
-      public String next() {
-        if (it.hasNext()) {
-          current = it.next();
-        } else {
-          current = null;
-        }
-
-        return current;
-      }
-
-      @Override
-      public TermsEnum terms() throws IOException {
-        Terms terms = codecs.get(getCodec(current)).terms(current);
-        if (terms != null) {
-          return terms.iterator();
-        } else {
-          return null;
-        }
-      }
-    }
-      
-    @Override
-    public FieldsEnum iterator() throws IOException {
-      return new FieldsIterator();
-    }
-
-    @Override
-    public Terms terms(String field) throws IOException {
-      Codec codec = getCodec(field);
-
-      FieldsProducer fields = codecs.get(codec);
-      assert fields != null;
-      return fields.terms(field);
-    }
-
-    @Override
-    public void close() throws IOException {
-      Iterator<FieldsProducer> it = codecs.values().iterator();
-      IOException err = null;
-      while(it.hasNext()) {
-        try {
-          it.next().close();
-        } catch (IOException ioe) {
-          // keep first IOException we hit but keep
-          // closing the rest
-          if (err == null) {
-            err = ioe;
-          }
-        }
-      }
-      if (err != null) {
-        throw err;
-      }
-    }
-
-    @Override
-    public void loadTermsIndex(int indexDivisor) throws IOException {
-      Iterator<FieldsProducer> it = codecs.values().iterator();
-      while(it.hasNext()) {
-        it.next().loadTermsIndex(indexDivisor);
-      }
-    }
-  }
-
-  public FieldsProducer fieldsProducer(SegmentReadState state)
-    throws IOException {
-    return new FieldsReader(state.dir, state.fieldInfos, state.segmentInfo, state.readBufferSize, state.termsIndexDivisor);
-  }
-
-  @Override
-  public void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
-    Iterator<Codec> it = fields.values().iterator();
-    Set<Codec> seen = new HashSet<Codec>();
-    while(it.hasNext()) {
-      final Codec codec = it.next();
-      if (!seen.contains(codec)) {
-        seen.add(codec);
-        codec.files(dir, info, files);
-      }
-    }
-  }
-
-  @Override
-  public void getExtensions(Set<String> extensions) {
-    Iterator<Codec> it = fields.values().iterator();
-    while(it.hasNext()) {
-      final Codec codec = it.next();
-      codec.getExtensions(extensions);
-    }
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/TestExternalCodecs.java b/lucene/src/test/org/apache/lucene/TestExternalCodecs.java
index 726da23..1f7fe63 100644
--- a/lucene/src/test/org/apache/lucene/TestExternalCodecs.java
+++ b/lucene/src/test/org/apache/lucene/TestExternalCodecs.java
@@ -71,7 +71,10 @@ public class TestExternalCodecs extends LuceneTestCase {
   //   - good improvement would be to write through to disk,
   //     and then load into ram from disk
   public static class RAMOnlyCodec extends Codec {
-
+    
+    public RAMOnlyCodec() {
+      name = "RamOnly";
+    }
     // Postings state:
     static class RAMPostings extends FieldsProducer {
       final Map<String,RAMField> fieldToTerms = new TreeMap<String,RAMField>();
@@ -468,6 +471,7 @@ public class TestExternalCodecs extends LuceneTestCase {
     @Override
     public FieldsProducer fieldsProducer(SegmentReadState readState)
       throws IOException {
+    
       return state.get(readState.segmentInfo.name);
     }
 
@@ -481,20 +485,10 @@ public class TestExternalCodecs extends LuceneTestCase {
   }
 
   public static class MyCodecs extends CodecProvider {
-    PerFieldCodecWrapper perField;
-
     MyCodecs() {
       Codec ram = new RAMOnlyCodec();
-      Codec pulsing = new PulsingReverseTermsCodec();
-      perField = new PerFieldCodecWrapper(ram);
-      perField.add("field2", pulsing);
-      perField.add("id", pulsing);
-      register(perField);
-    }
-    
-    @Override
-    public Codec getWriter(SegmentWriteState state) {
-      return perField;
+      register(ram);
+      setDefaultFieldCodec(ram.name);
     }
   }
 
@@ -617,20 +611,28 @@ public class TestExternalCodecs extends LuceneTestCase {
   // whose term sort is backwards unicode code point, and
   // storing "field1" as a custom entirely-in-RAM codec
   public void testPerFieldCodec() throws Exception {
+    CodecProvider provider = new MyCodecs();
+    Codec pulsing = new PulsingReverseTermsCodec();
+    provider.register(pulsing);
+    
     
     final int NUM_DOCS = 173;
     Directory dir = newDirectory();
     IndexWriter w = new IndexWriter(dir,
-                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).setCodecProvider(new MyCodecs()));
+                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).setCodecProvider(provider));
 
     w.setMergeFactor(3);
     Document doc = new Document();
     // uses default codec:
     doc.add(newField("field1", "this field uses the standard codec as the test", Field.Store.NO, Field.Index.ANALYZED));
     // uses pulsing codec:
-    doc.add(newField("field2", "this field uses the pulsing codec as the test", Field.Store.NO, Field.Index.ANALYZED));
+    Field field2 = newField("field2", "this field uses the pulsing codec as the test", Field.Store.NO, Field.Index.ANALYZED);
+    provider.setFieldCodec(field2.name(), pulsing.name);
+    doc.add(field2);
     
     Field idField = newField("id", "", Field.Store.NO, Field.Index.NOT_ANALYZED);
+    provider.setFieldCodec(idField.name(), pulsing.name);
+
     doc.add(idField);
     for(int i=0;i<NUM_DOCS;i++) {
       idField.setValue(""+i);
diff --git a/lucene/src/test/org/apache/lucene/index/RandomIndexWriter.java b/lucene/src/test/org/apache/lucene/index/RandomIndexWriter.java
index ce27f3d..7086b54 100644
--- a/lucene/src/test/org/apache/lucene/index/RandomIndexWriter.java
+++ b/lucene/src/test/org/apache/lucene/index/RandomIndexWriter.java
@@ -130,7 +130,7 @@ public class RandomIndexWriter implements Closeable {
     // If we are writing with PreFlexRW, force a full
     // IndexReader.open so terms are sorted in codepoint
     // order during searching:
-    if (!w.codecs.getWriter(null).name.equals("PreFlex") && r.nextBoolean()) {
+    if (!w.codecs.getDefaultFieldCodec().equals("PreFlex") && r.nextBoolean()) {
       if (LuceneTestCase.VERBOSE) {
         System.out.println("RIW.getReader: use NRT reader");
       }
diff --git a/lucene/src/test/org/apache/lucene/index/TestAddIndexes.java b/lucene/src/test/org/apache/lucene/index/TestAddIndexes.java
index 9e5f818..d1b0df9 100755
--- a/lucene/src/test/org/apache/lucene/index/TestAddIndexes.java
+++ b/lucene/src/test/org/apache/lucene/index/TestAddIndexes.java
@@ -21,20 +21,23 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 
-import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
+import org.apache.lucene.index.codecs.CodecProvider;
+import org.apache.lucene.index.codecs.mocksep.MockSepCodec;
+import org.apache.lucene.index.codecs.simpletext.SimpleTextCodec;
+import org.apache.lucene.index.codecs.standard.StandardCodec;
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.search.PhraseQuery;
 import org.apache.lucene.store.AlreadyClosedException;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.MockDirectoryWrapper;
 import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util._TestUtil;
 
-import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.search.PhraseQuery;
-
 public class TestAddIndexes extends LuceneTestCase {
   
   public void testSimpleCase() throws IOException {
@@ -848,4 +851,83 @@ public class TestAddIndexes extends LuceneTestCase {
 
     assertTrue(c.failures.size() == 0);
   }
+  
+  private void addDocs3(IndexWriter writer, int numDocs) throws IOException {
+    for (int i = 0; i < numDocs; i++) {
+      Document doc = new Document();
+      doc.add(newField("content", "aaa", Field.Store.NO, Field.Index.ANALYZED));
+      doc.add(newField("id", "" + i, Field.Store.YES, Field.Index.ANALYZED));
+      writer.addDocument(doc);
+    }
+  }
+
+  public void testSimpleCaseCustomCodecProvider() throws IOException {
+    // main directory
+    Directory dir = newDirectory();
+    // two auxiliary directories
+    Directory aux = newDirectory();
+    Directory aux2 = newDirectory();
+    CodecProvider provider = new MockCodecProvider();
+    IndexWriter writer = null;
+
+    writer = newWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT,
+        new MockAnalyzer()).setOpenMode(OpenMode.CREATE).setCodecProvider(
+        provider));
+    // add 100 documents
+    addDocs3(writer, 100);
+    assertEquals(100, writer.maxDoc());
+    writer.commit();
+    writer.close();
+    _TestUtil.checkIndex(dir, provider);
+
+    writer = newWriter(aux, newIndexWriterConfig(TEST_VERSION_CURRENT,
+        new MockAnalyzer()).setOpenMode(OpenMode.CREATE).setCodecProvider(
+        provider));
+    ((LogMergePolicy) writer.getConfig().getMergePolicy())
+        .setUseCompoundFile(false); // use one without a compound file
+    ((LogMergePolicy) writer.getConfig().getMergePolicy())
+        .setUseCompoundDocStore(false); // use one without a compound file
+    // add 40 documents in separate files
+    addDocs(writer, 40);
+    assertEquals(40, writer.maxDoc());
+    writer.commit();
+    writer.close();
+
+    writer = newWriter(aux2, newIndexWriterConfig(TEST_VERSION_CURRENT,
+        new MockAnalyzer()).setOpenMode(OpenMode.CREATE).setCodecProvider(
+        provider));
+    // add 40 documents in compound files
+    addDocs2(writer, 50);
+    assertEquals(50, writer.maxDoc());
+    writer.commit();
+    writer.close();
+
+    // test doc count before segments are merged
+    writer = newWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT,
+        new MockAnalyzer()).setOpenMode(OpenMode.APPEND).setCodecProvider(
+        provider));
+    assertEquals(100, writer.maxDoc());
+    writer.addIndexes(new Directory[] { aux, aux2 });
+    assertEquals(190, writer.maxDoc());
+    writer.close();
+    _TestUtil.checkIndex(dir, provider);
+
+    dir.close();
+    aux.close();
+    aux2.close();
+  }
+
+  public static class MockCodecProvider extends CodecProvider {
+    public MockCodecProvider() {
+      StandardCodec standardCodec = new StandardCodec();
+      SimpleTextCodec simpleTextCodec = new SimpleTextCodec();
+      MockSepCodec mockSepCodec = new MockSepCodec();
+      register(standardCodec);
+      register(mockSepCodec);
+      register(simpleTextCodec);
+      setFieldCodec("id", simpleTextCodec.name);
+      setFieldCodec("content", mockSepCodec.name);
+    }
+  }
+  
 }
diff --git a/lucene/src/test/org/apache/lucene/index/TestCodecs.java b/lucene/src/test/org/apache/lucene/index/TestCodecs.java
index 0cfc227..afb7646 100644
--- a/lucene/src/test/org/apache/lucene/index/TestCodecs.java
+++ b/lucene/src/test/org/apache/lucene/index/TestCodecs.java
@@ -25,7 +25,6 @@ import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.Field.Store;
-import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.index.codecs.CodecProvider;
 import org.apache.lucene.index.codecs.FieldsConsumer;
 import org.apache.lucene.index.codecs.FieldsProducer;
@@ -271,10 +270,10 @@ public class TestCodecs extends LuceneTestCase {
 
     final Directory dir = newDirectory();
     this.write(fieldInfos, dir, fields);
-    final SegmentInfo si = new SegmentInfo(SEGMENT, 10000, dir, false, -1, SEGMENT, false, true, CodecProvider.getDefault().getWriter(null));
+    final SegmentInfo si = new SegmentInfo(SEGMENT, 10000, dir, false, -1, SEGMENT, false, true, SegmentCodecs.build(fieldInfos, CodecProvider.getDefault()));
     si.setHasProx(false);
 
-    final FieldsProducer reader = si.getCodec().fieldsProducer(new SegmentReadState(dir, si, fieldInfos, 64, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR));
+    final FieldsProducer reader = si.getCodecInfo().codec().fieldsProducer(new SegmentReadState(dir, si, fieldInfos, 64, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR));
 
     final FieldsEnum fieldsEnum = reader.iterator();
     assertNotNull(fieldsEnum.next());
@@ -319,9 +318,9 @@ public class TestCodecs extends LuceneTestCase {
     final Directory dir = newDirectory();
 
     this.write(fieldInfos, dir, fields);
-    final SegmentInfo si = new SegmentInfo(SEGMENT, 10000, dir, false, -1, SEGMENT, false, true, CodecProvider.getDefault().getWriter(null));
+    final SegmentInfo si = new SegmentInfo(SEGMENT, 10000, dir, false, -1, SEGMENT, false, true, SegmentCodecs.build(fieldInfos, CodecProvider.getDefault()));
 
-    final FieldsProducer terms = si.getCodec().fieldsProducer(new SegmentReadState(dir, si, fieldInfos, 1024, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR));
+    final FieldsProducer terms = si.getCodecInfo().codec().fieldsProducer(new SegmentReadState(dir, si, fieldInfos, 1024, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR));
 
     final Verify[] threads = new Verify[NUM_TEST_THREADS-1];
     for(int i=0;i<NUM_TEST_THREADS-1;i++) {
@@ -402,13 +401,9 @@ public class TestCodecs extends LuceneTestCase {
 
     protected MockSepCodecs() {
       this.register(new MockSepCodec());
+      this.setDefaultFieldCodec("MockSep");
     }
-
-    @Override
-    public Codec getWriter(final SegmentWriteState state) {
-      return this.lookup("MockSep");
-    }
-
+    
   }
 
   private class Verify extends Thread {
@@ -611,11 +606,10 @@ public class TestCodecs extends LuceneTestCase {
   private void write(final FieldInfos fieldInfos, final Directory dir, final FieldData[] fields) throws Throwable {
 
     final int termIndexInterval = this.nextInt(13, 27);
+    final SegmentCodecs codecInfo = SegmentCodecs.build(fieldInfos, CodecProvider.getDefault());
+    final SegmentWriteState state = new SegmentWriteState(null, dir, SEGMENT, fieldInfos, null, 10000, 10000, termIndexInterval, codecInfo);
 
-    final SegmentWriteState state = new SegmentWriteState(null, dir, SEGMENT, fieldInfos, null, 10000, 10000, termIndexInterval,
-                                                    CodecProvider.getDefault());
-
-    final FieldsConsumer consumer = state.codec.fieldsConsumer(state);
+    final FieldsConsumer consumer = state.segmentCodecs.codec().fieldsConsumer(state);
     Arrays.sort(fields);
     for (final FieldData field : fields) {
       field.write(consumer);
diff --git a/lucene/src/test/org/apache/lucene/index/TestDoc.java b/lucene/src/test/org/apache/lucene/index/TestDoc.java
index e09f88f..8df6ee4 100644
--- a/lucene/src/test/org/apache/lucene/index/TestDoc.java
+++ b/lucene/src/test/org/apache/lucene/index/TestDoc.java
@@ -197,7 +197,7 @@ public class TestDoc extends LuceneTestCase {
       merger.closeReaders();
       
       final SegmentInfo info = new SegmentInfo(merged, si1.docCount + si2.docCount, si1.dir,
-                                               useCompoundFile, -1, null, false, merger.hasProx(), merger.getCodec());
+                                               useCompoundFile, -1, null, false, merger.hasProx(), merger.getSegmentCodecs());
       
       if (useCompoundFile) {
         List<String> filesToDelete = merger.createCompoundFile(merged + ".cfs", info);
diff --git a/lucene/src/test/org/apache/lucene/index/TestPerFieldCodecSupport.java b/lucene/src/test/org/apache/lucene/index/TestPerFieldCodecSupport.java
new file mode 100644
index 0000000..7286718
--- /dev/null
+++ b/lucene/src/test/org/apache/lucene/index/TestPerFieldCodecSupport.java
@@ -0,0 +1,289 @@
+package org.apache.lucene.index;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.Field.Index;
+import org.apache.lucene.index.CheckIndex.Status;
+import org.apache.lucene.index.CheckIndex.Status.SegmentInfoStatus;
+import org.apache.lucene.index.IndexWriterConfig.OpenMode;
+import org.apache.lucene.index.codecs.Codec;
+import org.apache.lucene.index.codecs.CodecProvider;
+import org.apache.lucene.index.codecs.mocksep.MockSepCodec;
+import org.apache.lucene.index.codecs.simpletext.SimpleTextCodec;
+import org.apache.lucene.index.codecs.standard.StandardCodec;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
+import org.junit.Test;
+
+/**
+ * 
+ *
+ */
+public class TestPerFieldCodecSupport extends LuceneTestCase {
+
+  private IndexWriter newWriter(Directory dir, IndexWriterConfig conf)
+      throws IOException {
+    LogDocMergePolicy logByteSizeMergePolicy = new LogDocMergePolicy();
+    logByteSizeMergePolicy.setUseCompoundFile(false); // make sure we use plain
+    // files
+    conf.setMergePolicy(logByteSizeMergePolicy);
+
+    final IndexWriter writer = new IndexWriter(dir, conf);
+    return writer;
+  }
+
+  private void addDocs(IndexWriter writer, int numDocs) throws IOException {
+    for (int i = 0; i < numDocs; i++) {
+      Document doc = new Document();
+      doc.add(newField("content", "aaa", Field.Store.NO, Field.Index.ANALYZED));
+      writer.addDocument(doc);
+    }
+  }
+
+  private void addDocs2(IndexWriter writer, int numDocs) throws IOException {
+    for (int i = 0; i < numDocs; i++) {
+      Document doc = new Document();
+      doc.add(newField("content", "bbb", Field.Store.NO, Field.Index.ANALYZED));
+      writer.addDocument(doc);
+    }
+  }
+
+  private void addDocs3(IndexWriter writer, int numDocs) throws IOException {
+    for (int i = 0; i < numDocs; i++) {
+      Document doc = new Document();
+      doc.add(newField("content", "ccc", Field.Store.NO, Field.Index.ANALYZED));
+      doc.add(newField("id", "" + i, Field.Store.YES, Field.Index.ANALYZED));
+      writer.addDocument(doc);
+    }
+  }
+
+  /*
+   * Test is hetrogenous index segements are merge sucessfully
+   */
+  @Test
+  public void testMergeUnusedPerFieldCodec() throws IOException {
+    Directory dir = newDirectory();
+    CodecProvider provider = new MockCodecProvider();
+    IndexWriterConfig iwconf = newIndexWriterConfig(TEST_VERSION_CURRENT,
+        new MockAnalyzer()).setOpenMode(OpenMode.CREATE).setCodecProvider(
+        provider);
+    IndexWriter writer = newWriter(dir, iwconf);
+    addDocs(writer, 10);
+    writer.commit();
+    addDocs3(writer, 10);
+    writer.commit();
+    addDocs2(writer, 10);
+    writer.commit();
+    assertEquals(30, writer.maxDoc());
+    _TestUtil.checkIndex(dir, provider);
+    writer.optimize();
+    assertEquals(30, writer.maxDoc());
+    writer.close();
+    dir.close();
+  }
+
+  /*
+   * Test is hetrogenous index segements are merge sucessfully
+   */
+  @Test
+  public void testChangeCodecAndMerge() throws IOException {
+    Directory dir = newDirectory();
+    CodecProvider provider = new MockCodecProvider();
+    IndexWriterConfig iwconf = newIndexWriterConfig(TEST_VERSION_CURRENT,
+        new MockAnalyzer()).setOpenMode(OpenMode.CREATE).setCodecProvider(
+        provider);
+    IndexWriter writer = newWriter(dir, iwconf);
+    addDocs(writer, 10);
+    writer.commit();
+    assertQuery(new Term("content", "aaa"), dir, 10, provider);
+    addDocs3(writer, 10);
+    writer.commit();
+    writer.close();
+
+    assertQuery(new Term("content", "ccc"), dir, 10, provider);
+    assertQuery(new Term("content", "aaa"), dir, 10, provider);
+    assertCodecPerField(_TestUtil.checkIndex(dir, provider), "content",
+        provider.lookup("MockSep"));
+
+    iwconf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())
+        .setOpenMode(OpenMode.APPEND).setCodecProvider(provider);
+    ((LogMergePolicy) iwconf.getMergePolicy()).setUseCompoundFile(false);
+    ((LogMergePolicy) iwconf.getMergePolicy()).setUseCompoundDocStore(false);
+
+    provider = new MockCodecProvider2(); // uses standard for field content
+    iwconf.setCodecProvider(provider);
+    writer = newWriter(dir, iwconf);
+    // swap in new codec for currently written segments
+    addDocs2(writer, 10);
+    writer.commit();
+    Codec origContentCodec = provider.lookup("MockSep");
+    Codec newContentCodec = provider.lookup("Standard");
+    assertHybridCodecPerField(_TestUtil.checkIndex(dir, provider), "content",
+        origContentCodec, origContentCodec, newContentCodec);
+    assertEquals(30, writer.maxDoc());
+    assertQuery(new Term("content", "bbb"), dir, 10, provider);
+    assertQuery(new Term("content", "ccc"), dir, 10, provider);
+    assertQuery(new Term("content", "aaa"), dir, 10, provider);
+
+    addDocs2(writer, 10);
+    writer.commit();
+    assertQuery(new Term("content", "ccc"), dir, 10, provider);
+    assertQuery(new Term("content", "bbb"), dir, 20, provider);
+    assertQuery(new Term("content", "aaa"), dir, 10, provider);
+    assertEquals(40, writer.maxDoc());
+
+    writer.optimize();
+    assertEquals(40, writer.maxDoc());
+    writer.close();
+    assertCodecPerFieldOptimized(_TestUtil.checkIndex(dir, provider),
+        "content", newContentCodec);
+    assertQuery(new Term("content", "ccc"), dir, 10, provider);
+    assertQuery(new Term("content", "bbb"), dir, 20, provider);
+    assertQuery(new Term("content", "aaa"), dir, 10, provider);
+
+    dir.close();
+  }
+
+  public void assertCodecPerFieldOptimized(Status checkIndex, String field,
+      Codec codec) {
+    assertEquals(1, checkIndex.segmentInfos.size());
+    final CodecProvider provider = checkIndex.segmentInfos.get(0).codec.provider;
+    assertEquals(codec, provider.lookup(provider.getFieldCodec(field)));
+
+  }
+
+  public void assertCodecPerField(Status checkIndex, String field, Codec codec) {
+    for (SegmentInfoStatus info : checkIndex.segmentInfos) {
+      final CodecProvider provider = info.codec.provider;
+      assertEquals(codec, provider.lookup(provider.getFieldCodec(field)));
+    }
+  }
+
+  public void assertHybridCodecPerField(Status checkIndex, String field,
+      Codec... codec) throws IOException {
+    List<SegmentInfoStatus> segmentInfos = checkIndex.segmentInfos;
+    assertEquals(segmentInfos.size(), codec.length);
+    for (int i = 0; i < codec.length; i++) {
+      SegmentCodecs codecInfo = segmentInfos.get(i).codec;
+      FieldInfos fieldInfos = new FieldInfos(checkIndex.dir, IndexFileNames
+          .segmentFileName(segmentInfos.get(i).name, "",
+              IndexFileNames.FIELD_INFOS_EXTENSION));
+      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
+      assertEquals("faild for segment index: " + i, codec[i],
+          codecInfo.codecs[fieldInfo.codecId]);
+    }
+  }
+
+  public void assertQuery(Term t, Directory dir, int num, CodecProvider codecs)
+      throws CorruptIndexException, IOException {
+    IndexReader reader = IndexReader.open(dir, null, true,
+        IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, codecs);
+    IndexSearcher searcher = new IndexSearcher(reader);
+    TopDocs search = searcher.search(new TermQuery(t), num + 10);
+    assertEquals(num, search.totalHits);
+    searcher.close();
+    reader.close();
+
+  }
+
+  public static class MockCodecProvider extends CodecProvider {
+
+    public MockCodecProvider() {
+      StandardCodec standardCodec = new StandardCodec();
+      setDefaultFieldCodec(standardCodec.name);
+      SimpleTextCodec simpleTextCodec = new SimpleTextCodec();
+      MockSepCodec mockSepCodec = new MockSepCodec();
+      register(standardCodec);
+      register(mockSepCodec);
+      register(simpleTextCodec);
+      setFieldCodec("id", simpleTextCodec.name);
+      setFieldCodec("content", mockSepCodec.name);
+    }
+  }
+
+  public static class MockCodecProvider2 extends CodecProvider {
+
+    public MockCodecProvider2() {
+      StandardCodec standardCodec = new StandardCodec();
+      setDefaultFieldCodec(standardCodec.name);
+      SimpleTextCodec simpleTextCodec = new SimpleTextCodec();
+      MockSepCodec mockSepCodec = new MockSepCodec();
+      register(standardCodec);
+      register(mockSepCodec);
+      register(simpleTextCodec);
+      setFieldCodec("id", simpleTextCodec.name);
+      setFieldCodec("content", standardCodec.name);
+    }
+  }
+
+  /*
+   * Test per field codec support - adding fields with random codecs
+   */
+  @Test
+  public void testStressPerFieldCodec() throws IOException {
+    Directory dir = newDirectory(random);
+    Index[] indexValue = new Index[] { Index.ANALYZED, Index.ANALYZED_NO_NORMS,
+        Index.NOT_ANALYZED, Index.NOT_ANALYZED_NO_NORMS };
+    final int docsPerRound = 97;
+    for (int i = 0; i < 5; i++) {
+      CodecProvider provider = new CodecProvider();
+      provider.register(new StandardCodec());
+      provider.register(new SimpleTextCodec());
+      // provider.register(new MockSepCodec()); // TODO enable once we have
+      // files per codec
+      // provider.register(new PulsingCodec());
+
+      for (int j = 0; j < 30 * RANDOM_MULTIPLIER; j++) {
+        provider.setFieldCodec("" + j, random.nextBoolean() ? "SimpleText"
+            : "Standard"); // TODO enable other codecs once possible
+      }
+      IndexWriterConfig config = newIndexWriterConfig(random,
+          TEST_VERSION_CURRENT, new MockAnalyzer());
+      config.setOpenMode(OpenMode.CREATE_OR_APPEND);
+      config.setCodecProvider(provider);
+      IndexWriter writer = newWriter(dir, config);
+      for (int j = 0; j < docsPerRound; j++) {
+        final Document doc = new Document();
+        for (int k = 0; k < 30 * RANDOM_MULTIPLIER; k++) {
+          Field field = newField("" + k, _TestUtil
+              .randomRealisticUnicodeString(random, 128), indexValue[random
+              .nextInt(indexValue.length)]);
+          doc.add(field);
+        }
+        writer.addDocument(doc);
+      }
+      if (random.nextBoolean()) {
+        writer.optimize();
+      }
+      writer.commit();
+      assertEquals((i + 1) * docsPerRound, writer.maxDoc());
+      writer.close();
+      _TestUtil.checkIndex(dir, provider);
+    }
+    dir.close();
+  }
+}
\ No newline at end of file
diff --git a/lucene/src/test/org/apache/lucene/index/TestSegmentMerger.java b/lucene/src/test/org/apache/lucene/index/TestSegmentMerger.java
index 60a80f6..2a3ba56 100644
--- a/lucene/src/test/org/apache/lucene/index/TestSegmentMerger.java
+++ b/lucene/src/test/org/apache/lucene/index/TestSegmentMerger.java
@@ -81,7 +81,7 @@ public class TestSegmentMerger extends LuceneTestCase {
     assertTrue(docsMerged == 2);
     //Should be able to open a new SegmentReader against the new directory
     SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, -1,
-        null, false, merger.hasProx(), merger.getCodec()), BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, null);
+        null, false, merger.hasProx(), merger.getSegmentCodecs()), BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);
 
     assertTrue(mergedReader != null);
     assertTrue(mergedReader.numDocs() == 2);
diff --git a/lucene/src/test/org/apache/lucene/util/LuceneTestCase.java b/lucene/src/test/org/apache/lucene/util/LuceneTestCase.java
index 6461486..861ca1e 100644
--- a/lucene/src/test/org/apache/lucene/util/LuceneTestCase.java
+++ b/lucene/src/test/org/apache/lucene/util/LuceneTestCase.java
@@ -230,6 +230,7 @@ public abstract class LuceneTestCase extends Assert {
     }
 
     CodecProvider.setDefaultCodec(codec);
+    cp.setDefaultFieldCodec(codec);
 
     if (codec.equals("PreFlex")) {
       // If we're running w/ PreFlex codec we must swap in the
@@ -262,6 +263,8 @@ public abstract class LuceneTestCase extends Assert {
     cp.unregister(cp.lookup("MockVariableIntBlock"));
     swapCodec(new PulsingCodec(1));
     CodecProvider.setDefaultCodec(savedDefaultCodec);
+    cp.setDefaultFieldCodec(savedDefaultCodec);
+
   }
 
   // randomly picks from core and test codecs
diff --git a/lucene/src/test/org/apache/lucene/util/_TestUtil.java b/lucene/src/test/org/apache/lucene/util/_TestUtil.java
index 76cc0b9..bb6c580 100644
--- a/lucene/src/test/org/apache/lucene/util/_TestUtil.java
+++ b/lucene/src/test/org/apache/lucene/util/_TestUtil.java
@@ -192,12 +192,7 @@ public class _TestUtil {
   }
 
   public static CodecProvider alwaysCodec(final Codec c) {
-    return new CodecProvider() {
-      @Override
-      public Codec getWriter(SegmentWriteState state) {
-        return c;
-      }
-
+    CodecProvider p = new CodecProvider() {
       @Override
       public Codec lookup(String name) {
         // can't do this until we fix PreFlexRW to not
@@ -209,6 +204,8 @@ public class _TestUtil {
         }
       }
     };
+    p.setDefaultFieldCodec(c.name);
+    return p;
   }
 
   /** Return a CodecProvider that can read any of the

