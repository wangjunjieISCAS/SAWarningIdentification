GitDiffStart: 8f3a71443bd538c96207db05d8616ba14d7ef23b | Sat Feb 7 07:38:44 2015 +0000
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ar/package-info.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ar/package-info.java
new file mode 100644
index 0000000..dfb2cab
--- /dev/null
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ar/package-info.java
@@ -0,0 +1,21 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Analyzer for Arabic.
+ */
+package org.apache.lucene.analysis.ar;
\ No newline at end of file
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ar/package.html b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ar/package.html
deleted file mode 100644
index 18803da..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ar/package.html
+++ /dev/null
@@ -1,22 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html><head></head>
-<body>
-Analyzer for Arabic.
-</body>
-</html>
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/bg/package-info.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/bg/package-info.java
new file mode 100644
index 0000000..c60a53b
--- /dev/null
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/bg/package-info.java
@@ -0,0 +1,21 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Analyzer for Bulgarian.
+ */
+package org.apache.lucene.analysis.bg;
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/bg/package.html b/lucene/analysis/common/src/java/org/apache/lucene/analysis/bg/package.html
deleted file mode 100644
index 74c4008..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/bg/package.html
+++ /dev/null
@@ -1,22 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html><head></head>
-<body>
-Analyzer for Bulgarian.
-</body>
-</html>
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/br/package-info.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/br/package-info.java
new file mode 100644
index 0000000..080389b
--- /dev/null
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/br/package-info.java
@@ -0,0 +1,21 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Analyzer for Brazilian Portuguese.
+ */
+package org.apache.lucene.analysis.br;
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/br/package.html b/lucene/analysis/common/src/java/org/apache/lucene/analysis/br/package.html
deleted file mode 100644
index 714632a..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/br/package.html
+++ /dev/null
@@ -1,22 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html><head></head>
-<body>
-Analyzer for Brazilian Portuguese.
-</body>
-</html>
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ca/package-info.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ca/package-info.java
new file mode 100644
index 0000000..7f8b0da
--- /dev/null
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ca/package-info.java
@@ -0,0 +1,21 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Analyzer for Catalan.
+ */
+package org.apache.lucene.analysis.ca;
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ca/package.html b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ca/package.html
deleted file mode 100644
index 4361a61..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ca/package.html
+++ /dev/null
@@ -1,22 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html><head></head>
-<body>
-Analyzer for Catalan.
-</body>
-</html>
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/package-info.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/package-info.java
new file mode 100644
index 0000000..3c416b5
--- /dev/null
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/package-info.java
@@ -0,0 +1,59 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Normalization of text before the tokenizer.
+ * </p>
+ * <p>
+ *   CharFilters are chainable filters that normalize text before tokenization 
+ *   and provide mappings between normalized text offsets and the corresponding 
+ *   offset in the original text.
+ * </p>
+ * <H2>CharFilter offset mappings</H2>
+ * <p>
+ *   CharFilters modify an input stream via a series of substring
+ *   replacements (including deletions and insertions) to produce an output
+ *   stream. There are three possible replacement cases: the replacement
+ *   string has the same length as the original substring; the replacement
+ *   is shorter; and the replacement is longer. In the latter two cases
+ *   (when the replacement has a different length than the original),
+ *   one or more offset correction mappings are required.
+ * </p>
+ * <p>
+ *   When the replacement is shorter than the original (e.g. when the
+ *   replacement is the empty string), a single offset correction mapping
+ *   should be added at the replacement's end offset in the output stream.
+ *   The <code>cumulativeDiff</code> parameter to the
+ *   <code>addOffCorrectMapping()</code> method will be the sum of all
+ *   previous replacement offset adjustments, with the addition of the
+ *   difference between the lengths of the original substring and the
+ *   replacement string (a positive value).
+ * </p>
+ * <p>
+ *   When the replacement is longer than the original (e.g. when the
+ *   original is the empty string), you should add as many offset
+ *   correction mappings as the difference between the lengths of the
+ *   replacement string and the original substring, starting at the
+ *   end offset the original substring would have had in the output stream.
+ *   The <code>cumulativeDiff</code> parameter to the
+ *   <code>addOffCorrectMapping()</code> method will be the sum of all
+ *   previous replacement offset adjustments, with the addition of the
+ *   difference between the lengths of the original substring and the
+ *   replacement string so far (a negative value).
+ * </p>
+ */
+package org.apache.lucene.analysis.charfilter;
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/package.html b/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/package.html
deleted file mode 100644
index 2bef1d1..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/package.html
+++ /dev/null
@@ -1,61 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html><head></head>
-<body>
-<p>
-Normalization of text before the tokenizer.
-</p>
-<p>
-  CharFilters are chainable filters that normalize text before tokenization 
-  and provide mappings between normalized text offsets and the corresponding 
-  offset in the original text.
-</p>
-<H2>CharFilter offset mappings</H2>
-<p>
-  CharFilters modify an input stream via a series of substring
-  replacements (including deletions and insertions) to produce an output
-  stream. There are three possible replacement cases: the replacement
-  string has the same length as the original substring; the replacement
-  is shorter; and the replacement is longer. In the latter two cases
-  (when the replacement has a different length than the original),
-  one or more offset correction mappings are required.
-</p>
-<p>
-  When the replacement is shorter than the original (e.g. when the
-  replacement is the empty string), a single offset correction mapping
-  should be added at the replacement's end offset in the output stream.
-  The <code>cumulativeDiff</code> parameter to the
-  <code>addOffCorrectMapping()</code> method will be the sum of all
-  previous replacement offset adjustments, with the addition of the
-  difference between the lengths of the original substring and the
-  replacement string (a positive value).
-</p>
-<p>
-  When the replacement is longer than the original (e.g. when the
-  original is the empty string), you should add as many offset
-  correction mappings as the difference between the lengths of the
-  replacement string and the original substring, starting at the
-  end offset the original substring would have had in the output stream.
-  The <code>cumulativeDiff</code> parameter to the
-  <code>addOffCorrectMapping()</code> method will be the sum of all
-  previous replacement offset adjustments, with the addition of the
-  difference between the lengths of the original substring and the
-  replacement string so far (a negative value).
-</p>
-</body>
-</html>
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/package-info.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/package-info.java
new file mode 100644
index 0000000..4649fb9
--- /dev/null
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/package-info.java
@@ -0,0 +1,37 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Analyzer for Chinese, Japanese, and Korean, which indexes bigrams. 
+ * This analyzer generates bigram terms, which are overlapping groups of two adjacent Han, Hiragana, Katakana, or Hangul characters.
+ * <p>
+ * Three analyzers are provided for Chinese, each of which treats Chinese text in a different way.
+ * <ul>
+ *  <li>ChineseAnalyzer (in the analyzers/cn package): Index unigrams (individual Chinese characters) as a token.
+ *  <li>CJKAnalyzer (in this package): Index bigrams (overlapping groups of two adjacent Chinese characters) as tokens.
+ *  <li>SmartChineseAnalyzer (in the analyzers/smartcn package): Index words (attempt to segment Chinese text into words) as tokens.
+ * </ul>
+ * 
+ * Example phraseï¼? "???ä¸??äº?"
+ * <ol>
+ *  <li>ChineseAnalyzer: ??????ä¸???½ï?äº?</li>
+ *  <li>CJKAnalyzer: ???ï¼??ä¸??ä¸??ï¼??äº?</li>
+ *  <li>SmartChineseAnalyzer: ??????ä¸??ï¼?ºº</li>
+ * </ol>
+ * </p>
+ */
+package org.apache.lucene.analysis.cjk;
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/package.html b/lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/package.html
deleted file mode 100644
index 38b3029..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/package.html
+++ /dev/null
@@ -1,42 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
-</head>
-<body>
-Analyzer for Chinese, Japanese, and Korean, which indexes bigrams. 
-This analyzer generates bigram terms, which are overlapping groups of two adjacent Han, Hiragana, Katakana, or Hangul characters.
-<p>
-Three analyzers are provided for Chinese, each of which treats Chinese text in a different way.
-<ul>
-	<li>ChineseAnalyzer (in the analyzers/cn package): Index unigrams (individual Chinese characters) as a token.
-	<li>CJKAnalyzer (in this package): Index bigrams (overlapping groups of two adjacent Chinese characters) as tokens.
-	<li>SmartChineseAnalyzer (in the analyzers/smartcn package): Index words (attempt to segment Chinese text into words) as tokens.
-</ul>
-
-Example phraseï¼? "???ä¸??äº?"
-<ol>
-	<li>ChineseAnalyzer: ??????ä¸???½ï?äº?</li>
-	<li>CJKAnalyzer: ???ï¼??ä¸??ä¸??ï¼??äº?</li>
-	<li>SmartChineseAnalyzer: ??????ä¸??ï¼?ºº</li>
-</ol>
-</p>
-
-</body>
-</html>
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ckb/package-info.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ckb/package-info.java
new file mode 100644
index 0000000..5fccddf
--- /dev/null
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ckb/package-info.java
@@ -0,0 +1,21 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Analyzer for Sorani Kurdish.
+ */
+package org.apache.lucene.analysis.ckb;
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ckb/package.html b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ckb/package.html
deleted file mode 100644
index 4b11c3a..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ckb/package.html
+++ /dev/null
@@ -1,22 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html><head></head>
-<body>
-Analyzer for Sorani Kurdish.
-</body>
-</html>
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/commongrams/package-info.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/commongrams/package-info.java
new file mode 100644
index 0000000..4c9eaba
--- /dev/null
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/commongrams/package-info.java
@@ -0,0 +1,21 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Construct n-grams for frequently occurring terms and phrases.
+ */
+package org.apache.lucene.analysis.commongrams;
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/commongrams/package.html b/lucene/analysis/common/src/java/org/apache/lucene/analysis/commongrams/package.html
deleted file mode 100644
index a729d58..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/commongrams/package.html
+++ /dev/null
@@ -1,22 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html><head></head>
-<body>
-Construct n-grams for frequently occurring terms and phrases.
-</body>
-</html>
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/hyphenation/package-info.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/hyphenation/package-info.java
new file mode 100644
index 0000000..27de951
--- /dev/null
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/hyphenation/package-info.java
@@ -0,0 +1,24 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** 
+ * Hyphenation code for the CompoundWordTokenFilter. 
+ * The code for the compound word hyphenation is taken from the 
+ * <a href="http://xmlgraphics.apache.org/fop/">Apache FOP project</a>. 
+ * All credits for the hyphenation code belongs to them.
+ */
+package org.apache.lucene.analysis.compound.hyphenation;
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/hyphenation/package.html b/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/hyphenation/package.html
deleted file mode 100644
index 3ed4ef0..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/hyphenation/package.html
+++ /dev/null
@@ -1,27 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-
-<html>
-  <head>
-    <title>Hyphenation code for the CompoundWordTokenFilter</title>
-  </head>
-  <body>
-    <p>
-       The code for the compound word hyphenation is taken from the <a href="http://xmlgraphics.apache.org/fop/">Apache FOP project</a>. All credits for the hyphenation code belongs to them.
-    </p>
-  </body>
-</html>
\ No newline at end of file
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/package-info.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/package-info.java
new file mode 100644
index 0000000..96d2183
--- /dev/null
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/package-info.java
@@ -0,0 +1,195 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * A filter that decomposes compound words you find in many Germanic
+ * languages into the word parts. This example shows what it does:
+ * <table border="1" summary="example input stream">
+ *  <tr>
+ *   <th>Input token stream</th>
+ *  </tr>
+ *  <tr>
+ *   <td>Rindfleisch&uuml;berwachungsgesetz Drahtschere abba</td>
+ *  </tr>
+ * </table>
+ * <br>
+ * <table border="1" summary="example output stream">
+ *  <tr>
+ *   <th>Output token stream</th>
+ *  </tr>
+ *  <tr>
+ *   <td>(Rindfleisch&uuml;berwachungsgesetz,0,29)</td>
+ *  </tr>
+ *  <tr>
+ *   <td>(Rind,0,4,posIncr=0)</td>
+ *  </tr>
+ *  <tr>
+ *   <td>(fleisch,4,11,posIncr=0)</td>
+ *  </tr>
+ *  <tr>
+ *   <td>(&uuml;berwachung,11,22,posIncr=0)</td>
+ *  </tr>
+ *  <tr>
+ *   <td>(gesetz,23,29,posIncr=0)</td>
+ *  </tr>
+ *  <tr>
+ *   <td>(Drahtschere,30,41)</td>
+ *  </tr>
+ *  <tr>
+ *   <td>(Draht,30,35,posIncr=0)</td>
+ *  </tr>
+ *  <tr>
+ *   <td>(schere,35,41,posIncr=0)</td>
+ *  </tr>
+ *  <tr>
+ *   <td>(abba,42,46)</td>
+ *  </tr>
+ * </table>
+ * 
+ * The input token is always preserved and the filters do not alter the case of word parts. There are two variants of the
+ * filter available:
+ * <ul>
+ *  <li><i>HyphenationCompoundWordTokenFilter</i>: it uses a
+ *  hyphenation grammar based approach to find potential word parts of a
+ *  given word.</li>
+ *  <li><i>DictionaryCompoundWordTokenFilter</i>: it uses a
+ *  brute-force dictionary-only based approach to find the word parts of a given
+ *  word.</li>
+ * </ul>
+ * 
+ * <h3>Compound word token filters</h3>
+ * <h4>HyphenationCompoundWordTokenFilter</h4>
+ * The {@link
+ * org.apache.lucene.analysis.compound.HyphenationCompoundWordTokenFilter
+ * HyphenationCompoundWordTokenFilter} uses hyphenation grammars to find
+ * potential subwords that a worth to check against the dictionary. It can be used
+ * without a dictionary as well but then produces a lot of "nonword" tokens.
+ * The quality of the output tokens is directly connected to the quality of the
+ * grammar file you use. For languages like German they are quite good.
+ * <h5>Grammar file</h5>
+ * Unfortunately we cannot bundle the hyphenation grammar files with Lucene
+ * because they do not use an ASF compatible license (they use the LaTeX
+ * Project Public License instead). You can find the XML based grammar
+ * files at the
+ * <a href="http://offo.sourceforge.net/hyphenation/index.html">Objects
+ * For Formatting Objects</a>
+ * (OFFO) Sourceforge project (direct link to download the pattern files:
+ * <a href="http://downloads.sourceforge.net/offo/offo-hyphenation.zip">http://downloads.sourceforge.net/offo/offo-hyphenation.zip</a>
+ * ). The files you need are in the subfolder
+ * <i>offo-hyphenation/hyph/</i>
+ * .
+ * <br />
+ * Credits for the hyphenation code go to the
+ * <a href="http://xmlgraphics.apache.org/fop/">Apache FOP project</a>
+ * .
+ * 
+ * <h4>DictionaryCompoundWordTokenFilter</h4>
+ * The {@link
+ * org.apache.lucene.analysis.compound.DictionaryCompoundWordTokenFilter
+ * DictionaryCompoundWordTokenFilter} uses a dictionary-only approach to
+ * find subwords in a compound word. It is much slower than the one that
+ * uses the hyphenation grammars. You can use it as a first start to
+ * see if your dictionary is good or not because it is much simpler in design.
+ * 
+ * <h3>Dictionary</h3>
+ * The output quality of both token filters is directly connected to the
+ * quality of the dictionary you use. They are language dependent of course.
+ * You always should use a dictionary
+ * that fits to the text you want to index. If you index medical text for
+ * example then you should use a dictionary that contains medical words.
+ * A good start for general text are the dictionaries you find at the
+ * <a href="http://wiki.services.openoffice.org/wiki/Dictionaries">OpenOffice
+ * dictionaries</a>
+ * Wiki.
+ * 
+ * <h3>Which variant should I use?</h3>
+ * This decision matrix should help you:
+ * <table border="1" summary="comparison of dictionary and hyphenation based decompounding">
+ *  <tr>
+ *   <th>Token filter</th>
+ *   <th>Output quality</th>
+ *   <th>Performance</th>
+ *  </tr>
+ *  <tr>
+ *   <td>HyphenationCompoundWordTokenFilter</td>
+ *   <td>good if grammar file is good &ndash; acceptable otherwise</td>
+ *   <td>fast</td>
+ *  </tr>
+ *  <tr>
+ *   <td>DictionaryCompoundWordTokenFilter</td>
+ *   <td>good</td>
+ *   <td>slow</td>
+ *  </tr>
+ * </table>
+ * <h3>Examples</h3>
+ * <pre class="prettyprint">
+ *   public void testHyphenationCompoundWordsDE() throws Exception {
+ *     String[] dict = { "Rind", "Fleisch", "Draht", "Schere", "Gesetz",
+ *         "Aufgabe", "&Uuml;berwachung" };
+ * 
+ *     Reader reader = new FileReader("de_DR.xml");
+ * 
+ *     HyphenationTree hyphenator = HyphenationCompoundWordTokenFilter
+ *         .getHyphenationTree(reader);
+ * 
+ *     HyphenationCompoundWordTokenFilter tf = new HyphenationCompoundWordTokenFilter(
+ *         new WhitespaceTokenizer(new StringReader(
+ *             "Rindfleisch&uuml;berwachungsgesetz Drahtschere abba")), hyphenator,
+ *         dict, CompoundWordTokenFilterBase.DEFAULT_MIN_WORD_SIZE,
+ *         CompoundWordTokenFilterBase.DEFAULT_MIN_SUBWORD_SIZE,
+ *         CompoundWordTokenFilterBase.DEFAULT_MAX_SUBWORD_SIZE, false);
+ *         
+ *     CharTermAttribute t = tf.addAttribute(CharTermAttribute.class);
+ *     while (tf.incrementToken()) {
+ *        System.out.println(t);
+ *     }
+ *   }
+ * 
+ *   public void testHyphenationCompoundWordsWithoutDictionaryDE() throws Exception {
+ *     Reader reader = new FileReader("de_DR.xml");
+ * 
+ *     HyphenationTree hyphenator = HyphenationCompoundWordTokenFilter
+ *         .getHyphenationTree(reader);
+ * 
+ *     HyphenationCompoundWordTokenFilter tf = new HyphenationCompoundWordTokenFilter(
+ *         new WhitespaceTokenizer(new StringReader(
+ *             "Rindfleisch&uuml;berwachungsgesetz Drahtschere abba")), hyphenator);
+ *         
+ *     CharTermAttribute t = tf.addAttribute(CharTermAttribute.class);
+ *     while (tf.incrementToken()) {
+ *        System.out.println(t);
+ *     }
+ *   }
+ *   
+ *   public void testDumbCompoundWordsSE() throws Exception {
+ *     String[] dict = { "Bil", "D&ouml;rr", "Motor", "Tak", "Borr", "Slag", "Hammar",
+ *         "Pelar", "Glas", "&Ouml;gon", "Fodral", "Bas", "Fiol", "Makare", "Ges&auml;ll",
+ *         "Sko", "Vind", "Rute", "Torkare", "Blad" };
+ * 
+ *     DictionaryCompoundWordTokenFilter tf = new DictionaryCompoundWordTokenFilter(
+ *         new WhitespaceTokenizer(
+ *             new StringReader(
+ *                 "Bild&ouml;rr Bilmotor Biltak Slagborr Hammarborr Pelarborr Glas&ouml;gonfodral Basfiolsfodral Basfiolsfodralmakareges&auml;ll Skomakare Vindrutetorkare Vindrutetorkarblad abba")),
+ *         dict);
+ *     CharTermAttribute t = tf.addAttribute(CharTermAttribute.class);
+ *     while (tf.incrementToken()) {
+ *        System.out.println(t);
+ *     }
+ *   }
+ * </pre>
+ */
+package org.apache.lucene.analysis.compound;
\ No newline at end of file
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/package.html b/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/package.html
deleted file mode 100644
index 453f386..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/package.html
+++ /dev/null
@@ -1,200 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-<title>CompoundWordTokenFilter</title>
-<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"></meta>
-</head>
-<body>
-A filter that decomposes compound words you find in many Germanic
-languages into the word parts. This example shows what it does:
-<table border="1">
-	<tr>
-		<th>Input token stream</th>
-	</tr>
-	<tr>
-		<td>Rindfleisch&uuml;berwachungsgesetz Drahtschere abba</td>
-	</tr>
-</table>
-<br>
-<table border="1">
-	<tr>
-		<th>Output token stream</th>
-	</tr>
-	<tr>
-		<td>(Rindfleisch&uuml;berwachungsgesetz,0,29)</td>
-	</tr>
-	<tr>
-		<td>(Rind,0,4,posIncr=0)</td>
-	</tr>
-	<tr>
-		<td>(fleisch,4,11,posIncr=0)</td>
-	</tr>
-	<tr>
-		<td>(&uuml;berwachung,11,22,posIncr=0)</td>
-	</tr>
-	<tr>
-		<td>(gesetz,23,29,posIncr=0)</td>
-	</tr>
-	<tr>
-		<td>(Drahtschere,30,41)</td>
-	</tr>
-	<tr>
-		<td>(Draht,30,35,posIncr=0)</td>
-	</tr>
-	<tr>
-		<td>(schere,35,41,posIncr=0)</td>
-	</tr>
-	<tr>
-		<td>(abba,42,46)</td>
-	</tr>
-</table>
-
-The input token is always preserved and the filters do not alter the case of word parts. There are two variants of the
-filter available:
-<ul>
-	<li><i>HyphenationCompoundWordTokenFilter</i>: it uses a
-	hyphenation grammar based approach to find potential word parts of a
-	given word.</li>
-	<li><i>DictionaryCompoundWordTokenFilter</i>: it uses a
-	brute-force dictionary-only based approach to find the word parts of a given
-	word.</li>
-</ul>
-
-<h3>Compound word token filters</h3>
-<h4>HyphenationCompoundWordTokenFilter</h4>
-The {@link
-org.apache.lucene.analysis.compound.HyphenationCompoundWordTokenFilter
-HyphenationCompoundWordTokenFilter} uses hyphenation grammars to find
-potential subwords that a worth to check against the dictionary. It can be used
-without a dictionary as well but then produces a lot of "nonword" tokens.
-The quality of the output tokens is directly connected to the quality of the
-grammar file you use. For languages like German they are quite good.
-<h5>Grammar file</h5>
-Unfortunately we cannot bundle the hyphenation grammar files with Lucene
-because they do not use an ASF compatible license (they use the LaTeX
-Project Public License instead). You can find the XML based grammar
-files at the
-<a href="http://offo.sourceforge.net/hyphenation/index.html">Objects
-For Formatting Objects</a>
-(OFFO) Sourceforge project (direct link to download the pattern files:
-<a href="http://downloads.sourceforge.net/offo/offo-hyphenation.zip">http://downloads.sourceforge.net/offo/offo-hyphenation.zip</a>
-). The files you need are in the subfolder
-<i>offo-hyphenation/hyph/</i>
-.
-<br />
-Credits for the hyphenation code go to the
-<a href="http://xmlgraphics.apache.org/fop/">Apache FOP project</a>
-.
-
-<h4>DictionaryCompoundWordTokenFilter</h4>
-The {@link
-org.apache.lucene.analysis.compound.DictionaryCompoundWordTokenFilter
-DictionaryCompoundWordTokenFilter} uses a dictionary-only approach to
-find subwords in a compound word. It is much slower than the one that
-uses the hyphenation grammars. You can use it as a first start to
-see if your dictionary is good or not because it is much simpler in design.
-
-<h3>Dictionary</h3>
-The output quality of both token filters is directly connected to the
-quality of the dictionary you use. They are language dependent of course.
-You always should use a dictionary
-that fits to the text you want to index. If you index medical text for
-example then you should use a dictionary that contains medical words.
-A good start for general text are the dictionaries you find at the
-<a href="http://wiki.services.openoffice.org/wiki/Dictionaries">OpenOffice
-dictionaries</a>
-Wiki.
-
-<h3>Which variant should I use?</h3>
-This decision matrix should help you:
-<table border="1">
-	<tr>
-		<th>Token filter</th>
-		<th>Output quality</th>
-		<th>Performance</th>
-	</tr>
-	<tr>
-		<td>HyphenationCompoundWordTokenFilter</td>
-		<td>good if grammar file is good &ndash; acceptable otherwise</td>
-		<td>fast</td>
-	</tr>
-	<tr>
-		<td>DictionaryCompoundWordTokenFilter</td>
-		<td>good</td>
-		<td>slow</td>
-	</tr>
-</table>
-<h3>Examples</h3>
-<pre class="prettyprint">
-  public void testHyphenationCompoundWordsDE() throws Exception {
-    String[] dict = { "Rind", "Fleisch", "Draht", "Schere", "Gesetz",
-        "Aufgabe", "&Uuml;berwachung" };
-
-    Reader reader = new FileReader("de_DR.xml");
-
-    HyphenationTree hyphenator = HyphenationCompoundWordTokenFilter
-        .getHyphenationTree(reader);
-
-    HyphenationCompoundWordTokenFilter tf = new HyphenationCompoundWordTokenFilter(
-        new WhitespaceTokenizer(new StringReader(
-            "Rindfleisch&uuml;berwachungsgesetz Drahtschere abba")), hyphenator,
-        dict, CompoundWordTokenFilterBase.DEFAULT_MIN_WORD_SIZE,
-        CompoundWordTokenFilterBase.DEFAULT_MIN_SUBWORD_SIZE,
-        CompoundWordTokenFilterBase.DEFAULT_MAX_SUBWORD_SIZE, false);
-        
-    CharTermAttribute t = tf.addAttribute(CharTermAttribute.class);
-    while (tf.incrementToken()) {
-       System.out.println(t);
-    }
-  }
-
-  public void testHyphenationCompoundWordsWithoutDictionaryDE() throws Exception {
-    Reader reader = new FileReader("de_DR.xml");
-
-    HyphenationTree hyphenator = HyphenationCompoundWordTokenFilter
-        .getHyphenationTree(reader);
-
-    HyphenationCompoundWordTokenFilter tf = new HyphenationCompoundWordTokenFilter(
-        new WhitespaceTokenizer(new StringReader(
-            "Rindfleisch&uuml;berwachungsgesetz Drahtschere abba")), hyphenator);
-        
-    CharTermAttribute t = tf.addAttribute(CharTermAttribute.class);
-    while (tf.incrementToken()) {
-       System.out.println(t);
-    }
-  }
-  
-  public void testDumbCompoundWordsSE() throws Exception {
-    String[] dict = { "Bil", "D&ouml;rr", "Motor", "Tak", "Borr", "Slag", "Hammar",
-        "Pelar", "Glas", "&Ouml;gon", "Fodral", "Bas", "Fiol", "Makare", "Ges&auml;ll",
-        "Sko", "Vind", "Rute", "Torkare", "Blad" };
-
-    DictionaryCompoundWordTokenFilter tf = new DictionaryCompoundWordTokenFilter(
-        new WhitespaceTokenizer(
-            new StringReader(
-                "Bild&ouml;rr Bilmotor Biltak Slagborr Hammarborr Pelarborr Glas&ouml;gonfodral Basfiolsfodral Basfiolsfodralmakareges&auml;ll Skomakare Vindrutetorkare Vindrutetorkarblad abba")),
-        dict);
-    CharTermAttribute t = tf.addAttribute(CharTermAttribute.class);
-    while (tf.incrementToken()) {
-       System.out.println(t);
-    }
-  }
-</pre>
-</body>
-</html>
\ No newline at end of file
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/package-info.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/package-info.java
new file mode 100644
index 0000000..7aba1e2
--- /dev/null
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/package-info.java
@@ -0,0 +1,21 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** 
+ * Basic, general-purpose analysis components.
+ */
+package org.apache.lucene.analysis.core;
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/package.html b/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/package.html
deleted file mode 100644
index 0e0ca06..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/package.html
+++ /dev/null
@@ -1,22 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html><head></head>
-<body>
-Basic, general-purpose analysis components.
-</body>
-</html>
diff --git a/lucene/analysis/icu/src/java/org/apache/lucene/collation/package.html b/lucene/analysis/icu/src/java/org/apache/lucene/collation/package.html
index 092cd41..2c81f4a 100644
--- a/lucene/analysis/icu/src/java/org/apache/lucene/collation/package.html
+++ b/lucene/analysis/icu/src/java/org/apache/lucene/collation/package.html
@@ -15,6 +15,7 @@
  See the License for the specific language governing permissions and
  limitations under the License.
 -->
+<!-- not a package-info.java, because we already defined this package in common/ -->
 <html>
 <body>
 Unicode Collation support.
diff --git a/lucene/analysis/icu/src/java/org/apache/lucene/collation/tokenattributes/package.html b/lucene/analysis/icu/src/java/org/apache/lucene/collation/tokenattributes/package.html
index a6966c9..6f21117 100644
--- a/lucene/analysis/icu/src/java/org/apache/lucene/collation/tokenattributes/package.html
+++ b/lucene/analysis/icu/src/java/org/apache/lucene/collation/tokenattributes/package.html
@@ -15,6 +15,7 @@
  See the License for the specific language governing permissions and
  limitations under the License.
 -->
+<!-- not a package-info.java, because we already defined this package in common/ -->
 <html>
 <body>
 Custom {@link org.apache.lucene.util.AttributeImpl} for indexing collation keys as index terms.
diff --git a/lucene/core/src/java/org/apache/lucene/analysis/package-info.java b/lucene/core/src/java/org/apache/lucene/analysis/package-info.java
new file mode 100644
index 0000000..396aced
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/analysis/package-info.java
@@ -0,0 +1,1001 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Text analysis. 
+ * <p>API and code to convert text into indexable/searchable tokens.  Covers {@link org.apache.lucene.analysis.Analyzer} and related classes.</p>
+ * <h2>Parsing? Tokenization? Analysis!</h2>
+ * <p>
+ * Lucene, an indexing and search library, accepts only plain text input.
+ * <p>
+ * <h2>Parsing</h2>
+ * <p>
+ * Applications that build their search capabilities upon Lucene may support documents in various formats &ndash; HTML, XML, PDF, Word &ndash; just to name a few.
+ * Lucene does not care about the <i>Parsing</i> of these and other document formats, and it is the responsibility of the 
+ * application using Lucene to use an appropriate <i>Parser</i> to convert the original format into plain text before passing that plain text to Lucene.
+ * <p>
+ * <h2>Tokenization</h2>
+ * <p>
+ * Plain text passed to Lucene for indexing goes through a process generally called tokenization. Tokenization is the process
+ * of breaking input text into small indexing elements &ndash; tokens.
+ * The way input text is broken into tokens heavily influences how people will then be able to search for that text. 
+ * For instance, sentences beginnings and endings can be identified to provide for more accurate phrase 
+ * and proximity searches (though sentence identification is not provided by Lucene).
+ * <p>
+ *   In some cases simply breaking the input text into tokens is not enough
+ *   &ndash; a deeper <i>Analysis</i> may be needed. Lucene includes both
+ *   pre- and post-tokenization analysis facilities.
+ * </p>
+ * <p>
+ *   Pre-tokenization analysis can include (but is not limited to) stripping
+ *   HTML markup, and transforming or removing text matching arbitrary patterns
+ *   or sets of fixed strings.
+ * </p>
+ * <p>
+ *   There are many post-tokenization steps that can be done, including 
+ *   (but not limited to):
+ * </p>
+ * <ul>
+ *   <li><a href="http://en.wikipedia.org/wiki/Stemming">Stemming</a> &ndash; 
+ *       Replacing words with their stems. 
+ *       For instance with English stemming "bikes" is replaced with "bike"; 
+ *       now query "bike" can find both documents containing "bike" and those containing "bikes".
+ *   </li>
+ *   <li><a href="http://en.wikipedia.org/wiki/Stop_words">Stop Words Filtering</a> &ndash; 
+ *       Common words like "the", "and" and "a" rarely add any value to a search.
+ *       Removing them shrinks the index size and increases performance.
+ *       It may also reduce some "noise" and actually improve search quality.
+ *   </li>
+ *   <li><a href="http://en.wikipedia.org/wiki/Text_normalization">Text Normalization</a> &ndash; 
+ *       Stripping accents and other character markings can make for better searching.
+ *   </li>
+ *   <li><a href="http://en.wikipedia.org/wiki/Synonym">Synonym Expansion</a> &ndash; 
+ *       Adding in synonyms at the same token position as the current word can mean better 
+ *       matching when users search with words in the synonym set.
+ *   </li>
+ * </ul> 
+ * <p>
+ * <h2>Core Analysis</h2>
+ * <p>
+ *   The analysis package provides the mechanism to convert Strings and Readers
+ *   into tokens that can be indexed by Lucene.  There are four main classes in 
+ *   the package from which all analysis processes are derived.  These are:
+ * </p>
+ * <ul>
+ *   <li>
+ *     {@link org.apache.lucene.analysis.Analyzer} &ndash; An <code>Analyzer</code> is 
+ *     responsible for supplying a
+ *     {@link org.apache.lucene.analysis.TokenStream} which can be consumed
+ *     by the indexing and searching processes.  See below for more information
+ *     on implementing your own {@link org.apache.lucene.analysis.Analyzer}. Most of the time, you can use
+ *     an anonymous subclass of {@link org.apache.lucene.analysis.Analyzer}.
+ *   </li>
+ *   <li>
+ *     {@link org.apache.lucene.analysis.CharFilter} &ndash; <code>CharFilter</code> extends
+ *     {@link java.io.Reader} to transform the text before it is
+ *     tokenized, while providing
+ *     corrected character offsets to account for these modifications.  This
+ *     capability allows highlighting to function over the original text when 
+ *     indexed tokens are created from <code>CharFilter</code>-modified text with offsets
+ *     that are not the same as those in the original text. {@link org.apache.lucene.analysis.Tokenizer#setReader(java.io.Reader)}
+ *     accept <code>CharFilter</code>s.  <code>CharFilter</code>s may
+ *     be chained to perform multiple pre-tokenization modifications.
+ *   </li>
+ *   <li>
+ *     {@link org.apache.lucene.analysis.Tokenizer} &ndash; A <code>Tokenizer</code> is a 
+ *     {@link org.apache.lucene.analysis.TokenStream} and is responsible for
+ *     breaking up incoming text into tokens. In many cases, an {@link org.apache.lucene.analysis.Analyzer} will
+ *     use a {@link org.apache.lucene.analysis.Tokenizer} as the first step in the analysis process.  However,
+ *     to modify text prior to tokenization, use a {@link org.apache.lucene.analysis.CharFilter} subclass (see
+ *     above).
+ *   </li>
+ *   <li>
+ *     {@link org.apache.lucene.analysis.TokenFilter} &ndash; A <code>TokenFilter</code> is
+ *     a {@link org.apache.lucene.analysis.TokenStream} and is responsible
+ *     for modifying tokens that have been created by the <code>Tokenizer</code>. Common 
+ *     modifications performed by a <code>TokenFilter</code> are: deletion, stemming, synonym 
+ *     injection, and case folding.  Not all <code>Analyzer</code>s require <code>TokenFilter</code>s.
+ *   </li>
+ * </ul>
+ * <h2>Hints, Tips and Traps</h2>
+ * <p>
+ *   The relationship between {@link org.apache.lucene.analysis.Analyzer} and 
+ *   {@link org.apache.lucene.analysis.CharFilter}s,
+ *   {@link org.apache.lucene.analysis.Tokenizer}s,
+ *   and {@link org.apache.lucene.analysis.TokenFilter}s is sometimes confusing. To ease
+ *   this confusion, here is some clarifications:
+ * </p>
+ * <ul>
+ *   <li>
+ *     The {@link org.apache.lucene.analysis.Analyzer} is a
+ *     <strong>factory</strong> for analysis chains. <code>Analyzer</code>s don't
+ *     process text, <code>Analyzer</code>s construct <code>CharFilter</code>s, <code>Tokenizer</code>s, and/or
+ *     <code>TokenFilter</code>s that process text. An <code>Analyzer</code> has two tasks: 
+ *     to produce {@link org.apache.lucene.analysis.TokenStream}s that accept a
+ *     reader and produces tokens, and to wrap or otherwise
+ *     pre-process {@link java.io.Reader} objects.
+ *   </li>
+ *   <li>
+ *   The {@link org.apache.lucene.analysis.CharFilter} is a subclass of
+ *  {@link java.io.Reader} that supports offset tracking.
+ *   </li>
+ *   <li>The{@link org.apache.lucene.analysis.Tokenizer}
+ *     is only responsible for <u>breaking</u> the input text into tokens.
+ *   </li>
+ *   <li>The{@link org.apache.lucene.analysis.TokenFilter} modifies a
+ *   stream of tokens and their contents.
+ *   </li>
+ *   <li>
+ *     {@link org.apache.lucene.analysis.Tokenizer} is a {@link org.apache.lucene.analysis.TokenStream}, 
+ *     but {@link org.apache.lucene.analysis.Analyzer} is not.
+ *   </li>
+ *   <li>
+ *     {@link org.apache.lucene.analysis.Analyzer} is "field aware", but 
+ *     {@link org.apache.lucene.analysis.Tokenizer} is not. {@link org.apache.lucene.analysis.Analyzer}s may
+ *     take a field name into account when constructing the {@link org.apache.lucene.analysis.TokenStream}.
+ *   </li>
+ * </ul>
+ * <p>
+ *   If you want to use a particular combination of <code>CharFilter</code>s, a
+ *   <code>Tokenizer</code>, and some <code>TokenFilter</code>s, the simplest thing is often an
+ *   create an anonymous subclass of {@link org.apache.lucene.analysis.Analyzer}, provide {@link
+ *   org.apache.lucene.analysis.Analyzer#createComponents(String)} and perhaps also
+ *   {@link org.apache.lucene.analysis.Analyzer#initReader(String,
+ *   java.io.Reader)}. However, if you need the same set of components
+ *   over and over in many places, you can make a subclass of
+ *   {@link org.apache.lucene.analysis.Analyzer}. In fact, Apache Lucene
+ *   supplies a large family of <code>Analyzer</code> classes that deliver useful
+ *   analysis chains. The most common of these is the <a href="{@docRoot}/../analyzers-common/org/apache/lucene/analysis/standard/StandardAnalyzer.html">StandardAnalyzer</a>.
+ *   Many applications will have a long and industrious life with nothing more
+ *   than the <code>StandardAnalyzer</code>. The <a href="{@docRoot}/../analyzers-common/overview-summary.html">analyzers-common</a>
+ *   library provides many pre-existing analyzers for various languages.
+ *   The analysis-common library also allows to configure a custom Analyzer without subclassing using the
+ *   <a href="{@docRoot}/../analyzers-common/org/apache/lucene/analysis/custom/CustomAnalyzer.html">CustomAnalyzer</a>
+ *   class.
+ * </p>
+ * <p>
+ *   Aside from the <code>StandardAnalyzer</code>,
+ *   Lucene includes several components containing analysis components,
+ *   all under the 'analysis' directory of the distribution. Some of
+ *   these support particular languages, others integrate external
+ *   components. The 'common' subdirectory has some noteworthy
+ *  general-purpose analyzers, including the <a href="{@docRoot}/../analyzers-common/org/apache/lucene/analysis/miscellaneous/PerFieldAnalyzerWrapper.html">PerFieldAnalyzerWrapper</a>. Most <code>Analyzer</code>s perform the same operation on all
+ *  {@link org.apache.lucene.document.Field}s.  The PerFieldAnalyzerWrapper can be used to associate a different <code>Analyzer</code> with different
+ *  {@link org.apache.lucene.document.Field}s. There is a great deal of
+ *  functionality in the analysis area, you should study it carefully to
+ *  find the pieces you need.
+ * </p>
+ * <p>
+ *   Analysis is one of the main causes of slow indexing.  Simply put, the more you analyze the slower the indexing (in most cases).
+ *   Perhaps your application would be just fine using the simple WhitespaceTokenizer combined with a StopFilter. The benchmark/ library can be useful 
+ *   for testing out the speed of the analysis process.
+ * </p>
+ * <h2>Invoking the Analyzer</h2>
+ * <p>
+ *   Applications usually do not invoke analysis &ndash; Lucene does it
+ *  for them. Applications construct <code>Analyzer</code>s and pass then into Lucene,
+ *  as follows:
+ * </p>
+ * <ul>
+ *   <li>
+ *     At indexing, as a consequence of 
+ *     {@link org.apache.lucene.index.IndexWriter#addDocument(org.apache.lucene.index.IndexDocument) addDocument(doc)},
+ *     the <code>Analyzer</code> in effect for indexing is invoked for each indexed field of the added document.
+ *   </li>
+ *   <li>
+ *     At search, a <code>QueryParser</code> may invoke the Analyzer during parsing.  Note that for some queries, analysis does not
+ *     take place, e.g. wildcard queries.
+ *   </li>
+ * </ul>
+ * <p>
+ *   However an application might invoke Analysis of any text for testing or for any other purpose, something like:
+ * </p>
+ * <PRE class="prettyprint" id="analysis-workflow">
+ *     Version matchVersion = Version.LUCENE_XY; // Substitute desired Lucene version for XY
+ *     Analyzer analyzer = new StandardAnalyzer(matchVersion); // or any other analyzer
+ *     TokenStream ts = analyzer.tokenStream("myfield", new StringReader("some text goes here"));
+ *     // The Analyzer class will construct the Tokenizer, TokenFilter(s), and CharFilter(s),
+ *     //   and pass the resulting Reader to the Tokenizer.
+ *     OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);
+ *     
+ *     try {
+ *       ts.reset(); // Resets this stream to the beginning. (Required)
+ *       while (ts.incrementToken()) {
+ *         // Use {@link org.apache.lucene.util.AttributeSource#reflectAsString(boolean)}
+ *         // for token stream debugging.
+ *         System.out.println("token: " + ts.reflectAsString(true));
+ * 
+ *         System.out.println("token start offset: " + offsetAtt.startOffset());
+ *         System.out.println("  token end offset: " + offsetAtt.endOffset());
+ *       }
+ *       ts.end();   // Perform end-of-stream operations, e.g. set the final offset.
+ *     } finally {
+ *       ts.close(); // Release resources associated with this stream.
+ *     }
+ * </PRE>
+ * <h2>Indexing Analysis vs. Search Analysis</h2>
+ * <p>
+ *   Selecting the "correct" analyzer is crucial
+ *   for search quality, and can also affect indexing and search performance.
+ *   The "correct" analyzer for your application will depend on what your input text
+ *   looks like and what problem you are trying to solve.
+ *   Lucene java's wiki page 
+ *   <a href="http://wiki.apache.org/lucene-java/AnalysisParalysis">AnalysisParalysis</a> 
+ *   provides some data on "analyzing your analyzer".
+ *   Here are some rules of thumb:
+ *   <ol>
+ *     <li>Test test test... (did we say test?)</li>
+ *     <li>Beware of too much analysis &ndash; it might hurt indexing performance.</li>
+ *     <li>Start with the same analyzer for indexing and search, otherwise searches would not find what they are supposed to...</li>
+ *     <li>In some cases a different analyzer is required for indexing and search, for instance:
+ *         <ul>
+ *            <li>Certain searches require more stop words to be filtered. (i.e. more than those that were filtered at indexing.)</li>
+ *            <li>Query expansion by synonyms, acronyms, auto spell correction, etc.</li>
+ *         </ul>
+ *         This might sometimes require a modified analyzer &ndash; see the next section on how to do that.
+ *     </li>
+ *   </ol>
+ * </p>
+ * <h2>Implementing your own Analyzer and Analysis Components</h2>
+ * <p>
+ *   Creating your own Analyzer is straightforward. Your Analyzer should subclass {@link org.apache.lucene.analysis.Analyzer}. It can use
+ *   existing analysis components &mdash; CharFilter(s) <i>(optional)</i>, a
+ *   Tokenizer, and TokenFilter(s) <i>(optional)</i> &mdash; or components you
+ *   create, or a combination of existing and newly created components.  Before
+ *   pursuing this approach, you may find it worthwhile to explore the
+ *   <a href="{@docRoot}/../analyzers-common/overview-summary.html">analyzers-common</a> library and/or ask on the 
+ *   <a href="http://lucene.apache.org/core/discussion.html">java-user@lucene.apache.org mailing list</a> first to see if what you
+ *   need already exists. If you are still committed to creating your own
+ *   Analyzer, have a look at the source code of any one of the many samples
+ *   located in this package.
+ * </p>
+ * <p>
+ *   The following sections discuss some aspects of implementing your own analyzer.
+ * </p>
+ * <h3>Field Section Boundaries</h3>
+ * <p>
+ *   When {@link org.apache.lucene.document.Document#add(org.apache.lucene.document.Field) document.add(field)}
+ *   is called multiple times for the same field name, we could say that each such call creates a new 
+ *   section for that field in that document. 
+ *   In fact, a separate call to 
+ *   {@link org.apache.lucene.analysis.Analyzer#tokenStream(java.lang.String, java.io.Reader) tokenStream(field,reader)}
+ *   would take place for each of these so called "sections".
+ *   However, the default Analyzer behavior is to treat all these sections as one large section. 
+ *   This allows phrase search and proximity search to seamlessly cross 
+ *   boundaries between these "sections".
+ *   In other words, if a certain field "f" is added like this:
+ * </p>
+ * <PRE class="prettyprint">
+ *     document.add(new Field("f","first ends",...);
+ *     document.add(new Field("f","starts two",...);
+ *     indexWriter.addDocument(document);
+ * </PRE>
+ * <p>
+ *   Then, a phrase search for "ends starts" would find that document.
+ *   Where desired, this behavior can be modified by introducing a "position gap" between consecutive field "sections", 
+ *   simply by overriding 
+ *   {@link org.apache.lucene.analysis.Analyzer#getPositionIncrementGap(java.lang.String) Analyzer.getPositionIncrementGap(fieldName)}:
+ * </p>
+ * <PRE class="prettyprint">
+ *   Version matchVersion = Version.LUCENE_XY; // Substitute desired Lucene version for XY
+ *   Analyzer myAnalyzer = new StandardAnalyzer(matchVersion) {
+ *     public int getPositionIncrementGap(String fieldName) {
+ *       return 10;
+ *     }
+ *   };
+ * </PRE>
+ * <h3>End of Input Cleanup</h3>
+ * <p>
+ *    At the ends of each field, Lucene will call the {@link org.apache.lucene.analysis.TokenStream#end()}.
+ *    The components of the token stream (the tokenizer and the token filters) <strong>must</strong>
+ *    put accurate values into the token attributes to reflect the situation at the end of the field.
+ *    The Offset attribute must contain the final offset (the total number of characters processed)
+ *    in both start and end. Attributes like PositionLength must be correct. 
+ * </p>
+ * <p>
+ *    The base method{@link org.apache.lucene.analysis.TokenStream#end()} sets PositionIncrement to 0, which is required.
+ *    Other components must override this method to fix up the other attributes.
+ * </p>
+ * <h3>Token Position Increments</h3>
+ * <p>
+ *    By default, TokenStream arranges for the 
+ *    {@link org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute#getPositionIncrement() position increment} of all tokens to be one.
+ *    This means that the position stored for that token in the index would be one more than
+ *    that of the previous token.
+ *    Recall that phrase and proximity searches rely on position info.
+ * </p>
+ * <p>
+ *    If the selected analyzer filters the stop words "is" and "the", then for a document 
+ *    containing the string "blue is the sky", only the tokens "blue", "sky" are indexed, 
+ *    with position("sky") = 3 + position("blue"). Now, a phrase query "blue is the sky"
+ *    would find that document, because the same analyzer filters the same stop words from
+ *    that query. But the phrase query "blue sky" would not find that document because the
+ *    position increment between "blue" and "sky" is only 1.
+ * </p>
+ * <p>   
+ *    If this behavior does not fit the application needs, the query parser needs to be
+ *    configured to not take position increments into account when generating phrase queries.
+ * </p>
+ * <p>
+ *   Note that a filter that filters <strong>out</strong> tokens <strong>must</strong> increment the position increment in order not to generate corrupt
+ *   tokenstream graphs. Here is the logic used by StopFilter to increment positions when filtering out tokens:
+ * </p>
+ * <PRE class="prettyprint">
+ *   public TokenStream tokenStream(final String fieldName, Reader reader) {
+ *     final TokenStream ts = someAnalyzer.tokenStream(fieldName, reader);
+ *     TokenStream res = new TokenStream() {
+ *       CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
+ *       PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);
+ * 
+ *       public boolean incrementToken() throws IOException {
+ *         int extraIncrement = 0;
+ *         while (true) {
+ *           boolean hasNext = ts.incrementToken();
+ *           if (hasNext) {
+ *             if (stopWords.contains(termAtt.toString())) {
+ *               extraIncrement += posIncrAtt.getPositionIncrement(); // filter this word
+ *               continue;
+ *             } 
+ *             if (extraIncrement &gt; 0) {
+ *               posIncrAtt.setPositionIncrement(posIncrAtt.getPositionIncrement()+extraIncrement);
+ *             }
+ *           }
+ *           return hasNext;
+ *         }
+ *       }
+ *     };
+ *     return res;
+ *   }
+ * </PRE>
+ * <p>
+ *    A few more use cases for modifying position increments are:
+ * </p>
+ * <ol>
+ *   <li>Inhibiting phrase and proximity matches in sentence boundaries &ndash; for this, a tokenizer that 
+ *     identifies a new sentence can add 1 to the position increment of the first token of the new sentence.</li>
+ *   <li>Injecting synonyms &ndash; here, synonyms of a token should be added after that token, 
+ *     and their position increment should be set to 0.
+ *     As result, all synonyms of a token would be considered to appear in exactly the 
+ *     same position as that token, and so would they be seen by phrase and proximity searches.</li>
+ * </ol>
+ * 
+ * <h3>Token Position Length</h3>
+ * <p>
+ *    By default, all tokens created by Analyzers and Tokenizers have a
+ *    {@link org.apache.lucene.analysis.tokenattributes.PositionLengthAttribute#getPositionLength() position length} of one.
+ *    This means that the token occupies a single position. This attribute is not indexed
+ *    and thus not taken into account for positional queries, but is used by eg. suggesters.
+ * </p>
+ * <p>
+ *    The main use case for positions lengths is multi-word synonyms. With single-word
+ *    synonyms, setting the position increment to 0 is enough to denote the fact that two
+ *    words are synonyms, for example:
+ * </p>
+ * <table summary="table showing position increments of 1 and 0 for red and magenta, respectively">
+ * <tr><td>Term</td><td>red</td><td>magenta</td></tr>
+ * <tr><td>Position increment</td><td>1</td><td>0</td></tr>
+ * </table>
+ * <p>
+ *    Given that position(magenta) = 0 + position(red), they are at the same position, so anything
+ *    working with analyzers will return the exact same result if you replace "magenta" with "red"
+ *    in the input. However, multi-word synonyms are more tricky. Let's say that you want to build
+ *    a TokenStream where "IBM" is a synonym of "Internal Business Machines". Position increments
+ *    are not enough anymore:
+ * </p>
+ * <table summary="position increments where international is zero">
+ * <tr><td>Term</td><td>IBM</td><td>International</td><td>Business</td><td>Machines</td></tr>
+ * <tr><td>Position increment</td><td>1</td><td>0</td><td>1</td><td>1</td></tr>
+ * </table>
+ * <p>
+ *    The problem with this token stream is that "IBM" is at the same position as "International"
+ *    although it is a synonym with "International Business Machines" as a whole. Setting
+ *    the position increment of "Business" and "Machines" to 0 wouldn't help as it would mean
+ *    than "International" is a synonym of "Business". The only way to solve this issue is to
+ *    make "IBM" span across 3 positions, this is where position lengths come to rescue.
+ * </p>
+ * <table summary="position lengths where IBM is three">
+ * <tr><td>Term</td><td>IBM</td><td>International</td><td>Business</td><td>Machines</td></tr>
+ * <tr><td>Position increment</td><td>1</td><td>0</td><td>1</td><td>1</td></tr>
+ * <tr><td>Position length</td><td>3</td><td>1</td><td>1</td><td>1</td></tr>
+ * </table>
+ * <p>
+ *    This new attribute makes clear that "IBM" and "International Business Machines" start and end
+ *    at the same positions.
+ * </p>
+ * <a name="corrupt" />
+ * <h3>How to not write corrupt token streams</h3>
+ * <p>
+ *    There are a few rules to observe when writing custom Tokenizers and TokenFilters:
+ * </p>
+ * <ul>
+ *   <li>The first position increment must be &gt; 0.</li>
+ *   <li>Positions must not go backward.</li>
+ *   <li>Tokens that have the same start position must have the same start offset.</li>
+ *   <li>Tokens that have the same end position (taking into account the
+ *   position length) must have the same end offset.</li>
+ *   <li>Tokenizers must call {@link
+ *   org.apache.lucene.util.AttributeSource#clearAttributes()} in
+ *   incrementToken().</li>
+ *   <li>Tokenizers must override {@link
+ *   org.apache.lucene.analysis.TokenStream#end()}, and pass the final
+ *   offset (the total number of input characters processed) to both
+ *   parameters of {@link org.apache.lucene.analysis.tokenattributes.OffsetAttribute#setOffset(int, int)}.</li>
+ * </ul>
+ * <p>
+ *    Although these rules might seem easy to follow, problems can quickly happen when chaining
+ *    badly implemented filters that play with positions and offsets, such as synonym or n-grams
+ *    filters. Here are good practices for writing correct filters:
+ * </p>
+ * <ul>
+ *   <li>Token filters should not modify offsets. If you feel that your filter would need to modify offsets, then it should probably be implemented as a tokenizer.</li>
+ *   <li>Token filters should not insert positions. If a filter needs to add tokens, then they should all have a position increment of 0.</li>
+ *   <li>When they add tokens, token filters should call {@link org.apache.lucene.util.AttributeSource#clearAttributes()} first.</li>
+ *   <li>When they remove tokens, token filters should increment the position increment of the following token.</li>
+ *   <li>Token filters should preserve position lengths.</li>
+ * </ul>
+ * <h2>TokenStream API</h2>
+ * <p>
+ *   "Flexible Indexing" summarizes the effort of making the Lucene indexer
+ *   pluggable and extensible for custom index formats.  A fully customizable
+ *   indexer means that users will be able to store custom data structures on
+ *   disk. Therefore the analysis API must transport custom types of
+ *   data from the documents to the indexer. (It also supports communications
+ *   amongst the analysis components.)
+ * </p>
+ * <h3>Attribute and AttributeSource</h3>
+ * <p>
+ *   Classes {@link org.apache.lucene.util.Attribute} and 
+ *   {@link org.apache.lucene.util.AttributeSource} serve as the basis upon which 
+ *   the analysis elements of "Flexible Indexing" are implemented. An Attribute 
+ *   holds a particular piece of information about a text token. For example, 
+ *   {@link org.apache.lucene.analysis.tokenattributes.CharTermAttribute} 
+ *   contains the term text of a token, and 
+ *   {@link org.apache.lucene.analysis.tokenattributes.OffsetAttribute} contains
+ *   the start and end character offsets of a token. An AttributeSource is a 
+ *   collection of Attributes with a restriction: there may be only one instance
+ *   of each attribute type. TokenStream now extends AttributeSource, which means
+ *   that one can add Attributes to a TokenStream. Since TokenFilter extends
+ *   TokenStream, all filters are also AttributeSources.
+ * </p>
+ * <p>
+ * Lucene provides seven Attributes out of the box:
+ * </p>
+ * <table rules="all" frame="box" cellpadding="3" summary="common bundled attributes">
+ *   <tr>
+ *     <td>{@link org.apache.lucene.analysis.tokenattributes.CharTermAttribute}</td>
+ *     <td>
+ *       The term text of a token.  Implements {@link java.lang.CharSequence} 
+ *       (providing methods length() and charAt(), and allowing e.g. for direct
+ *       use with regular expression {@link java.util.regex.Matcher}s) and 
+ *       {@link java.lang.Appendable} (allowing the term text to be appended to.)
+ *     </td>
+ *   </tr>
+ *   <tr>
+ *     <td>{@link org.apache.lucene.analysis.tokenattributes.OffsetAttribute}</td>
+ *     <td>The start and end offset of a token in characters.</td>
+ *   </tr>
+ *   <tr>
+ *     <td>{@link org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute}</td>
+ *     <td>See above for detailed information about position increment.</td>
+ *   </tr>
+ *   <tr>
+ *     <td>{@link org.apache.lucene.analysis.tokenattributes.PositionLengthAttribute}</td>
+ *     <td>The number of positions occupied by a token.</td>
+ *   </tr>
+ *   <tr>
+ *     <td>{@link org.apache.lucene.analysis.tokenattributes.PayloadAttribute}</td>
+ *     <td>The payload that a Token can optionally have.</td>
+ *   </tr>
+ *   <tr>
+ *     <td>{@link org.apache.lucene.analysis.tokenattributes.TypeAttribute}</td>
+ *     <td>The type of the token. Default is 'word'.</td>
+ *   </tr>
+ *   <tr>
+ *     <td>{@link org.apache.lucene.analysis.tokenattributes.FlagsAttribute}</td>
+ *     <td>Optional flags a token can have.</td>
+ *   </tr>
+ *   <tr>
+ *     <td>{@link org.apache.lucene.analysis.tokenattributes.KeywordAttribute}</td>
+ *     <td>
+ *       Keyword-aware TokenStreams/-Filters skip modification of tokens that
+ *       return true from this attribute's isKeyword() method. 
+ *     </td>
+ *   </tr>
+ * </table>
+ * <h3>More Requirements for Analysis Component Classes</h3>
+ * Due to the historical development of the API, there are some perhaps
+ * less than obvious requirements to implement analysis components
+ * classes.
+ * <h4 id="analysis-lifetime">Token Stream Lifetime</h4>
+ * The code fragment of the <a href="#analysis-workflow">analysis workflow
+ * protocol</a> above shows a token stream being obtained, used, and then
+ * left for garbage. However, that does not mean that the components of
+ * that token stream will, in fact, be discarded. The default is just the
+ * opposite. {@link org.apache.lucene.analysis.Analyzer} applies a reuse
+ * strategy to the tokenizer and the token filters. It will reuse
+ * them. For each new input, it calls {@link org.apache.lucene.analysis.Tokenizer#setReader(java.io.Reader)} 
+ * to set the input. Your components must be prepared for this scenario,
+ * as described below.
+ * <h4>Tokenizer</h4>
+ * <ul>
+ *   <li>
+ *   You should create your tokenizer class by extending {@link org.apache.lucene.analysis.Tokenizer}.
+ *   </li>
+ *   <li>
+ *   Your tokenizer <strong>must</strong> override {@link org.apache.lucene.analysis.TokenStream#end()}.
+ *   Your implementation <strong>must</strong> call
+ *   <code>super.end()</code>. It must set a correct final offset into
+ *   the offset attribute, and finish up and other attributes to reflect
+ *   the end of the stream.
+ *   </li>
+ *   <li>
+ *   If your tokenizer overrides {@link org.apache.lucene.analysis.TokenStream#reset()}
+ *   or {@link org.apache.lucene.analysis.TokenStream#close()}, it
+ *   <strong>must</strong> call the corresponding superclass method.
+ *   </li>
+ * </ul>
+ * <h4>Token Filter</h4>
+ *   You should create your token filter class by extending {@link org.apache.lucene.analysis.TokenFilter}.
+ *   If your token filter overrides {@link org.apache.lucene.analysis.TokenStream#reset()},
+ *   {@link org.apache.lucene.analysis.TokenStream#end()}
+ *   or {@link org.apache.lucene.analysis.TokenStream#close()}, it
+ *   <strong>must</strong> call the corresponding superclass method.
+ * <h4>Creating delegates</h4>
+ *   Forwarding classes (those which extend {@link org.apache.lucene.analysis.Tokenizer} but delegate
+ *   selected logic to another tokenizer) must also set the reader to the delegate in the overridden
+ *   {@link org.apache.lucene.analysis.Tokenizer#reset()} method, e.g.:
+ *   <pre class="prettyprint">
+ *     public class ForwardingTokenizer extends Tokenizer {
+ *        private Tokenizer delegate;
+ *        ...
+ *        {@literal @Override}
+ *        public void reset() {
+ *           super.reset();
+ *           delegate.setReader(this.input);
+ *           delegate.reset();
+ *        }
+ *     }
+ *   </pre>
+ * <h3>Testing Your Analysis Component</h3>
+ * <p>
+ *     The lucene-test-framework component defines
+ *     <a href="{@docRoot}/../test-framework/org/apache/lucene/analysis/BaseTokenStreamTestCase.html">BaseTokenStreamTestCase</a>. By extending
+ *     this class, you can create JUnit tests that validate that your
+ *     Analyzer and/or analysis components correctly implement the
+ *     protocol. The checkRandomData methods of that class are particularly effective in flushing out errors.
+ * </p>
+ * <h3>Using the TokenStream API</h3>
+ * There are a few important things to know in order to use the new API efficiently which are summarized here. You may want
+ * to walk through the example below first and come back to this section afterwards.
+ * <ol><li>
+ * Please keep in mind that an AttributeSource can only have one instance of a particular Attribute. Furthermore, if 
+ * a chain of a TokenStream and multiple TokenFilters is used, then all TokenFilters in that chain share the Attributes
+ * with the TokenStream.
+ * </li>
+ * <br>
+ * <li>
+ * Attribute instances are reused for all tokens of a document. Thus, a TokenStream/-Filter needs to update
+ * the appropriate Attribute(s) in incrementToken(). The consumer, commonly the Lucene indexer, consumes the data in the
+ * Attributes and then calls incrementToken() again until it returns false, which indicates that the end of the stream
+ * was reached. This means that in each call of incrementToken() a TokenStream/-Filter can safely overwrite the data in
+ * the Attribute instances.
+ * </li>
+ * <br>
+ * <li>
+ * For performance reasons a TokenStream/-Filter should add/get Attributes during instantiation; i.e., create an attribute in the
+ * constructor and store references to it in an instance variable.  Using an instance variable instead of calling addAttribute()/getAttribute() 
+ * in incrementToken() will avoid attribute lookups for every token in the document.
+ * </li>
+ * <br>
+ * <li>
+ * All methods in AttributeSource are idempotent, which means calling them multiple times always yields the same
+ * result. This is especially important to know for addAttribute(). The method takes the <b>type</b> (<code>Class</code>)
+ * of an Attribute as an argument and returns an <b>instance</b>. If an Attribute of the same type was previously added, then
+ * the already existing instance is returned, otherwise a new instance is created and returned. Therefore TokenStreams/-Filters
+ * can safely call addAttribute() with the same Attribute type multiple times. Even consumers of TokenStreams should
+ * normally call addAttribute() instead of getAttribute(), because it would not fail if the TokenStream does not have this
+ * Attribute (getAttribute() would throw an IllegalArgumentException, if the Attribute is missing). More advanced code
+ * could simply check with hasAttribute(), if a TokenStream has it, and may conditionally leave out processing for
+ * extra performance.
+ * </li></ol>
+ * <h3>Example</h3>
+ * <p>
+ *   In this example we will create a WhiteSpaceTokenizer and use a LengthFilter to suppress all words that have
+ *   only two or fewer characters. The LengthFilter is part of the Lucene core and its implementation will be explained
+ *   here to illustrate the usage of the TokenStream API.
+ * </p>
+ * <p>
+ *   Then we will develop a custom Attribute, a PartOfSpeechAttribute, and add another filter to the chain which
+ *   utilizes the new custom attribute, and call it PartOfSpeechTaggingFilter.
+ * </p>
+ * <h4>Whitespace tokenization</h4>
+ * <pre class="prettyprint">
+ * public class MyAnalyzer extends Analyzer {
+ * 
+ *   private Version matchVersion;
+ *   
+ *   public MyAnalyzer(Version matchVersion) {
+ *     this.matchVersion = matchVersion;
+ *   }
+ * 
+ *   {@literal @Override}
+ *   protected TokenStreamComponents createComponents(String fieldName) {
+ *     return new TokenStreamComponents(new WhitespaceTokenizer(matchVersion));
+ *   }
+ *   
+ *   public static void main(String[] args) throws IOException {
+ *     // text to tokenize
+ *     final String text = "This is a demo of the TokenStream API";
+ *     
+ *     Version matchVersion = Version.LUCENE_XY; // Substitute desired Lucene version for XY
+ *     MyAnalyzer analyzer = new MyAnalyzer(matchVersion);
+ *     TokenStream stream = analyzer.tokenStream("field", new StringReader(text));
+ *     
+ *     // get the CharTermAttribute from the TokenStream
+ *     CharTermAttribute termAtt = stream.addAttribute(CharTermAttribute.class);
+ * 
+ *     try {
+ *       stream.reset();
+ *     
+ *       // print all tokens until stream is exhausted
+ *       while (stream.incrementToken()) {
+ *         System.out.println(termAtt.toString());
+ *       }
+ *     
+ *       stream.end();
+ *     } finally {
+ *       stream.close();
+ *     }
+ *   }
+ * }
+ * </pre>
+ * In this easy example a simple white space tokenization is performed. In main() a loop consumes the stream and
+ * prints the term text of the tokens by accessing the CharTermAttribute that the WhitespaceTokenizer provides. 
+ * Here is the output:
+ * <pre>
+ * This
+ * is
+ * a
+ * demo
+ * of
+ * the
+ * new
+ * TokenStream
+ * API
+ * </pre>
+ * <h4>Adding a LengthFilter</h4>
+ * We want to suppress all tokens that have 2 or less characters. We can do that
+ * easily by adding a LengthFilter to the chain. Only the
+ * <code>createComponents()</code> method in our analyzer needs to be changed:
+ * <pre class="prettyprint">
+ *   {@literal @Override}
+ *   protected TokenStreamComponents createComponents(String fieldName) {
+ *     final Tokenizer source = new WhitespaceTokenizer(matchVersion);
+ *     TokenStream result = new LengthFilter(true, source, 3, Integer.MAX_VALUE);
+ *     return new TokenStreamComponents(source, result);
+ *   }
+ * </pre>
+ * Note how now only words with 3 or more characters are contained in the output:
+ * <pre>
+ * This
+ * demo
+ * the
+ * new
+ * TokenStream
+ * API
+ * </pre>
+ * Now let's take a look how the LengthFilter is implemented:
+ * <pre class="prettyprint">
+ * public final class LengthFilter extends FilteringTokenFilter {
+ * 
+ *   private final int min;
+ *   private final int max;
+ *   
+ *   private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
+ * 
+ *   &#47;**
+ *    * Create a new LengthFilter. This will filter out tokens whose
+ *    * CharTermAttribute is either too short
+ *    * (&lt; min) or too long (&gt; max).
+ *    * {@literal @param} version the Lucene match version
+ *    * {@literal @param} in      the TokenStream to consume
+ *    * {@literal @param} min     the minimum length
+ *    * {@literal @param} max     the maximum length
+ *    *&#47;
+ *  public LengthFilter(Version version, TokenStream in, int min, int max) {
+ *     super(version, in);
+ *     this.min = min;
+ *     this.max = max;
+ *   }
+ * 
+ *   {@literal @Override}
+ *   public boolean accept() {
+ *     final int len = termAtt.length();
+ *     return (len &gt;= min &amp;&amp; len &lt;= max);
+ *   }
+ * 
+ * }
+ * </pre>
+ * <p>
+ *   In LengthFilter, the CharTermAttribute is added and stored in the instance
+ *   variable <code>termAtt</code>.  Remember that there can only be a single
+ *   instance of CharTermAttribute in the chain, so in our example the
+ *   <code>addAttribute()</code> call in LengthFilter returns the
+ *   CharTermAttribute that the WhitespaceTokenizer already added.
+ * </p>
+ * <p>
+ *   The tokens are retrieved from the input stream in FilteringTokenFilter's 
+ *   <code>incrementToken()</code> method (see below), which calls LengthFilter's
+ *   <code>accept()</code> method. By looking at the term text in the
+ *   CharTermAttribute, the length of the term can be determined and tokens that
+ *   are either too short or too long are skipped.  Note how
+ *   <code>accept()</code> can efficiently access the instance variable; no 
+ *   attribute lookup is necessary. The same is true for the consumer, which can
+ *   simply use local references to the Attributes.
+ * </p>
+ * <p>
+ *   LengthFilter extends FilteringTokenFilter:
+ * </p>
+ * 
+ * <pre class="prettyprint">
+ * public abstract class FilteringTokenFilter extends TokenFilter {
+ * 
+ *   private final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);
+ * 
+ *   &#47;**
+ *    * Create a new FilteringTokenFilter.
+ *    * {@literal @param} in      the TokenStream to consume
+ *    *&#47;
+ *   public FilteringTokenFilter(Version version, TokenStream in) {
+ *     super(in);
+ *   }
+ * 
+ *   &#47;** Override this method and return if the current input token should be returned by incrementToken. *&#47;
+ *   protected abstract boolean accept() throws IOException;
+ * 
+ *   {@literal @Override}
+ *   public final boolean incrementToken() throws IOException {
+ *     int skippedPositions = 0;
+ *     while (input.incrementToken()) {
+ *       if (accept()) {
+ *         if (skippedPositions != 0) {
+ *           posIncrAtt.setPositionIncrement(posIncrAtt.getPositionIncrement() + skippedPositions);
+ *         }
+ *         return true;
+ *       }
+ *       skippedPositions += posIncrAtt.getPositionIncrement();
+ *     }
+ *     // reached EOS -- return false
+ *     return false;
+ *   }
+ * 
+ *   {@literal @Override}
+ *   public void reset() throws IOException {
+ *     super.reset();
+ *   }
+ * 
+ * }
+ * </pre>
+ * 
+ * <h4>Adding a custom Attribute</h4>
+ * Now we're going to implement our own custom Attribute for part-of-speech tagging and call it consequently 
+ * <code>PartOfSpeechAttribute</code>. First we need to define the interface of the new Attribute:
+ * <pre class="prettyprint">
+ *   public interface PartOfSpeechAttribute extends Attribute {
+ *     public static enum PartOfSpeech {
+ *       Noun, Verb, Adjective, Adverb, Pronoun, Preposition, Conjunction, Article, Unknown
+ *     }
+ *   
+ *     public void setPartOfSpeech(PartOfSpeech pos);
+ *   
+ *     public PartOfSpeech getPartOfSpeech();
+ *   }
+ * </pre>
+ * <p>
+ *   Now we also need to write the implementing class. The name of that class is important here: By default, Lucene
+ *   checks if there is a class with the name of the Attribute with the suffix 'Impl'. In this example, we would
+ *   consequently call the implementing class <code>PartOfSpeechAttributeImpl</code>.
+ * </p>
+ * <p>
+ *   This should be the usual behavior. However, there is also an expert-API that allows changing these naming conventions:
+ *   {@link org.apache.lucene.util.AttributeFactory}. The factory accepts an Attribute interface as argument
+ *   and returns an actual instance. You can implement your own factory if you need to change the default behavior.
+ * </p>
+ * <p>
+ *   Now here is the actual class that implements our new Attribute. Notice that the class has to extend
+ *   {@link org.apache.lucene.util.AttributeImpl}:
+ * </p>
+ * <pre class="prettyprint">
+ * public final class PartOfSpeechAttributeImpl extends AttributeImpl 
+ *                                   implements PartOfSpeechAttribute {
+ *   
+ *   private PartOfSpeech pos = PartOfSpeech.Unknown;
+ *   
+ *   public void setPartOfSpeech(PartOfSpeech pos) {
+ *     this.pos = pos;
+ *   }
+ *   
+ *   public PartOfSpeech getPartOfSpeech() {
+ *     return pos;
+ *   }
+ * 
+ *   {@literal @Override}
+ *   public void clear() {
+ *     pos = PartOfSpeech.Unknown;
+ *   }
+ * 
+ *   {@literal @Override}
+ *   public void copyTo(AttributeImpl target) {
+ *     ((PartOfSpeechAttribute) target).setPartOfSpeech(pos);
+ *   }
+ * }
+ * </pre>
+ * <p>
+ *   This is a simple Attribute implementation has only a single variable that
+ *   stores the part-of-speech of a token. It extends the
+ *   <code>AttributeImpl</code> class and therefore implements its abstract methods
+ *   <code>clear()</code> and <code>copyTo()</code>. Now we need a TokenFilter that
+ *   can set this new PartOfSpeechAttribute for each token. In this example we
+ *   show a very naive filter that tags every word with a leading upper-case letter
+ *   as a 'Noun' and all other words as 'Unknown'.
+ * </p>
+ * <pre class="prettyprint">
+ *   public static class PartOfSpeechTaggingFilter extends TokenFilter {
+ *     PartOfSpeechAttribute posAtt = addAttribute(PartOfSpeechAttribute.class);
+ *     CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
+ *     
+ *     protected PartOfSpeechTaggingFilter(TokenStream input) {
+ *       super(input);
+ *     }
+ *     
+ *     public boolean incrementToken() throws IOException {
+ *       if (!input.incrementToken()) {return false;}
+ *       posAtt.setPartOfSpeech(determinePOS(termAtt.buffer(), 0, termAtt.length()));
+ *       return true;
+ *     }
+ *     
+ *     // determine the part of speech for the given term
+ *     protected PartOfSpeech determinePOS(char[] term, int offset, int length) {
+ *       // naive implementation that tags every uppercased word as noun
+ *       if (length &gt; 0 &amp;&amp; Character.isUpperCase(term[0])) {
+ *         return PartOfSpeech.Noun;
+ *       }
+ *       return PartOfSpeech.Unknown;
+ *     }
+ *   }
+ * </pre>
+ * <p>
+ *   Just like the LengthFilter, this new filter stores references to the
+ *   attributes it needs in instance variables. Notice how you only need to pass
+ *   in the interface of the new Attribute and instantiating the correct class
+ *   is automatically taken care of.
+ * </p>
+ * <p>Now we need to add the filter to the chain in MyAnalyzer:</p>
+ * <pre class="prettyprint">
+ *   {@literal @Override}
+ *   protected TokenStreamComponents createComponents(String fieldName) {
+ *     final Tokenizer source = new WhitespaceTokenizer(matchVersion);
+ *     TokenStream result = new LengthFilter(true, source, 3, Integer.MAX_VALUE);
+ *     result = new PartOfSpeechTaggingFilter(result);
+ *     return new TokenStreamComponents(source, result);
+ *   }
+ * </pre>
+ * Now let's look at the output:
+ * <pre>
+ * This
+ * demo
+ * the
+ * new
+ * TokenStream
+ * API
+ * </pre>
+ * Apparently it hasn't changed, which shows that adding a custom attribute to a TokenStream/Filter chain does not
+ * affect any existing consumers, simply because they don't know the new Attribute. Now let's change the consumer
+ * to make use of the new PartOfSpeechAttribute and print it out:
+ * <pre class="prettyprint">
+ *   public static void main(String[] args) throws IOException {
+ *     // text to tokenize
+ *     final String text = "This is a demo of the TokenStream API";
+ *     
+ *     MyAnalyzer analyzer = new MyAnalyzer();
+ *     TokenStream stream = analyzer.tokenStream("field", new StringReader(text));
+ *     
+ *     // get the CharTermAttribute from the TokenStream
+ *     CharTermAttribute termAtt = stream.addAttribute(CharTermAttribute.class);
+ *     
+ *     // get the PartOfSpeechAttribute from the TokenStream
+ *     PartOfSpeechAttribute posAtt = stream.addAttribute(PartOfSpeechAttribute.class);
+ * 
+ *     try {
+ *       stream.reset();
+ * 
+ *       // print all tokens until stream is exhausted
+ *       while (stream.incrementToken()) {
+ *         System.out.println(termAtt.toString() + ": " + posAtt.getPartOfSpeech());
+ *       }
+ *     
+ *       stream.end();
+ *     } finally {
+ *       stream.close();
+ *     }
+ *   }
+ * </pre>
+ * The change that was made is to get the PartOfSpeechAttribute from the TokenStream and print out its contents in
+ * the while loop that consumes the stream. Here is the new output:
+ * <pre>
+ * This: Noun
+ * demo: Unknown
+ * the: Unknown
+ * new: Unknown
+ * TokenStream: Noun
+ * API: Noun
+ * </pre>
+ * Each word is now followed by its assigned PartOfSpeech tag. Of course this is a naive 
+ * part-of-speech tagging. The word 'This' should not even be tagged as noun; it is only spelled capitalized because it
+ * is the first word of a sentence. Actually this is a good opportunity for an exercise. To practice the usage of the new
+ * API the reader could now write an Attribute and TokenFilter that can specify for each word if it was the first token
+ * of a sentence or not. Then the PartOfSpeechTaggingFilter can make use of this knowledge and only tag capitalized words
+ * as nouns if not the first word of a sentence (we know, this is still not a correct behavior, but hey, it's a good exercise). 
+ * As a small hint, this is how the new Attribute class could begin:
+ * <pre class="prettyprint">
+ *   public class FirstTokenOfSentenceAttributeImpl extends AttributeImpl
+ *                               implements FirstTokenOfSentenceAttribute {
+ *     
+ *     private boolean firstToken;
+ *     
+ *     public void setFirstToken(boolean firstToken) {
+ *       this.firstToken = firstToken;
+ *     }
+ *     
+ *     public boolean getFirstToken() {
+ *       return firstToken;
+ *     }
+ * 
+ *     {@literal @Override}
+ *     public void clear() {
+ *       firstToken = false;
+ *     }
+ * 
+ *   ...
+ * </pre>
+ * <h4>Adding a CharFilter chain</h4>
+ * Analyzers take Java {@link java.io.Reader}s as input. Of course you can wrap your Readers with {@link java.io.FilterReader}s
+ * to manipulate content, but this would have the big disadvantage that character offsets might be inconsistent with your original
+ * text.
+ * <p>
+ * {@link org.apache.lucene.analysis.CharFilter} is designed to allow you to pre-process input like a FilterReader would, but also
+ * preserve the original offsets associated with those characters. This way mechanisms like highlighting still work correctly.
+ * CharFilters can be chained.
+ * <p>
+ * Example:
+ * <pre class="prettyprint">
+ * public class MyAnalyzer extends Analyzer {
+ * 
+ *   {@literal @Override}
+ *   protected TokenStreamComponents createComponents(String fieldName) {
+ *     return new TokenStreamComponents(new MyTokenizer());
+ *   }
+ *   
+ *   {@literal @Override}
+ *   protected Reader initReader(String fieldName, Reader reader) {
+ *     // wrap the Reader in a CharFilter chain.
+ *     return new SecondCharFilter(new FirstCharFilter(reader));
+ *   }
+ * }
+ * </pre>
+ */
+package org.apache.lucene.analysis;
diff --git a/lucene/core/src/java/org/apache/lucene/analysis/package.html b/lucene/core/src/java/org/apache/lucene/analysis/package.html
deleted file mode 100644
index 3bff32b..0000000
--- a/lucene/core/src/java/org/apache/lucene/analysis/package.html
+++ /dev/null
@@ -1,1005 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-<p>API and code to convert text into indexable/searchable tokens.  Covers {@link org.apache.lucene.analysis.Analyzer} and related classes.</p>
-<h2>Parsing? Tokenization? Analysis!</h2>
-<p>
-Lucene, an indexing and search library, accepts only plain text input.
-<p>
-<h2>Parsing</h2>
-<p>
-Applications that build their search capabilities upon Lucene may support documents in various formats &ndash; HTML, XML, PDF, Word &ndash; just to name a few.
-Lucene does not care about the <i>Parsing</i> of these and other document formats, and it is the responsibility of the 
-application using Lucene to use an appropriate <i>Parser</i> to convert the original format into plain text before passing that plain text to Lucene.
-<p>
-<h2>Tokenization</h2>
-<p>
-Plain text passed to Lucene for indexing goes through a process generally called tokenization. Tokenization is the process
-of breaking input text into small indexing elements &ndash; tokens.
-The way input text is broken into tokens heavily influences how people will then be able to search for that text. 
-For instance, sentences beginnings and endings can be identified to provide for more accurate phrase 
-and proximity searches (though sentence identification is not provided by Lucene).
-<p>
-  In some cases simply breaking the input text into tokens is not enough
-  &ndash; a deeper <i>Analysis</i> may be needed. Lucene includes both
-  pre- and post-tokenization analysis facilities.
-</p>
-<p>
-  Pre-tokenization analysis can include (but is not limited to) stripping
-  HTML markup, and transforming or removing text matching arbitrary patterns
-  or sets of fixed strings.
-</p>
-<p>
-  There are many post-tokenization steps that can be done, including 
-  (but not limited to):
-</p>
-<ul>
-  <li><a href="http://en.wikipedia.org/wiki/Stemming">Stemming</a> &ndash; 
-      Replacing words with their stems. 
-      For instance with English stemming "bikes" is replaced with "bike"; 
-      now query "bike" can find both documents containing "bike" and those containing "bikes".
-  </li>
-  <li><a href="http://en.wikipedia.org/wiki/Stop_words">Stop Words Filtering</a> &ndash; 
-      Common words like "the", "and" and "a" rarely add any value to a search.
-      Removing them shrinks the index size and increases performance.
-      It may also reduce some "noise" and actually improve search quality.
-  </li>
-  <li><a href="http://en.wikipedia.org/wiki/Text_normalization">Text Normalization</a> &ndash; 
-      Stripping accents and other character markings can make for better searching.
-  </li>
-  <li><a href="http://en.wikipedia.org/wiki/Synonym">Synonym Expansion</a> &ndash; 
-      Adding in synonyms at the same token position as the current word can mean better 
-      matching when users search with words in the synonym set.
-  </li>
-</ul> 
-<p>
-<h2>Core Analysis</h2>
-<p>
-  The analysis package provides the mechanism to convert Strings and Readers
-  into tokens that can be indexed by Lucene.  There are four main classes in 
-  the package from which all analysis processes are derived.  These are:
-</p>
-<ul>
-  <li>
-    {@link org.apache.lucene.analysis.Analyzer} &ndash; An <code>Analyzer</code> is 
-    responsible for supplying a
-    {@link org.apache.lucene.analysis.TokenStream} which can be consumed
-    by the indexing and searching processes.  See below for more information
-    on implementing your own {@link org.apache.lucene.analysis.Analyzer}. Most of the time, you can use
-    an anonymous subclass of {@link org.apache.lucene.analysis.Analyzer}.
-  </li>
-  <li>
-    {@link org.apache.lucene.analysis.CharFilter} &ndash; <code>CharFilter</code> extends
-    {@link java.io.Reader} to transform the text before it is
-    tokenized, while providing
-    corrected character offsets to account for these modifications.  This
-    capability allows highlighting to function over the original text when 
-    indexed tokens are created from <code>CharFilter</code>-modified text with offsets
-    that are not the same as those in the original text. {@link org.apache.lucene.analysis.Tokenizer#setReader(java.io.Reader)}
-    accept <code>CharFilter</code>s.  <code>CharFilter</code>s may
-    be chained to perform multiple pre-tokenization modifications.
-  </li>
-  <li>
-    {@link org.apache.lucene.analysis.Tokenizer} &ndash; A <code>Tokenizer</code> is a 
-    {@link org.apache.lucene.analysis.TokenStream} and is responsible for
-    breaking up incoming text into tokens. In many cases, an {@link org.apache.lucene.analysis.Analyzer} will
-    use a {@link org.apache.lucene.analysis.Tokenizer} as the first step in the analysis process.  However,
-    to modify text prior to tokenization, use a {@link org.apache.lucene.analysis.CharFilter} subclass (see
-    above).
-  </li>
-  <li>
-    {@link org.apache.lucene.analysis.TokenFilter} &ndash; A <code>TokenFilter</code> is
-    a {@link org.apache.lucene.analysis.TokenStream} and is responsible
-    for modifying tokens that have been created by the <code>Tokenizer</code>. Common 
-    modifications performed by a <code>TokenFilter</code> are: deletion, stemming, synonym 
-    injection, and case folding.  Not all <code>Analyzer</code>s require <code>TokenFilter</code>s.
-  </li>
-</ul>
-<h2>Hints, Tips and Traps</h2>
-<p>
-  The relationship between {@link org.apache.lucene.analysis.Analyzer} and 
-  {@link org.apache.lucene.analysis.CharFilter}s,
-  {@link org.apache.lucene.analysis.Tokenizer}s,
-  and {@link org.apache.lucene.analysis.TokenFilter}s is sometimes confusing. To ease
-  this confusion, here is some clarifications:
-</p>
-<ul>
-  <li>
-    The {@link org.apache.lucene.analysis.Analyzer} is a
-    <strong>factory</strong> for analysis chains. <code>Analyzer</code>s don't
-    process text, <code>Analyzer</code>s construct <code>CharFilter</code>s, <code>Tokenizer</code>s, and/or
-    <code>TokenFilter</code>s that process text. An <code>Analyzer</code> has two tasks: 
-    to produce {@link org.apache.lucene.analysis.TokenStream}s that accept a
-    reader and produces tokens, and to wrap or otherwise
-    pre-process {@link java.io.Reader} objects.
-  </li>
-  <li>
-  The {@link org.apache.lucene.analysis.CharFilter} is a subclass of
- {@link java.io.Reader} that supports offset tracking.
-  </li>
-  <li>The{@link org.apache.lucene.analysis.Tokenizer}
-    is only responsible for <u>breaking</u> the input text into tokens.
-  </li>
-  <li>The{@link org.apache.lucene.analysis.TokenFilter} modifies a
-  stream of tokens and their contents.
-  </li>
-  <li>
-    {@link org.apache.lucene.analysis.Tokenizer} is a {@link org.apache.lucene.analysis.TokenStream}, 
-    but {@link org.apache.lucene.analysis.Analyzer} is not.
-  </li>
-  <li>
-    {@link org.apache.lucene.analysis.Analyzer} is "field aware", but 
-    {@link org.apache.lucene.analysis.Tokenizer} is not. {@link org.apache.lucene.analysis.Analyzer}s may
-    take a field name into account when constructing the {@link org.apache.lucene.analysis.TokenStream}.
-  </li>
-</ul>
-<p>
-  If you want to use a particular combination of <code>CharFilter</code>s, a
-  <code>Tokenizer</code>, and some <code>TokenFilter</code>s, the simplest thing is often an
-  create an anonymous subclass of {@link org.apache.lucene.analysis.Analyzer}, provide {@link
-  org.apache.lucene.analysis.Analyzer#createComponents(String)} and perhaps also
-  {@link org.apache.lucene.analysis.Analyzer#initReader(String,
-  java.io.Reader)}. However, if you need the same set of components
-  over and over in many places, you can make a subclass of
-  {@link org.apache.lucene.analysis.Analyzer}. In fact, Apache Lucene
-  supplies a large family of <code>Analyzer</code> classes that deliver useful
-  analysis chains. The most common of these is the <a href="{@docRoot}/../analyzers-common/org/apache/lucene/analysis/standard/StandardAnalyzer.html">StandardAnalyzer</a>.
-  Many applications will have a long and industrious life with nothing more
-  than the <code>StandardAnalyzer</code>. The <a href="{@docRoot}/../analyzers-common/overview-summary.html">analyzers-common</a>
-  library provides many pre-existing analyzers for various languages.
-  The analysis-common library also allows to configure a custom Analyzer without subclassing using the
-  <a href="{@docRoot}/../analyzers-common/org/apache/lucene/analysis/custom/CustomAnalyzer.html">CustomAnalyzer</a>
-  class.
-</p>
-<p>
-  Aside from the <code>StandardAnalyzer</code>,
-  Lucene includes several components containing analysis components,
-  all under the 'analysis' directory of the distribution. Some of
-  these support particular languages, others integrate external
-  components. The 'common' subdirectory has some noteworthy
- general-purpose analyzers, including the <a href="{@docRoot}/../analyzers-common/org/apache/lucene/analysis/miscellaneous/PerFieldAnalyzerWrapper.html">PerFieldAnalyzerWrapper</a>. Most <code>Analyzer</code>s perform the same operation on all
- {@link org.apache.lucene.document.Field}s.  The PerFieldAnalyzerWrapper can be used to associate a different <code>Analyzer</code> with different
- {@link org.apache.lucene.document.Field}s. There is a great deal of
- functionality in the analysis area, you should study it carefully to
- find the pieces you need.
-</p>
-<p>
-  Analysis is one of the main causes of slow indexing.  Simply put, the more you analyze the slower the indexing (in most cases).
-  Perhaps your application would be just fine using the simple WhitespaceTokenizer combined with a StopFilter. The benchmark/ library can be useful 
-  for testing out the speed of the analysis process.
-</p>
-<h2>Invoking the Analyzer</h2>
-<p>
-  Applications usually do not invoke analysis &ndash; Lucene does it
- for them. Applications construct <code>Analyzer</code>s and pass then into Lucene,
- as follows:
-</p>
-<ul>
-  <li>
-    At indexing, as a consequence of 
-    {@link org.apache.lucene.index.IndexWriter#addDocument(IndexDocument) addDocument(doc)},
-    the <code>Analyzer</code> in effect for indexing is invoked for each indexed field of the added document.
-  </li>
-  <li>
-    At search, a <code>QueryParser</code> may invoke the Analyzer during parsing.  Note that for some queries, analysis does not
-    take place, e.g. wildcard queries.
-  </li>
-</ul>
-<p>
-  However an application might invoke Analysis of any text for testing or for any other purpose, something like:
-</p>
-<PRE class="prettyprint" id="analysis-workflow">
-    Version matchVersion = Version.LUCENE_XY; // Substitute desired Lucene version for XY
-    Analyzer analyzer = new StandardAnalyzer(matchVersion); // or any other analyzer
-    TokenStream ts = analyzer.tokenStream("myfield", new StringReader("some text goes here"));
-    /* The Analyzer class will construct the Tokenizer, TokenFilter(s), and CharFilter(s),
-       and pass the resulting Reader to the Tokenizer.
-    */
-    OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);
-    
-    try {
-      ts.reset(); // Resets this stream to the beginning. (Required)
-      while (ts.incrementToken()) {
-        // Use {@link org.apache.lucene.util.AttributeSource#reflectAsString(boolean)}
-        // for token stream debugging.
-        System.out.println("token: " + ts.reflectAsString(true));
-
-        System.out.println("token start offset: " + offsetAtt.startOffset());
-        System.out.println("  token end offset: " + offsetAtt.endOffset());
-      }
-      ts.end();   // Perform end-of-stream operations, e.g. set the final offset.
-    } finally {
-      ts.close(); // Release resources associated with this stream.
-    }
-</PRE>
-<h2>Indexing Analysis vs. Search Analysis</h2>
-<p>
-  Selecting the "correct" analyzer is crucial
-  for search quality, and can also affect indexing and search performance.
-  The "correct" analyzer for your application will depend on what your input text
-  looks like and what problem you are trying to solve.
-  Lucene java's wiki page 
-  <a href="http://wiki.apache.org/lucene-java/AnalysisParalysis">AnalysisParalysis</a> 
-  provides some data on "analyzing your analyzer".
-  Here are some rules of thumb:
-  <ol>
-    <li>Test test test... (did we say test?)</li>
-    <li>Beware of too much analysis &ndash; it might hurt indexing performance.</li>
-    <li>Start with the same analyzer for indexing and search, otherwise searches would not find what they are supposed to...</li>
-    <li>In some cases a different analyzer is required for indexing and search, for instance:
-        <ul>
-           <li>Certain searches require more stop words to be filtered. (i.e. more than those that were filtered at indexing.)</li>
-           <li>Query expansion by synonyms, acronyms, auto spell correction, etc.</li>
-        </ul>
-        This might sometimes require a modified analyzer &ndash; see the next section on how to do that.
-    </li>
-  </ol>
-</p>
-<h2>Implementing your own Analyzer and Analysis Components</h2>
-<p>
-  Creating your own Analyzer is straightforward. Your Analyzer should subclass {@link org.apache.lucene.analysis.Analyzer}. It can use
-  existing analysis components &mdash; CharFilter(s) <i>(optional)</i>, a
-  Tokenizer, and TokenFilter(s) <i>(optional)</i> &mdash; or components you
-  create, or a combination of existing and newly created components.  Before
-  pursuing this approach, you may find it worthwhile to explore the
-  <a href="{@docRoot}/../analyzers-common/overview-summary.html">analyzers-common</a> library and/or ask on the 
-  <a href="http://lucene.apache.org/core/discussion.html">java-user@lucene.apache.org mailing list</a> first to see if what you
-  need already exists. If you are still committed to creating your own
-  Analyzer, have a look at the source code of any one of the many samples
-  located in this package.
-</p>
-<p>
-  The following sections discuss some aspects of implementing your own analyzer.
-</p>
-<h3>Field Section Boundaries</h3>
-<p>
-  When {@link org.apache.lucene.document.Document#add(org.apache.lucene.document.Field) document.add(field)}
-  is called multiple times for the same field name, we could say that each such call creates a new 
-  section for that field in that document. 
-  In fact, a separate call to 
-  {@link org.apache.lucene.analysis.Analyzer#tokenStream(java.lang.String, java.io.Reader) tokenStream(field,reader)}
-  would take place for each of these so called "sections".
-  However, the default Analyzer behavior is to treat all these sections as one large section. 
-  This allows phrase search and proximity search to seamlessly cross 
-  boundaries between these "sections".
-  In other words, if a certain field "f" is added like this:
-</p>
-<PRE class="prettyprint">
-    document.add(new Field("f","first ends",...);
-    document.add(new Field("f","starts two",...);
-    indexWriter.addDocument(document);
-</PRE>
-<p>
-  Then, a phrase search for "ends starts" would find that document.
-  Where desired, this behavior can be modified by introducing a "position gap" between consecutive field "sections", 
-  simply by overriding 
-  {@link org.apache.lucene.analysis.Analyzer#getPositionIncrementGap(java.lang.String) Analyzer.getPositionIncrementGap(fieldName)}:
-</p>
-<PRE class="prettyprint">
-  Version matchVersion = Version.LUCENE_XY; // Substitute desired Lucene version for XY
-  Analyzer myAnalyzer = new StandardAnalyzer(matchVersion) {
-    public int getPositionIncrementGap(String fieldName) {
-      return 10;
-    }
-  };
-</PRE>
-<h3>End of Input Cleanup</h3>
-<p>
-   At the ends of each field, Lucene will call the {@link org.apache.lucene.analysis.TokenStream#end()}.
-   The components of the token stream (the tokenizer and the token filters) <strong>must</strong>
-   put accurate values into the token attributes to reflect the situation at the end of the field.
-   The Offset attribute must contain the final offset (the total number of characters processed)
-   in both start and end. Attributes like PositionLength must be correct. 
-</p>
-<p>
-   The base method{@link org.apache.lucene.analysis.TokenStream#end()} sets PositionIncrement to 0, which is required.
-   Other components must override this method to fix up the other attributes.
-</p>
-<h3>Token Position Increments</h3>
-<p>
-   By default, TokenStream arranges for the 
-   {@link org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute#getPositionIncrement() position increment} of all tokens to be one.
-   This means that the position stored for that token in the index would be one more than
-   that of the previous token.
-   Recall that phrase and proximity searches rely on position info.
-</p>
-<p>
-   If the selected analyzer filters the stop words "is" and "the", then for a document 
-   containing the string "blue is the sky", only the tokens "blue", "sky" are indexed, 
-   with position("sky") = 3 + position("blue"). Now, a phrase query "blue is the sky"
-   would find that document, because the same analyzer filters the same stop words from
-   that query. But the phrase query "blue sky" would not find that document because the
-   position increment between "blue" and "sky" is only 1.
-</p>
-<p>   
-   If this behavior does not fit the application needs, the query parser needs to be
-   configured to not take position increments into account when generating phrase queries.
-</p>
-<p>
-  Note that a filter that filters <strong>out</strong> tokens <strong>must</strong> increment the position increment in order not to generate corrupt
-  tokenstream graphs. Here is the logic used by StopFilter to increment positions when filtering out tokens:
-</p>
-<PRE class="prettyprint">
-  public TokenStream tokenStream(final String fieldName, Reader reader) {
-    final TokenStream ts = someAnalyzer.tokenStream(fieldName, reader);
-    TokenStream res = new TokenStream() {
-      CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-      PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);
-
-      public boolean incrementToken() throws IOException {
-        int extraIncrement = 0;
-        while (true) {
-          boolean hasNext = ts.incrementToken();
-          if (hasNext) {
-            if (stopWords.contains(termAtt.toString())) {
-              extraIncrement += posIncrAtt.getPositionIncrement(); // filter this word
-              continue;
-            } 
-            if (extraIncrement>0) {
-              posIncrAtt.setPositionIncrement(posIncrAtt.getPositionIncrement()+extraIncrement);
-            }
-          }
-          return hasNext;
-        }
-      }
-    };
-    return res;
-  }
-</PRE>
-<p>
-   A few more use cases for modifying position increments are:
-</p>
-<ol>
-  <li>Inhibiting phrase and proximity matches in sentence boundaries &ndash; for this, a tokenizer that 
-    identifies a new sentence can add 1 to the position increment of the first token of the new sentence.</li>
-  <li>Injecting synonyms &ndash; here, synonyms of a token should be added after that token, 
-    and their position increment should be set to 0.
-    As result, all synonyms of a token would be considered to appear in exactly the 
-    same position as that token, and so would they be seen by phrase and proximity searches.</li>
-</ol>
-
-<h3>Token Position Length</h3>
-<p>
-   By default, all tokens created by Analyzers and Tokenizers have a
-   {@link org.apache.lucene.analysis.tokenattributes.PositionLengthAttribute#getPositionLength() position length} of one.
-   This means that the token occupies a single position. This attribute is not indexed
-   and thus not taken into account for positional queries, but is used by eg. suggesters.
-</p>
-<p>
-   The main use case for positions lengths is multi-word synonyms. With single-word
-   synonyms, setting the position increment to 0 is enough to denote the fact that two
-   words are synonyms, for example:
-</p>
-<table>
-<tr><td>Term</td><td>red</td><td>magenta</td></tr>
-<tr><td>Position increment</td><td>1</td><td>0</td></tr>
-</table>
-<p>
-   Given that position(magenta) = 0 + position(red), they are at the same position, so anything
-   working with analyzers will return the exact same result if you replace "magenta" with "red"
-   in the input. However, multi-word synonyms are more tricky. Let's say that you want to build
-   a TokenStream where "IBM" is a synonym of "Internal Business Machines". Position increments
-   are not enough anymore:
-</p>
-<table>
-<tr><td>Term</td><td>IBM</td><td>International</td><td>Business</td><td>Machines</td></tr>
-<tr><td>Position increment</td><td>1</td><td>0</td><td>1</td><td>1</td></tr>
-</table>
-<p>
-   The problem with this token stream is that "IBM" is at the same position as "International"
-   although it is a synonym with "International Business Machines" as a whole. Setting
-   the position increment of "Business" and "Machines" to 0 wouldn't help as it would mean
-   than "International" is a synonym of "Business". The only way to solve this issue is to
-   make "IBM" span across 3 positions, this is where position lengths come to rescue.
-</p>
-<table>
-<tr><td>Term</td><td>IBM</td><td>International</td><td>Business</td><td>Machines</td></tr>
-<tr><td>Position increment</td><td>1</td><td>0</td><td>1</td><td>1</td></tr>
-<tr><td>Position length</td><td>3</td><td>1</td><td>1</td><td>1</td></tr>
-</table>
-<p>
-   This new attribute makes clear that "IBM" and "International Business Machines" start and end
-   at the same positions.
-</p>
-<a name="corrupt" />
-<h3>How to not write corrupt token streams</h3>
-<p>
-   There are a few rules to observe when writing custom Tokenizers and TokenFilters:
-</p>
-<ul>
-  <li>The first position increment must be &gt; 0.</li>
-  <li>Positions must not go backward.</li>
-  <li>Tokens that have the same start position must have the same start offset.</li>
-  <li>Tokens that have the same end position (taking into account the
-  position length) must have the same end offset.</li>
-  <li>Tokenizers must call {@link
-  org.apache.lucene.util.AttributeSource#clearAttributes()} in
-  incrementToken().</li>
-  <li>Tokenizers must override {@link
-  org.apache.lucene.analysis.TokenStream#end()}, and pass the final
-  offset (the total number of input characters processed) to both
-  parameters of {@link org.apache.lucene.analysis.tokenattributes.OffsetAttribute#setOffset(int, int)}.</li>
-</ul>
-<p>
-   Although these rules might seem easy to follow, problems can quickly happen when chaining
-   badly implemented filters that play with positions and offsets, such as synonym or n-grams
-   filters. Here are good practices for writing correct filters:
-</p>
-<ul>
-  <li>Token filters should not modify offsets. If you feel that your filter would need to modify offsets, then it should probably be implemented as a tokenizer.</li>
-  <li>Token filters should not insert positions. If a filter needs to add tokens, then they should all have a position increment of 0.</li>
-  <li>When they add tokens, token filters should call {@link org.apache.lucene.util.AttributeSource#clearAttributes()} first.</li>
-  <li>When they remove tokens, token filters should increment the position increment of the following token.</li>
-  <li>Token filters should preserve position lengths.</li>
-</ul>
-<h2>TokenStream API</h2>
-<p>
-  "Flexible Indexing" summarizes the effort of making the Lucene indexer
-  pluggable and extensible for custom index formats.  A fully customizable
-  indexer means that users will be able to store custom data structures on
-  disk. Therefore the analysis API must transport custom types of
-  data from the documents to the indexer. (It also supports communications
-  amongst the analysis components.)
-</p>
-<h3>Attribute and AttributeSource</h3>
-<p>
-  Classes {@link org.apache.lucene.util.Attribute} and 
-  {@link org.apache.lucene.util.AttributeSource} serve as the basis upon which 
-  the analysis elements of "Flexible Indexing" are implemented. An Attribute 
-  holds a particular piece of information about a text token. For example, 
-  {@link org.apache.lucene.analysis.tokenattributes.CharTermAttribute} 
-  contains the term text of a token, and 
-  {@link org.apache.lucene.analysis.tokenattributes.OffsetAttribute} contains
-  the start and end character offsets of a token. An AttributeSource is a 
-  collection of Attributes with a restriction: there may be only one instance
-  of each attribute type. TokenStream now extends AttributeSource, which means
-  that one can add Attributes to a TokenStream. Since TokenFilter extends
-  TokenStream, all filters are also AttributeSources.
-</p>
-<p>
-	Lucene provides seven Attributes out of the box:
-</p>
-<table rules="all" frame="box" cellpadding="3">
-  <tr>
-    <td>{@link org.apache.lucene.analysis.tokenattributes.CharTermAttribute}</td>
-    <td>
-      The term text of a token.  Implements {@link java.lang.CharSequence} 
-      (providing methods length() and charAt(), and allowing e.g. for direct
-      use with regular expression {@link java.util.regex.Matcher}s) and 
-      {@link java.lang.Appendable} (allowing the term text to be appended to.)
-    </td>
-  </tr>
-  <tr>
-    <td>{@link org.apache.lucene.analysis.tokenattributes.OffsetAttribute}</td>
-    <td>The start and end offset of a token in characters.</td>
-  </tr>
-  <tr>
-    <td>{@link org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute}</td>
-    <td>See above for detailed information about position increment.</td>
-  </tr>
-  <tr>
-    <td>{@link org.apache.lucene.analysis.tokenattributes.PositionLengthAttribute}</td>
-    <td>The number of positions occupied by a token.</td>
-  </tr>
-  <tr>
-    <td>{@link org.apache.lucene.analysis.tokenattributes.PayloadAttribute}</td>
-    <td>The payload that a Token can optionally have.</td>
-  </tr>
-  <tr>
-    <td>{@link org.apache.lucene.analysis.tokenattributes.TypeAttribute}</td>
-    <td>The type of the token. Default is 'word'.</td>
-  </tr>
-  <tr>
-    <td>{@link org.apache.lucene.analysis.tokenattributes.FlagsAttribute}</td>
-    <td>Optional flags a token can have.</td>
-  </tr>
-  <tr>
-    <td>{@link org.apache.lucene.analysis.tokenattributes.KeywordAttribute}</td>
-    <td>
-      Keyword-aware TokenStreams/-Filters skip modification of tokens that
-      return true from this attribute's isKeyword() method. 
-    </td>
-  </tr>
-</table>
-<h3>More Requirements for Analysis Component Classes</h3>
-Due to the historical development of the API, there are some perhaps
-less than obvious requirements to implement analysis components
-classes.
-<h4 id="analysis-lifetime">Token Stream Lifetime</h4>
-The code fragment of the <a href="#analysis-workflow">analysis workflow
-protocol</a> above shows a token stream being obtained, used, and then
-left for garbage. However, that does not mean that the components of
-that token stream will, in fact, be discarded. The default is just the
-opposite. {@link org.apache.lucene.analysis.Analyzer} applies a reuse
-strategy to the tokenizer and the token filters. It will reuse
-them. For each new input, it calls {@link org.apache.lucene.analysis.Tokenizer#setReader(java.io.Reader)} 
-to set the input. Your components must be prepared for this scenario,
-as described below.
-<h4>Tokenizer</h4>
-<ul>
-  <li>
-  You should create your tokenizer class by extending {@link org.apache.lucene.analysis.Tokenizer}.
-  </li>
-  <li>
-  Your tokenizer <strong>must</strong> override {@link org.apache.lucene.analysis.TokenStream#end()}.
-  Your implementation <strong>must</strong> call
-  <code>super.end()</code>. It must set a correct final offset into
-  the offset attribute, and finish up and other attributes to reflect
-  the end of the stream.
-  </li>
-  <li>
-  If your tokenizer overrides {@link org.apache.lucene.analysis.TokenStream#reset()}
-  or {@link org.apache.lucene.analysis.TokenStream#close()}, it
-  <strong>must</strong> call the corresponding superclass method.
-  </li>
-</ul>
-<h4>Token Filter</h4>
-  You should create your token filter class by extending {@link org.apache.lucene.analysis.TokenFilter}.
-  If your token filter overrides {@link org.apache.lucene.analysis.TokenStream#reset()},
-  {@link org.apache.lucene.analysis.TokenStream#end()}
-  or {@link org.apache.lucene.analysis.TokenStream#close()}, it
-  <strong>must</strong> call the corresponding superclass method.
-<h4>Creating delegates</h4>
-  Forwarding classes (those which extend {@link org.apache.lucene.analysis.Tokenizer} but delegate
-  selected logic to another tokenizer) must also set the reader to the delegate in the overridden
-  {@link org.apache.lucene.analysis.Tokenizer#reset()} method, e.g.:
-  <pre class="prettyprint">
-    public class ForwardingTokenizer extends Tokenizer {
-       private Tokenizer delegate;
-       ...
-       {@literal @Override}
-       public void reset() {
-          super.reset();
-          delegate.setReader(this.input);
-          delegate.reset();
-       }
-    }
-  </pre>
-<h3>Testing Your Analysis Component</h3>
-<p>
-    The lucene-test-framework component defines
-    <a href="{@docRoot}/../test-framework/org/apache/lucene/analysis/BaseTokenStreamTestCase.html">BaseTokenStreamTestCase</a>. By extending
-    this class, you can create JUnit tests that validate that your
-    Analyzer and/or analysis components correctly implement the
-    protocol. The checkRandomData methods of that class are particularly effective in flushing out errors.
-</p>
-<h3>Using the TokenStream API</h3>
-There are a few important things to know in order to use the new API efficiently which are summarized here. You may want
-to walk through the example below first and come back to this section afterwards.
-<ol><li>
-Please keep in mind that an AttributeSource can only have one instance of a particular Attribute. Furthermore, if 
-a chain of a TokenStream and multiple TokenFilters is used, then all TokenFilters in that chain share the Attributes
-with the TokenStream.
-</li>
-<br>
-<li>
-Attribute instances are reused for all tokens of a document. Thus, a TokenStream/-Filter needs to update
-the appropriate Attribute(s) in incrementToken(). The consumer, commonly the Lucene indexer, consumes the data in the
-Attributes and then calls incrementToken() again until it returns false, which indicates that the end of the stream
-was reached. This means that in each call of incrementToken() a TokenStream/-Filter can safely overwrite the data in
-the Attribute instances.
-</li>
-<br>
-<li>
-For performance reasons a TokenStream/-Filter should add/get Attributes during instantiation; i.e., create an attribute in the
-constructor and store references to it in an instance variable.  Using an instance variable instead of calling addAttribute()/getAttribute() 
-in incrementToken() will avoid attribute lookups for every token in the document.
-</li>
-<br>
-<li>
-All methods in AttributeSource are idempotent, which means calling them multiple times always yields the same
-result. This is especially important to know for addAttribute(). The method takes the <b>type</b> (<code>Class</code>)
-of an Attribute as an argument and returns an <b>instance</b>. If an Attribute of the same type was previously added, then
-the already existing instance is returned, otherwise a new instance is created and returned. Therefore TokenStreams/-Filters
-can safely call addAttribute() with the same Attribute type multiple times. Even consumers of TokenStreams should
-normally call addAttribute() instead of getAttribute(), because it would not fail if the TokenStream does not have this
-Attribute (getAttribute() would throw an IllegalArgumentException, if the Attribute is missing). More advanced code
-could simply check with hasAttribute(), if a TokenStream has it, and may conditionally leave out processing for
-extra performance.
-</li></ol>
-<h3>Example</h3>
-<p>
-  In this example we will create a WhiteSpaceTokenizer and use a LengthFilter to suppress all words that have
-  only two or fewer characters. The LengthFilter is part of the Lucene core and its implementation will be explained
-  here to illustrate the usage of the TokenStream API.
-</p>
-<p>
-  Then we will develop a custom Attribute, a PartOfSpeechAttribute, and add another filter to the chain which
-  utilizes the new custom attribute, and call it PartOfSpeechTaggingFilter.
-</p>
-<h4>Whitespace tokenization</h4>
-<pre class="prettyprint">
-public class MyAnalyzer extends Analyzer {
-
-  private Version matchVersion;
-  
-  public MyAnalyzer(Version matchVersion) {
-    this.matchVersion = matchVersion;
-  }
-
-  {@literal @Override}
-  protected TokenStreamComponents createComponents(String fieldName) {
-    return new TokenStreamComponents(new WhitespaceTokenizer(matchVersion));
-  }
-  
-  public static void main(String[] args) throws IOException {
-    // text to tokenize
-    final String text = "This is a demo of the TokenStream API";
-    
-    Version matchVersion = Version.LUCENE_XY; // Substitute desired Lucene version for XY
-    MyAnalyzer analyzer = new MyAnalyzer(matchVersion);
-    TokenStream stream = analyzer.tokenStream("field", new StringReader(text));
-    
-    // get the CharTermAttribute from the TokenStream
-    CharTermAttribute termAtt = stream.addAttribute(CharTermAttribute.class);
-
-    try {
-      stream.reset();
-    
-      // print all tokens until stream is exhausted
-      while (stream.incrementToken()) {
-        System.out.println(termAtt.toString());
-      }
-    
-      stream.end();
-    } finally {
-      stream.close();
-    }
-  }
-}
-</pre>
-In this easy example a simple white space tokenization is performed. In main() a loop consumes the stream and
-prints the term text of the tokens by accessing the CharTermAttribute that the WhitespaceTokenizer provides. 
-Here is the output:
-<pre>
-This
-is
-a
-demo
-of
-the
-new
-TokenStream
-API
-</pre>
-<h4>Adding a LengthFilter</h4>
-We want to suppress all tokens that have 2 or less characters. We can do that
-easily by adding a LengthFilter to the chain. Only the
-<code>createComponents()</code> method in our analyzer needs to be changed:
-<pre class="prettyprint">
-  {@literal @Override}
-  protected TokenStreamComponents createComponents(String fieldName) {
-    final Tokenizer source = new WhitespaceTokenizer(matchVersion);
-    TokenStream result = new LengthFilter(true, source, 3, Integer.MAX_VALUE);
-    return new TokenStreamComponents(source, result);
-  }
-</pre>
-Note how now only words with 3 or more characters are contained in the output:
-<pre>
-This
-demo
-the
-new
-TokenStream
-API
-</pre>
-Now let's take a look how the LengthFilter is implemented:
-<pre class="prettyprint">
-public final class LengthFilter extends FilteringTokenFilter {
-
-  private final int min;
-  private final int max;
-  
-  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-
-  /**
-   * Create a new LengthFilter. This will filter out tokens whose
-   * CharTermAttribute is either too short
-   * (&lt; min) or too long (&gt; max).
-   * @param version the Lucene match version
-   * @param in      the TokenStream to consume
-   * @param min     the minimum length
-   * @param max     the maximum length
-   */
-  public LengthFilter(Version version, TokenStream in, int min, int max) {
-    super(version, in);
-    this.min = min;
-    this.max = max;
-  }
-
-  {@literal @Override}
-  public boolean accept() {
-    final int len = termAtt.length();
-    return (len &gt;= min &amp;&amp; len <= max);
-  }
-
-}
-</pre>
-<p>
-  In LengthFilter, the CharTermAttribute is added and stored in the instance
-  variable <code>termAtt</code>.  Remember that there can only be a single
-  instance of CharTermAttribute in the chain, so in our example the
-  <code>addAttribute()</code> call in LengthFilter returns the
-  CharTermAttribute that the WhitespaceTokenizer already added.
-</p>
-<p>
-  The tokens are retrieved from the input stream in FilteringTokenFilter's 
-  <code>incrementToken()</code> method (see below), which calls LengthFilter's
-  <code>accept()</code> method. By looking at the term text in the
-  CharTermAttribute, the length of the term can be determined and tokens that
-  are either too short or too long are skipped.  Note how
-  <code>accept()</code> can efficiently access the instance variable; no 
-  attribute lookup is necessary. The same is true for the consumer, which can
-  simply use local references to the Attributes.
-</p>
-<p>
-  LengthFilter extends FilteringTokenFilter:
-</p>
-
-<pre class="prettyprint">
-public abstract class FilteringTokenFilter extends TokenFilter {
-
-  private final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);
-
-  /**
-   * Create a new FilteringTokenFilter.
-   * @param in      the TokenStream to consume
-   */
-  public FilteringTokenFilter(Version version, TokenStream in) {
-    super(in);
-  }
-
-  /** Override this method and return if the current input token should be returned by incrementToken. */
-  protected abstract boolean accept() throws IOException;
-
-  {@literal @Override}
-  public final boolean incrementToken() throws IOException {
-    int skippedPositions = 0;
-    while (input.incrementToken()) {
-      if (accept()) {
-        if (skippedPositions != 0) {
-          posIncrAtt.setPositionIncrement(posIncrAtt.getPositionIncrement() + skippedPositions);
-        }
-        return true;
-      }
-      skippedPositions += posIncrAtt.getPositionIncrement();
-    }
-    // reached EOS -- return false
-    return false;
-  }
-
-  {@literal @Override}
-  public void reset() throws IOException {
-    super.reset();
-  }
-
-}
-</pre>
-
-<h4>Adding a custom Attribute</h4>
-Now we're going to implement our own custom Attribute for part-of-speech tagging and call it consequently 
-<code>PartOfSpeechAttribute</code>. First we need to define the interface of the new Attribute:
-<pre class="prettyprint">
-  public interface PartOfSpeechAttribute extends Attribute {
-    public static enum PartOfSpeech {
-      Noun, Verb, Adjective, Adverb, Pronoun, Preposition, Conjunction, Article, Unknown
-    }
-  
-    public void setPartOfSpeech(PartOfSpeech pos);
-  
-    public PartOfSpeech getPartOfSpeech();
-  }
-</pre>
-<p>
-  Now we also need to write the implementing class. The name of that class is important here: By default, Lucene
-  checks if there is a class with the name of the Attribute with the suffix 'Impl'. In this example, we would
-  consequently call the implementing class <code>PartOfSpeechAttributeImpl</code>.
-</p>
-<p>
-  This should be the usual behavior. However, there is also an expert-API that allows changing these naming conventions:
-  {@link org.apache.lucene.util.AttributeFactory}. The factory accepts an Attribute interface as argument
-  and returns an actual instance. You can implement your own factory if you need to change the default behavior.
-</p>
-<p>
-  Now here is the actual class that implements our new Attribute. Notice that the class has to extend
-  {@link org.apache.lucene.util.AttributeImpl}:
-</p>
-<pre class="prettyprint">
-public final class PartOfSpeechAttributeImpl extends AttributeImpl 
-                                  implements PartOfSpeechAttribute {
-  
-  private PartOfSpeech pos = PartOfSpeech.Unknown;
-  
-  public void setPartOfSpeech(PartOfSpeech pos) {
-    this.pos = pos;
-  }
-  
-  public PartOfSpeech getPartOfSpeech() {
-    return pos;
-  }
-
-  {@literal @Override}
-  public void clear() {
-    pos = PartOfSpeech.Unknown;
-  }
-
-  {@literal @Override}
-  public void copyTo(AttributeImpl target) {
-    ((PartOfSpeechAttribute) target).setPartOfSpeech(pos);
-  }
-}
-</pre>
-<p>
-  This is a simple Attribute implementation has only a single variable that
-  stores the part-of-speech of a token. It extends the
-  <code>AttributeImpl</code> class and therefore implements its abstract methods
-  <code>clear()</code> and <code>copyTo()</code>. Now we need a TokenFilter that
-  can set this new PartOfSpeechAttribute for each token. In this example we
-  show a very naive filter that tags every word with a leading upper-case letter
-  as a 'Noun' and all other words as 'Unknown'.
-</p>
-<pre class="prettyprint">
-  public static class PartOfSpeechTaggingFilter extends TokenFilter {
-    PartOfSpeechAttribute posAtt = addAttribute(PartOfSpeechAttribute.class);
-    CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-    
-    protected PartOfSpeechTaggingFilter(TokenStream input) {
-      super(input);
-    }
-    
-    public boolean incrementToken() throws IOException {
-      if (!input.incrementToken()) {return false;}
-      posAtt.setPartOfSpeech(determinePOS(termAtt.buffer(), 0, termAtt.length()));
-      return true;
-    }
-    
-    // determine the part of speech for the given term
-    protected PartOfSpeech determinePOS(char[] term, int offset, int length) {
-      // naive implementation that tags every uppercased word as noun
-      if (length > 0 && Character.isUpperCase(term[0])) {
-        return PartOfSpeech.Noun;
-      }
-      return PartOfSpeech.Unknown;
-    }
-  }
-</pre>
-<p>
-  Just like the LengthFilter, this new filter stores references to the
-  attributes it needs in instance variables. Notice how you only need to pass
-  in the interface of the new Attribute and instantiating the correct class
-  is automatically taken care of.
-</p>
-<p>Now we need to add the filter to the chain in MyAnalyzer:</p>
-<pre class="prettyprint">
-  {@literal @Override}
-  protected TokenStreamComponents createComponents(String fieldName) {
-    final Tokenizer source = new WhitespaceTokenizer(matchVersion);
-    TokenStream result = new LengthFilter(true, source, 3, Integer.MAX_VALUE);
-    result = new PartOfSpeechTaggingFilter(result);
-    return new TokenStreamComponents(source, result);
-  }
-</pre>
-Now let's look at the output:
-<pre>
-This
-demo
-the
-new
-TokenStream
-API
-</pre>
-Apparently it hasn't changed, which shows that adding a custom attribute to a TokenStream/Filter chain does not
-affect any existing consumers, simply because they don't know the new Attribute. Now let's change the consumer
-to make use of the new PartOfSpeechAttribute and print it out:
-<pre class="prettyprint">
-  public static void main(String[] args) throws IOException {
-    // text to tokenize
-    final String text = "This is a demo of the TokenStream API";
-    
-    MyAnalyzer analyzer = new MyAnalyzer();
-    TokenStream stream = analyzer.tokenStream("field", new StringReader(text));
-    
-    // get the CharTermAttribute from the TokenStream
-    CharTermAttribute termAtt = stream.addAttribute(CharTermAttribute.class);
-    
-    // get the PartOfSpeechAttribute from the TokenStream
-    PartOfSpeechAttribute posAtt = stream.addAttribute(PartOfSpeechAttribute.class);
-
-    try {
-      stream.reset();
-
-      // print all tokens until stream is exhausted
-      while (stream.incrementToken()) {
-        System.out.println(termAtt.toString() + ": " + posAtt.getPartOfSpeech());
-      }
-    
-      stream.end();
-    } finally {
-      stream.close();
-    }
-  }
-</pre>
-The change that was made is to get the PartOfSpeechAttribute from the TokenStream and print out its contents in
-the while loop that consumes the stream. Here is the new output:
-<pre>
-This: Noun
-demo: Unknown
-the: Unknown
-new: Unknown
-TokenStream: Noun
-API: Noun
-</pre>
-Each word is now followed by its assigned PartOfSpeech tag. Of course this is a naive 
-part-of-speech tagging. The word 'This' should not even be tagged as noun; it is only spelled capitalized because it
-is the first word of a sentence. Actually this is a good opportunity for an exercise. To practice the usage of the new
-API the reader could now write an Attribute and TokenFilter that can specify for each word if it was the first token
-of a sentence or not. Then the PartOfSpeechTaggingFilter can make use of this knowledge and only tag capitalized words
-as nouns if not the first word of a sentence (we know, this is still not a correct behavior, but hey, it's a good exercise). 
-As a small hint, this is how the new Attribute class could begin:
-<pre class="prettyprint">
-  public class FirstTokenOfSentenceAttributeImpl extends AttributeImpl
-                              implements FirstTokenOfSentenceAttribute {
-    
-    private boolean firstToken;
-    
-    public void setFirstToken(boolean firstToken) {
-      this.firstToken = firstToken;
-    }
-    
-    public boolean getFirstToken() {
-      return firstToken;
-    }
-
-    {@literal @Override}
-    public void clear() {
-      firstToken = false;
-    }
-
-  ...
-</pre>
-<h4>Adding a CharFilter chain</h4>
-Analyzers take Java {@link java.io.Reader}s as input. Of course you can wrap your Readers with {@link java.io.FilterReader}s
-to manipulate content, but this would have the big disadvantage that character offsets might be inconsistent with your original
-text.
-<p>
-{@link org.apache.lucene.analysis.CharFilter} is designed to allow you to pre-process input like a FilterReader would, but also
-preserve the original offsets associated with those characters. This way mechanisms like highlighting still work correctly.
-CharFilters can be chained.
-<p>
-Example:
-<pre class="prettyprint">
-public class MyAnalyzer extends Analyzer {
-
-  {@literal @Override}
-  protected TokenStreamComponents createComponents(String fieldName) {
-    return new TokenStreamComponents(new MyTokenizer());
-  }
-  
-  {@literal @Override}
-  protected Reader initReader(String fieldName, Reader reader) {
-    // wrap the Reader in a CharFilter chain.
-    return new SecondCharFilter(new FirstCharFilter(reader));
-  }
-}
-</pre>
-</body>
-</html>
diff --git a/lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/package-info.java b/lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/package-info.java
new file mode 100644
index 0000000..7ad0029
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/package-info.java
@@ -0,0 +1,21 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * General-purpose attributes for text analysis.
+ */
+package org.apache.lucene.analysis.tokenattributes;
diff --git a/lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/package.html b/lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/package.html
deleted file mode 100644
index d988488..0000000
--- a/lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-General-purpose attributes for text analysis.
-</body>
-</html>
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/blocktree/package-info.java b/lucene/core/src/java/org/apache/lucene/codecs/blocktree/package-info.java
new file mode 100644
index 0000000..9cdbb02
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/blocktree/package-info.java
@@ -0,0 +1,34 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * BlockTree terms dictionary.
+ * 
+ * <p>
+ * This terms dictionary organizes all terms into blocks according to
+ * shared prefix, such that each block has enough terms, and then stores
+ * the prefix trie in memory as an FST as the index structure.  It allows
+ * you to plug in your own {@link
+ * org.apache.lucene.codecs.PostingsWriterBase} to implement the
+ * postings.
+ * </p>
+ * 
+ * <p>See {@link org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter}
+ *   for the file format.
+ * </p>
+ */
+package org.apache.lucene.codecs.blocktree;
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/blocktree/package.html b/lucene/core/src/java/org/apache/lucene/codecs/blocktree/package.html
deleted file mode 100644
index 4ae91f1..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/blocktree/package.html
+++ /dev/null
@@ -1,38 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-BlockTree terms dictionary.
-
-<p>
-This terms dictionary organizes all terms into blocks according to
-shared prefix, such that each block has enough terms, and then stores
-the prefix trie in memory as an FST as the index structure.  It allows
-you to plug in your own {@link
-org.apache.lucene.codecs.PostingsWriterBase} to implement the
-postings.
-</p>
-
-<p>See {@link org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter}
-  for the file format.
-</p>
-</body>
-</html>
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/compressing/package-info.java b/lucene/core/src/java/org/apache/lucene/codecs/compressing/package-info.java
new file mode 100644
index 0000000..9d3ce93
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/compressing/package-info.java
@@ -0,0 +1,21 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+ 
+/**
+ * StoredFieldsFormat that allows cross-document and cross-field compression of stored fields.
+ */
+package org.apache.lucene.codecs.compressing;
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/compressing/package.html b/lucene/core/src/java/org/apache/lucene/codecs/compressing/package.html
deleted file mode 100644
index 4d899f7..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/compressing/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-StoredFieldsFormat that allows cross-document and cross-field compression of stored fields.
-</body>
-</html>
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene50/package-info.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene50/package-info.java
new file mode 100755
index 0000000..c142d91
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene50/package-info.java
@@ -0,0 +1,400 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Lucene 5.0 file format.
+ * 
+ * <h1>Apache Lucene - Index File Formats</h1>
+ * <div>
+ * <ul>
+ * <li><a href="#Introduction">Introduction</a></li>
+ * <li><a href="#Definitions">Definitions</a>
+ *   <ul>
+ *   <li><a href="#Inverted_Indexing">Inverted Indexing</a></li>
+ *   <li><a href="#Types_of_Fields">Types of Fields</a></li>
+ *   <li><a href="#Segments">Segments</a></li>
+ *   <li><a href="#Document_Numbers">Document Numbers</a></li>
+ *   </ul>
+ * </li>
+ * <li><a href="#Overview">Index Structure Overview</a></li>
+ * <li><a href="#File_Naming">File Naming</a></li>
+ * <li><a href="#file-names">Summary of File Extensions</a></li>
+ *   <ul>
+ *   <li><a href="#Lock_File">Lock File</a></li>
+ *   <li><a href="#History">History</a></li>
+ *   <li><a href="#Limitations">Limitations</a></li>
+ *   </ul>
+ * </ul>
+ * </div>
+ * <a name="Introduction"></a>
+ * <h2>Introduction</h2>
+ * <div>
+ * <p>This document defines the index file formats used in this version of Lucene.
+ * If you are using a different version of Lucene, please consult the copy of
+ * <code>docs/</code> that was distributed with
+ * the version you are using.</p>
+ * <p>Apache Lucene is written in Java, but several efforts are underway to write
+ * <a href="http://wiki.apache.org/lucene-java/LuceneImplementations">versions of
+ * Lucene in other programming languages</a>. If these versions are to remain
+ * compatible with Apache Lucene, then a language-independent definition of the
+ * Lucene index format is required. This document thus attempts to provide a
+ * complete and independent definition of the Apache Lucene file formats.</p>
+ * <p>As Lucene evolves, this document should evolve. Versions of Lucene in
+ * different programming languages should endeavor to agree on file formats, and
+ * generate new versions of this document.</p>
+ * </div>
+ * <a name="Definitions" id="Definitions"></a>
+ * <h2>Definitions</h2>
+ * <div>
+ * <p>The fundamental concepts in Lucene are index, document, field and term.</p>
+ * <p>An index contains a sequence of documents.</p>
+ * <ul>
+ * <li>A document is a sequence of fields.</li>
+ * <li>A field is a named sequence of terms.</li>
+ * <li>A term is a sequence of bytes.</li>
+ * </ul>
+ * <p>The same sequence of bytes in two different fields is considered a different 
+ * term. Thus terms are represented as a pair: the string naming the field, and the
+ * bytes within the field.</p>
+ * <a name="Inverted_Indexing"></a>
+ * <h3>Inverted Indexing</h3>
+ * <p>The index stores statistics about terms in order to make term-based search
+ * more efficient. Lucene's index falls into the family of indexes known as an
+ * <i>inverted index.</i> This is because it can list, for a term, the documents
+ * that contain it. This is the inverse of the natural relationship, in which
+ * documents list terms.</p>
+ * <a name="Types_of_Fields"></a>
+ * <h3>Types of Fields</h3>
+ * <p>In Lucene, fields may be <i>stored</i>, in which case their text is stored
+ * in the index literally, in a non-inverted manner. Fields that are inverted are
+ * called <i>indexed</i>. A field may be both stored and indexed.</p>
+ * <p>The text of a field may be <i>tokenized</i> into terms to be indexed, or the
+ * text of a field may be used literally as a term to be indexed. Most fields are
+ * tokenized, but sometimes it is useful for certain identifier fields to be
+ * indexed literally.</p>
+ * <p>See the {@link org.apache.lucene.document.Field Field}
+ * java docs for more information on Fields.</p>
+ * <a name="Segments" id="Segments"></a>
+ * <h3>Segments</h3>
+ * <p>Lucene indexes may be composed of multiple sub-indexes, or <i>segments</i>.
+ * Each segment is a fully independent index, which could be searched separately.
+ * Indexes evolve by:</p>
+ * <ol>
+ * <li>Creating new segments for newly added documents.</li>
+ * <li>Merging existing segments.</li>
+ * </ol>
+ * <p>Searches may involve multiple segments and/or multiple indexes, each index
+ * potentially composed of a set of segments.</p>
+ * <a name="Document_Numbers"></a>
+ * <h3>Document Numbers</h3>
+ * <p>Internally, Lucene refers to documents by an integer <i>document number</i>.
+ * The first document added to an index is numbered zero, and each subsequent
+ * document added gets a number one greater than the previous.</p>
+ * <p>Note that a document's number may change, so caution should be taken when
+ * storing these numbers outside of Lucene. In particular, numbers may change in
+ * the following situations:</p>
+ * <ul>
+ * <li>
+ * <p>The numbers stored in each segment are unique only within the segment, and
+ * must be converted before they can be used in a larger context. The standard
+ * technique is to allocate each segment a range of values, based on the range of
+ * numbers used in that segment. To convert a document number from a segment to an
+ * external value, the segment's <i>base</i> document number is added. To convert
+ * an external value back to a segment-specific value, the segment is identified
+ * by the range that the external value is in, and the segment's base value is
+ * subtracted. For example two five document segments might be combined, so that
+ * the first segment has a base value of zero, and the second of five. Document
+ * three from the second segment would have an external value of eight.</p>
+ * </li>
+ * <li>
+ * <p>When documents are deleted, gaps are created in the numbering. These are
+ * eventually removed as the index evolves through merging. Deleted documents are
+ * dropped when segments are merged. A freshly-merged segment thus has no gaps in
+ * its numbering.</p>
+ * </li>
+ * </ul>
+ * </div>
+ * <a name="Overview" id="Overview"></a>
+ * <h2>Index Structure Overview</h2>
+ * <div>
+ * <p>Each segment index maintains the following:</p>
+ * <ul>
+ * <li>
+ * {@link org.apache.lucene.codecs.lucene50.Lucene50SegmentInfoFormat Segment info}.
+ *    This contains metadata about a segment, such as the number of documents,
+ *    what files it uses, 
+ * </li>
+ * <li>
+ * {@link org.apache.lucene.codecs.lucene50.Lucene50FieldInfosFormat Field names}. 
+ *    This contains the set of field names used in the index.
+ * </li>
+ * <li>
+ * {@link org.apache.lucene.codecs.lucene50.Lucene50StoredFieldsFormat Stored Field values}. 
+ * This contains, for each document, a list of attribute-value pairs, where the attributes 
+ * are field names. These are used to store auxiliary information about the document, such as 
+ * its title, url, or an identifier to access a database. The set of stored fields are what is 
+ * returned for each hit when searching. This is keyed by document number.
+ * </li>
+ * <li>
+ * {@link org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat Term dictionary}. 
+ * A dictionary containing all of the terms used in all of the
+ * indexed fields of all of the documents. The dictionary also contains the number
+ * of documents which contain the term, and pointers to the term's frequency and
+ * proximity data.
+ * </li>
+ * <li>
+ * {@link org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat Term Frequency data}. 
+ * For each term in the dictionary, the numbers of all the
+ * documents that contain that term, and the frequency of the term in that
+ * document, unless frequencies are omitted (IndexOptions.DOCS_ONLY)
+ * </li>
+ * <li>
+ * {@link org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat Term Proximity data}. 
+ * For each term in the dictionary, the positions that the
+ * term occurs in each document. Note that this will not exist if all fields in
+ * all documents omit position data.
+ * </li>
+ * <li>
+ * {@link org.apache.lucene.codecs.lucene50.Lucene50NormsFormat Normalization factors}. 
+ * For each field in each document, a value is stored
+ * that is multiplied into the score for hits on that field.
+ * </li>
+ * <li>
+ * {@link org.apache.lucene.codecs.lucene50.Lucene50TermVectorsFormat Term Vectors}. 
+ * For each field in each document, the term vector (sometimes
+ * called document vector) may be stored. A term vector consists of term text and
+ * term frequency. To add Term Vectors to your index see the 
+ * {@link org.apache.lucene.document.Field Field} constructors
+ * </li>
+ * <li>
+ * {@link org.apache.lucene.codecs.lucene50.Lucene50DocValuesFormat Per-document values}. 
+ * Like stored values, these are also keyed by document
+ * number, but are generally intended to be loaded into main memory for fast
+ * access. Whereas stored values are generally intended for summary results from
+ * searches, per-document values are useful for things like scoring factors.
+ * </li>
+ * <li>
+ * {@link org.apache.lucene.codecs.lucene50.Lucene50LiveDocsFormat Live documents}. 
+ * An optional file indicating which documents are live.
+ * </li>
+ * </ul>
+ * <p>Details on each of these are provided in their linked pages.</p>
+ * </div>
+ * <a name="File_Naming"></a>
+ * <h2>File Naming</h2>
+ * <div>
+ * <p>All files belonging to a segment have the same name with varying extensions.
+ * The extensions correspond to the different file formats described below. When
+ * using the Compound File format (default in 1.4 and greater) these files (except
+ * for the Segment info file, the Lock file, and Deleted documents file) are collapsed 
+ * into a single .cfs file (see below for details)</p>
+ * <p>Typically, all segments in an index are stored in a single directory,
+ * although this is not required.</p>
+ * <p>As of version 2.1 (lock-less commits), file names are never re-used.
+ * That is, when any file is saved
+ * to the Directory it is given a never before used filename. This is achieved
+ * using a simple generations approach. For example, the first segments file is
+ * segments_1, then segments_2, etc. The generation is a sequential long integer
+ * represented in alpha-numeric (base 36) form.</p>
+ * </div>
+ * <a name="file-names" id="file-names"></a>
+ * <h2>Summary of File Extensions</h2>
+ * <div>
+ * <p>The following table summarizes the names and extensions of the files in
+ * Lucene:</p>
+ * <table cellspacing="1" cellpadding="4" summary="lucene filenames by extension">
+ * <tr>
+ * <th>Name</th>
+ * <th>Extension</th>
+ * <th>Brief Description</th>
+ * </tr>
+ * <tr>
+ * <td>{@link org.apache.lucene.index.SegmentInfos Segments File}</td>
+ * <td>segments_N</td>
+ * <td>Stores information about a commit point</td>
+ * </tr>
+ * <tr>
+ * <td><a href="#Lock_File">Lock File</a></td>
+ * <td>write.lock</td>
+ * <td>The Write lock prevents multiple IndexWriters from writing to the same
+ * file.</td>
+ * </tr>
+ * <tr>
+ * <td>{@link org.apache.lucene.codecs.lucene50.Lucene50SegmentInfoFormat Segment Info}</td>
+ * <td>.si</td>
+ * <td>Stores metadata about a segment</td>
+ * </tr>
+ * <tr>
+ * <td>{@link org.apache.lucene.codecs.lucene50.Lucene50CompoundFormat Compound File}</td>
+ * <td>.cfs, .cfe</td>
+ * <td>An optional "virtual" file consisting of all the other index files for
+ * systems that frequently run out of file handles.</td>
+ * </tr>
+ * <tr>
+ * <td>{@link org.apache.lucene.codecs.lucene50.Lucene50FieldInfosFormat Fields}</td>
+ * <td>.fnm</td>
+ * <td>Stores information about the fields</td>
+ * </tr>
+ * <tr>
+ * <td>{@link org.apache.lucene.codecs.lucene50.Lucene50StoredFieldsFormat Field Index}</td>
+ * <td>.fdx</td>
+ * <td>Contains pointers to field data</td>
+ * </tr>
+ * <tr>
+ * <td>{@link org.apache.lucene.codecs.lucene50.Lucene50StoredFieldsFormat Field Data}</td>
+ * <td>.fdt</td>
+ * <td>The stored fields for documents</td>
+ * </tr>
+ * <tr>
+ * <td>{@link org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat Term Dictionary}</td>
+ * <td>.tim</td>
+ * <td>The term dictionary, stores term info</td>
+ * </tr>
+ * <tr>
+ * <td>{@link org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat Term Index}</td>
+ * <td>.tip</td>
+ * <td>The index into the Term Dictionary</td>
+ * </tr>
+ * <tr>
+ * <td>{@link org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat Frequencies}</td>
+ * <td>.doc</td>
+ * <td>Contains the list of docs which contain each term along with frequency</td>
+ * </tr>
+ * <tr>
+ * <td>{@link org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat Positions}</td>
+ * <td>.pos</td>
+ * <td>Stores position information about where a term occurs in the index</td>
+ * </tr>
+ * <tr>
+ * <td>{@link org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat Payloads}</td>
+ * <td>.pay</td>
+ * <td>Stores additional per-position metadata information such as character offsets and user payloads</td>
+ * </tr>
+ * <tr>
+ * <td>{@link org.apache.lucene.codecs.lucene50.Lucene50NormsFormat Norms}</td>
+ * <td>.nvd, .nvm</td>
+ * <td>Encodes length and boost factors for docs and fields</td>
+ * </tr>
+ * <tr>
+ * <td>{@link org.apache.lucene.codecs.lucene50.Lucene50DocValuesFormat Per-Document Values}</td>
+ * <td>.dvd, .dvm</td>
+ * <td>Encodes additional scoring factors or other per-document information.</td>
+ * </tr>
+ * <tr>
+ * <td>{@link org.apache.lucene.codecs.lucene50.Lucene50TermVectorsFormat Term Vector Index}</td>
+ * <td>.tvx</td>
+ * <td>Stores offset into the document data file</td>
+ * </tr>
+ * <tr>
+ * <td>{@link org.apache.lucene.codecs.lucene50.Lucene50TermVectorsFormat Term Vector Documents}</td>
+ * <td>.tvd</td>
+ * <td>Contains information about each document that has term vectors</td>
+ * </tr>
+ * <tr>
+ * <td>{@link org.apache.lucene.codecs.lucene50.Lucene50TermVectorsFormat Term Vector Fields}</td>
+ * <td>.tvf</td>
+ * <td>The field level info about term vectors</td>
+ * </tr>
+ * <tr>
+ * <td>{@link org.apache.lucene.codecs.lucene50.Lucene50LiveDocsFormat Live Documents}</td>
+ * <td>.liv</td>
+ * <td>Info about what files are live</td>
+ * </tr>
+ * </table>
+ * </div>
+ * <a name="Lock_File" id="Lock_File"></a>
+ * <h2>Lock File</h2>
+ * The write lock, which is stored in the index directory by default, is named
+ * "write.lock". If the lock directory is different from the index directory then
+ * the write lock will be named "XXXX-write.lock" where XXXX is a unique prefix
+ * derived from the full path to the index directory. When this file is present, a
+ * writer is currently modifying the index (adding or removing documents). This
+ * lock file ensures that only one writer is modifying the index at a time.</p>
+ * <a name="History"></a>
+ * <h2>History</h2>
+ * <p>Compatibility notes are provided in this document, describing how file
+ * formats have changed from prior versions:</p>
+ * <ul>
+ * <li>In version 2.1, the file format was changed to allow lock-less commits (ie,
+ * no more commit lock). The change is fully backwards compatible: you can open a
+ * pre-2.1 index for searching or adding/deleting of docs. When the new segments
+ * file is saved (committed), it will be written in the new file format (meaning
+ * no specific "upgrade" process is needed). But note that once a commit has
+ * occurred, pre-2.1 Lucene will not be able to read the index.</li>
+ * <li>In version 2.3, the file format was changed to allow segments to share a
+ * single set of doc store (vectors &amp; stored fields) files. This allows for
+ * faster indexing in certain cases. The change is fully backwards compatible (in
+ * the same way as the lock-less commits change in 2.1).</li>
+ * <li>In version 2.4, Strings are now written as true UTF-8 byte sequence, not
+ * Java's modified UTF-8. See <a href="http://issues.apache.org/jira/browse/LUCENE-510">
+ * LUCENE-510</a> for details.</li>
+ * <li>In version 2.9, an optional opaque Map&lt;String,String&gt; CommitUserData
+ * may be passed to IndexWriter's commit methods (and later retrieved), which is
+ * recorded in the segments_N file. See <a href="http://issues.apache.org/jira/browse/LUCENE-1382">
+ * LUCENE-1382</a> for details. Also,
+ * diagnostics were added to each segment written recording details about why it
+ * was written (due to flush, merge; which OS/JRE was used; etc.). See issue
+ * <a href="http://issues.apache.org/jira/browse/LUCENE-1654">LUCENE-1654</a> for details.</li>
+ * <li>In version 3.0, compressed fields are no longer written to the index (they
+ * can still be read, but on merge the new segment will write them, uncompressed).
+ * See issue <a href="http://issues.apache.org/jira/browse/LUCENE-1960">LUCENE-1960</a> 
+ * for details.</li>
+ * <li>In version 3.1, segments records the code version that created them. See
+ * <a href="http://issues.apache.org/jira/browse/LUCENE-2720">LUCENE-2720</a> for details. 
+ * Additionally segments track explicitly whether or not they have term vectors. 
+ * See <a href="http://issues.apache.org/jira/browse/LUCENE-2811">LUCENE-2811</a> 
+ * for details.</li>
+ * <li>In version 3.2, numeric fields are written as natively to stored fields
+ * file, previously they were stored in text format only.</li>
+ * <li>In version 3.4, fields can omit position data while still indexing term
+ * frequencies.</li>
+ * <li>In version 4.0, the format of the inverted index became extensible via
+ * the {@link org.apache.lucene.codecs.Codec Codec} api. Fast per-document storage
+ * ({@code DocValues}) was introduced. Normalization factors need no longer be a 
+ * single byte, they can be any {@link org.apache.lucene.index.NumericDocValues NumericDocValues}. 
+ * Terms need not be unicode strings, they can be any byte sequence. Term offsets 
+ * can optionally be indexed into the postings lists. Payloads can be stored in the 
+ * term vectors.</li>
+ * <li>In version 4.1, the format of the postings list changed to use either
+ * of FOR compression or variable-byte encoding, depending upon the frequency
+ * of the term. Terms appearing only once were changed to inline directly into
+ * the term dictionary. Stored fields are compressed by default. </li>
+ * <li>In version 4.2, term vectors are compressed by default. DocValues has 
+ * a new multi-valued type (SortedSet), that can be used for faceting/grouping/joining
+ * on multi-valued fields.</li>
+ * <li>In version 4.5, DocValues were extended to explicitly represent missing values.</li>
+ * <li>In version 4.6, FieldInfos were extended to support per-field DocValues generation, to 
+ * allow updating NumericDocValues fields.</li>
+ * <li>In version 4.8, checksum footers were added to the end of each index file 
+ * for improved data integrity. Specifically, the last 8 bytes of every index file
+ * contain the zlib-crc32 checksum of the file.</li>
+ * <li>In version 4.9, DocValues has a new multi-valued numeric type (SortedNumeric)
+ * that is suitable for faceting/sorting/analytics.
+ * </li>
+ * </ul>
+ * <a name="Limitations" id="Limitations"></a>
+ * <h2>Limitations</h2>
+ * <div>
+ * <p>Lucene uses a Java <code>int</code> to refer to
+ * document numbers, and the index file format uses an <code>Int32</code>
+ * on-disk to store document numbers. This is a limitation
+ * of both the index file format and the current implementation. Eventually these
+ * should be replaced with either <code>UInt64</code> values, or
+ * better yet, {@link org.apache.lucene.store.DataOutput#writeVInt VInt} values which have no limit.</p>
+ * </div>
+ */
+package org.apache.lucene.codecs.lucene50;
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene50/package.html b/lucene/core/src/java/org/apache/lucene/codecs/lucene50/package.html
deleted file mode 100755
index 76777ec..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene50/package.html
+++ /dev/null
@@ -1,404 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Lucene 5.0 file format.
-
-<h1>Apache Lucene - Index File Formats</h1>
-<div>
-<ul>
-<li><a href="#Introduction">Introduction</a></li>
-<li><a href="#Definitions">Definitions</a>
-  <ul>
-  <li><a href="#Inverted_Indexing">Inverted Indexing</a></li>
-  <li><a href="#Types_of_Fields">Types of Fields</a></li>
-  <li><a href="#Segments">Segments</a></li>
-  <li><a href="#Document_Numbers">Document Numbers</a></li>
-  </ul>
-</li>
-<li><a href="#Overview">Index Structure Overview</a></li>
-<li><a href="#File_Naming">File Naming</a></li>
-<li><a href="#file-names">Summary of File Extensions</a></li>
-  <ul>
-  <li><a href="#Lock_File">Lock File</a></li>
-  <li><a href="#History">History</a></li>
-  <li><a href="#Limitations">Limitations</a></li>
-  </ul>
-</ul>
-</div>
-<a name="Introduction"></a>
-<h2>Introduction</h2>
-<div>
-<p>This document defines the index file formats used in this version of Lucene.
-If you are using a different version of Lucene, please consult the copy of
-<code>docs/</code> that was distributed with
-the version you are using.</p>
-<p>Apache Lucene is written in Java, but several efforts are underway to write
-<a href="http://wiki.apache.org/lucene-java/LuceneImplementations">versions of
-Lucene in other programming languages</a>. If these versions are to remain
-compatible with Apache Lucene, then a language-independent definition of the
-Lucene index format is required. This document thus attempts to provide a
-complete and independent definition of the Apache Lucene file formats.</p>
-<p>As Lucene evolves, this document should evolve. Versions of Lucene in
-different programming languages should endeavor to agree on file formats, and
-generate new versions of this document.</p>
-</div>
-<a name="Definitions" id="Definitions"></a>
-<h2>Definitions</h2>
-<div>
-<p>The fundamental concepts in Lucene are index, document, field and term.</p>
-<p>An index contains a sequence of documents.</p>
-<ul>
-<li>A document is a sequence of fields.</li>
-<li>A field is a named sequence of terms.</li>
-<li>A term is a sequence of bytes.</li>
-</ul>
-<p>The same sequence of bytes in two different fields is considered a different 
-term. Thus terms are represented as a pair: the string naming the field, and the
-bytes within the field.</p>
-<a name="Inverted_Indexing"></a>
-<h3>Inverted Indexing</h3>
-<p>The index stores statistics about terms in order to make term-based search
-more efficient. Lucene's index falls into the family of indexes known as an
-<i>inverted index.</i> This is because it can list, for a term, the documents
-that contain it. This is the inverse of the natural relationship, in which
-documents list terms.</p>
-<a name="Types_of_Fields"></a>
-<h3>Types of Fields</h3>
-<p>In Lucene, fields may be <i>stored</i>, in which case their text is stored
-in the index literally, in a non-inverted manner. Fields that are inverted are
-called <i>indexed</i>. A field may be both stored and indexed.</p>
-<p>The text of a field may be <i>tokenized</i> into terms to be indexed, or the
-text of a field may be used literally as a term to be indexed. Most fields are
-tokenized, but sometimes it is useful for certain identifier fields to be
-indexed literally.</p>
-<p>See the {@link org.apache.lucene.document.Field Field}
-java docs for more information on Fields.</p>
-<a name="Segments" id="Segments"></a>
-<h3>Segments</h3>
-<p>Lucene indexes may be composed of multiple sub-indexes, or <i>segments</i>.
-Each segment is a fully independent index, which could be searched separately.
-Indexes evolve by:</p>
-<ol>
-<li>Creating new segments for newly added documents.</li>
-<li>Merging existing segments.</li>
-</ol>
-<p>Searches may involve multiple segments and/or multiple indexes, each index
-potentially composed of a set of segments.</p>
-<a name="Document_Numbers"></a>
-<h3>Document Numbers</h3>
-<p>Internally, Lucene refers to documents by an integer <i>document number</i>.
-The first document added to an index is numbered zero, and each subsequent
-document added gets a number one greater than the previous.</p>
-<p>Note that a document's number may change, so caution should be taken when
-storing these numbers outside of Lucene. In particular, numbers may change in
-the following situations:</p>
-<ul>
-<li>
-<p>The numbers stored in each segment are unique only within the segment, and
-must be converted before they can be used in a larger context. The standard
-technique is to allocate each segment a range of values, based on the range of
-numbers used in that segment. To convert a document number from a segment to an
-external value, the segment's <i>base</i> document number is added. To convert
-an external value back to a segment-specific value, the segment is identified
-by the range that the external value is in, and the segment's base value is
-subtracted. For example two five document segments might be combined, so that
-the first segment has a base value of zero, and the second of five. Document
-three from the second segment would have an external value of eight.</p>
-</li>
-<li>
-<p>When documents are deleted, gaps are created in the numbering. These are
-eventually removed as the index evolves through merging. Deleted documents are
-dropped when segments are merged. A freshly-merged segment thus has no gaps in
-its numbering.</p>
-</li>
-</ul>
-</div>
-<a name="Overview" id="Overview"></a>
-<h2>Index Structure Overview</h2>
-<div>
-<p>Each segment index maintains the following:</p>
-<ul>
-<li>
-{@link org.apache.lucene.codecs.lucene50.Lucene50SegmentInfoFormat Segment info}.
-   This contains metadata about a segment, such as the number of documents,
-   what files it uses, 
-</li>
-<li>
-{@link org.apache.lucene.codecs.lucene50.Lucene50FieldInfosFormat Field names}. 
-   This contains the set of field names used in the index.
-</li>
-<li>
-{@link org.apache.lucene.codecs.lucene50.Lucene50StoredFieldsFormat Stored Field values}. 
-This contains, for each document, a list of attribute-value pairs, where the attributes 
-are field names. These are used to store auxiliary information about the document, such as 
-its title, url, or an identifier to access a database. The set of stored fields are what is 
-returned for each hit when searching. This is keyed by document number.
-</li>
-<li>
-{@link org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat Term dictionary}. 
-A dictionary containing all of the terms used in all of the
-indexed fields of all of the documents. The dictionary also contains the number
-of documents which contain the term, and pointers to the term's frequency and
-proximity data.
-</li>
-<li>
-{@link org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat Term Frequency data}. 
-For each term in the dictionary, the numbers of all the
-documents that contain that term, and the frequency of the term in that
-document, unless frequencies are omitted (IndexOptions.DOCS_ONLY)
-</li>
-<li>
-{@link org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat Term Proximity data}. 
-For each term in the dictionary, the positions that the
-term occurs in each document. Note that this will not exist if all fields in
-all documents omit position data.
-</li>
-<li>
-{@link org.apache.lucene.codecs.lucene50.Lucene50NormsFormat Normalization factors}. 
-For each field in each document, a value is stored
-that is multiplied into the score for hits on that field.
-</li>
-<li>
-{@link org.apache.lucene.codecs.lucene50.Lucene50TermVectorsFormat Term Vectors}. 
-For each field in each document, the term vector (sometimes
-called document vector) may be stored. A term vector consists of term text and
-term frequency. To add Term Vectors to your index see the 
-{@link org.apache.lucene.document.Field Field} constructors
-</li>
-<li>
-{@link org.apache.lucene.codecs.lucene50.Lucene50DocValuesFormat Per-document values}. 
-Like stored values, these are also keyed by document
-number, but are generally intended to be loaded into main memory for fast
-access. Whereas stored values are generally intended for summary results from
-searches, per-document values are useful for things like scoring factors.
-</li>
-<li>
-{@link org.apache.lucene.codecs.lucene50.Lucene50LiveDocsFormat Live documents}. 
-An optional file indicating which documents are live.
-</li>
-</ul>
-<p>Details on each of these are provided in their linked pages.</p>
-</div>
-<a name="File_Naming"></a>
-<h2>File Naming</h2>
-<div>
-<p>All files belonging to a segment have the same name with varying extensions.
-The extensions correspond to the different file formats described below. When
-using the Compound File format (default in 1.4 and greater) these files (except
-for the Segment info file, the Lock file, and Deleted documents file) are collapsed 
-into a single .cfs file (see below for details)</p>
-<p>Typically, all segments in an index are stored in a single directory,
-although this is not required.</p>
-<p>As of version 2.1 (lock-less commits), file names are never re-used.
-That is, when any file is saved
-to the Directory it is given a never before used filename. This is achieved
-using a simple generations approach. For example, the first segments file is
-segments_1, then segments_2, etc. The generation is a sequential long integer
-represented in alpha-numeric (base 36) form.</p>
-</div>
-<a name="file-names" id="file-names"></a>
-<h2>Summary of File Extensions</h2>
-<div>
-<p>The following table summarizes the names and extensions of the files in
-Lucene:</p>
-<table cellspacing="1" cellpadding="4">
-<tr>
-<th>Name</th>
-<th>Extension</th>
-<th>Brief Description</th>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.index.SegmentInfos Segments File}</td>
-<td>segments_N</td>
-<td>Stores information about a commit point</td>
-</tr>
-<tr>
-<td><a href="#Lock_File">Lock File</a></td>
-<td>write.lock</td>
-<td>The Write lock prevents multiple IndexWriters from writing to the same
-file.</td>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.codecs.lucene50.Lucene50SegmentInfoFormat Segment Info}</td>
-<td>.si</td>
-<td>Stores metadata about a segment</td>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.codecs.lucene50.Lucene50CompoundFormat Compound File}</td>
-<td>.cfs, .cfe</td>
-<td>An optional "virtual" file consisting of all the other index files for
-systems that frequently run out of file handles.</td>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.codecs.lucene50.Lucene50FieldInfosFormat Fields}</td>
-<td>.fnm</td>
-<td>Stores information about the fields</td>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.codecs.lucene50.Lucene50StoredFieldsFormat Field Index}</td>
-<td>.fdx</td>
-<td>Contains pointers to field data</td>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.codecs.lucene50.Lucene50StoredFieldsFormat Field Data}</td>
-<td>.fdt</td>
-<td>The stored fields for documents</td>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat Term Dictionary}</td>
-<td>.tim</td>
-<td>The term dictionary, stores term info</td>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat Term Index}</td>
-<td>.tip</td>
-<td>The index into the Term Dictionary</td>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat Frequencies}</td>
-<td>.doc</td>
-<td>Contains the list of docs which contain each term along with frequency</td>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat Positions}</td>
-<td>.pos</td>
-<td>Stores position information about where a term occurs in the index</td>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat Payloads}</td>
-<td>.pay</td>
-<td>Stores additional per-position metadata information such as character offsets and user payloads</td>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.codecs.lucene50.Lucene50NormsFormat Norms}</td>
-<td>.nvd, .nvm</td>
-<td>Encodes length and boost factors for docs and fields</td>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.codecs.lucene50.Lucene50DocValuesFormat Per-Document Values}</td>
-<td>.dvd, .dvm</td>
-<td>Encodes additional scoring factors or other per-document information.</td>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.codecs.lucene50.Lucene50TermVectorsFormat Term Vector Index}</td>
-<td>.tvx</td>
-<td>Stores offset into the document data file</td>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.codecs.lucene50.Lucene50TermVectorsFormat Term Vector Documents}</td>
-<td>.tvd</td>
-<td>Contains information about each document that has term vectors</td>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.codecs.lucene50.Lucene50TermVectorsFormat Term Vector Fields}</td>
-<td>.tvf</td>
-<td>The field level info about term vectors</td>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.codecs.lucene50.Lucene50LiveDocsFormat Live Documents}</td>
-<td>.liv</td>
-<td>Info about what files are live</td>
-</tr>
-</table>
-</div>
-<a name="Lock_File" id="Lock_File"></a>
-<h2>Lock File</h2>
-The write lock, which is stored in the index directory by default, is named
-"write.lock". If the lock directory is different from the index directory then
-the write lock will be named "XXXX-write.lock" where XXXX is a unique prefix
-derived from the full path to the index directory. When this file is present, a
-writer is currently modifying the index (adding or removing documents). This
-lock file ensures that only one writer is modifying the index at a time.</p>
-<a name="History"></a>
-<h2>History</h2>
-<p>Compatibility notes are provided in this document, describing how file
-formats have changed from prior versions:</p>
-<ul>
-<li>In version 2.1, the file format was changed to allow lock-less commits (ie,
-no more commit lock). The change is fully backwards compatible: you can open a
-pre-2.1 index for searching or adding/deleting of docs. When the new segments
-file is saved (committed), it will be written in the new file format (meaning
-no specific "upgrade" process is needed). But note that once a commit has
-occurred, pre-2.1 Lucene will not be able to read the index.</li>
-<li>In version 2.3, the file format was changed to allow segments to share a
-single set of doc store (vectors &amp; stored fields) files. This allows for
-faster indexing in certain cases. The change is fully backwards compatible (in
-the same way as the lock-less commits change in 2.1).</li>
-<li>In version 2.4, Strings are now written as true UTF-8 byte sequence, not
-Java's modified UTF-8. See <a href="http://issues.apache.org/jira/browse/LUCENE-510">
-LUCENE-510</a> for details.</li>
-<li>In version 2.9, an optional opaque Map&lt;String,String&gt; CommitUserData
-may be passed to IndexWriter's commit methods (and later retrieved), which is
-recorded in the segments_N file. See <a href="http://issues.apache.org/jira/browse/LUCENE-1382">
-LUCENE-1382</a> for details. Also,
-diagnostics were added to each segment written recording details about why it
-was written (due to flush, merge; which OS/JRE was used; etc.). See issue
-<a href="http://issues.apache.org/jira/browse/LUCENE-1654">LUCENE-1654</a> for details.</li>
-<li>In version 3.0, compressed fields are no longer written to the index (they
-can still be read, but on merge the new segment will write them, uncompressed).
-See issue <a href="http://issues.apache.org/jira/browse/LUCENE-1960">LUCENE-1960</a> 
-for details.</li>
-<li>In version 3.1, segments records the code version that created them. See
-<a href="http://issues.apache.org/jira/browse/LUCENE-2720">LUCENE-2720</a> for details. 
-Additionally segments track explicitly whether or not they have term vectors. 
-See <a href="http://issues.apache.org/jira/browse/LUCENE-2811">LUCENE-2811</a> 
-for details.</li>
-<li>In version 3.2, numeric fields are written as natively to stored fields
-file, previously they were stored in text format only.</li>
-<li>In version 3.4, fields can omit position data while still indexing term
-frequencies.</li>
-<li>In version 4.0, the format of the inverted index became extensible via
-the {@link org.apache.lucene.codecs.Codec Codec} api. Fast per-document storage
-({@code DocValues}) was introduced. Normalization factors need no longer be a 
-single byte, they can be any {@link org.apache.lucene.index.NumericDocValues NumericDocValues}. 
-Terms need not be unicode strings, they can be any byte sequence. Term offsets 
-can optionally be indexed into the postings lists. Payloads can be stored in the 
-term vectors.</li>
-<li>In version 4.1, the format of the postings list changed to use either
-of FOR compression or variable-byte encoding, depending upon the frequency
-of the term. Terms appearing only once were changed to inline directly into
-the term dictionary. Stored fields are compressed by default. </li>
-<li>In version 4.2, term vectors are compressed by default. DocValues has 
-a new multi-valued type (SortedSet), that can be used for faceting/grouping/joining
-on multi-valued fields.</li>
-<li>In version 4.5, DocValues were extended to explicitly represent missing values.</li>
-<li>In version 4.6, FieldInfos were extended to support per-field DocValues generation, to 
-allow updating NumericDocValues fields.</li>
-<li>In version 4.8, checksum footers were added to the end of each index file 
-for improved data integrity. Specifically, the last 8 bytes of every index file
-contain the zlib-crc32 checksum of the file.</li>
-<li>In version 4.9, DocValues has a new multi-valued numeric type (SortedNumeric)
-that is suitable for faceting/sorting/analytics.
-</li>
-</ul>
-<a name="Limitations" id="Limitations"></a>
-<h2>Limitations</h2>
-<div>
-<p>Lucene uses a Java <code>int</code> to refer to
-document numbers, and the index file format uses an <code>Int32</code>
-on-disk to store document numbers. This is a limitation
-of both the index file format and the current implementation. Eventually these
-should be replaced with either <code>UInt64</code> values, or
-better yet, {@link org.apache.lucene.store.DataOutput#writeVInt VInt} values which have no limit.</p>
-</div>
-</body>
-</html>
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/package-info.java b/lucene/core/src/java/org/apache/lucene/codecs/package-info.java
new file mode 100644
index 0000000..43faa3a
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/package-info.java
@@ -0,0 +1,69 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Codecs API: API for customization of the encoding and structure of the index.
+ * 
+ * <p>
+ *   The Codec API allows you to customise the way the following pieces of index information are stored:
+ * <ul>
+ *   <li>Postings lists - see {@link org.apache.lucene.codecs.PostingsFormat}</li>
+ *   <li>DocValues - see {@link org.apache.lucene.codecs.DocValuesFormat}</li>
+ *   <li>Stored fields - see {@link org.apache.lucene.codecs.StoredFieldsFormat}</li>
+ *   <li>Term vectors - see {@link org.apache.lucene.codecs.TermVectorsFormat}</li>
+ *   <li>FieldInfos - see {@link org.apache.lucene.codecs.FieldInfosFormat}</li>
+ *   <li>SegmentInfo - see {@link org.apache.lucene.codecs.SegmentInfoFormat}</li>
+ *   <li>Norms - see {@link org.apache.lucene.codecs.NormsFormat}</li>
+ *   <li>Live documents - see {@link org.apache.lucene.codecs.LiveDocsFormat}</li>
+ * </ul>
+ * </p>
+ *  
+ *   For some concrete implementations beyond Lucene's official index format, see
+ *   the <a href="{@docRoot}/../codecs/overview-summary.html">Codecs module</a>.
+ * 
+ * <p>
+ *   Codecs are identified by name through the Java Service Provider Interface.  To create your own codec, extend
+ *   {@link org.apache.lucene.codecs.Codec} and pass the new codec's name to the super() constructor:
+ * <pre class="prettyprint">
+ * public class MyCodec extends Codec {
+ * 
+ *     public MyCodec() {
+ *         super("MyCodecName");
+ *     }
+ * 
+ *     ...
+ * }
+ * </pre>
+ * You will need to register the Codec class so that the {@link java.util.ServiceLoader ServiceLoader} can find it, by including a
+ * META-INF/services/org.apache.lucene.codecs.Codec file on your classpath that contains the package-qualified
+ * name of your codec.
+ * </p>
+ * 
+ * <p>
+ *   If you just want to customise the {@link org.apache.lucene.codecs.PostingsFormat}, or use different postings
+ *   formats for different fields, then you can register your custom postings format in the same way (in
+ *   META-INF/services/org.apache.lucene.codecs.PostingsFormat), and then extend the default
+ *   codec and override
+ *   {@code org.apache.lucene.codecs.luceneMN.LuceneMNCodec#getPostingsFormatForField(String)} to return your custom
+ *   postings format.
+ * </p>
+ * <p>
+ *   Similarly, if you just want to customise the {@link org.apache.lucene.codecs.DocValuesFormat} per-field, have 
+ *   a look at {@code LuceneMNCodec.getDocValuesFormatForField(String)}.
+ * </p>
+ */
+package org.apache.lucene.codecs;
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/package.html b/lucene/core/src/java/org/apache/lucene/codecs/package.html
deleted file mode 100644
index 1ca05b2..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/package.html
+++ /dev/null
@@ -1,73 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Codecs API: API for customization of the encoding and structure of the index.
-
-<p>
-  The Codec API allows you to customise the way the following pieces of index information are stored:
-<ul>
-  <li>Postings lists - see {@link org.apache.lucene.codecs.PostingsFormat}</li>
-  <li>DocValues - see {@link org.apache.lucene.codecs.DocValuesFormat}</li>
-  <li>Stored fields - see {@link org.apache.lucene.codecs.StoredFieldsFormat}</li>
-  <li>Term vectors - see {@link org.apache.lucene.codecs.TermVectorsFormat}</li>
-  <li>FieldInfos - see {@link org.apache.lucene.codecs.FieldInfosFormat}</li>
-  <li>SegmentInfo - see {@link org.apache.lucene.codecs.SegmentInfoFormat}</li>
-  <li>Norms - see {@link org.apache.lucene.codecs.NormsFormat}</li>
-  <li>Live documents - see {@link org.apache.lucene.codecs.LiveDocsFormat}</li>
-</ul>
-</p>
- 
-  For some concrete implementations beyond Lucene's official index format, see
-  the <a href="{@docRoot}/../codecs/overview-summary.html">Codecs module</a>.
-
-<p>
-  Codecs are identified by name through the Java Service Provider Interface.  To create your own codec, extend
-  {@link org.apache.lucene.codecs.Codec} and pass the new codec's name to the super() constructor:
-<pre class="prettyprint">
-public class MyCodec extends Codec {
-
-    public MyCodec() {
-        super("MyCodecName");
-    }
-
-    ...
-}
-</pre>
-You will need to register the Codec class so that the {@link java.util.ServiceLoader ServiceLoader} can find it, by including a
-META-INF/services/org.apache.lucene.codecs.Codec file on your classpath that contains the package-qualified
-name of your codec.
-</p>
-
-<p>
-  If you just want to customise the {@link org.apache.lucene.codecs.PostingsFormat}, or use different postings
-  formats for different fields, then you can register your custom postings format in the same way (in
-  META-INF/services/org.apache.lucene.codecs.PostingsFormat), and then extend the default
-  codec and override
-  {@code org.apache.lucene.codecs.luceneMN.LuceneMNCodec#getPostingsFormatForField(String)} to return your custom
-  postings format.
-</p>
-<p>
-  Similarly, if you just want to customise the {@link org.apache.lucene.codecs.DocValuesFormat} per-field, have 
-  a look at {@code LuceneMNCodec.getDocValuesFormatForField(String)}.
-</p>
-</body>
-</html>
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/perfield/package-info.java b/lucene/core/src/java/org/apache/lucene/codecs/perfield/package-info.java
new file mode 100644
index 0000000..4c0faa0
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/perfield/package-info.java
@@ -0,0 +1,21 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Postings format that can delegate to different formats per-field.
+ */
+package org.apache.lucene.codecs.perfield;
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/perfield/package.html b/lucene/core/src/java/org/apache/lucene/codecs/perfield/package.html
deleted file mode 100644
index c0770ad..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/perfield/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Postings format that can delegate to different formats per-field.
-</body>
-</html>
\ No newline at end of file
diff --git a/lucene/core/src/java/org/apache/lucene/document/package-info.java b/lucene/core/src/java/org/apache/lucene/document/package-info.java
new file mode 100644
index 0000000..41a530a
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/document/package-info.java
@@ -0,0 +1,43 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** 
+ * The logical representation of a {@link org.apache.lucene.document.Document} for indexing and searching.
+ * <p>The document package provides the user level logical representation of content to be indexed and searched.  The
+ * package also provides utilities for working with {@link org.apache.lucene.document.Document}s and {@link org.apache.lucene.index.IndexableField}s.</p>
+ * <h2>Document and IndexableField</h2>
+ * <p>A {@link org.apache.lucene.document.Document} is a collection of {@link org.apache.lucene.index.IndexableField}s.  A
+ *   {@link org.apache.lucene.index.IndexableField} is a logical representation of a user's content that needs to be indexed or stored.
+ *   {@link org.apache.lucene.index.IndexableField}s have a number of properties that tell Lucene how to treat the content (like indexed, tokenized,
+ *   stored, etc.)  See the {@link org.apache.lucene.document.Field} implementation of {@link org.apache.lucene.index.IndexableField}
+ *   for specifics on these properties.
+ * </p>
+ * <p>Note: it is common to refer to {@link org.apache.lucene.document.Document}s having {@link org.apache.lucene.document.Field}s, even though technically they have
+ * {@link org.apache.lucene.index.IndexableField}s.</p>
+ * <h2>Working with Documents</h2>
+ * <p>First and foremost, a {@link org.apache.lucene.document.Document} is something created by the user application.  It is your job
+ *   to create Documents based on the content of the files you are working with in your application (Word, txt, PDF, Excel or any other format.)
+ *   How this is done is completely up to you.  That being said, there are many tools available in other projects that can make
+ *   the process of taking a file and converting it into a Lucene {@link org.apache.lucene.document.Document}.
+ * </p>
+ * <p>The {@link org.apache.lucene.document.DateTools} is a utility class to make dates and times searchable
+ * (remember, Lucene only searches text). {@link org.apache.lucene.document.IntField}, {@link org.apache.lucene.document.LongField},
+ * {@link org.apache.lucene.document.FloatField} and {@link org.apache.lucene.document.DoubleField} are a special helper class
+ * to simplify indexing of numeric values (and also dates) for fast range range queries with {@link org.apache.lucene.search.NumericRangeQuery}
+ * (using a special sortable string representation of numeric values).</p>
+ */
+package org.apache.lucene.document;
diff --git a/lucene/core/src/java/org/apache/lucene/document/package.html b/lucene/core/src/java/org/apache/lucene/document/package.html
deleted file mode 100644
index 71508a4..0000000
--- a/lucene/core/src/java/org/apache/lucene/document/package.html
+++ /dev/null
@@ -1,47 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-<p>The logical representation of a {@link org.apache.lucene.document.Document} for indexing and searching.</p>
-<p>The document package provides the user level logical representation of content to be indexed and searched.  The
-package also provides utilities for working with {@link org.apache.lucene.document.Document}s and {@link org.apache.lucene.index.IndexableField}s.</p>
-<h2>Document and IndexableField</h2>
-<p>A {@link org.apache.lucene.document.Document} is a collection of {@link org.apache.lucene.index.IndexableField}s.  A
-  {@link org.apache.lucene.index.IndexableField} is a logical representation of a user's content that needs to be indexed or stored.
-  {@link org.apache.lucene.index.IndexableField}s have a number of properties that tell Lucene how to treat the content (like indexed, tokenized,
-  stored, etc.)  See the {@link org.apache.lucene.document.Field} implementation of {@link org.apache.lucene.index.IndexableField}
-  for specifics on these properties.
-</p>
-<p>Note: it is common to refer to {@link org.apache.lucene.document.Document}s having {@link org.apache.lucene.document.Field}s, even though technically they have
-{@link org.apache.lucene.index.IndexableField}s.</p>
-<h2>Working with Documents</h2>
-<p>First and foremost, a {@link org.apache.lucene.document.Document} is something created by the user application.  It is your job
-  to create Documents based on the content of the files you are working with in your application (Word, txt, PDF, Excel or any other format.)
-  How this is done is completely up to you.  That being said, there are many tools available in other projects that can make
-  the process of taking a file and converting it into a Lucene {@link org.apache.lucene.document.Document}.
-</p>
-<p>The {@link org.apache.lucene.document.DateTools} is a utility class to make dates and times searchable
-(remember, Lucene only searches text). {@link org.apache.lucene.document.IntField}, {@link org.apache.lucene.document.LongField},
-{@link org.apache.lucene.document.FloatField} and {@link org.apache.lucene.document.DoubleField} are a special helper class
-to simplify indexing of numeric values (and also dates) for fast range range queries with {@link org.apache.lucene.search.NumericRangeQuery}
-(using a special sortable string representation of numeric values).</p>
-</body>
-</html>
diff --git a/lucene/core/src/java/org/apache/lucene/index/package-info.java b/lucene/core/src/java/org/apache/lucene/index/package-info.java
new file mode 100644
index 0000000..e81409a
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/index/package-info.java
@@ -0,0 +1,258 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Code to maintain and access indices.
+ * <!-- TODO: add IndexWriter, IndexWriterConfig, DocValues, etc etc -->
+ * <h2>Table Of Contents</h2>
+ * <p>
+ *     <ol>
+ *         <li><a href="#postings">Postings APIs</a>
+ *             <ul>
+ *                 <li><a href="#fields">Fields</a></li>
+ *                 <li><a href="#terms">Terms</a></li>
+ *                 <li><a href="#documents">Documents</a></li>
+ *                 <li><a href="#positions">Positions</a></li>
+ *             </ul>
+ *         </li>
+ *         <li><a href="#stats">Index Statistics</a>
+ *             <ul>
+ *                 <li><a href="#termstats">Term-level</a></li>
+ *                 <li><a href="#fieldstats">Field-level</a></li>
+ *                 <li><a href="#segmentstats">Segment-level</a></li>
+ *                 <li><a href="#documentstats">Document-level</a></li>
+ *             </ul>
+ *         </li>
+ *     </ol>
+ * </p>
+ * <a name="postings"></a>
+ * <h2>Postings APIs</h2>
+ * <a name="fields"></a>
+ * <h3>
+ *     Fields
+ * </h3>
+ * <p>
+ * {@link org.apache.lucene.index.Fields} is the initial entry point into the 
+ * postings APIs, this can be obtained in several ways:
+ * <pre class="prettyprint">
+ * // access indexed fields for an index segment
+ * Fields fields = reader.fields();
+ * // access term vector fields for a specified document
+ * Fields fields = reader.getTermVectors(docid);
+ * </pre>
+ * Fields implements Java's Iterable interface, so it's easy to enumerate the
+ * list of fields:
+ * <pre class="prettyprint">
+ * // enumerate list of fields
+ * for (String field : fields) {
+ *   // access the terms for this field
+ *   Terms terms = fields.terms(field);
+ * }
+ * </pre>
+ * </p>
+ * <a name="terms"></a>
+ * <h3>
+ *     Terms
+ * </h3>
+ * <p>
+ * {@link org.apache.lucene.index.Terms} represents the collection of terms
+ * within a field, exposes some metadata and <a href="#fieldstats">statistics</a>,
+ * and an API for enumeration.
+ * <pre class="prettyprint">
+ * // metadata about the field
+ * System.out.println("positions? " + terms.hasPositions());
+ * System.out.println("offsets? " + terms.hasOffsets());
+ * System.out.println("payloads? " + terms.hasPayloads());
+ * // iterate through terms
+ * TermsEnum termsEnum = terms.iterator(null);
+ * BytesRef term = null;
+ * while ((term = termsEnum.next()) != null) {
+ *   doSomethingWith(termsEnum.term());
+ * }
+ * </pre>
+ * {@link org.apache.lucene.index.TermsEnum} provides an iterator over the list
+ * of terms within a field, some <a href="#termstats">statistics</a> about the term,
+ * and methods to access the term's <a href="#documents">documents</a> and
+ * <a href="#positions">positions</a>.
+ * <pre class="prettyprint">
+ * // seek to a specific term
+ * boolean found = termsEnum.seekExact(new BytesRef("foobar"));
+ * if (found) {
+ *   // get the document frequency
+ *   System.out.println(termsEnum.docFreq());
+ *   // enumerate through documents
+ *   DocsEnum docs = termsEnum.docs(null, null);
+ *   // enumerate through documents and positions
+ *   DocsAndPositionsEnum docsAndPositions = termsEnum.docsAndPositions(null, null);
+ * }
+ * </pre>
+ * </p>
+ * <a name="documents"></a>
+ * <h3>
+ *   Documents
+ * </h3>
+ * <p>
+ *   {@link org.apache.lucene.index.PostingsEnum} is an extension of
+ *   {@link org.apache.lucene.search.DocIdSetIterator}that iterates over the list of
+ *   documents for a term, along with the term frequency within that document.
+ * <pre class="prettyprint">
+ * int docid;
+ * while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
+ *   System.out.println(docid);
+ *   System.out.println(docsEnum.freq());
+ *  }
+ * </pre>
+ * </p>
+ * <a name="positions"></a>
+ * <h3>
+ *   Positions
+ * </h3>
+ * <p>
+ *   PostingsEnum also allows iteration
+ *   of the positions a term occurred within the document, and any additional
+ *   per-position information (offsets and payload).  The information available
+ *   is controlled by flags passed to TermsEnum#postings
+ * <pre class="prettyprint">
+ * int docid;
+ * PostingsEnum postings = termsEnum.postings(null, null, PostingsEnum.FLAG_PAYLOADS | PostingsEnum.FLAG_OFFSETS);
+ * while ((docid = postings.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
+ *   System.out.println(docid);
+ *   int freq = postings.freq();
+ *   for (int i = 0; i &lt; freq; i++) {
+ *      System.out.println(postings.nextPosition());
+ *      System.out.println(postings.startOffset());
+ *      System.out.println(postings.endOffset());
+ *      System.out.println(postings.getPayload());
+ *   }
+ * }
+ * </pre>
+ * </p>
+ * <a name="stats"></a>
+ * <h2>Index Statistics</h2>
+ * <a name="termstats"></a>
+ * <h3>
+ *     Term statistics
+ * </h3>
+ * <p>
+ *     <ul>
+ *        <li>{@link org.apache.lucene.index.TermsEnum#docFreq}: Returns the number of 
+ *            documents that contain at least one occurrence of the term. This statistic 
+ *            is always available for an indexed term. Note that it will also count 
+ *            deleted documents, when segments are merged the statistic is updated as 
+ *            those deleted documents are merged away.
+ *        <li>{@link org.apache.lucene.index.TermsEnum#totalTermFreq}: Returns the number 
+ *            of occurrences of this term across all documents. Note that this statistic 
+ *            is unavailable (returns <code>-1</code>) if term frequencies were omitted 
+ *            from the index 
+ *            ({@link org.apache.lucene.index.IndexOptions#DOCS DOCS}) 
+ *            for the field. Like docFreq(), it will also count occurrences that appear in 
+ *            deleted documents.
+ *     </ul>
+ * </p>
+ * <a name="fieldstats"></a>
+ * <h3>
+ *     Field statistics
+ * </h3>
+ * <p>
+ *     <ul>
+ *        <li>{@link org.apache.lucene.index.Terms#size}: Returns the number of 
+ *            unique terms in the field. This statistic may be unavailable 
+ *            (returns <code>-1</code>) for some Terms implementations such as
+ *            {@link org.apache.lucene.index.MultiTerms}, where it cannot be efficiently
+ *            computed.  Note that this count also includes terms that appear only
+ *            in deleted documents: when segments are merged such terms are also merged
+ *            away and the statistic is then updated.
+ *        <li>{@link org.apache.lucene.index.Terms#getDocCount}: Returns the number of
+ *            documents that contain at least one occurrence of any term for this field.
+ *            This can be thought of as a Field-level docFreq(). Like docFreq() it will
+ *            also count deleted documents.
+ *        <li>{@link org.apache.lucene.index.Terms#getSumDocFreq}: Returns the number of
+ *            postings (term-document mappings in the inverted index) for the field. This
+ *            can be thought of as the sum of {@link org.apache.lucene.index.TermsEnum#docFreq}
+ *            across all terms in the field, and like docFreq() it will also count postings
+ *            that appear in deleted documents.
+ *        <li>{@link org.apache.lucene.index.Terms#getSumTotalTermFreq}: Returns the number
+ *            of tokens for the field. This can be thought of as the sum of 
+ *            {@link org.apache.lucene.index.TermsEnum#totalTermFreq} across all terms in the
+ *            field, and like totalTermFreq() it will also count occurrences that appear in
+ *            deleted documents, and will be unavailable (returns <code>-1</code>) if term 
+ *            frequencies were omitted from the index 
+ *            ({@link org.apache.lucene.index.IndexOptions#DOCS DOCS}) 
+ *            for the field.
+ *     </ul>
+ * </p>
+ * <a name="segmentstats"></a>
+ * <h3>
+ *     Segment statistics
+ * </h3>
+ * <p>
+ *     <ul>
+ *        <li>{@link org.apache.lucene.index.IndexReader#maxDoc}: Returns the number of 
+ *            documents (including deleted documents) in the index. 
+ *        <li>{@link org.apache.lucene.index.IndexReader#numDocs}: Returns the number 
+ *            of live documents (excluding deleted documents) in the index.
+ *        <li>{@link org.apache.lucene.index.IndexReader#numDeletedDocs}: Returns the
+ *            number of deleted documents in the index.
+ *        <li>{@link org.apache.lucene.index.Fields#size}: Returns the number of indexed
+ *            fields.
+ *     </ul>
+ * </p>
+ * <a name="documentstats"></a>
+ * <h3>
+ *     Document statistics
+ * </h3>
+ * <p>
+ * Document statistics are available during the indexing process for an indexed field: typically
+ * a {@link org.apache.lucene.search.similarities.Similarity} implementation will store some
+ * of these values (possibly in a lossy way), into the normalization value for the document in
+ * its {@link org.apache.lucene.search.similarities.Similarity#computeNorm} method.
+ * </p>
+ * <p>
+ *     <ul>
+ *        <li>{@link org.apache.lucene.index.FieldInvertState#getLength}: Returns the number of 
+ *            tokens for this field in the document. Note that this is just the number
+ *            of times that {@link org.apache.lucene.analysis.TokenStream#incrementToken} returned
+ *            true, and is unrelated to the values in 
+ *            {@link org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute}.
+ *        <li>{@link org.apache.lucene.index.FieldInvertState#getNumOverlap}: Returns the number
+ *            of tokens for this field in the document that had a position increment of zero. This
+ *            can be used to compute a document length that discounts artificial tokens
+ *            such as synonyms.
+ *        <li>{@link org.apache.lucene.index.FieldInvertState#getPosition}: Returns the accumulated
+ *            position value for this field in the document: computed from the values of
+ *            {@link org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute} and including
+ *            {@link org.apache.lucene.analysis.Analyzer#getPositionIncrementGap}s across multivalued
+ *            fields.
+ *        <li>{@link org.apache.lucene.index.FieldInvertState#getOffset}: Returns the total
+ *            character offset value for this field in the document: computed from the values of
+ *            {@link org.apache.lucene.analysis.tokenattributes.OffsetAttribute} returned by 
+ *            {@link org.apache.lucene.analysis.TokenStream#end}, and including
+ *            {@link org.apache.lucene.analysis.Analyzer#getOffsetGap}s across multivalued
+ *            fields.
+ *        <li>{@link org.apache.lucene.index.FieldInvertState#getUniqueTermCount}: Returns the number
+ *            of unique terms encountered for this field in the document.
+ *        <li>{@link org.apache.lucene.index.FieldInvertState#getMaxTermFrequency}: Returns the maximum
+ *            frequency across all unique terms encountered for this field in the document. 
+ *     </ul>
+ * </p>
+ * <p>
+ * Additional user-supplied statistics can be added to the document as DocValues fields and
+ * accessed via {@link org.apache.lucene.index.LeafReader#getNumericDocValues}.
+ * </p>
+ * <p>
+ */
+package org.apache.lucene.index;
diff --git a/lucene/core/src/java/org/apache/lucene/index/package.html b/lucene/core/src/java/org/apache/lucene/index/package.html
deleted file mode 100644
index e2ce310..0000000
--- a/lucene/core/src/java/org/apache/lucene/index/package.html
+++ /dev/null
@@ -1,262 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Code to maintain and access indices.
-<!-- TODO: add IndexWriter, IndexWriterConfig, DocValues, etc etc -->
-<h2>Table Of Contents</h2>
-<p>
-    <ol>
-        <li><a href="#postings">Postings APIs</a>
-            <ul>
-                <li><a href="#fields">Fields</a></li>
-                <li><a href="#terms">Terms</a></li>
-                <li><a href="#documents">Documents</a></li>
-                <li><a href="#positions">Positions</a></li>
-            </ul>
-        </li>
-        <li><a href="#stats">Index Statistics</a>
-            <ul>
-                <li><a href="#termstats">Term-level</a></li>
-                <li><a href="#fieldstats">Field-level</a></li>
-                <li><a href="#segmentstats">Segment-level</a></li>
-                <li><a href="#documentstats">Document-level</a></li>
-            </ul>
-        </li>
-    </ol>
-</p>
-<a name="postings"></a>
-<h2>Postings APIs</h2>
-<a name="fields"></a>
-<h4>
-    Fields
-</h4>
-<p>
-{@link org.apache.lucene.index.Fields} is the initial entry point into the 
-postings APIs, this can be obtained in several ways:
-<pre class="prettyprint">
-// access indexed fields for an index segment
-Fields fields = reader.fields();
-// access term vector fields for a specified document
-Fields fields = reader.getTermVectors(docid);
-</pre>
-Fields implements Java's Iterable interface, so it's easy to enumerate the
-list of fields:
-<pre class="prettyprint">
-// enumerate list of fields
-for (String field : fields) {
-  // access the terms for this field
-  Terms terms = fields.terms(field);
-}
-</pre>
-</p>
-<a name="terms"></a>
-<h4>
-    Terms
-</h4>
-<p>
-{@link org.apache.lucene.index.Terms} represents the collection of terms
-within a field, exposes some metadata and <a href="#fieldstats">statistics</a>,
-and an API for enumeration.
-<pre class="prettyprint">
-// metadata about the field
-System.out.println("positions? " + terms.hasPositions());
-System.out.println("offsets? " + terms.hasOffsets());
-System.out.println("payloads? " + terms.hasPayloads());
-// iterate through terms
-TermsEnum termsEnum = terms.iterator(null);
-BytesRef term = null;
-while ((term = termsEnum.next()) != null) {
-  doSomethingWith(termsEnum.term());
-}
-</pre>
-{@link org.apache.lucene.index.TermsEnum} provides an iterator over the list
-of terms within a field, some <a href="#termstats">statistics</a> about the term,
-and methods to access the term's <a href="#documents">documents</a> and
-<a href="#positions">positions</a>.
-<pre class="prettyprint">
-// seek to a specific term
-boolean found = termsEnum.seekExact(new BytesRef("foobar"));
-if (found) {
-  // get the document frequency
-  System.out.println(termsEnum.docFreq());
-  // enumerate through documents
-  DocsEnum docs = termsEnum.docs(null, null);
-  // enumerate through documents and positions
-  DocsAndPositionsEnum docsAndPositions = termsEnum.docsAndPositions(null, null);
-}
-</pre>
-</p>
-<a name="documents"></a>
-<h4>
-  Documents
-</h4>
-<p>
-  {@link org.apache.lucene.index.PostingsEnum} is an extension of
-  {@link org.apache.lucene.search.DocIdSetIterator}that iterates over the list of
-  documents for a term, along with the term frequency within that document.
-<pre class="prettyprint">
-int docid;
-while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
-  System.out.println(docid);
-  System.out.println(docsEnum.freq());
-}
-</pre>
-</p>
-<a name="positions"></a>
-<h4>
-  Positions
-</h4>
-<p>
-  PostingsEnum also allows iteration
-  of the positions a term occurred within the document, and any additional
-  per-position information (offsets and payload).  The information available
-  is controlled by flags passed to TermsEnum#postings
-<pre class="prettyprint">
-int docid;
-PostingsEnum postings = termsEnum.postings(null, null, PostingsEnum.FLAG_PAYLOADS | PostingsEnum.FLAG_OFFSETS);
-while ((docid = postings.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
-  System.out.println(docid);
-  int freq = postings.freq();
-  for (int i = 0; i < freq; i++) {
-     System.out.println(postings.nextPosition());
-     System.out.println(postings.startOffset());
-     System.out.println(postings.endOffset());
-     System.out.println(postings.getPayload());
-  }
-}
-</pre>
-</p>
-<a name="stats"></a>
-<h2>Index Statistics</h2>
-<a name="termstats"></a>
-<h4>
-    Term statistics
-</h4>
-<p>
-    <ul>
-       <li>{@link org.apache.lucene.index.TermsEnum#docFreq}: Returns the number of 
-           documents that contain at least one occurrence of the term. This statistic 
-           is always available for an indexed term. Note that it will also count 
-           deleted documents, when segments are merged the statistic is updated as 
-           those deleted documents are merged away.
-       <li>{@link org.apache.lucene.index.TermsEnum#totalTermFreq}: Returns the number 
-           of occurrences of this term across all documents. Note that this statistic 
-           is unavailable (returns <code>-1</code>) if term frequencies were omitted 
-           from the index 
-           ({@link org.apache.lucene.index.IndexOptions#DOCS DOCS}) 
-           for the field. Like docFreq(), it will also count occurrences that appear in 
-           deleted documents.
-    </ul>
-</p>
-<a name="fieldstats"></a>
-<h4>
-    Field statistics
-</h4>
-<p>
-    <ul>
-       <li>{@link org.apache.lucene.index.Terms#size}: Returns the number of 
-           unique terms in the field. This statistic may be unavailable 
-           (returns <code>-1</code>) for some Terms implementations such as
-           {@link org.apache.lucene.index.MultiTerms}, where it cannot be efficiently
-           computed.  Note that this count also includes terms that appear only
-           in deleted documents: when segments are merged such terms are also merged
-           away and the statistic is then updated.
-       <li>{@link org.apache.lucene.index.Terms#getDocCount}: Returns the number of
-           documents that contain at least one occurrence of any term for this field.
-           This can be thought of as a Field-level docFreq(). Like docFreq() it will
-           also count deleted documents.
-       <li>{@link org.apache.lucene.index.Terms#getSumDocFreq}: Returns the number of
-           postings (term-document mappings in the inverted index) for the field. This
-           can be thought of as the sum of {@link org.apache.lucene.index.TermsEnum#docFreq}
-           across all terms in the field, and like docFreq() it will also count postings
-           that appear in deleted documents.
-       <li>{@link org.apache.lucene.index.Terms#getSumTotalTermFreq}: Returns the number
-           of tokens for the field. This can be thought of as the sum of 
-           {@link org.apache.lucene.index.TermsEnum#totalTermFreq} across all terms in the
-           field, and like totalTermFreq() it will also count occurrences that appear in
-           deleted documents, and will be unavailable (returns <code>-1</code>) if term 
-           frequencies were omitted from the index 
-           ({@link org.apache.lucene.index.IndexOptions#DOCS DOCS}) 
-           for the field.
-    </ul>
-</p>
-<a name="segmentstats"></a>
-<h4>
-    Segment statistics
-</h4>
-<p>
-    <ul>
-       <li>{@link org.apache.lucene.index.IndexReader#maxDoc}: Returns the number of 
-           documents (including deleted documents) in the index. 
-       <li>{@link org.apache.lucene.index.IndexReader#numDocs}: Returns the number 
-           of live documents (excluding deleted documents) in the index.
-       <li>{@link org.apache.lucene.index.IndexReader#numDeletedDocs}: Returns the
-           number of deleted documents in the index.
-       <li>{@link org.apache.lucene.index.Fields#size}: Returns the number of indexed
-           fields.
-    </ul>
-</p>
-<a name="documentstats"></a>
-<h4>
-    Document statistics
-</h4>
-<p>
-Document statistics are available during the indexing process for an indexed field: typically
-a {@link org.apache.lucene.search.similarities.Similarity} implementation will store some
-of these values (possibly in a lossy way), into the normalization value for the document in
-its {@link org.apache.lucene.search.similarities.Similarity#computeNorm} method.
-</p>
-<p>
-    <ul>
-       <li>{@link org.apache.lucene.index.FieldInvertState#getLength}: Returns the number of 
-           tokens for this field in the document. Note that this is just the number
-           of times that {@link org.apache.lucene.analysis.TokenStream#incrementToken} returned
-           true, and is unrelated to the values in 
-           {@link org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute}.
-       <li>{@link org.apache.lucene.index.FieldInvertState#getNumOverlap}: Returns the number
-           of tokens for this field in the document that had a position increment of zero. This
-           can be used to compute a document length that discounts artificial tokens
-           such as synonyms.
-       <li>{@link org.apache.lucene.index.FieldInvertState#getPosition}: Returns the accumulated
-           position value for this field in the document: computed from the values of
-           {@link org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute} and including
-           {@link org.apache.lucene.analysis.Analyzer#getPositionIncrementGap}s across multivalued
-           fields.
-       <li>{@link org.apache.lucene.index.FieldInvertState#getOffset}: Returns the total
-           character offset value for this field in the document: computed from the values of
-           {@link org.apache.lucene.analysis.tokenattributes.OffsetAttribute} returned by 
-           {@link org.apache.lucene.analysis.TokenStream#end}, and including
-           {@link org.apache.lucene.analysis.Analyzer#getOffsetGap}s across multivalued
-           fields.
-       <li>{@link org.apache.lucene.index.FieldInvertState#getUniqueTermCount}: Returns the number
-           of unique terms encountered for this field in the document.
-       <li>{@link org.apache.lucene.index.FieldInvertState#getMaxTermFrequency}: Returns the maximum
-           frequency across all unique terms encountered for this field in the document. 
-    </ul>
-</p>
-<p>
-Additional user-supplied statistics can be added to the document as DocValues fields and
-accessed via {@link org.apache.lucene.index.LeafReader#getNumericDocValues}.
-</p>
-<p>
-</body>
-</html>
diff --git a/lucene/core/src/java/org/apache/lucene/package-info.java b/lucene/core/src/java/org/apache/lucene/package-info.java
new file mode 100644
index 0000000..0e6eedd
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/package-info.java
@@ -0,0 +1,21 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+ 
+/** 
+ * Top-level package.
+ */
+package org.apache.lucene;
diff --git a/lucene/core/src/java/org/apache/lucene/package.html b/lucene/core/src/java/org/apache/lucene/package.html
deleted file mode 100644
index 4fc5809..0000000
--- a/lucene/core/src/java/org/apache/lucene/package.html
+++ /dev/null
@@ -1,17 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html><body>Top-level package.</body></html>
diff --git a/lucene/core/src/java/org/apache/lucene/search/payloads/package-info.java b/lucene/core/src/java/org/apache/lucene/search/payloads/package-info.java
new file mode 100644
index 0000000..da40d88
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/search/payloads/package-info.java
@@ -0,0 +1,29 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** 
+ * The payloads package provides Query mechanisms for finding and using payloads.
+ * <p>
+ *   The following Query implementations are provided:
+ *   <ol>
+ *    <li>{@link org.apache.lucene.search.payloads.PayloadTermQuery PayloadTermQuery} -- Boost a term's score based on the value of the payload located at that term.</li>
+ *    <li>{@link org.apache.lucene.search.payloads.PayloadNearQuery PayloadNearQuery} -- A {@link org.apache.lucene.search.spans.SpanNearQuery SpanNearQuery} that factors in the value of the payloads located 
+ *        at each of the positions where the spans occur.</li>
+ *   </ol>
+ * </p>
+ */
+package org.apache.lucene.search.payloads;
diff --git a/lucene/core/src/java/org/apache/lucene/search/payloads/package.html b/lucene/core/src/java/org/apache/lucene/search/payloads/package.html
deleted file mode 100644
index a193f89..0000000
--- a/lucene/core/src/java/org/apache/lucene/search/payloads/package.html
+++ /dev/null
@@ -1,32 +0,0 @@
-<HTML>
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<HEAD>
-    <TITLE>org.apache.lucene.search.payloads</TITLE>
-</HEAD>
-<BODY>
-The payloads package provides Query mechanisms for finding and using payloads.
-<p>
-  The following Query implementations are provided:
-  <ol>
-    <li>{@link org.apache.lucene.search.payloads.PayloadTermQuery PayloadTermQuery} -- Boost a term's score based on the value of the payload located at that term.</li>
-  	<li>{@link org.apache.lucene.search.payloads.PayloadNearQuery PayloadNearQuery} -- A {@link org.apache.lucene.search.spans.SpanNearQuery SpanNearQuery} that factors in the value of the payloads located 
-  	at each of the positions where the spans occur.</li>
-  </ol>
-</p>
-</BODY>
-</HTML>
diff --git a/lucene/core/src/java/org/apache/lucene/search/similarities/package-info.java b/lucene/core/src/java/org/apache/lucene/search/similarities/package-info.java
new file mode 100644
index 0000000..6e7d99e
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/search/similarities/package-info.java
@@ -0,0 +1,146 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * This package contains the various ranking models that can be used in Lucene. The
+ * abstract class {@link org.apache.lucene.search.similarities.Similarity} serves
+ * as the base for ranking functions. For searching, users can employ the models
+ * already implemented or create their own by extending one of the classes in this
+ * package.
+ * 
+ * <h2>Table Of Contents</h2>
+ * <p>
+ *     <ol>
+ *         <li><a href="#sims">Summary of the Ranking Methods</a></li>
+ *         <li><a href="#changingSimilarity">Changing the Similarity</a></li>
+ *     </ol>
+ * </p>
+ * 
+ * 
+ * <a name="sims"></a>
+ * <h2>Summary of the Ranking Methods</h2>
+ * 
+ * <p>{@link org.apache.lucene.search.similarities.DefaultSimilarity} is the original Lucene
+ * scoring function. It is based on a highly optimized 
+ * <a href="http://en.wikipedia.org/wiki/Vector_Space_Model">Vector Space Model</a>. For more
+ * information, see {@link org.apache.lucene.search.similarities.TFIDFSimilarity}.</p>
+ * 
+ * <p>{@link org.apache.lucene.search.similarities.BM25Similarity} is an optimized
+ * implementation of the successful Okapi BM25 model.</p>
+ * 
+ * <p>{@link org.apache.lucene.search.similarities.SimilarityBase} provides a basic
+ * implementation of the Similarity contract and exposes a highly simplified
+ * interface, which makes it an ideal starting point for new ranking functions.
+ * Lucene ships the following methods built on
+ * {@link org.apache.lucene.search.similarities.SimilarityBase}:
+ * 
+ * <a name="framework"></a>
+ * <ul>
+ *   <li>Amati and Rijsbergen's {@linkplain org.apache.lucene.search.similarities.DFRSimilarity DFR} framework;</li>
+ *   <li>Clinchant and Gaussier's {@linkplain org.apache.lucene.search.similarities.IBSimilarity Information-based models}
+ *     for IR;</li>
+ *   <li>The implementation of two {@linkplain org.apache.lucene.search.similarities.LMSimilarity language models} from
+ *   Zhai and Lafferty's paper.</li>
+ * </ul>
+ * 
+ * Since {@link org.apache.lucene.search.similarities.SimilarityBase} is not
+ * optimized to the same extent as
+ * {@link org.apache.lucene.search.similarities.DefaultSimilarity} and
+ * {@link org.apache.lucene.search.similarities.BM25Similarity}, a difference in
+ * performance is to be expected when using the methods listed above. However,
+ * optimizations can always be implemented in subclasses; see
+ * <a href="#changingSimilarity">below</a>.</p>
+ * 
+ * <a name="changingSimilarity"></a>
+ * <h2>Changing Similarity</h2>
+ * 
+ * <p>Chances are the available Similarities are sufficient for all
+ *     your searching needs.
+ *     However, in some applications it may be necessary to customize your <a
+ *         href="Similarity.html">Similarity</a> implementation. For instance, some
+ *     applications do not need to
+ *     distinguish between shorter and longer documents (see <a
+ *         href="http://www.gossamer-threads.com/lists/lucene/java-user/38967#38967">a "fair" similarity</a>).</p>
+ * 
+ * <p>To change {@link org.apache.lucene.search.similarities.Similarity}, one must do so for both indexing and
+ *     searching, and the changes must happen before
+ *     either of these actions take place. Although in theory there is nothing stopping you from changing mid-stream, it
+ *     just isn't well-defined what is going to happen.
+ * </p>
+ * 
+ * <p>To make this change, implement your own {@link org.apache.lucene.search.similarities.Similarity} (likely
+ *     you'll want to simply subclass an existing method, be it
+ *     {@link org.apache.lucene.search.similarities.DefaultSimilarity} or a descendant of
+ *     {@link org.apache.lucene.search.similarities.SimilarityBase}), and
+ *     then register the new class by calling
+ *     {@link org.apache.lucene.index.IndexWriterConfig#setSimilarity(Similarity)}
+ *     before indexing and
+ *     {@link org.apache.lucene.search.IndexSearcher#setSimilarity(Similarity)}
+ *     before searching.
+ * </p>
+ * 
+ * <h3>Extending {@linkplain org.apache.lucene.search.similarities.SimilarityBase}</h3>
+ * <p>
+ * The easiest way to quickly implement a new ranking method is to extend
+ * {@link org.apache.lucene.search.similarities.SimilarityBase}, which provides
+ * basic implementations for the low level . Subclasses are only required to
+ * implement the {@link org.apache.lucene.search.similarities.SimilarityBase#score(BasicStats, float, float)}
+ * and {@link org.apache.lucene.search.similarities.SimilarityBase#toString()}
+ * methods.</p>
+ * 
+ * <p>Another option is to extend one of the <a href="#framework">frameworks</a>
+ * based on {@link org.apache.lucene.search.similarities.SimilarityBase}. These
+ * Similarities are implemented modularly, e.g.
+ * {@link org.apache.lucene.search.similarities.DFRSimilarity} delegates
+ * computation of the three parts of its formula to the classes
+ * {@link org.apache.lucene.search.similarities.BasicModel},
+ * {@link org.apache.lucene.search.similarities.AfterEffect} and
+ * {@link org.apache.lucene.search.similarities.Normalization}. Instead of
+ * subclassing the Similarity, one can simply introduce a new basic model and tell
+ * {@link org.apache.lucene.search.similarities.DFRSimilarity} to use it.</p>
+ * 
+ * <h3>Changing {@linkplain org.apache.lucene.search.similarities.DefaultSimilarity}</h3>
+ * <p>
+ *     If you are interested in use cases for changing your similarity, see the Lucene users's mailing list at <a
+ *         href="http://www.gossamer-threads.com/lists/lucene/java-user/39125">Overriding Similarity</a>.
+ *     In summary, here are a few use cases:
+ *     <ol>
+ *         <li><p>The <code>SweetSpotSimilarity</code> in
+ *             <code>org.apache.lucene.misc</code> gives small
+ *             increases as the frequency increases a small amount
+ *             and then greater increases when you hit the "sweet spot", i.e. where
+ *             you think the frequency of terms is more significant.</p></li>
+ *         <li><p>Overriding tf &mdash; In some applications, it doesn't matter what the score of a document is as long as a
+ *             matching term occurs. In these
+ *             cases people have overridden Similarity to return 1 from the tf() method.</p></li>
+ *         <li><p>Changing Length Normalization &mdash; By overriding
+ *             {@link org.apache.lucene.search.similarities.Similarity#computeNorm(org.apache.lucene.index.FieldInvertState state)},
+ *             it is possible to discount how the length of a field contributes
+ *             to a score. In {@link org.apache.lucene.search.similarities.DefaultSimilarity},
+ *             lengthNorm = 1 / (numTerms in field)^0.5, but if one changes this to be
+ *             1 / (numTerms in field), all fields will be treated
+ *             <a href="http://www.gossamer-threads.com/lists/lucene/java-user/38967#38967">"fairly"</a>.</p></li>
+ *     </ol>
+ *     In general, Chris Hostetter sums it up best in saying (from <a
+ *         href="http://www.gossamer-threads.com/lists/lucene/java-user/39125#39125">the Lucene users's mailing list</a>):
+ *     <blockquote>[One would override the Similarity in] ... any situation where you know more about your data then just
+ *         that
+ *         it's "text" is a situation where it *might* make sense to to override your
+ *         Similarity method.</blockquote>
+ * </p>
+ */
+package org.apache.lucene.search.similarities;
diff --git a/lucene/core/src/java/org/apache/lucene/search/similarities/package.html b/lucene/core/src/java/org/apache/lucene/search/similarities/package.html
deleted file mode 100644
index 4ea2b31..0000000
--- a/lucene/core/src/java/org/apache/lucene/search/similarities/package.html
+++ /dev/null
@@ -1,151 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-This package contains the various ranking models that can be used in Lucene. The
-abstract class {@link org.apache.lucene.search.similarities.Similarity} serves
-as the base for ranking functions. For searching, users can employ the models
-already implemented or create their own by extending one of the classes in this
-package.
-
-<h2>Table Of Contents</h2>
-<p>
-    <ol>
-        <li><a href="#sims">Summary of the Ranking Methods</a></li>
-        <li><a href="#changingSimilarity">Changing the Similarity</a></li>
-    </ol>
-</p>
-
-
-<a name="sims"></a>
-<h2>Summary of the Ranking Methods</h2>
-
-<p>{@link org.apache.lucene.search.similarities.DefaultSimilarity} is the original Lucene
-scoring function. It is based on a highly optimized 
-<a href="http://en.wikipedia.org/wiki/Vector_Space_Model">Vector Space Model</a>. For more
-information, see {@link org.apache.lucene.search.similarities.TFIDFSimilarity}.</p>
-
-<p>{@link org.apache.lucene.search.similarities.BM25Similarity} is an optimized
-implementation of the successful Okapi BM25 model.</p>
-
-<p>{@link org.apache.lucene.search.similarities.SimilarityBase} provides a basic
-implementation of the Similarity contract and exposes a highly simplified
-interface, which makes it an ideal starting point for new ranking functions.
-Lucene ships the following methods built on
-{@link org.apache.lucene.search.similarities.SimilarityBase}:
-
-<a name="framework"></a>
-<ul>
-  <li>Amati and Rijsbergen's {@linkplain org.apache.lucene.search.similarities.DFRSimilarity DFR} framework;</li>
-  <li>Clinchant and Gaussier's {@linkplain org.apache.lucene.search.similarities.IBSimilarity Information-based models}
-    for IR;</li>
-  <li>The implementation of two {@linkplain org.apache.lucene.search.similarities.LMSimilarity language models} from
-  Zhai and Lafferty's paper.</li>
-</ul>
-
-Since {@link org.apache.lucene.search.similarities.SimilarityBase} is not
-optimized to the same extent as
-{@link org.apache.lucene.search.similarities.DefaultSimilarity} and
-{@link org.apache.lucene.search.similarities.BM25Similarity}, a difference in
-performance is to be expected when using the methods listed above. However,
-optimizations can always be implemented in subclasses; see
-<a href="#changingSimilarity">below</a>.</p>
-
-<a name="changingSimilarity"></a>
-<h2>Changing Similarity</h2>
-
-<p>Chances are the available Similarities are sufficient for all
-    your searching needs.
-    However, in some applications it may be necessary to customize your <a
-        href="Similarity.html">Similarity</a> implementation. For instance, some
-    applications do not need to
-    distinguish between shorter and longer documents (see <a
-        href="http://www.gossamer-threads.com/lists/lucene/java-user/38967#38967">a "fair" similarity</a>).</p>
-
-<p>To change {@link org.apache.lucene.search.similarities.Similarity}, one must do so for both indexing and
-    searching, and the changes must happen before
-    either of these actions take place. Although in theory there is nothing stopping you from changing mid-stream, it
-    just isn't well-defined what is going to happen.
-</p>
-
-<p>To make this change, implement your own {@link org.apache.lucene.search.similarities.Similarity} (likely
-    you'll want to simply subclass an existing method, be it
-    {@link org.apache.lucene.search.similarities.DefaultSimilarity} or a descendant of
-    {@link org.apache.lucene.search.similarities.SimilarityBase}), and
-    then register the new class by calling
-    {@link org.apache.lucene.index.IndexWriterConfig#setSimilarity(Similarity)}
-    before indexing and
-    {@link org.apache.lucene.search.IndexSearcher#setSimilarity(Similarity)}
-    before searching.
-</p>
-
-<h3>Extending {@linkplain org.apache.lucene.search.similarities.SimilarityBase}</h3>
-<p>
-The easiest way to quickly implement a new ranking method is to extend
-{@link org.apache.lucene.search.similarities.SimilarityBase}, which provides
-basic implementations for the low level . Subclasses are only required to
-implement the {@link org.apache.lucene.search.similarities.SimilarityBase#score(BasicStats, float, float)}
-and {@link org.apache.lucene.search.similarities.SimilarityBase#toString()}
-methods.</p>
-
-<p>Another option is to extend one of the <a href="#framework">frameworks</a>
-based on {@link org.apache.lucene.search.similarities.SimilarityBase}. These
-Similarities are implemented modularly, e.g.
-{@link org.apache.lucene.search.similarities.DFRSimilarity} delegates
-computation of the three parts of its formula to the classes
-{@link org.apache.lucene.search.similarities.BasicModel},
-{@link org.apache.lucene.search.similarities.AfterEffect} and
-{@link org.apache.lucene.search.similarities.Normalization}. Instead of
-subclassing the Similarity, one can simply introduce a new basic model and tell
-{@link org.apache.lucene.search.similarities.DFRSimilarity} to use it.</p>
-
-<h3>Changing {@linkplain org.apache.lucene.search.similarities.DefaultSimilarity}</h3>
-<p>
-    If you are interested in use cases for changing your similarity, see the Lucene users's mailing list at <a
-        href="http://www.gossamer-threads.com/lists/lucene/java-user/39125">Overriding Similarity</a>.
-    In summary, here are a few use cases:
-    <ol>
-        <li><p>The <code>SweetSpotSimilarity</code> in
-            <code>org.apache.lucene.misc</code> gives small
-            increases as the frequency increases a small amount
-            and then greater increases when you hit the "sweet spot", i.e. where
-            you think the frequency of terms is more significant.</p></li>
-        <li><p>Overriding tf &mdash; In some applications, it doesn't matter what the score of a document is as long as a
-            matching term occurs. In these
-            cases people have overridden Similarity to return 1 from the tf() method.</p></li>
-        <li><p>Changing Length Normalization &mdash; By overriding
-            {@link org.apache.lucene.search.similarities.Similarity#computeNorm(FieldInvertState state)},
-            it is possible to discount how the length of a field contributes
-            to a score. In {@link org.apache.lucene.search.similarities.DefaultSimilarity},
-            lengthNorm = 1 / (numTerms in field)^0.5, but if one changes this to be
-            1 / (numTerms in field), all fields will be treated
-            <a href="http://www.gossamer-threads.com/lists/lucene/java-user/38967#38967">"fairly"</a>.</p></li>
-    </ol>
-    In general, Chris Hostetter sums it up best in saying (from <a
-        href="http://www.gossamer-threads.com/lists/lucene/java-user/39125#39125">the Lucene users's mailing list</a>):
-    <blockquote>[One would override the Similarity in] ... any situation where you know more about your data then just
-        that
-        it's "text" is a situation where it *might* make sense to to override your
-        Similarity method.</blockquote>
-</p>
-
-</body>
-</html>
diff --git a/lucene/core/src/java/org/apache/lucene/search/spans/package-info.java b/lucene/core/src/java/org/apache/lucene/search/spans/package-info.java
new file mode 100644
index 0000000..58b3265
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/search/spans/package-info.java
@@ -0,0 +1,90 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * The calculus of spans.
+ * 
+ * <p>A span is a <code>&lt;doc,startPosition,endPosition&gt;</code> tuple.</p>
+ * 
+ * <p>The following span query operators are implemented:
+ * 
+ * <ul>
+ * 
+ * <li>A {@link org.apache.lucene.search.spans.SpanTermQuery SpanTermQuery} matches all spans
+ * containing a particular {@link org.apache.lucene.index.Term Term}.</li>
+ * 
+ * <li> A {@link org.apache.lucene.search.spans.SpanNearQuery SpanNearQuery} matches spans
+ * which occur near one another, and can be used to implement things like
+ * phrase search (when constructed from {@link org.apache.lucene.search.spans.SpanTermQuery}s)
+ * and inter-phrase proximity (when constructed from other {@link org.apache.lucene.search.spans.SpanNearQuery}s).</li>
+ * 
+ * <li>A {@link org.apache.lucene.search.spans.SpanOrQuery SpanOrQuery} merges spans from a
+ * number of other {@link org.apache.lucene.search.spans.SpanQuery}s.</li>
+ * 
+ * <li>A {@link org.apache.lucene.search.spans.SpanNotQuery SpanNotQuery} removes spans
+ * matching one {@link org.apache.lucene.search.spans.SpanQuery SpanQuery} which overlap (or comes
+ * near) another.  This can be used, e.g., to implement within-paragraph
+ * search.</li>
+ * 
+ * <li>A {@link org.apache.lucene.search.spans.SpanFirstQuery SpanFirstQuery} matches spans
+ * matching <code>q</code> whose end position is less than
+ * <code>n</code>.  This can be used to constrain matches to the first
+ * part of the document.</li>
+ * 
+ * <li>A {@link org.apache.lucene.search.spans.SpanPositionRangeQuery SpanPositionRangeQuery} is
+ * a more general form of SpanFirstQuery that can constrain matches to arbitrary portions of the document.</li>
+ * 
+ * </ul>
+ * 
+ * In all cases, output spans are minimally inclusive.  In other words, a
+ * span formed by matching a span in x and y starts at the lesser of the
+ * two starts and ends at the greater of the two ends.
+ * </p>
+ * 
+ * <p>For example, a span query which matches "John Kerry" within ten
+ * words of "George Bush" within the first 100 words of the document
+ * could be constructed with:
+ * <pre class="prettyprint">
+ * SpanQuery john   = new SpanTermQuery(new Term("content", "john"));
+ * SpanQuery kerry  = new SpanTermQuery(new Term("content", "kerry"));
+ * SpanQuery george = new SpanTermQuery(new Term("content", "george"));
+ * SpanQuery bush   = new SpanTermQuery(new Term("content", "bush"));
+ * 
+ * SpanQuery johnKerry =
+ *    new SpanNearQuery(new SpanQuery[] {john, kerry}, 0, true);
+ * 
+ * SpanQuery georgeBush =
+ *    new SpanNearQuery(new SpanQuery[] {george, bush}, 0, true);
+ * 
+ * SpanQuery johnKerryNearGeorgeBush =
+ *    new SpanNearQuery(new SpanQuery[] {johnKerry, georgeBush}, 10, false);
+ * 
+ * SpanQuery johnKerryNearGeorgeBushAtStart =
+ *    new SpanFirstQuery(johnKerryNearGeorgeBush, 100);
+ * </pre>
+ * 
+ * <p>Span queries may be freely intermixed with other Lucene queries.
+ * So, for example, the above query can be restricted to documents which
+ * also use the word "iraq" with:
+ * 
+ * <pre class="prettyprint">
+ * Query query = new BooleanQuery();
+ * query.add(johnKerryNearGeorgeBushAtStart, true, false);
+ * query.add(new TermQuery("content", "iraq"), true, false);
+ * </pre>
+ */
+package org.apache.lucene.search.spans;
diff --git a/lucene/core/src/java/org/apache/lucene/search/spans/package.html b/lucene/core/src/java/org/apache/lucene/search/spans/package.html
deleted file mode 100644
index 054336e..0000000
--- a/lucene/core/src/java/org/apache/lucene/search/spans/package.html
+++ /dev/null
@@ -1,93 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head></head>
-<body>
-The calculus of spans.
-
-<p>A span is a <code>&lt;doc,startPosition,endPosition&gt;</code> tuple.</p>
-
-<p>The following span query operators are implemented:
-
-<ul>
-
-<li>A {@link org.apache.lucene.search.spans.SpanTermQuery SpanTermQuery} matches all spans
-containing a particular {@link org.apache.lucene.index.Term Term}.</li>
-
-<li> A {@link org.apache.lucene.search.spans.SpanNearQuery SpanNearQuery} matches spans
-which occur near one another, and can be used to implement things like
-phrase search (when constructed from {@link org.apache.lucene.search.spans.SpanTermQuery}s)
-and inter-phrase proximity (when constructed from other {@link org.apache.lucene.search.spans.SpanNearQuery}s).</li>
-
-<li>A {@link org.apache.lucene.search.spans.SpanOrQuery SpanOrQuery} merges spans from a
-number of other {@link org.apache.lucene.search.spans.SpanQuery}s.</li>
-
-<li>A {@link org.apache.lucene.search.spans.SpanNotQuery SpanNotQuery} removes spans
-matching one {@link org.apache.lucene.search.spans.SpanQuery SpanQuery} which overlap (or comes
-near) another.  This can be used, e.g., to implement within-paragraph
-search.</li>
-
-<li>A {@link org.apache.lucene.search.spans.SpanFirstQuery SpanFirstQuery} matches spans
-matching <code>q</code> whose end position is less than
-<code>n</code>.  This can be used to constrain matches to the first
-part of the document.</li>
-
-<li>A {@link org.apache.lucene.search.spans.SpanPositionRangeQuery SpanPositionRangeQuery} is
-a more general form of SpanFirstQuery that can constrain matches to arbitrary portions of the document.</li>
-
-</ul>
-
-In all cases, output spans are minimally inclusive.  In other words, a
-span formed by matching a span in x and y starts at the lesser of the
-two starts and ends at the greater of the two ends.
-</p>
-
-<p>For example, a span query which matches "John Kerry" within ten
-words of "George Bush" within the first 100 words of the document
-could be constructed with:
-<pre class="prettyprint">
-SpanQuery john   = new SpanTermQuery(new Term("content", "john"));
-SpanQuery kerry  = new SpanTermQuery(new Term("content", "kerry"));
-SpanQuery george = new SpanTermQuery(new Term("content", "george"));
-SpanQuery bush   = new SpanTermQuery(new Term("content", "bush"));
-
-SpanQuery johnKerry =
-   new SpanNearQuery(new SpanQuery[] {john, kerry}, 0, true);
-
-SpanQuery georgeBush =
-   new SpanNearQuery(new SpanQuery[] {george, bush}, 0, true);
-
-SpanQuery johnKerryNearGeorgeBush =
-   new SpanNearQuery(new SpanQuery[] {johnKerry, georgeBush}, 10, false);
-
-SpanQuery johnKerryNearGeorgeBushAtStart =
-   new SpanFirstQuery(johnKerryNearGeorgeBush, 100);
-</pre>
-
-<p>Span queries may be freely intermixed with other Lucene queries.
-So, for example, the above query can be restricted to documents which
-also use the word "iraq" with:
-
-<pre class="prettyprint">
-Query query = new BooleanQuery();
-query.add(johnKerryNearGeorgeBushAtStart, true, false);
-query.add(new TermQuery("content", "iraq"), true, false);
-</pre>
-
-</body>
-</html>
diff --git a/lucene/core/src/java/org/apache/lucene/store/package-info.java b/lucene/core/src/java/org/apache/lucene/store/package-info.java
new file mode 100644
index 0000000..048a0a8
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/store/package-info.java
@@ -0,0 +1,21 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Binary i/o API, used for all index data.
+ */
+package org.apache.lucene.store;
diff --git a/lucene/core/src/java/org/apache/lucene/store/package.html b/lucene/core/src/java/org/apache/lucene/store/package.html
deleted file mode 100644
index f1755e1..0000000
--- a/lucene/core/src/java/org/apache/lucene/store/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Binary i/o API, used for all index data.
-</body>
-</html>
diff --git a/lucene/core/src/java/org/apache/lucene/util/automaton/package-info.java b/lucene/core/src/java/org/apache/lucene/util/automaton/package-info.java
new file mode 100644
index 0000000..7baea5c
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/util/automaton/package-info.java
@@ -0,0 +1,46 @@
+/*
+ * dk.brics.automaton
+ * 
+ * Copyright (c) 2001-2009 Anders Moeller
+ * All rights reserved.
+ * 
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ * 
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/**
+ * Finite-state automaton for regular expressions.
+ * <p>
+ * This package contains a full DFA/NFA implementation with Unicode
+ * alphabet and support for all standard (and a number of non-standard)
+ * regular expression operations.
+ * <p>
+ * The most commonly used functionality is located in the classes
+ * <tt>{@link org.apache.lucene.util.automaton.Automaton}</tt> and
+ * <tt>{@link org.apache.lucene.util.automaton.RegExp}</tt>.
+ * <p>
+ * For more information, go to the package home page at 
+ * <tt><a href="http://www.brics.dk/automaton/" 
+ * target="_top">http://www.brics.dk/automaton/</a></tt>.
+ * @lucene.experimental
+ */
+package org.apache.lucene.util.automaton;
diff --git a/lucene/core/src/java/org/apache/lucene/util/automaton/package.html b/lucene/core/src/java/org/apache/lucene/util/automaton/package.html
deleted file mode 100644
index 0ac5d80..0000000
--- a/lucene/core/src/java/org/apache/lucene/util/automaton/package.html
+++ /dev/null
@@ -1,47 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- dk.brics.automaton
- 
- Copyright (c) 2001-2009 Anders Moeller
- All rights reserved.
- 
- Redistribution and use in source and binary forms, with or without
- modification, are permitted provided that the following conditions
- are met:
- 1. Redistributions of source code must retain the above copyright
-    notice, this list of conditions and the following disclaimer.
- 2. Redistributions in binary form must reproduce the above copyright
-    notice, this list of conditions and the following disclaimer in the
-    documentation and/or other materials provided with the distribution.
- 3. The name of the author may not be used to endorse or promote products
-    derived from this software without specific prior written permission.
- 
- THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
- IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
- OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
- IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
- INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
- NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
- THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
--->
-<html>
-<body>
-Finite-state automaton for regular expressions.
-<p>
-This package contains a full DFA/NFA implementation with Unicode
-alphabet and support for all standard (and a number of non-standard)
-regular expression operations.
-<p>
-The most commonly used functionality is located in the classes
-<tt>{@link org.apache.lucene.util.automaton.Automaton}</tt> and
-<tt>{@link org.apache.lucene.util.automaton.RegExp}</tt>.
-<p>
-For more information, go to the package home page at 
-<tt><a href="http://www.brics.dk/automaton/" 
-target="_top">http://www.brics.dk/automaton/</a></tt>.
-@lucene.experimental
-</body>
-</html>
diff --git a/lucene/core/src/java/org/apache/lucene/util/fst/package-info.java b/lucene/core/src/java/org/apache/lucene/util/fst/package-info.java
new file mode 100644
index 0000000..41426f9
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/util/fst/package-info.java
@@ -0,0 +1,88 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Finite state transducers
+ * <p>
+ * This package implements <a href="http://en.wikipedia.org/wiki/Finite_state_transducer">
+ * Finite State Transducers</a> with the following characteristics:
+ * <ul>
+ *    <li>Fast and low memory overhead construction of the minimal FST 
+ *        (but inputs must be provided in sorted order)</li>
+ *    <li>Low object overhead and quick deserialization (byte[] representation)</li>
+ *    <li>Optional two-pass compression: {@link org.apache.lucene.util.fst.FST#pack FST.pack()}</li>
+ *    <li>{@link org.apache.lucene.util.fst.Util#getByOutput Lookup-by-output} when the 
+ *        outputs are in sorted order (e.g., ordinals or file pointers)</li>
+ *    <li>Pluggable {@link org.apache.lucene.util.fst.Outputs Outputs} representation</li>
+ *    <li>{@link org.apache.lucene.util.fst.Util#shortestPaths N-shortest-paths} search by
+ *        weight</li>
+ *    <li>Enumerators ({@link org.apache.lucene.util.fst.IntsRefFSTEnum IntsRef} and {@link org.apache.lucene.util.fst.BytesRefFSTEnum BytesRef}) that behave like {@link java.util.SortedMap SortedMap} iterators
+ * </ul>
+ * <p>
+ * FST Construction example:
+ * <pre class="prettyprint">
+ *     // Input values (keys). These must be provided to Builder in Unicode sorted order!
+ *     String inputValues[] = {"cat", "dog", "dogs"};
+ *     long outputValues[] = {5, 7, 12};
+ *     
+ *     PositiveIntOutputs outputs = PositiveIntOutputs.getSingleton();
+ *     Builder&lt;Long&gt; builder = new Builder&lt;Long&gt;(INPUT_TYPE.BYTE1, outputs);
+ *     BytesRef scratchBytes = new BytesRef();
+ *     IntsRefBuilder scratchInts = new IntsRefBuilder();
+ *     for (int i = 0; i &lt; inputValues.length; i++) {
+ *       scratchBytes.copyChars(inputValues[i]);
+ *       builder.add(Util.toIntsRef(scratchBytes, scratchInts), outputValues[i]);
+ *     }
+ *     FST&lt;Long&gt; fst = builder.finish();
+ * </pre>
+ * Retrieval by key:
+ * <pre class="prettyprint">
+ *     Long value = Util.get(fst, new BytesRef("dog"));
+ *     System.out.println(value); // 7
+ * </pre>
+ * Retrieval by value:
+ * <pre class="prettyprint">
+ *     // Only works because outputs are also in sorted order
+ *     IntsRef key = Util.getByOutput(fst, 12);
+ *     System.out.println(Util.toBytesRef(key, scratchBytes).utf8ToString()); // dogs
+ * </pre>
+ * Iterate over key-value pairs in sorted order:
+ * <pre class="prettyprint">
+ *     // Like TermsEnum, this also supports seeking (advance)
+ *     BytesRefFSTEnum&lt;Long&gt; iterator = new BytesRefFSTEnum&lt;Long&gt;(fst);
+ *     while (iterator.next() != null) {
+ *       InputOutput&lt;Long&gt; mapEntry = iterator.current();
+ *       System.out.println(mapEntry.input.utf8ToString());
+ *       System.out.println(mapEntry.output);
+ *     }
+ * </pre>
+ * N-shortest paths by weight:
+ * <pre class="prettyprint">
+ *     Comparator&lt;Long&gt; comparator = new Comparator&lt;Long&gt;() {
+ *       public int compare(Long left, Long right) {
+ *         return left.compareTo(right);
+ *       }
+ *     };
+ *     Arc&lt;Long&gt; firstArc = fst.getFirstArc(new Arc&lt;Long&gt;());
+ *     MinResult&lt;Long&gt; paths[] = Util.shortestPaths(fst, firstArc, comparator, 2);
+ *     System.out.println(Util.toBytesRef(paths[0].input, scratchBytes).utf8ToString()); // cat
+ *     System.out.println(paths[0].output); // 5
+ *     System.out.println(Util.toBytesRef(paths[1].input, scratchBytes).utf8ToString()); // dog
+ *     System.out.println(paths[1].output); // 7
+ * </pre>
+ */
+package org.apache.lucene.util.fst;
diff --git a/lucene/core/src/java/org/apache/lucene/util/fst/package.html b/lucene/core/src/java/org/apache/lucene/util/fst/package.html
deleted file mode 100644
index 10cb686..0000000
--- a/lucene/core/src/java/org/apache/lucene/util/fst/package.html
+++ /dev/null
@@ -1,92 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Finite state transducers
-<p>
-This package implements <a href="http://en.wikipedia.org/wiki/Finite_state_transducer">
-Finite State Transducers</a> with the following characteristics:
-<ul>
-   <li>Fast and low memory overhead construction of the minimal FST 
-       (but inputs must be provided in sorted order)</li>
-   <li>Low object overhead and quick deserialization (byte[] representation)</li>
-   <li>Optional two-pass compression: {@link org.apache.lucene.util.fst.FST#pack FST.pack()}</li>
-   <li>{@link org.apache.lucene.util.fst.Util#getByOutput Lookup-by-output} when the 
-       outputs are in sorted order (e.g., ordinals or file pointers)</li>
-   <li>Pluggable {@link org.apache.lucene.util.fst.Outputs Outputs} representation</li>
-   <li>{@link org.apache.lucene.util.fst.Util#shortestPaths N-shortest-paths} search by
-       weight</li>
-   <li>Enumerators ({@link org.apache.lucene.util.fst.IntsRefFSTEnum IntsRef} and {@link org.apache.lucene.util.fst.BytesRefFSTEnum BytesRef}) that behave like {@link java.util.SortedMap SortedMap} iterators
-</ul>
-<p>
-FST Construction example:
-<pre class="prettyprint">
-    // Input values (keys). These must be provided to Builder in Unicode sorted order!
-    String inputValues[] = {"cat", "dog", "dogs"};
-    long outputValues[] = {5, 7, 12};
-    
-    PositiveIntOutputs outputs = PositiveIntOutputs.getSingleton();
-    Builder&lt;Long&gt; builder = new Builder&lt;Long&gt;(INPUT_TYPE.BYTE1, outputs);
-    BytesRef scratchBytes = new BytesRef();
-    IntsRefBuilder scratchInts = new IntsRefBuilder();
-    for (int i = 0; i &lt; inputValues.length; i++) {
-      scratchBytes.copyChars(inputValues[i]);
-      builder.add(Util.toIntsRef(scratchBytes, scratchInts), outputValues[i]);
-    }
-    FST&lt;Long&gt; fst = builder.finish();
-</pre>
-Retrieval by key:
-<pre class="prettyprint">
-    Long value = Util.get(fst, new BytesRef("dog"));
-    System.out.println(value); // 7
-</pre>
-Retrieval by value:
-<pre class="prettyprint">
-    // Only works because outputs are also in sorted order
-    IntsRef key = Util.getByOutput(fst, 12);
-    System.out.println(Util.toBytesRef(key, scratchBytes).utf8ToString()); // dogs
-</pre>
-Iterate over key-value pairs in sorted order:
-<pre class="prettyprint">
-    // Like TermsEnum, this also supports seeking (advance)
-    BytesRefFSTEnum&lt;Long&gt; iterator = new BytesRefFSTEnum&lt;Long&gt;(fst);
-    while (iterator.next() != null) {
-      InputOutput&lt;Long&gt; mapEntry = iterator.current();
-      System.out.println(mapEntry.input.utf8ToString());
-      System.out.println(mapEntry.output);
-    }
-</pre>
-N-shortest paths by weight:
-<pre class="prettyprint">
-    Comparator&lt;Long&gt; comparator = new Comparator&lt;Long&gt;() {
-      public int compare(Long left, Long right) {
-        return left.compareTo(right);
-      }
-    };
-    Arc&lt;Long&gt; firstArc = fst.getFirstArc(new Arc&lt;Long&gt;());
-    MinResult&lt;Long&gt; paths[] = Util.shortestPaths(fst, firstArc, comparator, 2);
-    System.out.println(Util.toBytesRef(paths[0].input, scratchBytes).utf8ToString()); // cat
-    System.out.println(paths[0].output); // 5
-    System.out.println(Util.toBytesRef(paths[1].input, scratchBytes).utf8ToString()); // dog
-    System.out.println(paths[1].output); // 7
-</pre>
-</body>
-</html>
diff --git a/lucene/core/src/java/org/apache/lucene/util/mutable/package-info.java b/lucene/core/src/java/org/apache/lucene/util/mutable/package-info.java
new file mode 100644
index 0000000..02ae5fc
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/util/mutable/package-info.java
@@ -0,0 +1,21 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Comparable object wrappers 
+ */
+package org.apache.lucene.util.mutable;
\ No newline at end of file
diff --git a/lucene/core/src/java/org/apache/lucene/util/mutable/package.html b/lucene/core/src/java/org/apache/lucene/util/mutable/package.html
deleted file mode 100644
index 5d6252d..0000000
--- a/lucene/core/src/java/org/apache/lucene/util/mutable/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Comparable object wrappers 
-</body>
-</html>
\ No newline at end of file
diff --git a/lucene/core/src/java/org/apache/lucene/util/package-info.java b/lucene/core/src/java/org/apache/lucene/util/package-info.java
new file mode 100644
index 0000000..be74639
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/util/package-info.java
@@ -0,0 +1,21 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Some utility classes.
+ */
+package org.apache.lucene.util;
diff --git a/lucene/core/src/java/org/apache/lucene/util/package.html b/lucene/core/src/java/org/apache/lucene/util/package.html
deleted file mode 100644
index 3e7a90b..0000000
--- a/lucene/core/src/java/org/apache/lucene/util/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Some utility classes.
-</body>
-</html>
diff --git a/lucene/core/src/java/org/apache/lucene/util/packed/package-info.java b/lucene/core/src/java/org/apache/lucene/util/packed/package-info.java
new file mode 100644
index 0000000..f05a143
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/util/packed/package-info.java
@@ -0,0 +1,88 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** 
+ * Packed integer arrays and streams.
+ * 
+ * <p>
+ *     The packed package provides
+ *     <ul>
+ *      <li>sequential and random access capable arrays of positive longs,</li>
+ *      <li>routines for efficient serialization and deserialization of streams of packed integers.</li>
+ *     </ul>
+ * 
+ *     The implementations provide different trade-offs between memory usage and
+ *     access speed. The standard usage scenario is replacing large int or long
+ *     arrays in order to reduce the memory footprint.
+ * </p><p>
+ *     The main access point is the {@link org.apache.lucene.util.packed.PackedInts} factory.
+ * </p>
+ * 
+ * <h3>In-memory structures</h3>
+ * 
+ * <ul>
+ *     <li><b>{@link org.apache.lucene.util.packed.PackedInts.Mutable}</b><ul>
+ *         <li>Only supports positive longs.</li>
+ *         <li>Requires the number of bits per value to be known in advance.</li>
+ *         <li>Random-access for both writing and reading.</li>
+ *     </ul></li>
+ *     <li><b>{@link org.apache.lucene.util.packed.GrowableWriter}</b><ul>
+ *         <li>Same as PackedInts.Mutable but grows the number of bits per values when needed.</li>
+ *         <li>Useful to build a PackedInts.Mutable from a read-once stream of longs.</li>
+ *     </ul></li>
+ *     <li><b>{@link org.apache.lucene.util.packed.PagedGrowableWriter}</b><ul>
+ *         <li>Slices data into fixed-size blocks stored in GrowableWriters.</li>
+ *         <li>Supports more than 2B values.</li>
+ *         <li>You should use PackedLongValues instead if you don't need random write access.</li>
+ *     </ul></li>
+ *     <li><b>{@link org.apache.lucene.util.packed.PackedLongValues#deltaPackedBuilder}</b><ul>
+ *         <li>Can store any sequence of longs.</li>
+ *         <li>Compression is good when values are close to each other.</li>
+ *         <li>Supports random reads, but only sequential writes.</li>
+ *         <li>Can address up to 2^42 values.</li>
+ *     </ul></li>
+ *     <li><b>{@link org.apache.lucene.util.packed.PackedLongValues#packedBuilder}</b><ul>
+ *         <li>Same as deltaPackedBuilder but assumes values are 0-based.</li>
+ *     </ul></li>
+ *     <li><b>{@link org.apache.lucene.util.packed.PackedLongValues#monotonicBuilder}</b><ul>
+ *         <li>Same as deltaPackedBuilder except that compression is good when the stream is a succession of affine functions.</li>
+ *     </ul></li>
+ * </ul>
+ * 
+ * <h3>Disk-based structures</h3>
+ * 
+ * <ul>
+ *     <li><b>{@link org.apache.lucene.util.packed.PackedInts.Writer}, {@link org.apache.lucene.util.packed.PackedInts.Reader}, {@link org.apache.lucene.util.packed.PackedInts.ReaderIterator}</b><ul>
+ *         <li>Only supports positive longs.</li>
+ *         <li>Requires the number of bits per value to be known in advance.</li>
+ *         <li>Supports both fast sequential access with low memory footprint with ReaderIterator and random-access by either loading values in memory or leaving them on disk with Reader.</li>
+ *     </ul></li>
+ *     <li><b>{@link org.apache.lucene.util.packed.BlockPackedWriter}, {@link org.apache.lucene.util.packed.BlockPackedReader}, {@link org.apache.lucene.util.packed.BlockPackedReaderIterator}</b><ul>
+ *         <li>Splits the stream into fixed-size blocks.</li>
+ *         <li>Compression is good when values are close to each other.</li>
+ *         <li>Can address up to 2B * blockSize values.</li>
+ *     </ul></li>
+ *     <li><b>{@link org.apache.lucene.util.packed.MonotonicBlockPackedWriter}, {@link org.apache.lucene.util.packed.MonotonicBlockPackedReader}</b><ul>
+ *         <li>Same as the non-monotonic variants except that compression is good when the stream is a succession of affine functions.</li>
+ *         <li>The reason why there is no sequential access is that if you need sequential access, you should rather delta-encode and use BlockPackedWriter.</li>
+ *     </ul></li>
+ *     <li><b>{@link org.apache.lucene.util.packed.PackedDataOutput}, {@link org.apache.lucene.util.packed.PackedDataInput}</b><ul>
+ *         <li>Writes sequences of longs where each long can use any number of bits.</li>
+ *     </ul></li>
+ * </ul>
+ */
+package org.apache.lucene.util.packed;
diff --git a/lucene/core/src/java/org/apache/lucene/util/packed/package.html b/lucene/core/src/java/org/apache/lucene/util/packed/package.html
deleted file mode 100644
index d333619..0000000
--- a/lucene/core/src/java/org/apache/lucene/util/packed/package.html
+++ /dev/null
@@ -1,92 +0,0 @@
-<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head></head>
-<body bgcolor="white">
-
-<h2>Packed integer arrays and streams.</h2>
-
-<p>
-    The packed package provides
-    <ul>
-    	<li>sequential and random access capable arrays of positive longs,</li>
-    	<li>routines for efficient serialization and deserialization of streams of packed integers.</li>
-    </ul>
-
-    The implementations provide different trade-offs between memory usage and
-    access speed. The standard usage scenario is replacing large int or long
-    arrays in order to reduce the memory footprint.
-</p><p>
-    The main access point is the {@link org.apache.lucene.util.packed.PackedInts} factory.
-</p>
-
-<h3>In-memory structures</h3>
-
-<ul>
-    <li><b>{@link org.apache.lucene.util.packed.PackedInts.Mutable}</b><ul>
-        <li>Only supports positive longs.</li>
-        <li>Requires the number of bits per value to be known in advance.</li>
-        <li>Random-access for both writing and reading.</li>
-    </ul></li>
-    <li><b>{@link org.apache.lucene.util.packed.GrowableWriter}</b><ul>
-        <li>Same as PackedInts.Mutable but grows the number of bits per values when needed.</li>
-        <li>Useful to build a PackedInts.Mutable from a read-once stream of longs.</li>
-    </ul></li>
-    <li><b>{@link org.apache.lucene.util.packed.PagedGrowableWriter}</b><ul>
-        <li>Slices data into fixed-size blocks stored in GrowableWriters.</li>
-        <li>Supports more than 2B values.</li>
-        <li>You should use PackedLongValues instead if you don't need random write access.</li>
-    </ul></li>
-    <li><b>{@link org.apache.lucene.util.packed.PackedLongValues#deltaPackedBuilder}</b><ul>
-        <li>Can store any sequence of longs.</li>
-        <li>Compression is good when values are close to each other.</li>
-        <li>Supports random reads, but only sequential writes.</li>
-        <li>Can address up to 2^42 values.</li>
-    </ul></li>
-    <li><b>{@link org.apache.lucene.util.packed.PackedLongValues#packedBuilder}</b><ul>
-        <li>Same as deltaPackedBuilder but assumes values are 0-based.</li>
-    </ul></li>
-    <li><b>{@link org.apache.lucene.util.packed.PackedLongValues#monotonicBuilder}</b><ul>
-        <li>Same as deltaPackedBuilder except that compression is good when the stream is a succession of affine functions.</li>
-    </ul></li>
-</ul>
-
-<h3>Disk-based structures</h3>
-
-<ul>
-    <li><b>{@link org.apache.lucene.util.packed.PackedInts.Writer}, {@link org.apache.lucene.util.packed.PackedInts.Reader}, {@link org.apache.lucene.util.packed.PackedInts.ReaderIterator}</b><ul>
-        <li>Only supports positive longs.</li>
-        <li>Requires the number of bits per value to be known in advance.</li>
-        <li>Supports both fast sequential access with low memory footprint with ReaderIterator and random-access by either loading values in memory or leaving them on disk with Reader.</li>
-    </ul></li>
-    <li><b>{@link org.apache.lucene.util.packed.BlockPackedWriter}, {@link org.apache.lucene.util.packed.BlockPackedReader}, {@link org.apache.lucene.util.packed.BlockPackedReaderIterator}</b><ul>
-        <li>Splits the stream into fixed-size blocks.</li>
-        <li>Compression is good when values are close to each other.</li>
-        <li>Can address up to 2B * blockSize values.</li>
-    </ul></li>
-    <li><b>{@link org.apache.lucene.util.packed.MonotonicBlockPackedWriter}, {@link org.apache.lucene.util.packed.MonotonicBlockPackedReader}</b><ul>
-        <li>Same as the non-monotonic variants except that compression is good when the stream is a succession of affine functions.</li>
-        <li>The reason why there is no sequential access is that if you need sequential access, you should rather delta-encode and use BlockPackedWriter.</li>
-    </ul></li>
-    <li><b>{@link org.apache.lucene.util.packed.PackedDataOutput}, {@link org.apache.lucene.util.packed.PackedDataInput}</b><ul>
-        <li>Writes sequences of longs where each long can use any number of bits.</li>
-    </ul></li>
-</ul>
-
-</body>
-</html>
diff --git a/lucene/test-framework/src/java/org/apache/lucene/analysis/package.html b/lucene/test-framework/src/java/org/apache/lucene/analysis/package.html
index 5177298..be34d6c 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/analysis/package.html
+++ b/lucene/test-framework/src/java/org/apache/lucene/analysis/package.html
@@ -15,6 +15,7 @@
  See the License for the specific language governing permissions and
  limitations under the License.
 -->
+<!-- not a package-info.java, because we already defined this package in core/ -->
 <html>
 <head>
    <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/package-info.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/package-info.java
new file mode 100644
index 0000000..bccdd5d
--- /dev/null
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/package-info.java
@@ -0,0 +1,21 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Codec for testing that asserts various contracts of the codec apis.
+ */
+package org.apache.lucene.codecs.asserting;
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/package.html b/lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/package.html
deleted file mode 100644
index a4c8944..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Codec for testing that asserts various contracts of the codec apis.
-</body>
-</html>
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/blockterms/package.html b/lucene/test-framework/src/java/org/apache/lucene/codecs/blockterms/package.html
index f6c674a..5fc71d9 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/blockterms/package.html
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/blockterms/package.html
@@ -15,6 +15,7 @@
  See the License for the specific language governing permissions and
  limitations under the License.
 -->
+<!-- not a package-info.java, because we already defined this package in codecs/ -->
 <html>
 <head>
    <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/bloom/package.html b/lucene/test-framework/src/java/org/apache/lucene/codecs/bloom/package.html
index 60bbdcc..5142a33 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/bloom/package.html
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/bloom/package.html
@@ -15,6 +15,7 @@
  See the License for the specific language governing permissions and
  limitations under the License.
 -->
+<!-- not a package-info.java, because we already defined this package in codecs/ -->
 <html>
 <head>
    <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/cheapbastard/package-info.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/cheapbastard/package-info.java
new file mode 100644
index 0000000..f9d245e
--- /dev/null
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/cheapbastard/package-info.java
@@ -0,0 +1,22 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** 
+ * Codec that unreasonably tries to use as little RAM as possible.
+ * For testing, benchmarking, API purposes only!
+ */
+package org.apache.lucene.codecs.cheapbastard;
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/cheapbastard/package.html b/lucene/test-framework/src/java/org/apache/lucene/codecs/cheapbastard/package.html
deleted file mode 100644
index f5298b4..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/cheapbastard/package.html
+++ /dev/null
@@ -1,26 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Codec that unreasonably tries to use as little RAM as possible.
-For testing, benchmarking, API purposes only!
-</body>
-</html>
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/compressing/dummy/package-info.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/compressing/dummy/package-info.java
new file mode 100644
index 0000000..f8fcbdf
--- /dev/null
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/compressing/dummy/package-info.java
@@ -0,0 +1,21 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** 
+ * Dummy CompressingCodec implementation used for testing.
+ */
+package org.apache.lucene.codecs.compressing.dummy;
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/compressing/dummy/package.html b/lucene/test-framework/src/java/org/apache/lucene/codecs/compressing/dummy/package.html
deleted file mode 100644
index 4ab3a44..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/compressing/dummy/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Dummy CompressingCodec implementation used for testing.
-</body>
-</html>
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/compressing/package.html b/lucene/test-framework/src/java/org/apache/lucene/codecs/compressing/package.html
index eb1d676..dee9094 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/compressing/package.html
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/compressing/package.html
@@ -15,6 +15,7 @@
  See the License for the specific language governing permissions and
  limitations under the License.
 -->
+<!-- not a package-info.java, because we already defined this package in core/ -->
 <html>
 <head>
    <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/cranky/package-info.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/cranky/package-info.java
new file mode 100644
index 0000000..676beac
--- /dev/null
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/cranky/package-info.java
@@ -0,0 +1,21 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Codec for testing that throws random IOExceptions
+ */
+package org.apache.lucene.codecs.cranky;
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/cranky/package.html b/lucene/test-framework/src/java/org/apache/lucene/codecs/cranky/package.html
deleted file mode 100644
index 8266e07..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/cranky/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Codec for testing that throws random IOExceptions
-</body>
-</html>
\ No newline at end of file
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/package-info.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/package-info.java
new file mode 100644
index 0000000..8a755e4
--- /dev/null
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/package-info.java
@@ -0,0 +1,21 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** 
+ * Frankenstein codec for testing that pieces together random components.
+ */
+package org.apache.lucene.codecs.mockrandom;
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/package.html b/lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/package.html
deleted file mode 100644
index 44a742c..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Frankenstein codec for testing that pieces together random components.
-</body>
-</html>
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/package.html b/lucene/test-framework/src/java/org/apache/lucene/codecs/package.html
index ca70ffc..3185dec 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/package.html
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/package.html
@@ -15,6 +15,7 @@
  See the License for the specific language governing permissions and
  limitations under the License.
 -->
+<!-- not a package-info.java, because we already defined this package in core/ -->
 <html>
 <head>
    <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/ramonly/package-info.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/ramonly/package-info.java
new file mode 100644
index 0000000..8ea282f
--- /dev/null
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/ramonly/package-info.java
@@ -0,0 +1,21 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** 
+ * Codec for testing that never writes to disk.
+ */
+package org.apache.lucene.codecs.ramonly;
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/ramonly/package.html b/lucene/test-framework/src/java/org/apache/lucene/codecs/ramonly/package.html
deleted file mode 100644
index b777bd3..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/ramonly/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Codec for testing that never writes to disk.
-</body>
-</html>
diff --git a/lucene/test-framework/src/java/org/apache/lucene/index/package.html b/lucene/test-framework/src/java/org/apache/lucene/index/package.html
index 5c1835d..78568b4 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/index/package.html
+++ b/lucene/test-framework/src/java/org/apache/lucene/index/package.html
@@ -15,6 +15,7 @@
  See the License for the specific language governing permissions and
  limitations under the License.
 -->
+<!-- not a package-info.java, because we already defined this package in core/ -->
 <html>
 <head>
    <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
diff --git a/lucene/test-framework/src/java/org/apache/lucene/mockfile/package-info.java b/lucene/test-framework/src/java/org/apache/lucene/mockfile/package-info.java
new file mode 100644
index 0000000..a7b731c
--- /dev/null
+++ b/lucene/test-framework/src/java/org/apache/lucene/mockfile/package-info.java
@@ -0,0 +1,30 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Support for testing/debugging with virtual filesystems 
+ * <p>
+ * The primary classes are:
+ * <ul>
+ *   <li>{@link org.apache.lucene.mockfile.LeakFS}: Fails tests if they leave open filehandles.
+ *   <li>{@link org.apache.lucene.mockfile.VerboseFS}: Prints destructive filesystem operations to infostream.
+ *   <li>{@link org.apache.lucene.mockfile.WindowsFS}: Acts like windows.
+ *   <li>{@link org.apache.lucene.mockfile.DisableFsyncFS}: Makes actual fsync calls a no-op.
+ * </ul>
+ * </p>
+ */
+package org.apache.lucene.mockfile;
diff --git a/lucene/test-framework/src/java/org/apache/lucene/mockfile/package.html b/lucene/test-framework/src/java/org/apache/lucene/mockfile/package.html
deleted file mode 100644
index ef094f4..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/mockfile/package.html
+++ /dev/null
@@ -1,34 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Support for testing/debugging with virtual filesystems 
-<p>
-The primary classes are:
-<ul>
-  <li>{@link org.apache.lucene.mockfile.LeakFS}: Fails tests if they leave open filehandles.
-  <li>{@link org.apache.lucene.mockfile.VerboseFS}: Prints destructive filesystem operations to infostream.
-  <li>{@link org.apache.lucene.mockfile.WindowsFS}: Acts like windows.
-  <li>{@link org.apache.lucene.mockfile.DisableFsyncFS}: Makes actual fsync calls a no-op.
-</ul>
-</p>
-</body>
-</html>
\ No newline at end of file
diff --git a/lucene/test-framework/src/java/org/apache/lucene/search/package.html b/lucene/test-framework/src/java/org/apache/lucene/search/package.html
index cfb26da..1fe0c99 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/search/package.html
+++ b/lucene/test-framework/src/java/org/apache/lucene/search/package.html
@@ -15,6 +15,7 @@
  See the License for the specific language governing permissions and
  limitations under the License.
 -->
+<!-- not a package-info.java, because we already defined this package in core/ -->
 <html>
 <head>
    <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
diff --git a/lucene/test-framework/src/java/org/apache/lucene/store/package.html b/lucene/test-framework/src/java/org/apache/lucene/store/package.html
index fec5502..84c72a3 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/store/package.html
+++ b/lucene/test-framework/src/java/org/apache/lucene/store/package.html
@@ -15,6 +15,7 @@
  See the License for the specific language governing permissions and
  limitations under the License.
 -->
+<!-- not a package-info.java, because we already defined this package in core/ -->
 <html>
 <head>
    <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
diff --git a/lucene/test-framework/src/java/org/apache/lucene/util/automaton/package.html b/lucene/test-framework/src/java/org/apache/lucene/util/automaton/package.html
index 64a20ff..e6f6990 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/util/automaton/package.html
+++ b/lucene/test-framework/src/java/org/apache/lucene/util/automaton/package.html
@@ -15,6 +15,7 @@
  See the License for the specific language governing permissions and
  limitations under the License.
 -->
+<!-- not a package-info.java, because we already defined this package in core/ -->
 <html>
 <head>
    <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
diff --git a/lucene/test-framework/src/java/org/apache/lucene/util/fst/package.html b/lucene/test-framework/src/java/org/apache/lucene/util/fst/package.html
index 2628928..8e882fc 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/util/fst/package.html
+++ b/lucene/test-framework/src/java/org/apache/lucene/util/fst/package.html
@@ -15,6 +15,7 @@
  See the License for the specific language governing permissions and
  limitations under the License.
 -->
+<!-- not a package-info.java, because we already defined this package in core/ -->
 <html>
 <head>
    <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
diff --git a/lucene/test-framework/src/java/org/apache/lucene/util/package.html b/lucene/test-framework/src/java/org/apache/lucene/util/package.html
index 6f7d9ef..7abad7c 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/util/package.html
+++ b/lucene/test-framework/src/java/org/apache/lucene/util/package.html
@@ -15,6 +15,7 @@
  See the License for the specific language governing permissions and
  limitations under the License.
 -->
+<!-- not a package-info.java, because we already defined this package in core/ -->
 <html>
 <head>
    <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">

