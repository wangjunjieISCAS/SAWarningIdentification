GitDiffStart: bc3a3dc5d47af0c00748468b1ae14b4a18854366 | Thu May 31 02:07:11 2012 +0000
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ar/ArabicAnalyzer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ar/ArabicAnalyzer.java
index a96c1ef..b66388c 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ar/ArabicAnalyzer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ar/ArabicAnalyzer.java
@@ -137,8 +137,7 @@ public final class ArabicAnalyzer extends StopwordAnalyzerBase {
   @Override
   protected TokenStreamComponents createComponents(String fieldName,
       Reader reader) {
-    final Tokenizer source = matchVersion.onOrAfter(Version.LUCENE_31) ? 
-        new StandardTokenizer(matchVersion, reader) : new ArabicLetterTokenizer(matchVersion, reader);
+    final Tokenizer source = new StandardTokenizer(matchVersion, reader);
     TokenStream result = new LowerCaseFilter(matchVersion, source);
     // the order here is important: the stopword list is not normalized!
     result = new StopFilter( matchVersion, result, stopwords);
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ar/ArabicLetterTokenizer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ar/ArabicLetterTokenizer.java
deleted file mode 100644
index fff6148..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ar/ArabicLetterTokenizer.java
+++ /dev/null
@@ -1,96 +0,0 @@
-package org.apache.lucene.analysis.ar;
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Reader;
-
-import org.apache.lucene.analysis.core.LetterTokenizer;
-import org.apache.lucene.analysis.util.CharTokenizer;
-import org.apache.lucene.analysis.standard.StandardTokenizer; // javadoc @link
-import org.apache.lucene.util.AttributeSource;
-import org.apache.lucene.util.Version;
-
-/**
- * Tokenizer that breaks text into runs of letters and diacritics.
- * <p>
- * The problem with the standard Letter tokenizer is that it fails on diacritics.
- * Handling similar to this is necessary for Indic Scripts, Hebrew, Thaana, etc.
- * </p>
- * <p>
- * <a name="version"/>
- * You must specify the required {@link Version} compatibility when creating
- * {@link ArabicLetterTokenizer}:
- * <ul>
- * <li>As of 3.1, {@link CharTokenizer} uses an int based API to normalize and
- * detect token characters. See {@link #isTokenChar(int)} and
- * {@link #normalize(int)} for details.</li>
- * </ul>
- * @deprecated (3.1) Use {@link StandardTokenizer} instead.
- */
-@Deprecated
-public class ArabicLetterTokenizer extends LetterTokenizer {
-  /**
-   * Construct a new ArabicLetterTokenizer.
-   * @param matchVersion Lucene version
-   * to match See {@link <a href="#version">above</a>}
-   * 
-   * @param in
-   *          the input to split up into tokens
-   */
-  public ArabicLetterTokenizer(Version matchVersion, Reader in) {
-    super(matchVersion, in);
-  }
-
-  /**
-   * Construct a new ArabicLetterTokenizer using a given {@link AttributeSource}.
-   * 
-   * @param matchVersion
-   *          Lucene version to match See {@link <a href="#version">above</a>}
-   * @param source
-   *          the attribute source to use for this Tokenizer
-   * @param in
-   *          the input to split up into tokens
-   */
-  public ArabicLetterTokenizer(Version matchVersion, AttributeSource source, Reader in) {
-    super(matchVersion, source, in);
-  }
-
-  /**
-   * Construct a new ArabicLetterTokenizer using a given
-   * {@link org.apache.lucene.util.AttributeSource.AttributeFactory}. * @param
-   * matchVersion Lucene version to match See
-   * {@link <a href="#version">above</a>}
-   * 
-   * @param factory
-   *          the attribute factory to use for this Tokenizer
-   * @param in
-   *          the input to split up into tokens
-   */
-  public ArabicLetterTokenizer(Version matchVersion, AttributeFactory factory, Reader in) {
-    super(matchVersion, factory, in);
-  }
-  
-  /**
-   * Allows for Letter category or NonspacingMark category
-   * @see org.apache.lucene.analysis.core.LetterTokenizer#isTokenChar(int)
-   */
-  @Override
-  protected boolean isTokenChar(int c) {
-    return super.isTokenChar(c) || Character.getType(c) == Character.NON_SPACING_MARK;
-  }
-
-}
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ca/CatalanAnalyzer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ca/CatalanAnalyzer.java
index 294f18e..dbcd9c8 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ca/CatalanAnalyzer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ca/CatalanAnalyzer.java
@@ -38,14 +38,6 @@ import org.tartarus.snowball.ext.CatalanStemmer;
 
 /**
  * {@link Analyzer} for Catalan.
- * <p>
- * <a name="version"/>
- * <p>You must specify the required {@link Version}
- * compatibility when creating CatalanAnalyzer:
- * <ul>
- *   <li> As of 3.6, ElisionFilter with a set of Catalan 
- *        contractions is used by default.
- * </ul>
  */
 public final class CatalanAnalyzer extends StopwordAnalyzerBase {
   private final CharArraySet stemExclusionSet;
@@ -126,8 +118,8 @@ public final class CatalanAnalyzer extends StopwordAnalyzerBase {
    * @return A
    *         {@link org.apache.lucene.analysis.Analyzer.TokenStreamComponents}
    *         built from an {@link StandardTokenizer} filtered with
-   *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
-   *         , {@link KeywordMarkerFilter} if a stem exclusion set is
+   *         {@link StandardFilter}, {@link ElisionFilter}, {@link LowerCaseFilter}, 
+   *         {@link StopFilter}, {@link KeywordMarkerFilter} if a stem exclusion set is
    *         provided and {@link SnowballFilter}.
    */
   @Override
@@ -135,9 +127,7 @@ public final class CatalanAnalyzer extends StopwordAnalyzerBase {
       Reader reader) {
     final Tokenizer source = new StandardTokenizer(matchVersion, reader);
     TokenStream result = new StandardFilter(matchVersion, source);
-    if (matchVersion.onOrAfter(Version.LUCENE_36)) {
-      result = new ElisionFilter(matchVersion, result, DEFAULT_ARTICLES);
-    }
+    result = new ElisionFilter(matchVersion, result, DEFAULT_ARTICLES);
     result = new LowerCaseFilter(matchVersion, result);
     result = new StopFilter(matchVersion, result, stopwords);
     if(!stemExclusionSet.isEmpty())
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKAnalyzer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKAnalyzer.java
index 87aa5c8..2a6af08 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKAnalyzer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKAnalyzer.java
@@ -89,16 +89,11 @@ public final class CJKAnalyzer extends StopwordAnalyzerBase {
   @Override
   protected TokenStreamComponents createComponents(String fieldName,
       Reader reader) {
-    if (matchVersion.onOrAfter(Version.LUCENE_36)) {
-      final Tokenizer source = new StandardTokenizer(matchVersion, reader);
-      // run the widthfilter first before bigramming, it sometimes combines characters.
-      TokenStream result = new CJKWidthFilter(source);
-      result = new LowerCaseFilter(matchVersion, result);
-      result = new CJKBigramFilter(result);
-      return new TokenStreamComponents(source, new StopFilter(matchVersion, result, stopwords));
-    } else {
-      final Tokenizer source = new CJKTokenizer(reader);
-      return new TokenStreamComponents(source, new StopFilter(matchVersion, source, stopwords));
-    }
+    final Tokenizer source = new StandardTokenizer(matchVersion, reader);
+    // run the widthfilter first before bigramming, it sometimes combines characters.
+    TokenStream result = new CJKWidthFilter(source);
+    result = new LowerCaseFilter(matchVersion, result);
+    result = new CJKBigramFilter(result);
+    return new TokenStreamComponents(source, new StopFilter(matchVersion, result, stopwords));
   }
 }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKTokenizer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKTokenizer.java
deleted file mode 100644
index ab091b7..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKTokenizer.java
+++ /dev/null
@@ -1,317 +0,0 @@
-package org.apache.lucene.analysis.cjk;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.Reader;
-
-import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
-import org.apache.lucene.util.AttributeSource;
-
-/**
- * CJKTokenizer is designed for Chinese, Japanese, and Korean languages.
- * <p>  
- * The tokens returned are every two adjacent characters with overlap match.
- * </p>
- * <p>
- * Example: "java C1C2C3C4" will be segmented to: "java" "C1C2" "C2C3" "C3C4".
- * </p>
- * Additionally, the following is applied to Latin text (such as English):
- * <ul>
- * <li>Text is converted to lowercase.
- * <li>Numeric digits, '+', '#', and '_' are tokenized as letters.
- * <li>Full-width forms are converted to half-width forms.
- * </ul>
- * For more info on Asian language (Chinese, Japanese, and Korean) text segmentation:
- * please search  <a
- * href="http://www.google.com/search?q=word+chinese+segment">google</a>
- *
- * @deprecated Use StandardTokenizer, CJKWidthFilter, CJKBigramFilter, and LowerCaseFilter instead.
- */
-@Deprecated
-public final class CJKTokenizer extends Tokenizer {
-    //~ Static fields/initializers ---------------------------------------------
-    /** Word token type */
-    static final int WORD_TYPE = 0;
-  
-    /** Single byte token type */
-    static final int SINGLE_TOKEN_TYPE = 1;
-
-    /** Double byte token type */
-    static final int DOUBLE_TOKEN_TYPE = 2;
-  
-    /** Names for token types */
-    static final String[] TOKEN_TYPE_NAMES = { "word", "single", "double" };
-  
-    /** Max word length */
-    private static final int MAX_WORD_LEN = 255;
-
-    /** buffer size: */
-    private static final int IO_BUFFER_SIZE = 256;
-
-    //~ Instance fields --------------------------------------------------------
-
-    /** word offset, used to imply which character(in ) is parsed */
-    private int offset = 0;
-
-    /** the index used only for ioBuffer */
-    private int bufferIndex = 0;
-
-    /** data length */
-    private int dataLen = 0;
-
-    /**
-     * character buffer, store the characters which are used to compose <br>
-     * the returned Token
-     */
-    private final char[] buffer = new char[MAX_WORD_LEN];
-
-    /**
-     * I/O buffer, used to store the content of the input(one of the <br>
-     * members of Tokenizer)
-     */
-    private final char[] ioBuffer = new char[IO_BUFFER_SIZE];
-
-    /** word type: single=>ASCII  double=>non-ASCII word=>default */
-    private int tokenType = WORD_TYPE;
-
-    /**
-     * tag: previous character is a cached double-byte character  "C1C2C3C4"
-     * ----(set the C1 isTokened) C1C2 "C2C3C4" ----(set the C2 isTokened)
-     * C1C2 C2C3 "C3C4" ----(set the C3 isTokened) "C1C2 C2C3 C3C4"
-     */
-    private boolean preIsTokened = false;
-
-    private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-    private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
-    private final TypeAttribute typeAtt = addAttribute(TypeAttribute.class);
-    
-    //~ Constructors -----------------------------------------------------------
-
-    /**
-     * Construct a token stream processing the given input.
-     *
-     * @param in I/O reader
-     */
-    public CJKTokenizer(Reader in) {
-      super(in);
-    }
-
-    public CJKTokenizer(AttributeSource source, Reader in) {
-      super(source, in);
-    }
-
-    public CJKTokenizer(AttributeFactory factory, Reader in) {
-      super(factory, in);
-    }
-    
-    //~ Methods ----------------------------------------------------------------
-
-    /**
-     * Returns true for the next token in the stream, or false at EOS.
-     * See http://java.sun.com/j2se/1.3/docs/api/java/lang/Character.UnicodeBlock.html
-     * for detail.
-     *
-     * @return false for end of stream, true otherwise
-     *
-     * @throws java.io.IOException - throw IOException when read error <br>
-     *         happened in the InputStream
-     *
-     */
-    @Override
-    public boolean incrementToken() throws IOException {
-        clearAttributes();
-        /** how many character(s) has been stored in buffer */
-
-        while(true) { // loop until we find a non-empty token
-
-          int length = 0;
-
-          /** the position used to create Token */
-          int start = offset;
-
-          while (true) { // loop until we've found a full token
-            /** current character */
-            char c;
-
-            /** unicode block of current character for detail */
-            Character.UnicodeBlock ub;
-
-            offset++;
-
-            if (bufferIndex >= dataLen) {
-                dataLen = input.read(ioBuffer);
-                bufferIndex = 0;
-            }
-
-            if (dataLen == -1) {
-                if (length > 0) {
-                    if (preIsTokened == true) {
-                        length = 0;
-                        preIsTokened = false;
-                    }
-                    else{
-                      offset--;
-                    }
-
-                    break;
-                } else {
-                    offset--;
-                    return false;
-                }
-            } else {
-                //get current character
-                c = ioBuffer[bufferIndex++];
-
-                //get the UnicodeBlock of the current character
-                ub = Character.UnicodeBlock.of(c);
-            }
-
-            //if the current character is ASCII or Extend ASCII
-            if ((ub == Character.UnicodeBlock.BASIC_LATIN)
-                    || (ub == Character.UnicodeBlock.HALFWIDTH_AND_FULLWIDTH_FORMS)
-               ) {
-                if (ub == Character.UnicodeBlock.HALFWIDTH_AND_FULLWIDTH_FORMS) {
-                  int i = (int) c;
-                  if (i >= 65281 && i <= 65374) {
-                    // convert certain HALFWIDTH_AND_FULLWIDTH_FORMS to BASIC_LATIN
-                    i = i - 65248;
-                    c = (char) i;
-                  }
-                }
-
-                // if the current character is a letter or "_" "+" "#"
-                if (Character.isLetterOrDigit(c)
-                        || ((c == '_') || (c == '+') || (c == '#'))
-                   ) {
-                    if (length == 0) {
-                        // "javaC1C2C3C4linux" <br>
-                        //      ^--: the current character begin to token the ASCII
-                        // letter
-                        start = offset - 1;
-                    } else if (tokenType == DOUBLE_TOKEN_TYPE) {
-                        // "javaC1C2C3C4linux" <br>
-                        //              ^--: the previous non-ASCII
-                        // : the current character
-                        offset--;
-                        bufferIndex--;
-
-                        if (preIsTokened == true) {
-                            // there is only one non-ASCII has been stored
-                            length = 0;
-                            preIsTokened = false;
-                            break;
-                        } else {
-                            break;
-                        }
-                    }
-
-                    // store the LowerCase(c) in the buffer
-                    buffer[length++] = Character.toLowerCase(c);
-                    tokenType = SINGLE_TOKEN_TYPE;
-
-                    // break the procedure if buffer overflowed!
-                    if (length == MAX_WORD_LEN) {
-                        break;
-                    }
-                } else if (length > 0) {
-                    if (preIsTokened == true) {
-                        length = 0;
-                        preIsTokened = false;
-                    } else {
-                        break;
-                    }
-                }
-            } else {
-                // non-ASCII letter, e.g."C1C2C3C4"
-                if (Character.isLetter(c)) {
-                    if (length == 0) {
-                        start = offset - 1;
-                        buffer[length++] = c;
-                        tokenType = DOUBLE_TOKEN_TYPE;
-                    } else {
-                      if (tokenType == SINGLE_TOKEN_TYPE) {
-                            offset--;
-                            bufferIndex--;
-
-                            //return the previous ASCII characters
-                            break;
-                        } else {
-                            buffer[length++] = c;
-                            tokenType = DOUBLE_TOKEN_TYPE;
-
-                            if (length == 2) {
-                                offset--;
-                                bufferIndex--;
-                                preIsTokened = true;
-
-                                break;
-                            }
-                        }
-                    }
-                } else if (length > 0) {
-                    if (preIsTokened == true) {
-                        // empty the buffer
-                        length = 0;
-                        preIsTokened = false;
-                    } else {
-                        break;
-                    }
-                }
-            }
-        }
-      
-        if (length > 0) {
-          termAtt.copyBuffer(buffer, 0, length);
-          offsetAtt.setOffset(correctOffset(start), correctOffset(start+length));
-          typeAtt.setType(TOKEN_TYPE_NAMES[tokenType]);
-          return true;
-        } else if (dataLen == -1) {
-          offset--;
-          return false;
-        }
-
-        // Cycle back and try for the next token (don't
-        // return an empty string)
-      }
-    }
-    
-    @Override
-    public final void end() {
-      // set final offset
-      final int finalOffset = correctOffset(offset);
-      this.offsetAtt.setOffset(finalOffset, finalOffset);
-    }
-    
-    @Override
-    public void reset() throws IOException {
-      super.reset();
-      offset = bufferIndex = dataLen = 0;
-      preIsTokened = false;
-      tokenType = WORD_TYPE;
-    }
-    
-    @Override
-    public void reset(Reader reader) throws IOException {
-      super.reset(reader);
-      reset();
-    }
-}
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/cn/ChineseAnalyzer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/cn/ChineseAnalyzer.java
deleted file mode 100644
index 886f5e7..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/cn/ChineseAnalyzer.java
+++ /dev/null
@@ -1,50 +0,0 @@
-package org.apache.lucene.analysis.cn;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Reader;
-
-import org.apache.lucene.analysis.standard.StandardAnalyzer; // javadoc @link
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.Tokenizer;
-
-/**
- * An {@link Analyzer} that tokenizes text with {@link ChineseTokenizer} and
- * filters with {@link ChineseFilter}
- * @deprecated (3.1) Use {@link StandardAnalyzer} instead, which has the same functionality.
- * This analyzer will be removed in Lucene 5.0
- */
-@Deprecated
-public final class ChineseAnalyzer extends Analyzer {
-
-  /**
-   * Creates
-   * {@link org.apache.lucene.analysis.Analyzer.TokenStreamComponents}
-   * used to tokenize all the text in the provided {@link Reader}.
-   * 
-   * @return {@link org.apache.lucene.analysis.Analyzer.TokenStreamComponents}
-   *         built from a {@link ChineseTokenizer} filtered with
-   *         {@link ChineseFilter}
-   */
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName,
-        Reader reader) {
-      final Tokenizer source = new ChineseTokenizer(reader);
-      return new TokenStreamComponents(source, new ChineseFilter(source));
-    }
-}
\ No newline at end of file
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/cn/ChineseFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/cn/ChineseFilter.java
deleted file mode 100644
index aa9743b..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/cn/ChineseFilter.java
+++ /dev/null
@@ -1,104 +0,0 @@
-package org.apache.lucene.analysis.cn;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.analysis.TokenFilter;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.core.StopFilter;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.util.CharArraySet;
-import org.apache.lucene.util.Version;
-
-/**
- * A {@link TokenFilter} with a stop word table.  
- * <ul>
- * <li>Numeric tokens are removed.
- * <li>English tokens must be larger than 1 character.
- * <li>One Chinese character as one Chinese word.
- * </ul>
- * TO DO:
- * <ol>
- * <li>Add Chinese stop words, such as \ue400
- * <li>Dictionary based Chinese word extraction
- * <li>Intelligent Chinese word extraction
- * </ol>
- * 
- * @deprecated (3.1) Use {@link StopFilter} instead, which has the same functionality.
- * This filter will be removed in Lucene 5.0
- */
-@Deprecated
-public final class ChineseFilter extends TokenFilter {
-
-
-    // Only English now, Chinese to be added later.
-    public static final String[] STOP_WORDS = {
-    "and", "are", "as", "at", "be", "but", "by",
-    "for", "if", "in", "into", "is", "it",
-    "no", "not", "of", "on", "or", "such",
-    "that", "the", "their", "then", "there", "these",
-    "they", "this", "to", "was", "will", "with"
-    };
-
-
-    private CharArraySet stopTable;
-
-    private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-    
-    public ChineseFilter(TokenStream in) {
-        super(in);
-
-        stopTable = new CharArraySet(Version.LUCENE_CURRENT, Arrays.asList(STOP_WORDS), false);
-    }
-
-    @Override
-    public boolean incrementToken() throws IOException {
-
-        while (input.incrementToken()) {
-            char text[] = termAtt.buffer();
-            int termLength = termAtt.length();
-
-          // why not key off token type here assuming ChineseTokenizer comes first?
-            if (!stopTable.contains(text, 0, termLength)) {
-                switch (Character.getType(text[0])) {
-
-                case Character.LOWERCASE_LETTER:
-                case Character.UPPERCASE_LETTER:
-
-                    // English word/token should larger than 1 character.
-                    if (termLength>1) {
-                        return true;
-                    }
-                    break;
-                case Character.OTHER_LETTER:
-
-                    // One Chinese character as one Chinese word.
-                    // Chinese word extraction to be added later here.
-
-                    return true;
-                }
-
-            }
-
-        }
-        return false;
-    }
-
-}
\ No newline at end of file
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/cn/ChineseTokenizer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/cn/ChineseTokenizer.java
deleted file mode 100644
index ddbbf18..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/cn/ChineseTokenizer.java
+++ /dev/null
@@ -1,175 +0,0 @@
-package org.apache.lucene.analysis.cn;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-import java.io.IOException;
-import java.io.Reader;
-
-import org.apache.lucene.analysis.standard.StandardTokenizer;
-import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.util.AttributeSource;
-
-
-/**
- * Tokenize Chinese text as individual chinese characters.
- * 
- * <p>
- * The difference between ChineseTokenizer and
- * CJKTokenizer is that they have different
- * token parsing logic.
- * </p>
- * <p>
- * For example, if the Chinese text
- * "C1C2C3C4" is to be indexed:
- * <ul>
- * <li>The tokens returned from ChineseTokenizer are C1, C2, C3, C4. 
- * <li>The tokens returned from the CJKTokenizer are C1C2, C2C3, C3C4.
- * </ul>
- * </p>
- * <p>
- * Therefore the index created by CJKTokenizer is much larger.
- * </p>
- * <p>
- * The problem is that when searching for C1, C1C2, C1C3,
- * C4C2, C1C2C3 ... the ChineseTokenizer works, but the
- * CJKTokenizer will not work.
- * </p>
- * @deprecated (3.1) Use {@link StandardTokenizer} instead, which has the same functionality.
- * This filter will be removed in Lucene 5.0
- */
-@Deprecated
-public final class ChineseTokenizer extends Tokenizer {
-
-
-    public ChineseTokenizer(Reader in) {
-      super(in);
-    }
-
-    public ChineseTokenizer(AttributeSource source, Reader in) {
-      super(source, in);
-    }
-
-    public ChineseTokenizer(AttributeFactory factory, Reader in) {
-      super(factory, in);
-    }
-       
-    private int offset = 0, bufferIndex=0, dataLen=0;
-    private final static int MAX_WORD_LEN = 255;
-    private final static int IO_BUFFER_SIZE = 1024;
-    private final char[] buffer = new char[MAX_WORD_LEN];
-    private final char[] ioBuffer = new char[IO_BUFFER_SIZE];
-
-
-    private int length;
-    private int start;
-
-    private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-    private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
-    
-    private final void push(char c) {
-
-        if (length == 0) start = offset-1;            // start of token
-        buffer[length++] = Character.toLowerCase(c);  // buffer it
-
-    }
-
-    private final boolean flush() {
-
-        if (length>0) {
-            //System.out.println(new String(buffer, 0,
-            //length));
-          termAtt.copyBuffer(buffer, 0, length);
-          offsetAtt.setOffset(correctOffset(start), correctOffset(start+length));
-          return true;
-        }
-        else
-            return false;
-    }
-
-    @Override
-    public boolean incrementToken() throws IOException {
-        clearAttributes();
-
-        length = 0;
-        start = offset;
-
-
-        while (true) {
-
-            final char c;
-            offset++;
-
-            if (bufferIndex >= dataLen) {
-                dataLen = input.read(ioBuffer);
-                bufferIndex = 0;
-            }
-
-            if (dataLen == -1) {
-              offset--;
-              return flush();
-            } else
-                c = ioBuffer[bufferIndex++];
-
-
-            switch(Character.getType(c)) {
-
-            case Character.DECIMAL_DIGIT_NUMBER:
-            case Character.LOWERCASE_LETTER:
-            case Character.UPPERCASE_LETTER:
-                push(c);
-                if (length == MAX_WORD_LEN) return flush();
-                break;
-
-            case Character.OTHER_LETTER:
-                if (length>0) {
-                    bufferIndex--;
-                    offset--;
-                    return flush();
-                }
-                push(c);
-                return flush();
-
-            default:
-                if (length>0) return flush();
-                break;
-            }
-        }
-    }
-    
-    @Override
-    public final void end() {
-      // set final offset
-      final int finalOffset = correctOffset(offset);
-      this.offsetAtt.setOffset(finalOffset, finalOffset);
-    }
-
-    @Override
-    public void reset() throws IOException {
-      super.reset();
-      offset = bufferIndex = dataLen = 0;
-    }
-    
-    @Override
-    public void reset(Reader input) throws IOException {
-      super.reset(input);
-      reset();
-    }
-}
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/cn/package.html b/lucene/analysis/common/src/java/org/apache/lucene/analysis/cn/package.html
deleted file mode 100644
index 6d9ea04..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/cn/package.html
+++ /dev/null
@@ -1,41 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
-</head>
-<body>
-Analyzer for Chinese, which indexes unigrams (individual chinese characters).
-<p>
-Three analyzers are provided for Chinese, each of which treats Chinese text in a different way.
-<ul>
-	<li>StandardAnalyzer: Index unigrams (individual Chinese characters) as a token.
-	<li>CJKAnalyzer (in the analyzers/cjk package): Index bigrams (overlapping groups of two adjacent Chinese characters) as tokens.
-	<li>SmartChineseAnalyzer (in the analyzers/smartcn package): Index words (attempt to segment Chinese text into words) as tokens.
-</ul>
-
-Example phraseï¼? "???ä¸??äº?"
-<ol>
-	<li>StandardAnalyzer: ??????ä¸???½ï?äº?</li>
-	<li>CJKAnalyzer: ???ï¼??ä¸??ä¸??ï¼??äº?</li>
-	<li>SmartChineseAnalyzer: ??????ä¸??ï¼?ºº</li>
-</ol>
-</p>
-
-</body>
-</html>
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/cz/CzechAnalyzer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/cz/CzechAnalyzer.java
index b0d9c80..702a1f2 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/cz/CzechAnalyzer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/cz/CzechAnalyzer.java
@@ -40,17 +40,6 @@ import java.io.*;
  * all). A default set of stopwords is used unless an alternative list is
  * specified.
  * </p>
- * 
- * <a name="version"/>
- * <p>
- * You must specify the required {@link Version} compatibility when creating
- * CzechAnalyzer:
- * <ul>
- * <li>As of 3.1, words are stemmed with {@link CzechStemFilter}
- * <li>As of 2.9, StopFilter preserves position increments
- * <li>As of 2.4, Tokens incorrectly identified as acronyms are corrected (see
- * <a href="https://issues.apache.org/jira/browse/LUCENE-1068">LUCENE-1068</a>)
- * </ul>
  */
 public final class CzechAnalyzer extends StopwordAnalyzerBase {
   /** File containing default Czech stopwords. */
@@ -86,8 +75,7 @@ public final class CzechAnalyzer extends StopwordAnalyzerBase {
   /**
    * Builds an analyzer with the default stop words ({@link #getDefaultStopSet()}).
    *
-   * @param matchVersion Lucene version to match See
-   *          {@link <a href="#version">above</a>}
+   * @param matchVersion Lucene version to match
    */
 	public CzechAnalyzer(Version matchVersion) {
     this(matchVersion, DefaultSetHolder.DEFAULT_SET);
@@ -96,8 +84,7 @@ public final class CzechAnalyzer extends StopwordAnalyzerBase {
   /**
    * Builds an analyzer with the given stop words.
    *
-   * @param matchVersion Lucene version to match See
-   *          {@link <a href="#version">above</a>}
+   * @param matchVersion Lucene version to match
    * @param stopwords a stopword set
    */
   public CzechAnalyzer(Version matchVersion, CharArraySet stopwords) {
@@ -108,8 +95,7 @@ public final class CzechAnalyzer extends StopwordAnalyzerBase {
    * Builds an analyzer with the given stop words and a set of work to be
    * excluded from the {@link CzechStemFilter}.
    * 
-   * @param matchVersion Lucene version to match See
-   *          {@link <a href="#version">above</a>}
+   * @param matchVersion Lucene version to match
    * @param stopwords a stopword set
    * @param stemExclusionTable a stemming exclusion set
    */
@@ -127,7 +113,7 @@ public final class CzechAnalyzer extends StopwordAnalyzerBase {
    *         built from a {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
    *         , and {@link CzechStemFilter} (only if version is >= LUCENE_31). If
-   *         a version is >= LUCENE_31 and a stem exclusion set is provided via
+   *         a stem exclusion set is provided via
    *         {@link #CzechAnalyzer(Version, CharArraySet, CharArraySet)} a
    *         {@link KeywordMarkerFilter} is added before
    *         {@link CzechStemFilter}.
@@ -139,11 +125,9 @@ public final class CzechAnalyzer extends StopwordAnalyzerBase {
     TokenStream result = new StandardFilter(matchVersion, source);
     result = new LowerCaseFilter(matchVersion, result);
     result = new StopFilter( matchVersion, result, stopwords);
-    if (matchVersion.onOrAfter(Version.LUCENE_31)) {
-      if(!this.stemExclusionTable.isEmpty())
-        result = new KeywordMarkerFilter(result, stemExclusionTable);
-      result = new CzechStemFilter(result);
-    }
+    if(!this.stemExclusionTable.isEmpty())
+      result = new KeywordMarkerFilter(result, stemExclusionTable);
+    result = new CzechStemFilter(result);
     return new TokenStreamComponents(source, result);
   }
 }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/de/GermanAnalyzer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/de/GermanAnalyzer.java
index d64c93b..ffcd569 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/de/GermanAnalyzer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/de/GermanAnalyzer.java
@@ -20,7 +20,6 @@ package org.apache.lucene.analysis.de;
 
 import java.io.IOException;
 import java.io.Reader;
-import java.util.Arrays;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.core.LowerCaseFilter;
@@ -37,7 +36,6 @@ import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.Version;
-import org.tartarus.snowball.ext.German2Stemmer;
 
 /**
  * {@link Analyzer} for German language. 
@@ -49,39 +47,11 @@ import org.tartarus.snowball.ext.German2Stemmer;
  * exclusion list is empty by default.
  * </p>
  * 
- * <a name="version"/>
- * <p>You must specify the required {@link Version}
- * compatibility when creating GermanAnalyzer:
- * <ul>
- *   <li> As of 3.6, GermanLightStemFilter is used for less aggressive stemming.
- *   <li> As of 3.1, Snowball stemming is done with SnowballFilter, and 
- *        Snowball stopwords are used by default.
- *   <li> As of 2.9, StopFilter preserves position
- *        increments
- * </ul>
- * 
  * <p><b>NOTE</b>: This class uses the same {@link Version}
  * dependent settings as {@link StandardAnalyzer}.</p>
  */
 public final class GermanAnalyzer extends StopwordAnalyzerBase {
   
-  /** @deprecated in 3.1, remove in Lucene 5.0 (index bw compat) */
-  @Deprecated
-  private final static String[] GERMAN_STOP_WORDS = {
-    "einer", "eine", "eines", "einem", "einen",
-    "der", "die", "das", "dass", "da?",
-    "du", "er", "sie", "es",
-    "was", "wer", "wie", "wir",
-    "und", "oder", "ohne", "mit",
-    "am", "im", "in", "aus", "auf",
-    "ist", "sein", "war", "wird",
-    "ihr", "ihre", "ihres",
-    "als", "fÃ¼r", "von", "mit",
-    "dich", "dir", "mich", "mir",
-    "mein", "sein", "kein",
-    "durch", "wegen", "wird"
-  };
-  
   /** File containing default German stopwords. */
   public final static String DEFAULT_STOPWORD_FILE = "german_stop.txt";
   
@@ -94,10 +64,6 @@ public final class GermanAnalyzer extends StopwordAnalyzerBase {
   }
   
   private static class DefaultSetHolder {
-    /** @deprecated in 3.1, remove in Lucene 5.0 (index bw compat) */
-    @Deprecated
-    private static final CharArraySet DEFAULT_SET_30 = CharArraySet.unmodifiableSet(new CharArraySet(
-        Version.LUCENE_CURRENT, Arrays.asList(GERMAN_STOP_WORDS), false));
     private static final CharArraySet DEFAULT_SET;
     static {
       try {
@@ -125,9 +91,7 @@ public final class GermanAnalyzer extends StopwordAnalyzerBase {
    * {@link #getDefaultStopSet()}.
    */
   public GermanAnalyzer(Version matchVersion) {
-    this(matchVersion,
-        matchVersion.onOrAfter(Version.LUCENE_31) ? DefaultSetHolder.DEFAULT_SET
-            : DefaultSetHolder.DEFAULT_SET_30);
+    this(matchVersion, DefaultSetHolder.DEFAULT_SET);
   }
   
   /**
@@ -176,14 +140,8 @@ public final class GermanAnalyzer extends StopwordAnalyzerBase {
     result = new LowerCaseFilter(matchVersion, result);
     result = new StopFilter( matchVersion, result, stopwords);
     result = new KeywordMarkerFilter(result, exclusionSet);
-    if (matchVersion.onOrAfter(Version.LUCENE_36)) {
-      result = new GermanNormalizationFilter(result);
-      result = new GermanLightStemFilter(result);
-    } else if (matchVersion.onOrAfter(Version.LUCENE_31)) {
-      result = new SnowballFilter(result, new German2Stemmer());
-    } else {
-      result = new GermanStemFilter(result);
-    }
+    result = new GermanNormalizationFilter(result);
+    result = new GermanLightStemFilter(result);
     return new TokenStreamComponents(source, result);
   }
 }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/el/GreekAnalyzer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/el/GreekAnalyzer.java
index 3b1b5ac..625c9d1 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/el/GreekAnalyzer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/el/GreekAnalyzer.java
@@ -37,15 +37,6 @@ import org.apache.lucene.util.Version;
  * that will not be indexed at all).
  * A default set of stopwords is used unless an alternative list is specified.
  * </p>
- *
- * <a name="version"/>
- * <p>You must specify the required {@link Version}
- * compatibility when creating GreekAnalyzer:
- * <ul>
- *   <li> As of 3.1, StandardFilter and GreekStemmer are used by default.
- *   <li> As of 2.9, StopFilter preserves position
- *        increments
- * </ul>
  * 
  * <p><b>NOTE</b>: This class uses the same {@link Version}
  * dependent settings as {@link StandardAnalyzer}.</p>
@@ -78,8 +69,7 @@ public final class GreekAnalyzer extends StopwordAnalyzerBase {
   
   /**
    * Builds an analyzer with the default stop words.
-   * @param matchVersion Lucene compatibility version,
-   *   See <a href="#version">above</a>
+   * @param matchVersion Lucene compatibility version
    */
   public GreekAnalyzer(Version matchVersion) {
     this(matchVersion, DefaultSetHolder.DEFAULT_SET);
@@ -91,8 +81,7 @@ public final class GreekAnalyzer extends StopwordAnalyzerBase {
    * <b>NOTE:</b> The stopwords set should be pre-processed with the logic of 
    * {@link GreekLowerCaseFilter} for best results.
    *  
-   * @param matchVersion Lucene compatibility version,
-   *   See <a href="#version">above</a>
+   * @param matchVersion Lucene compatibility version
    * @param stopwords a stopword set
    */
   public GreekAnalyzer(Version matchVersion, CharArraySet stopwords) {
@@ -114,11 +103,9 @@ public final class GreekAnalyzer extends StopwordAnalyzerBase {
       Reader reader) {
     final Tokenizer source = new StandardTokenizer(matchVersion, reader);
     TokenStream result = new GreekLowerCaseFilter(matchVersion, source);
-    if (matchVersion.onOrAfter(Version.LUCENE_31))
-      result = new StandardFilter(matchVersion, result);
+    result = new StandardFilter(matchVersion, result);
     result = new StopFilter(matchVersion, result, stopwords);
-    if (matchVersion.onOrAfter(Version.LUCENE_31))
-      result = new GreekStemFilter(result);
+    result = new GreekStemFilter(result);
     return new TokenStreamComponents(source, result);
   }
 }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/el/GreekStemmer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/el/GreekStemmer.java
index bd26338..1ae6b8b 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/el/GreekStemmer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/el/GreekStemmer.java
@@ -196,7 +196,7 @@ public class GreekStemmer {
     return len;
   }
   
-  private static final CharArraySet exc4 = new CharArraySet(Version.LUCENE_31,
+  private static final CharArraySet exc4 = new CharArraySet(Version.LUCENE_50,
       Arrays.asList("Î¸", "Î´", "ÎµÎ»", "Î³Î±Î»", "Î½", "?", "Î¹Î´", "?Î±?"),
       false);
   
@@ -222,7 +222,7 @@ public class GreekStemmer {
     return len;
   }
 
-  private static final CharArraySet exc6 = new CharArraySet(Version.LUCENE_31,
+  private static final CharArraySet exc6 = new CharArraySet(Version.LUCENE_50,
       Arrays.asList("Î±Î»", "Î±Î´", "ÎµÎ½Î´", "Î±Î¼Î±Î½", "Î±Î¼Î¼Î¿?Î±Î»", "Î·Î¸", "Î±Î½Î·Î¸",
           "Î±Î½?Î¹Î´", "???", "Î²??Î¼", "Î³Îµ?", "ÎµÎ¾?Î´", "ÎºÎ±Î»?", "ÎºÎ±Î»Î»Î¹Î½", "ÎºÎ±?Î±Î´",
           "Î¼Î¿?Î»", "Î¼?Î±Î½", "Î¼?Î±Î³Î¹Î±?", "Î¼?Î¿Î»", "Î¼?Î¿?", "Î½Î¹?", "Î¾Î¹Îº", "??Î½Î¿Î¼Î·Î»",
@@ -247,7 +247,7 @@ public class GreekStemmer {
     return len;
   }
   
-  private static final CharArraySet exc7 = new CharArraySet(Version.LUCENE_31,
+  private static final CharArraySet exc7 = new CharArraySet(Version.LUCENE_50,
       Arrays.asList("Î±Î½Î±?", "Î±?Î¿Î¸", "Î±?Î¿Îº", "Î±?Î¿??", "Î²Î¿?Î²", "Î¾ÎµÎ¸", "Î¿?Î»",
           "?ÎµÎ¸", "?Î¹Îº?", "?Î¿?", "?Î¹?", "?"), 
       false);
@@ -274,11 +274,11 @@ public class GreekStemmer {
     return len;
   }
 
-  private static final CharArraySet exc8a = new CharArraySet(Version.LUCENE_31,
+  private static final CharArraySet exc8a = new CharArraySet(Version.LUCENE_50,
       Arrays.asList("??", "??"),
       false);
 
-  private static final CharArraySet exc8b = new CharArraySet(Version.LUCENE_31,
+  private static final CharArraySet exc8b = new CharArraySet(Version.LUCENE_50,
       Arrays.asList("Î²Îµ?Îµ?", "Î²Î¿?Î»Îº", "Î²?Î±?Î¼", "Î³", "Î´?Î±Î´Î¿?Î¼", "Î¸", "ÎºÎ±Î»?Î¿?Î¶",
           "ÎºÎ±??ÎµÎ»", "ÎºÎ¿?Î¼Î¿?", "Î»Î±Î¿?Î»", "Î¼?Î±Î¼ÎµÎ¸", "Î¼", "Î¼Î¿??Î¿?Î»Î¼", "Î½", "Î¿?Î»",
           "?", "?ÎµÎ»ÎµÎº", "?Î»", "?Î¿Î»Î¹?", "?Î¿??Î¿Î»", "?Î±?Î±ÎºÎ±??", "?Î¿?Î»?",
@@ -337,7 +337,7 @@ public class GreekStemmer {
     return len;
   }
   
-  private static final CharArraySet exc9 = new CharArraySet(Version.LUCENE_31,
+  private static final CharArraySet exc9 = new CharArraySet(Version.LUCENE_50,
       Arrays.asList("Î±Î²Î±?", "Î²ÎµÎ½", "ÎµÎ½Î±?", "Î±Î²?", "Î±Î´", "Î±Î¸", "Î±Î½", "Î±?Î»",
           "Î²Î±?Î¿Î½", "Î½??", "?Îº", "ÎºÎ¿?", "Î¼?Î¿?", "Î½Î¹?", "?Î±Î³", "?Î±?Î±ÎºÎ±Î»", "?Îµ??",
           "?ÎºÎµÎ»", "????", "?Î¿Îº", "?", "Î´", "ÎµÎ¼", "Î¸Î±??", "Î¸"), 
@@ -425,11 +425,11 @@ public class GreekStemmer {
     return len;
   }
 
-  private static final CharArraySet exc12a = new CharArraySet(Version.LUCENE_31,
+  private static final CharArraySet exc12a = new CharArraySet(Version.LUCENE_50,
       Arrays.asList("?", "Î±?", "??Î¼?", "Î±??Î¼?", "Î±ÎºÎ±?Î±?", "Î±Î¼Îµ?Î±Î¼?"),
       false);
 
-  private static final CharArraySet exc12b = new CharArraySet(Version.LUCENE_31,
+  private static final CharArraySet exc12b = new CharArraySet(Version.LUCENE_50,
       Arrays.asList("Î±Î»", "Î±?", "ÎµÎº?ÎµÎ»", "Î¶", "Î¼", "Î¾", "?Î±?Î±ÎºÎ±Î»", "Î±?", "??Î¿", "Î½Î¹?"),
       false);
   
@@ -449,7 +449,7 @@ public class GreekStemmer {
     return len;
   }
   
-  private static final CharArraySet exc13 = new CharArraySet(Version.LUCENE_31,
+  private static final CharArraySet exc13 = new CharArraySet(Version.LUCENE_50,
       Arrays.asList("Î´Î¹Î±Î¸", "Î¸", "?Î±?Î±ÎºÎ±?Î±Î¸", "??Î¿?Î¸", "??Î½Î¸"),
       false);
   
@@ -483,7 +483,7 @@ public class GreekStemmer {
     return len;
   }
   
-  private static final CharArraySet exc14 = new CharArraySet(Version.LUCENE_31,
+  private static final CharArraySet exc14 = new CharArraySet(Version.LUCENE_50,
       Arrays.asList("?Î±?Î¼Î±Îº", "?Î±Î´", "Î±Î³Îº", "Î±Î½Î±??", "Î²?Î¿Î¼", "ÎµÎºÎ»Î¹?", "Î»Î±Î¼?Î¹Î´",
           "Î»Îµ?", "Î¼", "?Î±?", "?", "Î»", "Î¼ÎµÎ´", "Î¼Îµ?Î±Î¶", "??Î¿?ÎµÎ¹Î½", "Î±Î¼", "Î±Î¹Î¸",
           "Î±Î½Î·Îº", "Î´Îµ??Î¿Î¶", "ÎµÎ½Î´Î¹Î±?Îµ?", "Î´Îµ", "Î´Îµ??Îµ?Îµ?", "ÎºÎ±Î¸Î±?Îµ?", "?Î»Îµ",
@@ -521,7 +521,7 @@ public class GreekStemmer {
    return len;
   }
   
-  private static final CharArraySet exc15a = new CharArraySet(Version.LUCENE_31,
+  private static final CharArraySet exc15a = new CharArraySet(Version.LUCENE_50,
       Arrays.asList("Î±Î²Î±??", "?Î¿Î»??", "Î±Î´Î·?", "?Î±Î¼?", "?", "Î±??", "Î±?", "Î±Î¼Î±Î»",
           "Î±Î¼Î±Î»Î»Î¹", "Î±Î½???", "Î±?Îµ?", "Î±??Î±?", "Î±?Î±?", "Î´Îµ?Î²ÎµÎ½", "Î´?Î¿?Î¿?",
           "Î¾Îµ?", "Î½ÎµÎ¿?", "Î½Î¿Î¼Î¿?", "Î¿Î»Î¿?", "Î¿Î¼Î¿?", "??Î¿??", "??Î¿???Î¿?", "??Î¼?",
@@ -530,7 +530,7 @@ public class GreekStemmer {
           "Î¿?Î»Î±Î¼", "Î¿??", "?", "??", "Î¼"), 
       false);
   
-  private static final CharArraySet exc15b = new CharArraySet(Version.LUCENE_31,
+  private static final CharArraySet exc15b = new CharArraySet(Version.LUCENE_50,
       Arrays.asList("?Î¿?", "Î½Î±?Î»Î¿?"),
       false);
   
@@ -567,7 +567,7 @@ public class GreekStemmer {
     return len;
   }
   
-  private static final CharArraySet exc16 = new CharArraySet(Version.LUCENE_31,
+  private static final CharArraySet exc16 = new CharArraySet(Version.LUCENE_50,
       Arrays.asList("Î½", "?Îµ??Î¿Î½", "Î´?Î´ÎµÎºÎ±Î½", "Îµ?Î·Î¼Î¿Î½", "Î¼ÎµÎ³Î±Î»Î¿Î½", "Îµ??Î±Î½"),
       false);
   
@@ -587,7 +587,7 @@ public class GreekStemmer {
     return len;
   }
   
-  private static final CharArraySet exc17 = new CharArraySet(Version.LUCENE_31,
+  private static final CharArraySet exc17 = new CharArraySet(Version.LUCENE_50,
       Arrays.asList("Î±?Î²", "?Î²", "Î±??", "??", "Î±?Î»", "Î±ÎµÎ¹Î¼Î½", "Î´????", "Îµ???", "ÎºÎ¿Î¹Î½Î¿??", "?Î±Î»Î¹Î¼?"),
       false);
   
@@ -601,7 +601,7 @@ public class GreekStemmer {
     return len;
   }
   
-  private static final CharArraySet exc18 = new CharArraySet(Version.LUCENE_31,
+  private static final CharArraySet exc18 = new CharArraySet(Version.LUCENE_50,
       Arrays.asList("Î½", "?", "??Î¹", "???Î±Î²Î¿Î¼Î¿???", "ÎºÎ±ÎºÎ¿Î¼Î¿???", "ÎµÎ¾?Î½"),
       false);
   
@@ -625,7 +625,7 @@ public class GreekStemmer {
     return len;
   }
   
-  private static final CharArraySet exc19 = new CharArraySet(Version.LUCENE_31,
+  private static final CharArraySet exc19 = new CharArraySet(Version.LUCENE_50,
       Arrays.asList("?Î±?Î±?Î¿??", "?", "?", "??Î¹Î¿?Î»", "Î±Î¶", "Î±Î»Î»Î¿?Î¿??", "Î±?Î¿??"),
       false);
   
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/EnglishAnalyzer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/EnglishAnalyzer.java
index 1648b9e..9e7cb5c 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/EnglishAnalyzer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/EnglishAnalyzer.java
@@ -94,7 +94,8 @@ public final class EnglishAnalyzer extends StopwordAnalyzerBase {
    * @return A
    *         {@link org.apache.lucene.analysis.Analyzer.TokenStreamComponents}
    *         built from an {@link StandardTokenizer} filtered with
-   *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
+   *         {@link StandardFilter}, {@link EnglishPossessiveFilter}, 
+   *         {@link LowerCaseFilter}, {@link StopFilter}
    *         , {@link KeywordMarkerFilter} if a stem exclusion set is
    *         provided and {@link PorterStemFilter}.
    */
@@ -103,9 +104,7 @@ public final class EnglishAnalyzer extends StopwordAnalyzerBase {
       Reader reader) {
     final Tokenizer source = new StandardTokenizer(matchVersion, reader);
     TokenStream result = new StandardFilter(matchVersion, source);
-    // prior to this we get the classic behavior, standardfilter does it for us.
-    if (matchVersion.onOrAfter(Version.LUCENE_31))
-      result = new EnglishPossessiveFilter(matchVersion, result);
+    result = new EnglishPossessiveFilter(matchVersion, result);
     result = new LowerCaseFilter(matchVersion, result);
     result = new StopFilter(matchVersion, result, stopwords);
     if(!stemExclusionSet.isEmpty())
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/EnglishPossessiveFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/EnglishPossessiveFilter.java
index 8d12099..597ac5a 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/EnglishPossessiveFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/EnglishPossessiveFilter.java
@@ -26,30 +26,13 @@ import org.apache.lucene.util.Version;
 
 /**
  * TokenFilter that removes possessives (trailing 's) from words.
- * <a name="version"/>
- * <p>You must specify the required {@link Version}
- * compatibility when creating EnglishPossessiveFilter:
- * <ul>
- *    <li> As of 3.6, U+2019 RIGHT SINGLE QUOTATION MARK and 
- *         U+FF07 FULLWIDTH APOSTROPHE are also treated as
- *         quotation marks.
- * </ul>
  */
 public final class EnglishPossessiveFilter extends TokenFilter {
   private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-  private Version matchVersion;
-
-  /**
-   * @deprecated Use {@link #EnglishPossessiveFilter(Version, TokenStream)} instead.
-   */
-  @Deprecated
-  public EnglishPossessiveFilter(TokenStream input) {
-    this(Version.LUCENE_35, input);
-  }
 
+  // NOTE: version now unused
   public EnglishPossessiveFilter(Version version, TokenStream input) {
     super(input);
-    this.matchVersion = version;
   }
 
   @Override
@@ -63,7 +46,8 @@ public final class EnglishPossessiveFilter extends TokenFilter {
     
     if (bufferLength >= 2 && 
         (buffer[bufferLength-2] == '\'' || 
-         (matchVersion.onOrAfter(Version.LUCENE_36) && (buffer[bufferLength-2] == '\u2019' || buffer[bufferLength-2] == '\uFF07'))) &&
+         buffer[bufferLength-2] == '\u2019' || 
+         buffer[bufferLength-2] == '\uFF07') &&
         (buffer[bufferLength-1] == 's' || buffer[bufferLength-1] == 'S')) {
       termAtt.setLength(bufferLength - 2); // Strip last 2 characters off
     }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/KStemmer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/KStemmer.java
index 3cfd813..9822734 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/KStemmer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/KStemmer.java
@@ -281,9 +281,9 @@ public class KStemmer {
     DictEntry entry;
 
     CharArrayMap<DictEntry> d = new CharArrayMap<DictEntry>(
-        Version.LUCENE_31, 1000, false);
+        Version.LUCENE_50, 1000, false);
     
-    d = new CharArrayMap<DictEntry>(Version.LUCENE_31, 1000, false);
+    d = new CharArrayMap<DictEntry>(Version.LUCENE_50, 1000, false);
     for (int i = 0; i < exceptionWords.length; i++) {
       if (!d.containsKey(exceptionWords[i])) {
         entry = new DictEntry(exceptionWords[i], true);
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/es/SpanishAnalyzer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/es/SpanishAnalyzer.java
index b4eb2e0..3aaf168 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/es/SpanishAnalyzer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/es/SpanishAnalyzer.java
@@ -34,17 +34,9 @@ import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.Version;
-import org.tartarus.snowball.ext.SpanishStemmer;
 
 /**
  * {@link Analyzer} for Spanish.
- * <p>
- * <a name="version"/>
- * <p>You must specify the required {@link Version}
- * compatibility when creating SpanishAnalyzer:
- * <ul>
- *   <li> As of 3.6, SpanishLightStemFilter is used for less aggressive stemming.
- * </ul>
  */
 public final class SpanishAnalyzer extends StopwordAnalyzerBase {
   private final CharArraySet stemExclusionSet;
@@ -132,11 +124,7 @@ public final class SpanishAnalyzer extends StopwordAnalyzerBase {
     result = new StopFilter(matchVersion, result, stopwords);
     if(!stemExclusionSet.isEmpty())
       result = new KeywordMarkerFilter(result, stemExclusionSet);
-    if (matchVersion.onOrAfter(Version.LUCENE_36)) {
-      result = new SpanishLightStemFilter(result);
-    } else {
-      result = new SnowballFilter(result, new SpanishStemmer());
-    }
+    result = new SpanishLightStemFilter(result);
     return new TokenStreamComponents(source, result);
   }
 }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/fa/PersianAnalyzer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/fa/PersianAnalyzer.java
index 2b47f68..4d455bb 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/fa/PersianAnalyzer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/fa/PersianAnalyzer.java
@@ -24,7 +24,6 @@ import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharReader;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.ar.ArabicLetterTokenizer;
 import org.apache.lucene.analysis.ar.ArabicNormalizationFilter;
 import org.apache.lucene.analysis.core.LowerCaseFilter;
 import org.apache.lucene.analysis.core.StopFilter;
@@ -36,7 +35,7 @@ import org.apache.lucene.util.Version;
 /**
  * {@link Analyzer} for Persian.
  * <p>
- * This Analyzer uses {@link ArabicLetterTokenizer} which implies tokenizing around
+ * This Analyzer uses {@link PersianCharFilter} which implies tokenizing around
  * zero-width non-joiner in addition to whitespace. Some persian-specific variant forms (such as farsi
  * yeh and keheh) are standardized. "Stemming" is accomplished via stopwords.
  * </p>
@@ -118,12 +117,7 @@ public final class PersianAnalyzer extends StopwordAnalyzerBase {
   @Override
   protected TokenStreamComponents createComponents(String fieldName,
       Reader reader) {
-    final Tokenizer source;
-    if (matchVersion.onOrAfter(Version.LUCENE_31)) {
-      source = new StandardTokenizer(matchVersion, reader);
-    } else {
-      source = new ArabicLetterTokenizer(matchVersion, reader);
-    }
+    final Tokenizer source = new StandardTokenizer(matchVersion, reader);
     TokenStream result = new LowerCaseFilter(matchVersion, source);
     result = new ArabicNormalizationFilter(result);
     /* additional persian-specific normalization */
@@ -140,8 +134,6 @@ public final class PersianAnalyzer extends StopwordAnalyzerBase {
    */
   @Override
   protected Reader initReader(Reader reader) {
-    return matchVersion.onOrAfter(Version.LUCENE_31) ? 
-       new PersianCharFilter(CharReader.get(reader)) :
-       reader;
+    return new PersianCharFilter(CharReader.get(reader)); 
   }
 }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/fr/FrenchAnalyzer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/fr/FrenchAnalyzer.java
index 1f8b47b..62d4a1f 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/fr/FrenchAnalyzer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/fr/FrenchAnalyzer.java
@@ -35,7 +35,6 @@ import org.apache.lucene.util.Version;
 
 import java.io.IOException;
 import java.io.Reader;
-import java.util.Arrays;
 
 /**
  * {@link Analyzer} for French language. 
@@ -47,53 +46,11 @@ import java.util.Arrays;
  * exclusion list is empty by default.
  * </p>
  *
- * <a name="version"/>
- * <p>You must specify the required {@link Version}
- * compatibility when creating FrenchAnalyzer:
- * <ul>
- *   <li> As of 3.6, FrenchLightStemFilter is used for less aggressive stemming.
- *   <li> As of 3.1, Snowball stemming is done with SnowballFilter, 
- *        LowerCaseFilter is used prior to StopFilter, and ElisionFilter and 
- *        Snowball stopwords are used by default.
- *   <li> As of 2.9, StopFilter preserves position
- *        increments
- * </ul>
- *
  * <p><b>NOTE</b>: This class uses the same {@link Version}
  * dependent settings as {@link StandardAnalyzer}.</p>
  */
 public final class FrenchAnalyzer extends StopwordAnalyzerBase {
 
-  /**
-   * Extended list of typical French stopwords.
-   * @deprecated (3.1) remove in Lucene 5.0 (index bw compat)
-   */
-  @Deprecated
-  private final static String[] FRENCH_STOP_WORDS = {
-    "a", "afin", "ai", "ainsi", "aprÃ¨s", "attendu", "au", "aujourd", "auquel", "aussi",
-    "autre", "autres", "aux", "auxquelles", "auxquels", "avait", "avant", "avec", "avoir",
-    "c", "car", "ce", "ceci", "cela", "celle", "celles", "celui", "cependant", "certain",
-    "certaine", "certaines", "certains", "ces", "cet", "cette", "ceux", "chez", "ci",
-    "combien", "comme", "comment", "concernant", "contre", "d", "dans", "de", "debout",
-    "dedans", "dehors", "del?", "depuis", "derriÃ¨re", "des", "dÃ©sormais", "desquelles",
-    "desquels", "dessous", "dessus", "devant", "devers", "devra", "divers", "diverse",
-    "diverses", "doit", "donc", "dont", "du", "duquel", "durant", "dÃ¨s", "elle", "elles",
-    "en", "entre", "environ", "est", "et", "etc", "etre", "eu", "eux", "exceptÃ©", "hormis",
-    "hors", "hÃ©las", "hui", "il", "ils", "j", "je", "jusqu", "jusque", "l", "la", "laquelle",
-    "le", "lequel", "les", "lesquelles", "lesquels", "leur", "leurs", "lorsque", "lui", "l?",
-    "ma", "mais", "malgrÃ©", "me", "merci", "mes", "mien", "mienne", "miennes", "miens", "moi",
-    "moins", "mon", "moyennant", "mÃªme", "mÃªmes", "n", "ne", "ni", "non", "nos", "notre",
-    "nous", "nÃ©anmoins", "nÃ´tre", "nÃ´tres", "on", "ont", "ou", "outre", "oÃ¹", "par", "parmi",
-    "partant", "pas", "passÃ©", "pendant", "plein", "plus", "plusieurs", "pour", "pourquoi",
-    "proche", "prÃ¨s", "puisque", "qu", "quand", "que", "quel", "quelle", "quelles", "quels",
-    "qui", "quoi", "quoique", "revoici", "revoil?", "s", "sa", "sans", "sauf", "se", "selon",
-    "seront", "ses", "si", "sien", "sienne", "siennes", "siens", "sinon", "soi", "soit",
-    "son", "sont", "sous", "suivant", "sur", "ta", "te", "tes", "tien", "tienne", "tiennes",
-    "tiens", "toi", "ton", "tous", "tout", "toute", "toutes", "tu", "un", "une", "va", "vers",
-    "voici", "voil?", "vos", "votre", "vous", "vu", "vÃ´tre", "vÃ´tres", "y", "?", "Ã§a", "Ã¨s",
-    "Ã©tÃ©", "Ãªtre", "Ã´"
-  };
-
   /** File containing default French stopwords. */
   public final static String DEFAULT_STOPWORD_FILE = "french_stop.txt";
   
@@ -111,11 +68,6 @@ public final class FrenchAnalyzer extends StopwordAnalyzerBase {
   }
   
   private static class DefaultSetHolder {
-    /** @deprecated (3.1) remove this in Lucene 5.0, index bw compat */
-    @Deprecated
-    static final CharArraySet DEFAULT_STOP_SET_30 = CharArraySet
-        .unmodifiableSet(new CharArraySet(Version.LUCENE_CURRENT, Arrays.asList(FRENCH_STOP_WORDS),
-            false));
     static final CharArraySet DEFAULT_STOP_SET;
     static {
       try {
@@ -133,9 +85,7 @@ public final class FrenchAnalyzer extends StopwordAnalyzerBase {
    * Builds an analyzer with the default stop words ({@link #getDefaultStopSet}).
    */
   public FrenchAnalyzer(Version matchVersion) {
-    this(matchVersion,
-        matchVersion.onOrAfter(Version.LUCENE_31) ? DefaultSetHolder.DEFAULT_STOP_SET
-            : DefaultSetHolder.DEFAULT_STOP_SET_30);
+    this(matchVersion, DefaultSetHolder.DEFAULT_STOP_SET);
   }
   
   /**
@@ -182,30 +132,15 @@ public final class FrenchAnalyzer extends StopwordAnalyzerBase {
   @Override
   protected TokenStreamComponents createComponents(String fieldName,
       Reader reader) {
-    if (matchVersion.onOrAfter(Version.LUCENE_31)) {
-      final Tokenizer source = new StandardTokenizer(matchVersion, reader);
-      TokenStream result = new StandardFilter(matchVersion, source);
-      result = new ElisionFilter(matchVersion, result);
-      result = new LowerCaseFilter(matchVersion, result);
-      result = new StopFilter(matchVersion, result, stopwords);
-      if(!excltable.isEmpty())
-        result = new KeywordMarkerFilter(result, excltable);
-      if (matchVersion.onOrAfter(Version.LUCENE_36)) {
-        result = new FrenchLightStemFilter(result);
-      } else {
-        result = new SnowballFilter(result, new org.tartarus.snowball.ext.FrenchStemmer());
-      }
-      return new TokenStreamComponents(source, result);
-    } else {
-      final Tokenizer source = new StandardTokenizer(matchVersion, reader);
-      TokenStream result = new StandardFilter(matchVersion, source);
-      result = new StopFilter(matchVersion, result, stopwords);
-      if(!excltable.isEmpty())
-        result = new KeywordMarkerFilter(result, excltable);
-      result = new FrenchStemFilter(result);
-      // Convert to lowercase after stemming!
-      return new TokenStreamComponents(source, new LowerCaseFilter(matchVersion, result));
-    }
+    final Tokenizer source = new StandardTokenizer(matchVersion, reader);
+    TokenStream result = new StandardFilter(matchVersion, source);
+    result = new ElisionFilter(matchVersion, result);
+    result = new LowerCaseFilter(matchVersion, result);
+    result = new StopFilter(matchVersion, result, stopwords);
+    if(!excltable.isEmpty())
+      result = new KeywordMarkerFilter(result, excltable);
+    result = new FrenchLightStemFilter(result);
+    return new TokenStreamComponents(source, result);
   }
 }
 
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/fr/FrenchStemFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/fr/FrenchStemFilter.java
deleted file mode 100644
index c7cfd6b..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/fr/FrenchStemFilter.java
+++ /dev/null
@@ -1,90 +0,0 @@
-package org.apache.lucene.analysis.fr;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter; // for javadoc
-import org.apache.lucene.analysis.TokenFilter;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.snowball.SnowballFilter;
-import org.apache.lucene.analysis.tokenattributes.KeywordAttribute;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-
-import java.io.IOException;
-
-/**
- * A {@link TokenFilter} that stems french words. 
- * <p>
- * The used stemmer can be changed at runtime after the
- * filter object is created (as long as it is a {@link FrenchStemmer}).
- * </p>
- * <p>
- * To prevent terms from being stemmed use an instance of
- * {@link KeywordMarkerFilter} or a custom {@link TokenFilter} that sets
- * the {@link KeywordAttribute} before this {@link TokenStream}.
- * </p>
- * @see KeywordMarkerFilter
- * @deprecated (3.1) Use {@link SnowballFilter} with 
- * {@link org.tartarus.snowball.ext.FrenchStemmer} instead, which has the
- * same functionality. This filter will be removed in Lucene 5.0
- */
-@Deprecated
-public final class FrenchStemFilter extends TokenFilter {
-
-	/**
-	 * The actual token in the input stream.
-	 */
-	private FrenchStemmer stemmer = new FrenchStemmer();
-	
-	private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-  private final KeywordAttribute keywordAttr = addAttribute(KeywordAttribute.class);
-
-	public FrenchStemFilter( TokenStream in ) {
-    super(in);
-	}
-
-	/**
-	 * @return  Returns true for the next token in the stream, or false at EOS
-	 */
-	@Override
-	public boolean incrementToken() throws IOException {
-	  if (input.incrementToken()) {
-	    String term = termAtt.toString();
-
-	    // Check the exclusion table
-	    if (!keywordAttr.isKeyword()) {
-	      String s = stemmer.stem( term );
-	      // If not stemmed, don't waste the time  adjusting the token.
-	      if ((s != null) && !s.equals( term ) )
-	        termAtt.setEmpty().append(s);
-	    }
-	    return true;
-	  } else {
-	    return false;
-	  }
-	}
-	/**
-	 * Set a alternative/custom {@link FrenchStemmer} for this filter.
-	 */
-	public void setStemmer( FrenchStemmer stemmer ) {
-		if ( stemmer != null ) {
-			this.stemmer = stemmer;
-		}
-	}
-}
-
-
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/fr/FrenchStemmer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/fr/FrenchStemmer.java
deleted file mode 100644
index cf741c7..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/fr/FrenchStemmer.java
+++ /dev/null
@@ -1,712 +0,0 @@
-package org.apache.lucene.analysis.fr;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A stemmer for French words. 
- * <p>
- * The algorithm is based on the work of
- * Dr Martin Porter on his snowball project<br>
- * refer to http://snowball.sourceforge.net/french/stemmer.html<br>
- * (French stemming algorithm) for details
- * </p>
- * @deprecated Use {@link org.tartarus.snowball.ext.FrenchStemmer} instead, 
- * which has the same functionality. This filter will be removed in Lucene 4.0
- */
-@Deprecated
-public class FrenchStemmer {
-
-    /**
-     * Buffer for the terms while stemming them.
-     */
-    private StringBuilder sb = new StringBuilder();
-
-    /**
-     * A temporary buffer, used to reconstruct R2
-     */
-     private StringBuilder tb = new StringBuilder();
-
-	/**
-	 * Region R0 is equal to the whole buffer
-	 */
-	private String R0;
-
-	/**
-	 * Region RV
-	 * "If the word begins with two vowels, RV is the region after the third letter,
-	 * otherwise the region after the first vowel not at the beginning of the word,
-	 * or the end of the word if these positions cannot be found."
-	 */
-    private String RV;
-
-	/**
-	 * Region R1
-	 * "R1 is the region after the first non-vowel following a vowel
-	 * or is the null region at the end of the word if there is no such non-vowel"
-	 */
-    private String R1;
-
-	/**
-	 * Region R2
-	 * "R2 is the region after the first non-vowel in R1 following a vowel
-	 * or is the null region at the end of the word if there is no such non-vowel"
-	 */
-    private String R2;
-
-
-	/**
-	 * Set to true if we need to perform step 2
-	 */
-    private boolean suite;
-
-	/**
-	 * Set to true if the buffer was modified
-	 */
-    private boolean modified;
-
-
-    /**
-     * Stems the given term to a unique <tt>discriminator</tt>.
-     *
-     * @param term  java.langString The term that should be stemmed
-     * @return java.lang.String  Discriminator for <tt>term</tt>
-     */
-    protected String stem( String term ) {
-		if ( !isStemmable( term ) ) {
-			return term;
-		}
-
-		// Use lowercase for medium stemming.
-		term = term.toLowerCase();
-
-		// Reset the StringBuilder.
-		sb.delete( 0, sb.length() );
-		sb.insert( 0, term );
-
-		// reset the booleans
-		modified = false;
-		suite = false;
-
-		sb = treatVowels( sb );
-
-		setStrings();
-
-		step1();
-
-		if (!modified || suite)
-		{
-			if (RV != null)
-			{
-				suite = step2a();
-				if (!suite)
-					step2b();
-			}
-		}
-
-		if (modified || suite)
-			step3();
-		else
-			step4();
-
-		step5();
-
-		step6();
-
-		return sb.toString();
-    }
-
-	/**
-	 * Sets the search region Strings<br>
-	 * it needs to be done each time the buffer was modified
-	 */
-	private void setStrings() {
-		// set the strings
-		R0 = sb.toString();
-		RV = retrieveRV( sb );
-		R1 = retrieveR( sb );
-		if ( R1 != null )
-		{
-			tb.delete( 0, tb.length() );
-			tb.insert( 0, R1 );
-			R2 = retrieveR( tb );
-		}
-		else
-			R2 = null;
-	}
-
-	/**
-	 * First step of the Porter Algorithm<br>
-	 * refer to http://snowball.sourceforge.net/french/stemmer.html for an explanation
-	 */
-	private void step1( ) {
-		String[] suffix = { "ances", "iqUes", "ismes", "ables", "istes", "ance", "iqUe", "isme", "able", "iste" };
-		deleteFrom( R2, suffix );
-
-		replaceFrom( R2, new String[] { "logies", "logie" }, "log" );
-		replaceFrom( R2, new String[] { "usions", "utions", "usion", "ution" }, "u" );
-		replaceFrom( R2, new String[] { "ences", "ence" }, "ent" );
-
-		String[] search = { "atrices", "ateurs", "ations", "atrice", "ateur", "ation"};
-		deleteButSuffixFromElseReplace( R2, search, "ic",  true, R0, "iqU" );
-
-		deleteButSuffixFromElseReplace( R2, new String[] { "ements", "ement" }, "eus", false, R0, "eux" );
-		deleteButSuffixFrom( R2, new String[] { "ements", "ement" }, "ativ", false );
-		deleteButSuffixFrom( R2, new String[] { "ements", "ement" }, "iv", false );
-		deleteButSuffixFrom( R2, new String[] { "ements", "ement" }, "abl", false );
-		deleteButSuffixFrom( R2, new String[] { "ements", "ement" }, "iqU", false );
-
-		deleteFromIfTestVowelBeforeIn( R1, new String[] { "issements", "issement" }, false, R0 );
-		deleteFrom( RV, new String[] { "ements", "ement" } );
-
-		deleteButSuffixFromElseReplace( R2, new String[] { "itÃ©s", "itÃ©" }, "abil", false, R0, "abl" );
-		deleteButSuffixFromElseReplace( R2, new String[] { "itÃ©s", "itÃ©" }, "ic", false, R0, "iqU" );
-		deleteButSuffixFrom( R2, new String[] { "itÃ©s", "itÃ©" }, "iv", true );
-
-		String[] autre = { "ifs", "ives", "if", "ive" };
-		deleteButSuffixFromElseReplace( R2, autre, "icat", false, R0, "iqU" );
-		deleteButSuffixFromElseReplace( R2, autre, "at", true, R2, "iqU" );
-
-		replaceFrom( R0, new String[] { "eaux" }, "eau" );
-
-		replaceFrom( R1, new String[] { "aux" }, "al" );
-
-		deleteButSuffixFromElseReplace( R2, new String[] { "euses", "euse" }, "", true, R1, "eux" );
-
-		deleteFrom( R2, new String[] { "eux" } );
-
-		// if one of the next steps is performed, we will need to perform step2a
-		boolean temp = false;
-		temp = replaceFrom( RV, new String[] { "amment" }, "ant" );
-		if (temp == true)
-			suite = true;
-		temp = replaceFrom( RV, new String[] { "emment" }, "ent" );
-		if (temp == true)
-			suite = true;
-		temp = deleteFromIfTestVowelBeforeIn( RV, new String[] { "ments", "ment" }, true, RV );
-		if (temp == true)
-			suite = true;
-
-	}
-
-	/**
-	 * Second step (A) of the Porter Algorithm<br>
-	 * Will be performed if nothing changed from the first step
-	 * or changed were done in the amment, emment, ments or ment suffixes<br>
-	 * refer to http://snowball.sourceforge.net/french/stemmer.html for an explanation
-	 *
-	 * @return boolean - true if something changed in the StringBuilder
-	 */
-	private boolean step2a() {
-		String[] search = { "Ã®mes", "Ã®tes", "iraIent", "irait", "irais", "irai", "iras", "ira",
-							"irent", "iriez", "irez", "irions", "irons", "iront",
-							"issaIent", "issais", "issantes", "issante", "issants", "issant",
-							"issait", "issais", "issions", "issons", "issiez", "issez", "issent",
-							"isses", "isse", "ir", "is", "Ã®t", "it", "ies", "ie", "i" };
-		return deleteFromIfTestVowelBeforeIn( RV, search, false, RV );
-	}
-
-	/**
-	 * Second step (B) of the Porter Algorithm<br>
-	 * Will be performed if step 2 A was performed unsuccessfully<br>
-	 * refer to http://snowball.sourceforge.net/french/stemmer.html for an explanation
-	 */
-	private void step2b() {
-		String[] suffix = { "eraIent", "erais", "erait", "erai", "eras", "erions", "eriez",
-							"erons", "eront","erez", "Ã¨rent", "era", "Ã©es", "iez",
-							"Ã©e", "Ã©s", "er", "ez", "Ã©" };
-		deleteFrom( RV, suffix );
-
-		String[] search = { "assions", "assiez", "assent", "asses", "asse", "aIent",
-							"antes", "aIent", "Aient", "ante", "Ã¢mes", "Ã¢tes", "ants", "ant",
-							"ait", "aÃ®t", "ais", "Ait", "AÃ®t", "Ais", "Ã¢t", "as", "ai", "Ai", "a" };
-		deleteButSuffixFrom( RV, search, "e", true );
-
-		deleteFrom( R2, new String[] { "ions" } );
-	}
-
-	/**
-	 * Third step of the Porter Algorithm<br>
-	 * refer to http://snowball.sourceforge.net/french/stemmer.html for an explanation
-	 */
-	private void step3() {
-		if (sb.length()>0)
-		{
-			char ch = sb.charAt( sb.length()-1 );
-			if (ch == 'Y')
-			{
-				sb.setCharAt( sb.length()-1, 'i' );
-				setStrings();
-			}
-			else if (ch == 'Ã§')
-			{
-				sb.setCharAt( sb.length()-1, 'c' );
-				setStrings();
-			}
-		}
-	}
-
-	/**
-	 * Fourth step of the Porter Algorithm<br>
-	 * refer to http://snowball.sourceforge.net/french/stemmer.html for an explanation
-	 */
-	private void step4() {
-		if (sb.length() > 1)
-		{
-			char ch = sb.charAt( sb.length()-1 );
-			if (ch == 's')
-			{
-				char b = sb.charAt( sb.length()-2 );
-				if (b != 'a' && b != 'i' && b != 'o' && b != 'u' && b != 'Ã¨' && b != 's')
-				{
-					sb.delete( sb.length() - 1, sb.length());
-					setStrings();
-				}
-			}
-		}
-		boolean found = deleteFromIfPrecededIn( R2, new String[] { "ion" }, RV, "s" );
-		if (!found)
-		found = deleteFromIfPrecededIn( R2, new String[] { "ion" }, RV, "t" );
-
-		replaceFrom( RV, new String[] { "IÃ¨re", "iÃ¨re", "Ier", "ier" }, "i" );
-		deleteFrom( RV, new String[] { "e" } );
-		deleteFromIfPrecededIn( RV, new String[] { "Ã«" }, R0, "gu" );
-	}
-
-	/**
-	 * Fifth step of the Porter Algorithm<br>
-	 * refer to http://snowball.sourceforge.net/french/stemmer.html for an explanation
-	 */
-	private void step5() {
-		if (R0 != null)
-		{
-			if (R0.endsWith("enn") || R0.endsWith("onn") || R0.endsWith("ett") || R0.endsWith("ell") || R0.endsWith("eill"))
-			{
-				sb.delete( sb.length() - 1, sb.length() );
-				setStrings();
-			}
-		}
-	}
-
-	/**
-	 * Sixth (and last!) step of the Porter Algorithm<br>
-	 * refer to http://snowball.sourceforge.net/french/stemmer.html for an explanation
-	 */
-	private void step6() {
-		if (R0!=null && R0.length()>0)
-		{
-			boolean seenVowel = false;
-			boolean seenConson = false;
-			int pos = -1;
-			for (int i = R0.length()-1; i > -1; i--)
-			{
-				char ch = R0.charAt(i);
-				if (isVowel(ch))
-				{
-					if (!seenVowel)
-					{
-						if (ch == 'Ã©' || ch == 'Ã¨')
-						{
-							pos = i;
-							break;
-						}
-					}
-					seenVowel = true;
-				}
-				else
-				{
-					if (seenVowel)
-						break;
-					else
-						seenConson = true;
-				}
-			}
-			if (pos > -1 && seenConson && !seenVowel)
-				sb.setCharAt(pos, 'e');
-		}
-	}
-
-	/**
-	 * Delete a suffix searched in zone "source" if zone "from" contains prefix + search string
-	 *
-	 * @param source java.lang.String - the primary source zone for search
-	 * @param search java.lang.String[] - the strings to search for suppression
-	 * @param from java.lang.String - the secondary source zone for search
-	 * @param prefix java.lang.String - the prefix to add to the search string to test
-	 * @return boolean - true if modified
-	 */
-	private boolean deleteFromIfPrecededIn( String source, String[] search, String from, String prefix ) {
-		boolean found = false;
-		if (source!=null )
-		{
-			for (int i = 0; i < search.length; i++) {
-				if ( source.endsWith( search[i] ))
-				{
-					if (from!=null && from.endsWith( prefix + search[i] ))
-					{
-						sb.delete( sb.length() - search[i].length(), sb.length());
-						found = true;
-						setStrings();
-						break;
-					}
-				}
-			}
-		}
-		return found;
-	}
-
-	/**
-	 * Delete a suffix searched in zone "source" if the preceding letter is (or isn't) a vowel
-	 *
-	 * @param source java.lang.String - the primary source zone for search
-	 * @param search java.lang.String[] - the strings to search for suppression
-	 * @param vowel boolean - true if we need a vowel before the search string
-	 * @param from java.lang.String - the secondary source zone for search (where vowel could be)
-	 * @return boolean - true if modified
-	 */
-	private boolean deleteFromIfTestVowelBeforeIn( String source, String[] search, boolean vowel, String from ) {
-		boolean found = false;
-		if (source!=null && from!=null)
-		{
-			for (int i = 0; i < search.length; i++) {
-				if ( source.endsWith( search[i] ))
-				{
-					if ((search[i].length() + 1) <= from.length())
-					{
-						boolean test = isVowel(sb.charAt(sb.length()-(search[i].length()+1)));
-						if (test == vowel)
-						{
-							sb.delete( sb.length() - search[i].length(), sb.length());
-							modified = true;
-							found = true;
-							setStrings();
-							break;
-						}
-					}
-				}
-			}
-		}
-		return found;
-	}
-
-	/**
-	 * Delete a suffix searched in zone "source" if preceded by the prefix
-	 *
-	 * @param source java.lang.String - the primary source zone for search
-	 * @param search java.lang.String[] - the strings to search for suppression
-	 * @param prefix java.lang.String - the prefix to add to the search string to test
-	 * @param without boolean - true if it will be deleted even without prefix found
-	 */
-	private void deleteButSuffixFrom( String source, String[] search, String prefix, boolean without ) {
-		if (source!=null)
-		{
-			for (int i = 0; i < search.length; i++) {
-				if ( source.endsWith( prefix + search[i] ))
-				{
-					sb.delete( sb.length() - (prefix.length() + search[i].length()), sb.length() );
-					modified = true;
-					setStrings();
-					break;
-				}
-				else if ( without && source.endsWith( search[i] ))
-				{
-					sb.delete( sb.length() - search[i].length(), sb.length() );
-					modified = true;
-					setStrings();
-					break;
-				}
-			}
-		}
-	}
-
-	/**
-	 * Delete a suffix searched in zone "source" if preceded by prefix<br>
-	 * or replace it with the replace string if preceded by the prefix in the zone "from"<br>
-	 * or delete the suffix if specified
-	 *
-	 * @param source java.lang.String - the primary source zone for search
-	 * @param search java.lang.String[] - the strings to search for suppression
-	 * @param prefix java.lang.String - the prefix to add to the search string to test
-	 * @param without boolean - true if it will be deleted even without prefix found
-	 */
-	private void deleteButSuffixFromElseReplace( String source, String[] search, String prefix, boolean without, String from, String replace ) {
-		if (source!=null)
-		{
-			for (int i = 0; i < search.length; i++) {
-				if ( source.endsWith( prefix + search[i] ))
-				{
-					sb.delete( sb.length() - (prefix.length() + search[i].length()), sb.length() );
-					modified = true;
-					setStrings();
-					break;
-				}
-				else if ( from!=null && from.endsWith( prefix + search[i] ))
-				{
-					sb.replace( sb.length() - (prefix.length() + search[i].length()), sb.length(), replace );
-					modified = true;
-					setStrings();
-					break;
-				}
-				else if ( without && source.endsWith( search[i] ))
-				{
-					sb.delete( sb.length() - search[i].length(), sb.length() );
-					modified = true;
-					setStrings();
-					break;
-				}
-			}
-		}
-	}
-
-	/**
-	 * Replace a search string with another within the source zone
-	 *
-	 * @param source java.lang.String - the source zone for search
-	 * @param search java.lang.String[] - the strings to search for replacement
-	 * @param replace java.lang.String - the replacement string
-	 */
-	private boolean replaceFrom( String source, String[] search, String replace ) {
-		boolean found = false;
-		if (source!=null)
-		{
-			for (int i = 0; i < search.length; i++) {
-				if ( source.endsWith( search[i] ))
-				{
-					sb.replace( sb.length() - search[i].length(), sb.length(), replace );
-					modified = true;
-					found = true;
-					setStrings();
-					break;
-				}
-			}
-		}
-		return found;
-	}
-
-	/**
-	 * Delete a search string within the source zone
-	 *
-	 * @param source the source zone for search
-	 * @param suffix the strings to search for suppression
-	 */
-	private void deleteFrom(String source, String[] suffix ) {
-		if (source!=null)
-		{
-			for (int i = 0; i < suffix.length; i++) {
-				if (source.endsWith( suffix[i] ))
-				{
-					sb.delete( sb.length() - suffix[i].length(), sb.length());
-					modified = true;
-					setStrings();
-					break;
-				}
-			}
-		}
-	}
-
-	/**
-	 * Test if a char is a french vowel, including accentuated ones
-	 *
-	 * @param ch the char to test
-	 * @return boolean - true if the char is a vowel
-	 */
-	private boolean isVowel(char ch) {
-		switch (ch)
-		{
-			case 'a':
-			case 'e':
-			case 'i':
-			case 'o':
-			case 'u':
-			case 'y':
-			case 'Ã¢':
-			case '?':
-			case 'Ã«':
-			case 'Ã©':
-			case 'Ãª':
-			case 'Ã¨':
-			case 'Ã¯':
-			case 'Ã®':
-			case 'Ã´':
-			case 'Ã¼':
-			case 'Ã¹':
-			case 'Ã»':
-				return true;
-			default:
-				return false;
-		}
-	}
-
-	/**
-	 * Retrieve the "R zone" (1 or 2 depending on the buffer) and return the corresponding string<br>
-	 * "R is the region after the first non-vowel following a vowel
-	 * or is the null region at the end of the word if there is no such non-vowel"<br>
-	 * @param buffer java.lang.StringBuilder - the in buffer
-	 * @return java.lang.String - the resulting string
-	 */
-	private String retrieveR( StringBuilder buffer ) {
-		int len = buffer.length();
-		int pos = -1;
-		for (int c = 0; c < len; c++) {
-			if (isVowel( buffer.charAt( c )))
-			{
-				pos = c;
-				break;
-			}
-		}
-		if (pos > -1)
-		{
-			int consonne = -1;
-			for (int c = pos; c < len; c++) {
-				if (!isVowel(buffer.charAt( c )))
-				{
-					consonne = c;
-					break;
-				}
-			}
-			if (consonne > -1 && (consonne+1) < len)
-				return buffer.substring( consonne+1, len );
-			else
-				return null;
-		}
-		else
-			return null;
-	}
-
-	/**
-	 * Retrieve the "RV zone" from a buffer an return the corresponding string<br>
-	 * "If the word begins with two vowels, RV is the region after the third letter,
-	 * otherwise the region after the first vowel not at the beginning of the word,
-	 * or the end of the word if these positions cannot be found."<br>
-	 * @param buffer java.lang.StringBuilder - the in buffer
-	 * @return java.lang.String - the resulting string
-	 */
-	private String retrieveRV( StringBuilder buffer ) {
-		int len = buffer.length();
-		if ( buffer.length() > 3)
-		{
-			if ( isVowel(buffer.charAt( 0 )) && isVowel(buffer.charAt( 1 ))) {
-				return buffer.substring(3,len);
-			}
-			else
-			{
-				int pos = 0;
-				for (int c = 1; c < len; c++) {
-					if (isVowel( buffer.charAt( c )))
-					{
-						pos = c;
-						break;
-					}
-				}
-				if ( pos+1 < len )
-					return buffer.substring( pos+1, len );
-				else
-					return null;
-			}
-		}
-		else
-			return null;
-	}
-
-
-
-    /**
-	 * Turns u and i preceded AND followed by a vowel to UpperCase<br>
-	 * Turns y preceded OR followed by a vowel to UpperCase<br>
-	 * Turns u preceded by q to UpperCase<br>
-     *
-     * @param buffer java.util.StringBuilder - the buffer to treat
-     * @return java.util.StringBuilder - the treated buffer
-     */
-    private StringBuilder treatVowels( StringBuilder buffer ) {
-		for ( int c = 0; c < buffer.length(); c++ ) {
-			char ch = buffer.charAt( c );
-
-			if (c == 0) // first char
-			{
-				if (buffer.length()>1)
-				{
-					if (ch == 'y' && isVowel(buffer.charAt( c + 1 )))
-						buffer.setCharAt( c, 'Y' );
-				}
-			}
-			else if (c == buffer.length()-1) // last char
-			{
-				if (ch == 'u' && buffer.charAt( c - 1 ) == 'q')
-					buffer.setCharAt( c, 'U' );
-				if (ch == 'y' && isVowel(buffer.charAt( c - 1 )))
-					buffer.setCharAt( c, 'Y' );
-			}
-			else // other cases
-			{
-				if (ch == 'u')
-				{
-					if (buffer.charAt( c - 1) == 'q')
-						buffer.setCharAt( c, 'U' );
-					else if (isVowel(buffer.charAt( c - 1 )) && isVowel(buffer.charAt( c + 1 )))
-						buffer.setCharAt( c, 'U' );
-				}
-				if (ch == 'i')
-				{
-					if (isVowel(buffer.charAt( c - 1 )) && isVowel(buffer.charAt( c + 1 )))
-						buffer.setCharAt( c, 'I' );
-				}
-				if (ch == 'y')
-				{
-					if (isVowel(buffer.charAt( c - 1 )) || isVowel(buffer.charAt( c + 1 )))
-						buffer.setCharAt( c, 'Y' );
-				}
-			}
-		}
-
-		return buffer;
-    }
-
-    /**
-     * Checks a term if it can be processed correctly.
-     *
-     * @return boolean - true if, and only if, the given term consists in letters.
-     */
-    private boolean isStemmable( String term ) {
-		boolean upper = false;
-		int first = -1;
-		for ( int c = 0; c < term.length(); c++ ) {
-			// Discard terms that contain non-letter characters.
-			if ( !Character.isLetter( term.charAt( c ) ) ) {
-				return false;
-			}
-			// Discard terms that contain multiple uppercase letters.
-			if ( Character.isUpperCase( term.charAt( c ) ) ) {
-				if ( upper ) {
-					return false;
-				}
-			// First encountered uppercase letter, set flag and save
-			// position.
-				else {
-					first = c;
-					upper = true;
-				}
-			}
-		}
-		// Discard the term if it contains a single uppercase letter that
-		// is not starting the term.
-		if ( first > 0 ) {
-			return false;
-		}
-		return true;
-    }
-}
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/hi/HindiAnalyzer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/hi/HindiAnalyzer.java
index a4d616c..a7c1c11 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/hi/HindiAnalyzer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/hi/HindiAnalyzer.java
@@ -29,18 +29,10 @@ import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.core.LowerCaseFilter;
 import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.in.IndicNormalizationFilter;
-import org.apache.lucene.analysis.in.IndicTokenizer;
 import org.apache.lucene.util.Version;
 
 /**
  * Analyzer for Hindi.
- * <p>
- * <a name="version"/>
- * <p>You must specify the required {@link Version}
- * compatibility when creating HindiAnalyzer:
- * <ul>
- *   <li> As of 3.6, StandardTokenizer is used for tokenization
- * </ul>
  */
 public final class HindiAnalyzer extends StopwordAnalyzerBase {
   private final CharArraySet stemExclusionSet;
@@ -126,12 +118,7 @@ public final class HindiAnalyzer extends StopwordAnalyzerBase {
   @Override
   protected TokenStreamComponents createComponents(String fieldName,
       Reader reader) {
-    final Tokenizer source;
-    if (matchVersion.onOrAfter(Version.LUCENE_36)) {
-      source = new StandardTokenizer(matchVersion, reader);
-    } else {
-      source = new IndicTokenizer(matchVersion, reader);
-    }
+    final Tokenizer source = new StandardTokenizer(matchVersion, reader);
     TokenStream result = new LowerCaseFilter(matchVersion, source);
     if (!stemExclusionSet.isEmpty())
       result = new KeywordMarkerFilter(result, stemExclusionSet);
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/in/IndicTokenizer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/in/IndicTokenizer.java
deleted file mode 100644
index e6ae4e7..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/in/IndicTokenizer.java
+++ /dev/null
@@ -1,53 +0,0 @@
-package org.apache.lucene.analysis.in;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Reader;
-
-import org.apache.lucene.analysis.util.CharTokenizer;
-import org.apache.lucene.analysis.standard.StandardTokenizer; // javadocs
-import org.apache.lucene.util.AttributeSource;
-import org.apache.lucene.util.Version;
-
-/**
- * Simple Tokenizer for text in Indian Languages.
- * @deprecated (3.6) Use {@link StandardTokenizer} instead.
- */
-@Deprecated
-public final class IndicTokenizer extends CharTokenizer {
- 
-  public IndicTokenizer(Version matchVersion, AttributeFactory factory, Reader input) {
-    super(matchVersion, factory, input);
-  }
-
-  public IndicTokenizer(Version matchVersion, AttributeSource source, Reader input) {
-    super(matchVersion, source, input);
-  }
-
-  public IndicTokenizer(Version matchVersion, Reader input) {
-    super(matchVersion, input);
-  }
-
-  @Override
-  protected boolean isTokenChar(int c) {
-    return Character.isLetter(c)
-    || Character.getType(c) == Character.NON_SPACING_MARK
-    || Character.getType(c) == Character.FORMAT
-    || Character.getType(c) == Character.COMBINING_SPACING_MARK;
-  }
-}
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/it/ItalianAnalyzer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/it/ItalianAnalyzer.java
index f5d2ef9..e448873 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/it/ItalianAnalyzer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/it/ItalianAnalyzer.java
@@ -36,19 +36,9 @@ import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.Version;
-import org.tartarus.snowball.ext.ItalianStemmer;
 
 /**
  * {@link Analyzer} for Italian.
- * <p>
- * <a name="version"/>
- * <p>You must specify the required {@link Version}
- * compatibility when creating ItalianAnalyzer:
- * <ul>
- *   <li> As of 3.6, ItalianLightStemFilter is used for less aggressive stemming.
- *   <li> As of 3.2, ElisionFilter with a set of Italian 
- *        contractions is used by default.
- * </ul>
  */
 public final class ItalianAnalyzer extends StopwordAnalyzerBase {
   private final CharArraySet stemExclusionSet;
@@ -139,18 +129,12 @@ public final class ItalianAnalyzer extends StopwordAnalyzerBase {
       Reader reader) {
     final Tokenizer source = new StandardTokenizer(matchVersion, reader);
     TokenStream result = new StandardFilter(matchVersion, source);
-    if (matchVersion.onOrAfter(Version.LUCENE_32)) {
-      result = new ElisionFilter(matchVersion, result, DEFAULT_ARTICLES);
-    }
+    result = new ElisionFilter(matchVersion, result, DEFAULT_ARTICLES);
     result = new LowerCaseFilter(matchVersion, result);
     result = new StopFilter(matchVersion, result, stopwords);
     if(!stemExclusionSet.isEmpty())
       result = new KeywordMarkerFilter(result, stemExclusionSet);
-    if (matchVersion.onOrAfter(Version.LUCENE_36)) {
-      result = new ItalianLightStemFilter(result);
-    } else {
-      result = new SnowballFilter(result, new ItalianStemmer());
-    }
+    result = new ItalianLightStemFilter(result);
     return new TokenStreamComponents(source, result);
   }
 }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/PatternAnalyzer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/PatternAnalyzer.java
deleted file mode 100644
index a3d2a11..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/PatternAnalyzer.java
+++ /dev/null
@@ -1,518 +0,0 @@
-package org.apache.lucene.analysis.miscellaneous;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.Reader;
-import java.io.StringReader;
-import java.util.Arrays;
-import java.util.Locale;
-import java.util.regex.Matcher;
-import java.util.regex.Pattern;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.core.StopAnalyzer;
-import org.apache.lucene.analysis.core.StopFilter;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.util.CharArraySet;
-import org.apache.lucene.util.Version;
-
-/**
- * Efficient Lucene analyzer/tokenizer that preferably operates on a String rather than a
- * {@link java.io.Reader}, that can flexibly separate text into terms via a regular expression {@link Pattern}
- * (with behaviour identical to {@link String#split(String)}),
- * and that combines the functionality of
- * {@link org.apache.lucene.analysis.core.LetterTokenizer},
- * {@link org.apache.lucene.analysis.core.LowerCaseTokenizer},
- * {@link org.apache.lucene.analysis.core.WhitespaceTokenizer},
- * {@link org.apache.lucene.analysis.core.StopFilter} into a single efficient
- * multi-purpose class.
- * <p>
- * If you are unsure how exactly a regular expression should look like, consider 
- * prototyping by simply trying various expressions on some test texts via
- * {@link String#split(String)}. Once you are satisfied, give that regex to 
- * PatternAnalyzer. Also see <a target="_blank" 
- * href="http://java.sun.com/docs/books/tutorial/extra/regex/">Java Regular Expression Tutorial</a>.
- * <p>
- * This class can be considerably faster than the "normal" Lucene tokenizers. 
- * It can also serve as a building block in a compound Lucene
- * {@link org.apache.lucene.analysis.TokenFilter} chain. For example as in this 
- * stemming example:
- * <pre>
- * PatternAnalyzer pat = ...
- * TokenStream tokenStream = new SnowballFilter(
- *     pat.tokenStream("content", "James is running round in the woods"), 
- *     "English"));
- * </pre>
- * @deprecated (4.0) use the pattern-based analysis in the analysis/pattern package instead.
- */
-@Deprecated
-public final class PatternAnalyzer extends Analyzer {
-  
-  /** <code>"\\W+"</code>; Divides text at non-letters (NOT Character.isLetter(c)) */
-  public static final Pattern NON_WORD_PATTERN = Pattern.compile("\\W+");
-  
-  /** <code>"\\s+"</code>; Divides text at whitespaces (Character.isWhitespace(c)) */
-  public static final Pattern WHITESPACE_PATTERN = Pattern.compile("\\s+");
-  
-  private static final CharArraySet EXTENDED_ENGLISH_STOP_WORDS =
-    CharArraySet.unmodifiableSet(new CharArraySet(Version.LUCENE_CURRENT, 
-        Arrays.asList(
-      "a", "about", "above", "across", "adj", "after", "afterwards",
-      "again", "against", "albeit", "all", "almost", "alone", "along",
-      "already", "also", "although", "always", "among", "amongst", "an",
-      "and", "another", "any", "anyhow", "anyone", "anything",
-      "anywhere", "are", "around", "as", "at", "be", "became", "because",
-      "become", "becomes", "becoming", "been", "before", "beforehand",
-      "behind", "being", "below", "beside", "besides", "between",
-      "beyond", "both", "but", "by", "can", "cannot", "co", "could",
-      "down", "during", "each", "eg", "either", "else", "elsewhere",
-      "enough", "etc", "even", "ever", "every", "everyone", "everything",
-      "everywhere", "except", "few", "first", "for", "former",
-      "formerly", "from", "further", "had", "has", "have", "he", "hence",
-      "her", "here", "hereafter", "hereby", "herein", "hereupon", "hers",
-      "herself", "him", "himself", "his", "how", "however", "i", "ie", "if",
-      "in", "inc", "indeed", "into", "is", "it", "its", "itself", "last",
-      "latter", "latterly", "least", "less", "ltd", "many", "may", "me",
-      "meanwhile", "might", "more", "moreover", "most", "mostly", "much",
-      "must", "my", "myself", "namely", "neither", "never",
-      "nevertheless", "next", "no", "nobody", "none", "noone", "nor",
-      "not", "nothing", "now", "nowhere", "of", "off", "often", "on",
-      "once one", "only", "onto", "or", "other", "others", "otherwise",
-      "our", "ours", "ourselves", "out", "over", "own", "per", "perhaps",
-      "rather", "s", "same", "seem", "seemed", "seeming", "seems",
-      "several", "she", "should", "since", "so", "some", "somehow",
-      "someone", "something", "sometime", "sometimes", "somewhere",
-      "still", "such", "t", "than", "that", "the", "their", "them",
-      "themselves", "then", "thence", "there", "thereafter", "thereby",
-      "therefor", "therein", "thereupon", "these", "they", "this",
-      "those", "though", "through", "throughout", "thru", "thus", "to",
-      "together", "too", "toward", "towards", "under", "until", "up",
-      "upon", "us", "very", "via", "was", "we", "well", "were", "what",
-      "whatever", "whatsoever", "when", "whence", "whenever",
-      "whensoever", "where", "whereafter", "whereas", "whereat",
-      "whereby", "wherefrom", "wherein", "whereinto", "whereof",
-      "whereon", "whereto", "whereunto", "whereupon", "wherever",
-      "wherewith", "whether", "which", "whichever", "whichsoever",
-      "while", "whilst", "whither", "who", "whoever", "whole", "whom",
-      "whomever", "whomsoever", "whose", "whosoever", "why", "will",
-      "with", "within", "without", "would", "xsubj", "xcal", "xauthor",
-      "xother ", "xnote", "yet", "you", "your", "yours", "yourself",
-      "yourselves"
-    ), true));
-    
-  /**
-   * A lower-casing word analyzer with English stop words (can be shared
-   * freely across threads without harm); global per class loader.
-   */
-  public static final PatternAnalyzer DEFAULT_ANALYZER = new PatternAnalyzer(
-    Version.LUCENE_CURRENT, NON_WORD_PATTERN, true, StopAnalyzer.ENGLISH_STOP_WORDS_SET);
-    
-  /**
-   * A lower-casing word analyzer with <b>extended </b> English stop words
-   * (can be shared freely across threads without harm); global per class
-   * loader. The stop words are borrowed from
-   * http://thomas.loc.gov/home/stopwords.html, see
-   * http://thomas.loc.gov/home/all.about.inquery.html
-   */
-  public static final PatternAnalyzer EXTENDED_ANALYZER = new PatternAnalyzer(
-    Version.LUCENE_CURRENT, NON_WORD_PATTERN, true, EXTENDED_ENGLISH_STOP_WORDS);
-    
-  private final Pattern pattern;
-  private final boolean toLowerCase;
-  private final CharArraySet stopWords;
-
-  private final Version matchVersion;
-  
-  /**
-   * Constructs a new instance with the given parameters.
-   * 
-   * @param matchVersion currently does nothing
-   * @param pattern
-   *            a regular expression delimiting tokens
-   * @param toLowerCase
-   *            if <code>true</code> returns tokens after applying
-   *            String.toLowerCase()
-   * @param stopWords
-   *            if non-null, ignores all tokens that are contained in the
-   *            given stop set (after previously having applied toLowerCase()
-   *            if applicable). For example, created via
-   *            {@link StopFilter#makeStopSet(Version, String[])}and/or
-   *            {@link org.apache.lucene.analysis.util.WordlistLoader}as in
-   *            <code>WordlistLoader.getWordSet(new File("samples/fulltext/stopwords.txt")</code>
-   *            or <a href="http://www.unine.ch/info/clef/">other stop words
-   *            lists </a>.
-   */
-  public PatternAnalyzer(Version matchVersion, Pattern pattern, boolean toLowerCase, CharArraySet stopWords) {
-    if (pattern == null) 
-      throw new IllegalArgumentException("pattern must not be null");
-    
-    if (eqPattern(NON_WORD_PATTERN, pattern)) pattern = NON_WORD_PATTERN;
-    else if (eqPattern(WHITESPACE_PATTERN, pattern)) pattern = WHITESPACE_PATTERN;
-    
-    if (stopWords != null && stopWords.size() == 0) stopWords = null;
-    
-    this.pattern = pattern;
-    this.toLowerCase = toLowerCase;
-    this.stopWords = stopWords;
-    this.matchVersion = matchVersion;
-  }
-  
-  /**
-   * Creates a token stream that tokenizes the given string into token terms
-   * (aka words).
-   * 
-   * @param fieldName
-   *            the name of the field to tokenize (currently ignored).
-   * @param reader
-   *            reader (e.g. charfilter) of the original text. can be null.
-   * @param text
-   *            the string to tokenize
-   * @return a new token stream
-   */
-  public TokenStreamComponents createComponents(String fieldName, Reader reader, String text) {
-    // Ideally the Analyzer superclass should have a method with the same signature, 
-    // with a default impl that simply delegates to the StringReader flavour. 
-    if (text == null) 
-      throw new IllegalArgumentException("text must not be null");
-    
-    if (pattern == NON_WORD_PATTERN) { // fast path
-      return new TokenStreamComponents(new FastStringTokenizer(reader, text, true, toLowerCase, stopWords));
-    } else if (pattern == WHITESPACE_PATTERN) { // fast path
-      return new TokenStreamComponents(new FastStringTokenizer(reader, text, false, toLowerCase, stopWords));
-    }
-
-    Tokenizer tokenizer = new PatternTokenizer(reader, text, pattern, toLowerCase);
-    TokenStream result = (stopWords != null) ? new StopFilter(matchVersion, tokenizer, stopWords) : tokenizer;
-    return new TokenStreamComponents(tokenizer, result);
-  }
-  
-  /**
-   * Creates a token stream that tokenizes all the text in the given Reader;
-   * This implementation forwards to <code>tokenStream(String, Reader, String)</code> and is
-   * less efficient than <code>tokenStream(String, Reader, String)</code>.
-   * 
-   * @param fieldName
-   *            the name of the field to tokenize (currently ignored).
-   * @param reader
-   *            the reader delivering the text
-   * @return a new token stream
-   */
-  @Override
-  public TokenStreamComponents createComponents(String fieldName, Reader reader) {
-    try {
-      String text = toString(reader);
-      return createComponents(fieldName, reader, text);
-    } catch (IOException e) {
-      throw new RuntimeException(e);
-    }
-  }
-  
-  /**
-   * Indicates whether some other object is "equal to" this one.
-   * 
-   * @param other
-   *            the reference object with which to compare.
-   * @return true if equal, false otherwise
-   */
-  @Override
-  public boolean equals(Object other) {
-    if (this  == other) return true;
-    if (this  == DEFAULT_ANALYZER && other == EXTENDED_ANALYZER) return false;
-    if (other == DEFAULT_ANALYZER && this  == EXTENDED_ANALYZER) return false;
-    
-    if (other instanceof PatternAnalyzer) {
-      PatternAnalyzer p2 = (PatternAnalyzer) other;
-      return 
-        toLowerCase == p2.toLowerCase &&
-        eqPattern(pattern, p2.pattern) &&
-        eq(stopWords, p2.stopWords);
-    }
-    return false;
-  }
-  
-  /**
-   * Returns a hash code value for the object.
-   * 
-   * @return the hash code.
-   */
-  @Override
-  public int hashCode() {
-    if (this == DEFAULT_ANALYZER) return -1218418418; // fast path
-    if (this == EXTENDED_ANALYZER) return 1303507063; // fast path
-    
-    int h = 1;
-    h = 31*h + pattern.pattern().hashCode();
-    h = 31*h + pattern.flags();
-    h = 31*h + (toLowerCase ? 1231 : 1237);
-    h = 31*h + (stopWords != null ? stopWords.hashCode() : 0);
-    return h;
-  }
-  
-  /** equality where o1 and/or o2 can be null */
-  private static boolean eq(Object o1, Object o2) {
-    return (o1 == o2) || (o1 != null ? o1.equals(o2) : false);
-  }
-  
-  /** assumes p1 and p2 are not null */
-  private static boolean eqPattern(Pattern p1, Pattern p2) {
-    return p1 == p2 || (p1.flags() == p2.flags() && p1.pattern().equals(p2.pattern()));
-  }
-    
-  /**
-   * Reads until end-of-stream and returns all read chars, finally closes the stream.
-   * 
-   * @param input the input stream
-   * @throws IOException if an I/O error occurs while reading the stream
-   */
-  private static String toString(Reader input) throws IOException {
-    if (input instanceof FastStringReader) { // fast path
-      return ((FastStringReader) input).getString();
-    }
-
-    try {
-      int len = 256;
-      char[] buffer = new char[len];
-      char[] output = new char[len];
-      
-      len = 0;
-      int n;
-      while ((n = input.read(buffer)) >= 0) {
-        if (len + n > output.length) { // grow capacity
-          char[] tmp = new char[Math.max(output.length << 1, len + n)];
-          System.arraycopy(output, 0, tmp, 0, len);
-          System.arraycopy(buffer, 0, tmp, len, n);
-          buffer = output; // use larger buffer for future larger bulk reads
-          output = tmp;
-        } else {
-          System.arraycopy(buffer, 0, output, len, n);
-        }
-        len += n;
-      }
-
-      return new String(output, 0, len);
-    } finally {
-      input.close();
-    }
-  }
-  
-  
-  ///////////////////////////////////////////////////////////////////////////////
-  // Nested classes:
-  ///////////////////////////////////////////////////////////////////////////////
-  /**
-   * The work horse; performance isn't fantastic, but it's not nearly as bad
-   * as one might think - kudos to the Sun regex developers.
-   */
-  private static final class PatternTokenizer extends Tokenizer {
-
-    private final Pattern pattern;
-    private String str;
-    private final boolean toLowerCase;
-    private Matcher matcher;
-    private int pos = 0;
-    private static final Locale locale = Locale.getDefault();
-    private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-    private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
-    
-    public PatternTokenizer(Reader input, String str, Pattern pattern, boolean toLowerCase) {
-      super(input);
-      this.pattern = pattern;
-      this.str = str;
-      this.matcher = pattern.matcher(str);
-      this.toLowerCase = toLowerCase;
-    }
-
-    @Override
-    public final boolean incrementToken() {
-      if (matcher == null) return false;
-      clearAttributes();
-      while (true) { // loop takes care of leading and trailing boundary cases
-        int start = pos;
-        int end;
-        boolean isMatch = matcher.find();
-        if (isMatch) {
-          end = matcher.start();
-          pos = matcher.end();
-        } else { 
-          end = str.length();
-          matcher = null; // we're finished
-        }
-        
-        if (start != end) { // non-empty match (header/trailer)
-          String text = str.substring(start, end);
-          if (toLowerCase) text = text.toLowerCase(locale);
-          termAtt.setEmpty().append(text);
-          offsetAtt.setOffset(correctOffset(start), correctOffset(end));
-          return true;
-        }
-        if (!isMatch) return false;
-      }
-    }
-    
-    @Override
-    public final void end() {
-      // set final offset
-      final int finalOffset = correctOffset(str.length());
-    	this.offsetAtt.setOffset(finalOffset, finalOffset);
-    }
-
-    @Override
-    public void reset(Reader input) throws IOException {
-      super.reset(input);
-      this.str = PatternAnalyzer.toString(input);
-      this.matcher = pattern.matcher(this.str);
-    }
-
-    @Override
-    public void reset() throws IOException {
-      super.reset();
-      this.pos = 0;
-    }
-  }
-  
-  
-  ///////////////////////////////////////////////////////////////////////////////
-  // Nested classes:
-  ///////////////////////////////////////////////////////////////////////////////
-  /**
-   * Special-case class for best performance in common cases; this class is
-   * otherwise unnecessary.
-   */
-  private static final class FastStringTokenizer extends Tokenizer {
-    
-    private String str;
-    private int pos;
-    private final boolean isLetter;
-    private final boolean toLowerCase;
-    private final CharArraySet stopWords;
-    private static final Locale locale = Locale.getDefault();
-    private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-    private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
-    
-    public FastStringTokenizer(Reader input, String str, boolean isLetter, boolean toLowerCase, CharArraySet stopWords) {
-      super(input);
-      this.str = str;
-      this.isLetter = isLetter;
-      this.toLowerCase = toLowerCase;
-      this.stopWords = stopWords;
-    }
-
-    @Override
-    public boolean incrementToken() {
-      clearAttributes();
-      // cache loop instance vars (performance)
-      String s = str;
-      int len = s.length();
-      int i = pos;
-      boolean letter = isLetter;
-      
-      int start = 0;
-      String text;
-      do {
-        // find beginning of token
-        text = null;
-        while (i < len && !isTokenChar(s.charAt(i), letter)) {
-          i++;
-        }
-        
-        if (i < len) { // found beginning; now find end of token
-          start = i;
-          while (i < len && isTokenChar(s.charAt(i), letter)) {
-            i++;
-          }
-          
-          text = s.substring(start, i);
-          if (toLowerCase) text = text.toLowerCase(locale);
-//          if (toLowerCase) {            
-////            use next line once JDK 1.5 String.toLowerCase() performance regression is fixed
-////            see http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6265809
-//            text = s.substring(start, i).toLowerCase(); 
-////            char[] chars = new char[i-start];
-////            for (int j=start; j < i; j++) chars[j-start] = Character.toLowerCase(s.charAt(j));
-////            text = new String(chars);
-//          } else {
-//            text = s.substring(start, i);
-//          }
-        }
-      } while (text != null && isStopWord(text));
-      
-      pos = i;
-      if (text == null)
-      {
-        return false;
-      }
-      termAtt.setEmpty().append(text);
-      offsetAtt.setOffset(correctOffset(start), correctOffset(i));
-      return true;
-    }
-    
-    @Override
-    public final void end() {
-      // set final offset
-      final int finalOffset = str.length();
-      this.offsetAtt.setOffset(correctOffset(finalOffset), correctOffset(finalOffset));
-    }    
-    
-    private boolean isTokenChar(char c, boolean isLetter) {
-      return isLetter ? Character.isLetter(c) : !Character.isWhitespace(c);
-    }
-    
-    private boolean isStopWord(String text) {
-      return stopWords != null && stopWords.contains(text);
-    }
-
-    @Override
-    public void reset(Reader input) throws IOException {
-      super.reset(input);
-      this.str = PatternAnalyzer.toString(input);
-    }
-
-    @Override
-    public void reset() throws IOException {
-      super.reset();
-      this.pos = 0;
-    }
-  }
-
-  
-  ///////////////////////////////////////////////////////////////////////////////
-  // Nested classes:
-  ///////////////////////////////////////////////////////////////////////////////
-  /**
-   * A StringReader that exposes it's contained string for fast direct access.
-   * Might make sense to generalize this to CharSequence and make it public?
-   */
-  static final class FastStringReader extends StringReader {
-
-    private final String s;
-    
-    FastStringReader(String s) {
-      super(s);
-      this.s = s;
-    }
-    
-    String getString() {
-      return s;
-    }
-  }
-  
-}
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/RemoveDuplicatesTokenFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/RemoveDuplicatesTokenFilter.java
index cdaba5b..e906c27 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/RemoveDuplicatesTokenFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/RemoveDuplicatesTokenFilter.java
@@ -35,7 +35,7 @@ public final class RemoveDuplicatesTokenFilter extends TokenFilter {
   private final PositionIncrementAttribute posIncAttribute =  addAttribute(PositionIncrementAttribute.class);
   
   // use a fixed version, as we don't care about case sensitivity.
-  private final CharArraySet previous = new CharArraySet(Version.LUCENE_31, 8, false);
+  private final CharArraySet previous = new CharArraySet(Version.LUCENE_50, 8, false);
 
   /**
    * Creates a new RemoveDuplicatesTokenFilter
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/nl/DutchAnalyzer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/nl/DutchAnalyzer.java
index 1780c54..e0d252a 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/nl/DutchAnalyzer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/nl/DutchAnalyzer.java
@@ -47,20 +47,6 @@ import java.io.Reader;
  * A default set of stopwords is used unless an alternative list is specified, but the
  * exclusion list is empty by default.
  * </p>
- *
- * <a name="version"/>
- * <p>You must specify the required {@link Version}
- * compatibility when creating DutchAnalyzer:
- * <ul>
- *   <li> As of 3.6, {@link #DutchAnalyzer(Version, CharArraySet)} and
- *        {@link #DutchAnalyzer(Version, CharArraySet, CharArraySet)} also populate
- *        the default entries for the stem override dictionary
- *   <li> As of 3.1, Snowball stemming is done with SnowballFilter, 
- *        LowerCaseFilter is used prior to StopFilter, and Snowball 
- *        stopwords are used by default.
- *   <li> As of 2.9, StopFilter preserves position
- *        increments
- * </ul>
  * 
  * <p><b>NOTE</b>: This class uses the same {@link Version}
  * dependent settings as {@link StandardAnalyzer}.</p>
@@ -119,26 +105,15 @@ public final class DutchAnalyzer extends Analyzer {
    * 
    */
   public DutchAnalyzer(Version matchVersion) {
-    // historically, only this ctor populated the stem dict!!!!!
     this(matchVersion, DefaultSetHolder.DEFAULT_STOP_SET, CharArraySet.EMPTY_SET, DefaultSetHolder.DEFAULT_STEM_DICT);
   }
   
   public DutchAnalyzer(Version matchVersion, CharArraySet stopwords){
-    // historically, this ctor never the stem dict!!!!!
-    // so we populate it only for >= 3.6
-    this(matchVersion, stopwords, CharArraySet.EMPTY_SET, 
-        matchVersion.onOrAfter(Version.LUCENE_36) 
-        ? DefaultSetHolder.DEFAULT_STEM_DICT 
-        : CharArrayMap.<String>emptyMap());
+    this(matchVersion, stopwords, CharArraySet.EMPTY_SET, DefaultSetHolder.DEFAULT_STEM_DICT);
   }
   
   public DutchAnalyzer(Version matchVersion, CharArraySet stopwords, CharArraySet stemExclusionTable){
-    // historically, this ctor never the stem dict!!!!!
-    // so we populate it only for >= 3.6
-    this(matchVersion, stopwords, stemExclusionTable,
-        matchVersion.onOrAfter(Version.LUCENE_36)
-        ? DefaultSetHolder.DEFAULT_STEM_DICT
-        : CharArrayMap.<String>emptyMap());
+    this(matchVersion, stopwords, stemExclusionTable, DefaultSetHolder.DEFAULT_STEM_DICT);
   }
   
   public DutchAnalyzer(Version matchVersion, CharArraySet stopwords, CharArraySet stemExclusionTable, CharArrayMap<String> stemOverrideDict) {
@@ -160,25 +135,15 @@ public final class DutchAnalyzer extends Analyzer {
   @Override
   protected TokenStreamComponents createComponents(String fieldName,
       Reader aReader) {
-    if (matchVersion.onOrAfter(Version.LUCENE_31)) {
-      final Tokenizer source = new StandardTokenizer(matchVersion, aReader);
-      TokenStream result = new StandardFilter(matchVersion, source);
-      result = new LowerCaseFilter(matchVersion, result);
-      result = new StopFilter(matchVersion, result, stoptable);
-      if (!excltable.isEmpty())
-        result = new KeywordMarkerFilter(result, excltable);
-      if (!stemdict.isEmpty())
-        result = new StemmerOverrideFilter(matchVersion, result, stemdict);
-      result = new SnowballFilter(result, new org.tartarus.snowball.ext.DutchStemmer());
-      return new TokenStreamComponents(source, result);
-    } else {
-      final Tokenizer source = new StandardTokenizer(matchVersion, aReader);
-      TokenStream result = new StandardFilter(matchVersion, source);
-      result = new StopFilter(matchVersion, result, stoptable);
-      if (!excltable.isEmpty())
-        result = new KeywordMarkerFilter(result, excltable);
-      result = new DutchStemFilter(result, stemdict);
-      return new TokenStreamComponents(source, result);
-    }
+    final Tokenizer source = new StandardTokenizer(matchVersion, aReader);
+    TokenStream result = new StandardFilter(matchVersion, source);
+    result = new LowerCaseFilter(matchVersion, result);
+    result = new StopFilter(matchVersion, result, stoptable);
+    if (!excltable.isEmpty())
+      result = new KeywordMarkerFilter(result, excltable);
+    if (!stemdict.isEmpty())
+      result = new StemmerOverrideFilter(matchVersion, result, stemdict);
+    result = new SnowballFilter(result, new org.tartarus.snowball.ext.DutchStemmer());
+    return new TokenStreamComponents(source, result);
   }
 }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/nl/DutchStemFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/nl/DutchStemFilter.java
deleted file mode 100644
index 252ce9e..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/nl/DutchStemFilter.java
+++ /dev/null
@@ -1,108 +0,0 @@
-package org.apache.lucene.analysis.nl;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter; // for javadoc
-import org.apache.lucene.analysis.TokenFilter;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.snowball.SnowballFilter;
-import org.apache.lucene.analysis.tokenattributes.KeywordAttribute;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-
-/**
- * A {@link TokenFilter} that stems Dutch words. 
- * <p>
- * It supports a table of words that should
- * not be stemmed at all. The stemmer used can be changed at runtime after the
- * filter object is created (as long as it is a {@link DutchStemmer}).
- * </p>
- * <p>
- * To prevent terms from being stemmed use an instance of
- * {@link KeywordMarkerFilter} or a custom {@link TokenFilter} that sets
- * the {@link KeywordAttribute} before this {@link TokenStream}.
- * </p>
- * @see KeywordMarkerFilter
- * @deprecated (3.1) Use {@link SnowballFilter} with 
- * {@link org.tartarus.snowball.ext.DutchStemmer} instead, which has the
- * same functionality. This filter will be removed in Lucene 5.0
- */
-@Deprecated
-public final class DutchStemFilter extends TokenFilter {
-  /**
-   * The actual token in the input stream.
-   */
-  private DutchStemmer stemmer = new DutchStemmer();
-  
-  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-  private final KeywordAttribute keywordAttr = addAttribute(KeywordAttribute.class);
-
-  public DutchStemFilter(TokenStream _in) {
-    super(_in);
-  }
-
-  /**
-   * @param stemdictionary Dictionary of word stem pairs, that overrule the algorithm
-   */
-  public DutchStemFilter(TokenStream _in,  Map<?,?> stemdictionary) {
-    this(_in);
-    stemmer.setStemDictionary(stemdictionary);
-  }
-
-  /**
-   * Returns the next token in the stream, or null at EOS
-   */
-  @Override
-  public boolean incrementToken() throws IOException {
-    if (input.incrementToken()) {
-      final String term = termAtt.toString();
-
-      // Check the exclusion table.
-      if (!keywordAttr.isKeyword()) {
-        final String s = stemmer.stem(term);
-        // If not stemmed, don't waste the time adjusting the token.
-        if ((s != null) && !s.equals(term))
-          termAtt.setEmpty().append(s);
-      }
-      return true;
-    } else {
-      return false;
-    }
-  }
-
-  /**
-   * Set a alternative/custom {@link DutchStemmer} for this filter.
-   */
-  public void setStemmer(DutchStemmer stemmer) {
-    if (stemmer != null) {
-      this.stemmer = stemmer;
-    }
-  }
-
-  /**
-   * Set dictionary for stemming, this dictionary overrules the algorithm,
-   * so you can correct for a particular unwanted word-stem pair.
-   */
-  public void setStemDictionary(HashMap<?,?> dict) {
-    if (stemmer != null)
-      stemmer.setStemDictionary(dict);
-  }
-}
\ No newline at end of file
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/nl/DutchStemmer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/nl/DutchStemmer.java
deleted file mode 100644
index d146fe6..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/nl/DutchStemmer.java
+++ /dev/null
@@ -1,409 +0,0 @@
-package org.apache.lucene.analysis.nl;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Map;
-
-/**
- * A stemmer for Dutch words. 
- * <p>
- * The algorithm is an implementation of
- * the <a href="http://snowball.tartarus.org/algorithms/dutch/stemmer.html">dutch stemming</a>
- * algorithm in Martin Porter's snowball project.
- * </p>
- * @deprecated (3.1) Use {@link org.tartarus.snowball.ext.DutchStemmer} instead, 
- * which has the same functionality. This filter will be removed in Lucene 5.0
- */
-@Deprecated
-public class DutchStemmer {
-  /**
-   * Buffer for the terms while stemming them.
-   */
-  private StringBuilder sb = new StringBuilder();
-  private boolean _removedE;
-  private Map _stemDict;
-
-  private int _R1;
-  private int _R2;
-
-  //TODO convert to internal
-  /*
-   * Stems the given term to an unique <tt>discriminator</tt>.
-   *
-   * @param term The term that should be stemmed.
-   * @return Discriminator for <tt>term</tt>
-   */
-  public String stem(String term) {
-    term = term.toLowerCase();
-    if (!isStemmable(term))
-      return term;
-    if (_stemDict != null && _stemDict.containsKey(term))
-      if (_stemDict.get(term) instanceof String)
-        return (String) _stemDict.get(term);
-      else
-        return null;
-
-    // Reset the StringBuilder.
-    sb.delete(0, sb.length());
-    sb.insert(0, term);
-    // Stemming starts here...
-    substitute(sb);
-    storeYandI(sb);
-    _R1 = getRIndex(sb, 0);
-    _R1 = Math.max(3, _R1);
-    step1(sb);
-    step2(sb);
-    _R2 = getRIndex(sb, _R1);
-    step3a(sb);
-    step3b(sb);
-    step4(sb);
-    reStoreYandI(sb);
-    return sb.toString();
-  }
-
-  private boolean enEnding(StringBuilder sb) {
-    String[] enend = new String[]{"ene", "en"};
-    for (int i = 0; i < enend.length; i++) {
-      String end = enend[i];
-      String s = sb.toString();
-      int index = s.length() - end.length();
-      if (s.endsWith(end) &&
-          index >= _R1 &&
-          isValidEnEnding(sb, index - 1)
-      ) {
-        sb.delete(index, index + end.length());
-        unDouble(sb, index);
-        return true;
-      }
-    }
-    return false;
-  }
-
-
-  private void step1(StringBuilder sb) {
-    if (_R1 >= sb.length())
-      return;
-
-    String s = sb.toString();
-    int lengthR1 = sb.length() - _R1;
-    int index;
-
-    if (s.endsWith("heden")) {
-      sb.replace(_R1, lengthR1 + _R1, sb.substring(_R1, lengthR1 + _R1).replaceAll("heden", "heid"));
-      return;
-    }
-
-    if (enEnding(sb))
-      return;
-
-    if (s.endsWith("se") &&
-        (index = s.length() - 2) >= _R1 &&
-        isValidSEnding(sb, index - 1)
-    ) {
-      sb.delete(index, index + 2);
-      return;
-    }
-    if (s.endsWith("s") &&
-        (index = s.length() - 1) >= _R1 &&
-        isValidSEnding(sb, index - 1)) {
-      sb.delete(index, index + 1);
-    }
-  }
-
-  /**
-   * Delete suffix e if in R1 and
-   * preceded by a non-vowel, and then undouble the ending
-   *
-   * @param sb String being stemmed
-   */
-  private void step2(StringBuilder sb) {
-    _removedE = false;
-    if (_R1 >= sb.length())
-      return;
-    String s = sb.toString();
-    int index = s.length() - 1;
-    if (index >= _R1 &&
-        s.endsWith("e") &&
-        !isVowel(sb.charAt(index - 1))) {
-      sb.delete(index, index + 1);
-      unDouble(sb);
-      _removedE = true;
-    }
-  }
-
-  /**
-   * Delete "heid"
-   *
-   * @param sb String being stemmed
-   */
-  private void step3a(StringBuilder sb) {
-    if (_R2 >= sb.length())
-      return;
-    String s = sb.toString();
-    int index = s.length() - 4;
-    if (s.endsWith("heid") && index >= _R2 && sb.charAt(index - 1) != 'c') {
-      sb.delete(index, index + 4); //remove heid
-      enEnding(sb);
-    }
-  }
-
-  /**
-   * <p>A d-suffix, or derivational suffix, enables a new word,
-   * often with a different grammatical category, or with a different
-   * sense, to be built from another word. Whether a d-suffix can be
-   * attached is discovered not from the rules of grammar, but by
-   * referring to a dictionary. So in English, ness can be added to
-   * certain adjectives to form corresponding nouns (littleness,
-   * kindness, foolishness ...) but not to all adjectives
-   * (not for example, to big, cruel, wise ...) d-suffixes can be
-   * used to change meaning, often in rather exotic ways.</p>
-   * Remove "ing", "end", "ig", "lijk", "baar" and "bar"
-   *
-   * @param sb String being stemmed
-   */
-  private void step3b(StringBuilder sb) {
-    if (_R2 >= sb.length())
-      return;
-    String s = sb.toString();
-    int index = 0;
-
-    if ((s.endsWith("end") || s.endsWith("ing")) &&
-        (index = s.length() - 3) >= _R2) {
-      sb.delete(index, index + 3);
-      if (sb.charAt(index - 2) == 'i' &&
-          sb.charAt(index - 1) == 'g') {
-        if (sb.charAt(index - 3) != 'e' & index - 2 >= _R2) {
-          index -= 2;
-          sb.delete(index, index + 2);
-        }
-      } else {
-        unDouble(sb, index);
-      }
-      return;
-    }
-    if (s.endsWith("ig") &&
-        (index = s.length() - 2) >= _R2
-    ) {
-      if (sb.charAt(index - 1) != 'e')
-        sb.delete(index, index + 2);
-      return;
-    }
-    if (s.endsWith("lijk") &&
-        (index = s.length() - 4) >= _R2
-    ) {
-      sb.delete(index, index + 4);
-      step2(sb);
-      return;
-    }
-    if (s.endsWith("baar") &&
-        (index = s.length() - 4) >= _R2
-    ) {
-      sb.delete(index, index + 4);
-      return;
-    }
-    if (s.endsWith("bar") &&
-        (index = s.length() - 3) >= _R2
-    ) {
-      if (_removedE)
-        sb.delete(index, index + 3);
-      return;
-    }
-  }
-
-  /**
-   * undouble vowel
-   * If the words ends CVD, where C is a non-vowel, D is a non-vowel other than I, and V is double a, e, o or u, remove one of the vowels from V (for example, maan -> man, brood -> brod).
-   *
-   * @param sb String being stemmed
-   */
-  private void step4(StringBuilder sb) {
-    if (sb.length() < 4)
-      return;
-    String end = sb.substring(sb.length() - 4, sb.length());
-    char c = end.charAt(0);
-    char v1 = end.charAt(1);
-    char v2 = end.charAt(2);
-    char d = end.charAt(3);
-    if (v1 == v2 &&
-        d != 'I' &&
-        v1 != 'i' &&
-        isVowel(v1) &&
-        !isVowel(d) &&
-        !isVowel(c)) {
-      sb.delete(sb.length() - 2, sb.length() - 1);
-    }
-  }
-
-  /**
-   * Checks if a term could be stemmed.
-   *
-   * @return true if, and only if, the given term consists in letters.
-   */
-  private boolean isStemmable(String term) {
-    for (int c = 0; c < term.length(); c++) {
-      if (!Character.isLetter(term.charAt(c))) return false;
-    }
-    return true;
-  }
-
-  /**
-   * Substitute Ã¤, Ã«, Ã¯, Ã¶, Ã¼, Ã¡ , Ã©, Ã­, Ã³, Ãº
-   */
-  private void substitute(StringBuilder buffer) {
-    for (int i = 0; i < buffer.length(); i++) {
-      switch (buffer.charAt(i)) {
-        case 'Ã¤':
-        case 'Ã¡':
-          {
-            buffer.setCharAt(i, 'a');
-            break;
-          }
-        case 'Ã«':
-        case 'Ã©':
-          {
-            buffer.setCharAt(i, 'e');
-            break;
-          }
-        case 'Ã¼':
-        case 'Ãº':
-          {
-            buffer.setCharAt(i, 'u');
-            break;
-          }
-        case 'Ã¯':
-        case 'i':
-          {
-            buffer.setCharAt(i, 'i');
-            break;
-          }
-        case 'Ã¶':
-        case 'Ã³':
-          {
-            buffer.setCharAt(i, 'o');
-            break;
-          }
-      }
-    }
-  }
-
-  /*private boolean isValidSEnding(StringBuilder sb) {
-    return isValidSEnding(sb, sb.length() - 1);
-  }*/
-
-  private boolean isValidSEnding(StringBuilder sb, int index) {
-    char c = sb.charAt(index);
-    if (isVowel(c) || c == 'j')
-      return false;
-    return true;
-  }
-
-  /*private boolean isValidEnEnding(StringBuilder sb) {
-    return isValidEnEnding(sb, sb.length() - 1);
-  }*/
-
-  private boolean isValidEnEnding(StringBuilder sb, int index) {
-    char c = sb.charAt(index);
-    if (isVowel(c))
-      return false;
-    if (c < 3)
-      return false;
-    // ends with "gem"?
-    if (c == 'm' && sb.charAt(index - 2) == 'g' && sb.charAt(index - 1) == 'e')
-      return false;
-    return true;
-  }
-
-  private void unDouble(StringBuilder sb) {
-    unDouble(sb, sb.length());
-  }
-
-  private void unDouble(StringBuilder sb, int endIndex) {
-    String s = sb.substring(0, endIndex);
-    if (s.endsWith("kk") || s.endsWith("tt") || s.endsWith("dd") || s.endsWith("nn") || s.endsWith("mm") || s.endsWith("ff")) {
-      sb.delete(endIndex - 1, endIndex);
-    }
-  }
-
-  private int getRIndex(StringBuilder sb, int start) {
-    if (start == 0)
-      start = 1;
-    int i = start;
-    for (; i < sb.length(); i++) {
-      //first non-vowel preceded by a vowel
-      if (!isVowel(sb.charAt(i)) && isVowel(sb.charAt(i - 1))) {
-        return i + 1;
-      }
-    }
-    return i + 1;
-  }
-
-  private void storeYandI(StringBuilder sb) {
-    if (sb.charAt(0) == 'y')
-      sb.setCharAt(0, 'Y');
-
-    int last = sb.length() - 1;
-
-    for (int i = 1; i < last; i++) {
-      switch (sb.charAt(i)) {
-        case 'i':
-          {
-            if (isVowel(sb.charAt(i - 1)) &&
-                isVowel(sb.charAt(i + 1))
-            )
-              sb.setCharAt(i, 'I');
-            break;
-          }
-        case 'y':
-          {
-            if (isVowel(sb.charAt(i - 1)))
-              sb.setCharAt(i, 'Y');
-            break;
-          }
-      }
-    }
-    if (last > 0 && sb.charAt(last) == 'y' && isVowel(sb.charAt(last - 1)))
-      sb.setCharAt(last, 'Y');
-  }
-
-  private void reStoreYandI(StringBuilder sb) {
-    String tmp = sb.toString();
-    sb.delete(0, sb.length());
-    sb.insert(0, tmp.replaceAll("I", "i").replaceAll("Y", "y"));
-  }
-
-  private boolean isVowel(char c) {
-    switch (c) {
-      case 'e':
-      case 'a':
-      case 'o':
-      case 'i':
-      case 'u':
-      case 'y':
-      case 'Ã¨':
-        {
-          return true;
-        }
-    }
-    return false;
-  }
-
-  void setStemDictionary(Map dict) {
-    _stemDict = dict;
-  }
-
-}
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/pattern/PatternReplaceCharFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/pattern/PatternReplaceCharFilter.java
index cb29447..264edbe 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/pattern/PatternReplaceCharFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/pattern/PatternReplaceCharFilter.java
@@ -49,8 +49,6 @@ import org.apache.lucene.analysis.charfilter.BaseCharFilter;
  * @since Solr 1.5
  */
 public class PatternReplaceCharFilter extends BaseCharFilter {
-  @Deprecated
-  public static final int DEFAULT_MAX_BLOCK_CHARS = 10000;
 
   private final Pattern pattern;
   private final String replacement;
@@ -62,12 +60,6 @@ public class PatternReplaceCharFilter extends BaseCharFilter {
     this.replacement = replacement;
   }
 
-  @Deprecated
-  public PatternReplaceCharFilter(Pattern pattern, String replacement, 
-      int maxBlockChars, String blockDelimiter, CharStream in) {
-    this(pattern, replacement, in);
-  }
-
   @Override
   public int read(char[] cbuf, int off, int len) throws IOException {
     // Buffer all input on the first call.
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/pt/PortugueseAnalyzer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/pt/PortugueseAnalyzer.java
index 554d227..6c87d35 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/pt/PortugueseAnalyzer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/pt/PortugueseAnalyzer.java
@@ -34,17 +34,9 @@ import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.Version;
-import org.tartarus.snowball.ext.PortugueseStemmer;
 
 /**
  * {@link Analyzer} for Portuguese.
- * <p>
- * <a name="version"/>
- * <p>You must specify the required {@link Version}
- * compatibility when creating PortugueseAnalyzer:
- * <ul>
- *   <li> As of 3.6, PortugueseLightStemFilter is used for less aggressive stemming.
- * </ul>
  */
 public final class PortugueseAnalyzer extends StopwordAnalyzerBase {
   private final CharArraySet stemExclusionSet;
@@ -132,11 +124,7 @@ public final class PortugueseAnalyzer extends StopwordAnalyzerBase {
     result = new StopFilter(matchVersion, result, stopwords);
     if(!stemExclusionSet.isEmpty())
       result = new KeywordMarkerFilter(result, stemExclusionSet);
-    if (matchVersion.onOrAfter(Version.LUCENE_36)) {
-      result = new PortugueseLightStemFilter(result);
-    } else {
-      result = new SnowballFilter(result, new PortugueseStemmer());
-    }
+    result = new PortugueseLightStemFilter(result);
     return new TokenStreamComponents(source, result);
   }
 }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/pt/RSLPStemmerBase.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/pt/RSLPStemmerBase.java
index 2fb7a1f..eb6398f 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/pt/RSLPStemmerBase.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/pt/RSLPStemmerBase.java
@@ -134,7 +134,7 @@ public abstract class RSLPStemmerBase {
         if (!exceptions[i].endsWith(suffix))
           System.err.println("warning: useless exception '" + exceptions[i] + "' does not end with '" + suffix + "'");
       }
-      this.exceptions = new CharArraySet(Version.LUCENE_31,
+      this.exceptions = new CharArraySet(Version.LUCENE_50,
            Arrays.asList(exceptions), false);
     }
 
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/reverse/ReverseStringFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/reverse/ReverseStringFilter.java
index bcf7545..ce954ac 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/reverse/ReverseStringFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/reverse/ReverseStringFilter.java
@@ -31,14 +31,6 @@ import java.io.IOException;
  * that character. For example, with a marker of &#x5C;u0001, "country" =>
  * "&#x5C;u0001yrtnuoc". This is useful when implementing efficient leading
  * wildcards search.
- * </p>
- * <a name="version"/>
- * <p>You must specify the required {@link Version}
- * compatibility when creating ReverseStringFilter, or when using any of
- * its static methods:
- * <ul>
- *   <li> As of 3.1, supplementary characters are handled correctly
- * </ul>
  */
 public final class ReverseStringFilter extends TokenFilter {
 
@@ -74,7 +66,7 @@ public final class ReverseStringFilter extends TokenFilter {
    * The reversed tokens will not be marked. 
    * </p>
    * 
-   * @param matchVersion See <a href="#version">above</a>
+   * @param matchVersion Lucene compatibility version
    * @param in {@link TokenStream} to filter
    */
   public ReverseStringFilter(Version matchVersion, TokenStream in) {
@@ -89,7 +81,7 @@ public final class ReverseStringFilter extends TokenFilter {
    * character.
    * </p>
    * 
-   * @param matchVersion See <a href="#version">above</a>
+   * @param matchVersion compatibility version
    * @param in {@link TokenStream} to filter
    * @param marker A character used to mark reversed tokens
    */
@@ -119,7 +111,7 @@ public final class ReverseStringFilter extends TokenFilter {
   /**
    * Reverses the given input string
    * 
-   * @param matchVersion See <a href="#version">above</a>
+   * @param matchVersion compatibility version
    * @param input the string to reverse
    * @return the given input string in reversed order
    */
@@ -131,7 +123,7 @@ public final class ReverseStringFilter extends TokenFilter {
   
   /**
    * Reverses the given input buffer in-place
-   * @param matchVersion See <a href="#version">above</a>
+   * @param matchVersion compatibility version
    * @param buffer the input char array to reverse
    */
   public static void reverse(Version matchVersion, final char[] buffer) {
@@ -141,7 +133,7 @@ public final class ReverseStringFilter extends TokenFilter {
   /**
    * Partially reverses the given input buffer in-place from offset 0
    * up to the given length.
-   * @param matchVersion See <a href="#version">above</a>
+   * @param matchVersion compatibility version
    * @param buffer the input char array to reverse
    * @param len the length in the buffer up to where the
    *        buffer should be reversed
@@ -152,23 +144,9 @@ public final class ReverseStringFilter extends TokenFilter {
   }
   
   /**
-   * @deprecated (3.1) Remove this when support for 3.0 indexes is no longer needed.
-   */
-  @Deprecated
-  private static void reverseUnicode3( char[] buffer, int start, int len ){
-    if( len <= 1 ) return;
-    int num = len>>1;
-    for( int i = start; i < ( start + num ); i++ ){
-      char c = buffer[i];
-      buffer[i] = buffer[start * 2 + len - i - 1];
-      buffer[start * 2 + len - i - 1] = c;
-    }
-  }
-  
-  /**
    * Partially reverses the given input buffer in-place from the given offset
    * up to the given length.
-   * @param matchVersion See <a href="#version">above</a>
+   * @param matchVersion compatibility version
    * @param buffer the input char array to reverse
    * @param start the offset from where to reverse the buffer
    * @param len the length in the buffer up to where the
@@ -176,10 +154,6 @@ public final class ReverseStringFilter extends TokenFilter {
    */
   public static void reverse(Version matchVersion, final char[] buffer,
       final int start, final int len) {
-    if (!matchVersion.onOrAfter(Version.LUCENE_31)) {
-      reverseUnicode3(buffer, start, len);
-      return;
-    }
     /* modified version of Apache Harmony AbstractStringBuilder reverse0() */
     if (len < 2)
       return;
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianAnalyzer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianAnalyzer.java
index 0aa1309..1228a02 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianAnalyzer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianAnalyzer.java
@@ -19,7 +19,6 @@ package org.apache.lucene.analysis.ru;
 
 import java.io.IOException;
 import java.io.Reader;
-import java.util.Arrays;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.snowball.SnowballFilter;
@@ -42,44 +41,13 @@ import org.apache.lucene.util.Version;
  * Supports an external list of stopwords (words that
  * will not be indexed at all).
  * A default set of stopwords is used unless an alternative list is specified.
- * </p>
- * <a name="version"/>
- * <p>You must specify the required {@link Version}
- * compatibility when creating RussianAnalyzer:
- * <ul>
- *   <li> As of 3.1, StandardTokenizer is used, Snowball stemming is done with
- *        SnowballFilter, and Snowball stopwords are used by default.
- * </ul>
  */
-public final class RussianAnalyzer extends StopwordAnalyzerBase
-{
-    /**
-     * List of typical Russian stopwords. (for backwards compatibility)
-     * @deprecated (3.1) Remove this for LUCENE 5.0
-     */
-    @Deprecated
-    private static final String[] RUSSIAN_STOP_WORDS_30 = {
-      "Ð°", "Ð±ÐµÐ·", "Ð±Ð¾Ð»ÐµÐµ", "Ð±?", "Ð±?Ð»", "Ð±?Ð»Ð°", "Ð±?Ð»Ð¸", "Ð±?Ð»Ð¾", "Ð±???", "Ð²",
-      "Ð²Ð°Ð¼", "Ð²Ð°?", "Ð²Ðµ??", "Ð²Ð¾", "Ð²Ð¾?", "Ð²?Ðµ", "Ð²?ÐµÐ³Ð¾", "Ð²?Ðµ?", "Ð²?", "Ð³Ð´Ðµ", 
-      "Ð´Ð°", "Ð´Ð°Ð¶Ðµ", "Ð´Ð»?", "Ð´Ð¾", "ÐµÐ³Ð¾", "ÐµÐµ", "ÐµÐ¹", "Ðµ?", "Ðµ?Ð»Ð¸", "Ðµ???", 
-      "Ðµ?Ðµ", "Ð¶Ðµ", "Ð·Ð°", "Ð·Ð´Ðµ??", "Ð¸", "Ð¸Ð·", "Ð¸Ð»Ð¸", "Ð¸Ð¼", "Ð¸?", "Ðº", "ÐºÐ°Ðº",
-      "ÐºÐ¾", "ÐºÐ¾Ð³Ð´Ð°", "Ðº?Ð¾", "Ð»Ð¸", "Ð»Ð¸Ð±Ð¾", "Ð¼Ð½Ðµ", "Ð¼Ð¾Ð¶Ðµ?", "Ð¼?", "Ð½Ð°", "Ð½Ð°Ð´Ð¾", 
-      "Ð½Ð°?", "Ð½Ðµ", "Ð½ÐµÐ³Ð¾", "Ð½ÐµÐµ", "Ð½Ðµ?", "Ð½Ð¸", "Ð½Ð¸?", "Ð½Ð¾", "Ð½?", "Ð¾", "Ð¾Ð±", 
-      "Ð¾Ð´Ð½Ð°ÐºÐ¾", "Ð¾Ð½", "Ð¾Ð½Ð°", "Ð¾Ð½Ð¸", "Ð¾Ð½Ð¾", "Ð¾?", "Ð¾?ÐµÐ½?", "Ð¿Ð¾", "Ð¿Ð¾Ð´", "Ð¿?Ð¸", 
-      "?", "?Ð¾", "?Ð°Ðº", "?Ð°ÐºÐ¶Ðµ", "?Ð°ÐºÐ¾Ð¹", "?Ð°Ð¼", "?Ðµ", "?ÐµÐ¼", "?Ð¾", "?Ð¾Ð³Ð¾", 
-      "?Ð¾Ð¶Ðµ", "?Ð¾Ð¹", "?Ð¾Ð»?ÐºÐ¾", "?Ð¾Ð¼", "??", "?", "?Ð¶Ðµ", "?Ð¾??", "?ÐµÐ³Ð¾", "?ÐµÐ¹", 
-      "?ÐµÐ¼", "??Ð¾", "??Ð¾Ð±?", "??Ðµ", "???", "??Ð°", "??Ð¸", "??Ð¾", "?"
-    };
+public final class RussianAnalyzer extends StopwordAnalyzerBase {
     
     /** File containing default Russian stopwords. */
     public final static String DEFAULT_STOPWORD_FILE = "russian_stop.txt";
     
     private static class DefaultSetHolder {
-      /** @deprecated (3.1) remove this for Lucene 5.0 */
-      @Deprecated
-      static final CharArraySet DEFAULT_STOP_SET_30 = CharArraySet
-          .unmodifiableSet(new CharArraySet(Version.LUCENE_CURRENT, 
-              Arrays.asList(RUSSIAN_STOP_WORDS_30), false));
       static final CharArraySet DEFAULT_STOP_SET;
       
       static {
@@ -106,9 +74,7 @@ public final class RussianAnalyzer extends StopwordAnalyzerBase
     }
 
     public RussianAnalyzer(Version matchVersion) {
-      this(matchVersion,
-        matchVersion.onOrAfter(Version.LUCENE_31) ? DefaultSetHolder.DEFAULT_STOP_SET
-            : DefaultSetHolder.DEFAULT_STOP_SET_30);
+      this(matchVersion, DefaultSetHolder.DEFAULT_STOP_SET);
     }
   
     /**
@@ -151,23 +117,13 @@ public final class RussianAnalyzer extends StopwordAnalyzerBase
     @Override
     protected TokenStreamComponents createComponents(String fieldName,
         Reader reader) {
-      if (matchVersion.onOrAfter(Version.LUCENE_31)) {
-        final Tokenizer source = new StandardTokenizer(matchVersion, reader);
-        TokenStream result = new StandardFilter(matchVersion, source);
-        result = new LowerCaseFilter(matchVersion, result);
-        result = new StopFilter(matchVersion, result, stopwords);
-        if (!stemExclusionSet.isEmpty()) result = new KeywordMarkerFilter(
-            result, stemExclusionSet);
-        result = new SnowballFilter(result, new org.tartarus.snowball.ext.RussianStemmer());
-        return new TokenStreamComponents(source, result);
-      } else {
-        final Tokenizer source = new RussianLetterTokenizer(matchVersion, reader);
-        TokenStream result = new LowerCaseFilter(matchVersion, source);
-        result = new StopFilter(matchVersion, result, stopwords);
-        if (!stemExclusionSet.isEmpty()) result = new KeywordMarkerFilter(
-          result, stemExclusionSet);
-        result = new SnowballFilter(result, new org.tartarus.snowball.ext.RussianStemmer());
-        return new TokenStreamComponents(source, result);
-      }
+      final Tokenizer source = new StandardTokenizer(matchVersion, reader);
+      TokenStream result = new StandardFilter(matchVersion, source);
+      result = new LowerCaseFilter(matchVersion, result);
+      result = new StopFilter(matchVersion, result, stopwords);
+      if (!stemExclusionSet.isEmpty()) 
+        result = new KeywordMarkerFilter(result, stemExclusionSet);
+      result = new SnowballFilter(result, new org.tartarus.snowball.ext.RussianStemmer());
+      return new TokenStreamComponents(source, result);
     }
 }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianLetterTokenizer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianLetterTokenizer.java
deleted file mode 100644
index 088b802..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianLetterTokenizer.java
+++ /dev/null
@@ -1,97 +0,0 @@
-package org.apache.lucene.analysis.ru;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Reader;
-import org.apache.lucene.analysis.Tokenizer; // for javadocs
-import org.apache.lucene.analysis.util.CharTokenizer;
-import org.apache.lucene.analysis.core.LetterTokenizer;
-import org.apache.lucene.analysis.standard.StandardTokenizer; // for javadocs
-import org.apache.lucene.util.AttributeSource;
-import org.apache.lucene.util.Version;
-
-/**
- * A RussianLetterTokenizer is a {@link Tokenizer} that extends {@link LetterTokenizer}
- * by also allowing the basic Latin digits 0-9.
- * <p>
- * <a name="version"/>
- * You must specify the required {@link Version} compatibility when creating
- * {@link RussianLetterTokenizer}:
- * <ul>
- * <li>As of 3.1, {@link CharTokenizer} uses an int based API to normalize and
- * detect token characters. See {@link CharTokenizer#isTokenChar(int)} and
- * {@link CharTokenizer#normalize(int)} for details.</li>
- * </ul>
- * @deprecated (3.1) Use {@link StandardTokenizer} instead, which has the same functionality.
- * This filter will be removed in Lucene 5.0 
- */
-@Deprecated
-public class RussianLetterTokenizer extends CharTokenizer
-{    
-    private static final int DIGIT_0 = '0';
-    private static final int DIGIT_9 = '9';
-    
-    /**
-     * Construct a new RussianLetterTokenizer. * @param matchVersion Lucene version
-     * to match See {@link <a href="#version">above</a>}
-     * 
-     * @param in
-     *          the input to split up into tokens
-     */
-    public RussianLetterTokenizer(Version matchVersion, Reader in) {
-      super(matchVersion, in);
-    }
-
-    /**
-     * Construct a new RussianLetterTokenizer using a given {@link AttributeSource}.
-     * 
-     * @param matchVersion
-     *          Lucene version to match See {@link <a href="#version">above</a>}
-     * @param source
-     *          the attribute source to use for this {@link Tokenizer}
-     * @param in
-     *          the input to split up into tokens
-     */
-    public RussianLetterTokenizer(Version matchVersion, AttributeSource source, Reader in) {
-      super(matchVersion, source, in);
-    }
-
-    /**
-     * Construct a new RussianLetterTokenizer using a given
-     * {@link org.apache.lucene.util.AttributeSource.AttributeFactory}. * @param
-     * matchVersion Lucene version to match See
-     * {@link <a href="#version">above</a>}
-     * 
-     * @param factory
-     *          the attribute factory to use for this {@link Tokenizer}
-     * @param in
-     *          the input to split up into tokens
-     */
-    public RussianLetterTokenizer(Version matchVersion, AttributeFactory factory, Reader in) {
-      super(matchVersion, factory, in);
-    }
-    
-     /**
-     * Collects only characters which satisfy
-     * {@link Character#isLetter(int)}.
-     */
-    @Override
-    protected boolean isTokenChar(int c) {
-        return Character.isLetter(c) || (c >= DIGIT_0 && c <= DIGIT_9);
-    }
-}
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/snowball/SnowballAnalyzer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/snowball/SnowballAnalyzer.java
deleted file mode 100644
index d9c624d..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/snowball/SnowballAnalyzer.java
+++ /dev/null
@@ -1,88 +0,0 @@
-package org.apache.lucene.analysis.snowball;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.*;
-import org.apache.lucene.analysis.core.LowerCaseFilter;
-import org.apache.lucene.analysis.core.StopFilter;
-import org.apache.lucene.analysis.en.EnglishPossessiveFilter;
-import org.apache.lucene.analysis.standard.*;
-import org.apache.lucene.analysis.tr.TurkishLowerCaseFilter;
-import org.apache.lucene.analysis.util.CharArraySet;
-import org.apache.lucene.util.Version;
-
-import java.io.Reader;
-
-/** Filters {@link StandardTokenizer} with {@link StandardFilter}, {@link
- * LowerCaseFilter}, {@link StopFilter} and {@link SnowballFilter}.
- *
- * Available stemmers are listed in org.tartarus.snowball.ext.  The name of a
- * stemmer is the part of the class name before "Stemmer", e.g., the stemmer in
- * {@link org.tartarus.snowball.ext.EnglishStemmer} is named "English".
- *
- * <p><b>NOTE</b>: This class uses the same {@link Version}
- * dependent settings as {@link StandardAnalyzer}, with the following addition:
- * <ul>
- *   <li> As of 3.1, uses {@link TurkishLowerCaseFilter} for Turkish language.
- * </ul>
- * </p>
- * @deprecated (3.1) Use the language-specific analyzer in modules/analysis instead. 
- * This analyzer will be removed in Lucene 5.0
- */
-@Deprecated
-public final class SnowballAnalyzer extends Analyzer {
-  private String name;
-  private CharArraySet stopSet;
-  private final Version matchVersion;
-
-  /** Builds the named analyzer with no stop words. */
-  public SnowballAnalyzer(Version matchVersion, String name) {
-    this.name = name;
-    this.matchVersion = matchVersion;
-  }
-
-  /** Builds the named analyzer with the given stop words. */
-  public SnowballAnalyzer(Version matchVersion, String name, CharArraySet stopWords) {
-    this(matchVersion, name);
-    stopSet = CharArraySet.unmodifiableSet(CharArraySet.copy(matchVersion,
-        stopWords));
-  }
-
-  /** Constructs a {@link StandardTokenizer} filtered by a {@link
-      StandardFilter}, a {@link LowerCaseFilter}, a {@link StopFilter},
-      and a {@link SnowballFilter} */
-  @Override
-  public TokenStreamComponents createComponents(String fieldName, Reader reader) {
-    Tokenizer tokenizer = new StandardTokenizer(matchVersion, reader);
-    TokenStream result = new StandardFilter(matchVersion, tokenizer);
-    // remove the possessive 's for english stemmers
-    if (matchVersion.onOrAfter(Version.LUCENE_31) && 
-        (name.equals("English") || name.equals("Porter") || name.equals("Lovins")))
-      result = new EnglishPossessiveFilter(result);
-    // Use a special lowercase filter for turkish, the stemmer expects it.
-    if (matchVersion.onOrAfter(Version.LUCENE_31) && name.equals("Turkish"))
-      result = new TurkishLowerCaseFilter(result);
-    else
-      result = new LowerCaseFilter(matchVersion, result);
-    if (stopSet != null)
-      result = new StopFilter(matchVersion,
-                              result, stopSet);
-    result = new SnowballFilter(result, name);
-    return new TokenStreamComponents(tokenizer, result);
-  }
-}
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardFilter.java
index 8771466..e27b950 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardFilter.java
@@ -21,61 +21,19 @@ import java.io.IOException;
 
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
 import org.apache.lucene.util.Version;
 
 /**
  * Normalizes tokens extracted with {@link StandardTokenizer}.
  */
 public class StandardFilter extends TokenFilter {
-  private final Version matchVersion;
   
   public StandardFilter(Version matchVersion, TokenStream in) {
     super(in);
-    this.matchVersion = matchVersion;
   }
   
-  private static final String APOSTROPHE_TYPE = ClassicTokenizer.TOKEN_TYPES[ClassicTokenizer.APOSTROPHE];
-  private static final String ACRONYM_TYPE = ClassicTokenizer.TOKEN_TYPES[ClassicTokenizer.ACRONYM];
-
-  // this filters uses attribute type
-  private final TypeAttribute typeAtt = addAttribute(TypeAttribute.class);
-  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-  
   @Override
   public final boolean incrementToken() throws IOException {
-    if (matchVersion.onOrAfter(Version.LUCENE_31))
-      return input.incrementToken(); // TODO: add some niceties for the new grammar
-    else
-      return incrementTokenClassic();
-  }
-  
-  public final boolean incrementTokenClassic() throws IOException {
-    if (!input.incrementToken()) {
-      return false;
-    }
-
-    final char[] buffer = termAtt.buffer();
-    final int bufferLength = termAtt.length();
-    final String type = typeAtt.type();
-
-    if (type == APOSTROPHE_TYPE &&      // remove 's
-        bufferLength >= 2 &&
-        buffer[bufferLength-2] == '\'' &&
-        (buffer[bufferLength-1] == 's' || buffer[bufferLength-1] == 'S')) {
-      // Strip last 2 characters off
-      termAtt.setLength(bufferLength - 2);
-    } else if (type == ACRONYM_TYPE) {      // remove dots
-      int upto = 0;
-      for(int i=0;i<bufferLength;i++) {
-        char c = buffer[i];
-        if (c != '.')
-          buffer[upto++] = c;
-      }
-      termAtt.setLength(upto);
-    }
-
-    return true;
+    return input.incrementToken(); // TODO: add some niceties for the new grammar
   }
 }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizer.java
index e48eea3..228cf47 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizer.java
@@ -21,7 +21,6 @@ import java.io.IOException;
 import java.io.Reader;
 
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.standard.std31.StandardTokenizerImpl31;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
@@ -31,31 +30,20 @@ import org.apache.lucene.util.Version;
 
 /** A grammar-based tokenizer constructed with JFlex.
  * <p>
- * As of Lucene version 3.1, this class implements the Word Break rules from the
+ * This class implements the Word Break rules from the
  * Unicode Text Segmentation algorithm, as specified in 
  * <a href="http://unicode.org/reports/tr29/">Unicode Standard Annex #29</a>.
  * <p/>
  * <p>Many applications have specific tokenizer needs.  If this tokenizer does
  * not suit your application, please consider copying this source code
  * directory to your project and maintaining your own grammar-based tokenizer.
- *
- * <a name="version"/>
- * <p>You must specify the required {@link Version}
- * compatibility when creating StandardTokenizer:
- * <ul>
- *   <li> As of 3.4, Hiragana and Han characters are no longer wrongly split
- *   from their combining characters. If you use a previous version number,
- *   you get the exact broken behavior for backwards compatibility.
- *   <li> As of 3.1, StandardTokenizer implements Unicode text segmentation.
- *   If you use a previous version number, you get the exact behavior of
- *   {@link ClassicTokenizer} for backwards compatibility.
- * </ul>
  */
 
 public final class StandardTokenizer extends Tokenizer {
   /** A private instance of the JFlex-constructed scanner */
   private StandardTokenizerInterface scanner;
 
+  // TODO: how can we remove these old types?!
   public static final int ALPHANUM          = 0;
   /** @deprecated (3.1) */
   @Deprecated
@@ -146,13 +134,7 @@ public final class StandardTokenizer extends Tokenizer {
   }
 
   private final void init(Version matchVersion) {
-    if (matchVersion.onOrAfter(Version.LUCENE_34)) {
-      this.scanner = new StandardTokenizerImpl(input);
-    } else if (matchVersion.onOrAfter(Version.LUCENE_31)) {
-      this.scanner = new StandardTokenizerImpl31(input);
-    } else {
-      this.scanner = new ClassicTokenizerImpl(input);
-    }
+    this.scanner = new StandardTokenizerImpl(input);
   }
 
   // this tokenizer generates three attributes:
@@ -184,15 +166,7 @@ public final class StandardTokenizer extends Tokenizer {
         scanner.getText(termAtt);
         final int start = scanner.yychar();
         offsetAtt.setOffset(correctOffset(start), correctOffset(start+termAtt.length()));
-        // This 'if' should be removed in the next release. For now, it converts
-        // invalid acronyms to HOST. When removed, only the 'else' part should
-        // remain.
-        if (tokenType == StandardTokenizer.ACRONYM_DEP) {
-          typeAtt.setType(StandardTokenizer.TOKEN_TYPES[StandardTokenizer.HOST]);
-          termAtt.setLength(termAtt.length() - 1); // remove extra '.'
-        } else {
-          typeAtt.setType(StandardTokenizer.TOKEN_TYPES[tokenType]);
-        }
+        typeAtt.setType(StandardTokenizer.TOKEN_TYPES[tokenType]);
         return true;
       } else
         // When we skip a too-long term, we still increment the
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizer.java
index 15432c8..3e43350 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizer.java
@@ -23,8 +23,6 @@ import java.io.InputStreamReader;
 import java.io.Reader;
 
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.standard.std31.UAX29URLEmailTokenizerImpl31;
-import org.apache.lucene.analysis.standard.std34.UAX29URLEmailTokenizerImpl34;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
@@ -50,14 +48,6 @@ import org.apache.lucene.util.AttributeSource.AttributeFactory;
  *   <li>&lt;IDEOGRAPHIC&gt;: A single CJKV ideographic character</li>
  *   <li>&lt;HIRAGANA&gt;: A single hiragana character</li>
  * </ul>
- * <a name="version"/>
- * <p>You must specify the required {@link Version}
- * compatibility when creating UAX29URLEmailTokenizer:
- * <ul>
- *   <li> As of 3.4, Hiragana and Han characters are no longer wrongly split
- *   from their combining characters. If you use a previous version number,
- *   you get the exact broken behavior for backwards compatibility.
- * </ul>
  */
 
 public final class UAX29URLEmailTokenizer extends Tokenizer {
@@ -128,13 +118,7 @@ public final class UAX29URLEmailTokenizer extends Tokenizer {
   }
 
   private static StandardTokenizerInterface getScannerFor(Version matchVersion, Reader input) {
-    if (matchVersion.onOrAfter(Version.LUCENE_36)) {
-      return new UAX29URLEmailTokenizerImpl(input);
-    } else if (matchVersion.onOrAfter(Version.LUCENE_34)) {
-      return new UAX29URLEmailTokenizerImpl34(input);
-    } else {
-      return new UAX29URLEmailTokenizerImpl31(input);
-    }
+    return new UAX29URLEmailTokenizerImpl(input);
   }
 
   // this tokenizer generates three attributes:
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/ASCIITLD.jflex-macro b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/ASCIITLD.jflex-macro
deleted file mode 100644
index ed8a0ab..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/ASCIITLD.jflex-macro
+++ /dev/null
@@ -1,330 +0,0 @@
-/*
- * Copyright 2001-2005 The Apache Software Foundation.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-// Generated from IANA Root Zone Database <http://www.internic.net/zones/root.zone>
-// file version from Wednesday, February 9, 2011 12:34:10 PM UTC
-// generated on Wednesday, February 9, 2011 4:45:18 PM UTC
-// by org.apache.lucene.analysis.standard.GenerateJflexTLDMacros
-
-ASCIITLD = "." (
-	  [aA][cC]
-	| [aA][dD]
-	| [aA][eE]
-	| [aA][eE][rR][oO]
-	| [aA][fF]
-	| [aA][gG]
-	| [aA][iI]
-	| [aA][lL]
-	| [aA][mM]
-	| [aA][nN]
-	| [aA][oO]
-	| [aA][qQ]
-	| [aA][rR]
-	| [aA][rR][pP][aA]
-	| [aA][sS]
-	| [aA][sS][iI][aA]
-	| [aA][tT]
-	| [aA][uU]
-	| [aA][wW]
-	| [aA][xX]
-	| [aA][zZ]
-	| [bB][aA]
-	| [bB][bB]
-	| [bB][dD]
-	| [bB][eE]
-	| [bB][fF]
-	| [bB][gG]
-	| [bB][hH]
-	| [bB][iI]
-	| [bB][iI][zZ]
-	| [bB][jJ]
-	| [bB][mM]
-	| [bB][nN]
-	| [bB][oO]
-	| [bB][rR]
-	| [bB][sS]
-	| [bB][tT]
-	| [bB][vV]
-	| [bB][wW]
-	| [bB][yY]
-	| [bB][zZ]
-	| [cC][aA]
-	| [cC][aA][tT]
-	| [cC][cC]
-	| [cC][dD]
-	| [cC][fF]
-	| [cC][gG]
-	| [cC][hH]
-	| [cC][iI]
-	| [cC][kK]
-	| [cC][lL]
-	| [cC][mM]
-	| [cC][nN]
-	| [cC][oO]
-	| [cC][oO][mM]
-	| [cC][oO][oO][pP]
-	| [cC][rR]
-	| [cC][uU]
-	| [cC][vV]
-	| [cC][xX]
-	| [cC][yY]
-	| [cC][zZ]
-	| [dD][eE]
-	| [dD][jJ]
-	| [dD][kK]
-	| [dD][mM]
-	| [dD][oO]
-	| [dD][zZ]
-	| [eE][cC]
-	| [eE][dD][uU]
-	| [eE][eE]
-	| [eE][gG]
-	| [eE][rR]
-	| [eE][sS]
-	| [eE][tT]
-	| [eE][uU]
-	| [fF][iI]
-	| [fF][jJ]
-	| [fF][kK]
-	| [fF][mM]
-	| [fF][oO]
-	| [fF][rR]
-	| [gG][aA]
-	| [gG][bB]
-	| [gG][dD]
-	| [gG][eE]
-	| [gG][fF]
-	| [gG][gG]
-	| [gG][hH]
-	| [gG][iI]
-	| [gG][lL]
-	| [gG][mM]
-	| [gG][nN]
-	| [gG][oO][vV]
-	| [gG][pP]
-	| [gG][qQ]
-	| [gG][rR]
-	| [gG][sS]
-	| [gG][tT]
-	| [gG][uU]
-	| [gG][wW]
-	| [gG][yY]
-	| [hH][kK]
-	| [hH][mM]
-	| [hH][nN]
-	| [hH][rR]
-	| [hH][tT]
-	| [hH][uU]
-	| [iI][dD]
-	| [iI][eE]
-	| [iI][lL]
-	| [iI][mM]
-	| [iI][nN]
-	| [iI][nN][fF][oO]
-	| [iI][nN][tT]
-	| [iI][oO]
-	| [iI][qQ]
-	| [iI][rR]
-	| [iI][sS]
-	| [iI][tT]
-	| [jJ][eE]
-	| [jJ][mM]
-	| [jJ][oO]
-	| [jJ][oO][bB][sS]
-	| [jJ][pP]
-	| [kK][eE]
-	| [kK][gG]
-	| [kK][hH]
-	| [kK][iI]
-	| [kK][mM]
-	| [kK][nN]
-	| [kK][pP]
-	| [kK][rR]
-	| [kK][wW]
-	| [kK][yY]
-	| [kK][zZ]
-	| [lL][aA]
-	| [lL][bB]
-	| [lL][cC]
-	| [lL][iI]
-	| [lL][kK]
-	| [lL][rR]
-	| [lL][sS]
-	| [lL][tT]
-	| [lL][uU]
-	| [lL][vV]
-	| [lL][yY]
-	| [mM][aA]
-	| [mM][cC]
-	| [mM][dD]
-	| [mM][eE]
-	| [mM][gG]
-	| [mM][hH]
-	| [mM][iI][lL]
-	| [mM][kK]
-	| [mM][lL]
-	| [mM][mM]
-	| [mM][nN]
-	| [mM][oO]
-	| [mM][oO][bB][iI]
-	| [mM][pP]
-	| [mM][qQ]
-	| [mM][rR]
-	| [mM][sS]
-	| [mM][tT]
-	| [mM][uU]
-	| [mM][uU][sS][eE][uU][mM]
-	| [mM][vV]
-	| [mM][wW]
-	| [mM][xX]
-	| [mM][yY]
-	| [mM][zZ]
-	| [nN][aA]
-	| [nN][aA][mM][eE]
-	| [nN][cC]
-	| [nN][eE]
-	| [nN][eE][tT]
-	| [nN][fF]
-	| [nN][gG]
-	| [nN][iI]
-	| [nN][lL]
-	| [nN][oO]
-	| [nN][pP]
-	| [nN][rR]
-	| [nN][uU]
-	| [nN][zZ]
-	| [oO][mM]
-	| [oO][rR][gG]
-	| [pP][aA]
-	| [pP][eE]
-	| [pP][fF]
-	| [pP][gG]
-	| [pP][hH]
-	| [pP][kK]
-	| [pP][lL]
-	| [pP][mM]
-	| [pP][nN]
-	| [pP][rR]
-	| [pP][rR][oO]
-	| [pP][sS]
-	| [pP][tT]
-	| [pP][wW]
-	| [pP][yY]
-	| [qQ][aA]
-	| [rR][eE]
-	| [rR][oO]
-	| [rR][sS]
-	| [rR][uU]
-	| [rR][wW]
-	| [sS][aA]
-	| [sS][bB]
-	| [sS][cC]
-	| [sS][dD]
-	| [sS][eE]
-	| [sS][gG]
-	| [sS][hH]
-	| [sS][iI]
-	| [sS][jJ]
-	| [sS][kK]
-	| [sS][lL]
-	| [sS][mM]
-	| [sS][nN]
-	| [sS][oO]
-	| [sS][rR]
-	| [sS][tT]
-	| [sS][uU]
-	| [sS][vV]
-	| [sS][yY]
-	| [sS][zZ]
-	| [tT][cC]
-	| [tT][dD]
-	| [tT][eE][lL]
-	| [tT][fF]
-	| [tT][gG]
-	| [tT][hH]
-	| [tT][jJ]
-	| [tT][kK]
-	| [tT][lL]
-	| [tT][mM]
-	| [tT][nN]
-	| [tT][oO]
-	| [tT][pP]
-	| [tT][rR]
-	| [tT][rR][aA][vV][eE][lL]
-	| [tT][tT]
-	| [tT][vV]
-	| [tT][wW]
-	| [tT][zZ]
-	| [uU][aA]
-	| [uU][gG]
-	| [uU][kK]
-	| [uU][sS]
-	| [uU][yY]
-	| [uU][zZ]
-	| [vV][aA]
-	| [vV][cC]
-	| [vV][eE]
-	| [vV][gG]
-	| [vV][iI]
-	| [vV][nN]
-	| [vV][uU]
-	| [wW][fF]
-	| [wW][sS]
-	| [xX][nN]--0[zZ][wW][mM]56[dD]
-	| [xX][nN]--11[bB]5[bB][sS]3[aA]9[aA][jJ]6[gG]
-	| [xX][nN]--3[eE]0[bB]707[eE]
-	| [xX][nN]--45[bB][rR][jJ]9[cC]
-	| [xX][nN]--80[aA][kK][hH][bB][yY][kK][nN][jJ]4[fF]
-	| [xX][nN]--9[tT]4[bB]11[yY][iI]5[aA]
-	| [xX][nN]--[cC][lL][cC][hH][cC]0[eE][aA]0[bB]2[gG]2[aA]9[gG][cC][dD]
-	| [xX][nN]--[dD][eE][bB][aA]0[aA][dD]
-	| [xX][nN]--[fF][iI][qQ][sS]8[sS]
-	| [xX][nN]--[fF][iI][qQ][zZ]9[sS]
-	| [xX][nN]--[fF][pP][cC][rR][jJ]9[cC]3[dD]
-	| [xX][nN]--[fF][zZ][cC]2[cC]9[eE]2[cC]
-	| [xX][nN]--[gG]6[wW]251[dD]
-	| [xX][nN]--[gG][eE][cC][rR][jJ]9[cC]
-	| [xX][nN]--[hH]2[bB][rR][jJ]9[cC]
-	| [xX][nN]--[hH][gG][bB][kK]6[aA][jJ]7[fF]53[bB][bB][aA]
-	| [xX][nN]--[hH][lL][cC][jJ]6[aA][yY][aA]9[eE][sS][cC]7[aA]
-	| [xX][nN]--[jJ]6[wW]193[gG]
-	| [xX][nN]--[jJ][xX][aA][lL][pP][dD][lL][pP]
-	| [xX][nN]--[kK][gG][bB][eE][cC][hH][tT][vV]
-	| [xX][nN]--[kK][pP][rR][wW]13[dD]
-	| [xX][nN]--[kK][pP][rR][yY]57[dD]
-	| [xX][nN]--[mM][gG][bB][aA][aA][mM]7[aA]8[hH]
-	| [xX][nN]--[mM][gG][bB][aA][yY][hH]7[gG][pP][aA]
-	| [xX][nN]--[mM][gG][bB][bB][hH]1[aA]71[eE]
-	| [xX][nN]--[mM][gG][bB][eE][rR][pP]4[aA]5[dD]4[aA][rR]
-	| [xX][nN]--[oO]3[cC][wW]4[hH]
-	| [xX][nN]--[oO][gG][bB][pP][fF]8[fF][lL]
-	| [xX][nN]--[pP]1[aA][iI]
-	| [xX][nN]--[pP][gG][bB][sS]0[dD][hH]
-	| [xX][nN]--[sS]9[bB][rR][jJ]9[cC]
-	| [xX][nN]--[wW][gG][bB][hH]1[cC]
-	| [xX][nN]--[wW][gG][bB][lL]6[aA]
-	| [xX][nN]--[xX][kK][cC]2[aA][lL]3[hH][yY][eE]2[aA]
-	| [xX][nN]--[xX][kK][cC]2[dD][lL]3[aA]5[eE][eE]0[hH]
-	| [xX][nN]--[yY][fF][rR][oO]4[iI]67[oO]
-	| [xX][nN]--[yY][gG][bB][iI]2[aA][mM][mM][xX]
-	| [xX][nN]--[zZ][cC][kK][zZ][aA][hH]
-	| [yY][eE]
-	| [yY][tT]
-	| [zZ][aA]
-	| [zZ][mM]
-	| [zZ][wW]
-	) "."?   // Accept trailing root (empty) domain
-
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/SUPPLEMENTARY.jflex-macro b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/SUPPLEMENTARY.jflex-macro
deleted file mode 100644
index c505bf4..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/SUPPLEMENTARY.jflex-macro
+++ /dev/null
@@ -1,125 +0,0 @@
-/*
- * Copyright 2010 The Apache Software Foundation.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-// Generated using ICU4J 4.6.0.0 on Wednesday, February 9, 2011 4:45:11 PM UTC
-// by org.apache.lucene.analysis.icu.GenerateJFlexSupplementaryMacros
-
-
-ALetterSupp = (
-	  ([\ud80d][\uDC00-\uDC2E])
-	| ([\ud80c][\uDC00-\uDFFF])
-	| ([\ud809][\uDC00-\uDC62])
-	| ([\ud808][\uDC00-\uDF6E])
-	| ([\ud81a][\uDC00-\uDE38])
-	| ([\ud804][\uDC03-\uDC37\uDC83-\uDCAF])
-	| ([\ud835][\uDC00-\uDC54\uDC56-\uDC9C\uDC9E\uDC9F\uDCA2\uDCA5\uDCA6\uDCA9-\uDCAC\uDCAE-\uDCB9\uDCBB\uDCBD-\uDCC3\uDCC5-\uDD05\uDD07-\uDD0A\uDD0D-\uDD14\uDD16-\uDD1C\uDD1E-\uDD39\uDD3B-\uDD3E\uDD40-\uDD44\uDD46\uDD4A-\uDD50\uDD52-\uDEA5\uDEA8-\uDEC0\uDEC2-\uDEDA\uDEDC-\uDEFA\uDEFC-\uDF14\uDF16-\uDF34\uDF36-\uDF4E\uDF50-\uDF6E\uDF70-\uDF88\uDF8A-\uDFA8\uDFAA-\uDFC2\uDFC4-\uDFCB])
-	| ([\ud801][\uDC00-\uDC9D])
-	| ([\ud800][\uDC00-\uDC0B\uDC0D-\uDC26\uDC28-\uDC3A\uDC3C\uDC3D\uDC3F-\uDC4D\uDC50-\uDC5D\uDC80-\uDCFA\uDD40-\uDD74\uDE80-\uDE9C\uDEA0-\uDED0\uDF00-\uDF1E\uDF30-\uDF4A\uDF80-\uDF9D\uDFA0-\uDFC3\uDFC8-\uDFCF\uDFD1-\uDFD5])
-	| ([\ud803][\uDC00-\uDC48])
-	| ([\ud802][\uDC00-\uDC05\uDC08\uDC0A-\uDC35\uDC37\uDC38\uDC3C\uDC3F-\uDC55\uDD00-\uDD15\uDD20-\uDD39\uDE00\uDE10-\uDE13\uDE15-\uDE17\uDE19-\uDE33\uDE60-\uDE7C\uDF00-\uDF35\uDF40-\uDF55\uDF60-\uDF72])
-)
-FormatSupp = (
-	  ([\ud804][\uDCBD])
-	| ([\ud834][\uDD73-\uDD7A])
-	| ([\udb40][\uDC01\uDC20-\uDC7F])
-)
-ExtendSupp = (
-	  ([\ud804][\uDC00-\uDC02\uDC38-\uDC46\uDC80-\uDC82\uDCB0-\uDCBA])
-	| ([\ud834][\uDD65-\uDD69\uDD6D-\uDD72\uDD7B-\uDD82\uDD85-\uDD8B\uDDAA-\uDDAD\uDE42-\uDE44])
-	| ([\ud800][\uDDFD])
-	| ([\udb40][\uDD00-\uDDEF])
-	| ([\ud802][\uDE01-\uDE03\uDE05\uDE06\uDE0C-\uDE0F\uDE38-\uDE3A\uDE3F])
-)
-NumericSupp = (
-	  ([\ud804][\uDC66-\uDC6F])
-	| ([\ud835][\uDFCE-\uDFFF])
-	| ([\ud801][\uDCA0-\uDCA9])
-)
-KatakanaSupp = (
-	  ([\ud82c][\uDC00])
-)
-MidLetterSupp = (
-	  []
-)
-MidNumSupp = (
-	  []
-)
-MidNumLetSupp = (
-	  []
-)
-ExtendNumLetSupp = (
-	  []
-)
-ExtendNumLetSupp = (
-	  []
-)
-ComplexContextSupp = (
-	  []
-)
-HanSupp = (
-	  ([\ud87e][\uDC00-\uDE1D])
-	| ([\ud86b][\uDC00-\uDFFF])
-	| ([\ud86a][\uDC00-\uDFFF])
-	| ([\ud869][\uDC00-\uDED6\uDF00-\uDFFF])
-	| ([\ud868][\uDC00-\uDFFF])
-	| ([\ud86e][\uDC00-\uDC1D])
-	| ([\ud86d][\uDC00-\uDF34\uDF40-\uDFFF])
-	| ([\ud86c][\uDC00-\uDFFF])
-	| ([\ud863][\uDC00-\uDFFF])
-	| ([\ud862][\uDC00-\uDFFF])
-	| ([\ud861][\uDC00-\uDFFF])
-	| ([\ud860][\uDC00-\uDFFF])
-	| ([\ud867][\uDC00-\uDFFF])
-	| ([\ud866][\uDC00-\uDFFF])
-	| ([\ud865][\uDC00-\uDFFF])
-	| ([\ud864][\uDC00-\uDFFF])
-	| ([\ud858][\uDC00-\uDFFF])
-	| ([\ud859][\uDC00-\uDFFF])
-	| ([\ud85a][\uDC00-\uDFFF])
-	| ([\ud85b][\uDC00-\uDFFF])
-	| ([\ud85c][\uDC00-\uDFFF])
-	| ([\ud85d][\uDC00-\uDFFF])
-	| ([\ud85e][\uDC00-\uDFFF])
-	| ([\ud85f][\uDC00-\uDFFF])
-	| ([\ud850][\uDC00-\uDFFF])
-	| ([\ud851][\uDC00-\uDFFF])
-	| ([\ud852][\uDC00-\uDFFF])
-	| ([\ud853][\uDC00-\uDFFF])
-	| ([\ud854][\uDC00-\uDFFF])
-	| ([\ud855][\uDC00-\uDFFF])
-	| ([\ud856][\uDC00-\uDFFF])
-	| ([\ud857][\uDC00-\uDFFF])
-	| ([\ud849][\uDC00-\uDFFF])
-	| ([\ud848][\uDC00-\uDFFF])
-	| ([\ud84b][\uDC00-\uDFFF])
-	| ([\ud84a][\uDC00-\uDFFF])
-	| ([\ud84d][\uDC00-\uDFFF])
-	| ([\ud84c][\uDC00-\uDFFF])
-	| ([\ud84f][\uDC00-\uDFFF])
-	| ([\ud84e][\uDC00-\uDFFF])
-	| ([\ud841][\uDC00-\uDFFF])
-	| ([\ud840][\uDC00-\uDFFF])
-	| ([\ud843][\uDC00-\uDFFF])
-	| ([\ud842][\uDC00-\uDFFF])
-	| ([\ud845][\uDC00-\uDFFF])
-	| ([\ud844][\uDC00-\uDFFF])
-	| ([\ud847][\uDC00-\uDFFF])
-	| ([\ud846][\uDC00-\uDFFF])
-)
-HiraganaSupp = (
-	  ([\ud83c][\uDE00])
-	| ([\ud82c][\uDC01])
-)
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/StandardTokenizerImpl31.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/StandardTokenizerImpl31.java
deleted file mode 100644
index ab2b9c2..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/StandardTokenizerImpl31.java
+++ /dev/null
@@ -1,1089 +0,0 @@
-/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 9/30/11 12:10 PM */
-
-package org.apache.lucene.analysis.standard.std31;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.standard.StandardTokenizer;
-import org.apache.lucene.analysis.standard.StandardTokenizerInterface;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-
-/**
- * This class implements StandardTokenizer, except with a bug 
- * (https://issues.apache.org/jira/browse/LUCENE-3358) where Han and Hiragana
- * characters would be split from combining characters:
- * @deprecated This class is only for exact backwards compatibility
- */
-@Deprecated
-
-/**
- * This class is a scanner generated by 
- * <a href="http://www.jflex.de/">JFlex</a> 1.5.0-SNAPSHOT
- * on 9/30/11 12:10 PM from the specification file
- * <tt>/lucene/jflex/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/StandardTokenizerImpl31.jflex</tt>
- */
-public final class StandardTokenizerImpl31 implements StandardTokenizerInterface {
-
-  /** This character denotes the end of file */
-  public static final int YYEOF = -1;
-
-  /** initial size of the lookahead buffer */
-  private static final int ZZ_BUFFERSIZE = 16384;
-
-  /** lexical states */
-  public static final int YYINITIAL = 0;
-
-  /**
-   * ZZ_LEXSTATE[l] is the state in the DFA for the lexical state l
-   * ZZ_LEXSTATE[l+1] is the state in the DFA for the lexical state l
-   *                  at the beginning of a line
-   * l is of the form l = 2*k, k a non negative integer
-   */
-  private static final int ZZ_LEXSTATE[] = { 
-     0, 0
-  };
-
-  /** 
-   * Translates characters to character classes
-   */
-  private static final String ZZ_CMAP_PACKED = 
-    "\47\0\1\140\4\0\1\137\1\0\1\140\1\0\12\134\1\136\1\137"+
-    "\5\0\32\132\4\0\1\141\1\0\32\132\57\0\1\132\2\0\1\133"+
-    "\7\0\1\132\1\0\1\136\2\0\1\132\5\0\27\132\1\0\37\132"+
-    "\1\0\u01ca\132\4\0\14\132\16\0\5\132\7\0\1\132\1\0\1\132"+
-    "\21\0\160\133\5\132\1\0\2\132\2\0\4\132\1\137\7\0\1\132"+
-    "\1\136\3\132\1\0\1\132\1\0\24\132\1\0\123\132\1\0\213\132"+
-    "\1\0\7\133\236\132\11\0\46\132\2\0\1\132\7\0\47\132\1\0"+
-    "\1\137\7\0\55\133\1\0\1\133\1\0\2\133\1\0\2\133\1\0"+
-    "\1\133\10\0\33\132\5\0\4\132\1\136\13\0\4\133\10\0\2\137"+
-    "\2\0\13\133\5\0\53\132\25\133\12\134\1\0\1\134\1\137\1\0"+
-    "\2\132\1\133\143\132\1\0\1\132\7\133\1\133\1\0\6\133\2\132"+
-    "\2\133\1\0\4\133\2\132\12\134\3\132\2\0\1\132\17\0\1\133"+
-    "\1\132\1\133\36\132\33\133\2\0\131\132\13\133\1\132\16\0\12\134"+
-    "\41\132\11\133\2\132\2\0\1\137\1\0\1\132\5\0\26\132\4\133"+
-    "\1\132\11\133\1\132\3\133\1\132\5\133\22\0\31\132\3\133\244\0"+
-    "\4\133\66\132\3\133\1\132\22\133\1\132\7\133\12\132\2\133\2\0"+
-    "\12\134\1\0\7\132\1\0\7\132\1\0\3\133\1\0\10\132\2\0"+
-    "\2\132\2\0\26\132\1\0\7\132\1\0\1\132\3\0\4\132\2\0"+
-    "\1\133\1\132\7\133\2\0\2\133\2\0\3\133\1\132\10\0\1\133"+
-    "\4\0\2\132\1\0\3\132\2\133\2\0\12\134\2\132\17\0\3\133"+
-    "\1\0\6\132\4\0\2\132\2\0\26\132\1\0\7\132\1\0\2\132"+
-    "\1\0\2\132\1\0\2\132\2\0\1\133\1\0\5\133\4\0\2\133"+
-    "\2\0\3\133\3\0\1\133\7\0\4\132\1\0\1\132\7\0\12\134"+
-    "\2\133\3\132\1\133\13\0\3\133\1\0\11\132\1\0\3\132\1\0"+
-    "\26\132\1\0\7\132\1\0\2\132\1\0\5\132\2\0\1\133\1\132"+
-    "\10\133\1\0\3\133\1\0\3\133\2\0\1\132\17\0\2\132\2\133"+
-    "\2\0\12\134\21\0\3\133\1\0\10\132\2\0\2\132\2\0\26\132"+
-    "\1\0\7\132\1\0\2\132\1\0\5\132\2\0\1\133\1\132\7\133"+
-    "\2\0\2\133\2\0\3\133\10\0\2\133\4\0\2\132\1\0\3\132"+
-    "\2\133\2\0\12\134\1\0\1\132\20\0\1\133\1\132\1\0\6\132"+
-    "\3\0\3\132\1\0\4\132\3\0\2\132\1\0\1\132\1\0\2\132"+
-    "\3\0\2\132\3\0\3\132\3\0\14\132\4\0\5\133\3\0\3\133"+
-    "\1\0\4\133\2\0\1\132\6\0\1\133\16\0\12\134\21\0\3\133"+
-    "\1\0\10\132\1\0\3\132\1\0\27\132\1\0\12\132\1\0\5\132"+
-    "\3\0\1\132\7\133\1\0\3\133\1\0\4\133\7\0\2\133\1\0"+
-    "\2\132\6\0\2\132\2\133\2\0\12\134\22\0\2\133\1\0\10\132"+
-    "\1\0\3\132\1\0\27\132\1\0\12\132\1\0\5\132\2\0\1\133"+
-    "\1\132\7\133\1\0\3\133\1\0\4\133\7\0\2\133\7\0\1\132"+
-    "\1\0\2\132\2\133\2\0\12\134\1\0\2\132\17\0\2\133\1\0"+
-    "\10\132\1\0\3\132\1\0\51\132\2\0\1\132\7\133\1\0\3\133"+
-    "\1\0\4\133\1\132\10\0\1\133\10\0\2\132\2\133\2\0\12\134"+
-    "\12\0\6\132\2\0\2\133\1\0\22\132\3\0\30\132\1\0\11\132"+
-    "\1\0\1\132\2\0\7\132\3\0\1\133\4\0\6\133\1\0\1\133"+
-    "\1\0\10\133\22\0\2\133\15\0\60\142\1\143\2\142\7\143\5\0"+
-    "\7\142\10\143\1\0\12\134\47\0\2\142\1\0\1\142\2\0\2\142"+
-    "\1\0\1\142\2\0\1\142\6\0\4\142\1\0\7\142\1\0\3\142"+
-    "\1\0\1\142\1\0\1\142\2\0\2\142\1\0\4\142\1\143\2\142"+
-    "\6\143\1\0\2\143\1\142\2\0\5\142\1\0\1\142\1\0\6\143"+
-    "\2\0\12\134\2\0\2\142\42\0\1\132\27\0\2\133\6\0\12\134"+
-    "\13\0\1\133\1\0\1\133\1\0\1\133\4\0\2\133\10\132\1\0"+
-    "\44\132\4\0\24\133\1\0\2\133\5\132\13\133\1\0\44\133\11\0"+
-    "\1\133\71\0\53\142\24\143\1\142\12\134\6\0\6\142\4\143\4\142"+
-    "\3\143\1\142\3\143\2\142\7\143\3\142\4\143\15\142\14\143\1\142"+
-    "\1\143\12\134\4\143\2\142\46\132\12\0\53\132\1\0\1\132\3\0"+
-    "\u0100\146\111\132\1\0\4\132\2\0\7\132\1\0\1\132\1\0\4\132"+
-    "\2\0\51\132\1\0\4\132\2\0\41\132\1\0\4\132\2\0\7\132"+
-    "\1\0\1\132\1\0\4\132\2\0\17\132\1\0\71\132\1\0\4\132"+
-    "\2\0\103\132\2\0\3\133\40\0\20\132\20\0\125\132\14\0\u026c\132"+
-    "\2\0\21\132\1\0\32\132\5\0\113\132\3\0\3\132\17\0\15\132"+
-    "\1\0\4\132\3\133\13\0\22\132\3\133\13\0\22\132\2\133\14\0"+
-    "\15\132\1\0\3\132\1\0\2\133\14\0\64\142\2\143\36\143\3\0"+
-    "\1\142\4\0\1\142\1\143\2\0\12\134\41\0\3\133\2\0\12\134"+
-    "\6\0\130\132\10\0\51\132\1\133\1\132\5\0\106\132\12\0\35\132"+
-    "\3\0\14\133\4\0\14\133\12\0\12\134\36\142\2\0\5\142\13\0"+
-    "\54\142\4\0\21\143\7\142\2\143\6\0\12\134\1\142\3\0\2\142"+
-    "\40\0\27\132\5\133\4\0\65\142\12\143\1\0\35\143\2\0\1\133"+
-    "\12\134\6\0\12\134\6\0\16\142\122\0\5\133\57\132\21\133\7\132"+
-    "\4\0\12\134\21\0\11\133\14\0\3\133\36\132\12\133\3\0\2\132"+
-    "\12\134\6\0\46\132\16\133\14\0\44\132\24\133\10\0\12\134\3\0"+
-    "\3\132\12\134\44\132\122\0\3\133\1\0\25\133\4\132\1\133\4\132"+
-    "\1\133\15\0\300\132\47\133\25\0\4\133\u0116\132\2\0\6\132\2\0"+
-    "\46\132\2\0\6\132\2\0\10\132\1\0\1\132\1\0\1\132\1\0"+
-    "\1\132\1\0\37\132\2\0\65\132\1\0\7\132\1\0\1\132\3\0"+
-    "\3\132\1\0\7\132\3\0\4\132\2\0\6\132\4\0\15\132\5\0"+
-    "\3\132\1\0\7\132\17\0\2\133\2\133\10\0\2\140\12\0\1\140"+
-    "\2\0\1\136\2\0\5\133\20\0\2\141\3\0\1\137\17\0\1\141"+
-    "\13\0\5\133\5\0\6\133\1\0\1\132\15\0\1\132\20\0\15\132"+
-    "\63\0\41\133\21\0\1\132\4\0\1\132\2\0\12\132\1\0\1\132"+
-    "\3\0\5\132\6\0\1\132\1\0\1\132\1\0\1\132\1\0\4\132"+
-    "\1\0\13\132\2\0\4\132\5\0\5\132\4\0\1\132\21\0\51\132"+
-    "\u032d\0\64\132\u0716\0\57\132\1\0\57\132\1\0\205\132\6\0\4\132"+
-    "\3\133\16\0\46\132\12\0\66\132\11\0\1\132\17\0\1\133\27\132"+
-    "\11\0\7\132\1\0\7\132\1\0\7\132\1\0\7\132\1\0\7\132"+
-    "\1\0\7\132\1\0\7\132\1\0\7\132\1\0\40\133\57\0\1\132"+
-    "\120\0\32\144\1\0\131\144\14\0\326\144\57\0\1\132\1\0\1\144"+
-    "\31\0\11\144\4\133\2\133\1\0\5\135\2\0\3\144\1\132\1\132"+
-    "\4\0\126\145\2\0\2\133\2\135\3\145\133\135\1\0\4\135\5\0"+
-    "\51\132\3\0\136\146\21\0\33\132\65\0\20\135\37\0\101\0\37\0"+
-    "\121\0\57\135\1\0\130\135\250\0\u19b6\144\112\0\u51cc\144\64\0\u048d\132"+
-    "\103\0\56\132\2\0\u010d\132\3\0\20\132\12\134\2\132\24\0\57\132"+
-    "\4\133\11\0\2\133\1\0\31\132\10\0\120\132\2\133\45\0\11\132"+
-    "\2\0\147\132\2\0\4\132\1\0\2\132\16\0\12\132\120\0\10\132"+
-    "\1\133\3\132\1\133\4\132\1\133\27\132\5\133\30\0\64\132\14\0"+
-    "\2\133\62\132\21\133\13\0\12\134\6\0\22\133\6\132\3\0\1\132"+
-    "\4\0\12\134\34\132\10\133\2\0\27\132\15\133\14\0\35\146\3\0"+
-    "\4\133\57\132\16\133\16\0\1\132\12\134\46\0\51\132\16\133\11\0"+
-    "\3\132\1\133\10\132\2\133\2\0\12\134\6\0\33\142\1\143\4\0"+
-    "\60\142\1\143\1\142\3\143\2\142\2\143\5\142\2\143\1\142\1\143"+
-    "\1\142\30\0\5\142\41\0\6\132\2\0\6\132\2\0\6\132\11\0"+
-    "\7\132\1\0\7\132\221\0\43\132\10\133\1\0\2\133\2\0\12\134"+
-    "\6\0\u2ba4\146\14\0\27\146\4\0\61\146\4\0\1\31\1\25\1\46"+
-    "\1\43\1\13\3\0\1\7\1\5\2\0\1\3\1\1\14\0\1\11"+
-    "\21\0\1\112\7\0\1\65\1\17\6\0\1\130\3\0\1\120\1\120"+
-    "\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120"+
-    "\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120"+
-    "\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120"+
-    "\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\121"+
-    "\1\120\1\120\1\120\1\125\1\123\17\0\1\114\u02c1\0\1\70\277\0"+
-    "\1\113\1\71\1\2\3\124\2\35\1\124\1\35\2\124\1\14\21\124"+
-    "\2\60\7\73\1\72\7\73\7\52\1\15\1\52\1\75\2\45\1\44"+
-    "\1\75\1\45\1\44\10\75\2\63\5\61\2\54\5\61\1\6\10\37"+
-    "\5\21\3\27\12\106\20\27\3\42\32\30\1\26\2\24\2\110\1\111"+
-    "\2\110\2\111\2\110\1\111\3\24\1\16\2\24\12\64\1\74\1\41"+
-    "\1\34\1\64\6\41\1\34\66\41\5\115\6\103\1\51\4\103\2\51"+
-    "\10\103\1\51\7\100\1\12\2\100\32\103\1\12\4\100\1\12\5\102"+
-    "\1\101\1\102\3\101\7\102\1\101\23\102\5\67\3\102\6\67\2\67"+
-    "\6\66\10\66\2\100\7\66\36\100\4\66\102\100\15\115\1\77\2\115"+
-    "\1\131\3\117\1\115\2\117\5\115\4\117\4\116\1\115\3\116\1\115"+
-    "\5\116\26\56\4\23\1\105\2\104\4\122\1\104\2\122\3\76\33\122"+
-    "\35\55\3\122\35\126\3\122\6\126\2\33\31\126\1\33\17\126\6\122"+
-    "\4\22\1\10\37\22\1\10\4\22\25\62\1\127\11\62\21\55\5\62"+
-    "\1\57\12\40\13\62\4\55\1\50\6\55\12\122\17\55\1\47\3\53"+
-    "\15\20\11\36\1\32\24\36\2\20\11\36\1\32\31\36\1\32\4\20"+
-    "\4\36\2\32\2\107\1\4\5\107\52\4\u1900\0\u012e\144\2\0\76\144"+
-    "\2\0\152\144\46\0\7\132\14\0\5\132\5\0\1\132\1\133\12\132"+
-    "\1\0\15\132\1\0\5\132\1\0\1\132\1\0\2\132\1\0\2\132"+
-    "\1\0\154\132\41\0\u016b\132\22\0\100\132\2\0\66\132\50\0\14\132"+
-    "\4\0\20\133\1\137\2\0\1\136\1\137\13\0\7\133\14\0\2\141"+
-    "\30\0\3\141\1\137\1\0\1\140\1\0\1\137\1\136\32\0\5\132"+
-    "\1\0\207\132\2\0\1\133\7\0\1\140\4\0\1\137\1\0\1\140"+
-    "\1\0\12\134\1\136\1\137\5\0\32\132\4\0\1\141\1\0\32\132"+
-    "\13\0\70\135\2\133\37\146\3\0\6\146\2\0\6\146\2\0\6\146"+
-    "\2\0\3\146\34\0\3\133\4\0";
-
-  /** 
-   * Translates characters to character classes
-   */
-  private static final char [] ZZ_CMAP = zzUnpackCMap(ZZ_CMAP_PACKED);
-
-  /** 
-   * Translates DFA states to action switch labels.
-   */
-  private static final int [] ZZ_ACTION = zzUnpackAction();
-
-  private static final String ZZ_ACTION_PACKED_0 =
-    "\1\0\23\1\1\2\1\3\1\4\1\1\1\5\1\6"+
-    "\1\7\1\10\15\0\1\2\1\0\1\2\10\0\1\3"+
-    "\15\0\1\2\57\0";
-
-  private static int [] zzUnpackAction() {
-    int [] result = new int[114];
-    int offset = 0;
-    offset = zzUnpackAction(ZZ_ACTION_PACKED_0, offset, result);
-    return result;
-  }
-
-  private static int zzUnpackAction(String packed, int offset, int [] result) {
-    int i = 0;       /* index in packed string  */
-    int j = offset;  /* index in unpacked array */
-    int l = packed.length();
-    while (i < l) {
-      int count = packed.charAt(i++);
-      int value = packed.charAt(i++);
-      do result[j++] = value; while (--count > 0);
-    }
-    return j;
-  }
-
-
-  /** 
-   * Translates a state to a row index in the transition table
-   */
-  private static final int [] ZZ_ROWMAP = zzUnpackRowMap();
-
-  private static final String ZZ_ROWMAP_PACKED_0 =
-    "\0\0\0\147\0\316\0\u0135\0\u019c\0\u0203\0\u026a\0\u02d1"+
-    "\0\u0338\0\u039f\0\u0406\0\u046d\0\u04d4\0\u053b\0\u05a2\0\u0609"+
-    "\0\u0670\0\u06d7\0\u073e\0\u07a5\0\u080c\0\u0873\0\u08da\0\u0941"+
-    "\0\u09a8\0\147\0\147\0\u0a0f\0\316\0\u0135\0\u019c\0\u0203"+
-    "\0\u026a\0\u0a76\0\u0add\0\u0b44\0\u0bab\0\u046d\0\u0c12\0\u0c79"+
-    "\0\u0ce0\0\u0d47\0\u0dae\0\u0e15\0\u0e7c\0\u0338\0\u039f\0\u0ee3"+
-    "\0\u0f4a\0\u0fb1\0\u1018\0\u107f\0\u10e6\0\u114d\0\u11b4\0\u121b"+
-    "\0\u1282\0\u12e9\0\u1350\0\u13b7\0\u141e\0\u1485\0\u14ec\0\u1553"+
-    "\0\u15ba\0\u0941\0\u1621\0\u1688\0\u16ef\0\u1756\0\u17bd\0\u1824"+
-    "\0\u188b\0\u18f2\0\u1959\0\u19c0\0\u1a27\0\u1a8e\0\u1af5\0\u1b5c"+
-    "\0\u1bc3\0\u1c2a\0\u1c91\0\u1cf8\0\u1d5f\0\u1dc6\0\u1e2d\0\u1e94"+
-    "\0\u1efb\0\u1f62\0\u1fc9\0\u2030\0\u2097\0\u20fe\0\u2165\0\u21cc"+
-    "\0\u2233\0\u229a\0\u2301\0\u2368\0\u23cf\0\u2436\0\u249d\0\u2504"+
-    "\0\u256b\0\u25d2\0\u2639\0\u26a0\0\u2707\0\u276e\0\u27d5\0\u283c"+
-    "\0\u28a3\0\u290a";
-
-  private static int [] zzUnpackRowMap() {
-    int [] result = new int[114];
-    int offset = 0;
-    offset = zzUnpackRowMap(ZZ_ROWMAP_PACKED_0, offset, result);
-    return result;
-  }
-
-  private static int zzUnpackRowMap(String packed, int offset, int [] result) {
-    int i = 0;  /* index in packed string  */
-    int j = offset;  /* index in unpacked array */
-    int l = packed.length();
-    while (i < l) {
-      int high = packed.charAt(i++) << 16;
-      result[j++] = high | packed.charAt(i++);
-    }
-    return j;
-  }
-
-  /** 
-   * The transition table of the DFA
-   */
-  private static final int [] ZZ_TRANS = zzUnpackTrans();
-
-  private static final String ZZ_TRANS_PACKED_0 =
-    "\1\2\1\3\1\2\1\4\1\2\1\5\1\2\1\6"+
-    "\1\2\1\7\1\2\1\10\3\2\1\11\5\2\1\12"+
-    "\3\2\1\13\11\2\1\14\2\2\1\15\43\2\1\16"+
-    "\1\2\1\17\3\2\1\20\1\21\1\2\1\22\1\2"+
-    "\1\23\2\2\1\24\1\2\1\25\1\2\1\26\1\27"+
-    "\3\2\1\30\2\31\1\32\1\33\1\34\151\0\1\25"+
-    "\11\0\1\25\20\0\1\25\22\0\1\25\10\0\3\25"+
-    "\17\0\1\25\10\0\1\25\24\0\1\25\1\0\1\25"+
-    "\1\0\1\25\1\0\1\25\1\0\1\25\1\0\3\25"+
-    "\1\0\5\25\1\0\3\25\1\0\11\25\1\0\2\25"+
-    "\1\0\16\25\1\0\2\25\1\0\21\25\1\0\1\25"+
-    "\1\0\3\25\2\0\1\25\1\0\1\25\1\0\2\25"+
-    "\1\0\1\25\17\0\1\25\3\0\1\25\5\0\2\25"+
-    "\3\0\1\25\13\0\1\25\1\0\1\25\4\0\2\25"+
-    "\4\0\1\25\1\0\1\25\3\0\2\25\1\0\1\25"+
-    "\5\0\3\25\1\0\1\25\15\0\1\25\10\0\1\25"+
-    "\24\0\1\25\3\0\1\25\1\0\1\25\1\0\1\25"+
-    "\1\0\3\25\2\0\4\25\1\0\3\25\2\0\3\25"+
-    "\1\0\4\25\1\0\2\25\2\0\3\25\1\0\11\25"+
-    "\1\0\2\25\1\0\16\25\1\0\2\25\1\0\1\25"+
-    "\1\0\3\25\2\0\1\25\1\0\1\25\1\0\2\25"+
-    "\1\0\1\25\17\0\1\25\3\0\1\25\3\0\1\25"+
-    "\1\0\3\25\2\0\1\25\1\0\2\25\1\0\3\25"+
-    "\3\0\2\25\1\0\1\25\1\0\2\25\1\0\2\25"+
-    "\3\0\2\25\1\0\1\25\1\0\1\25\1\0\2\25"+
-    "\1\0\2\25\1\0\2\25\1\0\5\25\1\0\5\25"+
-    "\1\0\2\25\1\0\2\25\1\0\1\25\1\0\3\25"+
-    "\4\0\1\25\4\0\1\25\31\0\3\25\5\0\1\25"+
-    "\1\0\1\25\1\0\1\25\4\0\1\25\14\0\1\25"+
-    "\5\0\1\25\11\0\2\25\12\0\1\26\1\0\2\25"+
-    "\12\0\1\25\24\0\1\25\1\0\1\26\7\0\2\25"+
-    "\2\0\5\25\2\0\2\25\4\0\6\25\1\0\2\25"+
-    "\4\0\5\25\1\0\5\25\1\0\2\25\1\0\3\25"+
-    "\1\0\4\25\1\0\5\25\1\26\1\0\1\25\1\0"+
-    "\1\25\1\0\3\25\2\0\1\25\1\0\1\25\1\0"+
-    "\1\25\2\0\1\25\17\0\1\25\3\0\1\25\5\0"+
-    "\2\25\3\0\1\25\4\0\3\25\4\0\1\25\1\0"+
-    "\1\25\2\0\1\25\1\0\2\25\4\0\1\25\1\0"+
-    "\1\25\3\0\2\25\1\0\1\25\5\0\3\25\1\0"+
-    "\1\25\10\0\1\25\1\0\2\26\1\0\1\25\10\0"+
-    "\1\25\24\0\1\25\3\0\1\25\6\0\2\25\5\0"+
-    "\1\25\1\0\1\25\1\0\1\25\1\0\11\25\2\0"+
-    "\1\25\4\0\1\25\4\0\6\25\2\0\1\25\1\0"+
-    "\1\25\1\0\3\25\3\0\2\25\4\0\3\25\1\0"+
-    "\1\25\10\0\1\25\1\0\2\25\21\0\1\25\11\0"+
-    "\2\25\17\0\1\25\6\0\2\25\4\0\1\25\5\0"+
-    "\1\25\2\0\1\25\5\0\3\25\1\0\1\25\15\0"+
-    "\1\25\10\0\1\25\24\0\1\25\3\0\1\25\5\0"+
-    "\1\25\32\0\15\25\5\0\3\25\1\0\1\25\5\0"+
-    "\1\25\7\0\1\25\2\0\1\25\5\0\1\25\2\0"+
-    "\1\25\1\0\1\25\106\0\1\33\21\0\1\27\35\0"+
-    "\1\32\3\0\1\32\3\0\1\32\1\0\3\32\2\0"+
-    "\1\32\2\0\1\32\1\0\3\32\3\0\2\32\1\0"+
-    "\1\32\1\0\2\32\1\0\2\32\3\0\2\32\1\0"+
-    "\1\32\3\0\2\32\1\0\2\32\1\0\2\32\1\0"+
-    "\5\32\1\0\5\32\2\0\1\32\1\0\2\32\1\0"+
-    "\1\32\1\0\3\32\4\0\1\32\4\0\1\32\17\0"+
-    "\1\32\1\0\1\32\1\0\1\32\1\0\1\32\1\0"+
-    "\1\32\1\0\3\32\1\0\5\32\1\0\3\32\1\0"+
-    "\11\32\1\0\2\32\1\0\16\32\1\0\2\32\1\0"+
-    "\21\32\1\0\1\32\1\0\3\32\2\0\1\32\1\0"+
-    "\1\32\1\0\2\32\1\0\1\32\17\0\1\32\1\0"+
-    "\1\32\1\0\1\32\3\0\1\32\1\0\3\32\1\0"+
-    "\2\32\1\0\2\32\1\0\3\32\1\0\11\32\1\0"+
-    "\2\32\1\0\16\32\1\0\2\32\1\0\21\32\1\0"+
-    "\1\32\1\0\3\32\2\0\1\32\1\0\1\32\1\0"+
-    "\2\32\1\0\1\32\17\0\1\32\11\0\1\32\20\0"+
-    "\1\32\33\0\1\32\21\0\1\32\10\0\1\32\24\0"+
-    "\1\32\1\0\1\32\1\0\1\32\1\0\1\32\1\0"+
-    "\1\32\1\0\3\32\1\0\5\32\1\0\3\32\1\0"+
-    "\6\32\1\0\2\32\1\0\2\32\1\0\10\32\1\0"+
-    "\5\32\1\0\2\32\1\0\21\32\1\0\1\32\1\0"+
-    "\3\32\2\0\1\32\1\0\1\32\1\0\2\32\1\0"+
-    "\1\32\146\0\1\33\16\0\1\35\1\0\1\36\1\0"+
-    "\1\37\1\0\1\40\1\0\1\41\1\0\1\42\3\0"+
-    "\1\43\5\0\1\44\3\0\1\45\11\0\1\46\2\0"+
-    "\1\47\16\0\1\50\2\0\1\51\41\0\2\25\1\52"+
-    "\1\0\1\53\1\0\1\53\1\54\1\0\1\25\2\0"+
-    "\1\25\1\0\1\35\1\0\1\36\1\0\1\37\1\0"+
-    "\1\40\1\0\1\41\1\0\1\55\3\0\1\56\5\0"+
-    "\1\57\3\0\1\60\11\0\1\46\2\0\1\61\16\0"+
-    "\1\62\2\0\1\63\41\0\1\25\2\26\2\0\2\64"+
-    "\1\65\1\0\1\26\2\0\1\25\13\0\1\66\15\0"+
-    "\1\67\14\0\1\70\16\0\1\71\2\0\1\72\21\0"+
-    "\1\73\20\0\1\27\1\0\1\27\3\0\1\54\1\0"+
-    "\1\27\4\0\1\35\1\0\1\36\1\0\1\37\1\0"+
-    "\1\40\1\0\1\41\1\0\1\74\3\0\1\56\5\0"+
-    "\1\57\3\0\1\75\11\0\1\46\2\0\1\76\16\0"+
-    "\1\77\2\0\1\100\21\0\1\101\17\0\1\25\1\102"+
-    "\1\26\1\103\3\0\1\102\1\0\1\102\2\0\1\25"+
-    "\142\0\2\31\4\0\1\35\1\0\1\36\1\0\1\37"+
-    "\1\0\1\40\1\0\1\41\1\0\1\104\3\0\1\43"+
-    "\5\0\1\44\3\0\1\105\11\0\1\46\2\0\1\106"+
-    "\16\0\1\107\2\0\1\110\41\0\1\25\1\34\1\52"+
-    "\1\0\1\53\1\0\1\53\1\54\1\0\1\34\2\0"+
-    "\1\34\2\0\1\25\11\0\3\25\5\0\1\25\1\0"+
-    "\1\25\1\0\1\25\4\0\1\25\4\0\1\25\1\0"+
-    "\2\25\4\0\1\25\5\0\1\25\3\0\1\25\4\0"+
-    "\5\25\10\0\1\52\1\0\2\25\1\0\1\25\10\0"+
-    "\1\25\24\0\1\25\1\0\1\52\7\0\2\25\2\0"+
-    "\5\25\2\0\2\25\4\0\6\25\1\0\2\25\4\0"+
-    "\5\25\1\0\5\25\1\0\2\25\1\0\3\25\1\0"+
-    "\4\25\1\0\5\25\1\52\1\0\1\25\1\0\1\25"+
-    "\1\0\3\25\2\0\1\25\1\0\1\25\1\0\1\25"+
-    "\2\0\1\25\17\0\1\25\3\0\1\25\5\0\2\25"+
-    "\3\0\1\25\4\0\3\25\4\0\1\25\1\0\1\25"+
-    "\2\0\1\25\1\0\2\25\4\0\1\25\1\0\1\25"+
-    "\3\0\2\25\1\0\1\25\5\0\3\25\1\0\1\25"+
-    "\10\0\1\25\1\0\2\52\1\0\1\25\10\0\1\25"+
-    "\24\0\1\25\3\0\1\25\6\0\2\25\5\0\1\25"+
-    "\1\0\1\25\1\0\1\25\1\0\11\25\2\0\1\25"+
-    "\4\0\1\25\4\0\6\25\2\0\1\25\1\0\1\25"+
-    "\1\0\3\25\1\0\1\25\1\0\2\25\4\0\3\25"+
-    "\1\0\1\25\10\0\1\25\1\0\2\25\21\0\1\25"+
-    "\3\0\1\25\5\0\1\25\32\0\15\25\5\0\3\25"+
-    "\1\0\1\25\5\0\3\25\5\0\1\25\2\0\2\25"+
-    "\4\0\1\25\2\0\1\25\1\0\1\25\103\0\2\25"+
-    "\6\0\1\25\56\0\1\25\3\0\1\25\2\0\1\25"+
-    "\3\0\1\25\5\0\1\25\7\0\1\25\4\0\2\25"+
-    "\3\0\2\25\1\0\1\25\4\0\1\25\1\0\1\25"+
-    "\2\0\2\25\1\0\3\25\1\0\1\25\2\0\4\25"+
-    "\2\0\1\25\41\0\1\35\1\0\1\36\1\0\1\37"+
-    "\1\0\1\40\1\0\1\41\1\0\1\111\3\0\1\43"+
-    "\5\0\1\44\3\0\1\112\11\0\1\46\2\0\1\113"+
-    "\16\0\1\114\2\0\1\115\41\0\1\25\2\52\2\0"+
-    "\2\116\1\54\1\0\1\52\2\0\1\25\1\0\1\35"+
-    "\1\0\1\36\1\0\1\37\1\0\1\40\1\0\1\41"+
-    "\1\0\1\117\3\0\1\120\5\0\1\121\3\0\1\122"+
-    "\11\0\1\46\2\0\1\123\16\0\1\124\2\0\1\125"+
-    "\41\0\1\25\1\53\7\0\1\53\2\0\1\25\1\0"+
-    "\1\35\1\0\1\36\1\0\1\37\1\0\1\40\1\0"+
-    "\1\41\1\0\1\126\3\0\1\43\5\0\1\44\3\0"+
-    "\1\127\11\0\1\46\2\0\1\130\16\0\1\131\2\0"+
-    "\1\132\21\0\1\101\17\0\1\25\1\54\1\52\1\103"+
-    "\3\0\1\54\1\0\1\54\2\0\1\25\2\0\1\26"+
-    "\11\0\3\25\5\0\1\25\1\0\1\25\1\0\1\25"+
-    "\4\0\1\25\4\0\1\26\1\0\2\26\4\0\1\25"+
-    "\5\0\1\25\3\0\1\26\4\0\1\26\2\25\2\26"+
-    "\10\0\1\26\1\0\2\25\1\0\1\26\10\0\1\25"+
-    "\24\0\1\25\3\0\1\25\6\0\2\25\5\0\1\25"+
-    "\1\0\1\25\1\0\1\25\1\0\11\25\2\0\1\25"+
-    "\4\0\1\25\4\0\6\25\2\0\1\25\1\0\1\25"+
-    "\1\0\3\25\1\0\1\26\1\0\2\25\4\0\3\25"+
-    "\1\0\1\25\10\0\1\25\1\0\2\25\21\0\1\25"+
-    "\3\0\1\25\5\0\1\25\32\0\15\25\5\0\3\25"+
-    "\1\0\1\25\5\0\1\25\2\26\5\0\1\25\2\0"+
-    "\1\25\1\26\4\0\1\25\2\0\1\25\1\0\1\25"+
-    "\103\0\2\26\6\0\1\26\56\0\1\26\3\0\1\26"+
-    "\2\0\1\26\3\0\1\26\5\0\1\26\7\0\1\26"+
-    "\4\0\2\26\3\0\2\26\1\0\1\26\4\0\1\26"+
-    "\1\0\1\26\2\0\2\26\1\0\3\26\1\0\1\26"+
-    "\2\0\4\26\2\0\1\26\53\0\1\133\3\0\1\134"+
-    "\5\0\1\135\3\0\1\136\14\0\1\137\16\0\1\140"+
-    "\2\0\1\141\42\0\1\64\1\26\6\0\1\64\4\0"+
-    "\1\35\1\0\1\36\1\0\1\37\1\0\1\40\1\0"+
-    "\1\41\1\0\1\142\3\0\1\56\5\0\1\57\3\0"+
-    "\1\143\11\0\1\46\2\0\1\144\16\0\1\145\2\0"+
-    "\1\146\21\0\1\101\17\0\1\25\1\65\1\26\1\103"+
-    "\3\0\1\65\1\0\1\65\2\0\1\25\2\0\1\27"+
-    "\37\0\1\27\1\0\2\27\16\0\1\27\4\0\1\27"+
-    "\2\0\2\27\15\0\1\27\132\0\1\27\153\0\2\27"+
-    "\11\0\1\27\115\0\2\27\6\0\1\27\56\0\1\27"+
-    "\3\0\1\27\2\0\1\27\3\0\1\27\5\0\1\27"+
-    "\7\0\1\27\4\0\2\27\3\0\2\27\1\0\1\27"+
-    "\4\0\1\27\1\0\1\27\2\0\2\27\1\0\3\27"+
-    "\1\0\1\27\2\0\4\27\2\0\1\27\153\0\1\27"+
-    "\35\0\1\102\11\0\3\25\5\0\1\25\1\0\1\25"+
-    "\1\0\1\25\4\0\1\25\4\0\1\102\1\0\2\102"+
-    "\4\0\1\25\5\0\1\25\3\0\1\102\4\0\1\102"+
-    "\2\25\2\102\10\0\1\26\1\0\2\25\1\0\1\102"+
-    "\10\0\1\25\24\0\1\25\3\0\1\25\6\0\2\25"+
-    "\5\0\1\25\1\0\1\25\1\0\1\25\1\0\11\25"+
-    "\2\0\1\25\4\0\1\25\4\0\6\25\2\0\1\25"+
-    "\1\0\1\25\1\0\3\25\1\0\1\102\1\0\2\25"+
-    "\4\0\3\25\1\0\1\25\10\0\1\25\1\0\2\25"+
-    "\21\0\1\25\3\0\1\25\5\0\1\25\32\0\15\25"+
-    "\5\0\3\25\1\0\1\25\5\0\1\25\2\102\5\0"+
-    "\1\25\2\0\1\25\1\102\4\0\1\25\2\0\1\25"+
-    "\1\0\1\25\103\0\2\102\6\0\1\102\56\0\1\102"+
-    "\3\0\1\102\2\0\1\102\3\0\1\102\5\0\1\102"+
-    "\7\0\1\102\4\0\2\102\3\0\2\102\1\0\1\102"+
-    "\4\0\1\102\1\0\1\102\2\0\2\102\1\0\3\102"+
-    "\1\0\1\102\2\0\4\102\2\0\1\102\153\0\1\103"+
-    "\46\0\1\147\15\0\1\150\14\0\1\151\16\0\1\152"+
-    "\2\0\1\153\21\0\1\101\20\0\1\103\1\0\1\103"+
-    "\3\0\1\54\1\0\1\103\5\0\1\34\11\0\3\25"+
-    "\5\0\1\25\1\0\1\25\1\0\1\25\4\0\1\25"+
-    "\4\0\1\34\1\0\2\34\4\0\1\25\5\0\1\25"+
-    "\3\0\1\34\4\0\1\34\2\25\2\34\10\0\1\52"+
-    "\1\0\2\25\1\0\1\34\10\0\1\25\24\0\1\25"+
-    "\3\0\1\25\6\0\2\25\5\0\1\25\1\0\1\25"+
-    "\1\0\1\25\1\0\11\25\2\0\1\25\4\0\1\25"+
-    "\4\0\6\25\2\0\1\25\1\0\1\25\1\0\3\25"+
-    "\1\0\1\34\1\0\2\25\4\0\3\25\1\0\1\25"+
-    "\10\0\1\25\1\0\2\25\21\0\1\25\3\0\1\25"+
-    "\5\0\1\25\32\0\15\25\5\0\3\25\1\0\1\25"+
-    "\5\0\1\25\2\34\5\0\1\25\2\0\1\25\1\34"+
-    "\4\0\1\25\2\0\1\25\1\0\1\25\103\0\2\34"+
-    "\6\0\1\34\56\0\1\34\3\0\1\34\2\0\1\34"+
-    "\3\0\1\34\5\0\1\34\7\0\1\34\4\0\2\34"+
-    "\3\0\2\34\1\0\1\34\4\0\1\34\1\0\1\34"+
-    "\2\0\2\34\1\0\3\34\1\0\1\34\2\0\4\34"+
-    "\2\0\1\34\42\0\1\52\11\0\3\25\5\0\1\25"+
-    "\1\0\1\25\1\0\1\25\4\0\1\25\4\0\1\52"+
-    "\1\0\2\52\4\0\1\25\5\0\1\25\3\0\1\52"+
-    "\4\0\1\52\2\25\2\52\10\0\1\52\1\0\2\25"+
-    "\1\0\1\52\10\0\1\25\24\0\1\25\3\0\1\25"+
-    "\6\0\2\25\5\0\1\25\1\0\1\25\1\0\1\25"+
-    "\1\0\11\25\2\0\1\25\4\0\1\25\4\0\6\25"+
-    "\2\0\1\25\1\0\1\25\1\0\3\25\1\0\1\52"+
-    "\1\0\2\25\4\0\3\25\1\0\1\25\10\0\1\25"+
-    "\1\0\2\25\21\0\1\25\3\0\1\25\5\0\1\25"+
-    "\32\0\15\25\5\0\3\25\1\0\1\25\5\0\1\25"+
-    "\2\52\5\0\1\25\2\0\1\25\1\52\4\0\1\25"+
-    "\2\0\1\25\1\0\1\25\103\0\2\52\6\0\1\52"+
-    "\56\0\1\52\3\0\1\52\2\0\1\52\3\0\1\52"+
-    "\5\0\1\52\7\0\1\52\4\0\2\52\3\0\2\52"+
-    "\1\0\1\52\4\0\1\52\1\0\1\52\2\0\2\52"+
-    "\1\0\3\52\1\0\1\52\2\0\4\52\2\0\1\52"+
-    "\53\0\1\154\3\0\1\155\5\0\1\156\3\0\1\157"+
-    "\14\0\1\160\16\0\1\161\2\0\1\162\42\0\1\116"+
-    "\1\52\6\0\1\116\5\0\1\53\11\0\3\25\5\0"+
-    "\1\25\1\0\1\25\1\0\1\25\4\0\1\25\4\0"+
-    "\1\53\1\0\2\53\4\0\1\25\5\0\1\25\3\0"+
-    "\1\53\4\0\1\53\2\25\2\53\12\0\2\25\1\0"+
-    "\1\53\10\0\1\25\24\0\1\25\11\0\2\25\2\0"+
-    "\5\25\2\0\2\25\4\0\6\25\1\0\2\25\4\0"+
-    "\5\25\1\0\5\25\1\0\2\25\1\0\3\25\1\0"+
-    "\4\25\1\0\5\25\2\0\1\25\1\0\1\25\1\0"+
-    "\3\25\2\0\1\25\1\0\1\25\1\0\1\25\2\0"+
-    "\1\25\17\0\1\25\3\0\1\25\5\0\2\25\3\0"+
-    "\1\25\4\0\3\25\4\0\1\25\1\0\1\25\2\0"+
-    "\1\25\1\0\2\25\4\0\1\25\1\0\1\25\3\0"+
-    "\2\25\1\0\1\25\5\0\3\25\1\0\1\25\10\0"+
-    "\1\25\4\0\1\25\10\0\1\25\24\0\1\25\3\0"+
-    "\1\25\6\0\2\25\5\0\1\25\1\0\1\25\1\0"+
-    "\1\25\1\0\11\25\2\0\1\25\4\0\1\25\4\0"+
-    "\6\25\2\0\1\25\1\0\1\25\1\0\3\25\1\0"+
-    "\1\53\1\0\2\25\4\0\3\25\1\0\1\25\10\0"+
-    "\1\25\1\0\2\25\21\0\1\25\3\0\1\25\5\0"+
-    "\1\25\32\0\15\25\5\0\3\25\1\0\1\25\5\0"+
-    "\1\25\2\53\5\0\1\25\2\0\1\25\1\53\4\0"+
-    "\1\25\2\0\1\25\1\0\1\25\103\0\2\53\6\0"+
-    "\1\53\56\0\1\53\3\0\1\53\2\0\1\53\3\0"+
-    "\1\53\5\0\1\53\7\0\1\53\4\0\2\53\3\0"+
-    "\2\53\1\0\1\53\4\0\1\53\1\0\1\53\2\0"+
-    "\2\53\1\0\3\53\1\0\1\53\2\0\4\53\2\0"+
-    "\1\53\42\0\1\54\11\0\3\25\5\0\1\25\1\0"+
-    "\1\25\1\0\1\25\4\0\1\25\4\0\1\54\1\0"+
-    "\2\54\4\0\1\25\5\0\1\25\3\0\1\54\4\0"+
-    "\1\54\2\25\2\54\10\0\1\52\1\0\2\25\1\0"+
-    "\1\54\10\0\1\25\24\0\1\25\3\0\1\25\6\0"+
-    "\2\25\5\0\1\25\1\0\1\25\1\0\1\25\1\0"+
-    "\11\25\2\0\1\25\4\0\1\25\4\0\6\25\2\0"+
-    "\1\25\1\0\1\25\1\0\3\25\1\0\1\54\1\0"+
-    "\2\25\4\0\3\25\1\0\1\25\10\0\1\25\1\0"+
-    "\2\25\21\0\1\25\3\0\1\25\5\0\1\25\32\0"+
-    "\15\25\5\0\3\25\1\0\1\25\5\0\1\25\2\54"+
-    "\5\0\1\25\2\0\1\25\1\54\4\0\1\25\2\0"+
-    "\1\25\1\0\1\25\103\0\2\54\6\0\1\54\56\0"+
-    "\1\54\3\0\1\54\2\0\1\54\3\0\1\54\5\0"+
-    "\1\54\7\0\1\54\4\0\2\54\3\0\2\54\1\0"+
-    "\1\54\4\0\1\54\1\0\1\54\2\0\2\54\1\0"+
-    "\3\54\1\0\1\54\2\0\4\54\2\0\1\54\42\0"+
-    "\1\64\37\0\1\64\1\0\2\64\16\0\1\64\4\0"+
-    "\1\64\2\0\2\64\10\0\1\26\4\0\1\64\37\0"+
-    "\1\26\102\0\1\26\147\0\2\26\134\0\1\64\153\0"+
-    "\2\64\11\0\1\64\115\0\2\64\6\0\1\64\56\0"+
-    "\1\64\3\0\1\64\2\0\1\64\3\0\1\64\5\0"+
-    "\1\64\7\0\1\64\4\0\2\64\3\0\2\64\1\0"+
-    "\1\64\4\0\1\64\1\0\1\64\2\0\2\64\1\0"+
-    "\3\64\1\0\1\64\2\0\4\64\2\0\1\64\42\0"+
-    "\1\65\11\0\3\25\5\0\1\25\1\0\1\25\1\0"+
-    "\1\25\4\0\1\25\4\0\1\65\1\0\2\65\4\0"+
-    "\1\25\5\0\1\25\3\0\1\65\4\0\1\65\2\25"+
-    "\2\65\10\0\1\26\1\0\2\25\1\0\1\65\10\0"+
-    "\1\25\24\0\1\25\3\0\1\25\6\0\2\25\5\0"+
-    "\1\25\1\0\1\25\1\0\1\25\1\0\11\25\2\0"+
-    "\1\25\4\0\1\25\4\0\6\25\2\0\1\25\1\0"+
-    "\1\25\1\0\3\25\1\0\1\65\1\0\2\25\4\0"+
-    "\3\25\1\0\1\25\10\0\1\25\1\0\2\25\21\0"+
-    "\1\25\3\0\1\25\5\0\1\25\32\0\15\25\5\0"+
-    "\3\25\1\0\1\25\5\0\1\25\2\65\5\0\1\25"+
-    "\2\0\1\25\1\65\4\0\1\25\2\0\1\25\1\0"+
-    "\1\25\103\0\2\65\6\0\1\65\56\0\1\65\3\0"+
-    "\1\65\2\0\1\65\3\0\1\65\5\0\1\65\7\0"+
-    "\1\65\4\0\2\65\3\0\2\65\1\0\1\65\4\0"+
-    "\1\65\1\0\1\65\2\0\2\65\1\0\3\65\1\0"+
-    "\1\65\2\0\4\65\2\0\1\65\42\0\1\103\37\0"+
-    "\1\103\1\0\2\103\16\0\1\103\4\0\1\103\2\0"+
-    "\2\103\15\0\1\103\132\0\1\103\153\0\2\103\11\0"+
-    "\1\103\115\0\2\103\6\0\1\103\56\0\1\103\3\0"+
-    "\1\103\2\0\1\103\3\0\1\103\5\0\1\103\7\0"+
-    "\1\103\4\0\2\103\3\0\2\103\1\0\1\103\4\0"+
-    "\1\103\1\0\1\103\2\0\2\103\1\0\3\103\1\0"+
-    "\1\103\2\0\4\103\2\0\1\103\42\0\1\116\37\0"+
-    "\1\116\1\0\2\116\16\0\1\116\4\0\1\116\2\0"+
-    "\2\116\10\0\1\52\4\0\1\116\37\0\1\52\102\0"+
-    "\1\52\147\0\2\52\134\0\1\116\153\0\2\116\11\0"+
-    "\1\116\115\0\2\116\6\0\1\116\56\0\1\116\3\0"+
-    "\1\116\2\0\1\116\3\0\1\116\5\0\1\116\7\0"+
-    "\1\116\4\0\2\116\3\0\2\116\1\0\1\116\4\0"+
-    "\1\116\1\0\1\116\2\0\2\116\1\0\3\116\1\0"+
-    "\1\116\2\0\4\116\2\0\1\116\40\0";
-
-  private static int [] zzUnpackTrans() {
-    int [] result = new int[10609];
-    int offset = 0;
-    offset = zzUnpackTrans(ZZ_TRANS_PACKED_0, offset, result);
-    return result;
-  }
-
-  private static int zzUnpackTrans(String packed, int offset, int [] result) {
-    int i = 0;       /* index in packed string  */
-    int j = offset;  /* index in unpacked array */
-    int l = packed.length();
-    while (i < l) {
-      int count = packed.charAt(i++);
-      int value = packed.charAt(i++);
-      value--;
-      do result[j++] = value; while (--count > 0);
-    }
-    return j;
-  }
-
-
-  /* error codes */
-  private static final int ZZ_UNKNOWN_ERROR = 0;
-  private static final int ZZ_NO_MATCH = 1;
-  private static final int ZZ_PUSHBACK_2BIG = 2;
-
-  /* error messages for the codes above */
-  private static final String ZZ_ERROR_MSG[] = {
-    "Unkown internal scanner error",
-    "Error: could not match input",
-    "Error: pushback value was too large"
-  };
-
-  /**
-   * ZZ_ATTRIBUTE[aState] contains the attributes of state <code>aState</code>
-   */
-  private static final int [] ZZ_ATTRIBUTE = zzUnpackAttribute();
-
-  private static final String ZZ_ATTRIBUTE_PACKED_0 =
-    "\1\0\1\11\27\1\2\11\1\1\15\0\1\1\1\0"+
-    "\1\1\10\0\1\1\15\0\1\1\57\0";
-
-  private static int [] zzUnpackAttribute() {
-    int [] result = new int[114];
-    int offset = 0;
-    offset = zzUnpackAttribute(ZZ_ATTRIBUTE_PACKED_0, offset, result);
-    return result;
-  }
-
-  private static int zzUnpackAttribute(String packed, int offset, int [] result) {
-    int i = 0;       /* index in packed string  */
-    int j = offset;  /* index in unpacked array */
-    int l = packed.length();
-    while (i < l) {
-      int count = packed.charAt(i++);
-      int value = packed.charAt(i++);
-      do result[j++] = value; while (--count > 0);
-    }
-    return j;
-  }
-
-  /** the input device */
-  private java.io.Reader zzReader;
-
-  /** the current state of the DFA */
-  private int zzState;
-
-  /** the current lexical state */
-  private int zzLexicalState = YYINITIAL;
-
-  /** this buffer contains the current text to be matched and is
-      the source of the yytext() string */
-  private char zzBuffer[] = new char[ZZ_BUFFERSIZE];
-
-  /** the textposition at the last accepting state */
-  private int zzMarkedPos;
-
-  /** the current text position in the buffer */
-  private int zzCurrentPos;
-
-  /** startRead marks the beginning of the yytext() string in the buffer */
-  private int zzStartRead;
-
-  /** endRead marks the last character in the buffer, that has been read
-      from input */
-  private int zzEndRead;
-
-  /** number of newlines encountered up to the start of the matched text */
-  private int yyline;
-
-  /** the number of characters up to the start of the matched text */
-  private int yychar;
-
-  /**
-   * the number of characters from the last newline up to the start of the 
-   * matched text
-   */
-  private int yycolumn;
-
-  /** 
-   * zzAtBOL == true <=> the scanner is currently at the beginning of a line
-   */
-  private boolean zzAtBOL = true;
-
-  /** zzAtEOF == true <=> the scanner is at the EOF */
-  private boolean zzAtEOF;
-
-  /** denotes if the user-EOF-code has already been executed */
-  private boolean zzEOFDone;
-
-  /* user code: */
-  /** Alphanumeric sequences */
-  public static final int WORD_TYPE = StandardTokenizer.ALPHANUM;
-  
-  /** Numbers */
-  public static final int NUMERIC_TYPE = StandardTokenizer.NUM;
-  
-  /**
-   * Chars in class \p{Line_Break = Complex_Context} are from South East Asian
-   * scripts (Thai, Lao, Myanmar, Khmer, etc.).  Sequences of these are kept 
-   * together as as a single token rather than broken up, because the logic
-   * required to break them at word boundaries is too complex for UAX#29.
-   * <p>
-   * See Unicode Line Breaking Algorithm: http://www.unicode.org/reports/tr14/#SA
-   */
-  public static final int SOUTH_EAST_ASIAN_TYPE = StandardTokenizer.SOUTHEAST_ASIAN;
-  
-  public static final int IDEOGRAPHIC_TYPE = StandardTokenizer.IDEOGRAPHIC;
-  
-  public static final int HIRAGANA_TYPE = StandardTokenizer.HIRAGANA;
-  
-  public static final int KATAKANA_TYPE = StandardTokenizer.KATAKANA;
-  
-  public static final int HANGUL_TYPE = StandardTokenizer.HANGUL;
-
-  public final int yychar()
-  {
-    return yychar;
-  }
-
-  /**
-   * Fills CharTermAttribute with the current token text.
-   */
-  public final void getText(CharTermAttribute t) {
-    t.copyBuffer(zzBuffer, zzStartRead, zzMarkedPos-zzStartRead);
-  }
-
-
-  /**
-   * Creates a new scanner
-   * There is also a java.io.InputStream version of this constructor.
-   *
-   * @param   in  the java.io.Reader to read input from.
-   */
-  public StandardTokenizerImpl31(java.io.Reader in) {
-    this.zzReader = in;
-  }
-
-  /**
-   * Creates a new scanner.
-   * There is also java.io.Reader version of this constructor.
-   *
-   * @param   in  the java.io.Inputstream to read input from.
-   */
-  public StandardTokenizerImpl31(java.io.InputStream in) {
-    this(new java.io.InputStreamReader(in));
-  }
-
-  /** 
-   * Unpacks the compressed character translation table.
-   *
-   * @param packed   the packed character translation table
-   * @return         the unpacked character translation table
-   */
-  private static char [] zzUnpackCMap(String packed) {
-    char [] map = new char[0x10000];
-    int i = 0;  /* index in packed string  */
-    int j = 0;  /* index in unpacked array */
-    while (i < 2650) {
-      int  count = packed.charAt(i++);
-      char value = packed.charAt(i++);
-      do map[j++] = value; while (--count > 0);
-    }
-    return map;
-  }
-
-
-  /**
-   * Refills the input buffer.
-   *
-   * @return      <code>false</code>, iff there was new input.
-   * 
-   * @exception   java.io.IOException  if any I/O-Error occurs
-   */
-  private boolean zzRefill() throws java.io.IOException {
-
-    /* first: make room (if you can) */
-    if (zzStartRead > 0) {
-      System.arraycopy(zzBuffer, zzStartRead,
-                       zzBuffer, 0,
-                       zzEndRead-zzStartRead);
-
-      /* translate stored positions */
-      zzEndRead-= zzStartRead;
-      zzCurrentPos-= zzStartRead;
-      zzMarkedPos-= zzStartRead;
-      zzStartRead = 0;
-    }
-
-    /* is the buffer big enough? */
-    if (zzCurrentPos >= zzBuffer.length) {
-      /* if not: blow it up */
-      char newBuffer[] = new char[zzCurrentPos*2];
-      System.arraycopy(zzBuffer, 0, newBuffer, 0, zzBuffer.length);
-      zzBuffer = newBuffer;
-    }
-
-    /* finally: fill the buffer with new input */
-    int numRead = zzReader.read(zzBuffer, zzEndRead,
-                                            zzBuffer.length-zzEndRead);
-
-    if (numRead > 0) {
-      zzEndRead+= numRead;
-      return false;
-    }
-    // unlikely but not impossible: read 0 characters, but not at end of stream    
-    if (numRead == 0) {
-      int c = zzReader.read();
-      if (c == -1) {
-        return true;
-      } else {
-        zzBuffer[zzEndRead++] = (char) c;
-        return false;
-      }     
-    }
-
-	// numRead < 0
-    return true;
-  }
-
-    
-  /**
-   * Closes the input stream.
-   */
-  public final void yyclose() throws java.io.IOException {
-    zzAtEOF = true;            /* indicate end of file */
-    zzEndRead = zzStartRead;  /* invalidate buffer    */
-
-    if (zzReader != null)
-      zzReader.close();
-  }
-
-
-  /**
-   * Resets the scanner to read from a new input stream.
-   * Does not close the old reader.
-   *
-   * All internal variables are reset, the old input stream 
-   * <b>cannot</b> be reused (internal buffer is discarded and lost).
-   * Lexical state is set to <tt>ZZ_INITIAL</tt>.
-   *
-   * Internal scan buffer is resized down to its initial length, if it has grown.
-   *
-   * @param reader   the new input stream 
-   */
-  public final void yyreset(java.io.Reader reader) {
-    zzReader = reader;
-    zzAtBOL  = true;
-    zzAtEOF  = false;
-    zzEOFDone = false;
-    zzEndRead = zzStartRead = 0;
-    zzCurrentPos = zzMarkedPos = 0;
-    yyline = yychar = yycolumn = 0;
-    zzLexicalState = YYINITIAL;
-    if (zzBuffer.length > ZZ_BUFFERSIZE)
-      zzBuffer = new char[ZZ_BUFFERSIZE];
-  }
-
-
-  /**
-   * Returns the current lexical state.
-   */
-  public final int yystate() {
-    return zzLexicalState;
-  }
-
-
-  /**
-   * Enters a new lexical state
-   *
-   * @param newState the new lexical state
-   */
-  public final void yybegin(int newState) {
-    zzLexicalState = newState;
-  }
-
-
-  /**
-   * Returns the text matched by the current regular expression.
-   */
-  public final String yytext() {
-    return new String( zzBuffer, zzStartRead, zzMarkedPos-zzStartRead );
-  }
-
-
-  /**
-   * Returns the character at position <tt>pos</tt> from the 
-   * matched text. 
-   * 
-   * It is equivalent to yytext().charAt(pos), but faster
-   *
-   * @param pos the position of the character to fetch. 
-   *            A value from 0 to yylength()-1.
-   *
-   * @return the character at position pos
-   */
-  public final char yycharat(int pos) {
-    return zzBuffer[zzStartRead+pos];
-  }
-
-
-  /**
-   * Returns the length of the matched text region.
-   */
-  public final int yylength() {
-    return zzMarkedPos-zzStartRead;
-  }
-
-
-  /**
-   * Reports an error that occured while scanning.
-   *
-   * In a wellformed scanner (no or only correct usage of 
-   * yypushback(int) and a match-all fallback rule) this method 
-   * will only be called with things that "Can't Possibly Happen".
-   * If this method is called, something is seriously wrong
-   * (e.g. a JFlex bug producing a faulty scanner etc.).
-   *
-   * Usual syntax/scanner level error handling should be done
-   * in error fallback rules.
-   *
-   * @param   errorCode  the code of the errormessage to display
-   */
-  private void zzScanError(int errorCode) {
-    String message;
-    try {
-      message = ZZ_ERROR_MSG[errorCode];
-    }
-    catch (ArrayIndexOutOfBoundsException e) {
-      message = ZZ_ERROR_MSG[ZZ_UNKNOWN_ERROR];
-    }
-
-    throw new Error(message);
-  } 
-
-
-  /**
-   * Pushes the specified amount of characters back into the input stream.
-   *
-   * They will be read again by then next call of the scanning method
-   *
-   * @param number  the number of characters to be read again.
-   *                This number must not be greater than yylength()!
-   */
-  public void yypushback(int number)  {
-    if ( number > yylength() )
-      zzScanError(ZZ_PUSHBACK_2BIG);
-
-    zzMarkedPos -= number;
-  }
-
-
-  /**
-   * Resumes scanning until the next regular expression is matched,
-   * the end of input is encountered or an I/O-Error occurs.
-   *
-   * @return      the next token
-   * @exception   java.io.IOException  if any I/O-Error occurs
-   */
-  public int getNextToken() throws java.io.IOException {
-    int zzInput;
-    int zzAction;
-
-    // cached fields:
-    int zzCurrentPosL;
-    int zzMarkedPosL;
-    int zzEndReadL = zzEndRead;
-    char [] zzBufferL = zzBuffer;
-    char [] zzCMapL = ZZ_CMAP;
-
-    int [] zzTransL = ZZ_TRANS;
-    int [] zzRowMapL = ZZ_ROWMAP;
-    int [] zzAttrL = ZZ_ATTRIBUTE;
-
-    while (true) {
-      zzMarkedPosL = zzMarkedPos;
-
-      yychar+= zzMarkedPosL-zzStartRead;
-
-      zzAction = -1;
-
-      zzCurrentPosL = zzCurrentPos = zzStartRead = zzMarkedPosL;
-  
-      zzState = ZZ_LEXSTATE[zzLexicalState];
-
-      // set up zzAction for empty match case:
-      int zzAttributes = zzAttrL[zzState];
-      if ( (zzAttributes & 1) == 1 ) {
-        zzAction = zzState;
-      }
-
-
-      zzForAction: {
-        while (true) {
-    
-          if (zzCurrentPosL < zzEndReadL)
-            zzInput = zzBufferL[zzCurrentPosL++];
-          else if (zzAtEOF) {
-            zzInput = YYEOF;
-            break zzForAction;
-          }
-          else {
-            // store back cached positions
-            zzCurrentPos  = zzCurrentPosL;
-            zzMarkedPos   = zzMarkedPosL;
-            boolean eof = zzRefill();
-            // get translated positions and possibly new buffer
-            zzCurrentPosL  = zzCurrentPos;
-            zzMarkedPosL   = zzMarkedPos;
-            zzBufferL      = zzBuffer;
-            zzEndReadL     = zzEndRead;
-            if (eof) {
-              zzInput = YYEOF;
-              break zzForAction;
-            }
-            else {
-              zzInput = zzBufferL[zzCurrentPosL++];
-            }
-          }
-          int zzNext = zzTransL[ zzRowMapL[zzState] + zzCMapL[zzInput] ];
-          if (zzNext == -1) break zzForAction;
-          zzState = zzNext;
-
-          zzAttributes = zzAttrL[zzState];
-          if ( (zzAttributes & 1) == 1 ) {
-            zzAction = zzState;
-            zzMarkedPosL = zzCurrentPosL;
-            if ( (zzAttributes & 8) == 8 ) break zzForAction;
-          }
-
-        }
-      }
-
-      // store back cached position
-      zzMarkedPos = zzMarkedPosL;
-
-      switch (zzAction < 0 ? zzAction : ZZ_ACTION[zzAction]) {
-        case 2: 
-          { return WORD_TYPE;
-          }
-        case 9: break;
-        case 5: 
-          { return SOUTH_EAST_ASIAN_TYPE;
-          }
-        case 10: break;
-        case 4: 
-          { return KATAKANA_TYPE;
-          }
-        case 11: break;
-        case 6: 
-          { return IDEOGRAPHIC_TYPE;
-          }
-        case 12: break;
-        case 8: 
-          { return HANGUL_TYPE;
-          }
-        case 13: break;
-        case 3: 
-          { return NUMERIC_TYPE;
-          }
-        case 14: break;
-        case 7: 
-          { return HIRAGANA_TYPE;
-          }
-        case 15: break;
-        case 1: 
-          { /* Break so we don't hit fall-through warning: */ break; /* Not numeric, word, ideographic, hiragana, or SE Asian -- ignore it. */
-          }
-        case 16: break;
-        default: 
-          if (zzInput == YYEOF && zzStartRead == zzCurrentPos) {
-            zzAtEOF = true;
-              {
-                return StandardTokenizerInterface.YYEOF;
-              }
-          } 
-          else {
-            zzScanError(ZZ_NO_MATCH);
-          }
-      }
-    }
-  }
-
-
-}
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/StandardTokenizerImpl31.jflex b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/StandardTokenizerImpl31.jflex
deleted file mode 100644
index c4b5dc9..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/StandardTokenizerImpl31.jflex
+++ /dev/null
@@ -1,184 +0,0 @@
-package org.apache.lucene.analysis.standard.std31;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.standard.StandardTokenizer;
-import org.apache.lucene.analysis.standard.StandardTokenizerInterface;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-
-/**
- * This class implements StandardTokenizer, except with a bug 
- * (https://issues.apache.org/jira/browse/LUCENE-3358) where Han and Hiragana
- * characters would be split from combining characters:
- * @deprecated This class is only for exact backwards compatibility
- */
-@Deprecated
-%%
-
-%unicode 6.0
-%integer
-%final
-%public
-%class StandardTokenizerImpl31
-%implements StandardTokenizerInterface
-%function getNextToken
-%char
-
-%include src/java/org/apache/lucene/analysis/standard/std31/SUPPLEMENTARY.jflex-macro
-ALetter = ([\p{WB:ALetter}] | {ALetterSupp})
-Format =  ([\p{WB:Format}] | {FormatSupp})
-Numeric = ([\p{WB:Numeric}] | {NumericSupp})
-Extend =  ([\p{WB:Extend}] | {ExtendSupp})
-Katakana = ([\p{WB:Katakana}] | {KatakanaSupp})
-MidLetter = ([\p{WB:MidLetter}] | {MidLetterSupp})
-MidNum = ([\p{WB:MidNum}] | {MidNumSupp})
-MidNumLet = ([\p{WB:MidNumLet}] | {MidNumLetSupp})
-ExtendNumLet = ([\p{WB:ExtendNumLet}] | {ExtendNumLetSupp})
-ComplexContext = ([\p{LB:Complex_Context}] | {ComplexContextSupp})
-Han = ([\p{Script:Han}] | {HanSupp})
-Hiragana = ([\p{Script:Hiragana}] | {HiraganaSupp})
-
-// Script=Hangul & Aletter
-HangulEx       = (!(!\p{Script:Hangul}|!\p{WB:ALetter})) ({Format} | {Extend})*
-// UAX#29 WB4. X (Extend | Format)* --> X
-//
-ALetterEx      = {ALetter}                     ({Format} | {Extend})*
-// TODO: Convert hard-coded full-width numeric range to property intersection (something like [\p{Full-Width}&&\p{Numeric}]) once JFlex supports it
-NumericEx      = ({Numeric} | [\uFF10-\uFF19]) ({Format} | {Extend})*
-KatakanaEx     = {Katakana}                    ({Format} | {Extend})* 
-MidLetterEx    = ({MidLetter} | {MidNumLet})   ({Format} | {Extend})* 
-MidNumericEx   = ({MidNum} | {MidNumLet})      ({Format} | {Extend})*
-ExtendNumLetEx = {ExtendNumLet}                ({Format} | {Extend})*
-
-
-%{
-  /** Alphanumeric sequences */
-  public static final int WORD_TYPE = StandardTokenizer.ALPHANUM;
-  
-  /** Numbers */
-  public static final int NUMERIC_TYPE = StandardTokenizer.NUM;
-  
-  /**
-   * Chars in class \p{Line_Break = Complex_Context} are from South East Asian
-   * scripts (Thai, Lao, Myanmar, Khmer, etc.).  Sequences of these are kept 
-   * together as as a single token rather than broken up, because the logic
-   * required to break them at word boundaries is too complex for UAX#29.
-   * <p>
-   * See Unicode Line Breaking Algorithm: http://www.unicode.org/reports/tr14/#SA
-   */
-  public static final int SOUTH_EAST_ASIAN_TYPE = StandardTokenizer.SOUTHEAST_ASIAN;
-  
-  public static final int IDEOGRAPHIC_TYPE = StandardTokenizer.IDEOGRAPHIC;
-  
-  public static final int HIRAGANA_TYPE = StandardTokenizer.HIRAGANA;
-  
-  public static final int KATAKANA_TYPE = StandardTokenizer.KATAKANA;
-  
-  public static final int HANGUL_TYPE = StandardTokenizer.HANGUL;
-
-  public final int yychar()
-  {
-    return yychar;
-  }
-
-  /**
-   * Fills CharTermAttribute with the current token text.
-   */
-  public final void getText(CharTermAttribute t) {
-    t.copyBuffer(zzBuffer, zzStartRead, zzMarkedPos-zzStartRead);
-  }
-%}
-
-%%
-
-// UAX#29 WB1. 	sot 	Ã· 	
-//        WB2. 		Ã· 	eot
-//
-<<EOF>> { return StandardTokenizerInterface.YYEOF; }
-
-// UAX#29 WB8.   Numeric ? Numeric
-//        WB11.  Numeric (MidNum | MidNumLet) ? Numeric
-//        WB12.  Numeric ? (MidNum | MidNumLet) Numeric
-//        WB13a. (ALetter | Numeric | Katakana | ExtendNumLet) ? ExtendNumLet
-//        WB13b. ExtendNumLet ? (ALetter | Numeric | Katakana)
-//
-{ExtendNumLetEx}* {NumericEx} ({ExtendNumLetEx}+ {NumericEx} 
-                              | {MidNumericEx} {NumericEx} 
-                              | {NumericEx})*
-{ExtendNumLetEx}* 
-  { return NUMERIC_TYPE; }
-
-// subset of the below for typing purposes only!
-{HangulEx}+
-  { return HANGUL_TYPE; }
-  
-{KatakanaEx}+
-  { return KATAKANA_TYPE; }
-
-// UAX#29 WB5.   ALetter ? ALetter
-//        WB6.   ALetter ? (MidLetter | MidNumLet) ALetter
-//        WB7.   ALetter (MidLetter | MidNumLet) ? ALetter
-//        WB9.   ALetter ? Numeric
-//        WB10.  Numeric ? ALetter
-//        WB13.  Katakana ? Katakana
-//        WB13a. (ALetter | Numeric | Katakana | ExtendNumLet) ? ExtendNumLet
-//        WB13b. ExtendNumLet ? (ALetter | Numeric | Katakana)
-//
-{ExtendNumLetEx}*  ( {KatakanaEx} ({ExtendNumLetEx}* {KatakanaEx})* 
-                   | ( {NumericEx}  ({ExtendNumLetEx}+ {NumericEx} | {MidNumericEx} {NumericEx} | {NumericEx})*
-                     | {ALetterEx}  ({ExtendNumLetEx}+ {ALetterEx} | {MidLetterEx}  {ALetterEx} | {ALetterEx})* )+ ) 
-({ExtendNumLetEx}+ ( {KatakanaEx} ({ExtendNumLetEx}* {KatakanaEx})* 
-                   | ( {NumericEx}  ({ExtendNumLetEx}+ {NumericEx} | {MidNumericEx} {NumericEx} | {NumericEx})*
-                     | {ALetterEx}  ({ExtendNumLetEx}+ {ALetterEx} | {MidLetterEx}  {ALetterEx} | {ALetterEx})* )+ ) )*
-{ExtendNumLetEx}*  
-  { return WORD_TYPE; }
-
-
-// From UAX #29:
-//
-//    [C]haracters with the Line_Break property values of Contingent_Break (CB), 
-//    Complex_Context (SA/South East Asian), and XX (Unknown) are assigned word 
-//    boundary property values based on criteria outside of the scope of this
-//    annex.  That means that satisfactory treatment of languages like Chinese
-//    or Thai requires special handling.
-// 
-// In Unicode 6.0, only one character has the \p{Line_Break = Contingent_Break}
-// property: U+FFFC ( ï¿? ) OBJECT REPLACEMENT CHARACTER.
-//
-// In the ICU implementation of UAX#29, \p{Line_Break = Complex_Context}
-// character sequences (from South East Asian scripts like Thai, Myanmar, Khmer,
-// Lao, etc.) are kept together.  This grammar does the same below.
-//
-// See also the Unicode Line Breaking Algorithm:
-//
-//    http://www.unicode.org/reports/tr14/#SA
-//
-{ComplexContext}+ { return SOUTH_EAST_ASIAN_TYPE; }
-
-// UAX#29 WB14.  Any Ã· Any
-//
-{Han} { return IDEOGRAPHIC_TYPE; }
-{Hiragana} { return HIRAGANA_TYPE; }
-
-
-// UAX#29 WB3.   CR ? LF
-//        WB3a.  (Newline | CR | LF) Ã·
-//        WB3b.  Ã· (Newline | CR | LF)
-//        WB14.  Any Ã· Any
-//
-[^] { /* Break so we don't hit fall-through warning: */ break; /* Not numeric, word, ideographic, hiragana, or SE Asian -- ignore it. */ }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/UAX29URLEmailTokenizerImpl31.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/UAX29URLEmailTokenizerImpl31.java
deleted file mode 100644
index 592f091..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/UAX29URLEmailTokenizerImpl31.java
+++ /dev/null
@@ -1,3672 +0,0 @@
-/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 9/30/11 12:10 PM */
-
-package org.apache.lucene.analysis.standard.std31;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.standard.UAX29URLEmailTokenizer;
-import org.apache.lucene.analysis.standard.StandardTokenizerInterface;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-
-/**
- * This class implements UAX29URLEmailTokenizer, except with a bug 
- * (https://issues.apache.org/jira/browse/LUCENE-3358) where Han and Hiragana
- * characters would be split from combining characters:
- * @deprecated This class is only for exact backwards compatibility
- */
- @Deprecated
-
-/**
- * This class is a scanner generated by 
- * <a href="http://www.jflex.de/">JFlex</a> 1.5.0-SNAPSHOT
- * on 9/30/11 12:10 PM from the specification file
- * <tt>/lucene/jflex/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/UAX29URLEmailTokenizerImpl31.jflex</tt>
- */
-public final class UAX29URLEmailTokenizerImpl31 implements StandardTokenizerInterface {
-
-  /** This character denotes the end of file */
-  public static final int YYEOF = -1;
-
-  /** initial size of the lookahead buffer */
-  private static final int ZZ_BUFFERSIZE = 16384;
-
-  /** lexical states */
-  public static final int YYINITIAL = 0;
-
-  /**
-   * ZZ_LEXSTATE[l] is the state in the DFA for the lexical state l
-   * ZZ_LEXSTATE[l+1] is the state in the DFA for the lexical state l
-   *                  at the beginning of a line
-   * l is of the form l = 2*k, k a non negative integer
-   */
-  private static final int ZZ_LEXSTATE[] = { 
-     0, 0
-  };
-
-  /** 
-   * Translates characters to character classes
-   */
-  private static final String ZZ_CMAP_PACKED = 
-    "\1\237\10\235\2\237\2\235\1\237\23\235\1\240\1\234\1\227\1\240"+
-    "\1\220\1\216\1\223\2\221\2\240\1\222\1\202\1\147\1\226\1\203"+
-    "\1\206\1\214\1\207\1\212\1\204\1\205\1\211\1\213\1\210\1\215"+
-    "\1\232\1\235\1\233\1\235\1\225\1\224\1\150\1\174\1\151\1\152"+
-    "\1\153\1\156\1\157\1\175\1\160\1\176\1\201\1\161\1\162\1\163"+
-    "\1\155\1\165\1\164\1\154\1\166\1\167\1\170\1\177\1\171\1\172"+
-    "\1\200\1\173\1\230\1\236\1\231\1\241\1\217\1\241\1\150\1\174"+
-    "\1\151\1\152\1\153\1\156\1\157\1\175\1\160\1\176\1\201\1\161"+
-    "\1\162\1\163\1\155\1\165\1\164\1\154\1\166\1\167\1\170\1\177"+
-    "\1\171\1\172\1\200\1\173\3\241\1\216\1\242\52\0\1\132\2\0"+
-    "\1\133\7\0\1\132\1\0\1\136\2\0\1\132\5\0\27\132\1\0"+
-    "\37\132\1\0\u01ca\132\4\0\14\132\16\0\5\132\7\0\1\132\1\0"+
-    "\1\132\21\0\160\133\5\132\1\0\2\132\2\0\4\132\1\137\7\0"+
-    "\1\132\1\136\3\132\1\0\1\132\1\0\24\132\1\0\123\132\1\0"+
-    "\213\132\1\0\7\133\236\132\11\0\46\132\2\0\1\132\7\0\47\132"+
-    "\1\0\1\137\7\0\55\133\1\0\1\133\1\0\2\133\1\0\2\133"+
-    "\1\0\1\133\10\0\33\132\5\0\4\132\1\136\13\0\4\133\10\0"+
-    "\2\137\2\0\13\133\5\0\53\132\25\133\12\134\1\0\1\134\1\137"+
-    "\1\0\2\132\1\133\143\132\1\0\1\132\7\133\1\133\1\0\6\133"+
-    "\2\132\2\133\1\0\4\133\2\132\12\134\3\132\2\0\1\132\17\0"+
-    "\1\133\1\132\1\133\36\132\33\133\2\0\131\132\13\133\1\132\16\0"+
-    "\12\134\41\132\11\133\2\132\2\0\1\137\1\0\1\132\5\0\26\132"+
-    "\4\133\1\132\11\133\1\132\3\133\1\132\5\133\22\0\31\132\3\133"+
-    "\244\0\4\133\66\132\3\133\1\132\22\133\1\132\7\133\12\132\2\133"+
-    "\2\0\12\134\1\0\7\132\1\0\7\132\1\0\3\133\1\0\10\132"+
-    "\2\0\2\132\2\0\26\132\1\0\7\132\1\0\1\132\3\0\4\132"+
-    "\2\0\1\133\1\132\7\133\2\0\2\133\2\0\3\133\1\132\10\0"+
-    "\1\133\4\0\2\132\1\0\3\132\2\133\2\0\12\134\2\132\17\0"+
-    "\3\133\1\0\6\132\4\0\2\132\2\0\26\132\1\0\7\132\1\0"+
-    "\2\132\1\0\2\132\1\0\2\132\2\0\1\133\1\0\5\133\4\0"+
-    "\2\133\2\0\3\133\3\0\1\133\7\0\4\132\1\0\1\132\7\0"+
-    "\12\134\2\133\3\132\1\133\13\0\3\133\1\0\11\132\1\0\3\132"+
-    "\1\0\26\132\1\0\7\132\1\0\2\132\1\0\5\132\2\0\1\133"+
-    "\1\132\10\133\1\0\3\133\1\0\3\133\2\0\1\132\17\0\2\132"+
-    "\2\133\2\0\12\134\21\0\3\133\1\0\10\132\2\0\2\132\2\0"+
-    "\26\132\1\0\7\132\1\0\2\132\1\0\5\132\2\0\1\133\1\132"+
-    "\7\133\2\0\2\133\2\0\3\133\10\0\2\133\4\0\2\132\1\0"+
-    "\3\132\2\133\2\0\12\134\1\0\1\132\20\0\1\133\1\132\1\0"+
-    "\6\132\3\0\3\132\1\0\4\132\3\0\2\132\1\0\1\132\1\0"+
-    "\2\132\3\0\2\132\3\0\3\132\3\0\14\132\4\0\5\133\3\0"+
-    "\3\133\1\0\4\133\2\0\1\132\6\0\1\133\16\0\12\134\21\0"+
-    "\3\133\1\0\10\132\1\0\3\132\1\0\27\132\1\0\12\132\1\0"+
-    "\5\132\3\0\1\132\7\133\1\0\3\133\1\0\4\133\7\0\2\133"+
-    "\1\0\2\132\6\0\2\132\2\133\2\0\12\134\22\0\2\133\1\0"+
-    "\10\132\1\0\3\132\1\0\27\132\1\0\12\132\1\0\5\132\2\0"+
-    "\1\133\1\132\7\133\1\0\3\133\1\0\4\133\7\0\2\133\7\0"+
-    "\1\132\1\0\2\132\2\133\2\0\12\134\1\0\2\132\17\0\2\133"+
-    "\1\0\10\132\1\0\3\132\1\0\51\132\2\0\1\132\7\133\1\0"+
-    "\3\133\1\0\4\133\1\132\10\0\1\133\10\0\2\132\2\133\2\0"+
-    "\12\134\12\0\6\132\2\0\2\133\1\0\22\132\3\0\30\132\1\0"+
-    "\11\132\1\0\1\132\2\0\7\132\3\0\1\133\4\0\6\133\1\0"+
-    "\1\133\1\0\10\133\22\0\2\133\15\0\60\142\1\143\2\142\7\143"+
-    "\5\0\7\142\10\143\1\0\12\134\47\0\2\142\1\0\1\142\2\0"+
-    "\2\142\1\0\1\142\2\0\1\142\6\0\4\142\1\0\7\142\1\0"+
-    "\3\142\1\0\1\142\1\0\1\142\2\0\2\142\1\0\4\142\1\143"+
-    "\2\142\6\143\1\0\2\143\1\142\2\0\5\142\1\0\1\142\1\0"+
-    "\6\143\2\0\12\134\2\0\2\142\42\0\1\132\27\0\2\133\6\0"+
-    "\12\134\13\0\1\133\1\0\1\133\1\0\1\133\4\0\2\133\10\132"+
-    "\1\0\44\132\4\0\24\133\1\0\2\133\5\132\13\133\1\0\44\133"+
-    "\11\0\1\133\71\0\53\142\24\143\1\142\12\134\6\0\6\142\4\143"+
-    "\4\142\3\143\1\142\3\143\2\142\7\143\3\142\4\143\15\142\14\143"+
-    "\1\142\1\143\12\134\4\143\2\142\46\132\12\0\53\132\1\0\1\132"+
-    "\3\0\u0100\146\111\132\1\0\4\132\2\0\7\132\1\0\1\132\1\0"+
-    "\4\132\2\0\51\132\1\0\4\132\2\0\41\132\1\0\4\132\2\0"+
-    "\7\132\1\0\1\132\1\0\4\132\2\0\17\132\1\0\71\132\1\0"+
-    "\4\132\2\0\103\132\2\0\3\133\40\0\20\132\20\0\125\132\14\0"+
-    "\u026c\132\2\0\21\132\1\0\32\132\5\0\113\132\3\0\3\132\17\0"+
-    "\15\132\1\0\4\132\3\133\13\0\22\132\3\133\13\0\22\132\2\133"+
-    "\14\0\15\132\1\0\3\132\1\0\2\133\14\0\64\142\2\143\36\143"+
-    "\3\0\1\142\4\0\1\142\1\143\2\0\12\134\41\0\3\133\2\0"+
-    "\12\134\6\0\130\132\10\0\51\132\1\133\1\132\5\0\106\132\12\0"+
-    "\35\132\3\0\14\133\4\0\14\133\12\0\12\134\36\142\2\0\5\142"+
-    "\13\0\54\142\4\0\21\143\7\142\2\143\6\0\12\134\1\142\3\0"+
-    "\2\142\40\0\27\132\5\133\4\0\65\142\12\143\1\0\35\143\2\0"+
-    "\1\133\12\134\6\0\12\134\6\0\16\142\122\0\5\133\57\132\21\133"+
-    "\7\132\4\0\12\134\21\0\11\133\14\0\3\133\36\132\12\133\3\0"+
-    "\2\132\12\134\6\0\46\132\16\133\14\0\44\132\24\133\10\0\12\134"+
-    "\3\0\3\132\12\134\44\132\122\0\3\133\1\0\25\133\4\132\1\133"+
-    "\4\132\1\133\15\0\300\132\47\133\25\0\4\133\u0116\132\2\0\6\132"+
-    "\2\0\46\132\2\0\6\132\2\0\10\132\1\0\1\132\1\0\1\132"+
-    "\1\0\1\132\1\0\37\132\2\0\65\132\1\0\7\132\1\0\1\132"+
-    "\3\0\3\132\1\0\7\132\3\0\4\132\2\0\6\132\4\0\15\132"+
-    "\5\0\3\132\1\0\7\132\17\0\2\133\2\133\10\0\2\140\12\0"+
-    "\1\140\2\0\1\136\2\0\5\133\20\0\2\141\3\0\1\137\17\0"+
-    "\1\141\13\0\5\133\5\0\6\133\1\0\1\132\15\0\1\132\20\0"+
-    "\15\132\63\0\41\133\21\0\1\132\4\0\1\132\2\0\12\132\1\0"+
-    "\1\132\3\0\5\132\6\0\1\132\1\0\1\132\1\0\1\132\1\0"+
-    "\4\132\1\0\13\132\2\0\4\132\5\0\5\132\4\0\1\132\21\0"+
-    "\51\132\u032d\0\64\132\u0716\0\57\132\1\0\57\132\1\0\205\132\6\0"+
-    "\4\132\3\133\16\0\46\132\12\0\66\132\11\0\1\132\17\0\1\133"+
-    "\27\132\11\0\7\132\1\0\7\132\1\0\7\132\1\0\7\132\1\0"+
-    "\7\132\1\0\7\132\1\0\7\132\1\0\7\132\1\0\40\133\57\0"+
-    "\1\132\120\0\32\144\1\0\131\144\14\0\326\144\57\0\1\132\1\0"+
-    "\1\144\31\0\11\144\4\133\2\133\1\0\5\135\2\0\3\144\1\132"+
-    "\1\132\4\0\126\145\2\0\2\133\2\135\3\145\133\135\1\0\4\135"+
-    "\5\0\51\132\3\0\136\146\21\0\33\132\65\0\20\135\37\0\101\0"+
-    "\37\0\121\0\57\135\1\0\130\135\250\0\u19b6\144\112\0\u51cc\144\64\0"+
-    "\u048d\132\103\0\56\132\2\0\u010d\132\3\0\20\132\12\134\2\132\24\0"+
-    "\57\132\4\133\11\0\2\133\1\0\31\132\10\0\120\132\2\133\45\0"+
-    "\11\132\2\0\147\132\2\0\4\132\1\0\2\132\16\0\12\132\120\0"+
-    "\10\132\1\133\3\132\1\133\4\132\1\133\27\132\5\133\30\0\64\132"+
-    "\14\0\2\133\62\132\21\133\13\0\12\134\6\0\22\133\6\132\3\0"+
-    "\1\132\4\0\12\134\34\132\10\133\2\0\27\132\15\133\14\0\35\146"+
-    "\3\0\4\133\57\132\16\133\16\0\1\132\12\134\46\0\51\132\16\133"+
-    "\11\0\3\132\1\133\10\132\2\133\2\0\12\134\6\0\33\142\1\143"+
-    "\4\0\60\142\1\143\1\142\3\143\2\142\2\143\5\142\2\143\1\142"+
-    "\1\143\1\142\30\0\5\142\41\0\6\132\2\0\6\132\2\0\6\132"+
-    "\11\0\7\132\1\0\7\132\221\0\43\132\10\133\1\0\2\133\2\0"+
-    "\12\134\6\0\u2ba4\146\14\0\27\146\4\0\61\146\4\0\1\31\1\25"+
-    "\1\46\1\43\1\13\3\0\1\7\1\5\2\0\1\3\1\1\14\0"+
-    "\1\11\21\0\1\112\7\0\1\65\1\17\6\0\1\130\3\0\1\120"+
-    "\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120"+
-    "\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120"+
-    "\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120"+
-    "\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120"+
-    "\1\121\1\120\1\120\1\120\1\125\1\123\17\0\1\114\u02c1\0\1\70"+
-    "\277\0\1\113\1\71\1\2\3\124\2\35\1\124\1\35\2\124\1\14"+
-    "\21\124\2\60\7\73\1\72\7\73\7\52\1\15\1\52\1\75\2\45"+
-    "\1\44\1\75\1\45\1\44\10\75\2\63\5\61\2\54\5\61\1\6"+
-    "\10\37\5\21\3\27\12\106\20\27\3\42\32\30\1\26\2\24\2\110"+
-    "\1\111\2\110\2\111\2\110\1\111\3\24\1\16\2\24\12\64\1\74"+
-    "\1\41\1\34\1\64\6\41\1\34\66\41\5\115\6\103\1\51\4\103"+
-    "\2\51\10\103\1\51\7\100\1\12\2\100\32\103\1\12\4\100\1\12"+
-    "\5\102\1\101\1\102\3\101\7\102\1\101\23\102\5\67\3\102\6\67"+
-    "\2\67\6\66\10\66\2\100\7\66\36\100\4\66\102\100\15\115\1\77"+
-    "\2\115\1\131\3\117\1\115\2\117\5\115\4\117\4\116\1\115\3\116"+
-    "\1\115\5\116\26\56\4\23\1\105\2\104\4\122\1\104\2\122\3\76"+
-    "\33\122\35\55\3\122\35\126\3\122\6\126\2\33\31\126\1\33\17\126"+
-    "\6\122\4\22\1\10\37\22\1\10\4\22\25\62\1\127\11\62\21\55"+
-    "\5\62\1\57\12\40\13\62\4\55\1\50\6\55\12\122\17\55\1\47"+
-    "\3\53\15\20\11\36\1\32\24\36\2\20\11\36\1\32\31\36\1\32"+
-    "\4\20\4\36\2\32\2\107\1\4\5\107\52\4\u1900\0\u012e\144\2\0"+
-    "\76\144\2\0\152\144\46\0\7\132\14\0\5\132\5\0\1\132\1\133"+
-    "\12\132\1\0\15\132\1\0\5\132\1\0\1\132\1\0\2\132\1\0"+
-    "\2\132\1\0\154\132\41\0\u016b\132\22\0\100\132\2\0\66\132\50\0"+
-    "\14\132\4\0\20\133\1\137\2\0\1\136\1\137\13\0\7\133\14\0"+
-    "\2\141\30\0\3\141\1\137\1\0\1\140\1\0\1\137\1\136\32\0"+
-    "\5\132\1\0\207\132\2\0\1\133\7\0\1\140\4\0\1\137\1\0"+
-    "\1\140\1\0\12\134\1\136\1\137\5\0\32\132\4\0\1\141\1\0"+
-    "\32\132\13\0\70\135\2\133\37\146\3\0\6\146\2\0\6\146\2\0"+
-    "\6\146\2\0\3\146\34\0\3\133\4\0";
-
-  /** 
-   * Translates characters to character classes
-   */
-  private static final char [] ZZ_CMAP = zzUnpackCMap(ZZ_CMAP_PACKED);
-
-  /** 
-   * Translates DFA states to action switch labels.
-   */
-  private static final int [] ZZ_ACTION = zzUnpackAction();
-
-  private static final String ZZ_ACTION_PACKED_0 =
-    "\1\0\23\1\1\2\1\3\1\4\1\1\1\5\1\6"+
-    "\1\7\1\10\1\1\3\2\3\3\3\1\15\0\1\2"+
-    "\1\0\1\2\10\0\1\3\15\0\1\2\12\0\2\2"+
-    "\1\0\3\2\1\0\1\3\1\0\2\3\1\2\1\3"+
-    "\53\0\32\2\3\0\4\2\32\0\4\3\17\0\1\11"+
-    "\1\0\6\12\3\2\2\12\1\2\4\12\1\2\2\12"+
-    "\2\0\1\2\1\0\1\2\6\12\3\0\2\12\1\0"+
-    "\4\12\1\0\2\12\1\0\2\3\10\0\1\12\32\0"+
-    "\1\12\1\0\3\12\6\2\1\0\1\2\2\0\1\2"+
-    "\1\0\1\12\10\0\3\3\15\0\3\12\6\11\3\0"+
-    "\2\11\1\0\4\11\1\0\2\11\2\12\1\0\2\12"+
-    "\1\0\2\12\1\0\1\12\2\2\7\0\2\3\20\0"+
-    "\1\11\10\0\1\12\3\0\1\2\36\0\3\12\23\0"+
-    "\1\12\36\0\1\12\4\0\1\12\6\0\1\12\4\0"+
-    "\2\12\42\0\1\12\57\0\1\12\51\0\1\12\60\0"+
-    "\1\12\140\0\1\12\135\0\1\12\123\0\1\12\106\0"+
-    "\1\12\57\0\1\12\362\0";
-
-  private static int [] zzUnpackAction() {
-    int [] result = new int[1331];
-    int offset = 0;
-    offset = zzUnpackAction(ZZ_ACTION_PACKED_0, offset, result);
-    return result;
-  }
-
-  private static int zzUnpackAction(String packed, int offset, int [] result) {
-    int i = 0;       /* index in packed string  */
-    int j = offset;  /* index in unpacked array */
-    int l = packed.length();
-    while (i < l) {
-      int count = packed.charAt(i++);
-      int value = packed.charAt(i++);
-      do result[j++] = value; while (--count > 0);
-    }
-    return j;
-  }
-
-
-  /** 
-   * Translates a state to a row index in the transition table
-   */
-  private static final int [] ZZ_ROWMAP = zzUnpackRowMap();
-
-  private static final String ZZ_ROWMAP_PACKED_0 =
-    "\0\0\0\243\0\u0146\0\u01e9\0\u028c\0\u032f\0\u03d2\0\u0475"+
-    "\0\u0518\0\u05bb\0\u065e\0\u0701\0\u07a4\0\u0847\0\u08ea\0\u098d"+
-    "\0\u0a30\0\u0ad3\0\u0b76\0\u0c19\0\u0cbc\0\u0d5f\0\u0e02\0\u0ea5"+
-    "\0\u0f48\0\243\0\243\0\u0feb\0\u108e\0\u1131\0\u11d4\0\u1277"+
-    "\0\u131a\0\u13bd\0\u1460\0\u1503\0\u15a6\0\u1649\0\u0146\0\u01e9"+
-    "\0\u028c\0\u032f\0\u03d2\0\u16ec\0\u178f\0\u1832\0\u18d5\0\u0701"+
-    "\0\u1978\0\u1a1b\0\u1abe\0\u1b61\0\u1c04\0\u1ca7\0\u1d4a\0\u0518"+
-    "\0\u05bb\0\u1ded\0\u1e90\0\u1f33\0\u1fd6\0\u2079\0\u211c\0\u21bf"+
-    "\0\u2262\0\u2305\0\u23a8\0\u244b\0\u24ee\0\u2591\0\u2634\0\u26d7"+
-    "\0\u277a\0\u281d\0\u28c0\0\u0ea5\0\u2963\0\u2a06\0\u2aa9\0\u2b4c"+
-    "\0\u2bef\0\u2c92\0\u2d35\0\u108e\0\u2dd8\0\u2e7b\0\u2f1e\0\u2fc1"+
-    "\0\u3064\0\u3107\0\u31aa\0\u324d\0\u32f0\0\u3393\0\u3436\0\u34d9"+
-    "\0\u357c\0\u361f\0\u36c2\0\u3765\0\u1503\0\u3808\0\u38ab\0\u1649"+
-    "\0\u394e\0\u39f1\0\u3a94\0\u3b37\0\u3bda\0\u3c7d\0\u3d20\0\u3dc3"+
-    "\0\u3e66\0\u3f09\0\u3fac\0\u404f\0\u40f2\0\u4195\0\u4238\0\u42db"+
-    "\0\u437e\0\u4421\0\u44c4\0\u4567\0\u460a\0\u46ad\0\u4750\0\u47f3"+
-    "\0\u4896\0\u4939\0\u49dc\0\u4a7f\0\u4b22\0\u4bc5\0\u4c68\0\u4d0b"+
-    "\0\u4dae\0\u4e51\0\u4ef4\0\u4f97\0\u503a\0\u50dd\0\u5180\0\u5223"+
-    "\0\u52c6\0\u5369\0\u540c\0\u54af\0\u5552\0\u55f5\0\u5698\0\u573b"+
-    "\0\u57de\0\u5881\0\u5924\0\u59c7\0\u5a6a\0\u5b0d\0\u5bb0\0\u5c53"+
-    "\0\u5cf6\0\u5d99\0\u5e3c\0\u5edf\0\u5f82\0\u6025\0\u60c8\0\u616b"+
-    "\0\u620e\0\u62b1\0\u6354\0\u63f7\0\u649a\0\u653d\0\u65e0\0\u6683"+
-    "\0\u6726\0\u67c9\0\u686c\0\u690f\0\u69b2\0\u6a55\0\u6af8\0\u6b9b"+
-    "\0\u6c3e\0\u6ce1\0\u6d84\0\u6e27\0\u6eca\0\u6f6d\0\u7010\0\u70b3"+
-    "\0\u7156\0\u71f9\0\u729c\0\u733f\0\u73e2\0\u7485\0\u7528\0\u75cb"+
-    "\0\u766e\0\u7711\0\u77b4\0\u7857\0\u78fa\0\u799d\0\u7a40\0\u7ae3"+
-    "\0\u7b86\0\u7c29\0\u7ccc\0\u7d6f\0\u7e12\0\u7eb5\0\u7f58\0\u7ffb"+
-    "\0\u809e\0\u8141\0\u81e4\0\u8287\0\u832a\0\243\0\u83cd\0\u8470"+
-    "\0\u8513\0\u85b6\0\u8659\0\u86fc\0\u879f\0\u8842\0\u88e5\0\u8988"+
-    "\0\u8a2b\0\u8ace\0\u8b71\0\u8c14\0\u8cb7\0\u8d5a\0\u8dfd\0\u8ea0"+
-    "\0\u8f43\0\u8fe6\0\u9089\0\u912c\0\u91cf\0\u9272\0\u9315\0\u93b8"+
-    "\0\u945b\0\u94fe\0\u95a1\0\u9644\0\u96e7\0\u978a\0\u982d\0\u98d0"+
-    "\0\u9973\0\u9a16\0\u9ab9\0\u9b5c\0\u9bff\0\u9ca2\0\u9d45\0\u9de8"+
-    "\0\u9e8b\0\u9f2e\0\u9fd1\0\ua074\0\ua117\0\ua1ba\0\ua25d\0\ua300"+
-    "\0\ua3a3\0\ua446\0\ua4e9\0\ua58c\0\ua62f\0\ua6d2\0\ua775\0\ua818"+
-    "\0\ua8bb\0\ua95e\0\uaa01\0\uaaa4\0\uab47\0\uabea\0\uac8d\0\uad30"+
-    "\0\uadd3\0\uae76\0\uaf19\0\uafbc\0\ub05f\0\ub102\0\ub1a5\0\ub248"+
-    "\0\ub2eb\0\ub38e\0\ub431\0\ub4d4\0\ub577\0\ub61a\0\ub6bd\0\ub760"+
-    "\0\ub803\0\ub8a6\0\ub949\0\ub9ec\0\uba8f\0\ubb32\0\ubbd5\0\ubc78"+
-    "\0\ubd1b\0\ubdbe\0\ube61\0\ubf04\0\ubfa7\0\uc04a\0\uc0ed\0\uc190"+
-    "\0\uc233\0\uc2d6\0\uc379\0\uc41c\0\uc4bf\0\uc562\0\uc605\0\uc6a8"+
-    "\0\uc74b\0\uc7ee\0\uc891\0\uc934\0\uc9d7\0\uca7a\0\ucb1d\0\ucbc0"+
-    "\0\ucc63\0\ucd06\0\ucda9\0\uce4c\0\uceef\0\ucf92\0\ud035\0\ud0d8"+
-    "\0\ud17b\0\ud21e\0\ud2c1\0\ud364\0\ud407\0\ud4aa\0\ud54d\0\ud5f0"+
-    "\0\ud693\0\ud736\0\ud7d9\0\ud87c\0\ud91f\0\ud9c2\0\uda65\0\udb08"+
-    "\0\udbab\0\udc4e\0\udcf1\0\udd94\0\ude37\0\udeda\0\udf7d\0\ue020"+
-    "\0\ue0c3\0\ue166\0\ue209\0\ue2ac\0\ue34f\0\ue3f2\0\ue495\0\ue538"+
-    "\0\ue5db\0\ue67e\0\ue721\0\ue7c4\0\ue867\0\ue90a\0\ue9ad\0\uea50"+
-    "\0\ueaf3\0\ueb96\0\uec39\0\uecdc\0\ued7f\0\uee22\0\ueec5\0\uef68"+
-    "\0\uf00b\0\uf0ae\0\uf151\0\uf1f4\0\uf297\0\uf33a\0\uf3dd\0\uf480"+
-    "\0\uf523\0\uf5c6\0\uf669\0\uf70c\0\uf7af\0\u8287\0\uf852\0\uf8f5"+
-    "\0\uf998\0\ufa3b\0\ufade\0\ufb81\0\ufc24\0\ufcc7\0\ufd6a\0\ufe0d"+
-    "\0\ufeb0\0\uff53\0\ufff6\1\231\1\u013c\1\u01df\1\u0282\1\u0325"+
-    "\1\u03c8\1\u046b\1\u050e\1\u05b1\1\u0654\1\u06f7\1\u079a\1\u083d"+
-    "\1\u08e0\1\u0983\1\u0a26\1\u0ac9\1\u0b6c\1\u0c0f\1\u0cb2\1\u0d55"+
-    "\1\u0df8\1\u0e9b\1\u0f3e\1\u0fe1\1\u1084\1\u1127\1\u11ca\1\u126d"+
-    "\1\u1310\1\u13b3\1\u1456\1\u14f9\1\u159c\1\u163f\1\u16e2\1\u1785"+
-    "\1\u1828\1\u18cb\1\u196e\1\u1a11\1\u1ab4\1\u1b57\1\u1bfa\1\u1c9d"+
-    "\1\u1d40\1\u1de3\1\u1e86\1\u1f29\1\u1fcc\1\u206f\1\u2112\1\u21b5"+
-    "\1\u2258\1\u22fb\1\u239e\1\u2441\1\u24e4\1\u2587\1\u262a\1\u26cd"+
-    "\1\u2770\1\u2813\1\u28b6\1\u2959\1\u29fc\1\u2a9f\1\u2b42\1\u2be5"+
-    "\1\u2c88\1\u2d2b\1\u2dce\1\u2e71\1\u2f14\1\u2fb7\1\u305a\1\u30fd"+
-    "\1\u31a0\1\u3243\1\u32e6\1\u3389\1\u342c\1\u34cf\1\u3572\1\u3615"+
-    "\1\u36b8\1\u375b\1\u37fe\1\u38a1\1\u3944\1\u39e7\1\u3a8a\1\u3b2d"+
-    "\1\u3bd0\1\u3c73\1\u3d16\1\u3db9\1\u3e5c\1\u3eff\0\u15a6\1\u3fa2"+
-    "\1\u4045\1\u40e8\1\u418b\1\u422e\1\u42d1\1\u4374\1\u4417\1\u44ba"+
-    "\1\u455d\1\u4600\1\u46a3\1\u4746\1\u47e9\1\u488c\1\u492f\1\u49d2"+
-    "\1\u4a75\1\u4b18\1\u4bbb\1\u4c5e\1\u4d01\1\u4da4\1\u4e47\1\u4eea"+
-    "\1\u4f8d\1\u5030\1\u50d3\1\u5176\1\u5219\1\u52bc\1\u535f\1\u5402"+
-    "\1\u54a5\1\u5548\1\u55eb\1\u568e\1\u5731\1\u57d4\1\u5877\1\u591a"+
-    "\1\u59bd\1\u5a60\1\u5b03\1\u5ba6\1\u5c49\1\u5cec\1\u5d8f\1\u5e32"+
-    "\1\u5ed5\1\u5f78\1\u601b\1\u60be\1\u6161\1\u6204\1\u62a7\1\u634a"+
-    "\1\u63ed\1\u6490\1\u6533\1\u65d6\1\u6679\1\u671c\1\u67bf\1\u6862"+
-    "\1\u6905\1\u69a8\1\u6a4b\1\u6aee\1\u6b91\1\u6c34\1\u6cd7\1\u6d7a"+
-    "\1\u6e1d\1\u6ec0\1\u6f63\1\u7006\1\u70a9\1\u714c\1\u71ef\1\u7292"+
-    "\1\u7335\1\u73d8\1\u747b\1\u751e\1\u75c1\1\u7664\1\u7707\1\u77aa"+
-    "\1\u784d\1\u78f0\1\u7993\1\u7a36\1\u7ad9\1\u7b7c\1\u7c1f\1\u7cc2"+
-    "\1\u7d65\1\u7e08\1\u7eab\1\u7f4e\1\u7ff1\1\u8094\1\u8137\1\u81da"+
-    "\1\u827d\1\u8320\1\u83c3\1\u8466\1\u8509\1\u85ac\1\u864f\1\u86f2"+
-    "\1\u8795\1\u8838\1\u88db\1\u897e\1\u8a21\1\u8ac4\1\u8b67\1\u8c0a"+
-    "\1\u8cad\1\u8d50\1\u8df3\1\u8e96\1\u8f39\1\u8fdc\1\u907f\1\u9122"+
-    "\1\u91c5\1\u9268\1\u930b\1\u93ae\1\u9451\1\u94f4\1\u9597\1\u963a"+
-    "\1\u96dd\1\u9780\1\u9823\1\u98c6\1\u9969\1\u9a0c\1\u9aaf\1\u9b52"+
-    "\1\u9bf5\1\u9c98\1\u9d3b\1\u9dde\1\u9e81\1\u9f24\1\u9fc7\1\ua06a"+
-    "\1\ua10d\1\ua1b0\1\ua253\1\ua2f6\1\ua399\1\ua43c\1\ua4df\1\ua582"+
-    "\1\ua625\1\ua6c8\1\ua76b\1\ua80e\1\ua8b1\1\ua954\1\ua9f7\1\uaa9a"+
-    "\1\uab3d\1\uabe0\1\uac83\1\uad26\1\uadc9\1\uae6c\1\uaf0f\1\uafb2"+
-    "\1\ub055\1\ub0f8\1\ub19b\1\ub23e\1\ub2e1\1\ub384\1\ub427\1\ub4ca"+
-    "\1\ub56d\1\ub610\1\ub6b3\1\ub756\1\ub7f9\1\ub89c\1\ub93f\1\ub9e2"+
-    "\1\uba85\1\ubb28\1\ubbcb\1\ubc6e\1\ubd11\1\ubdb4\1\ube57\1\ubefa"+
-    "\1\ubf9d\1\uc040\1\uc0e3\1\uc186\1\uc229\1\uc2cc\1\uc36f\1\uc412"+
-    "\1\uc4b5\1\uc558\1\uc5fb\1\uc69e\1\uc741\1\uc7e4\1\uc887\1\uc92a"+
-    "\1\uc9cd\1\uca70\1\ucb13\1\ucbb6\1\ucc59\1\uccfc\1\ucd9f\1\uce42"+
-    "\1\ucee5\1\ucf88\1\ud02b\1\ud0ce\1\ud171\1\ud214\1\ud2b7\1\ud35a"+
-    "\1\ud3fd\1\ud4a0\1\ud543\1\ud5e6\1\ud689\1\ud72c\1\ud7cf\1\ud872"+
-    "\1\ud915\1\ud9b8\1\uda5b\1\udafe\1\udba1\1\udc44\1\udce7\1\udd8a"+
-    "\1\ude2d\1\uded0\1\udf73\1\ue016\1\ue0b9\1\ue15c\1\ue1ff\1\ue2a2"+
-    "\1\ue345\1\ue3e8\1\ue48b\1\ue52e\1\ue5d1\1\ue674\1\ue717\1\ue7ba"+
-    "\1\ue85d\1\ue900\1\ue9a3\1\uea46\1\ueae9\1\ueb8c\1\uec2f\1\uecd2"+
-    "\1\ued75\1\uee18\1\ueebb\1\uef5e\1\uf001\1\uf0a4\1\uf147\1\uf1ea"+
-    "\1\uf28d\1\uf330\1\uf3d3\1\uf476\1\uf519\1\uf5bc\1\uf65f\1\uf702"+
-    "\1\uf7a5\1\uf848\1\uf8eb\1\uf98e\1\ufa31\1\ufad4\1\ufb77\1\ufc1a"+
-    "\1\ufcbd\1\ufd60\1\ufe03\1\ufea6\1\uff49\1\uffec\2\217\2\u0132"+
-    "\2\u01d5\2\u0278\2\u031b\2\u03be\2\u0461\2\u0504\2\u05a7\2\u064a"+
-    "\2\u06ed\2\u0790\2\u0833\2\u08d6\2\u0979\2\u0a1c\2\u0abf\2\u0b62"+
-    "\2\u0c05\2\u0ca8\2\u0d4b\2\u0dee\2\u0e91\2\u0f34\2\u0fd7\2\u107a"+
-    "\2\u111d\2\u11c0\2\u1263\2\u1306\2\u13a9\2\u144c\2\u14ef\2\u1592"+
-    "\2\u1635\2\u16d8\2\u177b\2\u181e\2\u18c1\2\u1964\2\u1a07\2\u1aaa"+
-    "\2\u1b4d\2\u1bf0\2\u1c93\2\u1d36\2\u1dd9\2\u1e7c\2\u1f1f\2\u1fc2"+
-    "\2\u2065\2\u2108\2\u21ab\2\u224e\2\u22f1\2\u2394\2\u2437\2\u24da"+
-    "\2\u257d\2\u2620\2\u26c3\2\u2766\2\u2809\2\u28ac\2\u294f\2\u29f2"+
-    "\2\u2a95\2\u2b38\2\u2bdb\2\u2c7e\2\u2d21\2\u2dc4\2\u2e67\2\u2f0a"+
-    "\2\u2fad\2\u3050\2\u30f3\2\u3196\2\u3239\2\u32dc\2\u337f\2\u3422"+
-    "\2\u34c5\2\u3568\2\u360b\2\u36ae\2\u3751\2\u37f4\2\u3897\2\u393a"+
-    "\2\u39dd\2\u3a80\2\u3b23\2\u3bc6\2\u3c69\2\u3d0c\2\u3daf\2\u3e52"+
-    "\2\u3ef5\2\u3f98\2\u403b\2\u40de\2\u4181\2\u4224\2\u42c7\2\u436a"+
-    "\2\u440d\2\u44b0\2\u4553\2\u45f6\2\u4699\2\u473c\2\u47df\2\u4882"+
-    "\2\u4925\2\u49c8\2\u4a6b\2\u4b0e\2\u4bb1\2\u4c54\2\u4cf7\2\u4d9a"+
-    "\2\u4e3d\2\u4ee0\2\u4f83\2\u5026\2\u50c9\2\u516c\2\u520f\2\u52b2"+
-    "\2\u5355\2\u53f8\2\u549b\2\u553e\2\u55e1\2\u5684\2\u5727\2\u57ca"+
-    "\2\u586d\2\u5910\2\u59b3\2\u5a56\2\u5af9\2\u5b9c\2\u5c3f\2\u5ce2"+
-    "\2\u5d85\2\u5e28\2\u5ecb\2\u5f6e\2\u6011\2\u60b4\2\u6157\2\u61fa"+
-    "\2\u629d\2\u6340\2\u63e3\2\u6486\2\u6529\2\u65cc\2\u666f\2\u6712"+
-    "\2\u67b5\2\u6858\2\u68fb\2\u699e\2\u6a41\2\u6ae4\2\u6b87\2\u6c2a"+
-    "\2\u6ccd\2\u6d70\2\u6e13\2\u6eb6\2\u6f59\2\u6ffc\2\u709f\2\u7142"+
-    "\2\u71e5\2\u7288\2\u732b\2\u73ce\2\u7471\2\u7514\2\u75b7\2\u765a"+
-    "\2\u76fd\2\u77a0\2\u7843\2\u78e6\2\u7989\2\u7a2c\2\u7acf\2\u7b72"+
-    "\2\u7c15\2\u7cb8\2\u7d5b\2\u7dfe\2\u7ea1\2\u7f44\2\u7fe7\2\u808a"+
-    "\2\u812d\2\u81d0\2\u8273\2\u8316\2\u83b9\2\u845c\2\u84ff\2\u85a2"+
-    "\2\u8645\2\u86e8\2\u878b\2\u882e\2\u88d1\2\u8974\2\u8a17\2\u8aba"+
-    "\2\u8b5d\2\u8c00\2\u8ca3\2\u8d46\2\u8de9\2\u8e8c\2\u8f2f\2\u8fd2"+
-    "\2\u9075\2\u9118\2\u91bb\2\u925e\2\u9301\2\u93a4\2\u9447\2\u94ea"+
-    "\2\u958d\2\u9630\2\u96d3\2\u9776\2\u9819\2\u98bc\2\u995f\2\u9a02"+
-    "\2\u9aa5\2\u9b48\2\u9beb\2\u9c8e\2\u9d31\2\u9dd4\2\u9e77\2\u9f1a"+
-    "\2\u9fbd\2\ua060\2\ua103\2\ua1a6\2\ua249\2\ua2ec\2\ua38f\2\ua432"+
-    "\2\ua4d5\2\ua578\2\ua61b\2\ua6be\2\ua761\2\ua804\2\ua8a7\2\ua94a"+
-    "\2\ua9ed\2\uaa90\2\uab33\2\uabd6\2\uac79\2\uad1c\2\uadbf\2\uae62"+
-    "\2\uaf05\2\uafa8\2\ub04b\2\ub0ee\2\ub191\2\ub234\2\ub2d7\2\ub37a"+
-    "\2\ub41d\2\ub4c0\2\ub563\2\ub606\2\ub6a9\2\ub74c\2\ub7ef\2\ub892"+
-    "\2\ub935\2\ub9d8\2\uba7b\2\ubb1e\2\ubbc1\2\ubc64\2\ubd07\2\ubdaa"+
-    "\2\ube4d\2\ubef0\2\ubf93\2\uc036\2\uc0d9\2\uc17c\2\uc21f\2\uc2c2"+
-    "\2\uc365\2\uc408\2\uc4ab\2\uc54e\2\uc5f1\2\uc694\2\uc737\2\uc7da"+
-    "\2\uc87d\2\uc920\2\uc9c3\2\uca66\2\ucb09\2\ucbac\2\ucc4f\2\uccf2"+
-    "\2\ucd95\2\uce38\2\ucedb\2\ucf7e\2\ud021\2\ud0c4\2\ud167\2\ud20a"+
-    "\2\ud2ad\2\ud350\2\ud3f3\2\ud496\2\ud539\2\ud5dc\2\ud67f\2\ud722"+
-    "\2\ud7c5\2\ud868\2\ud90b\2\ud9ae\2\uda51\2\udaf4\2\udb97\2\udc3a"+
-    "\2\udcdd\2\udd80\2\ude23\2\udec6\2\udf69\2\ue00c\2\ue0af\2\ue152"+
-    "\2\ue1f5\2\ue298\2\ue33b\2\ue3de\2\ue481\2\ue524\2\ue5c7\2\ue66a"+
-    "\2\ue70d\2\ue7b0\2\ue853\2\ue8f6\2\ue999\2\uea3c\2\ueadf\2\ueb82"+
-    "\2\uec25\2\uecc8\2\ued6b\2\uee0e\2\ueeb1\2\uef54\2\ueff7\2\uf09a"+
-    "\2\uf13d\2\uf1e0\2\uf283\2\uf326\2\uf3c9\2\uf46c\2\uf50f\2\uf5b2"+
-    "\2\uf655\2\uf6f8\2\uf79b\2\uf83e\2\uf8e1\2\uf984\2\ufa27\2\ufaca"+
-    "\2\ufb6d\2\ufc10\2\ufcb3\2\ufd56\2\ufdf9\2\ufe9c\2\uff3f\2\uffe2"+
-    "\3\205\3\u0128\3\u01cb\3\u026e\3\u0311\3\u03b4\3\u0457\3\u04fa"+
-    "\3\u059d\3\u0640\3\u06e3\3\u0786\3\u0829\3\u08cc\3\u096f\3\u0a12"+
-    "\3\u0ab5\3\u0b58\3\u0bfb\3\u0c9e\3\u0d41\3\u0de4\3\u0e87\3\u0f2a"+
-    "\3\u0fcd\3\u1070\3\u1113\3\u11b6\3\u1259\3\u12fc\3\u139f\3\u1442"+
-    "\3\u14e5\3\u1588\3\u162b\3\u16ce\3\u1771\3\u1814\3\u18b7\3\u195a"+
-    "\3\u19fd\3\u1aa0\3\u1b43\3\u1be6\3\u1c89\3\u1d2c\3\u1dcf\3\u1e72"+
-    "\3\u1f15\3\u1fb8\3\u205b\3\u20fe\3\u21a1\3\u2244\3\u22e7\3\u238a"+
-    "\3\u242d\3\u24d0\3\u2573\3\u2616\3\u26b9\3\u275c\3\u27ff\3\u28a2"+
-    "\3\u2945\3\u29e8\3\u2a8b\3\u2b2e\3\u2bd1\3\u2c74\3\u2d17\3\u2dba"+
-    "\3\u2e5d\3\u2f00\3\u2fa3\3\u3046\3\u30e9\3\u318c\3\u322f\3\u32d2"+
-    "\3\u3375\3\u3418\3\u34bb\3\u355e\3\u3601\3\u36a4\3\u3747\3\u37ea"+
-    "\3\u388d\3\u3930\3\u39d3\3\u3a76\3\u3b19\3\u3bbc\3\u3c5f\3\u3d02"+
-    "\3\u3da5\3\u3e48\3\u3eeb\3\u3f8e\3\u4031\3\u40d4\3\u4177\3\u421a"+
-    "\3\u42bd\3\u4360\3\u4403";
-
-  private static int [] zzUnpackRowMap() {
-    int [] result = new int[1331];
-    int offset = 0;
-    offset = zzUnpackRowMap(ZZ_ROWMAP_PACKED_0, offset, result);
-    return result;
-  }
-
-  private static int zzUnpackRowMap(String packed, int offset, int [] result) {
-    int i = 0;  /* index in packed string  */
-    int j = offset;  /* index in unpacked array */
-    int l = packed.length();
-    while (i < l) {
-      int high = packed.charAt(i++) << 16;
-      result[j++] = high | packed.charAt(i++);
-    }
-    return j;
-  }
-
-  /** 
-   * The transition table of the DFA
-   */
-  private static final int [] ZZ_TRANS = zzUnpackTrans();
-
-  private static final String ZZ_TRANS_PACKED_0 =
-    "\1\2\1\3\1\2\1\4\1\2\1\5\1\2\1\6"+
-    "\1\2\1\7\1\2\1\10\3\2\1\11\5\2\1\12"+
-    "\3\2\1\13\11\2\1\14\2\2\1\15\43\2\1\16"+
-    "\1\2\1\17\3\2\1\20\1\21\1\2\1\22\1\2"+
-    "\1\23\2\2\1\24\1\2\1\25\1\2\1\26\1\27"+
-    "\3\2\1\30\2\31\1\32\1\33\1\34\1\35\6\36"+
-    "\1\37\16\36\1\40\4\36\1\35\1\41\2\42\1\41"+
-    "\5\42\1\43\1\2\1\35\1\44\1\35\1\2\2\35"+
-    "\1\2\3\35\1\45\2\2\1\35\1\46\3\2\2\35"+
-    "\1\2\245\0\1\25\11\0\1\25\20\0\1\25\22\0"+
-    "\1\25\10\0\3\25\17\0\1\25\10\0\1\25\120\0"+
-    "\1\25\1\0\1\25\1\0\1\25\1\0\1\25\1\0"+
-    "\1\25\1\0\3\25\1\0\5\25\1\0\3\25\1\0"+
-    "\11\25\1\0\2\25\1\0\16\25\1\0\2\25\1\0"+
-    "\21\25\1\0\1\25\1\0\3\25\2\0\1\25\1\0"+
-    "\1\25\1\0\2\25\1\0\1\25\113\0\1\25\3\0"+
-    "\1\25\5\0\2\25\3\0\1\25\13\0\1\25\1\0"+
-    "\1\25\4\0\2\25\4\0\1\25\1\0\1\25\3\0"+
-    "\2\25\1\0\1\25\5\0\3\25\1\0\1\25\15\0"+
-    "\1\25\10\0\1\25\120\0\1\25\3\0\1\25\1\0"+
-    "\1\25\1\0\1\25\1\0\3\25\2\0\4\25\1\0"+
-    "\3\25\2\0\3\25\1\0\4\25\1\0\2\25\2\0"+
-    "\3\25\1\0\11\25\1\0\2\25\1\0\16\25\1\0"+
-    "\2\25\1\0\1\25\1\0\3\25\2\0\1\25\1\0"+
-    "\1\25\1\0\2\25\1\0\1\25\113\0\1\25\3\0"+
-    "\1\25\3\0\1\25\1\0\3\25\2\0\1\25\1\0"+
-    "\2\25\1\0\3\25\3\0\2\25\1\0\1\25\1\0"+
-    "\2\25\1\0\2\25\3\0\2\25\1\0\1\25\1\0"+
-    "\1\25\1\0\2\25\1\0\2\25\1\0\2\25\1\0"+
-    "\5\25\1\0\5\25\1\0\2\25\1\0\2\25\1\0"+
-    "\1\25\1\0\3\25\4\0\1\25\4\0\1\25\125\0"+
-    "\3\25\5\0\1\25\1\0\1\25\1\0\1\25\4\0"+
-    "\1\25\14\0\1\25\5\0\1\25\11\0\2\25\12\0"+
-    "\1\26\1\0\2\25\12\0\1\25\120\0\1\25\1\0"+
-    "\1\26\7\0\2\25\2\0\5\25\2\0\2\25\4\0"+
-    "\6\25\1\0\2\25\4\0\5\25\1\0\5\25\1\0"+
-    "\2\25\1\0\3\25\1\0\4\25\1\0\5\25\1\26"+
-    "\1\0\1\25\1\0\1\25\1\0\3\25\2\0\1\25"+
-    "\1\0\1\25\1\0\1\25\2\0\1\25\113\0\1\25"+
-    "\3\0\1\25\5\0\2\25\3\0\1\25\4\0\3\25"+
-    "\4\0\1\25\1\0\1\25\2\0\1\25\1\0\2\25"+
-    "\4\0\1\25\1\0\1\25\3\0\2\25\1\0\1\25"+
-    "\5\0\3\25\1\0\1\25\10\0\1\25\1\0\2\26"+
-    "\1\0\1\25\10\0\1\25\120\0\1\25\3\0\1\25"+
-    "\6\0\2\25\5\0\1\25\1\0\1\25\1\0\1\25"+
-    "\1\0\11\25\2\0\1\25\4\0\1\25\4\0\6\25"+
-    "\2\0\1\25\1\0\1\25\1\0\3\25\3\0\2\25"+
-    "\4\0\3\25\1\0\1\25\10\0\1\25\1\0\2\25"+
-    "\115\0\1\25\11\0\2\25\17\0\1\25\6\0\2\25"+
-    "\4\0\1\25\5\0\1\25\2\0\1\25\5\0\3\25"+
-    "\1\0\1\25\15\0\1\25\10\0\1\25\120\0\1\25"+
-    "\3\0\1\25\5\0\1\25\32\0\15\25\5\0\3\25"+
-    "\1\0\1\25\5\0\1\25\7\0\1\25\2\0\1\25"+
-    "\5\0\1\25\2\0\1\25\1\0\1\25\202\0\1\33"+
-    "\21\0\1\27\131\0\1\32\3\0\1\32\3\0\1\32"+
-    "\1\0\3\32\2\0\1\32\2\0\1\32\1\0\3\32"+
-    "\3\0\2\32\1\0\1\32\1\0\2\32\1\0\2\32"+
-    "\3\0\2\32\1\0\1\32\3\0\2\32\1\0\2\32"+
-    "\1\0\2\32\1\0\5\32\1\0\5\32\2\0\1\32"+
-    "\1\0\2\32\1\0\1\32\1\0\3\32\4\0\1\32"+
-    "\4\0\1\32\113\0\1\32\1\0\1\32\1\0\1\32"+
-    "\1\0\1\32\1\0\1\32\1\0\3\32\1\0\5\32"+
-    "\1\0\3\32\1\0\11\32\1\0\2\32\1\0\16\32"+
-    "\1\0\2\32\1\0\21\32\1\0\1\32\1\0\3\32"+
-    "\2\0\1\32\1\0\1\32\1\0\2\32\1\0\1\32"+
-    "\113\0\1\32\1\0\1\32\1\0\1\32\3\0\1\32"+
-    "\1\0\3\32\1\0\2\32\1\0\2\32\1\0\3\32"+
-    "\1\0\11\32\1\0\2\32\1\0\16\32\1\0\2\32"+
-    "\1\0\21\32\1\0\1\32\1\0\3\32\2\0\1\32"+
-    "\1\0\1\32\1\0\2\32\1\0\1\32\113\0\1\32"+
-    "\11\0\1\32\20\0\1\32\33\0\1\32\21\0\1\32"+
-    "\10\0\1\32\120\0\1\32\1\0\1\32\1\0\1\32"+
-    "\1\0\1\32\1\0\1\32\1\0\3\32\1\0\5\32"+
-    "\1\0\3\32\1\0\6\32\1\0\2\32\1\0\2\32"+
-    "\1\0\10\32\1\0\5\32\1\0\2\32\1\0\21\32"+
-    "\1\0\1\32\1\0\3\32\2\0\1\32\1\0\1\32"+
-    "\1\0\2\32\1\0\1\32\242\0\1\33\112\0\1\47"+
-    "\1\0\1\50\1\0\1\51\1\0\1\52\1\0\1\53"+
-    "\1\0\1\54\3\0\1\55\5\0\1\56\3\0\1\57"+
-    "\11\0\1\60\2\0\1\61\16\0\1\62\2\0\1\63"+
-    "\41\0\2\25\1\64\1\0\1\65\1\0\1\65\1\66"+
-    "\1\0\1\25\2\0\1\25\1\65\32\25\1\0\12\64"+
-    "\1\65\1\0\1\66\3\0\1\65\20\0\1\47\1\0"+
-    "\1\50\1\0\1\51\1\0\1\52\1\0\1\53\1\0"+
-    "\1\67\3\0\1\70\5\0\1\71\3\0\1\72\11\0"+
-    "\1\60\2\0\1\73\16\0\1\74\2\0\1\75\41\0"+
-    "\1\25\2\26\2\0\2\76\1\77\1\0\1\26\2\0"+
-    "\1\25\1\76\32\25\1\0\12\26\2\0\1\77\2\0"+
-    "\2\76\6\0\1\76\23\0\1\100\15\0\1\101\14\0"+
-    "\1\102\16\0\1\103\2\0\1\104\21\0\1\105\20\0"+
-    "\1\27\1\0\1\27\3\0\1\66\1\0\1\27\53\0"+
-    "\1\66\24\0\1\47\1\0\1\50\1\0\1\51\1\0"+
-    "\1\52\1\0\1\53\1\0\1\106\3\0\1\70\5\0"+
-    "\1\71\3\0\1\107\11\0\1\60\2\0\1\110\16\0"+
-    "\1\111\2\0\1\112\21\0\1\113\17\0\1\25\1\114"+
-    "\1\26\1\115\3\0\1\114\1\0\1\114\2\0\1\25"+
-    "\1\0\32\25\1\0\12\26\2\0\1\114\165\0\2\31"+
-    "\100\0\1\47\1\0\1\50\1\0\1\51\1\0\1\52"+
-    "\1\0\1\53\1\0\1\116\3\0\1\55\5\0\1\56"+
-    "\3\0\1\117\11\0\1\60\2\0\1\120\16\0\1\121"+
-    "\2\0\1\122\41\0\1\25\1\34\1\64\1\0\1\65"+
-    "\1\0\1\65\1\66\1\0\1\34\2\0\1\34\1\65"+
-    "\32\25\1\0\12\64\1\65\1\0\1\66\3\0\1\65"+
-    "\166\0\1\123\45\124\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\2\0\1\47\1\0"+
-    "\1\50\1\0\1\51\1\0\1\52\1\0\1\53\1\0"+
-    "\1\54\3\0\1\55\5\0\1\56\3\0\1\57\11\0"+
-    "\1\60\2\0\1\61\16\0\1\62\2\0\1\63\41\0"+
-    "\2\25\1\64\1\0\1\65\1\0\1\65\1\66\1\0"+
-    "\1\25\2\0\1\25\1\126\32\36\1\127\12\130\1\65"+
-    "\1\124\1\131\1\124\1\0\1\124\1\132\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\2\0\1\47\1\0\1\50"+
-    "\1\0\1\51\1\0\1\52\1\0\1\53\1\0\1\54"+
-    "\3\0\1\55\5\0\1\56\3\0\1\57\11\0\1\60"+
-    "\2\0\1\61\16\0\1\62\2\0\1\63\41\0\2\25"+
-    "\1\64\1\0\1\65\1\0\1\65\1\66\1\0\1\25"+
-    "\2\0\1\25\1\126\10\36\1\133\6\36\1\134\12\36"+
-    "\1\127\12\130\1\65\1\124\1\131\1\124\1\0\1\124"+
-    "\1\132\1\125\3\124\3\0\1\124\4\0\2\124\2\0"+
-    "\1\47\1\0\1\50\1\0\1\51\1\0\1\52\1\0"+
-    "\1\53\1\0\1\54\3\0\1\55\5\0\1\56\3\0"+
-    "\1\57\11\0\1\60\2\0\1\61\16\0\1\62\2\0"+
-    "\1\63\41\0\2\25\1\64\1\0\1\65\1\0\1\65"+
-    "\1\66\1\0\1\25\2\0\1\25\1\126\17\36\1\135"+
-    "\12\36\1\127\12\130\1\65\1\124\1\131\1\124\1\0"+
-    "\1\124\1\132\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\2\0\1\47\1\0\1\50\1\0\1\51\1\0\1\52"+
-    "\1\0\1\53\1\0\1\67\3\0\1\70\5\0\1\71"+
-    "\3\0\1\72\11\0\1\60\2\0\1\73\16\0\1\74"+
-    "\2\0\1\75\41\0\1\25\2\26\2\0\2\76\1\77"+
-    "\1\0\1\26\2\0\1\25\1\136\32\36\1\127\12\42"+
-    "\1\0\1\124\1\137\1\124\1\0\2\140\1\125\3\124"+
-    "\2\0\1\76\1\124\4\0\2\124\2\0\1\47\1\0"+
-    "\1\50\1\0\1\51\1\0\1\52\1\0\1\53\1\0"+
-    "\1\67\3\0\1\70\5\0\1\71\3\0\1\72\11\0"+
-    "\1\60\2\0\1\73\16\0\1\74\2\0\1\75\41\0"+
-    "\1\25\2\26\2\0\2\76\1\77\1\0\1\26\2\0"+
-    "\1\25\1\136\32\36\1\127\12\141\1\0\1\124\1\137"+
-    "\1\124\1\0\2\140\1\125\3\124\2\0\1\76\1\124"+
-    "\4\0\2\124\2\0\1\47\1\0\1\50\1\0\1\51"+
-    "\1\0\1\52\1\0\1\53\1\0\1\67\3\0\1\70"+
-    "\5\0\1\71\3\0\1\72\11\0\1\60\2\0\1\73"+
-    "\16\0\1\74\2\0\1\75\41\0\1\25\2\26\2\0"+
-    "\2\76\1\77\1\0\1\26\2\0\1\25\1\136\32\36"+
-    "\1\127\1\42\1\142\1\141\2\42\2\141\1\42\1\141"+
-    "\1\42\1\0\1\124\1\137\1\124\1\0\2\140\1\125"+
-    "\3\124\2\0\1\76\1\124\4\0\2\124\2\0\1\47"+
-    "\1\0\1\50\1\0\1\51\1\0\1\52\1\0\1\53"+
-    "\1\0\1\106\3\0\1\70\5\0\1\71\3\0\1\107"+
-    "\11\0\1\60\2\0\1\110\16\0\1\111\2\0\1\112"+
-    "\21\0\1\113\17\0\1\25\1\114\1\26\1\115\3\0"+
-    "\1\114\1\0\1\114\2\0\1\25\1\123\32\143\1\124"+
-    "\12\144\1\0\1\124\1\145\1\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\151\0\4\146\2\0"+
-    "\1\146\15\0\1\146\6\0\12\146\1\147\174\0\65\150"+
-    "\1\151\1\150\1\152\1\0\2\150\3\0\1\25\11\0"+
-    "\3\25\5\0\1\25\1\0\1\25\1\0\1\25\4\0"+
-    "\1\25\4\0\1\25\1\0\2\25\4\0\1\25\5\0"+
-    "\1\25\3\0\1\25\4\0\5\25\10\0\1\64\1\0"+
-    "\2\25\1\0\1\25\10\0\1\25\120\0\1\25\1\0"+
-    "\1\64\7\0\2\25\2\0\5\25\2\0\2\25\4\0"+
-    "\6\25\1\0\2\25\4\0\5\25\1\0\5\25\1\0"+
-    "\2\25\1\0\3\25\1\0\4\25\1\0\5\25\1\64"+
-    "\1\0\1\25\1\0\1\25\1\0\3\25\2\0\1\25"+
-    "\1\0\1\25\1\0\1\25\2\0\1\25\113\0\1\25"+
-    "\3\0\1\25\5\0\2\25\3\0\1\25\4\0\3\25"+
-    "\4\0\1\25\1\0\1\25\2\0\1\25\1\0\2\25"+
-    "\4\0\1\25\1\0\1\25\3\0\2\25\1\0\1\25"+
-    "\5\0\3\25\1\0\1\25\10\0\1\25\1\0\2\64"+
-    "\1\0\1\25\10\0\1\25\120\0\1\25\3\0\1\25"+
-    "\6\0\2\25\5\0\1\25\1\0\1\25\1\0\1\25"+
-    "\1\0\11\25\2\0\1\25\4\0\1\25\4\0\6\25"+
-    "\2\0\1\25\1\0\1\25\1\0\3\25\1\0\1\25"+
-    "\1\0\2\25\4\0\3\25\1\0\1\25\10\0\1\25"+
-    "\1\0\2\25\115\0\1\25\3\0\1\25\5\0\1\25"+
-    "\32\0\15\25\5\0\3\25\1\0\1\25\5\0\3\25"+
-    "\5\0\1\25\2\0\2\25\4\0\1\25\2\0\1\25"+
-    "\1\0\1\25\177\0\2\25\6\0\1\25\152\0\1\25"+
-    "\3\0\1\25\2\0\1\25\3\0\1\25\5\0\1\25"+
-    "\7\0\1\25\4\0\2\25\3\0\2\25\1\0\1\25"+
-    "\4\0\1\25\1\0\1\25\2\0\2\25\1\0\3\25"+
-    "\1\0\1\25\2\0\4\25\2\0\1\25\135\0\1\47"+
-    "\1\0\1\50\1\0\1\51\1\0\1\52\1\0\1\53"+
-    "\1\0\1\153\3\0\1\55\5\0\1\56\3\0\1\154"+
-    "\11\0\1\60\2\0\1\155\16\0\1\156\2\0\1\157"+
-    "\41\0\1\25\2\64\2\0\2\160\1\66\1\0\1\64"+
-    "\2\0\1\25\1\160\32\25\1\0\12\64\2\0\1\66"+
-    "\2\0\2\160\6\0\1\160\11\0\1\47\1\0\1\50"+
-    "\1\0\1\51\1\0\1\52\1\0\1\53\1\0\1\161"+
-    "\3\0\1\162\5\0\1\163\3\0\1\164\11\0\1\60"+
-    "\2\0\1\165\16\0\1\166\2\0\1\167\41\0\1\25"+
-    "\1\65\7\0\1\65\2\0\1\25\1\0\32\25\42\0"+
-    "\1\47\1\0\1\50\1\0\1\51\1\0\1\52\1\0"+
-    "\1\53\1\0\1\170\3\0\1\55\5\0\1\56\3\0"+
-    "\1\171\11\0\1\60\2\0\1\172\16\0\1\173\2\0"+
-    "\1\174\21\0\1\113\17\0\1\25\1\66\1\64\1\115"+
-    "\3\0\1\66\1\0\1\66\2\0\1\25\1\0\32\25"+
-    "\1\0\12\64\2\0\1\66\25\0\1\26\11\0\3\25"+
-    "\5\0\1\25\1\0\1\25\1\0\1\25\4\0\1\25"+
-    "\4\0\1\26\1\0\2\26\4\0\1\25\5\0\1\25"+
-    "\3\0\1\26\4\0\1\26\2\25\2\26\10\0\1\26"+
-    "\1\0\2\25\1\0\1\26\10\0\1\25\120\0\1\25"+
-    "\3\0\1\25\6\0\2\25\5\0\1\25\1\0\1\25"+
-    "\1\0\1\25\1\0\11\25\2\0\1\25\4\0\1\25"+
-    "\4\0\6\25\2\0\1\25\1\0\1\25\1\0\3\25"+
-    "\1\0\1\26\1\0\2\25\4\0\3\25\1\0\1\25"+
-    "\10\0\1\25\1\0\2\25\115\0\1\25\3\0\1\25"+
-    "\5\0\1\25\32\0\15\25\5\0\3\25\1\0\1\25"+
-    "\5\0\1\25\2\26\5\0\1\25\2\0\1\25\1\26"+
-    "\4\0\1\25\2\0\1\25\1\0\1\25\177\0\2\26"+
-    "\6\0\1\26\152\0\1\26\3\0\1\26\2\0\1\26"+
-    "\3\0\1\26\5\0\1\26\7\0\1\26\4\0\2\26"+
-    "\3\0\2\26\1\0\1\26\4\0\1\26\1\0\1\26"+
-    "\2\0\2\26\1\0\3\26\1\0\1\26\2\0\4\26"+
-    "\2\0\1\26\147\0\1\175\3\0\1\176\5\0\1\177"+
-    "\3\0\1\200\14\0\1\201\16\0\1\202\2\0\1\203"+
-    "\42\0\1\76\1\26\6\0\1\76\37\0\12\26\27\0"+
-    "\1\47\1\0\1\50\1\0\1\51\1\0\1\52\1\0"+
-    "\1\53\1\0\1\204\3\0\1\70\5\0\1\71\3\0"+
-    "\1\205\11\0\1\60\2\0\1\206\16\0\1\207\2\0"+
-    "\1\210\21\0\1\113\17\0\1\25\1\77\1\26\1\115"+
-    "\3\0\1\77\1\0\1\77\2\0\1\25\1\0\32\25"+
-    "\1\0\12\26\2\0\1\77\25\0\1\27\37\0\1\27"+
-    "\1\0\2\27\16\0\1\27\4\0\1\27\2\0\2\27"+
-    "\15\0\1\27\226\0\1\27\247\0\2\27\11\0\1\27"+
-    "\211\0\2\27\6\0\1\27\152\0\1\27\3\0\1\27"+
-    "\2\0\1\27\3\0\1\27\5\0\1\27\7\0\1\27"+
-    "\4\0\2\27\3\0\2\27\1\0\1\27\4\0\1\27"+
-    "\1\0\1\27\2\0\2\27\1\0\3\27\1\0\1\27"+
-    "\2\0\4\27\2\0\1\27\247\0\1\27\131\0\1\114"+
-    "\11\0\3\25\5\0\1\25\1\0\1\25\1\0\1\25"+
-    "\4\0\1\25\4\0\1\114\1\0\2\114\4\0\1\25"+
-    "\5\0\1\25\3\0\1\114\4\0\1\114\2\25\2\114"+
-    "\10\0\1\26\1\0\2\25\1\0\1\114\10\0\1\25"+
-    "\120\0\1\25\3\0\1\25\6\0\2\25\5\0\1\25"+
-    "\1\0\1\25\1\0\1\25\1\0\11\25\2\0\1\25"+
-    "\4\0\1\25\4\0\6\25\2\0\1\25\1\0\1\25"+
-    "\1\0\3\25\1\0\1\114\1\0\2\25\4\0\3\25"+
-    "\1\0\1\25\10\0\1\25\1\0\2\25\115\0\1\25"+
-    "\3\0\1\25\5\0\1\25\32\0\15\25\5\0\3\25"+
-    "\1\0\1\25\5\0\1\25\2\114\5\0\1\25\2\0"+
-    "\1\25\1\114\4\0\1\25\2\0\1\25\1\0\1\25"+
-    "\177\0\2\114\6\0\1\114\152\0\1\114\3\0\1\114"+
-    "\2\0\1\114\3\0\1\114\5\0\1\114\7\0\1\114"+
-    "\4\0\2\114\3\0\2\114\1\0\1\114\4\0\1\114"+
-    "\1\0\1\114\2\0\2\114\1\0\3\114\1\0\1\114"+
-    "\2\0\4\114\2\0\1\114\247\0\1\115\142\0\1\211"+
-    "\15\0\1\212\14\0\1\213\16\0\1\214\2\0\1\215"+
-    "\21\0\1\113\20\0\1\115\1\0\1\115\3\0\1\66"+
-    "\1\0\1\115\53\0\1\66\25\0\1\34\11\0\3\25"+
-    "\5\0\1\25\1\0\1\25\1\0\1\25\4\0\1\25"+
-    "\4\0\1\34\1\0\2\34\4\0\1\25\5\0\1\25"+
-    "\3\0\1\34\4\0\1\34\2\25\2\34\10\0\1\64"+
-    "\1\0\2\25\1\0\1\34\10\0\1\25\120\0\1\25"+
-    "\3\0\1\25\6\0\2\25\5\0\1\25\1\0\1\25"+
-    "\1\0\1\25\1\0\11\25\2\0\1\25\4\0\1\25"+
-    "\4\0\6\25\2\0\1\25\1\0\1\25\1\0\3\25"+
-    "\1\0\1\34\1\0\2\25\4\0\3\25\1\0\1\25"+
-    "\10\0\1\25\1\0\2\25\115\0\1\25\3\0\1\25"+
-    "\5\0\1\25\32\0\15\25\5\0\3\25\1\0\1\25"+
-    "\5\0\1\25\2\34\5\0\1\25\2\0\1\25\1\34"+
-    "\4\0\1\25\2\0\1\25\1\0\1\25\177\0\2\34"+
-    "\6\0\1\34\152\0\1\34\3\0\1\34\2\0\1\34"+
-    "\3\0\1\34\5\0\1\34\7\0\1\34\4\0\2\34"+
-    "\3\0\2\34\1\0\1\34\4\0\1\34\1\0\1\34"+
-    "\2\0\2\34\1\0\3\34\1\0\1\34\2\0\4\34"+
-    "\2\0\1\34\303\0\1\123\45\124\1\0\3\124\1\0"+
-    "\2\124\1\125\3\124\3\0\1\124\1\150\3\0\2\124"+
-    "\151\0\32\216\1\0\12\216\13\0\1\217\13\0\1\47"+
-    "\1\0\1\50\1\0\1\51\1\0\1\52\1\0\1\53"+
-    "\1\0\1\161\3\0\1\162\5\0\1\163\3\0\1\164"+
-    "\11\0\1\60\2\0\1\165\16\0\1\166\2\0\1\167"+
-    "\41\0\1\25\1\65\7\0\1\65\2\0\1\25\1\123"+
-    "\1\220\1\221\1\222\1\223\1\224\1\225\1\226\1\227"+
-    "\1\230\1\231\1\232\1\233\1\234\1\235\1\236\1\237"+
-    "\1\240\1\241\1\242\1\243\1\244\1\245\1\246\1\247"+
-    "\1\250\1\251\1\124\12\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\1\150\3\0\2\124\150\0"+
-    "\1\123\32\252\1\127\12\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\2\0\1\47"+
-    "\1\0\1\50\1\0\1\51\1\0\1\52\1\0\1\53"+
-    "\1\0\1\153\3\0\1\55\5\0\1\56\3\0\1\154"+
-    "\11\0\1\60\2\0\1\155\16\0\1\156\2\0\1\157"+
-    "\41\0\1\25\2\64\2\0\2\160\1\66\1\0\1\64"+
-    "\2\0\1\25\1\253\32\36\1\127\12\130\1\0\1\124"+
-    "\1\131\1\124\1\0\2\254\1\125\3\124\2\0\1\160"+
-    "\1\124\4\0\2\124\2\0\1\47\1\0\1\50\1\0"+
-    "\1\51\1\0\1\52\1\0\1\53\1\0\1\170\3\0"+
-    "\1\55\5\0\1\56\3\0\1\171\11\0\1\60\2\0"+
-    "\1\172\16\0\1\173\2\0\1\174\21\0\1\113\17\0"+
-    "\1\25\1\66\1\64\1\115\3\0\1\66\1\0\1\66"+
-    "\2\0\1\25\1\123\32\143\1\124\12\255\1\0\1\124"+
-    "\1\131\1\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\2\0\1\47\1\0\1\50\1\0\1\51"+
-    "\1\0\1\52\1\0\1\53\1\0\1\161\3\0\1\162"+
-    "\5\0\1\163\3\0\1\164\11\0\1\60\2\0\1\165"+
-    "\16\0\1\166\2\0\1\167\41\0\1\25\1\65\7\0"+
-    "\1\65\2\0\1\25\1\123\32\143\13\124\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\2\0\1\47\1\0\1\50\1\0\1\51\1\0\1\52"+
-    "\1\0\1\53\1\0\1\54\3\0\1\55\5\0\1\56"+
-    "\3\0\1\57\11\0\1\60\2\0\1\61\16\0\1\62"+
-    "\2\0\1\63\41\0\2\25\1\64\1\0\1\65\1\0"+
-    "\1\65\1\66\1\0\1\25\2\0\1\25\1\126\11\36"+
-    "\1\256\20\36\1\127\12\130\1\65\1\124\1\131\1\124"+
-    "\1\0\1\124\1\132\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\2\0\1\47\1\0\1\50\1\0\1\51\1\0"+
-    "\1\52\1\0\1\53\1\0\1\54\3\0\1\55\5\0"+
-    "\1\56\3\0\1\57\11\0\1\60\2\0\1\61\16\0"+
-    "\1\62\2\0\1\63\41\0\2\25\1\64\1\0\1\65"+
-    "\1\0\1\65\1\66\1\0\1\25\2\0\1\25\1\126"+
-    "\15\36\1\257\14\36\1\127\12\130\1\65\1\124\1\131"+
-    "\1\124\1\0\1\124\1\132\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\2\0\1\47\1\0\1\50\1\0\1\51"+
-    "\1\0\1\52\1\0\1\53\1\0\1\54\3\0\1\55"+
-    "\5\0\1\56\3\0\1\57\11\0\1\60\2\0\1\61"+
-    "\16\0\1\62\2\0\1\63\41\0\2\25\1\64\1\0"+
-    "\1\65\1\0\1\65\1\66\1\0\1\25\2\0\1\25"+
-    "\1\126\17\36\1\260\12\36\1\127\12\130\1\65\1\124"+
-    "\1\131\1\124\1\0\1\124\1\132\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\14\0\1\175\3\0\1\176\5\0"+
-    "\1\177\3\0\1\200\14\0\1\201\16\0\1\202\2\0"+
-    "\1\203\42\0\1\76\1\26\6\0\1\76\3\0\1\123"+
-    "\1\261\1\262\1\263\1\264\1\265\1\266\1\267\1\270"+
-    "\1\271\1\272\1\273\1\274\1\275\1\276\1\277\1\300"+
-    "\1\301\1\302\1\303\1\304\1\305\1\306\1\307\1\310"+
-    "\1\311\1\312\1\124\1\313\2\314\1\313\5\314\1\315"+
-    "\1\0\3\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\1\150\3\0\2\124\2\0\1\47\1\0\1\50\1\0"+
-    "\1\51\1\0\1\52\1\0\1\53\1\0\1\204\3\0"+
-    "\1\70\5\0\1\71\3\0\1\205\11\0\1\60\2\0"+
-    "\1\206\16\0\1\207\2\0\1\210\21\0\1\113\17\0"+
-    "\1\25\1\77\1\26\1\115\3\0\1\77\1\0\1\77"+
-    "\2\0\1\25\1\123\32\143\1\124\12\144\1\0\1\124"+
-    "\1\137\1\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\14\0\1\175\3\0\1\176\5\0\1\177"+
-    "\3\0\1\200\14\0\1\201\16\0\1\202\2\0\1\203"+
-    "\42\0\1\76\1\26\6\0\1\76\3\0\1\123\33\124"+
-    "\12\144\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\2\0\1\47\1\0\1\50\1\0"+
-    "\1\51\1\0\1\52\1\0\1\53\1\0\1\67\3\0"+
-    "\1\70\5\0\1\71\3\0\1\72\11\0\1\60\2\0"+
-    "\1\73\16\0\1\74\2\0\1\75\41\0\1\25\2\26"+
-    "\2\0\2\76\1\77\1\0\1\26\2\0\1\25\1\136"+
-    "\32\36\1\127\12\316\1\0\1\124\1\137\1\124\1\0"+
-    "\2\140\1\125\3\124\2\0\1\76\1\124\4\0\2\124"+
-    "\2\0\1\47\1\0\1\50\1\0\1\51\1\0\1\52"+
-    "\1\0\1\53\1\0\1\67\3\0\1\70\5\0\1\71"+
-    "\3\0\1\72\11\0\1\60\2\0\1\73\16\0\1\74"+
-    "\2\0\1\75\41\0\1\25\2\26\2\0\2\76\1\77"+
-    "\1\0\1\26\2\0\1\25\1\136\32\36\1\127\2\141"+
-    "\1\316\2\141\2\316\1\141\1\316\1\141\1\0\1\124"+
-    "\1\137\1\124\1\0\2\140\1\125\3\124\2\0\1\76"+
-    "\1\124\4\0\2\124\2\0\1\47\1\0\1\50\1\0"+
-    "\1\51\1\0\1\52\1\0\1\53\1\0\1\54\3\0"+
-    "\1\55\5\0\1\56\3\0\1\57\11\0\1\60\2\0"+
-    "\1\61\16\0\1\62\2\0\1\63\41\0\2\25\1\64"+
-    "\1\0\1\65\1\0\1\65\1\66\1\0\1\25\2\0"+
-    "\1\25\1\317\32\143\1\124\12\255\1\65\1\124\1\131"+
-    "\1\124\1\0\1\124\1\132\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\2\0\1\47\1\0\1\50\1\0\1\51"+
-    "\1\0\1\52\1\0\1\53\1\0\1\67\3\0\1\70"+
-    "\5\0\1\71\3\0\1\72\11\0\1\60\2\0\1\73"+
-    "\16\0\1\74\2\0\1\75\41\0\1\25\2\26\2\0"+
-    "\2\76\1\77\1\0\1\26\2\0\1\25\1\320\32\143"+
-    "\1\124\12\144\1\0\1\124\1\137\1\124\1\0\2\140"+
-    "\1\125\3\124\2\0\1\76\1\124\4\0\2\124\151\0"+
-    "\4\321\2\0\1\321\15\0\1\321\6\0\12\321\1\322"+
-    "\242\0\1\323\174\0\1\324\54\0\1\125\165\0\74\150"+
-    "\2\0\1\64\11\0\3\25\5\0\1\25\1\0\1\25"+
-    "\1\0\1\25\4\0\1\25\4\0\1\64\1\0\2\64"+
-    "\4\0\1\25\5\0\1\25\3\0\1\64\4\0\1\64"+
-    "\2\25\2\64\10\0\1\64\1\0\2\25\1\0\1\64"+
-    "\10\0\1\25\120\0\1\25\3\0\1\25\6\0\2\25"+
-    "\5\0\1\25\1\0\1\25\1\0\1\25\1\0\11\25"+
-    "\2\0\1\25\4\0\1\25\4\0\6\25\2\0\1\25"+
-    "\1\0\1\25\1\0\3\25\1\0\1\64\1\0\2\25"+
-    "\4\0\3\25\1\0\1\25\10\0\1\25\1\0\2\25"+
-    "\115\0\1\25\3\0\1\25\5\0\1\25\32\0\15\25"+
-    "\5\0\3\25\1\0\1\25\5\0\1\25\2\64\5\0"+
-    "\1\25\2\0\1\25\1\64\4\0\1\25\2\0\1\25"+
-    "\1\0\1\25\177\0\2\64\6\0\1\64\152\0\1\64"+
-    "\3\0\1\64\2\0\1\64\3\0\1\64\5\0\1\64"+
-    "\7\0\1\64\4\0\2\64\3\0\2\64\1\0\1\64"+
-    "\4\0\1\64\1\0\1\64\2\0\2\64\1\0\3\64"+
-    "\1\0\1\64\2\0\4\64\2\0\1\64\147\0\1\325"+
-    "\3\0\1\326\5\0\1\327\3\0\1\330\14\0\1\331"+
-    "\16\0\1\332\2\0\1\333\42\0\1\160\1\64\6\0"+
-    "\1\160\37\0\12\64\30\0\1\65\11\0\3\25\5\0"+
-    "\1\25\1\0\1\25\1\0\1\25\4\0\1\25\4\0"+
-    "\1\65\1\0\2\65\4\0\1\25\5\0\1\25\3\0"+
-    "\1\65\4\0\1\65\2\25\2\65\12\0\2\25\1\0"+
-    "\1\65\10\0\1\25\120\0\1\25\11\0\2\25\2\0"+
-    "\5\25\2\0\2\25\4\0\6\25\1\0\2\25\4\0"+
-    "\5\25\1\0\5\25\1\0\2\25\1\0\3\25\1\0"+
-    "\4\25\1\0\5\25\2\0\1\25\1\0\1\25\1\0"+
-    "\3\25\2\0\1\25\1\0\1\25\1\0\1\25\2\0"+
-    "\1\25\113\0\1\25\3\0\1\25\5\0\2\25\3\0"+
-    "\1\25\4\0\3\25\4\0\1\25\1\0\1\25\2\0"+
-    "\1\25\1\0\2\25\4\0\1\25\1\0\1\25\3\0"+
-    "\2\25\1\0\1\25\5\0\3\25\1\0\1\25\10\0"+
-    "\1\25\4\0\1\25\10\0\1\25\120\0\1\25\3\0"+
-    "\1\25\6\0\2\25\5\0\1\25\1\0\1\25\1\0"+
-    "\1\25\1\0\11\25\2\0\1\25\4\0\1\25\4\0"+
-    "\6\25\2\0\1\25\1\0\1\25\1\0\3\25\1\0"+
-    "\1\65\1\0\2\25\4\0\3\25\1\0\1\25\10\0"+
-    "\1\25\1\0\2\25\115\0\1\25\3\0\1\25\5\0"+
-    "\1\25\32\0\15\25\5\0\3\25\1\0\1\25\5\0"+
-    "\1\25\2\65\5\0\1\25\2\0\1\25\1\65\4\0"+
-    "\1\25\2\0\1\25\1\0\1\25\177\0\2\65\6\0"+
-    "\1\65\152\0\1\65\3\0\1\65\2\0\1\65\3\0"+
-    "\1\65\5\0\1\65\7\0\1\65\4\0\2\65\3\0"+
-    "\2\65\1\0\1\65\4\0\1\65\1\0\1\65\2\0"+
-    "\2\65\1\0\3\65\1\0\1\65\2\0\4\65\2\0"+
-    "\1\65\136\0\1\66\11\0\3\25\5\0\1\25\1\0"+
-    "\1\25\1\0\1\25\4\0\1\25\4\0\1\66\1\0"+
-    "\2\66\4\0\1\25\5\0\1\25\3\0\1\66\4\0"+
-    "\1\66\2\25\2\66\10\0\1\64\1\0\2\25\1\0"+
-    "\1\66\10\0\1\25\120\0\1\25\3\0\1\25\6\0"+
-    "\2\25\5\0\1\25\1\0\1\25\1\0\1\25\1\0"+
-    "\11\25\2\0\1\25\4\0\1\25\4\0\6\25\2\0"+
-    "\1\25\1\0\1\25\1\0\3\25\1\0\1\66\1\0"+
-    "\2\25\4\0\3\25\1\0\1\25\10\0\1\25\1\0"+
-    "\2\25\115\0\1\25\3\0\1\25\5\0\1\25\32\0"+
-    "\15\25\5\0\3\25\1\0\1\25\5\0\1\25\2\66"+
-    "\5\0\1\25\2\0\1\25\1\66\4\0\1\25\2\0"+
-    "\1\25\1\0\1\25\177\0\2\66\6\0\1\66\152\0"+
-    "\1\66\3\0\1\66\2\0\1\66\3\0\1\66\5\0"+
-    "\1\66\7\0\1\66\4\0\2\66\3\0\2\66\1\0"+
-    "\1\66\4\0\1\66\1\0\1\66\2\0\2\66\1\0"+
-    "\3\66\1\0\1\66\2\0\4\66\2\0\1\66\136\0"+
-    "\1\76\37\0\1\76\1\0\2\76\16\0\1\76\4\0"+
-    "\1\76\2\0\2\76\10\0\1\26\4\0\1\76\133\0"+
-    "\1\26\102\0\1\26\243\0\2\26\230\0\1\76\247\0"+
-    "\2\76\11\0\1\76\211\0\2\76\6\0\1\76\152\0"+
-    "\1\76\3\0\1\76\2\0\1\76\3\0\1\76\5\0"+
-    "\1\76\7\0\1\76\4\0\2\76\3\0\2\76\1\0"+
-    "\1\76\4\0\1\76\1\0\1\76\2\0\2\76\1\0"+
-    "\3\76\1\0\1\76\2\0\4\76\2\0\1\76\136\0"+
-    "\1\77\11\0\3\25\5\0\1\25\1\0\1\25\1\0"+
-    "\1\25\4\0\1\25\4\0\1\77\1\0\2\77\4\0"+
-    "\1\25\5\0\1\25\3\0\1\77\4\0\1\77\2\25"+
-    "\2\77\10\0\1\26\1\0\2\25\1\0\1\77\10\0"+
-    "\1\25\120\0\1\25\3\0\1\25\6\0\2\25\5\0"+
-    "\1\25\1\0\1\25\1\0\1\25\1\0\11\25\2\0"+
-    "\1\25\4\0\1\25\4\0\6\25\2\0\1\25\1\0"+
-    "\1\25\1\0\3\25\1\0\1\77\1\0\2\25\4\0"+
-    "\3\25\1\0\1\25\10\0\1\25\1\0\2\25\115\0"+
-    "\1\25\3\0\1\25\5\0\1\25\32\0\15\25\5\0"+
-    "\3\25\1\0\1\25\5\0\1\25\2\77\5\0\1\25"+
-    "\2\0\1\25\1\77\4\0\1\25\2\0\1\25\1\0"+
-    "\1\25\177\0\2\77\6\0\1\77\152\0\1\77\3\0"+
-    "\1\77\2\0\1\77\3\0\1\77\5\0\1\77\7\0"+
-    "\1\77\4\0\2\77\3\0\2\77\1\0\1\77\4\0"+
-    "\1\77\1\0\1\77\2\0\2\77\1\0\3\77\1\0"+
-    "\1\77\2\0\4\77\2\0\1\77\136\0\1\115\37\0"+
-    "\1\115\1\0\2\115\16\0\1\115\4\0\1\115\2\0"+
-    "\2\115\15\0\1\115\226\0\1\115\247\0\2\115\11\0"+
-    "\1\115\211\0\2\115\6\0\1\115\152\0\1\115\3\0"+
-    "\1\115\2\0\1\115\3\0\1\115\5\0\1\115\7\0"+
-    "\1\115\4\0\2\115\3\0\2\115\1\0\1\115\4\0"+
-    "\1\115\1\0\1\115\2\0\2\115\1\0\3\115\1\0"+
-    "\1\115\2\0\4\115\2\0\1\115\303\0\1\334\32\216"+
-    "\1\335\12\216\175\0\61\217\1\0\1\336\4\217\1\337"+
-    "\1\0\3\217\1\0\1\47\1\0\1\50\1\0\1\51"+
-    "\1\0\1\52\1\0\1\53\1\0\1\54\3\0\1\55"+
-    "\5\0\1\56\3\0\1\57\11\0\1\60\2\0\1\61"+
-    "\16\0\1\62\2\0\1\63\41\0\2\25\1\64\1\0"+
-    "\1\65\1\0\1\65\1\66\1\0\1\25\2\0\1\25"+
-    "\1\126\1\36\2\340\1\341\1\342\10\340\1\36\1\343"+
-    "\5\340\6\36\1\127\12\130\1\65\1\124\1\131\1\124"+
-    "\1\0\1\124\1\132\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\2\0\1\47\1\0\1\50\1\0\1\51\1\0"+
-    "\1\52\1\0\1\53\1\0\1\54\3\0\1\55\5\0"+
-    "\1\56\3\0\1\57\11\0\1\60\2\0\1\61\16\0"+
-    "\1\62\2\0\1\63\41\0\2\25\1\64\1\0\1\65"+
-    "\1\0\1\65\1\66\1\0\1\25\2\0\1\25\1\126"+
-    "\1\344\2\340\1\36\1\340\1\345\6\340\4\36\1\340"+
-    "\1\36\2\340\1\36\1\340\1\36\3\340\1\127\12\130"+
-    "\1\65\1\124\1\131\1\124\1\0\1\124\1\132\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\2\0\1\47\1\0"+
-    "\1\50\1\0\1\51\1\0\1\52\1\0\1\53\1\0"+
-    "\1\54\3\0\1\55\5\0\1\56\3\0\1\57\11\0"+
-    "\1\60\2\0\1\61\16\0\1\62\2\0\1\63\41\0"+
-    "\2\25\1\64\1\0\1\65\1\0\1\65\1\66\1\0"+
-    "\1\25\2\0\1\25\1\126\3\36\1\340\1\36\1\340"+
-    "\4\36\1\340\10\36\1\340\2\36\1\340\2\36\1\340"+
-    "\1\127\12\130\1\65\1\124\1\131\1\124\1\0\1\124"+
-    "\1\132\1\125\3\124\3\0\1\124\4\0\2\124\2\0"+
-    "\1\47\1\0\1\50\1\0\1\51\1\0\1\52\1\0"+
-    "\1\53\1\0\1\54\3\0\1\55\5\0\1\56\3\0"+
-    "\1\57\11\0\1\60\2\0\1\61\16\0\1\62\2\0"+
-    "\1\63\41\0\2\25\1\64\1\0\1\65\1\0\1\65"+
-    "\1\66\1\0\1\25\2\0\1\25\1\126\1\36\1\340"+
-    "\1\346\2\340\2\36\1\340\6\36\3\340\11\36\1\127"+
-    "\12\130\1\65\1\124\1\131\1\124\1\0\1\124\1\132"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\2\0\1\47"+
-    "\1\0\1\50\1\0\1\51\1\0\1\52\1\0\1\53"+
-    "\1\0\1\54\3\0\1\55\5\0\1\56\3\0\1\57"+
-    "\11\0\1\60\2\0\1\61\16\0\1\62\2\0\1\63"+
-    "\41\0\2\25\1\64\1\0\1\65\1\0\1\65\1\66"+
-    "\1\0\1\25\2\0\1\25\1\126\3\36\1\340\1\36"+
-    "\1\340\10\36\1\340\1\36\2\340\10\36\1\127\12\130"+
-    "\1\65\1\124\1\131\1\124\1\0\1\124\1\132\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\2\0\1\47\1\0"+
-    "\1\50\1\0\1\51\1\0\1\52\1\0\1\53\1\0"+
-    "\1\54\3\0\1\55\5\0\1\56\3\0\1\57\11\0"+
-    "\1\60\2\0\1\61\16\0\1\62\2\0\1\63\41\0"+
-    "\2\25\1\64\1\0\1\65\1\0\1\65\1\66\1\0"+
-    "\1\25\2\0\1\25\1\126\4\36\1\347\5\36\1\340"+
-    "\17\36\1\127\12\130\1\65\1\124\1\131\1\124\1\0"+
-    "\1\124\1\132\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\2\0\1\47\1\0\1\50\1\0\1\51\1\0\1\52"+
-    "\1\0\1\53\1\0\1\54\3\0\1\55\5\0\1\56"+
-    "\3\0\1\57\11\0\1\60\2\0\1\61\16\0\1\62"+
-    "\2\0\1\63\41\0\2\25\1\64\1\0\1\65\1\0"+
-    "\1\65\1\66\1\0\1\25\2\0\1\25\1\126\4\36"+
-    "\2\340\2\36\1\340\1\36\1\340\13\36\1\340\2\36"+
-    "\1\340\1\127\12\130\1\65\1\124\1\131\1\124\1\0"+
-    "\1\124\1\132\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\2\0\1\47\1\0\1\50\1\0\1\51\1\0\1\52"+
-    "\1\0\1\53\1\0\1\54\3\0\1\55\5\0\1\56"+
-    "\3\0\1\57\11\0\1\60\2\0\1\61\16\0\1\62"+
-    "\2\0\1\63\41\0\2\25\1\64\1\0\1\65\1\0"+
-    "\1\65\1\66\1\0\1\25\2\0\1\25\1\126\1\340"+
-    "\1\36\3\340\1\350\14\340\2\36\2\340\2\36\1\340"+
-    "\1\36\1\127\12\130\1\65\1\124\1\131\1\124\1\0"+
-    "\1\124\1\132\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\2\0\1\47\1\0\1\50\1\0\1\51\1\0\1\52"+
-    "\1\0\1\53\1\0\1\54\3\0\1\55\5\0\1\56"+
-    "\3\0\1\57\11\0\1\60\2\0\1\61\16\0\1\62"+
-    "\2\0\1\63\41\0\2\25\1\64\1\0\1\65\1\0"+
-    "\1\65\1\66\1\0\1\25\2\0\1\25\1\126\2\36"+
-    "\4\340\3\36\2\340\1\351\1\340\1\36\2\340\12\36"+
-    "\1\127\12\130\1\65\1\124\1\131\1\124\1\0\1\124"+
-    "\1\132\1\125\3\124\3\0\1\124\4\0\2\124\2\0"+
-    "\1\47\1\0\1\50\1\0\1\51\1\0\1\52\1\0"+
-    "\1\53\1\0\1\54\3\0\1\55\5\0\1\56\3\0"+
-    "\1\57\11\0\1\60\2\0\1\61\16\0\1\62\2\0"+
-    "\1\63\41\0\2\25\1\64\1\0\1\65\1\0\1\65"+
-    "\1\66\1\0\1\25\2\0\1\25\1\126\2\340\2\36"+
-    "\1\340\3\36\1\340\5\36\3\340\3\36\1\340\2\36"+
-    "\3\340\1\127\12\130\1\65\1\124\1\131\1\124\1\0"+
-    "\1\124\1\132\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\2\0\1\47\1\0\1\50\1\0\1\51\1\0\1\52"+
-    "\1\0\1\53\1\0\1\54\3\0\1\55\5\0\1\56"+
-    "\3\0\1\57\11\0\1\60\2\0\1\61\16\0\1\62"+
-    "\2\0\1\63\41\0\2\25\1\64\1\0\1\65\1\0"+
-    "\1\65\1\66\1\0\1\25\2\0\1\25\1\126\5\340"+
-    "\1\352\1\36\1\340\1\353\7\340\1\354\3\340\1\36"+
-    "\1\340\1\36\3\340\1\127\12\130\1\65\1\124\1\131"+
-    "\1\124\1\0\1\124\1\132\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\2\0\1\47\1\0\1\50\1\0\1\51"+
-    "\1\0\1\52\1\0\1\53\1\0\1\54\3\0\1\55"+
-    "\5\0\1\56\3\0\1\57\11\0\1\60\2\0\1\61"+
-    "\16\0\1\62\2\0\1\63\41\0\2\25\1\64\1\0"+
-    "\1\65\1\0\1\65\1\66\1\0\1\25\2\0\1\25"+
-    "\1\126\1\355\1\340\1\36\1\344\6\340\3\36\1\340"+
-    "\2\36\1\340\2\36\1\340\6\36\1\127\12\130\1\65"+
-    "\1\124\1\131\1\124\1\0\1\124\1\132\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\2\0\1\47\1\0\1\50"+
-    "\1\0\1\51\1\0\1\52\1\0\1\53\1\0\1\54"+
-    "\3\0\1\55\5\0\1\56\3\0\1\57\11\0\1\60"+
-    "\2\0\1\61\16\0\1\62\2\0\1\63\41\0\2\25"+
-    "\1\64\1\0\1\65\1\0\1\65\1\66\1\0\1\25"+
-    "\2\0\1\25\1\126\1\340\31\36\1\127\12\130\1\65"+
-    "\1\124\1\131\1\124\1\0\1\124\1\132\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\2\0\1\47\1\0\1\50"+
-    "\1\0\1\51\1\0\1\52\1\0\1\53\1\0\1\54"+
-    "\3\0\1\55\5\0\1\56\3\0\1\57\11\0\1\60"+
-    "\2\0\1\61\16\0\1\62\2\0\1\63\41\0\2\25"+
-    "\1\64\1\0\1\65\1\0\1\65\1\66\1\0\1\25"+
-    "\2\0\1\25\1\126\1\340\2\36\1\340\1\356\1\36"+
-    "\2\340\1\36\3\340\2\36\2\340\1\36\1\340\3\36"+
-    "\1\340\2\36\2\340\1\127\12\130\1\65\1\124\1\131"+
-    "\1\124\1\0\1\124\1\132\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\2\0\1\47\1\0\1\50\1\0\1\51"+
-    "\1\0\1\52\1\0\1\53\1\0\1\54\3\0\1\55"+
-    "\5\0\1\56\3\0\1\57\11\0\1\60\2\0\1\61"+
-    "\16\0\1\62\2\0\1\63\41\0\2\25\1\64\1\0"+
-    "\1\65\1\0\1\65\1\66\1\0\1\25\2\0\1\25"+
-    "\1\126\6\340\1\36\5\340\3\36\2\340\2\36\7\340"+
-    "\1\127\12\130\1\65\1\124\1\131\1\124\1\0\1\124"+
-    "\1\132\1\125\3\124\3\0\1\124\4\0\2\124\2\0"+
-    "\1\47\1\0\1\50\1\0\1\51\1\0\1\52\1\0"+
-    "\1\53\1\0\1\54\3\0\1\55\5\0\1\56\3\0"+
-    "\1\57\11\0\1\60\2\0\1\61\16\0\1\62\2\0"+
-    "\1\63\41\0\2\25\1\64\1\0\1\65\1\0\1\65"+
-    "\1\66\1\0\1\25\2\0\1\25\1\126\1\36\2\340"+
-    "\1\353\1\357\3\340\1\36\3\340\1\36\1\340\1\36"+
-    "\1\340\1\36\1\340\1\36\1\340\1\36\3\340\1\36"+
-    "\1\340\1\127\12\130\1\65\1\124\1\131\1\124\1\0"+
-    "\1\124\1\132\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\2\0\1\47\1\0\1\50\1\0\1\51\1\0\1\52"+
-    "\1\0\1\53\1\0\1\54\3\0\1\55\5\0\1\56"+
-    "\3\0\1\57\11\0\1\60\2\0\1\61\16\0\1\62"+
-    "\2\0\1\63\41\0\2\25\1\64\1\0\1\65\1\0"+
-    "\1\65\1\66\1\0\1\25\2\0\1\25\1\126\1\340"+
-    "\6\36\1\340\6\36\1\340\4\36\1\340\4\36\2\340"+
-    "\1\127\12\130\1\65\1\124\1\131\1\124\1\0\1\124"+
-    "\1\132\1\125\3\124\3\0\1\124\4\0\2\124\2\0"+
-    "\1\47\1\0\1\50\1\0\1\51\1\0\1\52\1\0"+
-    "\1\53\1\0\1\54\3\0\1\55\5\0\1\56\3\0"+
-    "\1\57\11\0\1\60\2\0\1\61\16\0\1\62\2\0"+
-    "\1\63\41\0\2\25\1\64\1\0\1\65\1\0\1\65"+
-    "\1\66\1\0\1\25\2\0\1\25\1\126\6\36\1\340"+
-    "\7\36\1\340\13\36\1\127\12\130\1\65\1\124\1\131"+
-    "\1\124\1\0\1\124\1\132\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\2\0\1\47\1\0\1\50\1\0\1\51"+
-    "\1\0\1\52\1\0\1\53\1\0\1\54\3\0\1\55"+
-    "\5\0\1\56\3\0\1\57\11\0\1\60\2\0\1\61"+
-    "\16\0\1\62\2\0\1\63\41\0\2\25\1\64\1\0"+
-    "\1\65\1\0\1\65\1\66\1\0\1\25\2\0\1\25"+
-    "\1\126\13\36\1\360\16\36\1\127\12\130\1\65\1\124"+
-    "\1\131\1\124\1\0\1\124\1\132\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\2\0\1\47\1\0\1\50\1\0"+
-    "\1\51\1\0\1\52\1\0\1\53\1\0\1\54\3\0"+
-    "\1\55\5\0\1\56\3\0\1\57\11\0\1\60\2\0"+
-    "\1\61\16\0\1\62\2\0\1\63\41\0\2\25\1\64"+
-    "\1\0\1\65\1\0\1\65\1\66\1\0\1\25\2\0"+
-    "\1\25\1\126\1\340\11\36\1\340\6\36\1\340\10\36"+
-    "\1\127\12\130\1\65\1\124\1\131\1\124\1\0\1\124"+
-    "\1\132\1\125\3\124\3\0\1\124\4\0\2\124\2\0"+
-    "\1\47\1\0\1\50\1\0\1\51\1\0\1\52\1\0"+
-    "\1\53\1\0\1\54\3\0\1\55\5\0\1\56\3\0"+
-    "\1\57\11\0\1\60\2\0\1\61\16\0\1\62\2\0"+
-    "\1\63\41\0\2\25\1\64\1\0\1\65\1\0\1\65"+
-    "\1\66\1\0\1\25\2\0\1\25\1\126\1\340\1\36"+
-    "\6\340\1\361\1\36\2\340\2\36\2\340\1\36\1\340"+
-    "\1\36\6\340\1\36\1\127\12\130\1\65\1\124\1\131"+
-    "\1\124\1\0\1\124\1\132\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\2\0\1\47\1\0\1\50\1\0\1\51"+
-    "\1\0\1\52\1\0\1\53\1\0\1\54\3\0\1\55"+
-    "\5\0\1\56\3\0\1\57\11\0\1\60\2\0\1\61"+
-    "\16\0\1\62\2\0\1\63\41\0\2\25\1\64\1\0"+
-    "\1\65\1\0\1\65\1\66\1\0\1\25\2\0\1\25"+
-    "\1\126\4\36\1\340\5\36\2\340\3\36\2\340\10\36"+
-    "\1\340\1\127\12\130\1\65\1\124\1\131\1\124\1\0"+
-    "\1\124\1\132\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\2\0\1\47\1\0\1\50\1\0\1\51\1\0\1\52"+
-    "\1\0\1\53\1\0\1\54\3\0\1\55\5\0\1\56"+
-    "\3\0\1\57\11\0\1\60\2\0\1\61\16\0\1\62"+
-    "\2\0\1\63\41\0\2\25\1\64\1\0\1\65\1\0"+
-    "\1\65\1\66\1\0\1\25\2\0\1\25\1\126\3\36"+
-    "\1\340\1\36\1\362\4\36\1\340\2\36\1\340\14\36"+
-    "\1\127\12\130\1\65\1\124\1\131\1\124\1\0\1\124"+
-    "\1\132\1\125\3\124\3\0\1\124\4\0\2\124\2\0"+
-    "\1\47\1\0\1\50\1\0\1\51\1\0\1\52\1\0"+
-    "\1\53\1\0\1\54\3\0\1\55\5\0\1\56\3\0"+
-    "\1\57\11\0\1\60\2\0\1\61\16\0\1\62\2\0"+
-    "\1\63\41\0\2\25\1\64\1\0\1\65\1\0\1\65"+
-    "\1\66\1\0\1\25\2\0\1\25\1\126\2\340\1\36"+
-    "\1\340\3\36\2\340\2\36\1\340\4\36\1\340\11\36"+
-    "\1\127\12\130\1\65\1\124\1\131\1\124\1\0\1\124"+
-    "\1\132\1\125\3\124\3\0\1\124\4\0\2\124\2\0"+
-    "\1\47\1\0\1\50\1\0\1\51\1\0\1\52\1\0"+
-    "\1\53\1\0\1\54\3\0\1\55\5\0\1\56\3\0"+
-    "\1\57\11\0\1\60\2\0\1\61\16\0\1\62\2\0"+
-    "\1\63\41\0\2\25\1\64\1\0\1\65\1\0\1\65"+
-    "\1\66\1\0\1\25\2\0\1\25\1\126\3\36\1\340"+
-    "\13\36\1\340\12\36\1\127\12\130\1\65\1\124\1\131"+
-    "\1\124\1\0\1\124\1\132\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\2\0\1\47\1\0\1\50\1\0\1\51"+
-    "\1\0\1\52\1\0\1\53\1\0\1\54\3\0\1\55"+
-    "\5\0\1\56\3\0\1\57\11\0\1\60\2\0\1\61"+
-    "\16\0\1\62\2\0\1\63\41\0\2\25\1\64\1\0"+
-    "\1\65\1\0\1\65\1\66\1\0\1\25\2\0\1\25"+
-    "\1\126\3\36\2\340\2\36\2\340\1\36\2\340\1\36"+
-    "\1\340\3\36\1\340\1\36\1\340\1\36\1\340\2\36"+
-    "\1\340\1\36\1\127\12\130\1\65\1\124\1\131\1\124"+
-    "\1\0\1\124\1\132\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\363\32\252\1\127\12\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\14\0\1\325\3\0\1\326\5\0\1\327\3\0\1\330"+
-    "\14\0\1\331\16\0\1\332\2\0\1\333\42\0\1\160"+
-    "\1\64\6\0\1\160\3\0\1\123\1\261\1\262\1\263"+
-    "\1\264\1\265\1\266\1\267\1\270\1\271\1\272\1\273"+
-    "\1\274\1\275\1\276\1\277\1\300\1\301\1\302\1\303"+
-    "\1\304\1\305\1\306\1\307\1\310\1\311\1\312\1\124"+
-    "\12\130\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\1\150\3\0\2\124\14\0\1\325\3\0\1\326"+
-    "\5\0\1\327\3\0\1\330\14\0\1\331\16\0\1\332"+
-    "\2\0\1\333\42\0\1\160\1\64\6\0\1\160\3\0"+
-    "\1\123\33\124\12\255\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\2\0\1\47\1\0"+
-    "\1\50\1\0\1\51\1\0\1\52\1\0\1\53\1\0"+
-    "\1\153\3\0\1\55\5\0\1\56\3\0\1\154\11\0"+
-    "\1\60\2\0\1\155\16\0\1\156\2\0\1\157\41\0"+
-    "\1\25\2\64\2\0\2\160\1\66\1\0\1\64\2\0"+
-    "\1\25\1\364\32\143\1\124\12\255\1\0\1\124\1\131"+
-    "\1\124\1\0\2\254\1\125\3\124\2\0\1\160\1\124"+
-    "\4\0\2\124\2\0\1\47\1\0\1\50\1\0\1\51"+
-    "\1\0\1\52\1\0\1\53\1\0\1\54\3\0\1\55"+
-    "\5\0\1\56\3\0\1\57\11\0\1\60\2\0\1\61"+
-    "\16\0\1\62\2\0\1\63\41\0\2\25\1\64\1\0"+
-    "\1\65\1\0\1\65\1\66\1\0\1\25\2\0\1\25"+
-    "\1\126\3\36\1\365\26\36\1\127\12\130\1\65\1\124"+
-    "\1\131\1\124\1\0\1\124\1\132\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\2\0\1\47\1\0\1\50\1\0"+
-    "\1\51\1\0\1\52\1\0\1\53\1\0\1\54\3\0"+
-    "\1\55\5\0\1\56\3\0\1\57\11\0\1\60\2\0"+
-    "\1\61\16\0\1\62\2\0\1\63\41\0\2\25\1\64"+
-    "\1\0\1\65\1\0\1\65\1\66\1\0\1\25\2\0"+
-    "\1\25\1\126\32\36\1\127\12\130\1\366\1\124\1\131"+
-    "\1\124\1\0\1\124\1\132\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\2\0\1\47\1\0\1\50\1\0\1\51"+
-    "\1\0\1\52\1\0\1\53\1\0\1\54\3\0\1\55"+
-    "\5\0\1\56\3\0\1\57\11\0\1\60\2\0\1\61"+
-    "\16\0\1\62\2\0\1\63\41\0\2\25\1\64\1\0"+
-    "\1\65\1\0\1\65\1\66\1\0\1\25\2\0\1\25"+
-    "\1\126\15\36\1\367\14\36\1\127\12\130\1\65\1\124"+
-    "\1\131\1\124\1\0\1\124\1\132\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\150\0\1\363\1\252\2\370\1\371"+
-    "\1\372\10\370\1\252\1\373\5\370\6\252\1\127\12\252"+
-    "\1\0\3\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\150\0\1\363\1\374\2\370\1\252\1\370"+
-    "\1\375\6\370\4\252\1\370\1\252\2\370\1\252\1\370"+
-    "\1\252\3\370\1\127\12\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\3\252\1\370\1\252\1\370\4\252\1\370\10\252\1\370"+
-    "\2\252\1\370\2\252\1\370\1\127\12\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\1\252\1\370\1\376\2\370\2\252\1\370"+
-    "\6\252\3\370\11\252\1\127\12\252\1\0\3\124\1\0"+
-    "\2\124\1\125\3\124\3\0\1\124\4\0\2\124\150\0"+
-    "\1\363\3\252\1\370\1\252\1\370\10\252\1\370\1\252"+
-    "\2\370\10\252\1\127\12\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\4\252\1\377\5\252\1\370\17\252\1\127\12\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\363\4\252\2\370\2\252\1\370\1\252"+
-    "\1\370\13\252\1\370\2\252\1\370\1\127\12\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\363\1\370\1\252\3\370\1\u0100\14\370"+
-    "\2\252\2\370\2\252\1\370\1\252\1\127\12\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\363\2\252\4\370\3\252\2\370\1\u0101"+
-    "\1\370\1\252\2\370\12\252\1\127\12\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\2\370\2\252\1\370\3\252\1\370\5\252"+
-    "\3\370\3\252\1\370\2\252\3\370\1\127\12\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\363\5\370\1\u0102\1\252\1\370\1\u0103"+
-    "\7\370\1\u0104\3\370\1\252\1\370\1\252\3\370\1\127"+
-    "\12\252\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\150\0\1\363\1\u0105\1\370\1\252"+
-    "\1\374\6\370\3\252\1\370\2\252\1\370\2\252\1\370"+
-    "\6\252\1\127\12\252\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\150\0\1\363\1\370"+
-    "\31\252\1\127\12\252\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\150\0\1\363\1\370"+
-    "\2\252\1\370\1\u0106\1\252\2\370\1\252\3\370\2\252"+
-    "\2\370\1\252\1\370\3\252\1\370\2\252\2\370\1\127"+
-    "\12\252\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\150\0\1\363\6\370\1\252\5\370"+
-    "\3\252\2\370\2\252\7\370\1\127\12\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\1\252\2\370\1\u0103\1\u0107\3\370\1\252"+
-    "\3\370\1\252\1\370\1\252\1\370\1\252\1\370\1\252"+
-    "\1\370\1\252\3\370\1\252\1\370\1\127\12\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\363\1\370\6\252\1\370\6\252\1\370"+
-    "\4\252\1\370\4\252\2\370\1\127\12\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\6\252\1\370\7\252\1\370\13\252\1\127"+
-    "\12\252\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\150\0\1\363\13\252\1\u0108\16\252"+
-    "\1\127\12\252\1\0\3\124\1\0\2\124\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\150\0\1\363\1\370\11\252"+
-    "\1\370\6\252\1\370\10\252\1\127\12\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\1\370\1\252\6\370\1\u0109\1\252\2\370"+
-    "\2\252\2\370\1\252\1\370\1\252\6\370\1\252\1\127"+
-    "\12\252\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\150\0\1\363\4\252\1\370\5\252"+
-    "\2\370\3\252\2\370\10\252\1\370\1\127\12\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\363\3\252\1\370\1\252\1\u010a\4\252"+
-    "\1\370\2\252\1\370\14\252\1\127\12\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\2\370\1\252\1\370\3\252\2\370\2\252"+
-    "\1\370\4\252\1\370\11\252\1\127\12\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\3\252\1\370\13\252\1\370\12\252\1\127"+
-    "\12\252\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\150\0\1\363\3\252\2\370\2\252"+
-    "\2\370\1\252\2\370\1\252\1\370\3\252\1\370\1\252"+
-    "\1\370\1\252\1\370\2\252\1\370\1\252\1\127\12\252"+
-    "\1\0\3\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\2\0\1\47\1\0\1\50\1\0\1\51"+
-    "\1\0\1\52\1\0\1\53\1\0\1\67\3\0\1\70"+
-    "\5\0\1\71\3\0\1\72\11\0\1\60\2\0\1\73"+
-    "\16\0\1\74\2\0\1\75\41\0\1\25\2\26\2\0"+
-    "\2\76\1\77\1\0\1\26\2\0\1\25\1\u010b\32\36"+
-    "\1\127\12\314\1\0\1\124\1\137\1\124\1\0\2\140"+
-    "\1\125\3\124\2\0\1\76\1\124\4\0\2\124\2\0"+
-    "\1\47\1\0\1\50\1\0\1\51\1\0\1\52\1\0"+
-    "\1\53\1\0\1\67\3\0\1\70\5\0\1\71\3\0"+
-    "\1\72\11\0\1\60\2\0\1\73\16\0\1\74\2\0"+
-    "\1\75\41\0\1\25\2\26\2\0\2\76\1\77\1\0"+
-    "\1\26\2\0\1\25\1\u010b\32\36\1\127\12\u010c\1\0"+
-    "\1\124\1\137\1\124\1\0\2\140\1\125\3\124\2\0"+
-    "\1\76\1\124\4\0\2\124\2\0\1\47\1\0\1\50"+
-    "\1\0\1\51\1\0\1\52\1\0\1\53\1\0\1\67"+
-    "\3\0\1\70\5\0\1\71\3\0\1\72\11\0\1\60"+
-    "\2\0\1\73\16\0\1\74\2\0\1\75\41\0\1\25"+
-    "\2\26\2\0\2\76\1\77\1\0\1\26\2\0\1\25"+
-    "\1\u010b\32\36\1\127\1\314\1\u010d\1\u010c\2\314\2\u010c"+
-    "\1\314\1\u010c\1\314\1\0\1\124\1\137\1\124\1\0"+
-    "\2\140\1\125\3\124\2\0\1\76\1\124\4\0\2\124"+
-    "\2\0\1\47\1\0\1\50\1\0\1\51\1\0\1\52"+
-    "\1\0\1\53\1\0\1\67\3\0\1\70\5\0\1\71"+
-    "\3\0\1\72\11\0\1\60\2\0\1\73\16\0\1\74"+
-    "\2\0\1\75\41\0\1\25\2\26\2\0\2\76\1\77"+
-    "\1\0\1\26\2\0\1\25\1\u010e\32\36\1\127\12\316"+
-    "\1\0\1\124\1\137\1\124\1\0\2\140\1\125\3\124"+
-    "\2\0\1\76\1\124\4\0\2\124\2\0\1\47\1\0"+
-    "\1\50\1\0\1\51\1\0\1\52\1\0\1\53\1\0"+
-    "\1\161\3\0\1\162\5\0\1\163\3\0\1\164\11\0"+
-    "\1\60\2\0\1\165\16\0\1\166\2\0\1\167\41\0"+
-    "\1\25\1\65\7\0\1\65\2\0\1\25\1\123\32\143"+
-    "\13\124\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\1\150\3\0\2\124\14\0\1\175\3\0\1\176"+
-    "\5\0\1\177\3\0\1\200\14\0\1\201\16\0\1\202"+
-    "\2\0\1\203\42\0\1\76\1\26\6\0\1\76\3\0"+
-    "\1\123\33\124\12\144\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\1\150\3\0\2\124\151\0\4\u010f"+
-    "\2\0\1\u010f\15\0\1\u010f\6\0\12\u010f\1\322\175\0"+
-    "\4\u0110\2\0\1\u0110\15\0\1\u0110\6\0\12\u0110\1\u0111"+
-    "\175\0\4\u0112\2\0\1\u0112\15\0\1\u0112\6\0\1\u0113"+
-    "\2\u0114\1\u0113\5\u0114\1\u0115\14\0\1\u0116\160\0\46\124"+
-    "\1\0\3\124\1\0\2\124\1\0\3\124\3\0\1\124"+
-    "\1\150\3\0\2\124\3\0\1\160\37\0\1\160\1\0"+
-    "\2\160\16\0\1\160\4\0\1\160\2\0\2\160\10\0"+
-    "\1\64\4\0\1\160\133\0\1\64\102\0\1\64\243\0"+
-    "\2\64\230\0\1\160\247\0\2\160\11\0\1\160\211\0"+
-    "\2\160\6\0\1\160\152\0\1\160\3\0\1\160\2\0"+
-    "\1\160\3\0\1\160\5\0\1\160\7\0\1\160\4\0"+
-    "\2\160\3\0\2\160\1\0\1\160\4\0\1\160\1\0"+
-    "\1\160\2\0\2\160\1\0\3\160\1\0\1\160\2\0"+
-    "\4\160\2\0\1\160\304\0\1\u0117\1\u0118\1\u0119\1\u011a"+
-    "\1\u011b\1\u011c\1\u011d\1\u011e\1\u011f\1\u0120\1\u0121\1\u0122"+
-    "\1\u0123\1\u0124\1\u0125\1\u0126\1\u0127\1\u0128\1\u0129\1\u012a"+
-    "\1\u012b\1\u012c\1\u012d\1\u012e\1\u012f\1\u0130\1\0\12\216"+
-    "\176\0\32\216\1\335\12\216\175\0\74\217\1\0\1\47"+
-    "\1\0\1\50\1\0\1\51\1\0\1\52\1\0\1\53"+
-    "\1\0\1\54\3\0\1\55\5\0\1\56\3\0\1\57"+
-    "\11\0\1\60\2\0\1\61\16\0\1\62\2\0\1\63"+
-    "\41\0\2\25\1\64\1\0\1\65\1\0\1\65\1\66"+
-    "\1\0\1\25\2\0\1\25\1\u0131\32\36\1\127\12\130"+
-    "\1\u0132\1\124\1\131\1\124\1\0\1\124\1\132\1\125"+
-    "\1\u0133\1\u0134\1\u0135\3\0\1\124\4\0\2\124\2\0"+
-    "\1\47\1\0\1\50\1\0\1\51\1\0\1\52\1\0"+
-    "\1\53\1\0\1\54\3\0\1\55\5\0\1\56\3\0"+
-    "\1\57\11\0\1\60\2\0\1\61\16\0\1\62\2\0"+
-    "\1\63\41\0\2\25\1\64\1\0\1\65\1\0\1\65"+
-    "\1\66\1\0\1\25\2\0\1\25\1\u0131\4\36\1\u0136"+
-    "\25\36\1\127\12\130\1\u0132\1\124\1\131\1\124\1\0"+
-    "\1\124\1\132\1\125\1\u0133\1\u0134\1\u0135\3\0\1\124"+
-    "\4\0\2\124\2\0\1\47\1\0\1\50\1\0\1\51"+
-    "\1\0\1\52\1\0\1\53\1\0\1\54\3\0\1\55"+
-    "\5\0\1\56\3\0\1\57\11\0\1\60\2\0\1\61"+
-    "\16\0\1\62\2\0\1\63\41\0\2\25\1\64\1\0"+
-    "\1\65\1\0\1\65\1\66\1\0\1\25\2\0\1\25"+
-    "\1\u0131\15\36\1\234\14\36\1\127\12\130\1\u0132\1\124"+
-    "\1\131\1\124\1\0\1\124\1\132\1\125\1\u0133\1\u0134"+
-    "\1\u0135\3\0\1\124\4\0\2\124\2\0\1\47\1\0"+
-    "\1\50\1\0\1\51\1\0\1\52\1\0\1\53\1\0"+
-    "\1\54\3\0\1\55\5\0\1\56\3\0\1\57\11\0"+
-    "\1\60\2\0\1\61\16\0\1\62\2\0\1\63\41\0"+
-    "\2\25\1\64\1\0\1\65\1\0\1\65\1\66\1\0"+
-    "\1\25\2\0\1\25\1\u0131\10\36\1\234\21\36\1\127"+
-    "\12\130\1\u0132\1\124\1\131\1\124\1\0\1\124\1\132"+
-    "\1\125\1\u0133\1\u0134\1\u0135\3\0\1\124\4\0\2\124"+
-    "\2\0\1\47\1\0\1\50\1\0\1\51\1\0\1\52"+
-    "\1\0\1\53\1\0\1\54\3\0\1\55\5\0\1\56"+
-    "\3\0\1\57\11\0\1\60\2\0\1\61\16\0\1\62"+
-    "\2\0\1\63\41\0\2\25\1\64\1\0\1\65\1\0"+
-    "\1\65\1\66\1\0\1\25\2\0\1\25\1\u0131\17\36"+
-    "\1\340\12\36\1\127\12\130\1\u0132\1\124\1\131\1\124"+
-    "\1\0\1\124\1\132\1\125\1\u0133\1\u0134\1\u0135\3\0"+
-    "\1\124\4\0\2\124\2\0\1\47\1\0\1\50\1\0"+
-    "\1\51\1\0\1\52\1\0\1\53\1\0\1\54\3\0"+
-    "\1\55\5\0\1\56\3\0\1\57\11\0\1\60\2\0"+
-    "\1\61\16\0\1\62\2\0\1\63\41\0\2\25\1\64"+
-    "\1\0\1\65\1\0\1\65\1\66\1\0\1\25\2\0"+
-    "\1\25\1\u0131\5\36\1\u0137\4\36\1\340\17\36\1\127"+
-    "\12\130\1\u0132\1\124\1\131\1\124\1\0\1\124\1\132"+
-    "\1\125\1\u0133\1\u0134\1\u0135\3\0\1\124\4\0\2\124"+
-    "\2\0\1\47\1\0\1\50\1\0\1\51\1\0\1\52"+
-    "\1\0\1\53\1\0\1\54\3\0\1\55\5\0\1\56"+
-    "\3\0\1\57\11\0\1\60\2\0\1\61\16\0\1\62"+
-    "\2\0\1\63\41\0\2\25\1\64\1\0\1\65\1\0"+
-    "\1\65\1\66\1\0\1\25\2\0\1\25\1\126\20\36"+
-    "\1\340\11\36\1\127\12\130\1\65\1\124\1\131\1\124"+
-    "\1\0\1\124\1\132\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\2\0\1\47\1\0\1\50\1\0\1\51\1\0"+
-    "\1\52\1\0\1\53\1\0\1\54\3\0\1\55\5\0"+
-    "\1\56\3\0\1\57\11\0\1\60\2\0\1\61\16\0"+
-    "\1\62\2\0\1\63\41\0\2\25\1\64\1\0\1\65"+
-    "\1\0\1\65\1\66\1\0\1\25\2\0\1\25\1\126"+
-    "\7\36\1\340\22\36\1\127\12\130\1\65\1\124\1\131"+
-    "\1\124\1\0\1\124\1\132\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\2\0\1\47\1\0\1\50\1\0\1\51"+
-    "\1\0\1\52\1\0\1\53\1\0\1\54\3\0\1\55"+
-    "\5\0\1\56\3\0\1\57\11\0\1\60\2\0\1\61"+
-    "\16\0\1\62\2\0\1\63\41\0\2\25\1\64\1\0"+
-    "\1\65\1\0\1\65\1\66\1\0\1\25\2\0\1\25"+
-    "\1\126\27\36\1\340\2\36\1\127\12\130\1\65\1\124"+
-    "\1\131\1\124\1\0\1\124\1\132\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\2\0\1\47\1\0\1\50\1\0"+
-    "\1\51\1\0\1\52\1\0\1\53\1\0\1\54\3\0"+
-    "\1\55\5\0\1\56\3\0\1\57\11\0\1\60\2\0"+
-    "\1\61\16\0\1\62\2\0\1\63\41\0\2\25\1\64"+
-    "\1\0\1\65\1\0\1\65\1\66\1\0\1\25\2\0"+
-    "\1\25\1\u0131\6\36\1\u0136\10\36\1\340\12\36\1\127"+
-    "\12\130\1\u0132\1\124\1\131\1\124\1\0\1\124\1\132"+
-    "\1\125\1\u0133\1\u0134\1\u0135\3\0\1\124\4\0\2\124"+
-    "\2\0\1\47\1\0\1\50\1\0\1\51\1\0\1\52"+
-    "\1\0\1\53\1\0\1\54\3\0\1\55\5\0\1\56"+
-    "\3\0\1\57\11\0\1\60\2\0\1\61\16\0\1\62"+
-    "\2\0\1\63\41\0\2\25\1\64\1\0\1\65\1\0"+
-    "\1\65\1\66\1\0\1\25\2\0\1\25\1\u0131\24\36"+
-    "\1\u0138\5\36\1\127\12\130\1\u0132\1\124\1\131\1\124"+
-    "\1\0\1\124\1\132\1\125\1\u0133\1\u0134\1\u0135\3\0"+
-    "\1\124\4\0\2\124\2\0\1\47\1\0\1\50\1\0"+
-    "\1\51\1\0\1\52\1\0\1\53\1\0\1\54\3\0"+
-    "\1\55\5\0\1\56\3\0\1\57\11\0\1\60\2\0"+
-    "\1\61\16\0\1\62\2\0\1\63\41\0\2\25\1\64"+
-    "\1\0\1\65\1\0\1\65\1\66\1\0\1\25\2\0"+
-    "\1\25\1\126\11\36\1\340\20\36\1\127\12\130\1\65"+
-    "\1\124\1\131\1\124\1\0\1\124\1\132\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\2\0\1\47\1\0\1\50"+
-    "\1\0\1\51\1\0\1\52\1\0\1\53\1\0\1\54"+
-    "\3\0\1\55\5\0\1\56\3\0\1\57\11\0\1\60"+
-    "\2\0\1\61\16\0\1\62\2\0\1\63\41\0\2\25"+
-    "\1\64\1\0\1\65\1\0\1\65\1\66\1\0\1\25"+
-    "\2\0\1\25\1\u0131\16\36\1\u0139\13\36\1\127\12\130"+
-    "\1\u0132\1\124\1\131\1\124\1\0\1\124\1\132\1\125"+
-    "\1\u0133\1\u0134\1\u0135\3\0\1\124\4\0\2\124\2\0"+
-    "\1\47\1\0\1\50\1\0\1\51\1\0\1\52\1\0"+
-    "\1\53\1\0\1\54\3\0\1\55\5\0\1\56\3\0"+
-    "\1\57\11\0\1\60\2\0\1\61\16\0\1\62\2\0"+
-    "\1\63\41\0\2\25\1\64\1\0\1\65\1\0\1\65"+
-    "\1\66\1\0\1\25\2\0\1\25\1\u0131\12\36\1\u013a"+
-    "\17\36\1\127\12\130\1\u0132\1\124\1\131\1\124\1\0"+
-    "\1\124\1\132\1\125\1\u0133\1\u0134\1\u0135\3\0\1\124"+
-    "\4\0\2\124\2\0\1\47\1\0\1\50\1\0\1\51"+
-    "\1\0\1\52\1\0\1\53\1\0\1\54\3\0\1\55"+
-    "\5\0\1\56\3\0\1\57\11\0\1\60\2\0\1\61"+
-    "\16\0\1\62\2\0\1\63\41\0\2\25\1\64\1\0"+
-    "\1\65\1\0\1\65\1\66\1\0\1\25\2\0\1\25"+
-    "\1\u0131\5\36\1\340\24\36\1\127\12\130\1\u0132\1\124"+
-    "\1\131\1\124\1\0\1\124\1\132\1\125\1\u0133\1\u0134"+
-    "\1\u0135\3\0\1\124\4\0\2\124\2\0\1\47\1\0"+
-    "\1\50\1\0\1\51\1\0\1\52\1\0\1\53\1\0"+
-    "\1\54\3\0\1\55\5\0\1\56\3\0\1\57\11\0"+
-    "\1\60\2\0\1\61\16\0\1\62\2\0\1\63\41\0"+
-    "\2\25\1\64\1\0\1\65\1\0\1\65\1\66\1\0"+
-    "\1\25\2\0\1\25\1\u0131\1\u013b\31\36\1\127\12\130"+
-    "\1\u0132\1\124\1\131\1\124\1\0\1\124\1\132\1\125"+
-    "\1\u0133\1\u0134\1\u0135\3\0\1\124\4\0\2\124\2\0"+
-    "\1\47\1\0\1\50\1\0\1\51\1\0\1\52\1\0"+
-    "\1\53\1\0\1\54\3\0\1\55\5\0\1\56\3\0"+
-    "\1\57\11\0\1\60\2\0\1\61\16\0\1\62\2\0"+
-    "\1\63\41\0\2\25\1\64\1\0\1\65\1\0\1\65"+
-    "\1\66\1\0\1\25\2\0\1\25\1\126\32\36\1\u013c"+
-    "\12\130\1\65\1\124\1\131\1\124\1\0\1\124\1\132"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\2\0\1\47"+
-    "\1\0\1\50\1\0\1\51\1\0\1\52\1\0\1\53"+
-    "\1\0\1\54\3\0\1\55\5\0\1\56\3\0\1\57"+
-    "\11\0\1\60\2\0\1\61\16\0\1\62\2\0\1\63"+
-    "\41\0\2\25\1\64\1\0\1\65\1\0\1\65\1\66"+
-    "\1\0\1\25\2\0\1\25\1\u0131\23\36\1\340\6\36"+
-    "\1\127\12\130\1\u0132\1\124\1\131\1\124\1\0\1\124"+
-    "\1\132\1\125\1\u0133\1\u0134\1\u0135\3\0\1\124\4\0"+
-    "\2\124\2\0\1\47\1\0\1\50\1\0\1\51\1\0"+
-    "\1\52\1\0\1\53\1\0\1\54\3\0\1\55\5\0"+
-    "\1\56\3\0\1\57\11\0\1\60\2\0\1\61\16\0"+
-    "\1\62\2\0\1\63\41\0\2\25\1\64\1\0\1\65"+
-    "\1\0\1\65\1\66\1\0\1\25\2\0\1\25\1\u0131"+
-    "\24\36\1\u013d\5\36\1\127\12\130\1\u0132\1\124\1\131"+
-    "\1\124\1\0\1\124\1\132\1\125\1\u0133\1\u0134\1\u0135"+
-    "\3\0\1\124\4\0\2\124\150\0\1\123\1\261\1\262"+
-    "\1\263\1\264\1\265\1\266\1\267\1\270\1\271\1\272"+
-    "\1\273\1\274\1\275\1\276\1\277\1\300\1\301\1\302"+
-    "\1\303\1\304\1\305\1\306\1\307\1\310\1\311\1\312"+
-    "\1\124\12\252\1\0\3\124\1\0\2\124\1\125\3\124"+
-    "\3\0\1\124\1\150\3\0\2\124\14\0\1\325\3\0"+
-    "\1\326\5\0\1\327\3\0\1\330\14\0\1\331\16\0"+
-    "\1\332\2\0\1\333\42\0\1\160\1\64\6\0\1\160"+
-    "\3\0\1\123\33\124\12\255\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\1\150\3\0\2\124\2\0"+
-    "\1\47\1\0\1\50\1\0\1\51\1\0\1\52\1\0"+
-    "\1\53\1\0\1\54\3\0\1\55\5\0\1\56\3\0"+
-    "\1\57\11\0\1\60\2\0\1\61\16\0\1\62\2\0"+
-    "\1\63\41\0\2\25\1\64\1\0\1\65\1\0\1\65"+
-    "\1\66\1\0\1\25\2\0\1\25\1\126\32\36\1\127"+
-    "\12\130\1\u013e\1\124\1\131\1\124\1\0\1\124\1\132"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\2\0\1\47"+
-    "\1\0\1\50\1\0\1\51\1\0\1\52\1\0\1\53"+
-    "\1\0\1\161\3\0\1\162\5\0\1\163\3\0\1\164"+
-    "\11\0\1\60\2\0\1\165\16\0\1\166\2\0\1\167"+
-    "\41\0\1\25\1\65\7\0\1\65\2\0\1\25\1\0"+
-    "\32\25\24\0\1\u013f\15\0\1\47\1\0\1\50\1\0"+
-    "\1\51\1\0\1\52\1\0\1\53\1\0\1\54\3\0"+
-    "\1\55\5\0\1\56\3\0\1\57\11\0\1\60\2\0"+
-    "\1\61\16\0\1\62\2\0\1\63\41\0\2\25\1\64"+
-    "\1\0\1\65\1\0\1\65\1\66\1\0\1\25\2\0"+
-    "\1\25\1\126\16\36\1\u0140\13\36\1\127\12\130\1\u0141"+
-    "\1\124\1\131\1\124\1\0\1\124\1\132\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\150\0\1\u0142\32\252\1\127"+
-    "\12\252\1\u0143\3\124\1\0\2\124\1\125\1\u0133\1\u0134"+
-    "\1\u0135\3\0\1\124\4\0\2\124\150\0\1\u0142\4\252"+
-    "\1\u0144\25\252\1\127\12\252\1\u0143\3\124\1\0\2\124"+
-    "\1\125\1\u0133\1\u0134\1\u0135\3\0\1\124\4\0\2\124"+
-    "\150\0\1\u0142\15\252\1\275\14\252\1\127\12\252\1\u0143"+
-    "\3\124\1\0\2\124\1\125\1\u0133\1\u0134\1\u0135\3\0"+
-    "\1\124\4\0\2\124\150\0\1\u0142\10\252\1\275\21\252"+
-    "\1\127\12\252\1\u0143\3\124\1\0\2\124\1\125\1\u0133"+
-    "\1\u0134\1\u0135\3\0\1\124\4\0\2\124\150\0\1\u0142"+
-    "\17\252\1\370\12\252\1\127\12\252\1\u0143\3\124\1\0"+
-    "\2\124\1\125\1\u0133\1\u0134\1\u0135\3\0\1\124\4\0"+
-    "\2\124\150\0\1\u0142\5\252\1\u0145\4\252\1\370\17\252"+
-    "\1\127\12\252\1\u0143\3\124\1\0\2\124\1\125\1\u0133"+
-    "\1\u0134\1\u0135\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\20\252\1\370\11\252\1\127\12\252\1\0\3\124\1\0"+
-    "\2\124\1\125\3\124\3\0\1\124\4\0\2\124\150\0"+
-    "\1\363\7\252\1\370\22\252\1\127\12\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\27\252\1\370\2\252\1\127\12\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\u0142\6\252\1\u0144\10\252\1\370\12\252"+
-    "\1\127\12\252\1\u0143\3\124\1\0\2\124\1\125\1\u0133"+
-    "\1\u0134\1\u0135\3\0\1\124\4\0\2\124\150\0\1\u0142"+
-    "\24\252\1\u0146\5\252\1\127\12\252\1\u0143\3\124\1\0"+
-    "\2\124\1\125\1\u0133\1\u0134\1\u0135\3\0\1\124\4\0"+
-    "\2\124\150\0\1\363\11\252\1\370\20\252\1\127\12\252"+
-    "\1\0\3\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\150\0\1\u0142\16\252\1\u0147\13\252\1\127"+
-    "\12\252\1\u0143\3\124\1\0\2\124\1\125\1\u0133\1\u0134"+
-    "\1\u0135\3\0\1\124\4\0\2\124\150\0\1\u0142\12\252"+
-    "\1\u0148\17\252\1\127\12\252\1\u0143\3\124\1\0\2\124"+
-    "\1\125\1\u0133\1\u0134\1\u0135\3\0\1\124\4\0\2\124"+
-    "\150\0\1\u0142\5\252\1\370\24\252\1\127\12\252\1\u0143"+
-    "\3\124\1\0\2\124\1\125\1\u0133\1\u0134\1\u0135\3\0"+
-    "\1\124\4\0\2\124\150\0\1\u0142\1\u0149\31\252\1\127"+
-    "\12\252\1\u0143\3\124\1\0\2\124\1\125\1\u0133\1\u0134"+
-    "\1\u0135\3\0\1\124\4\0\2\124\150\0\1\363\32\252"+
-    "\1\u013c\12\252\1\0\3\124\1\0\2\124\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\150\0\1\u0142\23\252\1\370"+
-    "\6\252\1\127\12\252\1\u0143\3\124\1\0\2\124\1\125"+
-    "\1\u0133\1\u0134\1\u0135\3\0\1\124\4\0\2\124\150\0"+
-    "\1\u0142\24\252\1\u014a\5\252\1\127\12\252\1\u0143\3\124"+
-    "\1\0\2\124\1\125\1\u0133\1\u0134\1\u0135\3\0\1\124"+
-    "\4\0\2\124\14\0\1\175\3\0\1\176\5\0\1\177"+
-    "\3\0\1\200\14\0\1\201\16\0\1\202\2\0\1\203"+
-    "\42\0\1\76\1\26\6\0\1\76\3\0\1\123\1\261"+
-    "\1\262\1\263\1\264\1\265\1\266\1\267\1\270\1\271"+
-    "\1\272\1\273\1\274\1\275\1\276\1\277\1\300\1\301"+
-    "\1\302\1\303\1\304\1\305\1\306\1\307\1\310\1\311"+
-    "\1\312\1\124\1\u014b\2\u014c\1\u014b\5\u014c\1\u014d\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\1\150"+
-    "\3\0\2\124\2\0\1\47\1\0\1\50\1\0\1\51"+
-    "\1\0\1\52\1\0\1\53\1\0\1\67\3\0\1\70"+
-    "\5\0\1\71\3\0\1\72\11\0\1\60\2\0\1\73"+
-    "\16\0\1\74\2\0\1\75\41\0\1\25\2\26\2\0"+
-    "\2\76\1\77\1\0\1\26\2\0\1\25\1\u010b\32\36"+
-    "\1\127\12\316\1\0\1\124\1\137\1\124\1\0\2\140"+
-    "\1\125\3\124\2\0\1\76\1\124\4\0\2\124\2\0"+
-    "\1\47\1\0\1\50\1\0\1\51\1\0\1\52\1\0"+
-    "\1\53\1\0\1\67\3\0\1\70\5\0\1\71\3\0"+
-    "\1\72\11\0\1\60\2\0\1\73\16\0\1\74\2\0"+
-    "\1\75\41\0\1\25\2\26\2\0\2\76\1\77\1\0"+
-    "\1\26\2\0\1\25\1\u010b\32\36\1\127\2\u010c\1\316"+
-    "\2\u010c\2\316\1\u010c\1\316\1\u010c\1\0\1\124\1\137"+
-    "\1\124\1\0\2\140\1\125\3\124\2\0\1\76\1\124"+
-    "\4\0\2\124\14\0\1\175\3\0\1\176\5\0\1\177"+
-    "\3\0\1\200\14\0\1\201\16\0\1\202\2\0\1\203"+
-    "\42\0\1\76\1\26\6\0\1\76\3\0\1\123\1\261"+
-    "\1\262\1\263\1\264\1\265\1\266\1\267\1\270\1\271"+
-    "\1\272\1\273\1\274\1\275\1\276\1\277\1\300\1\301"+
-    "\1\302\1\303\1\304\1\305\1\306\1\307\1\310\1\311"+
-    "\1\312\1\124\12\316\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\1\150\3\0\2\124\151\0\4\u014e"+
-    "\2\0\1\u014e\15\0\1\u014e\6\0\12\u014e\1\322\175\0"+
-    "\4\u014f\2\0\1\u014f\15\0\1\u014f\6\0\12\u014f\1\u0150"+
-    "\175\0\4\u0151\2\0\1\u0151\15\0\1\u0151\6\0\1\u0152"+
-    "\2\u0153\1\u0152\5\u0153\1\u0154\14\0\1\u0116\161\0\4\u0155"+
-    "\2\0\1\u0155\15\0\1\u0155\6\0\12\u0155\1\u0156\13\0"+
-    "\1\u0116\160\0\1\u0157\4\u0155\2\0\1\u0155\15\0\1\u0155"+
-    "\6\0\12\u0158\1\u0156\13\0\1\u0116\160\0\1\u0157\4\u0155"+
-    "\2\0\1\u0155\15\0\1\u0155\6\0\12\u0159\1\u0156\13\0"+
-    "\1\u0116\160\0\1\u0157\4\u0155\2\0\1\u0155\15\0\1\u0155"+
-    "\6\0\1\u0158\1\u015a\1\u0159\2\u0158\2\u0159\1\u0158\1\u0159"+
-    "\1\u0158\1\u0156\13\0\1\u0116\226\0\1\u0143\7\0\1\u015b"+
-    "\1\u015c\1\u015d\162\0\1\334\1\216\2\u015e\1\u015f\1\u0160"+
-    "\10\u015e\1\216\1\u0161\5\u015e\6\216\1\335\12\216\175\0"+
-    "\1\334\1\u0162\2\u015e\1\216\1\u015e\1\u0163\6\u015e\4\216"+
-    "\1\u015e\1\216\2\u015e\1\216\1\u015e\1\216\3\u015e\1\335"+
-    "\12\216\175\0\1\334\3\216\1\u015e\1\216\1\u015e\4\216"+
-    "\1\u015e\10\216\1\u015e\2\216\1\u015e\2\216\1\u015e\1\335"+
-    "\12\216\175\0\1\334\1\216\1\u015e\1\u0164\2\u015e\2\216"+
-    "\1\u015e\6\216\3\u015e\11\216\1\335\12\216\175\0\1\334"+
-    "\3\216\1\u015e\1\216\1\u015e\10\216\1\u015e\1\216\2\u015e"+
-    "\10\216\1\335\12\216\175\0\1\334\4\216\1\u0165\5\216"+
-    "\1\u015e\17\216\1\335\12\216\175\0\1\334\4\216\2\u015e"+
-    "\2\216\1\u015e\1\216\1\u015e\13\216\1\u015e\2\216\1\u015e"+
-    "\1\335\12\216\175\0\1\334\1\u015e\1\216\3\u015e\1\u0166"+
-    "\14\u015e\2\216\2\u015e\2\216\1\u015e\1\216\1\335\12\216"+
-    "\175\0\1\334\2\216\4\u015e\3\216\2\u015e\1\u0167\1\u015e"+
-    "\1\216\2\u015e\12\216\1\335\12\216\175\0\1\334\2\u015e"+
-    "\2\216\1\u015e\3\216\1\u015e\5\216\3\u015e\3\216\1\u015e"+
-    "\2\216\3\u015e\1\335\12\216\175\0\1\334\5\u015e\1\u0168"+
-    "\1\216\1\u015e\1\u0169\7\u015e\1\u016a\3\u015e\1\216\1\u015e"+
-    "\1\216\3\u015e\1\335\12\216\175\0\1\334\1\u016b\1\u015e"+
-    "\1\216\1\u0162\6\u015e\3\216\1\u015e\2\216\1\u015e\2\216"+
-    "\1\u015e\6\216\1\335\12\216\175\0\1\334\1\u015e\31\216"+
-    "\1\335\12\216\175\0\1\334\1\u015e\2\216\1\u015e\1\u016c"+
-    "\1\216\2\u015e\1\216\3\u015e\2\216\2\u015e\1\216\1\u015e"+
-    "\3\216\1\u015e\2\216\2\u015e\1\335\12\216\175\0\1\334"+
-    "\6\u015e\1\216\5\u015e\3\216\2\u015e\2\216\7\u015e\1\335"+
-    "\12\216\175\0\1\334\1\216\2\u015e\1\u0169\1\u016d\3\u015e"+
-    "\1\216\3\u015e\1\216\1\u015e\1\216\1\u015e\1\216\1\u015e"+
-    "\1\216\1\u015e\1\216\3\u015e\1\216\1\u015e\1\335\12\216"+
-    "\175\0\1\334\1\u015e\6\216\1\u015e\6\216\1\u015e\4\216"+
-    "\1\u015e\4\216\2\u015e\1\335\12\216\175\0\1\334\6\216"+
-    "\1\u015e\7\216\1\u015e\13\216\1\335\12\216\175\0\1\334"+
-    "\13\216\1\u016e\16\216\1\335\12\216\175\0\1\334\1\u015e"+
-    "\11\216\1\u015e\6\216\1\u015e\10\216\1\335\12\216\175\0"+
-    "\1\334\1\u015e\1\216\6\u015e\1\u016f\1\216\2\u015e\2\216"+
-    "\2\u015e\1\216\1\u015e\1\216\6\u015e\1\216\1\335\12\216"+
-    "\175\0\1\334\4\216\1\u015e\5\216\2\u015e\3\216\2\u015e"+
-    "\10\216\1\u015e\1\335\12\216\175\0\1\334\3\216\1\u015e"+
-    "\1\216\1\u0170\4\216\1\u015e\2\216\1\u015e\14\216\1\335"+
-    "\12\216\175\0\1\334\2\u015e\1\216\1\u015e\3\216\2\u015e"+
-    "\2\216\1\u015e\4\216\1\u015e\11\216\1\335\12\216\175\0"+
-    "\1\334\3\216\1\u015e\13\216\1\u015e\12\216\1\335\12\216"+
-    "\175\0\1\334\3\216\2\u015e\2\216\2\u015e\1\216\2\u015e"+
-    "\1\216\1\u015e\3\216\1\u015e\1\216\1\u015e\1\216\1\u015e"+
-    "\2\216\1\u015e\1\216\1\335\12\216\27\0\1\47\1\0"+
-    "\1\50\1\0\1\51\1\0\1\52\1\0\1\53\1\0"+
-    "\1\161\3\0\1\162\5\0\1\163\3\0\1\164\11\0"+
-    "\1\60\2\0\1\165\16\0\1\166\2\0\1\167\41\0"+
-    "\1\25\1\65\7\0\1\65\2\0\1\25\1\123\1\220"+
-    "\1\221\1\222\1\223\1\224\1\225\1\226\1\227\1\230"+
-    "\1\231\1\232\1\233\1\234\1\235\1\236\1\237\1\240"+
-    "\1\241\1\242\1\243\1\244\1\245\1\246\1\247\1\250"+
-    "\1\251\1\124\12\252\1\u0143\3\124\1\0\2\124\1\125"+
-    "\1\u0133\1\u0134\1\u0135\3\0\1\124\1\150\3\0\2\124"+
-    "\2\0\1\47\1\0\1\50\1\0\1\51\1\0\1\52"+
-    "\1\0\1\53\1\0\1\161\3\0\1\162\5\0\1\163"+
-    "\3\0\1\164\11\0\1\60\2\0\1\165\16\0\1\166"+
-    "\2\0\1\167\41\0\1\25\1\65\7\0\1\65\2\0"+
-    "\1\25\1\0\32\25\1\0\12\u0171\175\0\1\u0172\45\u0133"+
-    "\1\u015b\2\u0133\1\u0173\1\u015b\2\u0133\1\u0174\2\u0133\1\u0135"+
-    "\2\0\1\u015b\1\u0133\4\0\1\u0133\1\124\150\0\1\u0175"+
-    "\45\u0134\1\u015c\2\u0134\1\u0176\1\0\2\124\1\u0177\1\u0133"+
-    "\1\u0134\1\u0135\2\0\1\u015c\1\u0134\4\0\2\124\150\0"+
-    "\1\u0178\45\u0135\1\u015d\2\u0135\1\u0179\1\u015d\2\u0135\1\u017a"+
-    "\2\u0135\1\124\2\0\1\u015d\1\u0135\4\0\1\u0135\1\124"+
-    "\2\0\1\47\1\0\1\50\1\0\1\51\1\0\1\52"+
-    "\1\0\1\53\1\0\1\54\3\0\1\55\5\0\1\56"+
-    "\3\0\1\57\11\0\1\60\2\0\1\61\16\0\1\62"+
-    "\2\0\1\63\41\0\2\25\1\64\1\0\1\65\1\0"+
-    "\1\65\1\66\1\0\1\25\2\0\1\25\1\126\5\36"+
-    "\1\340\24\36\1\127\12\130\1\65\1\124\1\131\1\124"+
-    "\1\0\1\124\1\132\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\2\0\1\47\1\0\1\50\1\0\1\51\1\0"+
-    "\1\52\1\0\1\53\1\0\1\54\3\0\1\55\5\0"+
-    "\1\56\3\0\1\57\11\0\1\60\2\0\1\61\16\0"+
-    "\1\62\2\0\1\63\41\0\2\25\1\64\1\0\1\65"+
-    "\1\0\1\65\1\66\1\0\1\25\2\0\1\25\1\126"+
-    "\15\36\1\340\14\36\1\127\12\130\1\65\1\124\1\131"+
-    "\1\124\1\0\1\124\1\132\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\2\0\1\47\1\0\1\50\1\0\1\51"+
-    "\1\0\1\52\1\0\1\53\1\0\1\54\3\0\1\55"+
-    "\5\0\1\56\3\0\1\57\11\0\1\60\2\0\1\61"+
-    "\16\0\1\62\2\0\1\63\41\0\2\25\1\64\1\0"+
-    "\1\65\1\0\1\65\1\66\1\0\1\25\2\0\1\25"+
-    "\1\126\10\36\1\340\21\36\1\127\12\130\1\65\1\124"+
-    "\1\131\1\124\1\0\1\124\1\132\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\2\0\1\47\1\0\1\50\1\0"+
-    "\1\51\1\0\1\52\1\0\1\53\1\0\1\54\3\0"+
-    "\1\55\5\0\1\56\3\0\1\57\11\0\1\60\2\0"+
-    "\1\61\16\0\1\62\2\0\1\63\41\0\2\25\1\64"+
-    "\1\0\1\65\1\0\1\65\1\66\1\0\1\25\2\0"+
-    "\1\25\1\126\3\36\1\u017b\26\36\1\127\12\130\1\65"+
-    "\1\124\1\131\1\124\1\0\1\124\1\132\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\2\0\1\47\1\0\1\50"+
-    "\1\0\1\51\1\0\1\52\1\0\1\53\1\0\1\54"+
-    "\3\0\1\55\5\0\1\56\3\0\1\57\11\0\1\60"+
-    "\2\0\1\61\16\0\1\62\2\0\1\63\41\0\2\25"+
-    "\1\64\1\0\1\65\1\0\1\65\1\66\1\0\1\25"+
-    "\2\0\1\25\1\126\3\36\1\340\26\36\1\127\12\130"+
-    "\1\65\1\124\1\131\1\124\1\0\1\124\1\132\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\2\0\1\47\1\0"+
-    "\1\50\1\0\1\51\1\0\1\52\1\0\1\53\1\0"+
-    "\1\54\3\0\1\55\5\0\1\56\3\0\1\57\11\0"+
-    "\1\60\2\0\1\61\16\0\1\62\2\0\1\63\41\0"+
-    "\2\25\1\64\1\0\1\65\1\0\1\65\1\66\1\0"+
-    "\1\25\2\0\1\25\1\126\27\36\1\u017c\2\36\1\127"+
-    "\12\130\1\65\1\124\1\131\1\124\1\0\1\124\1\132"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\123"+
-    "\32\252\1\u017d\12\252\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\2\0\1\47\1\0"+
-    "\1\50\1\0\1\51\1\0\1\52\1\0\1\53\1\0"+
-    "\1\54\3\0\1\55\5\0\1\56\3\0\1\57\11\0"+
-    "\1\60\2\0\1\61\16\0\1\62\2\0\1\63\41\0"+
-    "\2\25\1\64\1\0\1\65\1\0\1\65\1\66\1\0"+
-    "\1\25\2\0\1\25\1\126\16\36\1\340\13\36\1\127"+
-    "\12\130\1\65\1\124\1\131\1\124\1\0\1\124\1\132"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\2\0\1\47"+
-    "\1\0\1\50\1\0\1\51\1\0\1\52\1\0\1\53"+
-    "\1\0\1\161\3\0\1\162\5\0\1\163\3\0\1\164"+
-    "\11\0\1\60\2\0\1\165\16\0\1\166\2\0\1\167"+
-    "\41\0\1\25\1\65\7\0\1\65\2\0\1\25\1\0"+
-    "\32\25\24\0\1\u017e\242\0\1\u017f\15\0\1\47\1\0"+
-    "\1\50\1\0\1\51\1\0\1\52\1\0\1\53\1\0"+
-    "\1\54\3\0\1\55\5\0\1\56\3\0\1\57\11\0"+
-    "\1\60\2\0\1\61\16\0\1\62\2\0\1\63\41\0"+
-    "\2\25\1\64\1\0\1\65\1\0\1\65\1\66\1\0"+
-    "\1\25\2\0\1\25\1\126\32\36\1\127\12\130\1\u0141"+
-    "\1\124\1\131\1\124\1\0\1\124\1\132\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\2\0\1\47\1\0\1\50"+
-    "\1\0\1\51\1\0\1\52\1\0\1\53\1\0\1\161"+
-    "\3\0\1\162\5\0\1\163\3\0\1\164\11\0\1\60"+
-    "\2\0\1\165\16\0\1\166\2\0\1\167\41\0\1\25"+
-    "\1\65\7\0\1\65\2\0\1\25\1\0\32\25\24\0"+
-    "\1\u0180\163\0\1\123\1\261\1\262\1\263\1\264\1\265"+
-    "\1\266\1\267\1\270\1\271\1\272\1\273\1\274\1\275"+
-    "\1\276\1\277\1\300\1\301\1\302\1\303\1\304\1\305"+
-    "\1\306\1\307\1\310\1\311\1\312\1\124\12\252\1\u0143"+
-    "\3\124\1\0\2\124\1\125\1\u0133\1\u0134\1\u0135\3\0"+
-    "\1\124\1\150\3\0\2\124\204\0\12\u0171\175\0\1\363"+
-    "\5\252\1\370\24\252\1\127\12\252\1\0\3\124\1\0"+
-    "\2\124\1\125\3\124\3\0\1\124\4\0\2\124\150\0"+
-    "\1\363\15\252\1\370\14\252\1\127\12\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\10\252\1\370\21\252\1\127\12\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\363\3\252\1\u0181\26\252\1\127\12\252"+
-    "\1\0\3\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\150\0\1\363\3\252\1\370\26\252\1\127"+
-    "\12\252\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\150\0\1\363\27\252\1\u0182\2\252"+
-    "\1\127\12\252\1\0\3\124\1\0\2\124\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\150\0\1\363\16\252\1\370"+
-    "\13\252\1\127\12\252\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\2\0\1\47\1\0"+
-    "\1\50\1\0\1\51\1\0\1\52\1\0\1\53\1\0"+
-    "\1\67\3\0\1\70\5\0\1\71\3\0\1\72\11\0"+
-    "\1\60\2\0\1\73\16\0\1\74\2\0\1\75\41\0"+
-    "\1\25\2\26\2\0\2\76\1\77\1\0\1\26\2\0"+
-    "\1\25\1\u0183\32\36\1\127\12\u014c\1\0\1\124\1\137"+
-    "\1\124\1\0\2\140\1\125\3\124\2\0\1\76\1\124"+
-    "\4\0\2\124\2\0\1\47\1\0\1\50\1\0\1\51"+
-    "\1\0\1\52\1\0\1\53\1\0\1\67\3\0\1\70"+
-    "\5\0\1\71\3\0\1\72\11\0\1\60\2\0\1\73"+
-    "\16\0\1\74\2\0\1\75\41\0\1\25\2\26\2\0"+
-    "\2\76\1\77\1\0\1\26\2\0\1\25\1\u0183\32\36"+
-    "\1\127\12\u0184\1\0\1\124\1\137\1\124\1\0\2\140"+
-    "\1\125\3\124\2\0\1\76\1\124\4\0\2\124\2\0"+
-    "\1\47\1\0\1\50\1\0\1\51\1\0\1\52\1\0"+
-    "\1\53\1\0\1\67\3\0\1\70\5\0\1\71\3\0"+
-    "\1\72\11\0\1\60\2\0\1\73\16\0\1\74\2\0"+
-    "\1\75\41\0\1\25\2\26\2\0\2\76\1\77\1\0"+
-    "\1\26\2\0\1\25\1\u0183\32\36\1\127\1\u014c\1\u0185"+
-    "\1\u0184\2\u014c\2\u0184\1\u014c\1\u0184\1\u014c\1\0\1\124"+
-    "\1\137\1\124\1\0\2\140\1\125\3\124\2\0\1\76"+
-    "\1\124\4\0\2\124\216\0\1\322\175\0\4\u0186\2\0"+
-    "\1\u0186\15\0\1\u0186\6\0\12\u0186\1\u0150\175\0\4\u0187"+
-    "\2\0\1\u0187\15\0\1\u0187\6\0\12\u0187\1\u0188\175\0"+
-    "\4\u0189\2\0\1\u0189\15\0\1\u0189\6\0\12\u0189\1\u018a"+
-    "\13\0\1\u0116\160\0\1\u0157\4\u0189\2\0\1\u0189\15\0"+
-    "\1\u0189\6\0\12\u018b\1\u018a\13\0\1\u0116\160\0\1\u0157"+
-    "\4\u0189\2\0\1\u0189\15\0\1\u0189\6\0\12\u018c\1\u018a"+
-    "\13\0\1\u0116\160\0\1\u0157\4\u0189\2\0\1\u0189\15\0"+
-    "\1\u0189\6\0\1\u018b\1\u018d\1\u018c\2\u018b\2\u018c\1\u018b"+
-    "\1\u018c\1\u018b\1\u018a\13\0\1\u0116\161\0\4\u018e\2\0"+
-    "\1\u018e\15\0\1\u018e\6\0\12\u018e\1\u0156\13\0\1\u0116"+
-    "\161\0\4\u0151\2\0\1\u0151\15\0\1\u0151\6\0\1\u0152"+
-    "\2\u0153\1\u0152\5\u0153\1\u0154\231\0\1\u018f\2\u0190\1\u018f"+
-    "\5\u0190\1\u0191\175\0\1\u0157\4\u018e\2\0\1\u018e\15\0"+
-    "\1\u018e\6\0\12\u0192\1\u0156\13\0\1\u0116\160\0\1\u0157"+
-    "\4\u018e\2\0\1\u018e\15\0\1\u018e\6\0\12\u018e\1\u0156"+
-    "\13\0\1\u0116\160\0\1\u0157\4\u018e\2\0\1\u018e\15\0"+
-    "\1\u018e\6\0\2\u0192\1\u018e\2\u0192\2\u018e\1\u0192\1\u018e"+
-    "\1\u0192\1\u0156\13\0\1\u0116\160\0\51\u015b\1\u0193\6\u015b"+
-    "\1\u015d\2\0\2\u015b\4\0\1\u015b\151\0\51\u015c\1\u0194"+
-    "\3\0\1\u015c\1\u015b\1\u015c\1\u015d\2\0\2\u015c\156\0"+
-    "\51\u015d\1\u0195\6\u015d\3\0\2\u015d\4\0\1\u015d\151\0"+
-    "\1\u0196\32\216\1\335\12\216\175\0\1\u0196\4\216\1\u0197"+
-    "\25\216\1\335\12\216\175\0\1\u0196\15\216\1\u0123\14\216"+
-    "\1\335\12\216\175\0\1\u0196\10\216\1\u0123\21\216\1\335"+
-    "\12\216\175\0\1\u0196\17\216\1\u015e\12\216\1\335\12\216"+
-    "\175\0\1\u0196\5\216\1\u0198\4\216\1\u015e\17\216\1\335"+
-    "\12\216\175\0\1\334\20\216\1\u015e\11\216\1\335\12\216"+
-    "\175\0\1\334\7\216\1\u015e\22\216\1\335\12\216\175\0"+
-    "\1\334\27\216\1\u015e\2\216\1\335\12\216\175\0\1\u0196"+
-    "\6\216\1\u0197\10\216\1\u015e\12\216\1\335\12\216\175\0"+
-    "\1\u0196\24\216\1\u0199\5\216\1\335\12\216\175\0\1\334"+
-    "\11\216\1\u015e\20\216\1\335\12\216\175\0\1\u0196\16\216"+
-    "\1\u019a\13\216\1\335\12\216\175\0\1\u0196\12\216\1\u019b"+
-    "\17\216\1\335\12\216\175\0\1\u0196\5\216\1\u015e\24\216"+
-    "\1\335\12\216\175\0\1\u0196\1\u019c\31\216\1\335\12\216"+
-    "\175\0\1\334\32\216\1\u019d\12\216\175\0\1\u0196\23\216"+
-    "\1\u015e\6\216\1\335\12\216\175\0\1\u0196\24\216\1\u019e"+
-    "\5\216\1\335\12\216\231\0\12\u019f\10\0\1\u015b\1\u015c"+
-    "\1\u015d\162\0\1\u0172\45\u0133\1\u015b\2\u0133\1\u0173\1\u015b"+
-    "\2\u0133\1\u0174\2\u0133\1\u0135\2\0\1\u015b\1\u0133\1\150"+
-    "\3\0\1\u0133\1\124\150\0\1\123\4\u01a0\2\124\1\u01a0"+
-    "\15\124\1\u01a0\6\124\12\u01a0\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\51\u015b"+
-    "\1\u0193\6\u015b\1\u015d\1\217\1\0\2\u015b\4\0\1\u015b"+
-    "\151\0\1\u0175\45\u0134\1\u015c\2\u0134\1\u0176\1\0\2\124"+
-    "\1\u0177\1\u0133\1\u0134\1\u0135\2\0\1\u015c\1\u0134\1\150"+
-    "\3\0\2\124\150\0\1\123\4\u01a1\2\124\1\u01a1\15\124"+
-    "\1\u01a1\6\124\12\u01a1\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\150\0\51\u015c\1\u0194"+
-    "\3\0\1\u015c\1\u015b\1\u015c\1\u015d\1\217\1\0\2\u015c"+
-    "\156\0\1\u0178\45\u0135\1\u015d\2\u0135\1\u0179\1\u015d\2\u0135"+
-    "\1\u017a\2\u0135\1\124\2\0\1\u015d\1\u0135\1\150\3\0"+
-    "\1\u0135\1\124\150\0\1\123\4\u01a2\2\124\1\u01a2\15\124"+
-    "\1\u01a2\6\124\12\u01a2\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\150\0\51\u015d\1\u0195"+
-    "\6\u015d\1\0\1\217\1\0\2\u015d\4\0\1\u015d\3\0"+
-    "\1\47\1\0\1\50\1\0\1\51\1\0\1\52\1\0"+
-    "\1\53\1\0\1\54\3\0\1\55\5\0\1\56\3\0"+
-    "\1\57\11\0\1\60\2\0\1\61\16\0\1\62\2\0"+
-    "\1\63\41\0\2\25\1\64\1\0\1\65\1\0\1\65"+
-    "\1\66\1\0\1\25\2\0\1\25\1\126\20\36\1\u01a3"+
-    "\11\36\1\127\12\130\1\65\1\124\1\131\1\124\1\0"+
-    "\1\124\1\132\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\2\0\1\47\1\0\1\50\1\0\1\51\1\0\1\52"+
-    "\1\0\1\53\1\0\1\54\3\0\1\55\5\0\1\56"+
-    "\3\0\1\57\11\0\1\60\2\0\1\61\16\0\1\62"+
-    "\2\0\1\63\41\0\2\25\1\64\1\0\1\65\1\0"+
-    "\1\65\1\66\1\0\1\25\2\0\1\25\1\126\3\36"+
-    "\1\353\26\36\1\127\12\130\1\65\1\124\1\131\1\124"+
-    "\1\0\1\124\1\132\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\123\1\252\1\u01a4\1\u01a5\2\252\1\u01a6"+
-    "\1\u01a7\1\u01a8\2\252\1\u01a9\2\252\1\u01aa\1\u01ab\2\252"+
-    "\1\u01ac\1\u01ad\1\u01ae\1\252\1\u01af\1\u01b0\1\252\1\u01b1"+
-    "\1\u01b2\1\127\1\u01b3\2\252\1\u01b4\1\u01b5\1\u01b6\1\252"+
-    "\1\u01b7\1\u01b8\1\252\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\227\0\1\u01b9\163\0"+
-    "\1\u01ba\32\u01bb\1\u01ba\12\u01bb\1\u01bc\2\u01ba\1\u01bd\3\u01ba"+
-    "\1\u01be\3\0\1\u01bf\1\0\2\u01ba\4\0\1\u01ba\230\0"+
-    "\1\u01c0\163\0\1\363\20\252\1\u01c1\11\252\1\127\12\252"+
-    "\1\0\3\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\150\0\1\363\3\252\1\u0103\26\252\1\127"+
-    "\12\252\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\14\0\1\175\3\0\1\176\5\0"+
-    "\1\177\3\0\1\200\14\0\1\201\16\0\1\202\2\0"+
-    "\1\203\42\0\1\76\1\26\6\0\1\76\3\0\1\123"+
-    "\1\261\1\262\1\263\1\264\1\265\1\266\1\267\1\270"+
-    "\1\271\1\272\1\273\1\274\1\275\1\276\1\277\1\300"+
-    "\1\301\1\302\1\303\1\304\1\305\1\306\1\307\1\310"+
-    "\1\311\1\312\1\124\1\u01c2\2\u01c3\1\u01c2\5\u01c3\1\u01c4"+
-    "\1\0\3\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\1\150\3\0\2\124\2\0\1\47\1\0\1\50\1\0"+
-    "\1\51\1\0\1\52\1\0\1\53\1\0\1\67\3\0"+
-    "\1\70\5\0\1\71\3\0\1\72\11\0\1\60\2\0"+
-    "\1\73\16\0\1\74\2\0\1\75\41\0\1\25\2\26"+
-    "\2\0\2\76\1\77\1\0\1\26\2\0\1\25\1\u0183"+
-    "\32\36\1\127\12\316\1\0\1\124\1\137\1\124\1\0"+
-    "\2\140\1\125\3\124\2\0\1\76\1\124\4\0\2\124"+
-    "\2\0\1\47\1\0\1\50\1\0\1\51\1\0\1\52"+
-    "\1\0\1\53\1\0\1\67\3\0\1\70\5\0\1\71"+
-    "\3\0\1\72\11\0\1\60\2\0\1\73\16\0\1\74"+
-    "\2\0\1\75\41\0\1\25\2\26\2\0\2\76\1\77"+
-    "\1\0\1\26\2\0\1\25\1\u0183\32\36\1\127\2\u0184"+
-    "\1\316\2\u0184\2\316\1\u0184\1\316\1\u0184\1\0\1\124"+
-    "\1\137\1\124\1\0\2\140\1\125\3\124\2\0\1\76"+
-    "\1\124\4\0\2\124\151\0\4\u01c5\2\0\1\u01c5\15\0"+
-    "\1\u01c5\6\0\12\u01c5\1\u0150\175\0\4\u01c6\2\0\1\u01c6"+
-    "\15\0\1\u01c6\6\0\12\u01c6\1\u01c7\175\0\4\u01c8\2\0"+
-    "\1\u01c8\15\0\1\u01c8\6\0\1\u01c9\2\u01ca\1\u01c9\5\u01ca"+
-    "\1\u01cb\14\0\1\u0116\161\0\4\u01cc\2\0\1\u01cc\15\0"+
-    "\1\u01cc\6\0\12\u01cc\1\u018a\13\0\1\u0116\161\0\4\u01c8"+
-    "\2\0\1\u01c8\15\0\1\u01c8\6\0\1\u01c9\2\u01ca\1\u01c9"+
-    "\5\u01ca\1\u01cb\175\0\1\u0157\4\u01cc\2\0\1\u01cc\15\0"+
-    "\1\u01cc\6\0\12\u01cd\1\u018a\13\0\1\u0116\160\0\1\u0157"+
-    "\4\u01cc\2\0\1\u01cc\15\0\1\u01cc\6\0\12\u01cc\1\u018a"+
-    "\13\0\1\u0116\160\0\1\u0157\4\u01cc\2\0\1\u01cc\15\0"+
-    "\1\u01cc\6\0\2\u01cd\1\u01cc\2\u01cd\2\u01cc\1\u01cd\1\u01cc"+
-    "\1\u01cd\1\u018a\13\0\1\u0116\161\0\4\u01ce\2\0\1\u01ce"+
-    "\15\0\1\u01ce\6\0\12\u01ce\1\u0156\13\0\1\u0116\160\0"+
-    "\1\u01cf\33\0\12\u0190\175\0\1\u01cf\33\0\12\u01d0\175\0"+
-    "\1\u01cf\33\0\1\u0190\1\u01d1\1\u01d0\2\u0190\2\u01d0\1\u0190"+
-    "\1\u01d0\1\u0190\175\0\1\u0157\4\u01ce\2\0\1\u01ce\15\0"+
-    "\1\u01ce\6\0\12\u01ce\1\u0156\13\0\1\u0116\161\0\4\u01d2"+
-    "\2\0\1\u01d2\15\0\1\u01d2\6\0\12\u01d2\176\0\4\u01d3"+
-    "\2\0\1\u01d3\15\0\1\u01d3\6\0\12\u01d3\176\0\4\u01d4"+
-    "\2\0\1\u01d4\15\0\1\u01d4\6\0\12\u01d4\175\0\1\334"+
-    "\5\216\1\u015e\24\216\1\335\12\216\175\0\1\334\15\216"+
-    "\1\u015e\14\216\1\335\12\216\175\0\1\334\10\216\1\u015e"+
-    "\21\216\1\335\12\216\175\0\1\334\3\216\1\u01d5\26\216"+
-    "\1\335\12\216\175\0\1\334\3\216\1\u015e\26\216\1\335"+
-    "\12\216\175\0\1\334\27\216\1\u01d6\2\216\1\335\12\216"+
-    "\176\0\32\216\1\u01d7\12\216\175\0\1\334\16\216\1\u015e"+
-    "\13\216\1\335\12\216\231\0\12\u01d8\10\0\1\u015b\1\u015c"+
-    "\1\u015d\162\0\1\123\4\u0133\2\124\1\u0133\15\124\1\u0133"+
-    "\6\124\12\u0133\1\0\3\124\1\0\2\124\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\150\0\1\123\4\u0134\2\124"+
-    "\1\u0134\15\124\1\u0134\6\124\12\u0134\1\0\3\124\1\0"+
-    "\2\124\1\125\3\124\3\0\1\124\4\0\2\124\150\0"+
-    "\1\123\4\u0135\2\124\1\u0135\15\124\1\u0135\6\124\12\u0135"+
-    "\1\0\3\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\2\0\1\47\1\0\1\50\1\0\1\51"+
-    "\1\0\1\52\1\0\1\53\1\0\1\54\3\0\1\55"+
-    "\5\0\1\56\3\0\1\57\11\0\1\60\2\0\1\61"+
-    "\16\0\1\62\2\0\1\63\41\0\2\25\1\64\1\0"+
-    "\1\65\1\0\1\65\1\66\1\0\1\25\2\0\1\25"+
-    "\1\126\12\36\1\340\17\36\1\127\12\130\1\65\1\124"+
-    "\1\131\1\124\1\0\1\124\1\132\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\150\0\1\363\11\252\1\u01d9\20\252"+
-    "\1\127\12\252\1\0\3\124\1\0\2\124\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\150\0\1\363\3\252\1\u01da"+
-    "\26\252\1\127\12\252\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\150\0\1\363\7\252"+
-    "\1\u01db\22\252\1\127\4\252\1\u01dc\5\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\10\252\1\u01dd\4\252\1\u01de\5\252\1\u01df"+
-    "\6\252\1\127\12\252\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\150\0\1\363\3\252"+
-    "\1\u01e0\26\252\1\127\2\252\1\u01e1\7\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\7\252\1\u01e2\22\252\1\127\12\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\363\7\252\1\u01e3\22\252\1\127\3\252"+
-    "\1\u01e4\6\252\1\0\3\124\1\0\2\124\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\150\0\1\363\32\252\1\127"+
-    "\5\252\1\u01e5\4\252\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\150\0\1\363\7\252"+
-    "\1\u01e6\22\252\1\127\12\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\31\252\1\u01e7\1\127\12\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\1\252\1\u01e8\30\252\1\127\12\252\1\0\3\124\1\0"+
-    "\2\124\1\125\3\124\3\0\1\124\4\0\2\124\150\0"+
-    "\1\363\7\252\1\u01e9\1\252\1\u01ea\20\252\1\127\11\252"+
-    "\1\u01e5\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\150\0\1\363\22\252\1\u01eb\7\252"+
-    "\1\127\2\252\1\u01ec\7\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\6\252\1\u01ed\1\u01ee\22\252\1\127\12\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\7\252\1\u01ef\5\252\1\u01f0\14\252\1\127"+
-    "\12\252\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\150\0\1\363\23\252\1\u01f1\6\252"+
-    "\1\127\12\252\1\0\3\124\1\0\2\124\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\150\0\1\363\32\252\1\127"+
-    "\3\252\1\u01f2\6\252\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\150\0\1\363\3\252"+
-    "\1\u01f3\26\252\1\127\12\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\17\252\1\u01f4\12\252\1\127\12\252\1\0\3\124\1\0"+
-    "\2\124\1\125\3\124\3\0\1\124\4\0\2\124\150\0"+
-    "\1\363\32\252\1\127\1\252\1\u01e5\10\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\32\252\1\127\1\u01f5\11\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\151\0\32\u01f6\1\0\12\u01f6\11\0\1\u01f7\1\0\1\u01f8"+
-    "\161\0\46\u01ba\1\u01bc\2\u01ba\1\u01bd\3\u01ba\1\u01be\5\0"+
-    "\2\u01ba\4\0\1\u01ba\151\0\1\u01f9\32\u01bb\1\u01fa\12\u01bb"+
-    "\1\u01fb\2\u01ba\1\u01bd\3\u01ba\1\u01be\1\0\1\u01fc\3\0"+
-    "\2\u01ba\4\0\1\u01ba\151\0\46\u01bc\1\0\2\u01bc\1\u01fd"+
-    "\3\u01bc\1\u01be\5\0\2\u01bc\4\0\1\u01bc\152\0\4\u01fe"+
-    "\2\0\1\u01fe\15\0\1\u01fe\6\0\12\u01fe\176\0\32\u01ff"+
-    "\1\0\12\u01ff\13\0\1\u01bf\162\0\4\u0200\2\0\1\u0200"+
-    "\15\0\1\u0200\6\0\12\u0200\1\u0201\174\0\1\u0202\32\u0203"+
-    "\1\u0202\12\u0203\1\u0204\2\u0202\1\u0205\3\u0202\1\u0206\3\0"+
-    "\1\u0207\1\0\2\u0202\4\0\1\u0202\151\0\1\363\12\252"+
-    "\1\370\17\252\1\127\12\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\2\0\1\47"+
-    "\1\0\1\50\1\0\1\51\1\0\1\52\1\0\1\53"+
-    "\1\0\1\67\3\0\1\70\5\0\1\71\3\0\1\72"+
-    "\11\0\1\60\2\0\1\73\16\0\1\74\2\0\1\75"+
-    "\41\0\1\25\2\26\2\0\2\76\1\77\1\0\1\26"+
-    "\2\0\1\25\1\u010e\32\36\1\127\12\u01c3\1\u0143\1\124"+
-    "\1\137\1\124\1\0\2\140\1\125\1\u0133\1\u0134\1\u0135"+
-    "\2\0\1\76\1\124\4\0\2\124\2\0\1\47\1\0"+
-    "\1\50\1\0\1\51\1\0\1\52\1\0\1\53\1\0"+
-    "\1\67\3\0\1\70\5\0\1\71\3\0\1\72\11\0"+
-    "\1\60\2\0\1\73\16\0\1\74\2\0\1\75\41\0"+
-    "\1\25\2\26\2\0\2\76\1\77\1\0\1\26\2\0"+
-    "\1\25\1\u010e\32\36\1\127\12\u0208\1\u0143\1\124\1\137"+
-    "\1\124\1\0\2\140\1\125\1\u0133\1\u0134\1\u0135\2\0"+
-    "\1\76\1\124\4\0\2\124\2\0\1\47\1\0\1\50"+
-    "\1\0\1\51\1\0\1\52\1\0\1\53\1\0\1\67"+
-    "\3\0\1\70\5\0\1\71\3\0\1\72\11\0\1\60"+
-    "\2\0\1\73\16\0\1\74\2\0\1\75\41\0\1\25"+
-    "\2\26\2\0\2\76\1\77\1\0\1\26\2\0\1\25"+
-    "\1\u010e\32\36\1\127\1\u01c3\1\u0209\1\u0208\2\u01c3\2\u0208"+
-    "\1\u01c3\1\u0208\1\u01c3\1\u0143\1\124\1\137\1\124\1\0"+
-    "\2\140\1\125\1\u0133\1\u0134\1\u0135\2\0\1\76\1\124"+
-    "\4\0\2\124\216\0\1\u0150\175\0\4\u020a\2\0\1\u020a"+
-    "\15\0\1\u020a\6\0\12\u020a\1\u01c7\175\0\4\u020b\2\0"+
-    "\1\u020b\15\0\1\u020b\6\0\12\u020b\1\u020c\175\0\4\u020d"+
-    "\2\0\1\u020d\15\0\1\u020d\6\0\12\u020d\1\u020e\13\0"+
-    "\1\u0116\160\0\1\u0157\4\u020d\2\0\1\u020d\15\0\1\u020d"+
-    "\6\0\12\u020f\1\u020e\13\0\1\u0116\160\0\1\u0157\4\u020d"+
-    "\2\0\1\u020d\15\0\1\u020d\6\0\12\u0210\1\u020e\13\0"+
-    "\1\u0116\160\0\1\u0157\4\u020d\2\0\1\u020d\15\0\1\u020d"+
-    "\6\0\1\u020f\1\u0211\1\u0210\2\u020f\2\u0210\1\u020f\1\u0210"+
-    "\1\u020f\1\u020e\13\0\1\u0116\161\0\4\u0212\2\0\1\u0212"+
-    "\15\0\1\u0212\6\0\12\u0212\1\u018a\13\0\1\u0116\160\0"+
-    "\1\u0157\4\u0212\2\0\1\u0212\15\0\1\u0212\6\0\12\u0212"+
-    "\1\u018a\13\0\1\u0116\226\0\1\u0156\13\0\1\u0116\214\0"+
-    "\1\u0213\2\u0214\1\u0213\5\u0214\1\u0215\175\0\1\u01cf\242\0"+
-    "\1\u01cf\33\0\2\u01d0\1\0\2\u01d0\2\0\1\u01d0\1\0"+
-    "\1\u01d0\176\0\4\u015b\2\0\1\u015b\15\0\1\u015b\6\0"+
-    "\12\u015b\176\0\4\u015c\2\0\1\u015c\15\0\1\u015c\6\0"+
-    "\12\u015c\176\0\4\u015d\2\0\1\u015d\15\0\1\u015d\6\0"+
-    "\12\u015d\175\0\1\334\20\216\1\u0216\11\216\1\335\12\216"+
-    "\175\0\1\334\3\216\1\u0169\26\216\1\335\12\216\176\0"+
-    "\1\216\1\u0217\1\u0218\2\216\1\u0219\1\u021a\1\u021b\2\216"+
-    "\1\u021c\2\216\1\u021d\1\u021e\2\216\1\u021f\1\u0220\1\u0221"+
-    "\1\216\1\u0222\1\u0223\1\216\1\u0224\1\u0225\1\335\1\u0226"+
-    "\2\216\1\u0227\1\u0228\1\u0229\1\216\1\u022a\1\u022b\1\216"+
-    "\231\0\12\u022c\10\0\1\u015b\1\u015c\1\u015d\162\0\1\363"+
-    "\1\252\1\u022d\30\252\1\127\12\252\1\0\3\124\1\0"+
-    "\2\124\1\125\3\124\3\0\1\124\4\0\2\124\150\0"+
-    "\1\363\24\252\1\u022e\5\252\1\127\12\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\24\252\1\u022f\5\252\1\127\12\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\363\1\252\1\u0230\30\252\1\127\12\252"+
-    "\1\0\3\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\150\0\1\363\14\252\1\u0231\15\252\1\127"+
-    "\12\252\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\150\0\1\363\1\252\1\u0232\30\252"+
-    "\1\127\12\252\1\0\3\124\1\0\2\124\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\150\0\1\363\1\252\1\u0233"+
-    "\30\252\1\127\12\252\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\150\0\1\363\1\252"+
-    "\1\u0234\30\252\1\127\12\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\21\252\1\u0235\10\252\1\127\12\252\1\0\3\124\1\0"+
-    "\2\124\1\125\3\124\3\0\1\124\4\0\2\124\150\0"+
-    "\1\363\24\252\1\u0236\5\252\1\127\12\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\24\252\1\u0237\5\252\1\127\12\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\363\1\u0146\31\252\1\127\12\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\363\24\252\1\u0234\5\252\1\127\12\252"+
-    "\1\0\3\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\150\0\1\363\24\252\1\u0238\5\252\1\127"+
-    "\12\252\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\150\0\1\363\1\252\1\u0239\30\252"+
-    "\1\127\12\252\1\0\3\124\1\0\2\124\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\150\0\1\363\31\252\1\u023a"+
-    "\1\127\12\252\1\0\3\124\1\0\2\124\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\150\0\1\363\24\252\1\u023b"+
-    "\5\252\1\127\12\252\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\150\0\1\363\1\252"+
-    "\1\u023c\30\252\1\127\12\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\1\u023d\31\252\1\127\12\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\21\252\1\u023e\10\252\1\127\12\252\1\0\3\124\1\0"+
-    "\2\124\1\125\3\124\3\0\1\124\4\0\2\124\150\0"+
-    "\1\363\4\252\1\u023f\25\252\1\127\12\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\24\252\1\u0240\5\252\1\127\12\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\363\24\252\1\u0241\5\252\1\127\12\252"+
-    "\1\0\3\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\150\0\1\363\4\252\1\u0242\25\252\1\127"+
-    "\12\252\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\150\0\1\363\21\252\1\u0243\10\252"+
-    "\1\127\12\252\1\0\3\124\1\0\2\124\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\150\0\1\363\24\252\1\u0244"+
-    "\5\252\1\127\12\252\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\150\0\1\363\32\252"+
-    "\1\127\1\u0245\11\252\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\150\0\1\363\32\252"+
-    "\1\127\7\252\1\u0246\2\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\1\u0247\31\252\1\127\12\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\u0248"+
-    "\32\u01f6\1\u0249\12\u01f6\11\0\1\u01f7\163\0\51\u01f7\1\u024a"+
-    "\3\0\3\u01f7\1\u015d\3\0\1\u01f7\157\0\4\u024b\2\0"+
-    "\1\u024b\15\0\1\u024b\6\0\12\u024b\1\u024c\174\0\1\u01ba"+
-    "\32\u01bb\1\u01ba\12\u01bb\1\u01bc\2\u01ba\1\u01bd\3\u01ba\1\u01be"+
-    "\5\0\2\u01ba\4\0\1\u01ba\151\0\1\u01ba\32\u01bb\1\u01fa"+
-    "\12\u01bb\1\u01bc\2\u01ba\1\u01bd\3\u01ba\1\u01be\5\0\2\u01ba"+
-    "\4\0\1\u01ba\151\0\34\u01bc\12\u024d\1\0\2\u01bc\1\u01fd"+
-    "\3\u01bc\1\u01be\5\0\2\u01bc\4\0\1\u01bc\151\0\51\u01fc"+
-    "\1\u024e\3\0\3\u01fc\1\u015d\2\0\1\u024f\1\u01fc\157\0"+
-    "\4\u0250\2\0\1\u0250\15\0\1\u0250\6\0\12\u0250\176\0"+
-    "\4\u01ba\2\0\1\u01ba\15\0\1\u01ba\6\0\12\u01ba\175\0"+
-    "\1\u0251\32\u01ff\1\u0252\12\u01ff\1\u0253\10\0\1\u01fc\164\0"+
-    "\4\u0254\2\0\1\u0254\15\0\1\u0254\6\0\12\u0254\1\u0255"+
-    "\242\0\1\u0256\174\0\46\u0202\1\u0204\2\u0202\1\u0205\3\u0202"+
-    "\1\u0206\5\0\2\u0202\4\0\1\u0202\151\0\1\u0257\32\u0203"+
-    "\1\u0258\12\u0203\1\u0259\2\u0202\1\u0205\3\u0202\1\u0206\1\u015b"+
-    "\1\u015c\1\u015d\2\0\2\u0202\4\0\1\u0202\151\0\46\u0204"+
-    "\1\0\2\u0204\1\u025a\3\u0204\1\u0206\5\0\2\u0204\4\0"+
-    "\1\u0204\152\0\4\u025b\2\0\1\u025b\15\0\1\u025b\6\0"+
-    "\12\u025b\176\0\32\u025c\1\0\12\u025c\13\0\1\u0207\13\0"+
-    "\1\47\1\0\1\50\1\0\1\51\1\0\1\52\1\0"+
-    "\1\53\1\0\1\67\3\0\1\70\5\0\1\71\3\0"+
-    "\1\72\11\0\1\60\2\0\1\73\16\0\1\74\2\0"+
-    "\1\75\41\0\1\25\2\26\2\0\2\76\1\77\1\0"+
-    "\1\26\2\0\1\25\1\u010e\32\36\1\127\12\316\1\u0143"+
-    "\1\124\1\137\1\124\1\0\2\140\1\125\1\u0133\1\u0134"+
-    "\1\u0135\2\0\1\76\1\124\4\0\2\124\2\0\1\47"+
-    "\1\0\1\50\1\0\1\51\1\0\1\52\1\0\1\53"+
-    "\1\0\1\67\3\0\1\70\5\0\1\71\3\0\1\72"+
-    "\11\0\1\60\2\0\1\73\16\0\1\74\2\0\1\75"+
-    "\41\0\1\25\2\26\2\0\2\76\1\77\1\0\1\26"+
-    "\2\0\1\25\1\u010e\32\36\1\127\2\u0208\1\316\2\u0208"+
-    "\2\316\1\u0208\1\316\1\u0208\1\u0143\1\124\1\137\1\124"+
-    "\1\0\2\140\1\125\1\u0133\1\u0134\1\u0135\2\0\1\76"+
-    "\1\124\4\0\2\124\151\0\4\u025d\2\0\1\u025d\15\0"+
-    "\1\u025d\6\0\12\u025d\1\u01c7\175\0\4\u025e\2\0\1\u025e"+
-    "\15\0\1\u025e\6\0\12\u025e\1\u025f\175\0\4\u0260\2\0"+
-    "\1\u0260\15\0\1\u0260\6\0\1\u0261\2\u0262\1\u0261\5\u0262"+
-    "\1\u0263\14\0\1\u0116\161\0\4\u0264\2\0\1\u0264\15\0"+
-    "\1\u0264\6\0\12\u0264\1\u020e\13\0\1\u0116\161\0\4\u0260"+
-    "\2\0\1\u0260\15\0\1\u0260\6\0\1\u0261\2\u0262\1\u0261"+
-    "\5\u0262\1\u0263\175\0\1\u0157\4\u0264\2\0\1\u0264\15\0"+
-    "\1\u0264\6\0\12\u0265\1\u020e\13\0\1\u0116\160\0\1\u0157"+
-    "\4\u0264\2\0\1\u0264\15\0\1\u0264\6\0\12\u0264\1\u020e"+
-    "\13\0\1\u0116\160\0\1\u0157\4\u0264\2\0\1\u0264\15\0"+
-    "\1\u0264\6\0\2\u0265\1\u0264\2\u0265\2\u0264\1\u0265\1\u0264"+
-    "\1\u0265\1\u020e\13\0\1\u0116\226\0\1\u018a\13\0\1\u0116"+
-    "\160\0\1\u0266\33\0\12\u0214\175\0\1\u0266\33\0\12\u0267"+
-    "\175\0\1\u0266\33\0\1\u0214\1\u0268\1\u0267\2\u0214\2\u0267"+
-    "\1\u0214\1\u0267\1\u0214\175\0\1\334\12\216\1\u015e\17\216"+
-    "\1\335\12\216\175\0\1\334\11\216\1\u0269\20\216\1\335"+
-    "\12\216\175\0\1\334\3\216\1\u026a\26\216\1\335\12\216"+
-    "\175\0\1\334\7\216\1\u026b\22\216\1\335\4\216\1\u026c"+
-    "\5\216\175\0\1\334\10\216\1\u026d\4\216\1\u026e\5\216"+
-    "\1\u026f\6\216\1\335\12\216\175\0\1\334\3\216\1\u0270"+
-    "\26\216\1\335\2\216\1\u0271\7\216\175\0\1\334\7\216"+
-    "\1\u0272\22\216\1\335\12\216\175\0\1\334\7\216\1\u0273"+
-    "\22\216\1\335\3\216\1\u0274\6\216\175\0\1\334\32\216"+
-    "\1\335\5\216\1\u0275\4\216\175\0\1\334\7\216\1\u0276"+
-    "\22\216\1\335\12\216\175\0\1\334\31\216\1\u0277\1\335"+
-    "\12\216\175\0\1\334\1\216\1\u0278\30\216\1\335\12\216"+
-    "\175\0\1\334\7\216\1\u0279\1\216\1\u027a\20\216\1\335"+
-    "\11\216\1\u0275\175\0\1\334\22\216\1\u027b\7\216\1\335"+
-    "\2\216\1\u027c\7\216\175\0\1\334\6\216\1\u027d\1\u027e"+
-    "\22\216\1\335\12\216\175\0\1\334\7\216\1\u027f\5\216"+
-    "\1\u0280\14\216\1\335\12\216\175\0\1\334\23\216\1\u0281"+
-    "\6\216\1\335\12\216\175\0\1\334\32\216\1\335\3\216"+
-    "\1\u0282\6\216\175\0\1\334\3\216\1\u0283\26\216\1\335"+
-    "\12\216\175\0\1\334\17\216\1\u0284\12\216\1\335\12\216"+
-    "\175\0\1\334\32\216\1\335\1\216\1\u0275\10\216\175\0"+
-    "\1\334\32\216\1\335\1\u0285\11\216\231\0\12\u0286\10\0"+
-    "\1\u015b\1\u015c\1\u015d\162\0\1\363\25\252\1\u0287\4\252"+
-    "\1\127\12\252\1\0\3\124\1\0\2\124\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\150\0\1\363\1\u0288\31\252"+
-    "\1\127\12\252\1\0\3\124\1\0\2\124\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\150\0\1\363\15\252\1\u0289"+
-    "\14\252\1\127\12\252\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\150\0\1\363\21\252"+
-    "\1\u028a\10\252\1\127\12\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\16\252\1\u028b\4\252\1\u028c\6\252\1\127\12\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\363\4\252\1\u028d\25\252\1\127\12\252"+
-    "\1\0\3\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\150\0\1\363\32\252\1\127\11\252\1\u028e"+
-    "\1\0\3\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\150\0\1\363\4\252\1\u028f\25\252\1\127"+
-    "\12\252\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\150\0\1\363\32\252\1\127\11\252"+
-    "\1\u0290\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\150\0\1\363\1\u0291\2\252\1\u0292"+
-    "\20\252\1\u0293\5\252\1\127\12\252\1\0\3\124\1\0"+
-    "\2\124\1\125\3\124\3\0\1\124\4\0\2\124\150\0"+
-    "\1\363\16\252\1\u0294\13\252\1\127\12\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\11\252\1\u0295\13\252\1\u0296\4\252\1\127"+
-    "\12\252\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\150\0\1\363\32\252\1\127\11\252"+
-    "\1\u0297\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\150\0\1\363\23\252\1\u0298\6\252"+
-    "\1\127\12\252\1\0\3\124\1\0\2\124\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\150\0\1\363\31\252\1\u0299"+
-    "\1\127\12\252\1\0\3\124\1\0\2\124\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\150\0\1\363\26\252\1\u029a"+
-    "\3\252\1\127\12\252\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\150\0\1\363\11\252"+
-    "\1\u029b\20\252\1\127\12\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\32\252\1\127\3\252\1\u029c\6\252\1\0\3\124\1\0"+
-    "\2\124\1\125\3\124\3\0\1\124\4\0\2\124\150\0"+
-    "\1\363\5\252\1\u029d\24\252\1\127\12\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\10\252\1\u029e\21\252\1\127\12\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\363\3\252\1\u029f\26\252\1\127\12\252"+
-    "\1\0\3\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\150\0\1\363\21\252\1\u02a0\6\252\1\u02a1"+
-    "\1\252\1\127\12\252\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\150\0\1\363\12\252"+
-    "\1\u02a2\17\252\1\127\12\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\32\252\1\127\1\252\1\u02a3\10\252\1\0\3\124\1\0"+
-    "\2\124\1\125\3\124\3\0\1\124\4\0\2\124\150\0"+
-    "\1\363\24\252\1\u02a4\5\252\1\127\12\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\24\252\1\u02a5\5\252\1\127\12\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\363\31\252\1\u02a6\1\127\12\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\151\0\32\u01f6\1\0\12\u01f6\176\0\32\u01f6\1\u0249"+
-    "\12\u01f6\176\0\4\u02a7\2\0\1\u02a7\15\0\1\u02a7\6\0"+
-    "\12\u02a7\176\0\4\u02a8\2\0\1\u02a8\15\0\1\u02a8\6\0"+
-    "\12\u02a8\1\u02a9\242\0\1\u02aa\174\0\34\u01bc\12\u02ab\1\0"+
-    "\2\u01bc\1\u01fd\3\u01bc\1\u01be\1\0\1\u01fc\3\0\2\u01bc"+
-    "\4\0\1\u01bc\152\0\4\u02ac\2\0\1\u02ac\15\0\1\u02ac"+
-    "\6\0\12\u02ac\215\0\1\u02ad\223\0\4\u01bc\2\0\1\u01bc"+
-    "\15\0\1\u01bc\6\0\12\u01bc\176\0\32\u01ff\1\0\12\u01ff"+
-    "\176\0\32\u01ff\1\u0252\12\u01ff\231\0\12\u02ae\176\0\4\u02af"+
-    "\2\0\1\u02af\15\0\1\u02af\6\0\12\u02af\1\u0255\175\0"+
-    "\4\u02b0\2\0\1\u02b0\15\0\1\u02b0\6\0\12\u02b0\1\u02b1"+
-    "\175\0\4\u02b2\2\0\1\u02b2\15\0\1\u02b2\6\0\1\u02b3"+
-    "\2\u02b4\1\u02b3\5\u02b4\1\u02b5\14\0\1\u02b6\160\0\1\u0202"+
-    "\32\u0203\1\u0202\12\u0203\1\u0204\2\u0202\1\u0205\3\u0202\1\u0206"+
-    "\5\0\2\u0202\4\0\1\u0202\151\0\1\u0202\32\u0203\1\u0258"+
-    "\12\u0203\1\u0204\2\u0202\1\u0205\3\u0202\1\u0206\5\0\2\u0202"+
-    "\4\0\1\u0202\151\0\34\u0204\12\u02b7\1\0\2\u0204\1\u025a"+
-    "\3\u0204\1\u0206\5\0\2\u0204\4\0\1\u0204\152\0\4\u02b8"+
-    "\2\0\1\u02b8\15\0\1\u02b8\6\0\12\u02b8\176\0\4\u0202"+
-    "\2\0\1\u0202\15\0\1\u0202\6\0\12\u0202\175\0\1\u02b9"+
-    "\32\u025c\1\u02ba\12\u025c\1\u0143\7\0\1\u015b\1\u015c\1\u015d"+
-    "\230\0\1\u01c7\175\0\4\u02bb\2\0\1\u02bb\15\0\1\u02bb"+
-    "\6\0\12\u02bb\1\u025f\175\0\4\u02bc\2\0\1\u02bc\15\0"+
-    "\1\u02bc\6\0\12\u02bc\1\u02bd\175\0\4\u02be\2\0\1\u02be"+
-    "\15\0\1\u02be\6\0\12\u02be\1\u02bf\13\0\1\u0116\160\0"+
-    "\1\u0157\4\u02be\2\0\1\u02be\15\0\1\u02be\6\0\12\u02c0"+
-    "\1\u02bf\13\0\1\u0116\160\0\1\u0157\4\u02be\2\0\1\u02be"+
-    "\15\0\1\u02be\6\0\12\u02c1\1\u02bf\13\0\1\u0116\160\0"+
-    "\1\u0157\4\u02be\2\0\1\u02be\15\0\1\u02be\6\0\1\u02c0"+
-    "\1\u02c2\1\u02c1\2\u02c0\2\u02c1\1\u02c0\1\u02c1\1\u02c0\1\u02bf"+
-    "\13\0\1\u0116\161\0\4\u02c3\2\0\1\u02c3\15\0\1\u02c3"+
-    "\6\0\12\u02c3\1\u020e\13\0\1\u0116\160\0\1\u0157\4\u02c3"+
-    "\2\0\1\u02c3\15\0\1\u02c3\6\0\12\u02c3\1\u020e\13\0"+
-    "\1\u0116\214\0\1\u02c4\2\u02c5\1\u02c4\5\u02c5\1\u02c6\175\0"+
-    "\1\u0266\242\0\1\u0266\33\0\2\u0267\1\0\2\u0267\2\0"+
-    "\1\u0267\1\0\1\u0267\175\0\1\334\1\216\1\u02c7\30\216"+
-    "\1\335\12\216\175\0\1\334\24\216\1\u02c8\5\216\1\335"+
-    "\12\216\175\0\1\334\24\216\1\u02c9\5\216\1\335\12\216"+
-    "\175\0\1\334\1\216\1\u02ca\30\216\1\335\12\216\175\0"+
-    "\1\334\14\216\1\u02cb\15\216\1\335\12\216\175\0\1\334"+
-    "\1\216\1\u02cc\30\216\1\335\12\216\175\0\1\334\1\216"+
-    "\1\u02cd\30\216\1\335\12\216\175\0\1\334\1\216\1\u02ce"+
-    "\30\216\1\335\12\216\175\0\1\334\21\216\1\u02cf\10\216"+
-    "\1\335\12\216\175\0\1\334\24\216\1\u02d0\5\216\1\335"+
-    "\12\216\175\0\1\334\24\216\1\u02d1\5\216\1\335\12\216"+
-    "\175\0\1\334\1\u0199\31\216\1\335\12\216\175\0\1\334"+
-    "\24\216\1\u02ce\5\216\1\335\12\216\175\0\1\334\24\216"+
-    "\1\u02d2\5\216\1\335\12\216\175\0\1\334\1\216\1\u02d3"+
-    "\30\216\1\335\12\216\175\0\1\334\31\216\1\u02d4\1\335"+
-    "\12\216\175\0\1\334\24\216\1\u02d5\5\216\1\335\12\216"+
-    "\175\0\1\334\1\216\1\u02d6\30\216\1\335\12\216\175\0"+
-    "\1\334\1\u02d7\31\216\1\335\12\216\175\0\1\334\21\216"+
-    "\1\u02d8\10\216\1\335\12\216\175\0\1\334\4\216\1\u02d9"+
-    "\25\216\1\335\12\216\175\0\1\334\24\216\1\u02da\5\216"+
-    "\1\335\12\216\175\0\1\334\24\216\1\u02db\5\216\1\335"+
-    "\12\216\175\0\1\334\4\216\1\u02dc\25\216\1\335\12\216"+
-    "\175\0\1\334\21\216\1\u02dd\10\216\1\335\12\216\175\0"+
-    "\1\334\24\216\1\u02de\5\216\1\335\12\216\175\0\1\334"+
-    "\32\216\1\335\1\u02df\11\216\175\0\1\334\32\216\1\335"+
-    "\7\216\1\u02e0\2\216\175\0\1\334\1\u02e1\31\216\1\335"+
-    "\12\216\253\0\1\u015b\1\u015c\1\u015d\162\0\1\363\1\252"+
-    "\1\u02e2\30\252\1\127\12\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\32\252\1\127\1\u02e3\11\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\6\252\1\u02e4\23\252\1\127\12\252\1\0\3\124\1\0"+
-    "\2\124\1\125\3\124\3\0\1\124\4\0\2\124\150\0"+
-    "\1\363\32\252\1\127\7\252\1\u02e5\2\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\32\252\1\127\10\252\1\u014a\1\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\363\32\252\1\127\5\252\1\u014a\4\252"+
-    "\1\0\3\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\150\0\1\363\26\252\1\u02e6\3\252\1\127"+
-    "\12\252\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\150\0\1\363\1\252\1\u02e7\30\252"+
-    "\1\127\12\252\1\0\3\124\1\0\2\124\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\150\0\1\363\26\252\1\u02e8"+
-    "\3\252\1\127\12\252\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\150\0\1\363\32\252"+
-    "\1\127\1\252\1\u02e9\10\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\1\u02ea\27\252\1\u02eb\1\252\1\127\12\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\4\252\1\u02ec\25\252\1\127\12\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\363\25\252\1\u02ed\4\252\1\127\12\252"+
-    "\1\0\3\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\150\0\1\363\32\252\1\127\1\u02ee\11\252"+
-    "\1\0\3\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\150\0\1\363\32\252\1\127\2\252\1\275"+
-    "\7\252\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\150\0\1\363\32\252\1\127\3\252"+
-    "\1\u02ef\6\252\1\0\3\124\1\0\2\124\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\150\0\1\363\1\u02f0\1\252"+
-    "\1\u02f1\27\252\1\127\12\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\1\u02e5\31\252\1\127\12\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\32\252\1\127\2\252\1\u02f2\7\252\1\0\3\124\1\0"+
-    "\2\124\1\125\3\124\3\0\1\124\4\0\2\124\150\0"+
-    "\1\363\32\252\1\127\2\252\1\u02f3\7\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\15\252\1\u02f4\14\252\1\127\12\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\363\32\252\1\127\5\252\1\u02f5\4\252"+
-    "\1\0\3\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\150\0\1\363\32\252\1\127\7\252\1\u02f6"+
-    "\2\252\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\150\0\1\363\32\252\1\127\11\252"+
-    "\1\u02f7\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\150\0\1\363\1\252\1\u02f8\30\252"+
-    "\1\127\12\252\1\0\3\124\1\0\2\124\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\150\0\1\363\32\252\1\127"+
-    "\3\252\1\u02f9\6\252\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\150\0\1\363\32\252"+
-    "\1\127\1\252\1\u02fa\10\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\32\252\1\127\1\252\1\u02fb\10\252\1\0\3\124\1\0"+
-    "\2\124\1\125\3\124\3\0\1\124\4\0\2\124\150\0"+
-    "\1\363\24\252\1\u02fc\5\252\1\127\12\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\32\252\1\127\6\252\1\u02fd\3\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\363\32\252\1\127\3\252\1\u02fe\6\252"+
-    "\1\0\3\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\150\0\1\363\25\252\1\u02ff\4\252\1\127"+
-    "\12\252\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\151\0\4\u01f7\2\0\1\u01f7\15\0"+
-    "\1\u01f7\6\0\12\u01f7\176\0\4\u0300\2\0\1\u0300\15\0"+
-    "\1\u0300\6\0\12\u0300\1\u02a9\175\0\4\u0301\2\0\1\u0301"+
-    "\15\0\1\u0301\6\0\12\u0301\1\u0302\175\0\4\u0303\2\0"+
-    "\1\u0303\15\0\1\u0303\6\0\1\u0304\2\u0305\1\u0304\5\u0305"+
-    "\1\u0306\14\0\1\u0307\160\0\34\u01bc\12\u0308\1\0\2\u01bc"+
-    "\1\u01fd\3\u01bc\1\u01be\1\0\1\u01fc\3\0\2\u01bc\4\0"+
-    "\1\u01bc\152\0\4\u01fc\2\0\1\u01fc\15\0\1\u01fc\6\0"+
-    "\12\u01fc\226\0\1\u0309\245\0\12\u030a\11\0\1\u01fc\164\0"+
-    "\4\u030b\2\0\1\u030b\15\0\1\u030b\6\0\12\u030b\1\u0255"+
-    "\175\0\4\u030c\2\0\1\u030c\15\0\1\u030c\6\0\12\u030c"+
-    "\1\u030d\175\0\4\u030e\2\0\1\u030e\15\0\1\u030e\6\0"+
-    "\1\u030f\2\u0310\1\u030f\5\u0310\1\u0311\14\0\1\u02b6\161\0"+
-    "\4\u0312\2\0\1\u0312\15\0\1\u0312\6\0\12\u0312\1\u0313"+
-    "\13\0\1\u02b6\160\0\1\u0314\4\u0312\2\0\1\u0312\15\0"+
-    "\1\u0312\6\0\12\u0315\1\u0313\13\0\1\u02b6\160\0\1\u0314"+
-    "\4\u0312\2\0\1\u0312\15\0\1\u0312\6\0\12\u0316\1\u0313"+
-    "\13\0\1\u02b6\160\0\1\u0314\4\u0312\2\0\1\u0312\15\0"+
-    "\1\u0312\6\0\1\u0315\1\u0317\1\u0316\2\u0315\2\u0316\1\u0315"+
-    "\1\u0316\1\u0315\1\u0313\13\0\1\u02b6\226\0\1\u0253\10\0"+
-    "\1\u01fc\163\0\34\u0204\12\u0318\1\0\2\u0204\1\u025a\3\u0204"+
-    "\1\u0206\1\u015b\1\u015c\1\u015d\2\0\2\u0204\4\0\1\u0204"+
-    "\152\0\4\u0204\2\0\1\u0204\15\0\1\u0204\6\0\12\u0204"+
-    "\176\0\32\u025c\1\0\12\u025c\176\0\32\u025c\1\u02ba\12\u025c"+
-    "\176\0\4\u0319\2\0\1\u0319\15\0\1\u0319\6\0\12\u0319"+
-    "\1\u025f\175\0\4\u031a\2\0\1\u031a\15\0\1\u031a\6\0"+
-    "\12\u031a\1\u031b\175\0\4\u031c\2\0\1\u031c\15\0\1\u031c"+
-    "\6\0\1\u031d\2\u031e\1\u031d\5\u031e\1\u031f\14\0\1\u0116"+
-    "\161\0\4\u0320\2\0\1\u0320\15\0\1\u0320\6\0\12\u0320"+
-    "\1\u02bf\13\0\1\u0116\161\0\4\u031c\2\0\1\u031c\15\0"+
-    "\1\u031c\6\0\1\u031d\2\u031e\1\u031d\5\u031e\1\u031f\175\0"+
-    "\1\u0157\4\u0320\2\0\1\u0320\15\0\1\u0320\6\0\12\u0321"+
-    "\1\u02bf\13\0\1\u0116\160\0\1\u0157\4\u0320\2\0\1\u0320"+
-    "\15\0\1\u0320\6\0\12\u0320\1\u02bf\13\0\1\u0116\160\0"+
-    "\1\u0157\4\u0320\2\0\1\u0320\15\0\1\u0320\6\0\2\u0321"+
-    "\1\u0320\2\u0321\2\u0320\1\u0321\1\u0320\1\u0321\1\u02bf\13\0"+
-    "\1\u0116\226\0\1\u020e\13\0\1\u0116\214\0\12\u02c5\14\0"+
-    "\1\u0116\214\0\12\u0322\14\0\1\u0116\214\0\1\u02c5\1\u0323"+
-    "\1\u0322\2\u02c5\2\u0322\1\u02c5\1\u0322\1\u02c5\14\0\1\u0116"+
-    "\160\0\1\334\25\216\1\u0324\4\216\1\335\12\216\175\0"+
-    "\1\334\1\u0325\31\216\1\335\12\216\175\0\1\334\15\216"+
-    "\1\u0326\14\216\1\335\12\216\175\0\1\334\21\216\1\u0327"+
-    "\10\216\1\335\12\216\175\0\1\334\16\216\1\u0328\4\216"+
-    "\1\u0329\6\216\1\335\12\216\175\0\1\334\4\216\1\u032a"+
-    "\25\216\1\335\12\216\175\0\1\334\32\216\1\335\11\216"+
-    "\1\u032b\175\0\1\334\4\216\1\u032c\25\216\1\335\12\216"+
-    "\175\0\1\334\32\216\1\335\11\216\1\u032d\175\0\1\334"+
-    "\1\u032e\2\216\1\u032f\20\216\1\u0330\5\216\1\335\12\216"+
-    "\175\0\1\334\16\216\1\u0331\13\216\1\335\12\216\175\0"+
-    "\1\334\11\216\1\u0332\13\216\1\u0333\4\216\1\335\12\216"+
-    "\175\0\1\334\32\216\1\335\11\216\1\u0334\175\0\1\334"+
-    "\23\216\1\u0335\6\216\1\335\12\216\175\0\1\334\31\216"+
-    "\1\u0336\1\335\12\216\175\0\1\334\26\216\1\u0337\3\216"+
-    "\1\335\12\216\175\0\1\334\11\216\1\u0338\20\216\1\335"+
-    "\12\216\175\0\1\334\32\216\1\335\3\216\1\u0339\6\216"+
-    "\175\0\1\334\5\216\1\u033a\24\216\1\335\12\216\175\0"+
-    "\1\334\10\216\1\u033b\21\216\1\335\12\216\175\0\1\334"+
-    "\3\216\1\u033c\26\216\1\335\12\216\175\0\1\334\21\216"+
-    "\1\u033d\6\216\1\u033e\1\216\1\335\12\216\175\0\1\334"+
-    "\12\216\1\u033f\17\216\1\335\12\216\175\0\1\334\32\216"+
-    "\1\335\1\216\1\u0340\10\216\175\0\1\334\24\216\1\u0341"+
-    "\5\216\1\335\12\216\175\0\1\334\24\216\1\u0342\5\216"+
-    "\1\335\12\216\175\0\1\334\31\216\1\u0343\1\335\12\216"+
-    "\175\0\1\363\32\252\1\127\1\u0344\11\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\1\u0345\31\252\1\127\12\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\32\252\1\127\10\252\1\u0346\1\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\363\25\252\1\370\4\252\1\127\12\252"+
-    "\1\0\3\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\150\0\1\363\32\252\1\127\5\252\1\u0347"+
-    "\4\252\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\150\0\1\363\32\252\1\127\5\252"+
-    "\1\u0348\4\252\1\0\3\124\1\0\2\124\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\150\0\1\363\32\252\1\127"+
-    "\5\252\1\u02ef\4\252\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\150\0\1\363\32\252"+
-    "\1\127\3\252\1\u0345\6\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\12\252\1\u0349\17\252\1\127\12\252\1\0\3\124\1\0"+
-    "\2\124\1\125\3\124\3\0\1\124\4\0\2\124\150\0"+
-    "\1\363\25\252\1\u034a\4\252\1\127\12\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\15\252\1\u034b\14\252\1\127\12\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\363\32\252\1\127\3\252\1\u034c\6\252"+
-    "\1\0\3\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\150\0\1\363\2\252\1\u02e5\27\252\1\127"+
-    "\12\252\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\150\0\1\363\1\252\1\370\30\252"+
-    "\1\127\12\252\1\0\3\124\1\0\2\124\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\150\0\1\363\11\252\1\u034d"+
-    "\20\252\1\127\12\252\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\150\0\1\363\11\252"+
-    "\1\u034e\20\252\1\127\12\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\1\u034f\31\252\1\127\12\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\1\u0350\31\252\1\127\12\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\2\252\1\u0351\27\252\1\127\12\252\1\0\3\124\1\0"+
-    "\2\124\1\125\3\124\3\0\1\124\4\0\2\124\150\0"+
-    "\1\363\32\252\1\127\4\252\1\377\5\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\10\252\1\u0352\21\252\1\127\12\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\363\1\u0353\31\252\1\127\12\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\363\25\252\1\u0354\4\252\1\127\12\252"+
-    "\1\0\3\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\150\0\1\363\32\252\1\127\4\252\1\u0345"+
-    "\5\252\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\150\0\1\363\32\252\1\127\6\252"+
-    "\1\u0345\3\252\1\0\3\124\1\0\2\124\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\150\0\1\363\32\252\1\127"+
-    "\2\252\1\u0345\7\252\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\150\0\1\363\16\252"+
-    "\1\u0355\13\252\1\127\12\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\32\252\1\127\1\u0356\11\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\32\252\1\127\3\252\1\u0357\6\252\1\0\3\124\1\0"+
-    "\2\124\1\125\3\124\3\0\1\124\4\0\2\124\150\0"+
-    "\1\363\24\252\1\u0358\5\252\1\127\12\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\151\0\4\u0359\2\0\1\u0359\15\0\1\u0359\6\0\12\u0359"+
-    "\1\u02a9\175\0\4\u035a\2\0\1\u035a\15\0\1\u035a\6\0"+
-    "\12\u035a\1\u035b\175\0\4\u035c\2\0\1\u035c\15\0\1\u035c"+
-    "\6\0\1\u035d\2\u035e\1\u035d\5\u035e\1\u035f\14\0\1\u0307"+
-    "\161\0\4\u0360\2\0\1\u0360\15\0\1\u0360\6\0\12\u0360"+
-    "\1\u0361\13\0\1\u0307\160\0\1\u0362\4\u0360\2\0\1\u0360"+
-    "\15\0\1\u0360\6\0\12\u0363\1\u0361\13\0\1\u0307\160\0"+
-    "\1\u0362\4\u0360\2\0\1\u0360\15\0\1\u0360\6\0\12\u0364"+
-    "\1\u0361\13\0\1\u0307\160\0\1\u0362\4\u0360\2\0\1\u0360"+
-    "\15\0\1\u0360\6\0\1\u0363\1\u0365\1\u0364\2\u0363\2\u0364"+
-    "\1\u0363\1\u0364\1\u0363\1\u0361\13\0\1\u0307\237\0\1\u01f7"+
-    "\163\0\34\u01bc\12\u0366\1\0\2\u01bc\1\u01fd\3\u01bc\1\u01be"+
-    "\1\0\1\u01fc\3\0\2\u01bc\4\0\1\u01bc\167\0\1\u0367"+
-    "\260\0\12\u0368\11\0\1\u01fc\231\0\1\u0255\175\0\4\u0369"+
-    "\2\0\1\u0369\15\0\1\u0369\6\0\12\u0369\1\u030d\175\0"+
-    "\4\u036a\2\0\1\u036a\15\0\1\u036a\6\0\12\u036a\1\u036b"+
-    "\175\0\4\u036c\2\0\1\u036c\15\0\1\u036c\6\0\12\u036c"+
-    "\1\u036d\13\0\1\u02b6\160\0\1\u0314\4\u036c\2\0\1\u036c"+
-    "\15\0\1\u036c\6\0\12\u036e\1\u036d\13\0\1\u02b6\160\0"+
-    "\1\u0314\4\u036c\2\0\1\u036c\15\0\1\u036c\6\0\12\u036f"+
-    "\1\u036d\13\0\1\u02b6\160\0\1\u0314\4\u036c\2\0\1\u036c"+
-    "\15\0\1\u036c\6\0\1\u036e\1\u0370\1\u036f\2\u036e\2\u036f"+
-    "\1\u036e\1\u036f\1\u036e\1\u036d\13\0\1\u02b6\161\0\4\u0371"+
-    "\2\0\1\u0371\15\0\1\u0371\6\0\12\u0371\1\u0313\13\0"+
-    "\1\u02b6\161\0\4\u030e\2\0\1\u030e\15\0\1\u030e\6\0"+
-    "\1\u030f\2\u0310\1\u030f\5\u0310\1\u0311\231\0\1\u0372\2\u0373"+
-    "\1\u0372\5\u0373\1\u0374\175\0\1\u0314\4\u0371\2\0\1\u0371"+
-    "\15\0\1\u0371\6\0\12\u0375\1\u0313\13\0\1\u02b6\160\0"+
-    "\1\u0314\4\u0371\2\0\1\u0371\15\0\1\u0371\6\0\12\u0371"+
-    "\1\u0313\13\0\1\u02b6\160\0\1\u0314\4\u0371\2\0\1\u0371"+
-    "\15\0\1\u0371\6\0\2\u0375\1\u0371\2\u0375\2\u0371\1\u0375"+
-    "\1\u0371\1\u0375\1\u0313\13\0\1\u02b6\160\0\34\u0204\12\u0376"+
-    "\1\0\2\u0204\1\u025a\3\u0204\1\u0206\1\u015b\1\u015c\1\u015d"+
-    "\2\0\2\u0204\4\0\1\u0204\217\0\1\u025f\175\0\4\u0377"+
-    "\2\0\1\u0377\15\0\1\u0377\6\0\12\u0377\1\u031b\175\0"+
-    "\4\u0378\2\0\1\u0378\15\0\1\u0378\6\0\12\u0378\1\u0379"+
-    "\175\0\4\u037a\2\0\1\u037a\15\0\1\u037a\6\0\12\u037a"+
-    "\1\u037b\13\0\1\u0116\160\0\1\u0157\4\u037a\2\0\1\u037a"+
-    "\15\0\1\u037a\6\0\12\u037c\1\u037b\13\0\1\u0116\160\0"+
-    "\1\u0157\4\u037a\2\0\1\u037a\15\0\1\u037a\6\0\12\u037d"+
-    "\1\u037b\13\0\1\u0116\160\0\1\u0157\4\u037a\2\0\1\u037a"+
-    "\15\0\1\u037a\6\0\1\u037c\1\u037e\1\u037d\2\u037c\2\u037d"+
-    "\1\u037c\1\u037d\1\u037c\1\u037b\13\0\1\u0116\161\0\4\u037f"+
-    "\2\0\1\u037f\15\0\1\u037f\6\0\12\u037f\1\u02bf\13\0"+
-    "\1\u0116\160\0\1\u0157\4\u037f\2\0\1\u037f\15\0\1\u037f"+
-    "\6\0\12\u037f\1\u02bf\13\0\1\u0116\242\0\1\u0116\214\0"+
-    "\2\u0322\1\0\2\u0322\2\0\1\u0322\1\0\1\u0322\14\0"+
-    "\1\u0116\160\0\1\334\1\216\1\u0380\30\216\1\335\12\216"+
-    "\175\0\1\334\32\216\1\335\1\u0381\11\216\175\0\1\334"+
-    "\6\216\1\u0382\23\216\1\335\12\216\175\0\1\334\32\216"+
-    "\1\335\7\216\1\u0383\2\216\175\0\1\334\32\216\1\335"+
-    "\10\216\1\u019e\1\216\175\0\1\334\32\216\1\335\5\216"+
-    "\1\u019e\4\216\175\0\1\334\26\216\1\u0384\3\216\1\335"+
-    "\12\216\175\0\1\334\1\216\1\u0385\30\216\1\335\12\216"+
-    "\175\0\1\334\26\216\1\u0386\3\216\1\335\12\216\175\0"+
-    "\1\334\32\216\1\335\1\216\1\u0387\10\216\175\0\1\334"+
-    "\1\u0388\27\216\1\u0389\1\216\1\335\12\216\175\0\1\334"+
-    "\4\216\1\u038a\25\216\1\335\12\216\175\0\1\334\25\216"+
-    "\1\u038b\4\216\1\335\12\216\175\0\1\334\32\216\1\335"+
-    "\1\u038c\11\216\175\0\1\334\32\216\1\335\2\216\1\u0123"+
-    "\7\216\175\0\1\334\32\216\1\335\3\216\1\u038d\6\216"+
-    "\175\0\1\334\1\u038e\1\216\1\u038f\27\216\1\335\12\216"+
-    "\175\0\1\334\1\u0383\31\216\1\335\12\216\175\0\1\334"+
-    "\32\216\1\335\2\216\1\u0390\7\216\175\0\1\334\32\216"+
-    "\1\335\2\216\1\u0391\7\216\175\0\1\334\15\216\1\u0392"+
-    "\14\216\1\335\12\216\175\0\1\334\32\216\1\335\5\216"+
-    "\1\u0393\4\216\175\0\1\334\32\216\1\335\7\216\1\u0394"+
-    "\2\216\175\0\1\334\32\216\1\335\11\216\1\u0395\175\0"+
-    "\1\334\1\216\1\u0396\30\216\1\335\12\216\175\0\1\334"+
-    "\32\216\1\335\3\216\1\u0397\6\216\175\0\1\334\32\216"+
-    "\1\335\1\216\1\u0398\10\216\175\0\1\334\32\216\1\335"+
-    "\1\216\1\u0399\10\216\175\0\1\334\24\216\1\u039a\5\216"+
-    "\1\335\12\216\175\0\1\334\32\216\1\335\6\216\1\u039b"+
-    "\3\216\175\0\1\334\32\216\1\335\3\216\1\u039c\6\216"+
-    "\175\0\1\334\25\216\1\u039d\4\216\1\335\12\216\175\0"+
-    "\1\363\3\252\1\u039e\26\252\1\127\12\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\2\252\1\370\27\252\1\127\12\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\363\6\252\1\u0103\23\252\1\127\12\252"+
-    "\1\0\3\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\150\0\1\363\1\252\1\u02f9\30\252\1\127"+
-    "\12\252\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\150\0\1\363\3\252\1\u039f\26\252"+
-    "\1\127\12\252\1\0\3\124\1\0\2\124\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\150\0\1\363\32\252\1\127"+
-    "\6\252\1\u03a0\3\252\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\150\0\1\363\32\252"+
-    "\1\127\6\252\1\u03a1\3\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\32\252\1\127\7\252\1\u03a2\2\252\1\0\3\124\1\0"+
-    "\2\124\1\125\3\124\3\0\1\124\4\0\2\124\150\0"+
-    "\1\363\1\u03a3\31\252\1\127\12\252\1\0\3\124\1\0"+
-    "\2\124\1\125\3\124\3\0\1\124\4\0\2\124\150\0"+
-    "\1\363\32\252\1\127\4\252\1\u03a4\5\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\32\252\1\127\4\252\1\u03a5\5\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\363\26\252\1\u03a6\3\252\1\127\12\252"+
-    "\1\0\3\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\150\0\1\363\30\252\1\u03a7\1\252\1\127"+
-    "\12\252\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\150\0\1\363\11\252\1\u0145\20\252"+
-    "\1\127\12\252\1\0\3\124\1\0\2\124\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\150\0\1\363\32\252\1\127"+
-    "\2\252\1\u03a8\7\252\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\150\0\1\363\12\252"+
-    "\1\u03a9\17\252\1\127\12\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\17\252\1\u0100\12\252\1\127\12\252\1\0\3\124\1\0"+
-    "\2\124\1\125\3\124\3\0\1\124\4\0\2\124\150\0"+
-    "\1\363\32\252\1\127\4\252\1\u03aa\5\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\32\252\1\127\6\252\1\u0148\3\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\363\30\252\1\u03ab\1\252\1\127\12\252"+
-    "\1\0\3\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\150\0\1\363\30\252\1\u03ac\1\252\1\127"+
-    "\12\252\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\216\0\1\u02a9\175\0\4\u03ad\2\0"+
-    "\1\u03ad\15\0\1\u03ad\6\0\12\u03ad\1\u035b\175\0\4\u03ae"+
-    "\2\0\1\u03ae\15\0\1\u03ae\6\0\12\u03ae\1\u03af\175\0"+
-    "\4\u03b0\2\0\1\u03b0\15\0\1\u03b0\6\0\12\u03b0\1\u03b1"+
-    "\13\0\1\u0307\160\0\1\u0362\4\u03b0\2\0\1\u03b0\15\0"+
-    "\1\u03b0\6\0\12\u03b2\1\u03b1\13\0\1\u0307\160\0\1\u0362"+
-    "\4\u03b0\2\0\1\u03b0\15\0\1\u03b0\6\0\12\u03b3\1\u03b1"+
-    "\13\0\1\u0307\160\0\1\u0362\4\u03b0\2\0\1\u03b0\15\0"+
-    "\1\u03b0\6\0\1\u03b2\1\u03b4\1\u03b3\2\u03b2\2\u03b3\1\u03b2"+
-    "\1\u03b3\1\u03b2\1\u03b1\13\0\1\u0307\161\0\4\u03b5\2\0"+
-    "\1\u03b5\15\0\1\u03b5\6\0\12\u03b5\1\u0361\13\0\1\u0307"+
-    "\161\0\4\u035c\2\0\1\u035c\15\0\1\u035c\6\0\1\u035d"+
-    "\2\u035e\1\u035d\5\u035e\1\u035f\231\0\1\u03b6\2\u03b7\1\u03b6"+
-    "\5\u03b7\1\u03b8\175\0\1\u0362\4\u03b5\2\0\1\u03b5\15\0"+
-    "\1\u03b5\6\0\12\u03b9\1\u0361\13\0\1\u0307\160\0\1\u0362"+
-    "\4\u03b5\2\0\1\u03b5\15\0\1\u03b5\6\0\12\u03b5\1\u0361"+
-    "\13\0\1\u0307\160\0\1\u0362\4\u03b5\2\0\1\u03b5\15\0"+
-    "\1\u03b5\6\0\2\u03b9\1\u03b5\2\u03b9\2\u03b5\1\u03b9\1\u03b5"+
-    "\1\u03b9\1\u0361\13\0\1\u0307\160\0\34\u01bc\12\u03ba\1\0"+
-    "\2\u01bc\1\u01fd\3\u01bc\1\u01be\1\0\1\u01fc\3\0\2\u01bc"+
-    "\4\0\1\u01bc\155\0\1\u03bb\272\0\12\u03bc\11\0\1\u01fc"+
-    "\164\0\4\u03bd\2\0\1\u03bd\15\0\1\u03bd\6\0\12\u03bd"+
-    "\1\u030d\175\0\4\u03be\2\0\1\u03be\15\0\1\u03be\6\0"+
-    "\12\u03be\1\u03bf\175\0\4\u03c0\2\0\1\u03c0\15\0\1\u03c0"+
-    "\6\0\1\u03c1\2\u03c2\1\u03c1\5\u03c2\1\u03c3\14\0\1\u02b6"+
-    "\161\0\4\u03c4\2\0\1\u03c4\15\0\1\u03c4\6\0\12\u03c4"+
-    "\1\u036d\13\0\1\u02b6\161\0\4\u03c0\2\0\1\u03c0\15\0"+
-    "\1\u03c0\6\0\1\u03c1\2\u03c2\1\u03c1\5\u03c2\1\u03c3\175\0"+
-    "\1\u0314\4\u03c4\2\0\1\u03c4\15\0\1\u03c4\6\0\12\u03c5"+
-    "\1\u036d\13\0\1\u02b6\160\0\1\u0314\4\u03c4\2\0\1\u03c4"+
-    "\15\0\1\u03c4\6\0\12\u03c4\1\u036d\13\0\1\u02b6\160\0"+
-    "\1\u0314\4\u03c4\2\0\1\u03c4\15\0\1\u03c4\6\0\2\u03c5"+
-    "\1\u03c4\2\u03c5\2\u03c4\1\u03c5\1\u03c4\1\u03c5\1\u036d\13\0"+
-    "\1\u02b6\161\0\4\u03c6\2\0\1\u03c6\15\0\1\u03c6\6\0"+
-    "\12\u03c6\1\u0313\13\0\1\u02b6\160\0\1\u03c7\33\0\12\u0373"+
-    "\175\0\1\u03c7\33\0\12\u03c8\175\0\1\u03c7\33\0\1\u0373"+
-    "\1\u03c9\1\u03c8\2\u0373\2\u03c8\1\u0373\1\u03c8\1\u0373\175\0"+
-    "\1\u0314\4\u03c6\2\0\1\u03c6\15\0\1\u03c6\6\0\12\u03c6"+
-    "\1\u0313\13\0\1\u02b6\160\0\34\u0204\12\u03ca\1\0\2\u0204"+
-    "\1\u025a\3\u0204\1\u0206\1\u015b\1\u015c\1\u015d\2\0\2\u0204"+
-    "\4\0\1\u0204\152\0\4\u03cb\2\0\1\u03cb\15\0\1\u03cb"+
-    "\6\0\12\u03cb\1\u031b\175\0\4\u03cc\2\0\1\u03cc\15\0"+
-    "\1\u03cc\6\0\12\u03cc\1\u03cd\175\0\4\u03ce\2\0\1\u03ce"+
-    "\15\0\1\u03ce\6\0\1\u03cf\2\u03d0\1\u03cf\5\u03d0\1\u03d1"+
-    "\14\0\1\u0116\161\0\4\u03d2\2\0\1\u03d2\15\0\1\u03d2"+
-    "\6\0\12\u03d2\1\u037b\13\0\1\u0116\161\0\4\u03ce\2\0"+
-    "\1\u03ce\15\0\1\u03ce\6\0\1\u03cf\2\u03d0\1\u03cf\5\u03d0"+
-    "\1\u03d1\175\0\1\u0157\4\u03d2\2\0\1\u03d2\15\0\1\u03d2"+
-    "\6\0\12\u03d3\1\u037b\13\0\1\u0116\160\0\1\u0157\4\u03d2"+
-    "\2\0\1\u03d2\15\0\1\u03d2\6\0\12\u03d2\1\u037b\13\0"+
-    "\1\u0116\160\0\1\u0157\4\u03d2\2\0\1\u03d2\15\0\1\u03d2"+
-    "\6\0\2\u03d3\1\u03d2\2\u03d3\2\u03d2\1\u03d3\1\u03d2\1\u03d3"+
-    "\1\u037b\13\0\1\u0116\226\0\1\u02bf\13\0\1\u0116\160\0"+
-    "\1\334\32\216\1\335\1\u03d4\11\216\175\0\1\334\1\u03d5"+
-    "\31\216\1\335\12\216\175\0\1\334\32\216\1\335\10\216"+
-    "\1\u03d6\1\216\175\0\1\334\25\216\1\u015e\4\216\1\335"+
-    "\12\216\175\0\1\334\32\216\1\335\5\216\1\u03d7\4\216"+
-    "\175\0\1\334\32\216\1\335\5\216\1\u03d8\4\216\175\0"+
-    "\1\334\32\216\1\335\5\216\1\u038d\4\216\175\0\1\334"+
-    "\32\216\1\335\3\216\1\u03d5\6\216\175\0\1\334\12\216"+
-    "\1\u03d9\17\216\1\335\12\216\175\0\1\334\25\216\1\u03da"+
-    "\4\216\1\335\12\216\175\0\1\334\15\216\1\u03db\14\216"+
-    "\1\335\12\216\175\0\1\334\32\216\1\335\3\216\1\u03dc"+
-    "\6\216\175\0\1\334\2\216\1\u0383\27\216\1\335\12\216"+
-    "\175\0\1\334\1\216\1\u015e\30\216\1\335\12\216\175\0"+
-    "\1\334\11\216\1\u03dd\20\216\1\335\12\216\175\0\1\334"+
-    "\11\216\1\u03de\20\216\1\335\12\216\175\0\1\334\1\u03df"+
-    "\31\216\1\335\12\216\175\0\1\334\1\u03e0\31\216\1\335"+
-    "\12\216\175\0\1\334\2\216\1\u03e1\27\216\1\335\12\216"+
-    "\175\0\1\334\32\216\1\335\4\216\1\u0165\5\216\175\0"+
-    "\1\334\10\216\1\u03e2\21\216\1\335\12\216\175\0\1\334"+
-    "\1\u03e3\31\216\1\335\12\216\175\0\1\334\25\216\1\u03e4"+
-    "\4\216\1\335\12\216\175\0\1\334\32\216\1\335\4\216"+
-    "\1\u03d5\5\216\175\0\1\334\32\216\1\335\6\216\1\u03d5"+
-    "\3\216\175\0\1\334\32\216\1\335\2\216\1\u03d5\7\216"+
-    "\175\0\1\334\16\216\1\u03e5\13\216\1\335\12\216\175\0"+
-    "\1\334\32\216\1\335\1\u03e6\11\216\175\0\1\334\32\216"+
-    "\1\335\3\216\1\u03e7\6\216\175\0\1\334\24\216\1\u03e8"+
-    "\5\216\1\335\12\216\175\0\1\363\1\u03e9\31\252\1\127"+
-    "\12\252\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\150\0\1\363\32\252\1\127\11\252"+
-    "\1\u02ef\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\150\0\1\363\1\u03ea\31\252\1\127"+
-    "\12\252\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\150\0\1\363\7\252\1\u03eb\22\252"+
-    "\1\127\12\252\1\0\3\124\1\0\2\124\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\150\0\1\363\1\u03ec\31\252"+
-    "\1\127\12\252\1\0\3\124\1\0\2\124\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\150\0\1\363\32\252\1\127"+
-    "\6\252\1\u03ed\3\252\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\150\0\1\363\25\252"+
-    "\1\u03ee\4\252\1\127\12\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\1\u03ef\31\252\1\127\12\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\32\252\1\127\6\252\1\u03f0\3\252\1\0\3\124\1\0"+
-    "\2\124\1\125\3\124\3\0\1\124\4\0\2\124\150\0"+
-    "\1\363\1\u03f1\31\252\1\127\12\252\1\0\3\124\1\0"+
-    "\2\124\1\125\3\124\3\0\1\124\4\0\2\124\150\0"+
-    "\1\363\32\252\1\127\6\252\1\u0144\3\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\12\252\1\u03f2\17\252\1\127\12\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\363\1\u03f3\31\252\1\127\12\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\363\10\252\1\u03f4\21\252\1\127\12\252"+
-    "\1\0\3\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\150\0\1\363\31\252\1\u03f5\1\127\12\252"+
-    "\1\0\3\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\151\0\4\u03f6\2\0\1\u03f6\15\0\1\u03f6"+
-    "\6\0\12\u03f6\1\u035b\175\0\4\u03f7\2\0\1\u03f7\15\0"+
-    "\1\u03f7\6\0\12\u03f7\1\u03f8\175\0\4\u03f9\2\0\1\u03f9"+
-    "\15\0\1\u03f9\6\0\1\u03fa\2\u03fb\1\u03fa\5\u03fb\1\u03fc"+
-    "\14\0\1\u0307\161\0\4\u03fd\2\0\1\u03fd\15\0\1\u03fd"+
-    "\6\0\12\u03fd\1\u03b1\13\0\1\u0307\161\0\4\u03f9\2\0"+
-    "\1\u03f9\15\0\1\u03f9\6\0\1\u03fa\2\u03fb\1\u03fa\5\u03fb"+
-    "\1\u03fc\175\0\1\u0362\4\u03fd\2\0\1\u03fd\15\0\1\u03fd"+
-    "\6\0\12\u03fe\1\u03b1\13\0\1\u0307\160\0\1\u0362\4\u03fd"+
-    "\2\0\1\u03fd\15\0\1\u03fd\6\0\12\u03fd\1\u03b1\13\0"+
-    "\1\u0307\160\0\1\u0362\4\u03fd\2\0\1\u03fd\15\0\1\u03fd"+
-    "\6\0\2\u03fe\1\u03fd\2\u03fe\2\u03fd\1\u03fe\1\u03fd\1\u03fe"+
-    "\1\u03b1\13\0\1\u0307\161\0\4\u03ff\2\0\1\u03ff\15\0"+
-    "\1\u03ff\6\0\12\u03ff\1\u0361\13\0\1\u0307\160\0\1\u0400"+
-    "\33\0\12\u03b7\175\0\1\u0400\33\0\12\u0401\175\0\1\u0400"+
-    "\33\0\1\u03b7\1\u0402\1\u0401\2\u03b7\2\u0401\1\u03b7\1\u0401"+
-    "\1\u03b7\175\0\1\u0362\4\u03ff\2\0\1\u03ff\15\0\1\u03ff"+
-    "\6\0\12\u03ff\1\u0361\13\0\1\u0307\160\0\46\u01bc\1\0"+
-    "\2\u01bc\1\u01fd\3\u01bc\1\u01be\1\0\1\u01fc\3\0\2\u01bc"+
-    "\4\0\1\u01bc\235\0\1\u0403\212\0\12\u0404\11\0\1\u01fc"+
-    "\231\0\1\u030d\175\0\4\u0405\2\0\1\u0405\15\0\1\u0405"+
-    "\6\0\12\u0405\1\u03bf\175\0\4\u0406\2\0\1\u0406\15\0"+
-    "\1\u0406\6\0\12\u0406\1\u0407\175\0\4\u0408\2\0\1\u0408"+
-    "\15\0\1\u0408\6\0\12\u0408\1\u0409\13\0\1\u02b6\160\0"+
-    "\1\u0314\4\u0408\2\0\1\u0408\15\0\1\u0408\6\0\12\u040a"+
-    "\1\u0409\13\0\1\u02b6\160\0\1\u0314\4\u0408\2\0\1\u0408"+
-    "\15\0\1\u0408\6\0\12\u040b\1\u0409\13\0\1\u02b6\160\0"+
-    "\1\u0314\4\u0408\2\0\1\u0408\15\0\1\u0408\6\0\1\u040a"+
-    "\1\u040c\1\u040b\2\u040a\2\u040b\1\u040a\1\u040b\1\u040a\1\u0409"+
-    "\13\0\1\u02b6\161\0\4\u040d\2\0\1\u040d\15\0\1\u040d"+
-    "\6\0\12\u040d\1\u036d\13\0\1\u02b6\160\0\1\u0314\4\u040d"+
-    "\2\0\1\u040d\15\0\1\u040d\6\0\12\u040d\1\u036d\13\0"+
-    "\1\u02b6\226\0\1\u0313\13\0\1\u02b6\214\0\1\u040e\2\u040f"+
-    "\1\u040e\5\u040f\1\u0410\175\0\1\u03c7\242\0\1\u03c7\33\0"+
-    "\2\u03c8\1\0\2\u03c8\2\0\1\u03c8\1\0\1\u03c8\175\0"+
-    "\34\u0204\12\u0411\1\0\2\u0204\1\u025a\3\u0204\1\u0206\1\u015b"+
-    "\1\u015c\1\u015d\2\0\2\u0204\4\0\1\u0204\217\0\1\u031b"+
-    "\175\0\4\u0412\2\0\1\u0412\15\0\1\u0412\6\0\12\u0412"+
-    "\1\u03cd\175\0\4\u0413\2\0\1\u0413\15\0\1\u0413\6\0"+
-    "\1\u0414\2\u0415\1\u0414\5\u0415\1\u0416\1\u0417\175\0\4\u0418"+
-    "\2\0\1\u0418\15\0\1\u0418\6\0\12\u0418\1\u0419\13\0"+
-    "\1\u0116\160\0\1\u0157\4\u0418\2\0\1\u0418\15\0\1\u0418"+
-    "\6\0\12\u041a\1\u0419\13\0\1\u0116\160\0\1\u0157\4\u0418"+
-    "\2\0\1\u0418\15\0\1\u0418\6\0\12\u041b\1\u0419\13\0"+
-    "\1\u0116\160\0\1\u0157\4\u0418\2\0\1\u0418\15\0\1\u0418"+
-    "\6\0\1\u041a\1\u041c\1\u041b\2\u041a\2\u041b\1\u041a\1\u041b"+
-    "\1\u041a\1\u0419\13\0\1\u0116\161\0\4\u041d\2\0\1\u041d"+
-    "\15\0\1\u041d\6\0\12\u041d\1\u037b\13\0\1\u0116\160\0"+
-    "\1\u0157\4\u041d\2\0\1\u041d\15\0\1\u041d\6\0\12\u041d"+
-    "\1\u037b\13\0\1\u0116\160\0\1\334\3\216\1\u041e\26\216"+
-    "\1\335\12\216\175\0\1\334\2\216\1\u015e\27\216\1\335"+
-    "\12\216\175\0\1\334\6\216\1\u0169\23\216\1\335\12\216"+
-    "\175\0\1\334\1\216\1\u0397\30\216\1\335\12\216\175\0"+
-    "\1\334\3\216\1\u041f\26\216\1\335\12\216\175\0\1\334"+
-    "\32\216\1\335\6\216\1\u0420\3\216\175\0\1\334\32\216"+
-    "\1\335\6\216\1\u0421\3\216\175\0\1\334\32\216\1\335"+
-    "\7\216\1\u0422\2\216\175\0\1\334\1\u0423\31\216\1\335"+
-    "\12\216\175\0\1\334\32\216\1\335\4\216\1\u0424\5\216"+
-    "\175\0\1\334\32\216\1\335\4\216\1\u0425\5\216\175\0"+
-    "\1\334\26\216\1\u0426\3\216\1\335\12\216\175\0\1\334"+
-    "\30\216\1\u0427\1\216\1\335\12\216\175\0\1\334\11\216"+
-    "\1\u0198\20\216\1\335\12\216\175\0\1\334\32\216\1\335"+
-    "\2\216\1\u0428\7\216\175\0\1\334\12\216\1\u0429\17\216"+
-    "\1\335\12\216\175\0\1\334\17\216\1\u0166\12\216\1\335"+
-    "\12\216\175\0\1\334\32\216\1\335\4\216\1\u042a\5\216"+
-    "\175\0\1\334\32\216\1\335\6\216\1\u019b\3\216\175\0"+
-    "\1\334\30\216\1\u042b\1\216\1\335\12\216\175\0\1\334"+
-    "\30\216\1\u042c\1\216\1\335\12\216\175\0\1\363\32\252"+
-    "\1\127\1\u042d\11\252\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\150\0\1\363\32\252"+
-    "\1\127\10\252\1\u02e5\1\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\15\252\1\275\14\252\1\127\12\252\1\0\3\124\1\0"+
-    "\2\124\1\125\3\124\3\0\1\124\4\0\2\124\150\0"+
-    "\1\363\32\252\1\127\1\252\1\u042e\10\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\32\252\1\127\3\252\1\u0148\6\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\363\30\252\1\u042f\1\252\1\127\12\252"+
-    "\1\0\3\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\150\0\1\363\32\252\1\127\1\252\1\u0430"+
-    "\10\252\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\150\0\1\363\6\252\1\u0431\23\252"+
-    "\1\127\12\252\1\0\3\124\1\0\2\124\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\150\0\1\363\32\252\1\127"+
-    "\5\252\1\u0432\4\252\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\150\0\1\363\22\252"+
-    "\1\370\7\252\1\127\12\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\32\252\1\127\5\252\1\u0433\4\252\1\0\3\124\1\0"+
-    "\2\124\1\125\3\124\3\0\1\124\4\0\2\124\150\0"+
-    "\1\363\32\252\1\127\1\252\1\275\10\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\13\252\1\u0434\16\252\1\127\12\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\216\0\1\u035b\175\0\4\u0435\2\0\1\u0435\15\0"+
-    "\1\u0435\6\0\12\u0435\1\u03f8\175\0\4\u0436\2\0\1\u0436"+
-    "\15\0\1\u0436\6\0\12\u0436\1\u0437\175\0\4\u0438\2\0"+
-    "\1\u0438\15\0\1\u0438\6\0\12\u0438\1\u0439\13\0\1\u0307"+
-    "\160\0\1\u0362\4\u0438\2\0\1\u0438\15\0\1\u0438\6\0"+
-    "\12\u043a\1\u0439\13\0\1\u0307\160\0\1\u0362\4\u0438\2\0"+
-    "\1\u0438\15\0\1\u0438\6\0\12\u043b\1\u0439\13\0\1\u0307"+
-    "\160\0\1\u0362\4\u0438\2\0\1\u0438\15\0\1\u0438\6\0"+
-    "\1\u043a\1\u043c\1\u043b\2\u043a\2\u043b\1\u043a\1\u043b\1\u043a"+
-    "\1\u0439\13\0\1\u0307\161\0\4\u043d\2\0\1\u043d\15\0"+
-    "\1\u043d\6\0\12\u043d\1\u03b1\13\0\1\u0307\160\0\1\u0362"+
-    "\4\u043d\2\0\1\u043d\15\0\1\u043d\6\0\12\u043d\1\u03b1"+
-    "\13\0\1\u0307\226\0\1\u0361\13\0\1\u0307\214\0\1\u043e"+
-    "\2\u043f\1\u043e\5\u043f\1\u0440\175\0\1\u0400\242\0\1\u0400"+
-    "\33\0\2\u0401\1\0\2\u0401\2\0\1\u0401\1\0\1\u0401"+
-    "\176\0\1\u0441\1\0\1\u0441\5\0\1\u0441\310\0\1\u01fc"+
-    "\164\0\4\u0442\2\0\1\u0442\15\0\1\u0442\6\0\12\u0442"+
-    "\1\u03bf\175\0\4\u0443\2\0\1\u0443\15\0\1\u0443\6\0"+
-    "\12\u0443\1\u0444\175\0\4\u0445\2\0\1\u0445\15\0\1\u0445"+
-    "\6\0\1\u0446\2\u0447\1\u0446\5\u0447\1\u0448\14\0\1\u02b6"+
-    "\161\0\4\u0449\2\0\1\u0449\15\0\1\u0449\6\0\12\u0449"+
-    "\1\u0409\13\0\1\u02b6\161\0\4\u0445\2\0\1\u0445\15\0"+
-    "\1\u0445\6\0\1\u0446\2\u0447\1\u0446\5\u0447\1\u0448\175\0"+
-    "\1\u0314\4\u0449\2\0\1\u0449\15\0\1\u0449\6\0\12\u044a"+
-    "\1\u0409\13\0\1\u02b6\160\0\1\u0314\4\u0449\2\0\1\u0449"+
-    "\15\0\1\u0449\6\0\12\u0449\1\u0409\13\0\1\u02b6\160\0"+
-    "\1\u0314\4\u0449\2\0\1\u0449\15\0\1\u0449\6\0\2\u044a"+
-    "\1\u0449\2\u044a\2\u0449\1\u044a\1\u0449\1\u044a\1\u0409\13\0"+
-    "\1\u02b6\226\0\1\u036d\13\0\1\u02b6\160\0\1\u044b\33\0"+
-    "\12\u040f\175\0\1\u044b\33\0\12\u044c\175\0\1\u044b\33\0"+
-    "\1\u040f\1\u044d\1\u044c\2\u040f\2\u044c\1\u040f\1\u044c\1\u040f"+
-    "\175\0\46\u0204\1\0\2\u0204\1\u025a\3\u0204\1\u0206\1\u015b"+
-    "\1\u015c\1\u015d\2\0\2\u0204\4\0\1\u0204\152\0\4\u044e"+
-    "\2\0\1\u044e\15\0\1\u044e\6\0\12\u044e\1\u03cd\175\0"+
-    "\4\u044f\2\0\1\u044f\15\0\1\u044f\6\0\12\u044f\1\u0450"+
-    "\174\0\1\u0157\4\u044f\2\0\1\u044f\15\0\1\u044f\6\0"+
-    "\12\u0451\1\u0450\174\0\1\u0157\4\u044f\2\0\1\u044f\15\0"+
-    "\1\u044f\6\0\12\u0452\1\u0450\174\0\1\u0157\4\u044f\2\0"+
-    "\1\u044f\15\0\1\u044f\6\0\1\u0451\1\u0453\1\u0452\2\u0451"+
-    "\2\u0452\1\u0451\1\u0452\1\u0451\1\u0450\175\0\4\u0454\2\0"+
-    "\1\u0454\15\0\1\u0454\6\0\12\u0454\14\0\1\u0116\161\0"+
-    "\4\u0455\2\0\1\u0455\15\0\1\u0455\6\0\12\u0455\1\u0419"+
-    "\13\0\1\u0116\161\0\4\u0454\2\0\1\u0454\15\0\1\u0454"+
-    "\6\0\12\u0454\175\0\1\u0157\4\u0455\2\0\1\u0455\15\0"+
-    "\1\u0455\6\0\12\u0456\1\u0419\13\0\1\u0116\160\0\1\u0157"+
-    "\4\u0455\2\0\1\u0455\15\0\1\u0455\6\0\12\u0455\1\u0419"+
-    "\13\0\1\u0116\160\0\1\u0157\4\u0455\2\0\1\u0455\15\0"+
-    "\1\u0455\6\0\2\u0456\1\u0455\2\u0456\2\u0455\1\u0456\1\u0455"+
-    "\1\u0456\1\u0419\13\0\1\u0116\226\0\1\u037b\13\0\1\u0116"+
-    "\160\0\1\334\1\u0457\31\216\1\335\12\216\175\0\1\334"+
-    "\32\216\1\335\11\216\1\u038d\175\0\1\334\1\u0458\31\216"+
-    "\1\335\12\216\175\0\1\334\7\216\1\u0459\22\216\1\335"+
-    "\12\216\175\0\1\334\1\u045a\31\216\1\335\12\216\175\0"+
-    "\1\334\32\216\1\335\6\216\1\u045b\3\216\175\0\1\334"+
-    "\25\216\1\u045c\4\216\1\335\12\216\175\0\1\334\1\u045d"+
-    "\31\216\1\335\12\216\175\0\1\334\32\216\1\335\6\216"+
-    "\1\u045e\3\216\175\0\1\334\1\u045f\31\216\1\335\12\216"+
-    "\175\0\1\334\32\216\1\335\6\216\1\u0197\3\216\175\0"+
-    "\1\334\12\216\1\u0460\17\216\1\335\12\216\175\0\1\334"+
-    "\1\u0461\31\216\1\335\12\216\175\0\1\334\10\216\1\u0462"+
-    "\21\216\1\335\12\216\175\0\1\334\31\216\1\u0463\1\335"+
-    "\12\216\175\0\1\363\24\252\1\u0464\5\252\1\127\12\252"+
-    "\1\0\3\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\150\0\1\363\2\252\1\u0465\27\252\1\127"+
-    "\12\252\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\150\0\1\363\3\252\1\u0466\26\252"+
-    "\1\127\12\252\1\0\3\124\1\0\2\124\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\150\0\1\363\3\252\1\u0467"+
-    "\26\252\1\127\12\252\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\150\0\1\363\32\252"+
-    "\1\127\1\252\1\u0468\10\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\3\252\1\u0469\26\252\1\127\12\252\1\0\3\124\1\0"+
-    "\2\124\1\125\3\124\3\0\1\124\4\0\2\124\150\0"+
-    "\1\363\1\u046a\31\252\1\127\12\252\1\0\3\124\1\0"+
-    "\2\124\1\125\3\124\3\0\1\124\4\0\2\124\150\0"+
-    "\1\363\26\252\1\u046b\3\252\1\127\12\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\151\0\4\u046c\2\0\1\u046c\15\0\1\u046c\6\0\12\u046c"+
-    "\1\u03f8\175\0\4\u046d\2\0\1\u046d\15\0\1\u046d\6\0"+
-    "\12\u046d\1\u046e\175\0\4\u046f\2\0\1\u046f\15\0\1\u046f"+
-    "\6\0\1\u0470\2\u0471\1\u0470\5\u0471\1\u0472\14\0\1\u0307"+
-    "\161\0\4\u0473\2\0\1\u0473\15\0\1\u0473\6\0\12\u0473"+
-    "\1\u0439\13\0\1\u0307\161\0\4\u046f\2\0\1\u046f\15\0"+
-    "\1\u046f\6\0\1\u0470\2\u0471\1\u0470\5\u0471\1\u0472\175\0"+
-    "\1\u0362\4\u0473\2\0\1\u0473\15\0\1\u0473\6\0\12\u0474"+
-    "\1\u0439\13\0\1\u0307\160\0\1\u0362\4\u0473\2\0\1\u0473"+
-    "\15\0\1\u0473\6\0\12\u0473\1\u0439\13\0\1\u0307\160\0"+
-    "\1\u0362\4\u0473\2\0\1\u0473\15\0\1\u0473\6\0\2\u0474"+
-    "\1\u0473\2\u0474\2\u0473\1\u0474\1\u0473\1\u0474\1\u0439\13\0"+
-    "\1\u0307\226\0\1\u03b1\13\0\1\u0307\160\0\1\u0475\33\0"+
-    "\12\u043f\175\0\1\u0475\33\0\12\u0476\175\0\1\u0475\33\0"+
-    "\1\u043f\1\u0477\1\u0476\2\u043f\2\u0476\1\u043f\1\u0476\1\u043f"+
-    "\255\0\1\u015d\230\0\1\u03bf\175\0\4\u0478\2\0\1\u0478"+
-    "\15\0\1\u0478\6\0\12\u0478\1\u0444\175\0\4\u0479\2\0"+
-    "\1\u0479\15\0\1\u0479\6\0\12\u0479\1\u047a\175\0\4\u047b"+
-    "\2\0\1\u047b\15\0\1\u047b\6\0\12\u047b\1\u047c\13\0"+
-    "\1\u02b6\160\0\1\u0314\4\u047b\2\0\1\u047b\15\0\1\u047b"+
-    "\6\0\12\u047d\1\u047c\13\0\1\u02b6\160\0\1\u0314\4\u047b"+
-    "\2\0\1\u047b\15\0\1\u047b\6\0\12\u047e\1\u047c\13\0"+
-    "\1\u02b6\160\0\1\u0314\4\u047b\2\0\1\u047b\15\0\1\u047b"+
-    "\6\0\1\u047d\1\u047f\1\u047e\2\u047d\2\u047e\1\u047d\1\u047e"+
-    "\1\u047d\1\u047c\13\0\1\u02b6\161\0\4\u0480\2\0\1\u0480"+
-    "\15\0\1\u0480\6\0\12\u0480\1\u0409\13\0\1\u02b6\160\0"+
-    "\1\u0314\4\u0480\2\0\1\u0480\15\0\1\u0480\6\0\12\u0480"+
-    "\1\u0409\13\0\1\u02b6\214\0\1\u0481\2\u0482\1\u0481\5\u0482"+
-    "\1\u0483\175\0\1\u044b\242\0\1\u044b\33\0\2\u044c\1\0"+
-    "\2\u044c\2\0\1\u044c\1\0\1\u044c\243\0\1\u03cd\175\0"+
-    "\4\u0484\2\0\1\u0484\15\0\1\u0484\6\0\12\u0484\1\u0450"+
-    "\175\0\4\u0454\2\0\1\u0454\15\0\1\u0454\6\0\12\u0454"+
-    "\1\u0322\174\0\1\u0157\4\u0484\2\0\1\u0484\15\0\1\u0484"+
-    "\6\0\12\u0485\1\u0450\174\0\1\u0157\4\u0484\2\0\1\u0484"+
-    "\15\0\1\u0484\6\0\12\u0484\1\u0450\174\0\1\u0157\4\u0484"+
-    "\2\0\1\u0484\15\0\1\u0484\6\0\2\u0485\1\u0484\2\u0485"+
-    "\2\u0484\1\u0485\1\u0484\1\u0485\1\u0450\175\0\4\u0486\2\0"+
-    "\1\u0486\15\0\1\u0486\6\0\12\u0486\14\0\1\u0116\161\0"+
-    "\4\u0487\2\0\1\u0487\15\0\1\u0487\6\0\12\u0487\1\u0419"+
-    "\13\0\1\u0116\160\0\1\u0157\4\u0487\2\0\1\u0487\15\0"+
-    "\1\u0487\6\0\12\u0487\1\u0419\13\0\1\u0116\160\0\1\334"+
-    "\32\216\1\335\1\u0488\11\216\175\0\1\334\32\216\1\335"+
-    "\10\216\1\u0383\1\216\175\0\1\334\15\216\1\u0123\14\216"+
-    "\1\335\12\216\175\0\1\334\32\216\1\335\1\216\1\u0489"+
-    "\10\216\175\0\1\334\32\216\1\335\3\216\1\u019b\6\216"+
-    "\175\0\1\334\30\216\1\u048a\1\216\1\335\12\216\175\0"+
-    "\1\334\32\216\1\335\1\216\1\u048b\10\216\175\0\1\334"+
-    "\6\216\1\u048c\23\216\1\335\12\216\175\0\1\334\32\216"+
-    "\1\335\5\216\1\u048d\4\216\175\0\1\334\22\216\1\u015e"+
-    "\7\216\1\335\12\216\175\0\1\334\32\216\1\335\5\216"+
-    "\1\u048e\4\216\175\0\1\334\32\216\1\335\1\216\1\u0123"+
-    "\10\216\175\0\1\334\13\216\1\u048f\16\216\1\335\12\216"+
-    "\175\0\1\363\32\252\1\127\11\252\1\u0490\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\150\0\1\363\32\252\1\127\7\252\1\u0491\2\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\363\32\252\1\127\11\252\1\275\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\150\0\1\363\3\252\1\u0492\26\252\1\127\12\252"+
-    "\1\0\3\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\150\0\1\363\32\252\1\127\4\252\1\u0493"+
-    "\5\252\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\150\0\1\363\16\252\1\u0494\13\252"+
-    "\1\127\12\252\1\0\3\124\1\0\2\124\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\150\0\1\363\26\252\1\u0495"+
-    "\3\252\1\127\12\252\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\150\0\1\363\32\252"+
-    "\1\127\7\252\1\u0496\2\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\216\0\1\u03f8"+
-    "\175\0\4\u0497\2\0\1\u0497\15\0\1\u0497\6\0\12\u0497"+
-    "\1\u046e\175\0\4\u0498\2\0\1\u0498\15\0\1\u0498\6\0"+
-    "\12\u0498\1\u0499\175\0\4\u049a\2\0\1\u049a\15\0\1\u049a"+
-    "\6\0\12\u049a\1\u049b\13\0\1\u0307\160\0\1\u0362\4\u049a"+
-    "\2\0\1\u049a\15\0\1\u049a\6\0\12\u049c\1\u049b\13\0"+
-    "\1\u0307\160\0\1\u0362\4\u049a\2\0\1\u049a\15\0\1\u049a"+
-    "\6\0\12\u049d\1\u049b\13\0\1\u0307\160\0\1\u0362\4\u049a"+
-    "\2\0\1\u049a\15\0\1\u049a\6\0\1\u049c\1\u049e\1\u049d"+
-    "\2\u049c\2\u049d\1\u049c\1\u049d\1\u049c\1\u049b\13\0\1\u0307"+
-    "\161\0\4\u049f\2\0\1\u049f\15\0\1\u049f\6\0\12\u049f"+
-    "\1\u0439\13\0\1\u0307\160\0\1\u0362\4\u049f\2\0\1\u049f"+
-    "\15\0\1\u049f\6\0\12\u049f\1\u0439\13\0\1\u0307\214\0"+
-    "\1\u04a0\2\u04a1\1\u04a0\5\u04a1\1\u04a2\175\0\1\u0475\242\0"+
-    "\1\u0475\33\0\2\u0476\1\0\2\u0476\2\0\1\u0476\1\0"+
-    "\1\u0476\176\0\4\u04a3\2\0\1\u04a3\15\0\1\u04a3\6\0"+
-    "\12\u04a3\1\u0444\175\0\4\u04a4\2\0\1\u04a4\15\0\1\u04a4"+
-    "\6\0\12\u04a4\1\u04a5\175\0\4\u04a6\2\0\1\u04a6\15\0"+
-    "\1\u04a6\6\0\1\u04a7\2\u04a8\1\u04a7\5\u04a8\1\u04a9\14\0"+
-    "\1\u02b6\161\0\4\u04aa\2\0\1\u04aa\15\0\1\u04aa\6\0"+
-    "\12\u04aa\1\u047c\13\0\1\u02b6\161\0\4\u04a6\2\0\1\u04a6"+
-    "\15\0\1\u04a6\6\0\1\u04a7\2\u04a8\1\u04a7\5\u04a8\1\u04a9"+
-    "\175\0\1\u0314\4\u04aa\2\0\1\u04aa\15\0\1\u04aa\6\0"+
-    "\12\u04ab\1\u047c\13\0\1\u02b6\160\0\1\u0314\4\u04aa\2\0"+
-    "\1\u04aa\15\0\1\u04aa\6\0\12\u04aa\1\u047c\13\0\1\u02b6"+
-    "\160\0\1\u0314\4\u04aa\2\0\1\u04aa\15\0\1\u04aa\6\0"+
-    "\2\u04ab\1\u04aa\2\u04ab\2\u04aa\1\u04ab\1\u04aa\1\u04ab\1\u047c"+
-    "\13\0\1\u02b6\226\0\1\u0409\13\0\1\u02b6\214\0\12\u0482"+
-    "\14\0\1\u02b6\214\0\12\u04ac\14\0\1\u02b6\214\0\1\u0482"+
-    "\1\u04ad\1\u04ac\2\u0482\2\u04ac\1\u0482\1\u04ac\1\u0482\14\0"+
-    "\1\u02b6\161\0\4\u04ae\2\0\1\u04ae\15\0\1\u04ae\6\0"+
-    "\12\u04ae\1\u0450\174\0\1\u0157\4\u04ae\2\0\1\u04ae\15\0"+
-    "\1\u04ae\6\0\12\u04ae\1\u0450\175\0\4\u04af\2\0\1\u04af"+
-    "\15\0\1\u04af\6\0\12\u04af\14\0\1\u0116\226\0\1\u0419"+
-    "\13\0\1\u0116\160\0\1\334\24\216\1\u04b0\5\216\1\335"+
-    "\12\216\175\0\1\334\2\216\1\u04b1\27\216\1\335\12\216"+
-    "\175\0\1\334\3\216\1\u04b2\26\216\1\335\12\216\175\0"+
-    "\1\334\3\216\1\u04b3\26\216\1\335\12\216\175\0\1\334"+
-    "\32\216\1\335\1\216\1\u04b4\10\216\175\0\1\334\3\216"+
-    "\1\u04b5\26\216\1\335\12\216\175\0\1\334\1\u04b6\31\216"+
-    "\1\335\12\216\175\0\1\334\26\216\1\u04b7\3\216\1\335"+
-    "\12\216\175\0\1\363\7\252\1\u04b8\22\252\1\127\12\252"+
-    "\1\0\3\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\150\0\1\363\1\u04b9\31\252\1\127\12\252"+
-    "\1\0\3\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\150\0\1\363\32\252\1\127\1\u02e5\11\252"+
-    "\1\0\3\124\1\0\2\124\1\125\3\124\3\0\1\124"+
-    "\4\0\2\124\150\0\1\363\24\252\1\u04ba\5\252\1\127"+
-    "\12\252\1\0\3\124\1\0\2\124\1\125\3\124\3\0"+
-    "\1\124\4\0\2\124\150\0\1\363\1\252\1\u04bb\30\252"+
-    "\1\127\12\252\1\0\3\124\1\0\2\124\1\125\3\124"+
-    "\3\0\1\124\4\0\2\124\150\0\1\363\32\252\1\127"+
-    "\2\252\1\377\7\252\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\150\0\1\363\6\252"+
-    "\1\370\23\252\1\127\12\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\151\0\4\u04bc"+
-    "\2\0\1\u04bc\15\0\1\u04bc\6\0\12\u04bc\1\u046e\175\0"+
-    "\4\u04bd\2\0\1\u04bd\15\0\1\u04bd\6\0\12\u04bd\1\u04be"+
-    "\175\0\4\u04bf\2\0\1\u04bf\15\0\1\u04bf\6\0\1\u04c0"+
-    "\2\u04c1\1\u04c0\5\u04c1\1\u04c2\14\0\1\u0307\161\0\4\u04c3"+
-    "\2\0\1\u04c3\15\0\1\u04c3\6\0\12\u04c3\1\u049b\13\0"+
-    "\1\u0307\161\0\4\u04bf\2\0\1\u04bf\15\0\1\u04bf\6\0"+
-    "\1\u04c0\2\u04c1\1\u04c0\5\u04c1\1\u04c2\175\0\1\u0362\4\u04c3"+
-    "\2\0\1\u04c3\15\0\1\u04c3\6\0\12\u04c4\1\u049b\13\0"+
-    "\1\u0307\160\0\1\u0362\4\u04c3\2\0\1\u04c3\15\0\1\u04c3"+
-    "\6\0\12\u04c3\1\u049b\13\0\1\u0307\160\0\1\u0362\4\u04c3"+
-    "\2\0\1\u04c3\15\0\1\u04c3\6\0\2\u04c4\1\u04c3\2\u04c4"+
-    "\2\u04c3\1\u04c4\1\u04c3\1\u04c4\1\u049b\13\0\1\u0307\226\0"+
-    "\1\u0439\13\0\1\u0307\214\0\12\u04a1\14\0\1\u0307\214\0"+
-    "\12\u04c5\14\0\1\u0307\214\0\1\u04a1\1\u04c6\1\u04c5\2\u04a1"+
-    "\2\u04c5\1\u04a1\1\u04c5\1\u04a1\14\0\1\u0307\226\0\1\u0444"+
-    "\175\0\4\u04c7\2\0\1\u04c7\15\0\1\u04c7\6\0\12\u04c7"+
-    "\1\u04a5\175\0\4\u04c8\2\0\1\u04c8\15\0\1\u04c8\6\0"+
-    "\12\u04c8\1\u04c9\175\0\4\u04ca\2\0\1\u04ca\15\0\1\u04ca"+
-    "\6\0\12\u04ca\1\u04cb\13\0\1\u02b6\160\0\1\u0314\4\u04ca"+
-    "\2\0\1\u04ca\15\0\1\u04ca\6\0\12\u04cc\1\u04cb\13\0"+
-    "\1\u02b6\160\0\1\u0314\4\u04ca\2\0\1\u04ca\15\0\1\u04ca"+
-    "\6\0\12\u04cd\1\u04cb\13\0\1\u02b6\160\0\1\u0314\4\u04ca"+
-    "\2\0\1\u04ca\15\0\1\u04ca\6\0\1\u04cc\1\u04ce\1\u04cd"+
-    "\2\u04cc\2\u04cd\1\u04cc\1\u04cd\1\u04cc\1\u04cb\13\0\1\u02b6"+
-    "\161\0\4\u04cf\2\0\1\u04cf\15\0\1\u04cf\6\0\12\u04cf"+
-    "\1\u047c\13\0\1\u02b6\160\0\1\u0314\4\u04cf\2\0\1\u04cf"+
-    "\15\0\1\u04cf\6\0\12\u04cf\1\u047c\13\0\1\u02b6\242\0"+
-    "\1\u02b6\214\0\2\u04ac\1\0\2\u04ac\2\0\1\u04ac\1\0"+
-    "\1\u04ac\14\0\1\u02b6\226\0\1\u0450\175\0\4\u0322\2\0"+
-    "\1\u0322\15\0\1\u0322\6\0\12\u0322\14\0\1\u0116\160\0"+
-    "\1\334\32\216\1\335\11\216\1\u04d0\175\0\1\334\32\216"+
-    "\1\335\7\216\1\u04d1\2\216\175\0\1\334\32\216\1\335"+
-    "\11\216\1\u0123\175\0\1\334\3\216\1\u04d2\26\216\1\335"+
-    "\12\216\175\0\1\334\32\216\1\335\4\216\1\u04d3\5\216"+
-    "\175\0\1\334\16\216\1\u04d4\13\216\1\335\12\216\175\0"+
-    "\1\334\26\216\1\u04d5\3\216\1\335\12\216\175\0\1\334"+
-    "\32\216\1\335\7\216\1\u04d6\2\216\175\0\1\363\32\252"+
-    "\1\127\11\252\1\u04d7\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\150\0\1\363\4\252"+
-    "\1\370\25\252\1\127\12\252\1\0\3\124\1\0\2\124"+
-    "\1\125\3\124\3\0\1\124\4\0\2\124\150\0\1\363"+
-    "\24\252\1\275\5\252\1\127\12\252\1\0\3\124\1\0"+
-    "\2\124\1\125\3\124\3\0\1\124\4\0\2\124\150\0"+
-    "\1\363\32\252\1\127\6\252\1\275\3\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\216\0\1\u046e\175\0\4\u04d8\2\0\1\u04d8\15\0\1\u04d8"+
-    "\6\0\12\u04d8\1\u04be\175\0\4\u04d9\2\0\1\u04d9\15\0"+
-    "\1\u04d9\6\0\12\u04d9\1\u04da\175\0\4\u04db\2\0\1\u04db"+
-    "\15\0\1\u04db\6\0\12\u04db\1\u04dc\13\0\1\u0307\160\0"+
-    "\1\u0362\4\u04db\2\0\1\u04db\15\0\1\u04db\6\0\12\u04dd"+
-    "\1\u04dc\13\0\1\u0307\160\0\1\u0362\4\u04db\2\0\1\u04db"+
-    "\15\0\1\u04db\6\0\12\u04de\1\u04dc\13\0\1\u0307\160\0"+
-    "\1\u0362\4\u04db\2\0\1\u04db\15\0\1\u04db\6\0\1\u04dd"+
-    "\1\u04df\1\u04de\2\u04dd\2\u04de\1\u04dd\1\u04de\1\u04dd\1\u04dc"+
-    "\13\0\1\u0307\161\0\4\u04e0\2\0\1\u04e0\15\0\1\u04e0"+
-    "\6\0\12\u04e0\1\u049b\13\0\1\u0307\160\0\1\u0362\4\u04e0"+
-    "\2\0\1\u04e0\15\0\1\u04e0\6\0\12\u04e0\1\u049b\13\0"+
-    "\1\u0307\242\0\1\u0307\214\0\2\u04c5\1\0\2\u04c5\2\0"+
-    "\1\u04c5\1\0\1\u04c5\14\0\1\u0307\161\0\4\u04e1\2\0"+
-    "\1\u04e1\15\0\1\u04e1\6\0\12\u04e1\1\u04a5\175\0\4\u04e2"+
-    "\2\0\1\u04e2\15\0\1\u04e2\6\0\12\u04e2\1\u04e3\175\0"+
-    "\4\u04e4\2\0\1\u04e4\15\0\1\u04e4\6\0\1\u04e5\2\u04e6"+
-    "\1\u04e5\5\u04e6\1\u04e7\14\0\1\u02b6\161\0\4\u04e8\2\0"+
-    "\1\u04e8\15\0\1\u04e8\6\0\12\u04e8\1\u04cb\13\0\1\u02b6"+
-    "\161\0\4\u04e4\2\0\1\u04e4\15\0\1\u04e4\6\0\1\u04e5"+
-    "\2\u04e6\1\u04e5\5\u04e6\1\u04e7\175\0\1\u0314\4\u04e8\2\0"+
-    "\1\u04e8\15\0\1\u04e8\6\0\12\u04e9\1\u04cb\13\0\1\u02b6"+
-    "\160\0\1\u0314\4\u04e8\2\0\1\u04e8\15\0\1\u04e8\6\0"+
-    "\12\u04e8\1\u04cb\13\0\1\u02b6\160\0\1\u0314\4\u04e8\2\0"+
-    "\1\u04e8\15\0\1\u04e8\6\0\2\u04e9\1\u04e8\2\u04e9\2\u04e8"+
-    "\1\u04e9\1\u04e8\1\u04e9\1\u04cb\13\0\1\u02b6\226\0\1\u047c"+
-    "\13\0\1\u02b6\160\0\1\334\7\216\1\u04ea\22\216\1\335"+
-    "\12\216\175\0\1\334\1\u04eb\31\216\1\335\12\216\175\0"+
-    "\1\334\32\216\1\335\1\u0383\11\216\175\0\1\334\24\216"+
-    "\1\u04ec\5\216\1\335\12\216\175\0\1\334\1\216\1\u04ed"+
-    "\30\216\1\335\12\216\175\0\1\334\32\216\1\335\2\216"+
-    "\1\u0165\7\216\175\0\1\334\6\216\1\u015e\23\216\1\335"+
-    "\12\216\175\0\1\363\1\u04ee\31\252\1\127\12\252\1\0"+
-    "\3\124\1\0\2\124\1\125\3\124\3\0\1\124\4\0"+
-    "\2\124\151\0\4\u04ef\2\0\1\u04ef\15\0\1\u04ef\6\0"+
-    "\12\u04ef\1\u04be\175\0\4\u04f0\2\0\1\u04f0\15\0\1\u04f0"+
-    "\6\0\12\u04f0\1\u04f1\175\0\4\u04f2\2\0\1\u04f2\15\0"+
-    "\1\u04f2\6\0\1\u04f3\2\u04f4\1\u04f3\5\u04f4\1\u04f5\14\0"+
-    "\1\u0307\161\0\4\u04f6\2\0\1\u04f6\15\0\1\u04f6\6\0"+
-    "\12\u04f6\1\u04dc\13\0\1\u0307\161\0\4\u04f2\2\0\1\u04f2"+
-    "\15\0\1\u04f2\6\0\1\u04f3\2\u04f4\1\u04f3\5\u04f4\1\u04f5"+
-    "\175\0\1\u0362\4\u04f6\2\0\1\u04f6\15\0\1\u04f6\6\0"+
-    "\12\u04f7\1\u04dc\13\0\1\u0307\160\0\1\u0362\4\u04f6\2\0"+
-    "\1\u04f6\15\0\1\u04f6\6\0\12\u04f6\1\u04dc\13\0\1\u0307"+
-    "\160\0\1\u0362\4\u04f6\2\0\1\u04f6\15\0\1\u04f6\6\0"+
-    "\2\u04f7\1\u04f6\2\u04f7\2\u04f6\1\u04f7\1\u04f6\1\u04f7\1\u04dc"+
-    "\13\0\1\u0307\226\0\1\u049b\13\0\1\u0307\226\0\1\u04a5"+
-    "\175\0\4\u04f8\2\0\1\u04f8\15\0\1\u04f8\6\0\12\u04f8"+
-    "\1\u04e3\175\0\4\u04f9\2\0\1\u04f9\15\0\1\u04f9\6\0"+
-    "\1\u04fa\2\u04fb\1\u04fa\5\u04fb\1\u04fc\1\u04fd\175\0\4\u04fe"+
-    "\2\0\1\u04fe\15\0\1\u04fe\6\0\12\u04fe\1\u04ff\13\0"+
-    "\1\u02b6\160\0\1\u0314\4\u04fe\2\0\1\u04fe\15\0\1\u04fe"+
-    "\6\0\12\u0500\1\u04ff\13\0\1\u02b6\160\0\1\u0314\4\u04fe"+
-    "\2\0\1\u04fe\15\0\1\u04fe\6\0\12\u0501\1\u04ff\13\0"+
-    "\1\u02b6\160\0\1\u0314\4\u04fe\2\0\1\u04fe\15\0\1\u04fe"+
-    "\6\0\1\u0500\1\u0502\1\u0501\2\u0500\2\u0501\1\u0500\1\u0501"+
-    "\1\u0500\1\u04ff\13\0\1\u02b6\161\0\4\u0503\2\0\1\u0503"+
-    "\15\0\1\u0503\6\0\12\u0503\1\u04cb\13\0\1\u02b6\160\0"+
-    "\1\u0314\4\u0503\2\0\1\u0503\15\0\1\u0503\6\0\12\u0503"+
-    "\1\u04cb\13\0\1\u02b6\160\0\1\334\32\216\1\335\11\216"+
-    "\1\u0504\175\0\1\334\4\216\1\u015e\25\216\1\335\12\216"+
-    "\175\0\1\334\24\216\1\u0123\5\216\1\335\12\216\175\0"+
-    "\1\334\32\216\1\335\6\216\1\u0123\3\216\175\0\1\363"+
-    "\32\252\1\127\5\252\1\u0505\4\252\1\0\3\124\1\0"+
-    "\2\124\1\125\3\124\3\0\1\124\4\0\2\124\216\0"+
-    "\1\u04be\175\0\4\u0506\2\0\1\u0506\15\0\1\u0506\6\0"+
-    "\12\u0506\1\u04f1\175\0\4\u0507\2\0\1\u0507\15\0\1\u0507"+
-    "\6\0\1\u0508\2\u0509\1\u0508\5\u0509\1\u050a\1\u050b\175\0"+
-    "\4\u050c\2\0\1\u050c\15\0\1\u050c\6\0\12\u050c\1\u050d"+
-    "\13\0\1\u0307\160\0\1\u0362\4\u050c\2\0\1\u050c\15\0"+
-    "\1\u050c\6\0\12\u050e\1\u050d\13\0\1\u0307\160\0\1\u0362"+
-    "\4\u050c\2\0\1\u050c\15\0\1\u050c\6\0\12\u050f\1\u050d"+
-    "\13\0\1\u0307\160\0\1\u0362\4\u050c\2\0\1\u050c\15\0"+
-    "\1\u050c\6\0\1\u050e\1\u0510\1\u050f\2\u050e\2\u050f\1\u050e"+
-    "\1\u050f\1\u050e\1\u050d\13\0\1\u0307\161\0\4\u0511\2\0"+
-    "\1\u0511\15\0\1\u0511\6\0\12\u0511\1\u04dc\13\0\1\u0307"+
-    "\160\0\1\u0362\4\u0511\2\0\1\u0511\15\0\1\u0511\6\0"+
-    "\12\u0511\1\u04dc\13\0\1\u0307\161\0\4\u0512\2\0\1\u0512"+
-    "\15\0\1\u0512\6\0\12\u0512\1\u04e3\175\0\4\u0513\2\0"+
-    "\1\u0513\15\0\1\u0513\6\0\12\u0513\1\u0514\174\0\1\u0314"+
-    "\4\u0513\2\0\1\u0513\15\0\1\u0513\6\0\12\u0515\1\u0514"+
-    "\174\0\1\u0314\4\u0513\2\0\1\u0513\15\0\1\u0513\6\0"+
-    "\12\u0516\1\u0514\174\0\1\u0314\4\u0513\2\0\1\u0513\15\0"+
-    "\1\u0513\6\0\1\u0515\1\u0517\1\u0516\2\u0515\2\u0516\1\u0515"+
-    "\1\u0516\1\u0515\1\u0514\175\0\4\u0518\2\0\1\u0518\15\0"+
-    "\1\u0518\6\0\12\u0518\14\0\1\u02b6\161\0\4\u0519\2\0"+
-    "\1\u0519\15\0\1\u0519\6\0\12\u0519\1\u04ff\13\0\1\u02b6"+
-    "\161\0\4\u0518\2\0\1\u0518\15\0\1\u0518\6\0\12\u0518"+
-    "\175\0\1\u0314\4\u0519\2\0\1\u0519\15\0\1\u0519\6\0"+
-    "\12\u051a\1\u04ff\13\0\1\u02b6\160\0\1\u0314\4\u0519\2\0"+
-    "\1\u0519\15\0\1\u0519\6\0\12\u0519\1\u04ff\13\0\1\u02b6"+
-    "\160\0\1\u0314\4\u0519\2\0\1\u0519\15\0\1\u0519\6\0"+
-    "\2\u051a\1\u0519\2\u051a\2\u0519\1\u051a\1\u0519\1\u051a\1\u04ff"+
-    "\13\0\1\u02b6\226\0\1\u04cb\13\0\1\u02b6\160\0\1\334"+
-    "\1\u051b\31\216\1\335\12\216\175\0\1\363\7\252\1\u051c"+
-    "\22\252\1\127\12\252\1\0\3\124\1\0\2\124\1\125"+
-    "\3\124\3\0\1\124\4\0\2\124\151\0\4\u051d\2\0"+
-    "\1\u051d\15\0\1\u051d\6\0\12\u051d\1\u04f1\175\0\4\u051e"+
-    "\2\0\1\u051e\15\0\1\u051e\6\0\12\u051e\1\u051f\174\0"+
-    "\1\u0362\4\u051e\2\0\1\u051e\15\0\1\u051e\6\0\12\u0520"+
-    "\1\u051f\174\0\1\u0362\4\u051e\2\0\1\u051e\15\0\1\u051e"+
-    "\6\0\12\u0521\1\u051f\174\0\1\u0362\4\u051e\2\0\1\u051e"+
-    "\15\0\1\u051e\6\0\1\u0520\1\u0522\1\u0521\2\u0520\2\u0521"+
-    "\1\u0520\1\u0521\1\u0520\1\u051f\175\0\4\u0523\2\0\1\u0523"+
-    "\15\0\1\u0523\6\0\12\u0523\14\0\1\u0307\161\0\4\u0524"+
-    "\2\0\1\u0524\15\0\1\u0524\6\0\12\u0524\1\u050d\13\0"+
-    "\1\u0307\161\0\4\u0523\2\0\1\u0523\15\0\1\u0523\6\0"+
-    "\12\u0523\175\0\1\u0362\4\u0524\2\0\1\u0524\15\0\1\u0524"+
-    "\6\0\12\u0525\1\u050d\13\0\1\u0307\160\0\1\u0362\4\u0524"+
-    "\2\0\1\u0524\15\0\1\u0524\6\0\12\u0524\1\u050d\13\0"+
-    "\1\u0307\160\0\1\u0362\4\u0524\2\0\1\u0524\15\0\1\u0524"+
-    "\6\0\2\u0525\1\u0524\2\u0525\2\u0524\1\u0525\1\u0524\1\u0525"+
-    "\1\u050d\13\0\1\u0307\226\0\1\u04dc\13\0\1\u0307\226\0"+
-    "\1\u04e3\175\0\4\u0526\2\0\1\u0526\15\0\1\u0526\6\0"+
-    "\12\u0526\1\u0514\175\0\4\u0518\2\0\1\u0518\15\0\1\u0518"+
-    "\6\0\12\u0518\1\u04ac\174\0\1\u0314\4\u0526\2\0\1\u0526"+
-    "\15\0\1\u0526\6\0\12\u0527\1\u0514\174\0\1\u0314\4\u0526"+
-    "\2\0\1\u0526\15\0\1\u0526\6\0\12\u0526\1\u0514\174\0"+
-    "\1\u0314\4\u0526\2\0\1\u0526\15\0\1\u0526\6\0\2\u0527"+
-    "\1\u0526\2\u0527\2\u0526\1\u0527\1\u0526\1\u0527\1\u0514\175\0"+
-    "\4\u0528\2\0\1\u0528\15\0\1\u0528\6\0\12\u0528\14\0"+
-    "\1\u02b6\161\0\4\u0529\2\0\1\u0529\15\0\1\u0529\6\0"+
-    "\12\u0529\1\u04ff\13\0\1\u02b6\160\0\1\u0314\4\u0529\2\0"+
-    "\1\u0529\15\0\1\u0529\6\0\12\u0529\1\u04ff\13\0\1\u02b6"+
-    "\160\0\1\334\32\216\1\335\5\216\1\u052a\4\216\175\0"+
-    "\1\363\1\252\1\u0345\30\252\1\127\12\252\1\0\3\124"+
-    "\1\0\2\124\1\125\3\124\3\0\1\124\4\0\2\124"+
-    "\216\0\1\u04f1\175\0\4\u052b\2\0\1\u052b\15\0\1\u052b"+
-    "\6\0\12\u052b\1\u051f\175\0\4\u0523\2\0\1\u0523\15\0"+
-    "\1\u0523\6\0\12\u0523\1\u04c5\174\0\1\u0362\4\u052b\2\0"+
-    "\1\u052b\15\0\1\u052b\6\0\12\u052c\1\u051f\174\0\1\u0362"+
-    "\4\u052b\2\0\1\u052b\15\0\1\u052b\6\0\12\u052b\1\u051f"+
-    "\174\0\1\u0362\4\u052b\2\0\1\u052b\15\0\1\u052b\6\0"+
-    "\2\u052c\1\u052b\2\u052c\2\u052b\1\u052c\1\u052b\1\u052c\1\u051f"+
-    "\175\0\4\u052d\2\0\1\u052d\15\0\1\u052d\6\0\12\u052d"+
-    "\14\0\1\u0307\161\0\4\u052e\2\0\1\u052e\15\0\1\u052e"+
-    "\6\0\12\u052e\1\u050d\13\0\1\u0307\160\0\1\u0362\4\u052e"+
-    "\2\0\1\u052e\15\0\1\u052e\6\0\12\u052e\1\u050d\13\0"+
-    "\1\u0307\161\0\4\u052f\2\0\1\u052f\15\0\1\u052f\6\0"+
-    "\12\u052f\1\u0514\174\0\1\u0314\4\u052f\2\0\1\u052f\15\0"+
-    "\1\u052f\6\0\12\u052f\1\u0514\175\0\4\u0530\2\0\1\u0530"+
-    "\15\0\1\u0530\6\0\12\u0530\14\0\1\u02b6\226\0\1\u04ff"+
-    "\13\0\1\u02b6\160\0\1\334\7\216\1\u0531\22\216\1\335"+
-    "\12\216\176\0\4\u0532\2\0\1\u0532\15\0\1\u0532\6\0"+
-    "\12\u0532\1\u051f\174\0\1\u0362\4\u0532\2\0\1\u0532\15\0"+
-    "\1\u0532\6\0\12\u0532\1\u051f\175\0\4\u0533\2\0\1\u0533"+
-    "\15\0\1\u0533\6\0\12\u0533\14\0\1\u0307\226\0\1\u050d"+
-    "\13\0\1\u0307\226\0\1\u0514\175\0\4\u04ac\2\0\1\u04ac"+
-    "\15\0\1\u04ac\6\0\12\u04ac\14\0\1\u02b6\160\0\1\334"+
-    "\1\216\1\u03d5\30\216\1\335\12\216\243\0\1\u051f\175\0"+
-    "\4\u04c5\2\0\1\u04c5\15\0\1\u04c5\6\0\12\u04c5\14\0"+
-    "\1\u0307\11\0";
-
-  private static int [] zzUnpackTrans() {
-    int [] result = new int[214182];
-    int offset = 0;
-    offset = zzUnpackTrans(ZZ_TRANS_PACKED_0, offset, result);
-    return result;
-  }
-
-  private static int zzUnpackTrans(String packed, int offset, int [] result) {
-    int i = 0;       /* index in packed string  */
-    int j = offset;  /* index in unpacked array */
-    int l = packed.length();
-    while (i < l) {
-      int count = packed.charAt(i++);
-      int value = packed.charAt(i++);
-      value--;
-      do result[j++] = value; while (--count > 0);
-    }
-    return j;
-  }
-
-
-  /* error codes */
-  private static final int ZZ_UNKNOWN_ERROR = 0;
-  private static final int ZZ_NO_MATCH = 1;
-  private static final int ZZ_PUSHBACK_2BIG = 2;
-
-  /* error messages for the codes above */
-  private static final String ZZ_ERROR_MSG[] = {
-    "Unkown internal scanner error",
-    "Error: could not match input",
-    "Error: pushback value was too large"
-  };
-
-  /**
-   * ZZ_ATTRIBUTE[aState] contains the attributes of state <code>aState</code>
-   */
-  private static final int [] ZZ_ATTRIBUTE = zzUnpackAttribute();
-
-  private static final String ZZ_ATTRIBUTE_PACKED_0 =
-    "\1\0\1\11\27\1\2\11\13\1\15\0\1\1\1\0"+
-    "\1\1\10\0\1\1\15\0\1\1\12\0\2\1\1\0"+
-    "\3\1\1\0\1\1\1\0\4\1\53\0\32\1\3\0"+
-    "\4\1\32\0\4\1\17\0\1\11\1\0\23\1\2\0"+
-    "\1\1\1\0\7\1\3\0\2\1\1\0\4\1\1\0"+
-    "\2\1\1\0\2\1\10\0\1\1\32\0\1\1\1\0"+
-    "\11\1\1\0\1\1\2\0\1\1\1\0\1\1\10\0"+
-    "\3\1\15\0\11\1\3\0\2\1\1\0\4\1\1\0"+
-    "\4\1\1\0\2\1\1\0\2\1\1\0\3\1\7\0"+
-    "\2\1\20\0\1\1\10\0\1\1\3\0\1\1\36\0"+
-    "\3\1\23\0\1\1\36\0\1\1\4\0\1\1\6\0"+
-    "\1\1\4\0\2\1\42\0\1\1\57\0\1\1\51\0"+
-    "\1\1\60\0\1\1\140\0\1\1\135\0\1\1\123\0"+
-    "\1\1\106\0\1\1\57\0\1\1\362\0";
-
-  private static int [] zzUnpackAttribute() {
-    int [] result = new int[1331];
-    int offset = 0;
-    offset = zzUnpackAttribute(ZZ_ATTRIBUTE_PACKED_0, offset, result);
-    return result;
-  }
-
-  private static int zzUnpackAttribute(String packed, int offset, int [] result) {
-    int i = 0;       /* index in packed string  */
-    int j = offset;  /* index in unpacked array */
-    int l = packed.length();
-    while (i < l) {
-      int count = packed.charAt(i++);
-      int value = packed.charAt(i++);
-      do result[j++] = value; while (--count > 0);
-    }
-    return j;
-  }
-
-  /** the input device */
-  private java.io.Reader zzReader;
-
-  /** the current state of the DFA */
-  private int zzState;
-
-  /** the current lexical state */
-  private int zzLexicalState = YYINITIAL;
-
-  /** this buffer contains the current text to be matched and is
-      the source of the yytext() string */
-  private char zzBuffer[] = new char[ZZ_BUFFERSIZE];
-
-  /** the textposition at the last accepting state */
-  private int zzMarkedPos;
-
-  /** the current text position in the buffer */
-  private int zzCurrentPos;
-
-  /** startRead marks the beginning of the yytext() string in the buffer */
-  private int zzStartRead;
-
-  /** endRead marks the last character in the buffer, that has been read
-      from input */
-  private int zzEndRead;
-
-  /** number of newlines encountered up to the start of the matched text */
-  private int yyline;
-
-  /** the number of characters up to the start of the matched text */
-  private int yychar;
-
-  /**
-   * the number of characters from the last newline up to the start of the 
-   * matched text
-   */
-  private int yycolumn;
-
-  /** 
-   * zzAtBOL == true <=> the scanner is currently at the beginning of a line
-   */
-  private boolean zzAtBOL = true;
-
-  /** zzAtEOF == true <=> the scanner is at the EOF */
-  private boolean zzAtEOF;
-
-  /** denotes if the user-EOF-code has already been executed */
-  private boolean zzEOFDone;
-
-  /* user code: */
-  /** Alphanumeric sequences */
-  public static final int WORD_TYPE = UAX29URLEmailTokenizer.ALPHANUM;
-  
-  /** Numbers */
-  public static final int NUMERIC_TYPE = UAX29URLEmailTokenizer.NUM;
-  
-  /**
-   * Chars in class \p{Line_Break = Complex_Context} are from South East Asian
-   * scripts (Thai, Lao, Myanmar, Khmer, etc.).  Sequences of these are kept 
-   * together as as a single token rather than broken up, because the logic
-   * required to break them at word boundaries is too complex for UAX#29.
-   * <p>
-   * See Unicode Line Breaking Algorithm: http://www.unicode.org/reports/tr14/#SA
-   */
-  public static final int SOUTH_EAST_ASIAN_TYPE = UAX29URLEmailTokenizer.SOUTHEAST_ASIAN;
-  
-  public static final int IDEOGRAPHIC_TYPE = UAX29URLEmailTokenizer.IDEOGRAPHIC;
-  
-  public static final int HIRAGANA_TYPE = UAX29URLEmailTokenizer.HIRAGANA;
-  
-  public static final int KATAKANA_TYPE = UAX29URLEmailTokenizer.KATAKANA;
-  
-  public static final int HANGUL_TYPE = UAX29URLEmailTokenizer.HANGUL;
-  
-  public static final int EMAIL_TYPE = UAX29URLEmailTokenizer.EMAIL;
-  
-  public static final int URL_TYPE = UAX29URLEmailTokenizer.URL;
-
-  public final int yychar()
-  {
-    return yychar;
-  }
-
-  /**
-   * Fills CharTermAttribute with the current token text.
-   */
-  public final void getText(CharTermAttribute t) {
-    t.copyBuffer(zzBuffer, zzStartRead, zzMarkedPos-zzStartRead);
-  }
-
-
-  /**
-   * Creates a new scanner
-   * There is also a java.io.InputStream version of this constructor.
-   *
-   * @param   in  the java.io.Reader to read input from.
-   */
-  public UAX29URLEmailTokenizerImpl31(java.io.Reader in) {
-    this.zzReader = in;
-  }
-
-  /**
-   * Creates a new scanner.
-   * There is also java.io.Reader version of this constructor.
-   *
-   * @param   in  the java.io.Inputstream to read input from.
-   */
-  public UAX29URLEmailTokenizerImpl31(java.io.InputStream in) {
-    this(new java.io.InputStreamReader(in));
-  }
-
-  /** 
-   * Unpacks the compressed character translation table.
-   *
-   * @param packed   the packed character translation table
-   * @return         the unpacked character translation table
-   */
-  private static char [] zzUnpackCMap(String packed) {
-    char [] map = new char[0x10000];
-    int i = 0;  /* index in packed string  */
-    int j = 0;  /* index in unpacked array */
-    while (i < 2812) {
-      int  count = packed.charAt(i++);
-      char value = packed.charAt(i++);
-      do map[j++] = value; while (--count > 0);
-    }
-    return map;
-  }
-
-
-  /**
-   * Refills the input buffer.
-   *
-   * @return      <code>false</code>, iff there was new input.
-   * 
-   * @exception   java.io.IOException  if any I/O-Error occurs
-   */
-  private boolean zzRefill() throws java.io.IOException {
-
-    /* first: make room (if you can) */
-    if (zzStartRead > 0) {
-      System.arraycopy(zzBuffer, zzStartRead,
-                       zzBuffer, 0,
-                       zzEndRead-zzStartRead);
-
-      /* translate stored positions */
-      zzEndRead-= zzStartRead;
-      zzCurrentPos-= zzStartRead;
-      zzMarkedPos-= zzStartRead;
-      zzStartRead = 0;
-    }
-
-    /* is the buffer big enough? */
-    if (zzCurrentPos >= zzBuffer.length) {
-      /* if not: blow it up */
-      char newBuffer[] = new char[zzCurrentPos*2];
-      System.arraycopy(zzBuffer, 0, newBuffer, 0, zzBuffer.length);
-      zzBuffer = newBuffer;
-    }
-
-    /* finally: fill the buffer with new input */
-    int numRead = zzReader.read(zzBuffer, zzEndRead,
-                                            zzBuffer.length-zzEndRead);
-
-    if (numRead > 0) {
-      zzEndRead+= numRead;
-      return false;
-    }
-    // unlikely but not impossible: read 0 characters, but not at end of stream    
-    if (numRead == 0) {
-      int c = zzReader.read();
-      if (c == -1) {
-        return true;
-      } else {
-        zzBuffer[zzEndRead++] = (char) c;
-        return false;
-      }     
-    }
-
-	// numRead < 0
-    return true;
-  }
-
-    
-  /**
-   * Closes the input stream.
-   */
-  public final void yyclose() throws java.io.IOException {
-    zzAtEOF = true;            /* indicate end of file */
-    zzEndRead = zzStartRead;  /* invalidate buffer    */
-
-    if (zzReader != null)
-      zzReader.close();
-  }
-
-
-  /**
-   * Resets the scanner to read from a new input stream.
-   * Does not close the old reader.
-   *
-   * All internal variables are reset, the old input stream 
-   * <b>cannot</b> be reused (internal buffer is discarded and lost).
-   * Lexical state is set to <tt>ZZ_INITIAL</tt>.
-   *
-   * Internal scan buffer is resized down to its initial length, if it has grown.
-   *
-   * @param reader   the new input stream 
-   */
-  public final void yyreset(java.io.Reader reader) {
-    zzReader = reader;
-    zzAtBOL  = true;
-    zzAtEOF  = false;
-    zzEOFDone = false;
-    zzEndRead = zzStartRead = 0;
-    zzCurrentPos = zzMarkedPos = 0;
-    yyline = yychar = yycolumn = 0;
-    zzLexicalState = YYINITIAL;
-    if (zzBuffer.length > ZZ_BUFFERSIZE)
-      zzBuffer = new char[ZZ_BUFFERSIZE];
-  }
-
-
-  /**
-   * Returns the current lexical state.
-   */
-  public final int yystate() {
-    return zzLexicalState;
-  }
-
-
-  /**
-   * Enters a new lexical state
-   *
-   * @param newState the new lexical state
-   */
-  public final void yybegin(int newState) {
-    zzLexicalState = newState;
-  }
-
-
-  /**
-   * Returns the text matched by the current regular expression.
-   */
-  public final String yytext() {
-    return new String( zzBuffer, zzStartRead, zzMarkedPos-zzStartRead );
-  }
-
-
-  /**
-   * Returns the character at position <tt>pos</tt> from the 
-   * matched text. 
-   * 
-   * It is equivalent to yytext().charAt(pos), but faster
-   *
-   * @param pos the position of the character to fetch. 
-   *            A value from 0 to yylength()-1.
-   *
-   * @return the character at position pos
-   */
-  public final char yycharat(int pos) {
-    return zzBuffer[zzStartRead+pos];
-  }
-
-
-  /**
-   * Returns the length of the matched text region.
-   */
-  public final int yylength() {
-    return zzMarkedPos-zzStartRead;
-  }
-
-
-  /**
-   * Reports an error that occured while scanning.
-   *
-   * In a wellformed scanner (no or only correct usage of 
-   * yypushback(int) and a match-all fallback rule) this method 
-   * will only be called with things that "Can't Possibly Happen".
-   * If this method is called, something is seriously wrong
-   * (e.g. a JFlex bug producing a faulty scanner etc.).
-   *
-   * Usual syntax/scanner level error handling should be done
-   * in error fallback rules.
-   *
-   * @param   errorCode  the code of the errormessage to display
-   */
-  private void zzScanError(int errorCode) {
-    String message;
-    try {
-      message = ZZ_ERROR_MSG[errorCode];
-    }
-    catch (ArrayIndexOutOfBoundsException e) {
-      message = ZZ_ERROR_MSG[ZZ_UNKNOWN_ERROR];
-    }
-
-    throw new Error(message);
-  } 
-
-
-  /**
-   * Pushes the specified amount of characters back into the input stream.
-   *
-   * They will be read again by then next call of the scanning method
-   *
-   * @param number  the number of characters to be read again.
-   *                This number must not be greater than yylength()!
-   */
-  public void yypushback(int number)  {
-    if ( number > yylength() )
-      zzScanError(ZZ_PUSHBACK_2BIG);
-
-    zzMarkedPos -= number;
-  }
-
-
-  /**
-   * Resumes scanning until the next regular expression is matched,
-   * the end of input is encountered or an I/O-Error occurs.
-   *
-   * @return      the next token
-   * @exception   java.io.IOException  if any I/O-Error occurs
-   */
-  public int getNextToken() throws java.io.IOException {
-    int zzInput;
-    int zzAction;
-
-    // cached fields:
-    int zzCurrentPosL;
-    int zzMarkedPosL;
-    int zzEndReadL = zzEndRead;
-    char [] zzBufferL = zzBuffer;
-    char [] zzCMapL = ZZ_CMAP;
-
-    int [] zzTransL = ZZ_TRANS;
-    int [] zzRowMapL = ZZ_ROWMAP;
-    int [] zzAttrL = ZZ_ATTRIBUTE;
-
-    while (true) {
-      zzMarkedPosL = zzMarkedPos;
-
-      yychar+= zzMarkedPosL-zzStartRead;
-
-      zzAction = -1;
-
-      zzCurrentPosL = zzCurrentPos = zzStartRead = zzMarkedPosL;
-  
-      zzState = ZZ_LEXSTATE[zzLexicalState];
-
-      // set up zzAction for empty match case:
-      int zzAttributes = zzAttrL[zzState];
-      if ( (zzAttributes & 1) == 1 ) {
-        zzAction = zzState;
-      }
-
-
-      zzForAction: {
-        while (true) {
-    
-          if (zzCurrentPosL < zzEndReadL)
-            zzInput = zzBufferL[zzCurrentPosL++];
-          else if (zzAtEOF) {
-            zzInput = YYEOF;
-            break zzForAction;
-          }
-          else {
-            // store back cached positions
-            zzCurrentPos  = zzCurrentPosL;
-            zzMarkedPos   = zzMarkedPosL;
-            boolean eof = zzRefill();
-            // get translated positions and possibly new buffer
-            zzCurrentPosL  = zzCurrentPos;
-            zzMarkedPosL   = zzMarkedPos;
-            zzBufferL      = zzBuffer;
-            zzEndReadL     = zzEndRead;
-            if (eof) {
-              zzInput = YYEOF;
-              break zzForAction;
-            }
-            else {
-              zzInput = zzBufferL[zzCurrentPosL++];
-            }
-          }
-          int zzNext = zzTransL[ zzRowMapL[zzState] + zzCMapL[zzInput] ];
-          if (zzNext == -1) break zzForAction;
-          zzState = zzNext;
-
-          zzAttributes = zzAttrL[zzState];
-          if ( (zzAttributes & 1) == 1 ) {
-            zzAction = zzState;
-            zzMarkedPosL = zzCurrentPosL;
-            if ( (zzAttributes & 8) == 8 ) break zzForAction;
-          }
-
-        }
-      }
-
-      // store back cached position
-      zzMarkedPos = zzMarkedPosL;
-
-      switch (zzAction < 0 ? zzAction : ZZ_ACTION[zzAction]) {
-        case 2: 
-          { return WORD_TYPE;
-          }
-        case 11: break;
-        case 5: 
-          { return SOUTH_EAST_ASIAN_TYPE;
-          }
-        case 12: break;
-        case 1: 
-          { /* Break so we don't hit fall-through warning: */ break;/* Not numeric, word, ideographic, hiragana, or SE Asian -- ignore it. */
-          }
-        case 13: break;
-        case 10: 
-          { return URL_TYPE;
-          }
-        case 14: break;
-        case 9: 
-          { return EMAIL_TYPE;
-          }
-        case 15: break;
-        case 4: 
-          { return KATAKANA_TYPE;
-          }
-        case 16: break;
-        case 6: 
-          { return IDEOGRAPHIC_TYPE;
-          }
-        case 17: break;
-        case 8: 
-          { return HANGUL_TYPE;
-          }
-        case 18: break;
-        case 3: 
-          { return NUMERIC_TYPE;
-          }
-        case 19: break;
-        case 7: 
-          { return HIRAGANA_TYPE;
-          }
-        case 20: break;
-        default: 
-          if (zzInput == YYEOF && zzStartRead == zzCurrentPos) {
-            zzAtEOF = true;
-              {
-                return StandardTokenizerInterface.YYEOF;
-              }
-          } 
-          else {
-            zzScanError(ZZ_NO_MATCH);
-          }
-      }
-    }
-  }
-
-
-}
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/UAX29URLEmailTokenizerImpl31.jflex b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/UAX29URLEmailTokenizerImpl31.jflex
deleted file mode 100644
index bb949a6..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/UAX29URLEmailTokenizerImpl31.jflex
+++ /dev/null
@@ -1,269 +0,0 @@
-package org.apache.lucene.analysis.standard.std31;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.standard.UAX29URLEmailTokenizer;
-import org.apache.lucene.analysis.standard.StandardTokenizerInterface;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-
-/**
- * This class implements UAX29URLEmailTokenizer, except with a bug 
- * (https://issues.apache.org/jira/browse/LUCENE-3358) where Han and Hiragana
- * characters would be split from combining characters:
- * @deprecated This class is only for exact backwards compatibility
- */
- @Deprecated
-%%
-
-%unicode 6.0
-%integer
-%final
-%public
-%class UAX29URLEmailTokenizerImpl31
-%implements StandardTokenizerInterface
-%function getNextToken
-%char
-
-%include src/java/org/apache/lucene/analysis/standard/std31/SUPPLEMENTARY.jflex-macro
-ALetter = ([\p{WB:ALetter}] | {ALetterSupp})
-Format =  ([\p{WB:Format}] | {FormatSupp})
-Numeric = ([\p{WB:Numeric}] | {NumericSupp})
-Extend =  ([\p{WB:Extend}] | {ExtendSupp})
-Katakana = ([\p{WB:Katakana}] | {KatakanaSupp})
-MidLetter = ([\p{WB:MidLetter}] | {MidLetterSupp})
-MidNum = ([\p{WB:MidNum}] | {MidNumSupp})
-MidNumLet = ([\p{WB:MidNumLet}] | {MidNumLetSupp})
-ExtendNumLet = ([\p{WB:ExtendNumLet}] | {ExtendNumLetSupp})
-ComplexContext = ([\p{LB:Complex_Context}] | {ComplexContextSupp})
-Han = ([\p{Script:Han}] | {HanSupp})
-Hiragana = ([\p{Script:Hiragana}] | {HiraganaSupp})
-
-// Script=Hangul & Aletter
-HangulEx       = (!(!\p{Script:Hangul}|!\p{WB:ALetter})) ({Format} | {Extend})*
-// UAX#29 WB4. X (Extend | Format)* --> X
-//
-ALetterEx      = {ALetter}                     ({Format} | {Extend})*
-// TODO: Convert hard-coded full-width numeric range to property intersection (something like [\p{Full-Width}&&\p{Numeric}]) once JFlex supports it
-NumericEx      = ({Numeric} | [\uFF10-\uFF19]) ({Format} | {Extend})*
-KatakanaEx     = {Katakana}                    ({Format} | {Extend})* 
-MidLetterEx    = ({MidLetter} | {MidNumLet})   ({Format} | {Extend})* 
-MidNumericEx   = ({MidNum} | {MidNumLet})      ({Format} | {Extend})*
-ExtendNumLetEx = {ExtendNumLet}                ({Format} | {Extend})*
-
-
-// URL and E-mail syntax specifications:
-//
-//     RFC-952:  DOD INTERNET HOST TABLE SPECIFICATION
-//     RFC-1035: DOMAIN NAMES - IMPLEMENTATION AND SPECIFICATION
-//     RFC-1123: Requirements for Internet Hosts - Application and Support
-//     RFC-1738: Uniform Resource Locators (URL)
-//     RFC-3986: Uniform Resource Identifier (URI): Generic Syntax
-//     RFC-5234: Augmented BNF for Syntax Specifications: ABNF
-//     RFC-5321: Simple Mail Transfer Protocol
-//     RFC-5322: Internet Message Format
-
-%include src/java/org/apache/lucene/analysis/standard/std31/ASCIITLD.jflex-macro
-
-DomainLabel = [A-Za-z0-9] ([-A-Za-z0-9]* [A-Za-z0-9])?
-DomainNameStrict = {DomainLabel} ("." {DomainLabel})* {ASCIITLD}
-DomainNameLoose  = {DomainLabel} ("." {DomainLabel})*
-
-IPv4DecimalOctet = "0"{0,2} [0-9] | "0"? [1-9][0-9] | "1" [0-9][0-9] | "2" ([0-4][0-9] | "5" [0-5])
-IPv4Address  = {IPv4DecimalOctet} ("." {IPv4DecimalOctet}){3} 
-IPv6Hex16Bit = [0-9A-Fa-f]{1,4}
-IPv6LeastSignificant32Bits = {IPv4Address} | ({IPv6Hex16Bit} ":" {IPv6Hex16Bit})
-IPv6Address =                                                  ({IPv6Hex16Bit} ":"){6} {IPv6LeastSignificant32Bits}
-            |                                             "::" ({IPv6Hex16Bit} ":"){5} {IPv6LeastSignificant32Bits}
-            |                            {IPv6Hex16Bit}?  "::" ({IPv6Hex16Bit} ":"){4} {IPv6LeastSignificant32Bits}
-            | (({IPv6Hex16Bit} ":"){0,1} {IPv6Hex16Bit})? "::" ({IPv6Hex16Bit} ":"){3} {IPv6LeastSignificant32Bits}
-            | (({IPv6Hex16Bit} ":"){0,2} {IPv6Hex16Bit})? "::" ({IPv6Hex16Bit} ":"){2} {IPv6LeastSignificant32Bits}
-            | (({IPv6Hex16Bit} ":"){0,3} {IPv6Hex16Bit})? "::"  {IPv6Hex16Bit} ":"     {IPv6LeastSignificant32Bits}
-            | (({IPv6Hex16Bit} ":"){0,4} {IPv6Hex16Bit})? "::"                         {IPv6LeastSignificant32Bits}
-            | (({IPv6Hex16Bit} ":"){0,5} {IPv6Hex16Bit})? "::"                         {IPv6Hex16Bit}
-            | (({IPv6Hex16Bit} ":"){0,6} {IPv6Hex16Bit})? "::"
-
-URIunreserved = [-._~A-Za-z0-9]
-URIpercentEncoded = "%" [0-9A-Fa-f]{2}
-URIsubDelims = [!$&'()*+,;=]
-URIloginSegment = ({URIunreserved} | {URIpercentEncoded} | {URIsubDelims})*
-URIlogin = {URIloginSegment} (":" {URIloginSegment})? "@"
-URIquery    = "?" ({URIunreserved} | {URIpercentEncoded} | {URIsubDelims} | [:@/?])*
-URIfragment = "#" ({URIunreserved} | {URIpercentEncoded} | {URIsubDelims} | [:@/?])*
-URIport = ":" [0-9]{1,5}
-URIhostStrict = ("[" {IPv6Address} "]") | {IPv4Address} | {DomainNameStrict}  
-URIhostLoose  = ("[" {IPv6Address} "]") | {IPv4Address} | {DomainNameLoose} 
-
-URIauthorityStrict =             {URIhostStrict} {URIport}?
-URIauthorityLoose  = {URIlogin}? {URIhostLoose}  {URIport}?
-
-HTTPsegment = ({URIunreserved} | {URIpercentEncoded} | [;:@&=])*
-HTTPpath = ("/" {HTTPsegment})*
-HTTPscheme = [hH][tT][tT][pP][sS]? "://"
-HTTPurlFull = {HTTPscheme} {URIauthorityLoose}  {HTTPpath}? {URIquery}? {URIfragment}?
-// {HTTPurlNoScheme} excludes {URIlogin}, because it could otherwise accept e-mail addresses
-HTTPurlNoScheme =          {URIauthorityStrict} {HTTPpath}? {URIquery}? {URIfragment}?
-HTTPurl = {HTTPurlFull} | {HTTPurlNoScheme}
-
-FTPorFILEsegment = ({URIunreserved} | {URIpercentEncoded} | [?:@&=])*
-FTPorFILEpath = "/" {FTPorFILEsegment} ("/" {FTPorFILEsegment})*
-FTPtype = ";" [tT][yY][pP][eE] "=" [aAiIdD]
-FTPscheme = [fF][tT][pP] "://"
-FTPurl = {FTPscheme} {URIauthorityLoose} {FTPorFILEpath} {FTPtype}? {URIfragment}?
-
-FILEscheme = [fF][iI][lL][eE] "://"
-FILEurl = {FILEscheme} {URIhostLoose}? {FTPorFILEpath} {URIfragment}?
-
-URL = {HTTPurl} | {FTPurl} | {FILEurl}
-
-EMAILquotedString = [\"] ([\u0001-\u0008\u000B\u000C\u000E-\u0021\u0023-\u005B\u005D-\u007E] | [\\] [\u0000-\u007F])* [\"]
-EMAILatomText = [A-Za-z0-9!#$%&'*+-/=?\^_`{|}~]
-EMAILlabel = {EMAILatomText}+ | {EMAILquotedString}
-EMAILlocalPart = {EMAILlabel} ("." {EMAILlabel})*
-EMAILdomainLiteralText = [\u0001-\u0008\u000B\u000C\u000E-\u005A\u005E-\u007F] | [\\] [\u0000-\u007F]
-// DFA minimization allows {IPv6Address} and {IPv4Address} to be included 
-// in the {EMAILbracketedHost} definition without incurring any size penalties, 
-// since {EMAILdomainLiteralText} recognizes all valid IP addresses.
-// The IP address regexes are included in {EMAILbracketedHost} simply as a 
-// reminder that they are acceptable bracketed host forms.
-EMAILbracketedHost = "[" ({EMAILdomainLiteralText}* | {IPv4Address} | [iI][pP][vV] "6:" {IPv6Address}) "]"
-EMAIL = {EMAILlocalPart} "@" ({DomainNameStrict} | {EMAILbracketedHost})
-
-
-%{
-  /** Alphanumeric sequences */
-  public static final int WORD_TYPE = UAX29URLEmailTokenizer.ALPHANUM;
-  
-  /** Numbers */
-  public static final int NUMERIC_TYPE = UAX29URLEmailTokenizer.NUM;
-  
-  /**
-   * Chars in class \p{Line_Break = Complex_Context} are from South East Asian
-   * scripts (Thai, Lao, Myanmar, Khmer, etc.).  Sequences of these are kept 
-   * together as as a single token rather than broken up, because the logic
-   * required to break them at word boundaries is too complex for UAX#29.
-   * <p>
-   * See Unicode Line Breaking Algorithm: http://www.unicode.org/reports/tr14/#SA
-   */
-  public static final int SOUTH_EAST_ASIAN_TYPE = UAX29URLEmailTokenizer.SOUTHEAST_ASIAN;
-  
-  public static final int IDEOGRAPHIC_TYPE = UAX29URLEmailTokenizer.IDEOGRAPHIC;
-  
-  public static final int HIRAGANA_TYPE = UAX29URLEmailTokenizer.HIRAGANA;
-  
-  public static final int KATAKANA_TYPE = UAX29URLEmailTokenizer.KATAKANA;
-  
-  public static final int HANGUL_TYPE = UAX29URLEmailTokenizer.HANGUL;
-  
-  public static final int EMAIL_TYPE = UAX29URLEmailTokenizer.EMAIL;
-  
-  public static final int URL_TYPE = UAX29URLEmailTokenizer.URL;
-
-  public final int yychar()
-  {
-    return yychar;
-  }
-
-  /**
-   * Fills CharTermAttribute with the current token text.
-   */
-  public final void getText(CharTermAttribute t) {
-    t.copyBuffer(zzBuffer, zzStartRead, zzMarkedPos-zzStartRead);
-  }
-%}
-
-%%
-
-// UAX#29 WB1. 	sot 	Ã· 	
-//        WB2. 		Ã· 	eot
-//
-<<EOF>> { return StandardTokenizerInterface.YYEOF; }
-
-{URL}   { return URL_TYPE; }
-{EMAIL} { return EMAIL_TYPE; }
-
-// UAX#29 WB8.   Numeric ? Numeric
-//        WB11.  Numeric (MidNum | MidNumLet) ? Numeric
-//        WB12.  Numeric ? (MidNum | MidNumLet) Numeric
-//        WB13a. (ALetter | Numeric | Katakana | ExtendNumLet) ? ExtendNumLet
-//        WB13b. ExtendNumLet ? (ALetter | Numeric | Katakana)
-//
-{ExtendNumLetEx}* {NumericEx} ({ExtendNumLetEx}+ {NumericEx} 
-                              | {MidNumericEx} {NumericEx} 
-                              | {NumericEx})*
-{ExtendNumLetEx}* 
-  { return NUMERIC_TYPE; }
-
-// subset of the below for typing purposes only!
-{HangulEx}+
-  { return HANGUL_TYPE; }
-
-{KatakanaEx}+
-  { return KATAKANA_TYPE; }
-
-// UAX#29 WB5.   ALetter ? ALetter
-//        WB6.   ALetter ? (MidLetter | MidNumLet) ALetter
-//        WB7.   ALetter (MidLetter | MidNumLet) ? ALetter
-//        WB9.   ALetter ? Numeric
-//        WB10.  Numeric ? ALetter
-//        WB13.  Katakana ? Katakana
-//        WB13a. (ALetter | Numeric | Katakana | ExtendNumLet) ? ExtendNumLet
-//        WB13b. ExtendNumLet ? (ALetter | Numeric | Katakana)
-//
-{ExtendNumLetEx}*  ( {KatakanaEx} ({ExtendNumLetEx}* {KatakanaEx})* 
-                   | ( {NumericEx}  ({ExtendNumLetEx}+ {NumericEx} | {MidNumericEx} {NumericEx} | {NumericEx})*
-                     | {ALetterEx}  ({ExtendNumLetEx}+ {ALetterEx} | {MidLetterEx}  {ALetterEx} | {ALetterEx})* )+ ) 
-({ExtendNumLetEx}+ ( {KatakanaEx} ({ExtendNumLetEx}* {KatakanaEx})* 
-                   | ( {NumericEx}  ({ExtendNumLetEx}+ {NumericEx} | {MidNumericEx} {NumericEx} | {NumericEx})*
-                     | {ALetterEx}  ({ExtendNumLetEx}+ {ALetterEx} | {MidLetterEx}  {ALetterEx} | {ALetterEx})* )+ ) )*
-{ExtendNumLetEx}*  
-  { return WORD_TYPE; }
-
-
-// From UAX #29:
-//
-//    [C]haracters with the Line_Break property values of Contingent_Break (CB), 
-//    Complex_Context (SA/South East Asian), and XX (Unknown) are assigned word 
-//    boundary property values based on criteria outside of the scope of this
-//    annex.  That means that satisfactory treatment of languages like Chinese
-//    or Thai requires special handling.
-// 
-// In Unicode 6.0, only one character has the \p{Line_Break = Contingent_Break}
-// property: U+FFFC ( ï¿? ) OBJECT REPLACEMENT CHARACTER.
-//
-// In the ICU implementation of UAX#29, \p{Line_Break = Complex_Context}
-// character sequences (from South East Asian scripts like Thai, Myanmar, Khmer,
-// Lao, etc.) are kept together.  This grammar does the same below.
-//
-// See also the Unicode Line Breaking Algorithm:
-//
-//    http://www.unicode.org/reports/tr14/#SA
-//
-{ComplexContext}+ { return SOUTH_EAST_ASIAN_TYPE; }
-
-// UAX#29 WB14.  Any Ã· Any
-//
-{Han} { return IDEOGRAPHIC_TYPE; }
-{Hiragana} { return HIRAGANA_TYPE; }
-
-
-// UAX#29 WB3.   CR ? LF
-//        WB3a.  (Newline | CR | LF) Ã·
-//        WB3b.  Ã· (Newline | CR | LF)
-//        WB14.  Any Ã· Any
-//
-[^] { /* Break so we don't hit fall-through warning: */ break;/* Not numeric, word, ideographic, hiragana, or SE Asian -- ignore it. */ }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/package.html b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/package.html
deleted file mode 100644
index 6967e2f..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/package.html
+++ /dev/null
@@ -1,22 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html><head></head>
-<body>
-Backwards-compatible implementation to match {@link org.apache.lucene.util.Version#LUCENE_31}
-</body>
-</html>
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/ASCIITLD.jflex-macro b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/ASCIITLD.jflex-macro
deleted file mode 100644
index c31fae4..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/ASCIITLD.jflex-macro
+++ /dev/null
@@ -1,334 +0,0 @@
-/*
- * Copyright 2001-2005 The Apache Software Foundation.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-// Generated from IANA Root Zone Database <http://www.internic.net/zones/root.zone>
-// file version from Thursday, August 4, 2011 11:34:20 AM UTC
-// generated on Thursday, August 4, 2011 11:46:19 PM UTC
-// by org.apache.lucene.analysis.standard.GenerateJflexTLDMacros
-
-ASCIITLD = "." (
-	  [aA][cC]
-	| [aA][dD]
-	| [aA][eE]
-	| [aA][eE][rR][oO]
-	| [aA][fF]
-	| [aA][gG]
-	| [aA][iI]
-	| [aA][lL]
-	| [aA][mM]
-	| [aA][nN]
-	| [aA][oO]
-	| [aA][qQ]
-	| [aA][rR]
-	| [aA][rR][pP][aA]
-	| [aA][sS]
-	| [aA][sS][iI][aA]
-	| [aA][tT]
-	| [aA][uU]
-	| [aA][wW]
-	| [aA][xX]
-	| [aA][zZ]
-	| [bB][aA]
-	| [bB][bB]
-	| [bB][dD]
-	| [bB][eE]
-	| [bB][fF]
-	| [bB][gG]
-	| [bB][hH]
-	| [bB][iI]
-	| [bB][iI][zZ]
-	| [bB][jJ]
-	| [bB][mM]
-	| [bB][nN]
-	| [bB][oO]
-	| [bB][rR]
-	| [bB][sS]
-	| [bB][tT]
-	| [bB][vV]
-	| [bB][wW]
-	| [bB][yY]
-	| [bB][zZ]
-	| [cC][aA]
-	| [cC][aA][tT]
-	| [cC][cC]
-	| [cC][dD]
-	| [cC][fF]
-	| [cC][gG]
-	| [cC][hH]
-	| [cC][iI]
-	| [cC][kK]
-	| [cC][lL]
-	| [cC][mM]
-	| [cC][nN]
-	| [cC][oO]
-	| [cC][oO][mM]
-	| [cC][oO][oO][pP]
-	| [cC][rR]
-	| [cC][uU]
-	| [cC][vV]
-	| [cC][xX]
-	| [cC][yY]
-	| [cC][zZ]
-	| [dD][eE]
-	| [dD][jJ]
-	| [dD][kK]
-	| [dD][mM]
-	| [dD][oO]
-	| [dD][zZ]
-	| [eE][cC]
-	| [eE][dD][uU]
-	| [eE][eE]
-	| [eE][gG]
-	| [eE][rR]
-	| [eE][sS]
-	| [eE][tT]
-	| [eE][uU]
-	| [fF][iI]
-	| [fF][jJ]
-	| [fF][kK]
-	| [fF][mM]
-	| [fF][oO]
-	| [fF][rR]
-	| [gG][aA]
-	| [gG][bB]
-	| [gG][dD]
-	| [gG][eE]
-	| [gG][fF]
-	| [gG][gG]
-	| [gG][hH]
-	| [gG][iI]
-	| [gG][lL]
-	| [gG][mM]
-	| [gG][nN]
-	| [gG][oO][vV]
-	| [gG][pP]
-	| [gG][qQ]
-	| [gG][rR]
-	| [gG][sS]
-	| [gG][tT]
-	| [gG][uU]
-	| [gG][wW]
-	| [gG][yY]
-	| [hH][kK]
-	| [hH][mM]
-	| [hH][nN]
-	| [hH][rR]
-	| [hH][tT]
-	| [hH][uU]
-	| [iI][dD]
-	| [iI][eE]
-	| [iI][lL]
-	| [iI][mM]
-	| [iI][nN]
-	| [iI][nN][fF][oO]
-	| [iI][nN][tT]
-	| [iI][oO]
-	| [iI][qQ]
-	| [iI][rR]
-	| [iI][sS]
-	| [iI][tT]
-	| [jJ][eE]
-	| [jJ][mM]
-	| [jJ][oO]
-	| [jJ][oO][bB][sS]
-	| [jJ][pP]
-	| [kK][eE]
-	| [kK][gG]
-	| [kK][hH]
-	| [kK][iI]
-	| [kK][mM]
-	| [kK][nN]
-	| [kK][pP]
-	| [kK][rR]
-	| [kK][wW]
-	| [kK][yY]
-	| [kK][zZ]
-	| [lL][aA]
-	| [lL][bB]
-	| [lL][cC]
-	| [lL][iI]
-	| [lL][kK]
-	| [lL][rR]
-	| [lL][sS]
-	| [lL][tT]
-	| [lL][uU]
-	| [lL][vV]
-	| [lL][yY]
-	| [mM][aA]
-	| [mM][cC]
-	| [mM][dD]
-	| [mM][eE]
-	| [mM][gG]
-	| [mM][hH]
-	| [mM][iI][lL]
-	| [mM][kK]
-	| [mM][lL]
-	| [mM][mM]
-	| [mM][nN]
-	| [mM][oO]
-	| [mM][oO][bB][iI]
-	| [mM][pP]
-	| [mM][qQ]
-	| [mM][rR]
-	| [mM][sS]
-	| [mM][tT]
-	| [mM][uU]
-	| [mM][uU][sS][eE][uU][mM]
-	| [mM][vV]
-	| [mM][wW]
-	| [mM][xX]
-	| [mM][yY]
-	| [mM][zZ]
-	| [nN][aA]
-	| [nN][aA][mM][eE]
-	| [nN][cC]
-	| [nN][eE]
-	| [nN][eE][tT]
-	| [nN][fF]
-	| [nN][gG]
-	| [nN][iI]
-	| [nN][lL]
-	| [nN][oO]
-	| [nN][pP]
-	| [nN][rR]
-	| [nN][uU]
-	| [nN][zZ]
-	| [oO][mM]
-	| [oO][rR][gG]
-	| [pP][aA]
-	| [pP][eE]
-	| [pP][fF]
-	| [pP][gG]
-	| [pP][hH]
-	| [pP][kK]
-	| [pP][lL]
-	| [pP][mM]
-	| [pP][nN]
-	| [pP][rR]
-	| [pP][rR][oO]
-	| [pP][sS]
-	| [pP][tT]
-	| [pP][wW]
-	| [pP][yY]
-	| [qQ][aA]
-	| [rR][eE]
-	| [rR][oO]
-	| [rR][sS]
-	| [rR][uU]
-	| [rR][wW]
-	| [sS][aA]
-	| [sS][bB]
-	| [sS][cC]
-	| [sS][dD]
-	| [sS][eE]
-	| [sS][gG]
-	| [sS][hH]
-	| [sS][iI]
-	| [sS][jJ]
-	| [sS][kK]
-	| [sS][lL]
-	| [sS][mM]
-	| [sS][nN]
-	| [sS][oO]
-	| [sS][rR]
-	| [sS][tT]
-	| [sS][uU]
-	| [sS][vV]
-	| [sS][yY]
-	| [sS][zZ]
-	| [tT][cC]
-	| [tT][dD]
-	| [tT][eE][lL]
-	| [tT][fF]
-	| [tT][gG]
-	| [tT][hH]
-	| [tT][jJ]
-	| [tT][kK]
-	| [tT][lL]
-	| [tT][mM]
-	| [tT][nN]
-	| [tT][oO]
-	| [tT][pP]
-	| [tT][rR]
-	| [tT][rR][aA][vV][eE][lL]
-	| [tT][tT]
-	| [tT][vV]
-	| [tT][wW]
-	| [tT][zZ]
-	| [uU][aA]
-	| [uU][gG]
-	| [uU][kK]
-	| [uU][sS]
-	| [uU][yY]
-	| [uU][zZ]
-	| [vV][aA]
-	| [vV][cC]
-	| [vV][eE]
-	| [vV][gG]
-	| [vV][iI]
-	| [vV][nN]
-	| [vV][uU]
-	| [wW][fF]
-	| [wW][sS]
-	| [xX][nN]--0[zZ][wW][mM]56[dD]
-	| [xX][nN]--11[bB]5[bB][sS]3[aA]9[aA][jJ]6[gG]
-	| [xX][nN]--3[eE]0[bB]707[eE]
-	| [xX][nN]--45[bB][rR][jJ]9[cC]
-	| [xX][nN]--80[aA][kK][hH][bB][yY][kK][nN][jJ]4[fF]
-	| [xX][nN]--90[aA]3[aA][cC]
-	| [xX][nN]--9[tT]4[bB]11[yY][iI]5[aA]
-	| [xX][nN]--[cC][lL][cC][hH][cC]0[eE][aA]0[bB]2[gG]2[aA]9[gG][cC][dD]
-	| [xX][nN]--[dD][eE][bB][aA]0[aA][dD]
-	| [xX][nN]--[fF][iI][qQ][sS]8[sS]
-	| [xX][nN]--[fF][iI][qQ][zZ]9[sS]
-	| [xX][nN]--[fF][pP][cC][rR][jJ]9[cC]3[dD]
-	| [xX][nN]--[fF][zZ][cC]2[cC]9[eE]2[cC]
-	| [xX][nN]--[gG]6[wW]251[dD]
-	| [xX][nN]--[gG][eE][cC][rR][jJ]9[cC]
-	| [xX][nN]--[hH]2[bB][rR][jJ]9[cC]
-	| [xX][nN]--[hH][gG][bB][kK]6[aA][jJ]7[fF]53[bB][bB][aA]
-	| [xX][nN]--[hH][lL][cC][jJ]6[aA][yY][aA]9[eE][sS][cC]7[aA]
-	| [xX][nN]--[jJ]6[wW]193[gG]
-	| [xX][nN]--[jJ][xX][aA][lL][pP][dD][lL][pP]
-	| [xX][nN]--[kK][gG][bB][eE][cC][hH][tT][vV]
-	| [xX][nN]--[kK][pP][rR][wW]13[dD]
-	| [xX][nN]--[kK][pP][rR][yY]57[dD]
-	| [xX][nN]--[lL][gG][bB][bB][aA][tT]1[aA][dD]8[jJ]
-	| [xX][nN]--[mM][gG][bB][aA][aA][mM]7[aA]8[hH]
-	| [xX][nN]--[mM][gG][bB][aA][yY][hH]7[gG][pP][aA]
-	| [xX][nN]--[mM][gG][bB][bB][hH]1[aA]71[eE]
-	| [xX][nN]--[mM][gG][bB][cC]0[aA]9[aA][zZ][cC][gG]
-	| [xX][nN]--[mM][gG][bB][eE][rR][pP]4[aA]5[dD]4[aA][rR]
-	| [xX][nN]--[oO]3[cC][wW]4[hH]
-	| [xX][nN]--[oO][gG][bB][pP][fF]8[fF][lL]
-	| [xX][nN]--[pP]1[aA][iI]
-	| [xX][nN]--[pP][gG][bB][sS]0[dD][hH]
-	| [xX][nN]--[sS]9[bB][rR][jJ]9[cC]
-	| [xX][nN]--[wW][gG][bB][hH]1[cC]
-	| [xX][nN]--[wW][gG][bB][lL]6[aA]
-	| [xX][nN]--[xX][kK][cC]2[aA][lL]3[hH][yY][eE]2[aA]
-	| [xX][nN]--[xX][kK][cC]2[dD][lL]3[aA]5[eE][eE]0[hH]
-	| [xX][nN]--[yY][fF][rR][oO]4[iI]67[oO]
-	| [xX][nN]--[yY][gG][bB][iI]2[aA][mM][mM][xX]
-	| [xX][nN]--[zZ][cC][kK][zZ][aA][hH]
-	| [xX][xX][xX]
-	| [yY][eE]
-	| [yY][tT]
-	| [zZ][aA]
-	| [zZ][mM]
-	| [zZ][wW]
-	) "."?   // Accept trailing root (empty) domain
-
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/SUPPLEMENTARY.jflex-macro b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/SUPPLEMENTARY.jflex-macro
deleted file mode 100644
index 53d36f7..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/SUPPLEMENTARY.jflex-macro
+++ /dev/null
@@ -1,125 +0,0 @@
-/*
- * Copyright 2010 The Apache Software Foundation.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-// Generated using ICU4J 4.8.0.0 on Friday, September 30, 2011 4:10:42 PM UTC
-// by org.apache.lucene.analysis.icu.GenerateJFlexSupplementaryMacros
-
-
-ALetterSupp = (
-	  ([\ud80d][\uDC00-\uDC2E])
-	| ([\ud80c][\uDC00-\uDFFF])
-	| ([\ud809][\uDC00-\uDC62])
-	| ([\ud808][\uDC00-\uDF6E])
-	| ([\ud81a][\uDC00-\uDE38])
-	| ([\ud804][\uDC03-\uDC37\uDC83-\uDCAF])
-	| ([\ud835][\uDC00-\uDC54\uDC56-\uDC9C\uDC9E\uDC9F\uDCA2\uDCA5\uDCA6\uDCA9-\uDCAC\uDCAE-\uDCB9\uDCBB\uDCBD-\uDCC3\uDCC5-\uDD05\uDD07-\uDD0A\uDD0D-\uDD14\uDD16-\uDD1C\uDD1E-\uDD39\uDD3B-\uDD3E\uDD40-\uDD44\uDD46\uDD4A-\uDD50\uDD52-\uDEA5\uDEA8-\uDEC0\uDEC2-\uDEDA\uDEDC-\uDEFA\uDEFC-\uDF14\uDF16-\uDF34\uDF36-\uDF4E\uDF50-\uDF6E\uDF70-\uDF88\uDF8A-\uDFA8\uDFAA-\uDFC2\uDFC4-\uDFCB])
-	| ([\ud801][\uDC00-\uDC9D])
-	| ([\ud800][\uDC00-\uDC0B\uDC0D-\uDC26\uDC28-\uDC3A\uDC3C\uDC3D\uDC3F-\uDC4D\uDC50-\uDC5D\uDC80-\uDCFA\uDD40-\uDD74\uDE80-\uDE9C\uDEA0-\uDED0\uDF00-\uDF1E\uDF30-\uDF4A\uDF80-\uDF9D\uDFA0-\uDFC3\uDFC8-\uDFCF\uDFD1-\uDFD5])
-	| ([\ud803][\uDC00-\uDC48])
-	| ([\ud802][\uDC00-\uDC05\uDC08\uDC0A-\uDC35\uDC37\uDC38\uDC3C\uDC3F-\uDC55\uDD00-\uDD15\uDD20-\uDD39\uDE00\uDE10-\uDE13\uDE15-\uDE17\uDE19-\uDE33\uDE60-\uDE7C\uDF00-\uDF35\uDF40-\uDF55\uDF60-\uDF72])
-)
-FormatSupp = (
-	  ([\ud804][\uDCBD])
-	| ([\ud834][\uDD73-\uDD7A])
-	| ([\udb40][\uDC01\uDC20-\uDC7F])
-)
-ExtendSupp = (
-	  ([\ud804][\uDC00-\uDC02\uDC38-\uDC46\uDC80-\uDC82\uDCB0-\uDCBA])
-	| ([\ud834][\uDD65-\uDD69\uDD6D-\uDD72\uDD7B-\uDD82\uDD85-\uDD8B\uDDAA-\uDDAD\uDE42-\uDE44])
-	| ([\ud800][\uDDFD])
-	| ([\udb40][\uDD00-\uDDEF])
-	| ([\ud802][\uDE01-\uDE03\uDE05\uDE06\uDE0C-\uDE0F\uDE38-\uDE3A\uDE3F])
-)
-NumericSupp = (
-	  ([\ud804][\uDC66-\uDC6F])
-	| ([\ud835][\uDFCE-\uDFFF])
-	| ([\ud801][\uDCA0-\uDCA9])
-)
-KatakanaSupp = (
-	  ([\ud82c][\uDC00])
-)
-MidLetterSupp = (
-	  []
-)
-MidNumSupp = (
-	  []
-)
-MidNumLetSupp = (
-	  []
-)
-ExtendNumLetSupp = (
-	  []
-)
-ExtendNumLetSupp = (
-	  []
-)
-ComplexContextSupp = (
-	  []
-)
-HanSupp = (
-	  ([\ud87e][\uDC00-\uDE1D])
-	| ([\ud86b][\uDC00-\uDFFF])
-	| ([\ud86a][\uDC00-\uDFFF])
-	| ([\ud869][\uDC00-\uDED6\uDF00-\uDFFF])
-	| ([\ud868][\uDC00-\uDFFF])
-	| ([\ud86e][\uDC00-\uDC1D])
-	| ([\ud86d][\uDC00-\uDF34\uDF40-\uDFFF])
-	| ([\ud86c][\uDC00-\uDFFF])
-	| ([\ud863][\uDC00-\uDFFF])
-	| ([\ud862][\uDC00-\uDFFF])
-	| ([\ud861][\uDC00-\uDFFF])
-	| ([\ud860][\uDC00-\uDFFF])
-	| ([\ud867][\uDC00-\uDFFF])
-	| ([\ud866][\uDC00-\uDFFF])
-	| ([\ud865][\uDC00-\uDFFF])
-	| ([\ud864][\uDC00-\uDFFF])
-	| ([\ud858][\uDC00-\uDFFF])
-	| ([\ud859][\uDC00-\uDFFF])
-	| ([\ud85a][\uDC00-\uDFFF])
-	| ([\ud85b][\uDC00-\uDFFF])
-	| ([\ud85c][\uDC00-\uDFFF])
-	| ([\ud85d][\uDC00-\uDFFF])
-	| ([\ud85e][\uDC00-\uDFFF])
-	| ([\ud85f][\uDC00-\uDFFF])
-	| ([\ud850][\uDC00-\uDFFF])
-	| ([\ud851][\uDC00-\uDFFF])
-	| ([\ud852][\uDC00-\uDFFF])
-	| ([\ud853][\uDC00-\uDFFF])
-	| ([\ud854][\uDC00-\uDFFF])
-	| ([\ud855][\uDC00-\uDFFF])
-	| ([\ud856][\uDC00-\uDFFF])
-	| ([\ud857][\uDC00-\uDFFF])
-	| ([\ud849][\uDC00-\uDFFF])
-	| ([\ud848][\uDC00-\uDFFF])
-	| ([\ud84b][\uDC00-\uDFFF])
-	| ([\ud84a][\uDC00-\uDFFF])
-	| ([\ud84d][\uDC00-\uDFFF])
-	| ([\ud84c][\uDC00-\uDFFF])
-	| ([\ud84f][\uDC00-\uDFFF])
-	| ([\ud84e][\uDC00-\uDFFF])
-	| ([\ud841][\uDC00-\uDFFF])
-	| ([\ud840][\uDC00-\uDFFF])
-	| ([\ud843][\uDC00-\uDFFF])
-	| ([\ud842][\uDC00-\uDFFF])
-	| ([\ud845][\uDC00-\uDFFF])
-	| ([\ud844][\uDC00-\uDFFF])
-	| ([\ud847][\uDC00-\uDFFF])
-	| ([\ud846][\uDC00-\uDFFF])
-)
-HiraganaSupp = (
-	  ([\ud83c][\uDE00])
-	| ([\ud82c][\uDC01])
-)
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/UAX29URLEmailTokenizerImpl34.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/UAX29URLEmailTokenizerImpl34.java
deleted file mode 100644
index 35832d2..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/UAX29URLEmailTokenizerImpl34.java
+++ /dev/null
@@ -1,3766 +0,0 @@
-/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 3/18/12 12:09 PM */
-
-package org.apache.lucene.analysis.standard.std34;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.standard.StandardTokenizerInterface;
-import org.apache.lucene.analysis.standard.UAX29URLEmailTokenizer;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-
-/**
- * This class implements UAX29URLEmailTokenizer, except with a bug
- * (https://issues.apache.org/jira/browse/LUCENE-3880) where "mailto:"
- * URI scheme prepended to an email address will disrupt recognition
- * of the email address.
- * @deprecated This class is only for exact backwards compatibility
- */
- @Deprecated
-
-/**
- * This class is a scanner generated by 
- * <a href="http://www.jflex.de/">JFlex</a> 1.5.0-SNAPSHOT
- * on 3/18/12 12:09 PM from the specification file
- * <tt>C:/cygwin/home/s/svn/lucene/dev/trunk/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/UAX29URLEmailTokenizerImpl34.jflex</tt>
- */
-public final class UAX29URLEmailTokenizerImpl34 implements StandardTokenizerInterface {
-
-  /** This character denotes the end of file */
-  public static final int YYEOF = -1;
-
-  /** initial size of the lookahead buffer */
-  private static final int ZZ_BUFFERSIZE = 16384;
-
-  /** lexical states */
-  public static final int YYINITIAL = 0;
-
-  /**
-   * ZZ_LEXSTATE[l] is the state in the DFA for the lexical state l
-   * ZZ_LEXSTATE[l+1] is the state in the DFA for the lexical state l
-   *                  at the beginning of a line
-   * l is of the form l = 2*k, k a non negative integer
-   */
-  private static final int ZZ_LEXSTATE[] = { 
-     0, 0
-  };
-
-  /** 
-   * Translates characters to character classes
-   */
-  private static final String ZZ_CMAP_PACKED = 
-    "\1\237\10\235\2\237\2\235\1\237\23\235\1\240\1\234\1\227\1\240"+
-    "\1\220\1\216\1\223\2\221\2\240\1\222\1\202\1\147\1\226\1\203"+
-    "\1\206\1\214\1\207\1\212\1\204\1\205\1\211\1\213\1\210\1\215"+
-    "\1\232\1\235\1\233\1\235\1\225\1\224\1\150\1\174\1\151\1\152"+
-    "\1\153\1\156\1\157\1\175\1\160\1\176\1\201\1\161\1\162\1\163"+
-    "\1\155\1\165\1\164\1\154\1\166\1\167\1\170\1\177\1\171\1\172"+
-    "\1\200\1\173\1\230\1\236\1\231\1\241\1\217\1\241\1\150\1\174"+
-    "\1\151\1\152\1\153\1\156\1\157\1\175\1\160\1\176\1\201\1\161"+
-    "\1\162\1\163\1\155\1\165\1\164\1\154\1\166\1\167\1\170\1\177"+
-    "\1\171\1\172\1\200\1\173\3\241\1\216\1\242\52\0\1\132\2\0"+
-    "\1\133\7\0\1\132\1\0\1\136\2\0\1\132\5\0\27\132\1\0"+
-    "\37\132\1\0\u01ca\132\4\0\14\132\16\0\5\132\7\0\1\132\1\0"+
-    "\1\132\21\0\160\133\5\132\1\0\2\132\2\0\4\132\1\137\7\0"+
-    "\1\132\1\136\3\132\1\0\1\132\1\0\24\132\1\0\123\132\1\0"+
-    "\213\132\1\0\7\133\236\132\11\0\46\132\2\0\1\132\7\0\47\132"+
-    "\1\0\1\137\7\0\55\133\1\0\1\133\1\0\2\133\1\0\2\133"+
-    "\1\0\1\133\10\0\33\132\5\0\4\132\1\136\13\0\4\133\10\0"+
-    "\2\137\2\0\13\133\5\0\53\132\25\133\12\134\1\0\1\134\1\137"+
-    "\1\0\2\132\1\133\143\132\1\0\1\132\7\133\1\133\1\0\6\133"+
-    "\2\132\2\133\1\0\4\133\2\132\12\134\3\132\2\0\1\132\17\0"+
-    "\1\133\1\132\1\133\36\132\33\133\2\0\131\132\13\133\1\132\16\0"+
-    "\12\134\41\132\11\133\2\132\2\0\1\137\1\0\1\132\5\0\26\132"+
-    "\4\133\1\132\11\133\1\132\3\133\1\132\5\133\22\0\31\132\3\133"+
-    "\244\0\4\133\66\132\3\133\1\132\22\133\1\132\7\133\12\132\2\133"+
-    "\2\0\12\134\1\0\7\132\1\0\7\132\1\0\3\133\1\0\10\132"+
-    "\2\0\2\132\2\0\26\132\1\0\7\132\1\0\1\132\3\0\4\132"+
-    "\2\0\1\133\1\132\7\133\2\0\2\133\2\0\3\133\1\132\10\0"+
-    "\1\133\4\0\2\132\1\0\3\132\2\133\2\0\12\134\2\132\17\0"+
-    "\3\133\1\0\6\132\4\0\2\132\2\0\26\132\1\0\7\132\1\0"+
-    "\2\132\1\0\2\132\1\0\2\132\2\0\1\133\1\0\5\133\4\0"+
-    "\2\133\2\0\3\133\3\0\1\133\7\0\4\132\1\0\1\132\7\0"+
-    "\12\134\2\133\3\132\1\133\13\0\3\133\1\0\11\132\1\0\3\132"+
-    "\1\0\26\132\1\0\7\132\1\0\2\132\1\0\5\132\2\0\1\133"+
-    "\1\132\10\133\1\0\3\133\1\0\3\133\2\0\1\132\17\0\2\132"+
-    "\2\133\2\0\12\134\21\0\3\133\1\0\10\132\2\0\2\132\2\0"+
-    "\26\132\1\0\7\132\1\0\2\132\1\0\5\132\2\0\1\133\1\132"+
-    "\7\133\2\0\2\133\2\0\3\133\10\0\2\133\4\0\2\132\1\0"+
-    "\3\132\2\133\2\0\12\134\1\0\1\132\20\0\1\133\1\132\1\0"+
-    "\6\132\3\0\3\132\1\0\4\132\3\0\2\132\1\0\1\132\1\0"+
-    "\2\132\3\0\2\132\3\0\3\132\3\0\14\132\4\0\5\133\3\0"+
-    "\3\133\1\0\4\133\2\0\1\132\6\0\1\133\16\0\12\134\21\0"+
-    "\3\133\1\0\10\132\1\0\3\132\1\0\27\132\1\0\12\132\1\0"+
-    "\5\132\3\0\1\132\7\133\1\0\3\133\1\0\4\133\7\0\2\133"+
-    "\1\0\2\132\6\0\2\132\2\133\2\0\12\134\22\0\2\133\1\0"+
-    "\10\132\1\0\3\132\1\0\27\132\1\0\12\132\1\0\5\132\2\0"+
-    "\1\133\1\132\7\133\1\0\3\133\1\0\4\133\7\0\2\133\7\0"+
-    "\1\132\1\0\2\132\2\133\2\0\12\134\1\0\2\132\17\0\2\133"+
-    "\1\0\10\132\1\0\3\132\1\0\51\132\2\0\1\132\7\133\1\0"+
-    "\3\133\1\0\4\133\1\132\10\0\1\133\10\0\2\132\2\133\2\0"+
-    "\12\134\12\0\6\132\2\0\2\133\1\0\22\132\3\0\30\132\1\0"+
-    "\11\132\1\0\1\132\2\0\7\132\3\0\1\133\4\0\6\133\1\0"+
-    "\1\133\1\0\10\133\22\0\2\133\15\0\60\142\1\143\2\142\7\143"+
-    "\5\0\7\142\10\143\1\0\12\134\47\0\2\142\1\0\1\142\2\0"+
-    "\2\142\1\0\1\142\2\0\1\142\6\0\4\142\1\0\7\142\1\0"+
-    "\3\142\1\0\1\142\1\0\1\142\2\0\2\142\1\0\4\142\1\143"+
-    "\2\142\6\143\1\0\2\143\1\142\2\0\5\142\1\0\1\142\1\0"+
-    "\6\143\2\0\12\134\2\0\2\142\42\0\1\132\27\0\2\133\6\0"+
-    "\12\134\13\0\1\133\1\0\1\133\1\0\1\133\4\0\2\133\10\132"+
-    "\1\0\44\132\4\0\24\133\1\0\2\133\5\132\13\133\1\0\44\133"+
-    "\11\0\1\133\71\0\53\142\24\143\1\142\12\134\6\0\6\142\4\143"+
-    "\4\142\3\143\1\142\3\143\2\142\7\143\3\142\4\143\15\142\14\143"+
-    "\1\142\1\143\12\134\4\143\2\142\46\132\12\0\53\132\1\0\1\132"+
-    "\3\0\u0100\146\111\132\1\0\4\132\2\0\7\132\1\0\1\132\1\0"+
-    "\4\132\2\0\51\132\1\0\4\132\2\0\41\132\1\0\4\132\2\0"+
-    "\7\132\1\0\1\132\1\0\4\132\2\0\17\132\1\0\71\132\1\0"+
-    "\4\132\2\0\103\132\2\0\3\133\40\0\20\132\20\0\125\132\14\0"+
-    "\u026c\132\2\0\21\132\1\0\32\132\5\0\113\132\3\0\3\132\17\0"+
-    "\15\132\1\0\4\132\3\133\13\0\22\132\3\133\13\0\22\132\2\133"+
-    "\14\0\15\132\1\0\3\132\1\0\2\133\14\0\64\142\2\143\36\143"+
-    "\3\0\1\142\4\0\1\142\1\143\2\0\12\134\41\0\3\133\2\0"+
-    "\12\134\6\0\130\132\10\0\51\132\1\133\1\132\5\0\106\132\12\0"+
-    "\35\132\3\0\14\133\4\0\14\133\12\0\12\134\36\142\2\0\5\142"+
-    "\13\0\54\142\4\0\21\143\7\142\2\143\6\0\12\134\1\142\3\0"+
-    "\2\142\40\0\27\132\5\133\4\0\65\142\12\143\1\0\35\143\2\0"+
-    "\1\133\12\134\6\0\12\134\6\0\16\142\122\0\5\133\57\132\21\133"+
-    "\7\132\4\0\12\134\21\0\11\133\14\0\3\133\36\132\12\133\3\0"+
-    "\2\132\12\134\6\0\46\132\16\133\14\0\44\132\24\133\10\0\12\134"+
-    "\3\0\3\132\12\134\44\132\122\0\3\133\1\0\25\133\4\132\1\133"+
-    "\4\132\1\133\15\0\300\132\47\133\25\0\4\133\u0116\132\2\0\6\132"+
-    "\2\0\46\132\2\0\6\132\2\0\10\132\1\0\1\132\1\0\1\132"+
-    "\1\0\1\132\1\0\37\132\2\0\65\132\1\0\7\132\1\0\1\132"+
-    "\3\0\3\132\1\0\7\132\3\0\4\132\2\0\6\132\4\0\15\132"+
-    "\5\0\3\132\1\0\7\132\17\0\2\133\2\133\10\0\2\140\12\0"+
-    "\1\140\2\0\1\136\2\0\5\133\20\0\2\141\3\0\1\137\17\0"+
-    "\1\141\13\0\5\133\5\0\6\133\1\0\1\132\15\0\1\132\20\0"+
-    "\15\132\63\0\41\133\21\0\1\132\4\0\1\132\2\0\12\132\1\0"+
-    "\1\132\3\0\5\132\6\0\1\132\1\0\1\132\1\0\1\132\1\0"+
-    "\4\132\1\0\13\132\2\0\4\132\5\0\5\132\4\0\1\132\21\0"+
-    "\51\132\u032d\0\64\132\u0716\0\57\132\1\0\57\132\1\0\205\132\6\0"+
-    "\4\132\3\133\16\0\46\132\12\0\66\132\11\0\1\132\17\0\1\133"+
-    "\27\132\11\0\7\132\1\0\7\132\1\0\7\132\1\0\7\132\1\0"+
-    "\7\132\1\0\7\132\1\0\7\132\1\0\7\132\1\0\40\133\57\0"+
-    "\1\132\120\0\32\144\1\0\131\144\14\0\326\144\57\0\1\132\1\0"+
-    "\1\144\31\0\11\144\4\133\2\133\1\0\5\135\2\0\3\144\1\132"+
-    "\1\132\4\0\126\145\2\0\2\133\2\135\3\145\133\135\1\0\4\135"+
-    "\5\0\51\132\3\0\136\146\21\0\33\132\65\0\20\135\37\0\101\0"+
-    "\37\0\121\0\57\135\1\0\130\135\250\0\u19b6\144\112\0\u51cc\144\64\0"+
-    "\u048d\132\103\0\56\132\2\0\u010d\132\3\0\20\132\12\134\2\132\24\0"+
-    "\57\132\4\133\11\0\2\133\1\0\31\132\10\0\120\132\2\133\45\0"+
-    "\11\132\2\0\147\132\2\0\4\132\1\0\2\132\16\0\12\132\120\0"+
-    "\10\132\1\133\3\132\1\133\4\132\1\133\27\132\5\133\30\0\64\132"+
-    "\14\0\2\133\62\132\21\133\13\0\12\134\6\0\22\133\6\132\3\0"+
-    "\1\132\4\0\12\134\34\132\10\133\2\0\27\132\15\133\14\0\35\146"+
-    "\3\0\4\133\57\132\16\133\16\0\1\132\12\134\46\0\51\132\16\133"+
-    "\11\0\3\132\1\133\10\132\2\133\2\0\12\134\6\0\33\142\1\143"+
-    "\4\0\60\142\1\143\1\142\3\143\2\142\2\143\5\142\2\143\1\142"+
-    "\1\143\1\142\30\0\5\142\41\0\6\132\2\0\6\132\2\0\6\132"+
-    "\11\0\7\132\1\0\7\132\221\0\43\132\10\133\1\0\2\133\2\0"+
-    "\12\134\6\0\u2ba4\146\14\0\27\146\4\0\61\146\4\0\1\31\1\25"+
-    "\1\46\1\43\1\13\3\0\1\7\1\5\2\0\1\3\1\1\14\0"+
-    "\1\11\21\0\1\112\7\0\1\65\1\17\6\0\1\130\3\0\1\120"+
-    "\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120"+
-    "\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120"+
-    "\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120"+
-    "\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120\1\120"+
-    "\1\121\1\120\1\120\1\120\1\125\1\123\17\0\1\114\u02c1\0\1\70"+
-    "\277\0\1\113\1\71\1\2\3\124\2\35\1\124\1\35\2\124\1\14"+
-    "\21\124\2\60\7\73\1\72\7\73\7\52\1\15\1\52\1\75\2\45"+
-    "\1\44\1\75\1\45\1\44\10\75\2\63\5\61\2\54\5\61\1\6"+
-    "\10\37\5\21\3\27\12\106\20\27\3\42\32\30\1\26\2\24\2\110"+
-    "\1\111\2\110\2\111\2\110\1\111\3\24\1\16\2\24\12\64\1\74"+
-    "\1\41\1\34\1\64\6\41\1\34\66\41\5\115\6\103\1\51\4\103"+
-    "\2\51\10\103\1\51\7\100\1\12\2\100\32\103\1\12\4\100\1\12"+
-    "\5\102\1\101\1\102\3\101\7\102\1\101\23\102\5\67\3\102\6\67"+
-    "\2\67\6\66\10\66\2\100\7\66\36\100\4\66\102\100\15\115\1\77"+
-    "\2\115\1\131\3\117\1\115\2\117\5\115\4\117\4\116\1\115\3\116"+
-    "\1\115\5\116\26\56\4\23\1\105\2\104\4\122\1\104\2\122\3\76"+
-    "\33\122\35\55\3\122\35\126\3\122\6\126\2\33\31\126\1\33\17\126"+
-    "\6\122\4\22\1\10\37\22\1\10\4\22\25\62\1\127\11\62\21\55"+
-    "\5\62\1\57\12\40\13\62\4\55\1\50\6\55\12\122\17\55\1\47"+
-    "\3\53\15\20\11\36\1\32\24\36\2\20\11\36\1\32\31\36\1\32"+
-    "\4\20\4\36\2\32\2\107\1\4\5\107\52\4\u1900\0\u012e\144\2\0"+
-    "\76\144\2\0\152\144\46\0\7\132\14\0\5\132\5\0\1\132\1\133"+
-    "\12\132\1\0\15\132\1\0\5\132\1\0\1\132\1\0\2\132\1\0"+
-    "\2\132\1\0\154\132\41\0\u016b\132\22\0\100\132\2\0\66\132\50\0"+
-    "\14\132\4\0\20\133\1\137\2\0\1\136\1\137\13\0\7\133\14\0"+
-    "\2\141\30\0\3\141\1\137\1\0\1\140\1\0\1\137\1\136\32\0"+
-    "\5\132\1\0\207\132\2\0\1\133\7\0\1\140\4\0\1\137\1\0"+
-    "\1\140\1\0\12\134\1\136\1\137\5\0\32\132\4\0\1\141\1\0"+
-    "\32\132\13\0\70\135\2\133\37\146\3\0\6\146\2\0\6\146\2\0"+
-    "\6\146\2\0\3\146\34\0\3\133\4\0";
-
-  /** 
-   * Translates characters to character classes
-   */
-  private static final char [] ZZ_CMAP = zzUnpackCMap(ZZ_CMAP_PACKED);
-
-  /** 
-   * Translates DFA states to action switch labels.
-   */
-  private static final int [] ZZ_ACTION = zzUnpackAction();
-
-  private static final String ZZ_ACTION_PACKED_0 =
-    "\1\0\23\1\1\2\1\3\1\4\1\1\1\5\1\6"+
-    "\1\7\1\10\1\1\3\2\3\3\3\1\15\0\1\2"+
-    "\1\0\1\2\10\0\1\3\15\0\1\2\24\0\2\2"+
-    "\1\0\3\2\1\0\1\3\1\0\2\3\1\2\1\3"+
-    "\53\0\32\2\3\0\4\2\32\0\4\3\17\0\1\11"+
-    "\1\0\6\12\3\2\2\12\1\2\4\12\2\2\2\12"+
-    "\2\0\1\2\1\0\1\2\6\12\3\0\2\12\1\0"+
-    "\4\12\2\0\2\12\1\0\2\3\10\0\1\12\32\0"+
-    "\1\12\1\0\3\12\6\2\1\0\1\2\2\0\1\2"+
-    "\1\0\1\12\10\0\3\3\15\0\3\12\6\11\3\0"+
-    "\2\11\1\0\4\11\2\0\2\11\2\12\1\0\2\12"+
-    "\1\0\2\12\1\0\1\12\2\2\7\0\2\3\20\0"+
-    "\1\11\10\0\1\12\3\0\1\2\37\0\3\12\23\0"+
-    "\1\12\40\0\1\12\4\0\1\12\6\0\1\12\4\0"+
-    "\2\12\43\0\1\12\61\0\1\12\53\0\1\12\64\0"+
-    "\1\12\145\0\1\12\143\0\1\12\130\0\1\12\111\0"+
-    "\1\12\63\0\1\12\367\0";
-
-  private static int [] zzUnpackAction() {
-    int [] result = new int[1384];
-    int offset = 0;
-    offset = zzUnpackAction(ZZ_ACTION_PACKED_0, offset, result);
-    return result;
-  }
-
-  private static int zzUnpackAction(String packed, int offset, int [] result) {
-    int i = 0;       /* index in packed string  */
-    int j = offset;  /* index in unpacked array */
-    int l = packed.length();
-    while (i < l) {
-      int count = packed.charAt(i++);
-      int value = packed.charAt(i++);
-      do result[j++] = value; while (--count > 0);
-    }
-    return j;
-  }
-
-
-  /** 
-   * Translates a state to a row index in the transition table
-   */
-  private static final int [] ZZ_ROWMAP = zzUnpackRowMap();
-
-  private static final String ZZ_ROWMAP_PACKED_0 =
-    "\0\0\0\243\0\u0146\0\u01e9\0\u028c\0\u032f\0\u03d2\0\u0475"+
-    "\0\u0518\0\u05bb\0\u065e\0\u0701\0\u07a4\0\u0847\0\u08ea\0\u098d"+
-    "\0\u0a30\0\u0ad3\0\u0b76\0\u0c19\0\u0cbc\0\u0d5f\0\u0e02\0\u0ea5"+
-    "\0\u0f48\0\u0feb\0\u108e\0\u1131\0\u11d4\0\u1277\0\u131a\0\u13bd"+
-    "\0\u1460\0\u1503\0\u15a6\0\u1649\0\u16ec\0\u178f\0\u0146\0\u01e9"+
-    "\0\u028c\0\u032f\0\u03d2\0\u1832\0\u18d5\0\u1978\0\u1a1b\0\u0701"+
-    "\0\u1abe\0\u1b61\0\u1c04\0\u1ca7\0\u1d4a\0\u1ded\0\u1e90\0\u0518"+
-    "\0\u05bb\0\u1f33\0\u1fd6\0\u2079\0\u211c\0\u21bf\0\u2262\0\u2305"+
-    "\0\u23a8\0\u244b\0\u24ee\0\u2591\0\u2634\0\u26d7\0\u277a\0\u281d"+
-    "\0\u28c0\0\u2963\0\u2a06\0\u0ea5\0\u2aa9\0\u2b4c\0\u2bef\0\u2c92"+
-    "\0\u2d35\0\u2dd8\0\u2e7b\0\u2f1e\0\u2fc1\0\u3064\0\u3107\0\u31aa"+
-    "\0\u324d\0\u32f0\0\u3393\0\u3436\0\u34d9\0\u11d4\0\u357c\0\u361f"+
-    "\0\u36c2\0\u3765\0\u3808\0\u38ab\0\u394e\0\u39f1\0\u3a94\0\u3b37"+
-    "\0\u3bda\0\u3c7d\0\u3d20\0\u3dc3\0\u3e66\0\u3f09\0\u1649\0\u3fac"+
-    "\0\u404f\0\u178f\0\u40f2\0\u4195\0\u4238\0\u42db\0\u437e\0\u4421"+
-    "\0\u44c4\0\u4567\0\u460a\0\u46ad\0\u4750\0\u47f3\0\u4896\0\u4939"+
-    "\0\u49dc\0\u4a7f\0\u4b22\0\u4bc5\0\u4c68\0\u4d0b\0\u4dae\0\u4e51"+
-    "\0\u4ef4\0\u4f97\0\u503a\0\u50dd\0\u5180\0\u5223\0\u52c6\0\u5369"+
-    "\0\u540c\0\u54af\0\u5552\0\u55f5\0\u5698\0\u573b\0\u57de\0\u5881"+
-    "\0\u5924\0\u59c7\0\u5a6a\0\u5b0d\0\u5bb0\0\u5c53\0\u5cf6\0\u5d99"+
-    "\0\u5e3c\0\u5edf\0\u5f82\0\u6025\0\u60c8\0\u616b\0\u620e\0\u62b1"+
-    "\0\u6354\0\u63f7\0\u649a\0\u653d\0\u65e0\0\u6683\0\u6726\0\u67c9"+
-    "\0\u686c\0\u690f\0\u69b2\0\u6a55\0\u6af8\0\u6b9b\0\u6c3e\0\u6ce1"+
-    "\0\u6d84\0\u6e27\0\u6eca\0\u6f6d\0\u7010\0\u70b3\0\u7156\0\u71f9"+
-    "\0\u729c\0\u733f\0\u73e2\0\u7485\0\u7528\0\u75cb\0\u766e\0\u7711"+
-    "\0\u77b4\0\u7857\0\u78fa\0\u799d\0\u7a40\0\u7ae3\0\u7b86\0\u7c29"+
-    "\0\u7ccc\0\u7d6f\0\u7e12\0\u7eb5\0\u7f58\0\u7ffb\0\u809e\0\u8141"+
-    "\0\u81e4\0\u8287\0\u832a\0\u83cd\0\u8470\0\u8513\0\u85b6\0\u8659"+
-    "\0\u86fc\0\u879f\0\u8842\0\u88e5\0\u8988\0\u8a2b\0\u8ace\0\243"+
-    "\0\u8b71\0\u8c14\0\u8cb7\0\u8d5a\0\u8dfd\0\u8ea0\0\u8f43\0\u8fe6"+
-    "\0\u9089\0\u912c\0\u91cf\0\u9272\0\u9315\0\u93b8\0\u945b\0\u94fe"+
-    "\0\u95a1\0\u9644\0\u96e7\0\u978a\0\u982d\0\u98d0\0\u9973\0\u9a16"+
-    "\0\u9ab9\0\u9b5c\0\u9bff\0\u9ca2\0\u9d45\0\u9de8\0\u9e8b\0\u9f2e"+
-    "\0\u9fd1\0\ua074\0\ua117\0\ua1ba\0\ua25d\0\ua300\0\ua3a3\0\ua446"+
-    "\0\ua4e9\0\ua58c\0\ua62f\0\ua6d2\0\ua775\0\ua818\0\ua8bb\0\ua95e"+
-    "\0\uaa01\0\uaaa4\0\uab47\0\uabea\0\uac8d\0\uad30\0\uadd3\0\uae76"+
-    "\0\uaf19\0\uafbc\0\ub05f\0\ub102\0\ub1a5\0\ub248\0\ub2eb\0\ub38e"+
-    "\0\ub431\0\ub4d4\0\ub577\0\ub61a\0\ub6bd\0\ub760\0\ub803\0\ub8a6"+
-    "\0\ub949\0\ub9ec\0\uba8f\0\ubb32\0\ubbd5\0\ubc78\0\ubd1b\0\ubdbe"+
-    "\0\ube61\0\ubf04\0\ubfa7\0\uc04a\0\uc0ed\0\uc190\0\uc233\0\uc2d6"+
-    "\0\uc379\0\uc41c\0\uc4bf\0\uc562\0\uc605\0\uc6a8\0\uc74b\0\uc7ee"+
-    "\0\uc891\0\uc934\0\uc9d7\0\uca7a\0\ucb1d\0\ucbc0\0\ucc63\0\ucd06"+
-    "\0\ucda9\0\uce4c\0\uceef\0\ucf92\0\ud035\0\ud0d8\0\ud17b\0\ud21e"+
-    "\0\ud2c1\0\ud364\0\ud407\0\ud4aa\0\ud54d\0\ud5f0\0\ud693\0\ud736"+
-    "\0\ud7d9\0\ud87c\0\ud91f\0\ud9c2\0\uda65\0\udb08\0\udbab\0\udc4e"+
-    "\0\udcf1\0\udd94\0\ude37\0\udeda\0\udf7d\0\ue020\0\ue0c3\0\ue166"+
-    "\0\ue209\0\ue2ac\0\ue34f\0\ue3f2\0\ue495\0\ue538\0\ue5db\0\ue67e"+
-    "\0\ue721\0\ue7c4\0\ue867\0\ue90a\0\ue9ad\0\uea50\0\ueaf3\0\ueb96"+
-    "\0\uec39\0\uecdc\0\ued7f\0\uee22\0\ueec5\0\uef68\0\uf00b\0\uf0ae"+
-    "\0\uf151\0\uf1f4\0\uf297\0\uf33a\0\uf3dd\0\uf480\0\uf523\0\uf5c6"+
-    "\0\uf669\0\uf70c\0\uf7af\0\uf852\0\uf8f5\0\uf998\0\ufa3b\0\ufade"+
-    "\0\ufb81\0\ufc24\0\ufcc7\0\ufd6a\0\ufe0d\0\ufeb0\0\uff53\0\ufff6"+
-    "\1\231\1\u013c\0\u8a2b\1\u01df\1\u0282\1\u0325\1\u03c8\1\u046b"+
-    "\1\u050e\1\u05b1\1\u0654\1\u06f7\1\u079a\1\u083d\1\u08e0\1\u0983"+
-    "\1\u0a26\1\u0ac9\1\u0b6c\1\u0c0f\1\u0cb2\1\u0d55\1\u0df8\1\u0e9b"+
-    "\1\u0f3e\1\u0fe1\1\u1084\1\u1127\1\u11ca\1\u126d\1\u1310\1\u13b3"+
-    "\1\u1456\1\u14f9\1\u159c\1\u163f\1\u16e2\1\u1785\1\u1828\1\u18cb"+
-    "\1\u196e\1\u1a11\1\u1ab4\1\u1b57\1\u1bfa\1\u1c9d\1\u1d40\1\u1de3"+
-    "\1\u1e86\1\u1f29\1\u1fcc\1\u206f\1\u2112\1\u21b5\1\u2258\1\u22fb"+
-    "\1\u239e\1\u2441\1\u24e4\1\u2587\1\u262a\1\u26cd\1\u2770\1\u2813"+
-    "\1\u28b6\1\u2959\1\u29fc\1\u2a9f\1\u2b42\1\u2be5\1\u2c88\1\u2d2b"+
-    "\1\u2dce\1\u2e71\1\u2f14\1\u2fb7\1\u305a\1\u30fd\1\u31a0\1\u3243"+
-    "\1\u32e6\1\u3389\1\u342c\1\u34cf\1\u3572\1\u3615\1\u36b8\1\u375b"+
-    "\1\u37fe\1\u38a1\1\u3944\1\u39e7\1\u3a8a\1\u3b2d\1\u3bd0\1\u3c73"+
-    "\1\u3d16\1\u3db9\1\u3e5c\1\u3eff\1\u3fa2\1\u4045\1\u40e8\1\u418b"+
-    "\1\u422e\1\u42d1\1\u4374\1\u4417\1\u44ba\1\u455d\1\u4600\1\u46a3"+
-    "\1\u4746\1\u47e9\1\u488c\1\u492f\1\u49d2\1\u4a75\0\u16ec\1\u4b18"+
-    "\1\u4bbb\1\u4c5e\1\u4d01\1\u4da4\1\u4e47\1\u4eea\1\u4f8d\1\u5030"+
-    "\1\u50d3\1\u5176\1\u5219\1\u52bc\1\u535f\1\u5402\1\u54a5\1\u5548"+
-    "\1\u55eb\1\u568e\1\u5731\1\u57d4\1\u5877\1\u591a\1\u59bd\1\u5a60"+
-    "\1\u5b03\1\u5ba6\1\u5c49\1\u5cec\1\u5d8f\1\u5e32\1\u5ed5\1\u5f78"+
-    "\1\u601b\1\u60be\1\u6161\1\u6204\1\u62a7\1\u634a\1\u63ed\1\u6490"+
-    "\1\u6533\1\u65d6\1\u6679\1\u671c\1\u67bf\1\u6862\1\u6905\1\u69a8"+
-    "\1\u6a4b\1\u6aee\1\u6b91\1\u6c34\1\u6cd7\1\u6d7a\1\u6e1d\1\u6ec0"+
-    "\1\u6f63\1\u7006\1\u70a9\1\u714c\1\u71ef\1\u7292\1\u7335\1\u73d8"+
-    "\1\u747b\1\u751e\1\u75c1\1\u7664\1\u7707\1\u77aa\1\u784d\1\u78f0"+
-    "\1\u7993\1\u7a36\1\u7ad9\1\u7b7c\1\u7c1f\1\u7cc2\1\u7d65\1\u7e08"+
-    "\1\u7eab\1\u7f4e\1\u7ff1\1\u8094\1\u8137\1\u81da\1\u827d\1\u8320"+
-    "\1\u83c3\1\u8466\1\u8509\1\u85ac\1\u864f\1\u86f2\1\u8795\1\u8838"+
-    "\1\u88db\1\u897e\1\u8a21\1\u8ac4\1\u8b67\1\u8c0a\1\u8cad\1\u8d50"+
-    "\1\u8df3\1\u8e96\1\u8f39\1\u8fdc\1\u907f\1\u9122\1\u91c5\1\u9268"+
-    "\1\u930b\1\u93ae\1\u9451\1\u94f4\1\u9597\1\u963a\1\u96dd\1\u9780"+
-    "\1\u9823\1\u98c6\1\u9969\1\u9a0c\1\u9aaf\1\u9b52\1\u9bf5\1\u9c98"+
-    "\1\u9d3b\1\u9dde\1\u9e81\1\u9f24\1\u9fc7\1\ua06a\1\ua10d\1\ua1b0"+
-    "\1\ua253\1\ua2f6\1\ua399\1\ua43c\1\ua4df\1\ua582\1\ua625\1\ua6c8"+
-    "\1\ua76b\1\ua80e\1\ua8b1\1\ua954\1\ua9f7\1\uaa9a\1\uab3d\1\uabe0"+
-    "\1\uac83\1\uad26\1\uadc9\1\uae6c\1\uaf0f\1\uafb2\1\ub055\1\ub0f8"+
-    "\1\ub19b\1\ub23e\1\ub2e1\1\ub384\1\ub427\1\ub4ca\1\ub56d\1\ub610"+
-    "\1\ub6b3\1\ub756\1\ub7f9\1\ub89c\1\ub93f\1\ub9e2\1\uba85\1\ubb28"+
-    "\1\ubbcb\1\ubc6e\1\ubd11\1\ubdb4\1\ube57\1\ubefa\1\ubf9d\1\uc040"+
-    "\1\uc0e3\1\uc186\1\uc229\1\uc2cc\1\uc36f\1\uc412\1\uc4b5\1\uc558"+
-    "\1\uc5fb\1\uc69e\1\uc741\1\uc7e4\1\uc887\1\uc92a\1\uc9cd\1\uca70"+
-    "\1\ucb13\1\ucbb6\1\ucc59\1\uccfc\1\ucd9f\1\uce42\1\ucee5\1\ucf88"+
-    "\1\ud02b\1\ud0ce\1\ud171\1\ud214\1\ud2b7\1\ud35a\1\ud3fd\1\ud4a0"+
-    "\1\ud543\1\ud5e6\1\ud689\1\ud72c\1\ud7cf\1\ud872\1\ud915\1\ud9b8"+
-    "\1\uda5b\1\udafe\1\udba1\1\udc44\1\udce7\1\udd8a\1\ude2d\1\uded0"+
-    "\1\udf73\1\ue016\1\ue0b9\1\ue15c\1\ue1ff\1\ue2a2\1\ue345\1\ue3e8"+
-    "\1\ue48b\1\ue52e\1\ue5d1\1\ue674\1\ue717\1\ue7ba\1\ue85d\1\ue900"+
-    "\1\ue9a3\1\uea46\1\ueae9\1\ueb8c\1\uec2f\1\uecd2\1\ued75\1\uee18"+
-    "\1\ueebb\1\uef5e\1\uf001\1\uf0a4\1\uf147\1\uf1ea\1\uf28d\1\uf330"+
-    "\1\uf3d3\1\uf476\1\uf519\1\uf5bc\1\uf65f\1\uf702\1\uf7a5\1\uf848"+
-    "\1\uf8eb\1\uf98e\1\ufa31\1\ufad4\1\ufb77\1\ufc1a\1\ufcbd\1\ufd60"+
-    "\1\ufe03\1\ufea6\1\uff49\1\uffec\2\217\2\u0132\2\u01d5\2\u0278"+
-    "\2\u031b\2\u03be\2\u0461\2\u0504\2\u05a7\2\u064a\2\u06ed\2\u0790"+
-    "\2\u0833\2\u08d6\2\u0979\2\u0a1c\2\u0abf\2\u0b62\2\u0c05\2\u0ca8"+
-    "\2\u0d4b\2\u0dee\2\u0e91\2\u0f34\2\u0fd7\2\u107a\2\u111d\2\u11c0"+
-    "\2\u1263\2\u1306\2\u13a9\2\u144c\2\u14ef\2\u1592\2\u1635\2\u16d8"+
-    "\2\u177b\2\u181e\2\u18c1\2\u1964\2\u1a07\2\u1aaa\2\u1b4d\2\u1bf0"+
-    "\2\u1c93\2\u1d36\2\u1dd9\2\u1e7c\2\u1f1f\2\u1fc2\2\u2065\2\u2108"+
-    "\2\u21ab\2\u224e\2\u22f1\2\u2394\2\u2437\2\u24da\2\u257d\2\u2620"+
-    "\2\u26c3\2\u2766\2\u2809\2\u28ac\2\u294f\2\u29f2\2\u2a95\2\u2b38"+
-    "\2\u2bdb\2\u2c7e\2\u2d21\2\u2dc4\2\u2e67\2\u2f0a\2\u2fad\2\u3050"+
-    "\2\u30f3\2\u3196\2\u3239\2\u32dc\2\u337f\2\u3422\2\u34c5\2\u3568"+
-    "\2\u360b\2\u36ae\2\u3751\2\u37f4\2\u3897\2\u393a\2\u39dd\2\u3a80"+
-    "\2\u3b23\2\u3bc6\2\u3c69\2\u3d0c\2\u3daf\2\u3e52\2\u3ef5\2\u3f98"+
-    "\2\u403b\2\u40de\2\u4181\2\u4224\2\u42c7\2\u436a\2\u440d\2\u44b0"+
-    "\2\u4553\2\u45f6\2\u4699\2\u473c\2\u47df\2\u4882\2\u4925\2\u49c8"+
-    "\2\u4a6b\2\u4b0e\2\u4bb1\2\u4c54\2\u4cf7\2\u4d9a\2\u4e3d\2\u4ee0"+
-    "\2\u4f83\2\u5026\2\u50c9\2\u516c\2\u520f\2\u52b2\2\u5355\2\u53f8"+
-    "\2\u549b\2\u553e\2\u55e1\2\u5684\2\u5727\2\u57ca\2\u586d\2\u5910"+
-    "\2\u59b3\2\u5a56\2\u5af9\2\u5b9c\2\u5c3f\2\u5ce2\2\u5d85\2\u5e28"+
-    "\2\u5ecb\2\u5f6e\2\u6011\2\u60b4\2\u6157\2\u61fa\2\u629d\2\u6340"+
-    "\2\u63e3\2\u6486\2\u6529\2\u65cc\2\u666f\2\u6712\2\u67b5\2\u6858"+
-    "\2\u68fb\2\u699e\2\u6a41\2\u6ae4\2\u6b87\2\u6c2a\2\u6ccd\2\u6d70"+
-    "\2\u6e13\2\u6eb6\2\u6f59\2\u6ffc\2\u709f\2\u7142\2\u71e5\2\u7288"+
-    "\2\u732b\2\u73ce\2\u7471\2\u7514\2\u75b7\2\u765a\2\u76fd\2\u77a0"+
-    "\2\u7843\2\u78e6\2\u7989\2\u7a2c\2\u7acf\2\u7b72\2\u7c15\2\u7cb8"+
-    "\2\u7d5b\2\u7dfe\2\u7ea1\2\u7f44\2\u7fe7\2\u808a\2\u812d\2\u81d0"+
-    "\2\u8273\2\u8316\2\u83b9\2\u845c\2\u84ff\2\u85a2\2\u8645\2\u86e8"+
-    "\2\u878b\2\u882e\2\u88d1\2\u8974\2\u8a17\2\u8aba\2\u8b5d\2\u8c00"+
-    "\2\u8ca3\2\u8d46\2\u8de9\2\u8e8c\2\u8f2f\2\u8fd2\2\u9075\2\u9118"+
-    "\2\u91bb\2\u925e\2\u9301\2\u93a4\2\u9447\2\u94ea\2\u958d\2\u9630"+
-    "\2\u96d3\2\u9776\2\u9819\2\u98bc\2\u995f\2\u9a02\2\u9aa5\2\u9b48"+
-    "\2\u9beb\2\u9c8e\2\u9d31\2\u9dd4\2\u9e77\2\u9f1a\2\u9fbd\2\ua060"+
-    "\2\ua103\2\ua1a6\2\ua249\2\ua2ec\2\ua38f\2\ua432\2\ua4d5\2\ua578"+
-    "\2\ua61b\2\ua6be\2\ua761\2\ua804\2\ua8a7\2\ua94a\2\ua9ed\2\uaa90"+
-    "\2\uab33\2\uabd6\2\uac79\2\uad1c\2\uadbf\2\uae62\2\uaf05\2\uafa8"+
-    "\2\ub04b\2\ub0ee\2\ub191\2\ub234\2\ub2d7\2\ub37a\2\ub41d\2\ub4c0"+
-    "\2\ub563\2\ub606\2\ub6a9\2\ub74c\2\ub7ef\2\ub892\2\ub935\2\ub9d8"+
-    "\2\uba7b\2\ubb1e\2\ubbc1\2\ubc64\2\ubd07\2\ubdaa\2\ube4d\2\ubef0"+
-    "\2\ubf93\2\uc036\2\uc0d9\2\uc17c\2\uc21f\2\uc2c2\2\uc365\2\uc408"+
-    "\2\uc4ab\2\uc54e\2\uc5f1\2\uc694\2\uc737\2\uc7da\2\uc87d\2\uc920"+
-    "\2\uc9c3\2\uca66\2\ucb09\2\ucbac\2\ucc4f\2\uccf2\2\ucd95\2\uce38"+
-    "\2\ucedb\2\ucf7e\2\ud021\2\ud0c4\2\ud167\2\ud20a\2\ud2ad\2\ud350"+
-    "\2\ud3f3\2\ud496\2\ud539\2\ud5dc\2\ud67f\2\ud722\2\ud7c5\2\ud868"+
-    "\2\ud90b\2\ud9ae\2\uda51\2\udaf4\2\udb97\2\udc3a\2\udcdd\2\udd80"+
-    "\2\ude23\2\udec6\2\udf69\2\ue00c\2\ue0af\2\ue152\2\ue1f5\2\ue298"+
-    "\2\ue33b\2\ue3de\2\ue481\2\ue524\2\ue5c7\2\ue66a\2\ue70d\2\ue7b0"+
-    "\2\ue853\2\ue8f6\2\ue999\2\uea3c\2\ueadf\2\ueb82\2\uec25\2\uecc8"+
-    "\2\ued6b\2\uee0e\2\ueeb1\2\uef54\2\ueff7\2\uf09a\2\uf13d\2\uf1e0"+
-    "\2\uf283\2\uf326\2\uf3c9\2\uf46c\2\uf50f\2\uf5b2\2\uf655\2\uf6f8"+
-    "\2\uf79b\2\uf83e\2\uf8e1\2\uf984\2\ufa27\2\ufaca\2\ufb6d\2\ufc10"+
-    "\2\ufcb3\2\ufd56\2\ufdf9\2\ufe9c\2\uff3f\2\uffe2\3\205\3\u0128"+
-    "\3\u01cb\3\u026e\3\u0311\3\u03b4\3\u0457\3\u04fa\3\u059d\3\u0640"+
-    "\3\u06e3\3\u0786\3\u0829\3\u08cc\3\u096f\3\u0a12\3\u0ab5\3\u0b58"+
-    "\3\u0bfb\3\u0c9e\3\u0d41\3\u0de4\3\u0e87\3\u0f2a\3\u0fcd\3\u1070"+
-    "\3\u1113\3\u11b6\3\u1259\3\u12fc\3\u139f\3\u1442\3\u14e5\3\u1588"+
-    "\3\u162b\3\u16ce\3\u1771\3\u1814\3\u18b7\3\u195a\3\u19fd\3\u1aa0"+
-    "\3\u1b43\3\u1be6\3\u1c89\3\u1d2c\3\u1dcf\3\u1e72\3\u1f15\3\u1fb8"+
-    "\3\u205b\3\u20fe\3\u21a1\3\u2244\3\u22e7\3\u238a\3\u242d\3\u24d0"+
-    "\3\u2573\3\u2616\3\u26b9\3\u275c\3\u27ff\3\u28a2\3\u2945\3\u29e8"+
-    "\3\u2a8b\3\u2b2e\3\u2bd1\3\u2c74\3\u2d17\3\u2dba\3\u2e5d\3\u2f00"+
-    "\3\u2fa3\3\u3046\3\u30e9\3\u318c\3\u322f\3\u32d2\3\u3375\3\u3418"+
-    "\3\u34bb\3\u355e\3\u3601\3\u36a4\3\u3747\3\u37ea\3\u388d\3\u3930"+
-    "\3\u39d3\3\u3a76\3\u3b19\3\u3bbc\3\u3c5f\3\u3d02\3\u3da5\3\u3e48"+
-    "\3\u3eeb\3\u3f8e\3\u4031\3\u40d4\3\u4177\3\u421a\3\u42bd\3\u4360"+
-    "\3\u4403\3\u44a6\3\u4549\3\u45ec\3\u468f\3\u4732\3\u47d5\3\u4878"+
-    "\3\u491b\3\u49be\3\u4a61\3\u4b04\3\u4ba7\3\u4c4a\3\u4ced\3\u4d90"+
-    "\3\u4e33\3\u4ed6\3\u4f79\3\u501c\3\u50bf\3\u5162\3\u5205\3\u52a8"+
-    "\3\u534b\3\u53ee\3\u5491\3\u5534\3\u55d7\3\u567a\3\u571d\3\u57c0"+
-    "\3\u5863\3\u5906\3\u59a9\3\u5a4c\3\u5aef\3\u5b92\3\u5c35\3\u5cd8"+
-    "\3\u5d7b\3\u5e1e\3\u5ec1\3\u5f64\3\u6007\3\u60aa\3\u614d\3\u61f0"+
-    "\3\u6293\3\u6336\3\u63d9\3\u647c\3\u651f\3\u65c2\3\u6665\3\u6708";
-
-  private static int [] zzUnpackRowMap() {
-    int [] result = new int[1384];
-    int offset = 0;
-    offset = zzUnpackRowMap(ZZ_ROWMAP_PACKED_0, offset, result);
-    return result;
-  }
-
-  private static int zzUnpackRowMap(String packed, int offset, int [] result) {
-    int i = 0;  /* index in packed string  */
-    int j = offset;  /* index in unpacked array */
-    int l = packed.length();
-    while (i < l) {
-      int high = packed.charAt(i++) << 16;
-      result[j++] = high | packed.charAt(i++);
-    }
-    return j;
-  }
-
-  /** 
-   * The transition table of the DFA
-   */
-  private static final int [] ZZ_TRANS = zzUnpackTrans();
-
-  private static final String ZZ_TRANS_PACKED_0 =
-    "\1\2\1\3\1\2\1\4\1\2\1\5\1\2\1\6"+
-    "\1\2\1\7\1\2\1\10\3\2\1\11\5\2\1\12"+
-    "\3\2\1\13\11\2\1\14\2\2\1\15\43\2\1\16"+
-    "\1\2\1\17\3\2\1\20\1\21\1\2\1\22\1\2"+
-    "\1\23\2\2\1\24\1\2\1\25\1\2\1\26\1\27"+
-    "\3\2\1\30\2\31\1\32\1\33\1\34\1\35\6\36"+
-    "\1\37\16\36\1\40\4\36\1\35\1\41\2\42\1\41"+
-    "\5\42\1\43\1\2\1\35\1\44\1\35\1\2\2\35"+
-    "\1\2\3\35\1\45\2\2\1\35\1\46\3\2\2\35"+
-    "\1\2\245\0\1\25\11\0\1\25\20\0\1\25\22\0"+
-    "\1\25\10\0\3\25\17\0\1\25\10\0\1\25\120\0"+
-    "\1\25\1\0\1\25\1\0\1\25\1\0\1\25\1\0"+
-    "\1\25\1\0\3\25\1\0\5\25\1\0\3\25\1\0"+
-    "\11\25\1\0\2\25\1\0\16\25\1\0\2\25\1\0"+
-    "\21\25\1\0\1\25\1\0\3\25\2\0\1\25\1\0"+
-    "\1\25\1\0\2\25\1\0\1\25\113\0\1\25\3\0"+
-    "\1\25\5\0\2\25\3\0\1\25\13\0\1\25\1\0"+
-    "\1\25\4\0\2\25\4\0\1\25\1\0\1\25\3\0"+
-    "\2\25\1\0\1\25\5\0\3\25\1\0\1\25\15\0"+
-    "\1\25\10\0\1\25\120\0\1\25\3\0\1\25\1\0"+
-    "\1\25\1\0\1\25\1\0\3\25\2\0\4\25\1\0"+
-    "\3\25\2\0\3\25\1\0\4\25\1\0\2\25\2\0"+
-    "\3\25\1\0\11\25\1\0\2\25\1\0\16\25\1\0"+
-    "\2\25\1\0\1\25\1\0\3\25\2\0\1\25\1\0"+
-    "\1\25\1\0\2\25\1\0\1\25\113\0\1\25\3\0"+
-    "\1\25\3\0\1\25\1\0\3\25\2\0\1\25\1\0"+
-    "\2\25\1\0\3\25\3\0\2\25\1\0\1\25\1\0"+
-    "\2\25\1\0\2\25\3\0\2\25\1\0\1\25\1\0"+
-    "\1\25\1\0\2\25\1\0\2\25\1\0\2\25\1\0"+
-    "\5\25\1\0\5\25\1\0\2\25\1\0\2\25\1\0"+
-    "\1\25\1\0\3\25\4\0\1\25\4\0\1\25\125\0"+
-    "\3\25\5\0\1\25\1\0\1\25\1\0\1\25\4\0"+
-    "\1\25\14\0\1\25\5\0\1\25\11\0\2\25\12\0"+
-    "\1\26\1\0\2\25\12\0\1\25\120\0\1\25\1\0"+
-    "\1\26\7\0\2\25\2\0\5\25\2\0\2\25\4\0"+
-    "\6\25\1\0\2\25\4\0\5\25\1\0\5\25\1\0"+
-    "\2\25\1\0\3\25\1\0\4\25\1\0\5\25\1\26"+
-    "\1\0\1\25\1\0\1\25\1\0\3\25\2\0\1\25"+
-    "\1\0\1\25\1\0\1\25\2\0\1\25\113\0\1\25"+
-    "\3\0\1\25\5\0\2\25\3\0\1\25\4\0\3\25"+
-    "\4\0\1\25\1\0\1\25\2\0\1\25\1\0\2\25"+
-    "\4\0\1\25\1\0\1\25\3\0\2\25\1\0\1\25"+
-    "\5\0\3\25\1\0\1\25\10\0\1\25\1\0\2\26"+
-    "\1\0\1\25\10\0\1\25\120\0\1\25\3\0\1\25"+
-    "\6\0\2\25\5\0\1\25\1\0\1\25\1\0\1\25"+
-    "\1\0\11\25\2\0\1\25\4\0\1\25\4\0\6\25"+
-    "\2\0\1\25\1\0\1\25\1\0\3\25\3\0\2\25"+
-    "\4\0\3\25\1\0\1\25\10\0\1\25\1\0\2\25"+
-    "\115\0\1\25\11\0\2\25\17\0\1\25\6\0\2\25"+
-    "\4\0\1\25\5\0\1\25\2\0\1\25\5\0\3\25"+
-    "\1\0\1\25\15\0\1\25\10\0\1\25\120\0\1\25"+
-    "\3\0\1\25\5\0\1\25\32\0\15\25\5\0\3\25"+
-    "\1\0\1\25\5\0\1\25\7\0\1\25\2\0\1\25"+
-    "\5\0\1\25\2\0\1\25\1\0\1\25\202\0\1\33"+
-    "\21\0\1\27\131\0\1\32\3\0\1\32\3\0\1\32"+
-    "\1\0\3\32\2\0\1\32\2\0\1\32\1\0\3\32"+
-    "\3\0\2\32\1\0\1\32\1\0\2\32\1\0\2\32"+
-    "\3\0\2\32\1\0\1\32\3\0\2\32\1\0\2\32"+
-    "\1\0\2\32\1\0\5\32\1\0\5\32\2\0\1\32"+
-    "\1\0\2\32\1\0\1\32\1\0\3\32\4\0\1\32"+
-    "\4\0\1\32\113\0\1\32\1\0\1\32\1\0\1\32"+
-    "\1\0\1\32\1\0\1\32\1\0\3\32\1\0\5\32"+
-    "\1\0\3\32\1\0\11\32\1\0\2\32\1\0\16\32"+
-    "\1\0\2\32\1\0\21\32\1\0\1\32\1\0\3\32"+
-    "\2\0\1\32\1\0\1\32\1\0\2\32\1\0\1\32"+
-    "\113\0\1\32\1\0\1\32\1\0\1\32\3\0\1\32"+
-    "\1\0\3\32\1\0\2\32\1\0\2\32\1\0\3\32"+
-    "\1\0\11\32\1\0\2\32\1\0\16\32\1\0\2\32"+
-    "\1\0\21\32\1\0\1\32\1\0\3\32\2\0\1\32"+
-    "\1\0\1\32\1\0\2\32\1\0\1\32\113\0\1\32"+
-    "\11\0\1\32\20\0\1\32\33\0\1\32\21\0\1\32"+
-    "\10\0\1\32\120\0\1\32\1\0\1\32\1\0\1\32"+
-    "\1\0\1\32\1\0\1\32\1\0\3\32\1\0\5\32"+
-    "\1\0\3\32\1\0\6\32\1\0\2\32\1\0\2\32"+
-    "\1\0\10\32\1\0\5\32\1\0\2\32\1\0\21\32"+
-    "\1\0\1\32\1\0\3\32\2\0\1\32\1\0\1\32"+
-    "\1\0\2\32\1\0\1\32\242\0\1\33\112\0\1\47"+
-    "\1\0\1\50\1\0\1\51\1\0\1\52\1\0\1\53"+
-    "\1\0\1\54\3\0\1\55\5\0\1\56\3\0\1\57"+
-    "\11\0\1\60\2\0\1\61\16\0\1\62\2\0\1\63"+
-    "\41\0\2\25\1\64\1\0\1\65\1\0\1\65\1\66"+
-    "\1\0\1\25\2\0\1\25\1\65\32\25\1\0\12\64"+
-    "\1\65\1\0\1\66\3\0\1\65\20\0\1\47\1\0"+
-    "\1\50\1\0\1\51\1\0\1\52\1\0\1\53\1\0"+
-    "\1\67\3\0\1\70\5\0\1\71\3\0\1\72\11\0"+
-    "\1\60\2\0\1\73\16\0\1\74\2\0\1\75\41\0"+
-    "\1\25\2\26\2\0\2\76\1\77\1\0\1\26\2\0"+
-    "\1\25\1\76\32\25\1\0\12\26\2\0\1\77\2\0"+
-    "\2\76\6\0\1\76\23\0\1\100\15\0\1\101\14\0"+
-    "\1\102\16\0\1\103\2\0\1\104\21\0\1\105\20\0"+
-    "\1\27\1\0\1\27\3\0\1\66\1\0\1\27\53\0"+
-    "\1\66\24\0\1\47\1\0\1\50\1\0\1\51\1\0"+
-    "\1\52\1\0\1\53\1\0\1\106\3\0\1\70\5\0"+
-    "\1\71\3\0\1\107\11\0\1\60\2\0\1\110\16\0"+
-    "\1\111\2\0\1\112\21\0\1\113\17\0\1\25\1\114"+
-    "\1\26\1\115\3\0\1\114\1\0\1\114\2\0\1\25"+
-    "\1\0\32\25\1\0\12\26\2\0\1\114\165\0\2\31"+
-    "\112\0\1\116\15\0\1\117\14\0\1\120\16\0\1\121"+
-    "\2\0\1\122\42\0\1\32\7\0\1\32\112\0\1\123"+
-    "\15\0\1\124\14\0\1\125\16\0\1\126\2\0\1\127"+
-    "\42\0\1\33\7\0\1\33\100\0\1\47\1\0\1\50"+
-    "\1\0\1\51\1\0\1\52\1\0\1\53\1\0\1\130"+
-    "\3\0\1\55\5\0\1\56\3\0\1\131\11\0\1\60"+
-    "\2\0\1\132\16\0\1\133\2\0\1\134\41\0\1\25"+
-    "\1\34\1\64\1\0\1\65\1\0\1\65\1\66\1\0"+
-    "\1\34\2\0\1\34\1\65\32\25\1\0\12\64\1\65"+
-    "\1\0\1\66\3\0\1\65\166\0\1\135\45\136\1\0"+
-    "\3\136\1\0\2\136\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\2\0\1\47\1\0\1\50\1\0\1\51\1\0"+
-    "\1\52\1\0\1\53\1\0\1\54\3\0\1\55\5\0"+
-    "\1\56\3\0\1\57\11\0\1\60\2\0\1\61\16\0"+
-    "\1\62\2\0\1\63\41\0\2\25\1\64\1\0\1\65"+
-    "\1\0\1\65\1\66\1\0\1\25\2\0\1\25\1\140"+
-    "\32\36\1\141\12\142\1\65\1\136\1\143\1\136\1\0"+
-    "\1\136\1\144\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\2\0\1\47\1\0\1\50\1\0\1\51\1\0\1\52"+
-    "\1\0\1\53\1\0\1\54\3\0\1\55\5\0\1\56"+
-    "\3\0\1\57\11\0\1\60\2\0\1\61\16\0\1\62"+
-    "\2\0\1\63\41\0\2\25\1\64\1\0\1\65\1\0"+
-    "\1\65\1\66\1\0\1\25\2\0\1\25\1\140\10\36"+
-    "\1\145\6\36\1\146\12\36\1\141\12\142\1\65\1\136"+
-    "\1\143\1\136\1\0\1\136\1\144\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\2\0\1\47\1\0\1\50\1\0"+
-    "\1\51\1\0\1\52\1\0\1\53\1\0\1\54\3\0"+
-    "\1\55\5\0\1\56\3\0\1\57\11\0\1\60\2\0"+
-    "\1\61\16\0\1\62\2\0\1\63\41\0\2\25\1\64"+
-    "\1\0\1\65\1\0\1\65\1\66\1\0\1\25\2\0"+
-    "\1\25\1\140\17\36\1\147\12\36\1\141\12\142\1\65"+
-    "\1\136\1\143\1\136\1\0\1\136\1\144\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\2\0\1\47\1\0\1\50"+
-    "\1\0\1\51\1\0\1\52\1\0\1\53\1\0\1\67"+
-    "\3\0\1\70\5\0\1\71\3\0\1\72\11\0\1\60"+
-    "\2\0\1\73\16\0\1\74\2\0\1\75\41\0\1\25"+
-    "\2\26\2\0\2\76\1\77\1\0\1\26\2\0\1\25"+
-    "\1\150\32\36\1\141\12\42\1\0\1\136\1\151\1\136"+
-    "\1\0\2\152\1\137\3\136\2\0\1\76\1\136\4\0"+
-    "\2\136\2\0\1\47\1\0\1\50\1\0\1\51\1\0"+
-    "\1\52\1\0\1\53\1\0\1\67\3\0\1\70\5\0"+
-    "\1\71\3\0\1\72\11\0\1\60\2\0\1\73\16\0"+
-    "\1\74\2\0\1\75\41\0\1\25\2\26\2\0\2\76"+
-    "\1\77\1\0\1\26\2\0\1\25\1\150\32\36\1\141"+
-    "\12\153\1\0\1\136\1\151\1\136\1\0\2\152\1\137"+
-    "\3\136\2\0\1\76\1\136\4\0\2\136\2\0\1\47"+
-    "\1\0\1\50\1\0\1\51\1\0\1\52\1\0\1\53"+
-    "\1\0\1\67\3\0\1\70\5\0\1\71\3\0\1\72"+
-    "\11\0\1\60\2\0\1\73\16\0\1\74\2\0\1\75"+
-    "\41\0\1\25\2\26\2\0\2\76\1\77\1\0\1\26"+
-    "\2\0\1\25\1\150\32\36\1\141\1\42\1\154\1\153"+
-    "\2\42\2\153\1\42\1\153\1\42\1\0\1\136\1\151"+
-    "\1\136\1\0\2\152\1\137\3\136\2\0\1\76\1\136"+
-    "\4\0\2\136\2\0\1\47\1\0\1\50\1\0\1\51"+
-    "\1\0\1\52\1\0\1\53\1\0\1\106\3\0\1\70"+
-    "\5\0\1\71\3\0\1\107\11\0\1\60\2\0\1\110"+
-    "\16\0\1\111\2\0\1\112\21\0\1\113\17\0\1\25"+
-    "\1\114\1\26\1\115\3\0\1\114\1\0\1\114\2\0"+
-    "\1\25\1\135\32\155\1\136\12\156\1\0\1\136\1\157"+
-    "\1\136\1\0\2\136\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\151\0\4\160\2\0\1\160\15\0\1\160\6\0"+
-    "\12\160\1\161\174\0\65\162\1\163\1\162\1\164\1\0"+
-    "\2\162\3\0\1\25\11\0\3\25\5\0\1\25\1\0"+
-    "\1\25\1\0\1\25\4\0\1\25\4\0\1\25\1\0"+
-    "\2\25\4\0\1\25\5\0\1\25\3\0\1\25\4\0"+
-    "\5\25\10\0\1\64\1\0\2\25\1\0\1\25\10\0"+
-    "\1\25\120\0\1\25\1\0\1\64\7\0\2\25\2\0"+
-    "\5\25\2\0\2\25\4\0\6\25\1\0\2\25\4\0"+
-    "\5\25\1\0\5\25\1\0\2\25\1\0\3\25\1\0"+
-    "\4\25\1\0\5\25\1\64\1\0\1\25\1\0\1\25"+
-    "\1\0\3\25\2\0\1\25\1\0\1\25\1\0\1\25"+
-    "\2\0\1\25\113\0\1\25\3\0\1\25\5\0\2\25"+
-    "\3\0\1\25\4\0\3\25\4\0\1\25\1\0\1\25"+
-    "\2\0\1\25\1\0\2\25\4\0\1\25\1\0\1\25"+
-    "\3\0\2\25\1\0\1\25\5\0\3\25\1\0\1\25"+
-    "\10\0\1\25\1\0\2\64\1\0\1\25\10\0\1\25"+
-    "\120\0\1\25\3\0\1\25\6\0\2\25\5\0\1\25"+
-    "\1\0\1\25\1\0\1\25\1\0\11\25\2\0\1\25"+
-    "\4\0\1\25\4\0\6\25\2\0\1\25\1\0\1\25"+
-    "\1\0\3\25\1\0\1\25\1\0\2\25\4\0\3\25"+
-    "\1\0\1\25\10\0\1\25\1\0\2\25\115\0\1\25"+
-    "\3\0\1\25\5\0\1\25\32\0\15\25\5\0\3\25"+
-    "\1\0\1\25\5\0\3\25\5\0\1\25\2\0\2\25"+
-    "\4\0\1\25\2\0\1\25\1\0\1\25\177\0\2\25"+
-    "\6\0\1\25\152\0\1\25\3\0\1\25\2\0\1\25"+
-    "\3\0\1\25\5\0\1\25\7\0\1\25\4\0\2\25"+
-    "\3\0\2\25\1\0\1\25\4\0\1\25\1\0\1\25"+
-    "\2\0\2\25\1\0\3\25\1\0\1\25\2\0\4\25"+
-    "\2\0\1\25\135\0\1\47\1\0\1\50\1\0\1\51"+
-    "\1\0\1\52\1\0\1\53\1\0\1\165\3\0\1\55"+
-    "\5\0\1\56\3\0\1\166\11\0\1\60\2\0\1\167"+
-    "\16\0\1\170\2\0\1\171\41\0\1\25\2\64\2\0"+
-    "\2\172\1\66\1\0\1\64\2\0\1\25\1\172\32\25"+
-    "\1\0\12\64\2\0\1\66\2\0\2\172\6\0\1\172"+
-    "\11\0\1\47\1\0\1\50\1\0\1\51\1\0\1\52"+
-    "\1\0\1\53\1\0\1\173\3\0\1\174\5\0\1\175"+
-    "\3\0\1\176\11\0\1\60\2\0\1\177\16\0\1\200"+
-    "\2\0\1\201\41\0\1\25\1\65\7\0\1\65\2\0"+
-    "\1\25\1\0\32\25\42\0\1\47\1\0\1\50\1\0"+
-    "\1\51\1\0\1\52\1\0\1\53\1\0\1\202\3\0"+
-    "\1\55\5\0\1\56\3\0\1\203\11\0\1\60\2\0"+
-    "\1\204\16\0\1\205\2\0\1\206\21\0\1\113\17\0"+
-    "\1\25\1\66\1\64\1\115\3\0\1\66\1\0\1\66"+
-    "\2\0\1\25\1\0\32\25\1\0\12\64\2\0\1\66"+
-    "\25\0\1\26\11\0\3\25\5\0\1\25\1\0\1\25"+
-    "\1\0\1\25\4\0\1\25\4\0\1\26\1\0\2\26"+
-    "\4\0\1\25\5\0\1\25\3\0\1\26\4\0\1\26"+
-    "\2\25\2\26\10\0\1\26\1\0\2\25\1\0\1\26"+
-    "\10\0\1\25\120\0\1\25\3\0\1\25\6\0\2\25"+
-    "\5\0\1\25\1\0\1\25\1\0\1\25\1\0\11\25"+
-    "\2\0\1\25\4\0\1\25\4\0\6\25\2\0\1\25"+
-    "\1\0\1\25\1\0\3\25\1\0\1\26\1\0\2\25"+
-    "\4\0\3\25\1\0\1\25\10\0\1\25\1\0\2\25"+
-    "\115\0\1\25\3\0\1\25\5\0\1\25\32\0\15\25"+
-    "\5\0\3\25\1\0\1\25\5\0\1\25\2\26\5\0"+
-    "\1\25\2\0\1\25\1\26\4\0\1\25\2\0\1\25"+
-    "\1\0\1\25\177\0\2\26\6\0\1\26\152\0\1\26"+
-    "\3\0\1\26\2\0\1\26\3\0\1\26\5\0\1\26"+
-    "\7\0\1\26\4\0\2\26\3\0\2\26\1\0\1\26"+
-    "\4\0\1\26\1\0\1\26\2\0\2\26\1\0\3\26"+
-    "\1\0\1\26\2\0\4\26\2\0\1\26\147\0\1\207"+
-    "\3\0\1\210\5\0\1\211\3\0\1\212\14\0\1\213"+
-    "\16\0\1\214\2\0\1\215\42\0\1\76\1\26\6\0"+
-    "\1\76\37\0\12\26\27\0\1\47\1\0\1\50\1\0"+
-    "\1\51\1\0\1\52\1\0\1\53\1\0\1\216\3\0"+
-    "\1\70\5\0\1\71\3\0\1\217\11\0\1\60\2\0"+
-    "\1\220\16\0\1\221\2\0\1\222\21\0\1\113\17\0"+
-    "\1\25\1\77\1\26\1\115\3\0\1\77\1\0\1\77"+
-    "\2\0\1\25\1\0\32\25\1\0\12\26\2\0\1\77"+
-    "\25\0\1\27\37\0\1\27\1\0\2\27\16\0\1\27"+
-    "\4\0\1\27\2\0\2\27\15\0\1\27\226\0\1\27"+
-    "\247\0\2\27\11\0\1\27\211\0\2\27\6\0\1\27"+
-    "\152\0\1\27\3\0\1\27\2\0\1\27\3\0\1\27"+
-    "\5\0\1\27\7\0\1\27\4\0\2\27\3\0\2\27"+
-    "\1\0\1\27\4\0\1\27\1\0\1\27\2\0\2\27"+
-    "\1\0\3\27\1\0\1\27\2\0\4\27\2\0\1\27"+
-    "\247\0\1\27\131\0\1\114\11\0\3\25\5\0\1\25"+
-    "\1\0\1\25\1\0\1\25\4\0\1\25\4\0\1\114"+
-    "\1\0\2\114\4\0\1\25\5\0\1\25\3\0\1\114"+
-    "\4\0\1\114\2\25\2\114\10\0\1\26\1\0\2\25"+
-    "\1\0\1\114\10\0\1\25\120\0\1\25\3\0\1\25"+
-    "\6\0\2\25\5\0\1\25\1\0\1\25\1\0\1\25"+
-    "\1\0\11\25\2\0\1\25\4\0\1\25\4\0\6\25"+
-    "\2\0\1\25\1\0\1\25\1\0\3\25\1\0\1\114"+
-    "\1\0\2\25\4\0\3\25\1\0\1\25\10\0\1\25"+
-    "\1\0\2\25\115\0\1\25\3\0\1\25\5\0\1\25"+
-    "\32\0\15\25\5\0\3\25\1\0\1\25\5\0\1\25"+
-    "\2\114\5\0\1\25\2\0\1\25\1\114\4\0\1\25"+
-    "\2\0\1\25\1\0\1\25\177\0\2\114\6\0\1\114"+
-    "\152\0\1\114\3\0\1\114\2\0\1\114\3\0\1\114"+
-    "\5\0\1\114\7\0\1\114\4\0\2\114\3\0\2\114"+
-    "\1\0\1\114\4\0\1\114\1\0\1\114\2\0\2\114"+
-    "\1\0\3\114\1\0\1\114\2\0\4\114\2\0\1\114"+
-    "\247\0\1\115\142\0\1\223\15\0\1\224\14\0\1\225"+
-    "\16\0\1\226\2\0\1\227\21\0\1\113\20\0\1\115"+
-    "\1\0\1\115\3\0\1\66\1\0\1\115\53\0\1\66"+
-    "\25\0\1\32\37\0\1\32\1\0\2\32\16\0\1\32"+
-    "\4\0\1\32\2\0\2\32\15\0\1\32\226\0\1\32"+
-    "\247\0\2\32\11\0\1\32\211\0\2\32\6\0\1\32"+
-    "\152\0\1\32\3\0\1\32\2\0\1\32\3\0\1\32"+
-    "\5\0\1\32\7\0\1\32\4\0\2\32\3\0\2\32"+
-    "\1\0\1\32\4\0\1\32\1\0\1\32\2\0\2\32"+
-    "\1\0\3\32\1\0\1\32\2\0\4\32\2\0\1\32"+
-    "\136\0\1\33\37\0\1\33\1\0\2\33\16\0\1\33"+
-    "\4\0\1\33\2\0\2\33\15\0\1\33\226\0\1\33"+
-    "\247\0\2\33\11\0\1\33\211\0\2\33\6\0\1\33"+
-    "\152\0\1\33\3\0\1\33\2\0\1\33\3\0\1\33"+
-    "\5\0\1\33\7\0\1\33\4\0\2\33\3\0\2\33"+
-    "\1\0\1\33\4\0\1\33\1\0\1\33\2\0\2\33"+
-    "\1\0\3\33\1\0\1\33\2\0\4\33\2\0\1\33"+
-    "\136\0\1\34\11\0\3\25\5\0\1\25\1\0\1\25"+
-    "\1\0\1\25\4\0\1\25\4\0\1\34\1\0\2\34"+
-    "\4\0\1\25\5\0\1\25\3\0\1\34\4\0\1\34"+
-    "\2\25\2\34\10\0\1\64\1\0\2\25\1\0\1\34"+
-    "\10\0\1\25\120\0\1\25\3\0\1\25\6\0\2\25"+
-    "\5\0\1\25\1\0\1\25\1\0\1\25\1\0\11\25"+
-    "\2\0\1\25\4\0\1\25\4\0\6\25\2\0\1\25"+
-    "\1\0\1\25\1\0\3\25\1\0\1\34\1\0\2\25"+
-    "\4\0\3\25\1\0\1\25\10\0\1\25\1\0\2\25"+
-    "\115\0\1\25\3\0\1\25\5\0\1\25\32\0\15\25"+
-    "\5\0\3\25\1\0\1\25\5\0\1\25\2\34\5\0"+
-    "\1\25\2\0\1\25\1\34\4\0\1\25\2\0\1\25"+
-    "\1\0\1\25\177\0\2\34\6\0\1\34\152\0\1\34"+
-    "\3\0\1\34\2\0\1\34\3\0\1\34\5\0\1\34"+
-    "\7\0\1\34\4\0\2\34\3\0\2\34\1\0\1\34"+
-    "\4\0\1\34\1\0\1\34\2\0\2\34\1\0\3\34"+
-    "\1\0\1\34\2\0\4\34\2\0\1\34\303\0\1\135"+
-    "\45\136\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\1\162\3\0\2\136\151\0\32\230\1\0\12\230"+
-    "\13\0\1\231\13\0\1\47\1\0\1\50\1\0\1\51"+
-    "\1\0\1\52\1\0\1\53\1\0\1\173\3\0\1\174"+
-    "\5\0\1\175\3\0\1\176\11\0\1\60\2\0\1\177"+
-    "\16\0\1\200\2\0\1\201\41\0\1\25\1\65\7\0"+
-    "\1\65\2\0\1\25\1\135\1\232\1\233\1\234\1\235"+
-    "\1\236\1\237\1\240\1\241\1\242\1\243\1\244\1\245"+
-    "\1\246\1\247\1\250\1\251\1\252\1\253\1\254\1\255"+
-    "\1\256\1\257\1\260\1\261\1\262\1\263\1\136\12\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\1\162\3\0\2\136\150\0\1\135\32\264\1\141\12\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\2\0\1\47\1\0\1\50\1\0\1\51"+
-    "\1\0\1\52\1\0\1\53\1\0\1\165\3\0\1\55"+
-    "\5\0\1\56\3\0\1\166\11\0\1\60\2\0\1\167"+
-    "\16\0\1\170\2\0\1\171\41\0\1\25\2\64\2\0"+
-    "\2\172\1\66\1\0\1\64\2\0\1\25\1\265\32\36"+
-    "\1\141\12\142\1\0\1\136\1\143\1\136\1\0\2\266"+
-    "\1\137\3\136\2\0\1\172\1\136\4\0\2\136\2\0"+
-    "\1\47\1\0\1\50\1\0\1\51\1\0\1\52\1\0"+
-    "\1\53\1\0\1\202\3\0\1\55\5\0\1\56\3\0"+
-    "\1\203\11\0\1\60\2\0\1\204\16\0\1\205\2\0"+
-    "\1\206\21\0\1\113\17\0\1\25\1\66\1\64\1\115"+
-    "\3\0\1\66\1\0\1\66\2\0\1\25\1\135\32\155"+
-    "\1\136\12\267\1\0\1\136\1\143\1\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\2\0\1\47"+
-    "\1\0\1\50\1\0\1\51\1\0\1\52\1\0\1\53"+
-    "\1\0\1\173\3\0\1\174\5\0\1\175\3\0\1\176"+
-    "\11\0\1\60\2\0\1\177\16\0\1\200\2\0\1\201"+
-    "\41\0\1\25\1\65\7\0\1\65\2\0\1\25\1\135"+
-    "\32\155\13\136\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\2\0\1\47\1\0\1\50"+
-    "\1\0\1\51\1\0\1\52\1\0\1\53\1\0\1\54"+
-    "\3\0\1\55\5\0\1\56\3\0\1\57\11\0\1\60"+
-    "\2\0\1\61\16\0\1\62\2\0\1\63\41\0\2\25"+
-    "\1\64\1\0\1\65\1\0\1\65\1\66\1\0\1\25"+
-    "\2\0\1\25\1\140\11\36\1\270\20\36\1\141\12\142"+
-    "\1\65\1\136\1\143\1\136\1\0\1\136\1\144\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\2\0\1\47\1\0"+
-    "\1\50\1\0\1\51\1\0\1\52\1\0\1\53\1\0"+
-    "\1\54\3\0\1\55\5\0\1\56\3\0\1\57\11\0"+
-    "\1\60\2\0\1\61\16\0\1\62\2\0\1\63\41\0"+
-    "\2\25\1\64\1\0\1\65\1\0\1\65\1\66\1\0"+
-    "\1\25\2\0\1\25\1\140\15\36\1\271\14\36\1\141"+
-    "\12\142\1\65\1\136\1\143\1\136\1\0\1\136\1\144"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\2\0\1\47"+
-    "\1\0\1\50\1\0\1\51\1\0\1\52\1\0\1\53"+
-    "\1\0\1\54\3\0\1\55\5\0\1\56\3\0\1\57"+
-    "\11\0\1\60\2\0\1\61\16\0\1\62\2\0\1\63"+
-    "\41\0\2\25\1\64\1\0\1\65\1\0\1\65\1\66"+
-    "\1\0\1\25\2\0\1\25\1\140\17\36\1\272\12\36"+
-    "\1\141\12\142\1\65\1\136\1\143\1\136\1\0\1\136"+
-    "\1\144\1\137\3\136\3\0\1\136\4\0\2\136\14\0"+
-    "\1\207\3\0\1\210\5\0\1\211\3\0\1\212\14\0"+
-    "\1\213\16\0\1\214\2\0\1\215\42\0\1\76\1\26"+
-    "\6\0\1\76\3\0\1\135\1\273\1\274\1\275\1\276"+
-    "\1\277\1\300\1\301\1\302\1\303\1\304\1\305\1\306"+
-    "\1\307\1\310\1\311\1\312\1\313\1\314\1\315\1\316"+
-    "\1\317\1\320\1\321\1\322\1\323\1\324\1\136\1\325"+
-    "\2\326\1\325\5\326\1\327\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\1\162\3\0\2\136\2\0"+
-    "\1\47\1\0\1\50\1\0\1\51\1\0\1\52\1\0"+
-    "\1\53\1\0\1\216\3\0\1\70\5\0\1\71\3\0"+
-    "\1\217\11\0\1\60\2\0\1\220\16\0\1\221\2\0"+
-    "\1\222\21\0\1\113\17\0\1\25\1\77\1\26\1\115"+
-    "\3\0\1\77\1\0\1\77\2\0\1\25\1\135\32\155"+
-    "\1\136\12\156\1\0\1\136\1\151\1\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\14\0\1\207"+
-    "\3\0\1\210\5\0\1\211\3\0\1\212\14\0\1\213"+
-    "\16\0\1\214\2\0\1\215\42\0\1\76\1\26\6\0"+
-    "\1\76\3\0\1\135\33\136\12\156\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\2\0"+
-    "\1\47\1\0\1\50\1\0\1\51\1\0\1\52\1\0"+
-    "\1\53\1\0\1\67\3\0\1\70\5\0\1\71\3\0"+
-    "\1\72\11\0\1\60\2\0\1\73\16\0\1\74\2\0"+
-    "\1\75\41\0\1\25\2\26\2\0\2\76\1\77\1\0"+
-    "\1\26\2\0\1\25\1\150\32\36\1\141\12\330\1\0"+
-    "\1\136\1\151\1\136\1\0\2\152\1\137\3\136\2\0"+
-    "\1\76\1\136\4\0\2\136\2\0\1\47\1\0\1\50"+
-    "\1\0\1\51\1\0\1\52\1\0\1\53\1\0\1\67"+
-    "\3\0\1\70\5\0\1\71\3\0\1\72\11\0\1\60"+
-    "\2\0\1\73\16\0\1\74\2\0\1\75\41\0\1\25"+
-    "\2\26\2\0\2\76\1\77\1\0\1\26\2\0\1\25"+
-    "\1\150\32\36\1\141\2\153\1\330\2\153\2\330\1\153"+
-    "\1\330\1\153\1\0\1\136\1\151\1\136\1\0\2\152"+
-    "\1\137\3\136\2\0\1\76\1\136\4\0\2\136\2\0"+
-    "\1\47\1\0\1\50\1\0\1\51\1\0\1\52\1\0"+
-    "\1\53\1\0\1\54\3\0\1\55\5\0\1\56\3\0"+
-    "\1\57\11\0\1\60\2\0\1\61\16\0\1\62\2\0"+
-    "\1\63\41\0\2\25\1\64\1\0\1\65\1\0\1\65"+
-    "\1\66\1\0\1\25\2\0\1\25\1\331\32\155\1\136"+
-    "\12\267\1\65\1\136\1\143\1\136\1\0\1\136\1\144"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\2\0\1\47"+
-    "\1\0\1\50\1\0\1\51\1\0\1\52\1\0\1\53"+
-    "\1\0\1\67\3\0\1\70\5\0\1\71\3\0\1\72"+
-    "\11\0\1\60\2\0\1\73\16\0\1\74\2\0\1\75"+
-    "\41\0\1\25\2\26\2\0\2\76\1\77\1\0\1\26"+
-    "\2\0\1\25\1\332\32\155\1\136\12\156\1\0\1\136"+
-    "\1\151\1\136\1\0\2\152\1\137\3\136\2\0\1\76"+
-    "\1\136\4\0\2\136\151\0\4\333\2\0\1\333\15\0"+
-    "\1\333\6\0\12\333\1\334\242\0\1\335\174\0\1\336"+
-    "\54\0\1\137\165\0\74\162\2\0\1\64\11\0\3\25"+
-    "\5\0\1\25\1\0\1\25\1\0\1\25\4\0\1\25"+
-    "\4\0\1\64\1\0\2\64\4\0\1\25\5\0\1\25"+
-    "\3\0\1\64\4\0\1\64\2\25\2\64\10\0\1\64"+
-    "\1\0\2\25\1\0\1\64\10\0\1\25\120\0\1\25"+
-    "\3\0\1\25\6\0\2\25\5\0\1\25\1\0\1\25"+
-    "\1\0\1\25\1\0\11\25\2\0\1\25\4\0\1\25"+
-    "\4\0\6\25\2\0\1\25\1\0\1\25\1\0\3\25"+
-    "\1\0\1\64\1\0\2\25\4\0\3\25\1\0\1\25"+
-    "\10\0\1\25\1\0\2\25\115\0\1\25\3\0\1\25"+
-    "\5\0\1\25\32\0\15\25\5\0\3\25\1\0\1\25"+
-    "\5\0\1\25\2\64\5\0\1\25\2\0\1\25\1\64"+
-    "\4\0\1\25\2\0\1\25\1\0\1\25\177\0\2\64"+
-    "\6\0\1\64\152\0\1\64\3\0\1\64\2\0\1\64"+
-    "\3\0\1\64\5\0\1\64\7\0\1\64\4\0\2\64"+
-    "\3\0\2\64\1\0\1\64\4\0\1\64\1\0\1\64"+
-    "\2\0\2\64\1\0\3\64\1\0\1\64\2\0\4\64"+
-    "\2\0\1\64\147\0\1\337\3\0\1\340\5\0\1\341"+
-    "\3\0\1\342\14\0\1\343\16\0\1\344\2\0\1\345"+
-    "\42\0\1\172\1\64\6\0\1\172\37\0\12\64\30\0"+
-    "\1\65\11\0\3\25\5\0\1\25\1\0\1\25\1\0"+
-    "\1\25\4\0\1\25\4\0\1\65\1\0\2\65\4\0"+
-    "\1\25\5\0\1\25\3\0\1\65\4\0\1\65\2\25"+
-    "\2\65\12\0\2\25\1\0\1\65\10\0\1\25\120\0"+
-    "\1\25\11\0\2\25\2\0\5\25\2\0\2\25\4\0"+
-    "\6\25\1\0\2\25\4\0\5\25\1\0\5\25\1\0"+
-    "\2\25\1\0\3\25\1\0\4\25\1\0\5\25\2\0"+
-    "\1\25\1\0\1\25\1\0\3\25\2\0\1\25\1\0"+
-    "\1\25\1\0\1\25\2\0\1\25\113\0\1\25\3\0"+
-    "\1\25\5\0\2\25\3\0\1\25\4\0\3\25\4\0"+
-    "\1\25\1\0\1\25\2\0\1\25\1\0\2\25\4\0"+
-    "\1\25\1\0\1\25\3\0\2\25\1\0\1\25\5\0"+
-    "\3\25\1\0\1\25\10\0\1\25\4\0\1\25\10\0"+
-    "\1\25\120\0\1\25\3\0\1\25\6\0\2\25\5\0"+
-    "\1\25\1\0\1\25\1\0\1\25\1\0\11\25\2\0"+
-    "\1\25\4\0\1\25\4\0\6\25\2\0\1\25\1\0"+
-    "\1\25\1\0\3\25\1\0\1\65\1\0\2\25\4\0"+
-    "\3\25\1\0\1\25\10\0\1\25\1\0\2\25\115\0"+
-    "\1\25\3\0\1\25\5\0\1\25\32\0\15\25\5\0"+
-    "\3\25\1\0\1\25\5\0\1\25\2\65\5\0\1\25"+
-    "\2\0\1\25\1\65\4\0\1\25\2\0\1\25\1\0"+
-    "\1\25\177\0\2\65\6\0\1\65\152\0\1\65\3\0"+
-    "\1\65\2\0\1\65\3\0\1\65\5\0\1\65\7\0"+
-    "\1\65\4\0\2\65\3\0\2\65\1\0\1\65\4\0"+
-    "\1\65\1\0\1\65\2\0\2\65\1\0\3\65\1\0"+
-    "\1\65\2\0\4\65\2\0\1\65\136\0\1\66\11\0"+
-    "\3\25\5\0\1\25\1\0\1\25\1\0\1\25\4\0"+
-    "\1\25\4\0\1\66\1\0\2\66\4\0\1\25\5\0"+
-    "\1\25\3\0\1\66\4\0\1\66\2\25\2\66\10\0"+
-    "\1\64\1\0\2\25\1\0\1\66\10\0\1\25\120\0"+
-    "\1\25\3\0\1\25\6\0\2\25\5\0\1\25\1\0"+
-    "\1\25\1\0\1\25\1\0\11\25\2\0\1\25\4\0"+
-    "\1\25\4\0\6\25\2\0\1\25\1\0\1\25\1\0"+
-    "\3\25\1\0\1\66\1\0\2\25\4\0\3\25\1\0"+
-    "\1\25\10\0\1\25\1\0\2\25\115\0\1\25\3\0"+
-    "\1\25\5\0\1\25\32\0\15\25\5\0\3\25\1\0"+
-    "\1\25\5\0\1\25\2\66\5\0\1\25\2\0\1\25"+
-    "\1\66\4\0\1\25\2\0\1\25\1\0\1\25\177\0"+
-    "\2\66\6\0\1\66\152\0\1\66\3\0\1\66\2\0"+
-    "\1\66\3\0\1\66\5\0\1\66\7\0\1\66\4\0"+
-    "\2\66\3\0\2\66\1\0\1\66\4\0\1\66\1\0"+
-    "\1\66\2\0\2\66\1\0\3\66\1\0\1\66\2\0"+
-    "\4\66\2\0\1\66\136\0\1\76\37\0\1\76\1\0"+
-    "\2\76\16\0\1\76\4\0\1\76\2\0\2\76\10\0"+
-    "\1\26\4\0\1\76\133\0\1\26\102\0\1\26\243\0"+
-    "\2\26\230\0\1\76\247\0\2\76\11\0\1\76\211\0"+
-    "\2\76\6\0\1\76\152\0\1\76\3\0\1\76\2\0"+
-    "\1\76\3\0\1\76\5\0\1\76\7\0\1\76\4\0"+
-    "\2\76\3\0\2\76\1\0\1\76\4\0\1\76\1\0"+
-    "\1\76\2\0\2\76\1\0\3\76\1\0\1\76\2\0"+
-    "\4\76\2\0\1\76\136\0\1\77\11\0\3\25\5\0"+
-    "\1\25\1\0\1\25\1\0\1\25\4\0\1\25\4\0"+
-    "\1\77\1\0\2\77\4\0\1\25\5\0\1\25\3\0"+
-    "\1\77\4\0\1\77\2\25\2\77\10\0\1\26\1\0"+
-    "\2\25\1\0\1\77\10\0\1\25\120\0\1\25\3\0"+
-    "\1\25\6\0\2\25\5\0\1\25\1\0\1\25\1\0"+
-    "\1\25\1\0\11\25\2\0\1\25\4\0\1\25\4\0"+
-    "\6\25\2\0\1\25\1\0\1\25\1\0\3\25\1\0"+
-    "\1\77\1\0\2\25\4\0\3\25\1\0\1\25\10\0"+
-    "\1\25\1\0\2\25\115\0\1\25\3\0\1\25\5\0"+
-    "\1\25\32\0\15\25\5\0\3\25\1\0\1\25\5\0"+
-    "\1\25\2\77\5\0\1\25\2\0\1\25\1\77\4\0"+
-    "\1\25\2\0\1\25\1\0\1\25\177\0\2\77\6\0"+
-    "\1\77\152\0\1\77\3\0\1\77\2\0\1\77\3\0"+
-    "\1\77\5\0\1\77\7\0\1\77\4\0\2\77\3\0"+
-    "\2\77\1\0\1\77\4\0\1\77\1\0\1\77\2\0"+
-    "\2\77\1\0\3\77\1\0\1\77\2\0\4\77\2\0"+
-    "\1\77\136\0\1\115\37\0\1\115\1\0\2\115\16\0"+
-    "\1\115\4\0\1\115\2\0\2\115\15\0\1\115\226\0"+
-    "\1\115\247\0\2\115\11\0\1\115\211\0\2\115\6\0"+
-    "\1\115\152\0\1\115\3\0\1\115\2\0\1\115\3\0"+
-    "\1\115\5\0\1\115\7\0\1\115\4\0\2\115\3\0"+
-    "\2\115\1\0\1\115\4\0\1\115\1\0\1\115\2\0"+
-    "\2\115\1\0\3\115\1\0\1\115\2\0\4\115\2\0"+
-    "\1\115\303\0\1\346\32\230\1\347\12\230\175\0\61\231"+
-    "\1\0\1\350\4\231\1\351\1\0\3\231\1\0\1\47"+
-    "\1\0\1\50\1\0\1\51\1\0\1\52\1\0\1\53"+
-    "\1\0\1\54\3\0\1\55\5\0\1\56\3\0\1\57"+
-    "\11\0\1\60\2\0\1\61\16\0\1\62\2\0\1\63"+
-    "\41\0\2\25\1\64\1\0\1\65\1\0\1\65\1\66"+
-    "\1\0\1\25\2\0\1\25\1\140\1\36\2\352\1\353"+
-    "\1\354\10\352\1\36\1\355\5\352\6\36\1\141\12\142"+
-    "\1\65\1\136\1\143\1\136\1\0\1\136\1\144\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\2\0\1\47\1\0"+
-    "\1\50\1\0\1\51\1\0\1\52\1\0\1\53\1\0"+
-    "\1\54\3\0\1\55\5\0\1\56\3\0\1\57\11\0"+
-    "\1\60\2\0\1\61\16\0\1\62\2\0\1\63\41\0"+
-    "\2\25\1\64\1\0\1\65\1\0\1\65\1\66\1\0"+
-    "\1\25\2\0\1\25\1\140\1\356\2\352\1\36\1\352"+
-    "\1\357\6\352\4\36\4\352\1\36\1\352\1\36\3\352"+
-    "\1\141\12\142\1\65\1\136\1\143\1\136\1\0\1\136"+
-    "\1\144\1\137\3\136\3\0\1\136\4\0\2\136\2\0"+
-    "\1\47\1\0\1\50\1\0\1\51\1\0\1\52\1\0"+
-    "\1\53\1\0\1\54\3\0\1\55\5\0\1\56\3\0"+
-    "\1\57\11\0\1\60\2\0\1\61\16\0\1\62\2\0"+
-    "\1\63\41\0\2\25\1\64\1\0\1\65\1\0\1\65"+
-    "\1\66\1\0\1\25\2\0\1\25\1\140\3\36\1\352"+
-    "\1\36\1\352\4\36\1\352\10\36\1\352\2\36\1\352"+
-    "\2\36\1\352\1\141\12\142\1\65\1\136\1\143\1\136"+
-    "\1\0\1\136\1\144\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\2\0\1\47\1\0\1\50\1\0\1\51\1\0"+
-    "\1\52\1\0\1\53\1\0\1\54\3\0\1\55\5\0"+
-    "\1\56\3\0\1\57\11\0\1\60\2\0\1\61\16\0"+
-    "\1\62\2\0\1\63\41\0\2\25\1\64\1\0\1\65"+
-    "\1\0\1\65\1\66\1\0\1\25\2\0\1\25\1\140"+
-    "\1\36\1\352\1\360\2\352\2\36\1\352\6\36\3\352"+
-    "\11\36\1\141\12\142\1\65\1\136\1\143\1\136\1\0"+
-    "\1\136\1\144\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\2\0\1\47\1\0\1\50\1\0\1\51\1\0\1\52"+
-    "\1\0\1\53\1\0\1\54\3\0\1\55\5\0\1\56"+
-    "\3\0\1\57\11\0\1\60\2\0\1\61\16\0\1\62"+
-    "\2\0\1\63\41\0\2\25\1\64\1\0\1\65\1\0"+
-    "\1\65\1\66\1\0\1\25\2\0\1\25\1\140\3\36"+
-    "\1\352\1\36\1\352\10\36\1\352\1\36\2\352\10\36"+
-    "\1\141\12\142\1\65\1\136\1\143\1\136\1\0\1\136"+
-    "\1\144\1\137\3\136\3\0\1\136\4\0\2\136\2\0"+
-    "\1\47\1\0\1\50\1\0\1\51\1\0\1\52\1\0"+
-    "\1\53\1\0\1\54\3\0\1\55\5\0\1\56\3\0"+
-    "\1\57\11\0\1\60\2\0\1\61\16\0\1\62\2\0"+
-    "\1\63\41\0\2\25\1\64\1\0\1\65\1\0\1\65"+
-    "\1\66\1\0\1\25\2\0\1\25\1\140\4\36\1\361"+
-    "\5\36\1\352\17\36\1\141\12\142\1\65\1\136\1\143"+
-    "\1\136\1\0\1\136\1\144\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\2\0\1\47\1\0\1\50\1\0\1\51"+
-    "\1\0\1\52\1\0\1\53\1\0\1\54\3\0\1\55"+
-    "\5\0\1\56\3\0\1\57\11\0\1\60\2\0\1\61"+
-    "\16\0\1\62\2\0\1\63\41\0\2\25\1\64\1\0"+
-    "\1\65\1\0\1\65\1\66\1\0\1\25\2\0\1\25"+
-    "\1\140\4\36\2\352\2\36\1\352\1\36\1\352\13\36"+
-    "\1\352\2\36\1\352\1\141\12\142\1\65\1\136\1\143"+
-    "\1\136\1\0\1\136\1\144\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\2\0\1\47\1\0\1\50\1\0\1\51"+
-    "\1\0\1\52\1\0\1\53\1\0\1\54\3\0\1\55"+
-    "\5\0\1\56\3\0\1\57\11\0\1\60\2\0\1\61"+
-    "\16\0\1\62\2\0\1\63\41\0\2\25\1\64\1\0"+
-    "\1\65\1\0\1\65\1\66\1\0\1\25\2\0\1\25"+
-    "\1\140\1\352\1\36\3\352\1\362\14\352\2\36\2\352"+
-    "\2\36\1\352\1\36\1\141\12\142\1\65\1\136\1\143"+
-    "\1\136\1\0\1\136\1\144\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\2\0\1\47\1\0\1\50\1\0\1\51"+
-    "\1\0\1\52\1\0\1\53\1\0\1\54\3\0\1\55"+
-    "\5\0\1\56\3\0\1\57\11\0\1\60\2\0\1\61"+
-    "\16\0\1\62\2\0\1\63\41\0\2\25\1\64\1\0"+
-    "\1\65\1\0\1\65\1\66\1\0\1\25\2\0\1\25"+
-    "\1\140\2\36\4\352\3\36\2\352\1\363\1\352\1\36"+
-    "\2\352\12\36\1\141\12\142\1\65\1\136\1\143\1\136"+
-    "\1\0\1\136\1\144\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\2\0\1\47\1\0\1\50\1\0\1\51\1\0"+
-    "\1\52\1\0\1\53\1\0\1\54\3\0\1\55\5\0"+
-    "\1\56\3\0\1\57\11\0\1\60\2\0\1\61\16\0"+
-    "\1\62\2\0\1\63\41\0\2\25\1\64\1\0\1\65"+
-    "\1\0\1\65\1\66\1\0\1\25\2\0\1\25\1\140"+
-    "\2\352\2\36\1\352\3\36\1\352\5\36\3\352\3\36"+
-    "\1\352\2\36\3\352\1\141\12\142\1\65\1\136\1\143"+
-    "\1\136\1\0\1\136\1\144\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\2\0\1\47\1\0\1\50\1\0\1\51"+
-    "\1\0\1\52\1\0\1\53\1\0\1\54\3\0\1\55"+
-    "\5\0\1\56\3\0\1\57\11\0\1\60\2\0\1\61"+
-    "\16\0\1\62\2\0\1\63\41\0\2\25\1\64\1\0"+
-    "\1\65\1\0\1\65\1\66\1\0\1\25\2\0\1\25"+
-    "\1\140\5\352\1\364\1\36\1\352\1\365\7\352\1\366"+
-    "\3\352\1\36\1\352\1\36\3\352\1\141\12\142\1\65"+
-    "\1\136\1\143\1\136\1\0\1\136\1\144\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\2\0\1\47\1\0\1\50"+
-    "\1\0\1\51\1\0\1\52\1\0\1\53\1\0\1\54"+
-    "\3\0\1\55\5\0\1\56\3\0\1\57\11\0\1\60"+
-    "\2\0\1\61\16\0\1\62\2\0\1\63\41\0\2\25"+
-    "\1\64\1\0\1\65\1\0\1\65\1\66\1\0\1\25"+
-    "\2\0\1\25\1\140\1\367\1\352\1\36\1\356\6\352"+
-    "\3\36\1\352\2\36\1\352\2\36\1\352\6\36\1\141"+
-    "\12\142\1\65\1\136\1\143\1\136\1\0\1\136\1\144"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\2\0\1\47"+
-    "\1\0\1\50\1\0\1\51\1\0\1\52\1\0\1\53"+
-    "\1\0\1\54\3\0\1\55\5\0\1\56\3\0\1\57"+
-    "\11\0\1\60\2\0\1\61\16\0\1\62\2\0\1\63"+
-    "\41\0\2\25\1\64\1\0\1\65\1\0\1\65\1\66"+
-    "\1\0\1\25\2\0\1\25\1\140\1\352\31\36\1\141"+
-    "\12\142\1\65\1\136\1\143\1\136\1\0\1\136\1\144"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\2\0\1\47"+
-    "\1\0\1\50\1\0\1\51\1\0\1\52\1\0\1\53"+
-    "\1\0\1\54\3\0\1\55\5\0\1\56\3\0\1\57"+
-    "\11\0\1\60\2\0\1\61\16\0\1\62\2\0\1\63"+
-    "\41\0\2\25\1\64\1\0\1\65\1\0\1\65\1\66"+
-    "\1\0\1\25\2\0\1\25\1\140\1\352\2\36\1\352"+
-    "\1\370\1\36\2\352\1\36\3\352\2\36\2\352\1\36"+
-    "\1\352\3\36\1\352\2\36\2\352\1\141\12\142\1\65"+
-    "\1\136\1\143\1\136\1\0\1\136\1\144\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\2\0\1\47\1\0\1\50"+
-    "\1\0\1\51\1\0\1\52\1\0\1\53\1\0\1\54"+
-    "\3\0\1\55\5\0\1\56\3\0\1\57\11\0\1\60"+
-    "\2\0\1\61\16\0\1\62\2\0\1\63\41\0\2\25"+
-    "\1\64\1\0\1\65\1\0\1\65\1\66\1\0\1\25"+
-    "\2\0\1\25\1\140\6\352\1\36\5\352\3\36\2\352"+
-    "\1\36\10\352\1\141\12\142\1\65\1\136\1\143\1\136"+
-    "\1\0\1\136\1\144\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\2\0\1\47\1\0\1\50\1\0\1\51\1\0"+
-    "\1\52\1\0\1\53\1\0\1\54\3\0\1\55\5\0"+
-    "\1\56\3\0\1\57\11\0\1\60\2\0\1\61\16\0"+
-    "\1\62\2\0\1\63\41\0\2\25\1\64\1\0\1\65"+
-    "\1\0\1\65\1\66\1\0\1\25\2\0\1\25\1\140"+
-    "\1\36\2\352\1\365\1\371\3\352\1\36\3\352\1\36"+
-    "\1\352\1\36\1\352\1\36\1\352\1\36\1\352\1\36"+
-    "\3\352\1\36\1\352\1\141\12\142\1\65\1\136\1\143"+
-    "\1\136\1\0\1\136\1\144\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\2\0\1\47\1\0\1\50\1\0\1\51"+
-    "\1\0\1\52\1\0\1\53\1\0\1\54\3\0\1\55"+
-    "\5\0\1\56\3\0\1\57\11\0\1\60\2\0\1\61"+
-    "\16\0\1\62\2\0\1\63\41\0\2\25\1\64\1\0"+
-    "\1\65\1\0\1\65\1\66\1\0\1\25\2\0\1\25"+
-    "\1\140\1\352\6\36\1\352\6\36\1\352\4\36\1\352"+
-    "\4\36\2\352\1\141\12\142\1\65\1\136\1\143\1\136"+
-    "\1\0\1\136\1\144\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\2\0\1\47\1\0\1\50\1\0\1\51\1\0"+
-    "\1\52\1\0\1\53\1\0\1\54\3\0\1\55\5\0"+
-    "\1\56\3\0\1\57\11\0\1\60\2\0\1\61\16\0"+
-    "\1\62\2\0\1\63\41\0\2\25\1\64\1\0\1\65"+
-    "\1\0\1\65\1\66\1\0\1\25\2\0\1\25\1\140"+
-    "\6\36\1\352\7\36\1\352\13\36\1\141\12\142\1\65"+
-    "\1\136\1\143\1\136\1\0\1\136\1\144\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\2\0\1\47\1\0\1\50"+
-    "\1\0\1\51\1\0\1\52\1\0\1\53\1\0\1\54"+
-    "\3\0\1\55\5\0\1\56\3\0\1\57\11\0\1\60"+
-    "\2\0\1\61\16\0\1\62\2\0\1\63\41\0\2\25"+
-    "\1\64\1\0\1\65\1\0\1\65\1\66\1\0\1\25"+
-    "\2\0\1\25\1\140\13\36\1\372\6\36\1\373\7\36"+
-    "\1\141\12\142\1\65\1\136\1\143\1\136\1\0\1\136"+
-    "\1\144\1\137\3\136\3\0\1\136\4\0\2\136\2\0"+
-    "\1\47\1\0\1\50\1\0\1\51\1\0\1\52\1\0"+
-    "\1\53\1\0\1\54\3\0\1\55\5\0\1\56\3\0"+
-    "\1\57\11\0\1\60\2\0\1\61\16\0\1\62\2\0"+
-    "\1\63\41\0\2\25\1\64\1\0\1\65\1\0\1\65"+
-    "\1\66\1\0\1\25\2\0\1\25\1\140\1\352\11\36"+
-    "\1\352\6\36\1\352\10\36\1\141\12\142\1\65\1\136"+
-    "\1\143\1\136\1\0\1\136\1\144\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\2\0\1\47\1\0\1\50\1\0"+
-    "\1\51\1\0\1\52\1\0\1\53\1\0\1\54\3\0"+
-    "\1\55\5\0\1\56\3\0\1\57\11\0\1\60\2\0"+
-    "\1\61\16\0\1\62\2\0\1\63\41\0\2\25\1\64"+
-    "\1\0\1\65\1\0\1\65\1\66\1\0\1\25\2\0"+
-    "\1\25\1\140\1\352\1\36\6\352\1\374\1\36\2\352"+
-    "\2\36\2\352\1\36\1\352\1\36\6\352\1\36\1\141"+
-    "\12\142\1\65\1\136\1\143\1\136\1\0\1\136\1\144"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\2\0\1\47"+
-    "\1\0\1\50\1\0\1\51\1\0\1\52\1\0\1\53"+
-    "\1\0\1\54\3\0\1\55\5\0\1\56\3\0\1\57"+
-    "\11\0\1\60\2\0\1\61\16\0\1\62\2\0\1\63"+
-    "\41\0\2\25\1\64\1\0\1\65\1\0\1\65\1\66"+
-    "\1\0\1\25\2\0\1\25\1\140\4\36\1\352\5\36"+
-    "\2\352\3\36\2\352\10\36\1\352\1\141\12\142\1\65"+
-    "\1\136\1\143\1\136\1\0\1\136\1\144\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\2\0\1\47\1\0\1\50"+
-    "\1\0\1\51\1\0\1\52\1\0\1\53\1\0\1\54"+
-    "\3\0\1\55\5\0\1\56\3\0\1\57\11\0\1\60"+
-    "\2\0\1\61\16\0\1\62\2\0\1\63\41\0\2\25"+
-    "\1\64\1\0\1\65\1\0\1\65\1\66\1\0\1\25"+
-    "\2\0\1\25\1\140\3\36\1\352\1\36\1\375\4\36"+
-    "\1\352\2\36\1\352\14\36\1\141\12\142\1\65\1\136"+
-    "\1\143\1\136\1\0\1\136\1\144\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\2\0\1\47\1\0\1\50\1\0"+
-    "\1\51\1\0\1\52\1\0\1\53\1\0\1\54\3\0"+
-    "\1\55\5\0\1\56\3\0\1\57\11\0\1\60\2\0"+
-    "\1\61\16\0\1\62\2\0\1\63\41\0\2\25\1\64"+
-    "\1\0\1\65\1\0\1\65\1\66\1\0\1\25\2\0"+
-    "\1\25\1\140\2\352\1\36\1\352\3\36\2\352\2\36"+
-    "\1\352\4\36\1\352\11\36\1\141\12\142\1\65\1\136"+
-    "\1\143\1\136\1\0\1\136\1\144\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\2\0\1\47\1\0\1\50\1\0"+
-    "\1\51\1\0\1\52\1\0\1\53\1\0\1\54\3\0"+
-    "\1\55\5\0\1\56\3\0\1\57\11\0\1\60\2\0"+
-    "\1\61\16\0\1\62\2\0\1\63\41\0\2\25\1\64"+
-    "\1\0\1\65\1\0\1\65\1\66\1\0\1\25\2\0"+
-    "\1\25\1\140\3\36\1\352\13\36\1\352\12\36\1\141"+
-    "\12\142\1\65\1\136\1\143\1\136\1\0\1\136\1\144"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\2\0\1\47"+
-    "\1\0\1\50\1\0\1\51\1\0\1\52\1\0\1\53"+
-    "\1\0\1\54\3\0\1\55\5\0\1\56\3\0\1\57"+
-    "\11\0\1\60\2\0\1\61\16\0\1\62\2\0\1\63"+
-    "\41\0\2\25\1\64\1\0\1\65\1\0\1\65\1\66"+
-    "\1\0\1\25\2\0\1\25\1\140\3\36\2\352\2\36"+
-    "\2\352\1\36\2\352\1\36\1\352\3\36\1\352\1\36"+
-    "\1\352\1\36\1\352\2\36\1\352\1\36\1\141\12\142"+
-    "\1\65\1\136\1\143\1\136\1\0\1\136\1\144\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\32\264"+
-    "\1\141\12\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\14\0\1\337\3\0\1\340"+
-    "\5\0\1\341\3\0\1\342\14\0\1\343\16\0\1\344"+
-    "\2\0\1\345\42\0\1\172\1\64\6\0\1\172\3\0"+
-    "\1\135\1\273\1\274\1\275\1\276\1\277\1\300\1\301"+
-    "\1\302\1\303\1\304\1\305\1\306\1\307\1\310\1\311"+
-    "\1\312\1\313\1\314\1\315\1\316\1\317\1\320\1\321"+
-    "\1\322\1\323\1\324\1\136\12\142\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\1\162\3\0\2\136"+
-    "\14\0\1\337\3\0\1\340\5\0\1\341\3\0\1\342"+
-    "\14\0\1\343\16\0\1\344\2\0\1\345\42\0\1\172"+
-    "\1\64\6\0\1\172\3\0\1\135\33\136\12\267\1\0"+
-    "\3\136\1\0\2\136\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\2\0\1\47\1\0\1\50\1\0\1\51\1\0"+
-    "\1\52\1\0\1\53\1\0\1\165\3\0\1\55\5\0"+
-    "\1\56\3\0\1\166\11\0\1\60\2\0\1\167\16\0"+
-    "\1\170\2\0\1\171\41\0\1\25\2\64\2\0\2\172"+
-    "\1\66\1\0\1\64\2\0\1\25\1\377\32\155\1\136"+
-    "\12\267\1\0\1\136\1\143\1\136\1\0\2\266\1\137"+
-    "\3\136\2\0\1\172\1\136\4\0\2\136\2\0\1\47"+
-    "\1\0\1\50\1\0\1\51\1\0\1\52\1\0\1\53"+
-    "\1\0\1\54\3\0\1\55\5\0\1\56\3\0\1\57"+
-    "\11\0\1\60\2\0\1\61\16\0\1\62\2\0\1\63"+
-    "\41\0\2\25\1\64\1\0\1\65\1\0\1\65\1\66"+
-    "\1\0\1\25\2\0\1\25\1\140\3\36\1\u0100\26\36"+
-    "\1\141\12\142\1\65\1\136\1\143\1\136\1\0\1\136"+
-    "\1\144\1\137\3\136\3\0\1\136\4\0\2\136\2\0"+
-    "\1\47\1\0\1\50\1\0\1\51\1\0\1\52\1\0"+
-    "\1\53\1\0\1\54\3\0\1\55\5\0\1\56\3\0"+
-    "\1\57\11\0\1\60\2\0\1\61\16\0\1\62\2\0"+
-    "\1\63\41\0\2\25\1\64\1\0\1\65\1\0\1\65"+
-    "\1\66\1\0\1\25\2\0\1\25\1\140\32\36\1\141"+
-    "\12\142\1\u0101\1\136\1\143\1\136\1\0\1\136\1\144"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\2\0\1\47"+
-    "\1\0\1\50\1\0\1\51\1\0\1\52\1\0\1\53"+
-    "\1\0\1\54\3\0\1\55\5\0\1\56\3\0\1\57"+
-    "\11\0\1\60\2\0\1\61\16\0\1\62\2\0\1\63"+
-    "\41\0\2\25\1\64\1\0\1\65\1\0\1\65\1\66"+
-    "\1\0\1\25\2\0\1\25\1\140\15\36\1\u0102\14\36"+
-    "\1\141\12\142\1\65\1\136\1\143\1\136\1\0\1\136"+
-    "\1\144\1\137\3\136\3\0\1\136\4\0\2\136\150\0"+
-    "\1\376\1\264\2\u0103\1\u0104\1\u0105\10\u0103\1\264\1\u0106"+
-    "\5\u0103\6\264\1\141\12\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\1\u0107\2\u0103\1\264\1\u0103\1\u0108\6\u0103\4\264\4\u0103"+
-    "\1\264\1\u0103\1\264\3\u0103\1\141\12\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\376\3\264\1\u0103\1\264\1\u0103\4\264\1\u0103"+
-    "\10\264\1\u0103\2\264\1\u0103\2\264\1\u0103\1\141\12\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\1\264\1\u0103\1\u0109\2\u0103"+
-    "\2\264\1\u0103\6\264\3\u0103\11\264\1\141\12\264\1\0"+
-    "\3\136\1\0\2\136\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\150\0\1\376\3\264\1\u0103\1\264\1\u0103\10\264"+
-    "\1\u0103\1\264\2\u0103\10\264\1\141\12\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\376\4\264\1\u010a\5\264\1\u0103\17\264\1\141"+
-    "\12\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\4\264\2\u0103\2\264"+
-    "\1\u0103\1\264\1\u0103\13\264\1\u0103\2\264\1\u0103\1\141"+
-    "\12\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\1\u0103\1\264\3\u0103"+
-    "\1\u010b\14\u0103\2\264\2\u0103\2\264\1\u0103\1\264\1\141"+
-    "\12\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\2\264\4\u0103\3\264"+
-    "\2\u0103\1\u010c\1\u0103\1\264\2\u0103\12\264\1\141\12\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\2\u0103\2\264\1\u0103\3\264"+
-    "\1\u0103\5\264\3\u0103\3\264\1\u0103\2\264\3\u0103\1\141"+
-    "\12\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\5\u0103\1\u010d\1\264"+
-    "\1\u0103\1\u010e\7\u0103\1\u010f\3\u0103\1\264\1\u0103\1\264"+
-    "\3\u0103\1\141\12\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\1\u0110"+
-    "\1\u0103\1\264\1\u0107\6\u0103\3\264\1\u0103\2\264\1\u0103"+
-    "\2\264\1\u0103\6\264\1\141\12\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\150\0"+
-    "\1\376\1\u0103\31\264\1\141\12\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\150\0"+
-    "\1\376\1\u0103\2\264\1\u0103\1\u0111\1\264\2\u0103\1\264"+
-    "\3\u0103\2\264\2\u0103\1\264\1\u0103\3\264\1\u0103\2\264"+
-    "\2\u0103\1\141\12\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\6\u0103"+
-    "\1\264\5\u0103\3\264\2\u0103\1\264\10\u0103\1\141\12\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\1\264\2\u0103\1\u010e\1\u0112"+
-    "\3\u0103\1\264\3\u0103\1\264\1\u0103\1\264\1\u0103\1\264"+
-    "\1\u0103\1\264\1\u0103\1\264\3\u0103\1\264\1\u0103\1\141"+
-    "\12\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\1\u0103\6\264\1\u0103"+
-    "\6\264\1\u0103\4\264\1\u0103\4\264\2\u0103\1\141\12\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\6\264\1\u0103\7\264\1\u0103"+
-    "\13\264\1\141\12\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\13\264"+
-    "\1\u0113\6\264\1\u0114\7\264\1\141\12\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\376\1\u0103\11\264\1\u0103\6\264\1\u0103\10\264"+
-    "\1\141\12\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\1\u0103\1\264"+
-    "\6\u0103\1\u0115\1\264\2\u0103\2\264\2\u0103\1\264\1\u0103"+
-    "\1\264\6\u0103\1\264\1\141\12\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\150\0"+
-    "\1\376\4\264\1\u0103\5\264\2\u0103\3\264\2\u0103\10\264"+
-    "\1\u0103\1\141\12\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\3\264"+
-    "\1\u0103\1\264\1\u0116\4\264\1\u0103\2\264\1\u0103\14\264"+
-    "\1\141\12\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\2\u0103\1\264"+
-    "\1\u0103\3\264\2\u0103\2\264\1\u0103\4\264\1\u0103\11\264"+
-    "\1\141\12\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\3\264\1\u0103"+
-    "\13\264\1\u0103\12\264\1\141\12\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\150\0"+
-    "\1\376\3\264\2\u0103\2\264\2\u0103\1\264\2\u0103\1\264"+
-    "\1\u0103\3\264\1\u0103\1\264\1\u0103\1\264\1\u0103\2\264"+
-    "\1\u0103\1\264\1\141\12\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\2\0\1\47"+
-    "\1\0\1\50\1\0\1\51\1\0\1\52\1\0\1\53"+
-    "\1\0\1\67\3\0\1\70\5\0\1\71\3\0\1\72"+
-    "\11\0\1\60\2\0\1\73\16\0\1\74\2\0\1\75"+
-    "\41\0\1\25\2\26\2\0\2\76\1\77\1\0\1\26"+
-    "\2\0\1\25\1\u0117\32\36\1\141\12\326\1\0\1\136"+
-    "\1\151\1\136\1\0\2\152\1\137\3\136\2\0\1\76"+
-    "\1\136\4\0\2\136\2\0\1\47\1\0\1\50\1\0"+
-    "\1\51\1\0\1\52\1\0\1\53\1\0\1\67\3\0"+
-    "\1\70\5\0\1\71\3\0\1\72\11\0\1\60\2\0"+
-    "\1\73\16\0\1\74\2\0\1\75\41\0\1\25\2\26"+
-    "\2\0\2\76\1\77\1\0\1\26\2\0\1\25\1\u0117"+
-    "\32\36\1\141\12\u0118\1\0\1\136\1\151\1\136\1\0"+
-    "\2\152\1\137\3\136\2\0\1\76\1\136\4\0\2\136"+
-    "\2\0\1\47\1\0\1\50\1\0\1\51\1\0\1\52"+
-    "\1\0\1\53\1\0\1\67\3\0\1\70\5\0\1\71"+
-    "\3\0\1\72\11\0\1\60\2\0\1\73\16\0\1\74"+
-    "\2\0\1\75\41\0\1\25\2\26\2\0\2\76\1\77"+
-    "\1\0\1\26\2\0\1\25\1\u0117\32\36\1\141\1\326"+
-    "\1\u0119\1\u0118\2\326\2\u0118\1\326\1\u0118\1\326\1\0"+
-    "\1\136\1\151\1\136\1\0\2\152\1\137\3\136\2\0"+
-    "\1\76\1\136\4\0\2\136\2\0\1\47\1\0\1\50"+
-    "\1\0\1\51\1\0\1\52\1\0\1\53\1\0\1\67"+
-    "\3\0\1\70\5\0\1\71\3\0\1\72\11\0\1\60"+
-    "\2\0\1\73\16\0\1\74\2\0\1\75\41\0\1\25"+
-    "\2\26\2\0\2\76\1\77\1\0\1\26\2\0\1\25"+
-    "\1\u011a\32\36\1\141\12\330\1\0\1\136\1\151\1\136"+
-    "\1\0\2\152\1\137\3\136\2\0\1\76\1\136\4\0"+
-    "\2\136\2\0\1\47\1\0\1\50\1\0\1\51\1\0"+
-    "\1\52\1\0\1\53\1\0\1\173\3\0\1\174\5\0"+
-    "\1\175\3\0\1\176\11\0\1\60\2\0\1\177\16\0"+
-    "\1\200\2\0\1\201\41\0\1\25\1\65\7\0\1\65"+
-    "\2\0\1\25\1\135\32\155\13\136\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\1\162\3\0\2\136"+
-    "\14\0\1\207\3\0\1\210\5\0\1\211\3\0\1\212"+
-    "\14\0\1\213\16\0\1\214\2\0\1\215\42\0\1\76"+
-    "\1\26\6\0\1\76\3\0\1\135\33\136\12\156\1\0"+
-    "\3\136\1\0\2\136\1\137\3\136\3\0\1\136\1\162"+
-    "\3\0\2\136\151\0\4\u011b\2\0\1\u011b\15\0\1\u011b"+
-    "\6\0\12\u011b\1\334\175\0\4\u011c\2\0\1\u011c\15\0"+
-    "\1\u011c\6\0\12\u011c\1\u011d\175\0\4\u011e\2\0\1\u011e"+
-    "\15\0\1\u011e\6\0\1\u011f\2\u0120\1\u011f\5\u0120\1\u0121"+
-    "\14\0\1\u0122\160\0\46\136\1\0\3\136\1\0\2\136"+
-    "\1\0\3\136\3\0\1\136\1\162\3\0\2\136\3\0"+
-    "\1\172\37\0\1\172\1\0\2\172\16\0\1\172\4\0"+
-    "\1\172\2\0\2\172\10\0\1\64\4\0\1\172\133\0"+
-    "\1\64\102\0\1\64\243\0\2\64\230\0\1\172\247\0"+
-    "\2\172\11\0\1\172\211\0\2\172\6\0\1\172\152\0"+
-    "\1\172\3\0\1\172\2\0\1\172\3\0\1\172\5\0"+
-    "\1\172\7\0\1\172\4\0\2\172\3\0\2\172\1\0"+
-    "\1\172\4\0\1\172\1\0\1\172\2\0\2\172\1\0"+
-    "\3\172\1\0\1\172\2\0\4\172\2\0\1\172\304\0"+
-    "\1\u0123\1\u0124\1\u0125\1\u0126\1\u0127\1\u0128\1\u0129\1\u012a"+
-    "\1\u012b\1\u012c\1\u012d\1\u012e\1\u012f\1\u0130\1\u0131\1\u0132"+
-    "\1\u0133\1\u0134\1\u0135\1\u0136\1\u0137\1\u0138\1\u0139\1\u013a"+
-    "\1\u013b\1\u013c\1\0\12\230\176\0\32\230\1\347\12\230"+
-    "\175\0\74\231\1\0\1\47\1\0\1\50\1\0\1\51"+
-    "\1\0\1\52\1\0\1\53\1\0\1\54\3\0\1\55"+
-    "\5\0\1\56\3\0\1\57\11\0\1\60\2\0\1\61"+
-    "\16\0\1\62\2\0\1\63\41\0\2\25\1\64\1\0"+
-    "\1\65\1\0\1\65\1\66\1\0\1\25\2\0\1\25"+
-    "\1\u013d\32\36\1\141\12\142\1\u013e\1\136\1\143\1\136"+
-    "\1\0\1\136\1\144\1\137\1\u013f\1\u0140\1\u0141\3\0"+
-    "\1\136\4\0\2\136\2\0\1\47\1\0\1\50\1\0"+
-    "\1\51\1\0\1\52\1\0\1\53\1\0\1\54\3\0"+
-    "\1\55\5\0\1\56\3\0\1\57\11\0\1\60\2\0"+
-    "\1\61\16\0\1\62\2\0\1\63\41\0\2\25\1\64"+
-    "\1\0\1\65\1\0\1\65\1\66\1\0\1\25\2\0"+
-    "\1\25\1\u013d\4\36\1\u0142\25\36\1\141\12\142\1\u013e"+
-    "\1\136\1\143\1\136\1\0\1\136\1\144\1\137\1\u013f"+
-    "\1\u0140\1\u0141\3\0\1\136\4\0\2\136\2\0\1\47"+
-    "\1\0\1\50\1\0\1\51\1\0\1\52\1\0\1\53"+
-    "\1\0\1\54\3\0\1\55\5\0\1\56\3\0\1\57"+
-    "\11\0\1\60\2\0\1\61\16\0\1\62\2\0\1\63"+
-    "\41\0\2\25\1\64\1\0\1\65\1\0\1\65\1\66"+
-    "\1\0\1\25\2\0\1\25\1\u013d\15\36\1\246\14\36"+
-    "\1\141\12\142\1\u013e\1\136\1\143\1\136\1\0\1\136"+
-    "\1\144\1\137\1\u013f\1\u0140\1\u0141\3\0\1\136\4\0"+
-    "\2\136\2\0\1\47\1\0\1\50\1\0\1\51\1\0"+
-    "\1\52\1\0\1\53\1\0\1\54\3\0\1\55\5\0"+
-    "\1\56\3\0\1\57\11\0\1\60\2\0\1\61\16\0"+
-    "\1\62\2\0\1\63\41\0\2\25\1\64\1\0\1\65"+
-    "\1\0\1\65\1\66\1\0\1\25\2\0\1\25\1\u013d"+
-    "\10\36\1\246\21\36\1\141\12\142\1\u013e\1\136\1\143"+
-    "\1\136\1\0\1\136\1\144\1\137\1\u013f\1\u0140\1\u0141"+
-    "\3\0\1\136\4\0\2\136\2\0\1\47\1\0\1\50"+
-    "\1\0\1\51\1\0\1\52\1\0\1\53\1\0\1\54"+
-    "\3\0\1\55\5\0\1\56\3\0\1\57\11\0\1\60"+
-    "\2\0\1\61\16\0\1\62\2\0\1\63\41\0\2\25"+
-    "\1\64\1\0\1\65\1\0\1\65\1\66\1\0\1\25"+
-    "\2\0\1\25\1\u013d\17\36\1\352\12\36\1\141\12\142"+
-    "\1\u013e\1\136\1\143\1\136\1\0\1\136\1\144\1\137"+
-    "\1\u013f\1\u0140\1\u0141\3\0\1\136\4\0\2\136\2\0"+
-    "\1\47\1\0\1\50\1\0\1\51\1\0\1\52\1\0"+
-    "\1\53\1\0\1\54\3\0\1\55\5\0\1\56\3\0"+
-    "\1\57\11\0\1\60\2\0\1\61\16\0\1\62\2\0"+
-    "\1\63\41\0\2\25\1\64\1\0\1\65\1\0\1\65"+
-    "\1\66\1\0\1\25\2\0\1\25\1\u013d\5\36\1\u0143"+
-    "\4\36\1\352\17\36\1\141\12\142\1\u013e\1\136\1\143"+
-    "\1\136\1\0\1\136\1\144\1\137\1\u013f\1\u0140\1\u0141"+
-    "\3\0\1\136\4\0\2\136\2\0\1\47\1\0\1\50"+
-    "\1\0\1\51\1\0\1\52\1\0\1\53\1\0\1\54"+
-    "\3\0\1\55\5\0\1\56\3\0\1\57\11\0\1\60"+
-    "\2\0\1\61\16\0\1\62\2\0\1\63\41\0\2\25"+
-    "\1\64\1\0\1\65\1\0\1\65\1\66\1\0\1\25"+
-    "\2\0\1\25\1\140\20\36\1\352\11\36\1\141\12\142"+
-    "\1\65\1\136\1\143\1\136\1\0\1\136\1\144\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\2\0\1\47\1\0"+
-    "\1\50\1\0\1\51\1\0\1\52\1\0\1\53\1\0"+
-    "\1\54\3\0\1\55\5\0\1\56\3\0\1\57\11\0"+
-    "\1\60\2\0\1\61\16\0\1\62\2\0\1\63\41\0"+
-    "\2\25\1\64\1\0\1\65\1\0\1\65\1\66\1\0"+
-    "\1\25\2\0\1\25\1\140\7\36\1\352\22\36\1\141"+
-    "\12\142\1\65\1\136\1\143\1\136\1\0\1\136\1\144"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\2\0\1\47"+
-    "\1\0\1\50\1\0\1\51\1\0\1\52\1\0\1\53"+
-    "\1\0\1\54\3\0\1\55\5\0\1\56\3\0\1\57"+
-    "\11\0\1\60\2\0\1\61\16\0\1\62\2\0\1\63"+
-    "\41\0\2\25\1\64\1\0\1\65\1\0\1\65\1\66"+
-    "\1\0\1\25\2\0\1\25\1\140\27\36\1\352\2\36"+
-    "\1\141\12\142\1\65\1\136\1\143\1\136\1\0\1\136"+
-    "\1\144\1\137\3\136\3\0\1\136\4\0\2\136\2\0"+
-    "\1\47\1\0\1\50\1\0\1\51\1\0\1\52\1\0"+
-    "\1\53\1\0\1\54\3\0\1\55\5\0\1\56\3\0"+
-    "\1\57\11\0\1\60\2\0\1\61\16\0\1\62\2\0"+
-    "\1\63\41\0\2\25\1\64\1\0\1\65\1\0\1\65"+
-    "\1\66\1\0\1\25\2\0\1\25\1\u013d\6\36\1\u0142"+
-    "\10\36\1\352\12\36\1\141\12\142\1\u013e\1\136\1\143"+
-    "\1\136\1\0\1\136\1\144\1\137\1\u013f\1\u0140\1\u0141"+
-    "\3\0\1\136\4\0\2\136\2\0\1\47\1\0\1\50"+
-    "\1\0\1\51\1\0\1\52\1\0\1\53\1\0\1\54"+
-    "\3\0\1\55\5\0\1\56\3\0\1\57\11\0\1\60"+
-    "\2\0\1\61\16\0\1\62\2\0\1\63\41\0\2\25"+
-    "\1\64\1\0\1\65\1\0\1\65\1\66\1\0\1\25"+
-    "\2\0\1\25\1\u013d\24\36\1\u0144\5\36\1\141\12\142"+
-    "\1\u013e\1\136\1\143\1\136\1\0\1\136\1\144\1\137"+
-    "\1\u013f\1\u0140\1\u0141\3\0\1\136\4\0\2\136\2\0"+
-    "\1\47\1\0\1\50\1\0\1\51\1\0\1\52\1\0"+
-    "\1\53\1\0\1\54\3\0\1\55\5\0\1\56\3\0"+
-    "\1\57\11\0\1\60\2\0\1\61\16\0\1\62\2\0"+
-    "\1\63\41\0\2\25\1\64\1\0\1\65\1\0\1\65"+
-    "\1\66\1\0\1\25\2\0\1\25\1\140\11\36\1\352"+
-    "\20\36\1\141\12\142\1\65\1\136\1\143\1\136\1\0"+
-    "\1\136\1\144\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\2\0\1\47\1\0\1\50\1\0\1\51\1\0\1\52"+
-    "\1\0\1\53\1\0\1\54\3\0\1\55\5\0\1\56"+
-    "\3\0\1\57\11\0\1\60\2\0\1\61\16\0\1\62"+
-    "\2\0\1\63\41\0\2\25\1\64\1\0\1\65\1\0"+
-    "\1\65\1\66\1\0\1\25\2\0\1\25\1\u013d\16\36"+
-    "\1\u0145\13\36\1\141\12\142\1\u013e\1\136\1\143\1\136"+
-    "\1\0\1\136\1\144\1\137\1\u013f\1\u0140\1\u0141\3\0"+
-    "\1\136\4\0\2\136\2\0\1\47\1\0\1\50\1\0"+
-    "\1\51\1\0\1\52\1\0\1\53\1\0\1\54\3\0"+
-    "\1\55\5\0\1\56\3\0\1\57\11\0\1\60\2\0"+
-    "\1\61\16\0\1\62\2\0\1\63\41\0\2\25\1\64"+
-    "\1\0\1\65\1\0\1\65\1\66\1\0\1\25\2\0"+
-    "\1\25\1\u013d\12\36\1\u0146\17\36\1\141\12\142\1\u013e"+
-    "\1\136\1\143\1\136\1\0\1\136\1\144\1\137\1\u013f"+
-    "\1\u0140\1\u0141\3\0\1\136\4\0\2\136\2\0\1\47"+
-    "\1\0\1\50\1\0\1\51\1\0\1\52\1\0\1\53"+
-    "\1\0\1\54\3\0\1\55\5\0\1\56\3\0\1\57"+
-    "\11\0\1\60\2\0\1\61\16\0\1\62\2\0\1\63"+
-    "\41\0\2\25\1\64\1\0\1\65\1\0\1\65\1\66"+
-    "\1\0\1\25\2\0\1\25\1\u013d\5\36\1\352\24\36"+
-    "\1\141\12\142\1\u013e\1\136\1\143\1\136\1\0\1\136"+
-    "\1\144\1\137\1\u013f\1\u0140\1\u0141\3\0\1\136\4\0"+
-    "\2\136\2\0\1\47\1\0\1\50\1\0\1\51\1\0"+
-    "\1\52\1\0\1\53\1\0\1\54\3\0\1\55\5\0"+
-    "\1\56\3\0\1\57\11\0\1\60\2\0\1\61\16\0"+
-    "\1\62\2\0\1\63\41\0\2\25\1\64\1\0\1\65"+
-    "\1\0\1\65\1\66\1\0\1\25\2\0\1\25\1\u013d"+
-    "\1\u0147\31\36\1\141\12\142\1\u013e\1\136\1\143\1\136"+
-    "\1\0\1\136\1\144\1\137\1\u013f\1\u0140\1\u0141\3\0"+
-    "\1\136\4\0\2\136\2\0\1\47\1\0\1\50\1\0"+
-    "\1\51\1\0\1\52\1\0\1\53\1\0\1\54\3\0"+
-    "\1\55\5\0\1\56\3\0\1\57\11\0\1\60\2\0"+
-    "\1\61\16\0\1\62\2\0\1\63\41\0\2\25\1\64"+
-    "\1\0\1\65\1\0\1\65\1\66\1\0\1\25\2\0"+
-    "\1\25\1\140\32\36\1\u0148\12\142\1\65\1\136\1\143"+
-    "\1\136\1\0\1\136\1\144\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\2\0\1\47\1\0\1\50\1\0\1\51"+
-    "\1\0\1\52\1\0\1\53\1\0\1\54\3\0\1\55"+
-    "\5\0\1\56\3\0\1\57\11\0\1\60\2\0\1\61"+
-    "\16\0\1\62\2\0\1\63\41\0\2\25\1\64\1\0"+
-    "\1\65\1\0\1\65\1\66\1\0\1\25\2\0\1\25"+
-    "\1\140\22\36\1\352\7\36\1\141\12\142\1\65\1\136"+
-    "\1\143\1\136\1\0\1\136\1\144\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\2\0\1\47\1\0\1\50\1\0"+
-    "\1\51\1\0\1\52\1\0\1\53\1\0\1\54\3\0"+
-    "\1\55\5\0\1\56\3\0\1\57\11\0\1\60\2\0"+
-    "\1\61\16\0\1\62\2\0\1\63\41\0\2\25\1\64"+
-    "\1\0\1\65\1\0\1\65\1\66\1\0\1\25\2\0"+
-    "\1\25\1\u013d\23\36\1\352\6\36\1\141\12\142\1\u013e"+
-    "\1\136\1\143\1\136\1\0\1\136\1\144\1\137\1\u013f"+
-    "\1\u0140\1\u0141\3\0\1\136\4\0\2\136\2\0\1\47"+
-    "\1\0\1\50\1\0\1\51\1\0\1\52\1\0\1\53"+
-    "\1\0\1\54\3\0\1\55\5\0\1\56\3\0\1\57"+
-    "\11\0\1\60\2\0\1\61\16\0\1\62\2\0\1\63"+
-    "\41\0\2\25\1\64\1\0\1\65\1\0\1\65\1\66"+
-    "\1\0\1\25\2\0\1\25\1\u013d\24\36\1\u0149\5\36"+
-    "\1\141\12\142\1\u013e\1\136\1\143\1\136\1\0\1\136"+
-    "\1\144\1\137\1\u013f\1\u0140\1\u0141\3\0\1\136\4\0"+
-    "\2\136\150\0\1\135\1\273\1\274\1\275\1\276\1\277"+
-    "\1\300\1\301\1\302\1\303\1\304\1\305\1\306\1\307"+
-    "\1\310\1\311\1\312\1\313\1\314\1\315\1\316\1\317"+
-    "\1\320\1\321\1\322\1\323\1\324\1\136\12\264\1\0"+
-    "\3\136\1\0\2\136\1\137\3\136\3\0\1\136\1\162"+
-    "\3\0\2\136\14\0\1\337\3\0\1\340\5\0\1\341"+
-    "\3\0\1\342\14\0\1\343\16\0\1\344\2\0\1\345"+
-    "\42\0\1\172\1\64\6\0\1\172\3\0\1\135\33\136"+
-    "\12\267\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\1\162\3\0\2\136\2\0\1\47\1\0\1\50"+
-    "\1\0\1\51\1\0\1\52\1\0\1\53\1\0\1\54"+
-    "\3\0\1\55\5\0\1\56\3\0\1\57\11\0\1\60"+
-    "\2\0\1\61\16\0\1\62\2\0\1\63\41\0\2\25"+
-    "\1\64\1\0\1\65\1\0\1\65\1\66\1\0\1\25"+
-    "\2\0\1\25\1\140\32\36\1\141\12\142\1\u014a\1\136"+
-    "\1\143\1\136\1\0\1\136\1\144\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\2\0\1\47\1\0\1\50\1\0"+
-    "\1\51\1\0\1\52\1\0\1\53\1\0\1\173\3\0"+
-    "\1\174\5\0\1\175\3\0\1\176\11\0\1\60\2\0"+
-    "\1\177\16\0\1\200\2\0\1\201\41\0\1\25\1\65"+
-    "\7\0\1\65\2\0\1\25\1\0\32\25\24\0\1\u014b"+
-    "\15\0\1\47\1\0\1\50\1\0\1\51\1\0\1\52"+
-    "\1\0\1\53\1\0\1\54\3\0\1\55\5\0\1\56"+
-    "\3\0\1\57\11\0\1\60\2\0\1\61\16\0\1\62"+
-    "\2\0\1\63\41\0\2\25\1\64\1\0\1\65\1\0"+
-    "\1\65\1\66\1\0\1\25\2\0\1\25\1\140\16\36"+
-    "\1\u014c\13\36\1\141\12\142\1\u014d\1\136\1\143\1\136"+
-    "\1\0\1\136\1\144\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\150\0\1\u014e\32\264\1\141\12\264\1\u014f\3\136"+
-    "\1\0\2\136\1\137\1\u013f\1\u0140\1\u0141\3\0\1\136"+
-    "\4\0\2\136\150\0\1\u014e\4\264\1\u0150\25\264\1\141"+
-    "\12\264\1\u014f\3\136\1\0\2\136\1\137\1\u013f\1\u0140"+
-    "\1\u0141\3\0\1\136\4\0\2\136\150\0\1\u014e\15\264"+
-    "\1\307\14\264\1\141\12\264\1\u014f\3\136\1\0\2\136"+
-    "\1\137\1\u013f\1\u0140\1\u0141\3\0\1\136\4\0\2\136"+
-    "\150\0\1\u014e\10\264\1\307\21\264\1\141\12\264\1\u014f"+
-    "\3\136\1\0\2\136\1\137\1\u013f\1\u0140\1\u0141\3\0"+
-    "\1\136\4\0\2\136\150\0\1\u014e\17\264\1\u0103\12\264"+
-    "\1\141\12\264\1\u014f\3\136\1\0\2\136\1\137\1\u013f"+
-    "\1\u0140\1\u0141\3\0\1\136\4\0\2\136\150\0\1\u014e"+
-    "\5\264\1\u0151\4\264\1\u0103\17\264\1\141\12\264\1\u014f"+
-    "\3\136\1\0\2\136\1\137\1\u013f\1\u0140\1\u0141\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\20\264\1\u0103\11\264"+
-    "\1\141\12\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\7\264\1\u0103"+
-    "\22\264\1\141\12\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\27\264"+
-    "\1\u0103\2\264\1\141\12\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\u014e"+
-    "\6\264\1\u0150\10\264\1\u0103\12\264\1\141\12\264\1\u014f"+
-    "\3\136\1\0\2\136\1\137\1\u013f\1\u0140\1\u0141\3\0"+
-    "\1\136\4\0\2\136\150\0\1\u014e\24\264\1\u0152\5\264"+
-    "\1\141\12\264\1\u014f\3\136\1\0\2\136\1\137\1\u013f"+
-    "\1\u0140\1\u0141\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\11\264\1\u0103\20\264\1\141\12\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\150\0"+
-    "\1\u014e\16\264\1\u0153\13\264\1\141\12\264\1\u014f\3\136"+
-    "\1\0\2\136\1\137\1\u013f\1\u0140\1\u0141\3\0\1\136"+
-    "\4\0\2\136\150\0\1\u014e\12\264\1\u0154\17\264\1\141"+
-    "\12\264\1\u014f\3\136\1\0\2\136\1\137\1\u013f\1\u0140"+
-    "\1\u0141\3\0\1\136\4\0\2\136\150\0\1\u014e\5\264"+
-    "\1\u0103\24\264\1\141\12\264\1\u014f\3\136\1\0\2\136"+
-    "\1\137\1\u013f\1\u0140\1\u0141\3\0\1\136\4\0\2\136"+
-    "\150\0\1\u014e\1\u0155\31\264\1\141\12\264\1\u014f\3\136"+
-    "\1\0\2\136\1\137\1\u013f\1\u0140\1\u0141\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\32\264\1\u0148\12\264\1\0"+
-    "\3\136\1\0\2\136\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\150\0\1\376\22\264\1\u0103\7\264\1\141\12\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\u014e\23\264\1\u0103\6\264\1\141"+
-    "\12\264\1\u014f\3\136\1\0\2\136\1\137\1\u013f\1\u0140"+
-    "\1\u0141\3\0\1\136\4\0\2\136\150\0\1\u014e\24\264"+
-    "\1\u0156\5\264\1\141\12\264\1\u014f\3\136\1\0\2\136"+
-    "\1\137\1\u013f\1\u0140\1\u0141\3\0\1\136\4\0\2\136"+
-    "\14\0\1\207\3\0\1\210\5\0\1\211\3\0\1\212"+
-    "\14\0\1\213\16\0\1\214\2\0\1\215\42\0\1\76"+
-    "\1\26\6\0\1\76\3\0\1\135\1\273\1\274\1\275"+
-    "\1\276\1\277\1\300\1\301\1\302\1\303\1\304\1\305"+
-    "\1\306\1\307\1\310\1\311\1\312\1\313\1\314\1\315"+
-    "\1\316\1\317\1\320\1\321\1\322\1\323\1\324\1\136"+
-    "\1\u0157\2\u0158\1\u0157\5\u0158\1\u0159\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\1\162\3\0\2\136"+
-    "\2\0\1\47\1\0\1\50\1\0\1\51\1\0\1\52"+
-    "\1\0\1\53\1\0\1\67\3\0\1\70\5\0\1\71"+
-    "\3\0\1\72\11\0\1\60\2\0\1\73\16\0\1\74"+
-    "\2\0\1\75\41\0\1\25\2\26\2\0\2\76\1\77"+
-    "\1\0\1\26\2\0\1\25\1\u0117\32\36\1\141\12\330"+
-    "\1\0\1\136\1\151\1\136\1\0\2\152\1\137\3\136"+
-    "\2\0\1\76\1\136\4\0\2\136\2\0\1\47\1\0"+
-    "\1\50\1\0\1\51\1\0\1\52\1\0\1\53\1\0"+
-    "\1\67\3\0\1\70\5\0\1\71\3\0\1\72\11\0"+
-    "\1\60\2\0\1\73\16\0\1\74\2\0\1\75\41\0"+
-    "\1\25\2\26\2\0\2\76\1\77\1\0\1\26\2\0"+
-    "\1\25\1\u0117\32\36\1\141\2\u0118\1\330\2\u0118\2\330"+
-    "\1\u0118\1\330\1\u0118\1\0\1\136\1\151\1\136\1\0"+
-    "\2\152\1\137\3\136\2\0\1\76\1\136\4\0\2\136"+
-    "\14\0\1\207\3\0\1\210\5\0\1\211\3\0\1\212"+
-    "\14\0\1\213\16\0\1\214\2\0\1\215\42\0\1\76"+
-    "\1\26\6\0\1\76\3\0\1\135\1\273\1\274\1\275"+
-    "\1\276\1\277\1\300\1\301\1\302\1\303\1\304\1\305"+
-    "\1\306\1\307\1\310\1\311\1\312\1\313\1\314\1\315"+
-    "\1\316\1\317\1\320\1\321\1\322\1\323\1\324\1\136"+
-    "\12\330\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\1\162\3\0\2\136\151\0\4\u015a\2\0\1\u015a"+
-    "\15\0\1\u015a\6\0\12\u015a\1\334\175\0\4\u015b\2\0"+
-    "\1\u015b\15\0\1\u015b\6\0\12\u015b\1\u015c\175\0\4\u015d"+
-    "\2\0\1\u015d\15\0\1\u015d\6\0\1\u015e\2\u015f\1\u015e"+
-    "\5\u015f\1\u0160\14\0\1\u0122\161\0\4\u0161\2\0\1\u0161"+
-    "\15\0\1\u0161\6\0\12\u0161\1\u0162\13\0\1\u0122\160\0"+
-    "\1\u0163\4\u0161\2\0\1\u0161\15\0\1\u0161\6\0\12\u0164"+
-    "\1\u0162\13\0\1\u0122\160\0\1\u0163\4\u0161\2\0\1\u0161"+
-    "\15\0\1\u0161\6\0\12\u0165\1\u0162\13\0\1\u0122\160\0"+
-    "\1\u0163\4\u0161\2\0\1\u0161\15\0\1\u0161\6\0\1\u0164"+
-    "\1\u0166\1\u0165\2\u0164\2\u0165\1\u0164\1\u0165\1\u0164\1\u0162"+
-    "\13\0\1\u0122\226\0\1\u014f\7\0\1\u0167\1\u0168\1\u0169"+
-    "\162\0\1\346\1\230\2\u016a\1\u016b\1\u016c\10\u016a\1\230"+
-    "\1\u016d\5\u016a\6\230\1\347\12\230\175\0\1\346\1\u016e"+
-    "\2\u016a\1\230\1\u016a\1\u016f\6\u016a\4\230\4\u016a\1\230"+
-    "\1\u016a\1\230\3\u016a\1\347\12\230\175\0\1\346\3\230"+
-    "\1\u016a\1\230\1\u016a\4\230\1\u016a\10\230\1\u016a\2\230"+
-    "\1\u016a\2\230\1\u016a\1\347\12\230\175\0\1\346\1\230"+
-    "\1\u016a\1\u0170\2\u016a\2\230\1\u016a\6\230\3\u016a\11\230"+
-    "\1\347\12\230\175\0\1\346\3\230\1\u016a\1\230\1\u016a"+
-    "\10\230\1\u016a\1\230\2\u016a\10\230\1\347\12\230\175\0"+
-    "\1\346\4\230\1\u0171\5\230\1\u016a\17\230\1\347\12\230"+
-    "\175\0\1\346\4\230\2\u016a\2\230\1\u016a\1\230\1\u016a"+
-    "\13\230\1\u016a\2\230\1\u016a\1\347\12\230\175\0\1\346"+
-    "\1\u016a\1\230\3\u016a\1\u0172\14\u016a\2\230\2\u016a\2\230"+
-    "\1\u016a\1\230\1\347\12\230\175\0\1\346\2\230\4\u016a"+
-    "\3\230\2\u016a\1\u0173\1\u016a\1\230\2\u016a\12\230\1\347"+
-    "\12\230\175\0\1\346\2\u016a\2\230\1\u016a\3\230\1\u016a"+
-    "\5\230\3\u016a\3\230\1\u016a\2\230\3\u016a\1\347\12\230"+
-    "\175\0\1\346\5\u016a\1\u0174\1\230\1\u016a\1\u0175\7\u016a"+
-    "\1\u0176\3\u016a\1\230\1\u016a\1\230\3\u016a\1\347\12\230"+
-    "\175\0\1\346\1\u0177\1\u016a\1\230\1\u016e\6\u016a\3\230"+
-    "\1\u016a\2\230\1\u016a\2\230\1\u016a\6\230\1\347\12\230"+
-    "\175\0\1\346\1\u016a\31\230\1\347\12\230\175\0\1\346"+
-    "\1\u016a\2\230\1\u016a\1\u0178\1\230\2\u016a\1\230\3\u016a"+
-    "\2\230\2\u016a\1\230\1\u016a\3\230\1\u016a\2\230\2\u016a"+
-    "\1\347\12\230\175\0\1\346\6\u016a\1\230\5\u016a\3\230"+
-    "\2\u016a\1\230\10\u016a\1\347\12\230\175\0\1\346\1\230"+
-    "\2\u016a\1\u0175\1\u0179\3\u016a\1\230\3\u016a\1\230\1\u016a"+
-    "\1\230\1\u016a\1\230\1\u016a\1\230\1\u016a\1\230\3\u016a"+
-    "\1\230\1\u016a\1\347\12\230\175\0\1\346\1\u016a\6\230"+
-    "\1\u016a\6\230\1\u016a\4\230\1\u016a\4\230\2\u016a\1\347"+
-    "\12\230\175\0\1\346\6\230\1\u016a\7\230\1\u016a\13\230"+
-    "\1\347\12\230\175\0\1\346\13\230\1\u017a\6\230\1\u017b"+
-    "\7\230\1\347\12\230\175\0\1\346\1\u016a\11\230\1\u016a"+
-    "\6\230\1\u016a\10\230\1\347\12\230\175\0\1\346\1\u016a"+
-    "\1\230\6\u016a\1\u017c\1\230\2\u016a\2\230\2\u016a\1\230"+
-    "\1\u016a\1\230\6\u016a\1\230\1\347\12\230\175\0\1\346"+
-    "\4\230\1\u016a\5\230\2\u016a\3\230\2\u016a\10\230\1\u016a"+
-    "\1\347\12\230\175\0\1\346\3\230\1\u016a\1\230\1\u017d"+
-    "\4\230\1\u016a\2\230\1\u016a\14\230\1\347\12\230\175\0"+
-    "\1\346\2\u016a\1\230\1\u016a\3\230\2\u016a\2\230\1\u016a"+
-    "\4\230\1\u016a\11\230\1\347\12\230\175\0\1\346\3\230"+
-    "\1\u016a\13\230\1\u016a\12\230\1\347\12\230\175\0\1\346"+
-    "\3\230\2\u016a\2\230\2\u016a\1\230\2\u016a\1\230\1\u016a"+
-    "\3\230\1\u016a\1\230\1\u016a\1\230\1\u016a\2\230\1\u016a"+
-    "\1\230\1\347\12\230\27\0\1\47\1\0\1\50\1\0"+
-    "\1\51\1\0\1\52\1\0\1\53\1\0\1\173\3\0"+
-    "\1\174\5\0\1\175\3\0\1\176\11\0\1\60\2\0"+
-    "\1\177\16\0\1\200\2\0\1\201\41\0\1\25\1\65"+
-    "\7\0\1\65\2\0\1\25\1\135\1\232\1\233\1\234"+
-    "\1\235\1\236\1\237\1\240\1\241\1\242\1\243\1\244"+
-    "\1\245\1\246\1\247\1\250\1\251\1\252\1\253\1\254"+
-    "\1\255\1\256\1\257\1\260\1\261\1\262\1\263\1\136"+
-    "\12\264\1\u014f\3\136\1\0\2\136\1\137\1\u013f\1\u0140"+
-    "\1\u0141\3\0\1\136\1\162\3\0\2\136\2\0\1\47"+
-    "\1\0\1\50\1\0\1\51\1\0\1\52\1\0\1\53"+
-    "\1\0\1\173\3\0\1\174\5\0\1\175\3\0\1\176"+
-    "\11\0\1\60\2\0\1\177\16\0\1\200\2\0\1\201"+
-    "\41\0\1\25\1\65\7\0\1\65\2\0\1\25\1\0"+
-    "\32\25\1\0\12\u017e\175\0\1\u017f\45\u013f\1\u0167\2\u013f"+
-    "\1\u0180\1\u0167\2\u013f\1\u0181\2\u013f\1\u0141\2\0\1\u0167"+
-    "\1\u013f\4\0\1\u013f\1\136\150\0\1\u0182\45\u0140\1\u0168"+
-    "\2\u0140\1\u0183\1\0\2\136\1\u0184\1\u013f\1\u0140\1\u0141"+
-    "\2\0\1\u0168\1\u0140\4\0\2\136\150\0\1\u0185\45\u0141"+
-    "\1\u0169\2\u0141\1\u0186\1\u0169\2\u0141\1\u0187\2\u0141\1\136"+
-    "\2\0\1\u0169\1\u0141\4\0\1\u0141\1\136\2\0\1\47"+
-    "\1\0\1\50\1\0\1\51\1\0\1\52\1\0\1\53"+
-    "\1\0\1\54\3\0\1\55\5\0\1\56\3\0\1\57"+
-    "\11\0\1\60\2\0\1\61\16\0\1\62\2\0\1\63"+
-    "\41\0\2\25\1\64\1\0\1\65\1\0\1\65\1\66"+
-    "\1\0\1\25\2\0\1\25\1\140\5\36\1\352\24\36"+
-    "\1\141\12\142\1\65\1\136\1\143\1\136\1\0\1\136"+
-    "\1\144\1\137\3\136\3\0\1\136\4\0\2\136\2\0"+
-    "\1\47\1\0\1\50\1\0\1\51\1\0\1\52\1\0"+
-    "\1\53\1\0\1\54\3\0\1\55\5\0\1\56\3\0"+
-    "\1\57\11\0\1\60\2\0\1\61\16\0\1\62\2\0"+
-    "\1\63\41\0\2\25\1\64\1\0\1\65\1\0\1\65"+
-    "\1\66\1\0\1\25\2\0\1\25\1\140\15\36\1\352"+
-    "\14\36\1\141\12\142\1\65\1\136\1\143\1\136\1\0"+
-    "\1\136\1\144\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\2\0\1\47\1\0\1\50\1\0\1\51\1\0\1\52"+
-    "\1\0\1\53\1\0\1\54\3\0\1\55\5\0\1\56"+
-    "\3\0\1\57\11\0\1\60\2\0\1\61\16\0\1\62"+
-    "\2\0\1\63\41\0\2\25\1\64\1\0\1\65\1\0"+
-    "\1\65\1\66\1\0\1\25\2\0\1\25\1\140\10\36"+
-    "\1\352\21\36\1\141\12\142\1\65\1\136\1\143\1\136"+
-    "\1\0\1\136\1\144\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\2\0\1\47\1\0\1\50\1\0\1\51\1\0"+
-    "\1\52\1\0\1\53\1\0\1\54\3\0\1\55\5\0"+
-    "\1\56\3\0\1\57\11\0\1\60\2\0\1\61\16\0"+
-    "\1\62\2\0\1\63\41\0\2\25\1\64\1\0\1\65"+
-    "\1\0\1\65\1\66\1\0\1\25\2\0\1\25\1\140"+
-    "\3\36\1\u0188\26\36\1\141\12\142\1\65\1\136\1\143"+
-    "\1\136\1\0\1\136\1\144\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\2\0\1\47\1\0\1\50\1\0\1\51"+
-    "\1\0\1\52\1\0\1\53\1\0\1\54\3\0\1\55"+
-    "\5\0\1\56\3\0\1\57\11\0\1\60\2\0\1\61"+
-    "\16\0\1\62\2\0\1\63\41\0\2\25\1\64\1\0"+
-    "\1\65\1\0\1\65\1\66\1\0\1\25\2\0\1\25"+
-    "\1\140\3\36\1\352\26\36\1\141\12\142\1\65\1\136"+
-    "\1\143\1\136\1\0\1\136\1\144\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\2\0\1\47\1\0\1\50\1\0"+
-    "\1\51\1\0\1\52\1\0\1\53\1\0\1\54\3\0"+
-    "\1\55\5\0\1\56\3\0\1\57\11\0\1\60\2\0"+
-    "\1\61\16\0\1\62\2\0\1\63\41\0\2\25\1\64"+
-    "\1\0\1\65\1\0\1\65\1\66\1\0\1\25\2\0"+
-    "\1\25\1\140\27\36\1\u0189\2\36\1\141\12\142\1\65"+
-    "\1\136\1\143\1\136\1\0\1\136\1\144\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\135\32\264\1\u018a"+
-    "\12\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\2\0\1\47\1\0\1\50\1\0"+
-    "\1\51\1\0\1\52\1\0\1\53\1\0\1\54\3\0"+
-    "\1\55\5\0\1\56\3\0\1\57\11\0\1\60\2\0"+
-    "\1\61\16\0\1\62\2\0\1\63\41\0\2\25\1\64"+
-    "\1\0\1\65\1\0\1\65\1\66\1\0\1\25\2\0"+
-    "\1\25\1\140\16\36\1\352\13\36\1\141\12\142\1\65"+
-    "\1\136\1\143\1\136\1\0\1\136\1\144\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\2\0\1\47\1\0\1\50"+
-    "\1\0\1\51\1\0\1\52\1\0\1\53\1\0\1\173"+
-    "\3\0\1\174\5\0\1\175\3\0\1\176\11\0\1\60"+
-    "\2\0\1\177\16\0\1\200\2\0\1\201\41\0\1\25"+
-    "\1\65\7\0\1\65\2\0\1\25\1\0\32\25\24\0"+
-    "\1\u018b\242\0\1\u018c\15\0\1\47\1\0\1\50\1\0"+
-    "\1\51\1\0\1\52\1\0\1\53\1\0\1\54\3\0"+
-    "\1\55\5\0\1\56\3\0\1\57\11\0\1\60\2\0"+
-    "\1\61\16\0\1\62\2\0\1\63\41\0\2\25\1\64"+
-    "\1\0\1\65\1\0\1\65\1\66\1\0\1\25\2\0"+
-    "\1\25\1\140\32\36\1\141\12\142\1\u014d\1\136\1\143"+
-    "\1\136\1\0\1\136\1\144\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\2\0\1\47\1\0\1\50\1\0\1\51"+
-    "\1\0\1\52\1\0\1\53\1\0\1\173\3\0\1\174"+
-    "\5\0\1\175\3\0\1\176\11\0\1\60\2\0\1\177"+
-    "\16\0\1\200\2\0\1\201\41\0\1\25\1\65\7\0"+
-    "\1\65\2\0\1\25\1\0\32\25\24\0\1\u018d\163\0"+
-    "\1\135\1\273\1\274\1\275\1\276\1\277\1\300\1\301"+
-    "\1\302\1\303\1\304\1\305\1\306\1\307\1\310\1\311"+
-    "\1\312\1\313\1\314\1\315\1\316\1\317\1\320\1\321"+
-    "\1\322\1\323\1\324\1\136\12\264\1\u014f\3\136\1\0"+
-    "\2\136\1\137\1\u013f\1\u0140\1\u0141\3\0\1\136\1\162"+
-    "\3\0\2\136\204\0\12\u017e\175\0\1\376\5\264\1\u0103"+
-    "\24\264\1\141\12\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\15\264"+
-    "\1\u0103\14\264\1\141\12\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\10\264\1\u0103\21\264\1\141\12\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\150\0"+
-    "\1\376\3\264\1\u018e\26\264\1\141\12\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\376\3\264\1\u0103\26\264\1\141\12\264\1\0"+
-    "\3\136\1\0\2\136\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\150\0\1\376\27\264\1\u018f\2\264\1\141\12\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\16\264\1\u0103\13\264\1\141"+
-    "\12\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\2\0\1\47\1\0\1\50\1\0"+
-    "\1\51\1\0\1\52\1\0\1\53\1\0\1\67\3\0"+
-    "\1\70\5\0\1\71\3\0\1\72\11\0\1\60\2\0"+
-    "\1\73\16\0\1\74\2\0\1\75\41\0\1\25\2\26"+
-    "\2\0\2\76\1\77\1\0\1\26\2\0\1\25\1\u0190"+
-    "\32\36\1\141\12\u0158\1\0\1\136\1\151\1\136\1\0"+
-    "\2\152\1\137\3\136\2\0\1\76\1\136\4\0\2\136"+
-    "\2\0\1\47\1\0\1\50\1\0\1\51\1\0\1\52"+
-    "\1\0\1\53\1\0\1\67\3\0\1\70\5\0\1\71"+
-    "\3\0\1\72\11\0\1\60\2\0\1\73\16\0\1\74"+
-    "\2\0\1\75\41\0\1\25\2\26\2\0\2\76\1\77"+
-    "\1\0\1\26\2\0\1\25\1\u0190\32\36\1\141\12\u0191"+
-    "\1\0\1\136\1\151\1\136\1\0\2\152\1\137\3\136"+
-    "\2\0\1\76\1\136\4\0\2\136\2\0\1\47\1\0"+
-    "\1\50\1\0\1\51\1\0\1\52\1\0\1\53\1\0"+
-    "\1\67\3\0\1\70\5\0\1\71\3\0\1\72\11\0"+
-    "\1\60\2\0\1\73\16\0\1\74\2\0\1\75\41\0"+
-    "\1\25\2\26\2\0\2\76\1\77\1\0\1\26\2\0"+
-    "\1\25\1\u0190\32\36\1\141\1\u0158\1\u0192\1\u0191\2\u0158"+
-    "\2\u0191\1\u0158\1\u0191\1\u0158\1\0\1\136\1\151\1\136"+
-    "\1\0\2\152\1\137\3\136\2\0\1\76\1\136\4\0"+
-    "\2\136\216\0\1\334\175\0\4\u0193\2\0\1\u0193\15\0"+
-    "\1\u0193\6\0\12\u0193\1\u015c\175\0\4\u0194\2\0\1\u0194"+
-    "\15\0\1\u0194\6\0\12\u0194\1\u0195\175\0\4\u0196\2\0"+
-    "\1\u0196\15\0\1\u0196\6\0\12\u0196\1\u0197\13\0\1\u0122"+
-    "\160\0\1\u0163\4\u0196\2\0\1\u0196\15\0\1\u0196\6\0"+
-    "\12\u0198\1\u0197\13\0\1\u0122\160\0\1\u0163\4\u0196\2\0"+
-    "\1\u0196\15\0\1\u0196\6\0\12\u0199\1\u0197\13\0\1\u0122"+
-    "\160\0\1\u0163\4\u0196\2\0\1\u0196\15\0\1\u0196\6\0"+
-    "\1\u0198\1\u019a\1\u0199\2\u0198\2\u0199\1\u0198\1\u0199\1\u0198"+
-    "\1\u0197\13\0\1\u0122\161\0\4\u019b\2\0\1\u019b\15\0"+
-    "\1\u019b\6\0\12\u019b\1\u0162\13\0\1\u0122\161\0\4\u015d"+
-    "\2\0\1\u015d\15\0\1\u015d\6\0\1\u015e\2\u015f\1\u015e"+
-    "\5\u015f\1\u0160\231\0\1\u019c\2\u019d\1\u019c\5\u019d\1\u019e"+
-    "\175\0\1\u0163\4\u019b\2\0\1\u019b\15\0\1\u019b\6\0"+
-    "\12\u019f\1\u0162\13\0\1\u0122\160\0\1\u0163\4\u019b\2\0"+
-    "\1\u019b\15\0\1\u019b\6\0\12\u019b\1\u0162\13\0\1\u0122"+
-    "\160\0\1\u0163\4\u019b\2\0\1\u019b\15\0\1\u019b\6\0"+
-    "\2\u019f\1\u019b\2\u019f\2\u019b\1\u019f\1\u019b\1\u019f\1\u0162"+
-    "\13\0\1\u0122\160\0\51\u0167\1\u01a0\6\u0167\1\u0169\2\0"+
-    "\2\u0167\4\0\1\u0167\151\0\51\u0168\1\u01a1\3\0\1\u0168"+
-    "\1\u0167\1\u0168\1\u0169\2\0\2\u0168\156\0\51\u0169\1\u01a2"+
-    "\6\u0169\3\0\2\u0169\4\0\1\u0169\151\0\1\u01a3\32\230"+
-    "\1\347\12\230\175\0\1\u01a3\4\230\1\u01a4\25\230\1\347"+
-    "\12\230\175\0\1\u01a3\15\230\1\u012f\14\230\1\347\12\230"+
-    "\175\0\1\u01a3\10\230\1\u012f\21\230\1\347\12\230\175\0"+
-    "\1\u01a3\17\230\1\u016a\12\230\1\347\12\230\175\0\1\u01a3"+
-    "\5\230\1\u01a5\4\230\1\u016a\17\230\1\347\12\230\175\0"+
-    "\1\346\20\230\1\u016a\11\230\1\347\12\230\175\0\1\346"+
-    "\7\230\1\u016a\22\230\1\347\12\230\175\0\1\346\27\230"+
-    "\1\u016a\2\230\1\347\12\230\175\0\1\u01a3\6\230\1\u01a4"+
-    "\10\230\1\u016a\12\230\1\347\12\230\175\0\1\u01a3\24\230"+
-    "\1\u01a6\5\230\1\347\12\230\175\0\1\346\11\230\1\u016a"+
-    "\20\230\1\347\12\230\175\0\1\u01a3\16\230\1\u01a7\13\230"+
-    "\1\347\12\230\175\0\1\u01a3\12\230\1\u01a8\17\230\1\347"+
-    "\12\230\175\0\1\u01a3\5\230\1\u016a\24\230\1\347\12\230"+
-    "\175\0\1\u01a3\1\u01a9\31\230\1\347\12\230\175\0\1\346"+
-    "\32\230\1\u01aa\12\230\175\0\1\346\22\230\1\u016a\7\230"+
-    "\1\347\12\230\175\0\1\u01a3\23\230\1\u016a\6\230\1\347"+
-    "\12\230\175\0\1\u01a3\24\230\1\u01ab\5\230\1\347\12\230"+
-    "\231\0\12\u01ac\10\0\1\u0167\1\u0168\1\u0169\162\0\1\u017f"+
-    "\45\u013f\1\u0167\2\u013f\1\u0180\1\u0167\2\u013f\1\u0181\2\u013f"+
-    "\1\u0141\2\0\1\u0167\1\u013f\1\162\3\0\1\u013f\1\136"+
-    "\150\0\1\135\4\u01ad\2\136\1\u01ad\15\136\1\u01ad\6\136"+
-    "\12\u01ad\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\51\u0167\1\u01a0\6\u0167\1\u0169"+
-    "\1\231\1\0\2\u0167\4\0\1\u0167\151\0\1\u0182\45\u0140"+
-    "\1\u0168\2\u0140\1\u0183\1\0\2\136\1\u0184\1\u013f\1\u0140"+
-    "\1\u0141\2\0\1\u0168\1\u0140\1\162\3\0\2\136\150\0"+
-    "\1\135\4\u01ae\2\136\1\u01ae\15\136\1\u01ae\6\136\12\u01ae"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\51\u0168\1\u01a1\3\0\1\u0168\1\u0167"+
-    "\1\u0168\1\u0169\1\231\1\0\2\u0168\156\0\1\u0185\45\u0141"+
-    "\1\u0169\2\u0141\1\u0186\1\u0169\2\u0141\1\u0187\2\u0141\1\136"+
-    "\2\0\1\u0169\1\u0141\1\162\3\0\1\u0141\1\136\150\0"+
-    "\1\135\4\u01af\2\136\1\u01af\15\136\1\u01af\6\136\12\u01af"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\51\u0169\1\u01a2\6\u0169\1\0\1\231"+
-    "\1\0\2\u0169\4\0\1\u0169\3\0\1\47\1\0\1\50"+
-    "\1\0\1\51\1\0\1\52\1\0\1\53\1\0\1\54"+
-    "\3\0\1\55\5\0\1\56\3\0\1\57\11\0\1\60"+
-    "\2\0\1\61\16\0\1\62\2\0\1\63\41\0\2\25"+
-    "\1\64\1\0\1\65\1\0\1\65\1\66\1\0\1\25"+
-    "\2\0\1\25\1\140\20\36\1\u01b0\11\36\1\141\12\142"+
-    "\1\65\1\136\1\143\1\136\1\0\1\136\1\144\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\2\0\1\47\1\0"+
-    "\1\50\1\0\1\51\1\0\1\52\1\0\1\53\1\0"+
-    "\1\54\3\0\1\55\5\0\1\56\3\0\1\57\11\0"+
-    "\1\60\2\0\1\61\16\0\1\62\2\0\1\63\41\0"+
-    "\2\25\1\64\1\0\1\65\1\0\1\65\1\66\1\0"+
-    "\1\25\2\0\1\25\1\140\3\36\1\365\26\36\1\141"+
-    "\12\142\1\65\1\136\1\143\1\136\1\0\1\136\1\144"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\135"+
-    "\1\264\1\u01b1\1\u01b2\2\264\1\u01b3\1\u01b4\1\u01b5\1\264"+
-    "\1\u01b6\1\u01b7\2\264\1\u01b8\1\u01b9\2\264\1\u01ba\1\u01bb"+
-    "\1\u01bc\1\264\1\u01bd\1\u01be\1\264\1\u01bf\1\u01c0\1\141"+
-    "\1\u01c1\2\264\1\u01c2\1\u01c3\1\u01c4\1\264\1\u01c5\1\u01c6"+
-    "\1\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\227\0\1\u01c7\163\0\1\u01c8\32\u01c9"+
-    "\1\u01c8\12\u01c9\1\u01ca\2\u01c8\1\u01cb\3\u01c8\1\u01cc\3\0"+
-    "\1\u01cd\1\0\2\u01c8\4\0\1\u01c8\230\0\1\u01ce\163\0"+
-    "\1\376\20\264\1\u01cf\11\264\1\141\12\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\376\3\264\1\u010e\26\264\1\141\12\264\1\0"+
-    "\3\136\1\0\2\136\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\14\0\1\207\3\0\1\210\5\0\1\211\3\0"+
-    "\1\212\14\0\1\213\16\0\1\214\2\0\1\215\42\0"+
-    "\1\76\1\26\6\0\1\76\3\0\1\135\1\273\1\274"+
-    "\1\275\1\276\1\277\1\300\1\301\1\302\1\303\1\304"+
-    "\1\305\1\306\1\307\1\310\1\311\1\312\1\313\1\314"+
-    "\1\315\1\316\1\317\1\320\1\321\1\322\1\323\1\324"+
-    "\1\136\1\u01d0\2\u01d1\1\u01d0\5\u01d1\1\u01d2\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\1\162\3\0"+
-    "\2\136\2\0\1\47\1\0\1\50\1\0\1\51\1\0"+
-    "\1\52\1\0\1\53\1\0\1\67\3\0\1\70\5\0"+
-    "\1\71\3\0\1\72\11\0\1\60\2\0\1\73\16\0"+
-    "\1\74\2\0\1\75\41\0\1\25\2\26\2\0\2\76"+
-    "\1\77\1\0\1\26\2\0\1\25\1\u0190\32\36\1\141"+
-    "\12\330\1\0\1\136\1\151\1\136\1\0\2\152\1\137"+
-    "\3\136\2\0\1\76\1\136\4\0\2\136\2\0\1\47"+
-    "\1\0\1\50\1\0\1\51\1\0\1\52\1\0\1\53"+
-    "\1\0\1\67\3\0\1\70\5\0\1\71\3\0\1\72"+
-    "\11\0\1\60\2\0\1\73\16\0\1\74\2\0\1\75"+
-    "\41\0\1\25\2\26\2\0\2\76\1\77\1\0\1\26"+
-    "\2\0\1\25\1\u0190\32\36\1\141\2\u0191\1\330\2\u0191"+
-    "\2\330\1\u0191\1\330\1\u0191\1\0\1\136\1\151\1\136"+
-    "\1\0\2\152\1\137\3\136\2\0\1\76\1\136\4\0"+
-    "\2\136\151\0\4\u01d3\2\0\1\u01d3\15\0\1\u01d3\6\0"+
-    "\12\u01d3\1\u015c\175\0\4\u01d4\2\0\1\u01d4\15\0\1\u01d4"+
-    "\6\0\12\u01d4\1\u01d5\175\0\4\u01d6\2\0\1\u01d6\15\0"+
-    "\1\u01d6\6\0\1\u01d7\2\u01d8\1\u01d7\5\u01d8\1\u01d9\14\0"+
-    "\1\u0122\161\0\4\u01da\2\0\1\u01da\15\0\1\u01da\6\0"+
-    "\12\u01da\1\u0197\13\0\1\u0122\161\0\4\u01d6\2\0\1\u01d6"+
-    "\15\0\1\u01d6\6\0\1\u01d7\2\u01d8\1\u01d7\5\u01d8\1\u01d9"+
-    "\175\0\1\u0163\4\u01da\2\0\1\u01da\15\0\1\u01da\6\0"+
-    "\12\u01db\1\u0197\13\0\1\u0122\160\0\1\u0163\4\u01da\2\0"+
-    "\1\u01da\15\0\1\u01da\6\0\12\u01da\1\u0197\13\0\1\u0122"+
-    "\160\0\1\u0163\4\u01da\2\0\1\u01da\15\0\1\u01da\6\0"+
-    "\2\u01db\1\u01da\2\u01db\2\u01da\1\u01db\1\u01da\1\u01db\1\u0197"+
-    "\13\0\1\u0122\161\0\4\u01dc\2\0\1\u01dc\15\0\1\u01dc"+
-    "\6\0\12\u01dc\1\u0162\13\0\1\u0122\160\0\1\u01dd\33\0"+
-    "\12\u019d\175\0\1\u01dd\33\0\12\u01de\175\0\1\u01dd\33\0"+
-    "\1\u019d\1\u01df\1\u01de\2\u019d\2\u01de\1\u019d\1\u01de\1\u019d"+
-    "\175\0\1\u0163\4\u01dc\2\0\1\u01dc\15\0\1\u01dc\6\0"+
-    "\12\u01dc\1\u0162\13\0\1\u0122\161\0\4\u01e0\2\0\1\u01e0"+
-    "\15\0\1\u01e0\6\0\12\u01e0\176\0\4\u01e1\2\0\1\u01e1"+
-    "\15\0\1\u01e1\6\0\12\u01e1\176\0\4\u01e2\2\0\1\u01e2"+
-    "\15\0\1\u01e2\6\0\12\u01e2\175\0\1\346\5\230\1\u016a"+
-    "\24\230\1\347\12\230\175\0\1\346\15\230\1\u016a\14\230"+
-    "\1\347\12\230\175\0\1\346\10\230\1\u016a\21\230\1\347"+
-    "\12\230\175\0\1\346\3\230\1\u01e3\26\230\1\347\12\230"+
-    "\175\0\1\346\3\230\1\u016a\26\230\1\347\12\230\175\0"+
-    "\1\346\27\230\1\u01e4\2\230\1\347\12\230\176\0\32\230"+
-    "\1\u01e5\12\230\175\0\1\346\16\230\1\u016a\13\230\1\347"+
-    "\12\230\231\0\12\u01e6\10\0\1\u0167\1\u0168\1\u0169\162\0"+
-    "\1\135\4\u013f\2\136\1\u013f\15\136\1\u013f\6\136\12\u013f"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\135\4\u0140\2\136\1\u0140\15\136"+
-    "\1\u0140\6\136\12\u0140\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\135\4\u0141"+
-    "\2\136\1\u0141\15\136\1\u0141\6\136\12\u0141\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\2\0\1\47\1\0\1\50\1\0\1\51\1\0\1\52"+
-    "\1\0\1\53\1\0\1\54\3\0\1\55\5\0\1\56"+
-    "\3\0\1\57\11\0\1\60\2\0\1\61\16\0\1\62"+
-    "\2\0\1\63\41\0\2\25\1\64\1\0\1\65\1\0"+
-    "\1\65\1\66\1\0\1\25\2\0\1\25\1\140\12\36"+
-    "\1\352\17\36\1\141\12\142\1\65\1\136\1\143\1\136"+
-    "\1\0\1\136\1\144\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\150\0\1\376\11\264\1\u01e7\20\264\1\141\12\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\3\264\1\u01e8\26\264\1\141"+
-    "\12\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\7\264\1\u01e9\22\264"+
-    "\1\141\4\264\1\u01ea\5\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\10\264\1\u01eb\4\264\1\u01ec\5\264\1\u01ed\6\264\1\141"+
-    "\12\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\3\264\1\u01ee\26\264"+
-    "\1\141\2\264\1\u01ef\7\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\7\264\1\u01f0\22\264\1\141\12\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\150\0"+
-    "\1\376\7\264\1\u01f1\22\264\1\141\12\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\376\7\264\1\u01f2\22\264\1\141\3\264\1\u01f3"+
-    "\6\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\32\264\1\141\5\264"+
-    "\1\u01f4\4\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\7\264\1\u01f5"+
-    "\22\264\1\141\12\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\31\264"+
-    "\1\u01f6\1\141\12\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\1\264"+
-    "\1\u01f7\30\264\1\141\12\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\7\264\1\u01f8\1\264\1\u01f9\20\264\1\141\11\264\1\u01f4"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\22\264\1\u01fa\7\264\1\141"+
-    "\2\264\1\u01fb\7\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\6\264"+
-    "\1\u01fc\1\u01fd\22\264\1\141\12\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\150\0"+
-    "\1\376\7\264\1\u01fe\5\264\1\u01ff\14\264\1\141\12\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\23\264\1\u0200\6\264\1\141"+
-    "\12\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\32\264\1\141\3\264"+
-    "\1\u0201\6\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\3\264\1\u0202"+
-    "\26\264\1\141\12\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\17\264"+
-    "\1\u0203\12\264\1\141\1\u0204\11\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\150\0"+
-    "\1\376\32\264\1\141\1\264\1\u01f4\10\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\376\32\264\1\141\1\u0205\11\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\151\0\32\u0206\1\0\12\u0206\11\0\1\u0207\1\0\1\u0208"+
-    "\161\0\46\u01c8\1\u01ca\2\u01c8\1\u01cb\3\u01c8\1\u01cc\5\0"+
-    "\2\u01c8\4\0\1\u01c8\151\0\1\u0209\32\u01c9\1\u020a\12\u01c9"+
-    "\1\u020b\2\u01c8\1\u01cb\3\u01c8\1\u01cc\1\0\1\u020c\3\0"+
-    "\2\u01c8\4\0\1\u01c8\151\0\46\u01ca\1\0\2\u01ca\1\u020d"+
-    "\3\u01ca\1\u01cc\5\0\2\u01ca\4\0\1\u01ca\152\0\4\u020e"+
-    "\2\0\1\u020e\15\0\1\u020e\6\0\12\u020e\176\0\32\u020f"+
-    "\1\0\12\u020f\13\0\1\u01cd\162\0\4\u0210\2\0\1\u0210"+
-    "\15\0\1\u0210\6\0\12\u0210\1\u0211\174\0\1\u0212\32\u0213"+
-    "\1\u0212\12\u0213\1\u0214\2\u0212\1\u0215\3\u0212\1\u0216\3\0"+
-    "\1\u0217\1\0\2\u0212\4\0\1\u0212\151\0\1\376\12\264"+
-    "\1\u0103\17\264\1\141\12\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\2\0\1\47"+
-    "\1\0\1\50\1\0\1\51\1\0\1\52\1\0\1\53"+
-    "\1\0\1\67\3\0\1\70\5\0\1\71\3\0\1\72"+
-    "\11\0\1\60\2\0\1\73\16\0\1\74\2\0\1\75"+
-    "\41\0\1\25\2\26\2\0\2\76\1\77\1\0\1\26"+
-    "\2\0\1\25\1\u011a\32\36\1\141\12\u01d1\1\u014f\1\136"+
-    "\1\151\1\136\1\0\2\152\1\137\1\u013f\1\u0140\1\u0141"+
-    "\2\0\1\76\1\136\4\0\2\136\2\0\1\47\1\0"+
-    "\1\50\1\0\1\51\1\0\1\52\1\0\1\53\1\0"+
-    "\1\67\3\0\1\70\5\0\1\71\3\0\1\72\11\0"+
-    "\1\60\2\0\1\73\16\0\1\74\2\0\1\75\41\0"+
-    "\1\25\2\26\2\0\2\76\1\77\1\0\1\26\2\0"+
-    "\1\25\1\u011a\32\36\1\141\12\u0218\1\u014f\1\136\1\151"+
-    "\1\136\1\0\2\152\1\137\1\u013f\1\u0140\1\u0141\2\0"+
-    "\1\76\1\136\4\0\2\136\2\0\1\47\1\0\1\50"+
-    "\1\0\1\51\1\0\1\52\1\0\1\53\1\0\1\67"+
-    "\3\0\1\70\5\0\1\71\3\0\1\72\11\0\1\60"+
-    "\2\0\1\73\16\0\1\74\2\0\1\75\41\0\1\25"+
-    "\2\26\2\0\2\76\1\77\1\0\1\26\2\0\1\25"+
-    "\1\u011a\32\36\1\141\1\u01d1\1\u0219\1\u0218\2\u01d1\2\u0218"+
-    "\1\u01d1\1\u0218\1\u01d1\1\u014f\1\136\1\151\1\136\1\0"+
-    "\2\152\1\137\1\u013f\1\u0140\1\u0141\2\0\1\76\1\136"+
-    "\4\0\2\136\216\0\1\u015c\175\0\4\u021a\2\0\1\u021a"+
-    "\15\0\1\u021a\6\0\12\u021a\1\u01d5\175\0\4\u021b\2\0"+
-    "\1\u021b\15\0\1\u021b\6\0\12\u021b\1\u021c\175\0\4\u021d"+
-    "\2\0\1\u021d\15\0\1\u021d\6\0\12\u021d\1\u021e\13\0"+
-    "\1\u0122\160\0\1\u0163\4\u021d\2\0\1\u021d\15\0\1\u021d"+
-    "\6\0\12\u021f\1\u021e\13\0\1\u0122\160\0\1\u0163\4\u021d"+
-    "\2\0\1\u021d\15\0\1\u021d\6\0\12\u0220\1\u021e\13\0"+
-    "\1\u0122\160\0\1\u0163\4\u021d\2\0\1\u021d\15\0\1\u021d"+
-    "\6\0\1\u021f\1\u0221\1\u0220\2\u021f\2\u0220\1\u021f\1\u0220"+
-    "\1\u021f\1\u021e\13\0\1\u0122\161\0\4\u0222\2\0\1\u0222"+
-    "\15\0\1\u0222\6\0\12\u0222\1\u0197\13\0\1\u0122\160\0"+
-    "\1\u0163\4\u0222\2\0\1\u0222\15\0\1\u0222\6\0\12\u0222"+
-    "\1\u0197\13\0\1\u0122\226\0\1\u0162\13\0\1\u0122\214\0"+
-    "\1\u0223\2\u0224\1\u0223\5\u0224\1\u0225\175\0\1\u01dd\242\0"+
-    "\1\u01dd\33\0\2\u01de\1\0\2\u01de\2\0\1\u01de\1\0"+
-    "\1\u01de\176\0\4\u0167\2\0\1\u0167\15\0\1\u0167\6\0"+
-    "\12\u0167\176\0\4\u0168\2\0\1\u0168\15\0\1\u0168\6\0"+
-    "\12\u0168\176\0\4\u0169\2\0\1\u0169\15\0\1\u0169\6\0"+
-    "\12\u0169\175\0\1\346\20\230\1\u0226\11\230\1\347\12\230"+
-    "\175\0\1\346\3\230\1\u0175\26\230\1\347\12\230\176\0"+
-    "\1\230\1\u0227\1\u0228\2\230\1\u0229\1\u022a\1\u022b\1\230"+
-    "\1\u022c\1\u022d\2\230\1\u022e\1\u022f\2\230\1\u0230\1\u0231"+
-    "\1\u0232\1\230\1\u0233\1\u0234\1\230\1\u0235\1\u0236\1\347"+
-    "\1\u0237\2\230\1\u0238\1\u0239\1\u023a\1\230\1\u023b\1\u023c"+
-    "\1\230\231\0\12\u023d\10\0\1\u0167\1\u0168\1\u0169\162\0"+
-    "\1\376\1\264\1\u023e\30\264\1\141\12\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\376\24\264\1\u023f\5\264\1\141\12\264\1\0"+
-    "\3\136\1\0\2\136\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\150\0\1\376\24\264\1\u0240\5\264\1\141\12\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\1\264\1\u0241\30\264\1\141"+
-    "\12\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\14\264\1\u0242\15\264"+
-    "\1\141\12\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\1\264\1\u0243"+
-    "\30\264\1\141\12\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\1\264"+
-    "\1\u0244\30\264\1\141\12\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\1\264\1\u0245\30\264\1\141\12\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\150\0"+
-    "\1\376\21\264\1\u0246\10\264\1\141\12\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\376\24\264\1\u0247\5\264\1\141\12\264\1\0"+
-    "\3\136\1\0\2\136\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\150\0\1\376\24\264\1\u0248\5\264\1\141\12\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\24\264\1\u0249\5\264\1\141"+
-    "\12\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\1\u0152\31\264\1\141"+
-    "\12\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\24\264\1\u0245\5\264"+
-    "\1\141\12\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\24\264\1\u024a"+
-    "\5\264\1\141\12\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\1\264"+
-    "\1\u024b\30\264\1\141\12\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\31\264\1\u024c\1\141\12\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\24\264\1\u024d\5\264\1\141\12\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\150\0"+
-    "\1\376\1\264\1\u024e\30\264\1\141\12\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\376\1\u024f\31\264\1\141\12\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\376\21\264\1\u0250\10\264\1\141\12\264\1\0"+
-    "\3\136\1\0\2\136\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\150\0\1\376\4\264\1\u0251\25\264\1\141\12\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\24\264\1\u0252\5\264\1\141"+
-    "\12\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\24\264\1\u0253\5\264"+
-    "\1\141\12\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\4\264\1\u0254"+
-    "\25\264\1\141\12\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\21\264"+
-    "\1\u0255\10\264\1\141\12\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\24\264\1\u0256\5\264\1\141\12\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\150\0"+
-    "\1\376\32\264\1\141\1\u0257\11\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\150\0"+
-    "\1\376\32\264\1\141\7\264\1\u0258\2\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\376\1\u0259\31\264\1\141\12\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\376\1\u025a\31\264\1\141\12\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\u025b\32\u0206\1\u025c\12\u0206\11\0\1\u0207\163\0"+
-    "\51\u0207\1\u025d\3\0\3\u0207\1\u0169\3\0\1\u0207\157\0"+
-    "\4\u025e\2\0\1\u025e\15\0\1\u025e\6\0\12\u025e\1\u025f"+
-    "\174\0\1\u01c8\32\u01c9\1\u01c8\12\u01c9\1\u01ca\2\u01c8\1\u01cb"+
-    "\3\u01c8\1\u01cc\5\0\2\u01c8\4\0\1\u01c8\151\0\1\u01c8"+
-    "\32\u01c9\1\u020a\12\u01c9\1\u01ca\2\u01c8\1\u01cb\3\u01c8\1\u01cc"+
-    "\5\0\2\u01c8\4\0\1\u01c8\151\0\34\u01ca\12\u0260\1\0"+
-    "\2\u01ca\1\u020d\3\u01ca\1\u01cc\5\0\2\u01ca\4\0\1\u01ca"+
-    "\151\0\51\u020c\1\u0261\3\0\3\u020c\1\u0169\2\0\1\u0262"+
-    "\1\u020c\157\0\4\u0263\2\0\1\u0263\15\0\1\u0263\6\0"+
-    "\12\u0263\176\0\4\u01c8\2\0\1\u01c8\15\0\1\u01c8\6\0"+
-    "\12\u01c8\175\0\1\u0264\32\u020f\1\u0265\12\u020f\1\u0266\10\0"+
-    "\1\u020c\164\0\4\u0267\2\0\1\u0267\15\0\1\u0267\6\0"+
-    "\12\u0267\1\u0268\242\0\1\u0269\174\0\46\u0212\1\u0214\2\u0212"+
-    "\1\u0215\3\u0212\1\u0216\5\0\2\u0212\4\0\1\u0212\151\0"+
-    "\1\u026a\32\u0213\1\u026b\12\u0213\1\u026c\2\u0212\1\u0215\3\u0212"+
-    "\1\u0216\1\u0167\1\u0168\1\u0169\2\0\2\u0212\4\0\1\u0212"+
-    "\151\0\46\u0214\1\0\2\u0214\1\u026d\3\u0214\1\u0216\5\0"+
-    "\2\u0214\4\0\1\u0214\152\0\4\u026e\2\0\1\u026e\15\0"+
-    "\1\u026e\6\0\12\u026e\176\0\32\u026f\1\0\12\u026f\13\0"+
-    "\1\u0217\13\0\1\47\1\0\1\50\1\0\1\51\1\0"+
-    "\1\52\1\0\1\53\1\0\1\67\3\0\1\70\5\0"+
-    "\1\71\3\0\1\72\11\0\1\60\2\0\1\73\16\0"+
-    "\1\74\2\0\1\75\41\0\1\25\2\26\2\0\2\76"+
-    "\1\77\1\0\1\26\2\0\1\25\1\u011a\32\36\1\141"+
-    "\12\330\1\u014f\1\136\1\151\1\136\1\0\2\152\1\137"+
-    "\1\u013f\1\u0140\1\u0141\2\0\1\76\1\136\4\0\2\136"+
-    "\2\0\1\47\1\0\1\50\1\0\1\51\1\0\1\52"+
-    "\1\0\1\53\1\0\1\67\3\0\1\70\5\0\1\71"+
-    "\3\0\1\72\11\0\1\60\2\0\1\73\16\0\1\74"+
-    "\2\0\1\75\41\0\1\25\2\26\2\0\2\76\1\77"+
-    "\1\0\1\26\2\0\1\25\1\u011a\32\36\1\141\2\u0218"+
-    "\1\330\2\u0218\2\330\1\u0218\1\330\1\u0218\1\u014f\1\136"+
-    "\1\151\1\136\1\0\2\152\1\137\1\u013f\1\u0140\1\u0141"+
-    "\2\0\1\76\1\136\4\0\2\136\151\0\4\u0270\2\0"+
-    "\1\u0270\15\0\1\u0270\6\0\12\u0270\1\u01d5\175\0\4\u0271"+
-    "\2\0\1\u0271\15\0\1\u0271\6\0\12\u0271\1\u0272\175\0"+
-    "\4\u0273\2\0\1\u0273\15\0\1\u0273\6\0\1\u0274\2\u0275"+
-    "\1\u0274\5\u0275\1\u0276\14\0\1\u0122\161\0\4\u0277\2\0"+
-    "\1\u0277\15\0\1\u0277\6\0\12\u0277\1\u021e\13\0\1\u0122"+
-    "\161\0\4\u0273\2\0\1\u0273\15\0\1\u0273\6\0\1\u0274"+
-    "\2\u0275\1\u0274\5\u0275\1\u0276\175\0\1\u0163\4\u0277\2\0"+
-    "\1\u0277\15\0\1\u0277\6\0\12\u0278\1\u021e\13\0\1\u0122"+
-    "\160\0\1\u0163\4\u0277\2\0\1\u0277\15\0\1\u0277\6\0"+
-    "\12\u0277\1\u021e\13\0\1\u0122\160\0\1\u0163\4\u0277\2\0"+
-    "\1\u0277\15\0\1\u0277\6\0\2\u0278\1\u0277\2\u0278\2\u0277"+
-    "\1\u0278\1\u0277\1\u0278\1\u021e\13\0\1\u0122\226\0\1\u0197"+
-    "\13\0\1\u0122\160\0\1\u0279\33\0\12\u0224\175\0\1\u0279"+
-    "\33\0\12\u027a\175\0\1\u0279\33\0\1\u0224\1\u027b\1\u027a"+
-    "\2\u0224\2\u027a\1\u0224\1\u027a\1\u0224\175\0\1\346\12\230"+
-    "\1\u016a\17\230\1\347\12\230\175\0\1\346\11\230\1\u027c"+
-    "\20\230\1\347\12\230\175\0\1\346\3\230\1\u027d\26\230"+
-    "\1\347\12\230\175\0\1\346\7\230\1\u027e\22\230\1\347"+
-    "\4\230\1\u027f\5\230\175\0\1\346\10\230\1\u0280\4\230"+
-    "\1\u0281\5\230\1\u0282\6\230\1\347\12\230\175\0\1\346"+
-    "\3\230\1\u0283\26\230\1\347\2\230\1\u0284\7\230\175\0"+
-    "\1\346\7\230\1\u0285\22\230\1\347\12\230\175\0\1\346"+
-    "\7\230\1\u0286\22\230\1\347\12\230\175\0\1\346\7\230"+
-    "\1\u0287\22\230\1\347\3\230\1\u0288\6\230\175\0\1\346"+
-    "\32\230\1\347\5\230\1\u0289\4\230\175\0\1\346\7\230"+
-    "\1\u028a\22\230\1\347\12\230\175\0\1\346\31\230\1\u028b"+
-    "\1\347\12\230\175\0\1\346\1\230\1\u028c\30\230\1\347"+
-    "\12\230\175\0\1\346\7\230\1\u028d\1\230\1\u028e\20\230"+
-    "\1\347\11\230\1\u0289\175\0\1\346\22\230\1\u028f\7\230"+
-    "\1\347\2\230\1\u0290\7\230\175\0\1\346\6\230\1\u0291"+
-    "\1\u0292\22\230\1\347\12\230\175\0\1\346\7\230\1\u0293"+
-    "\5\230\1\u0294\14\230\1\347\12\230\175\0\1\346\23\230"+
-    "\1\u0295\6\230\1\347\12\230\175\0\1\346\32\230\1\347"+
-    "\3\230\1\u0296\6\230\175\0\1\346\3\230\1\u0297\26\230"+
-    "\1\347\12\230\175\0\1\346\17\230\1\u0298\12\230\1\347"+
-    "\1\u0299\11\230\175\0\1\346\32\230\1\347\1\230\1\u0289"+
-    "\10\230\175\0\1\346\32\230\1\347\1\u029a\11\230\231\0"+
-    "\12\u029b\10\0\1\u0167\1\u0168\1\u0169\162\0\1\376\25\264"+
-    "\1\u029c\4\264\1\141\12\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\1\u029d\31\264\1\141\12\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\15\264\1\u029e\14\264\1\141\12\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\150\0"+
-    "\1\376\21\264\1\u029f\10\264\1\141\12\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\376\16\264\1\u02a0\4\264\1\u02a1\6\264\1\141"+
-    "\12\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\4\264\1\u02a2\25\264"+
-    "\1\141\12\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\32\264\1\141"+
-    "\11\264\1\u02a3\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\4\264\1\u02a4"+
-    "\25\264\1\141\12\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\32\264"+
-    "\1\141\11\264\1\u02a5\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\24\264"+
-    "\1\u02a6\5\264\1\141\12\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\1\u02a7\1\u02a8\1\264\1\u02a9\20\264\1\u02aa\5\264\1\141"+
-    "\12\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\16\264\1\u02ab\13\264"+
-    "\1\141\12\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\11\264\1\u02ac"+
-    "\13\264\1\u02ad\4\264\1\141\12\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\150\0"+
-    "\1\376\32\264\1\141\11\264\1\u02ae\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\150\0"+
-    "\1\376\23\264\1\u02af\6\264\1\141\12\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\376\31\264\1\u02b0\1\141\12\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\376\26\264\1\u02b1\3\264\1\141\12\264\1\0"+
-    "\3\136\1\0\2\136\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\150\0\1\376\11\264\1\u02b2\20\264\1\141\12\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\32\264\1\141\3\264\1\u02b3"+
-    "\6\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\5\264\1\u02b4\24\264"+
-    "\1\141\12\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\10\264\1\u02b5"+
-    "\21\264\1\141\12\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\3\264"+
-    "\1\u02b6\26\264\1\141\12\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\21\264\1\u02b7\6\264\1\u02b8\1\264\1\141\12\264\1\0"+
-    "\3\136\1\0\2\136\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\150\0\1\376\12\264\1\u02b9\17\264\1\141\12\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\32\264\1\141\1\264\1\u02ba"+
-    "\10\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\24\264\1\u02bb\5\264"+
-    "\1\141\12\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\24\264\1\u02bc"+
-    "\5\264\1\141\12\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\32\264"+
-    "\1\141\4\264\1\u02bd\5\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\5\264\1\u02be\23\264\1\u02bf\1\141\12\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\151\0\32\u0206\1\0\12\u0206\176\0\32\u0206\1\u025c\12\u0206"+
-    "\176\0\4\u02c0\2\0\1\u02c0\15\0\1\u02c0\6\0\12\u02c0"+
-    "\176\0\4\u02c1\2\0\1\u02c1\15\0\1\u02c1\6\0\12\u02c1"+
-    "\1\u02c2\242\0\1\u02c3\174\0\34\u01ca\12\u02c4\1\0\2\u01ca"+
-    "\1\u020d\3\u01ca\1\u01cc\1\0\1\u020c\3\0\2\u01ca\4\0"+
-    "\1\u01ca\152\0\4\u02c5\2\0\1\u02c5\15\0\1\u02c5\6\0"+
-    "\12\u02c5\215\0\1\u02c6\223\0\4\u01ca\2\0\1\u01ca\15\0"+
-    "\1\u01ca\6\0\12\u01ca\176\0\32\u020f\1\0\12\u020f\176\0"+
-    "\32\u020f\1\u0265\12\u020f\231\0\12\u02c7\176\0\4\u02c8\2\0"+
-    "\1\u02c8\15\0\1\u02c8\6\0\12\u02c8\1\u0268\175\0\4\u02c9"+
-    "\2\0\1\u02c9\15\0\1\u02c9\6\0\12\u02c9\1\u02ca\175\0"+
-    "\4\u02cb\2\0\1\u02cb\15\0\1\u02cb\6\0\1\u02cc\2\u02cd"+
-    "\1\u02cc\5\u02cd\1\u02ce\14\0\1\u02cf\160\0\1\u0212\32\u0213"+
-    "\1\u0212\12\u0213\1\u0214\2\u0212\1\u0215\3\u0212\1\u0216\5\0"+
-    "\2\u0212\4\0\1\u0212\151\0\1\u0212\32\u0213\1\u026b\12\u0213"+
-    "\1\u0214\2\u0212\1\u0215\3\u0212\1\u0216\5\0\2\u0212\4\0"+
-    "\1\u0212\151\0\34\u0214\12\u02d0\1\0\2\u0214\1\u026d\3\u0214"+
-    "\1\u0216\5\0\2\u0214\4\0\1\u0214\152\0\4\u02d1\2\0"+
-    "\1\u02d1\15\0\1\u02d1\6\0\12\u02d1\176\0\4\u0212\2\0"+
-    "\1\u0212\15\0\1\u0212\6\0\12\u0212\175\0\1\u02d2\32\u026f"+
-    "\1\u02d3\12\u026f\1\u014f\7\0\1\u0167\1\u0168\1\u0169\230\0"+
-    "\1\u01d5\175\0\4\u02d4\2\0\1\u02d4\15\0\1\u02d4\6\0"+
-    "\12\u02d4\1\u0272\175\0\4\u02d5\2\0\1\u02d5\15\0\1\u02d5"+
-    "\6\0\12\u02d5\1\u02d6\175\0\4\u02d7\2\0\1\u02d7\15\0"+
-    "\1\u02d7\6\0\12\u02d7\1\u02d8\13\0\1\u0122\160\0\1\u0163"+
-    "\4\u02d7\2\0\1\u02d7\15\0\1\u02d7\6\0\12\u02d9\1\u02d8"+
-    "\13\0\1\u0122\160\0\1\u0163\4\u02d7\2\0\1\u02d7\15\0"+
-    "\1\u02d7\6\0\12\u02da\1\u02d8\13\0\1\u0122\160\0\1\u0163"+
-    "\4\u02d7\2\0\1\u02d7\15\0\1\u02d7\6\0\1\u02d9\1\u02db"+
-    "\1\u02da\2\u02d9\2\u02da\1\u02d9\1\u02da\1\u02d9\1\u02d8\13\0"+
-    "\1\u0122\161\0\4\u02dc\2\0\1\u02dc\15\0\1\u02dc\6\0"+
-    "\12\u02dc\1\u021e\13\0\1\u0122\160\0\1\u0163\4\u02dc\2\0"+
-    "\1\u02dc\15\0\1\u02dc\6\0\12\u02dc\1\u021e\13\0\1\u0122"+
-    "\214\0\1\u02dd\2\u02de\1\u02dd\5\u02de\1\u02df\175\0\1\u0279"+
-    "\242\0\1\u0279\33\0\2\u027a\1\0\2\u027a\2\0\1\u027a"+
-    "\1\0\1\u027a\175\0\1\346\1\230\1\u02e0\30\230\1\347"+
-    "\12\230\175\0\1\346\24\230\1\u02e1\5\230\1\347\12\230"+
-    "\175\0\1\346\24\230\1\u02e2\5\230\1\347\12\230\175\0"+
-    "\1\346\1\230\1\u02e3\30\230\1\347\12\230\175\0\1\346"+
-    "\14\230\1\u02e4\15\230\1\347\12\230\175\0\1\346\1\230"+
-    "\1\u02e5\30\230\1\347\12\230\175\0\1\346\1\230\1\u02e6"+
-    "\30\230\1\347\12\230\175\0\1\346\1\230\1\u02e7\30\230"+
-    "\1\347\12\230\175\0\1\346\21\230\1\u02e8\10\230\1\347"+
-    "\12\230\175\0\1\346\24\230\1\u02e9\5\230\1\347\12\230"+
-    "\175\0\1\346\24\230\1\u02ea\5\230\1\347\12\230\175\0"+
-    "\1\346\24\230\1\u02eb\5\230\1\347\12\230\175\0\1\346"+
-    "\1\u01a6\31\230\1\347\12\230\175\0\1\346\24\230\1\u02e7"+
-    "\5\230\1\347\12\230\175\0\1\346\24\230\1\u02ec\5\230"+
-    "\1\347\12\230\175\0\1\346\1\230\1\u02ed\30\230\1\347"+
-    "\12\230\175\0\1\346\31\230\1\u02ee\1\347\12\230\175\0"+
-    "\1\346\24\230\1\u02ef\5\230\1\347\12\230\175\0\1\346"+
-    "\1\230\1\u02f0\30\230\1\347\12\230\175\0\1\346\1\u02f1"+
-    "\31\230\1\347\12\230\175\0\1\346\21\230\1\u02f2\10\230"+
-    "\1\347\12\230\175\0\1\346\4\230\1\u02f3\25\230\1\347"+
-    "\12\230\175\0\1\346\24\230\1\u02f4\5\230\1\347\12\230"+
-    "\175\0\1\346\24\230\1\u02f5\5\230\1\347\12\230\175\0"+
-    "\1\346\4\230\1\u02f6\25\230\1\347\12\230\175\0\1\346"+
-    "\21\230\1\u02f7\10\230\1\347\12\230\175\0\1\346\24\230"+
-    "\1\u02f8\5\230\1\347\12\230\175\0\1\346\32\230\1\347"+
-    "\1\u02f9\11\230\175\0\1\346\32\230\1\347\7\230\1\u02fa"+
-    "\2\230\175\0\1\346\1\u02fb\31\230\1\347\12\230\175\0"+
-    "\1\346\1\u02fc\31\230\1\347\12\230\253\0\1\u0167\1\u0168"+
-    "\1\u0169\162\0\1\376\1\264\1\u02fd\30\264\1\141\12\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\32\264\1\141\1\u02fe\11\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\6\264\1\u02ff\23\264\1\141"+
-    "\12\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\32\264\1\141\7\264"+
-    "\1\u0300\2\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\32\264\1\141"+
-    "\10\264\1\u0156\1\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\32\264"+
-    "\1\141\5\264\1\u0156\4\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\26\264\1\u0301\3\264\1\141\12\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\150\0"+
-    "\1\376\1\264\1\u0302\30\264\1\141\12\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\376\26\264\1\u0303\3\264\1\141\12\264\1\0"+
-    "\3\136\1\0\2\136\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\150\0\1\376\32\264\1\141\1\264\1\u0304\10\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\1\u0305\31\264\1\141\12\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\1\u0306\27\264\1\u0307\1\264"+
-    "\1\141\12\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\32\264\1\141"+
-    "\1\u0308\11\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\4\264\1\u0309"+
-    "\25\264\1\141\12\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\25\264"+
-    "\1\u030a\4\264\1\141\12\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\32\264\1\141\1\u030b\11\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\32\264\1\141\2\264\1\307\7\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\150\0"+
-    "\1\376\32\264\1\141\3\264\1\u030c\6\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\376\1\u030d\1\264\1\u030e\27\264\1\141\12\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\1\u0300\31\264\1\141\12\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\32\264\1\141\2\264\1\u030f"+
-    "\7\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\32\264\1\141\2\264"+
-    "\1\u0310\7\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\15\264\1\u0311"+
-    "\14\264\1\141\12\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\32\264"+
-    "\1\141\5\264\1\u0312\4\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\32\264\1\141\7\264\1\u0313\2\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\150\0"+
-    "\1\376\32\264\1\141\11\264\1\u0314\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\150\0"+
-    "\1\376\1\264\1\u0315\30\264\1\141\12\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\376\32\264\1\141\3\264\1\u0316\6\264\1\0"+
-    "\3\136\1\0\2\136\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\150\0\1\376\32\264\1\141\1\264\1\u0317\10\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\32\264\1\141\1\264\1\u0318"+
-    "\10\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\24\264\1\u0319\5\264"+
-    "\1\141\12\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\32\264\1\141"+
-    "\6\264\1\u031a\3\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\32\264"+
-    "\1\141\3\264\1\u031b\6\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\1\u030c\31\264\1\141\12\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\32\264\1\141\11\264\1\u031c\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\25\264\1\u031d\4\264\1\141\12\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\151\0"+
-    "\4\u0207\2\0\1\u0207\15\0\1\u0207\6\0\12\u0207\176\0"+
-    "\4\u031e\2\0\1\u031e\15\0\1\u031e\6\0\12\u031e\1\u02c2"+
-    "\175\0\4\u031f\2\0\1\u031f\15\0\1\u031f\6\0\12\u031f"+
-    "\1\u0320\175\0\4\u0321\2\0\1\u0321\15\0\1\u0321\6\0"+
-    "\1\u0322\2\u0323\1\u0322\5\u0323\1\u0324\14\0\1\u0325\160\0"+
-    "\34\u01ca\12\u0326\1\0\2\u01ca\1\u020d\3\u01ca\1\u01cc\1\0"+
-    "\1\u020c\3\0\2\u01ca\4\0\1\u01ca\152\0\4\u020c\2\0"+
-    "\1\u020c\15\0\1\u020c\6\0\12\u020c\226\0\1\u0327\245\0"+
-    "\12\u0328\11\0\1\u020c\164\0\4\u0329\2\0\1\u0329\15\0"+
-    "\1\u0329\6\0\12\u0329\1\u0268\175\0\4\u032a\2\0\1\u032a"+
-    "\15\0\1\u032a\6\0\12\u032a\1\u032b\175\0\4\u032c\2\0"+
-    "\1\u032c\15\0\1\u032c\6\0\1\u032d\2\u032e\1\u032d\5\u032e"+
-    "\1\u032f\14\0\1\u02cf\161\0\4\u0330\2\0\1\u0330\15\0"+
-    "\1\u0330\6\0\12\u0330\1\u0331\13\0\1\u02cf\160\0\1\u0332"+
-    "\4\u0330\2\0\1\u0330\15\0\1\u0330\6\0\12\u0333\1\u0331"+
-    "\13\0\1\u02cf\160\0\1\u0332\4\u0330\2\0\1\u0330\15\0"+
-    "\1\u0330\6\0\12\u0334\1\u0331\13\0\1\u02cf\160\0\1\u0332"+
-    "\4\u0330\2\0\1\u0330\15\0\1\u0330\6\0\1\u0333\1\u0335"+
-    "\1\u0334\2\u0333\2\u0334\1\u0333\1\u0334\1\u0333\1\u0331\13\0"+
-    "\1\u02cf\226\0\1\u0266\10\0\1\u020c\163\0\34\u0214\12\u0336"+
-    "\1\0\2\u0214\1\u026d\3\u0214\1\u0216\1\u0167\1\u0168\1\u0169"+
-    "\2\0\2\u0214\4\0\1\u0214\152\0\4\u0214\2\0\1\u0214"+
-    "\15\0\1\u0214\6\0\12\u0214\176\0\32\u026f\1\0\12\u026f"+
-    "\176\0\32\u026f\1\u02d3\12\u026f\176\0\4\u0337\2\0\1\u0337"+
-    "\15\0\1\u0337\6\0\12\u0337\1\u0272\175\0\4\u0338\2\0"+
-    "\1\u0338\15\0\1\u0338\6\0\12\u0338\1\u0339\175\0\4\u033a"+
-    "\2\0\1\u033a\15\0\1\u033a\6\0\1\u033b\2\u033c\1\u033b"+
-    "\5\u033c\1\u033d\14\0\1\u0122\161\0\4\u033e\2\0\1\u033e"+
-    "\15\0\1\u033e\6\0\12\u033e\1\u02d8\13\0\1\u0122\161\0"+
-    "\4\u033a\2\0\1\u033a\15\0\1\u033a\6\0\1\u033b\2\u033c"+
-    "\1\u033b\5\u033c\1\u033d\175\0\1\u0163\4\u033e\2\0\1\u033e"+
-    "\15\0\1\u033e\6\0\12\u033f\1\u02d8\13\0\1\u0122\160\0"+
-    "\1\u0163\4\u033e\2\0\1\u033e\15\0\1\u033e\6\0\12\u033e"+
-    "\1\u02d8\13\0\1\u0122\160\0\1\u0163\4\u033e\2\0\1\u033e"+
-    "\15\0\1\u033e\6\0\2\u033f\1\u033e\2\u033f\2\u033e\1\u033f"+
-    "\1\u033e\1\u033f\1\u02d8\13\0\1\u0122\226\0\1\u021e\13\0"+
-    "\1\u0122\214\0\12\u02de\14\0\1\u0122\214\0\12\u0340\14\0"+
-    "\1\u0122\214\0\1\u02de\1\u0341\1\u0340\2\u02de\2\u0340\1\u02de"+
-    "\1\u0340\1\u02de\14\0\1\u0122\160\0\1\346\25\230\1\u0342"+
-    "\4\230\1\347\12\230\175\0\1\346\1\u0343\31\230\1\347"+
-    "\12\230\175\0\1\346\15\230\1\u0344\14\230\1\347\12\230"+
-    "\175\0\1\346\21\230\1\u0345\10\230\1\347\12\230\175\0"+
-    "\1\346\16\230\1\u0346\4\230\1\u0347\6\230\1\347\12\230"+
-    "\175\0\1\346\4\230\1\u0348\25\230\1\347\12\230\175\0"+
-    "\1\346\32\230\1\347\11\230\1\u0349\175\0\1\346\4\230"+
-    "\1\u034a\25\230\1\347\12\230\175\0\1\346\32\230\1\347"+
-    "\11\230\1\u034b\175\0\1\346\24\230\1\u034c\5\230\1\347"+
-    "\12\230\175\0\1\346\1\u034d\1\u034e\1\230\1\u034f\20\230"+
-    "\1\u0350\5\230\1\347\12\230\175\0\1\346\16\230\1\u0351"+
-    "\13\230\1\347\12\230\175\0\1\346\11\230\1\u0352\13\230"+
-    "\1\u0353\4\230\1\347\12\230\175\0\1\346\32\230\1\347"+
-    "\11\230\1\u0354\175\0\1\346\23\230\1\u0355\6\230\1\347"+
-    "\12\230\175\0\1\346\31\230\1\u0356\1\347\12\230\175\0"+
-    "\1\346\26\230\1\u0357\3\230\1\347\12\230\175\0\1\346"+
-    "\11\230\1\u0358\20\230\1\347\12\230\175\0\1\346\32\230"+
-    "\1\347\3\230\1\u0359\6\230\175\0\1\346\5\230\1\u035a"+
-    "\24\230\1\347\12\230\175\0\1\346\10\230\1\u035b\21\230"+
-    "\1\347\12\230\175\0\1\346\3\230\1\u035c\26\230\1\347"+
-    "\12\230\175\0\1\346\21\230\1\u035d\6\230\1\u035e\1\230"+
-    "\1\347\12\230\175\0\1\346\12\230\1\u035f\17\230\1\347"+
-    "\12\230\175\0\1\346\32\230\1\347\1\230\1\u0360\10\230"+
-    "\175\0\1\346\24\230\1\u0361\5\230\1\347\12\230\175\0"+
-    "\1\346\24\230\1\u0362\5\230\1\347\12\230\175\0\1\346"+
-    "\32\230\1\347\4\230\1\u0363\5\230\175\0\1\346\5\230"+
-    "\1\u0364\23\230\1\u0365\1\347\12\230\175\0\1\376\32\264"+
-    "\1\141\1\u0366\11\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\1\u0367"+
-    "\31\264\1\141\12\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\32\264"+
-    "\1\141\10\264\1\u0368\1\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\25\264\1\u0103\4\264\1\141\12\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\150\0"+
-    "\1\376\32\264\1\141\5\264\1\u0369\4\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\376\32\264\1\141\5\264\1\u036a\4\264\1\0"+
-    "\3\136\1\0\2\136\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\150\0\1\376\32\264\1\141\5\264\1\u030c\4\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\32\264\1\141\3\264\1\u0367"+
-    "\6\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\17\264\1\u036b\12\264"+
-    "\1\141\12\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\12\264\1\u036c"+
-    "\17\264\1\141\12\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\25\264"+
-    "\1\u036d\4\264\1\141\12\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\1\u036e\31\264\1\141\12\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\15\264\1\u036f\14\264\1\141\12\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\150\0"+
-    "\1\376\32\264\1\141\3\264\1\u0370\6\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\376\2\264\1\u0300\27\264\1\141\12\264\1\0"+
-    "\3\136\1\0\2\136\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\150\0\1\376\1\264\1\u0103\30\264\1\141\12\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\11\264\1\u0371\20\264\1\141"+
-    "\12\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\11\264\1\u0372\20\264"+
-    "\1\141\12\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\1\u0373\31\264"+
-    "\1\141\12\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\1\u0374\31\264"+
-    "\1\141\12\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\2\264\1\u0375"+
-    "\27\264\1\141\12\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\32\264"+
-    "\1\141\4\264\1\u010a\5\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\10\264\1\u0376\21\264\1\141\12\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\150\0"+
-    "\1\376\1\u0377\31\264\1\141\12\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\150\0"+
-    "\1\376\25\264\1\u0378\4\264\1\141\12\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\376\32\264\1\141\4\264\1\u0367\5\264\1\0"+
-    "\3\136\1\0\2\136\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\150\0\1\376\32\264\1\141\6\264\1\u0367\3\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\32\264\1\141\2\264\1\u0367"+
-    "\7\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\16\264\1\u0379\13\264"+
-    "\1\141\12\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\32\264\1\141"+
-    "\1\u037a\11\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\32\264\1\141"+
-    "\3\264\1\u037b\6\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\32\264"+
-    "\1\141\3\264\1\307\6\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\24\264\1\u037c\5\264\1\141\12\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\151\0"+
-    "\4\u037d\2\0\1\u037d\15\0\1\u037d\6\0\12\u037d\1\u02c2"+
-    "\175\0\4\u037e\2\0\1\u037e\15\0\1\u037e\6\0\12\u037e"+
-    "\1\u037f\175\0\4\u0380\2\0\1\u0380\15\0\1\u0380\6\0"+
-    "\1\u0381\2\u0382\1\u0381\5\u0382\1\u0383\14\0\1\u0325\161\0"+
-    "\4\u0384\2\0\1\u0384\15\0\1\u0384\6\0\12\u0384\1\u0385"+
-    "\13\0\1\u0325\160\0\1\u0386\4\u0384\2\0\1\u0384\15\0"+
-    "\1\u0384\6\0\12\u0387\1\u0385\13\0\1\u0325\160\0\1\u0386"+
-    "\4\u0384\2\0\1\u0384\15\0\1\u0384\6\0\12\u0388\1\u0385"+
-    "\13\0\1\u0325\160\0\1\u0386\4\u0384\2\0\1\u0384\15\0"+
-    "\1\u0384\6\0\1\u0387\1\u0389\1\u0388\2\u0387\2\u0388\1\u0387"+
-    "\1\u0388\1\u0387\1\u0385\13\0\1\u0325\237\0\1\u0207\163\0"+
-    "\34\u01ca\12\u038a\1\0\2\u01ca\1\u020d\3\u01ca\1\u01cc\1\0"+
-    "\1\u020c\3\0\2\u01ca\4\0\1\u01ca\167\0\1\u038b\260\0"+
-    "\12\u038c\11\0\1\u020c\231\0\1\u0268\175\0\4\u038d\2\0"+
-    "\1\u038d\15\0\1\u038d\6\0\12\u038d\1\u032b\175\0\4\u038e"+
-    "\2\0\1\u038e\15\0\1\u038e\6\0\12\u038e\1\u038f\175\0"+
-    "\4\u0390\2\0\1\u0390\15\0\1\u0390\6\0\12\u0390\1\u0391"+
-    "\13\0\1\u02cf\160\0\1\u0332\4\u0390\2\0\1\u0390\15\0"+
-    "\1\u0390\6\0\12\u0392\1\u0391\13\0\1\u02cf\160\0\1\u0332"+
-    "\4\u0390\2\0\1\u0390\15\0\1\u0390\6\0\12\u0393\1\u0391"+
-    "\13\0\1\u02cf\160\0\1\u0332\4\u0390\2\0\1\u0390\15\0"+
-    "\1\u0390\6\0\1\u0392\1\u0394\1\u0393\2\u0392\2\u0393\1\u0392"+
-    "\1\u0393\1\u0392\1\u0391\13\0\1\u02cf\161\0\4\u0395\2\0"+
-    "\1\u0395\15\0\1\u0395\6\0\12\u0395\1\u0331\13\0\1\u02cf"+
-    "\161\0\4\u032c\2\0\1\u032c\15\0\1\u032c\6\0\1\u032d"+
-    "\2\u032e\1\u032d\5\u032e\1\u032f\231\0\1\u0396\2\u0397\1\u0396"+
-    "\5\u0397\1\u0398\175\0\1\u0332\4\u0395\2\0\1\u0395\15\0"+
-    "\1\u0395\6\0\12\u0399\1\u0331\13\0\1\u02cf\160\0\1\u0332"+
-    "\4\u0395\2\0\1\u0395\15\0\1\u0395\6\0\12\u0395\1\u0331"+
-    "\13\0\1\u02cf\160\0\1\u0332\4\u0395\2\0\1\u0395\15\0"+
-    "\1\u0395\6\0\2\u0399\1\u0395\2\u0399\2\u0395\1\u0399\1\u0395"+
-    "\1\u0399\1\u0331\13\0\1\u02cf\160\0\34\u0214\12\u039a\1\0"+
-    "\2\u0214\1\u026d\3\u0214\1\u0216\1\u0167\1\u0168\1\u0169\2\0"+
-    "\2\u0214\4\0\1\u0214\217\0\1\u0272\175\0\4\u039b\2\0"+
-    "\1\u039b\15\0\1\u039b\6\0\12\u039b\1\u0339\175\0\4\u039c"+
-    "\2\0\1\u039c\15\0\1\u039c\6\0\12\u039c\1\u039d\175\0"+
-    "\4\u039e\2\0\1\u039e\15\0\1\u039e\6\0\12\u039e\1\u039f"+
-    "\13\0\1\u0122\160\0\1\u0163\4\u039e\2\0\1\u039e\15\0"+
-    "\1\u039e\6\0\12\u03a0\1\u039f\13\0\1\u0122\160\0\1\u0163"+
-    "\4\u039e\2\0\1\u039e\15\0\1\u039e\6\0\12\u03a1\1\u039f"+
-    "\13\0\1\u0122\160\0\1\u0163\4\u039e\2\0\1\u039e\15\0"+
-    "\1\u039e\6\0\1\u03a0\1\u03a2\1\u03a1\2\u03a0\2\u03a1\1\u03a0"+
-    "\1\u03a1\1\u03a0\1\u039f\13\0\1\u0122\161\0\4\u03a3\2\0"+
-    "\1\u03a3\15\0\1\u03a3\6\0\12\u03a3\1\u02d8\13\0\1\u0122"+
-    "\160\0\1\u0163\4\u03a3\2\0\1\u03a3\15\0\1\u03a3\6\0"+
-    "\12\u03a3\1\u02d8\13\0\1\u0122\242\0\1\u0122\214\0\2\u0340"+
-    "\1\0\2\u0340\2\0\1\u0340\1\0\1\u0340\14\0\1\u0122"+
-    "\160\0\1\346\1\230\1\u03a4\30\230\1\347\12\230\175\0"+
-    "\1\346\32\230\1\347\1\u03a5\11\230\175\0\1\346\6\230"+
-    "\1\u03a6\23\230\1\347\12\230\175\0\1\346\32\230\1\347"+
-    "\7\230\1\u03a7\2\230\175\0\1\346\32\230\1\347\10\230"+
-    "\1\u01ab\1\230\175\0\1\346\32\230\1\347\5\230\1\u01ab"+
-    "\4\230\175\0\1\346\26\230\1\u03a8\3\230\1\347\12\230"+
-    "\175\0\1\346\1\230\1\u03a9\30\230\1\347\12\230\175\0"+
-    "\1\346\26\230\1\u03aa\3\230\1\347\12\230\175\0\1\346"+
-    "\32\230\1\347\1\230\1\u03ab\10\230\175\0\1\346\1\u03ac"+
-    "\31\230\1\347\12\230\175\0\1\346\1\u03ad\27\230\1\u03ae"+
-    "\1\230\1\347\12\230\175\0\1\346\32\230\1\347\1\u03af"+
-    "\11\230\175\0\1\346\4\230\1\u03b0\25\230\1\347\12\230"+
-    "\175\0\1\346\25\230\1\u03b1\4\230\1\347\12\230\175\0"+
-    "\1\346\32\230\1\347\1\u03b2\11\230\175\0\1\346\32\230"+
-    "\1\347\2\230\1\u012f\7\230\175\0\1\346\32\230\1\347"+
-    "\3\230\1\u03b3\6\230\175\0\1\346\1\u03b4\1\230\1\u03b5"+
-    "\27\230\1\347\12\230\175\0\1\346\1\u03a7\31\230\1\347"+
-    "\12\230\175\0\1\346\32\230\1\347\2\230\1\u03b6\7\230"+
-    "\175\0\1\346\32\230\1\347\2\230\1\u03b7\7\230\175\0"+
-    "\1\346\15\230\1\u03b8\14\230\1\347\12\230\175\0\1\346"+
-    "\32\230\1\347\5\230\1\u03b9\4\230\175\0\1\346\32\230"+
-    "\1\347\7\230\1\u03ba\2\230\175\0\1\346\32\230\1\347"+
-    "\11\230\1\u03bb\175\0\1\346\1\230\1\u03bc\30\230\1\347"+
-    "\12\230\175\0\1\346\32\230\1\347\3\230\1\u03bd\6\230"+
-    "\175\0\1\346\32\230\1\347\1\230\1\u03be\10\230\175\0"+
-    "\1\346\32\230\1\347\1\230\1\u03bf\10\230\175\0\1\346"+
-    "\24\230\1\u03c0\5\230\1\347\12\230\175\0\1\346\32\230"+
-    "\1\347\6\230\1\u03c1\3\230\175\0\1\346\32\230\1\347"+
-    "\3\230\1\u03c2\6\230\175\0\1\346\1\u03b3\31\230\1\347"+
-    "\12\230\175\0\1\346\32\230\1\347\11\230\1\u03c3\175\0"+
-    "\1\346\25\230\1\u03c4\4\230\1\347\12\230\175\0\1\376"+
-    "\3\264\1\u03c5\26\264\1\141\12\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\150\0"+
-    "\1\376\2\264\1\u0103\27\264\1\141\12\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\376\6\264\1\u010e\23\264\1\141\12\264\1\0"+
-    "\3\136\1\0\2\136\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\150\0\1\376\1\264\1\u0316\30\264\1\141\12\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\3\264\1\u03c6\26\264\1\141"+
-    "\12\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\32\264\1\141\3\264"+
-    "\1\u03c7\6\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\32\264\1\141"+
-    "\6\264\1\u03c8\3\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\32\264"+
-    "\1\141\6\264\1\u03c9\3\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\32\264\1\141\5\264\1\u03ca\4\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\150\0"+
-    "\1\376\32\264\1\141\7\264\1\u03cb\2\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\376\1\u03cc\31\264\1\141\12\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\376\32\264\1\141\4\264\1\u03cd\5\264\1\0"+
-    "\3\136\1\0\2\136\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\150\0\1\376\32\264\1\141\4\264\1\u03ce\5\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\26\264\1\u03cf\3\264\1\141"+
-    "\12\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\30\264\1\u03d0\1\264"+
-    "\1\141\12\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\11\264\1\u0151"+
-    "\20\264\1\141\12\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\32\264"+
-    "\1\141\2\264\1\u03d1\7\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\12\264\1\u03d2\17\264\1\141\12\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\150\0"+
-    "\1\376\17\264\1\u010b\12\264\1\141\12\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\376\32\264\1\141\4\264\1\u03d3\5\264\1\0"+
-    "\3\136\1\0\2\136\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\150\0\1\376\32\264\1\141\6\264\1\u0154\3\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\30\264\1\u03d4\1\264\1\141"+
-    "\12\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\30\264\1\u03d5\1\264"+
-    "\1\141\12\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\216\0\1\u02c2\175\0\4\u03d6"+
-    "\2\0\1\u03d6\15\0\1\u03d6\6\0\12\u03d6\1\u037f\175\0"+
-    "\4\u03d7\2\0\1\u03d7\15\0\1\u03d7\6\0\12\u03d7\1\u03d8"+
-    "\175\0\4\u03d9\2\0\1\u03d9\15\0\1\u03d9\6\0\12\u03d9"+
-    "\1\u03da\13\0\1\u0325\160\0\1\u0386\4\u03d9\2\0\1\u03d9"+
-    "\15\0\1\u03d9\6\0\12\u03db\1\u03da\13\0\1\u0325\160\0"+
-    "\1\u0386\4\u03d9\2\0\1\u03d9\15\0\1\u03d9\6\0\12\u03dc"+
-    "\1\u03da\13\0\1\u0325\160\0\1\u0386\4\u03d9\2\0\1\u03d9"+
-    "\15\0\1\u03d9\6\0\1\u03db\1\u03dd\1\u03dc\2\u03db\2\u03dc"+
-    "\1\u03db\1\u03dc\1\u03db\1\u03da\13\0\1\u0325\161\0\4\u03de"+
-    "\2\0\1\u03de\15\0\1\u03de\6\0\12\u03de\1\u0385\13\0"+
-    "\1\u0325\161\0\4\u0380\2\0\1\u0380\15\0\1\u0380\6\0"+
-    "\1\u0381\2\u0382\1\u0381\5\u0382\1\u0383\231\0\1\u03df\2\u03e0"+
-    "\1\u03df\5\u03e0\1\u03e1\175\0\1\u0386\4\u03de\2\0\1\u03de"+
-    "\15\0\1\u03de\6\0\12\u03e2\1\u0385\13\0\1\u0325\160\0"+
-    "\1\u0386\4\u03de\2\0\1\u03de\15\0\1\u03de\6\0\12\u03de"+
-    "\1\u0385\13\0\1\u0325\160\0\1\u0386\4\u03de\2\0\1\u03de"+
-    "\15\0\1\u03de\6\0\2\u03e2\1\u03de\2\u03e2\2\u03de\1\u03e2"+
-    "\1\u03de\1\u03e2\1\u0385\13\0\1\u0325\160\0\34\u01ca\12\u03e3"+
-    "\1\0\2\u01ca\1\u020d\3\u01ca\1\u01cc\1\0\1\u020c\3\0"+
-    "\2\u01ca\4\0\1\u01ca\155\0\1\u03e4\272\0\12\u03e5\11\0"+
-    "\1\u020c\164\0\4\u03e6\2\0\1\u03e6\15\0\1\u03e6\6\0"+
-    "\12\u03e6\1\u032b\175\0\4\u03e7\2\0\1\u03e7\15\0\1\u03e7"+
-    "\6\0\12\u03e7\1\u03e8\175\0\4\u03e9\2\0\1\u03e9\15\0"+
-    "\1\u03e9\6\0\1\u03ea\2\u03eb\1\u03ea\5\u03eb\1\u03ec\14\0"+
-    "\1\u02cf\161\0\4\u03ed\2\0\1\u03ed\15\0\1\u03ed\6\0"+
-    "\12\u03ed\1\u0391\13\0\1\u02cf\161\0\4\u03e9\2\0\1\u03e9"+
-    "\15\0\1\u03e9\6\0\1\u03ea\2\u03eb\1\u03ea\5\u03eb\1\u03ec"+
-    "\175\0\1\u0332\4\u03ed\2\0\1\u03ed\15\0\1\u03ed\6\0"+
-    "\12\u03ee\1\u0391\13\0\1\u02cf\160\0\1\u0332\4\u03ed\2\0"+
-    "\1\u03ed\15\0\1\u03ed\6\0\12\u03ed\1\u0391\13\0\1\u02cf"+
-    "\160\0\1\u0332\4\u03ed\2\0\1\u03ed\15\0\1\u03ed\6\0"+
-    "\2\u03ee\1\u03ed\2\u03ee\2\u03ed\1\u03ee\1\u03ed\1\u03ee\1\u0391"+
-    "\13\0\1\u02cf\161\0\4\u03ef\2\0\1\u03ef\15\0\1\u03ef"+
-    "\6\0\12\u03ef\1\u0331\13\0\1\u02cf\160\0\1\u03f0\33\0"+
-    "\12\u0397\175\0\1\u03f0\33\0\12\u03f1\175\0\1\u03f0\33\0"+
-    "\1\u0397\1\u03f2\1\u03f1\2\u0397\2\u03f1\1\u0397\1\u03f1\1\u0397"+
-    "\175\0\1\u0332\4\u03ef\2\0\1\u03ef\15\0\1\u03ef\6\0"+
-    "\12\u03ef\1\u0331\13\0\1\u02cf\160\0\34\u0214\12\u03f3\1\0"+
-    "\2\u0214\1\u026d\3\u0214\1\u0216\1\u0167\1\u0168\1\u0169\2\0"+
-    "\2\u0214\4\0\1\u0214\152\0\4\u03f4\2\0\1\u03f4\15\0"+
-    "\1\u03f4\6\0\12\u03f4\1\u0339\175\0\4\u03f5\2\0\1\u03f5"+
-    "\15\0\1\u03f5\6\0\12\u03f5\1\u03f6\175\0\4\u03f7\2\0"+
-    "\1\u03f7\15\0\1\u03f7\6\0\1\u03f8\2\u03f9\1\u03f8\5\u03f9"+
-    "\1\u03fa\14\0\1\u0122\161\0\4\u03fb\2\0\1\u03fb\15\0"+
-    "\1\u03fb\6\0\12\u03fb\1\u039f\13\0\1\u0122\161\0\4\u03f7"+
-    "\2\0\1\u03f7\15\0\1\u03f7\6\0\1\u03f8\2\u03f9\1\u03f8"+
-    "\5\u03f9\1\u03fa\175\0\1\u0163\4\u03fb\2\0\1\u03fb\15\0"+
-    "\1\u03fb\6\0\12\u03fc\1\u039f\13\0\1\u0122\160\0\1\u0163"+
-    "\4\u03fb\2\0\1\u03fb\15\0\1\u03fb\6\0\12\u03fb\1\u039f"+
-    "\13\0\1\u0122\160\0\1\u0163\4\u03fb\2\0\1\u03fb\15\0"+
-    "\1\u03fb\6\0\2\u03fc\1\u03fb\2\u03fc\2\u03fb\1\u03fc\1\u03fb"+
-    "\1\u03fc\1\u039f\13\0\1\u0122\226\0\1\u02d8\13\0\1\u0122"+
-    "\160\0\1\346\32\230\1\347\1\u03fd\11\230\175\0\1\346"+
-    "\1\u03fe\31\230\1\347\12\230\175\0\1\346\32\230\1\347"+
-    "\10\230\1\u03ff\1\230\175\0\1\346\25\230\1\u016a\4\230"+
-    "\1\347\12\230\175\0\1\346\32\230\1\347\5\230\1\u0400"+
-    "\4\230\175\0\1\346\32\230\1\347\5\230\1\u0401\4\230"+
-    "\175\0\1\346\32\230\1\347\5\230\1\u03b3\4\230\175\0"+
-    "\1\346\32\230\1\347\3\230\1\u03fe\6\230\175\0\1\346"+
-    "\17\230\1\u0402\12\230\1\347\12\230\175\0\1\346\12\230"+
-    "\1\u0403\17\230\1\347\12\230\175\0\1\346\25\230\1\u0404"+
-    "\4\230\1\347\12\230\175\0\1\346\1\u0405\31\230\1\347"+
-    "\12\230\175\0\1\346\15\230\1\u0406\14\230\1\347\12\230"+
-    "\175\0\1\346\32\230\1\347\3\230\1\u0407\6\230\175\0"+
-    "\1\346\2\230\1\u03a7\27\230\1\347\12\230\175\0\1\346"+
-    "\1\230\1\u016a\30\230\1\347\12\230\175\0\1\346\11\230"+
-    "\1\u0408\20\230\1\347\12\230\175\0\1\346\11\230\1\u0409"+
-    "\20\230\1\347\12\230\175\0\1\346\1\u040a\31\230\1\347"+
-    "\12\230\175\0\1\346\1\u040b\31\230\1\347\12\230\175\0"+
-    "\1\346\2\230\1\u040c\27\230\1\347\12\230\175\0\1\346"+
-    "\32\230\1\347\4\230\1\u0171\5\230\175\0\1\346\10\230"+
-    "\1\u040d\21\230\1\347\12\230\175\0\1\346\1\u040e\31\230"+
-    "\1\347\12\230\175\0\1\346\25\230\1\u040f\4\230\1\347"+
-    "\12\230\175\0\1\346\32\230\1\347\4\230\1\u03fe\5\230"+
-    "\175\0\1\346\32\230\1\347\6\230\1\u03fe\3\230\175\0"+
-    "\1\346\32\230\1\347\2\230\1\u03fe\7\230\175\0\1\346"+
-    "\16\230\1\u0410\13\230\1\347\12\230\175\0\1\346\32\230"+
-    "\1\347\1\u0411\11\230\175\0\1\346\32\230\1\347\3\230"+
-    "\1\u0412\6\230\175\0\1\346\32\230\1\347\3\230\1\u012f"+
-    "\6\230\175\0\1\346\24\230\1\u0413\5\230\1\347\12\230"+
-    "\175\0\1\376\1\u0414\31\264\1\141\12\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\376\32\264\1\141\11\264\1\u030c\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\376\1\u0415\31\264\1\141\12\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\376\1\u0416\31\264\1\141\12\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\376\7\264\1\u0417\22\264\1\141\12\264\1\0"+
-    "\3\136\1\0\2\136\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\150\0\1\376\1\u0418\31\264\1\141\12\264\1\0"+
-    "\3\136\1\0\2\136\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\150\0\1\376\1\u0419\31\264\1\141\12\264\1\0"+
-    "\3\136\1\0\2\136\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\150\0\1\376\32\264\1\141\6\264\1\u041a\3\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\25\264\1\u041b\4\264\1\141"+
-    "\12\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\1\u041c\31\264\1\141"+
-    "\12\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\32\264\1\141\6\264"+
-    "\1\u041d\3\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\1\u041e\31\264"+
-    "\1\141\12\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\32\264\1\141"+
-    "\6\264\1\u0150\3\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\12\264"+
-    "\1\u0114\17\264\1\141\12\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\1\u041f\31\264\1\141\12\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\10\264\1\u0420\21\264\1\141\12\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\150\0"+
-    "\1\376\31\264\1\u0421\1\141\12\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\151\0"+
-    "\4\u0422\2\0\1\u0422\15\0\1\u0422\6\0\12\u0422\1\u037f"+
-    "\175\0\4\u0423\2\0\1\u0423\15\0\1\u0423\6\0\12\u0423"+
-    "\1\u0424\175\0\4\u0425\2\0\1\u0425\15\0\1\u0425\6\0"+
-    "\1\u0426\2\u0427\1\u0426\5\u0427\1\u0428\14\0\1\u0325\161\0"+
-    "\4\u0429\2\0\1\u0429\15\0\1\u0429\6\0\12\u0429\1\u03da"+
-    "\13\0\1\u0325\161\0\4\u0425\2\0\1\u0425\15\0\1\u0425"+
-    "\6\0\1\u0426\2\u0427\1\u0426\5\u0427\1\u0428\175\0\1\u0386"+
-    "\4\u0429\2\0\1\u0429\15\0\1\u0429\6\0\12\u042a\1\u03da"+
-    "\13\0\1\u0325\160\0\1\u0386\4\u0429\2\0\1\u0429\15\0"+
-    "\1\u0429\6\0\12\u0429\1\u03da\13\0\1\u0325\160\0\1\u0386"+
-    "\4\u0429\2\0\1\u0429\15\0\1\u0429\6\0\2\u042a\1\u0429"+
-    "\2\u042a\2\u0429\1\u042a\1\u0429\1\u042a\1\u03da\13\0\1\u0325"+
-    "\161\0\4\u042b\2\0\1\u042b\15\0\1\u042b\6\0\12\u042b"+
-    "\1\u0385\13\0\1\u0325\160\0\1\u042c\33\0\12\u03e0\175\0"+
-    "\1\u042c\33\0\12\u042d\175\0\1\u042c\33\0\1\u03e0\1\u042e"+
-    "\1\u042d\2\u03e0\2\u042d\1\u03e0\1\u042d\1\u03e0\175\0\1\u0386"+
-    "\4\u042b\2\0\1\u042b\15\0\1\u042b\6\0\12\u042b\1\u0385"+
-    "\13\0\1\u0325\160\0\46\u01ca\1\0\2\u01ca\1\u020d\3\u01ca"+
-    "\1\u01cc\1\0\1\u020c\3\0\2\u01ca\4\0\1\u01ca\235\0"+
-    "\1\u042f\212\0\12\u0430\11\0\1\u020c\231\0\1\u032b\175\0"+
-    "\4\u0431\2\0\1\u0431\15\0\1\u0431\6\0\12\u0431\1\u03e8"+
-    "\175\0\4\u0432\2\0\1\u0432\15\0\1\u0432\6\0\12\u0432"+
-    "\1\u0433\175\0\4\u0434\2\0\1\u0434\15\0\1\u0434\6\0"+
-    "\12\u0434\1\u0435\13\0\1\u02cf\160\0\1\u0332\4\u0434\2\0"+
-    "\1\u0434\15\0\1\u0434\6\0\12\u0436\1\u0435\13\0\1\u02cf"+
-    "\160\0\1\u0332\4\u0434\2\0\1\u0434\15\0\1\u0434\6\0"+
-    "\12\u0437\1\u0435\13\0\1\u02cf\160\0\1\u0332\4\u0434\2\0"+
-    "\1\u0434\15\0\1\u0434\6\0\1\u0436\1\u0438\1\u0437\2\u0436"+
-    "\2\u0437\1\u0436\1\u0437\1\u0436\1\u0435\13\0\1\u02cf\161\0"+
-    "\4\u0439\2\0\1\u0439\15\0\1\u0439\6\0\12\u0439\1\u0391"+
-    "\13\0\1\u02cf\160\0\1\u0332\4\u0439\2\0\1\u0439\15\0"+
-    "\1\u0439\6\0\12\u0439\1\u0391\13\0\1\u02cf\226\0\1\u0331"+
-    "\13\0\1\u02cf\214\0\1\u043a\2\u043b\1\u043a\5\u043b\1\u043c"+
-    "\175\0\1\u03f0\242\0\1\u03f0\33\0\2\u03f1\1\0\2\u03f1"+
-    "\2\0\1\u03f1\1\0\1\u03f1\175\0\34\u0214\12\u043d\1\0"+
-    "\2\u0214\1\u026d\3\u0214\1\u0216\1\u0167\1\u0168\1\u0169\2\0"+
-    "\2\u0214\4\0\1\u0214\217\0\1\u0339\175\0\4\u043e\2\0"+
-    "\1\u043e\15\0\1\u043e\6\0\12\u043e\1\u03f6\175\0\4\u043f"+
-    "\2\0\1\u043f\15\0\1\u043f\6\0\1\u0440\2\u0441\1\u0440"+
-    "\5\u0441\1\u0442\1\u0443\175\0\4\u0444\2\0\1\u0444\15\0"+
-    "\1\u0444\6\0\12\u0444\1\u0445\13\0\1\u0122\160\0\1\u0163"+
-    "\4\u0444\2\0\1\u0444\15\0\1\u0444\6\0\12\u0446\1\u0445"+
-    "\13\0\1\u0122\160\0\1\u0163\4\u0444\2\0\1\u0444\15\0"+
-    "\1\u0444\6\0\12\u0447\1\u0445\13\0\1\u0122\160\0\1\u0163"+
-    "\4\u0444\2\0\1\u0444\15\0\1\u0444\6\0\1\u0446\1\u0448"+
-    "\1\u0447\2\u0446\2\u0447\1\u0446\1\u0447\1\u0446\1\u0445\13\0"+
-    "\1\u0122\161\0\4\u0449\2\0\1\u0449\15\0\1\u0449\6\0"+
-    "\12\u0449\1\u039f\13\0\1\u0122\160\0\1\u0163\4\u0449\2\0"+
-    "\1\u0449\15\0\1\u0449\6\0\12\u0449\1\u039f\13\0\1\u0122"+
-    "\160\0\1\346\3\230\1\u044a\26\230\1\347\12\230\175\0"+
-    "\1\346\2\230\1\u016a\27\230\1\347\12\230\175\0\1\346"+
-    "\6\230\1\u0175\23\230\1\347\12\230\175\0\1\346\1\230"+
-    "\1\u03bd\30\230\1\347\12\230\175\0\1\346\3\230\1\u044b"+
-    "\26\230\1\347\12\230\175\0\1\346\32\230\1\347\3\230"+
-    "\1\u044c\6\230\175\0\1\346\32\230\1\347\6\230\1\u044d"+
-    "\3\230\175\0\1\346\32\230\1\347\6\230\1\u044e\3\230"+
-    "\175\0\1\346\32\230\1\347\5\230\1\u044f\4\230\175\0"+
-    "\1\346\32\230\1\347\7\230\1\u0450\2\230\175\0\1\346"+
-    "\1\u0451\31\230\1\347\12\230\175\0\1\346\32\230\1\347"+
-    "\4\230\1\u0452\5\230\175\0\1\346\32\230\1\347\4\230"+
-    "\1\u0453\5\230\175\0\1\346\26\230\1\u0454\3\230\1\347"+
-    "\12\230\175\0\1\346\30\230\1\u0455\1\230\1\347\12\230"+
-    "\175\0\1\346\11\230\1\u01a5\20\230\1\347\12\230\175\0"+
-    "\1\346\32\230\1\347\2\230\1\u0456\7\230\175\0\1\346"+
-    "\12\230\1\u0457\17\230\1\347\12\230\175\0\1\346\17\230"+
-    "\1\u0172\12\230\1\347\12\230\175\0\1\346\32\230\1\347"+
-    "\4\230\1\u0458\5\230\175\0\1\346\32\230\1\347\6\230"+
-    "\1\u01a8\3\230\175\0\1\346\30\230\1\u0459\1\230\1\347"+
-    "\12\230\175\0\1\346\30\230\1\u045a\1\230\1\347\12\230"+
-    "\175\0\1\376\32\264\1\141\1\u045b\11\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\376\2\264\1\u045c\27\264\1\141\12\264\1\0"+
-    "\3\136\1\0\2\136\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\150\0\1\376\32\264\1\141\10\264\1\u0300\1\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\15\264\1\307\14\264\1\141"+
-    "\12\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\23\264\1\u045d\6\264"+
-    "\1\141\12\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\32\264\1\141"+
-    "\1\264\1\u045e\10\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\32\264"+
-    "\1\141\3\264\1\u0154\6\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\30\264\1\u045f\1\264\1\141\12\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\150\0"+
-    "\1\376\32\264\1\141\1\264\1\u0460\10\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\376\6\264\1\u0461\23\264\1\141\12\264\1\0"+
-    "\3\136\1\0\2\136\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\150\0\1\376\32\264\1\141\5\264\1\u0462\4\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\32\264\1\141\5\264\1\u0463"+
-    "\4\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\32\264\1\141\1\264"+
-    "\1\307\10\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\13\264\1\u0464"+
-    "\16\264\1\141\12\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\216\0\1\u037f\175\0"+
-    "\4\u0465\2\0\1\u0465\15\0\1\u0465\6\0\12\u0465\1\u0424"+
-    "\175\0\4\u0466\2\0\1\u0466\15\0\1\u0466\6\0\12\u0466"+
-    "\1\u0467\175\0\4\u0468\2\0\1\u0468\15\0\1\u0468\6\0"+
-    "\12\u0468\1\u0469\13\0\1\u0325\160\0\1\u0386\4\u0468\2\0"+
-    "\1\u0468\15\0\1\u0468\6\0\12\u046a\1\u0469\13\0\1\u0325"+
-    "\160\0\1\u0386\4\u0468\2\0\1\u0468\15\0\1\u0468\6\0"+
-    "\12\u046b\1\u0469\13\0\1\u0325\160\0\1\u0386\4\u0468\2\0"+
-    "\1\u0468\15\0\1\u0468\6\0\1\u046a\1\u046c\1\u046b\2\u046a"+
-    "\2\u046b\1\u046a\1\u046b\1\u046a\1\u0469\13\0\1\u0325\161\0"+
-    "\4\u046d\2\0\1\u046d\15\0\1\u046d\6\0\12\u046d\1\u03da"+
-    "\13\0\1\u0325\160\0\1\u0386\4\u046d\2\0\1\u046d\15\0"+
-    "\1\u046d\6\0\12\u046d\1\u03da\13\0\1\u0325\226\0\1\u0385"+
-    "\13\0\1\u0325\214\0\1\u046e\2\u046f\1\u046e\5\u046f\1\u0470"+
-    "\175\0\1\u042c\242\0\1\u042c\33\0\2\u042d\1\0\2\u042d"+
-    "\2\0\1\u042d\1\0\1\u042d\176\0\1\u0471\1\0\1\u0471"+
-    "\5\0\1\u0471\310\0\1\u020c\164\0\4\u0472\2\0\1\u0472"+
-    "\15\0\1\u0472\6\0\12\u0472\1\u03e8\175\0\4\u0473\2\0"+
-    "\1\u0473\15\0\1\u0473\6\0\12\u0473\1\u0474\175\0\4\u0475"+
-    "\2\0\1\u0475\15\0\1\u0475\6\0\1\u0476\2\u0477\1\u0476"+
-    "\5\u0477\1\u0478\14\0\1\u02cf\161\0\4\u0479\2\0\1\u0479"+
-    "\15\0\1\u0479\6\0\12\u0479\1\u0435\13\0\1\u02cf\161\0"+
-    "\4\u0475\2\0\1\u0475\15\0\1\u0475\6\0\1\u0476\2\u0477"+
-    "\1\u0476\5\u0477\1\u0478\175\0\1\u0332\4\u0479\2\0\1\u0479"+
-    "\15\0\1\u0479\6\0\12\u047a\1\u0435\13\0\1\u02cf\160\0"+
-    "\1\u0332\4\u0479\2\0\1\u0479\15\0\1\u0479\6\0\12\u0479"+
-    "\1\u0435\13\0\1\u02cf\160\0\1\u0332\4\u0479\2\0\1\u0479"+
-    "\15\0\1\u0479\6\0\2\u047a\1\u0479\2\u047a\2\u0479\1\u047a"+
-    "\1\u0479\1\u047a\1\u0435\13\0\1\u02cf\226\0\1\u0391\13\0"+
-    "\1\u02cf\160\0\1\u047b\33\0\12\u043b\175\0\1\u047b\33\0"+
-    "\12\u047c\175\0\1\u047b\33\0\1\u043b\1\u047d\1\u047c\2\u043b"+
-    "\2\u047c\1\u043b\1\u047c\1\u043b\175\0\46\u0214\1\0\2\u0214"+
-    "\1\u026d\3\u0214\1\u0216\1\u0167\1\u0168\1\u0169\2\0\2\u0214"+
-    "\4\0\1\u0214\152\0\4\u047e\2\0\1\u047e\15\0\1\u047e"+
-    "\6\0\12\u047e\1\u03f6\175\0\4\u047f\2\0\1\u047f\15\0"+
-    "\1\u047f\6\0\12\u047f\1\u0480\174\0\1\u0163\4\u047f\2\0"+
-    "\1\u047f\15\0\1\u047f\6\0\12\u0481\1\u0480\174\0\1\u0163"+
-    "\4\u047f\2\0\1\u047f\15\0\1\u047f\6\0\12\u0482\1\u0480"+
-    "\174\0\1\u0163\4\u047f\2\0\1\u047f\15\0\1\u047f\6\0"+
-    "\1\u0481\1\u0483\1\u0482\2\u0481\2\u0482\1\u0481\1\u0482\1\u0481"+
-    "\1\u0480\175\0\4\u0484\2\0\1\u0484\15\0\1\u0484\6\0"+
-    "\12\u0484\14\0\1\u0122\161\0\4\u0485\2\0\1\u0485\15\0"+
-    "\1\u0485\6\0\12\u0485\1\u0445\13\0\1\u0122\161\0\4\u0484"+
-    "\2\0\1\u0484\15\0\1\u0484\6\0\12\u0484\175\0\1\u0163"+
-    "\4\u0485\2\0\1\u0485\15\0\1\u0485\6\0\12\u0486\1\u0445"+
-    "\13\0\1\u0122\160\0\1\u0163\4\u0485\2\0\1\u0485\15\0"+
-    "\1\u0485\6\0\12\u0485\1\u0445\13\0\1\u0122\160\0\1\u0163"+
-    "\4\u0485\2\0\1\u0485\15\0\1\u0485\6\0\2\u0486\1\u0485"+
-    "\2\u0486\2\u0485\1\u0486\1\u0485\1\u0486\1\u0445\13\0\1\u0122"+
-    "\226\0\1\u039f\13\0\1\u0122\160\0\1\346\1\u0487\31\230"+
-    "\1\347\12\230\175\0\1\346\32\230\1\347\11\230\1\u03b3"+
-    "\175\0\1\346\1\u0488\31\230\1\347\12\230\175\0\1\346"+
-    "\1\u0489\31\230\1\347\12\230\175\0\1\346\7\230\1\u048a"+
-    "\22\230\1\347\12\230\175\0\1\346\1\u048b\31\230\1\347"+
-    "\12\230\175\0\1\346\1\u048c\31\230\1\347\12\230\175\0"+
-    "\1\346\32\230\1\347\6\230\1\u048d\3\230\175\0\1\346"+
-    "\25\230\1\u048e\4\230\1\347\12\230\175\0\1\346\1\u048f"+
-    "\31\230\1\347\12\230\175\0\1\346\32\230\1\347\6\230"+
-    "\1\u0490\3\230\175\0\1\346\1\u0491\31\230\1\347\12\230"+
-    "\175\0\1\346\32\230\1\347\6\230\1\u01a4\3\230\175\0"+
-    "\1\346\12\230\1\u017b\17\230\1\347\12\230\175\0\1\346"+
-    "\1\u0492\31\230\1\347\12\230\175\0\1\346\10\230\1\u0493"+
-    "\21\230\1\347\12\230\175\0\1\346\31\230\1\u0494\1\347"+
-    "\12\230\175\0\1\376\24\264\1\u0495\5\264\1\141\12\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\32\264\1\141\10\264\1\u0496"+
-    "\1\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\1\264\1\u010a\30\264"+
-    "\1\141\12\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\2\264\1\u0497"+
-    "\27\264\1\141\12\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\3\264"+
-    "\1\u0498\26\264\1\141\12\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\3\264\1\u0499\26\264\1\141\12\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\150\0"+
-    "\1\376\32\264\1\141\1\264\1\u049a\10\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\150\0\1\376\3\264\1\u049b\26\264\1\141\12\264\1\0"+
-    "\3\136\1\0\2\136\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\150\0\1\376\1\u049c\31\264\1\141\12\264\1\0"+
-    "\3\136\1\0\2\136\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\150\0\1\376\26\264\1\u049d\3\264\1\141\12\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\151\0\4\u049e\2\0\1\u049e\15\0\1\u049e"+
-    "\6\0\12\u049e\1\u0424\175\0\4\u049f\2\0\1\u049f\15\0"+
-    "\1\u049f\6\0\12\u049f\1\u04a0\175\0\4\u04a1\2\0\1\u04a1"+
-    "\15\0\1\u04a1\6\0\1\u04a2\2\u04a3\1\u04a2\5\u04a3\1\u04a4"+
-    "\14\0\1\u0325\161\0\4\u04a5\2\0\1\u04a5\15\0\1\u04a5"+
-    "\6\0\12\u04a5\1\u0469\13\0\1\u0325\161\0\4\u04a1\2\0"+
-    "\1\u04a1\15\0\1\u04a1\6\0\1\u04a2\2\u04a3\1\u04a2\5\u04a3"+
-    "\1\u04a4\175\0\1\u0386\4\u04a5\2\0\1\u04a5\15\0\1\u04a5"+
-    "\6\0\12\u04a6\1\u0469\13\0\1\u0325\160\0\1\u0386\4\u04a5"+
-    "\2\0\1\u04a5\15\0\1\u04a5\6\0\12\u04a5\1\u0469\13\0"+
-    "\1\u0325\160\0\1\u0386\4\u04a5\2\0\1\u04a5\15\0\1\u04a5"+
-    "\6\0\2\u04a6\1\u04a5\2\u04a6\2\u04a5\1\u04a6\1\u04a5\1\u04a6"+
-    "\1\u0469\13\0\1\u0325\226\0\1\u03da\13\0\1\u0325\160\0"+
-    "\1\u04a7\33\0\12\u046f\175\0\1\u04a7\33\0\12\u04a8\175\0"+
-    "\1\u04a7\33\0\1\u046f\1\u04a9\1\u04a8\2\u046f\2\u04a8\1\u046f"+
-    "\1\u04a8\1\u046f\255\0\1\u0169\230\0\1\u03e8\175\0\4\u04aa"+
-    "\2\0\1\u04aa\15\0\1\u04aa\6\0\12\u04aa\1\u0474\175\0"+
-    "\4\u04ab\2\0\1\u04ab\15\0\1\u04ab\6\0\12\u04ab\1\u04ac"+
-    "\175\0\4\u04ad\2\0\1\u04ad\15\0\1\u04ad\6\0\12\u04ad"+
-    "\1\u04ae\13\0\1\u02cf\160\0\1\u0332\4\u04ad\2\0\1\u04ad"+
-    "\15\0\1\u04ad\6\0\12\u04af\1\u04ae\13\0\1\u02cf\160\0"+
-    "\1\u0332\4\u04ad\2\0\1\u04ad\15\0\1\u04ad\6\0\12\u04b0"+
-    "\1\u04ae\13\0\1\u02cf\160\0\1\u0332\4\u04ad\2\0\1\u04ad"+
-    "\15\0\1\u04ad\6\0\1\u04af\1\u04b1\1\u04b0\2\u04af\2\u04b0"+
-    "\1\u04af\1\u04b0\1\u04af\1\u04ae\13\0\1\u02cf\161\0\4\u04b2"+
-    "\2\0\1\u04b2\15\0\1\u04b2\6\0\12\u04b2\1\u0435\13\0"+
-    "\1\u02cf\160\0\1\u0332\4\u04b2\2\0\1\u04b2\15\0\1\u04b2"+
-    "\6\0\12\u04b2\1\u0435\13\0\1\u02cf\214\0\1\u04b3\2\u04b4"+
-    "\1\u04b3\5\u04b4\1\u04b5\175\0\1\u047b\242\0\1\u047b\33\0"+
-    "\2\u047c\1\0\2\u047c\2\0\1\u047c\1\0\1\u047c\243\0"+
-    "\1\u03f6\175\0\4\u04b6\2\0\1\u04b6\15\0\1\u04b6\6\0"+
-    "\12\u04b6\1\u0480\175\0\4\u0484\2\0\1\u0484\15\0\1\u0484"+
-    "\6\0\12\u0484\1\u0340\174\0\1\u0163\4\u04b6\2\0\1\u04b6"+
-    "\15\0\1\u04b6\6\0\12\u04b7\1\u0480\174\0\1\u0163\4\u04b6"+
-    "\2\0\1\u04b6\15\0\1\u04b6\6\0\12\u04b6\1\u0480\174\0"+
-    "\1\u0163\4\u04b6\2\0\1\u04b6\15\0\1\u04b6\6\0\2\u04b7"+
-    "\1\u04b6\2\u04b7\2\u04b6\1\u04b7\1\u04b6\1\u04b7\1\u0480\175\0"+
-    "\4\u04b8\2\0\1\u04b8\15\0\1\u04b8\6\0\12\u04b8\14\0"+
-    "\1\u0122\161\0\4\u04b9\2\0\1\u04b9\15\0\1\u04b9\6\0"+
-    "\12\u04b9\1\u0445\13\0\1\u0122\160\0\1\u0163\4\u04b9\2\0"+
-    "\1\u04b9\15\0\1\u04b9\6\0\12\u04b9\1\u0445\13\0\1\u0122"+
-    "\160\0\1\346\32\230\1\347\1\u04ba\11\230\175\0\1\346"+
-    "\2\230\1\u04bb\27\230\1\347\12\230\175\0\1\346\32\230"+
-    "\1\347\10\230\1\u03a7\1\230\175\0\1\346\15\230\1\u012f"+
-    "\14\230\1\347\12\230\175\0\1\346\23\230\1\u04bc\6\230"+
-    "\1\347\12\230\175\0\1\346\32\230\1\347\1\230\1\u04bd"+
-    "\10\230\175\0\1\346\32\230\1\347\3\230\1\u01a8\6\230"+
-    "\175\0\1\346\30\230\1\u04be\1\230\1\347\12\230\175\0"+
-    "\1\346\32\230\1\347\1\230\1\u04bf\10\230\175\0\1\346"+
-    "\6\230\1\u04c0\23\230\1\347\12\230\175\0\1\346\32\230"+
-    "\1\347\5\230\1\u04c1\4\230\175\0\1\346\32\230\1\347"+
-    "\5\230\1\u04c2\4\230\175\0\1\346\32\230\1\347\1\230"+
-    "\1\u012f\10\230\175\0\1\346\13\230\1\u04c3\16\230\1\347"+
-    "\12\230\175\0\1\376\32\264\1\141\11\264\1\u04c4\1\0"+
-    "\3\136\1\0\2\136\1\137\3\136\3\0\1\136\4\0"+
-    "\2\136\150\0\1\376\26\264\1\u0103\3\264\1\141\12\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\32\264\1\141\7\264\1\u04c5"+
-    "\2\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\32\264\1\141\11\264"+
-    "\1\307\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\3\264\1\u04c6\26\264"+
-    "\1\141\12\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\32\264\1\141"+
-    "\4\264\1\u04c7\5\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\16\264"+
-    "\1\u04c8\13\264\1\141\12\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\26\264\1\u04c9\3\264\1\141\12\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\150\0"+
-    "\1\376\32\264\1\141\7\264\1\u04ca\2\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\216\0\1\u0424\175\0\4\u04cb\2\0\1\u04cb\15\0\1\u04cb"+
-    "\6\0\12\u04cb\1\u04a0\175\0\4\u04cc\2\0\1\u04cc\15\0"+
-    "\1\u04cc\6\0\12\u04cc\1\u04cd\175\0\4\u04ce\2\0\1\u04ce"+
-    "\15\0\1\u04ce\6\0\12\u04ce\1\u04cf\13\0\1\u0325\160\0"+
-    "\1\u0386\4\u04ce\2\0\1\u04ce\15\0\1\u04ce\6\0\12\u04d0"+
-    "\1\u04cf\13\0\1\u0325\160\0\1\u0386\4\u04ce\2\0\1\u04ce"+
-    "\15\0\1\u04ce\6\0\12\u04d1\1\u04cf\13\0\1\u0325\160\0"+
-    "\1\u0386\4\u04ce\2\0\1\u04ce\15\0\1\u04ce\6\0\1\u04d0"+
-    "\1\u04d2\1\u04d1\2\u04d0\2\u04d1\1\u04d0\1\u04d1\1\u04d0\1\u04cf"+
-    "\13\0\1\u0325\161\0\4\u04d3\2\0\1\u04d3\15\0\1\u04d3"+
-    "\6\0\12\u04d3\1\u0469\13\0\1\u0325\160\0\1\u0386\4\u04d3"+
-    "\2\0\1\u04d3\15\0\1\u04d3\6\0\12\u04d3\1\u0469\13\0"+
-    "\1\u0325\214\0\1\u04d4\2\u04d5\1\u04d4\5\u04d5\1\u04d6\175\0"+
-    "\1\u04a7\242\0\1\u04a7\33\0\2\u04a8\1\0\2\u04a8\2\0"+
-    "\1\u04a8\1\0\1\u04a8\176\0\4\u04d7\2\0\1\u04d7\15\0"+
-    "\1\u04d7\6\0\12\u04d7\1\u0474\175\0\4\u04d8\2\0\1\u04d8"+
-    "\15\0\1\u04d8\6\0\12\u04d8\1\u04d9\175\0\4\u04da\2\0"+
-    "\1\u04da\15\0\1\u04da\6\0\1\u04db\2\u04dc\1\u04db\5\u04dc"+
-    "\1\u04dd\14\0\1\u02cf\161\0\4\u04de\2\0\1\u04de\15\0"+
-    "\1\u04de\6\0\12\u04de\1\u04ae\13\0\1\u02cf\161\0\4\u04da"+
-    "\2\0\1\u04da\15\0\1\u04da\6\0\1\u04db\2\u04dc\1\u04db"+
-    "\5\u04dc\1\u04dd\175\0\1\u0332\4\u04de\2\0\1\u04de\15\0"+
-    "\1\u04de\6\0\12\u04df\1\u04ae\13\0\1\u02cf\160\0\1\u0332"+
-    "\4\u04de\2\0\1\u04de\15\0\1\u04de\6\0\12\u04de\1\u04ae"+
-    "\13\0\1\u02cf\160\0\1\u0332\4\u04de\2\0\1\u04de\15\0"+
-    "\1\u04de\6\0\2\u04df\1\u04de\2\u04df\2\u04de\1\u04df\1\u04de"+
-    "\1\u04df\1\u04ae\13\0\1\u02cf\226\0\1\u0435\13\0\1\u02cf"+
-    "\214\0\12\u04b4\14\0\1\u02cf\214\0\12\u04e0\14\0\1\u02cf"+
-    "\214\0\1\u04b4\1\u04e1\1\u04e0\2\u04b4\2\u04e0\1\u04b4\1\u04e0"+
-    "\1\u04b4\14\0\1\u02cf\161\0\4\u04e2\2\0\1\u04e2\15\0"+
-    "\1\u04e2\6\0\12\u04e2\1\u0480\174\0\1\u0163\4\u04e2\2\0"+
-    "\1\u04e2\15\0\1\u04e2\6\0\12\u04e2\1\u0480\175\0\4\u04e3"+
-    "\2\0\1\u04e3\15\0\1\u04e3\6\0\12\u04e3\14\0\1\u0122"+
-    "\226\0\1\u0445\13\0\1\u0122\160\0\1\346\24\230\1\u04e4"+
-    "\5\230\1\347\12\230\175\0\1\346\32\230\1\347\10\230"+
-    "\1\u04e5\1\230\175\0\1\346\1\230\1\u0171\30\230\1\347"+
-    "\12\230\175\0\1\346\2\230\1\u04e6\27\230\1\347\12\230"+
-    "\175\0\1\346\3\230\1\u04e7\26\230\1\347\12\230\175\0"+
-    "\1\346\3\230\1\u04e8\26\230\1\347\12\230\175\0\1\346"+
-    "\32\230\1\347\1\230\1\u04e9\10\230\175\0\1\346\3\230"+
-    "\1\u04ea\26\230\1\347\12\230\175\0\1\346\1\u04eb\31\230"+
-    "\1\347\12\230\175\0\1\346\26\230\1\u04ec\3\230\1\347"+
-    "\12\230\175\0\1\376\7\264\1\u04ed\22\264\1\141\12\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\1\u04ee\31\264\1\141\12\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\32\264\1\141\1\u0300\11\264"+
-    "\1\0\3\136\1\0\2\136\1\137\3\136\3\0\1\136"+
-    "\4\0\2\136\150\0\1\376\24\264\1\u04ef\5\264\1\141"+
-    "\12\264\1\0\3\136\1\0\2\136\1\137\3\136\3\0"+
-    "\1\136\4\0\2\136\150\0\1\376\1\264\1\u04f0\30\264"+
-    "\1\141\12\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\32\264\1\141"+
-    "\2\264\1\u010a\7\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\6\264"+
-    "\1\u0103\23\264\1\141\12\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\151\0\4\u04f1"+
-    "\2\0\1\u04f1\15\0\1\u04f1\6\0\12\u04f1\1\u04a0\175\0"+
-    "\4\u04f2\2\0\1\u04f2\15\0\1\u04f2\6\0\12\u04f2\1\u04f3"+
-    "\175\0\4\u04f4\2\0\1\u04f4\15\0\1\u04f4\6\0\1\u04f5"+
-    "\2\u04f6\1\u04f5\5\u04f6\1\u04f7\14\0\1\u0325\161\0\4\u04f8"+
-    "\2\0\1\u04f8\15\0\1\u04f8\6\0\12\u04f8\1\u04cf\13\0"+
-    "\1\u0325\161\0\4\u04f4\2\0\1\u04f4\15\0\1\u04f4\6\0"+
-    "\1\u04f5\2\u04f6\1\u04f5\5\u04f6\1\u04f7\175\0\1\u0386\4\u04f8"+
-    "\2\0\1\u04f8\15\0\1\u04f8\6\0\12\u04f9\1\u04cf\13\0"+
-    "\1\u0325\160\0\1\u0386\4\u04f8\2\0\1\u04f8\15\0\1\u04f8"+
-    "\6\0\12\u04f8\1\u04cf\13\0\1\u0325\160\0\1\u0386\4\u04f8"+
-    "\2\0\1\u04f8\15\0\1\u04f8\6\0\2\u04f9\1\u04f8\2\u04f9"+
-    "\2\u04f8\1\u04f9\1\u04f8\1\u04f9\1\u04cf\13\0\1\u0325\226\0"+
-    "\1\u0469\13\0\1\u0325\214\0\12\u04d5\14\0\1\u0325\214\0"+
-    "\12\u04fa\14\0\1\u0325\214\0\1\u04d5\1\u04fb\1\u04fa\2\u04d5"+
-    "\2\u04fa\1\u04d5\1\u04fa\1\u04d5\14\0\1\u0325\226\0\1\u0474"+
-    "\175\0\4\u04fc\2\0\1\u04fc\15\0\1\u04fc\6\0\12\u04fc"+
-    "\1\u04d9\175\0\4\u04fd\2\0\1\u04fd\15\0\1\u04fd\6\0"+
-    "\12\u04fd\1\u04fe\175\0\4\u04ff\2\0\1\u04ff\15\0\1\u04ff"+
-    "\6\0\12\u04ff\1\u0500\13\0\1\u02cf\160\0\1\u0332\4\u04ff"+
-    "\2\0\1\u04ff\15\0\1\u04ff\6\0\12\u0501\1\u0500\13\0"+
-    "\1\u02cf\160\0\1\u0332\4\u04ff\2\0\1\u04ff\15\0\1\u04ff"+
-    "\6\0\12\u0502\1\u0500\13\0\1\u02cf\160\0\1\u0332\4\u04ff"+
-    "\2\0\1\u04ff\15\0\1\u04ff\6\0\1\u0501\1\u0503\1\u0502"+
-    "\2\u0501\2\u0502\1\u0501\1\u0502\1\u0501\1\u0500\13\0\1\u02cf"+
-    "\161\0\4\u0504\2\0\1\u0504\15\0\1\u0504\6\0\12\u0504"+
-    "\1\u04ae\13\0\1\u02cf\160\0\1\u0332\4\u0504\2\0\1\u0504"+
-    "\15\0\1\u0504\6\0\12\u0504\1\u04ae\13\0\1\u02cf\242\0"+
-    "\1\u02cf\214\0\2\u04e0\1\0\2\u04e0\2\0\1\u04e0\1\0"+
-    "\1\u04e0\14\0\1\u02cf\226\0\1\u0480\175\0\4\u0340\2\0"+
-    "\1\u0340\15\0\1\u0340\6\0\12\u0340\14\0\1\u0122\160\0"+
-    "\1\346\32\230\1\347\11\230\1\u0505\175\0\1\346\26\230"+
-    "\1\u016a\3\230\1\347\12\230\175\0\1\346\32\230\1\347"+
-    "\7\230\1\u0506\2\230\175\0\1\346\32\230\1\347\11\230"+
-    "\1\u012f\175\0\1\346\3\230\1\u0507\26\230\1\347\12\230"+
-    "\175\0\1\346\32\230\1\347\4\230\1\u0508\5\230\175\0"+
-    "\1\346\16\230\1\u0509\13\230\1\347\12\230\175\0\1\346"+
-    "\26\230\1\u050a\3\230\1\347\12\230\175\0\1\346\32\230"+
-    "\1\347\7\230\1\u050b\2\230\175\0\1\376\32\264\1\141"+
-    "\11\264\1\u050c\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\150\0\1\376\4\264\1\u0103"+
-    "\25\264\1\141\12\264\1\0\3\136\1\0\2\136\1\137"+
-    "\3\136\3\0\1\136\4\0\2\136\150\0\1\376\24\264"+
-    "\1\307\5\264\1\141\12\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\150\0\1\376"+
-    "\32\264\1\141\6\264\1\307\3\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\216\0"+
-    "\1\u04a0\175\0\4\u050d\2\0\1\u050d\15\0\1\u050d\6\0"+
-    "\12\u050d\1\u04f3\175\0\4\u050e\2\0\1\u050e\15\0\1\u050e"+
-    "\6\0\12\u050e\1\u050f\175\0\4\u0510\2\0\1\u0510\15\0"+
-    "\1\u0510\6\0\12\u0510\1\u0511\13\0\1\u0325\160\0\1\u0386"+
-    "\4\u0510\2\0\1\u0510\15\0\1\u0510\6\0\12\u0512\1\u0511"+
-    "\13\0\1\u0325\160\0\1\u0386\4\u0510\2\0\1\u0510\15\0"+
-    "\1\u0510\6\0\12\u0513\1\u0511\13\0\1\u0325\160\0\1\u0386"+
-    "\4\u0510\2\0\1\u0510\15\0\1\u0510\6\0\1\u0512\1\u0514"+
-    "\1\u0513\2\u0512\2\u0513\1\u0512\1\u0513\1\u0512\1\u0511\13\0"+
-    "\1\u0325\161\0\4\u0515\2\0\1\u0515\15\0\1\u0515\6\0"+
-    "\12\u0515\1\u04cf\13\0\1\u0325\160\0\1\u0386\4\u0515\2\0"+
-    "\1\u0515\15\0\1\u0515\6\0\12\u0515\1\u04cf\13\0\1\u0325"+
-    "\242\0\1\u0325\214\0\2\u04fa\1\0\2\u04fa\2\0\1\u04fa"+
-    "\1\0\1\u04fa\14\0\1\u0325\161\0\4\u0516\2\0\1\u0516"+
-    "\15\0\1\u0516\6\0\12\u0516\1\u04d9\175\0\4\u0517\2\0"+
-    "\1\u0517\15\0\1\u0517\6\0\12\u0517\1\u0518\175\0\4\u0519"+
-    "\2\0\1\u0519\15\0\1\u0519\6\0\1\u051a\2\u051b\1\u051a"+
-    "\5\u051b\1\u051c\14\0\1\u02cf\161\0\4\u051d\2\0\1\u051d"+
-    "\15\0\1\u051d\6\0\12\u051d\1\u0500\13\0\1\u02cf\161\0"+
-    "\4\u0519\2\0\1\u0519\15\0\1\u0519\6\0\1\u051a\2\u051b"+
-    "\1\u051a\5\u051b\1\u051c\175\0\1\u0332\4\u051d\2\0\1\u051d"+
-    "\15\0\1\u051d\6\0\12\u051e\1\u0500\13\0\1\u02cf\160\0"+
-    "\1\u0332\4\u051d\2\0\1\u051d\15\0\1\u051d\6\0\12\u051d"+
-    "\1\u0500\13\0\1\u02cf\160\0\1\u0332\4\u051d\2\0\1\u051d"+
-    "\15\0\1\u051d\6\0\2\u051e\1\u051d\2\u051e\2\u051d\1\u051e"+
-    "\1\u051d\1\u051e\1\u0500\13\0\1\u02cf\226\0\1\u04ae\13\0"+
-    "\1\u02cf\160\0\1\346\7\230\1\u051f\22\230\1\347\12\230"+
-    "\175\0\1\346\1\u0520\31\230\1\347\12\230\175\0\1\346"+
-    "\32\230\1\347\1\u03a7\11\230\175\0\1\346\24\230\1\u0521"+
-    "\5\230\1\347\12\230\175\0\1\346\1\230\1\u0522\30\230"+
-    "\1\347\12\230\175\0\1\346\32\230\1\347\2\230\1\u0171"+
-    "\7\230\175\0\1\346\6\230\1\u016a\23\230\1\347\12\230"+
-    "\175\0\1\376\1\u0523\31\264\1\141\12\264\1\0\3\136"+
-    "\1\0\2\136\1\137\3\136\3\0\1\136\4\0\2\136"+
-    "\151\0\4\u0524\2\0\1\u0524\15\0\1\u0524\6\0\12\u0524"+
-    "\1\u04f3\175\0\4\u0525\2\0\1\u0525\15\0\1\u0525\6\0"+
-    "\12\u0525\1\u0526\175\0\4\u0527\2\0\1\u0527\15\0\1\u0527"+
-    "\6\0\1\u0528\2\u0529\1\u0528\5\u0529\1\u052a\14\0\1\u0325"+
-    "\161\0\4\u052b\2\0\1\u052b\15\0\1\u052b\6\0\12\u052b"+
-    "\1\u0511\13\0\1\u0325\161\0\4\u0527\2\0\1\u0527\15\0"+
-    "\1\u0527\6\0\1\u0528\2\u0529\1\u0528\5\u0529\1\u052a\175\0"+
-    "\1\u0386\4\u052b\2\0\1\u052b\15\0\1\u052b\6\0\12\u052c"+
-    "\1\u0511\13\0\1\u0325\160\0\1\u0386\4\u052b\2\0\1\u052b"+
-    "\15\0\1\u052b\6\0\12\u052b\1\u0511\13\0\1\u0325\160\0"+
-    "\1\u0386\4\u052b\2\0\1\u052b\15\0\1\u052b\6\0\2\u052c"+
-    "\1\u052b\2\u052c\2\u052b\1\u052c\1\u052b\1\u052c\1\u0511\13\0"+
-    "\1\u0325\226\0\1\u04cf\13\0\1\u0325\226\0\1\u04d9\175\0"+
-    "\4\u052d\2\0\1\u052d\15\0\1\u052d\6\0\12\u052d\1\u0518"+
-    "\175\0\4\u052e\2\0\1\u052e\15\0\1\u052e\6\0\1\u052f"+
-    "\2\u0530\1\u052f\5\u0530\1\u0531\1\u0532\175\0\4\u0533\2\0"+
-    "\1\u0533\15\0\1\u0533\6\0\12\u0533\1\u0534\13\0\1\u02cf"+
-    "\160\0\1\u0332\4\u0533\2\0\1\u0533\15\0\1\u0533\6\0"+
-    "\12\u0535\1\u0534\13\0\1\u02cf\160\0\1\u0332\4\u0533\2\0"+
-    "\1\u0533\15\0\1\u0533\6\0\12\u0536\1\u0534\13\0\1\u02cf"+
-    "\160\0\1\u0332\4\u0533\2\0\1\u0533\15\0\1\u0533\6\0"+
-    "\1\u0535\1\u0537\1\u0536\2\u0535\2\u0536\1\u0535\1\u0536\1\u0535"+
-    "\1\u0534\13\0\1\u02cf\161\0\4\u0538\2\0\1\u0538\15\0"+
-    "\1\u0538\6\0\12\u0538\1\u0500\13\0\1\u02cf\160\0\1\u0332"+
-    "\4\u0538\2\0\1\u0538\15\0\1\u0538\6\0\12\u0538\1\u0500"+
-    "\13\0\1\u02cf\160\0\1\346\32\230\1\347\11\230\1\u0539"+
-    "\175\0\1\346\4\230\1\u016a\25\230\1\347\12\230\175\0"+
-    "\1\346\24\230\1\u012f\5\230\1\347\12\230\175\0\1\346"+
-    "\32\230\1\347\6\230\1\u012f\3\230\175\0\1\376\32\264"+
-    "\1\141\5\264\1\u053a\4\264\1\0\3\136\1\0\2\136"+
-    "\1\137\3\136\3\0\1\136\4\0\2\136\216\0\1\u04f3"+
-    "\175\0\4\u053b\2\0\1\u053b\15\0\1\u053b\6\0\12\u053b"+
-    "\1\u0526\175\0\4\u053c\2\0\1\u053c\15\0\1\u053c\6\0"+
-    "\1\u053d\2\u053e\1\u053d\5\u053e\1\u053f\1\u0540\175\0\4\u0541"+
-    "\2\0\1\u0541\15\0\1\u0541\6\0\12\u0541\1\u0542\13\0"+
-    "\1\u0325\160\0\1\u0386\4\u0541\2\0\1\u0541\15\0\1\u0541"+
-    "\6\0\12\u0543\1\u0542\13\0\1\u0325\160\0\1\u0386\4\u0541"+
-    "\2\0\1\u0541\15\0\1\u0541\6\0\12\u0544\1\u0542\13\0"+
-    "\1\u0325\160\0\1\u0386\4\u0541\2\0\1\u0541\15\0\1\u0541"+
-    "\6\0\1\u0543\1\u0545\1\u0544\2\u0543\2\u0544\1\u0543\1\u0544"+
-    "\1\u0543\1\u0542\13\0\1\u0325\161\0\4\u0546\2\0\1\u0546"+
-    "\15\0\1\u0546\6\0\12\u0546\1\u0511\13\0\1\u0325\160\0"+
-    "\1\u0386\4\u0546\2\0\1\u0546\15\0\1\u0546\6\0\12\u0546"+
-    "\1\u0511\13\0\1\u0325\161\0\4\u0547\2\0\1\u0547\15\0"+
-    "\1\u0547\6\0\12\u0547\1\u0518\175\0\4\u0548\2\0\1\u0548"+
-    "\15\0\1\u0548\6\0\12\u0548\1\u0549\174\0\1\u0332\4\u0548"+
-    "\2\0\1\u0548\15\0\1\u0548\6\0\12\u054a\1\u0549\174\0"+
-    "\1\u0332\4\u0548\2\0\1\u0548\15\0\1\u0548\6\0\12\u054b"+
-    "\1\u0549\174\0\1\u0332\4\u0548\2\0\1\u0548\15\0\1\u0548"+
-    "\6\0\1\u054a\1\u054c\1\u054b\2\u054a\2\u054b\1\u054a\1\u054b"+
-    "\1\u054a\1\u0549\175\0\4\u054d\2\0\1\u054d\15\0\1\u054d"+
-    "\6\0\12\u054d\14\0\1\u02cf\161\0\4\u054e\2\0\1\u054e"+
-    "\15\0\1\u054e\6\0\12\u054e\1\u0534\13\0\1\u02cf\161\0"+
-    "\4\u054d\2\0\1\u054d\15\0\1\u054d\6\0\12\u054d\175\0"+
-    "\1\u0332\4\u054e\2\0\1\u054e\15\0\1\u054e\6\0\12\u054f"+
-    "\1\u0534\13\0\1\u02cf\160\0\1\u0332\4\u054e\2\0\1\u054e"+
-    "\15\0\1\u054e\6\0\12\u054e\1\u0534\13\0\1\u02cf\160\0"+
-    "\1\u0332\4\u054e\2\0\1\u054e\15\0\1\u054e\6\0\2\u054f"+
-    "\1\u054e\2\u054f\2\u054e\1\u054f\1\u054e\1\u054f\1\u0534\13\0"+
-    "\1\u02cf\226\0\1\u0500\13\0\1\u02cf\160\0\1\346\1\u0550"+
-    "\31\230\1\347\12\230\175\0\1\376\7\264\1\u0551\22\264"+
-    "\1\141\12\264\1\0\3\136\1\0\2\136\1\137\3\136"+
-    "\3\0\1\136\4\0\2\136\151\0\4\u0552\2\0\1\u0552"+
-    "\15\0\1\u0552\6\0\12\u0552\1\u0526\175\0\4\u0553\2\0"+
-    "\1\u0553\15\0\1\u0553\6\0\12\u0553\1\u0554\174\0\1\u0386"+
-    "\4\u0553\2\0\1\u0553\15\0\1\u0553\6\0\12\u0555\1\u0554"+
-    "\174\0\1\u0386\4\u0553\2\0\1\u0553\15\0\1\u0553\6\0"+
-    "\12\u0556\1\u0554\174\0\1\u0386\4\u0553\2\0\1\u0553\15\0"+
-    "\1\u0553\6\0\1\u0555\1\u0557\1\u0556\2\u0555\2\u0556\1\u0555"+
-    "\1\u0556\1\u0555\1\u0554\175\0\4\u0558\2\0\1\u0558\15\0"+
-    "\1\u0558\6\0\12\u0558\14\0\1\u0325\161\0\4\u0559\2\0"+
-    "\1\u0559\15\0\1\u0559\6\0\12\u0559\1\u0542\13\0\1\u0325"+
-    "\161\0\4\u0558\2\0\1\u0558\15\0\1\u0558\6\0\12\u0558"+
-    "\175\0\1\u0386\4\u0559\2\0\1\u0559\15\0\1\u0559\6\0"+
-    "\12\u055a\1\u0542\13\0\1\u0325\160\0\1\u0386\4\u0559\2\0"+
-    "\1\u0559\15\0\1\u0559\6\0\12\u0559\1\u0542\13\0\1\u0325"+
-    "\160\0\1\u0386\4\u0559\2\0\1\u0559\15\0\1\u0559\6\0"+
-    "\2\u055a\1\u0559\2\u055a\2\u0559\1\u055a\1\u0559\1\u055a\1\u0542"+
-    "\13\0\1\u0325\226\0\1\u0511\13\0\1\u0325\226\0\1\u0518"+
-    "\175\0\4\u055b\2\0\1\u055b\15\0\1\u055b\6\0\12\u055b"+
-    "\1\u0549\175\0\4\u054d\2\0\1\u054d\15\0\1\u054d\6\0"+
-    "\12\u054d\1\u04e0\174\0\1\u0332\4\u055b\2\0\1\u055b\15\0"+
-    "\1\u055b\6\0\12\u055c\1\u0549\174\0\1\u0332\4\u055b\2\0"+
-    "\1\u055b\15\0\1\u055b\6\0\12\u055b\1\u0549\174\0\1\u0332"+
-    "\4\u055b\2\0\1\u055b\15\0\1\u055b\6\0\2\u055c\1\u055b"+
-    "\2\u055c\2\u055b\1\u055c\1\u055b\1\u055c\1\u0549\175\0\4\u055d"+
-    "\2\0\1\u055d\15\0\1\u055d\6\0\12\u055d\14\0\1\u02cf"+
-    "\161\0\4\u055e\2\0\1\u055e\15\0\1\u055e\6\0\12\u055e"+
-    "\1\u0534\13\0\1\u02cf\160\0\1\u0332\4\u055e\2\0\1\u055e"+
-    "\15\0\1\u055e\6\0\12\u055e\1\u0534\13\0\1\u02cf\160\0"+
-    "\1\346\32\230\1\347\5\230\1\u055f\4\230\175\0\1\376"+
-    "\1\264\1\u0367\30\264\1\141\12\264\1\0\3\136\1\0"+
-    "\2\136\1\137\3\136\3\0\1\136\4\0\2\136\216\0"+
-    "\1\u0526\175\0\4\u0560\2\0\1\u0560\15\0\1\u0560\6\0"+
-    "\12\u0560\1\u0554\175\0\4\u0558\2\0\1\u0558\15\0\1\u0558"+
-    "\6\0\12\u0558\1\u04fa\174\0\1\u0386\4\u0560\2\0\1\u0560"+
-    "\15\0\1\u0560\6\0\12\u0561\1\u0554\174\0\1\u0386\4\u0560"+
-    "\2\0\1\u0560\15\0\1\u0560\6\0\12\u0560\1\u0554\174\0"+
-    "\1\u0386\4\u0560\2\0\1\u0560\15\0\1\u0560\6\0\2\u0561"+
-    "\1\u0560\2\u0561\2\u0560\1\u0561\1\u0560\1\u0561\1\u0554\175\0"+
-    "\4\u0562\2\0\1\u0562\15\0\1\u0562\6\0\12\u0562\14\0"+
-    "\1\u0325\161\0\4\u0563\2\0\1\u0563\15\0\1\u0563\6\0"+
-    "\12\u0563\1\u0542\13\0\1\u0325\160\0\1\u0386\4\u0563\2\0"+
-    "\1\u0563\15\0\1\u0563\6\0\12\u0563\1\u0542\13\0\1\u0325"+
-    "\161\0\4\u0564\2\0\1\u0564\15\0\1\u0564\6\0\12\u0564"+
-    "\1\u0549\174\0\1\u0332\4\u0564\2\0\1\u0564\15\0\1\u0564"+
-    "\6\0\12\u0564\1\u0549\175\0\4\u0565\2\0\1\u0565\15\0"+
-    "\1\u0565\6\0\12\u0565\14\0\1\u02cf\226\0\1\u0534\13\0"+
-    "\1\u02cf\160\0\1\346\7\230\1\u0566\22\230\1\347\12\230"+
-    "\176\0\4\u0567\2\0\1\u0567\15\0\1\u0567\6\0\12\u0567"+
-    "\1\u0554\174\0\1\u0386\4\u0567\2\0\1\u0567\15\0\1\u0567"+
-    "\6\0\12\u0567\1\u0554\175\0\4\u0568\2\0\1\u0568\15\0"+
-    "\1\u0568\6\0\12\u0568\14\0\1\u0325\226\0\1\u0542\13\0"+
-    "\1\u0325\226\0\1\u0549\175\0\4\u04e0\2\0\1\u04e0\15\0"+
-    "\1\u04e0\6\0\12\u04e0\14\0\1\u02cf\160\0\1\346\1\230"+
-    "\1\u03fe\30\230\1\347\12\230\243\0\1\u0554\175\0\4\u04fa"+
-    "\2\0\1\u04fa\15\0\1\u04fa\6\0\12\u04fa\14\0\1\u0325"+
-    "\11\0";
-
-  private static int [] zzUnpackTrans() {
-    int [] result = new int[223147];
-    int offset = 0;
-    offset = zzUnpackTrans(ZZ_TRANS_PACKED_0, offset, result);
-    return result;
-  }
-
-  private static int zzUnpackTrans(String packed, int offset, int [] result) {
-    int i = 0;       /* index in packed string  */
-    int j = offset;  /* index in unpacked array */
-    int l = packed.length();
-    while (i < l) {
-      int count = packed.charAt(i++);
-      int value = packed.charAt(i++);
-      value--;
-      do result[j++] = value; while (--count > 0);
-    }
-    return j;
-  }
-
-
-  /* error codes */
-  private static final int ZZ_UNKNOWN_ERROR = 0;
-  private static final int ZZ_NO_MATCH = 1;
-  private static final int ZZ_PUSHBACK_2BIG = 2;
-
-  /* error messages for the codes above */
-  private static final String ZZ_ERROR_MSG[] = {
-    "Unkown internal scanner error",
-    "Error: could not match input",
-    "Error: pushback value was too large"
-  };
-
-  /**
-   * ZZ_ATTRIBUTE[aState] contains the attributes of state <code>aState</code>
-   */
-  private static final int [] ZZ_ATTRIBUTE = zzUnpackAttribute();
-
-  private static final String ZZ_ATTRIBUTE_PACKED_0 =
-    "\1\0\1\11\44\1\15\0\1\1\1\0\1\1\10\0"+
-    "\1\1\15\0\1\1\24\0\2\1\1\0\3\1\1\0"+
-    "\1\1\1\0\4\1\53\0\32\1\3\0\4\1\32\0"+
-    "\4\1\17\0\1\11\1\0\24\1\2\0\1\1\1\0"+
-    "\7\1\3\0\2\1\1\0\4\1\2\0\2\1\1\0"+
-    "\2\1\10\0\1\1\32\0\1\1\1\0\11\1\1\0"+
-    "\1\1\2\0\1\1\1\0\1\1\10\0\3\1\15\0"+
-    "\11\1\3\0\2\1\1\0\4\1\2\0\4\1\1\0"+
-    "\2\1\1\0\2\1\1\0\3\1\7\0\2\1\20\0"+
-    "\1\1\10\0\1\1\3\0\1\1\37\0\3\1\23\0"+
-    "\1\1\40\0\1\1\4\0\1\1\6\0\1\1\4\0"+
-    "\2\1\43\0\1\1\61\0\1\1\53\0\1\1\64\0"+
-    "\1\1\145\0\1\1\143\0\1\1\130\0\1\1\111\0"+
-    "\1\1\63\0\1\1\367\0";
-
-  private static int [] zzUnpackAttribute() {
-    int [] result = new int[1384];
-    int offset = 0;
-    offset = zzUnpackAttribute(ZZ_ATTRIBUTE_PACKED_0, offset, result);
-    return result;
-  }
-
-  private static int zzUnpackAttribute(String packed, int offset, int [] result) {
-    int i = 0;       /* index in packed string  */
-    int j = offset;  /* index in unpacked array */
-    int l = packed.length();
-    while (i < l) {
-      int count = packed.charAt(i++);
-      int value = packed.charAt(i++);
-      do result[j++] = value; while (--count > 0);
-    }
-    return j;
-  }
-
-  /** the input device */
-  private java.io.Reader zzReader;
-
-  /** the current state of the DFA */
-  private int zzState;
-
-  /** the current lexical state */
-  private int zzLexicalState = YYINITIAL;
-
-  /** this buffer contains the current text to be matched and is
-      the source of the yytext() string */
-  private char zzBuffer[] = new char[ZZ_BUFFERSIZE];
-
-  /** the textposition at the last accepting state */
-  private int zzMarkedPos;
-
-  /** the current text position in the buffer */
-  private int zzCurrentPos;
-
-  /** startRead marks the beginning of the yytext() string in the buffer */
-  private int zzStartRead;
-
-  /** endRead marks the last character in the buffer, that has been read
-      from input */
-  private int zzEndRead;
-
-  /** number of newlines encountered up to the start of the matched text */
-  private int yyline;
-
-  /** the number of characters up to the start of the matched text */
-  private int yychar;
-
-  /**
-   * the number of characters from the last newline up to the start of the 
-   * matched text
-   */
-  private int yycolumn;
-
-  /** 
-   * zzAtBOL == true <=> the scanner is currently at the beginning of a line
-   */
-  private boolean zzAtBOL = true;
-
-  /** zzAtEOF == true <=> the scanner is at the EOF */
-  private boolean zzAtEOF;
-
-  /** denotes if the user-EOF-code has already been executed */
-  private boolean zzEOFDone;
-
-  /* user code: */
-  /** Alphanumeric sequences */
-  public static final int WORD_TYPE = UAX29URLEmailTokenizer.ALPHANUM;
-  
-  /** Numbers */
-  public static final int NUMERIC_TYPE = UAX29URLEmailTokenizer.NUM;
-  
-  /**
-   * Chars in class \p{Line_Break = Complex_Context} are from South East Asian
-   * scripts (Thai, Lao, Myanmar, Khmer, etc.).  Sequences of these are kept 
-   * together as as a single token rather than broken up, because the logic
-   * required to break them at word boundaries is too complex for UAX#29.
-   * <p>
-   * See Unicode Line Breaking Algorithm: http://www.unicode.org/reports/tr14/#SA
-   */
-  public static final int SOUTH_EAST_ASIAN_TYPE = UAX29URLEmailTokenizer.SOUTHEAST_ASIAN;
-  
-  public static final int IDEOGRAPHIC_TYPE = UAX29URLEmailTokenizer.IDEOGRAPHIC;
-  
-  public static final int HIRAGANA_TYPE = UAX29URLEmailTokenizer.HIRAGANA;
-  
-  public static final int KATAKANA_TYPE = UAX29URLEmailTokenizer.KATAKANA;
-  
-  public static final int HANGUL_TYPE = UAX29URLEmailTokenizer.HANGUL;
-  
-  public static final int EMAIL_TYPE = UAX29URLEmailTokenizer.EMAIL;
-  
-  public static final int URL_TYPE = UAX29URLEmailTokenizer.URL;
-
-  public final int yychar()
-  {
-    return yychar;
-  }
-
-  /**
-   * Fills CharTermAttribute with the current token text.
-   */
-  public final void getText(CharTermAttribute t) {
-    t.copyBuffer(zzBuffer, zzStartRead, zzMarkedPos-zzStartRead);
-  }
-
-
-  /**
-   * Creates a new scanner
-   * There is also a java.io.InputStream version of this constructor.
-   *
-   * @param   in  the java.io.Reader to read input from.
-   */
-  public UAX29URLEmailTokenizerImpl34(java.io.Reader in) {
-    this.zzReader = in;
-  }
-
-  /**
-   * Creates a new scanner.
-   * There is also java.io.Reader version of this constructor.
-   *
-   * @param   in  the java.io.Inputstream to read input from.
-   */
-  public UAX29URLEmailTokenizerImpl34(java.io.InputStream in) {
-    this(new java.io.InputStreamReader(in));
-  }
-
-  /** 
-   * Unpacks the compressed character translation table.
-   *
-   * @param packed   the packed character translation table
-   * @return         the unpacked character translation table
-   */
-  private static char [] zzUnpackCMap(String packed) {
-    char [] map = new char[0x10000];
-    int i = 0;  /* index in packed string  */
-    int j = 0;  /* index in unpacked array */
-    while (i < 2812) {
-      int  count = packed.charAt(i++);
-      char value = packed.charAt(i++);
-      do map[j++] = value; while (--count > 0);
-    }
-    return map;
-  }
-
-
-  /**
-   * Refills the input buffer.
-   *
-   * @return      <code>false</code>, iff there was new input.
-   * 
-   * @exception   java.io.IOException  if any I/O-Error occurs
-   */
-  private boolean zzRefill() throws java.io.IOException {
-
-    /* first: make room (if you can) */
-    if (zzStartRead > 0) {
-      System.arraycopy(zzBuffer, zzStartRead,
-                       zzBuffer, 0,
-                       zzEndRead-zzStartRead);
-
-      /* translate stored positions */
-      zzEndRead-= zzStartRead;
-      zzCurrentPos-= zzStartRead;
-      zzMarkedPos-= zzStartRead;
-      zzStartRead = 0;
-    }
-
-    /* is the buffer big enough? */
-    if (zzCurrentPos >= zzBuffer.length) {
-      /* if not: blow it up */
-      char newBuffer[] = new char[zzCurrentPos*2];
-      System.arraycopy(zzBuffer, 0, newBuffer, 0, zzBuffer.length);
-      zzBuffer = newBuffer;
-    }
-
-    /* finally: fill the buffer with new input */
-    int numRead = zzReader.read(zzBuffer, zzEndRead,
-                                            zzBuffer.length-zzEndRead);
-
-    if (numRead > 0) {
-      zzEndRead+= numRead;
-      return false;
-    }
-    // unlikely but not impossible: read 0 characters, but not at end of stream    
-    if (numRead == 0) {
-      int c = zzReader.read();
-      if (c == -1) {
-        return true;
-      } else {
-        zzBuffer[zzEndRead++] = (char) c;
-        return false;
-      }     
-    }
-
-	// numRead < 0
-    return true;
-  }
-
-    
-  /**
-   * Closes the input stream.
-   */
-  public final void yyclose() throws java.io.IOException {
-    zzAtEOF = true;            /* indicate end of file */
-    zzEndRead = zzStartRead;  /* invalidate buffer    */
-
-    if (zzReader != null)
-      zzReader.close();
-  }
-
-
-  /**
-   * Resets the scanner to read from a new input stream.
-   * Does not close the old reader.
-   *
-   * All internal variables are reset, the old input stream 
-   * <b>cannot</b> be reused (internal buffer is discarded and lost).
-   * Lexical state is set to <tt>ZZ_INITIAL</tt>.
-   *
-   * Internal scan buffer is resized down to its initial length, if it has grown.
-   *
-   * @param reader   the new input stream 
-   */
-  public final void yyreset(java.io.Reader reader) {
-    zzReader = reader;
-    zzAtBOL  = true;
-    zzAtEOF  = false;
-    zzEOFDone = false;
-    zzEndRead = zzStartRead = 0;
-    zzCurrentPos = zzMarkedPos = 0;
-    yyline = yychar = yycolumn = 0;
-    zzLexicalState = YYINITIAL;
-    if (zzBuffer.length > ZZ_BUFFERSIZE)
-      zzBuffer = new char[ZZ_BUFFERSIZE];
-  }
-
-
-  /**
-   * Returns the current lexical state.
-   */
-  public final int yystate() {
-    return zzLexicalState;
-  }
-
-
-  /**
-   * Enters a new lexical state
-   *
-   * @param newState the new lexical state
-   */
-  public final void yybegin(int newState) {
-    zzLexicalState = newState;
-  }
-
-
-  /**
-   * Returns the text matched by the current regular expression.
-   */
-  public final String yytext() {
-    return new String( zzBuffer, zzStartRead, zzMarkedPos-zzStartRead );
-  }
-
-
-  /**
-   * Returns the character at position <tt>pos</tt> from the 
-   * matched text. 
-   * 
-   * It is equivalent to yytext().charAt(pos), but faster
-   *
-   * @param pos the position of the character to fetch. 
-   *            A value from 0 to yylength()-1.
-   *
-   * @return the character at position pos
-   */
-  public final char yycharat(int pos) {
-    return zzBuffer[zzStartRead+pos];
-  }
-
-
-  /**
-   * Returns the length of the matched text region.
-   */
-  public final int yylength() {
-    return zzMarkedPos-zzStartRead;
-  }
-
-
-  /**
-   * Reports an error that occured while scanning.
-   *
-   * In a wellformed scanner (no or only correct usage of 
-   * yypushback(int) and a match-all fallback rule) this method 
-   * will only be called with things that "Can't Possibly Happen".
-   * If this method is called, something is seriously wrong
-   * (e.g. a JFlex bug producing a faulty scanner etc.).
-   *
-   * Usual syntax/scanner level error handling should be done
-   * in error fallback rules.
-   *
-   * @param   errorCode  the code of the errormessage to display
-   */
-  private void zzScanError(int errorCode) {
-    String message;
-    try {
-      message = ZZ_ERROR_MSG[errorCode];
-    }
-    catch (ArrayIndexOutOfBoundsException e) {
-      message = ZZ_ERROR_MSG[ZZ_UNKNOWN_ERROR];
-    }
-
-    throw new Error(message);
-  } 
-
-
-  /**
-   * Pushes the specified amount of characters back into the input stream.
-   *
-   * They will be read again by then next call of the scanning method
-   *
-   * @param number  the number of characters to be read again.
-   *                This number must not be greater than yylength()!
-   */
-  public void yypushback(int number)  {
-    if ( number > yylength() )
-      zzScanError(ZZ_PUSHBACK_2BIG);
-
-    zzMarkedPos -= number;
-  }
-
-
-  /**
-   * Resumes scanning until the next regular expression is matched,
-   * the end of input is encountered or an I/O-Error occurs.
-   *
-   * @return      the next token
-   * @exception   java.io.IOException  if any I/O-Error occurs
-   */
-  public int getNextToken() throws java.io.IOException {
-    int zzInput;
-    int zzAction;
-
-    // cached fields:
-    int zzCurrentPosL;
-    int zzMarkedPosL;
-    int zzEndReadL = zzEndRead;
-    char [] zzBufferL = zzBuffer;
-    char [] zzCMapL = ZZ_CMAP;
-
-    int [] zzTransL = ZZ_TRANS;
-    int [] zzRowMapL = ZZ_ROWMAP;
-    int [] zzAttrL = ZZ_ATTRIBUTE;
-
-    while (true) {
-      zzMarkedPosL = zzMarkedPos;
-
-      yychar+= zzMarkedPosL-zzStartRead;
-
-      zzAction = -1;
-
-      zzCurrentPosL = zzCurrentPos = zzStartRead = zzMarkedPosL;
-  
-      zzState = ZZ_LEXSTATE[zzLexicalState];
-
-      // set up zzAction for empty match case:
-      int zzAttributes = zzAttrL[zzState];
-      if ( (zzAttributes & 1) == 1 ) {
-        zzAction = zzState;
-      }
-
-
-      zzForAction: {
-        while (true) {
-    
-          if (zzCurrentPosL < zzEndReadL)
-            zzInput = zzBufferL[zzCurrentPosL++];
-          else if (zzAtEOF) {
-            zzInput = YYEOF;
-            break zzForAction;
-          }
-          else {
-            // store back cached positions
-            zzCurrentPos  = zzCurrentPosL;
-            zzMarkedPos   = zzMarkedPosL;
-            boolean eof = zzRefill();
-            // get translated positions and possibly new buffer
-            zzCurrentPosL  = zzCurrentPos;
-            zzMarkedPosL   = zzMarkedPos;
-            zzBufferL      = zzBuffer;
-            zzEndReadL     = zzEndRead;
-            if (eof) {
-              zzInput = YYEOF;
-              break zzForAction;
-            }
-            else {
-              zzInput = zzBufferL[zzCurrentPosL++];
-            }
-          }
-          int zzNext = zzTransL[ zzRowMapL[zzState] + zzCMapL[zzInput] ];
-          if (zzNext == -1) break zzForAction;
-          zzState = zzNext;
-
-          zzAttributes = zzAttrL[zzState];
-          if ( (zzAttributes & 1) == 1 ) {
-            zzAction = zzState;
-            zzMarkedPosL = zzCurrentPosL;
-            if ( (zzAttributes & 8) == 8 ) break zzForAction;
-          }
-
-        }
-      }
-
-      // store back cached position
-      zzMarkedPos = zzMarkedPosL;
-
-      switch (zzAction < 0 ? zzAction : ZZ_ACTION[zzAction]) {
-        case 2: 
-          { return WORD_TYPE;
-          }
-        case 11: break;
-        case 5: 
-          { return SOUTH_EAST_ASIAN_TYPE;
-          }
-        case 12: break;
-        case 1: 
-          { /* Break so we don't hit fall-through warning: */ break;/* Not numeric, word, ideographic, hiragana, or SE Asian -- ignore it. */
-          }
-        case 13: break;
-        case 10: 
-          { return URL_TYPE;
-          }
-        case 14: break;
-        case 9: 
-          { return EMAIL_TYPE;
-          }
-        case 15: break;
-        case 4: 
-          { return KATAKANA_TYPE;
-          }
-        case 16: break;
-        case 6: 
-          { return IDEOGRAPHIC_TYPE;
-          }
-        case 17: break;
-        case 8: 
-          { return HANGUL_TYPE;
-          }
-        case 18: break;
-        case 3: 
-          { return NUMERIC_TYPE;
-          }
-        case 19: break;
-        case 7: 
-          { return HIRAGANA_TYPE;
-          }
-        case 20: break;
-        default: 
-          if (zzInput == YYEOF && zzStartRead == zzCurrentPos) {
-            zzAtEOF = true;
-              {
-                return StandardTokenizerInterface.YYEOF;
-              }
-          } 
-          else {
-            zzScanError(ZZ_NO_MATCH);
-          }
-      }
-    }
-  }
-
-
-}
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/UAX29URLEmailTokenizerImpl34.jflex b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/UAX29URLEmailTokenizerImpl34.jflex
deleted file mode 100644
index dc78fa3..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/UAX29URLEmailTokenizerImpl34.jflex
+++ /dev/null
@@ -1,272 +0,0 @@
-package org.apache.lucene.analysis.standard.std34;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.standard.StandardTokenizerInterface;
-import org.apache.lucene.analysis.standard.UAX29URLEmailTokenizer;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-
-/**
- * This class implements UAX29URLEmailTokenizer, except with a bug
- * (https://issues.apache.org/jira/browse/LUCENE-3880) where "mailto:"
- * URI scheme prepended to an email address will disrupt recognition
- * of the email address.
- * @deprecated This class is only for exact backwards compatibility
- */
- @Deprecated
-%%
-
-%unicode 6.0
-%integer
-%final
-%public
-%class UAX29URLEmailTokenizerImpl34
-%implements StandardTokenizerInterface
-%function getNextToken
-%char
-
-%include src/java/org/apache/lucene/analysis/standard/SUPPLEMENTARY.jflex-macro
-ALetter = ([\p{WB:ALetter}] | {ALetterSupp})
-Format =  ([\p{WB:Format}] | {FormatSupp})
-Numeric = ([\p{WB:Numeric}] | {NumericSupp})
-Extend =  ([\p{WB:Extend}] | {ExtendSupp})
-Katakana = ([\p{WB:Katakana}] | {KatakanaSupp})
-MidLetter = ([\p{WB:MidLetter}] | {MidLetterSupp})
-MidNum = ([\p{WB:MidNum}] | {MidNumSupp})
-MidNumLet = ([\p{WB:MidNumLet}] | {MidNumLetSupp})
-ExtendNumLet = ([\p{WB:ExtendNumLet}] | {ExtendNumLetSupp})
-ComplexContext = ([\p{LB:Complex_Context}] | {ComplexContextSupp})
-Han = ([\p{Script:Han}] | {HanSupp})
-Hiragana = ([\p{Script:Hiragana}] | {HiraganaSupp})
-
-// Script=Hangul & Aletter
-HangulEx       = (!(!\p{Script:Hangul}|!\p{WB:ALetter})) ({Format} | {Extend})*
-// UAX#29 WB4. X (Extend | Format)* --> X
-//
-ALetterEx      = {ALetter}                     ({Format} | {Extend})*
-// TODO: Convert hard-coded full-width numeric range to property intersection (something like [\p{Full-Width}&&\p{Numeric}]) once JFlex supports it
-NumericEx      = ({Numeric} | [\uFF10-\uFF19]) ({Format} | {Extend})*
-KatakanaEx     = {Katakana}                    ({Format} | {Extend})* 
-MidLetterEx    = ({MidLetter} | {MidNumLet})   ({Format} | {Extend})* 
-MidNumericEx   = ({MidNum} | {MidNumLet})      ({Format} | {Extend})*
-ExtendNumLetEx = {ExtendNumLet}                ({Format} | {Extend})*
-
-HanEx = {Han} ({Format} | {Extend})*
-HiraganaEx = {Hiragana} ({Format} | {Extend})*
-
-// URL and E-mail syntax specifications:
-//
-//     RFC-952:  DOD INTERNET HOST TABLE SPECIFICATION
-//     RFC-1035: DOMAIN NAMES - IMPLEMENTATION AND SPECIFICATION
-//     RFC-1123: Requirements for Internet Hosts - Application and Support
-//     RFC-1738: Uniform Resource Locators (URL)
-//     RFC-3986: Uniform Resource Identifier (URI): Generic Syntax
-//     RFC-5234: Augmented BNF for Syntax Specifications: ABNF
-//     RFC-5321: Simple Mail Transfer Protocol
-//     RFC-5322: Internet Message Format
-
-%include src/java/org/apache/lucene/analysis/standard/ASCIITLD.jflex-macro
-
-DomainLabel = [A-Za-z0-9] ([-A-Za-z0-9]* [A-Za-z0-9])?
-DomainNameStrict = {DomainLabel} ("." {DomainLabel})* {ASCIITLD}
-DomainNameLoose  = {DomainLabel} ("." {DomainLabel})*
-
-IPv4DecimalOctet = "0"{0,2} [0-9] | "0"? [1-9][0-9] | "1" [0-9][0-9] | "2" ([0-4][0-9] | "5" [0-5])
-IPv4Address  = {IPv4DecimalOctet} ("." {IPv4DecimalOctet}){3} 
-IPv6Hex16Bit = [0-9A-Fa-f]{1,4}
-IPv6LeastSignificant32Bits = {IPv4Address} | ({IPv6Hex16Bit} ":" {IPv6Hex16Bit})
-IPv6Address =                                                  ({IPv6Hex16Bit} ":"){6} {IPv6LeastSignificant32Bits}
-            |                                             "::" ({IPv6Hex16Bit} ":"){5} {IPv6LeastSignificant32Bits}
-            |                            {IPv6Hex16Bit}?  "::" ({IPv6Hex16Bit} ":"){4} {IPv6LeastSignificant32Bits}
-            | (({IPv6Hex16Bit} ":"){0,1} {IPv6Hex16Bit})? "::" ({IPv6Hex16Bit} ":"){3} {IPv6LeastSignificant32Bits}
-            | (({IPv6Hex16Bit} ":"){0,2} {IPv6Hex16Bit})? "::" ({IPv6Hex16Bit} ":"){2} {IPv6LeastSignificant32Bits}
-            | (({IPv6Hex16Bit} ":"){0,3} {IPv6Hex16Bit})? "::"  {IPv6Hex16Bit} ":"     {IPv6LeastSignificant32Bits}
-            | (({IPv6Hex16Bit} ":"){0,4} {IPv6Hex16Bit})? "::"                         {IPv6LeastSignificant32Bits}
-            | (({IPv6Hex16Bit} ":"){0,5} {IPv6Hex16Bit})? "::"                         {IPv6Hex16Bit}
-            | (({IPv6Hex16Bit} ":"){0,6} {IPv6Hex16Bit})? "::"
-
-URIunreserved = [-._~A-Za-z0-9]
-URIpercentEncoded = "%" [0-9A-Fa-f]{2}
-URIsubDelims = [!$&'()*+,;=]
-URIloginSegment = ({URIunreserved} | {URIpercentEncoded} | {URIsubDelims})*
-URIlogin = {URIloginSegment} (":" {URIloginSegment})? "@"
-URIquery    = "?" ({URIunreserved} | {URIpercentEncoded} | {URIsubDelims} | [:@/?])*
-URIfragment = "#" ({URIunreserved} | {URIpercentEncoded} | {URIsubDelims} | [:@/?])*
-URIport = ":" [0-9]{1,5}
-URIhostStrict = ("[" {IPv6Address} "]") | {IPv4Address} | {DomainNameStrict}  
-URIhostLoose  = ("[" {IPv6Address} "]") | {IPv4Address} | {DomainNameLoose} 
-
-URIauthorityStrict =             {URIhostStrict} {URIport}?
-URIauthorityLoose  = {URIlogin}? {URIhostLoose}  {URIport}?
-
-HTTPsegment = ({URIunreserved} | {URIpercentEncoded} | [;:@&=])*
-HTTPpath = ("/" {HTTPsegment})*
-HTTPscheme = [hH][tT][tT][pP][sS]? "://"
-HTTPurlFull = {HTTPscheme} {URIauthorityLoose}  {HTTPpath}? {URIquery}? {URIfragment}?
-// {HTTPurlNoScheme} excludes {URIlogin}, because it could otherwise accept e-mail addresses
-HTTPurlNoScheme =          {URIauthorityStrict} {HTTPpath}? {URIquery}? {URIfragment}?
-HTTPurl = {HTTPurlFull} | {HTTPurlNoScheme}
-
-FTPorFILEsegment = ({URIunreserved} | {URIpercentEncoded} | [?:@&=])*
-FTPorFILEpath = "/" {FTPorFILEsegment} ("/" {FTPorFILEsegment})*
-FTPtype = ";" [tT][yY][pP][eE] "=" [aAiIdD]
-FTPscheme = [fF][tT][pP] "://"
-FTPurl = {FTPscheme} {URIauthorityLoose} {FTPorFILEpath} {FTPtype}? {URIfragment}?
-
-FILEscheme = [fF][iI][lL][eE] "://"
-FILEurl = {FILEscheme} {URIhostLoose}? {FTPorFILEpath} {URIfragment}?
-
-URL = {HTTPurl} | {FTPurl} | {FILEurl}
-
-EMAILquotedString = [\"] ([\u0001-\u0008\u000B\u000C\u000E-\u0021\u0023-\u005B\u005D-\u007E] | [\\] [\u0000-\u007F])* [\"]
-EMAILatomText = [A-Za-z0-9!#$%&'*+-/=?\^_`{|}~]
-EMAILlabel = {EMAILatomText}+ | {EMAILquotedString}
-EMAILlocalPart = {EMAILlabel} ("." {EMAILlabel})*
-EMAILdomainLiteralText = [\u0001-\u0008\u000B\u000C\u000E-\u005A\u005E-\u007F] | [\\] [\u0000-\u007F]
-// DFA minimization allows {IPv6Address} and {IPv4Address} to be included 
-// in the {EMAILbracketedHost} definition without incurring any size penalties, 
-// since {EMAILdomainLiteralText} recognizes all valid IP addresses.
-// The IP address regexes are included in {EMAILbracketedHost} simply as a 
-// reminder that they are acceptable bracketed host forms.
-EMAILbracketedHost = "[" ({EMAILdomainLiteralText}* | {IPv4Address} | [iI][pP][vV] "6:" {IPv6Address}) "]"
-EMAIL = {EMAILlocalPart} "@" ({DomainNameStrict} | {EMAILbracketedHost})
-
-
-%{
-  /** Alphanumeric sequences */
-  public static final int WORD_TYPE = UAX29URLEmailTokenizer.ALPHANUM;
-  
-  /** Numbers */
-  public static final int NUMERIC_TYPE = UAX29URLEmailTokenizer.NUM;
-  
-  /**
-   * Chars in class \p{Line_Break = Complex_Context} are from South East Asian
-   * scripts (Thai, Lao, Myanmar, Khmer, etc.).  Sequences of these are kept 
-   * together as as a single token rather than broken up, because the logic
-   * required to break them at word boundaries is too complex for UAX#29.
-   * <p>
-   * See Unicode Line Breaking Algorithm: http://www.unicode.org/reports/tr14/#SA
-   */
-  public static final int SOUTH_EAST_ASIAN_TYPE = UAX29URLEmailTokenizer.SOUTHEAST_ASIAN;
-  
-  public static final int IDEOGRAPHIC_TYPE = UAX29URLEmailTokenizer.IDEOGRAPHIC;
-  
-  public static final int HIRAGANA_TYPE = UAX29URLEmailTokenizer.HIRAGANA;
-  
-  public static final int KATAKANA_TYPE = UAX29URLEmailTokenizer.KATAKANA;
-  
-  public static final int HANGUL_TYPE = UAX29URLEmailTokenizer.HANGUL;
-  
-  public static final int EMAIL_TYPE = UAX29URLEmailTokenizer.EMAIL;
-  
-  public static final int URL_TYPE = UAX29URLEmailTokenizer.URL;
-
-  public final int yychar()
-  {
-    return yychar;
-  }
-
-  /**
-   * Fills CharTermAttribute with the current token text.
-   */
-  public final void getText(CharTermAttribute t) {
-    t.copyBuffer(zzBuffer, zzStartRead, zzMarkedPos-zzStartRead);
-  }
-%}
-
-%%
-
-// UAX#29 WB1. 	sot 	Ã· 	
-//        WB2. 		Ã· 	eot
-//
-<<EOF>> { return StandardTokenizerInterface.YYEOF; }
-
-{URL}   { return URL_TYPE; }
-{EMAIL} { return EMAIL_TYPE; }
-
-// UAX#29 WB8.   Numeric ? Numeric
-//        WB11.  Numeric (MidNum | MidNumLet) ? Numeric
-//        WB12.  Numeric ? (MidNum | MidNumLet) Numeric
-//        WB13a. (ALetter | Numeric | Katakana | ExtendNumLet) ? ExtendNumLet
-//        WB13b. ExtendNumLet ? (ALetter | Numeric | Katakana)
-//
-{ExtendNumLetEx}* {NumericEx} ({ExtendNumLetEx}+ {NumericEx} 
-                              | {MidNumericEx} {NumericEx} 
-                              | {NumericEx})*
-{ExtendNumLetEx}* 
-  { return NUMERIC_TYPE; }
-
-// subset of the below for typing purposes only!
-{HangulEx}+
-  { return HANGUL_TYPE; }
-
-{KatakanaEx}+
-  { return KATAKANA_TYPE; }
-
-// UAX#29 WB5.   ALetter ? ALetter
-//        WB6.   ALetter ? (MidLetter | MidNumLet) ALetter
-//        WB7.   ALetter (MidLetter | MidNumLet) ? ALetter
-//        WB9.   ALetter ? Numeric
-//        WB10.  Numeric ? ALetter
-//        WB13.  Katakana ? Katakana
-//        WB13a. (ALetter | Numeric | Katakana | ExtendNumLet) ? ExtendNumLet
-//        WB13b. ExtendNumLet ? (ALetter | Numeric | Katakana)
-//
-{ExtendNumLetEx}*  ( {KatakanaEx} ({ExtendNumLetEx}* {KatakanaEx})* 
-                   | ( {NumericEx}  ({ExtendNumLetEx}+ {NumericEx} | {MidNumericEx} {NumericEx} | {NumericEx})*
-                     | {ALetterEx}  ({ExtendNumLetEx}+ {ALetterEx} | {MidLetterEx}  {ALetterEx} | {ALetterEx})* )+ ) 
-({ExtendNumLetEx}+ ( {KatakanaEx} ({ExtendNumLetEx}* {KatakanaEx})* 
-                   | ( {NumericEx}  ({ExtendNumLetEx}+ {NumericEx} | {MidNumericEx} {NumericEx} | {NumericEx})*
-                     | {ALetterEx}  ({ExtendNumLetEx}+ {ALetterEx} | {MidLetterEx}  {ALetterEx} | {ALetterEx})* )+ ) )*
-{ExtendNumLetEx}*  
-  { return WORD_TYPE; }
-
-
-// From UAX #29:
-//
-//    [C]haracters with the Line_Break property values of Contingent_Break (CB), 
-//    Complex_Context (SA/South East Asian), and XX (Unknown) are assigned word 
-//    boundary property values based on criteria outside of the scope of this
-//    annex.  That means that satisfactory treatment of languages like Chinese
-//    or Thai requires special handling.
-// 
-// In Unicode 6.0, only one character has the \p{Line_Break = Contingent_Break}
-// property: U+FFFC ( ï¿? ) OBJECT REPLACEMENT CHARACTER.
-//
-// In the ICU implementation of UAX#29, \p{Line_Break = Complex_Context}
-// character sequences (from South East Asian scripts like Thai, Myanmar, Khmer,
-// Lao, etc.) are kept together.  This grammar does the same below.
-//
-// See also the Unicode Line Breaking Algorithm:
-//
-//    http://www.unicode.org/reports/tr14/#SA
-//
-{ComplexContext}+ { return SOUTH_EAST_ASIAN_TYPE; }
-
-// UAX#29 WB14.  Any Ã· Any
-//
-{HanEx} { return IDEOGRAPHIC_TYPE; }
-{HiraganaEx} { return HIRAGANA_TYPE; }
-
-
-// UAX#29 WB3.   CR ? LF
-//        WB3a.  (Newline | CR | LF) Ã·
-//        WB3b.  Ã· (Newline | CR | LF)
-//        WB14.  Any Ã· Any
-//
-[^] { /* Break so we don't hit fall-through warning: */ break;/* Not numeric, word, ideographic, hiragana, or SE Asian -- ignore it. */ }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/package.html b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/package.html
deleted file mode 100644
index f34c995..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/package.html
+++ /dev/null
@@ -1,22 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html><head></head>
-<body>
-Backwards-compatible implementation to match {@link org.apache.lucene.util.Version#LUCENE_34}
-</body>
-</html>
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiAnalyzer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiAnalyzer.java
index 7dd505c..a28263b 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiAnalyzer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiAnalyzer.java
@@ -33,13 +33,6 @@ import org.apache.lucene.util.Version;
 
 /**
  * {@link Analyzer} for Thai language. It uses {@link java.text.BreakIterator} to break words.
- * <p>
- * <a name="version"/>
- * <p>You must specify the required {@link Version}
- * compatibility when creating ThaiAnalyzer:
- * <ul>
- *   <li> As of 3.6, a set of Thai stopwords is used by default
- * </ul>
  */
 public final class ThaiAnalyzer extends StopwordAnalyzerBase {
   
@@ -84,7 +77,7 @@ public final class ThaiAnalyzer extends StopwordAnalyzerBase {
    * @param matchVersion lucene compatibility version
    */
   public ThaiAnalyzer(Version matchVersion) {
-    this(matchVersion, matchVersion.onOrAfter(Version.LUCENE_36) ? DefaultSetHolder.DEFAULT_STOP_SET : StopAnalyzer.ENGLISH_STOP_WORDS_SET);
+    this(matchVersion, DefaultSetHolder.DEFAULT_STOP_SET);
   }
   
   /**
@@ -112,8 +105,7 @@ public final class ThaiAnalyzer extends StopwordAnalyzerBase {
       Reader reader) {
     final Tokenizer source = new StandardTokenizer(matchVersion, reader);
     TokenStream result = new StandardFilter(matchVersion, source);
-    if (matchVersion.onOrAfter(Version.LUCENE_31))
-      result = new LowerCaseFilter(matchVersion, result);
+    result = new LowerCaseFilter(matchVersion, result);
     result = new ThaiWordFilter(matchVersion, result);
     return new TokenStreamComponents(source, new StopFilter(matchVersion,
         result, stopwords));
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiWordFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiWordFilter.java
index b2bc64f..1705b23 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiWordFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiWordFilter.java
@@ -23,7 +23,6 @@ import java.util.Locale;
 
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.core.LowerCaseFilter;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
@@ -34,10 +33,6 @@ import org.apache.lucene.util.Version;
 /**
  * {@link TokenFilter} that use {@link java.text.BreakIterator} to break each 
  * Token that is Thai into separate Token(s) for each Thai word.
- * <p>Please note: Since matchVersion 3.1 on, this filter no longer lowercases non-thai text.
- * {@link ThaiAnalyzer} will insert a {@link LowerCaseFilter} before this filter
- * so the behaviour of the Analyzer does not change. With version 3.1, the filter handles
- * position increments correctly.
  * <p>WARNING: this filter may not be supported by all JREs.
  *    It is known to work with Sun/Oracle and Harmony JREs.
  *    If your application needs to be fully portable, consider using ICUTokenizer instead,
@@ -58,8 +53,6 @@ public final class ThaiWordFilter extends TokenFilter {
   private final BreakIterator breaker = (BreakIterator) proto.clone();
   private final CharArrayIterator charIterator = CharArrayIterator.newWordInstance();
   
-  private final boolean handlePosIncr;
-  
   private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
   private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
   private final PositionIncrementAttribute posAtt = addAttribute(PositionIncrementAttribute.class);
@@ -72,11 +65,9 @@ public final class ThaiWordFilter extends TokenFilter {
 
   /** Creates a new ThaiWordFilter with the specified match version. */
   public ThaiWordFilter(Version matchVersion, TokenStream input) {
-    super(matchVersion.onOrAfter(Version.LUCENE_31) ?
-      input : new LowerCaseFilter(matchVersion, input));
+    super(input);
     if (!DBBI_AVAILABLE)
       throw new UnsupportedOperationException("This JRE does not have support for Thai segmentation");
-    handlePosIncr = matchVersion.onOrAfter(Version.LUCENE_31);
   }
   
   @Override
@@ -92,7 +83,7 @@ public final class ThaiWordFilter extends TokenFilter {
         } else {
           offsetAtt.setOffset(clonedOffsetAtt.startOffset() + start, clonedOffsetAtt.startOffset() + end);
         }
-        if (handlePosIncr) posAtt.setPositionIncrement(1);
+        posAtt.setPositionIncrement(1);
         return true;
       }
       hasMoreTokensInClone = false;
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharTokenizer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharTokenizer.java
index f1899ff..d05dc2c 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharTokenizer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharTokenizer.java
@@ -30,40 +30,6 @@ import org.apache.lucene.analysis.util.CharacterUtils.CharacterBuffer;
 
 /**
  * An abstract base class for simple, character-oriented tokenizers. 
- * <p>
- * <a name="version">You must specify the required {@link Version} compatibility
- * when creating {@link CharTokenizer}:
- * <ul>
- * <li>As of 3.1, {@link CharTokenizer} uses an int based API to normalize and
- * detect token codepoints. See {@link #isTokenChar(int)} and
- * {@link #normalize(int)} for details.</li>
- * </ul>
- * <p>
- * A new {@link CharTokenizer} API has been introduced with Lucene 3.1. This API
- * moved from UTF-16 code units to UTF-32 codepoints to eventually add support
- * for <a href=
- * "http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Character.html#supplementary"
- * >supplementary characters</a>. The old <i>char</i> based API has been
- * deprecated and should be replaced with the <i>int</i> based methods
- * {@link #isTokenChar(int)} and {@link #normalize(int)}.
- * </p>
- * <p>
- * As of Lucene 3.1 each {@link CharTokenizer} - constructor expects a
- * {@link Version} argument. Based on the given {@link Version} either the new
- * API or a backwards compatibility layer is used at runtime. For
- * {@link Version} < 3.1 the backwards compatibility layer ensures correct
- * behavior even for indexes build with previous versions of Lucene. If a
- * {@link Version} >= 3.1 is used {@link CharTokenizer} requires the new API to
- * be implemented by the instantiated class. Yet, the old <i>char</i> based API
- * is not required anymore even if backwards compatibility must be preserved.
- * {@link CharTokenizer} subclasses implementing the new API are fully backwards
- * compatible if instantiated with {@link Version} < 3.1.
- * </p>
- * <p>
- * <strong>Note:</strong> If you use a subclass of {@link CharTokenizer} with {@link Version} >=
- * 3.1 on an index build with a version < 3.1, created tokens might not be
- * compatible with the terms in your index.
- * </p>
  **/
 public abstract class CharTokenizer extends Tokenizer {
   
@@ -71,7 +37,7 @@ public abstract class CharTokenizer extends Tokenizer {
    * Creates a new {@link CharTokenizer} instance
    * 
    * @param matchVersion
-   *          Lucene version to match See {@link <a href="#version">above</a>}
+   *          Lucene version to match
    * @param input
    *          the input to split up into tokens
    */
@@ -84,7 +50,7 @@ public abstract class CharTokenizer extends Tokenizer {
    * Creates a new {@link CharTokenizer} instance
    * 
    * @param matchVersion
-   *          Lucene version to match See {@link <a href="#version">above</a>}
+   *          Lucene version to match
    * @param source
    *          the attribute source to use for this {@link Tokenizer}
    * @param input
@@ -100,7 +66,7 @@ public abstract class CharTokenizer extends Tokenizer {
    * Creates a new {@link CharTokenizer} instance
    * 
    * @param matchVersion
-   *          Lucene version to match See {@link <a href="#version">above</a>}
+   *          Lucene version to match
    * @param factory
    *          the attribute factory to use for this {@link Tokenizer}
    * @param input
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharacterUtils.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharacterUtils.java
index 55dcf97..fbdada2 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharacterUtils.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharacterUtils.java
@@ -43,7 +43,12 @@ public abstract class CharacterUtils {
    *         {@link Version} instance.
    */
   public static CharacterUtils getInstance(final Version matchVersion) {
-    return matchVersion.onOrAfter(Version.LUCENE_31) ? JAVA_5 : JAVA_4;
+    return JAVA_5;
+  }
+  
+  /** explicitly returns a version matching java 4 semantics */
+  public static CharacterUtils getJava4Instance() {
+    return JAVA_4;
   }
 
   /**
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/util/StopwordAnalyzerBase.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/util/StopwordAnalyzerBase.java
index e371bee..6f0b023 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/util/StopwordAnalyzerBase.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/util/StopwordAnalyzerBase.java
@@ -98,7 +98,7 @@ public abstract class StopwordAnalyzerBase extends Analyzer {
     Reader reader = null;
     try {
       reader = IOUtils.getDecodingReader(aClass.getResourceAsStream(resource), IOUtils.CHARSET_UTF_8);
-      return WordlistLoader.getWordSet(reader, comment, new CharArraySet(Version.LUCENE_31, 16, ignoreCase));
+      return WordlistLoader.getWordSet(reader, comment, new CharArraySet(Version.LUCENE_CURRENT, 16, ignoreCase));
     } finally {
       IOUtils.close(reader);
     }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/collation/CollationKeyAnalyzer.java b/lucene/analysis/common/src/java/org/apache/lucene/collation/CollationKeyAnalyzer.java
index 948847e..ee00c29 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/collation/CollationKeyAnalyzer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/collation/CollationKeyAnalyzer.java
@@ -20,7 +20,6 @@ package org.apache.lucene.collation;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.core.KeywordTokenizer;
-import org.apache.lucene.util.IndexableBinaryStringTools; // javadoc @link
 import org.apache.lucene.util.Version;
 
 import java.text.Collator;
@@ -28,12 +27,11 @@ import java.io.Reader;
 
 /**
  * <p>
- *   Filters {@link KeywordTokenizer} with {@link CollationKeyFilter}.
+ *   Configures {@link KeywordTokenizer} with {@link CollationAttributeFactory}.
  * </p>
  * <p>
  *   Converts the token into its {@link java.text.CollationKey}, and then
- *   encodes the CollationKey either directly or with 
- *   {@link IndexableBinaryStringTools} (see <a href="#version">below</a>), to allow 
+ *   encodes the CollationKey directly to allow 
  *   it to be stored as an index term.
  * </p>
  * <p>
@@ -74,49 +72,24 @@ import java.io.Reader;
  *   CollationKeyAnalyzer to generate index terms, do not use
  *   ICUCollationKeyAnalyzer on the query side, or vice versa.
  * </p>
- * <a name="version"/>
- * <p>You must specify the required {@link Version}
- * compatibility when creating CollationKeyAnalyzer:
- * <ul>
- *   <li> As of 4.0, Collation Keys are directly encoded as bytes. Previous
- *   versions will encode the bytes with {@link IndexableBinaryStringTools}.
- * </ul>
  */
 public final class CollationKeyAnalyzer extends Analyzer {
-  private final Collator collator;
   private final CollationAttributeFactory factory;
-  private final Version matchVersion;
   
   /**
    * Create a new CollationKeyAnalyzer, using the specified collator.
    * 
-   * @param matchVersion See <a href="#version">above</a>
+   * @param matchVersion compatibility version
    * @param collator CollationKey generator
    */
   public CollationKeyAnalyzer(Version matchVersion, Collator collator) {
-    this.matchVersion = matchVersion;
-    this.collator = collator;
     this.factory = new CollationAttributeFactory(collator);
   }
-  
-  /**
-   * @deprecated Use {@link CollationKeyAnalyzer#CollationKeyAnalyzer(Version, Collator)}
-   *   and specify a version instead. This ctor will be removed in Lucene 5.0
-   */
-  @Deprecated
-  public CollationKeyAnalyzer(Collator collator) {
-    this(Version.LUCENE_31, collator);
-  }
 
   @Override
   protected TokenStreamComponents createComponents(String fieldName,
       Reader reader) {
-    if (matchVersion.onOrAfter(Version.LUCENE_40)) {
-      KeywordTokenizer tokenizer = new KeywordTokenizer(factory, reader, KeywordTokenizer.DEFAULT_BUFFER_SIZE);
-      return new TokenStreamComponents(tokenizer, tokenizer);
-    } else {
-      KeywordTokenizer tokenizer = new KeywordTokenizer(reader);
-      return new TokenStreamComponents(tokenizer, new CollationKeyFilter(tokenizer, collator));
-    }
+    KeywordTokenizer tokenizer = new KeywordTokenizer(factory, reader, KeywordTokenizer.DEFAULT_BUFFER_SIZE);
+    return new TokenStreamComponents(tokenizer, tokenizer);
   }
 }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/collation/CollationKeyFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/collation/CollationKeyFilter.java
deleted file mode 100644
index 05b10ac..0000000
--- a/lucene/analysis/common/src/java/org/apache/lucene/collation/CollationKeyFilter.java
+++ /dev/null
@@ -1,108 +0,0 @@
-package org.apache.lucene.collation;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-import org.apache.lucene.analysis.TokenFilter;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.util.IndexableBinaryStringTools;
-
-import java.io.IOException;
-import java.text.Collator;
-
-
-/**
- * <p>
- *   Converts each token into its {@link java.text.CollationKey}, and then
- *   encodes the CollationKey with {@link IndexableBinaryStringTools}, to allow 
- *   it to be stored as an index term.
- * </p>
- * <p>
- *   <strong>WARNING:</strong> Make sure you use exactly the same Collator at
- *   index and query time -- CollationKeys are only comparable when produced by
- *   the same Collator.  Since {@link java.text.RuleBasedCollator}s are not
- *   independently versioned, it is unsafe to search against stored
- *   CollationKeys unless the following are exactly the same (best practice is
- *   to store this information with the index and check that they remain the
- *   same at query time):
- * </p>
- * <ol>
- *   <li>JVM vendor</li>
- *   <li>JVM version, including patch version</li>
- *   <li>
- *     The language (and country and variant, if specified) of the Locale
- *     used when constructing the collator via
- *     {@link Collator#getInstance(java.util.Locale)}.
- *   </li>
- *   <li>
- *     The collation strength used - see {@link Collator#setStrength(int)}
- *   </li>
- * </ol> 
- * <p>
- *   The <code>ICUCollationKeyFilter</code> in the analysis-icu package 
- *   uses ICU4J's Collator, which makes its
- *   version available, thus allowing collation to be versioned independently
- *   from the JVM.  ICUCollationKeyFilter is also significantly faster and
- *   generates significantly shorter keys than CollationKeyFilter.  See
- *   <a href="http://site.icu-project.org/charts/collation-icu4j-sun"
- *   >http://site.icu-project.org/charts/collation-icu4j-sun</a> for key
- *   generation timing and key length comparisons between ICU4J and
- *   java.text.Collator over several languages.
- * </p>
- * <p>
- *   CollationKeys generated by java.text.Collators are not compatible
- *   with those those generated by ICU Collators.  Specifically, if you use 
- *   CollationKeyFilter to generate index terms, do not use
- *   ICUCollationKeyFilter on the query side, or vice versa.
- * </p>
- * @deprecated Use {@link CollationAttributeFactory} instead, which encodes
- *  terms directly as bytes. This filter will be removed in Lucene 5.0
- */
-@Deprecated
-public final class CollationKeyFilter extends TokenFilter {
-  private final Collator collator;
-  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-
-  /**
-   * @param input Source token stream
-   * @param collator CollationKey generator
-   */
-  public CollationKeyFilter(TokenStream input, Collator collator) {
-    super(input);
-    // clone in case JRE doesnt properly sync,
-    // or to reduce contention in case they do
-    this.collator = (Collator) collator.clone();
-  }
-
-  @Override
-  public boolean incrementToken() throws IOException {
-    if (input.incrementToken()) {
-      byte[] collationKey = collator.getCollationKey(termAtt.toString()).toByteArray();
-      int encodedLength = IndexableBinaryStringTools.getEncodedLength(
-          collationKey, 0, collationKey.length);
-      termAtt.resizeBuffer(encodedLength);
-      termAtt.setLength(encodedLength);
-      IndexableBinaryStringTools.encode(collationKey, 0, collationKey.length,
-          termAtt.buffer(), 0, encodedLength);
-      return true;
-    } else {
-      return false;
-    }
-  }
-}
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/ar/TestArabicLetterTokenizer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/ar/TestArabicLetterTokenizer.java
deleted file mode 100644
index cde265f..0000000
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/ar/TestArabicLetterTokenizer.java
+++ /dev/null
@@ -1,47 +0,0 @@
-package org.apache.lucene.analysis.ar;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.StringReader;
-
-import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.util.Version;
-
-/**
- * Testcase for {@link TestArabicLetterTokenizer}
- * @deprecated (3.1) Remove in Lucene 5.0
- */
-@Deprecated
-public class TestArabicLetterTokenizer extends BaseTokenStreamTestCase {
-  
-  public void testArabicLetterTokenizer() throws IOException {
-    StringReader reader = new StringReader("1234567890 Tokenizer \ud801\udc1c\u0300test");
-    ArabicLetterTokenizer tokenizer = new ArabicLetterTokenizer(Version.LUCENE_31,
-        reader);
-    assertTokenStreamContents(tokenizer, new String[] {"Tokenizer",
-        "\ud801\udc1c\u0300test"});
-  }
-  
-  public void testArabicLetterTokenizerBWCompat() throws IOException {
-    StringReader reader = new StringReader("1234567890 Tokenizer \ud801\udc1c\u0300test");
-    ArabicLetterTokenizer tokenizer = new ArabicLetterTokenizer(Version.LUCENE_30,
-        reader);
-    assertTokenStreamContents(tokenizer, new String[] {"Tokenizer", "\u0300test"});
-  }
-}
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/ar/TestArabicNormalizationFilter.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/ar/TestArabicNormalizationFilter.java
index 18b2465..454eed1 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/ar/TestArabicNormalizationFilter.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/ar/TestArabicNormalizationFilter.java
@@ -23,6 +23,7 @@ import java.io.StringReader;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.core.KeywordTokenizer;
 
@@ -88,7 +89,7 @@ public class TestArabicNormalizationFilter extends BaseTokenStreamTestCase {
   }  
   
   private void check(final String input, final String expected) throws IOException {
-    ArabicLetterTokenizer tokenStream = new ArabicLetterTokenizer(TEST_VERSION_CURRENT, new StringReader(input));
+    MockTokenizer tokenStream = new MockTokenizer(new StringReader(input), MockTokenizer.WHITESPACE, false);
     ArabicNormalizationFilter filter = new ArabicNormalizationFilter(tokenStream);
     assertTokenStreamContents(filter, new String[]{expected});
   }
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/ar/TestArabicStemFilter.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/ar/TestArabicStemFilter.java
index a87ab77..46afb86 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/ar/TestArabicStemFilter.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/ar/TestArabicStemFilter.java
@@ -23,6 +23,7 @@ import java.io.StringReader;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.core.KeywordTokenizer;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
@@ -121,14 +122,14 @@ public class TestArabicStemFilter extends BaseTokenStreamTestCase {
   public void testWithKeywordAttribute() throws IOException {
     CharArraySet set = new CharArraySet(TEST_VERSION_CURRENT, 1, true);
     set.add("Ø³Ø§?Ø¯?Ø§Øª");
-    ArabicLetterTokenizer tokenStream  = new ArabicLetterTokenizer(TEST_VERSION_CURRENT, new StringReader("Ø³Ø§?Ø¯?Ø§Øª"));
+    MockTokenizer tokenStream  = new MockTokenizer(new StringReader("Ø³Ø§?Ø¯?Ø§Øª"), MockTokenizer.WHITESPACE, false);
 
     ArabicStemFilter filter = new ArabicStemFilter(new KeywordMarkerFilter(tokenStream, set));
     assertTokenStreamContents(filter, new String[]{"Ø³Ø§?Ø¯?Ø§Øª"});
   }
 
   private void check(final String input, final String expected) throws IOException {
-    ArabicLetterTokenizer tokenStream  = new ArabicLetterTokenizer(TEST_VERSION_CURRENT, new StringReader(input));
+    MockTokenizer tokenStream  = new MockTokenizer(new StringReader(input), MockTokenizer.WHITESPACE, false);
     ArabicStemFilter filter = new ArabicStemFilter(tokenStream);
     assertTokenStreamContents(filter, new String[]{expected});
   }
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/bg/TestBulgarianAnalyzer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/bg/TestBulgarianAnalyzer.java
index 85df41d..e0f3cd9 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/bg/TestBulgarianAnalyzer.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/bg/TestBulgarianAnalyzer.java
@@ -68,7 +68,7 @@ public class TestBulgarianAnalyzer extends BaseTokenStreamTestCase {
   }
   
   public void testWithStemExclusionSet() throws IOException {
-    CharArraySet set = new CharArraySet(Version.LUCENE_31, 1, true);
+    CharArraySet set = new CharArraySet(TEST_VERSION_CURRENT, 1, true);
     set.add("???Ð¾ÐµÐ²Ðµ");
     Analyzer a = new BulgarianAnalyzer(TEST_VERSION_CURRENT, CharArraySet.EMPTY_SET, set);
     assertAnalyzesTo(a, "???Ð¾ÐµÐ²Ðµ?Ðµ ???Ð¾ÐµÐ²Ðµ", new String[] { "???Ð¾Ð¹", "???Ð¾ÐµÐ²Ðµ" });
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/bg/TestBulgarianStemmer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/bg/TestBulgarianStemmer.java
index 21f256e..80e31a8 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/bg/TestBulgarianStemmer.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/bg/TestBulgarianStemmer.java
@@ -217,7 +217,7 @@ public class TestBulgarianStemmer extends BaseTokenStreamTestCase {
   }
 
   public void testWithKeywordAttribute() throws IOException {
-    CharArraySet set = new CharArraySet(Version.LUCENE_31, 1, true);
+    CharArraySet set = new CharArraySet(TEST_VERSION_CURRENT, 1, true);
     set.add("???Ð¾ÐµÐ²Ðµ");
     MockTokenizer tokenStream = new MockTokenizer(new StringReader("???Ð¾ÐµÐ²Ðµ?Ðµ ???Ð¾ÐµÐ²Ðµ"), MockTokenizer.WHITESPACE, false);
 
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer.java
deleted file mode 100644
index 6a0f0ab..0000000
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer.java
+++ /dev/null
@@ -1,281 +0,0 @@
-package org.apache.lucene.analysis.cjk;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.util.Version;
-
-/** @deprecated Remove when CJKTokenizer is removed (5.0) */
-@Deprecated
-public class TestCJKTokenizer extends BaseTokenStreamTestCase {
-  
-  class TestToken {
-    String termText;
-    int start;
-    int end;
-    String type;
-  }
-
-  public TestToken newToken(String termText, int start, int end, int type) {
-    TestToken token = new TestToken();
-    token.termText = termText;
-    token.type = CJKTokenizer.TOKEN_TYPE_NAMES[type];
-    token.start = start;
-    token.end = end;
-    return token;
-  }
-
-  public void checkCJKToken(final String str, final TestToken[] out_tokens) throws IOException {
-    Analyzer analyzer = new CJKAnalyzer(Version.LUCENE_30);
-    String terms[] = new String[out_tokens.length];
-    int startOffsets[] = new int[out_tokens.length];
-    int endOffsets[] = new int[out_tokens.length];
-    String types[] = new String[out_tokens.length];
-    for (int i = 0; i < out_tokens.length; i++) {
-      terms[i] = out_tokens[i].termText;
-      startOffsets[i] = out_tokens[i].start;
-      endOffsets[i] = out_tokens[i].end;
-      types[i] = out_tokens[i].type;
-    }
-    assertAnalyzesTo(analyzer, str, terms, startOffsets, endOffsets, types, null);
-  }
-  
-  public void checkCJKTokenReusable(final Analyzer a, final String str, final TestToken[] out_tokens) throws IOException {
-    Analyzer analyzer = new CJKAnalyzer(Version.LUCENE_30);
-    String terms[] = new String[out_tokens.length];
-    int startOffsets[] = new int[out_tokens.length];
-    int endOffsets[] = new int[out_tokens.length];
-    String types[] = new String[out_tokens.length];
-    for (int i = 0; i < out_tokens.length; i++) {
-      terms[i] = out_tokens[i].termText;
-      startOffsets[i] = out_tokens[i].start;
-      endOffsets[i] = out_tokens[i].end;
-      types[i] = out_tokens[i].type;
-    }
-    assertAnalyzesToReuse(analyzer, str, terms, startOffsets, endOffsets, types, null);
-  }
-  
-  public void testJa1() throws IOException {
-    String str = "\u4e00\u4e8c\u4e09\u56db\u4e94\u516d\u4e03\u516b\u4e5d\u5341";
-       
-    TestToken[] out_tokens = { 
-      newToken("\u4e00\u4e8c", 0, 2, CJKTokenizer.DOUBLE_TOKEN_TYPE), 
-      newToken("\u4e8c\u4e09", 1, 3, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-      newToken("\u4e09\u56db", 2, 4, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-      newToken("\u56db\u4e94", 3, 5, CJKTokenizer.DOUBLE_TOKEN_TYPE), 
-      newToken("\u4e94\u516d", 4, 6, CJKTokenizer.DOUBLE_TOKEN_TYPE), 
-      newToken("\u516d\u4e03", 5, 7, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-      newToken("\u4e03\u516b", 6, 8, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-      newToken("\u516b\u4e5d", 7, 9, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-      newToken("\u4e5d\u5341", 8,10, CJKTokenizer.DOUBLE_TOKEN_TYPE)
-    };
-    checkCJKToken(str, out_tokens);
-  }
-  
-  public void testJa2() throws IOException {
-    String str = "\u4e00 \u4e8c\u4e09\u56db \u4e94\u516d\u4e03\u516b\u4e5d \u5341";
-       
-    TestToken[] out_tokens = { 
-      newToken("\u4e00", 0, 1, CJKTokenizer.DOUBLE_TOKEN_TYPE), 
-      newToken("\u4e8c\u4e09", 2, 4, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-      newToken("\u4e09\u56db", 3, 5, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-      newToken("\u4e94\u516d", 6, 8, CJKTokenizer.DOUBLE_TOKEN_TYPE), 
-      newToken("\u516d\u4e03", 7, 9, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-      newToken("\u4e03\u516b", 8, 10, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-      newToken("\u516b\u4e5d", 9, 11, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-      newToken("\u5341", 12,13, CJKTokenizer.DOUBLE_TOKEN_TYPE)
-    };
-    checkCJKToken(str, out_tokens);
-  }
-  
-  public void testC() throws IOException {
-    String str = "abc defgh ijklmn opqrstu vwxy z";
-       
-    TestToken[] out_tokens = { 
-      newToken("abc", 0, 3, CJKTokenizer.SINGLE_TOKEN_TYPE), 
-      newToken("defgh", 4, 9, CJKTokenizer.SINGLE_TOKEN_TYPE),
-      newToken("ijklmn", 10, 16, CJKTokenizer.SINGLE_TOKEN_TYPE),
-      newToken("opqrstu", 17, 24, CJKTokenizer.SINGLE_TOKEN_TYPE), 
-      newToken("vwxy", 25, 29, CJKTokenizer.SINGLE_TOKEN_TYPE), 
-      newToken("z", 30, 31, CJKTokenizer.SINGLE_TOKEN_TYPE),
-    };
-    checkCJKToken(str, out_tokens);
-  }
-  
-  public void testMix() throws IOException {
-    String str = "\u3042\u3044\u3046\u3048\u304aabc\u304b\u304d\u304f\u3051\u3053";
-       
-    TestToken[] out_tokens = { 
-      newToken("\u3042\u3044", 0, 2, CJKTokenizer.DOUBLE_TOKEN_TYPE), 
-      newToken("\u3044\u3046", 1, 3, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-      newToken("\u3046\u3048", 2, 4, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-      newToken("\u3048\u304a", 3, 5, CJKTokenizer.DOUBLE_TOKEN_TYPE), 
-      newToken("abc", 5, 8, CJKTokenizer.SINGLE_TOKEN_TYPE), 
-      newToken("\u304b\u304d", 8, 10, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-      newToken("\u304d\u304f", 9, 11, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-      newToken("\u304f\u3051", 10,12, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-      newToken("\u3051\u3053", 11,13, CJKTokenizer.DOUBLE_TOKEN_TYPE)
-    };
-    checkCJKToken(str, out_tokens);
-  }
-  
-  public void testMix2() throws IOException {
-    String str = "\u3042\u3044\u3046\u3048\u304aab\u3093c\u304b\u304d\u304f\u3051 \u3053";
-       
-    TestToken[] out_tokens = { 
-      newToken("\u3042\u3044", 0, 2, CJKTokenizer.DOUBLE_TOKEN_TYPE), 
-      newToken("\u3044\u3046", 1, 3, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-      newToken("\u3046\u3048", 2, 4, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-      newToken("\u3048\u304a", 3, 5, CJKTokenizer.DOUBLE_TOKEN_TYPE), 
-      newToken("ab", 5, 7, CJKTokenizer.SINGLE_TOKEN_TYPE), 
-      newToken("\u3093", 7, 8, CJKTokenizer.DOUBLE_TOKEN_TYPE), 
-      newToken("c", 8, 9, CJKTokenizer.SINGLE_TOKEN_TYPE), 
-      newToken("\u304b\u304d", 9, 11, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-      newToken("\u304d\u304f", 10, 12, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-      newToken("\u304f\u3051", 11,13, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-      newToken("\u3053", 14,15, CJKTokenizer.DOUBLE_TOKEN_TYPE)
-    };
-    checkCJKToken(str, out_tokens);
-  }
-
-  public void testSingleChar() throws IOException {
-    String str = "\u4e00";
-       
-    TestToken[] out_tokens = { 
-      newToken("\u4e00", 0, 1, CJKTokenizer.DOUBLE_TOKEN_TYPE), 
-    };
-    checkCJKToken(str, out_tokens);
-  }
-  
-  /*
-   * Full-width text is normalized to half-width 
-   */
-  public void testFullWidth() throws Exception {
-    String str = "ï¼´ï?ï½?? ï¼??ï¼??";
-    TestToken[] out_tokens = { 
-        newToken("test", 0, 4, CJKTokenizer.SINGLE_TOKEN_TYPE), 
-        newToken("1234", 5, 9, CJKTokenizer.SINGLE_TOKEN_TYPE)
-    };
-    checkCJKToken(str, out_tokens);
-  }
-  
-  /*
-   * Non-english text (not just CJK) is treated the same as CJK: C1C2 C2C3 
-   */
-  public void testNonIdeographic() throws Exception {
-    String str = "\u4e00 Ø±?Ø¨Ø±Øª ???Ø±";
-    TestToken[] out_tokens = {
-        newToken("\u4e00", 0, 1, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-        newToken("Ø±?", 2, 4, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-        newToken("?Ø¨", 3, 5, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-        newToken("Ø¨Ø±", 4, 6, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-        newToken("Ø±Øª", 5, 7, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-        newToken("??", 8, 10, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-        newToken("??", 9, 11, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-        newToken("?Ø±", 10, 12, CJKTokenizer.DOUBLE_TOKEN_TYPE)
-    };
-    checkCJKToken(str, out_tokens);
-  }
-  
-  /*
-   * Non-english text with nonletters (non-spacing marks,etc) is treated as C1C2 C2C3,
-   * except for words are split around non-letters.
-   */
-  public void testNonIdeographicNonLetter() throws Exception {
-    String str = "\u4e00 Ø±??Ø¨Ø±Øª ???Ø±";
-    TestToken[] out_tokens = {
-        newToken("\u4e00", 0, 1, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-        newToken("Ø±", 2, 3, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-        newToken("?Ø¨", 4, 6, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-        newToken("Ø¨Ø±", 5, 7, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-        newToken("Ø±Øª", 6, 8, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-        newToken("??", 9, 11, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-        newToken("??", 10, 12, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-        newToken("?Ø±", 11, 13, CJKTokenizer.DOUBLE_TOKEN_TYPE)
-    };
-    checkCJKToken(str, out_tokens);
-  }
-  
-  public void testTokenStream() throws Exception {
-    Analyzer analyzer = new CJKAnalyzer(Version.LUCENE_30);
-    assertAnalyzesTo(analyzer, "\u4e00\u4e01\u4e02", 
-        new String[] { "\u4e00\u4e01", "\u4e01\u4e02"});
-  }
-  
-  public void testReusableTokenStream() throws Exception {
-    Analyzer analyzer = new CJKAnalyzer(Version.LUCENE_30);
-    String str = "\u3042\u3044\u3046\u3048\u304aabc\u304b\u304d\u304f\u3051\u3053";
-    
-    TestToken[] out_tokens = { 
-      newToken("\u3042\u3044", 0, 2, CJKTokenizer.DOUBLE_TOKEN_TYPE), 
-      newToken("\u3044\u3046", 1, 3, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-      newToken("\u3046\u3048", 2, 4, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-      newToken("\u3048\u304a", 3, 5, CJKTokenizer.DOUBLE_TOKEN_TYPE), 
-      newToken("abc", 5, 8, CJKTokenizer.SINGLE_TOKEN_TYPE), 
-      newToken("\u304b\u304d", 8, 10, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-      newToken("\u304d\u304f", 9, 11, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-      newToken("\u304f\u3051", 10,12, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-      newToken("\u3051\u3053", 11,13, CJKTokenizer.DOUBLE_TOKEN_TYPE)
-    };
-    checkCJKTokenReusable(analyzer, str, out_tokens);
-    
-    str = "\u3042\u3044\u3046\u3048\u304aab\u3093c\u304b\u304d\u304f\u3051 \u3053";
-    TestToken[] out_tokens2 = { 
-      newToken("\u3042\u3044", 0, 2, CJKTokenizer.DOUBLE_TOKEN_TYPE), 
-      newToken("\u3044\u3046", 1, 3, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-      newToken("\u3046\u3048", 2, 4, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-      newToken("\u3048\u304a", 3, 5, CJKTokenizer.DOUBLE_TOKEN_TYPE), 
-      newToken("ab", 5, 7, CJKTokenizer.SINGLE_TOKEN_TYPE), 
-      newToken("\u3093", 7, 8, CJKTokenizer.DOUBLE_TOKEN_TYPE), 
-      newToken("c", 8, 9, CJKTokenizer.SINGLE_TOKEN_TYPE), 
-      newToken("\u304b\u304d", 9, 11, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-      newToken("\u304d\u304f", 10, 12, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-      newToken("\u304f\u3051", 11,13, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-      newToken("\u3053", 14,15, CJKTokenizer.DOUBLE_TOKEN_TYPE)
-    };
-    checkCJKTokenReusable(analyzer, str, out_tokens2);
-  }
-  
-  /**
-   * LUCENE-2207: wrong offset calculated by end() 
-   */
-  public void testFinalOffset() throws IOException {
-    checkCJKToken("???", new TestToken[] { 
-        newToken("???", 0, 2, CJKTokenizer.DOUBLE_TOKEN_TYPE) });
-    checkCJKToken("???   ", new TestToken[] { 
-        newToken("???", 0, 2, CJKTokenizer.DOUBLE_TOKEN_TYPE) });
-    checkCJKToken("test", new TestToken[] { 
-        newToken("test", 0, 4, CJKTokenizer.SINGLE_TOKEN_TYPE) });
-    checkCJKToken("test   ", new TestToken[] { 
-        newToken("test", 0, 4, CJKTokenizer.SINGLE_TOKEN_TYPE) });
-    checkCJKToken("???test", new TestToken[] {
-        newToken("???", 0, 2, CJKTokenizer.DOUBLE_TOKEN_TYPE),
-        newToken("test", 2, 6, CJKTokenizer.SINGLE_TOKEN_TYPE) });
-    checkCJKToken("test???    ", new TestToken[] { 
-        newToken("test", 0, 4, CJKTokenizer.SINGLE_TOKEN_TYPE),
-        newToken("???", 4, 6, CJKTokenizer.DOUBLE_TOKEN_TYPE) });
-  }
-  
-  /** blast some random strings through the analyzer */
-  public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new CJKAnalyzer(Version.LUCENE_30), 10000*RANDOM_MULTIPLIER);
-  }
-}
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/cn/TestChineseTokenizer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/cn/TestChineseTokenizer.java
deleted file mode 100644
index f0a32c0..0000000
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/cn/TestChineseTokenizer.java
+++ /dev/null
@@ -1,126 +0,0 @@
-package org.apache.lucene.analysis.cn;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.Reader;
-import java.io.StringReader;
-
-import org.apache.lucene.analysis.*;
-import org.apache.lucene.analysis.core.WhitespaceTokenizer;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.util.Version;
-
-
-/** @deprecated Remove this test when ChineseAnalyzer is removed. */
-@Deprecated
-public class TestChineseTokenizer extends BaseTokenStreamTestCase
-{
-    public void testOtherLetterOffset() throws IOException
-    {
-        String s = "aå¤?";
-        ChineseTokenizer tokenizer = new ChineseTokenizer(new StringReader(s));
-
-        int correctStartOffset = 0;
-        int correctEndOffset = 1;
-        OffsetAttribute offsetAtt = tokenizer.getAttribute(OffsetAttribute.class);
-        while (tokenizer.incrementToken()) {
-          assertEquals(correctStartOffset, offsetAtt.startOffset());
-          assertEquals(correctEndOffset, offsetAtt.endOffset());
-          correctStartOffset++;
-          correctEndOffset++;
-        }
-    }
-    
-    public void testReusableTokenStream() throws Exception
-    {
-      Analyzer a = new ChineseAnalyzer();
-      assertAnalyzesToReuse(a, "ä¸??äººæ??±å???", 
-        new String[] { "ä¸?", "??", "äº?", "æ°?", "??", "??", "??" },
-        new int[] { 0, 1, 2, 3, 4, 5, 6 },
-        new int[] { 1, 2, 3, 4, 5, 6, 7 });
-      assertAnalyzesToReuse(a, "??º¬å¸?", 
-        new String[] { "??", "äº?", "å¸?" },
-        new int[] { 0, 1, 2 },
-        new int[] { 1, 2, 3 });
-    }
-    
-    /*
-     * Analyzer that just uses ChineseTokenizer, not ChineseFilter.
-     * convenience to show the behavior of the tokenizer
-     */
-    private class JustChineseTokenizerAnalyzer extends Analyzer {
-      @Override
-      public TokenStreamComponents createComponents(String fieldName, Reader reader) {
-        return new TokenStreamComponents(new ChineseTokenizer(reader));
-      }   
-    }
-    
-    /*
-     * Analyzer that just uses ChineseFilter, not ChineseTokenizer.
-     * convenience to show the behavior of the filter.
-     */
-    private class JustChineseFilterAnalyzer extends Analyzer {
-      @Override
-      public TokenStreamComponents createComponents(String fieldName, Reader reader) {
-        Tokenizer tokenizer = new WhitespaceTokenizer(Version.LUCENE_CURRENT, reader);
-        return new TokenStreamComponents(tokenizer, new ChineseFilter(tokenizer));
-      }
-    }
-    
-    /*
-     * ChineseTokenizer tokenizes numbers as one token, but they are filtered by ChineseFilter
-     */
-    public void testNumerics() throws Exception
-    { 
-      Analyzer justTokenizer = new JustChineseTokenizerAnalyzer();
-      assertAnalyzesTo(justTokenizer, "ä¸?1234", new String[] { "ä¸?", "1234" });
-          
-      // in this case the ChineseAnalyzer (which applies ChineseFilter) will remove the numeric token.
-      Analyzer a = new ChineseAnalyzer(); 
-      assertAnalyzesTo(a, "ä¸?1234", new String[] { "ä¸?" });
-    }
-    
-    /*
-     * ChineseTokenizer tokenizes english similar to SimpleAnalyzer.
-     * it will lowercase terms automatically.
-     * 
-     * ChineseFilter has an english stopword list, it also removes any single character tokens.
-     * the stopword list is case-sensitive.
-     */
-    public void testEnglish() throws Exception
-    {
-      Analyzer chinese = new ChineseAnalyzer();
-      assertAnalyzesTo(chinese, "This is a Test. b c d",
-          new String[] { "test" });
-      
-      Analyzer justTokenizer = new JustChineseTokenizerAnalyzer();
-      assertAnalyzesTo(justTokenizer, "This is a Test. b c d",
-          new String[] { "this", "is", "a", "test", "b", "c", "d" });
-      
-      Analyzer justFilter = new JustChineseFilterAnalyzer();
-      assertAnalyzesTo(justFilter, "This is a Test. b c d", 
-          new String[] { "This", "Test." });
-    }
-    
-    /** blast some random strings through the analyzer */
-    public void testRandomStrings() throws Exception {
-      checkRandomData(random(), new ChineseAnalyzer(), 10000*RANDOM_MULTIPLIER);
-    }
-
-}
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestAnalyzers.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestAnalyzers.java
index 39226b5..a50a753 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestAnalyzers.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestAnalyzers.java
@@ -27,7 +27,6 @@ import org.apache.lucene.analysis.standard.StandardTokenizer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
 import org.apache.lucene.index.Payload;
-import org.apache.lucene.util.Version;
 
 public class TestAnalyzers extends BaseTokenStreamTestCase {
 
@@ -182,15 +181,6 @@ public class TestAnalyzers extends BaseTokenStreamTestCase {
         "\ud801\udc44test" });
   }
 
-  /** @deprecated (3.1) */
-  @Deprecated
-  public void testLowerCaseTokenizerBWCompat() throws IOException {
-    StringReader reader = new StringReader("Tokenizer \ud801\udc1ctest");
-    LowerCaseTokenizer tokenizer = new LowerCaseTokenizer(Version.LUCENE_30,
-        reader);
-    assertTokenStreamContents(tokenizer, new String[] { "tokenizer", "test" });
-  }
-
   public void testWhitespaceTokenizer() throws IOException {
     StringReader reader = new StringReader("Tokenizer \ud801\udc1ctest");
     WhitespaceTokenizer tokenizer = new WhitespaceTokenizer(TEST_VERSION_CURRENT,
@@ -198,16 +188,6 @@ public class TestAnalyzers extends BaseTokenStreamTestCase {
     assertTokenStreamContents(tokenizer, new String[] { "Tokenizer",
         "\ud801\udc1ctest" });
   }
-
-  /** @deprecated (3.1) */
-  @Deprecated
-  public void testWhitespaceTokenizerBWCompat() throws IOException {
-    StringReader reader = new StringReader("Tokenizer \ud801\udc1ctest");
-    WhitespaceTokenizer tokenizer = new WhitespaceTokenizer(Version.LUCENE_30,
-        reader);
-    assertTokenStreamContents(tokenizer, new String[] { "Tokenizer",
-        "\ud801\udc1ctest" });
-  }
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer.java
index 3ab1f37..49e0d22 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer.java
@@ -5,8 +5,8 @@ import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.standard.ClassicAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -15,7 +15,6 @@ import org.apache.lucene.index.Term;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Version;
 
 import java.io.IOException;
 import java.util.Arrays;
@@ -137,7 +136,7 @@ public class TestClassicAnalyzer extends BaseTokenStreamTestCase {
 
     // 2.4 should not show the bug. But, alas, it's also obsolete,
     // so we check latest released (Robert's gonna break this on 4.0 soon :) )
-    a2 = new ClassicAnalyzer(Version.LUCENE_31);
+    a2 = new ClassicAnalyzer(TEST_VERSION_CURRENT);
     assertAnalyzesTo(a2, "www.nutch.org.", new String[]{ "www.nutch.org" }, new String[] { "<HOST>" });
   }
 
@@ -244,7 +243,7 @@ public class TestClassicAnalyzer extends BaseTokenStreamTestCase {
   }
 
   public void testJava14BWCompatibility() throws Exception {
-    ClassicAnalyzer sa = new ClassicAnalyzer(Version.LUCENE_30);
+    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);
     assertAnalyzesTo(sa, "test\u02C6test", new String[] { "test", "test" });
   }
 
@@ -272,7 +271,7 @@ public class TestClassicAnalyzer extends BaseTokenStreamTestCase {
     writer.addDocument(doc);
     writer.close();
 
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
 
     // Make sure all terms < max size were indexed
     assertEquals(2, reader.docFreq(new Term("content", "abc")));
@@ -306,7 +305,7 @@ public class TestClassicAnalyzer extends BaseTokenStreamTestCase {
     writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));
     writer.addDocument(doc);
     writer.close();
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
     assertEquals(1, reader.docFreq(new Term("content", bigTerm)));
     reader.close();
 
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestKeywordAnalyzer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestKeywordAnalyzer.java
index 0d1b36a..54c5fd4 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestKeywordAnalyzer.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestKeywordAnalyzer.java
@@ -26,6 +26,7 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
@@ -58,7 +59,7 @@ public class TestKeywordAnalyzer extends BaseTokenStreamTestCase {
 
     writer.close();
 
-    reader = IndexReader.open(directory);
+    reader = DirectoryReader.open(directory);
     searcher = new IndexSearcher(reader);
   }
   
@@ -95,7 +96,7 @@ public class TestKeywordAnalyzer extends BaseTokenStreamTestCase {
     writer.addDocument(doc);
     writer.close();
 
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     DocsEnum td = _TestUtil.docs(random(),
                                  reader,
                                  "partnum",
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestStandardAnalyzer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestStandardAnalyzer.java
index 3fe6e6f..71261ac 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestStandardAnalyzer.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestStandardAnalyzer.java
@@ -230,16 +230,6 @@ public class TestStandardAnalyzer extends BaseTokenStreamTestCase {
     checkOneTerm(a, "å£¹ã?", "å£¹ã?"); // ideographic
     checkOneTerm(a, "???",  "???"); // hangul
   }
-  
-  /** @deprecated remove this and sophisticated backwards layer in 5.0 */
-  @Deprecated
-  public void testCombiningMarksBackwards() throws Exception {
-    Analyzer a = new StandardAnalyzer(Version.LUCENE_33);
-    checkOneTerm(a, "???", "??"); // hiragana Bug
-    checkOneTerm(a, "?µã?", "?µã?"); // katakana Works
-    checkOneTerm(a, "å£¹ã?", "å£?"); // ideographic Bug
-    checkOneTerm(a, "???",  "???"); // hangul Works
-  }
 
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestUAX29URLEmailAnalyzer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestUAX29URLEmailAnalyzer.java
index 0b6ec54..57ce8e1 100755
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestUAX29URLEmailAnalyzer.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestUAX29URLEmailAnalyzer.java
@@ -209,16 +209,6 @@ public class TestUAX29URLEmailAnalyzer extends BaseTokenStreamTestCase {
     checkOneTerm(a, "å£¹ã?", "å£¹ã?"); // ideographic
     checkOneTerm(a, "???",  "???"); // hangul
   }
-  
-  /** @deprecated remove this and sophisticated backwards layer in 5.0 */
-  @Deprecated
-  public void testCombiningMarksBackwards() throws Exception {
-    Analyzer a = new UAX29URLEmailAnalyzer(Version.LUCENE_33);
-    checkOneTerm(a, "???", "??"); // hiragana Bug
-    checkOneTerm(a, "?µã?", "?µã?"); // katakana Works
-    checkOneTerm(a, "å£¹ã?", "å£?"); // ideographic Bug
-    checkOneTerm(a, "???",  "???"); // hangul Works
-  }
 
   public void testBasicEmails() throws Exception {
     BaseTokenStreamTestCase.assertAnalyzesTo(a,
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestUAX29URLEmailTokenizer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestUAX29URLEmailTokenizer.java
index 24f70df..8bead8e 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestUAX29URLEmailTokenizer.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestUAX29URLEmailTokenizer.java
@@ -453,39 +453,6 @@ public class TestUAX29URLEmailTokenizer extends BaseTokenStreamTestCase {
     checkOneTerm(a, "???",  "???"); // hangul
   }
 
-  /** @deprecated remove this and sophisticated backwards layer in 5.0 */
-  @Deprecated
-  public void testCombiningMarksBackwards() throws Exception {
-    Analyzer a = new Analyzer() {
-      @Override
-      protected TokenStreamComponents createComponents
-        (String fieldName, Reader reader) {
-
-        Tokenizer tokenizer = new UAX29URLEmailTokenizer(Version.LUCENE_31, reader);
-        return new TokenStreamComponents(tokenizer);
-      }
-    };
-    checkOneTerm(a, "???", "??"); // hiragana Bug
-    checkOneTerm(a, "?µã?", "?µã?"); // katakana Works
-    checkOneTerm(a, "å£¹ã?", "å£?"); // ideographic Bug
-    checkOneTerm(a, "???",  "???"); // hangul Works
-  }
-  
-  // LUCENE-3880
-  /** @deprecated remove this and sophisticated backwards layer in 5.0 */
-  @Deprecated
-  public void testMailtoBackwards()  throws Exception {
-    Analyzer a = new Analyzer() {
-      @Override
-      protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
-        Tokenizer tokenizer = new UAX29URLEmailTokenizer(Version.LUCENE_34, reader);
-        return new TokenStreamComponents(tokenizer);
-      }
-    };
-    assertAnalyzesTo(a, "mailto:test@example.org",
-        new String[] { "mailto:test", "example.org" });
-  }
-
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
     checkRandomData(random(), a, 10000*RANDOM_MULTIPLIER);
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/cz/TestCzechAnalyzer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/cz/TestCzechAnalyzer.java
index a81447c..8d2d1bc 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/cz/TestCzechAnalyzer.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/cz/TestCzechAnalyzer.java
@@ -31,30 +31,12 @@ import org.apache.lucene.util.Version;
  *
  */
 public class TestCzechAnalyzer extends BaseTokenStreamTestCase {
-  /**
-   * @deprecated (3.1) Remove this test when support for 3.0 indexes is no longer needed.
-   */
-  @Deprecated
-  public void testStopWordLegacy() throws Exception {
-    assertAnalyzesTo(new CzechAnalyzer(Version.LUCENE_30), "Pokud mluvime o volnem", 
-        new String[] { "mluvime", "volnem" });
-  }
   
   public void testStopWord() throws Exception {
     assertAnalyzesTo(new CzechAnalyzer(TEST_VERSION_CURRENT), "Pokud mluvime o volnem", 
         new String[] { "mluvim", "voln" });
   }
   
-  /**
-   * @deprecated (3.1) Remove this test when support for 3.0 indexes is no longer needed.
-   */
-  @Deprecated
-  public void testReusableTokenStreamLegacy() throws Exception {
-    Analyzer analyzer = new CzechAnalyzer(Version.LUCENE_30);
-    assertAnalyzesToReuse(analyzer, "Pokud mluvime o volnem", new String[] { "mluvime", "volnem" });
-    assertAnalyzesToReuse(analyzer, "?eskÃ¡ Republika", new String[] { "?eskÃ¡", "republika" });
-  }
-  
   public void testReusableTokenStream() throws Exception {
     Analyzer analyzer = new CzechAnalyzer(TEST_VERSION_CURRENT);
     assertAnalyzesToReuse(analyzer, "Pokud mluvime o volnem", new String[] { "mluvim", "voln" });
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanAnalyzer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanAnalyzer.java
index 86c7de4..7da253e 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanAnalyzer.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanAnalyzer.java
@@ -25,7 +25,6 @@ import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.core.LowerCaseTokenizer;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
 import org.apache.lucene.analysis.util.CharArraySet;
-import org.apache.lucene.util.Version;
 
 public class TestGermanAnalyzer extends BaseTokenStreamTestCase {
   public void testReusableTokenStream() throws Exception {
@@ -58,10 +57,6 @@ public class TestGermanAnalyzer extends BaseTokenStreamTestCase {
     // a/o/u + e is equivalent to the umlaut form
     checkOneTermReuse(a, "SchaltflÃ¤chen", "schaltflach");
     checkOneTermReuse(a, "Schaltflaechen", "schaltflach");
-    // here they are with the old stemmer
-    a = new GermanAnalyzer(Version.LUCENE_30);
-    checkOneTermReuse(a, "SchaltflÃ¤chen", "schaltflach");
-    checkOneTermReuse(a, "Schaltflaechen", "schaltflaech");
   }
   
   /** blast some random strings through the analyzer */
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java
index 37515b1..572a67c 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java
@@ -18,7 +18,6 @@ package org.apache.lucene.analysis.el;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.util.Version;
 
 /**
  * A unit test class for verifying the correct operation of the GreekAnalyzer.
@@ -47,29 +46,6 @@ public class GreekAnalyzerTest extends BaseTokenStreamTestCase {
     assertAnalyzesTo(a, "?Î¡?Î«????Î£??Î£  ??Î¿Î³Î¿?, Î¿ Î¼Îµ???? ÎºÎ±Î¹ Î¿Î¹ Î¬Î»Î»Î¿Î¹",
         new String[] { "??Î¿??Î¿Î¸Îµ?", "Î±?Î¿Î³", "Î¼Îµ??", "Î±Î»Î»" });
   }
-  
-	/**
-	 * Test the analysis of various greek strings.
-	 *
-	 * @throws Exception in case an error occurs
-	 * @deprecated (3.1) Remove this test when support for 3.0 is no longer needed
-	 */
-  @Deprecated
-	public void testAnalyzerBWCompat() throws Exception {
-		Analyzer a = new GreekAnalyzer(Version.LUCENE_30);
-		// Verify the correct analysis of capitals and small accented letters
-		assertAnalyzesTo(a, "?Î¯Î± ÎµÎ¾Î±Î¹?Îµ?Î¹ÎºÎ¬ ÎºÎ±Î»Î® ÎºÎ±Î¹ ?Î»Î¿??Î¹Î± ?ÎµÎ¹?Î¬ ?Î±?Î±Îº?Î®??Î½ ?Î·? ?Î»Î»Î·Î½Î¹ÎºÎ®? Î³Î»???Î±?",
-				new String[] { "Î¼Î¹Î±", "ÎµÎ¾Î±Î¹?Îµ?Î¹ÎºÎ±", "ÎºÎ±Î»Î·", "?Î»Î¿??Î¹Î±", "?ÎµÎ¹?Î±", "?Î±?Î±Îº?Î·??Î½",
-				"ÎµÎ»Î»Î·Î½Î¹ÎºÎ·?", "Î³Î»???Î±?" });
-		// Verify the correct analysis of small letters with diaeresis and the elimination
-		// of punctuation marks
-		assertAnalyzesTo(a, "??Î¿??Î½?Î± (ÎºÎ±Î¹)     [?Î¿Î»Î»Î±?Î»Î­?] - ??????Î£",
-				new String[] { "??Î¿Î¹Î¿Î½?Î±", "?Î¿Î»Î»Î±?Î»Îµ?", "Î±Î½Î±Î³ÎºÎµ?" });
-		// Verify the correct analysis of capital accented letters and capital letters with diaeresis,
-		// as well as the elimination of stop words
-		assertAnalyzesTo(a, "?Î¡?Î«????Î£??Î£  ??Î¿Î³Î¿?, Î¿ Î¼Îµ???? ÎºÎ±Î¹ Î¿Î¹ Î¬Î»Î»Î¿Î¹",
-				new String[] { "??Î¿??Î¿Î¸Îµ?ÎµÎ¹?", "Î±?Î¿Î³Î¿?", "Î¼Îµ??Î¿?", "Î±Î»Î»Î¿Î¹" });
-	}
 	
   public void testReusableTokenStream() throws Exception {
     Analyzer a = new GreekAnalyzer(TEST_VERSION_CURRENT);
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/fa/TestPersianNormalizationFilter.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/fa/TestPersianNormalizationFilter.java
index 8ca3933..f720aec 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/fa/TestPersianNormalizationFilter.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/fa/TestPersianNormalizationFilter.java
@@ -23,8 +23,8 @@ import java.io.StringReader;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.ar.ArabicLetterTokenizer;
 import org.apache.lucene.analysis.core.KeywordTokenizer;
 
 /**
@@ -58,8 +58,7 @@ public class TestPersianNormalizationFilter extends BaseTokenStreamTestCase {
   }
 
   private void check(final String input, final String expected) throws IOException {
-    ArabicLetterTokenizer tokenStream = new ArabicLetterTokenizer(TEST_VERSION_CURRENT, 
-        new StringReader(input));
+    MockTokenizer tokenStream = new MockTokenizer(new StringReader(input), MockTokenizer.WHITESPACE, false);
     PersianNormalizationFilter filter = new PersianNormalizationFilter(
         tokenStream);
     assertTokenStreamContents(filter, new String[]{expected});
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestFrenchAnalyzer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestFrenchAnalyzer.java
index 48b031d..15b5e08 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestFrenchAnalyzer.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestFrenchAnalyzer.java
@@ -115,94 +115,6 @@ public class TestFrenchAnalyzer extends BaseTokenStreamTestCase {
 
 	}
 	
-	/**
-	 * @deprecated (3.1) remove this test for Lucene 5.0
-	 */
-	@Deprecated
-	public void testAnalyzer30() throws Exception {
-	    FrenchAnalyzer fa = new FrenchAnalyzer(Version.LUCENE_30);
-	  
-	    assertAnalyzesTo(fa, "", new String[] {
-	    });
-
-	    assertAnalyzesTo(
-	      fa,
-	      "chien chat cheval",
-	      new String[] { "chien", "chat", "cheval" });
-
-	    assertAnalyzesTo(
-	      fa,
-	      "chien CHAT CHEVAL",
-	      new String[] { "chien", "chat", "cheval" });
-
-	    assertAnalyzesTo(
-	      fa,
-	      "  chien  ,? + = -  CHAT /: > CHEVAL",
-	      new String[] { "chien", "chat", "cheval" });
-
-	    assertAnalyzesTo(fa, "chien++", new String[] { "chien" });
-
-	    assertAnalyzesTo(
-	      fa,
-	      "mot \"entreguillemet\"",
-	      new String[] { "mot", "entreguillemet" });
-
-	    // let's do some french specific tests now  
-
-	    /* 1. couldn't resist
-	     I would expect this to stay one term as in French the minus 
-	    sign is often used for composing words */
-	    assertAnalyzesTo(
-	      fa,
-	      "Jean-FranÃ§ois",
-	      new String[] { "jean", "franÃ§ois" });
-
-	    // 2. stopwords
-	    assertAnalyzesTo(
-	      fa,
-	      "le la chien les aux chat du des ? cheval",
-	      new String[] { "chien", "chat", "cheval" });
-
-	    // some nouns and adjectives
-	    assertAnalyzesTo(
-	      fa,
-	      "lances chismes habitable chiste Ã©lÃ©ments captifs",
-	      new String[] {
-	        "lanc",
-	        "chism",
-	        "habit",
-	        "chist",
-	        "Ã©lÃ©ment",
-	        "captif" });
-
-	    // some verbs
-	    assertAnalyzesTo(
-	      fa,
-	      "finissions souffrirent rugissante",
-	      new String[] { "fin", "souffr", "rug" });
-
-	    // some everything else
-	    // aujourd'hui stays one term which is OK
-	    assertAnalyzesTo(
-	      fa,
-	      "C3PO aujourd'hui oeuf Ã¯Ã¢Ã¶Ã»?Ã¤ anticonstitutionnellement Java++ ",
-	      new String[] {
-	        "c3po",
-	        "aujourd'hui",
-	        "oeuf",
-	        "Ã¯Ã¢Ã¶Ã»?Ã¤",
-	        "anticonstitutionnel",
-	        "jav" });
-
-	    // some more everything else
-	    // here 1940-1945 stays as one term, 1940:1945 not ?
-	    assertAnalyzesTo(
-	      fa,
-	      "33Bis 1940-1945 1940:1945 (---i+++)*",
-	      new String[] { "33bis", "1940-1945", "1940", "1945", "i" });
-
-	  }
-	
 	public void testReusableTokenStream() throws Exception {
 	  FrenchAnalyzer fa = new FrenchAnalyzer(TEST_VERSION_CURRENT);
 	  // stopwords
@@ -243,21 +155,10 @@ public class TestFrenchAnalyzer extends BaseTokenStreamTestCase {
   }
   
   /**
-   * Prior to 3.1, this analyzer had no lowercase filter.
-   * stopwords were case sensitive. Preserve this for back compat.
-   * @deprecated (3.1) Remove this test in Lucene 5.0
-   */
-  @Deprecated
-  public void testBuggyStopwordsCasing() throws IOException {
-    FrenchAnalyzer a = new FrenchAnalyzer(Version.LUCENE_30);
-    assertAnalyzesTo(a, "Votre", new String[] { "votr" });
-  }
-  
-  /**
    * Test that stopwords are not case sensitive
    */
   public void testStopwordsCasing() throws IOException {
-    FrenchAnalyzer a = new FrenchAnalyzer(Version.LUCENE_31);
+    FrenchAnalyzer a = new FrenchAnalyzer(TEST_VERSION_CURRENT);
     assertAnalyzesTo(a, "Votre", new String[] { });
   }
   
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/it/TestItalianAnalyzer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/it/TestItalianAnalyzer.java
index 6366577..209da71 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/it/TestItalianAnalyzer.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/it/TestItalianAnalyzer.java
@@ -63,11 +63,4 @@ public class TestItalianAnalyzer extends BaseTokenStreamTestCase {
     assertAnalyzesTo(a, "dell'Italia", new String[] { "ital" });
     assertAnalyzesTo(a, "l'Italiano", new String[] { "italian" });
   }
-  
-  /** test that we don't enable this before 3.2*/
-  public void testContractionsBackwards() throws IOException {
-    Analyzer a = new ItalianAnalyzer(Version.LUCENE_31);
-    assertAnalyzesTo(a, "dell'Italia", new String[] { "dell'ital" });
-    assertAnalyzesTo(a, "l'Italiano", new String[] { "l'ital" });
-  }
 }
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/PatternAnalyzerTest.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/PatternAnalyzerTest.java
deleted file mode 100644
index 64560d0..0000000
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/PatternAnalyzerTest.java
+++ /dev/null
@@ -1,181 +0,0 @@
-package org.apache.lucene.analysis.miscellaneous;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.StringReader;
-import java.lang.Thread.UncaughtExceptionHandler;
-import java.util.Arrays;
-import java.util.regex.Pattern;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.core.StopAnalyzer;
-
-/**
- * Verifies the behavior of PatternAnalyzer.
- */
-public class PatternAnalyzerTest extends BaseTokenStreamTestCase {
-
-  /**
-   * Test PatternAnalyzer when it is configured with a non-word pattern.
-   * Behavior can be similar to SimpleAnalyzer (depending upon options)
-   */
-  public void testNonWordPattern() throws IOException {
-    // Split on non-letter pattern, do not lowercase, no stopwords
-    PatternAnalyzer a = new PatternAnalyzer(TEST_VERSION_CURRENT, PatternAnalyzer.NON_WORD_PATTERN,
-        false, null);
-    check(a, "The quick brown Fox,the abcd1234 (56.78) dc.", new String[] {
-        "The", "quick", "brown", "Fox", "the", "abcd", "dc" });
-
-    // split on non-letter pattern, lowercase, english stopwords
-    PatternAnalyzer b = new PatternAnalyzer(TEST_VERSION_CURRENT, PatternAnalyzer.NON_WORD_PATTERN,
-        true, StopAnalyzer.ENGLISH_STOP_WORDS_SET);
-    check(b, "The quick brown Fox,the abcd1234 (56.78) dc.", new String[] {
-        "quick", "brown", "fox", "abcd", "dc" });
-  }
-
-  /**
-   * Test PatternAnalyzer when it is configured with a whitespace pattern.
-   * Behavior can be similar to WhitespaceAnalyzer (depending upon options)
-   */
-  public void testWhitespacePattern() throws IOException {
-    // Split on whitespace patterns, do not lowercase, no stopwords
-    PatternAnalyzer a = new PatternAnalyzer(TEST_VERSION_CURRENT, PatternAnalyzer.WHITESPACE_PATTERN,
-        false, null);
-    check(a, "The quick brown Fox,the abcd1234 (56.78) dc.", new String[] {
-        "The", "quick", "brown", "Fox,the", "abcd1234", "(56.78)", "dc." });
-
-    // Split on whitespace patterns, lowercase, english stopwords
-    PatternAnalyzer b = new PatternAnalyzer(TEST_VERSION_CURRENT, PatternAnalyzer.WHITESPACE_PATTERN,
-        true, StopAnalyzer.ENGLISH_STOP_WORDS_SET);
-    check(b, "The quick brown Fox,the abcd1234 (56.78) dc.", new String[] {
-        "quick", "brown", "fox,the", "abcd1234", "(56.78)", "dc." });
-  }
-
-  /**
-   * Test PatternAnalyzer when it is configured with a custom pattern. In this
-   * case, text is tokenized on the comma ","
-   */
-  public void testCustomPattern() throws IOException {
-    // Split on comma, do not lowercase, no stopwords
-    PatternAnalyzer a = new PatternAnalyzer(TEST_VERSION_CURRENT, Pattern.compile(","), false, null);
-    check(a, "Here,Are,some,Comma,separated,words,", new String[] { "Here",
-        "Are", "some", "Comma", "separated", "words" });
-
-    // split on comma, lowercase, english stopwords
-    PatternAnalyzer b = new PatternAnalyzer(TEST_VERSION_CURRENT, Pattern.compile(","), true,
-        StopAnalyzer.ENGLISH_STOP_WORDS_SET);
-    check(b, "Here,Are,some,Comma,separated,words,", new String[] { "here",
-        "some", "comma", "separated", "words" });
-  }
-
-  /**
-   * Test PatternAnalyzer against a large document.
-   */
-  public void testHugeDocument() throws IOException {
-    StringBuilder document = new StringBuilder();
-    // 5000 a's
-    char largeWord[] = new char[5000];
-    Arrays.fill(largeWord, 'a');
-    document.append(largeWord);
-
-    // a space
-    document.append(' ');
-
-    // 2000 b's
-    char largeWord2[] = new char[2000];
-    Arrays.fill(largeWord2, 'b');
-    document.append(largeWord2);
-
-    // Split on whitespace patterns, do not lowercase, no stopwords
-    PatternAnalyzer a = new PatternAnalyzer(TEST_VERSION_CURRENT, PatternAnalyzer.WHITESPACE_PATTERN,
-        false, null);
-    check(a, document.toString(), new String[] { new String(largeWord),
-        new String(largeWord2) });
-  }
-
-  /**
-   * Verify the analyzer analyzes to the expected contents. For PatternAnalyzer,
-   * several methods are verified:
-   * <ul>
-   * <li>Analysis with a normal Reader
-   * <li>Analysis with a FastStringReader
-   * <li>Analysis with a String
-   * </ul>
-   */
-  private void check(PatternAnalyzer analyzer, String document,
-      String expected[]) throws IOException {
-    // ordinary analysis of a Reader
-    assertAnalyzesTo(analyzer, document, expected);
-
-    // analysis with a "FastStringReader"
-    TokenStream ts = analyzer.tokenStream("dummy",
-        new PatternAnalyzer.FastStringReader(document));
-    assertTokenStreamContents(ts, expected);
-
-    // analysis of a String, uses PatternAnalyzer.tokenStream(String, String)
-    TokenStream ts2 = analyzer.tokenStream("dummy", new StringReader(document));
-    assertTokenStreamContents(ts2, expected);
-  }
-  
-  /** blast some random strings through the analyzer */
-  public void testRandomStrings() throws Exception {
-    Analyzer a = new PatternAnalyzer(TEST_VERSION_CURRENT, Pattern.compile(","), true, StopAnalyzer.ENGLISH_STOP_WORDS_SET);
-    
-    // dodge jre bug http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=7104012
-    final UncaughtExceptionHandler savedHandler = Thread.getDefaultUncaughtExceptionHandler();
-    Thread.setDefaultUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {
-      @Override
-      public void uncaughtException(Thread thread, Throwable throwable) {
-        assumeTrue("not failing due to jre bug ", !isJREBug7104012(throwable));
-        // otherwise its some other bug, pass to default handler
-        savedHandler.uncaughtException(thread, throwable);
-      }
-    });
-    
-    try {
-      Thread.getDefaultUncaughtExceptionHandler();
-      checkRandomData(random(), a, 10000*RANDOM_MULTIPLIER);
-    } catch (ArrayIndexOutOfBoundsException ex) {
-      assumeTrue("not failing due to jre bug ", !isJREBug7104012(ex));
-      throw ex; // otherwise rethrow
-    } finally {
-      Thread.setDefaultUncaughtExceptionHandler(savedHandler);
-    }
-  }
-  
-  static boolean isJREBug7104012(Throwable t) {
-    if (!(t instanceof ArrayIndexOutOfBoundsException)) {
-      // BaseTokenStreamTestCase now wraps exc in a new RuntimeException:
-      t = t.getCause();
-      if (!(t instanceof ArrayIndexOutOfBoundsException)) {
-        return false;
-      }
-    }
-    StackTraceElement trace[] = t.getStackTrace();
-    for (StackTraceElement st : trace) {
-      if ("java.text.RuleBasedBreakIterator".equals(st.getClassName()) 
-          && "lookupBackwardState".equals(st.getMethodName())) {
-        return true;
-      }
-    }
-    return false;
-  }
-}
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer.java
index 8a561b2..9e05bd7 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer.java
@@ -27,6 +27,7 @@ import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -61,7 +62,7 @@ public class TestLimitTokenCountAnalyzer extends BaseTokenStreamTestCase {
     writer.addDocument(doc);
     writer.close();
 
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     Term t = new Term("field", "x");
     assertEquals(1, reader.docFreq(t));
     reader.close();
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/nl/TestDutchStemmer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/nl/TestDutchStemmer.java
index dacc306..20785bd 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/nl/TestDutchStemmer.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/nl/TestDutchStemmer.java
@@ -112,17 +112,6 @@ public class TestDutchStemmer extends BaseTokenStreamTestCase {
 	 check("ophouden", "ophoud");
   }
   
-  /**
-   * @deprecated (3.1) remove this test in Lucene 5.0
-   */
-  @Deprecated
-  public void testOldBuggyStemmer() throws Exception {
-    Analyzer a = new DutchAnalyzer(Version.LUCENE_30);
-    checkOneTermReuse(a, "opheffen", "ophef"); // versus snowball 'opheff'
-    checkOneTermReuse(a, "opheffende", "ophef"); // versus snowball 'opheff'
-    checkOneTermReuse(a, "opheffing", "ophef"); // versus snowball 'opheff'
-  }
-  
   public void testSnowballCorrectness() throws Exception {
     Analyzer a = new DutchAnalyzer(TEST_VERSION_CURRENT);
     checkOneTermReuse(a, "opheffen", "opheff");
@@ -139,7 +128,7 @@ public class TestDutchStemmer extends BaseTokenStreamTestCase {
   }
   
   public void testExclusionTableViaCtor() throws IOException {
-    CharArraySet set = new CharArraySet(Version.LUCENE_30, 1, true);
+    CharArraySet set = new CharArraySet(TEST_VERSION_CURRENT, 1, true);
     set.add("lichamelijk");
     DutchAnalyzer a = new DutchAnalyzer(TEST_VERSION_CURRENT, CharArraySet.EMPTY_SET, set);
     assertAnalyzesToReuse(a, "lichamelijk lichamelijke", new String[] { "lichamelijk", "licham" });
@@ -159,32 +148,10 @@ public class TestDutchStemmer extends BaseTokenStreamTestCase {
   }
   
   /**
-   * prior to 3.6, this confusingly did not happen if 
-   * you specified your own stoplist!!!!
-   * @deprecated (3.6) Remove this test in Lucene 5.0
-   */
-  @Deprecated
-  public void testBuggyStemOverrides() throws IOException {
-    DutchAnalyzer a = new DutchAnalyzer(Version.LUCENE_35, CharArraySet.EMPTY_SET);
-    checkOneTerm(a, "fiets", "fiet");
-  }
-  
-  /**
-   * Prior to 3.1, this analyzer had no lowercase filter.
-   * stopwords were case sensitive. Preserve this for back compat.
-   * @deprecated (3.1) Remove this test in Lucene 5.0
-   */
-  @Deprecated
-  public void testBuggyStopwordsCasing() throws IOException {
-    DutchAnalyzer a = new DutchAnalyzer(Version.LUCENE_30);
-    assertAnalyzesTo(a, "Zelf", new String[] { "zelf" });
-  }
-  
-  /**
    * Test that stopwords are not case sensitive
    */
   public void testStopwordsCasing() throws IOException {
-    DutchAnalyzer a = new DutchAnalyzer(Version.LUCENE_31);
+    DutchAnalyzer a = new DutchAnalyzer(TEST_VERSION_CURRENT);
     assertAnalyzesTo(a, "Zelf", new String[] { });
   }
   
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzerTest.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzerTest.java
index b6551f6..2716cd6 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzerTest.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzerTest.java
@@ -20,6 +20,7 @@ import org.apache.lucene.analysis.*;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -53,7 +54,7 @@ public class QueryAutoStopWordAnalyzerTest extends BaseTokenStreamTestCase {
       writer.addDocument(doc);
     }
     writer.close();
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
   }
 
   @Override
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/reverse/TestReverseStringFilter.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/reverse/TestReverseStringFilter.java
index 29c8d53..65a27de 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/reverse/TestReverseStringFilter.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/reverse/TestReverseStringFilter.java
@@ -57,15 +57,6 @@ public class TestReverseStringFilter extends BaseTokenStreamTestCase {
     assertEquals( "ABEDCF", new String( buffer ) );
   }
   
-  /**
-   * Test the broken 3.0 behavior, for back compat
-   * @deprecated (3.1) Remove in Lucene 5.0
-   */
-  @Deprecated
-  public void testBackCompat() throws Exception {
-    assertEquals("\uDF05\uD866\uDF05\uD866", ReverseStringFilter.reverse(Version.LUCENE_30, "ð©?ð©?"));
-  }
-  
   public void testReverseSupplementary() throws Exception {
     // supplementary at end
     assertEquals("ð©??±é??¹æ???", ReverseStringFilter.reverse(TEST_VERSION_CURRENT, "????¹é??±ð©¬?"));
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/ru/TestRussianAnalyzer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/ru/TestRussianAnalyzer.java
index 2beade8..4913b6e 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/ru/TestRussianAnalyzer.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/ru/TestRussianAnalyzer.java
@@ -37,16 +37,6 @@ public class TestRussianAnalyzer extends BaseTokenStreamTestCase {
       assertAnalyzesTo(ra, "text 1000", new String[] { "text", "1000" });
     }
     
-    /** @deprecated (3.1) remove this test in Lucene 5.0: stopwords changed */
-    @Deprecated
-    public void testReusableTokenStream30() throws Exception {
-      Analyzer a = new RussianAnalyzer(Version.LUCENE_30);
-      assertAnalyzesToReuse(a, "?Ð¼Ðµ??Ðµ ? ?ÐµÐ¼ Ð¾ ?Ð¸Ð»Ðµ ?Ð»ÐµÐº??Ð¾Ð¼Ð°Ð³Ð½Ð¸?Ð½Ð¾Ð¹ ?Ð½Ðµ?Ð³Ð¸Ð¸ Ð¸Ð¼ÐµÐ»Ð¸ Ð¿?ÐµÐ´??Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ðµ?Ðµ",
-          new String[] { "Ð²Ð¼Ðµ??", "?Ð¸Ð»", "?Ð»ÐµÐº??Ð¾Ð¼Ð°Ð³Ð½Ð¸?Ð½", "?Ð½Ðµ?Ð³", "Ð¸Ð¼ÐµÐ»", "Ð¿?ÐµÐ´??Ð°Ð²Ð»ÐµÐ½" });
-      assertAnalyzesToReuse(a, "?Ð¾ Ð·Ð½Ð°Ð½Ð¸Ðµ ??Ð¾ ??Ð°Ð½Ð¸Ð»Ð¾?? Ð² ?Ð°Ð¹Ð½Ðµ",
-          new String[] { "Ð·Ð½Ð°Ð½", "??Ð°Ð½", "?Ð°Ð¹Ð½" });
-    }
-    
     public void testReusableTokenStream() throws Exception {
       Analyzer a = new RussianAnalyzer(TEST_VERSION_CURRENT);
       assertAnalyzesToReuse(a, "?Ð¼Ðµ??Ðµ ? ?ÐµÐ¼ Ð¾ ?Ð¸Ð»Ðµ ?Ð»ÐµÐº??Ð¾Ð¼Ð°Ð³Ð½Ð¸?Ð½Ð¾Ð¹ ?Ð½Ðµ?Ð³Ð¸Ð¸ Ð¸Ð¼ÐµÐ»Ð¸ Ð¿?ÐµÐ´??Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ðµ?Ðµ",
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/ru/TestRussianLetterTokenizer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/ru/TestRussianLetterTokenizer.java
deleted file mode 100644
index 2fedf78..0000000
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/ru/TestRussianLetterTokenizer.java
+++ /dev/null
@@ -1,47 +0,0 @@
-package org.apache.lucene.analysis.ru;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.StringReader;
-
-import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.util.Version;
-
-/**
- * Testcase for {@link RussianLetterTokenizer}
- * @deprecated (3.1) Remove this test class in Lucene 5.0
- */
-@Deprecated
-public class TestRussianLetterTokenizer extends BaseTokenStreamTestCase {
-  
-  public void testRussianLetterTokenizer() throws IOException {
-    StringReader reader = new StringReader("1234567890 ?Ð¼Ðµ??Ðµ \ud801\udc1ctest");
-    RussianLetterTokenizer tokenizer = new RussianLetterTokenizer(Version.LUCENE_CURRENT,
-        reader);
-    assertTokenStreamContents(tokenizer, new String[] {"1234567890", "?Ð¼Ðµ??Ðµ",
-        "\ud801\udc1ctest"});
-  }
-  
-  public void testRussianLetterTokenizerBWCompat() throws IOException {
-    StringReader reader = new StringReader("1234567890 ?Ð¼Ðµ??Ðµ \ud801\udc1ctest");
-    RussianLetterTokenizer tokenizer = new RussianLetterTokenizer(Version.LUCENE_30,
-        reader);
-    assertTokenStreamContents(tokenizer, new String[] {"1234567890", "?Ð¼Ðµ??Ðµ", "test"});
-  }
-}
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java
index 0e02076..8cc3e86 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java
@@ -29,6 +29,7 @@ import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -74,7 +75,7 @@ public class ShingleAnalyzerWrapperTest extends BaseTokenStreamTestCase {
 
     writer.close();
 
-    reader = IndexReader.open(directory);
+    reader = DirectoryReader.open(directory);
     searcher = new IndexSearcher(reader);
   }
   
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter.java
index b9598c2..d597a08 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter.java
@@ -29,6 +29,7 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.DocsAndPositionsEnum;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
@@ -103,7 +104,7 @@ public class TestTeeSinkTokenFilter extends BaseTokenStreamTestCase {
     w.addDocument(doc);
     w.close();
 
-    IndexReader r = IndexReader.open(dir);
+    IndexReader r = DirectoryReader.open(dir);
     Terms vector = r.getTermVectors(0).terms("field");
     assertEquals(1, vector.size());
     TermsEnum termsEnum = vector.iterator(null);
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/snowball/TestSnowball.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/snowball/TestSnowball.java
index 7791fb4..019231e 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/snowball/TestSnowball.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/snowball/TestSnowball.java
@@ -22,6 +22,7 @@ import java.io.Reader;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.index.Payload;
 import org.apache.lucene.analysis.TokenStream;
@@ -38,63 +39,16 @@ import org.apache.lucene.util.Version;
 public class TestSnowball extends BaseTokenStreamTestCase {
 
   public void testEnglish() throws Exception {
-    Analyzer a = new SnowballAnalyzer(TEST_VERSION_CURRENT, "English");
-    assertAnalyzesTo(a, "he abhorred accents",
-        new String[]{"he", "abhor", "accent"});
-  }
-  
-  public void testStopwords() throws Exception {
-    Analyzer a = new SnowballAnalyzer(TEST_VERSION_CURRENT, "English",
-        StandardAnalyzer.STOP_WORDS_SET);
-    assertAnalyzesTo(a, "the quick brown fox jumped",
-        new String[]{"quick", "brown", "fox", "jump"});
-  }
-
-  /**
-   * Test english lowercasing. Test both cases (pre-3.1 and post-3.1) to ensure
-   * we lowercase I correct for non-Turkish languages in either case.
-   */
-  public void testEnglishLowerCase() throws Exception {
-    Analyzer a = new SnowballAnalyzer(TEST_VERSION_CURRENT, "English");
-    assertAnalyzesTo(a, "cryogenic", new String[] { "cryogen" });
-    assertAnalyzesTo(a, "CRYOGENIC", new String[] { "cryogen" });
+    Analyzer a = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
+        Tokenizer tokenizer = new MockTokenizer(reader);
+        return new TokenStreamComponents(tokenizer, new SnowballFilter(tokenizer, "English"));
+      }
+    };
     
-    Analyzer b = new SnowballAnalyzer(Version.LUCENE_30, "English");
-    assertAnalyzesTo(b, "cryogenic", new String[] { "cryogen" });
-    assertAnalyzesTo(b, "CRYOGENIC", new String[] { "cryogen" });
-  }
-  
-  /**
-   * Test turkish lowercasing
-   */
-  public void testTurkish() throws Exception {
-    Analyzer a = new SnowballAnalyzer(TEST_VERSION_CURRENT, "Turkish");
-
-    assertAnalyzesTo(a, "a?acÄ±", new String[] { "a?aÃ§" });
-    assertAnalyzesTo(a, "A?ACI", new String[] { "a?aÃ§" });
-  }
-  
-  /**
-   * Test turkish lowercasing (old buggy behavior)
-   * @deprecated (3.1) Remove this when support for 3.0 indexes is no longer required (5.0)
-   */
-  @Deprecated
-  public void testTurkishBWComp() throws Exception {
-    Analyzer a = new SnowballAnalyzer(Version.LUCENE_30, "Turkish");
-    // A?ACI in turkish lowercases to a?acÄ±, but with lowercase filter a?aci.
-    // this fails due to wrong casing, because the stemmer
-    // will only remove -Ä±, not -i
-    assertAnalyzesTo(a, "a?acÄ±", new String[] { "a?aÃ§" });
-    assertAnalyzesTo(a, "A?ACI", new String[] { "a?aci" });
-  }
-
-  
-  public void testReusableTokenStream() throws Exception {
-    Analyzer a = new SnowballAnalyzer(TEST_VERSION_CURRENT, "English");
-    assertAnalyzesToReuse(a, "he abhorred accents",
+    assertAnalyzesTo(a, "he abhorred accents",
         new String[]{"he", "abhor", "accent"});
-    assertAnalyzesToReuse(a, "she abhorred him",
-        new String[]{"she", "abhor", "him"});
   }
   
   public void testFilterTokens() throws Exception {
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/th/TestThaiAnalyzer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/th/TestThaiAnalyzer.java
index 68da30e..90cc92a 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/th/TestThaiAnalyzer.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/th/TestThaiAnalyzer.java
@@ -62,13 +62,6 @@ public class TestThaiAnalyzer extends BaseTokenStreamTestCase {
 	      new int[] { 5, 2, 1 });
 	}
 	
-	public void testBackwardsStopWords() throws Exception {
-	   assertAnalyzesTo(new ThaiAnalyzer(Version.LUCENE_35), "à¸?¸²à¸£à?à¸µà?à¹??à¹??à¹?¸­à¸??à¸??à¸?¸§à¹?¸²à¸?¸²à¸??à¸?", 
-	        new String[] { "à¸?¸²à¸?", "à¸?¸µà¹?", "à¹??à¹?", "à¸??à¸??", "à¹?¸ªà¸??", "à¸§à?à¸?", "à¸?¸²à¸?", "à¸?¸µ" },
-	        new int[] { 0, 3, 6, 9, 13, 17, 20, 23 },
-	        new int[] { 3, 6, 9, 13, 17, 20, 23, 25 });
-	}
-	
 	public void testTokenType() throws Exception {
       assertAnalyzesTo(new ThaiAnalyzer(TEST_VERSION_CURRENT, CharArraySet.EMPTY_SET), "à¸?¸²à¸£à?à¸µà?à¹??à¹??à¹?¸­à¸??à¸??à¸?¸§à¹?¸²à¸?¸²à¸??à¸? à¹??à¹?", 
                        new String[] { "à¸?¸²à¸?", "à¸?¸µà¹?", "à¹??à¹?", "à¸??à¸??", "à¹?¸ªà¸??", "à¸§à?à¸?", "à¸?¸²à¸?", "à¸?¸µ", "à¹??à¹?" },
@@ -79,43 +72,6 @@ public class TestThaiAnalyzer extends BaseTokenStreamTestCase {
                                       "<NUM>" });
 	}
 
-	/**
-	 * Thai numeric tokens were typed as <ALPHANUM> instead of <NUM>.
-	 * @deprecated (3.1) testing backwards behavior
- 	 */
-	@Deprecated
-	public void testBuggyTokenType30() throws Exception {
-		assertAnalyzesTo(new ThaiAnalyzer(Version.LUCENE_30), "à¸?¸²à¸£à?à¸µà?à¹??à¹??à¹?¸­à¸??à¸??à¸?¸§à¹?¸²à¸?¸²à¸??à¸? à¹??à¹?", 
-                         new String[] { "à¸?¸²à¸?", "à¸?¸µà¹?", "à¹??à¹?", "à¸??à¸??", "à¹?¸ªà¸??", "à¸§à?à¸?", "à¸?¸²à¸?", "à¸?¸µ", "à¹??à¹?" },
-                         new String[] { "<ALPHANUM>", "<ALPHANUM>", "<ALPHANUM>", 
-                                        "<ALPHANUM>", "<ALPHANUM>", "<ALPHANUM>", 
-                                        "<ALPHANUM>", "<ALPHANUM>", "<ALPHANUM>" });
-	}
-	
-	/** @deprecated (3.1) testing backwards behavior */
-	@Deprecated
-    public void testAnalyzer30() throws Exception {
-        ThaiAnalyzer analyzer = new ThaiAnalyzer(Version.LUCENE_30);
-	
-		assertAnalyzesTo(analyzer, "", new String[] {});
-
-		assertAnalyzesTo(
-			analyzer,
-			"à¸?¸²à¸£à?à¸µà?à¹??à¹??à¹?¸­à¸??à¸??à¸?¸§à¹?¸²à¸?¸²à¸??à¸?",
-			new String[] { "à¸?¸²à¸?", "à¸?¸µà¹?", "à¹??à¹?", "à¸??à¸??", "à¹?¸ªà¸??", "à¸§à?à¸?", "à¸?¸²à¸?", "à¸?¸µ"});
-
-		assertAnalyzesTo(
-			analyzer,
-			"à¸?¸£à¸´à¸©à¸±à?à¸?¸·à¹?¸­ XY&Z - à¸?¸¸à¸¢à?à¸±à? xyz@demo.com",
-			new String[] { "à¸?¸£à¸´à¸©à¸±à?", "à¸?¸·à¹?¸­", "xy&z", "à¸?¸¸à¸?", "à¸?¸±à¸?", "xyz@demo.com" });
-
-    // English stop words
-		assertAnalyzesTo(
-			analyzer,
-			"à¸?¸£à¸°à?à¸¢à?à¸§à?à¸? The quick brown fox jumped over the lazy dogs",
-			new String[] { "à¸?¸£à¸°à?à¸¢à?", "à¸§à?à¸?", "quick", "brown", "fox", "jumped", "over", "lazy", "dogs" });
-	}
-	
 	/*
 	 * Test that position increments are adjusted correctly for stopwords.
 	 */
@@ -151,23 +107,6 @@ public class TestThaiAnalyzer extends BaseTokenStreamTestCase {
           new String[] { "à¸?¸£à¸´à¸©à¸±à?", "à¸?¸·à¹?¸­", "xy", "z", "à¸?¸¸à¸?", "à¸?¸±à¸?", "xyz", "demo.com" });
 	}
 	
-	/** @deprecated (3.1) for version back compat */
-	@Deprecated
-	public void testReusableTokenStream30() throws Exception {
-	    ThaiAnalyzer analyzer = new ThaiAnalyzer(Version.LUCENE_30);
-	    assertAnalyzesToReuse(analyzer, "", new String[] {});
-
-	    assertAnalyzesToReuse(
-            analyzer,
-            "à¸?¸²à¸£à?à¸µà?à¹??à¹??à¹?¸­à¸??à¸??à¸?¸§à¹?¸²à¸?¸²à¸??à¸?",
-            new String[] { "à¸?¸²à¸?", "à¸?¸µà¹?", "à¹??à¹?", "à¸??à¸??", "à¹?¸ªà¸??", "à¸§à?à¸?", "à¸?¸²à¸?", "à¸?¸µ"});
-
-	    assertAnalyzesToReuse(
-            analyzer,
-            "à¸?¸£à¸´à¸©à¸±à?à¸?¸·à¹?¸­ XY&Z - à¸?¸¸à¸¢à?à¸±à? xyz@demo.com",
-            new String[] { "à¸?¸£à¸´à¸©à¸±à?", "à¸?¸·à¹?¸­", "xy&z", "à¸?¸¸à¸?", "à¸?¸±à¸?", "xyz@demo.com" });
-  }
-	
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
     checkRandomData(random(), new ThaiAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
@@ -181,7 +120,7 @@ public class TestThaiAnalyzer extends BaseTokenStreamTestCase {
   
   // LUCENE-3044
   public void testAttributeReuse() throws Exception {
-    ThaiAnalyzer analyzer = new ThaiAnalyzer(Version.LUCENE_30);
+    ThaiAnalyzer analyzer = new ThaiAnalyzer(TEST_VERSION_CURRENT);
     // just consume
     TokenStream ts = analyzer.tokenStream("dummy", new StringReader("à¸?¸²à¸©à¸²à¹??à¸?"));
     assertTokenStreamContents(ts, new String[] { "à¸?¸²à¸©à¸²", "à¹??à¸?" });
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharArraySet.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharArraySet.java
index 9cb0757..849a498 100755
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharArraySet.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharArraySet.java
@@ -250,77 +250,6 @@ public class TestCharArraySet extends LuceneTestCase {
     }
   }
   
-  /**
-   * @deprecated (3.1) remove this test when lucene 3.0 "broken unicode 4" support is
-   *             no longer needed.
-   */
-  @Deprecated
-  public void testSupplementaryCharsBWCompat() {
-    String missing = "Term %s is missing in the set";
-    String falsePos = "Term %s is in the set but shouldn't";
-    // for reference see
-    // http://unicode.org/cldr/utility/list-unicodeset.jsp?a=[[%3ACase_Sensitive%3DTrue%3A]%26[^[\u0000-\uFFFF]]]&esc=on
-    String[] upperArr = new String[] {"Abc\ud801\udc1c",
-        "\ud801\udc1c\ud801\udc1cCDE", "A\ud801\udc1cB"};
-    String[] lowerArr = new String[] {"abc\ud801\udc44",
-        "\ud801\udc44\ud801\udc44cde", "a\ud801\udc44b"};
-    CharArraySet set = new CharArraySet(Version.LUCENE_30, Arrays.asList(TEST_STOP_WORDS), true);
-    for (String upper : upperArr) {
-      set.add(upper);
-    }
-    for (int i = 0; i < upperArr.length; i++) {
-      assertTrue(String.format(missing, upperArr[i]), set.contains(upperArr[i]));
-      assertFalse(String.format(falsePos, lowerArr[i]), set.contains(lowerArr[i]));
-    }
-    set = new CharArraySet(Version.LUCENE_30, Arrays.asList(TEST_STOP_WORDS), false);
-    for (String upper : upperArr) {
-      set.add(upper);
-    }
-    for (int i = 0; i < upperArr.length; i++) {
-      assertTrue(String.format(missing, upperArr[i]), set.contains(upperArr[i]));
-      assertFalse(String.format(falsePos, lowerArr[i]), set.contains(lowerArr[i]));
-    }
-  }
-
-  /**
-   * @deprecated (3.1) remove this test when lucene 3.0 "broken unicode 4" support is
-   *             no longer needed.
-   */
-  @Deprecated
-  public void testSingleHighSurrogateBWComapt() {
-    String missing = "Term %s is missing in the set";
-    String falsePos = "Term %s is in the set but shouldn't";
-    String[] upperArr = new String[] { "ABC\uD800", "ABC\uD800EfG",
-        "\uD800EfG", "\uD800\ud801\udc1cB" };
-
-    String[] lowerArr = new String[] { "abc\uD800", "abc\uD800efg",
-        "\uD800efg", "\uD800\ud801\udc44b" };
-    CharArraySet set = new CharArraySet(Version.LUCENE_30, Arrays
-        .asList(TEST_STOP_WORDS), true);
-    for (String upper : upperArr) {
-      set.add(upper);
-    }
-    for (int i = 0; i < upperArr.length; i++) {
-      assertTrue(String.format(missing, upperArr[i]), set.contains(upperArr[i]));
-      if (i == lowerArr.length - 1)
-        assertFalse(String.format(falsePos, lowerArr[i]), set
-            .contains(lowerArr[i]));
-      else
-        assertTrue(String.format(missing, lowerArr[i]), set
-            .contains(lowerArr[i]));
-    }
-    set = new CharArraySet(Version.LUCENE_30, Arrays.asList(TEST_STOP_WORDS),
-        false);
-    for (String upper : upperArr) {
-      set.add(upper);
-    }
-    for (int i = 0; i < upperArr.length; i++) {
-      assertTrue(String.format(missing, upperArr[i]), set.contains(upperArr[i]));
-      assertFalse(String.format(falsePos, lowerArr[i]), set
-          .contains(lowerArr[i]));
-    }
-  }
-  
   @SuppressWarnings("deprecated")
   public void testCopyCharArraySetBWCompat() {
     CharArraySet setIngoreCase = new CharArraySet(TEST_VERSION_CURRENT, 10, true);
@@ -499,10 +428,5 @@ public class TestCharArraySet extends LuceneTestCase {
     assertEquals("[test]", set.toString());
     set.add("test2");
     assertTrue(set.toString().contains(", "));
-    
-    set = CharArraySet.copy(Version.LUCENE_30, Collections.singleton("test"));
-    assertEquals("[test]", set.toString());
-    set.add("test2");
-    assertTrue(set.toString().contains(", "));
   }
 }
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharacterUtils.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharacterUtils.java
index 4e9fdbf..1066179 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharacterUtils.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharacterUtils.java
@@ -33,7 +33,7 @@ public class TestCharacterUtils extends LuceneTestCase {
 
   @Test
   public void testCodePointAtCharArrayInt() {
-    CharacterUtils java4 = CharacterUtils.getInstance(Version.LUCENE_30);
+    CharacterUtils java4 = CharacterUtils.getJava4Instance();
     char[] cpAt3 = "Abc\ud801\udc1c".toCharArray();
     char[] highSurrogateAt3 = "Abc\ud801".toCharArray();
     assertEquals((int) 'A', java4.codePointAt(cpAt3, 0));
@@ -59,7 +59,7 @@ public class TestCharacterUtils extends LuceneTestCase {
 
   @Test
   public void testCodePointAtCharSequenceInt() {
-    CharacterUtils java4 = CharacterUtils.getInstance(Version.LUCENE_30);
+    CharacterUtils java4 = CharacterUtils.getJava4Instance();
     String cpAt3 = "Abc\ud801\udc1c";
     String highSurrogateAt3 = "Abc\ud801";
     assertEquals((int) 'A', java4.codePointAt(cpAt3, 0));
@@ -86,7 +86,7 @@ public class TestCharacterUtils extends LuceneTestCase {
 
   @Test
   public void testCodePointAtCharArrayIntInt() {
-    CharacterUtils java4 = CharacterUtils.getInstance(Version.LUCENE_30);
+    CharacterUtils java4 = CharacterUtils.getJava4Instance();
     char[] cpAt3 = "Abc\ud801\udc1c".toCharArray();
     char[] highSurrogateAt3 = "Abc\ud801".toCharArray();
     assertEquals((int) 'A', java4.codePointAt(cpAt3, 0, 2));
@@ -122,9 +122,10 @@ public class TestCharacterUtils extends LuceneTestCase {
 
   @Test
   public void testFillNoHighSurrogate() throws IOException {
-    Version[] versions = new Version[] { Version.LUCENE_30, TEST_VERSION_CURRENT };
-    for (Version version : versions) {
-      CharacterUtils instance = CharacterUtils.getInstance(version);
+    CharacterUtils versions[] = new CharacterUtils[] { 
+        CharacterUtils.getInstance(TEST_VERSION_CURRENT), 
+        CharacterUtils.getJava4Instance() };
+    for (CharacterUtils instance : versions) {
       Reader reader = new StringReader("helloworld");
       CharacterBuffer buffer = CharacterUtils.newCharacterBuffer(6);
       assertTrue(instance.fill(buffer,reader));
@@ -172,7 +173,7 @@ public class TestCharacterUtils extends LuceneTestCase {
   @Test
   public void testFillJava14() throws IOException {
     String input = "1234\ud801\udc1c789123\ud801\ud801\udc1c\ud801";
-    CharacterUtils instance = CharacterUtils.getInstance(Version.LUCENE_30);
+    CharacterUtils instance = CharacterUtils.getJava4Instance();
     Reader reader = new StringReader(input);
     CharacterBuffer buffer = CharacterUtils.newCharacterBuffer(5);
     assertTrue(instance.fill(buffer, reader));
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyFilter.java b/lucene/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyFilter.java
deleted file mode 100644
index 929f048..0000000
--- a/lucene/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyFilter.java
+++ /dev/null
@@ -1,102 +0,0 @@
-package org.apache.lucene.collation;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-import org.apache.lucene.analysis.*;
-import org.apache.lucene.analysis.core.KeywordTokenizer;
-import org.apache.lucene.util.BytesRef;
-
-import java.text.Collator;
-import java.util.Locale;
-import java.io.Reader;
-
-/**
- * @deprecated remove when CollationKeyFilter is removed.
- */
-@Deprecated
-public class TestCollationKeyFilter extends CollationTestBase {
-  // the sort order of ? versus U depends on the version of the rules being used
-  // for the inherited root locale: ?'s order isnt specified in Locale.US since 
-  // its not used in english.
-  boolean oStrokeFirst = Collator.getInstance(new Locale("")).compare("?", "U") < 0;
-  
-  // Neither Java 1.4.2 nor 1.5.0 has Farsi Locale collation available in
-  // RuleBasedCollator.  However, the Arabic Locale seems to order the Farsi
-  // characters properly.
-  private Collator collator = Collator.getInstance(new Locale("ar"));
-  private Analyzer analyzer = new TestAnalyzer(collator);
-
-  private BytesRef firstRangeBeginning = new BytesRef(encodeCollationKey
-    (collator.getCollationKey(firstRangeBeginningOriginal).toByteArray()));
-  private BytesRef firstRangeEnd = new BytesRef(encodeCollationKey
-    (collator.getCollationKey(firstRangeEndOriginal).toByteArray()));
-  private BytesRef secondRangeBeginning = new BytesRef(encodeCollationKey
-    (collator.getCollationKey(secondRangeBeginningOriginal).toByteArray()));
-  private BytesRef secondRangeEnd = new BytesRef(encodeCollationKey
-    (collator.getCollationKey(secondRangeEndOriginal).toByteArray()));
-
-  
-  public final class TestAnalyzer extends Analyzer {
-    private Collator _collator;
-
-    TestAnalyzer(Collator collator) {
-      _collator = collator;
-    }
-
-    @Override
-    public TokenStreamComponents createComponents(String fieldName, Reader reader) {
-      Tokenizer result = new KeywordTokenizer(reader);
-      return new TokenStreamComponents(result, new CollationKeyFilter(result, _collator));
-    }
-  }
-
-  public void testFarsiRangeFilterCollating() throws Exception {
-    testFarsiRangeFilterCollating
-      (analyzer, firstRangeBeginning, firstRangeEnd, 
-       secondRangeBeginning, secondRangeEnd);
-  }
- 
-  public void testFarsiRangeQueryCollating() throws Exception {
-    testFarsiRangeQueryCollating
-      (analyzer, firstRangeBeginning, firstRangeEnd, 
-       secondRangeBeginning, secondRangeEnd);
-  }
-
-  public void testFarsiTermRangeQuery() throws Exception {
-    testFarsiTermRangeQuery
-      (analyzer, firstRangeBeginning, firstRangeEnd, 
-       secondRangeBeginning, secondRangeEnd);
-  }
-  
-  public void testCollationKeySort() throws Exception {
-    Analyzer usAnalyzer = new TestAnalyzer(Collator.getInstance(Locale.US));
-    Analyzer franceAnalyzer 
-      = new TestAnalyzer(Collator.getInstance(Locale.FRANCE));
-    Analyzer swedenAnalyzer 
-      = new TestAnalyzer(Collator.getInstance(new Locale("sv", "se")));
-    Analyzer denmarkAnalyzer 
-      = new TestAnalyzer(Collator.getInstance(new Locale("da", "dk")));
-    
-    // The ICU Collator and Sun java.text.Collator implementations differ in their
-    // orderings - "BFJDH" is the ordering for java.text.Collator for Locale.US.
-    testCollationKeySort
-    (usAnalyzer, franceAnalyzer, swedenAnalyzer, denmarkAnalyzer, 
-     oStrokeFirst ? "BFJHD" : "BFJDH", "EACGI", "BJDFH", "BJDHF");
-  }
-}
diff --git a/lucene/analysis/icu/src/java/org/apache/lucene/collation/ICUCollationKeyAnalyzer.java b/lucene/analysis/icu/src/java/org/apache/lucene/collation/ICUCollationKeyAnalyzer.java
index 95fafb4..3cacea2 100644
--- a/lucene/analysis/icu/src/java/org/apache/lucene/collation/ICUCollationKeyAnalyzer.java
+++ b/lucene/analysis/icu/src/java/org/apache/lucene/collation/ICUCollationKeyAnalyzer.java
@@ -22,18 +22,16 @@ import com.ibm.icu.text.Collator;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.core.KeywordTokenizer;
 import org.apache.lucene.collation.CollationKeyAnalyzer; // javadocs
-import org.apache.lucene.util.IndexableBinaryStringTools; // javadocs
 import org.apache.lucene.util.Version;
 
 import java.io.Reader;
 
 /**
  * <p>
- *   Filters {@link KeywordTokenizer} with {@link ICUCollationKeyFilter}.
+ *   Configures {@link KeywordTokenizer} with {@link ICUCollationAttributeFactory}.
  * <p>
  *   Converts the token into its {@link com.ibm.icu.text.CollationKey}, and
- *   then encodes the CollationKey either directly or with 
- *   {@link IndexableBinaryStringTools} (see <a href="#version">below</a>), to allow it to
+ *   then encodes the CollationKey directly to allow it to
  *   be stored as an index term.
  * </p>
  * <p>
@@ -67,48 +65,24 @@ import java.io.Reader;
  *   generation timing and key length comparisons between ICU4J and
  *   java.text.Collator over several languages.
  * </p>
- * <a name="version"/>
- * <p>You must specify the required {@link Version}
- * compatibility when creating ICUCollationKeyAnalyzer:
- * <ul>
- *   <li> As of 4.0, Collation Keys are directly encoded as bytes. Previous
- *   versions will encode the bytes with {@link IndexableBinaryStringTools}.
- * </ul>
  */
 public final class ICUCollationKeyAnalyzer extends Analyzer {
-  private final Collator collator;
   private final ICUCollationAttributeFactory factory;
-  private final Version matchVersion;
 
   /**
    * Create a new ICUCollationKeyAnalyzer, using the specified collator.
    * 
-   * @param matchVersion See <a href="#version">above</a>
+   * @param matchVersion compatibility version
    * @param collator CollationKey generator
    */
   public ICUCollationKeyAnalyzer(Version matchVersion, Collator collator) {
-    this.matchVersion = matchVersion;
-    this.collator = collator;
     this.factory = new ICUCollationAttributeFactory(collator);
   }
 
-  /**
-   * @deprecated Use {@link ICUCollationKeyAnalyzer#ICUCollationKeyAnalyzer(Version, Collator)}
-   *   and specify a version instead. This ctor will be removed in Lucene 5.0
-   */
-  @Deprecated
-  public ICUCollationKeyAnalyzer(Collator collator) {
-    this(Version.LUCENE_31, collator);
-  }
 
   @Override
   protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
-    if (matchVersion.onOrAfter(Version.LUCENE_40)) {
-      KeywordTokenizer tokenizer = new KeywordTokenizer(factory, reader, KeywordTokenizer.DEFAULT_BUFFER_SIZE);
-      return new TokenStreamComponents(tokenizer, tokenizer);
-    } else {
-      KeywordTokenizer tokenizer = new KeywordTokenizer(reader);
-      return new TokenStreamComponents(tokenizer, new ICUCollationKeyFilter(tokenizer, collator));
-    }
+    KeywordTokenizer tokenizer = new KeywordTokenizer(factory, reader, KeywordTokenizer.DEFAULT_BUFFER_SIZE);
+    return new TokenStreamComponents(tokenizer, tokenizer);
   }
 }
diff --git a/lucene/analysis/icu/src/java/org/apache/lucene/collation/ICUCollationKeyFilter.java b/lucene/analysis/icu/src/java/org/apache/lucene/collation/ICUCollationKeyFilter.java
deleted file mode 100644
index c69aebe..0000000
--- a/lucene/analysis/icu/src/java/org/apache/lucene/collation/ICUCollationKeyFilter.java
+++ /dev/null
@@ -1,114 +0,0 @@
-package org.apache.lucene.collation;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-import com.ibm.icu.text.Collator;
-import com.ibm.icu.text.RawCollationKey;
-
-import org.apache.lucene.analysis.TokenFilter;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.util.IndexableBinaryStringTools;
-
-import org.apache.lucene.collation.CollationKeyFilter; // javadocs
-
-import java.io.IOException;
-
-
-/**
- * <p>
- *   Converts each token into its {@link com.ibm.icu.text.CollationKey}, and
- *   then encodes the CollationKey with {@link IndexableBinaryStringTools}, to
- *   allow it to be stored as an index term.
- * </p>
- * <p>
- *   <strong>WARNING:</strong> Make sure you use exactly the same Collator at
- *   index and query time -- CollationKeys are only comparable when produced by
- *   the same Collator.  {@link com.ibm.icu.text.RuleBasedCollator}s are 
- *   independently versioned, so it is safe to search against stored
- *   CollationKeys if the following are exactly the same (best practice is
- *   to store this information with the index and check that they remain the
- *   same at query time):
- * </p>
- * <ol>
- *   <li>
- *     Collator version - see {@link Collator#getVersion()}
- *   </li>
- *   <li>
- *     The collation strength used - see {@link Collator#setStrength(int)}
- *   </li>
- * </ol> 
- * <p>
- *   CollationKeys generated by ICU Collators are not compatible with those
- *   generated by java.text.Collators.  Specifically, if you use 
- *   ICUCollationKeyFilter to generate index terms, do not use 
- *   {@link CollationKeyFilter} on the query side, or vice versa.
- * </p>
- * <p>
- *   ICUCollationKeyFilter is significantly faster and generates significantly
- *   shorter keys than CollationKeyFilter.  See
- *   <a href="http://site.icu-project.org/charts/collation-icu4j-sun"
- *   >http://site.icu-project.org/charts/collation-icu4j-sun</a> for key
- *   generation timing and key length comparisons between ICU4J and
- *   java.text.Collator over several languages.
- * </p>
- *  @deprecated Use {@link ICUCollationAttributeFactory} instead, which encodes
- *  terms directly as bytes. This filter will be removed in Lucene 5.0
- */
-@Deprecated
-public final class ICUCollationKeyFilter extends TokenFilter {
-  private Collator collator = null;
-  private RawCollationKey reusableKey = new RawCollationKey();
-  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-
-  /**
-   * 
-   * @param input Source token stream
-   * @param collator CollationKey generator
-   */
-  public ICUCollationKeyFilter(TokenStream input, Collator collator) {
-    super(input);
-    // clone the collator: see http://userguide.icu-project.org/collation/architecture
-    try {
-      this.collator = (Collator) collator.clone();
-    } catch (CloneNotSupportedException e) {
-      throw new RuntimeException(e);
-    }
-  }
-
-  @Override
-  public boolean incrementToken() throws IOException {
-    if (input.incrementToken()) {
-      char[] termBuffer = termAtt.buffer();
-      String termText = new String(termBuffer, 0, termAtt.length());
-      collator.getRawCollationKey(termText, reusableKey);
-      int encodedLength = IndexableBinaryStringTools.getEncodedLength(
-          reusableKey.bytes, 0, reusableKey.size);
-      if (encodedLength > termBuffer.length) {
-        termAtt.resizeBuffer(encodedLength);
-      }
-      termAtt.setLength(encodedLength);
-      IndexableBinaryStringTools.encode(reusableKey.bytes, 0, reusableKey.size,
-          termAtt.buffer(), 0, encodedLength);
-      return true;
-    } else {
-      return false;
-    }
-  }
-}
diff --git a/lucene/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationKeyFilter.java b/lucene/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationKeyFilter.java
deleted file mode 100644
index f1a038a..0000000
--- a/lucene/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationKeyFilter.java
+++ /dev/null
@@ -1,98 +0,0 @@
-package org.apache.lucene.collation;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-import com.ibm.icu.text.Collator;
-
-import org.apache.lucene.analysis.*;
-import org.apache.lucene.analysis.core.KeywordTokenizer;
-import org.apache.lucene.util.BytesRef;
-
-import java.io.Reader;
-import java.util.Locale;
-
-/** @deprecated remove this when ICUCollationKeyFilter is removed */
-@Deprecated
-public class TestICUCollationKeyFilter extends CollationTestBase {
-
-  private Collator collator = Collator.getInstance(new Locale("fa"));
-  private Analyzer analyzer = new TestAnalyzer(collator);
-
-  private BytesRef firstRangeBeginning = new BytesRef(encodeCollationKey
-    (collator.getCollationKey(firstRangeBeginningOriginal).toByteArray()));
-  private BytesRef firstRangeEnd = new BytesRef(encodeCollationKey
-    (collator.getCollationKey(firstRangeEndOriginal).toByteArray()));
-  private BytesRef secondRangeBeginning = new BytesRef(encodeCollationKey
-    (collator.getCollationKey(secondRangeBeginningOriginal).toByteArray()));
-  private BytesRef secondRangeEnd = new BytesRef(encodeCollationKey
-    (collator.getCollationKey(secondRangeEndOriginal).toByteArray()));
-
-  
-  public final class TestAnalyzer extends Analyzer {
-    private Collator _collator;
-
-    TestAnalyzer(Collator collator) {
-      _collator = collator;
-    }
-
-    @Override
-    public TokenStreamComponents createComponents(String fieldName, Reader reader) {
-      Tokenizer result = new KeywordTokenizer(reader);
-      return new TokenStreamComponents(result, new ICUCollationKeyFilter(result, _collator));
-    }
-  }
-
-  public void testFarsiRangeFilterCollating() throws Exception {
-    testFarsiRangeFilterCollating(analyzer, firstRangeBeginning, firstRangeEnd, 
-                                  secondRangeBeginning, secondRangeEnd);
-  }
- 
-  public void testFarsiRangeQueryCollating() throws Exception {
-    testFarsiRangeQueryCollating(analyzer, firstRangeBeginning, firstRangeEnd, 
-                                 secondRangeBeginning, secondRangeEnd);
-  }
-
-  public void testFarsiTermRangeQuery() throws Exception {
-    testFarsiTermRangeQuery
-      (analyzer, firstRangeBeginning, firstRangeEnd, 
-       secondRangeBeginning, secondRangeEnd);
-  }
-
-  // Test using various international locales with accented characters (which
-  // sort differently depending on locale)
-  //
-  // Copied (and slightly modified) from 
-  // org.apache.lucene.search.TestSort.testInternationalSort()
-  //  
-  public void testCollationKeySort() throws Exception {
-    Analyzer usAnalyzer = new TestAnalyzer(Collator.getInstance(Locale.US));
-    Analyzer franceAnalyzer 
-      = new TestAnalyzer(Collator.getInstance(Locale.FRANCE));
-    Analyzer swedenAnalyzer 
-      = new TestAnalyzer(Collator.getInstance(new Locale("sv", "se")));
-    Analyzer denmarkAnalyzer 
-      = new TestAnalyzer(Collator.getInstance(new Locale("da", "dk")));
-
-    // The ICU Collator and java.text.Collator implementations differ in their
-    // orderings - "BFJHD" is the ordering for the ICU Collator for Locale.US.
-    testCollationKeySort
-    (usAnalyzer, franceAnalyzer, swedenAnalyzer, denmarkAnalyzer, 
-     "BFJHD", "ECAGI", "BJDFH", "BJDHF");
-  }
-}
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NearRealtimeReaderTask.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NearRealtimeReaderTask.java
index 6606dc4..c5ae6e1 100644
--- a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NearRealtimeReaderTask.java
+++ b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NearRealtimeReaderTask.java
@@ -60,7 +60,7 @@ public class NearRealtimeReaderTask extends PerfTask {
     }
     
     long t = System.currentTimeMillis();
-    DirectoryReader r = IndexReader.open(w, true);
+    DirectoryReader r = DirectoryReader.open(w, true);
     runData.setIndexReader(r);
     // Transfer our reference to runData
     r.decRef();
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenReaderTask.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenReaderTask.java
index 8468ea6..dca7377 100644
--- a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenReaderTask.java
+++ b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenReaderTask.java
@@ -45,9 +45,9 @@ public class OpenReaderTask extends PerfTask {
     Directory dir = getRunData().getDirectory();
     DirectoryReader r = null;
     if (commitUserData != null) {
-      r = IndexReader.open(OpenReaderTask.findIndexCommit(dir, commitUserData)); 
+      r = DirectoryReader.open(OpenReaderTask.findIndexCommit(dir, commitUserData)); 
     } else {
-      r = IndexReader.open(dir); 
+      r = DirectoryReader.open(dir); 
     }
     getRunData().setIndexReader(r);
     // We transfer reference to the run data
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/PrintReaderTask.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/PrintReaderTask.java
index db6422e..68def9b 100644
--- a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/PrintReaderTask.java
+++ b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/PrintReaderTask.java
@@ -18,6 +18,7 @@ package org.apache.lucene.benchmark.byTask.tasks;
  */
 
 import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.store.Directory;
 
@@ -47,9 +48,9 @@ public class PrintReaderTask extends PerfTask {
     Directory dir = getRunData().getDirectory();
     IndexReader r = null;
     if (userData == null) 
-      r = IndexReader.open(dir);
+      r = DirectoryReader.open(dir);
     else
-      r = IndexReader.open(OpenReaderTask.findIndexCommit(dir, userData));
+      r = DirectoryReader.open(OpenReaderTask.findIndexCommit(dir, userData));
     System.out.println("--> numDocs:"+r.numDocs()+" dels:"+r.numDeletedDocs());
     r.close();
     return 1;
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTask.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTask.java
index 93bcb95..2c99ecf 100644
--- a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTask.java
+++ b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTask.java
@@ -28,6 +28,7 @@ import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.benchmark.byTask.PerfRunData;
 import org.apache.lucene.benchmark.byTask.feeds.QueryMaker;
 import org.apache.lucene.document.Document;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexableField;
 import org.apache.lucene.index.MultiFields;
@@ -84,7 +85,7 @@ public abstract class ReadTask extends PerfTask {
     if (searcher == null) {
       // open our own reader
       Directory dir = getRunData().getDirectory();
-      reader = IndexReader.open(dir);
+      reader = DirectoryReader.open(dir);
       searcher = new IndexSearcher(reader);
       closeSearcher = true;
     } else {
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/QueryDriver.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/QueryDriver.java
index afdd014..c14166c 100644
--- a/lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/QueryDriver.java
+++ b/lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/QueryDriver.java
@@ -20,6 +20,7 @@ package org.apache.lucene.benchmark.quality.trec;
 import org.apache.lucene.benchmark.quality.utils.SimpleQQParser;
 import org.apache.lucene.benchmark.quality.utils.SubmissionReport;
 import org.apache.lucene.benchmark.quality.*;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.store.FSDirectory;
@@ -53,7 +54,7 @@ public class QueryDriver {
     SubmissionReport submitLog = new SubmissionReport(new PrintWriter(args[2]), "lucene");
     FSDirectory dir = FSDirectory.open(new File(args[3]));
     String fieldSpec = args.length == 5 ? args[4] : "T"; // default to Title-only if not specified.
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     IndexSearcher searcher = new IndexSearcher(reader);
 
     int maxResults = 1000;
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/QualityQueriesFinder.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/QualityQueriesFinder.java
index 1ca9abf..be4643d 100755
--- a/lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/QualityQueriesFinder.java
+++ b/lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/QualityQueriesFinder.java
@@ -19,6 +19,7 @@ package org.apache.lucene.benchmark.quality.utils;
 import java.io.File;
 import java.io.IOException;
 
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.index.Terms;
@@ -86,7 +87,7 @@ public class QualityQueriesFinder {
   
   private String [] bestTerms(String field,int numTerms) throws IOException {
     PriorityQueue<TermDf> pq = new TermsDfQueue(numTerms);
-    IndexReader ir = IndexReader.open(dir);
+    IndexReader ir = DirectoryReader.open(dir);
     try {
       int threshold = ir.maxDoc() / 10; // ignore words too common.
       Terms terms = MultiFields.getTerms(ir, field);
diff --git a/lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic.java b/lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic.java
index 476f7e6..2d65f76 100755
--- a/lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic.java
+++ b/lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic.java
@@ -105,7 +105,7 @@ public class TestPerfTasksLogic extends BenchmarkTestCase {
         new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))
             .setOpenMode(OpenMode.APPEND));
     iw.close();
-    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory());
+    IndexReader ir = DirectoryReader.open(benchmark.getRunData().getDirectory());
     assertEquals("1000 docs were added to the index, this is what we expect to find!",1000,ir.numDocs());
     ir.close();
   }
@@ -191,7 +191,7 @@ public class TestPerfTasksLogic extends BenchmarkTestCase {
     // now we should be able to open the index for write.
     IndexWriter iw = new IndexWriter(benchmark.getRunData().getDirectory(), new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND));
     iw.close();
-    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory());
+    IndexReader ir = DirectoryReader.open(benchmark.getRunData().getDirectory());
     assertEquals("100 docs were added to the index, this is what we expect to find!",100,ir.numDocs());
     ir.close();
   }
@@ -230,7 +230,7 @@ public class TestPerfTasksLogic extends BenchmarkTestCase {
     // now we should be able to open the index for write.
     IndexWriter iw = new IndexWriter(benchmark.getRunData().getDirectory(), new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND));
     iw.close();
-    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory());
+    IndexReader ir = DirectoryReader.open(benchmark.getRunData().getDirectory());
     assertEquals("1000 docs were added to the index, this is what we expect to find!",1000,ir.numDocs());
     ir.close();
   }
@@ -303,7 +303,7 @@ public class TestPerfTasksLogic extends BenchmarkTestCase {
     // now we should be able to open the index for write. 
     IndexWriter iw = new IndexWriter(benchmark.getRunData().getDirectory(), new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND));
     iw.close();
-    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory());
+    IndexReader ir = DirectoryReader.open(benchmark.getRunData().getDirectory());
     assertEquals("1 docs were added to the index, this is what we expect to find!",1,ir.numDocs());
     ir.close();
   }
@@ -334,7 +334,7 @@ public class TestPerfTasksLogic extends BenchmarkTestCase {
     // 3. execute the algorithm  (required in every "logic" test)
     Benchmark benchmark = execBenchmark(algLines);
 
-    DirectoryReader r = IndexReader.open(benchmark.getRunData().getDirectory());
+    DirectoryReader r = DirectoryReader.open(benchmark.getRunData().getDirectory());
     DocTermsIndex idx = FieldCache.DEFAULT.getTermsIndex(new SlowCompositeReaderWrapper(r), "country");
     final int maxDoc = r.maxDoc();
     assertEquals(1000, maxDoc);
@@ -370,7 +370,7 @@ public class TestPerfTasksLogic extends BenchmarkTestCase {
     Benchmark benchmark = execBenchmark(algLines);
 
     // 3. test number of docs in the index
-    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory());
+    IndexReader ir = DirectoryReader.open(benchmark.getRunData().getDirectory());
     int ndocsExpected = 20; // first 20 reuters docs.
     assertEquals("wrong number of docs in the index!", ndocsExpected, ir.numDocs());
     ir.close();
@@ -435,7 +435,7 @@ public class TestPerfTasksLogic extends BenchmarkTestCase {
             .setOpenMode(OpenMode.APPEND));
     iw.close();
 
-    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory());
+    IndexReader ir = DirectoryReader.open(benchmark.getRunData().getDirectory());
     assertEquals(numLines + " lines were created but " + ir.numDocs() + " docs are in the index", numLines, ir.numDocs());
     ir.close();
 
@@ -479,7 +479,7 @@ public class TestPerfTasksLogic extends BenchmarkTestCase {
     }
 
     // Separately count how many tokens are actually in the index:
-    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory());
+    IndexReader reader = DirectoryReader.open(benchmark.getRunData().getDirectory());
     assertEquals(NUM_DOCS, reader.numDocs());
 
     int totalTokenCount2 = 0;
@@ -538,7 +538,7 @@ public class TestPerfTasksLogic extends BenchmarkTestCase {
     Benchmark benchmark = execBenchmark(algLines);
 
     // 3. test number of docs in the index
-    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory());
+    IndexReader ir = DirectoryReader.open(benchmark.getRunData().getDirectory());
     int ndocsExpected = 2 * 20; // first 20 reuters docs.
     assertEquals("wrong number of docs in the index!", ndocsExpected, ir.numDocs());
     ir.close();
@@ -575,7 +575,7 @@ public class TestPerfTasksLogic extends BenchmarkTestCase {
     Benchmark benchmark = execBenchmark(algLines);
 
     // 3. test number of docs in the index
-    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory());
+    IndexReader ir = DirectoryReader.open(benchmark.getRunData().getDirectory());
     int ndocsExpected = 20;  // first 20 reuters docs.
     assertEquals("wrong number of docs in the index!", ndocsExpected, ir.numDocs());
     ir.close();
@@ -612,7 +612,7 @@ public class TestPerfTasksLogic extends BenchmarkTestCase {
     Benchmark benchmark = execBenchmark(algLines);
 
     // 3. test number of docs in the index
-    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory());
+    IndexReader ir = DirectoryReader.open(benchmark.getRunData().getDirectory());
     int ndocsExpected = 20; // first 20 reuters docs.
     assertEquals("wrong number of docs in the index!", ndocsExpected, ir.numDocs());
     ir.close();
@@ -659,7 +659,7 @@ public class TestPerfTasksLogic extends BenchmarkTestCase {
     benchmark.getRunData().getIndexWriter().close();
 
     // 3. test number of docs in the index
-    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory());
+    IndexReader ir = DirectoryReader.open(benchmark.getRunData().getDirectory());
     int ndocsExpected = 20; // first 20 reuters docs.
     assertEquals("wrong number of docs in the index!", ndocsExpected, ir.numDocs());
     ir.close();
@@ -705,7 +705,7 @@ public class TestPerfTasksLogic extends BenchmarkTestCase {
     benchmark.getRunData().getIndexWriter().close();
     
     // 3. test number of docs in the index
-    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory());
+    IndexReader ir = DirectoryReader.open(benchmark.getRunData().getDirectory());
     int ndocsExpected = 20; // first 20 reuters docs.
     assertEquals("wrong number of docs in the index!", ndocsExpected, ir.numDocs());
     ir.close();
@@ -749,7 +749,7 @@ public class TestPerfTasksLogic extends BenchmarkTestCase {
     assertFalse(((LogMergePolicy) writer.getConfig().getMergePolicy()).getUseCompoundFile());
     writer.close();
     Directory dir = benchmark.getRunData().getDirectory();
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     Fields tfv = reader.getTermVectors(0);
     assertNotNull(tfv);
     assertTrue(tfv.size() > 0);
@@ -825,7 +825,7 @@ public class TestPerfTasksLogic extends BenchmarkTestCase {
     Benchmark benchmark = execBenchmark(algLines);
 
     // 3. test number of docs in the index
-    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory());
+    IndexReader ir = DirectoryReader.open(benchmark.getRunData().getDirectory());
     int ndocsExpected = 20; // first 20 reuters docs.
     assertEquals("wrong number of docs in the index!", ndocsExpected, ir.numDocs());
     ir.close();
diff --git a/lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/feeds/LineDocSourceTest.java b/lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/feeds/LineDocSourceTest.java
index ffe4ad3..6c4db60 100644
--- a/lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/feeds/LineDocSourceTest.java
+++ b/lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/feeds/LineDocSourceTest.java
@@ -37,6 +37,7 @@ import org.apache.lucene.benchmark.byTask.tasks.CreateIndexTask;
 import org.apache.lucene.benchmark.byTask.tasks.TaskSequence;
 import org.apache.lucene.benchmark.byTask.tasks.WriteLineDocTask;
 import org.apache.lucene.benchmark.byTask.utils.Config;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.IndexSearcher;
@@ -149,7 +150,7 @@ public class LineDocSourceTest extends BenchmarkTestCase {
         tasks.close(); 
       }
       
-      reader = IndexReader.open(runData.getDirectory());
+      reader = DirectoryReader.open(runData.getDirectory());
       searcher = new IndexSearcher(reader);
       TopDocs td = searcher.search(new TermQuery(new Term("body", "body")), 10);
       assertEquals(numAdds, td.totalHits);
diff --git a/lucene/benchmark/src/test/org/apache/lucene/benchmark/quality/TestQualityRun.java b/lucene/benchmark/src/test/org/apache/lucene/benchmark/quality/TestQualityRun.java
index fffeb35..5cc8d9f 100644
--- a/lucene/benchmark/src/test/org/apache/lucene/benchmark/quality/TestQualityRun.java
+++ b/lucene/benchmark/src/test/org/apache/lucene/benchmark/quality/TestQualityRun.java
@@ -22,6 +22,7 @@ import org.apache.lucene.benchmark.quality.trec.TrecJudge;
 import org.apache.lucene.benchmark.quality.trec.TrecTopicsReader;
 import org.apache.lucene.benchmark.quality.utils.SimpleQQParser;
 import org.apache.lucene.benchmark.quality.utils.SubmissionReport;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.store.Directory;
@@ -69,7 +70,7 @@ public class TestQualityRun extends BenchmarkTestCase {
     judge.validateData(qqs, logger);
     
     Directory dir = newFSDirectory(new File(getWorkDir(),"index"));
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     IndexSearcher searcher = new IndexSearcher(reader);
 
     QualityQueryParser qqParser = new SimpleQQParser("title","body");
diff --git a/lucene/core/src/java/org/apache/lucene/document/Field.java b/lucene/core/src/java/org/apache/lucene/document/Field.java
index 9c50dd3..9f1e14b 100644
--- a/lucene/core/src/java/org/apache/lucene/document/Field.java
+++ b/lucene/core/src/java/org/apache/lucene/document/Field.java
@@ -496,447 +496,4 @@ public class Field implements IndexableField {
 
     throw new IllegalArgumentException("Field must have either TokenStream, String, Reader or Number value");
   }
-
-  
-  //
-  // Deprecated transition API below:
-  //
-
-  /** Specifies whether and how a field should be stored.
-   *
-   *  @deprecated This is here only to ease transition from
-   *  the pre-4.0 APIs. */
-  @Deprecated
-  public static enum Store {
-
-    /** Store the original field value in the index. This is useful for short texts
-     * like a document's title which should be displayed with the results. The
-     * value is stored in its original form, i.e. no analyzer is used before it is
-     * stored.
-     */
-    YES {
-      @Override
-      public boolean isStored() { return true; }
-    },
-
-    /** Do not store the field value in the index. */
-    NO {
-      @Override
-      public boolean isStored() { return false; }
-    };
-
-    public abstract boolean isStored();
-  }
-
-  /** Specifies whether and how a field should be indexed.
-   *
-   *  @deprecated This is here only to ease transition from
-   *  the pre-4.0 APIs. */
-  @Deprecated
-  public static enum Index {
-
-    /** Do not index the field value. This field can thus not be searched,
-     * but one can still access its contents provided it is
-     * {@link Field.Store stored}. */
-    NO {
-      @Override
-      public boolean isIndexed()  { return false; }
-      @Override
-      public boolean isAnalyzed() { return false; }
-      @Override
-      public boolean omitNorms()  { return true;  }   
-    },
-
-    /** Index the tokens produced by running the field's
-     * value through an Analyzer.  This is useful for
-     * common text. */
-    ANALYZED {
-      @Override
-      public boolean isIndexed()  { return true;  }
-      @Override
-      public boolean isAnalyzed() { return true;  }
-      @Override
-      public boolean omitNorms()  { return false; }   	
-    },
-
-    /** Index the field's value without using an Analyzer, so it can be searched.
-     * As no analyzer is used the value will be stored as a single term. This is
-     * useful for unique Ids like product numbers.
-     */
-    NOT_ANALYZED {
-      @Override
-      public boolean isIndexed()  { return true;  }
-      @Override
-      public boolean isAnalyzed() { return false; }
-      @Override
-      public boolean omitNorms()  { return false; }   	
-    },
-
-    /** Expert: Index the field's value without an Analyzer,
-     * and also disable the indexing of norms.  Note that you
-     * can also separately enable/disable norms by calling
-     * {@link FieldType#setOmitNorms}.  No norms means that
-     * index-time field and document boosting and field
-     * length normalization are disabled.  The benefit is
-     * less memory usage as norms take up one byte of RAM
-     * per indexed field for every document in the index,
-     * during searching.  Note that once you index a given
-     * field <i>with</i> norms enabled, disabling norms will
-     * have no effect.  In other words, for this to have the
-     * above described effect on a field, all instances of
-     * that field must be indexed with NOT_ANALYZED_NO_NORMS
-     * from the beginning. */
-    NOT_ANALYZED_NO_NORMS {
-      @Override
-      public boolean isIndexed()  { return true;  }
-      @Override
-      public boolean isAnalyzed() { return false; }
-      @Override
-      public boolean omitNorms()  { return true;  }   	
-    },
-
-    /** Expert: Index the tokens produced by running the
-     *  field's value through an Analyzer, and also
-     *  separately disable the storing of norms.  See
-     *  {@link #NOT_ANALYZED_NO_NORMS} for what norms are
-     *  and why you may want to disable them. */
-    ANALYZED_NO_NORMS {
-      @Override
-      public boolean isIndexed()  { return true;  }
-      @Override
-      public boolean isAnalyzed() { return true;  }
-      @Override
-      public boolean omitNorms()  { return true;  }   	
-    };
-
-    /** Get the best representation of the index given the flags. */
-    public static Index toIndex(boolean indexed, boolean analyzed) {
-      return toIndex(indexed, analyzed, false);
-    }
-
-    /** Expert: Get the best representation of the index given the flags. */
-    public static Index toIndex(boolean indexed, boolean analyzed, boolean omitNorms) {
-
-      // If it is not indexed nothing else matters
-      if (!indexed) {
-        return Index.NO;
-      }
-
-      // typical, non-expert
-      if (!omitNorms) {
-        if (analyzed) {
-          return Index.ANALYZED;
-        }
-        return Index.NOT_ANALYZED;
-      }
-
-      // Expert: Norms omitted
-      if (analyzed) {
-        return Index.ANALYZED_NO_NORMS;
-      }
-      return Index.NOT_ANALYZED_NO_NORMS;
-    }
-
-    public abstract boolean isIndexed();
-    public abstract boolean isAnalyzed();
-    public abstract boolean omitNorms();  	
-  }
-
-  /** Specifies whether and how a field should have term vectors.
-   *
-   *  @deprecated This is here only to ease transition from
-   *  the pre-4.0 APIs. */
-  @Deprecated
-  public static enum TermVector {
-    
-    /** Do not store term vectors. 
-     */
-    NO {
-      @Override
-      public boolean isStored()      { return false; }
-      @Override
-      public boolean withPositions() { return false; }
-      @Override
-      public boolean withOffsets()   { return false; }
-    },
-    
-    /** Store the term vectors of each document. A term vector is a list
-     * of the document's terms and their number of occurrences in that document. */
-    YES {
-      @Override
-      public boolean isStored()      { return true;  }
-      @Override
-      public boolean withPositions() { return false; }
-      @Override
-      public boolean withOffsets()   { return false; }
-    },
-    
-    /**
-     * Store the term vector + token position information
-     * 
-     * @see #YES
-     */ 
-    WITH_POSITIONS {
-      @Override
-      public boolean isStored()      { return true;  }
-      @Override
-      public boolean withPositions() { return true;  }
-      @Override
-      public boolean withOffsets()   { return false; }
-    },
-    
-    /**
-     * Store the term vector + Token offset information
-     * 
-     * @see #YES
-     */ 
-    WITH_OFFSETS {
-      @Override
-      public boolean isStored()      { return true;  }
-      @Override
-      public boolean withPositions() { return false; }
-      @Override
-      public boolean withOffsets()   { return true;  }
-    },
-    
-    /**
-     * Store the term vector + Token position and offset information
-     * 
-     * @see #YES
-     * @see #WITH_POSITIONS
-     * @see #WITH_OFFSETS
-     */ 
-    WITH_POSITIONS_OFFSETS {
-      @Override
-      public boolean isStored()      { return true;  }
-      @Override
-      public boolean withPositions() { return true;  }
-      @Override
-      public boolean withOffsets()   { return true;  }
-    };
-
-    /** Get the best representation of a TermVector given the flags. */
-    public static TermVector toTermVector(boolean stored, boolean withOffsets, boolean withPositions) {
-
-      // If it is not stored, nothing else matters.
-      if (!stored) {
-        return TermVector.NO;
-      }
-
-      if (withOffsets) {
-        if (withPositions) {
-          return Field.TermVector.WITH_POSITIONS_OFFSETS;
-        }
-        return Field.TermVector.WITH_OFFSETS;
-      }
-
-      if (withPositions) {
-        return Field.TermVector.WITH_POSITIONS;
-      }
-      return Field.TermVector.YES;
-    }
-
-    public abstract boolean isStored();
-    public abstract boolean withPositions();
-    public abstract boolean withOffsets();
-  }
-
-  /** Translates the pre-4.0 enums for specifying how a
-   *  field should be indexed into the 4.0 {@link FieldType}
-   *  approach.
-   *
-   * @deprecated This is here only to ease transition from
-   * the pre-4.0 APIs.
-   */
-  @Deprecated
-  public static final FieldType translateFieldType(Store store, Index index, TermVector termVector) {
-    final FieldType ft = new FieldType();
-
-    ft.setStored(store == Store.YES);
-
-    switch(index) {
-    case ANALYZED:
-      ft.setIndexed(true);
-      ft.setTokenized(true);
-      break;
-    case ANALYZED_NO_NORMS:
-      ft.setIndexed(true);
-      ft.setTokenized(true);
-      ft.setOmitNorms(true);
-      break;
-    case NOT_ANALYZED:
-      ft.setIndexed(true);
-      break;
-    case NOT_ANALYZED_NO_NORMS:
-      ft.setIndexed(true);
-      ft.setOmitNorms(true);
-      break;
-    case NO:
-      break;
-    }
-
-    switch(termVector) {
-    case NO:
-      break;
-    case YES:
-      ft.setStoreTermVectors(true);
-      break;
-    case WITH_POSITIONS:
-      ft.setStoreTermVectors(true);
-      ft.setStoreTermVectorPositions(true);
-      break;
-    case WITH_OFFSETS:
-      ft.setStoreTermVectors(true);
-      ft.setStoreTermVectorOffsets(true);
-      break;
-    case WITH_POSITIONS_OFFSETS:
-      ft.setStoreTermVectors(true);
-      ft.setStoreTermVectorPositions(true);
-      ft.setStoreTermVectorOffsets(true);
-      break;
-    }
-    ft.freeze();
-    return ft;
-  }
-
-  /**
-   * Create a field by specifying its name, value and how it will
-   * be saved in the index. Term vectors will not be stored in the index.
-   * 
-   * @param name The name of the field
-   * @param value The string to process
-   * @param store Whether <code>value</code> should be stored in the index
-   * @param index Whether the field should be indexed, and if so, if it should
-   *  be tokenized before indexing 
-   * @throws NullPointerException if name or value is <code>null</code>
-   * @throws IllegalArgumentException if the field is neither stored nor indexed 
-   *
-   * @deprecated Use {@link StringField}, {@link TextField} instead. */
-  @Deprecated
-  public Field(String name, String value, Store store, Index index) {
-    this(name, value, translateFieldType(store, index, TermVector.NO));
-  }
-
-  /**
-   * Create a field by specifying its name, value and how it will
-   * be saved in the index.
-   * 
-   * @param name The name of the field
-   * @param value The string to process
-   * @param store Whether <code>value</code> should be stored in the index
-   * @param index Whether the field should be indexed, and if so, if it should
-   *  be tokenized before indexing 
-   * @param termVector Whether term vector should be stored
-   * @throws NullPointerException if name or value is <code>null</code>
-   * @throws IllegalArgumentException in any of the following situations:
-   * <ul> 
-   *  <li>the field is neither stored nor indexed</li> 
-   *  <li>the field is not indexed but termVector is <code>TermVector.YES</code></li>
-   * </ul> 
-   *
-   * @deprecated Use {@link StringField}, {@link TextField} instead. */
-  @Deprecated
-  public Field(String name, String value, Store store, Index index, TermVector termVector) {  
-    this(name, value, translateFieldType(store, index, termVector));
-  }
-
-  /**
-   * Create a tokenized and indexed field that is not stored. Term vectors will
-   * not be stored.  The Reader is read only when the Document is added to the index,
-   * i.e. you may not close the Reader until {@link IndexWriter#addDocument}
-   * has been called.
-   * 
-   * @param name The name of the field
-   * @param reader The reader with the content
-   * @throws NullPointerException if name or reader is <code>null</code>
-   *
-   * @deprecated Use {@link TextField} instead.
-   */
-  @Deprecated
-  public Field(String name, Reader reader) {
-    this(name, reader, TermVector.NO);
-  }
-
-  /**
-   * Create a tokenized and indexed field that is not stored, optionally with 
-   * storing term vectors.  The Reader is read only when the Document is added to the index,
-   * i.e. you may not close the Reader until {@link IndexWriter#addDocument}
-   * has been called.
-   * 
-   * @param name The name of the field
-   * @param reader The reader with the content
-   * @param termVector Whether term vector should be stored
-   * @throws NullPointerException if name or reader is <code>null</code>
-   *
-   * @deprecated Use {@link TextField} instead.
-   */ 
-  @Deprecated
-  public Field(String name, Reader reader, TermVector termVector) {
-    this(name, reader, translateFieldType(Store.NO, Index.ANALYZED, termVector));
-  }
-
-  /**
-   * Create a tokenized and indexed field that is not stored. Term vectors will
-   * not be stored. This is useful for pre-analyzed fields.
-   * The TokenStream is read only when the Document is added to the index,
-   * i.e. you may not close the TokenStream until {@link IndexWriter#addDocument}
-   * has been called.
-   * 
-   * @param name The name of the field
-   * @param tokenStream The TokenStream with the content
-   * @throws NullPointerException if name or tokenStream is <code>null</code>
-   *
-   * @deprecated Use {@link TextField} instead
-   */ 
-  @Deprecated
-  public Field(String name, TokenStream tokenStream) {
-    this(name, tokenStream, TermVector.NO);
-  }
-
-  /**
-   * Create a tokenized and indexed field that is not stored, optionally with 
-   * storing term vectors.  This is useful for pre-analyzed fields.
-   * The TokenStream is read only when the Document is added to the index,
-   * i.e. you may not close the TokenStream until {@link IndexWriter#addDocument}
-   * has been called.
-   * 
-   * @param name The name of the field
-   * @param tokenStream The TokenStream with the content
-   * @param termVector Whether term vector should be stored
-   * @throws NullPointerException if name or tokenStream is <code>null</code>
-   *
-   * @deprecated Use {@link TextField} instead
-   */ 
-  @Deprecated
-  public Field(String name, TokenStream tokenStream, TermVector termVector) {
-    this(name, tokenStream, translateFieldType(Store.NO, Index.ANALYZED, termVector));
-  }
-
-  /**
-   * Create a stored field with binary value. Optionally the value may be compressed.
-   * 
-   * @param name The name of the field
-   * @param value The binary value
-   *
-   * @deprecated Use {@link StoredField} instead.
-   */
-  @Deprecated
-  public Field(String name, byte[] value) {
-    this(name, value, translateFieldType(Store.YES, Index.NO, TermVector.NO));
-  }
-
-  /**
-   * Create a stored field with binary value. Optionally the value may be compressed.
-   * 
-   * @param name The name of the field
-   * @param value The binary value
-   * @param offset Starting offset in value where this Field's bytes are
-   * @param length Number of bytes to use for this Field, starting at offset
-   *
-   * @deprecated Use {@link StoredField} instead.
-   */
-  @Deprecated
-  public Field(String name, byte[] value, int offset, int length) {
-    this(name, value, offset, length, translateFieldType(Store.YES, Index.NO, TermVector.NO));
-  }
 }
diff --git a/lucene/core/src/java/org/apache/lucene/index/AtomicReader.java b/lucene/core/src/java/org/apache/lucene/index/AtomicReader.java
index 108afc7..8a8d9c1 100644
--- a/lucene/core/src/java/org/apache/lucene/index/AtomicReader.java
+++ b/lucene/core/src/java/org/apache/lucene/index/AtomicReader.java
@@ -60,19 +60,6 @@ public abstract class AtomicReader extends IndexReader {
     return readerContext;
   }
 
-  /** 
-   * Returns true if there are norms stored for this field.
-   * @deprecated (4.0) use {@link #getFieldInfos()} and check {@link FieldInfo#hasNorms()} 
-   *                   for the field instead.
-   */
-  @Deprecated
-  public final boolean hasNorms(String field) throws IOException {
-    ensureOpen();
-    // note: using normValues(field) != null would potentially cause i/o
-    FieldInfo fi = getFieldInfos().fieldInfo(field);
-    return fi != null && fi.hasNorms();
-  }
-
   /**
    * Returns {@link Fields} for this reader.
    * This method may return null if the reader has no
diff --git a/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java b/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
index 4ee5c87..7f94e6e 100644
--- a/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
+++ b/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
@@ -643,12 +643,10 @@ public class CheckIndex {
       }
       for (FieldInfo info : fieldInfos) {
         if (info.hasNorms()) {
-          assert reader.hasNorms(info.name); // deprecated path
           DocValues dv = reader.normValues(info.name);
           checkDocValues(dv, info.name, info.getNormType(), reader.maxDoc());
           ++status.totFields;
         } else {
-          assert !reader.hasNorms(info.name); // deprecated path
           if (reader.normValues(info.name) != null) {
             throw new RuntimeException("field: " + info.name + " should omit norms but has them!");
           }
diff --git a/lucene/core/src/java/org/apache/lucene/index/CompositeReader.java b/lucene/core/src/java/org/apache/lucene/index/CompositeReader.java
index 74697c7..a5e7734 100644
--- a/lucene/core/src/java/org/apache/lucene/index/CompositeReader.java
+++ b/lucene/core/src/java/org/apache/lucene/index/CompositeReader.java
@@ -33,7 +33,7 @@ import org.apache.lucene.store.*;
  the {@code CompositeReader} interface, it is not possible to directly get postings.
  <p> Concrete subclasses of IndexReader are usually constructed with a call to
  one of the static <code>open()</code> methods, e.g. {@link
- #open(Directory)}.
+ DirectoryReader#open(Directory)}.
 
  <p> For efficiency, in this API documents are often referred to via
  <i>document numbers</i>, non-negative integers which each name a unique
diff --git a/lucene/core/src/java/org/apache/lucene/index/DirectoryReader.java b/lucene/core/src/java/org/apache/lucene/index/DirectoryReader.java
index d561a84..d877589 100644
--- a/lucene/core/src/java/org/apache/lucene/index/DirectoryReader.java
+++ b/lucene/core/src/java/org/apache/lucene/index/DirectoryReader.java
@@ -253,7 +253,7 @@ public abstract class DirectoryReader extends BaseCompositeReader<AtomicReader>
    *  one commit point.  But if you're using a custom {@link
    *  IndexDeletionPolicy} then there could be many commits.
    *  Once you have a given commit, you can open a reader on
-   *  it by calling {@link IndexReader#open(IndexCommit)}
+   *  it by calling {@link DirectoryReader#open(IndexCommit)}
    *  There must be at least one commit in
    *  the Directory, else this method throws {@link
    *  IndexNotFoundException}.  Note that if a commit is in
@@ -397,9 +397,9 @@ public abstract class DirectoryReader extends BaseCompositeReader<AtomicReader>
    *
    * <p>If instead this reader is a near real-time reader
    * (ie, obtained by a call to {@link
-   * IndexReader#open(IndexWriter,boolean)}, or by calling {@link #openIfChanged}
+   * DirectoryReader#open(IndexWriter,boolean)}, or by calling {@link #openIfChanged}
    * on a near real-time reader), then this method checks if
-   * either a new commmit has occurred, or any new
+   * either a new commit has occurred, or any new
    * uncommitted changes have taken place via the writer.
    * Note that even if the writer has only performed
    * merging, this method will still return false.</p>
diff --git a/lucene/core/src/java/org/apache/lucene/index/IndexReader.java b/lucene/core/src/java/org/apache/lucene/index/IndexReader.java
index 3952fc7..5f4df6f 100644
--- a/lucene/core/src/java/org/apache/lucene/index/IndexReader.java
+++ b/lucene/core/src/java/org/apache/lucene/index/IndexReader.java
@@ -277,100 +277,6 @@ public abstract class IndexReader implements Closeable {
   public final int hashCode() {
     return System.identityHashCode(this);
   }
-  
-  /** Returns a IndexReader reading the index in the given
-   *  Directory
-   * @param directory the index directory
-   * @throws CorruptIndexException if the index is corrupt
-   * @throws IOException if there is a low-level IO error
-   * @deprecated Use {@link DirectoryReader#open(Directory)}
-   */
-  @Deprecated
-  public static DirectoryReader open(final Directory directory) throws CorruptIndexException, IOException {
-    return DirectoryReader.open(directory);
-  }
-  
-  /** Expert: Returns a IndexReader reading the index in the given
-   *  Directory with the given termInfosIndexDivisor.
-   * @param directory the index directory
-   * @param termInfosIndexDivisor Subsamples which indexed
-   *  terms are loaded into RAM. This has the same effect as {@link
-   *  IndexWriterConfig#setTermIndexInterval} except that setting
-   *  must be done at indexing time while this setting can be
-   *  set per reader.  When set to N, then one in every
-   *  N*termIndexInterval terms in the index is loaded into
-   *  memory.  By setting this to a value > 1 you can reduce
-   *  memory usage, at the expense of higher latency when
-   *  loading a TermInfo.  The default value is 1.  Set this
-   *  to -1 to skip loading the terms index entirely.
-   * @throws CorruptIndexException if the index is corrupt
-   * @throws IOException if there is a low-level IO error
-   * @deprecated Use {@link DirectoryReader#open(Directory,int)}
-   */
-  @Deprecated
-  public static DirectoryReader open(final Directory directory, int termInfosIndexDivisor) throws CorruptIndexException, IOException {
-    return DirectoryReader.open(directory, termInfosIndexDivisor);
-  }
-  
-  /**
-   * Open a near real time IndexReader from the {@link org.apache.lucene.index.IndexWriter}.
-   *
-   * @param writer The IndexWriter to open from
-   * @param applyAllDeletes If true, all buffered deletes will
-   * be applied (made visible) in the returned reader.  If
-   * false, the deletes are not applied but remain buffered
-   * (in IndexWriter) so that they will be applied in the
-   * future.  Applying deletes can be costly, so if your app
-   * can tolerate deleted documents being returned you might
-   * gain some performance by passing false.
-   * @return The new IndexReader
-   * @throws CorruptIndexException
-   * @throws IOException if there is a low-level IO error
-   *
-   * @see DirectoryReader#openIfChanged(DirectoryReader,IndexWriter,boolean)
-   *
-   * @lucene.experimental
-   * @deprecated Use {@link DirectoryReader#open(IndexWriter,boolean)}
-   */
-  @Deprecated
-  public static DirectoryReader open(final IndexWriter writer, boolean applyAllDeletes) throws CorruptIndexException, IOException {
-    return DirectoryReader.open(writer, applyAllDeletes);
-  }
-
-  /** Expert: returns an IndexReader reading the index in the given
-   *  {@link IndexCommit}.
-   * @param commit the commit point to open
-   * @throws CorruptIndexException if the index is corrupt
-   * @throws IOException if there is a low-level IO error
-   * @deprecated Use {@link DirectoryReader#open(IndexCommit)}
-   */
-  @Deprecated
-  public static DirectoryReader open(final IndexCommit commit) throws CorruptIndexException, IOException {
-    return DirectoryReader.open(commit);
-  }
-
-
-  /** Expert: returns an IndexReader reading the index in the given
-   *  {@link IndexCommit} and termInfosIndexDivisor.
-   * @param commit the commit point to open
-   * @param termInfosIndexDivisor Subsamples which indexed
-   *  terms are loaded into RAM. This has the same effect as {@link
-   *  IndexWriterConfig#setTermIndexInterval} except that setting
-   *  must be done at indexing time while this setting can be
-   *  set per reader.  When set to N, then one in every
-   *  N*termIndexInterval terms in the index is loaded into
-   *  memory.  By setting this to a value > 1 you can reduce
-   *  memory usage, at the expense of higher latency when
-   *  loading a TermInfo.  The default value is 1.  Set this
-   *  to -1 to skip loading the terms index entirely.
-   * @throws CorruptIndexException if the index is corrupt
-   * @throws IOException if there is a low-level IO error
-   * @deprecated Use {@link DirectoryReader#open(IndexCommit,int)}
-   */
-  @Deprecated
-  public static DirectoryReader open(final IndexCommit commit, int termInfosIndexDivisor) throws CorruptIndexException, IOException {
-    return DirectoryReader.open(commit, termInfosIndexDivisor);
-  }
 
   /** Retrieve term vectors for this document, or null if
    *  term vectors were not indexed.  The returned Fields
diff --git a/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java b/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
index cec87d1..e89b6e1 100644
--- a/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
@@ -3859,7 +3859,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit {
     directory.makeLock(IndexWriter.WRITE_LOCK_NAME).release();
   }
 
-  /** If {@link IndexReader#open(IndexWriter,boolean)} has
+  /** If {@link DirectoryReader#open(IndexWriter,boolean)} has
    *  been called (ie, this writer is in near real-time
    *  mode), then after a merge completes, this class can be
    *  invoked to warm the reader on the newly merged
diff --git a/lucene/core/src/java/org/apache/lucene/index/IndexWriterConfig.java b/lucene/core/src/java/org/apache/lucene/index/IndexWriterConfig.java
index 41c3512..e14937f 100644
--- a/lucene/core/src/java/org/apache/lucene/index/IndexWriterConfig.java
+++ b/lucene/core/src/java/org/apache/lucene/index/IndexWriterConfig.java
@@ -161,12 +161,11 @@ public final class IndexWriterConfig implements Cloneable {
   /**
    * Creates a new config that with defaults that match the specified
    * {@link Version} as well as the default {@link
-   * Analyzer}. If matchVersion is >= {@link
-   * Version#LUCENE_32}, {@link TieredMergePolicy} is used
-   * for merging; else {@link LogByteSizeMergePolicy}.
+   * Analyzer}. By default, {@link TieredMergePolicy} is used
+   * for merging;
    * Note that {@link TieredMergePolicy} is free to select
    * non-contiguous merges, which means docIDs may not
-   * remain montonic over time.  If this is a problem you
+   * remain monotonic over time.  If this is a problem you
    * should switch to {@link LogByteSizeMergePolicy} or
    * {@link LogDocMergePolicy}.
    */
@@ -187,11 +186,7 @@ public final class IndexWriterConfig implements Cloneable {
     mergedSegmentWarmer = null;
     codec = Codec.getDefault();
     infoStream = InfoStream.getDefault();
-    if (matchVersion.onOrAfter(Version.LUCENE_32)) {
-      mergePolicy = new TieredMergePolicy();
-    } else {
-      mergePolicy = new LogByteSizeMergePolicy();
-    }
+    mergePolicy = new TieredMergePolicy();
     flushPolicy = new FlushByRamOrCountsPolicy();
     readerPooling = DEFAULT_READER_POOLING;
     indexerThreadPool = new ThreadAffinityDocumentsWriterThreadPool(DEFAULT_MAX_THREAD_STATES);
diff --git a/lucene/core/src/java/org/apache/lucene/util/IndexableBinaryStringTools.java b/lucene/core/src/java/org/apache/lucene/util/IndexableBinaryStringTools.java
deleted file mode 100644
index fdb1e71..0000000
--- a/lucene/core/src/java/org/apache/lucene/util/IndexableBinaryStringTools.java
+++ /dev/null
@@ -1,241 +0,0 @@
-package org.apache.lucene.util;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute; // javadoc
-
-/**
- * Provides support for converting byte sequences to Strings and back again.
- * The resulting Strings preserve the original byte sequences' sort order.
- * <p/>
- * The Strings are constructed using a Base 8000h encoding of the original
- * binary data - each char of an encoded String represents a 15-bit chunk
- * from the byte sequence.  Base 8000h was chosen because it allows for all
- * lower 15 bits of char to be used without restriction; the surrogate range 
- * [U+D8000-U+DFFF] does not represent valid chars, and would require
- * complicated handling to avoid them and allow use of char's high bit.
- * <p/>
- * Although unset bits are used as padding in the final char, the original
- * byte sequence could contain trailing bytes with no set bits (null bytes):
- * padding is indistinguishable from valid information.  To overcome this
- * problem, a char is appended, indicating the number of encoded bytes in the
- * final content char.
- * <p/>
- *
- * @lucene.experimental
- * @deprecated Implement {@link TermToBytesRefAttribute} and store bytes directly
- * instead. This class will be removed in Lucene 5.0
- */
-@Deprecated
-public final class IndexableBinaryStringTools {
-
-  private static final CodingCase[] CODING_CASES = {
-    // CodingCase(int initialShift, int finalShift)
-    new CodingCase( 7, 1   ),
-    // CodingCase(int initialShift, int middleShift, int finalShift)
-    new CodingCase(14, 6, 2),
-    new CodingCase(13, 5, 3),
-    new CodingCase(12, 4, 4),
-    new CodingCase(11, 3, 5),
-    new CodingCase(10, 2, 6),
-    new CodingCase( 9, 1, 7),
-    new CodingCase( 8, 0   )
-  };
-
-  // Export only static methods
-  private IndexableBinaryStringTools() {}
-
-  /**
-   * Returns the number of chars required to encode the given bytes.
-   * 
-   * @param inputArray byte sequence to be encoded
-   * @param inputOffset initial offset into inputArray
-   * @param inputLength number of bytes in inputArray
-   * @return The number of chars required to encode the number of bytes.
-   */
-  public static int getEncodedLength(byte[] inputArray, int inputOffset,
-      int inputLength) {
-    // Use long for intermediaries to protect against overflow
-    return (int)((8L * inputLength + 14L) / 15L) + 1;
-  }
-
-  /**
-   * Returns the number of bytes required to decode the given char sequence.
-   * 
-   * @param encoded char sequence to be decoded
-   * @param offset initial offset
-   * @param length number of characters
-   * @return The number of bytes required to decode the given char sequence
-   */
-  public static int getDecodedLength(char[] encoded, int offset, int length) {
-    final int numChars = length - 1;
-    if (numChars <= 0) {
-      return 0;
-    } else {
-      // Use long for intermediaries to protect against overflow
-      final long numFullBytesInFinalChar = encoded[offset + length - 1];
-      final long numEncodedChars = numChars - 1;
-      return (int)((numEncodedChars * 15L + 7L) / 8L + numFullBytesInFinalChar);
-    }
-  }
-
-  /**
-   * Encodes the input byte sequence into the output char sequence.  Before
-   * calling this method, ensure that the output array has sufficient
-   * capacity by calling {@link #getEncodedLength(byte[], int, int)}.
-   * 
-   * @param inputArray byte sequence to be encoded
-   * @param inputOffset initial offset into inputArray
-   * @param inputLength number of bytes in inputArray
-   * @param outputArray char sequence to store encoded result
-   * @param outputOffset initial offset into outputArray
-   * @param outputLength length of output, must be getEncodedLength
-   */
-  public static void encode(byte[] inputArray, int inputOffset,
-      int inputLength, char[] outputArray, int outputOffset, int outputLength) {
-    assert (outputLength == getEncodedLength(inputArray, inputOffset,
-        inputLength));
-    if (inputLength > 0) {
-      int inputByteNum = inputOffset;
-      int caseNum = 0;
-      int outputCharNum = outputOffset;
-      CodingCase codingCase;
-      for (; inputByteNum + CODING_CASES[caseNum].numBytes <= inputLength; ++outputCharNum) {
-        codingCase = CODING_CASES[caseNum];
-        if (2 == codingCase.numBytes) {
-          outputArray[outputCharNum] = (char) (((inputArray[inputByteNum] & 0xFF) << codingCase.initialShift)
-              + (((inputArray[inputByteNum + 1] & 0xFF) >>> codingCase.finalShift) & codingCase.finalMask) & (short) 0x7FFF);
-        } else { // numBytes is 3
-          outputArray[outputCharNum] = (char) (((inputArray[inputByteNum] & 0xFF) << codingCase.initialShift)
-              + ((inputArray[inputByteNum + 1] & 0xFF) << codingCase.middleShift)
-              + (((inputArray[inputByteNum + 2] & 0xFF) >>> codingCase.finalShift) & codingCase.finalMask) & (short) 0x7FFF);
-        }
-        inputByteNum += codingCase.advanceBytes;
-        if (++caseNum == CODING_CASES.length) {
-          caseNum = 0;
-        }
-      }
-      // Produce final char (if any) and trailing count chars.
-      codingCase = CODING_CASES[caseNum];
-
-      if (inputByteNum + 1 < inputLength) { // codingCase.numBytes must be 3
-        outputArray[outputCharNum++] = (char) ((((inputArray[inputByteNum] & 0xFF) << codingCase.initialShift) + ((inputArray[inputByteNum + 1] & 0xFF) << codingCase.middleShift)) & (short) 0x7FFF);
-        // Add trailing char containing the number of full bytes in final char
-        outputArray[outputCharNum++] = (char) 1;
-      } else if (inputByteNum < inputLength) {
-        outputArray[outputCharNum++] = (char) (((inputArray[inputByteNum] & 0xFF) << codingCase.initialShift) & (short) 0x7FFF);
-        // Add trailing char containing the number of full bytes in final char
-        outputArray[outputCharNum++] = caseNum == 0 ? (char) 1 : (char) 0;
-      } else { // No left over bits - last char is completely filled.
-        // Add trailing char containing the number of full bytes in final char
-        outputArray[outputCharNum++] = (char) 1;
-      }
-    }
-  }
-
-  /**
-   * Decodes the input char sequence into the output byte sequence. Before
-   * calling this method, ensure that the output array has sufficient capacity
-   * by calling {@link #getDecodedLength(char[], int, int)}.
-   * 
-   * @param inputArray char sequence to be decoded
-   * @param inputOffset initial offset into inputArray
-   * @param inputLength number of chars in inputArray
-   * @param outputArray byte sequence to store encoded result
-   * @param outputOffset initial offset into outputArray
-   * @param outputLength length of output, must be
-   *        getDecodedLength(inputArray, inputOffset, inputLength)
-   */
-  public static void decode(char[] inputArray, int inputOffset,
-      int inputLength, byte[] outputArray, int outputOffset, int outputLength) {
-    assert (outputLength == getDecodedLength(inputArray, inputOffset,
-        inputLength));
-    final int numInputChars = inputLength - 1;
-    final int numOutputBytes = outputLength;
-
-    if (numOutputBytes > 0) {
-      int caseNum = 0;
-      int outputByteNum = outputOffset;
-      int inputCharNum = inputOffset;
-      short inputChar;
-      CodingCase codingCase;
-      for (; inputCharNum < numInputChars - 1; ++inputCharNum) {
-        codingCase = CODING_CASES[caseNum];
-        inputChar = (short) inputArray[inputCharNum];
-        if (2 == codingCase.numBytes) {
-          if (0 == caseNum) {
-            outputArray[outputByteNum] = (byte) (inputChar >>> codingCase.initialShift);
-          } else {
-            outputArray[outputByteNum] += (byte) (inputChar >>> codingCase.initialShift);
-          }
-          outputArray[outputByteNum + 1] = (byte) ((inputChar & codingCase.finalMask) << codingCase.finalShift);
-        } else { // numBytes is 3
-          outputArray[outputByteNum] += (byte) (inputChar >>> codingCase.initialShift);
-          outputArray[outputByteNum + 1] = (byte) ((inputChar & codingCase.middleMask) >>> codingCase.middleShift);
-          outputArray[outputByteNum + 2] = (byte) ((inputChar & codingCase.finalMask) << codingCase.finalShift);
-        }
-        outputByteNum += codingCase.advanceBytes;
-        if (++caseNum == CODING_CASES.length) {
-          caseNum = 0;
-        }
-      }
-      // Handle final char
-      inputChar = (short) inputArray[inputCharNum];
-      codingCase = CODING_CASES[caseNum];
-      if (0 == caseNum) {
-        outputArray[outputByteNum] = 0;
-      }
-      outputArray[outputByteNum] += (byte) (inputChar >>> codingCase.initialShift);
-      final int bytesLeft = numOutputBytes - outputByteNum;
-      if (bytesLeft > 1) {
-        if (2 == codingCase.numBytes) {
-          outputArray[outputByteNum + 1] = (byte) ((inputChar & codingCase.finalMask) >>> codingCase.finalShift);
-        } else { // numBytes is 3
-          outputArray[outputByteNum + 1] = (byte) ((inputChar & codingCase.middleMask) >>> codingCase.middleShift);
-          if (bytesLeft > 2) {
-            outputArray[outputByteNum + 2] = (byte) ((inputChar & codingCase.finalMask) << codingCase.finalShift);
-          }
-        }
-      }
-    }
-  }
-
-  static class CodingCase {
-    int numBytes, initialShift, middleShift, finalShift, advanceBytes = 2;
-    short middleMask, finalMask;
-
-    CodingCase(int initialShift, int middleShift, int finalShift) {
-      this.numBytes = 3;
-      this.initialShift = initialShift;
-      this.middleShift = middleShift;
-      this.finalShift = finalShift;
-      this.finalMask = (short)((short)0xFF >>> finalShift);
-      this.middleMask = (short)((short)0xFF << middleShift);
-    }
-
-    CodingCase(int initialShift, int finalShift) {
-      this.numBytes = 2;
-      this.initialShift = initialShift;
-      this.finalShift = finalShift;
-      this.finalMask = (short)((short)0xFF >>> finalShift);
-      if (finalShift != 0) {
-        advanceBytes = 1; 
-      }
-    }
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/util/Version.java b/lucene/core/src/java/org/apache/lucene/util/Version.java
index 002fda1..f12f7f6 100644
--- a/lucene/core/src/java/org/apache/lucene/util/Version.java
+++ b/lucene/core/src/java/org/apache/lucene/util/Version.java
@@ -32,56 +32,7 @@ import java.util.Locale;
 // remove me when java 5 is no longer supported
 // this is a workaround for a JDK bug that wrongly emits a warning.
 @SuppressWarnings("dep-ann")
-public enum Version {
-  /**
-   * Match settings and bugs in Lucene's 3.0 release.
-   * @deprecated (4.0) Use latest
-   */
-  @Deprecated
-  LUCENE_30,
-
-  /**
-   * Match settings and bugs in Lucene's 3.1 release.
-   * @deprecated (4.0) Use latest
-   */
-  @Deprecated
-  LUCENE_31,
-  
-  /**
-   * Match settings and bugs in Lucene's 3.2 release.
-   * @deprecated (4.0) Use latest
-   */
-  @Deprecated
-  LUCENE_32,
-  
-  /**
-   * Match settings and bugs in Lucene's 3.3 release.
-   * @deprecated (4.0) Use latest
-   */
-  @Deprecated
-  LUCENE_33,
-  
-  /**
-   * Match settings and bugs in Lucene's 3.4 release.
-   * @deprecated (4.0) Use latest
-   */
-  @Deprecated
-  LUCENE_34,
-  
-  /**
-   * Match settings and bugs in Lucene's 3.5 release.
-   * @deprecated (4.0) Use latest
-   */
-  @Deprecated
-  LUCENE_35,
-  
-  /**
-   * Match settings and bugs in Lucene's 3.6 release.
-   * @deprecated (4.0) Use latest
-   */
-  @Deprecated
-  LUCENE_36,
-  
+public enum Version { 
   /**
    * Match settings and bugs in Lucene's 4.0 release.
    * @deprecated (5.0) Use latest
diff --git a/lucene/core/src/test/org/apache/lucene/TestDemo.java b/lucene/core/src/test/org/apache/lucene/TestDemo.java
index ebc608c..da5d642 100644
--- a/lucene/core/src/test/org/apache/lucene/TestDemo.java
+++ b/lucene/core/src/test/org/apache/lucene/TestDemo.java
@@ -23,6 +23,7 @@ import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.RandomIndexWriter;
@@ -54,7 +55,7 @@ public class TestDemo extends LuceneTestCase {
     iwriter.close();
     
     // Now search the index:
-    IndexReader ireader = IndexReader.open(directory); // read-only=true
+    IndexReader ireader = DirectoryReader.open(directory); // read-only=true
     IndexSearcher isearcher = new IndexSearcher(ireader);
 
     assertEquals(1, isearcher.search(new TermQuery(new Term("fieldname", longTerm)), 1).totalHits);
diff --git a/lucene/core/src/test/org/apache/lucene/TestExternalCodecs.java b/lucene/core/src/test/org/apache/lucene/TestExternalCodecs.java
index 6f1d6be..abc1c26 100644
--- a/lucene/core/src/test/org/apache/lucene/TestExternalCodecs.java
+++ b/lucene/core/src/test/org/apache/lucene/TestExternalCodecs.java
@@ -89,7 +89,7 @@ public class TestExternalCodecs extends LuceneTestCase {
     }
     w.deleteDocuments(new Term("id", "77"));
 
-    IndexReader r = IndexReader.open(w, true);
+    IndexReader r = DirectoryReader.open(w, true);
     
     assertEquals(NUM_DOCS-1, r.numDocs());
     IndexSearcher s = newSearcher(r);
@@ -109,7 +109,7 @@ public class TestExternalCodecs extends LuceneTestCase {
     if (VERBOSE) {
       System.out.println("\nTEST: now open reader");
     }
-    r = IndexReader.open(w, true);
+    r = DirectoryReader.open(w, true);
     assertEquals(NUM_DOCS-2, r.maxDoc());
     assertEquals(NUM_DOCS-2, r.numDocs());
     s = newSearcher(r);
diff --git a/lucene/core/src/test/org/apache/lucene/TestSearch.java b/lucene/core/src/test/org/apache/lucene/TestSearch.java
index fdae73b..c609c4a 100644
--- a/lucene/core/src/test/org/apache/lucene/TestSearch.java
+++ b/lucene/core/src/test/org/apache/lucene/TestSearch.java
@@ -92,7 +92,7 @@ public class TestSearch extends LuceneTestCase {
       }
       writer.close();
 
-      IndexReader reader = IndexReader.open(directory);
+      IndexReader reader = DirectoryReader.open(directory);
       IndexSearcher searcher = new IndexSearcher(reader);
 
       ScoreDoc[] hits = null;
diff --git a/lucene/core/src/test/org/apache/lucene/TestSearchForDuplicates.java b/lucene/core/src/test/org/apache/lucene/TestSearchForDuplicates.java
index 6f78f84..cd7b367 100644
--- a/lucene/core/src/test/org/apache/lucene/TestSearchForDuplicates.java
+++ b/lucene/core/src/test/org/apache/lucene/TestSearchForDuplicates.java
@@ -89,7 +89,7 @@ public class TestSearchForDuplicates extends LuceneTestCase {
       writer.close();
 
       // try a search without OR
-      IndexReader reader = IndexReader.open(directory);
+      IndexReader reader = DirectoryReader.open(directory);
       IndexSearcher searcher = new IndexSearcher(reader);
 
       Query query = new TermQuery(new Term(PRIORITY_FIELD, HIGH_PRIORITY));
diff --git a/lucene/core/src/test/org/apache/lucene/codecs/appending/TestAppendingCodec.java b/lucene/core/src/test/org/apache/lucene/codecs/appending/TestAppendingCodec.java
index c6ad9e7..422d932 100644
--- a/lucene/core/src/test/org/apache/lucene/codecs/appending/TestAppendingCodec.java
+++ b/lucene/core/src/test/org/apache/lucene/codecs/appending/TestAppendingCodec.java
@@ -25,6 +25,7 @@ import org.apache.lucene.codecs.appending.AppendingCodec;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.Fields;
 import org.apache.lucene.index.IndexReader;
@@ -125,7 +126,7 @@ public class TestAppendingCodec extends LuceneTestCase {
     writer.addDocument(doc);
     writer.forceMerge(1);
     writer.close();
-    IndexReader reader = IndexReader.open(dir, 1);
+    IndexReader reader = DirectoryReader.open(dir, 1);
     assertEquals(2, reader.numDocs());
     Document doc2 = reader.document(0);
     assertEquals(text, doc2.get("f"));
diff --git a/lucene/core/src/test/org/apache/lucene/codecs/perfield/TestPerFieldPostingsFormat.java b/lucene/core/src/test/org/apache/lucene/codecs/perfield/TestPerFieldPostingsFormat.java
index 9292202..1f719b7 100644
--- a/lucene/core/src/test/org/apache/lucene/codecs/perfield/TestPerFieldPostingsFormat.java
+++ b/lucene/core/src/test/org/apache/lucene/codecs/perfield/TestPerFieldPostingsFormat.java
@@ -32,6 +32,7 @@ import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -193,7 +194,7 @@ public class TestPerFieldPostingsFormat extends LuceneTestCase {
     if (VERBOSE) {
       System.out.println("\nTEST: assertQuery " + t);
     }
-    IndexReader reader = IndexReader.open(dir, 1);
+    IndexReader reader = DirectoryReader.open(dir, 1);
     IndexSearcher searcher = newSearcher(reader);
     TopDocs search = searcher.search(new TermQuery(t), num + 10);
     assertEquals(num, search.totalHits);
diff --git a/lucene/core/src/test/org/apache/lucene/document/TestDocument.java b/lucene/core/src/test/org/apache/lucene/document/TestDocument.java
index 2e54a6a..a2f31db 100644
--- a/lucene/core/src/test/org/apache/lucene/document/TestDocument.java
+++ b/lucene/core/src/test/org/apache/lucene/document/TestDocument.java
@@ -296,71 +296,6 @@ public class TestDocument extends LuceneTestCase {
     }
   }
 
-  // LUCENE-3682
-  public void testTransitionAPI() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir);
-
-    Document doc = new Document();
-    doc.add(new Field("stored", "abc", Field.Store.YES, Field.Index.NO));
-    doc.add(new Field("stored_indexed", "abc xyz", Field.Store.YES, Field.Index.NOT_ANALYZED));
-    doc.add(new Field("stored_tokenized", "abc xyz", Field.Store.YES, Field.Index.ANALYZED));
-    doc.add(new Field("indexed", "abc xyz", Field.Store.NO, Field.Index.NOT_ANALYZED));
-    doc.add(new Field("tokenized", "abc xyz", Field.Store.NO, Field.Index.ANALYZED));
-    doc.add(new Field("tokenized_reader", new StringReader("abc xyz")));
-    doc.add(new Field("tokenized_tokenstream", w.w.getAnalyzer().tokenStream("tokenized_tokenstream", new StringReader("abc xyz"))));
-    doc.add(new Field("binary", new byte[10]));
-    doc.add(new Field("tv", "abc xyz", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));
-    doc.add(new Field("tv_pos", "abc xyz", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));
-    doc.add(new Field("tv_off", "abc xyz", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));
-    doc.add(new Field("tv_pos_off", "abc xyz", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
-    w.addDocument(doc);
-    IndexReader r = w.getReader();
-    w.close();
-
-    doc = r.document(0);
-    // 4 stored fields
-    assertEquals(4, doc.getFields().size());
-    assertEquals("abc", doc.get("stored"));
-    assertEquals("abc xyz", doc.get("stored_indexed"));
-    assertEquals("abc xyz", doc.get("stored_tokenized"));
-    final BytesRef br = doc.getBinaryValue("binary");
-    assertNotNull(br);
-    assertEquals(10, br.length);
-
-    IndexSearcher s = new IndexSearcher(r);
-    assertEquals(1, s.search(new TermQuery(new Term("stored_indexed", "abc xyz")), 1).totalHits);
-    assertEquals(1, s.search(new TermQuery(new Term("stored_tokenized", "abc")), 1).totalHits);
-    assertEquals(1, s.search(new TermQuery(new Term("stored_tokenized", "xyz")), 1).totalHits);
-    assertEquals(1, s.search(new TermQuery(new Term("indexed", "abc xyz")), 1).totalHits);
-    assertEquals(1, s.search(new TermQuery(new Term("tokenized", "abc")), 1).totalHits);
-    assertEquals(1, s.search(new TermQuery(new Term("tokenized", "xyz")), 1).totalHits);
-    assertEquals(1, s.search(new TermQuery(new Term("tokenized_reader", "abc")), 1).totalHits);
-    assertEquals(1, s.search(new TermQuery(new Term("tokenized_reader", "xyz")), 1).totalHits);
-    assertEquals(1, s.search(new TermQuery(new Term("tokenized_tokenstream", "abc")), 1).totalHits);
-    assertEquals(1, s.search(new TermQuery(new Term("tokenized_tokenstream", "xyz")), 1).totalHits);
-
-    for(String field : new String[] {"tv", "tv_pos", "tv_off", "tv_pos_off"}) {
-      Fields tvFields = r.getTermVectors(0);
-      Terms tvs = tvFields.terms(field);
-      assertNotNull(tvs);
-      assertEquals(2, tvs.size());
-      TermsEnum tvsEnum = tvs.iterator(null);
-      assertEquals(new BytesRef("abc"), tvsEnum.next());
-      final DocsAndPositionsEnum dpEnum = tvsEnum.docsAndPositions(null, null, false);
-      if (field.equals("tv")) {
-        assertNull(dpEnum);
-      } else {
-        assertNotNull(dpEnum);
-      }
-      assertEquals(new BytesRef("xyz"), tvsEnum.next());
-      assertNull(tvsEnum.next());
-    }
-
-    r.close();
-    dir.close();
-  }
-  
   public void testBoost() throws Exception {
     Directory dir = newDirectory();
     IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
diff --git a/lucene/core/src/test/org/apache/lucene/index/Test2BTerms.java b/lucene/core/src/test/org/apache/lucene/index/Test2BTerms.java
index d8f19a8..f20dfcd 100644
--- a/lucene/core/src/test/org/apache/lucene/index/Test2BTerms.java
+++ b/lucene/core/src/test/org/apache/lucene/index/Test2BTerms.java
@@ -196,7 +196,7 @@ public class Test2BTerms extends LuceneTestCase {
     }
 
     System.out.println("TEST: open reader");
-    final IndexReader r = IndexReader.open(dir);
+    final IndexReader r = DirectoryReader.open(dir);
     if (savedTerms == null) {
       savedTerms = findTerms(r);
     }
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestAddIndexes.java b/lucene/core/src/test/org/apache/lucene/index/TestAddIndexes.java
index 6c261dd..51afd27 100755
--- a/lucene/core/src/test/org/apache/lucene/index/TestAddIndexes.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestAddIndexes.java
@@ -433,7 +433,7 @@ public class TestAddIndexes extends LuceneTestCase {
       writer.deleteDocuments(new Term("id", "" + i));
     }
     writer.close();
-    IndexReader reader = IndexReader.open(aux);
+    IndexReader reader = DirectoryReader.open(aux);
     assertEquals(10, reader.numDocs());
     reader.close();
 
@@ -485,7 +485,7 @@ public class TestAddIndexes extends LuceneTestCase {
       writer.deleteDocuments(new Term("id", "" + i));
     }
     writer.close();
-    IndexReader reader = IndexReader.open(aux);
+    IndexReader reader = DirectoryReader.open(aux);
     assertEquals(3, reader.numDocs());
     reader.close();
 
@@ -496,7 +496,7 @@ public class TestAddIndexes extends LuceneTestCase {
       writer.deleteDocuments(new Term("id", "" + i));
     }
     writer.close();
-    reader = IndexReader.open(aux2);
+    reader = DirectoryReader.open(aux2);
     assertEquals(22, reader.numDocs());
     reader.close();
 
@@ -541,7 +541,7 @@ public class TestAddIndexes extends LuceneTestCase {
   }
 
   private void verifyNumDocs(Directory dir, int numDocs) throws IOException {
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     assertEquals(numDocs, reader.maxDoc());
     assertEquals(numDocs, reader.numDocs());
     reader.close();
@@ -549,7 +549,7 @@ public class TestAddIndexes extends LuceneTestCase {
 
   private void verifyTermDocs(Directory dir, Term term, int numDocs)
       throws IOException {
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     DocsEnum docsEnum = _TestUtil.docs(random(), reader, term.field, term.bytes, null, null, false);
     int count = 0;
     while (docsEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS)
@@ -687,7 +687,7 @@ public class TestAddIndexes extends LuceneTestCase {
 
       readers = new IndexReader[NUM_COPY];
       for(int i=0;i<NUM_COPY;i++)
-        readers[i] = IndexReader.open(dir);
+        readers[i] = DirectoryReader.open(dir);
     }
 
     void launchThreads(final int numIter) {
@@ -813,7 +813,7 @@ public class TestAddIndexes extends LuceneTestCase {
 
     assertTrue("found unexpected failures: " + c.failures, c.failures.isEmpty());
 
-    IndexReader reader = IndexReader.open(c.dir2);
+    IndexReader reader = DirectoryReader.open(c.dir2);
     assertEquals(expectedNumDocs, reader.numDocs());
     reader.close();
 
@@ -984,7 +984,7 @@ public class TestAddIndexes extends LuceneTestCase {
 
     // Now delete the document
     writer.deleteDocuments(new Term("id", "myid"));
-    IndexReader r = IndexReader.open(dirs[1]);
+    IndexReader r = DirectoryReader.open(dirs[1]);
     try {
       writer.addIndexes(r);
     } finally {
@@ -1103,7 +1103,7 @@ public class TestAddIndexes extends LuceneTestCase {
       w.close();
     }
     
-    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };
+    IndexReader[] readers = new IndexReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };
     
     Directory dir = new MockDirectoryWrapper(random(), new RAMDirectory());
     IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy());
@@ -1199,14 +1199,14 @@ public class TestAddIndexes extends LuceneTestCase {
         // expected
       }
       w.close();
-      IndexReader open = IndexReader.open(dir);
+      IndexReader open = DirectoryReader.open(dir);
       assertEquals(0, open.numDocs());
       open.close();
       dir.close();
     }
 
     try {
-      IndexReader.open(toAdd);
+      DirectoryReader.open(toAdd);
       fail("no such codec");
     } catch (IllegalArgumentException ex) {
       // expected
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestAtomicUpdate.java b/lucene/core/src/test/org/apache/lucene/index/TestAtomicUpdate.java
index b2c4086..1130c13 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestAtomicUpdate.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestAtomicUpdate.java
@@ -106,7 +106,7 @@ public class TestAtomicUpdate extends LuceneTestCase {
 
     @Override
     public void doWork() throws Throwable {
-      IndexReader r = IndexReader.open(directory);
+      IndexReader r = DirectoryReader.open(directory);
       assertEquals(100, r.numDocs());
       r.close();
     }
@@ -138,7 +138,7 @@ public class TestAtomicUpdate extends LuceneTestCase {
     }
     writer.commit();
 
-    IndexReader r = IndexReader.open(directory);
+    IndexReader r = DirectoryReader.open(directory);
     assertEquals(100, r.numDocs());
     r.close();
 
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java b/lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
index 42ff838..454f26d 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
@@ -171,8 +171,8 @@ public class TestBackwardsCompatibility extends LuceneTestCase {
       IndexReader reader = null;
       IndexWriter writer = null;
       try {
-        reader = IndexReader.open(dir);
-        fail("IndexReader.open should not pass for "+unsupportedNames[i]);
+        reader = DirectoryReader.open(dir);
+        fail("DirectoryReader.open should not pass for "+unsupportedNames[i]);
       } catch (IndexFormatTooOldException e) {
         // pass
       } finally {
@@ -251,7 +251,7 @@ public class TestBackwardsCompatibility extends LuceneTestCase {
 
   public void testAddOldIndexesReader() throws IOException {
     for (String name : oldNames) {
-      IndexReader reader = IndexReader.open(oldIndexDirs.get(name));
+      IndexReader reader = DirectoryReader.open(oldIndexDirs.get(name));
       
       Directory targetDir = newDirectory();
       IndexWriter w = new IndexWriter(targetDir, newIndexWriterConfig(
@@ -302,7 +302,7 @@ public class TestBackwardsCompatibility extends LuceneTestCase {
     //QueryParser parser = new QueryParser("contents", new MockAnalyzer(random));
     //Query query = parser.parse("handle:1");
 
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     IndexSearcher searcher = new IndexSearcher(reader);
 
     _TestUtil.checkIndex(dir);
@@ -385,7 +385,7 @@ public class TestBackwardsCompatibility extends LuceneTestCase {
     writer.close();
 
     // make sure searching sees right # hits
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     IndexSearcher searcher = new IndexSearcher(reader);
     ScoreDoc[] hits = searcher.search(new TermQuery(new Term("content", "aaa")), null, 1000).scoreDocs;
     Document d = searcher.getIndexReader().document(hits[0].doc);
@@ -398,7 +398,7 @@ public class TestBackwardsCompatibility extends LuceneTestCase {
     writer.forceMerge(1);
     writer.close();
 
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
     searcher = new IndexSearcher(reader);
     hits = searcher.search(new TermQuery(new Term("content", "aaa")), null, 1000).scoreDocs;
     assertEquals("wrong number of hits", 44, hits.length);
@@ -410,7 +410,7 @@ public class TestBackwardsCompatibility extends LuceneTestCase {
 
   public void changeIndexNoAdds(Random random, Directory dir) throws IOException {
     // make sure searching sees right # hits
-    IndexReader reader = IndexReader.open(dir);
+    DirectoryReader reader = DirectoryReader.open(dir);
     IndexSearcher searcher = new IndexSearcher(reader);
     ScoreDoc[] hits = searcher.search(new TermQuery(new Term("content", "aaa")), null, 1000).scoreDocs;
     assertEquals("wrong number of hits", 34, hits.length);
@@ -423,7 +423,7 @@ public class TestBackwardsCompatibility extends LuceneTestCase {
     writer.forceMerge(1);
     writer.close();
 
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
     searcher = new IndexSearcher(reader);
     hits = searcher.search(new TermQuery(new Term("content", "aaa")), null, 1000).scoreDocs;
     assertEquals("wrong number of hits", 34, hits.length);
@@ -599,7 +599,7 @@ public class TestBackwardsCompatibility extends LuceneTestCase {
   public void testNextIntoWrongField() throws Exception {
     for (String name : oldNames) {
       Directory dir = oldIndexDirs.get(name);
-      IndexReader r = IndexReader.open(dir);
+      IndexReader r = DirectoryReader.open(dir);
       TermsEnum terms = MultiFields.getFields(r).terms("content").iterator(null);
       BytesRef t = terms.next();
       assertNotNull(t);
@@ -641,7 +641,7 @@ public class TestBackwardsCompatibility extends LuceneTestCase {
     for (String name : oldNames) {
       
       Directory dir = oldIndexDirs.get(name);
-      IndexReader reader = IndexReader.open(dir);
+      IndexReader reader = DirectoryReader.open(dir);
       IndexSearcher searcher = new IndexSearcher(reader);
       
       for (int id=10; id<15; id++) {
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestCodecs.java b/lucene/core/src/test/org/apache/lucene/index/TestCodecs.java
index d53239a..78afd41 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestCodecs.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestCodecs.java
@@ -339,8 +339,9 @@ public class TestCodecs extends LuceneTestCase {
 
   public void testSepPositionAfterMerge() throws IOException {
     final Directory dir = newDirectory();
-    final IndexWriterConfig config = newIndexWriterConfig(Version.LUCENE_31,
+    final IndexWriterConfig config = newIndexWriterConfig(TEST_VERSION_CURRENT,
       new MockAnalyzer(random()));
+    config.setMergePolicy(newLogMergePolicy());
     config.setCodec(_TestUtil.alwaysPostingsFormat(new MockSepPostingsFormat()));
     final IndexWriter writer = new IndexWriter(dir, config);
 
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler.java b/lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler.java
index d921652..6622b9b 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler.java
@@ -113,7 +113,7 @@ public class TestConcurrentMergeScheduler extends LuceneTestCase {
     }
 
     writer.close();
-    IndexReader reader = IndexReader.open(directory);
+    IndexReader reader = DirectoryReader.open(directory);
     assertEquals(200+extraCount, reader.numDocs());
     reader.close();
     directory.close();
@@ -158,7 +158,7 @@ public class TestConcurrentMergeScheduler extends LuceneTestCase {
     }
 
     writer.close();
-    IndexReader reader = IndexReader.open(directory);
+    IndexReader reader = DirectoryReader.open(directory);
     // Verify that we did not lose any deletes...
     assertEquals(450, reader.numDocs());
     reader.close();
@@ -230,7 +230,7 @@ public class TestConcurrentMergeScheduler extends LuceneTestCase {
 
       writer.close(false);
 
-      IndexReader reader = IndexReader.open(directory);
+      IndexReader reader = DirectoryReader.open(directory);
       assertEquals((1+iter)*182, reader.numDocs());
       reader.close();
 
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestCrash.java b/lucene/core/src/test/org/apache/lucene/index/TestCrash.java
index a8b76b8..bf6bebf 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestCrash.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestCrash.java
@@ -68,7 +68,7 @@ public class TestCrash extends LuceneTestCase {
     IndexWriter writer = initIndex(random(), true);
     MockDirectoryWrapper dir = (MockDirectoryWrapper) writer.getDirectory();
     crash(writer);
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     assertTrue(reader.numDocs() < 157);
     reader.close();
     dir.close();
@@ -85,7 +85,7 @@ public class TestCrash extends LuceneTestCase {
     writer = initIndex(random(), dir, false);
     writer.close();
 
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     assertTrue(reader.numDocs() < 314);
     reader.close();
     dir.close();
@@ -108,7 +108,7 @@ public class TestCrash extends LuceneTestCase {
     dir.fileLength(l[i]) + " bytes");
     */
 
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     assertTrue(reader.numDocs() >= 157);
     reader.close();
     dir.close();
@@ -129,7 +129,7 @@ public class TestCrash extends LuceneTestCase {
       System.out.println("file " + i + " = " + l[i] + " " + dir.fileLength(l[i]) + " bytes");
     */
 
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     assertEquals(157, reader.numDocs());
     reader.close();
     dir.close();
@@ -150,7 +150,7 @@ public class TestCrash extends LuceneTestCase {
     for(int i=0;i<l.length;i++)
       System.out.println("file " + i + " = " + l[i] + " " + dir.fileLength(l[i]) + " bytes");
     */
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     assertEquals(157, reader.numDocs());
     reader.close();
     dir.close();
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestCrashCausesCorruptIndex.java b/lucene/core/src/test/org/apache/lucene/index/TestCrashCausesCorruptIndex.java
index ef265ec..dc16285 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestCrashCausesCorruptIndex.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestCrashCausesCorruptIndex.java
@@ -121,7 +121,7 @@ public class TestCrashCausesCorruptIndex extends LuceneTestCase  {
    */
   private void searchForFleas(final int expectedTotalHits) throws IOException {
     Directory realDirectory = newFSDirectory(path);
-    IndexReader indexReader = IndexReader.open(realDirectory);
+    IndexReader indexReader = DirectoryReader.open(realDirectory);
     IndexSearcher indexSearcher = newSearcher(indexReader);
     TopDocs topDocs = indexSearcher.search(new TermQuery(new Term(TEXT_FIELD, "fleas")), 10);
     assertNotNull(topDocs);
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestDeletionPolicy.java b/lucene/core/src/test/org/apache/lucene/index/TestDeletionPolicy.java
index 8ef6186..a769cce 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestDeletionPolicy.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestDeletionPolicy.java
@@ -263,7 +263,7 @@ public class TestDeletionPolicy extends LuceneTestCase {
 
     while(gen > 0) {
       try {
-        IndexReader reader = IndexReader.open(dir);
+        IndexReader reader = DirectoryReader.open(dir);
         reader.close();
         fileName = IndexFileNames.fileNameFromGeneration(IndexFileNames.SEGMENTS,
                                                          "",
@@ -357,7 +357,7 @@ public class TestDeletionPolicy extends LuceneTestCase {
 
       // Make sure we can open a reader on each commit:
       for (final IndexCommit commit : commits) {
-        IndexReader r = IndexReader.open(commit);
+        IndexReader r = DirectoryReader.open(commit);
         r.close();
       }
 
@@ -366,7 +366,7 @@ public class TestDeletionPolicy extends LuceneTestCase {
       dir.deleteFile(IndexFileNames.SEGMENTS_GEN);
       long gen = SegmentInfos.getLastCommitGeneration(dir);
       while(gen > 0) {
-        IndexReader reader = IndexReader.open(dir);
+        IndexReader reader = DirectoryReader.open(dir);
         reader.close();
         dir.deleteFile(IndexFileNames.fileNameFromGeneration(IndexFileNames.SEGMENTS, "", gen));
         gen--;
@@ -468,7 +468,7 @@ public class TestDeletionPolicy extends LuceneTestCase {
     writer.forceMerge(1);
     writer.close();
 
-    r = IndexReader.open(dir);
+    r = DirectoryReader.open(dir);
     assertEquals(1, r.getSequentialSubReaders().length);
     assertEquals(10, r.numDocs());
     r.close();
@@ -480,7 +480,7 @@ public class TestDeletionPolicy extends LuceneTestCase {
     
     // Reader still sees fully merged index, because writer
     // opened on the prior commit has not yet committed:
-    r = IndexReader.open(dir);
+    r = DirectoryReader.open(dir);
     assertEquals(1, r.getSequentialSubReaders().length);
     assertEquals(10, r.numDocs());
     r.close();
@@ -488,7 +488,7 @@ public class TestDeletionPolicy extends LuceneTestCase {
     writer.close();
 
     // Now reader sees not-fully-merged index:
-    r = IndexReader.open(dir);
+    r = DirectoryReader.open(dir);
     assertTrue(r.getSequentialSubReaders().length > 1);
     assertEquals(10, r.numDocs());
     r.close();
@@ -541,7 +541,7 @@ public class TestDeletionPolicy extends LuceneTestCase {
 
       // Simplistic check: just verify the index is in fact
       // readable:
-      IndexReader reader = IndexReader.open(dir);
+      IndexReader reader = DirectoryReader.open(dir);
       reader.close();
 
       dir.close();
@@ -589,7 +589,7 @@ public class TestDeletionPolicy extends LuceneTestCase {
       long gen = SegmentInfos.getLastCommitGeneration(dir);
       for(int i=0;i<N+1;i++) {
         try {
-          IndexReader reader = IndexReader.open(dir);
+          IndexReader reader = DirectoryReader.open(dir);
           reader.close();
           if (i == N) {
             fail("should have failed on commits prior to last " + N);
@@ -660,7 +660,7 @@ public class TestDeletionPolicy extends LuceneTestCase {
         writer.deleteDocuments(new Term("id", "" + (i*(N+1)+3)));
         // this is a commit
         writer.close();
-        IndexReader reader = IndexReader.open(dir);
+        IndexReader reader = DirectoryReader.open(dir);
         IndexSearcher searcher = newSearcher(reader);
         ScoreDoc[] hits = searcher.search(query, null, 1000).scoreDocs;
         assertEquals(16, hits.length);
@@ -677,7 +677,7 @@ public class TestDeletionPolicy extends LuceneTestCase {
       assertEquals(3*(N+1), policy.numOnInit);
       assertEquals(3*(N+1)+1, policy.numOnCommit);
 
-      IndexReader rwReader = IndexReader.open(dir);
+      IndexReader rwReader = DirectoryReader.open(dir);
       IndexSearcher searcher = new IndexSearcher(rwReader);
       ScoreDoc[] hits = searcher.search(query, null, 1000).scoreDocs;
       assertEquals(0, hits.length);
@@ -693,7 +693,7 @@ public class TestDeletionPolicy extends LuceneTestCase {
 
       for(int i=0;i<N+1;i++) {
         try {
-          IndexReader reader = IndexReader.open(dir);
+          IndexReader reader = DirectoryReader.open(dir);
 
           // Work backwards in commits on what the expected
           // count should be.
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter.java b/lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter.java
index eab5a63..e6fdc5f 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter.java
@@ -286,7 +286,7 @@ public class TestDocumentWriter extends LuceneTestCase {
 
     _TestUtil.checkIndex(dir);
 
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     // f1
     Terms tfv1 = reader.getTermVectors(0).terms("f1");
     assertNotNull(tfv1);
@@ -327,7 +327,7 @@ public class TestDocumentWriter extends LuceneTestCase {
 
     _TestUtil.checkIndex(dir);
 
-    SegmentReader reader = getOnlySegmentReader(IndexReader.open(dir));
+    SegmentReader reader = getOnlySegmentReader(DirectoryReader.open(dir));
     FieldInfos fi = reader.getFieldInfos();
     // f1
     assertFalse("f1 should have no norms", fi.fieldInfo("f1").hasNorms());
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestFieldsReader.java b/lucene/core/src/test/org/apache/lucene/index/TestFieldsReader.java
index c035c81..c5a959c 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestFieldsReader.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestFieldsReader.java
@@ -78,7 +78,7 @@ public class TestFieldsReader extends LuceneTestCase {
   public void test() throws IOException {
     assertTrue(dir != null);
     assertTrue(fieldInfos != null);
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     Document doc = reader.document(0);
     assertTrue(doc != null);
     assertTrue(doc.getField(DocHelper.TEXT_FIELD_1_KEY) != null);
@@ -203,7 +203,7 @@ public class TestFieldsReader extends LuceneTestCase {
       writer.forceMerge(1);
       writer.close();
 
-      IndexReader reader = IndexReader.open(dir);
+      IndexReader reader = DirectoryReader.open(dir);
 
       FaultyIndexInput.doFail = true;
 
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestFilterAtomicReader.java b/lucene/core/src/test/org/apache/lucene/index/TestFilterAtomicReader.java
index 55a3880..dc7425e 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestFilterAtomicReader.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestFilterAtomicReader.java
@@ -151,11 +151,11 @@ public class TestFilterAtomicReader extends LuceneTestCase {
     ((MockDirectoryWrapper) target).setCrossCheckTermVectorsOnClose(false);
 
     writer = new IndexWriter(target, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    IndexReader reader = new TestReader(IndexReader.open(directory));
+    IndexReader reader = new TestReader(DirectoryReader.open(directory));
     writer.addIndexes(reader);
     writer.close();
     reader.close();
-    reader = IndexReader.open(target);
+    reader = DirectoryReader.open(target);
     
     TermsEnum terms = MultiFields.getTerms(reader, "default").iterator(null);
     while (terms.next() != null) {
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestFlex.java b/lucene/core/src/test/org/apache/lucene/index/TestFlex.java
index 0c7be51..7edef1b 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestFlex.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestFlex.java
@@ -33,8 +33,8 @@ public class TestFlex extends LuceneTestCase {
 
     IndexWriter w = new IndexWriter(
         d,
-        new IndexWriterConfig(Version.LUCENE_31, new MockAnalyzer(random())).
-            setMaxBufferedDocs(7)
+        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).
+            setMaxBufferedDocs(7).setMergePolicy(newLogMergePolicy())
     );
 
     for(int iter=0;iter<2;iter++) {
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestFlushByRamOrCountsPolicy.java b/lucene/core/src/test/org/apache/lucene/index/TestFlushByRamOrCountsPolicy.java
index e613deb..dfc6189 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestFlushByRamOrCountsPolicy.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestFlushByRamOrCountsPolicy.java
@@ -218,7 +218,7 @@ public class TestFlushByRamOrCountsPolicy extends LuceneTestCase {
     assertActiveBytesAfter(flushControl);
     writer.commit();
     assertEquals(0, flushControl.activeBytes());
-    IndexReader r = IndexReader.open(dir);
+    IndexReader r = DirectoryReader.open(dir);
     assertEquals(numDocumentsToIndex, r.numDocs());
     assertEquals(numDocumentsToIndex, r.maxDoc());
     if (!flushPolicy.flushOnRAM()) {
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java
index e12236f..2ac94ef 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java
@@ -99,7 +99,7 @@ public class TestIndexWriter extends LuceneTestCase {
         }
         writer.close();
 
-        reader = IndexReader.open(dir);
+        reader = DirectoryReader.open(dir);
         assertEquals(60, reader.numDocs());
         reader.close();
 
@@ -112,7 +112,7 @@ public class TestIndexWriter extends LuceneTestCase {
         writer.close();
 
         // check that the index reader gives the same numbers.
-        reader = IndexReader.open(dir);
+        reader = DirectoryReader.open(dir);
         assertEquals(60, reader.maxDoc());
         assertEquals(60, reader.numDocs());
         reader.close();
@@ -179,7 +179,7 @@ public class TestIndexWriter extends LuceneTestCase {
       writer.close();
 
       // now open reader:
-      IndexReader reader = IndexReader.open(dir);
+      IndexReader reader = DirectoryReader.open(dir);
       assertEquals("should be one document", reader.numDocs(), 1);
 
       // now open index for create:
@@ -189,7 +189,7 @@ public class TestIndexWriter extends LuceneTestCase {
       writer.close();
 
       assertEquals("should be one document", reader.numDocs(), 1);
-      IndexReader reader2 = IndexReader.open(dir);
+      IndexReader reader2 = DirectoryReader.open(dir);
       assertEquals("should be one document", reader2.numDocs(), 1);
       reader.close();
       reader2.close();
@@ -224,7 +224,7 @@ public class TestIndexWriter extends LuceneTestCase {
       writer.commit();
       writer.close();
 
-      IndexReader reader = IndexReader.open(dir);
+      IndexReader reader = DirectoryReader.open(dir);
       assertEquals(0, reader.maxDoc());
       assertEquals(0, reader.numDocs());
       reader.close();
@@ -233,7 +233,7 @@ public class TestIndexWriter extends LuceneTestCase {
       writer.commit();
       writer.close();
 
-      reader = IndexReader.open(dir);
+      reader = DirectoryReader.open(dir);
       assertEquals(0, reader.maxDoc());
       assertEquals(0, reader.numDocs());
       reader.close();
@@ -255,7 +255,7 @@ public class TestIndexWriter extends LuceneTestCase {
       }
       writer.close();
 
-      IndexReader reader = IndexReader.open(dir);
+      IndexReader reader = DirectoryReader.open(dir);
       assertEquals(100, reader.maxDoc());
       assertEquals(100, reader.numDocs());
       for(int j=0;j<100;j++) {
@@ -450,7 +450,7 @@ public class TestIndexWriter extends LuceneTestCase {
       }
       writer.close();
 
-      IndexReader reader = IndexReader.open(dir);
+      IndexReader reader = DirectoryReader.open(dir);
       IndexSearcher searcher = new IndexSearcher(reader);
       int totalHits = searcher.search(new TermQuery(new Term("field", "aaa")), null, 1).totalHits;
       assertEquals(n*100, totalHits);
@@ -481,7 +481,7 @@ public class TestIndexWriter extends LuceneTestCase {
 
       Term searchTerm = new Term("field", "aaa");
 
-      IndexReader reader = IndexReader.open(dir);
+      IndexReader reader = DirectoryReader.open(dir);
       IndexSearcher searcher = new IndexSearcher(reader);
       ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;
       assertEquals(10, hits.length);
@@ -503,13 +503,13 @@ public class TestIndexWriter extends LuceneTestCase {
         writer.addDocument(doc);
       }
       writer.close();
-      reader = IndexReader.open(dir);
+      reader = DirectoryReader.open(dir);
       searcher = new IndexSearcher(reader);
       hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;
       assertEquals(27, hits.length);
       reader.close();
 
-      reader = IndexReader.open(dir);
+      reader = DirectoryReader.open(dir);
       reader.close();
 
       dir.close();
@@ -536,7 +536,7 @@ public class TestIndexWriter extends LuceneTestCase {
       writer.addDocument(doc);
       writer.close();
 
-      IndexReader reader = IndexReader.open(dir);
+      IndexReader reader = DirectoryReader.open(dir);
       assertEquals(1, reader.maxDoc());
       assertEquals(1, reader.numDocs());
       Term t = new Term("field", "a");
@@ -581,7 +581,7 @@ public class TestIndexWriter extends LuceneTestCase {
       }
       writer.close();
       Term searchTerm = new Term("content", "aaa");
-      IndexReader reader = IndexReader.open(dir);
+      IndexReader reader = DirectoryReader.open(dir);
       IndexSearcher searcher = new IndexSearcher(reader);
       ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;
       assertEquals("did not get right number of hits", 100, hits.length);
@@ -637,7 +637,7 @@ public class TestIndexWriter extends LuceneTestCase {
       }
       writer.addDocument(new Document());
       writer.close();
-      IndexReader reader = IndexReader.open(dir);
+      IndexReader reader = DirectoryReader.open(dir);
       assertEquals(2, reader.numDocs());
       reader.close();
       dir.close();
@@ -750,7 +750,7 @@ public class TestIndexWriter extends LuceneTestCase {
     writer.addDocument(doc);
     writer.close();
 
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     Term t = new Term("field", "x");
     assertEquals(1, reader.docFreq(t));
     reader.close();
@@ -777,7 +777,7 @@ public class TestIndexWriter extends LuceneTestCase {
     doc.add(newField("", "a b c", TextField.TYPE_UNSTORED));
     writer.addDocument(doc);  
     writer.close();
-    DirectoryReader reader = IndexReader.open(dir);
+    DirectoryReader reader = DirectoryReader.open(dir);
     AtomicReader subreader = getOnlySegmentReader(reader);
     TermsEnum te = subreader.fields().terms("").iterator(null);
     assertEquals(new BytesRef("a"), te.next());
@@ -798,7 +798,7 @@ public class TestIndexWriter extends LuceneTestCase {
     doc.add(newField("", "c", StringField.TYPE_UNSTORED));
     writer.addDocument(doc);  
     writer.close();
-    DirectoryReader reader = IndexReader.open(dir);
+    DirectoryReader reader = DirectoryReader.open(dir);
     AtomicReader subreader = getOnlySegmentReader(reader);
     TermsEnum te = subreader.fields().terms("").iterator(null);
     assertEquals(new BytesRef(""), te.next());
@@ -852,7 +852,7 @@ public class TestIndexWriter extends LuceneTestCase {
     assertTrue(w.afterWasCalled);
     w.close();
 
-    IndexReader ir = IndexReader.open(dir);
+    IndexReader ir = DirectoryReader.open(dir);
     assertEquals(0, ir.numDocs());
     ir.close();
 
@@ -886,7 +886,7 @@ public class TestIndexWriter extends LuceneTestCase {
     w.addDocument(doc);
     w.commit();
 
-    IndexReader r = IndexReader.open(dir);
+    IndexReader r = DirectoryReader.open(dir);
     IndexSearcher s = new IndexSearcher(r);
     PhraseQuery pq = new PhraseQuery();
     pq.add(new Term("field", "a"));
@@ -935,7 +935,7 @@ public class TestIndexWriter extends LuceneTestCase {
     w.addDocument(doc);
     w.close();
 
-    IndexReader ir = IndexReader.open(dir);
+    IndexReader ir = DirectoryReader.open(dir);
     Document doc2 = ir.document(0);
     IndexableField f2 = doc2.getField("binary");
     b = f2.binaryValue().bytes;
@@ -964,7 +964,7 @@ public class TestIndexWriter extends LuceneTestCase {
     w.addDocument(doc);
     w.close();
 
-    IndexReader r = IndexReader.open(dir);
+    IndexReader r = DirectoryReader.open(dir);
     Terms tpv = r.getTermVectors(0).terms("field");
     TermsEnum termsEnum = tpv.iterator(null);
     assertNotNull(termsEnum.next());
@@ -1008,11 +1008,11 @@ public class TestIndexWriter extends LuceneTestCase {
     writer2.addDocument(doc);
     writer2.close();
 
-    IndexReader r1 = IndexReader.open(dir2);
+    IndexReader r1 = DirectoryReader.open(dir2);
     writer.addIndexes(r1, r1);
     writer.close();
 
-    IndexReader r3 = IndexReader.open(dir);
+    IndexReader r3 = DirectoryReader.open(dir);
     assertEquals(5, r3.numDocs());
     r3.close();
 
@@ -1056,7 +1056,7 @@ public class TestIndexWriter extends LuceneTestCase {
             w.close();
             w = null;
             _TestUtil.checkIndex(dir);
-            IndexReader.open(dir).close();
+            DirectoryReader.open(dir).close();
 
             // Strangely, if we interrupt a thread before
             // all classes are loaded, the class loader
@@ -1106,12 +1106,12 @@ public class TestIndexWriter extends LuceneTestCase {
           e.printStackTrace(System.out);
         }
         try {
-          IndexReader r = IndexReader.open(dir);
+          IndexReader r = DirectoryReader.open(dir);
           //System.out.println("doc count=" + r.numDocs());
           r.close();
         } catch (Exception e) {
           failed = true;
-          System.out.println("IndexReader.open FAILED: unexpected exception");
+          System.out.println("DirectoryReader.open FAILED: unexpected exception");
           e.printStackTrace(System.out);
         }
       }
@@ -1192,7 +1192,7 @@ public class TestIndexWriter extends LuceneTestCase {
     w.forceMerge(1);   // force segment merge.
     w.close();
 
-    IndexReader ir = IndexReader.open(dir);
+    IndexReader ir = DirectoryReader.open(dir);
     Document doc2 = ir.document(0);
     IndexableField f3 = doc2.getField("binary");
     b = f3.binaryValue().bytes;
@@ -1320,7 +1320,7 @@ public class TestIndexWriter extends LuceneTestCase {
       } else {
         // don't use NRT
         w.commit();
-        r = IndexReader.open(dir);
+        r = DirectoryReader.open(dir);
       }
 
       List<String> files = Arrays.asList(dir.listAll());
@@ -1525,7 +1525,7 @@ public class TestIndexWriter extends LuceneTestCase {
     _TestUtil.checkIndex(dir);
 
     assertNoUnreferencedFiles(dir, "no tv files");
-    DirectoryReader r0 = IndexReader.open(dir);
+    DirectoryReader r0 = DirectoryReader.open(dir);
     for (IndexReader r : r0.getSequentialSubReaders()) {
       SegmentInfoPerCommit s = ((SegmentReader) r).getSegmentInfo();
       assertFalse(((SegmentReader) r).getFieldInfos().hasVectors());
@@ -1687,7 +1687,7 @@ public class TestIndexWriter extends LuceneTestCase {
         w.addDocument(doc);
       }
       w.commit();
-      IndexReader.open(w, true).close();
+      DirectoryReader.open(w, true).close();
 
       w.deleteAll();
       w.commit();
@@ -1795,7 +1795,7 @@ public class TestIndexWriter extends LuceneTestCase {
     }
     w.commit();
     w.close();
-    IndexReader r = IndexReader.open(dir);
+    IndexReader r = DirectoryReader.open(dir);
     assertEquals(0, r.maxDoc());
     r.close();
     dir.close();
@@ -1824,7 +1824,7 @@ public class TestIndexWriter extends LuceneTestCase {
     w.prepareCommit();
     w.rollback();
     assertTrue(DirectoryReader.indexExists(dir));
-    IndexReader r = IndexReader.open(dir);
+    IndexReader r = DirectoryReader.open(dir);
     assertEquals(0, r.maxDoc());
     r.close();
     dir.close();
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit.java b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit.java
index dedfa9d..021ab09 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit.java
@@ -51,20 +51,20 @@ public class TestIndexWriterCommit extends LuceneTestCase {
       writer.close();
 
       Term searchTerm = new Term("content", "aaa");
-      DirectoryReader reader = IndexReader.open(dir);
+      DirectoryReader reader = DirectoryReader.open(dir);
       IndexSearcher searcher = new IndexSearcher(reader);
       ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;
       assertEquals("first number of hits", 14, hits.length);
       reader.close();
 
-      reader = IndexReader.open(dir);
+      reader = DirectoryReader.open(dir);
 
       writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())));
       for(int i=0;i<3;i++) {
         for(int j=0;j<11;j++) {
           TestIndexWriter.addDoc(writer);
         }
-        IndexReader r = IndexReader.open(dir);
+        IndexReader r = DirectoryReader.open(dir);
         searcher = new IndexSearcher(r);
         hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;
         assertEquals("reader incorrectly sees changes from writer", 14, hits.length);
@@ -76,7 +76,7 @@ public class TestIndexWriterCommit extends LuceneTestCase {
       writer.close();
       assertFalse("reader should not be current now", reader.isCurrent());
 
-      IndexReader r = IndexReader.open(dir);
+      IndexReader r = DirectoryReader.open(dir);
       searcher = new IndexSearcher(r);
       hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;
       assertEquals("reader did not see changes after writer was closed", 47, hits.length);
@@ -102,7 +102,7 @@ public class TestIndexWriterCommit extends LuceneTestCase {
     writer.close();
 
     Term searchTerm = new Term("content", "aaa");
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     IndexSearcher searcher = new IndexSearcher(reader);
     ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;
     assertEquals("first number of hits", 14, hits.length);
@@ -116,7 +116,7 @@ public class TestIndexWriterCommit extends LuceneTestCase {
     // Delete all docs:
     writer.deleteDocuments(searchTerm);
 
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
     searcher = new IndexSearcher(reader);
     hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;
     assertEquals("reader incorrectly sees changes from writer", 14, hits.length);
@@ -127,7 +127,7 @@ public class TestIndexWriterCommit extends LuceneTestCase {
 
     TestIndexWriter.assertNoUnreferencedFiles(dir, "unreferenced files remain after rollback()");
 
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
     searcher = new IndexSearcher(reader);
     hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;
     assertEquals("saw changes after writer.abort", 14, hits.length);
@@ -146,7 +146,7 @@ public class TestIndexWriterCommit extends LuceneTestCase {
       for(int j=0;j<17;j++) {
         TestIndexWriter.addDoc(writer);
       }
-      IndexReader r = IndexReader.open(dir);
+      IndexReader r = DirectoryReader.open(dir);
       searcher = new IndexSearcher(r);
       hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;
       assertEquals("reader incorrectly sees changes from writer", 14, hits.length);
@@ -154,7 +154,7 @@ public class TestIndexWriterCommit extends LuceneTestCase {
     }
 
     writer.close();
-    IndexReader r = IndexReader.open(dir);
+    IndexReader r = DirectoryReader.open(dir);
     searcher = new IndexSearcher(r);
     hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;
     assertEquals("didn't see changes after close", 218, hits.length);
@@ -235,7 +235,7 @@ public class TestIndexWriterCommit extends LuceneTestCase {
     writer.forceMerge(1);
     writer.close();
 
-    IndexReader.open(dir).close();
+    DirectoryReader.open(dir).close();
 
     long endDiskUsage = dir.getMaxUsedSizeInBytes();
 
@@ -279,7 +279,7 @@ public class TestIndexWriterCommit extends LuceneTestCase {
     writer.forceMerge(1);
 
     // Open a reader before closing (commiting) the writer:
-    DirectoryReader reader = IndexReader.open(dir);
+    DirectoryReader reader = DirectoryReader.open(dir);
 
     // Reader should see index as multi-seg at this
     // point:
@@ -291,7 +291,7 @@ public class TestIndexWriterCommit extends LuceneTestCase {
     TestIndexWriter.assertNoUnreferencedFiles(dir, "aborted writer after forceMerge");
 
     // Open a reader after aborting writer:
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
 
     // Reader should still see index as multi-segment
     assertTrue("Reader incorrectly sees one segment", reader.getSequentialSubReaders().length > 1);
@@ -310,7 +310,7 @@ public class TestIndexWriterCommit extends LuceneTestCase {
     TestIndexWriter.assertNoUnreferencedFiles(dir, "aborted writer after forceMerge");
 
     // Open a reader after aborting writer:
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
 
     // Reader should see index as one segment
     assertEquals("Reader incorrectly sees more than one segment", 1, reader.getSequentialSubReaders().length);
@@ -339,7 +339,7 @@ public class TestIndexWriterCommit extends LuceneTestCase {
           public void run() {
             try {
               final Document doc = new Document();
-              DirectoryReader r = IndexReader.open(dir);
+              DirectoryReader r = DirectoryReader.open(dir);
               Field f = newField("f", "", StringField.TYPE_UNSTORED);
               doc.add(f);
               int count = 0;
@@ -390,7 +390,7 @@ public class TestIndexWriterCommit extends LuceneTestCase {
     for (int i = 0; i < 23; i++)
       TestIndexWriter.addDoc(writer);
 
-    DirectoryReader reader = IndexReader.open(dir);
+    DirectoryReader reader = DirectoryReader.open(dir);
     assertEquals(0, reader.numDocs());
     writer.commit();
     DirectoryReader reader2 = DirectoryReader.openIfChanged(reader);
@@ -403,12 +403,12 @@ public class TestIndexWriterCommit extends LuceneTestCase {
       TestIndexWriter.addDoc(writer);
     assertEquals(23, reader2.numDocs());
     reader2.close();
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
     assertEquals(23, reader.numDocs());
     reader.close();
     writer.commit();
 
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
     assertEquals(40, reader.numDocs());
     reader.close();
     writer.close();
@@ -501,12 +501,12 @@ public class TestIndexWriterCommit extends LuceneTestCase {
     for (int i = 0; i < 23; i++)
       TestIndexWriter.addDoc(writer);
 
-    DirectoryReader reader = IndexReader.open(dir);
+    DirectoryReader reader = DirectoryReader.open(dir);
     assertEquals(0, reader.numDocs());
 
     writer.prepareCommit();
 
-    IndexReader reader2 = IndexReader.open(dir);
+    IndexReader reader2 = DirectoryReader.open(dir);
     assertEquals(0, reader2.numDocs());
 
     writer.commit();
@@ -524,18 +524,18 @@ public class TestIndexWriterCommit extends LuceneTestCase {
 
     assertEquals(23, reader3.numDocs());
     reader3.close();
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
     assertEquals(23, reader.numDocs());
     reader.close();
 
     writer.prepareCommit();
 
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
     assertEquals(23, reader.numDocs());
     reader.close();
 
     writer.commit();
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
     assertEquals(40, reader.numDocs());
     reader.close();
     writer.close();
@@ -558,12 +558,12 @@ public class TestIndexWriterCommit extends LuceneTestCase {
     for (int i = 0; i < 23; i++)
       TestIndexWriter.addDoc(writer);
 
-    DirectoryReader reader = IndexReader.open(dir);
+    DirectoryReader reader = DirectoryReader.open(dir);
     assertEquals(0, reader.numDocs());
 
     writer.prepareCommit();
 
-    IndexReader reader2 = IndexReader.open(dir);
+    IndexReader reader2 = DirectoryReader.open(dir);
     assertEquals(0, reader2.numDocs());
 
     writer.rollback();
@@ -579,18 +579,18 @@ public class TestIndexWriterCommit extends LuceneTestCase {
     for (int i = 0; i < 17; i++)
       TestIndexWriter.addDoc(writer);
 
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
     assertEquals(0, reader.numDocs());
     reader.close();
 
     writer.prepareCommit();
 
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
     assertEquals(0, reader.numDocs());
     reader.close();
 
     writer.commit();
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
     assertEquals(17, reader.numDocs());
     reader.close();
     writer.close();
@@ -606,7 +606,7 @@ public class TestIndexWriterCommit extends LuceneTestCase {
     writer.commit();
     writer.close();
 
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     assertEquals(0, reader.numDocs());
     reader.close();
     dir.close();
@@ -620,7 +620,7 @@ public class TestIndexWriterCommit extends LuceneTestCase {
       TestIndexWriter.addDoc(w);
     w.close();
 
-    DirectoryReader r = IndexReader.open(dir);
+    DirectoryReader r = DirectoryReader.open(dir);
     // commit(Map) never called for this index
     assertEquals(0, r.getIndexCommit().getUserData().size());
     r.close();
@@ -633,7 +633,7 @@ public class TestIndexWriterCommit extends LuceneTestCase {
     w.commit(data);
     w.close();
 
-    r = IndexReader.open(dir);
+    r = DirectoryReader.open(dir);
     assertEquals("test1", r.getIndexCommit().getUserData().get("label"));
     r.close();
 
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete.java b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete.java
index d7db99a..b289631 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete.java
@@ -106,7 +106,7 @@ public class TestIndexWriterDelete extends LuceneTestCase {
 
     modifier.commit();
 
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     assertEquals(7, reader.numDocs());
     reader.close();
 
@@ -114,7 +114,7 @@ public class TestIndexWriterDelete extends LuceneTestCase {
 
     modifier.commit();
 
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
     assertEquals(0, reader.numDocs());
     reader.close();
     modifier.close();
@@ -166,7 +166,7 @@ public class TestIndexWriterDelete extends LuceneTestCase {
       assertEquals(0, modifier.getSegmentCount());
       modifier.commit();
 
-      IndexReader reader = IndexReader.open(dir);
+      IndexReader reader = DirectoryReader.open(dir);
       assertEquals(1, reader.numDocs());
 
       int hitCount = getHitCount(dir, new Term("id", String.valueOf(id)));
@@ -204,7 +204,7 @@ public class TestIndexWriterDelete extends LuceneTestCase {
 
     modifier.commit();
 
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     assertEquals(5, reader.numDocs());
     modifier.close();
     reader.close();
@@ -226,7 +226,7 @@ public class TestIndexWriterDelete extends LuceneTestCase {
     }
     modifier.commit();
 
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     assertEquals(7, reader.numDocs());
     reader.close();
 
@@ -236,7 +236,7 @@ public class TestIndexWriterDelete extends LuceneTestCase {
 
     modifier.commit();
 
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
     assertEquals(5, reader.numDocs());
     reader.close();
 
@@ -246,7 +246,7 @@ public class TestIndexWriterDelete extends LuceneTestCase {
     }
     modifier.deleteDocuments(terms);
     modifier.commit();
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
     assertEquals(2, reader.numDocs());
     reader.close();
 
@@ -269,7 +269,7 @@ public class TestIndexWriterDelete extends LuceneTestCase {
     }
     modifier.commit();
 
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     assertEquals(7, reader.numDocs());
     reader.close();
 
@@ -280,7 +280,7 @@ public class TestIndexWriterDelete extends LuceneTestCase {
     modifier.deleteAll();
 
     // Delete all shouldn't be on disk yet
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
     assertEquals(7, reader.numDocs());
     reader.close();
 
@@ -292,7 +292,7 @@ public class TestIndexWriterDelete extends LuceneTestCase {
     modifier.commit();
 
     // Validate there are no docs left
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
     assertEquals(2, reader.numDocs());
     reader.close();
 
@@ -317,7 +317,7 @@ public class TestIndexWriterDelete extends LuceneTestCase {
 
     addDoc(modifier, ++id, value);
 
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     assertEquals(7, reader.numDocs());
     reader.close();
 
@@ -329,7 +329,7 @@ public class TestIndexWriterDelete extends LuceneTestCase {
     modifier.close();
 
     // Validate that the docs are still there
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
     assertEquals(7, reader.numDocs());
     reader.close();
 
@@ -372,7 +372,7 @@ public class TestIndexWriterDelete extends LuceneTestCase {
     modifier.close();
 
     // Validate that the docs are still there
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
     assertEquals(7, reader.numDocs());
     reader.close();
 
@@ -400,7 +400,7 @@ public class TestIndexWriterDelete extends LuceneTestCase {
   }
 
   private int getHitCount(Directory dir, Term term) throws IOException {
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     IndexSearcher searcher = new IndexSearcher(reader);
     int hitCount = searcher.search(new TermQuery(term), null, 1000).totalHits;
     reader.close();
@@ -566,7 +566,7 @@ public class TestIndexWriterDelete extends LuceneTestCase {
         // changed (transactional semantics):
         IndexReader newReader = null;
         try {
-          newReader = IndexReader.open(dir);
+          newReader = DirectoryReader.open(dir);
         }
         catch (IOException e) {
           e.printStackTrace();
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java
index 1a64144..0a6d7de 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java
@@ -255,7 +255,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
     }
 
     // Confirm that when doc hits exception partway through tokenization, it's deleted:
-    IndexReader r2 = IndexReader.open(dir);
+    IndexReader r2 = DirectoryReader.open(dir);
     final int count = r2.docFreq(new Term("content4", "aaa"));
     final int count2 = r2.docFreq(new Term("content4", "ddd"));
     assertEquals(count, count2);
@@ -301,7 +301,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
     }
 
     // Confirm that when doc hits exception partway through tokenization, it's deleted:
-    IndexReader r2 = IndexReader.open(dir);
+    IndexReader r2 = DirectoryReader.open(dir);
     final int count = r2.docFreq(new Term("content4", "aaa"));
     final int count2 = r2.docFreq(new Term("content4", "ddd"));
     assertEquals(count, count2);
@@ -495,7 +495,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
     writer.addDocument(doc);
 
     writer.close();
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     final Term t = new Term("content", "aa");
     assertEquals(3, reader.docFreq(t));
 
@@ -577,7 +577,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
     }
     assertTrue(hitError);
     writer.close();
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     assertEquals(198, reader.docFreq(new Term("content", "aa")));
     reader.close();
     dir.close();
@@ -632,7 +632,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
       if (VERBOSE) {
         System.out.println("TEST: open reader");
       }
-      IndexReader reader = IndexReader.open(dir);
+      IndexReader reader = DirectoryReader.open(dir);
       if (i == 0) { 
         int expected = 5;
         assertEquals(expected, reader.docFreq(new Term("contents", "here")));
@@ -661,7 +661,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
       writer.forceMerge(1);
       writer.close();
 
-      reader = IndexReader.open(dir);
+      reader = DirectoryReader.open(dir);
       int expected = 19+(1-i)*2;
       assertEquals(expected, reader.docFreq(new Term("contents", "here")));
       assertEquals(expected, reader.maxDoc());
@@ -747,7 +747,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
         writer.close();
       }
 
-      IndexReader reader = IndexReader.open(dir);
+      IndexReader reader = DirectoryReader.open(dir);
       int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;
       assertEquals("i=" + i, expected, reader.docFreq(new Term("contents", "here")));
       assertEquals(expected, reader.maxDoc());
@@ -775,7 +775,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
       writer.forceMerge(1);
       writer.close();
 
-      reader = IndexReader.open(dir);
+      reader = DirectoryReader.open(dir);
       expected += 17-NUM_THREAD*NUM_ITER;
       assertEquals(expected, reader.docFreq(new Term("contents", "here")));
       assertEquals(expected, reader.maxDoc());
@@ -850,7 +850,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
     failure.clearDoFail();
     writer.close();
 
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     assertEquals(23, reader.numDocs());
     reader.close();
     dir.close();
@@ -1063,7 +1063,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
 
     IndexReader reader = null;
     try {
-      reader = IndexReader.open(dir);
+      reader = DirectoryReader.open(dir);
     } catch (IOException e) {
       e.printStackTrace(System.out);
       fail("segmentInfos failed to retry fallback to correct segments_N file");
@@ -1110,7 +1110,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
 
       IndexReader reader = null;
       try {
-        reader = IndexReader.open(dir);
+        reader = DirectoryReader.open(dir);
         fail("reader did not hit IOException on opening a corrupt index");
       } catch (Exception e) {
       }
@@ -1159,7 +1159,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
 
     IndexReader reader = null;
     try {
-      reader = IndexReader.open(dir);
+      reader = DirectoryReader.open(dir);
       fail("reader did not hit IOException on opening a corrupt index");
     } catch (Exception e) {
     }
@@ -1210,7 +1210,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
 
       IndexReader reader = null;
       try {
-        reader = IndexReader.open(dir);
+        reader = DirectoryReader.open(dir);
       } catch (Exception e) {
         fail("reader failed to open on a crashed index");
       }
@@ -1286,7 +1286,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
         document.add(new Field("field", "a field", TextField.TYPE_STORED));
         w.addDocument(document);
         w.close();
-        IndexReader reader = IndexReader.open(dir);
+        IndexReader reader = DirectoryReader.open(dir);
         assertTrue(reader.numDocs() > 0);
         SegmentInfos sis = new SegmentInfos();
         sis.read(dir);
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterForceMerge.java b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterForceMerge.java
index 5117fce..2e3108e 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterForceMerge.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterForceMerge.java
@@ -186,7 +186,7 @@ public class TestIndexWriterForceMerge extends LuceneTestCase {
 
       if (0 == pass) {
         writer.close();
-        DirectoryReader reader = IndexReader.open(dir);
+        DirectoryReader reader = DirectoryReader.open(dir);
         assertEquals(1, reader.getSequentialSubReaders().length);
         reader.close();
       } else {
@@ -196,7 +196,7 @@ public class TestIndexWriterForceMerge extends LuceneTestCase {
         writer.addDocument(doc);
         writer.close();
 
-        DirectoryReader reader = IndexReader.open(dir);
+        DirectoryReader reader = DirectoryReader.open(dir);
         assertTrue(reader.getSequentialSubReaders().length > 1);
         reader.close();
 
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterMerging.java b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterMerging.java
index 24e481f..f0c59bc 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterMerging.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterMerging.java
@@ -80,7 +80,7 @@ public class TestIndexWriterMerging extends LuceneTestCase
   private boolean verifyIndex(Directory directory, int startAt) throws IOException
   {
     boolean fail = false;
-    IndexReader reader = IndexReader.open(directory);
+    IndexReader reader = DirectoryReader.open(directory);
 
     int max = reader.maxDoc();
     for (int i = 0; i < max; i++)
@@ -149,7 +149,7 @@ public class TestIndexWriterMerging extends LuceneTestCase
     }
     writer.close();
 
-    IndexReader ir = IndexReader.open(dir);
+    IndexReader ir = DirectoryReader.open(dir);
     assertEquals(10, ir.maxDoc());
     assertEquals(10, ir.numDocs());
     ir.close();
@@ -161,7 +161,7 @@ public class TestIndexWriterMerging extends LuceneTestCase
     writer.deleteDocuments(new Term("id", "7"));
     writer.close();
     
-    ir = IndexReader.open(dir);
+    ir = DirectoryReader.open(dir);
     assertEquals(8, ir.numDocs());
     ir.close();
 
@@ -171,7 +171,7 @@ public class TestIndexWriterMerging extends LuceneTestCase
     writer.forceMergeDeletes();
     assertEquals(8, writer.numDocs());
     writer.close();
-    ir = IndexReader.open(dir);
+    ir = DirectoryReader.open(dir);
     assertEquals(8, ir.maxDoc());
     assertEquals(8, ir.numDocs());
     ir.close();
@@ -212,7 +212,7 @@ public class TestIndexWriterMerging extends LuceneTestCase
     }
     writer.close();
 
-    IndexReader ir = IndexReader.open(dir);
+    IndexReader ir = DirectoryReader.open(dir);
     assertEquals(98, ir.maxDoc());
     assertEquals(98, ir.numDocs());
     ir.close();
@@ -225,7 +225,7 @@ public class TestIndexWriterMerging extends LuceneTestCase
     }
     writer.close();
     
-    ir = IndexReader.open(dir);
+    ir = DirectoryReader.open(dir);
     assertEquals(49, ir.numDocs());
     ir.close();
 
@@ -237,7 +237,7 @@ public class TestIndexWriterMerging extends LuceneTestCase
     assertEquals(49, writer.numDocs());
     writer.forceMergeDeletes();
     writer.close();
-    ir = IndexReader.open(dir);
+    ir = DirectoryReader.open(dir);
     assertEquals(49, ir.maxDoc());
     assertEquals(49, ir.numDocs());
     ir.close();
@@ -278,7 +278,7 @@ public class TestIndexWriterMerging extends LuceneTestCase
     }
     writer.close();
 
-    IndexReader ir = IndexReader.open(dir);
+    IndexReader ir = DirectoryReader.open(dir);
     assertEquals(98, ir.maxDoc());
     assertEquals(98, ir.numDocs());
     ir.close();
@@ -290,7 +290,7 @@ public class TestIndexWriterMerging extends LuceneTestCase
       writer.deleteDocuments(new Term("id", "" + i));
     }
     writer.close();
-    ir = IndexReader.open(dir);
+    ir = DirectoryReader.open(dir);
     assertEquals(49, ir.numDocs());
     ir.close();
 
@@ -301,7 +301,7 @@ public class TestIndexWriterMerging extends LuceneTestCase
     );
     writer.forceMergeDeletes(false);
     writer.close();
-    ir = IndexReader.open(dir);
+    ir = DirectoryReader.open(dir);
     assertEquals(49, ir.maxDoc());
     assertEquals(49, ir.numDocs());
     ir.close();
@@ -438,7 +438,7 @@ public class TestIndexWriterMerging extends LuceneTestCase
         t1.join();
 
         // Make sure reader can read
-        IndexReader reader = IndexReader.open(directory);
+        IndexReader reader = DirectoryReader.open(directory);
         reader.close();
 
         // Reopen
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull.java b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull.java
index 0129ef5..b0f3c50 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull.java
@@ -114,7 +114,7 @@ public class TestIndexWriterOnDiskFull extends LuceneTestCase {
             assertNoUnreferencedFiles(dir, "after disk full during addDocument");
             
             // Make sure reader can open the index:
-            IndexReader.open(dir).close();
+            DirectoryReader.open(dir).close();
           }
             
           dir.close();
@@ -190,7 +190,7 @@ public class TestIndexWriterOnDiskFull extends LuceneTestCase {
     
     // Make sure starting index seems to be working properly:
     Term searchTerm = new Term("content", "aaa");        
-    IndexReader reader = IndexReader.open(startDir);
+    IndexReader reader = DirectoryReader.open(startDir);
     assertEquals("first docFreq", 57, reader.docFreq(searchTerm));
     
     IndexSearcher searcher = newSearcher(reader);
@@ -313,7 +313,7 @@ public class TestIndexWriterOnDiskFull extends LuceneTestCase {
             } else if (1 == method) {
               IndexReader readers[] = new IndexReader[dirs.length];
               for(int i=0;i<dirs.length;i++) {
-                readers[i] = IndexReader.open(dirs[i]);
+                readers[i] = DirectoryReader.open(dirs[i]);
               }
               try {
                 writer.addIndexes(readers);
@@ -362,7 +362,7 @@ public class TestIndexWriterOnDiskFull extends LuceneTestCase {
           // failed, we see either all docs or no docs added
           // (transactional semantics):
           try {
-            reader = IndexReader.open(dir);
+            reader = DirectoryReader.open(dir);
           } catch (IOException e) {
             e.printStackTrace(System.out);
             fail(testName + ": exception when creating IndexReader: " + e);
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterReader.java b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterReader.java
index f301a84..1d9642b 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterReader.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterReader.java
@@ -161,7 +161,7 @@ public class TestIndexWriterReader extends LuceneTestCase {
     writer.close();
     assertTrue(r2.isCurrent());
     
-    DirectoryReader r3 = IndexReader.open(dir1);
+    DirectoryReader r3 = DirectoryReader.open(dir1);
     assertTrue(r3.isCurrent());
     assertTrue(r2.isCurrent());
     assertEquals(0, count(new Term("id", id10), r3));
@@ -207,7 +207,7 @@ public class TestIndexWriterReader extends LuceneTestCase {
     assertFalse(nrtReader.isCurrent());
     nrtReader.close();
     
-    DirectoryReader dirReader = IndexReader.open(dir);
+    DirectoryReader dirReader = DirectoryReader.open(dir);
     nrtReader = writer.getReader();
     
     assertTrue(dirReader.isCurrent());
@@ -386,7 +386,7 @@ public class TestIndexWriterReader extends LuceneTestCase {
 
     _TestUtil.checkIndex(mainDir);
 
-    IndexReader reader = IndexReader.open(mainDir);
+    IndexReader reader = DirectoryReader.open(mainDir);
     assertEquals(addDirThreads.count.intValue(), reader.numDocs());
     //assertEquals(100 + numDirs * (3 * numIter / 4) * addDirThreads.numThreads
     //    * addDirThreads.NUM_INIT_DOCS, reader.numDocs());
@@ -422,7 +422,7 @@ public class TestIndexWriterReader extends LuceneTestCase {
       
       readers = new IndexReader[numDirs];
       for (int i = 0; i < numDirs; i++)
-        readers[i] = IndexReader.open(addDir);
+        readers[i] = DirectoryReader.open(addDir);
     }
     
     void joinThreads() {
@@ -890,7 +890,7 @@ public class TestIndexWriterReader extends LuceneTestCase {
     w.forceMergeDeletes();
     w.close();
     r.close();
-    r = IndexReader.open(dir);
+    r = DirectoryReader.open(dir);
     assertEquals(1, r.numDocs());
     assertFalse(r.hasDeletions());
     r.close();
@@ -984,7 +984,7 @@ public class TestIndexWriterReader extends LuceneTestCase {
     Document doc = new Document();
     doc.add(new TextField("f", "val"));
     w.addDocument(doc);
-    IndexReader r = IndexReader.open(w, true).getSequentialSubReaders()[0];
+    IndexReader r = DirectoryReader.open(w, true).getSequentialSubReaders()[0];
     try {
       _TestUtil.docs(random(), r, "f", new BytesRef("val"), null, null, false);
       fail("should have failed to seek since terms index was not loaded.");
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterUnicode.java b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterUnicode.java
index 578dfbf..2cc3434 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterUnicode.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterUnicode.java
@@ -261,7 +261,7 @@ public class TestIndexWriterUnicode extends LuceneTestCase {
     w.addDocument(doc);
     w.close();
 
-    IndexReader ir = IndexReader.open(dir);
+    IndexReader ir = DirectoryReader.open(dir);
     Document doc2 = ir.document(0);
     for(int i=0;i<count;i++) {
       assertEquals("field " + i + " was not indexed correctly", 1, ir.docFreq(new Term("f"+i, utf8Data[2*i+1])));
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterWithThreads.java b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterWithThreads.java
index 8e909fe..202d923 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterWithThreads.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterWithThreads.java
@@ -210,7 +210,7 @@ public class TestIndexWriterWithThreads extends LuceneTestCase {
       }
 
       // Quick test to make sure index is not corrupt:
-      IndexReader reader = IndexReader.open(dir);
+      IndexReader reader = DirectoryReader.open(dir);
       DocsEnum tdocs = _TestUtil.docs(random(), reader,
                                       "field",
                                       new BytesRef("aaa"),
@@ -277,7 +277,7 @@ public class TestIndexWriterWithThreads extends LuceneTestCase {
       }
 
       if (success) {
-        IndexReader reader = IndexReader.open(dir);
+        IndexReader reader = DirectoryReader.open(dir);
         final Bits delDocs = MultiFields.getLiveDocs(reader);
         for(int j=0;j<reader.maxDoc();j++) {
           if (delDocs == null || !delDocs.get(j)) {
@@ -448,7 +448,7 @@ public class TestIndexWriterWithThreads extends LuceneTestCase {
      assertFalse("Failed due to: " + thread1.failure, thread1.failed);
      assertFalse("Failed due to: " + thread2.failure, thread2.failed);
      // now verify that we have two documents in the index
-     IndexReader reader = IndexReader.open(dir);
+     IndexReader reader = DirectoryReader.open(dir);
      assertEquals("IndexReader should have one document per thread running", 2,
          reader.numDocs());
      
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestLazyProxSkipping.java b/lucene/core/src/test/org/apache/lucene/index/TestLazyProxSkipping.java
index f970ea4..d74eab9 100755
--- a/lucene/core/src/test/org/apache/lucene/index/TestLazyProxSkipping.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestLazyProxSkipping.java
@@ -105,7 +105,7 @@ public class TestLazyProxSkipping extends LuceneTestCase {
         writer.forceMerge(1);
         writer.close();
 
-      SegmentReader reader = getOnlySegmentReader(IndexReader.open(directory));
+      SegmentReader reader = getOnlySegmentReader(DirectoryReader.open(directory));
 
       this.searcher = newSearcher(reader);
     }
@@ -151,7 +151,7 @@ public class TestLazyProxSkipping extends LuceneTestCase {
         }
         
         writer.close();
-        IndexReader reader = IndexReader.open(directory);
+        IndexReader reader = DirectoryReader.open(directory);
 
         DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,
                                                                    MultiFields.getLiveDocs(reader),
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestLongPostings.java b/lucene/core/src/test/org/apache/lucene/index/TestLongPostings.java
index 266f388..da0683d 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestLongPostings.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestLongPostings.java
@@ -323,7 +323,7 @@ public class TestLongPostings extends LuceneTestCase {
       r = riw.getReader();
       riw.close();
     } else {
-      r = IndexReader.open(dir);
+      r = DirectoryReader.open(dir);
     }
 
     /*
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestMultiLevelSkipList.java b/lucene/core/src/test/org/apache/lucene/index/TestMultiLevelSkipList.java
index 820e86a..9dabc71 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestMultiLevelSkipList.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestMultiLevelSkipList.java
@@ -80,7 +80,7 @@ public class TestMultiLevelSkipList extends LuceneTestCase {
     writer.forceMerge(1);
     writer.close();
 
-    AtomicReader reader = getOnlySegmentReader(IndexReader.open(dir));
+    AtomicReader reader = getOnlySegmentReader(DirectoryReader.open(dir));
     
     for (int i = 0; i < 2; i++) {
       counter = 0;
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestNeverDelete.java b/lucene/core/src/test/org/apache/lucene/index/TestNeverDelete.java
index f31f63a..91b0a76 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestNeverDelete.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestNeverDelete.java
@@ -80,7 +80,7 @@ public class TestNeverDelete extends LuceneTestCase {
 
     final Set<String> allFiles = new HashSet<String>();
 
-    DirectoryReader r = IndexReader.open(d);
+    DirectoryReader r = DirectoryReader.open(d);
     while(System.currentTimeMillis() < stopTime) {
       final IndexCommit ic = r.getIndexCommit();
       if (VERBOSE) {
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestNorms.java b/lucene/core/src/test/org/apache/lucene/index/TestNorms.java
index 6dee50c..d82202d 100755
--- a/lucene/core/src/test/org/apache/lucene/index/TestNorms.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestNorms.java
@@ -95,7 +95,7 @@ public class TestNorms extends LuceneTestCase {
   public void testMaxByteNorms() throws IOException {
     Directory dir = newFSDirectory(_TestUtil.getTempDir("TestNorms.testMaxByteNorms"));
     buildIndex(dir, true);
-    AtomicReader open = SlowCompositeReaderWrapper.wrap(IndexReader.open(dir));
+    AtomicReader open = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir));
     DocValues normValues = open.normValues(byteTestField);
     assertNotNull(normValues);
     Source source = normValues.getSource();
@@ -126,7 +126,7 @@ public class TestNorms extends LuceneTestCase {
     boolean secondWriteNorm = random().nextBoolean();
     buildIndex(otherDir, secondWriteNorm);
 
-    AtomicReader reader = SlowCompositeReaderWrapper.wrap(IndexReader.open(otherDir));
+    AtomicReader reader = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(otherDir));
     FieldInfos fieldInfos = reader.getFieldInfos();
     FieldInfo fieldInfo = fieldInfos.fieldInfo(byteTestField);
     assertFalse(fieldInfo.omitsNorms());
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestOmitNorms.java b/lucene/core/src/test/org/apache/lucene/index/TestOmitNorms.java
index 0a0fd40..a3d387d 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestOmitNorms.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestOmitNorms.java
@@ -66,7 +66,7 @@ public class TestOmitNorms extends LuceneTestCase {
     // flush
     writer.close();
 
-    SegmentReader reader = getOnlySegmentReader(IndexReader.open(ram));
+    SegmentReader reader = getOnlySegmentReader(DirectoryReader.open(ram));
     FieldInfos fi = reader.getFieldInfos();
     assertTrue("OmitNorms field bit should be set.", fi.fieldInfo("f1").omitsNorms());
     assertTrue("OmitNorms field bit should be set.", fi.fieldInfo("f2").omitsNorms());
@@ -120,7 +120,7 @@ public class TestOmitNorms extends LuceneTestCase {
     // flush
     writer.close();
 
-    SegmentReader reader = getOnlySegmentReader(IndexReader.open(ram));
+    SegmentReader reader = getOnlySegmentReader(DirectoryReader.open(ram));
     FieldInfos fi = reader.getFieldInfos();
     assertTrue("OmitNorms field bit should be set.", fi.fieldInfo("f1").omitsNorms());
     assertTrue("OmitNorms field bit should be set.", fi.fieldInfo("f2").omitsNorms());
@@ -168,7 +168,7 @@ public class TestOmitNorms extends LuceneTestCase {
     // flush
     writer.close();
 
-    SegmentReader reader = getOnlySegmentReader(IndexReader.open(ram));
+    SegmentReader reader = getOnlySegmentReader(DirectoryReader.open(ram));
     FieldInfos fi = reader.getFieldInfos();
     assertTrue("OmitNorms field bit should not be set.", !fi.fieldInfo("f1").omitsNorms());
     assertTrue("OmitNorms field bit should be set.", fi.fieldInfo("f2").omitsNorms());
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestOmitPositions.java b/lucene/core/src/test/org/apache/lucene/index/TestOmitPositions.java
index 6cc2e92..2a5b846 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestOmitPositions.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestOmitPositions.java
@@ -153,7 +153,7 @@ public class TestOmitPositions extends LuceneTestCase {
     // flush
     writer.close();
 
-    SegmentReader reader = getOnlySegmentReader(IndexReader.open(ram));
+    SegmentReader reader = getOnlySegmentReader(DirectoryReader.open(ram));
     FieldInfos fi = reader.getFieldInfos();
     // docs + docs = docs
     assertEquals(IndexOptions.DOCS_ONLY, fi.fieldInfo("f1").getIndexOptions());
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestOmitTf.java b/lucene/core/src/test/org/apache/lucene/index/TestOmitTf.java
index 0e71ce9..39a8083 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestOmitTf.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestOmitTf.java
@@ -93,7 +93,7 @@ public class TestOmitTf extends LuceneTestCase {
     // flush
     writer.close();
 
-    SegmentReader reader = getOnlySegmentReader(IndexReader.open(ram));
+    SegmentReader reader = getOnlySegmentReader(DirectoryReader.open(ram));
     FieldInfos fi = reader.getFieldInfos();
     assertEquals("OmitTermFreqAndPositions field bit should be set.", IndexOptions.DOCS_ONLY, fi.fieldInfo("f1").getIndexOptions());
     assertEquals("OmitTermFreqAndPositions field bit should be set.", IndexOptions.DOCS_ONLY, fi.fieldInfo("f2").getIndexOptions());
@@ -145,7 +145,7 @@ public class TestOmitTf extends LuceneTestCase {
     // flush
     writer.close();
 
-    SegmentReader reader = getOnlySegmentReader(IndexReader.open(ram));
+    SegmentReader reader = getOnlySegmentReader(DirectoryReader.open(ram));
     FieldInfos fi = reader.getFieldInfos();
     assertEquals("OmitTermFreqAndPositions field bit should be set.", IndexOptions.DOCS_ONLY, fi.fieldInfo("f1").getIndexOptions());
     assertEquals("OmitTermFreqAndPositions field bit should be set.", IndexOptions.DOCS_ONLY, fi.fieldInfo("f2").getIndexOptions());
@@ -188,7 +188,7 @@ public class TestOmitTf extends LuceneTestCase {
     // flush
     writer.close();
 
-    SegmentReader reader = getOnlySegmentReader(IndexReader.open(ram));
+    SegmentReader reader = getOnlySegmentReader(DirectoryReader.open(ram));
     FieldInfos fi = reader.getFieldInfos();
     assertEquals("OmitTermFreqAndPositions field bit should not be set.", IndexOptions.DOCS_AND_FREQS_AND_POSITIONS, fi.fieldInfo("f1").getIndexOptions());
     assertEquals("OmitTermFreqAndPositions field bit should be set.", IndexOptions.DOCS_ONLY, fi.fieldInfo("f2").getIndexOptions());
@@ -279,7 +279,7 @@ public class TestOmitTf extends LuceneTestCase {
     /*
      * Verify the index
      */         
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     IndexSearcher searcher = new IndexSearcher(reader);
     searcher.setSimilarity(new SimpleSimilarity());
         
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestParallelReaderEmptyIndex.java b/lucene/core/src/test/org/apache/lucene/index/TestParallelReaderEmptyIndex.java
index 1f54d79..80f8899 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestParallelReaderEmptyIndex.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestParallelReaderEmptyIndex.java
@@ -115,7 +115,7 @@ public class TestParallelReaderEmptyIndex extends LuceneTestCase {
       
       writer.deleteDocuments(new Term("id", "1"));
       writer.close();
-      IndexReader ir = IndexReader.open(rd1);
+      IndexReader ir = DirectoryReader.open(rd1);
       assertEquals(2, ir.maxDoc());
       assertEquals(1, ir.numDocs());
       ir.close();
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestPayloadProcessorProvider.java b/lucene/core/src/test/org/apache/lucene/index/TestPayloadProcessorProvider.java
index 8336bdc..40edeef 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestPayloadProcessorProvider.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestPayloadProcessorProvider.java
@@ -156,7 +156,7 @@ public class TestPayloadProcessorProvider extends LuceneTestCase {
 
   private void verifyPayloadExists(Directory dir, String field, BytesRef text, int numExpected)
       throws IOException {
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     try {
       int numPayloads = 0;
       DocsAndPositionsEnum tpe = MultiFields.getTermPositionsEnum(reader, null, field, text, false);
@@ -198,7 +198,7 @@ public class TestPayloadProcessorProvider extends LuceneTestCase {
 
     IndexReader[] readers = new IndexReader[dirs.length];
     for (int i = 0; i < readers.length; i++) {
-      readers[i] = IndexReader.open(dirs[i]);
+      readers[i] = DirectoryReader.open(dirs[i]);
     }
     try {
       writer.addIndexes(readers);
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestPayloads.java b/lucene/core/src/test/org/apache/lucene/index/TestPayloads.java
index 6b697fb..1967b93 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestPayloads.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestPayloads.java
@@ -111,7 +111,7 @@ public class TestPayloads extends LuceneTestCase {
         // flush
         writer.close();
 
-      SegmentReader reader = getOnlySegmentReader(IndexReader.open(ram));
+      SegmentReader reader = getOnlySegmentReader(DirectoryReader.open(ram));
         FieldInfos fi = reader.getFieldInfos();
         assertFalse("Payload field bit should not be set.", fi.fieldInfo("f1").hasPayloads());
         assertTrue("Payload field bit should be set.", fi.fieldInfo("f2").hasPayloads());
@@ -138,7 +138,7 @@ public class TestPayloads extends LuceneTestCase {
         // flush
         writer.close();
 
-      reader = getOnlySegmentReader(IndexReader.open(ram));
+      reader = getOnlySegmentReader(DirectoryReader.open(ram));
         fi = reader.getFieldInfos();
         assertFalse("Payload field bit should not be set.", fi.fieldInfo("f1").hasPayloads());
         assertTrue("Payload field bit should be set.", fi.fieldInfo("f2").hasPayloads());
@@ -213,7 +213,7 @@ public class TestPayloads extends LuceneTestCase {
          * Verify the index
          * first we test if all payloads are stored correctly
          */        
-        IndexReader reader = IndexReader.open(dir);
+        IndexReader reader = DirectoryReader.open(dir);
 
         byte[] verifyPayloadData = new byte[payloadDataLength];
         offset = 0;
@@ -329,7 +329,7 @@ public class TestPayloads extends LuceneTestCase {
         // flush
         writer.close();
         
-        reader = IndexReader.open(dir);
+        reader = DirectoryReader.open(dir);
         tp = MultiFields.getTermPositionsEnum(reader,
                                               MultiFields.getLiveDocs(reader),
                                               fieldName,
@@ -530,7 +530,7 @@ public class TestPayloads extends LuceneTestCase {
           ingesters[i].join();
         }
         writer.close();
-        IndexReader reader = IndexReader.open(dir);
+        IndexReader reader = DirectoryReader.open(dir);
         TermsEnum terms = MultiFields.getFields(reader).terms(field).iterator(null);
         Bits liveDocs = MultiFields.getLiveDocs(reader);
         DocsAndPositionsEnum tp = null;
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestRollback.java b/lucene/core/src/test/org/apache/lucene/index/TestRollback.java
index 9b356e2..2a817bb 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestRollback.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestRollback.java
@@ -48,7 +48,7 @@ public class TestRollback extends LuceneTestCase {
     }
     w.rollback();
 
-    IndexReader r = IndexReader.open(dir);
+    IndexReader r = DirectoryReader.open(dir);
     assertEquals("index should contain same number of docs post rollback", 5, r.numDocs());
     r.close();
     dir.close();
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestRollingUpdates.java b/lucene/core/src/test/org/apache/lucene/index/TestRollingUpdates.java
index fe0a942..9bb5256 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestRollingUpdates.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestRollingUpdates.java
@@ -110,7 +110,7 @@ public class TestRollingUpdates extends LuceneTestCase {
       w.close();
     }
 
-    IndexReader open = IndexReader.open(dir);
+    IndexReader open = DirectoryReader.open(dir);
     assertEquals(1, open.numDocs());
     open.close();
     docs.close();
@@ -138,7 +138,7 @@ public class TestRollingUpdates extends LuceneTestCase {
           writer.updateDocument(new Term("id", "test"), doc);
           if (random().nextInt(3) == 0) {
             if (open == null) {
-              open = IndexReader.open(writer, true);
+              open = DirectoryReader.open(writer, true);
             }
             DirectoryReader reader = DirectoryReader.openIfChanged(open);
             if (reader != null) {
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestSegmentTermDocs.java b/lucene/core/src/test/org/apache/lucene/index/TestSegmentTermDocs.java
index 27f4bc3..80d13f5 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestSegmentTermDocs.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestSegmentTermDocs.java
@@ -131,7 +131,7 @@ public class TestSegmentTermDocs extends LuceneTestCase {
     writer.forceMerge(1);
     writer.close();
     
-    IndexReader reader = IndexReader.open(dir, indexDivisor);
+    IndexReader reader = DirectoryReader.open(dir, indexDivisor);
 
     DocsEnum tdocs = _TestUtil.docs(random(), reader,
                                     ta.field(),
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestSegmentTermEnum.java b/lucene/core/src/test/org/apache/lucene/index/TestSegmentTermEnum.java
index c2b6d32..67d6d7d 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestSegmentTermEnum.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestSegmentTermEnum.java
@@ -78,7 +78,7 @@ public class TestSegmentTermEnum extends LuceneTestCase {
     IndexWriter writer  = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setCodec(_TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat())));
     addDoc(writer, "aaa bbb");
     writer.close();
-    SegmentReader reader = getOnlySegmentReader(IndexReader.open(dir));
+    SegmentReader reader = getOnlySegmentReader(DirectoryReader.open(dir));
     TermsEnum terms = reader.fields().terms("content").iterator(null);
     assertNotNull(terms.next());
     assertEquals("aaa", terms.term().utf8ToString());
@@ -102,7 +102,7 @@ public class TestSegmentTermEnum extends LuceneTestCase {
   private void verifyDocFreq()
       throws IOException
   {
-      IndexReader reader = IndexReader.open(dir);
+      IndexReader reader = DirectoryReader.open(dir);
       TermsEnum termEnum = MultiFields.getTerms(reader, "content").iterator(null);
 
     // create enumeration of all terms
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestSnapshotDeletionPolicy.java b/lucene/core/src/test/org/apache/lucene/index/TestSnapshotDeletionPolicy.java
index 7d0ac66..ee7dfca 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestSnapshotDeletionPolicy.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestSnapshotDeletionPolicy.java
@@ -54,7 +54,7 @@ public class TestSnapshotDeletionPolicy extends LuceneTestCase {
   }
 
   protected void checkMaxDoc(IndexCommit commit, int expectedMaxDoc) throws Exception {
-    IndexReader reader = IndexReader.open(commit);
+    IndexReader reader = DirectoryReader.open(commit);
     try {
       assertEquals(expectedMaxDoc, reader.maxDoc());
     } finally {
@@ -245,7 +245,7 @@ public class TestSnapshotDeletionPolicy extends LuceneTestCase {
     assertSnapshotExists(dir, sdp, numSnapshots);
 
     // open a reader on a snapshot - should succeed.
-    IndexReader.open(sdp.getSnapshot("snapshot0")).close();
+    DirectoryReader.open(sdp.getSnapshot("snapshot0")).close();
 
     // open a new IndexWriter w/ no snapshots to keep and assert that all snapshots are gone.
     sdp = getDeletionPolicy();
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestStressIndexing.java b/lucene/core/src/test/org/apache/lucene/index/TestStressIndexing.java
index 178c8d4..5d18d9d 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestStressIndexing.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestStressIndexing.java
@@ -103,7 +103,7 @@ public class TestStressIndexing extends LuceneTestCase {
     @Override
     public void doWork() throws Throwable {
       for (int i=0; i<100; i++) {
-        IndexReader ir = IndexReader.open(directory);
+        IndexReader ir = DirectoryReader.open(directory);
         IndexSearcher is = new IndexSearcher(ir);
         ir.close();
       }
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2.java b/lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2.java
index 06c2afd..651b559 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2.java
@@ -267,14 +267,14 @@ public class TestStressIndexing2 extends LuceneTestCase {
   }
   
   public void verifyEquals(Random r, DirectoryReader r1, Directory dir2, String idField) throws Throwable {
-    DirectoryReader r2 = IndexReader.open(dir2);
+    DirectoryReader r2 = DirectoryReader.open(dir2);
     verifyEquals(r1, r2, idField);
     r2.close();
   }
 
   public void verifyEquals(Directory dir1, Directory dir2, String idField) throws Throwable {
-    DirectoryReader r1 = IndexReader.open(dir1);
-    DirectoryReader r2 = IndexReader.open(dir2);
+    DirectoryReader r1 = DirectoryReader.open(dir1);
+    DirectoryReader r2 = DirectoryReader.open(dir2);
     verifyEquals(r1, r2, idField);
     r1.close();
     r2.close();
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestStressNRT.java b/lucene/core/src/test/org/apache/lucene/index/TestStressNRT.java
index 5e1d356..8182ec5 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestStressNRT.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestStressNRT.java
@@ -109,7 +109,7 @@ public class TestStressNRT extends LuceneTestCase {
     final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
     writer.setDoRandomForceMergeAssert(false);
     writer.commit();
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
 
     for (int i=0; i<nWriteThreads; i++) {
       Thread thread = new Thread("WRITER"+i) {
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestSumDocFreq.java b/lucene/core/src/test/org/apache/lucene/index/TestSumDocFreq.java
index 7afaf85..7a8d8f7 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestSumDocFreq.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestSumDocFreq.java
@@ -67,7 +67,7 @@ public class TestSumDocFreq extends LuceneTestCase {
     writer.forceMerge(1);
     writer.close();
     
-    ir = IndexReader.open(dir);
+    ir = DirectoryReader.open(dir);
     assertSumDocFreq(ir);
     ir.close();
     dir.close();
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader.java b/lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader.java
index 6786996..91e2f22 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader.java
@@ -185,7 +185,7 @@ public class TestTermVectorsReader extends LuceneTestCase {
 
   public void test() throws IOException {
     //Check to see the files were created properly in setup
-    DirectoryReader reader = IndexReader.open(dir);
+    DirectoryReader reader = DirectoryReader.open(dir);
     for (IndexReader r : reader.getSequentialSubReaders()) {
       SegmentInfoPerCommit s = ((SegmentReader) r).getSegmentInfo();
       assertTrue(((SegmentReader) r).getFieldInfos().hasVectors());
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter.java b/lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter.java
index 7c5725a..15a42ef 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter.java
@@ -59,7 +59,7 @@ public class TestTermVectorsWriter extends LuceneTestCase {
     w.addDocument(doc);
     w.close();
 
-    IndexReader r = IndexReader.open(dir);
+    IndexReader r = DirectoryReader.open(dir);
     Terms vector = r.getTermVectors(0).terms("field");
     assertNotNull(vector);
     TermsEnum termsEnum = vector.iterator(null);
@@ -115,7 +115,7 @@ public class TestTermVectorsWriter extends LuceneTestCase {
     w.addDocument(doc);
     w.close();
 
-    IndexReader r = IndexReader.open(dir);
+    IndexReader r = DirectoryReader.open(dir);
     TermsEnum termsEnum = r.getTermVectors(0).terms("field").iterator(null);
     assertNotNull(termsEnum.next());
     DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null, true);
@@ -150,7 +150,7 @@ public class TestTermVectorsWriter extends LuceneTestCase {
     w.addDocument(doc);
     w.close();
 
-    IndexReader r = IndexReader.open(dir);
+    IndexReader r = DirectoryReader.open(dir);
     TermsEnum termsEnum = r.getTermVectors(0).terms("field").iterator(null);
     assertNotNull(termsEnum.next());
     DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null, true);
@@ -189,7 +189,7 @@ public class TestTermVectorsWriter extends LuceneTestCase {
     w.addDocument(doc);
     w.close();
 
-    IndexReader r = IndexReader.open(dir);
+    IndexReader r = DirectoryReader.open(dir);
     TermsEnum termsEnum = r.getTermVectors(0).terms("field").iterator(null);
     assertNotNull(termsEnum.next());
     DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null, true);
@@ -225,7 +225,7 @@ public class TestTermVectorsWriter extends LuceneTestCase {
     w.addDocument(doc);
     w.close();
 
-    IndexReader r = IndexReader.open(dir);
+    IndexReader r = DirectoryReader.open(dir);
     TermsEnum termsEnum = r.getTermVectors(0).terms("field").iterator(null);
     assertNotNull(termsEnum.next());
     DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null, true);
@@ -262,7 +262,7 @@ public class TestTermVectorsWriter extends LuceneTestCase {
     w.addDocument(doc);
     w.close();
 
-    IndexReader r = IndexReader.open(dir);
+    IndexReader r = DirectoryReader.open(dir);
     TermsEnum termsEnum = r.getTermVectors(0).terms("field").iterator(null);
     assertNotNull(termsEnum.next());
     DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null, true);
@@ -307,7 +307,7 @@ public class TestTermVectorsWriter extends LuceneTestCase {
     w.addDocument(doc);
     w.close();
 
-    IndexReader r = IndexReader.open(dir);
+    IndexReader r = DirectoryReader.open(dir);
     TermsEnum termsEnum = r.getTermVectors(0).terms("field").iterator(null);
     assertNotNull(termsEnum.next());
     DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null, true);
@@ -350,7 +350,7 @@ public class TestTermVectorsWriter extends LuceneTestCase {
     w.addDocument(doc);
     w.close();
 
-    IndexReader r = IndexReader.open(dir);
+    IndexReader r = DirectoryReader.open(dir);
     TermsEnum termsEnum = r.getTermVectors(0).terms("field").iterator(null);
     assertNotNull(termsEnum.next());
     DocsAndPositionsEnum dpEnum = termsEnum.docsAndPositions(null, null, true);
@@ -407,7 +407,7 @@ public class TestTermVectorsWriter extends LuceneTestCase {
       writer.forceMerge(1);
       writer.close();
 
-      IndexReader reader = IndexReader.open(dir);
+      IndexReader reader = DirectoryReader.open(dir);
       for(int i=0;i<reader.numDocs();i++) {
         reader.document(i);
         reader.getTermVectors(i);
@@ -461,7 +461,7 @@ public class TestTermVectorsWriter extends LuceneTestCase {
       writer.forceMerge(1);
       writer.close();
 
-      IndexReader reader = IndexReader.open(dir);
+      IndexReader reader = DirectoryReader.open(dir);
       assertNull(reader.getTermVectors(0));
       assertNull(reader.getTermVectors(1));
       assertNotNull(reader.getTermVectors(2));
@@ -506,7 +506,7 @@ public class TestTermVectorsWriter extends LuceneTestCase {
     writer.forceMerge(1);
     writer.close();
 
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     for(int i=0;i<10;i++) {
       reader.getTermVectors(i);
       reader.document(i);
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestTermdocPerf.java b/lucene/core/src/test/org/apache/lucene/index/TestTermdocPerf.java
index 9decb28..b3467a1 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestTermdocPerf.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestTermdocPerf.java
@@ -113,7 +113,7 @@ public class TestTermdocPerf extends LuceneTestCase {
     long end = System.currentTimeMillis();
     if (VERBOSE) System.out.println("milliseconds for creation of " + ndocs + " docs = " + (end-start));
 
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
 
     TermsEnum tenum = MultiFields.getTerms(reader, "foo").iterator(null);
 
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestThreadedForceMerge.java b/lucene/core/src/test/org/apache/lucene/index/TestThreadedForceMerge.java
index 84be8d0..373b9ea 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestThreadedForceMerge.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestThreadedForceMerge.java
@@ -129,7 +129,7 @@ public class TestThreadedForceMerge extends LuceneTestCase {
           TEST_VERSION_CURRENT, ANALYZER).setOpenMode(
           OpenMode.APPEND).setMaxBufferedDocs(2));
       
-      DirectoryReader reader = IndexReader.open(directory);
+      DirectoryReader reader = DirectoryReader.open(directory);
       assertEquals("reader=" + reader, 1, reader.getSequentialSubReaders().length);
       assertEquals(expectedDocCount, reader.numDocs());
       reader.close();
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestTransactionRollback.java b/lucene/core/src/test/org/apache/lucene/index/TestTransactionRollback.java
index a244209..e6dbedd 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestTransactionRollback.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestTransactionRollback.java
@@ -86,7 +86,7 @@ public class TestTransactionRollback extends LuceneTestCase {
   }
 	
   private void checkExpecteds(BitSet expecteds) throws Exception {
-    IndexReader r = IndexReader.open(dir);
+    IndexReader r = DirectoryReader.open(dir);
 		
     //Perhaps not the most efficient approach but meets our
     //needs here.
@@ -204,7 +204,7 @@ public class TestTransactionRollback extends LuceneTestCase {
       // should not work:
       new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random()))
           .setIndexDeletionPolicy(new DeleteLastCommitPolicy())).close();
-      IndexReader r = IndexReader.open(dir);
+      IndexReader r = DirectoryReader.open(dir);
       assertEquals(100, r.numDocs());
       r.close();
     }
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestTransactions.java b/lucene/core/src/test/org/apache/lucene/index/TestTransactions.java
index 02834f4b..24d3c1b 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestTransactions.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestTransactions.java
@@ -183,8 +183,8 @@ public class TestTransactions extends LuceneTestCase {
     public void doWork() throws Throwable {
       IndexReader r1, r2;
       synchronized(lock) {
-        r1 = IndexReader.open(dir1);
-        r2 = IndexReader.open(dir2);
+        r1 = DirectoryReader.open(dir1);
+        r2 = DirectoryReader.open(dir2);
       }
       if (r1.numDocs() != r2.numDocs())
         throw new RuntimeException("doc counts differ: r1=" + r1.numDocs() + " r2=" + r2.numDocs());
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestTypePromotion.java b/lucene/core/src/test/org/apache/lucene/index/TestTypePromotion.java
index 29467ee..e46cd6f 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestTypePromotion.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestTypePromotion.java
@@ -100,7 +100,7 @@ public class TestTypePromotion extends LuceneTestCase {
         writer.addIndexes(dir_2);
       } else {
         // do a real merge here
-        IndexReader open = maybeWrapReader(IndexReader.open(dir_2));
+        IndexReader open = maybeWrapReader(DirectoryReader.open(dir_2));
         writer.addIndexes(open);
         open.close();
       }
@@ -344,7 +344,7 @@ public class TestTypePromotion extends LuceneTestCase {
         writer.addIndexes(dir_2);
       } else {
         // do a real merge here
-        IndexReader open = IndexReader.open(dir_2);
+        IndexReader open = DirectoryReader.open(dir_2);
         writer.addIndexes(open);
         open.close();
       }
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestBoolean2.java b/lucene/core/src/test/org/apache/lucene/search/TestBoolean2.java
index 5dc6bb1..8fd3dc6 100644
--- a/lucene/core/src/test/org/apache/lucene/search/TestBoolean2.java
+++ b/lucene/core/src/test/org/apache/lucene/search/TestBoolean2.java
@@ -23,6 +23,7 @@ import java.util.Random;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.IndexReader;
@@ -63,7 +64,7 @@ public class TestBoolean2 extends LuceneTestCase {
       writer.addDocument(doc);
     }
     writer.close();
-    littleReader = IndexReader.open(directory);
+    littleReader = DirectoryReader.open(directory);
     searcher = new IndexSearcher(littleReader);
 
     // Make big index
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestCachingWrapperFilter.java b/lucene/core/src/test/org/apache/lucene/search/TestCachingWrapperFilter.java
index 4949cd7..4bcd84f 100644
--- a/lucene/core/src/test/org/apache/lucene/search/TestCachingWrapperFilter.java
+++ b/lucene/core/src/test/org/apache/lucene/search/TestCachingWrapperFilter.java
@@ -42,7 +42,7 @@ public class TestCachingWrapperFilter extends LuceneTestCase {
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
     writer.close();
 
-    IndexReader reader = SlowCompositeReaderWrapper.wrap(IndexReader.open(dir));
+    IndexReader reader = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir));
     AtomicReaderContext context = (AtomicReaderContext) reader.getTopReaderContext();
     MockFilter filter = new MockFilter();
     CachingWrapperFilter cacher = new CachingWrapperFilter(filter);
@@ -68,7 +68,7 @@ public class TestCachingWrapperFilter extends LuceneTestCase {
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
     writer.close();
 
-    IndexReader reader = SlowCompositeReaderWrapper.wrap(IndexReader.open(dir));
+    IndexReader reader = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir));
     AtomicReaderContext context = (AtomicReaderContext) reader.getTopReaderContext();
 
     final Filter filter = new Filter() {
@@ -91,7 +91,7 @@ public class TestCachingWrapperFilter extends LuceneTestCase {
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
     writer.close();
 
-    IndexReader reader = SlowCompositeReaderWrapper.wrap(IndexReader.open(dir));
+    IndexReader reader = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir));
     AtomicReaderContext context = (AtomicReaderContext) reader.getTopReaderContext();
 
     final Filter filter = new Filter() {
@@ -136,7 +136,7 @@ public class TestCachingWrapperFilter extends LuceneTestCase {
     writer.addDocument(new Document());
     writer.close();
 
-    IndexReader reader = SlowCompositeReaderWrapper.wrap(IndexReader.open(dir));
+    IndexReader reader = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir));
 
     // not cacheable:
     assertDocIdSetCacheable(reader, new QueryWrapperFilter(new TermQuery(new Term("test","value"))), false);
@@ -172,7 +172,7 @@ public class TestCachingWrapperFilter extends LuceneTestCase {
     // flipping a coin) may give us a newly opened reader,
     // but we use .reopen on this reader below and expect to
     // (must) get an NRT reader:
-    DirectoryReader reader = IndexReader.open(writer.w, true);
+    DirectoryReader reader = DirectoryReader.open(writer.w, true);
     // same reason we don't wrap?
     IndexSearcher searcher = newSearcher(reader, false);
 
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestElevationComparator.java b/lucene/core/src/test/org/apache/lucene/search/TestElevationComparator.java
index 688bc92..cbc6450 100644
--- a/lucene/core/src/test/org/apache/lucene/search/TestElevationComparator.java
+++ b/lucene/core/src/test/org/apache/lucene/search/TestElevationComparator.java
@@ -51,7 +51,7 @@ public class TestElevationComparator extends LuceneTestCase {
     writer.addDocument(adoc(new String[] {"id", "y", "title", "boosted boosted", "str_s","y"}));
     writer.addDocument(adoc(new String[] {"id", "z", "title", "boosted boosted boosted","str_s", "z"}));
 
-    IndexReader r = IndexReader.open(writer, true);
+    IndexReader r = DirectoryReader.open(writer, true);
     writer.close();
 
     IndexSearcher searcher = newSearcher(r);
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestFieldCacheRangeFilter.java b/lucene/core/src/test/org/apache/lucene/search/TestFieldCacheRangeFilter.java
index a71b969..669e179 100644
--- a/lucene/core/src/test/org/apache/lucene/search/TestFieldCacheRangeFilter.java
+++ b/lucene/core/src/test/org/apache/lucene/search/TestFieldCacheRangeFilter.java
@@ -20,6 +20,7 @@ package org.apache.lucene.search;
 import java.io.IOException;
 
 
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.IndexWriter;
@@ -537,7 +538,7 @@ public class TestFieldCacheRangeFilter extends BaseTestRangeFilter {
     writer.deleteDocuments(new Term("id","0"));
     writer.close();
 
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     IndexSearcher search = newSearcher(reader);
     assertTrue(reader.hasDeletions());
 
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestFieldValueFilter.java b/lucene/core/src/test/org/apache/lucene/search/TestFieldValueFilter.java
index 3175b4f..dd8b54f 100644
--- a/lucene/core/src/test/org/apache/lucene/search/TestFieldValueFilter.java
+++ b/lucene/core/src/test/org/apache/lucene/search/TestFieldValueFilter.java
@@ -22,6 +22,7 @@ import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
@@ -46,7 +47,7 @@ public class TestFieldValueFilter extends LuceneTestCase {
       }
     }
 
-    IndexReader reader = IndexReader.open(directory);
+    IndexReader reader = DirectoryReader.open(directory);
     IndexSearcher searcher = new IndexSearcher(reader);
     TopDocs search = searcher.search(new TermQuery(new Term("all", "test")),
         new FieldValueFilter("some", true), docs);
@@ -73,7 +74,7 @@ public class TestFieldValueFilter extends LuceneTestCase {
         numDocsWithValue++;
       }
     }
-    IndexReader reader = IndexReader.open(directory);
+    IndexReader reader = DirectoryReader.open(directory);
     IndexSearcher searcher = new IndexSearcher(reader);
     TopDocs search = searcher.search(new TermQuery(new Term("all", "test")),
         new FieldValueFilter("some"), docs);
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestFilteredSearch.java b/lucene/core/src/test/org/apache/lucene/search/TestFilteredSearch.java
index 7cfdd3e..7eeb9bb 100644
--- a/lucene/core/src/test/org/apache/lucene/search/TestFilteredSearch.java
+++ b/lucene/core/src/test/org/apache/lucene/search/TestFilteredSearch.java
@@ -25,6 +25,7 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.Term;
@@ -78,7 +79,7 @@ public class TestFilteredSearch extends LuceneTestCase {
       booleanQuery.add(new TermQuery(new Term(FIELD, "36")), BooleanClause.Occur.SHOULD);
      
      
-      IndexReader reader = IndexReader.open(directory);
+      IndexReader reader = DirectoryReader.open(directory);
       IndexSearcher indexSearcher = new IndexSearcher(reader);
       ScoreDoc[] hits = indexSearcher.search(booleanQuery, filter, 1000).scoreDocs;
       assertEquals("Number of matched documents", 1, hits.length);
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestMatchAllDocsQuery.java b/lucene/core/src/test/org/apache/lucene/search/TestMatchAllDocsQuery.java
index c3eb150..211fac5 100644
--- a/lucene/core/src/test/org/apache/lucene/search/TestMatchAllDocsQuery.java
+++ b/lucene/core/src/test/org/apache/lucene/search/TestMatchAllDocsQuery.java
@@ -23,6 +23,7 @@ import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.IndexReader;
@@ -50,7 +51,7 @@ public class TestMatchAllDocsQuery extends LuceneTestCase {
     addDoc("one", iw, 1f);
     addDoc("two", iw, 20f);
     addDoc("three four", iw, 300f);
-    IndexReader ir = IndexReader.open(iw, true);
+    IndexReader ir = DirectoryReader.open(iw, true);
 
     IndexSearcher is = newSearcher(ir);
     ScoreDoc[] hits;
@@ -77,7 +78,7 @@ public class TestMatchAllDocsQuery extends LuceneTestCase {
 
     iw.deleteDocuments(new Term("key", "one"));
     ir.close();
-    ir = IndexReader.open(iw, true);
+    ir = DirectoryReader.open(iw, true);
     is = newSearcher(ir);
     
     hits = is.search(new MatchAllDocsQuery(), null, 1000).scoreDocs;
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestMultiPhraseQuery.java b/lucene/core/src/test/org/apache/lucene/search/TestMultiPhraseQuery.java
index 70dffd4..c696ae8 100644
--- a/lucene/core/src/test/org/apache/lucene/search/TestMultiPhraseQuery.java
+++ b/lucene/core/src/test/org/apache/lucene/search/TestMultiPhraseQuery.java
@@ -25,6 +25,7 @@ import org.apache.lucene.analysis.Token;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -470,7 +471,7 @@ public class TestMultiPhraseQuery extends LuceneTestCase {
     Document doc = new Document();
     doc.add(new TextField("field", new CannedTokenStream(INCR_0_DOC_TOKENS)));
     writer.addDocument(doc);
-    IndexReader r = IndexReader.open(writer,false);
+    IndexReader r = DirectoryReader.open(writer,false);
     writer.close();
     IndexSearcher s = new IndexSearcher(r);
     
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestMultiTermQueryRewrites.java b/lucene/core/src/test/org/apache/lucene/search/TestMultiTermQueryRewrites.java
index 533fc7c..4aceb1e 100644
--- a/lucene/core/src/test/org/apache/lucene/search/TestMultiTermQueryRewrites.java
+++ b/lucene/core/src/test/org/apache/lucene/search/TestMultiTermQueryRewrites.java
@@ -20,6 +20,7 @@ package org.apache.lucene.search;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.StringField;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.MultiReader;
 import org.apache.lucene.index.Term;
@@ -59,16 +60,16 @@ public class TestMultiTermQueryRewrites extends LuceneTestCase {
     writer.forceMerge(1); swriter1.forceMerge(1); swriter2.forceMerge(1);
     writer.close(); swriter1.close(); swriter2.close();
     
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
     searcher = newSearcher(reader);
     
     multiReader = new MultiReader(new IndexReader[] {
-      IndexReader.open(sdir1), IndexReader.open(sdir2) 
+      DirectoryReader.open(sdir1), DirectoryReader.open(sdir2) 
     }, true);
     multiSearcher = newSearcher(multiReader);
     
     multiReaderDupls = new MultiReader(new IndexReader[] {
-      IndexReader.open(sdir1), IndexReader.open(dir) 
+      DirectoryReader.open(sdir1), DirectoryReader.open(dir) 
     }, true);
     multiSearcherDupls = newSearcher(multiReaderDupls);
   }
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestMultiThreadTermVectors.java b/lucene/core/src/test/org/apache/lucene/search/TestMultiThreadTermVectors.java
index 2d96504..e98df81 100644
--- a/lucene/core/src/test/org/apache/lucene/search/TestMultiThreadTermVectors.java
+++ b/lucene/core/src/test/org/apache/lucene/search/TestMultiThreadTermVectors.java
@@ -21,6 +21,7 @@ import java.io.IOException;
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.*;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.Fields;
 import org.apache.lucene.index.FieldsEnum;
 import org.apache.lucene.index.IndexReader;
@@ -67,7 +68,7 @@ public class TestMultiThreadTermVectors extends LuceneTestCase {
     IndexReader reader = null;
     
     try {
-      reader = IndexReader.open(directory);
+      reader = DirectoryReader.open(directory);
       for(int i = 1; i <= numThreads; i++)
         testTermPositionVectors(reader, i);
       
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestNGramPhraseQuery.java b/lucene/core/src/test/org/apache/lucene/search/TestNGramPhraseQuery.java
index 11a22b3..09a9ba2 100644
--- a/lucene/core/src/test/org/apache/lucene/search/TestNGramPhraseQuery.java
+++ b/lucene/core/src/test/org/apache/lucene/search/TestNGramPhraseQuery.java
@@ -17,6 +17,7 @@ package org.apache.lucene.search;
  * limitations under the License.
  */
 
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
@@ -35,7 +36,7 @@ public class TestNGramPhraseQuery extends LuceneTestCase {
     directory = newDirectory();
     RandomIndexWriter writer = new RandomIndexWriter(random(), directory);
     writer.close();
-    reader = IndexReader.open(directory);
+    reader = DirectoryReader.open(directory);
   }
 
   @AfterClass
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestNumericRangeQuery32.java b/lucene/core/src/test/org/apache/lucene/search/TestNumericRangeQuery32.java
index cdb966a..e2fe478 100644
--- a/lucene/core/src/test/org/apache/lucene/search/TestNumericRangeQuery32.java
+++ b/lucene/core/src/test/org/apache/lucene/search/TestNumericRangeQuery32.java
@@ -23,6 +23,7 @@ import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.FloatField;
 import org.apache.lucene.document.IntField;
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.MultiFields;
 import org.apache.lucene.index.RandomIndexWriter;
@@ -324,7 +325,7 @@ public class TestNumericRangeQuery32 extends LuceneTestCase {
     
     writer.close();
     
-    IndexReader r = IndexReader.open(dir);
+    IndexReader r = DirectoryReader.open(dir);
     IndexSearcher s = new IndexSearcher(r);
     
     Query q=NumericRangeQuery.newIntRange("int", null, null, true, true);
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestNumericRangeQuery64.java b/lucene/core/src/test/org/apache/lucene/search/TestNumericRangeQuery64.java
index f228e51..bd78d00 100644
--- a/lucene/core/src/test/org/apache/lucene/search/TestNumericRangeQuery64.java
+++ b/lucene/core/src/test/org/apache/lucene/search/TestNumericRangeQuery64.java
@@ -23,6 +23,7 @@ import org.apache.lucene.document.DoubleField;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.LongField;
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.MultiFields;
 import org.apache.lucene.index.RandomIndexWriter;
@@ -349,7 +350,7 @@ public class TestNumericRangeQuery64 extends LuceneTestCase {
     
     writer.close();
     
-    IndexReader r = IndexReader.open(dir);
+    IndexReader r = DirectoryReader.open(dir);
     IndexSearcher s = new IndexSearcher(r);
     
     Query q=NumericRangeQuery.newLongRange("long", null, null, true, true);
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestScorerPerf.java b/lucene/core/src/test/org/apache/lucene/search/TestScorerPerf.java
index 4953c39..c8d5bb5 100755
--- a/lucene/core/src/test/org/apache/lucene/search/TestScorerPerf.java
+++ b/lucene/core/src/test/org/apache/lucene/search/TestScorerPerf.java
@@ -8,6 +8,7 @@ import java.util.BitSet;
 import java.io.IOException;
 
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.Term;
@@ -51,7 +52,7 @@ public class TestScorerPerf extends LuceneTestCase {
     IndexWriter iw = new IndexWriter(d, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())));
     iw.addDocument(new Document());
     iw.close();
-    r = IndexReader.open(d);
+    r = DirectoryReader.open(d);
     s = new IndexSearcher(r);
   }
 
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestSort.java b/lucene/core/src/test/org/apache/lucene/search/TestSort.java
index 85e5eea..85e568e 100644
--- a/lucene/core/src/test/org/apache/lucene/search/TestSort.java
+++ b/lucene/core/src/test/org/apache/lucene/search/TestSort.java
@@ -268,7 +268,7 @@ public class TestSort extends LuceneTestCase {
     //writer.forceMerge(1);
     //System.out.println(writer.getSegmentCount());
     writer.close();
-    IndexReader reader = IndexReader.open(indexStore);
+    IndexReader reader = DirectoryReader.open(indexStore);
     return newSearcher(reader);
   }
   
@@ -1254,7 +1254,7 @@ public class TestSort extends LuceneTestCase {
     doc.add(newField("t", "1", StringField.TYPE_UNSTORED));
     w.addDocument(doc);
 
-    IndexReader r = IndexReader.open(w, true);
+    IndexReader r = DirectoryReader.open(w, true);
     w.close();
     IndexSearcher s = newSearcher(r);
     TopDocs hits = s.search(new TermQuery(new Term("t", "1")), null, 10, new Sort(new SortField("f", SortField.Type.STRING)));
@@ -1282,7 +1282,7 @@ public class TestSort extends LuceneTestCase {
         new SortField("string", SortField.Type.STRING),
         SortField.FIELD_DOC );
     // this should not throw AIOOBE or RuntimeEx
-    IndexReader reader = IndexReader.open(indexStore);
+    IndexReader reader = DirectoryReader.open(indexStore);
     IndexSearcher searcher = new IndexSearcher(reader);
     searcher.search(new MatchAllDocsQuery(), null, 500, sort);
     reader.close();
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestTermRangeQuery.java b/lucene/core/src/test/org/apache/lucene/search/TestTermRangeQuery.java
index 397389f..d6e5874 100644
--- a/lucene/core/src/test/org/apache/lucene/search/TestTermRangeQuery.java
+++ b/lucene/core/src/test/org/apache/lucene/search/TestTermRangeQuery.java
@@ -26,6 +26,7 @@ import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.MultiFields;
@@ -55,21 +56,21 @@ public class TestTermRangeQuery extends LuceneTestCase {
   public void testExclusive() throws Exception {
     Query query = TermRangeQuery.newStringRange("content", "A", "C", false, false);
     initializeIndex(new String[] {"A", "B", "C", "D"});
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     IndexSearcher searcher = new IndexSearcher(reader);
     ScoreDoc[] hits = searcher.search(query, null, 1000).scoreDocs;
     assertEquals("A,B,C,D, only B in range", 1, hits.length);
     reader.close();
 
     initializeIndex(new String[] {"A", "B", "D"});
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
     searcher = new IndexSearcher(reader);
     hits = searcher.search(query, null, 1000).scoreDocs;
     assertEquals("A,B,D, only B in range", 1, hits.length);
     reader.close();
 
     addDoc("C");
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
     searcher = new IndexSearcher(reader);
     hits = searcher.search(query, null, 1000).scoreDocs;
     assertEquals("C added, still only B in range", 1, hits.length);
@@ -80,21 +81,21 @@ public class TestTermRangeQuery extends LuceneTestCase {
     Query query = TermRangeQuery.newStringRange("content", "A", "C", true, true);
 
     initializeIndex(new String[]{"A", "B", "C", "D"});
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     IndexSearcher searcher = new IndexSearcher(reader);
     ScoreDoc[] hits = searcher.search(query, null, 1000).scoreDocs;
     assertEquals("A,B,C,D - A,B,C in range", 3, hits.length);
     reader.close();
 
     initializeIndex(new String[]{"A", "B", "D"});
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
     searcher = new IndexSearcher(reader);
     hits = searcher.search(query, null, 1000).scoreDocs;
     assertEquals("A,B,D - A and B in range", 2, hits.length);
     reader.close();
 
     addDoc("C");
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
     searcher = new IndexSearcher(reader);
     hits = searcher.search(query, null, 1000).scoreDocs;
     assertEquals("C added - A, B, C in range", 3, hits.length);
@@ -103,7 +104,7 @@ public class TestTermRangeQuery extends LuceneTestCase {
   
   public void testAllDocs() throws Exception {
     initializeIndex(new String[]{"A", "B", "C", "D"});
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     IndexSearcher searcher = new IndexSearcher(reader);
     TermRangeQuery query = new TermRangeQuery("content", null, null, true, true);
     Terms terms = MultiFields.getTerms(searcher.getIndexReader(), "content");
@@ -127,7 +128,7 @@ public class TestTermRangeQuery extends LuceneTestCase {
   public void testTopTermsRewrite() throws Exception {
     initializeIndex(new String[]{"A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K"});
 
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     IndexSearcher searcher = new IndexSearcher(reader);
     TermRangeQuery query = TermRangeQuery.newStringRange("content", "B", "J", true, true);
     checkBooleanTerms(searcher, query, "B", "C", "D", "E", "F", "G", "H", "I", "J");
@@ -276,7 +277,7 @@ public class TestTermRangeQuery extends LuceneTestCase {
     Query query = TermRangeQuery.newStringRange("content", null, "C",
                                  false, false);
     initializeIndex(new String[] {"A", "B", "", "C", "D"}, analyzer);
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     IndexSearcher searcher = new IndexSearcher(reader);
     int numHits = searcher.search(query, null, 1000).totalHits;
     // When Lucene-38 is fixed, use the assert on the next line:
@@ -286,7 +287,7 @@ public class TestTermRangeQuery extends LuceneTestCase {
 
     reader.close();
     initializeIndex(new String[] {"A", "B", "", "D"}, analyzer);
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
     searcher = new IndexSearcher(reader);
     numHits = searcher.search(query, null, 1000).totalHits;
     // When Lucene-38 is fixed, use the assert on the next line:
@@ -295,7 +296,7 @@ public class TestTermRangeQuery extends LuceneTestCase {
     //assertEquals("A,B,<empty string>,D => A, B & <empty string> are in range", 2, hits.length());
     reader.close();
     addDoc("C");
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
     searcher = new IndexSearcher(reader);
     numHits = searcher.search(query, null, 1000).totalHits;
     // When Lucene-38 is fixed, use the assert on the next line:
@@ -311,7 +312,7 @@ public class TestTermRangeQuery extends LuceneTestCase {
     Analyzer analyzer = new SingleCharAnalyzer();
     Query query = TermRangeQuery.newStringRange("content", null, "C", true, true);
     initializeIndex(new String[]{"A", "B", "","C", "D"}, analyzer);
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     IndexSearcher searcher = new IndexSearcher(reader);
     int numHits = searcher.search(query, null, 1000).totalHits;
     // When Lucene-38 is fixed, use the assert on the next line:
@@ -320,7 +321,7 @@ public class TestTermRangeQuery extends LuceneTestCase {
     //assertEquals("A,B,<empty string>,C,D => A,B,<empty string>,C in range", 3, hits.length());
     reader.close();
     initializeIndex(new String[]{"A", "B", "", "D"}, analyzer);
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
     searcher = new IndexSearcher(reader);
     numHits = searcher.search(query, null, 1000).totalHits;
     // When Lucene-38 is fixed, use the assert on the next line:
@@ -329,7 +330,7 @@ public class TestTermRangeQuery extends LuceneTestCase {
     //assertEquals("A,B,<empty string>,D => A, B and <empty string> in range", 2, hits.length());
     reader.close();
     addDoc("C");
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
     searcher = new IndexSearcher(reader);
     numHits = searcher.search(query, null, 1000).totalHits;
     // When Lucene-38 is fixed, use the assert on the next line:
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestTermVectors.java b/lucene/core/src/test/org/apache/lucene/search/TestTermVectors.java
index e8cec93..605840d 100644
--- a/lucene/core/src/test/org/apache/lucene/search/TestTermVectors.java
+++ b/lucene/core/src/test/org/apache/lucene/search/TestTermVectors.java
@@ -468,7 +468,7 @@ public class TestTermVectors extends LuceneTestCase {
   }
 
   private void verifyIndex(Directory dir) throws IOException {
-    IndexReader r = IndexReader.open(dir);
+    IndexReader r = DirectoryReader.open(dir);
     int numDocs = r.numDocs();
     for (int i = 0; i < numDocs; i++) {
       assertNotNull("term vectors should not have been null for document " + i, r.getTermVectors(i).terms("c"));
@@ -519,7 +519,7 @@ public class TestTermVectors extends LuceneTestCase {
     
     IndexWriter writer = createWriter(target);
     for (Directory dir : input) {
-      IndexReader r = IndexReader.open(dir);
+      IndexReader r = DirectoryReader.open(dir);
       writer.addIndexes(r);
       r.close();
     }
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestWildcard.java b/lucene/core/src/test/org/apache/lucene/search/TestWildcard.java
index bc97479..e2ffa3f 100644
--- a/lucene/core/src/test/org/apache/lucene/search/TestWildcard.java
+++ b/lucene/core/src/test/org/apache/lucene/search/TestWildcard.java
@@ -22,6 +22,7 @@ import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.MultiFields;
 import org.apache.lucene.index.RandomIndexWriter;
@@ -68,7 +69,7 @@ public class TestWildcard
    */
   public void testTermWithoutWildcard() throws IOException {
       Directory indexStore = getIndexStore("field", new String[]{"nowildcard", "nowildcardx"});
-      IndexReader reader = IndexReader.open(indexStore);
+      IndexReader reader = DirectoryReader.open(indexStore);
       IndexSearcher searcher = new IndexSearcher(reader);
 
       MultiTermQuery wq = new WildcardQuery(new Term("field", "nowildcard"));
@@ -106,7 +107,7 @@ public class TestWildcard
    */
   public void testEmptyTerm() throws IOException {
     Directory indexStore = getIndexStore("field", new String[]{"nowildcard", "nowildcardx"});
-    IndexReader reader = IndexReader.open(indexStore);
+    IndexReader reader = DirectoryReader.open(indexStore);
     IndexSearcher searcher = new IndexSearcher(reader);
 
     MultiTermQuery wq = new WildcardQuery(new Term("field", ""));
@@ -126,7 +127,7 @@ public class TestWildcard
    */
   public void testPrefixTerm() throws IOException {
     Directory indexStore = getIndexStore("field", new String[]{"prefix", "prefixx"});
-    IndexReader reader = IndexReader.open(indexStore);
+    IndexReader reader = DirectoryReader.open(indexStore);
     IndexSearcher searcher = new IndexSearcher(reader);
 
     MultiTermQuery wq = new WildcardQuery(new Term("field", "prefix*"));
@@ -149,7 +150,7 @@ public class TestWildcard
       throws IOException {
     Directory indexStore = getIndexStore("body", new String[]
     {"metal", "metals"});
-    IndexReader reader = IndexReader.open(indexStore);
+    IndexReader reader = DirectoryReader.open(indexStore);
     IndexSearcher searcher = new IndexSearcher(reader);
     Query query1 = new TermQuery(new Term("body", "metal"));
     Query query2 = new WildcardQuery(new Term("body", "metal*"));
@@ -191,7 +192,7 @@ public class TestWildcard
       throws IOException {
     Directory indexStore = getIndexStore("body", new String[]
     {"metal", "metals", "mXtals", "mXtXls"});
-    IndexReader reader = IndexReader.open(indexStore);
+    IndexReader reader = DirectoryReader.open(indexStore);
     IndexSearcher searcher = new IndexSearcher(reader);
     Query query1 = new WildcardQuery(new Term("body", "m?tal"));
     Query query2 = new WildcardQuery(new Term("body", "metal?"));
@@ -216,7 +217,7 @@ public class TestWildcard
   public void testEscapes() throws Exception {
     Directory indexStore = getIndexStore("field", 
         new String[]{"foo*bar", "foo??bar", "fooCDbar", "fooSOMETHINGbar", "foo\\"});
-    IndexReader reader = IndexReader.open(indexStore);
+    IndexReader reader = DirectoryReader.open(indexStore);
     IndexSearcher searcher = new IndexSearcher(reader);
 
     // without escape: matches foo??bar, fooCDbar, foo*bar, and fooSOMETHINGbar
@@ -354,7 +355,7 @@ public class TestWildcard
     }
     iw.close();
     
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     IndexSearcher searcher = new IndexSearcher(reader);
     
     // test queries that must find all
diff --git a/lucene/core/src/test/org/apache/lucene/search/payloads/PayloadHelper.java b/lucene/core/src/test/org/apache/lucene/search/payloads/PayloadHelper.java
index 132b33d..0e92850 100644
--- a/lucene/core/src/test/org/apache/lucene/search/payloads/PayloadHelper.java
+++ b/lucene/core/src/test/org/apache/lucene/search/payloads/PayloadHelper.java
@@ -19,6 +19,7 @@ package org.apache.lucene.search.payloads;
 
 import org.apache.lucene.analysis.*;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.Payload;
 import org.apache.lucene.index.IndexWriter;
@@ -130,7 +131,7 @@ public class PayloadHelper {
       doc.add(new Field(NO_PAYLOAD_FIELD, English.intToEnglish(i), TextField.TYPE_STORED));
       writer.addDocument(doc);
     }
-    reader = IndexReader.open(writer, true);
+    reader = DirectoryReader.open(writer, true);
     writer.close();
 
     IndexSearcher searcher = LuceneTestCase.newSearcher(reader);
diff --git a/lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadTermQuery.java b/lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadTermQuery.java
index 68456b1..b53273d 100644
--- a/lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadTermQuery.java
+++ b/lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadTermQuery.java
@@ -33,6 +33,7 @@ import org.apache.lucene.search.spans.MultiSpansWrapper;
 import org.apache.lucene.search.spans.SpanTermQuery;
 import org.apache.lucene.search.spans.Spans;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.FieldInvertState;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Norm;
@@ -230,7 +231,7 @@ public class TestPayloadTermQuery extends LuceneTestCase {
     PayloadTermQuery query = new PayloadTermQuery(new Term(PayloadHelper.MULTI_FIELD, "seventy"),
             new MaxPayloadFunction(), false);
 
-    IndexReader reader = IndexReader.open(directory);
+    IndexReader reader = DirectoryReader.open(directory);
     IndexSearcher theSearcher = new IndexSearcher(reader);
     theSearcher.setSimilarity(new FullSimilarity());
     TopDocs hits = searcher.search(query, null, 100);
diff --git a/lucene/core/src/test/org/apache/lucene/search/spans/TestSpans.java b/lucene/core/src/test/org/apache/lucene/search/spans/TestSpans.java
index eb5e7c0..7d334a2 100644
--- a/lucene/core/src/test/org/apache/lucene/search/spans/TestSpans.java
+++ b/lucene/core/src/test/org/apache/lucene/search/spans/TestSpans.java
@@ -24,6 +24,7 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.index.IndexWriter;
@@ -484,7 +485,7 @@ public class TestSpans extends LuceneTestCase {
     writer.close();
 
     // Get searcher
-    final IndexReader reader = IndexReader.open(dir);
+    final IndexReader reader = DirectoryReader.open(dir);
     final IndexSearcher searcher = newSearcher(reader);
 
     // Control (make sure docs indexed)
diff --git a/lucene/core/src/test/org/apache/lucene/search/spans/TestSpansAdvanced2.java b/lucene/core/src/test/org/apache/lucene/search/spans/TestSpansAdvanced2.java
index 9aaba8b..c6c97d5 100644
--- a/lucene/core/src/test/org/apache/lucene/search/spans/TestSpansAdvanced2.java
+++ b/lucene/core/src/test/org/apache/lucene/search/spans/TestSpansAdvanced2.java
@@ -22,6 +22,7 @@ import java.io.IOException;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenFilter;
 import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
@@ -75,7 +76,7 @@ public class TestSpansAdvanced2 extends TestSpansAdvanced {
    * @throws Exception
    */
   public void testVerifyIndex() throws Exception {
-    final IndexReader reader = IndexReader.open(mDirectory);
+    final IndexReader reader = DirectoryReader.open(mDirectory);
     assertEquals(8, reader.numDocs());
     reader.close();
   }
diff --git a/lucene/core/src/test/org/apache/lucene/store/TestBufferedIndexInput.java b/lucene/core/src/test/org/apache/lucene/store/TestBufferedIndexInput.java
index eff89f0..3e1aea1 100755
--- a/lucene/core/src/test/org/apache/lucene/store/TestBufferedIndexInput.java
+++ b/lucene/core/src/test/org/apache/lucene/store/TestBufferedIndexInput.java
@@ -29,6 +29,7 @@ import java.util.Random;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -259,7 +260,7 @@ public class TestBufferedIndexInput extends LuceneTestCase {
 
         dir.allIndexInputs.clear();
 
-        IndexReader reader = IndexReader.open(writer, true);
+        IndexReader reader = DirectoryReader.open(writer, true);
         Term aaa = new Term("content", "aaa");
         Term bbb = new Term("content", "bbb");
         
@@ -267,7 +268,7 @@ public class TestBufferedIndexInput extends LuceneTestCase {
         
         dir.tweakBufferSizes();
         writer.deleteDocuments(new Term("id", "0"));
-        reader = IndexReader.open(writer, true);
+        reader = DirectoryReader.open(writer, true);
         IndexSearcher searcher = newSearcher(reader);
         ScoreDoc[] hits = searcher.search(new TermQuery(bbb), null, 1000).scoreDocs;
         dir.tweakBufferSizes();
@@ -277,7 +278,7 @@ public class TestBufferedIndexInput extends LuceneTestCase {
         
         dir.tweakBufferSizes();
         writer.deleteDocuments(new Term("id", "4"));
-        reader = IndexReader.open(writer, true);
+        reader = DirectoryReader.open(writer, true);
         searcher = newSearcher(reader);
 
         hits = searcher.search(new TermQuery(bbb), null, 1000).scoreDocs;
diff --git a/lucene/core/src/test/org/apache/lucene/store/TestFileSwitchDirectory.java b/lucene/core/src/test/org/apache/lucene/store/TestFileSwitchDirectory.java
index d118648..ba950b5 100644
--- a/lucene/core/src/test/org/apache/lucene/store/TestFileSwitchDirectory.java
+++ b/lucene/core/src/test/org/apache/lucene/store/TestFileSwitchDirectory.java
@@ -26,6 +26,7 @@ import java.util.Set;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsWriter;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -56,7 +57,7 @@ public class TestFileSwitchDirectory extends LuceneTestCase {
             setMergePolicy(newLogMergePolicy(false)).setCodec(Codec.forName("Lucene40"))
     );
     TestIndexWriterReader.createIndexNoClose(true, "ram", writer);
-    IndexReader reader = IndexReader.open(writer, true);
+    IndexReader reader = DirectoryReader.open(writer, true);
     assertEquals(100, reader.maxDoc());
     writer.commit();
     // we should see only fdx,fdt files here
@@ -94,7 +95,7 @@ public class TestFileSwitchDirectory extends LuceneTestCase {
   public void testNoDir() throws Throwable {
     Directory dir = newFSSwitchDirectory(Collections.<String>emptySet());
     try {
-      IndexReader.open(dir);
+      DirectoryReader.open(dir);
       fail("did not hit expected exception");
     } catch (NoSuchDirectoryException nsde) {
       // expected
diff --git a/lucene/core/src/test/org/apache/lucene/store/TestLockFactory.java b/lucene/core/src/test/org/apache/lucene/store/TestLockFactory.java
index 277dd21..f517fcb 100755
--- a/lucene/core/src/test/org/apache/lucene/store/TestLockFactory.java
+++ b/lucene/core/src/test/org/apache/lucene/store/TestLockFactory.java
@@ -26,6 +26,7 @@ import java.util.Map;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -347,7 +348,7 @@ public class TestLockFactory extends LuceneTestCase {
             Query query = new TermQuery(new Term("content", "aaa"));
             for(int i=0;i<this.numIteration;i++) {
                 try{
-                    reader = IndexReader.open(dir);
+                    reader = DirectoryReader.open(dir);
                     searcher = new IndexSearcher(reader);
                 } catch (Exception e) {
                     hitException = true;
diff --git a/lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory.java b/lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory.java
index e84f34a..274229f 100644
--- a/lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory.java
+++ b/lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory.java
@@ -63,7 +63,7 @@ public class TestNRTCachingDirectory extends LuceneTestCase {
       w.addDocument(doc);
       if (random().nextInt(20) == 17) {
         if (r == null) {
-          r = IndexReader.open(w.w, false);
+          r = DirectoryReader.open(w.w, false);
         } else {
           final DirectoryReader r2 = DirectoryReader.openIfChanged(r);
           if (r2 != null) {
@@ -93,7 +93,7 @@ public class TestNRTCachingDirectory extends LuceneTestCase {
     }
     assertEquals(0, cachedFiles.length);
     
-    r = IndexReader.open(dir);
+    r = DirectoryReader.open(dir);
     for(BytesRef id : ids) {
       assertEquals(1, r.docFreq("docid", id));
     }
@@ -109,7 +109,7 @@ public class TestNRTCachingDirectory extends LuceneTestCase {
 
     Directory fsDir = FSDirectory.open(new File("/path/to/index"));
     NRTCachingDirectory cachedFSDir = new NRTCachingDirectory(fsDir, 2.0, 25.0);
-    IndexWriterConfig conf = new IndexWriterConfig(Version.LUCENE_32, analyzer);
+    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
     IndexWriter writer = new IndexWriter(cachedFSDir, conf);
   }
 
@@ -125,7 +125,7 @@ public class TestNRTCachingDirectory extends LuceneTestCase {
   public void testNoDir() throws Throwable {
     Directory dir = new NRTCachingDirectory(newFSDirectory(_TestUtil.getTempDir("doesnotexist")), 2.0, 25.0);
     try {
-      IndexReader.open(dir);
+      DirectoryReader.open(dir);
       fail("did not hit expected exception");
     } catch (NoSuchDirectoryException nsde) {
       // expected
diff --git a/lucene/core/src/test/org/apache/lucene/store/TestRAMDirectory.java b/lucene/core/src/test/org/apache/lucene/store/TestRAMDirectory.java
index ff63824..3d0a911 100644
--- a/lucene/core/src/test/org/apache/lucene/store/TestRAMDirectory.java
+++ b/lucene/core/src/test/org/apache/lucene/store/TestRAMDirectory.java
@@ -25,6 +25,7 @@ import org.apache.lucene.util._TestUtil;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.StringField;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -76,7 +77,7 @@ public class TestRAMDirectory extends LuceneTestCase {
     assertEquals(ramDir.sizeInBytes(), ramDir.getRecomputedSizeInBytes());
     
     // open reader to test document count
-    IndexReader reader = IndexReader.open(ramDir);
+    IndexReader reader = DirectoryReader.open(ramDir);
     assertEquals(docsToAdd, reader.numDocs());
     
     // open search zo check if all doc's are there
diff --git a/lucene/core/src/test/org/apache/lucene/store/TestWindowsMMap.java b/lucene/core/src/test/org/apache/lucene/store/TestWindowsMMap.java
index 7819929..037776a 100644
--- a/lucene/core/src/test/org/apache/lucene/store/TestWindowsMMap.java
+++ b/lucene/core/src/test/org/apache/lucene/store/TestWindowsMMap.java
@@ -25,6 +25,7 @@ import org.apache.lucene.util._TestUtil;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -76,7 +77,7 @@ public class TestWindowsMMap extends LuceneTestCase {
         TEST_VERSION_CURRENT, analyzer)
         .setOpenMode(OpenMode.CREATE));
     writer.commit();
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     IndexSearcher searcher = new IndexSearcher(reader);
     
     int num = atLeast(1000);
diff --git a/lucene/core/src/test/org/apache/lucene/util/TestIndexableBinaryStringTools.java b/lucene/core/src/test/org/apache/lucene/util/TestIndexableBinaryStringTools.java
deleted file mode 100644
index e94cefb..0000000
--- a/lucene/core/src/test/org/apache/lucene/util/TestIndexableBinaryStringTools.java
+++ /dev/null
@@ -1,230 +0,0 @@
-package org.apache.lucene.util;
-
-import org.junit.BeforeClass;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * @deprecated Remove when IndexableBinaryStringTools is removed.
- */
-@Deprecated
-public class TestIndexableBinaryStringTools extends LuceneTestCase {
-  private static int NUM_RANDOM_TESTS;
-  private static int MAX_RANDOM_BINARY_LENGTH;
-  
-  @BeforeClass
-  public static void beforeClass() throws Exception {
-    NUM_RANDOM_TESTS = atLeast(200);
-    MAX_RANDOM_BINARY_LENGTH = atLeast(300);
-  }
-  
-  public void testSingleBinaryRoundTrip() {
-    byte[] binary = new byte[] { (byte) 0x23, (byte) 0x98, (byte) 0x13,
-        (byte) 0xE4, (byte) 0x76, (byte) 0x41, (byte) 0xB2, (byte) 0xC9,
-        (byte) 0x7F, (byte) 0x0A, (byte) 0xA6, (byte) 0xD8 };
-
-    int encodedLen = IndexableBinaryStringTools.getEncodedLength(binary, 0,
-        binary.length);
-    char encoded[] = new char[encodedLen];
-    IndexableBinaryStringTools.encode(binary, 0, binary.length, encoded, 0,
-        encoded.length);
-
-    int decodedLen = IndexableBinaryStringTools.getDecodedLength(encoded, 0,
-        encoded.length);
-    byte decoded[] = new byte[decodedLen];
-    IndexableBinaryStringTools.decode(encoded, 0, encoded.length, decoded, 0,
-        decoded.length);
-
-    assertEquals("Round trip decode/decode returned different results:"
-        + System.getProperty("line.separator") + "original: "
-        + binaryDump(binary, binary.length)
-        + System.getProperty("line.separator") + " encoded: "
-        + charArrayDump(encoded, encoded.length)
-        + System.getProperty("line.separator") + " decoded: "
-        + binaryDump(decoded, decoded.length),
-        binaryDump(binary, binary.length), binaryDump(decoded, decoded.length));
-  }
-  
-  public void testEncodedSortability() {
-    byte[] originalArray1 = new byte[MAX_RANDOM_BINARY_LENGTH];
-    char[] originalString1 = new char[MAX_RANDOM_BINARY_LENGTH];
-    char[] encoded1 = new char[MAX_RANDOM_BINARY_LENGTH * 10];
-    byte[] original2 = new byte[MAX_RANDOM_BINARY_LENGTH];
-    char[] originalString2 = new char[MAX_RANDOM_BINARY_LENGTH];
-    char[] encoded2 = new char[MAX_RANDOM_BINARY_LENGTH * 10];
-
-    for (int testNum = 0; testNum < NUM_RANDOM_TESTS; ++testNum) {
-      int numBytes1 = random().nextInt(MAX_RANDOM_BINARY_LENGTH - 1) + 1; // Min == 1
-
-      for (int byteNum = 0; byteNum < numBytes1; ++byteNum) {
-        int randomInt = random().nextInt(0x100);
-        originalArray1[byteNum] = (byte) randomInt;
-        originalString1[byteNum] = (char) randomInt;
-      }
-
-      int numBytes2 = random().nextInt(MAX_RANDOM_BINARY_LENGTH - 1) + 1; // Min == 1
-
-      for (int byteNum = 0; byteNum < numBytes2; ++byteNum) {
-        int randomInt = random().nextInt(0x100);
-        original2[byteNum] = (byte) randomInt;
-        originalString2[byteNum] = (char) randomInt;
-      }
-      int originalComparison = new String(originalString1, 0, numBytes1)
-          .compareTo(new String(originalString2, 0, numBytes2));
-      originalComparison = originalComparison < 0 ? -1
-          : originalComparison > 0 ? 1 : 0;
-
-      int encodedLen1 = IndexableBinaryStringTools.getEncodedLength(
-          originalArray1, 0, numBytes1);
-      if (encodedLen1 > encoded1.length)
-        encoded1 = new char[ArrayUtil.oversize(encodedLen1, RamUsageEstimator.NUM_BYTES_CHAR)];
-      IndexableBinaryStringTools.encode(originalArray1, 0, numBytes1, encoded1,
-          0, encodedLen1);
-
-      int encodedLen2 = IndexableBinaryStringTools.getEncodedLength(original2,
-          0, numBytes2);
-      if (encodedLen2 > encoded2.length)
-        encoded2 = new char[ArrayUtil.oversize(encodedLen2, RamUsageEstimator.NUM_BYTES_CHAR)];
-      IndexableBinaryStringTools.encode(original2, 0, numBytes2, encoded2, 0,
-          encodedLen2);
-
-      int encodedComparison = new String(encoded1, 0, encodedLen1)
-          .compareTo(new String(encoded2, 0, encodedLen2));
-      encodedComparison = encodedComparison < 0 ? -1
-          : encodedComparison > 0 ? 1 : 0;
-
-      assertEquals("Test #" + (testNum + 1)
-          + ": Original bytes and encoded chars compare differently:"
-          + System.getProperty("line.separator") + " binary 1: "
-          + binaryDump(originalArray1, numBytes1)
-          + System.getProperty("line.separator") + " binary 2: "
-          + binaryDump(original2, numBytes2)
-          + System.getProperty("line.separator") + "encoded 1: "
-          + charArrayDump(encoded1, encodedLen1)
-          + System.getProperty("line.separator") + "encoded 2: "
-          + charArrayDump(encoded2, encodedLen2)
-          + System.getProperty("line.separator"), originalComparison,
-          encodedComparison);
-    }
-  }
-
-  public void testEmptyInput() {
-    byte[] binary = new byte[0];
-
-    int encodedLen = IndexableBinaryStringTools.getEncodedLength(binary, 0,
-        binary.length);
-    char[] encoded = new char[encodedLen];
-    IndexableBinaryStringTools.encode(binary, 0, binary.length, encoded, 0,
-        encoded.length);
-
-    int decodedLen = IndexableBinaryStringTools.getDecodedLength(encoded, 0,
-        encoded.length);
-    byte[] decoded = new byte[decodedLen];
-    IndexableBinaryStringTools.decode(encoded, 0, encoded.length, decoded, 0,
-        decoded.length);
-
-    assertEquals("decoded empty input was not empty", decoded.length, 0);
-  }
-  
-  public void testAllNullInput() {
-    byte[] binary = new byte[] { 0, 0, 0, 0, 0, 0, 0, 0, 0 };
-
-    int encodedLen = IndexableBinaryStringTools.getEncodedLength(binary, 0,
-        binary.length);
-    char encoded[] = new char[encodedLen];
-    IndexableBinaryStringTools.encode(binary, 0, binary.length, encoded, 0,
-        encoded.length);
-
-    int decodedLen = IndexableBinaryStringTools.getDecodedLength(encoded, 0,
-        encoded.length);
-    byte[] decoded = new byte[decodedLen];
-    IndexableBinaryStringTools.decode(encoded, 0, encoded.length, decoded, 0,
-        decoded.length);
-
-    assertEquals("Round trip decode/decode returned different results:"
-        + System.getProperty("line.separator") + "  original: "
-        + binaryDump(binary, binary.length)
-        + System.getProperty("line.separator") + "decodedBuf: "
-        + binaryDump(decoded, decoded.length),
-        binaryDump(binary, binary.length), binaryDump(decoded, decoded.length));
-  }
-  
-  public void testRandomBinaryRoundTrip() {
-    byte[] binary = new byte[MAX_RANDOM_BINARY_LENGTH];
-    char[] encoded = new char[MAX_RANDOM_BINARY_LENGTH * 10];
-    byte[] decoded = new byte[MAX_RANDOM_BINARY_LENGTH];
-    for (int testNum = 0; testNum < NUM_RANDOM_TESTS; ++testNum) {
-      int numBytes = random().nextInt(MAX_RANDOM_BINARY_LENGTH - 1) + 1; // Min == 1                                                                   
-
-      for (int byteNum = 0; byteNum < numBytes; ++byteNum) {
-        binary[byteNum] = (byte) random().nextInt(0x100);
-      }
-
-      int encodedLen = IndexableBinaryStringTools.getEncodedLength(binary, 0,
-          numBytes);
-      if (encoded.length < encodedLen)
-        encoded = new char[ArrayUtil.oversize(encodedLen, RamUsageEstimator.NUM_BYTES_CHAR)];
-      IndexableBinaryStringTools.encode(binary, 0, numBytes, encoded, 0,
-          encodedLen);
-
-      int decodedLen = IndexableBinaryStringTools.getDecodedLength(encoded, 0,
-          encodedLen);
-      IndexableBinaryStringTools.decode(encoded, 0, encodedLen, decoded, 0,
-          decodedLen);
-
-      assertEquals("Test #" + (testNum + 1)
-          + ": Round trip decode/decode returned different results:"
-          + System.getProperty("line.separator") + "  original: "
-          + binaryDump(binary, numBytes) + System.getProperty("line.separator")
-          + "encodedBuf: " + charArrayDump(encoded, encodedLen)
-          + System.getProperty("line.separator") + "decodedBuf: "
-          + binaryDump(decoded, decodedLen), binaryDump(binary, numBytes),
-          binaryDump(decoded, decodedLen));
-    }
-  }
-  
-  public String binaryDump(byte[] binary, int numBytes) {
-    StringBuilder buf = new StringBuilder();
-    for (int byteNum = 0 ; byteNum < numBytes ; ++byteNum) {
-      String hex = Integer.toHexString(binary[byteNum] & 0xFF);
-      if (hex.length() == 1) {
-        buf.append('0');
-      }
-      buf.append(hex.toUpperCase());
-      if (byteNum < numBytes - 1) {
-        buf.append(' ');
-      }
-    }
-    return buf.toString();
-  }
-
-  public String charArrayDump(char[] charArray, int numBytes) {
-    StringBuilder buf = new StringBuilder();
-    for (int charNum = 0 ; charNum < numBytes ; ++charNum) {
-      String hex = Integer.toHexString(charArray[charNum]);
-      for (int digit = 0 ; digit < 4 - hex.length() ; ++digit) {
-        buf.append('0');
-      }
-      buf.append(hex.toUpperCase());
-      if (charNum < numBytes - 1) {
-        buf.append(' ');
-      }
-    }
-    return buf.toString();
-  }
-}
diff --git a/lucene/core/src/test/org/apache/lucene/util/TestVersion.java b/lucene/core/src/test/org/apache/lucene/util/TestVersion.java
index 5497787..564c3b1 100644
--- a/lucene/core/src/test/org/apache/lucene/util/TestVersion.java
+++ b/lucene/core/src/test/org/apache/lucene/util/TestVersion.java
@@ -23,9 +23,8 @@ public class TestVersion extends LuceneTestCase {
     for (Version v : Version.values()) {
       assertTrue("LUCENE_CURRENT must be always onOrAfter("+v+")", Version.LUCENE_CURRENT.onOrAfter(v));
     }
-    assertTrue(Version.LUCENE_40.onOrAfter(Version.LUCENE_31));
-    assertTrue(Version.LUCENE_40.onOrAfter(Version.LUCENE_40));
-    assertFalse(Version.LUCENE_30.onOrAfter(Version.LUCENE_31));
+    assertTrue(Version.LUCENE_50.onOrAfter(Version.LUCENE_40));
+    assertFalse(Version.LUCENE_40.onOrAfter(Version.LUCENE_50));
   }
 
   public void testParseLeniently() {
diff --git a/lucene/core/src/test/org/apache/lucene/util/fst/TestFSTs.java b/lucene/core/src/test/org/apache/lucene/util/fst/TestFSTs.java
index d180eb9..dc33992 100644
--- a/lucene/core/src/test/org/apache/lucene/util/fst/TestFSTs.java
+++ b/lucene/core/src/test/org/apache/lucene/util/fst/TestFSTs.java
@@ -34,6 +34,7 @@ import org.apache.lucene.codecs.lucene40.Lucene40PostingsFormat;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.StringField;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -1110,7 +1111,7 @@ public class TestFSTs extends LuceneTestCase {
       writer.addDocument(doc);
       docCount++;
     }
-    IndexReader r = IndexReader.open(writer, true);
+    IndexReader r = DirectoryReader.open(writer, true);
     writer.close();
     final PositiveIntOutputs outputs = PositiveIntOutputs.getSingleton(random().nextBoolean());
 
diff --git a/lucene/demo/src/java/org/apache/lucene/demo/SearchFiles.java b/lucene/demo/src/java/org/apache/lucene/demo/SearchFiles.java
index 7f5aa2f..f937d69 100644
--- a/lucene/demo/src/java/org/apache/lucene/demo/SearchFiles.java
+++ b/lucene/demo/src/java/org/apache/lucene/demo/SearchFiles.java
@@ -27,6 +27,7 @@ import java.util.Date;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.document.Document;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.queryparser.classic.QueryParser;
 import org.apache.lucene.search.IndexSearcher;
@@ -86,7 +87,7 @@ public class SearchFiles {
       }
     }
     
-    IndexReader reader = IndexReader.open(FSDirectory.open(new File(index)));
+    IndexReader reader = DirectoryReader.open(FSDirectory.open(new File(index)));
     IndexSearcher searcher = new IndexSearcher(reader);
     Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_40);
 
diff --git a/lucene/facet/src/examples/org/apache/lucene/facet/example/adaptive/AdaptiveSearcher.java b/lucene/facet/src/examples/org/apache/lucene/facet/example/adaptive/AdaptiveSearcher.java
index 6cf8083..593ac04 100644
--- a/lucene/facet/src/examples/org/apache/lucene/facet/example/adaptive/AdaptiveSearcher.java
+++ b/lucene/facet/src/examples/org/apache/lucene/facet/example/adaptive/AdaptiveSearcher.java
@@ -2,6 +2,7 @@ package org.apache.lucene.facet.example.adaptive;
 
 import java.util.List;
 
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.IndexSearcher;
@@ -56,7 +57,7 @@ public class AdaptiveSearcher {
   public static List<FacetResult> searchWithFacets (Directory indexDir, Directory taxoDir) throws Exception {
     // prepare index reader and taxonomy.
     TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
-    IndexReader indexReader = IndexReader.open(indexDir);
+    IndexReader indexReader = DirectoryReader.open(indexDir);
     
     // prepare searcher to search against
     IndexSearcher searcher = new IndexSearcher(indexReader);
diff --git a/lucene/facet/src/examples/org/apache/lucene/facet/example/association/AssociationSearcher.java b/lucene/facet/src/examples/org/apache/lucene/facet/example/association/AssociationSearcher.java
index f429c43..9bd8c25 100644
--- a/lucene/facet/src/examples/org/apache/lucene/facet/example/association/AssociationSearcher.java
+++ b/lucene/facet/src/examples/org/apache/lucene/facet/example/association/AssociationSearcher.java
@@ -2,6 +2,7 @@ package org.apache.lucene.facet.example.association;
 
 import java.util.List;
 
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.store.Directory;
 
@@ -42,7 +43,7 @@ public class AssociationSearcher {
   public static List<FacetResult> searchSumIntAssociation(Directory indexDir,
       Directory taxoDir) throws Exception {
     // prepare index reader 
-    IndexReader indexReader = IndexReader.open(indexDir);
+    IndexReader indexReader = DirectoryReader.open(indexDir);
     TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
     
     AssociationIntSumFacetRequest facetRequest = new AssociationIntSumFacetRequest(
@@ -62,7 +63,7 @@ public class AssociationSearcher {
   public static List<FacetResult> searchSumFloatAssociation(Directory indexDir,
       Directory taxoDir) throws Exception {
     // prepare index reader 
-    IndexReader indexReader = IndexReader.open(indexDir);
+    IndexReader indexReader = DirectoryReader.open(indexDir);
     TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
     
     AssociationFloatSumFacetRequest facetRequest = new AssociationFloatSumFacetRequest(
diff --git a/lucene/facet/src/examples/org/apache/lucene/facet/example/merge/TaxonomyMergeUtils.java b/lucene/facet/src/examples/org/apache/lucene/facet/example/merge/TaxonomyMergeUtils.java
index 835c8e8..000dd70 100644
--- a/lucene/facet/src/examples/org/apache/lucene/facet/example/merge/TaxonomyMergeUtils.java
+++ b/lucene/facet/src/examples/org/apache/lucene/facet/example/merge/TaxonomyMergeUtils.java
@@ -2,6 +2,7 @@ package org.apache.lucene.facet.example.merge;
 
 import java.io.IOException;
 
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -87,7 +88,7 @@ public class TaxonomyMergeUtils {
         srcIndexDir, map.getMap(), new DefaultFacetIndexingParams());
     destIndexWriter.setPayloadProcessorProvider(payloadProcessor);
 
-    IndexReader reader = IndexReader.open(srcIndexDir);
+    IndexReader reader = DirectoryReader.open(srcIndexDir);
     try {
       destIndexWriter.addIndexes(reader);
       
diff --git a/lucene/facet/src/examples/org/apache/lucene/facet/example/multiCL/MultiCLSearcher.java b/lucene/facet/src/examples/org/apache/lucene/facet/example/multiCL/MultiCLSearcher.java
index 650b420..7814458 100644
--- a/lucene/facet/src/examples/org/apache/lucene/facet/example/multiCL/MultiCLSearcher.java
+++ b/lucene/facet/src/examples/org/apache/lucene/facet/example/multiCL/MultiCLSearcher.java
@@ -2,6 +2,7 @@ package org.apache.lucene.facet.example.multiCL;
 
 import java.util.List;
 
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.IndexSearcher;
@@ -63,7 +64,7 @@ public class MultiCLSearcher {
       Directory taxoDir, FacetIndexingParams iParams) throws Exception {
     
     // prepare index reader and taxonomy.
-    IndexReader indexReader = IndexReader.open(indexDir);
+    IndexReader indexReader = DirectoryReader.open(indexDir);
     TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
     
     // Get results
diff --git a/lucene/facet/src/examples/org/apache/lucene/facet/example/simple/SimpleMain.java b/lucene/facet/src/examples/org/apache/lucene/facet/example/simple/SimpleMain.java
index e81c23c..777abd1 100644
--- a/lucene/facet/src/examples/org/apache/lucene/facet/example/simple/SimpleMain.java
+++ b/lucene/facet/src/examples/org/apache/lucene/facet/example/simple/SimpleMain.java
@@ -2,6 +2,7 @@ package org.apache.lucene.facet.example.simple;
 
 import java.util.List;
 
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.RAMDirectory;
@@ -57,7 +58,7 @@ public class SimpleMain {
 
     // open readers
     TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
-    IndexReader indexReader = IndexReader.open(indexDir);
+    IndexReader indexReader = DirectoryReader.open(indexDir);
 
     ExampleUtils.log("search the sample documents...");
     List<FacetResult> facetRes = SimpleSearcher.searchWithFacets(indexReader, taxo);
@@ -82,7 +83,7 @@ public class SimpleMain {
 
     // open readers
     TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
-    IndexReader indexReader = IndexReader.open(indexDir);
+    IndexReader indexReader = DirectoryReader.open(indexDir);
 
     ExampleUtils.log("search the sample documents...");
     List<FacetResult> facetRes = SimpleSearcher.searchWithDrillDown(indexReader, taxo);
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/FacetTestBase.java b/lucene/facet/src/test/org/apache/lucene/facet/FacetTestBase.java
index 3350ac5..161f8a0 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/FacetTestBase.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/FacetTestBase.java
@@ -16,6 +16,7 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -173,7 +174,7 @@ public abstract class FacetTestBase extends LuceneTestCase {
     
     // prepare for searching
     taxoReader = new DirectoryTaxonomyReader(pair.taxoDir);
-    indexReader = IndexReader.open(pair.searchDir);
+    indexReader = DirectoryReader.open(pair.searchDir);
     searcher = newSearcher(indexReader);
   }
   
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/FacetTestUtils.java b/lucene/facet/src/test/org/apache/lucene/facet/FacetTestUtils.java
index 9c60f8d..de6e8bd 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/FacetTestUtils.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/FacetTestUtils.java
@@ -68,7 +68,7 @@ public class FacetTestUtils {
     IndexTaxonomyReaderPair[] pairs = new IndexTaxonomyReaderPair[dirs.length];
     for (int i = 0; i < dirs.length; i++) {
       IndexTaxonomyReaderPair pair = new IndexTaxonomyReaderPair();
-      pair.indexReader = IndexReader.open(dirs[i][0]);
+      pair.indexReader = DirectoryReader.open(dirs[i][0]);
       pair.indexSearcher = new IndexSearcher(pair.indexReader);
       pair.taxReader = new DirectoryTaxonomyReader(dirs[i][1]);
       pairs[i] = pair;
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/enhancements/EnhancementsPayloadIteratorTest.java b/lucene/facet/src/test/org/apache/lucene/facet/enhancements/EnhancementsPayloadIteratorTest.java
index d038cba..724b1dc 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/enhancements/EnhancementsPayloadIteratorTest.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/enhancements/EnhancementsPayloadIteratorTest.java
@@ -2,6 +2,7 @@ package org.apache.lucene.facet.enhancements;
 
 import java.io.IOException;
 
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.store.Directory;
@@ -61,7 +62,7 @@ public class EnhancementsPayloadIteratorTest extends LuceneTestCase {
 
   @Test
   public void testFullIterator() throws IOException {
-    IndexReader indexReader = IndexReader.open(indexDir);
+    IndexReader indexReader = DirectoryReader.open(indexDir);
     Term term = DrillDown.term(indexingParams, new CategoryPath("tags", "lucene"));
     EnhancementsPayloadIterator iterator = new EnhancementsPayloadIterator(
         indexingParams.getCategoryEnhancements(), indexReader, term);
@@ -77,7 +78,7 @@ public class EnhancementsPayloadIteratorTest extends LuceneTestCase {
 
   @Test
   public void testEmptyIterator() throws IOException {
-    IndexReader indexReader = IndexReader.open(indexDir);
+    IndexReader indexReader = DirectoryReader.open(indexDir);
     Term term = DrillDown.term(indexingParams, new CategoryPath("root","a", "f2"));
     EnhancementsPayloadIterator iterator = new EnhancementsPayloadIterator(
         indexingParams.getCategoryEnhancements(), indexReader, term);
@@ -89,7 +90,7 @@ public class EnhancementsPayloadIteratorTest extends LuceneTestCase {
 
   @Test
   public void testPartialIterator() throws IOException {
-    IndexReader indexReader = IndexReader.open(indexDir);
+    IndexReader indexReader = DirectoryReader.open(indexDir);
     Term term = DrillDown.term(indexingParams, new CategoryPath("genre","software"));
     EnhancementsPayloadIterator iterator = new EnhancementsPayloadIterator(
         indexingParams.getCategoryEnhancements(), indexReader, term);
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/TestTotalFacetCountsCache.java b/lucene/facet/src/test/org/apache/lucene/facet/search/TestTotalFacetCountsCache.java
index 6b2800d..2924f25 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/TestTotalFacetCountsCache.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/search/TestTotalFacetCountsCache.java
@@ -156,7 +156,7 @@ public class TestTotalFacetCountsCache extends LuceneTestCase {
     slowTaxoDir.setSleepMillis(sleepMillis);
     
     // Open the slow readers
-    IndexReader slowIndexReader = IndexReader.open(indexDir);
+    IndexReader slowIndexReader = DirectoryReader.open(indexDir);
     TaxonomyReader slowTaxoReader = new DirectoryTaxonomyReader(taxoDir);
 
     // Class to perform search and return results as threads
@@ -421,7 +421,7 @@ public class TestTotalFacetCountsCache extends LuceneTestCase {
     indexDir.setSleepMillis(1);
     taxoDir.setSleepMillis(1);
 
-    IndexReader r = IndexReader.open(indexDir);
+    IndexReader r = DirectoryReader.open(indexDir);
     DirectoryTaxonomyReader tr = new DirectoryTaxonomyReader(taxoDir);
 
     // Create and start threads. Thread1 should lock the cache and calculate
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/params/MultiIteratorsPerCLParamsTest.java b/lucene/facet/src/test/org/apache/lucene/facet/search/params/MultiIteratorsPerCLParamsTest.java
index 679c924..29f9cc3 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/params/MultiIteratorsPerCLParamsTest.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/search/params/MultiIteratorsPerCLParamsTest.java
@@ -8,6 +8,7 @@ import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.store.Directory;
@@ -94,7 +95,7 @@ public class MultiIteratorsPerCLParamsTest extends LuceneTestCase {
     populateIndex(iParams, indexDir, taxoDir);
 
     TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
-    IndexReader reader = IndexReader.open(indexDir);
+    IndexReader reader = DirectoryReader.open(indexDir);
 
     CategoryListCache clCache = null;
     if (cacheCLI) {
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/util/TestScoredDocIDsUtils.java b/lucene/facet/src/test/org/apache/lucene/facet/util/TestScoredDocIDsUtils.java
index d5e7949..8bab9c3 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/util/TestScoredDocIDsUtils.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/util/TestScoredDocIDsUtils.java
@@ -9,6 +9,7 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.StringField;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.MultiFields;
 import org.apache.lucene.index.RandomIndexWriter;
@@ -235,6 +236,6 @@ public class TestScoredDocIDsUtils extends LuceneTestCase {
     writer.close();
 
     // Open a fresh read-only reader with the deletions in place
-    return IndexReader.open(dir);
+    return DirectoryReader.open(dir);
   }
 }
diff --git a/lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterPhraseTest.java b/lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterPhraseTest.java
index 5c2b64f..a40cabf 100644
--- a/lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterPhraseTest.java
+++ b/lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterPhraseTest.java
@@ -32,6 +32,7 @@ import org.apache.lucene.document.Field;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.Term;
@@ -68,7 +69,7 @@ public class HighlighterPhraseTest extends LuceneTestCase {
     } finally {
       indexWriter.close();
     }
-    final IndexReader indexReader = IndexReader.open(directory);
+    final IndexReader indexReader = DirectoryReader.open(directory);
     try {
       assertEquals(1, indexReader.numDocs());
       final IndexSearcher indexSearcher = newSearcher(indexReader);
@@ -111,7 +112,7 @@ public class HighlighterPhraseTest extends LuceneTestCase {
     } finally {
       indexWriter.close();
     }
-    final IndexReader indexReader = IndexReader.open(directory);
+    final IndexReader indexReader = DirectoryReader.open(directory);
     try {
       assertEquals(1, indexReader.numDocs());
       final IndexSearcher indexSearcher = newSearcher(indexReader);
@@ -182,7 +183,7 @@ public class HighlighterPhraseTest extends LuceneTestCase {
     } finally {
       indexWriter.close();
     }
-    final IndexReader indexReader = IndexReader.open(directory);
+    final IndexReader indexReader = DirectoryReader.open(directory);
     try {
       assertEquals(1, indexReader.numDocs());
       final IndexSearcher indexSearcher = newSearcher(indexReader);
@@ -224,7 +225,7 @@ public class HighlighterPhraseTest extends LuceneTestCase {
     } finally {
       indexWriter.close();
     }
-    final IndexReader indexReader = IndexReader.open(directory);
+    final IndexReader indexReader = DirectoryReader.open(directory);
     try {
       assertEquals(1, indexReader.numDocs());
       final IndexSearcher indexSearcher = newSearcher(indexReader);
@@ -264,7 +265,7 @@ public class HighlighterPhraseTest extends LuceneTestCase {
     } finally {
       indexWriter.close();
     }
-    final IndexReader indexReader = IndexReader.open(directory);
+    final IndexReader indexReader = DirectoryReader.open(directory);
     try {
       assertEquals(1, indexReader.numDocs());
       final IndexSearcher indexSearcher = newSearcher(indexReader);
diff --git a/lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest.java b/lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest.java
index e4d5978..674ec0a 100644
--- a/lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest.java
+++ b/lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest.java
@@ -39,6 +39,7 @@ import org.apache.lucene.document.Field;
 import org.apache.lucene.document.IntField;
 import org.apache.lucene.document.StoredField;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
@@ -1645,7 +1646,7 @@ public class HighlighterTest extends BaseTokenStreamTestCase implements Formatte
   
   private void searchIndex() throws IOException, InvalidTokenOffsetsException {
     Query query = new TermQuery(new Term("t_text1", "random"));
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     IndexSearcher searcher = new IndexSearcher(reader);
     // This scorer can return negative idf -> null fragment
     Scorer scorer = new QueryTermScorer( query, searcher.getIndexReader(), "t_text1" );
@@ -1672,7 +1673,7 @@ public class HighlighterTest extends BaseTokenStreamTestCase implements Formatte
    * writer = new IndexWriter(ramDir,bigramAnalyzer , true); Document d = new
    * Document(); Field f = new Field(FIELD_NAME, "java abc def", true, true,
    * true); d.add(f); writer.addDocument(d); writer.close(); IndexReader reader =
-   * IndexReader.open(ramDir);
+   * DirectoryReader.open(ramDir);
    * 
    * IndexSearcher searcher=new IndexSearcher(reader); query =
    * QueryParser.parse("abc", FIELD_NAME, bigramAnalyzer);
@@ -1760,7 +1761,7 @@ public class HighlighterTest extends BaseTokenStreamTestCase implements Formatte
 
     writer.forceMerge(1);
     writer.close();
-    reader = IndexReader.open(ramDir);
+    reader = DirectoryReader.open(ramDir);
     numHighlights = 0;
   }
 
diff --git a/lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest.java b/lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest.java
index 61735ec..4704c68 100644
--- a/lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest.java
+++ b/lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest.java
@@ -28,6 +28,7 @@ import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.Term;
@@ -100,7 +101,7 @@ public class TokenSourcesTest extends LuceneTestCase {
     } finally {
       indexWriter.close();
     }
-    final IndexReader indexReader = IndexReader.open(directory);
+    final IndexReader indexReader = DirectoryReader.open(directory);
     assertEquals(1, indexReader.numDocs());
     final IndexSearcher indexSearcher = newSearcher(indexReader);
     try {
@@ -145,7 +146,7 @@ public class TokenSourcesTest extends LuceneTestCase {
     } finally {
       indexWriter.close();
     }
-    final IndexReader indexReader = IndexReader.open(directory);
+    final IndexReader indexReader = DirectoryReader.open(directory);
     try {
       assertEquals(1, indexReader.numDocs());
       final IndexSearcher indexSearcher = newSearcher(indexReader);
@@ -189,7 +190,7 @@ public class TokenSourcesTest extends LuceneTestCase {
     } finally {
       indexWriter.close();
     }
-    final IndexReader indexReader = IndexReader.open(directory);
+    final IndexReader indexReader = DirectoryReader.open(directory);
     try {
       assertEquals(1, indexReader.numDocs());
       final IndexSearcher indexSearcher = newSearcher(indexReader);
@@ -234,7 +235,7 @@ public class TokenSourcesTest extends LuceneTestCase {
     } finally {
       indexWriter.close();
     }
-    final IndexReader indexReader = IndexReader.open(directory);
+    final IndexReader indexReader = DirectoryReader.open(directory);
     try {
       assertEquals(1, indexReader.numDocs());
       final IndexSearcher indexSearcher = newSearcher(indexReader);
diff --git a/lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase.java b/lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase.java
index e0f66e0..24e2069 100644
--- a/lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase.java
+++ b/lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase.java
@@ -32,6 +32,7 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
@@ -364,7 +365,7 @@ public abstract class AbstractTestCase extends LuceneTestCase {
     writer.addDocument( doc );
     writer.close();
     if (reader != null) reader.close();
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
   }
   
   // make 1 doc with multi valued & not analyzed field
@@ -383,7 +384,7 @@ public abstract class AbstractTestCase extends LuceneTestCase {
     writer.addDocument( doc );
     writer.close();
     if (reader != null) reader.close();
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
   }
   
   protected void makeIndexShortMV() throws Exception {
diff --git a/lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/SimpleFragmentsBuilderTest.java b/lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/SimpleFragmentsBuilderTest.java
index 16fe102..24134a3 100644
--- a/lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/SimpleFragmentsBuilderTest.java
+++ b/lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/SimpleFragmentsBuilderTest.java
@@ -21,6 +21,7 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -147,7 +148,7 @@ public class SimpleFragmentsBuilderTest extends AbstractTestCase {
     writer.addDocument( doc );
     writer.close();
     if (reader != null) reader.close();
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
   }
   
   public void test1StrMV() throws Exception {
diff --git a/lucene/misc/src/java/org/apache/lucene/misc/GetTermInfo.java b/lucene/misc/src/java/org/apache/lucene/misc/GetTermInfo.java
index 45ab8d5..1745aae 100644
--- a/lucene/misc/src/java/org/apache/lucene/misc/GetTermInfo.java
+++ b/lucene/misc/src/java/org/apache/lucene/misc/GetTermInfo.java
@@ -21,6 +21,7 @@ import java.io.File;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.FSDirectory;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 
 /**
@@ -47,7 +48,7 @@ public class GetTermInfo {
   }
   
   public static void getTermInfo(Directory dir, String field, BytesRef termtext) throws Exception {
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     long totalTF = HighFreqTerms.getTotalTermFreq(reader, field, termtext);
     System.out.printf("%s:%s \t totalTF = %,d \t doc freq = %,d \n",
          field, termtext.utf8ToString(), totalTF, reader.docFreq(field, termtext)); 
diff --git a/lucene/misc/src/java/org/apache/lucene/misc/HighFreqTerms.java b/lucene/misc/src/java/org/apache/lucene/misc/HighFreqTerms.java
index 52faf49..2edfe6b 100644
--- a/lucene/misc/src/java/org/apache/lucene/misc/HighFreqTerms.java
+++ b/lucene/misc/src/java/org/apache/lucene/misc/HighFreqTerms.java
@@ -18,6 +18,7 @@ package org.apache.lucene.misc;
  */
 
 import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.MultiFields;
 import org.apache.lucene.index.Fields;
@@ -78,7 +79,7 @@ public class HighFreqTerms {
       }
     }
     
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
     TermStats[] terms = getHighFreqTerms(reader, numTerms, field);
     if (!IncludeTermFreqs) {
       //default HighFreqTerms behavior
diff --git a/lucene/misc/src/test/org/apache/lucene/index/TestMultiPassIndexSplitter.java b/lucene/misc/src/test/org/apache/lucene/index/TestMultiPassIndexSplitter.java
index a6837ac..97fbc9c 100644
--- a/lucene/misc/src/test/org/apache/lucene/index/TestMultiPassIndexSplitter.java
+++ b/lucene/misc/src/test/org/apache/lucene/index/TestMultiPassIndexSplitter.java
@@ -45,7 +45,7 @@ public class TestMultiPassIndexSplitter extends LuceneTestCase {
     w.commit();
     w.deleteDocuments(new Term("id", "" + (NUM_DOCS-1)));
     w.close();
-    input = IndexReader.open(dir);
+    input = DirectoryReader.open(dir);
   }
   
   @Override
@@ -67,7 +67,7 @@ public class TestMultiPassIndexSplitter extends LuceneTestCase {
     };
     splitter.split(TEST_VERSION_CURRENT, input, dirs, false);
     IndexReader ir;
-    ir = IndexReader.open(dirs[0]);
+    ir = DirectoryReader.open(dirs[0]);
     assertTrue(ir.numDocs() - NUM_DOCS / 3 <= 1); // rounding error
     Document doc = ir.document(0);
     assertEquals("0", doc.get("id"));
@@ -75,7 +75,7 @@ public class TestMultiPassIndexSplitter extends LuceneTestCase {
     assertEquals(TermsEnum.SeekStatus.NOT_FOUND, te.seekCeil(new BytesRef("1")));
     assertNotSame("1", te.term().utf8ToString());
     ir.close();
-    ir = IndexReader.open(dirs[1]);
+    ir = DirectoryReader.open(dirs[1]);
     assertTrue(ir.numDocs() - NUM_DOCS / 3 <= 1);
     doc = ir.document(0);
     assertEquals("1", doc.get("id"));
@@ -84,7 +84,7 @@ public class TestMultiPassIndexSplitter extends LuceneTestCase {
 
     assertNotSame("0", te.term().utf8ToString());
     ir.close();
-    ir = IndexReader.open(dirs[2]);
+    ir = DirectoryReader.open(dirs[2]);
     assertTrue(ir.numDocs() - NUM_DOCS / 3 <= 1);
     doc = ir.document(0);
     assertEquals("2", doc.get("id"));
@@ -112,19 +112,19 @@ public class TestMultiPassIndexSplitter extends LuceneTestCase {
     };
     splitter.split(TEST_VERSION_CURRENT, input, dirs, true);
     IndexReader ir;
-    ir = IndexReader.open(dirs[0]);
+    ir = DirectoryReader.open(dirs[0]);
     assertTrue(ir.numDocs() - NUM_DOCS / 3 <= 1);
     Document doc = ir.document(0);
     assertEquals("0", doc.get("id"));
     int start = ir.numDocs();
     ir.close();
-    ir = IndexReader.open(dirs[1]);
+    ir = DirectoryReader.open(dirs[1]);
     assertTrue(ir.numDocs() - NUM_DOCS / 3 <= 1);
     doc = ir.document(0);
     assertEquals(start + "", doc.get("id"));
     start += ir.numDocs();
     ir.close();
-    ir = IndexReader.open(dirs[2]);
+    ir = DirectoryReader.open(dirs[2]);
     assertTrue(ir.numDocs() - NUM_DOCS / 3 <= 1);
     doc = ir.document(0);
     assertEquals(start + "", doc.get("id"));
diff --git a/lucene/misc/src/test/org/apache/lucene/index/TestPKIndexSplitter.java b/lucene/misc/src/test/org/apache/lucene/index/TestPKIndexSplitter.java
index ffdf51c..5c872ac 100644
--- a/lucene/misc/src/test/org/apache/lucene/index/TestPKIndexSplitter.java
+++ b/lucene/misc/src/test/org/apache/lucene/index/TestPKIndexSplitter.java
@@ -75,8 +75,8 @@ public class TestPKIndexSplitter extends LuceneTestCase {
         newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
     splitter.split();
     
-    IndexReader ir1 = IndexReader.open(dir1);
-    IndexReader ir2 = IndexReader.open(dir2);
+    IndexReader ir1 = DirectoryReader.open(dir1);
+    IndexReader ir2 = DirectoryReader.open(dir2);
     assertEquals(leftCount, ir1.numDocs());
     assertEquals(rightCount, ir2.numDocs());
     
diff --git a/lucene/misc/src/test/org/apache/lucene/misc/TestHighFreqTerms.java b/lucene/misc/src/test/org/apache/lucene/misc/TestHighFreqTerms.java
index 21090b2..60ffd04 100644
--- a/lucene/misc/src/test/org/apache/lucene/misc/TestHighFreqTerms.java
+++ b/lucene/misc/src/test/org/apache/lucene/misc/TestHighFreqTerms.java
@@ -23,6 +23,7 @@ import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.store.Directory;
@@ -45,7 +46,7 @@ public class TestHighFreqTerms extends LuceneTestCase {
        TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false))
        .setMaxBufferedDocs(2));
     indexDocs(writer);
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
     _TestUtil.checkIndex(dir);
   }
   
diff --git a/lucene/queries/src/test/org/apache/lucene/queries/TestCustomScoreQuery.java b/lucene/queries/src/test/org/apache/lucene/queries/TestCustomScoreQuery.java
index 5233aba..cd85f74 100755
--- a/lucene/queries/src/test/org/apache/lucene/queries/TestCustomScoreQuery.java
+++ b/lucene/queries/src/test/org/apache/lucene/queries/TestCustomScoreQuery.java
@@ -29,6 +29,7 @@ import java.util.HashMap;
 import java.util.Map;
 
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 
@@ -199,7 +200,7 @@ public class TestCustomScoreQuery extends FunctionTestSetup {
     final Query q = new CustomExternalQuery(q1);
     log(q);
 
-    IndexReader r = IndexReader.open(dir);
+    IndexReader r = DirectoryReader.open(dir);
     IndexSearcher s = new IndexSearcher(r);
     TopDocs hits = s.search(q, 1000);
     assertEquals(N_DOCS, hits.totalHits);
@@ -213,7 +214,7 @@ public class TestCustomScoreQuery extends FunctionTestSetup {
   
   @Test
   public void testRewrite() throws Exception {
-    IndexReader r = IndexReader.open(dir);
+    IndexReader r = DirectoryReader.open(dir);
     final IndexSearcher s = new IndexSearcher(r);
 
     Query q = new TermQuery(new Term(TEXT_FIELD, "first"));
@@ -238,7 +239,7 @@ public class TestCustomScoreQuery extends FunctionTestSetup {
   private void doTestCustomScore(ValueSource valueSource, double dboost) throws Exception {
     float boost = (float) dboost;
     FunctionQuery functionQuery = new FunctionQuery(valueSource);
-    IndexReader r = IndexReader.open(dir);
+    IndexReader r = DirectoryReader.open(dir);
     IndexSearcher s = new IndexSearcher(r);
 
     // regular (boolean) query.
diff --git a/lucene/queries/src/test/org/apache/lucene/queries/function/TestFieldScoreQuery.java b/lucene/queries/src/test/org/apache/lucene/queries/function/TestFieldScoreQuery.java
index 5aea75a..f5de11a 100755
--- a/lucene/queries/src/test/org/apache/lucene/queries/function/TestFieldScoreQuery.java
+++ b/lucene/queries/src/test/org/apache/lucene/queries/function/TestFieldScoreQuery.java
@@ -17,6 +17,7 @@ package org.apache.lucene.queries.function;
  * limitations under the License.
  */
 
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.queries.function.FunctionQuery;
 import org.apache.lucene.queries.function.ValueSource;
@@ -81,7 +82,7 @@ public class TestFieldScoreQuery extends FunctionTestSetup {
   // Test that FieldScoreQuery returns docs in expected order.
   private void doTestRank (ValueSource valueSource) throws Exception {
     FunctionQuery functionQuery = new FunctionQuery(valueSource);
-    IndexReader r = IndexReader.open(dir);
+    IndexReader r = DirectoryReader.open(dir);
     IndexSearcher s = new IndexSearcher(r);
     log("test: "+ functionQuery);
     QueryUtils.check(random(), functionQuery,s);
@@ -130,7 +131,7 @@ public class TestFieldScoreQuery extends FunctionTestSetup {
   // Test that FieldScoreQuery returns docs with expected score.
   private void doTestExactScore (ValueSource valueSource) throws Exception {
     FunctionQuery functionQuery = new FunctionQuery(valueSource);
-    IndexReader r = IndexReader.open(dir);
+    IndexReader r = DirectoryReader.open(dir);
     IndexSearcher s = new IndexSearcher(r);
     TopDocs td = s.search(functionQuery,null,1000);
     assertEquals("All docs should be matched!",N_DOCS,td.totalHits);
diff --git a/lucene/queries/src/test/org/apache/lucene/queries/function/TestOrdValues.java b/lucene/queries/src/test/org/apache/lucene/queries/function/TestOrdValues.java
index 0890971..d6f3d3f 100644
--- a/lucene/queries/src/test/org/apache/lucene/queries/function/TestOrdValues.java
+++ b/lucene/queries/src/test/org/apache/lucene/queries/function/TestOrdValues.java
@@ -18,6 +18,7 @@ package org.apache.lucene.queries.function;
  */
 
 import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.queries.function.valuesource.OrdFieldSource;
 import org.apache.lucene.queries.function.valuesource.ReverseOrdFieldSource;
@@ -61,7 +62,7 @@ public class TestOrdValues extends FunctionTestSetup {
 
   // Test that queries based on reverse/ordFieldScore scores correctly
   private void doTestRank(String field, boolean inOrder) throws CorruptIndexException, Exception {
-    IndexReader r = IndexReader.open(dir);
+    IndexReader r = DirectoryReader.open(dir);
     IndexSearcher s = new IndexSearcher(r);
     ValueSource vs;
     if (inOrder) {
@@ -112,7 +113,7 @@ public class TestOrdValues extends FunctionTestSetup {
 
   // Test that queries based on reverse/ordFieldScore returns docs with expected score.
   private void doTestExactScore(String field, boolean inOrder) throws CorruptIndexException, Exception {
-    IndexReader r = IndexReader.open(dir);
+    IndexReader r = DirectoryReader.open(dir);
     IndexSearcher s = new IndexSearcher(r);
     ValueSource vs;
     if (inOrder) {
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParser.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParser.java
index c0df24d..8180f15 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParser.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParser.java
@@ -76,14 +76,6 @@ import org.apache.lucene.util.Version;
  * <p><b>NOTE</b>: there is a new QueryParser in contrib, which matches
  * the same syntax as this class, but is more modular,
  * enabling substantial customization to how a query is created.
- *
- * <a name="version"/>
- * <p><b>NOTE</b>: You must specify the required {@link Version}
- * compatibility when creating QueryParser:
- * <ul>
- *    <li> As of 3.1, {@link #setAutoGeneratePhraseQueries} is false by
- *         default.
- * </ul>
  */
 public class QueryParser extends QueryParserBase implements QueryParserConstants {
   /** The default operator for parsing queries.
@@ -92,7 +84,7 @@ public class QueryParser extends QueryParserBase implements QueryParserConstants
   static public enum Operator { OR, AND }
 
   /** Create a query parser.
-   *  @param matchVersion  Lucene version to match. See <a href="#version">above</a>.
+   *  @param matchVersion  Lucene version to match.
    *  @param f  the default field for query terms.
    *  @param a   used to find terms in the query text.
    */
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserBase.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserBase.java
index 3d99fc4..c7b766f 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserBase.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserBase.java
@@ -88,18 +88,14 @@ public abstract class QueryParserBase {
   }
 
   /** Initializes a query parser.  Called by the QueryParser constructor
-   *  @param matchVersion  Lucene version to match. See <a href="QueryParser.html#version">here</a>.
+   *  @param matchVersion  Lucene version to match.
    *  @param f  the default field for query terms.
    *  @param a   used to find terms in the query text.
    */
   public void init(Version matchVersion, String f, Analyzer a) {
     analyzer = a;
     field = f;
-    if (matchVersion.onOrAfter(Version.LUCENE_31)) {
-      setAutoGeneratePhraseQueries(false);
-    } else {
-      setAutoGeneratePhraseQueries(true);
-    }
+    setAutoGeneratePhraseQueries(false);
   }
 
   // the generated parser will create these in QueryParser
diff --git a/lucene/queryparser/src/test/org/apache/lucene/queryparser/classic/TestMultiFieldQueryParser.java b/lucene/queryparser/src/test/org/apache/lucene/queryparser/classic/TestMultiFieldQueryParser.java
index ce4b5c9..02cec5a 100644
--- a/lucene/queryparser/src/test/org/apache/lucene/queryparser/classic/TestMultiFieldQueryParser.java
+++ b/lucene/queryparser/src/test/org/apache/lucene/queryparser/classic/TestMultiFieldQueryParser.java
@@ -24,6 +24,7 @@ import java.util.Map;
 import org.apache.lucene.analysis.*;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.search.BooleanClause.Occur;
@@ -290,7 +291,7 @@ public class TestMultiFieldQueryParser extends LuceneTestCase {
       new MultiFieldQueryParser(TEST_VERSION_CURRENT, new String[] {"body"}, analyzer);
     mfqp.setDefaultOperator(QueryParser.Operator.AND);
     Query q = mfqp.parse("the footest");
-    IndexReader ir = IndexReader.open(ramDir);
+    IndexReader ir = DirectoryReader.open(ramDir);
     IndexSearcher is = new IndexSearcher(ir);
     ScoreDoc[] hits = is.search(q, null, 1000).scoreDocs;
     assertEquals(1, hits.length);
diff --git a/lucene/queryparser/src/test/org/apache/lucene/queryparser/complexPhrase/TestComplexPhraseQuery.java b/lucene/queryparser/src/test/org/apache/lucene/queryparser/complexPhrase/TestComplexPhraseQuery.java
index 25c0efb..d5c71de 100644
--- a/lucene/queryparser/src/test/org/apache/lucene/queryparser/complexPhrase/TestComplexPhraseQuery.java
+++ b/lucene/queryparser/src/test/org/apache/lucene/queryparser/complexPhrase/TestComplexPhraseQuery.java
@@ -23,6 +23,7 @@ import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.queryparser.classic.QueryParser;
@@ -124,7 +125,7 @@ public class TestComplexPhraseQuery extends LuceneTestCase {
       w.addDocument(doc);
     }
     w.close();
-    reader = IndexReader.open(rd);
+    reader = DirectoryReader.open(rd);
     searcher = new IndexSearcher(reader);
   }
 
diff --git a/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/standard/TestMultiFieldQPHelper.java b/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/standard/TestMultiFieldQPHelper.java
index bb99a37..9c5e923 100644
--- a/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/standard/TestMultiFieldQPHelper.java
+++ b/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/standard/TestMultiFieldQPHelper.java
@@ -26,6 +26,7 @@ import java.util.Map;
 import org.apache.lucene.analysis.*;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.queryparser.flexible.core.QueryNodeException;
@@ -331,7 +332,7 @@ public class TestMultiFieldQPHelper extends LuceneTestCase {
     mfqp.setAnalyzer(analyzer);
     mfqp.setDefaultOperator(StandardQueryConfigHandler.Operator.AND);
     Query q = mfqp.parse("the footest", null);
-    IndexReader ir = IndexReader.open(ramDir);
+    IndexReader ir = DirectoryReader.open(ramDir);
     IndexSearcher is = new IndexSearcher(ir);
     ScoreDoc[] hits = is.search(q, null, 1000).scoreDocs;
     assertEquals(1, hits.length);
diff --git a/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/standard/TestQPHelper.java b/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/standard/TestQPHelper.java
index 91d25f8..0add593 100644
--- a/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/standard/TestQPHelper.java
+++ b/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/standard/TestQPHelper.java
@@ -36,6 +36,7 @@ import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.document.DateTools;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.Term;
@@ -1290,7 +1291,7 @@ public class TestQPHelper extends LuceneTestCase {
     Document doc = new Document();
     doc.add(newField("field", "", TextField.TYPE_UNSTORED));
     w.addDocument(doc);
-    IndexReader r = IndexReader.open(w, true);
+    IndexReader r = DirectoryReader.open(w, true);
     IndexSearcher s = newSearcher(r);
     
     Query q = new StandardQueryParser(new CannedAnalyzer()).parse("\"a\"", "field");
diff --git a/lucene/queryparser/src/test/org/apache/lucene/queryparser/surround/query/BooleanQueryTst.java b/lucene/queryparser/src/test/org/apache/lucene/queryparser/surround/query/BooleanQueryTst.java
index 81fbf41..2f86e68 100644
--- a/lucene/queryparser/src/test/org/apache/lucene/queryparser/surround/query/BooleanQueryTst.java
+++ b/lucene/queryparser/src/test/org/apache/lucene/queryparser/surround/query/BooleanQueryTst.java
@@ -20,6 +20,7 @@ package org.apache.lucene.queryparser.surround.query;
 import java.io.IOException;
 
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Collector;
@@ -122,7 +123,7 @@ public class BooleanQueryTst {
     /* if (verbose) System.out.println("Lucene: " + query.toString()); */
 
     TestCollector tc = new TestCollector();
-    IndexReader reader = IndexReader.open(dBase.getDb());
+    IndexReader reader = DirectoryReader.open(dBase.getDb());
     IndexSearcher searcher = new IndexSearcher(reader);
     try {
       searcher.search(query, tc);
diff --git a/lucene/queryparser/src/test/org/apache/lucene/queryparser/util/QueryParserTestBase.java b/lucene/queryparser/src/test/org/apache/lucene/queryparser/util/QueryParserTestBase.java
index bb79d1e..7719ae9 100644
--- a/lucene/queryparser/src/test/org/apache/lucene/queryparser/util/QueryParserTestBase.java
+++ b/lucene/queryparser/src/test/org/apache/lucene/queryparser/util/QueryParserTestBase.java
@@ -32,6 +32,7 @@ import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.document.DateTools;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.Term;
@@ -1112,7 +1113,7 @@ public abstract class QueryParserTestBase extends LuceneTestCase {
     Document doc = new Document();
     doc.add(newField("f", "the wizard of ozzy", TextField.TYPE_UNSTORED));
     w.addDocument(doc);
-    IndexReader r = IndexReader.open(w, true);
+    IndexReader r = DirectoryReader.open(w, true);
     w.close();
     IndexSearcher s = newSearcher(r);
     QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "f", a);
diff --git a/lucene/queryparser/src/test/org/apache/lucene/queryparser/xml/TestParser.java b/lucene/queryparser/src/test/org/apache/lucene/queryparser/xml/TestParser.java
index e1b5c16..9e6f8ca 100644
--- a/lucene/queryparser/src/test/org/apache/lucene/queryparser/xml/TestParser.java
+++ b/lucene/queryparser/src/test/org/apache/lucene/queryparser/xml/TestParser.java
@@ -25,6 +25,7 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.IntField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.search.DisjunctionMaxQuery;
@@ -76,7 +77,7 @@ public class TestParser extends LuceneTestCase {
     }
     d.close();
     writer.close();
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
     searcher = newSearcher(reader);
 
   }
diff --git a/lucene/queryparser/src/test/org/apache/lucene/queryparser/xml/TestQueryTemplateManager.java b/lucene/queryparser/src/test/org/apache/lucene/queryparser/xml/TestQueryTemplateManager.java
index 868880a..35e3641 100644
--- a/lucene/queryparser/src/test/org/apache/lucene/queryparser/xml/TestQueryTemplateManager.java
+++ b/lucene/queryparser/src/test/org/apache/lucene/queryparser/xml/TestQueryTemplateManager.java
@@ -20,6 +20,7 @@ package org.apache.lucene.queryparser.xml;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.search.IndexSearcher;
@@ -150,7 +151,7 @@ public class TestQueryTemplateManager extends LuceneTestCase {
     }
     w.forceMerge(1);
     w.close();
-    reader = IndexReader.open(dir);
+    reader = DirectoryReader.open(dir);
     searcher = new IndexSearcher(reader);
 
     //initialize the parser
diff --git a/lucene/queryparser/src/test/org/apache/lucene/queryparser/xml/builders/TestNumericRangeFilterBuilder.java b/lucene/queryparser/src/test/org/apache/lucene/queryparser/xml/builders/TestNumericRangeFilterBuilder.java
index 1c0a59a..ec80eac 100644
--- a/lucene/queryparser/src/test/org/apache/lucene/queryparser/xml/builders/TestNumericRangeFilterBuilder.java
+++ b/lucene/queryparser/src/test/org/apache/lucene/queryparser/xml/builders/TestNumericRangeFilterBuilder.java
@@ -18,6 +18,7 @@ package org.apache.lucene.queryparser.xml.builders;
  */
 
 import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.SlowCompositeReaderWrapper;
@@ -63,7 +64,7 @@ public class TestNumericRangeFilterBuilder extends LuceneTestCase {
     IndexWriter writer = new IndexWriter(ramDir, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
     writer.commit();
     try {
-      AtomicReader reader = new SlowCompositeReaderWrapper(IndexReader.open(ramDir));
+      AtomicReader reader = new SlowCompositeReaderWrapper(DirectoryReader.open(ramDir));
       try {
         assertNull(filter.getDocIdSet(reader.getTopReaderContext(), reader.getLiveDocs()));
       }
diff --git a/lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/regex/TestSpanRegexQuery.java b/lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/regex/TestSpanRegexQuery.java
index 058b98e..ea2602f 100644
--- a/lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/regex/TestSpanRegexQuery.java
+++ b/lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/regex/TestSpanRegexQuery.java
@@ -24,6 +24,7 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.Term;
@@ -72,7 +73,7 @@ public class TestSpanRegexQuery extends LuceneTestCase {
     writer.forceMerge(1);
     writer.close();
 
-    IndexReader reader = IndexReader.open(directory);
+    IndexReader reader = DirectoryReader.open(directory);
     IndexSearcher searcher = new IndexSearcher(reader);
     SpanQuery srq = new SpanMultiTermQueryWrapper<RegexQuery>(new RegexQuery(new Term("field", "aut.*")));
     SpanFirstQuery sfq = new SpanFirstQuery(srq, 1);
diff --git a/lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java b/lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java
index 858804d..975a6ef 100755
--- a/lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java
+++ b/lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java
@@ -674,7 +674,7 @@ public class SpellChecker implements java.io.Closeable {
    */
   // for testing purposes
   IndexSearcher createSearcher(final Directory dir) throws IOException{
-    return new IndexSearcher(IndexReader.open(dir));
+    return new IndexSearcher(DirectoryReader.open(dir));
   }
   
   /**
diff --git a/lucene/suggest/src/test/org/apache/lucene/search/spell/TestSpellChecker.java b/lucene/suggest/src/test/org/apache/lucene/search/spell/TestSpellChecker.java
index 9dd9e01..d6ece66 100755
--- a/lucene/suggest/src/test/org/apache/lucene/search/spell/TestSpellChecker.java
+++ b/lucene/suggest/src/test/org/apache/lucene/search/spell/TestSpellChecker.java
@@ -30,6 +30,7 @@ import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -104,7 +105,7 @@ public class TestSpellChecker extends LuceneTestCase {
 
 
   public void testBuild() throws CorruptIndexException, IOException {
-    IndexReader r = IndexReader.open(userindex);
+    IndexReader r = DirectoryReader.open(userindex);
 
     spellChecker.clearIndex();
 
@@ -144,7 +145,7 @@ public class TestSpellChecker extends LuceneTestCase {
   }
 
   public void testComparator() throws Exception {
-    IndexReader r = IndexReader.open(userindex);
+    IndexReader r = DirectoryReader.open(userindex);
     Directory compIdx = newDirectory();
     SpellChecker compareSP = new SpellCheckerMock(compIdx, new LevensteinDistance(), new SuggestWordFrequencyComparator());
     addwords(r, compareSP, "field3");
@@ -162,7 +163,7 @@ public class TestSpellChecker extends LuceneTestCase {
   }
   
   public void testBogusField() throws Exception {
-    IndexReader r = IndexReader.open(userindex);
+    IndexReader r = DirectoryReader.open(userindex);
     Directory compIdx = newDirectory();
     SpellChecker compareSP = new SpellCheckerMock(compIdx, new LevensteinDistance(), new SuggestWordFrequencyComparator());
     addwords(r, compareSP, "field3");
@@ -177,7 +178,7 @@ public class TestSpellChecker extends LuceneTestCase {
   }
   
   public void testSuggestModes() throws Exception {
-    IndexReader r = IndexReader.open(userindex);
+    IndexReader r = DirectoryReader.open(userindex);
     spellChecker.clearIndex();
     addwords(r, spellChecker, "field1");
     
@@ -337,7 +338,7 @@ public class TestSpellChecker extends LuceneTestCase {
   }
 
   private int numdoc() throws IOException {
-    IndexReader rs = IndexReader.open(spellindex);
+    IndexReader rs = DirectoryReader.open(spellindex);
     int num = rs.numDocs();
     assertTrue(num != 0);
     //System.out.println("num docs: " + num);
@@ -346,7 +347,7 @@ public class TestSpellChecker extends LuceneTestCase {
   }
   
   public void testClose() throws IOException {
-    IndexReader r = IndexReader.open(userindex);
+    IndexReader r = DirectoryReader.open(userindex);
     spellChecker.clearIndex();
     String field = "field1";
     addwords(r, spellChecker, "field1");
@@ -402,7 +403,7 @@ public class TestSpellChecker extends LuceneTestCase {
    */
   public void testConcurrentAccess() throws IOException, InterruptedException {
     assertEquals(1, searchers.size());
-    final IndexReader r = IndexReader.open(userindex);
+    final IndexReader r = DirectoryReader.open(userindex);
     spellChecker.clearIndex();
     assertEquals(2, searchers.size());
     addwords(r, spellChecker, "field1");
diff --git a/lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase.java b/lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase.java
index 4bc3a09..f61e9e3 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase.java
@@ -29,6 +29,7 @@ import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -44,7 +45,6 @@ import org.apache.lucene.search.TermRangeFilter;
 import org.apache.lucene.search.TermRangeQuery;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IndexableBinaryStringTools;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util._TestUtil;
 
@@ -59,24 +59,6 @@ public abstract class CollationTestBase extends LuceneTestCase {
   protected String secondRangeBeginningOriginal = "\u0633";
   protected String secondRangeEndOriginal = "\u0638";
   
-  /**
-   * Convenience method to perform the same function as CollationKeyFilter.
-   *  
-   * @param keyBits the result from 
-   *  collator.getCollationKey(original).toByteArray()
-   * @return The encoded collation key for the original String
-   * @deprecated only for testing deprecated filters
-   */
-  @Deprecated
-  protected String encodeCollationKey(byte[] keyBits) {
-    // Ensure that the backing char[] array is large enough to hold the encoded
-    // Binary String
-    int encodedLength = IndexableBinaryStringTools.getEncodedLength(keyBits, 0, keyBits.length);
-    char[] encodedBegArray = new char[encodedLength];
-    IndexableBinaryStringTools.encode(keyBits, 0, keyBits.length, encodedBegArray, 0, encodedLength);
-    return new String(encodedBegArray);
-  }
-  
   public void testFarsiRangeFilterCollating(Analyzer analyzer, BytesRef firstBeg, 
                                             BytesRef firstEnd, BytesRef secondBeg,
                                             BytesRef secondEnd) throws Exception {
@@ -88,7 +70,7 @@ public abstract class CollationTestBase extends LuceneTestCase {
     doc.add(new Field("body", "body", StringField.TYPE_STORED));
     writer.addDocument(doc);
     writer.close();
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     IndexSearcher searcher = new IndexSearcher(reader);
     Query query = new TermQuery(new Term("body","body"));
 
@@ -124,7 +106,7 @@ public abstract class CollationTestBase extends LuceneTestCase {
     doc.add(new Field("content", "\u0633\u0627\u0628", TextField.TYPE_STORED));
     writer.addDocument(doc);
     writer.close();
-    IndexReader reader = IndexReader.open(dir);
+    IndexReader reader = DirectoryReader.open(dir);
     IndexSearcher searcher = new IndexSearcher(reader);
 
     Query query = new TermRangeQuery("content", firstBeg, firstEnd, true, true);
@@ -150,7 +132,7 @@ public abstract class CollationTestBase extends LuceneTestCase {
     writer.addDocument(doc);
     writer.close();
 
-    IndexReader reader = IndexReader.open(farsiIndex);
+    IndexReader reader = DirectoryReader.open(farsiIndex);
     IndexSearcher search = newSearcher(reader);
         
     // Unicode order would include U+0633 in [ U+062F - U+0698 ], but Farsi
@@ -226,7 +208,7 @@ public abstract class CollationTestBase extends LuceneTestCase {
     }
     writer.forceMerge(1);
     writer.close();
-    IndexReader reader = IndexReader.open(indexStore);
+    IndexReader reader = DirectoryReader.open(indexStore);
     IndexSearcher searcher = new IndexSearcher(reader);
 
     Sort sort = new Sort();
diff --git a/lucene/test-framework/src/java/org/apache/lucene/index/RandomIndexWriter.java b/lucene/test-framework/src/java/org/apache/lucene/index/RandomIndexWriter.java
index 6f523ab..84062dd 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/index/RandomIndexWriter.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/index/RandomIndexWriter.java
@@ -418,7 +418,7 @@ public class RandomIndexWriter implements Closeable {
       w.commit();
       switchDoDocValues();
       if (r.nextBoolean()) {
-        return IndexReader.open(w.getDirectory(), _TestUtil.nextInt(r, 1, 10));
+        return DirectoryReader.open(w.getDirectory(), _TestUtil.nextInt(r, 1, 10));
       } else {
         return w.getReader(applyDeletions);
       }
diff --git a/solr/contrib/analysis-extras/src/java/org/apache/solr/analysis/ICUCollationKeyFilterFactory.java b/solr/contrib/analysis-extras/src/java/org/apache/solr/analysis/ICUCollationKeyFilterFactory.java
deleted file mode 100644
index dfa9c7a..0000000
--- a/solr/contrib/analysis-extras/src/java/org/apache/solr/analysis/ICUCollationKeyFilterFactory.java
+++ /dev/null
@@ -1,194 +0,0 @@
-package org.apache.solr.analysis;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.InputStream;
-
-import org.apache.commons.io.IOUtils;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.util.*;
-import org.apache.lucene.collation.ICUCollationKeyFilter;
-import org.apache.solr.common.SolrException;
-import org.apache.solr.common.SolrException.ErrorCode;
-
-import com.ibm.icu.text.Collator;
-import com.ibm.icu.text.RuleBasedCollator;
-import com.ibm.icu.util.ULocale;
-
-/**
- * <!-- see LUCENE-4015 for why we cannot link -->
- * Factory for <code>ICUCollationKeyFilter</code>.
- * <p>
- * This factory can be created in two ways: 
- * <ul>
- *  <li>Based upon a system collator associated with a Locale.
- *  <li>Based upon a tailored ruleset.
- * </ul>
- * <p>
- * Using a System collator:
- * <ul>
- *  <li>locale: RFC 3066 locale ID (mandatory)
- *  <li>strength: 'primary','secondary','tertiary', 'quaternary', or 'identical' (optional)
- *  <li>decomposition: 'no', or 'canonical' (optional)
- * </ul>
- * <p>
- * Using a Tailored ruleset:
- * <ul>
- *  <li>custom: UTF-8 text file containing rules supported by RuleBasedCollator (mandatory)
- *  <li>strength: 'primary','secondary','tertiary', 'quaternary', or 'identical' (optional)
- *  <li>decomposition: 'no' or 'canonical' (optional)
- * </ul>
- * <p>
- * Expert options:
- * <ul>
- *  <li>alternate: 'shifted' or 'non-ignorable'. Can be used to ignore punctuation/whitespace.
- *  <li>caseLevel: 'true' or 'false'. Useful with strength=primary to ignore accents but not case.
- *  <li>caseFirst: 'lower' or 'upper'. Useful to control which is sorted first when case is not ignored.
- *  <li>numeric: 'true' or 'false'. Digits are sorted according to numeric value, e.g. foobar-9 sorts before foobar-10
- *  <li>variableTop: single character or contraction. Controls what is variable for 'alternate'
- * </ul>
- *
- * @see Collator
- * @see ULocale
- * @see RuleBasedCollator
- * @deprecated use {@link org.apache.solr.schema.ICUCollationField} instead.
- */
-@Deprecated
-public class ICUCollationKeyFilterFactory extends TokenFilterFactory implements MultiTermAwareComponent, ResourceLoaderAware {
-  private Collator collator;
-
-  public void inform(ResourceLoader loader) {
-    String custom = args.get("custom");
-    String localeID = args.get("locale");
-    String strength = args.get("strength");
-    String decomposition = args.get("decomposition");
-
-    String alternate = args.get("alternate");
-    String caseLevel = args.get("caseLevel");
-    String caseFirst = args.get("caseFirst");
-    String numeric = args.get("numeric");
-    String variableTop = args.get("variableTop");
-    
-    if (custom == null && localeID == null)
-      throw new SolrException(ErrorCode.SERVER_ERROR, "Either custom or locale is required.");
-    
-    if (custom != null && localeID != null)
-      throw new SolrException(ErrorCode.SERVER_ERROR, "Cannot specify both locale and custom. "
-          + "To tailor rules for a built-in language, see the javadocs for RuleBasedCollator. "
-          + "Then save the entire customized ruleset to a file, and use with the custom parameter");
-    
-    if (localeID != null) { 
-      // create from a system collator, based on Locale.
-      collator = createFromLocale(localeID);
-    } else { 
-      // create from a custom ruleset
-      collator = createFromRules(custom, loader);
-    }
-    
-    // set the strength flag, otherwise it will be the default.
-    if (strength != null) {
-      if (strength.equalsIgnoreCase("primary"))
-        collator.setStrength(Collator.PRIMARY);
-      else if (strength.equalsIgnoreCase("secondary"))
-        collator.setStrength(Collator.SECONDARY);
-      else if (strength.equalsIgnoreCase("tertiary"))
-        collator.setStrength(Collator.TERTIARY);
-      else if (strength.equalsIgnoreCase("quaternary"))
-        collator.setStrength(Collator.QUATERNARY);
-      else if (strength.equalsIgnoreCase("identical"))
-        collator.setStrength(Collator.IDENTICAL);
-      else
-        throw new SolrException(ErrorCode.SERVER_ERROR, "Invalid strength: " + strength);
-    }
-    
-    // set the decomposition flag, otherwise it will be the default.
-    if (decomposition != null) {
-      if (decomposition.equalsIgnoreCase("no"))
-        collator.setDecomposition(Collator.NO_DECOMPOSITION);
-      else if (decomposition.equalsIgnoreCase("canonical"))
-        collator.setDecomposition(Collator.CANONICAL_DECOMPOSITION);
-      else
-        throw new SolrException(ErrorCode.SERVER_ERROR, "Invalid decomposition: " + decomposition);
-    }
-    
-    // expert options: concrete subclasses are always a RuleBasedCollator
-    RuleBasedCollator rbc = (RuleBasedCollator) collator;
-    if (alternate != null) {
-      if (alternate.equalsIgnoreCase("shifted")) {
-        rbc.setAlternateHandlingShifted(true);
-      } else if (alternate.equalsIgnoreCase("non-ignorable")) {
-        rbc.setAlternateHandlingShifted(false);
-      } else {
-        throw new SolrException(ErrorCode.SERVER_ERROR, "Invalid alternate: " + alternate);
-      }
-    }
-    if (caseLevel != null) {
-      rbc.setCaseLevel(Boolean.parseBoolean(caseLevel));
-    }
-    if (caseFirst != null) {
-      if (caseFirst.equalsIgnoreCase("lower")) {
-        rbc.setLowerCaseFirst(true);
-      } else if (caseFirst.equalsIgnoreCase("upper")) {
-        rbc.setUpperCaseFirst(true);
-      } else {
-        throw new SolrException(ErrorCode.SERVER_ERROR, "Invalid caseFirst: " + caseFirst);
-      }
-    }
-    if (numeric != null) {
-      rbc.setNumericCollation(Boolean.parseBoolean(numeric));
-    }
-    if (variableTop != null) {
-      rbc.setVariableTop(variableTop);
-    }
-  }
-  
-  public TokenStream create(TokenStream input) {
-    return new ICUCollationKeyFilter(input, collator);
-  }
-  
-  /*
-   * Create a locale from localeID.
-   * Then return the appropriate collator for the locale.
-   */
-  private Collator createFromLocale(String localeID) {
-    return Collator.getInstance(new ULocale(localeID));
-  }
-  
-  /*
-   * Read custom rules from a file, and create a RuleBasedCollator
-   * The file cannot support comments, as # might be in the rules!
-   */
-  private Collator createFromRules(String fileName, ResourceLoader loader) {
-    InputStream input = null;
-    try {
-     input = loader.openResource(fileName);
-     String rules = IOUtils.toString(input, "UTF-8");
-     return new RuleBasedCollator(rules);
-    } catch (Exception e) {
-      // io error or invalid rules
-      throw new RuntimeException(e);
-    } finally {
-      IOUtils.closeQuietly(input);
-    }
-  }
-  
-  @Override
-  public AbstractAnalysisFactory getMultiTermComponent() {
-    return this;
-  }
-}
diff --git a/solr/contrib/analysis-extras/src/test-files/analysis-extras/solr/conf/schema-icucollatefilter.xml b/solr/contrib/analysis-extras/src/test-files/analysis-extras/solr/conf/schema-icucollatefilter.xml
deleted file mode 100644
index dba7aeb..0000000
--- a/solr/contrib/analysis-extras/src/test-files/analysis-extras/solr/conf/schema-icucollatefilter.xml
+++ /dev/null
@@ -1,61 +0,0 @@
-<?xml version="1.0" ?>
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-
-<!-- Test schema file for ICUCollationKeyFilter (deprecated: use ICUCollationField instead) -->
-
-<schema name="test" version="1.0">
-  <types>
-    <fieldType name="int" class="solr.TrieIntField" precisionStep="0" omitNorms="true" positionIncrementGap="0"/>
-
-    <!-- basic text field -->
-    <fieldtype name="text" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.StandardTokenizerFactory"/>
-        <filter class="solr.LowerCaseFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-    
-    <fieldtype name="sort_ar_t" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.KeywordTokenizerFactory"/>
-        <filter class="solr.ICUCollationKeyFilterFactory" locale="ar"/>
-      </analyzer>
-    </fieldtype>
-    
-    <fieldtype name="sort_de_t" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.KeywordTokenizerFactory"/>
-        <filter class="solr.ICUCollationKeyFilterFactory" locale="de" strength="primary"/>
-      </analyzer>
-    </fieldtype>
-  </types>
-
-  <fields>
-    <field name="id" type="int" indexed="true" stored="true" multiValued="false" required="false"/>
-    <field name="text" type="text" indexed="true" stored="false"/>
-    <field name="sort_ar"       type="sort_ar_t"       indexed="true" stored="false" multiValued="false"/>
-    <field name="sort_de"       type="sort_de_t"       indexed="true" stored="false" multiValued="false"/>
-  </fields>
-
-  <defaultSearchField>text</defaultSearchField>
-  <uniqueKey>id</uniqueKey>
-
-  <!-- copy our text to some sort fields with different orders -->
-  <copyField source="text" dest="sort_ar"/>
-  <copyField source="text" dest="sort_de"/>
-</schema>
diff --git a/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestICUCollationKeyFilterFactory.java b/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestICUCollationKeyFilterFactory.java
deleted file mode 100644
index 2386533..0000000
--- a/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestICUCollationKeyFilterFactory.java
+++ /dev/null
@@ -1,278 +0,0 @@
-package org.apache.solr.analysis;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.StringReader;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.core.KeywordTokenizer;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-
-import com.ibm.icu.text.Collator;
-import com.ibm.icu.text.RuleBasedCollator;
-import com.ibm.icu.util.ULocale;
-
-@Deprecated
-public class TestICUCollationKeyFilterFactory extends BaseTokenStreamTestCase {
-
-  /*
-   * Turkish has some funny casing.
-   * This test shows how you can solve this kind of thing easily with collation.
-   * Instead of using LowerCaseFilter, use a turkish collator with primary strength.
-   * Then things will sort and match correctly.
-   */
-  public void testBasicUsage() throws IOException {
-    String turkishUpperCase = "I WÄ°LL USE TURKÄ°SH CASING";
-    String turkishLowerCase = "Ä± will use turkish casÄ±ng";
-    ICUCollationKeyFilterFactory factory = new ICUCollationKeyFilterFactory();
-    Map<String,String> args = new HashMap<String,String>();
-    args.put("locale", "tr");
-    args.put("strength", "primary");
-    factory.init(args);
-    factory.inform(new StringMockSolrResourceLoader(""));
-    TokenStream tsUpper = factory.create(
-        new KeywordTokenizer(new StringReader(turkishUpperCase)));
-    TokenStream tsLower = factory.create(
-        new KeywordTokenizer(new StringReader(turkishLowerCase)));
-    assertCollatesToSame(tsUpper, tsLower);
-  }
-  
-  /*
-   * Test usage of the decomposition option for unicode normalization.
-   */
-  public void testNormalization() throws IOException {
-    String turkishUpperCase = "I W\u0049\u0307LL USE TURKÄ°SH CASING";
-    String turkishLowerCase = "Ä± will use turkish casÄ±ng";
-    ICUCollationKeyFilterFactory factory = new ICUCollationKeyFilterFactory();
-    Map<String,String> args = new HashMap<String,String>();
-    args.put("locale", "tr");
-    args.put("strength", "primary");
-    args.put("decomposition", "canonical");
-    factory.init(args);
-    factory.inform(new StringMockSolrResourceLoader(""));
-    TokenStream tsUpper = factory.create(
-        new KeywordTokenizer(new StringReader(turkishUpperCase)));
-    TokenStream tsLower = factory.create(
-        new KeywordTokenizer(new StringReader(turkishLowerCase)));
-    assertCollatesToSame(tsUpper, tsLower);
-  }
-  
-  /*
-   * Test secondary strength, for english case is not significant.
-   */
-  public void testSecondaryStrength() throws IOException {
-    String upperCase = "TESTING";
-    String lowerCase = "testing";
-    ICUCollationKeyFilterFactory factory = new ICUCollationKeyFilterFactory();
-    Map<String,String> args = new HashMap<String,String>();
-    args.put("locale", "en");
-    args.put("strength", "secondary");
-    args.put("decomposition", "no");
-    factory.init(args);
-    factory.inform(new StringMockSolrResourceLoader(""));
-    TokenStream tsUpper = factory.create(
-        new KeywordTokenizer(new StringReader(upperCase)));
-    TokenStream tsLower = factory.create(
-        new KeywordTokenizer(new StringReader(lowerCase)));
-    assertCollatesToSame(tsUpper, tsLower);
-  }
-  
-  /*
-   * Setting alternate=shifted to shift whitespace, punctuation and symbols
-   * to quaternary level 
-   */
-  public void testIgnorePunctuation() throws IOException {
-    String withPunctuation = "foo-bar";
-    String withoutPunctuation = "foo bar";
-    ICUCollationKeyFilterFactory factory = new ICUCollationKeyFilterFactory();
-    Map<String,String> args = new HashMap<String,String>();
-    args.put("locale", "en");
-    args.put("strength", "primary");
-    args.put("alternate", "shifted");
-    factory.init(args);
-    factory.inform(new StringMockSolrResourceLoader(""));
-    TokenStream tsPunctuation = factory.create(
-        new KeywordTokenizer(new StringReader(withPunctuation)));
-    TokenStream tsWithoutPunctuation = factory.create(
-        new KeywordTokenizer(new StringReader(withoutPunctuation)));
-    assertCollatesToSame(tsPunctuation, tsWithoutPunctuation);
-  }
-  
-  /*
-   * Setting alternate=shifted and variableTop to shift whitespace, but not 
-   * punctuation or symbols, to quaternary level 
-   */
-  public void testIgnoreWhitespace() throws IOException {
-    String withSpace = "foo bar";
-    String withoutSpace = "foobar";
-    String withPunctuation = "foo-bar";
-    ICUCollationKeyFilterFactory factory = new ICUCollationKeyFilterFactory();
-    Map<String,String> args = new HashMap<String,String>();
-    args.put("locale", "en");
-    args.put("strength", "primary");
-    args.put("alternate", "shifted");
-    args.put("variableTop", " ");
-    factory.init(args);
-    factory.inform(new StringMockSolrResourceLoader(""));
-    TokenStream tsWithSpace = factory.create(
-        new KeywordTokenizer(new StringReader(withSpace)));
-    TokenStream tsWithoutSpace = factory.create(
-        new KeywordTokenizer(new StringReader(withoutSpace)));
-    assertCollatesToSame(tsWithSpace, tsWithoutSpace);
-    // now assert that punctuation still matters: foo-bar < foo bar
-    tsWithSpace = factory.create(
-        new KeywordTokenizer(new StringReader(withSpace)));
-    TokenStream tsWithPunctuation = factory.create(
-        new KeywordTokenizer(new StringReader(withPunctuation)));
-    assertCollation(tsWithPunctuation, tsWithSpace, -1);
-  }
-  
-  /*
-   * Setting numeric to encode digits with numeric value, so that
-   * foobar-9 sorts before foobar-10
-   */
-  public void testNumerics() throws IOException {
-    String nine = "foobar-9";
-    String ten = "foobar-10";
-    ICUCollationKeyFilterFactory factory = new ICUCollationKeyFilterFactory();
-    Map<String,String> args = new HashMap<String,String>();
-    args.put("locale", "en");
-    args.put("numeric", "true");
-    factory.init(args);
-    factory.inform(new StringMockSolrResourceLoader(""));
-    TokenStream tsNine = factory.create(
-        new KeywordTokenizer(new StringReader(nine)));
-    TokenStream tsTen = factory.create(
-        new KeywordTokenizer(new StringReader(ten)));
-    assertCollation(tsNine, tsTen, -1);
-  }
-  
-  /*
-   * Setting caseLevel=true to create an additional case level between
-   * secondary and tertiary
-   */
-  public void testIgnoreAccentsButNotCase() throws IOException {
-    String withAccents = "rÃ©sumÃ©";
-    String withoutAccents = "resume";
-    String withAccentsUpperCase = "RÃ©sumÃ©";
-    String withoutAccentsUpperCase = "Resume";
-    ICUCollationKeyFilterFactory factory = new ICUCollationKeyFilterFactory();
-    Map<String,String> args = new HashMap<String,String>();
-    args.put("locale", "en");
-    args.put("strength", "primary");
-    args.put("caseLevel", "true");
-    factory.init(args);
-    factory.inform(new StringMockSolrResourceLoader(""));
-    TokenStream tsWithAccents = factory.create(
-        new KeywordTokenizer(new StringReader(withAccents)));
-    TokenStream tsWithoutAccents = factory.create(
-        new KeywordTokenizer(new StringReader(withoutAccents)));
-    assertCollatesToSame(tsWithAccents, tsWithoutAccents);
-    
-    TokenStream tsWithAccentsUpperCase = factory.create(
-        new KeywordTokenizer(new StringReader(withAccentsUpperCase)));
-    TokenStream tsWithoutAccentsUpperCase = factory.create(
-        new KeywordTokenizer(new StringReader(withoutAccentsUpperCase)));
-    assertCollatesToSame(tsWithAccentsUpperCase, tsWithoutAccentsUpperCase);
-    
-    // now assert that case still matters: resume < Resume
-    TokenStream tsLower = factory.create(
-        new KeywordTokenizer(new StringReader(withoutAccents)));
-    TokenStream tsUpper = factory.create(
-        new KeywordTokenizer(new StringReader(withoutAccentsUpperCase)));
-    assertCollation(tsLower, tsUpper, -1);
-  }
-  
-  /*
-   * Setting caseFirst=upper to cause uppercase strings to sort
-   * before lowercase ones.
-   */
-  public void testUpperCaseFirst() throws IOException {
-    String lower = "resume";
-    String upper = "Resume";
-    ICUCollationKeyFilterFactory factory = new ICUCollationKeyFilterFactory();
-    Map<String,String> args = new HashMap<String,String>();
-    args.put("locale", "en");
-    args.put("strength", "tertiary");
-    args.put("caseFirst", "upper");
-    factory.init(args);
-    factory.inform(new StringMockSolrResourceLoader(""));
-    TokenStream tsLower = factory.create(
-        new KeywordTokenizer(new StringReader(lower)));
-    TokenStream tsUpper = factory.create(
-        new KeywordTokenizer(new StringReader(upper)));
-    assertCollation(tsUpper, tsLower, -1);
-  }
-
-  /*
-   * For german, you might want oe to sort and match with o umlaut.
-   * This is not the default, but you can make a customized ruleset to do this.
-   *
-   * The default is DIN 5007-1, this shows how to tailor a collator to get DIN 5007-2 behavior.
-   *  http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4423383
-   */
-  public void testCustomRules() throws Exception {
-    RuleBasedCollator baseCollator = (RuleBasedCollator) Collator.getInstance(new ULocale("de_DE"));
-
-    String DIN5007_2_tailorings =
-      "& ae , a\u0308 & AE , A\u0308"+
-      "& oe , o\u0308 & OE , O\u0308"+
-      "& ue , u\u0308 & UE , u\u0308";
-
-    RuleBasedCollator tailoredCollator = new RuleBasedCollator(baseCollator.getRules() + DIN5007_2_tailorings);
-    String tailoredRules = tailoredCollator.getRules();
-    //
-    // at this point, you would save these tailoredRules to a file, 
-    // and use the custom parameter.
-    //
-    String germanUmlaut = "TÃ¶ne";
-    String germanOE = "Toene";
-    ICUCollationKeyFilterFactory factory = new ICUCollationKeyFilterFactory();
-    Map<String,String> args = new HashMap<String,String>();
-    args.put("custom", "rules.txt");
-    args.put("strength", "primary");
-    factory.init(args);
-    factory.inform(new StringMockSolrResourceLoader(tailoredRules));
-    TokenStream tsUmlaut = factory.create(
-        new KeywordTokenizer(new StringReader(germanUmlaut)));
-    TokenStream tsOE = factory.create(
-        new KeywordTokenizer(new StringReader(germanOE)));
-
-    assertCollatesToSame(tsUmlaut, tsOE);
-  }
-  
-  private void assertCollatesToSame(TokenStream stream1, TokenStream stream2) throws IOException {
-    assertCollation(stream1, stream2, 0);
-  }
-  
-  private void assertCollation(TokenStream stream1, TokenStream stream2, int comparison) throws IOException {
-    CharTermAttribute term1 = stream1
-        .addAttribute(CharTermAttribute.class);
-    CharTermAttribute term2 = stream2
-        .addAttribute(CharTermAttribute.class);
-    assertTrue(stream1.incrementToken());
-    assertTrue(stream2.incrementToken());
-    assertEquals(Integer.signum(comparison), Integer.signum(term1.toString().compareTo(term2.toString())));
-    assertFalse(stream1.incrementToken());
-    assertFalse(stream2.incrementToken());
-  }
-}
diff --git a/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestICUCollationKeyRangeQueries.java b/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestICUCollationKeyRangeQueries.java
deleted file mode 100644
index 800dd31..0000000
--- a/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestICUCollationKeyRangeQueries.java
+++ /dev/null
@@ -1,84 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.solr.analysis;
-
-import org.apache.solr.SolrTestCaseJ4;
-import org.junit.BeforeClass;
-
-/**
- * Tests {@link ICUCollationKeyFilterFactory} with RangeQueries
- */
-public class TestICUCollationKeyRangeQueries extends SolrTestCaseJ4 {
-  
-  @BeforeClass
-  public static void beforeClass() throws Exception {
-    initCore("solrconfig-icucollate.xml","schema-icucollatefilter.xml", "analysis-extras/solr");
-    // add some docs
-    assertU(adoc("id", "1", "text", "\u0633\u0627\u0628"));
-    assertU(adoc("id", "2", "text", "I WÄ°LL USE TURKÄ°SH CASING"));
-    assertU(adoc("id", "3", "text", "Ä± will use turkish casÄ±ng"));
-    assertU(adoc("id", "4", "text", "TÃ¶ne"));
-    assertU(adoc("id", "5", "text", "I W\u0049\u0307LL USE TURKÄ°SH CASING"));
-    assertU(adoc("id", "6", "text", "ï¼´ï?ï½??ï½??ï½?"));
-    assertU(adoc("id", "7", "text", "Tone"));
-    assertU(adoc("id", "8", "text", "Testing"));
-    assertU(adoc("id", "9", "text", "testing"));
-    assertU(adoc("id", "10", "text", "toene"));
-    assertU(adoc("id", "11", "text", "Tzne"));
-    assertU(adoc("id", "12", "text", "\u0698\u0698"));
-    assertU(commit());
-  }
-  
-  /** 
-   * Test termquery with german DIN 5007-1 primary strength.
-   * In this case, Ã¶ is equivalent to o (but not oe) 
-   */
-  public void testBasicTermQuery() {
-    assertQ("Collated TQ: ",
-       req("fl", "id", "q", "sort_de:tone", "sort", "id asc" ),
-              "//*[@numFound='2']",
-              "//result/doc[1]/int[@name='id'][.=4]",
-              "//result/doc[2]/int[@name='id'][.=7]"
-    );
-  }
-  
-  /** 
-   * Test rangequery again with the DIN 5007-1 collator.
-   * We do a range query of tone .. tp, in binary order this
-   * would retrieve nothing due to case and accent differences.
-   */
-  public void testBasicRangeQuery() {
-    assertQ("Collated RangeQ: ",
-        req("fl", "id", "q", "sort_de:[tone TO tp]", "sort", "id asc" ),
-               "//*[@numFound='2']",
-               "//result/doc[1]/int[@name='id'][.=4]",
-               "//result/doc[2]/int[@name='id'][.=7]"
-     );
-  }
-
-  /** 
-   * Test rangequery again with an Arabic collator.
-   * Binary order would normally order U+0633 in this range.
-   */
-  public void testNegativeRangeQuery() {
-    assertQ("Collated RangeQ: ",
-        req("fl", "id", "q", "sort_ar:[\u062F TO \u0698]", "sort", "id asc" ),
-               "//*[@numFound='0']"
-     );
-  }
-}
diff --git a/solr/core/src/java/org/apache/solr/analysis/ArabicLetterTokenizerFactory.java b/solr/core/src/java/org/apache/solr/analysis/ArabicLetterTokenizerFactory.java
deleted file mode 100644
index 60a852c..0000000
--- a/solr/core/src/java/org/apache/solr/analysis/ArabicLetterTokenizerFactory.java
+++ /dev/null
@@ -1,47 +0,0 @@
-package org.apache.solr.analysis;
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.ar.ArabicLetterTokenizer;
-import org.apache.lucene.analysis.util.TokenizerFactory;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.io.Reader;
-import java.util.Map;
-
-
-/**
- * Factory for {@link ArabicLetterTokenizer}
- * @deprecated (3.1) Use StandardTokenizerFactory instead.
- **/
-@Deprecated
-public class ArabicLetterTokenizerFactory extends TokenizerFactory {
-
-  private static final Logger log = LoggerFactory.getLogger(ArabicLetterTokenizerFactory.class);
-
-  @Override
-  public void init(Map<String,String> args) {
-    super.init(args);
-    assureMatchVersion();
-    log.warn(getClass().getSimpleName() + " is deprecated. Use StandardTokenizeFactory instead.");
-  }
-
-  public ArabicLetterTokenizer create(Reader input) {
-    return new ArabicLetterTokenizer(luceneMatchVersion, input);
-  }
-}
diff --git a/solr/core/src/java/org/apache/solr/analysis/CJKTokenizerFactory.java b/solr/core/src/java/org/apache/solr/analysis/CJKTokenizerFactory.java
deleted file mode 100644
index 8e2e7c7..0000000
--- a/solr/core/src/java/org/apache/solr/analysis/CJKTokenizerFactory.java
+++ /dev/null
@@ -1,43 +0,0 @@
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-package org.apache.solr.analysis;
-
-import org.apache.lucene.analysis.cjk.CJKTokenizer;
-import org.apache.lucene.analysis.util.TokenizerFactory;
-
-import java.io.Reader;
-
-/** 
- * Factory for {@link CJKTokenizer}. 
- * <pre class="prettyprint" >
- * &lt;fieldType name="text_cjk" class="solr.TextField" positionIncrementGap="100"&gt;
- *   &lt;analyzer&gt;
- *     &lt;tokenizer class="solr.CJKTokenizerFactory"/&gt;
- *   &lt;/analyzer&gt;
- * &lt;/fieldType&gt;</pre>
- * @deprecated
- */
-@Deprecated
-public class CJKTokenizerFactory extends TokenizerFactory {
-  public CJKTokenizer create(Reader in) {
-    return new CJKTokenizer(in);
-  }
-}
-
diff --git a/solr/core/src/java/org/apache/solr/analysis/ChineseFilterFactory.java b/solr/core/src/java/org/apache/solr/analysis/ChineseFilterFactory.java
deleted file mode 100644
index 002ee3e..0000000
--- a/solr/core/src/java/org/apache/solr/analysis/ChineseFilterFactory.java
+++ /dev/null
@@ -1,48 +0,0 @@
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-package org.apache.solr.analysis;
-import java.util.Map;
-
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.cn.ChineseFilter;
-import org.apache.lucene.analysis.util.TokenFilterFactory;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * Factory for {@link ChineseFilter}
- * @deprecated Use {@link StopFilterFactory} instead.
- */
-@Deprecated
-public class ChineseFilterFactory extends TokenFilterFactory {
-
-  private static final Logger log = LoggerFactory.getLogger(ChineseFilterFactory.class);
-
-  @Override
-  public void init(Map<String,String> args) {
-    super.init(args);
-    log.warn(getClass().getSimpleName() + " is deprecated. Use StopFilterFactory instead.");
-  }
-  
-  public ChineseFilter create(TokenStream in) {
-    return new ChineseFilter(in);
-  }
-}
-
diff --git a/solr/core/src/java/org/apache/solr/analysis/ChineseTokenizerFactory.java b/solr/core/src/java/org/apache/solr/analysis/ChineseTokenizerFactory.java
deleted file mode 100644
index 27e12cf..0000000
--- a/solr/core/src/java/org/apache/solr/analysis/ChineseTokenizerFactory.java
+++ /dev/null
@@ -1,49 +0,0 @@
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-package org.apache.solr.analysis;
-
-import java.io.Reader;
-import java.util.Map;
-
-import org.apache.lucene.analysis.cn.ChineseTokenizer;
-import org.apache.lucene.analysis.util.TokenizerFactory;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/** 
- * Factory for {@link ChineseTokenizer}
- * @deprecated Use {@link StandardTokenizerFactory} instead.
- */
-@Deprecated
-public class ChineseTokenizerFactory extends TokenizerFactory {
-
-  private static final Logger log = LoggerFactory.getLogger(ChineseTokenizerFactory.class);
-
-  @Override
-  public void init(Map<String,String> args) {
-    super.init(args);
-    log.warn(getClass().getSimpleName() + " is deprecated. Use StandardTokenizeFactory instead.");
-  }
-  
-  public ChineseTokenizer create(Reader in) {
-    return new ChineseTokenizer(in);
-  }
-}
-
diff --git a/solr/core/src/java/org/apache/solr/analysis/CollationKeyFilterFactory.java b/solr/core/src/java/org/apache/solr/analysis/CollationKeyFilterFactory.java
deleted file mode 100644
index 8d412a7..0000000
--- a/solr/core/src/java/org/apache/solr/analysis/CollationKeyFilterFactory.java
+++ /dev/null
@@ -1,175 +0,0 @@
-package org.apache.solr.analysis;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.InputStream;
-import java.text.Collator;
-import java.text.ParseException;
-import java.text.RuleBasedCollator;
-import java.util.Locale;
-
-import org.apache.commons.io.IOUtils;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.util.*;
-import org.apache.lucene.collation.CollationKeyFilter;
-
-/**
- * Factory for {@link CollationKeyFilter}.
- * <p>
- * This factory can be created in two ways: 
- * <ul>
- *  <li>Based upon a system collator associated with a Locale.
- *  <li>Based upon a tailored ruleset.
- * </ul>
- * <p>
- * Using a System collator:
- * <ul>
- *  <li>language: ISO-639 language code (mandatory)
- *  <li>country: ISO-3166 country code (optional)
- *  <li>variant: vendor or browser-specific code (optional)
- *  <li>strength: 'primary','secondary','tertiary', or 'identical' (optional)
- *  <li>decomposition: 'no','canonical', or 'full' (optional)
- * </ul>
- * <p>
- * Using a Tailored ruleset:
- * <ul>
- *  <li>custom: UTF-8 text file containing rules supported by RuleBasedCollator (mandatory)
- *  <li>strength: 'primary','secondary','tertiary', or 'identical' (optional)
- *  <li>decomposition: 'no','canonical', or 'full' (optional)
- * </ul>
- * 
- * <pre class="prettyprint" >
- * &lt;fieldType name="text_clltnky" class="solr.TextField" positionIncrementGap="100"&gt;
- *   &lt;analyzer&gt;
- *     &lt;tokenizer class="solr.KeywordTokenizerFactory"/&gt;
- *     &lt;filter class="solr.CollationKeyFilterFactory" language="ja" country="JP"/&gt;
- *   &lt;/analyzer&gt;
- * &lt;/fieldType&gt;</pre>
- * 
- * @see Collator
- * @see Locale
- * @see RuleBasedCollator
- * @since solr 3.1
- * @deprecated use {@link org.apache.solr.schema.CollationField} instead.
- */
-@Deprecated
-public class CollationKeyFilterFactory extends TokenFilterFactory implements MultiTermAwareComponent, ResourceLoaderAware {
-  private Collator collator;
-
-  public void inform(ResourceLoader loader) {
-    String custom = args.get("custom");
-    String language = args.get("language");
-    String country = args.get("country");
-    String variant = args.get("variant");
-    String strength = args.get("strength");
-    String decomposition = args.get("decomposition");
-    
-    if (custom == null && language == null)
-      throw new InitializationException("Either custom or language is required.");
-    
-    if (custom != null && 
-        (language != null || country != null || variant != null))
-      throw new InitializationException("Cannot specify both language and custom. "
-          + "To tailor rules for a built-in language, see the javadocs for RuleBasedCollator. "
-          + "Then save the entire customized ruleset to a file, and use with the custom parameter");
-    
-    if (language != null) { 
-      // create from a system collator, based on Locale.
-      collator = createFromLocale(language, country, variant);
-    } else { 
-      // create from a custom ruleset
-      collator = createFromRules(custom, loader);
-    }
-    
-    // set the strength flag, otherwise it will be the default.
-    if (strength != null) {
-      if (strength.equalsIgnoreCase("primary"))
-        collator.setStrength(Collator.PRIMARY);
-      else if (strength.equalsIgnoreCase("secondary"))
-        collator.setStrength(Collator.SECONDARY);
-      else if (strength.equalsIgnoreCase("tertiary"))
-        collator.setStrength(Collator.TERTIARY);
-      else if (strength.equalsIgnoreCase("identical"))
-        collator.setStrength(Collator.IDENTICAL);
-      else
-        throw new InitializationException("Invalid strength: " + strength);
-    }
-    
-    // set the decomposition flag, otherwise it will be the default.
-    if (decomposition != null) {
-      if (decomposition.equalsIgnoreCase("no"))
-        collator.setDecomposition(Collator.NO_DECOMPOSITION);
-      else if (decomposition.equalsIgnoreCase("canonical"))
-        collator.setDecomposition(Collator.CANONICAL_DECOMPOSITION);
-      else if (decomposition.equalsIgnoreCase("full"))
-        collator.setDecomposition(Collator.FULL_DECOMPOSITION);
-      else
-        throw new InitializationException("Invalid decomposition: " + decomposition);
-    }
-  }
-  
-  public TokenStream create(TokenStream input) {
-    return new CollationKeyFilter(input, collator);
-  }
-  
-  /*
-   * Create a locale from language, with optional country and variant.
-   * Then return the appropriate collator for the locale.
-   */
-  private Collator createFromLocale(String language, String country, String variant) {
-    Locale locale;
-    
-    if (language != null && country == null && variant != null)
-      throw new InitializationException("To specify variant, country is required");
-    else if (language != null && country != null && variant != null)
-      locale = new Locale(language, country, variant);
-    else if (language != null && country != null)
-      locale = new Locale(language, country);
-    else 
-      locale = new Locale(language);
-    
-    return Collator.getInstance(locale);
-  }
-  
-  /*
-   * Read custom rules from a file, and create a RuleBasedCollator
-   * The file cannot support comments, as # might be in the rules!
-   */
-  private Collator createFromRules(String fileName, ResourceLoader loader) {
-    InputStream input = null;
-    try {
-     input = loader.openResource(fileName);
-     String rules = IOUtils.toString(input, "UTF-8");
-     return new RuleBasedCollator(rules);
-    } catch (IOException e) {
-      // io error
-      throw new InitializationException("IOException thrown while loading rules", e);
-    } catch (ParseException e) {
-      // invalid rules
-      throw new InitializationException("ParseException thrown while parsing rules", e);
-    } finally {
-      IOUtils.closeQuietly(input);
-    }
-  }
-  
-  @Override
-  public AbstractAnalysisFactory getMultiTermComponent() {
-    return this;
-  }
-}
diff --git a/solr/core/src/java/org/apache/solr/analysis/FSTSynonymFilterFactory.java b/solr/core/src/java/org/apache/solr/analysis/FSTSynonymFilterFactory.java
deleted file mode 100644
index 495c448..0000000
--- a/solr/core/src/java/org/apache/solr/analysis/FSTSynonymFilterFactory.java
+++ /dev/null
@@ -1,168 +0,0 @@
-package org.apache.solr.analysis;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.File;
-import java.io.IOException;
-import java.io.InputStreamReader;
-import java.io.Reader;
-import java.nio.charset.Charset;
-import java.nio.charset.CharsetDecoder;
-import java.nio.charset.CodingErrorAction;
-import java.text.ParseException;
-import java.util.List;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.core.LowerCaseFilter;
-import org.apache.lucene.analysis.core.WhitespaceTokenizer;
-import org.apache.lucene.analysis.synonym.SynonymFilter;
-import org.apache.lucene.analysis.synonym.SynonymMap;
-import org.apache.lucene.analysis.synonym.SolrSynonymParser;
-import org.apache.lucene.analysis.synonym.WordnetSynonymParser;
-import org.apache.lucene.analysis.util.*;
-import org.apache.lucene.util.Version;
-import org.apache.solr.common.util.StrUtils;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * @deprecated (3.4) use {@link SynonymFilterFactory} instead. this is only a backwards compatibility
- *                   mechanism that will be removed in Lucene 5.0
- */
-// NOTE: rename this to "SynonymFilterFactory" and nuke that delegator in Lucene 5.0!
-@Deprecated
-final class FSTSynonymFilterFactory extends TokenFilterFactory implements ResourceLoaderAware {
-
-  public static final Logger log = LoggerFactory.getLogger(FSTSynonymFilterFactory.class);
-
-  private SynonymMap map;
-  private boolean ignoreCase;
-  
-  @Override
-  public TokenStream create(TokenStream input) {
-    // if the fst is null, it means there's actually no synonyms... just return the original stream
-    // as there is nothing to do here.
-    return map.fst == null ? input : new SynonymFilter(input, map, ignoreCase);
-  }
-
-  @Override
-  public void inform(ResourceLoader loader) {
-    final boolean ignoreCase = getBoolean("ignoreCase", false); 
-    this.ignoreCase = ignoreCase;
-
-    String tf = args.get("tokenizerFactory");
-
-    final TokenizerFactory factory = tf == null ? null : loadTokenizerFactory(loader, tf);
-    
-    Analyzer analyzer = new Analyzer() {
-      @Override
-      protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
-        Tokenizer tokenizer = factory == null ? new WhitespaceTokenizer(Version.LUCENE_31, reader) : factory.create(reader);
-        TokenStream stream = ignoreCase ? new LowerCaseFilter(Version.LUCENE_31, tokenizer) : tokenizer;
-        return new TokenStreamComponents(tokenizer, stream);
-      }
-    };
-
-    String format = args.get("format");
-    try {
-      if (format == null || format.equals("solr")) {
-        // TODO: expose dedup as a parameter?
-        map = loadSolrSynonyms(loader, true, analyzer);
-      } else if (format.equals("wordnet")) {
-        map = loadWordnetSynonyms(loader, true, analyzer);
-      } else {
-        // TODO: somehow make this more pluggable
-        throw new InitializationException("Unrecognized synonyms format: " + format);
-      }
-    } catch (Exception e) {
-      throw new InitializationException("Exception thrown while loading synonyms", e);
-    }
-    
-    if (map.fst == null) {
-      log.warn("Synonyms loaded with " + args + " has empty rule set!");
-    }
-  }
-  
-  /**
-   * Load synonyms from the solr format, "format=solr".
-   */
-  private SynonymMap loadSolrSynonyms(ResourceLoader loader, boolean dedup, Analyzer analyzer) throws IOException, ParseException {
-    final boolean expand = getBoolean("expand", true);
-    String synonyms = args.get("synonyms");
-    if (synonyms == null)
-      throw new InitializationException("Missing required argument 'synonyms'.");
-    
-    CharsetDecoder decoder = Charset.forName("UTF-8").newDecoder()
-      .onMalformedInput(CodingErrorAction.REPORT)
-      .onUnmappableCharacter(CodingErrorAction.REPORT);
-    
-    SolrSynonymParser parser = new SolrSynonymParser(dedup, expand, analyzer);
-    File synonymFile = new File(synonyms);
-    if (synonymFile.exists()) {
-      decoder.reset();
-      parser.add(new InputStreamReader(loader.openResource(synonyms), decoder));
-    } else {
-      List<String> files = StrUtils.splitFileNames(synonyms);
-      for (String file : files) {
-        decoder.reset();
-        parser.add(new InputStreamReader(loader.openResource(file), decoder));
-      }
-    }
-    return parser.build();
-  }
-  
-  /**
-   * Load synonyms from the wordnet format, "format=wordnet".
-   */
-  private SynonymMap loadWordnetSynonyms(ResourceLoader loader, boolean dedup, Analyzer analyzer) throws IOException, ParseException {
-    final boolean expand = getBoolean("expand", true);
-    String synonyms = args.get("synonyms");
-    if (synonyms == null)
-      throw new InitializationException("Missing required argument 'synonyms'.");
-    
-    CharsetDecoder decoder = Charset.forName("UTF-8").newDecoder()
-      .onMalformedInput(CodingErrorAction.REPORT)
-      .onUnmappableCharacter(CodingErrorAction.REPORT);
-    
-    WordnetSynonymParser parser = new WordnetSynonymParser(dedup, expand, analyzer);
-    File synonymFile = new File(synonyms);
-    if (synonymFile.exists()) {
-      decoder.reset();
-      parser.add(new InputStreamReader(loader.openResource(synonyms), decoder));
-    } else {
-      List<String> files = StrUtils.splitFileNames(synonyms);
-      for (String file : files) {
-        decoder.reset();
-        parser.add(new InputStreamReader(loader.openResource(file), decoder));
-      }
-    }
-    return parser.build();
-  }
-  
-  private TokenizerFactory loadTokenizerFactory(ResourceLoader loader, String cname){
-    TokenizerFactory tokFactory = loader.newInstance(cname, TokenizerFactory.class);
-    tokFactory.setLuceneMatchVersion(luceneMatchVersion);
-    tokFactory.init(args);
-    if (tokFactory instanceof ResourceLoaderAware) {
-      ((ResourceLoaderAware) tokFactory).inform(loader);
-    }
-    return tokFactory;
-  }
-}
diff --git a/solr/core/src/java/org/apache/solr/analysis/PatternReplaceCharFilterFactory.java b/solr/core/src/java/org/apache/solr/analysis/PatternReplaceCharFilterFactory.java
index 0801893..fdaeca1 100644
--- a/solr/core/src/java/org/apache/solr/analysis/PatternReplaceCharFilterFactory.java
+++ b/solr/core/src/java/org/apache/solr/analysis/PatternReplaceCharFilterFactory.java
@@ -19,20 +19,18 @@ package org.apache.solr.analysis;
 
 import java.util.Map;
 import java.util.regex.Pattern;
-import java.util.regex.PatternSyntaxException;
 
 import org.apache.lucene.analysis.CharStream;
 import org.apache.lucene.analysis.pattern.PatternReplaceCharFilter;
 import org.apache.lucene.analysis.util.CharFilterFactory;
-import org.apache.lucene.analysis.util.InitializationException;
 
 /**
  * Factory for {@link PatternReplaceCharFilter}. 
  * <pre class="prettyprint" >
  * &lt;fieldType name="text_ptnreplace" class="solr.TextField" positionIncrementGap="100"&gt;
  *   &lt;analyzer&gt;
- *     &lt;charFilter class="solr.PatternReplaceCharFilterFactory" pattern="([^a-z])" replacement=""
- *                 maxBlockChars="10000" blockDelimiters="|"/&gt;
+ *     &lt;charFilter class="solr.PatternReplaceCharFilterFactory" 
+ *                    pattern="([^a-z])" replacement=""/&gt;
  *     &lt;tokenizer class="solr.KeywordTokenizerFactory"/&gt;
  *   &lt;/analyzer&gt;
  * &lt;/fieldType&gt;</pre>
@@ -44,8 +42,6 @@ public class PatternReplaceCharFilterFactory extends CharFilterFactory {
   
   private Pattern p;
   private String replacement;
-  private int maxBlockChars;
-  private String blockDelimiters;
 
   @Override
   public void init(Map<String, String> args) {
@@ -54,11 +50,10 @@ public class PatternReplaceCharFilterFactory extends CharFilterFactory {
     replacement = args.get( "replacement" );
     if( replacement == null )
       replacement = "";
-    maxBlockChars = getInt( "maxBlockChars", PatternReplaceCharFilter.DEFAULT_MAX_BLOCK_CHARS );
-    blockDelimiters = args.get( "blockDelimiters" );
+    // TODO: throw exception if you set maxBlockChars or blockDelimiters ?
   }
 
   public CharStream create(CharStream input) {
-    return new PatternReplaceCharFilter( p, replacement, maxBlockChars, blockDelimiters, input );
+    return new PatternReplaceCharFilter( p, replacement, input );
   }
 }
diff --git a/solr/core/src/java/org/apache/solr/analysis/RussianLetterTokenizerFactory.java b/solr/core/src/java/org/apache/solr/analysis/RussianLetterTokenizerFactory.java
deleted file mode 100644
index 340dfc0..0000000
--- a/solr/core/src/java/org/apache/solr/analysis/RussianLetterTokenizerFactory.java
+++ /dev/null
@@ -1,52 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.solr.analysis;
-
-import java.io.Reader;
-import java.util.Map;
-
-import org.apache.lucene.analysis.ru.RussianLetterTokenizer;
-import org.apache.lucene.analysis.util.InitializationException;
-import org.apache.lucene.analysis.util.TokenizerFactory;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/** @deprecated Use {@link StandardTokenizerFactory} instead.
- *  This tokenizer has no Russian-specific functionality.
- */
-@Deprecated
-public class RussianLetterTokenizerFactory extends TokenizerFactory {
-
-  private static final Logger log = LoggerFactory.getLogger(RussianLetterTokenizerFactory.class);
-
-  @Override
-  public void init(Map<String, String> args) {
-    super.init(args);
-    if (args.containsKey("charset"))
-      throw new InitializationException(
-          "The charset parameter is no longer supported.  "
-          + "Please process your documents as Unicode instead.");
-    assureMatchVersion();
-    log.warn(getClass().getSimpleName() + " is deprecated. Use StandardTokenizerFactory instead.");
-  }
-
-  public RussianLetterTokenizer create(Reader in) {
-    return new RussianLetterTokenizer(luceneMatchVersion,in);
-  }
-}
-
diff --git a/solr/core/src/java/org/apache/solr/analysis/SlowSynonymFilter.java b/solr/core/src/java/org/apache/solr/analysis/SlowSynonymFilter.java
deleted file mode 100644
index d97cacd..0000000
--- a/solr/core/src/java/org/apache/solr/analysis/SlowSynonymFilter.java
+++ /dev/null
@@ -1,261 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.solr.analysis;
-
-import org.apache.lucene.analysis.Token;
-import org.apache.lucene.analysis.TokenFilter;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
-import org.apache.lucene.util.AttributeSource;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.LinkedList;
-
-/** SynonymFilter handles multi-token synonyms with variable position increment offsets.
- * <p>
- * The matched tokens from the input stream may be optionally passed through (includeOrig=true)
- * or discarded.  If the original tokens are included, the position increments may be modified
- * to retain absolute positions after merging with the synonym tokenstream.
- * <p>
- * Generated synonyms will start at the same position as the first matched source token.
- * @deprecated (3.4) use {@link SynonymFilterFactory} instead. only for precise index backwards compatibility. this factory will be removed in Lucene 5.0
- */
-@Deprecated
-final class SlowSynonymFilter extends TokenFilter {
-
-  private final SlowSynonymMap map;  // Map<String, SynonymMap>
-  private Iterator<AttributeSource> replacement;  // iterator over generated tokens
-
-  public SlowSynonymFilter(TokenStream in, SlowSynonymMap map) {
-    super(in);
-    if (map == null)
-      throw new IllegalArgumentException("map is required");
-
-    this.map = map;
-    // just ensuring these attributes exist...
-    addAttribute(CharTermAttribute.class);
-    addAttribute(PositionIncrementAttribute.class);
-    addAttribute(OffsetAttribute.class);
-    addAttribute(TypeAttribute.class);
-  }
-
-
-  /*
-   * Need to worry about multiple scenarios:
-   *  - need to go for the longest match
-   *    a b => foo      #shouldn't match if "a b" is followed by "c d"
-   *    a b c d => bar
-   *  - need to backtrack - retry matches for tokens already read
-   *     a b c d => foo
-   *       b c => bar
-   *     If the input stream is "a b c x", one will consume "a b c d"
-   *     trying to match the first rule... all but "a" should be
-   *     pushed back so a match may be made on "b c".
-   *  - don't try and match generated tokens (thus need separate queue)
-   *    matching is not recursive.
-   *  - handle optional generation of original tokens in all these cases,
-   *    merging token streams to preserve token positions.
-   *  - preserve original positionIncrement of first matched token
-   */
-  @Override
-  public boolean incrementToken() throws IOException {
-    while (true) {
-      // if there are any generated tokens, return them... don't try any
-      // matches against them, as we specifically don't want recursion.
-      if (replacement!=null && replacement.hasNext()) {
-        copy(this, replacement.next());
-        return true;
-      }
-
-      // common case fast-path of first token not matching anything
-      AttributeSource firstTok = nextTok();
-      if (firstTok == null) return false;
-      CharTermAttribute termAtt = firstTok.addAttribute(CharTermAttribute.class);
-      SlowSynonymMap result = map.submap!=null ? map.submap.get(termAtt.buffer(), 0, termAtt.length()) : null;
-      if (result == null) {
-        copy(this, firstTok);
-        return true;
-      }
-
-      // fast-path failed, clone ourselves if needed
-      if (firstTok == this)
-        firstTok = cloneAttributes();
-      // OK, we matched a token, so find the longest match.
-
-      matched = new LinkedList<AttributeSource>();
-
-      result = match(result);
-
-      if (result==null) {
-        // no match, simply return the first token read.
-        copy(this, firstTok);
-        return true;
-      }
-
-      // reuse, or create new one each time?
-      ArrayList<AttributeSource> generated = new ArrayList<AttributeSource>(result.synonyms.length + matched.size() + 1);
-
-      //
-      // there was a match... let's generate the new tokens, merging
-      // in the matched tokens (position increments need adjusting)
-      //
-      AttributeSource lastTok = matched.isEmpty() ? firstTok : matched.getLast();
-      boolean includeOrig = result.includeOrig();
-
-      AttributeSource origTok = includeOrig ? firstTok : null;
-      PositionIncrementAttribute firstPosIncAtt = firstTok.addAttribute(PositionIncrementAttribute.class);
-      int origPos = firstPosIncAtt.getPositionIncrement();  // position of origTok in the original stream
-      int repPos=0; // curr position in replacement token stream
-      int pos=0;  // current position in merged token stream
-
-      for (int i=0; i<result.synonyms.length; i++) {
-        Token repTok = result.synonyms[i];
-        AttributeSource newTok = firstTok.cloneAttributes();
-        CharTermAttribute newTermAtt = newTok.addAttribute(CharTermAttribute.class);
-        OffsetAttribute newOffsetAtt = newTok.addAttribute(OffsetAttribute.class);
-        PositionIncrementAttribute newPosIncAtt = newTok.addAttribute(PositionIncrementAttribute.class);
-
-        OffsetAttribute lastOffsetAtt = lastTok.addAttribute(OffsetAttribute.class);
-
-        newOffsetAtt.setOffset(newOffsetAtt.startOffset(), lastOffsetAtt.endOffset());
-        newTermAtt.copyBuffer(repTok.buffer(), 0, repTok.length());
-        repPos += repTok.getPositionIncrement();
-        if (i==0) repPos=origPos;  // make position of first token equal to original
-
-        // if necessary, insert original tokens and adjust position increment
-        while (origTok != null && origPos <= repPos) {
-          PositionIncrementAttribute origPosInc = origTok.addAttribute(PositionIncrementAttribute.class);
-          origPosInc.setPositionIncrement(origPos-pos);
-          generated.add(origTok);
-          pos += origPosInc.getPositionIncrement();
-          origTok = matched.isEmpty() ? null : matched.removeFirst();
-          if (origTok != null) {
-            origPosInc = origTok.addAttribute(PositionIncrementAttribute.class);
-            origPos += origPosInc.getPositionIncrement();
-          }
-        }
-
-        newPosIncAtt.setPositionIncrement(repPos - pos);
-        generated.add(newTok);
-        pos += newPosIncAtt.getPositionIncrement();
-      }
-
-      // finish up any leftover original tokens
-      while (origTok!=null) {
-        PositionIncrementAttribute origPosInc = origTok.addAttribute(PositionIncrementAttribute.class);
-        origPosInc.setPositionIncrement(origPos-pos);
-        generated.add(origTok);
-        pos += origPosInc.getPositionIncrement();
-        origTok = matched.isEmpty() ? null : matched.removeFirst();
-        if (origTok != null) {
-          origPosInc = origTok.addAttribute(PositionIncrementAttribute.class);
-          origPos += origPosInc.getPositionIncrement();
-        }
-      }
-
-      // what if we replaced a longer sequence with a shorter one?
-      // a/0 b/5 =>  foo/0
-      // should I re-create the gap on the next buffered token?
-
-      replacement = generated.iterator();
-      // Now return to the top of the loop to read and return the first
-      // generated token.. The reason this is done is that we may have generated
-      // nothing at all, and may need to continue with more matching logic.
-    }
-  }
-
-
-  //
-  // Defer creation of the buffer until the first time it is used to
-  // optimize short fields with no matches.
-  //
-  private LinkedList<AttributeSource> buffer;
-  private LinkedList<AttributeSource> matched;
-
-  private boolean exhausted;
-
-  private AttributeSource nextTok() throws IOException {
-    if (buffer!=null && !buffer.isEmpty()) {
-      return buffer.removeFirst();
-    } else {
-      if (!exhausted && input.incrementToken()) {
-        return this;
-      } else {
-        exhausted = true;
-        return null;
-      }
-    }
-  }
-
-  private void pushTok(AttributeSource t) {
-    if (buffer==null) buffer=new LinkedList<AttributeSource>();
-    buffer.addFirst(t);
-  }
-
-  private SlowSynonymMap match(SlowSynonymMap map) throws IOException {
-    SlowSynonymMap result = null;
-
-    if (map.submap != null) {
-      AttributeSource tok = nextTok();
-      if (tok != null) {
-        // clone ourselves.
-        if (tok == this)
-          tok = cloneAttributes();
-        // check for positionIncrement!=1?  if>1, should not match, if==0, check multiple at this level?
-        CharTermAttribute termAtt = tok.getAttribute(CharTermAttribute.class);
-        SlowSynonymMap subMap = map.submap.get(termAtt.buffer(), 0, termAtt.length());
-
-        if (subMap != null) {
-          // recurse
-          result = match(subMap);
-        }
-
-        if (result != null) {
-          matched.addFirst(tok);
-        } else {
-          // push back unmatched token
-          pushTok(tok);
-        }
-      }
-    }
-
-    // if no longer sequence matched, so if this node has synonyms, it's the match.
-    if (result==null && map.synonyms!=null) {
-      result = map;
-    }
-
-    return result;
-  }
-
-  private void copy(AttributeSource target, AttributeSource source) {
-    if (target != source)
-      source.copyTo(target);
-  }
-
-  @Override
-  public void reset() throws IOException {
-    input.reset();
-    replacement = null;
-    exhausted = false;
-  }
-}
diff --git a/solr/core/src/java/org/apache/solr/analysis/SlowSynonymFilterFactory.java b/solr/core/src/java/org/apache/solr/analysis/SlowSynonymFilterFactory.java
deleted file mode 100644
index b898a03..0000000
--- a/solr/core/src/java/org/apache/solr/analysis/SlowSynonymFilterFactory.java
+++ /dev/null
@@ -1,189 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.solr.analysis;
-
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.util.*;
-import org.apache.solr.common.util.StrUtils;
-
-import java.io.File;
-import java.io.IOException;
-import java.io.Reader;
-import java.io.StringReader;
-import java.util.ArrayList;
-import java.util.List;
-
-/**
- * Factory for {@link SlowSynonymFilter} (only used with luceneMatchVersion < 3.4)
- * <pre class="prettyprint" >
- * &lt;fieldType name="text_synonym" class="solr.TextField" positionIncrementGap="100"&gt;
- *   &lt;analyzer&gt;
- *     &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
- *     &lt;filter class="solr.SynonymFilterFactory" synonyms="synonyms.txt" ignoreCase="false"
- *             expand="true" tokenizerFactory="solr.WhitespaceTokenizerFactory"/&gt;
- *   &lt;/analyzer&gt;
- * &lt;/fieldType&gt;</pre>
- * @deprecated (3.4) use {@link SynonymFilterFactory} instead. only for precise index backwards compatibility. this factory will be removed in Lucene 5.0
- */
-@Deprecated
-final class SlowSynonymFilterFactory extends TokenFilterFactory implements ResourceLoaderAware {
-
-  public void inform(ResourceLoader loader) {
-    String synonyms = args.get("synonyms");
-    if (synonyms == null)
-      throw new InitializationException("Missing required argument 'synonyms'.");
-    boolean ignoreCase = getBoolean("ignoreCase", false);
-    boolean expand = getBoolean("expand", true);
-
-    String tf = args.get("tokenizerFactory");
-    TokenizerFactory tokFactory = null;
-    if( tf != null ){
-      tokFactory = loadTokenizerFactory(loader, tf);
-    }
-
-    Iterable<String> wlist=loadRules( synonyms, loader );
-    
-    synMap = new SlowSynonymMap(ignoreCase);
-    parseRules(wlist, synMap, "=>", ",", expand,tokFactory);
-  }
-  
-  /**
-   * @return a list of all rules
-   */
-  protected Iterable<String> loadRules( String synonyms, ResourceLoader loader ) {
-    List<String> wlist=null;
-    try {
-      File synonymFile = new File(synonyms);
-      if (synonymFile.exists()) {
-        wlist = loader.getLines(synonyms);
-      } else  {
-        List<String> files = StrUtils.splitFileNames(synonyms);
-        wlist = new ArrayList<String>();
-        for (String file : files) {
-          List<String> lines = loader.getLines(file.trim());
-          wlist.addAll(lines);
-        }
-      }
-    } catch (IOException e) {
-      throw new InitializationException("IOException thrown while loading synonym rules", e);
-    }
-    return wlist;
-  }
-
-  private SlowSynonymMap synMap;
-
-  static void parseRules(Iterable<String> rules, SlowSynonymMap map, String mappingSep,
-    String synSep, boolean expansion, TokenizerFactory tokFactory) {
-    int count=0;
-    for (String rule : rules) {
-      // To use regexes, we need an expression that specifies an odd number of chars.
-      // This can't really be done with string.split(), and since we need to
-      // do unescaping at some point anyway, we wouldn't be saving any effort
-      // by using regexes.
-
-      List<String> mapping = StrUtils.splitSmart(rule, mappingSep, false);
-
-      List<List<String>> source;
-      List<List<String>> target;
-
-      if (mapping.size() > 2) {
-        throw new InitializationException("Invalid Synonym Rule:" + rule);
-      } else if (mapping.size()==2) {
-        source = getSynList(mapping.get(0), synSep, tokFactory);
-        target = getSynList(mapping.get(1), synSep, tokFactory);
-      } else {
-        source = getSynList(mapping.get(0), synSep, tokFactory);
-        if (expansion) {
-          // expand to all arguments
-          target = source;
-        } else {
-          // reduce to first argument
-          target = new ArrayList<List<String>>(1);
-          target.add(source.get(0));
-        }
-      }
-
-      boolean includeOrig=false;
-      for (List<String> fromToks : source) {
-        count++;
-        for (List<String> toToks : target) {
-          map.add(fromToks,
-                  SlowSynonymMap.makeTokens(toToks),
-                  includeOrig,
-                  true
-          );
-        }
-      }
-    }
-  }
-
-  // a , b c , d e f => [[a],[b,c],[d,e,f]]
-  private static List<List<String>> getSynList(String str, String separator, TokenizerFactory tokFactory) {
-    List<String> strList = StrUtils.splitSmart(str, separator, false);
-    // now split on whitespace to get a list of token strings
-    List<List<String>> synList = new ArrayList<List<String>>();
-    for (String toks : strList) {
-      List<String> tokList = tokFactory == null ?
-        StrUtils.splitWS(toks, true) : splitByTokenizer(toks, tokFactory);
-      synList.add(tokList);
-    }
-    return synList;
-  }
-
-  private static List<String> splitByTokenizer(String source, TokenizerFactory tokFactory){
-    StringReader reader = new StringReader( source );
-    TokenStream ts = loadTokenizer(tokFactory, reader);
-    List<String> tokList = new ArrayList<String>();
-    try {
-      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);
-      while (ts.incrementToken()){
-        if( termAtt.length() > 0 )
-          tokList.add( termAtt.toString() );
-      }
-    } catch (IOException e) {
-      throw new InitializationException("IOException thrown while tokenizing source", e);
-    }
-    finally{
-      reader.close();
-    }
-    return tokList;
-  }
-
-  private TokenizerFactory loadTokenizerFactory(ResourceLoader loader, String cname) {
-    TokenizerFactory tokFactory = loader.newInstance(cname, TokenizerFactory.class);
-    tokFactory.setLuceneMatchVersion(luceneMatchVersion);
-    tokFactory.init( args );
-    if (tokFactory instanceof ResourceLoaderAware) {
-      ((ResourceLoaderAware) tokFactory).inform(loader);
-    }
-    return tokFactory;
-  }
-
-  private static TokenStream loadTokenizer(TokenizerFactory tokFactory, Reader reader){
-    return tokFactory.create( reader );
-  }
-
-  public SlowSynonymMap getSynonymMap() {
-    return synMap;
-  }
-
-  public SlowSynonymFilter create(TokenStream input) {
-    return new SlowSynonymFilter(input,synMap);
-  }
-}
diff --git a/solr/core/src/java/org/apache/solr/analysis/SlowSynonymMap.java b/solr/core/src/java/org/apache/solr/analysis/SlowSynonymMap.java
deleted file mode 100644
index 8082281..0000000
--- a/solr/core/src/java/org/apache/solr/analysis/SlowSynonymMap.java
+++ /dev/null
@@ -1,163 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.solr.analysis;
-
-import org.apache.lucene.analysis.Token;
-import org.apache.lucene.analysis.util.CharArrayMap;
-import org.apache.lucene.analysis.util.InitializationException;
-import org.apache.lucene.util.Version;
-
-import java.util.*;
-
-/** Mapping rules for use with {@link SlowSynonymFilter}
- * @deprecated (3.4) use {@link SynonymFilterFactory} instead. only for precise index backwards compatibility. this factory will be removed in Lucene 5.0
- */
-@Deprecated
-class SlowSynonymMap {
-  /** @lucene.internal */
-  public CharArrayMap<SlowSynonymMap> submap; // recursive: Map<String, SynonymMap>
-  /** @lucene.internal */
-  public Token[] synonyms;
-  int flags;
-
-  static final int INCLUDE_ORIG=0x01;
-  static final int IGNORE_CASE=0x02;
-
-  public SlowSynonymMap() {}
-  public SlowSynonymMap(boolean ignoreCase) {
-    if (ignoreCase) flags |= IGNORE_CASE;
-  }
-
-  public boolean includeOrig() { return (flags & INCLUDE_ORIG) != 0; }
-  public boolean ignoreCase() { return (flags & IGNORE_CASE) != 0; }
-
-  /**
-   * @param singleMatch  List<String>, the sequence of strings to match
-   * @param replacement  List<Token> the list of tokens to use on a match
-   * @param includeOrig  sets a flag on this mapping signaling the generation of matched tokens in addition to the replacement tokens
-   * @param mergeExisting merge the replacement tokens with any other mappings that exist
-   */
-  public void add(List<String> singleMatch, List<Token> replacement, boolean includeOrig, boolean mergeExisting) {
-    SlowSynonymMap currMap = this;
-    for (String str : singleMatch) {
-      if (currMap.submap==null) {
-        // for now hardcode at 4.0, as its what the old code did.
-        // would be nice to fix, but shouldn't store a version in each submap!!!
-        currMap.submap = new CharArrayMap<SlowSynonymMap>(Version.LUCENE_40, 1, ignoreCase());
-      }
-
-      SlowSynonymMap map = currMap.submap.get(str);
-      if (map==null) {
-        map = new SlowSynonymMap();
-        map.flags |= flags & IGNORE_CASE;
-        currMap.submap.put(str, map);
-      }
-
-      currMap = map;
-    }
-
-    if (currMap.synonyms != null && !mergeExisting) {
-      throw new InitializationException("SynonymFilter: there is already a mapping for " + singleMatch);
-    }
-    List<Token> superset = currMap.synonyms==null ? replacement :
-          mergeTokens(Arrays.asList(currMap.synonyms), replacement);
-    currMap.synonyms = superset.toArray(new Token[superset.size()]);
-    if (includeOrig) currMap.flags |= INCLUDE_ORIG;
-  }
-
-
-  @Override
-  public String toString() {
-    StringBuilder sb = new StringBuilder("<");
-    if (synonyms!=null) {
-      sb.append("[");
-      for (int i=0; i<synonyms.length; i++) {
-        if (i!=0) sb.append(',');
-        sb.append(synonyms[i]);
-      }
-      if ((flags & INCLUDE_ORIG)!=0) {
-        sb.append(",ORIG");
-      }
-      sb.append("],");
-    }
-    sb.append(submap);
-    sb.append(">");
-    return sb.toString();
-  }
-
-
-
-  /** Produces a List<Token> from a List<String> */
-  public static List<Token> makeTokens(List<String> strings) {
-    List<Token> ret = new ArrayList<Token>(strings.size());
-    for (String str : strings) {
-      //Token newTok = new Token(str,0,0,"SYNONYM");
-      Token newTok = new Token(str, 0,0,"SYNONYM");
-      ret.add(newTok);
-    }
-    return ret;
-  }
-
-
-  /**
-   * Merge two lists of tokens, producing a single list with manipulated positionIncrements so that
-   * the tokens end up at the same position.
-   *
-   * Example:  [a b] merged with [c d] produces [a/b c/d]  ('/' denotes tokens in the same position)
-   * Example:  [a,5 b,2] merged with [c d,4 e,4] produces [c a,5/d b,2 e,2]  (a,n means a has posInc=n)
-   *
-   */
-  public static List<Token> mergeTokens(List<Token> lst1, List<Token> lst2) {
-    ArrayList<Token> result = new ArrayList<Token>();
-    if (lst1 ==null || lst2 ==null) {
-      if (lst2 != null) result.addAll(lst2);
-      if (lst1 != null) result.addAll(lst1);
-      return result;
-    }
-
-    int pos=0;
-    Iterator<Token> iter1=lst1.iterator();
-    Iterator<Token> iter2=lst2.iterator();
-    Token tok1 = iter1.hasNext() ? iter1.next() : null;
-    Token tok2 = iter2.hasNext() ? iter2.next() : null;
-    int pos1 = tok1!=null ? tok1.getPositionIncrement() : 0;
-    int pos2 = tok2!=null ? tok2.getPositionIncrement() : 0;
-    while(tok1!=null || tok2!=null) {
-      while (tok1 != null && (pos1 <= pos2 || tok2==null)) {
-        Token tok = new Token(tok1.startOffset(), tok1.endOffset(), tok1.type());
-        tok.copyBuffer(tok1.buffer(), 0, tok1.length());
-        tok.setPositionIncrement(pos1-pos);
-        result.add(tok);
-        pos=pos1;
-        tok1 = iter1.hasNext() ? iter1.next() : null;
-        pos1 += tok1!=null ? tok1.getPositionIncrement() : 0;
-      }
-      while (tok2 != null && (pos2 <= pos1 || tok1==null)) {
-        Token tok = new Token(tok2.startOffset(), tok2.endOffset(), tok2.type());
-        tok.copyBuffer(tok2.buffer(), 0, tok2.length());
-        tok.setPositionIncrement(pos2-pos);
-        result.add(tok);
-        pos=pos2;
-        tok2 = iter2.hasNext() ? iter2.next() : null;
-        pos2 += tok2!=null ? tok2.getPositionIncrement() : 0;
-      }
-    }
-    return result;
-  }
-
-}
diff --git a/solr/core/src/java/org/apache/solr/analysis/SynonymFilterFactory.java b/solr/core/src/java/org/apache/solr/analysis/SynonymFilterFactory.java
index df0a93e..0234c1b 100644
--- a/solr/core/src/java/org/apache/solr/analysis/SynonymFilterFactory.java
+++ b/solr/core/src/java/org/apache/solr/analysis/SynonymFilterFactory.java
@@ -17,15 +17,30 @@ package org.apache.solr.analysis;
  * limitations under the License.
  */
 
-import java.util.Map;
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStreamReader;
+import java.io.Reader;
+import java.nio.charset.Charset;
+import java.nio.charset.CharsetDecoder;
+import java.nio.charset.CodingErrorAction;
+import java.text.ParseException;
+import java.util.List;
 
+import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.synonym.SynonymFilter;
-import org.apache.lucene.analysis.util.InitializationException;
+import org.apache.lucene.analysis.synonym.SynonymMap;
+import org.apache.lucene.analysis.synonym.SolrSynonymParser;
+import org.apache.lucene.analysis.synonym.WordnetSynonymParser;
+import org.apache.lucene.analysis.util.*;
 import org.apache.lucene.util.Version;
-import org.apache.lucene.analysis.util.ResourceLoader;
-import org.apache.lucene.analysis.util.ResourceLoaderAware;
-import org.apache.lucene.analysis.util.TokenFilterFactory;
+import org.apache.solr.common.util.StrUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 /**
  * Factory for {@link SynonymFilter}.
@@ -40,34 +55,120 @@ import org.apache.lucene.analysis.util.TokenFilterFactory;
  * &lt;/fieldType&gt;</pre>
  */
 public class SynonymFilterFactory extends TokenFilterFactory implements ResourceLoaderAware {
-  private TokenFilterFactory delegator;
 
-  @Override
-  public void init(Map<String,String> args) {
-    super.init(args);
-    assureMatchVersion();
-    if (luceneMatchVersion.onOrAfter(Version.LUCENE_34)) {
-      delegator = new FSTSynonymFilterFactory();
-    } else {
-      // check if you use the new optional arg "format". this makes no sense for the old one, 
-      // as its wired to solr's synonyms format only.
-      if (args.containsKey("format") && !args.get("format").equals("solr")) {
-        throw new InitializationException("You must specify luceneMatchVersion >= 3.4 to use alternate synonyms formats");
-      }
-      delegator = new SlowSynonymFilterFactory();
-    }
-    delegator.init(args);
-  }
+  public static final Logger log = LoggerFactory.getLogger(SynonymFilterFactory.class);
 
+  private SynonymMap map;
+  private boolean ignoreCase;
+  
   @Override
   public TokenStream create(TokenStream input) {
-    assert delegator != null : "init() was not called!";
-    return delegator.create(input);
+    // if the fst is null, it means there's actually no synonyms... just return the original stream
+    // as there is nothing to do here.
+    return map.fst == null ? input : new SynonymFilter(input, map, ignoreCase);
   }
 
   @Override
   public void inform(ResourceLoader loader) {
-    assert delegator != null : "init() was not called!";
-    ((ResourceLoaderAware) delegator).inform(loader);
+    final boolean ignoreCase = getBoolean("ignoreCase", false); 
+    this.ignoreCase = ignoreCase;
+
+    String tf = args.get("tokenizerFactory");
+
+    final TokenizerFactory factory = tf == null ? null : loadTokenizerFactory(loader, tf);
+    
+    Analyzer analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
+        Tokenizer tokenizer = factory == null ? new WhitespaceTokenizer(Version.LUCENE_50, reader) : factory.create(reader);
+        TokenStream stream = ignoreCase ? new LowerCaseFilter(Version.LUCENE_50, tokenizer) : tokenizer;
+        return new TokenStreamComponents(tokenizer, stream);
+      }
+    };
+
+    String format = args.get("format");
+    try {
+      if (format == null || format.equals("solr")) {
+        // TODO: expose dedup as a parameter?
+        map = loadSolrSynonyms(loader, true, analyzer);
+      } else if (format.equals("wordnet")) {
+        map = loadWordnetSynonyms(loader, true, analyzer);
+      } else {
+        // TODO: somehow make this more pluggable
+        throw new InitializationException("Unrecognized synonyms format: " + format);
+      }
+    } catch (Exception e) {
+      throw new InitializationException("Exception thrown while loading synonyms", e);
+    }
+    
+    if (map.fst == null) {
+      log.warn("Synonyms loaded with " + args + " has empty rule set!");
+    }
+  }
+  
+  /**
+   * Load synonyms from the solr format, "format=solr".
+   */
+  private SynonymMap loadSolrSynonyms(ResourceLoader loader, boolean dedup, Analyzer analyzer) throws IOException, ParseException {
+    final boolean expand = getBoolean("expand", true);
+    String synonyms = args.get("synonyms");
+    if (synonyms == null)
+      throw new InitializationException("Missing required argument 'synonyms'.");
+    
+    CharsetDecoder decoder = Charset.forName("UTF-8").newDecoder()
+      .onMalformedInput(CodingErrorAction.REPORT)
+      .onUnmappableCharacter(CodingErrorAction.REPORT);
+    
+    SolrSynonymParser parser = new SolrSynonymParser(dedup, expand, analyzer);
+    File synonymFile = new File(synonyms);
+    if (synonymFile.exists()) {
+      decoder.reset();
+      parser.add(new InputStreamReader(loader.openResource(synonyms), decoder));
+    } else {
+      List<String> files = StrUtils.splitFileNames(synonyms);
+      for (String file : files) {
+        decoder.reset();
+        parser.add(new InputStreamReader(loader.openResource(file), decoder));
+      }
+    }
+    return parser.build();
+  }
+  
+  /**
+   * Load synonyms from the wordnet format, "format=wordnet".
+   */
+  private SynonymMap loadWordnetSynonyms(ResourceLoader loader, boolean dedup, Analyzer analyzer) throws IOException, ParseException {
+    final boolean expand = getBoolean("expand", true);
+    String synonyms = args.get("synonyms");
+    if (synonyms == null)
+      throw new InitializationException("Missing required argument 'synonyms'.");
+    
+    CharsetDecoder decoder = Charset.forName("UTF-8").newDecoder()
+      .onMalformedInput(CodingErrorAction.REPORT)
+      .onUnmappableCharacter(CodingErrorAction.REPORT);
+    
+    WordnetSynonymParser parser = new WordnetSynonymParser(dedup, expand, analyzer);
+    File synonymFile = new File(synonyms);
+    if (synonymFile.exists()) {
+      decoder.reset();
+      parser.add(new InputStreamReader(loader.openResource(synonyms), decoder));
+    } else {
+      List<String> files = StrUtils.splitFileNames(synonyms);
+      for (String file : files) {
+        decoder.reset();
+        parser.add(new InputStreamReader(loader.openResource(file), decoder));
+      }
+    }
+    return parser.build();
+  }
+  
+  private TokenizerFactory loadTokenizerFactory(ResourceLoader loader, String cname){
+    TokenizerFactory tokFactory = loader.newInstance(cname, TokenizerFactory.class);
+    tokFactory.setLuceneMatchVersion(luceneMatchVersion);
+    tokFactory.init(args);
+    if (tokFactory instanceof ResourceLoaderAware) {
+      ((ResourceLoaderAware) tokFactory).inform(loader);
+    }
+    return tokFactory;
   }
 }
diff --git a/solr/core/src/java/org/apache/solr/spelling/FileBasedSpellChecker.java b/solr/core/src/java/org/apache/solr/spelling/FileBasedSpellChecker.java
index 45702e7..31beae1 100644
--- a/solr/core/src/java/org/apache/solr/spelling/FileBasedSpellChecker.java
+++ b/solr/core/src/java/org/apache/solr/spelling/FileBasedSpellChecker.java
@@ -111,7 +111,7 @@ public class FileBasedSpellChecker extends AbstractLuceneSpellChecker {
         writer.forceMerge(1);
         writer.close();
 
-        dictionary = new HighFrequencyDictionary(IndexReader.open(ramDir),
+        dictionary = new HighFrequencyDictionary(DirectoryReader.open(ramDir),
                 WORD_FIELD_NAME, 0.0f);
       } else {
         // check if character encoding is defined
diff --git a/solr/core/src/java/org/apache/solr/spelling/IndexBasedSpellChecker.java b/solr/core/src/java/org/apache/solr/spelling/IndexBasedSpellChecker.java
index d482e2e..39d00a5 100644
--- a/solr/core/src/java/org/apache/solr/spelling/IndexBasedSpellChecker.java
+++ b/solr/core/src/java/org/apache/solr/spelling/IndexBasedSpellChecker.java
@@ -16,6 +16,7 @@ package org.apache.solr.spelling;
  * limitations under the License.
  */
 
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.store.FSDirectory;
@@ -64,7 +65,7 @@ public class IndexBasedSpellChecker extends AbstractLuceneSpellChecker {
     if (sourceLocation != null) {
       try {
         FSDirectory luceneIndexDir = FSDirectory.open(new File(sourceLocation));
-        this.reader = IndexReader.open(luceneIndexDir);
+        this.reader = DirectoryReader.open(luceneIndexDir);
       } catch (IOException e) {
         throw new RuntimeException(e);
       }
diff --git a/solr/core/src/java/org/apache/solr/update/SolrIndexConfig.java b/solr/core/src/java/org/apache/solr/update/SolrIndexConfig.java
index 4f79f78..168e4fd 100644
--- a/solr/core/src/java/org/apache/solr/update/SolrIndexConfig.java
+++ b/solr/core/src/java/org/apache/solr/update/SolrIndexConfig.java
@@ -68,19 +68,17 @@ public class SolrIndexConfig {
   @SuppressWarnings("deprecation")
   private SolrIndexConfig(SolrConfig solrConfig) {
     luceneVersion = solrConfig.luceneMatchVersion;
-    useCompoundFile = luceneVersion.onOrAfter(Version.LUCENE_36) ? false : true;
+    useCompoundFile = false;
     maxBufferedDocs = -1;
     maxMergeDocs = -1;
     mergeFactor = -1;
-    ramBufferSizeMB = luceneVersion.onOrAfter(Version.LUCENE_36) ? 32 : 16;
+    ramBufferSizeMB = 32;
     writeLockTimeout = -1;
-    lockType = luceneVersion.onOrAfter(Version.LUCENE_36) ? 
-               LOCK_TYPE_NATIVE : 
-               LOCK_TYPE_SIMPLE;
+    lockType = LOCK_TYPE_NATIVE;
     termIndexInterval = IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL;
     mergePolicyInfo = null;
     mergeSchedulerInfo = null;
-    defaultMergePolicyClassName = luceneVersion.onOrAfter(Version.LUCENE_33) ? TieredMergePolicy.class.getName() : LogByteSizeMergePolicy.class.getName();
+    defaultMergePolicyClassName = TieredMergePolicy.class.getName();
   }
   
   /**
@@ -106,13 +104,13 @@ public class SolrIndexConfig {
     // Warn for luceneMatchVersion's before LUCENE_36, fail fast above
     assertWarnOrFail("The <mergeScheduler>myclass</mergeScheduler> syntax is no longer supported in solrconfig.xml. Please use syntax <mergeScheduler class=\"myclass\"/> instead.",
         !((solrConfig.get(prefix+"/mergeScheduler/text()",null) != null) && (solrConfig.get(prefix+"/mergeScheduler/@class",null) == null)),
-        luceneVersion.onOrAfter(Version.LUCENE_36));
+        true);
     assertWarnOrFail("The <mergePolicy>myclass</mergePolicy> syntax is no longer supported in solrconfig.xml. Please use syntax <mergePolicy class=\"myclass\"/> instead.",
         !((solrConfig.get(prefix+"/mergePolicy/text()",null) != null) && (solrConfig.get(prefix+"/mergePolicy/@class",null) == null)),
-        luceneVersion.onOrAfter(Version.LUCENE_36));
+        true);
     assertWarnOrFail("The <luceneAutoCommit>true|false</luceneAutoCommit> parameter is no longer valid in solrconfig.xml.",
         solrConfig.get(prefix+"/luceneAutoCommit", null) == null,
-        luceneVersion.onOrAfter(Version.LUCENE_36));
+        true);
 
     defaultMergePolicyClassName = def.defaultMergePolicyClassName;
     useCompoundFile=solrConfig.getBool(prefix+"/useCompoundFile", def.useCompoundFile);
diff --git a/solr/core/src/test-files/solr/conf/schema-collatefilter.xml b/solr/core/src/test-files/solr/conf/schema-collatefilter.xml
deleted file mode 100644
index 19e3eb3..0000000
--- a/solr/core/src/test-files/solr/conf/schema-collatefilter.xml
+++ /dev/null
@@ -1,61 +0,0 @@
-<?xml version="1.0" ?>
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-
-<!-- Test schema file for CollationKeyFilter (deprecated: use CollationField instead) -->
-
-<schema name="test" version="1.0">
-  <types>
-    <fieldType name="int" class="solr.TrieIntField" precisionStep="0" omitNorms="true" positionIncrementGap="0"/>
-
-    <!-- basic text field -->
-    <fieldtype name="text" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.StandardTokenizerFactory"/>
-        <filter class="solr.LowerCaseFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-    
-    <fieldtype name="sort_ar_t" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.KeywordTokenizerFactory"/>
-        <filter class="solr.CollationKeyFilterFactory" language="ar"/>
-      </analyzer>
-    </fieldtype>
-    
-    <fieldtype name="sort_de_t" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.KeywordTokenizerFactory"/>
-        <filter class="solr.CollationKeyFilterFactory" language="de" strength="primary"/>
-      </analyzer>
-    </fieldtype>
-  </types>
-
-  <fields>
-    <field name="id" type="int" indexed="true" stored="true" multiValued="false" required="false"/>
-    <field name="text" type="text" indexed="true" stored="false"/>
-    <field name="sort_ar"       type="sort_ar_t"       indexed="true" stored="false" multiValued="false"/>
-    <field name="sort_de"       type="sort_de_t"       indexed="true" stored="false" multiValued="false"/>
-  </fields>
-
-  <defaultSearchField>text</defaultSearchField>
-  <uniqueKey>id</uniqueKey>
-
-  <!-- copy our text to some sort fields with different orders -->
-  <copyField source="text" dest="sort_ar"/>
-  <copyField source="text" dest="sort_de"/>
-</schema>
diff --git a/solr/core/src/test-files/solr/conf/schema-luceneMatchVersion.xml b/solr/core/src/test-files/solr/conf/schema-luceneMatchVersion.xml
index 7c439d7..13e12f3 100644
--- a/solr/core/src/test-files/solr/conf/schema-luceneMatchVersion.xml
+++ b/solr/core/src/test-files/solr/conf/schema-luceneMatchVersion.xml
@@ -18,12 +18,12 @@
 <schema name="luceneMatchVersionTest" version="1.1">
  <types>
   <fieldtype name="string" class="solr.StrField"/>
-  <fieldtype name="text30" class="solr.TextField">
+  <fieldtype name="text40" class="solr.TextField">
     <analyzer>
-      <tokenizer class="solr.StandardTokenizerFactory" luceneMatchVersion="LUCENE_30"/>
+      <tokenizer class="solr.StandardTokenizerFactory" luceneMatchVersion="LUCENE_40"/>
       <filter class="solr.StandardFilterFactory"/>
       <filter class="solr.LowerCaseFilterFactory"/>
-      <filter class="solr.StopFilterFactory" luceneMatchVersion="3.1"/>
+      <filter class="solr.StopFilterFactory" luceneMatchVersion="5.0"/>
       <filter class="solr.PorterStemFilterFactory"/>
     </analyzer>
   </fieldtype>
@@ -36,8 +36,8 @@
       <filter class="solr.PorterStemFilterFactory"/>
     </analyzer>
   </fieldtype>
-  <fieldtype name="textStandardAnalyzer30" class="solr.TextField">
-    <analyzer class="org.apache.lucene.analysis.standard.StandardAnalyzer" luceneMatchVersion="LUCENE_30"/>
+  <fieldtype name="textStandardAnalyzer40" class="solr.TextField">
+    <analyzer class="org.apache.lucene.analysis.standard.StandardAnalyzer" luceneMatchVersion="LUCENE_40"/>
   </fieldtype>
   <fieldtype name="textStandardAnalyzerDefault" class="solr.TextField">
     <analyzer class="org.apache.lucene.analysis.standard.StandardAnalyzer"/>
@@ -46,9 +46,9 @@
  <fields>
    <field name="id" type="string" indexed="true" stored="true"/>
    <field name="signatureField" type="string" indexed="true" stored="false"/>
-   <field name="text30" type="text30" indexed="true" stored="false" />
+   <field name="text40" type="text40" indexed="true" stored="false" />
    <field name="textDefault" type="textDefault" indexed="true" stored="false" />
-   <field name="textStandardAnalyzer30" type="textStandardAnalyzer30" indexed="true" stored="false" />
+   <field name="textStandardAnalyzer40" type="textStandardAnalyzer40" indexed="true" stored="false" />
    <field name="textStandardAnalyzerDefault" type="textStandardAnalyzerDefault" indexed="true" stored="false" />
    <dynamicField name="*_sS" type="string"  indexed="false" stored="true"/>
  </fields>
diff --git a/solr/core/src/test-files/solr/conf/solrconfig-basic-luceneVersion31.xml b/solr/core/src/test-files/solr/conf/solrconfig-basic-luceneVersion31.xml
deleted file mode 100644
index 50ac5f1..0000000
--- a/solr/core/src/test-files/solr/conf/solrconfig-basic-luceneVersion31.xml
+++ /dev/null
@@ -1,26 +0,0 @@
-<?xml version="1.0" ?>
-
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-
-<!-- a basic solrconfig that tests can use when they want simple minimal solrconfig/schema
-     DO NOT ADD THINGS TO THIS CONFIG! -->
-<config>
-  <luceneMatchVersion>LUCENE_31</luceneMatchVersion>
-  <directoryFactory name="DirectoryFactory" class="${solr.directoryFactory:solr.RAMDirectoryFactory}"/>
-  <requestHandler name="standard" class="solr.StandardRequestHandler"></requestHandler>
-</config>
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestArabicFilters.java b/solr/core/src/test/org/apache/solr/analysis/TestArabicFilters.java
index c0e45ec..23e6ae1 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestArabicFilters.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestArabicFilters.java
@@ -31,20 +31,6 @@ import org.apache.lucene.analysis.Tokenizer;
  * Simple tests to ensure the Arabic filter Factories are working.
  */
 public class TestArabicFilters extends BaseTokenStreamTestCase {
-  /**
-   * Test ArabicLetterTokenizerFactory
-   * @deprecated (3.1) Remove in Lucene 5.0
-   */
-  @Deprecated
-  public void testTokenizer() throws Exception {
-    Reader reader = new StringReader("Ø§?Ø°?? ????Øª Ø£??Ø§???");
-    ArabicLetterTokenizerFactory factory = new ArabicLetterTokenizerFactory();
-    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
-    Map<String, String> args = Collections.emptyMap();
-    factory.init(args);
-    Tokenizer stream = factory.create(reader);
-    assertTokenStreamContents(stream, new String[] {"Ø§?Ø°??", "????Øª", "Ø£??Ø§???"});
-  }
   
   /**
    * Test ArabicNormalizationFilterFactory
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestCJKTokenizerFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestCJKTokenizerFactory.java
deleted file mode 100644
index 9369b97..0000000
--- a/solr/core/src/test/org/apache/solr/analysis/TestCJKTokenizerFactory.java
+++ /dev/null
@@ -1,41 +0,0 @@
-package org.apache.solr.analysis;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Reader;
-import java.io.StringReader;
-
-import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.TokenStream;
-
-/**
- * Simple tests to ensure the CJK tokenizer factory is working.
- * @deprecated
- */
-@Deprecated
-public class TestCJKTokenizerFactory extends BaseTokenStreamTestCase {
-  /**
-   * Ensure the tokenizer actually tokenizes CJK text correctly
-   */
-  public void testTokenizer() throws Exception {
-    Reader reader = new StringReader("???ä¸??äº?");
-    CJKTokenizerFactory factory = new CJKTokenizerFactory();
-    TokenStream stream = factory.create(reader);
-    assertTokenStreamContents(stream, new String[] {"???", "??¸­", "ä¸??", "?½äºº"});
-  }
-}
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestChineseFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestChineseFilterFactory.java
deleted file mode 100644
index fd5b227..0000000
--- a/solr/core/src/test/org/apache/solr/analysis/TestChineseFilterFactory.java
+++ /dev/null
@@ -1,42 +0,0 @@
-package org.apache.solr.analysis;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Reader;
-import java.io.StringReader;
-
-import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.Tokenizer;
-
-/**
- * Simple tests to ensure the Chinese filter factory is working.
- */
-public class TestChineseFilterFactory extends BaseTokenStreamTestCase {
-  /**
-   * Ensure the filter actually normalizes text (numerics, stopwords)
-   */
-  public void testFiltering() throws Exception {
-    Reader reader = new StringReader("this 1234 Is such a silly filter");
-    Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);
-    ChineseFilterFactory factory = new ChineseFilterFactory();
-    TokenStream stream = factory.create(tokenizer);
-    assertTokenStreamContents(stream, new String[] { "Is", "silly", "filter" });
-  }
-}
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestChineseTokenizerFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestChineseTokenizerFactory.java
deleted file mode 100644
index fce6114..0000000
--- a/solr/core/src/test/org/apache/solr/analysis/TestChineseTokenizerFactory.java
+++ /dev/null
@@ -1,39 +0,0 @@
-package org.apache.solr.analysis;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Reader;
-import java.io.StringReader;
-
-import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.TokenStream;
-
-/**
- * Simple tests to ensure the Chinese tokenizer factory is working.
- */
-public class TestChineseTokenizerFactory extends BaseTokenStreamTestCase {
-  /**
-   * Ensure the tokenizer actually tokenizes chinese text correctly
-   */
-  public void testTokenizer() throws Exception {
-    Reader reader = new StringReader("???ä¸??äº?");
-    ChineseTokenizerFactory factory = new ChineseTokenizerFactory();
-    TokenStream stream = factory.create(reader);
-    assertTokenStreamContents(stream, new String[] {"??", "??", "ä¸?", "??", "äº?"});
-  }
-}
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestCollationKeyFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestCollationKeyFilterFactory.java
deleted file mode 100644
index 494c9c7..0000000
--- a/solr/core/src/test/org/apache/solr/analysis/TestCollationKeyFilterFactory.java
+++ /dev/null
@@ -1,197 +0,0 @@
-package org.apache.solr.analysis;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.ByteArrayInputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.StringReader;
-import java.text.Collator;
-import java.text.RuleBasedCollator;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-
-import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.util.ResourceLoader;
-
-public class TestCollationKeyFilterFactory extends BaseTokenStreamTestCase {
-
-  /*
-   * Turkish has some funny casing.
-   * This test shows how you can solve this kind of thing easily with collation.
-   * Instead of using LowerCaseFilter, use a turkish collator with primary strength.
-   * Then things will sort and match correctly.
-   */
-  public void testBasicUsage() throws IOException {
-    String turkishUpperCase = "I WÄ°LL USE TURKÄ°SH CASING";
-    String turkishLowerCase = "Ä± will use turkish casÄ±ng";
-    CollationKeyFilterFactory factory = new CollationKeyFilterFactory();
-    Map<String,String> args = new HashMap<String,String>();
-    args.put("language", "tr");
-    args.put("strength", "primary");
-    factory.init(args);
-    factory.inform(new StringMockSolrResourceLoader(""));
-    TokenStream tsUpper = factory.create(
-        new MockTokenizer(new StringReader(turkishUpperCase), MockTokenizer.KEYWORD, false));
-    TokenStream tsLower = factory.create(
-        new MockTokenizer(new StringReader(turkishLowerCase), MockTokenizer.KEYWORD, false));
-    assertCollatesToSame(tsUpper, tsLower);
-  }
-  
-  /*
-   * Test usage of the decomposition option for unicode normalization.
-   */
-  public void testNormalization() throws IOException {
-    String turkishUpperCase = "I W\u0049\u0307LL USE TURKÄ°SH CASING";
-    String turkishLowerCase = "Ä± will use turkish casÄ±ng";
-    CollationKeyFilterFactory factory = new CollationKeyFilterFactory();
-    Map<String,String> args = new HashMap<String,String>();
-    args.put("language", "tr");
-    args.put("strength", "primary");
-    args.put("decomposition", "canonical");
-    factory.init(args);
-    factory.inform(new StringMockSolrResourceLoader(""));
-    TokenStream tsUpper = factory.create(
-        new MockTokenizer(new StringReader(turkishUpperCase), MockTokenizer.KEYWORD, false));
-    TokenStream tsLower = factory.create(
-        new MockTokenizer(new StringReader(turkishLowerCase), MockTokenizer.KEYWORD, false));
-    assertCollatesToSame(tsUpper, tsLower);
-  }
-  
-  /*
-   * Test usage of the K decomposition option for unicode normalization.
-   * This works even with identical strength.
-   */
-  public void testFullDecomposition() throws IOException {
-    String fullWidth = "ï¼´ï?ï½??ï½??ï½?";
-    String halfWidth = "Testing";
-    CollationKeyFilterFactory factory = new CollationKeyFilterFactory();
-    Map<String,String> args = new HashMap<String,String>();
-    args.put("language", "zh");
-    args.put("strength", "identical");
-    args.put("decomposition", "full");
-    factory.init(args);
-    factory.inform(new StringMockSolrResourceLoader(""));
-    TokenStream tsFull = factory.create(
-        new MockTokenizer(new StringReader(fullWidth), MockTokenizer.KEYWORD, false));
-    TokenStream tsHalf = factory.create(
-        new MockTokenizer(new StringReader(halfWidth), MockTokenizer.KEYWORD, false));
-    assertCollatesToSame(tsFull, tsHalf);
-  }
-  
-  /*
-   * Test secondary strength, for english case is not significant.
-   */
-  public void testSecondaryStrength() throws IOException {
-    String upperCase = "TESTING";
-    String lowerCase = "testing";
-    CollationKeyFilterFactory factory = new CollationKeyFilterFactory();
-    Map<String,String> args = new HashMap<String,String>();
-    args.put("language", "en");
-    args.put("strength", "secondary");
-    args.put("decomposition", "no");
-    factory.init(args);
-    factory.inform(new StringMockSolrResourceLoader(""));
-    TokenStream tsUpper = factory.create(
-        new MockTokenizer(new StringReader(upperCase), MockTokenizer.KEYWORD, false));
-    TokenStream tsLower = factory.create(
-        new MockTokenizer(new StringReader(lowerCase), MockTokenizer.KEYWORD, false));
-    assertCollatesToSame(tsUpper, tsLower);
-  }
-
-  /*
-   * For german, you might want oe to sort and match with o umlaut.
-   * This is not the default, but you can make a customized ruleset to do this.
-   *
-   * The default is DIN 5007-1, this shows how to tailor a collator to get DIN 5007-2 behavior.
-   *  http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4423383
-   */
-  public void testCustomRules() throws Exception {
-    RuleBasedCollator baseCollator = (RuleBasedCollator) Collator.getInstance(new Locale("de", "DE"));
-
-    String DIN5007_2_tailorings =
-      "& ae , a\u0308 & AE , A\u0308"+
-      "& oe , o\u0308 & OE , O\u0308"+
-      "& ue , u\u0308 & UE , u\u0308";
-
-    RuleBasedCollator tailoredCollator = new RuleBasedCollator(baseCollator.getRules() + DIN5007_2_tailorings);
-    String tailoredRules = tailoredCollator.getRules();
-    //
-    // at this point, you would save these tailoredRules to a file, 
-    // and use the custom parameter.
-    //
-    String germanUmlaut = "TÃ¶ne";
-    String germanOE = "Toene";
-    CollationKeyFilterFactory factory = new CollationKeyFilterFactory();
-    Map<String,String> args = new HashMap<String,String>();
-    args.put("custom", "rules.txt");
-    args.put("strength", "primary");
-    factory.init(args);
-    factory.inform(new StringMockSolrResourceLoader(tailoredRules));
-    TokenStream tsUmlaut = factory.create(
-        new MockTokenizer(new StringReader(germanUmlaut), MockTokenizer.KEYWORD, false));
-    TokenStream tsOE = factory.create(
-        new MockTokenizer(new StringReader(germanOE), MockTokenizer.KEYWORD, false));
-
-    assertCollatesToSame(tsUmlaut, tsOE);
-  }
-  
-  private class StringMockSolrResourceLoader implements ResourceLoader {
-    String text;
-
-    StringMockSolrResourceLoader(String text) {
-      this.text = text;
-    }
-
-    public List<String> getLines(String resource) throws IOException {
-      return null;
-    }
-
-    public <T> T newInstance(String cname, Class<T> expectedType, String... subpackages) {
-      return null;
-    }
-
-    public InputStream openResource(String resource) throws IOException {
-      return new ByteArrayInputStream(text.getBytes("UTF-8"));
-    }
-  }
-  
-  private void assertCollatesToSame(TokenStream stream1, TokenStream stream2)
-      throws IOException {
-    stream1.reset();
-    stream2.reset();
-    CharTermAttribute term1 = stream1
-        .addAttribute(CharTermAttribute.class);
-    CharTermAttribute term2 = stream2
-        .addAttribute(CharTermAttribute.class);
-    assertTrue(stream1.incrementToken());
-    assertTrue(stream2.incrementToken());
-    assertEquals(term1.toString(), term2.toString());
-    assertFalse(stream1.incrementToken());
-    assertFalse(stream2.incrementToken());
-    stream1.end();
-    stream2.end();
-    stream1.close();
-    stream2.close();
-  }
-}
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestCollationKeyRangeQueries.java b/solr/core/src/test/org/apache/solr/analysis/TestCollationKeyRangeQueries.java
deleted file mode 100644
index 09d0aa4..0000000
--- a/solr/core/src/test/org/apache/solr/analysis/TestCollationKeyRangeQueries.java
+++ /dev/null
@@ -1,84 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.solr.analysis;
-
-import org.apache.solr.SolrTestCaseJ4;
-import org.junit.BeforeClass;
-
-/**
- * Tests {@link CollationKeyFilterFactory} with RangeQueries
- */
-public class TestCollationKeyRangeQueries extends SolrTestCaseJ4 {
-  
-  @BeforeClass
-  public static void beforeClass() throws Exception {
-    initCore("solrconfig-basic.xml","schema-collatefilter.xml");
-    // add some docs
-    assertU(adoc("id", "1", "text", "\u0633\u0627\u0628"));
-    assertU(adoc("id", "2", "text", "I WÄ°LL USE TURKÄ°SH CASING"));
-    assertU(adoc("id", "3", "text", "Ä± will use turkish casÄ±ng"));
-    assertU(adoc("id", "4", "text", "TÃ¶ne"));
-    assertU(adoc("id", "5", "text", "I W\u0049\u0307LL USE TURKÄ°SH CASING"));
-    assertU(adoc("id", "6", "text", "ï¼´ï?ï½??ï½??ï½?"));
-    assertU(adoc("id", "7", "text", "Tone"));
-    assertU(adoc("id", "8", "text", "Testing"));
-    assertU(adoc("id", "9", "text", "testing"));
-    assertU(adoc("id", "10", "text", "toene"));
-    assertU(adoc("id", "11", "text", "Tzne"));
-    assertU(adoc("id", "12", "text", "\u0698\u0698"));
-    assertU(commit());
-  }
-  
-  /** 
-   * Test termquery with german DIN 5007-1 primary strength.
-   * In this case, Ã¶ is equivalent to o (but not oe) 
-   */
-  public void testBasicTermQuery() {
-    assertQ("Collated TQ: ",
-       req("fl", "id", "q", "sort_de:tone", "sort", "id asc" ),
-              "//*[@numFound='2']",
-              "//result/doc[1]/int[@name='id'][.=4]",
-              "//result/doc[2]/int[@name='id'][.=7]"
-    );
-  }
-  
-  /** 
-   * Test rangequery again with the DIN 5007-1 collator.
-   * We do a range query of tone .. tp, in binary order this
-   * would retrieve nothing due to case and accent differences.
-   */
-  public void testBasicRangeQuery() {
-    assertQ("Collated RangeQ: ",
-        req("fl", "id", "q", "sort_de:[tone TO tp]", "sort", "id asc" ),
-               "//*[@numFound='2']",
-               "//result/doc[1]/int[@name='id'][.=4]",
-               "//result/doc[2]/int[@name='id'][.=7]"
-     );
-  }
-
-  /** 
-   * Test rangequery again with an Arabic collator.
-   * Binary order would normally order U+0633 in this range.
-   */
-  public void testNegativeRangeQuery() {
-    assertQ("Collated RangeQ: ",
-        req("fl", "id", "q", "sort_ar:[\u062F TO \u0698]", "sort", "id asc" ),
-               "//*[@numFound='0']"
-     );
-  }
-}
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestLuceneMatchVersion.java b/solr/core/src/test/org/apache/solr/analysis/TestLuceneMatchVersion.java
index 0521f7f..d135374 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestLuceneMatchVersion.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestLuceneMatchVersion.java
@@ -52,10 +52,10 @@ public class TestLuceneMatchVersion extends SolrTestCaseJ4 {
     assertEquals(DEFAULT_VERSION, (ana.getTokenizerFactory()).getLuceneMatchVersion());
     assertEquals(DEFAULT_VERSION, (ana.getTokenFilterFactories()[2]).getLuceneMatchVersion());
 
-    type = schema.getFieldType("text30");
+    type = schema.getFieldType("text40");
     ana = (TokenizerChain) type.getAnalyzer();
-    assertEquals(Version.LUCENE_30, (ana.getTokenizerFactory()).getLuceneMatchVersion());
-    assertEquals(Version.LUCENE_31, (ana.getTokenFilterFactories()[2]).getLuceneMatchVersion());
+    assertEquals(Version.LUCENE_40, (ana.getTokenizerFactory()).getLuceneMatchVersion());
+    assertEquals(Version.LUCENE_50, (ana.getTokenFilterFactories()[2]).getLuceneMatchVersion());
 
     // this is a hack to get the private matchVersion field in StandardAnalyzer's superclass, may break in later lucene versions - we have no getter :(
     final Field matchVersionField = StandardAnalyzer.class.getSuperclass().getDeclaredField("matchVersion");
@@ -66,9 +66,9 @@ public class TestLuceneMatchVersion extends SolrTestCaseJ4 {
     assertTrue(ana1 instanceof StandardAnalyzer);
     assertEquals(DEFAULT_VERSION, matchVersionField.get(ana1));
 
-    type = schema.getFieldType("textStandardAnalyzer30");
+    type = schema.getFieldType("textStandardAnalyzer40");
     ana1 = type.getAnalyzer();
     assertTrue(ana1 instanceof StandardAnalyzer);
-    assertEquals(Version.LUCENE_30, matchVersionField.get(ana1));
+    assertEquals(Version.LUCENE_40, matchVersionField.get(ana1));
   }
 }
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestMultiWordSynonyms.java b/solr/core/src/test/org/apache/solr/analysis/TestMultiWordSynonyms.java
index dcbb160..6612689 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestMultiWordSynonyms.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestMultiWordSynonyms.java
@@ -26,7 +26,6 @@ import java.io.ByteArrayInputStream;
 import java.io.IOException;
 import java.io.InputStream;
 import java.io.StringReader;
-import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -35,21 +34,6 @@ import java.util.Map;
  * @since solr 1.4
  */
 public class TestMultiWordSynonyms extends BaseTokenStreamTestCase {
-
-  /**
-   * @deprecated Remove this test in 5.0
-   */
-  @Deprecated
-  public void testMultiWordSynonymsOld() throws IOException {
-    List<String> rules = new ArrayList<String>();
-    rules.add("a b c,d");
-    SlowSynonymMap synMap = new SlowSynonymMap(true);
-    SlowSynonymFilterFactory.parseRules(rules, synMap, "=>", ",", true, null);
-
-    SlowSynonymFilter ts = new SlowSynonymFilter(new MockTokenizer(new StringReader("a e"), MockTokenizer.WHITESPACE, false), synMap);
-    // This fails because ["e","e"] is the value of the token stream
-    assertTokenStreamContents(ts, new String[] { "a", "e" });
-  }
   
   public void testMultiWordSynonyms() throws IOException {
     SynonymFilterFactory factory = new SynonymFilterFactory();
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestRussianFilters.java b/solr/core/src/test/org/apache/solr/analysis/TestRussianFilters.java
deleted file mode 100644
index 20be1e5..0000000
--- a/solr/core/src/test/org/apache/solr/analysis/TestRussianFilters.java
+++ /dev/null
@@ -1,45 +0,0 @@
-package org.apache.solr.analysis;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Reader;
-import java.io.StringReader;
-import java.util.Collections;
-import java.util.Map;
-
-import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.Tokenizer;
-
-/**
- * Simple tests to ensure the Russian filter factories are working.
- */
-public class TestRussianFilters extends BaseTokenStreamTestCase {
-  /**
-   * Test RussianLetterTokenizerFactory
-   */
-  public void testTokenizer() throws Exception {
-    Reader reader = new StringReader("?Ð¼Ðµ??Ðµ ? ?ÐµÐ¼ Ð¾ ?Ð¸Ð»Ðµ ?Ð»ÐµÐº??Ð¾Ð¼Ð°Ð³Ð½Ð¸?Ð½Ð¾Ð¹ 100");
-    RussianLetterTokenizerFactory factory = new RussianLetterTokenizerFactory();
-    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
-    Map<String, String> args = Collections.emptyMap();
-    factory.init(args);
-    Tokenizer stream = factory.create(reader);
-    assertTokenStreamContents(stream, new String[] {"?Ð¼Ðµ??Ðµ", "?", "?ÐµÐ¼", "Ð¾",
-        "?Ð¸Ð»Ðµ", "?Ð»ÐµÐº??Ð¾Ð¼Ð°Ð³Ð½Ð¸?Ð½Ð¾Ð¹", "100"});
-  }
-}
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestSlowSynonymFilter.java b/solr/core/src/test/org/apache/solr/analysis/TestSlowSynonymFilter.java
deleted file mode 100644
index 22da7fa..0000000
--- a/solr/core/src/test/org/apache/solr/analysis/TestSlowSynonymFilter.java
+++ /dev/null
@@ -1,416 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.solr.analysis;
-
-import java.io.IOException;
-import java.io.StringReader;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collection;
-import java.util.List;
-
-import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.analysis.Token;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.tokenattributes.*;
-
-/**
- * @deprecated Remove this test in Lucene 5.0
- */
-@Deprecated
-public class TestSlowSynonymFilter extends BaseTokenStreamTestCase {
-
-  static List<String> strings(String str) {
-    String[] arr = str.split(" ");
-    return Arrays.asList(arr);
-  }
-
-  static void assertTokenizesTo(SlowSynonymMap dict, String input,
-      String expected[]) throws IOException {
-    Tokenizer tokenizer = new MockTokenizer(new StringReader(input), MockTokenizer.WHITESPACE, false);
-    SlowSynonymFilter stream = new SlowSynonymFilter(tokenizer, dict);
-    assertTokenStreamContents(stream, expected);
-  }
-  
-  static void assertTokenizesTo(SlowSynonymMap dict, String input,
-      String expected[], int posIncs[]) throws IOException {
-    Tokenizer tokenizer = new MockTokenizer(new StringReader(input), MockTokenizer.WHITESPACE, false);
-    SlowSynonymFilter stream = new SlowSynonymFilter(tokenizer, dict);
-    assertTokenStreamContents(stream, expected, posIncs);
-  }
-  
-  static void assertTokenizesTo(SlowSynonymMap dict, List<Token> input,
-      String expected[], int posIncs[])
-      throws IOException {
-    TokenStream tokenizer = new IterTokenStream(input);
-    SlowSynonymFilter stream = new SlowSynonymFilter(tokenizer, dict);
-    assertTokenStreamContents(stream, expected, posIncs);
-  }
-  
-  static void assertTokenizesTo(SlowSynonymMap dict, List<Token> input,
-      String expected[], int startOffsets[], int endOffsets[], int posIncs[])
-      throws IOException {
-    TokenStream tokenizer = new IterTokenStream(input);
-    SlowSynonymFilter stream = new SlowSynonymFilter(tokenizer, dict);
-    assertTokenStreamContents(stream, expected, startOffsets, endOffsets,
-        posIncs);
-  }
-  
-  public void testMatching() throws IOException {
-    SlowSynonymMap map = new SlowSynonymMap();
-
-    boolean orig = false;
-    boolean merge = true;
-    map.add(strings("a b"), tokens("ab"), orig, merge);
-    map.add(strings("a c"), tokens("ac"), orig, merge);
-    map.add(strings("a"), tokens("aa"), orig, merge);
-    map.add(strings("b"), tokens("bb"), orig, merge);
-    map.add(strings("z x c v"), tokens("zxcv"), orig, merge);
-    map.add(strings("x c"), tokens("xc"), orig, merge);
-
-    assertTokenizesTo(map, "$", new String[] { "$" });
-    assertTokenizesTo(map, "a", new String[] { "aa" });
-    assertTokenizesTo(map, "a $", new String[] { "aa", "$" });
-    assertTokenizesTo(map, "$ a", new String[] { "$", "aa" });
-    assertTokenizesTo(map, "a a", new String[] { "aa", "aa" });
-    assertTokenizesTo(map, "b", new String[] { "bb" });
-    assertTokenizesTo(map, "z x c v", new String[] { "zxcv" });
-    assertTokenizesTo(map, "z x c $", new String[] { "z", "xc", "$" });
-
-    // repeats
-    map.add(strings("a b"), tokens("ab"), orig, merge);
-    map.add(strings("a b"), tokens("ab"), orig, merge);
-    
-    // FIXME: the below test intended to be { "ab" }
-    assertTokenizesTo(map, "a b", new String[] { "ab", "ab", "ab"  });
-
-    // check for lack of recursion
-    map.add(strings("zoo"), tokens("zoo"), orig, merge);
-    assertTokenizesTo(map, "zoo zoo $ zoo", new String[] { "zoo", "zoo", "$", "zoo" });
-    map.add(strings("zoo"), tokens("zoo zoo"), orig, merge);
-    // FIXME: the below test intended to be { "zoo", "zoo", "zoo", "zoo", "$", "zoo", "zoo" }
-    // maybe this was just a typo in the old test????
-    assertTokenizesTo(map, "zoo zoo $ zoo", new String[] { "zoo", "zoo", "zoo", "zoo", "zoo", "zoo", "$", "zoo", "zoo", "zoo" });
-  }
-
-  public void testIncludeOrig() throws IOException {
-    SlowSynonymMap map = new SlowSynonymMap();
-
-    boolean orig = true;
-    boolean merge = true;
-    map.add(strings("a b"), tokens("ab"), orig, merge);
-    map.add(strings("a c"), tokens("ac"), orig, merge);
-    map.add(strings("a"), tokens("aa"), orig, merge);
-    map.add(strings("b"), tokens("bb"), orig, merge);
-    map.add(strings("z x c v"), tokens("zxcv"), orig, merge);
-    map.add(strings("x c"), tokens("xc"), orig, merge);
-
-    assertTokenizesTo(map, "$", 
-        new String[] { "$" },
-        new int[] { 1 });
-    assertTokenizesTo(map, "a", 
-        new String[] { "a", "aa" },
-        new int[] { 1, 0 });
-    assertTokenizesTo(map, "a", 
-        new String[] { "a", "aa" },
-        new int[] { 1, 0 });
-    assertTokenizesTo(map, "$ a", 
-        new String[] { "$", "a", "aa" },
-        new int[] { 1, 1, 0 });
-    assertTokenizesTo(map, "a $", 
-        new String[] { "a", "aa", "$" },
-        new int[] { 1, 0, 1 });
-    assertTokenizesTo(map, "$ a !", 
-        new String[] { "$", "a", "aa", "!" },
-        new int[] { 1, 1, 0, 1 });
-    assertTokenizesTo(map, "a a", 
-        new String[] { "a", "aa", "a", "aa" },
-        new int[] { 1, 0, 1, 0 });
-    assertTokenizesTo(map, "b", 
-        new String[] { "b", "bb" },
-        new int[] { 1, 0 });
-    assertTokenizesTo(map, "z x c v",
-        new String[] { "z", "zxcv", "x", "c", "v" },
-        new int[] { 1, 0, 1, 1, 1 });
-    assertTokenizesTo(map, "z x c $",
-        new String[] { "z", "x", "xc", "c", "$" },
-        new int[] { 1, 1, 0, 1, 1 });
-
-    // check for lack of recursion
-    map.add(strings("zoo zoo"), tokens("zoo"), orig, merge);
-    // CHECKME: I think the previous test (with 4 zoo's), was just a typo.
-    assertTokenizesTo(map, "zoo zoo $ zoo",
-        new String[] { "zoo", "zoo", "zoo", "$", "zoo" },
-        new int[] { 1, 0, 1, 1, 1 });
-
-    map.add(strings("zoo"), tokens("zoo zoo"), orig, merge);
-    assertTokenizesTo(map, "zoo zoo $ zoo",
-        new String[] { "zoo", "zoo", "zoo", "$", "zoo", "zoo", "zoo" },
-        new int[] { 1, 0, 1, 1, 1, 0, 1 });
-  }
-
-
-  public void testMapMerge() throws IOException {
-    SlowSynonymMap map = new SlowSynonymMap();
-
-    boolean orig = false;
-    boolean merge = true;
-    map.add(strings("a"), tokens("a5,5"), orig, merge);
-    map.add(strings("a"), tokens("a3,3"), orig, merge);
-
-    assertTokenizesTo(map, "a",
-        new String[] { "a3", "a5" },
-        new int[] { 1, 2 });
-
-    map.add(strings("b"), tokens("b3,3"), orig, merge);
-    map.add(strings("b"), tokens("b5,5"), orig, merge);
-
-    assertTokenizesTo(map, "b",
-        new String[] { "b3", "b5" },
-        new int[] { 1, 2 });
-
-    map.add(strings("a"), tokens("A3,3"), orig, merge);
-    map.add(strings("a"), tokens("A5,5"), orig, merge);
-    
-    assertTokenizesTo(map, "a",
-        new String[] { "a3", "A3", "a5", "A5" },
-        new int[] { 1, 0, 2, 0 });
-
-    map.add(strings("a"), tokens("a1"), orig, merge);
-    assertTokenizesTo(map, "a",
-        new String[] { "a1", "a3", "A3", "a5", "A5" },
-        new int[] { 1, 2, 0, 2, 0 });
-
-    map.add(strings("a"), tokens("a2,2"), orig, merge);
-    map.add(strings("a"), tokens("a4,4 a6,2"), orig, merge);
-    assertTokenizesTo(map, "a",
-        new String[] { "a1", "a2", "a3", "A3", "a4", "a5", "A5", "a6" },
-        new int[] { 1, 1, 1, 0, 1, 1, 0, 1  });
-  }
-
-
-  public void testOverlap() throws IOException {
-    SlowSynonymMap map = new SlowSynonymMap();
-
-    boolean orig = false;
-    boolean merge = true;
-    map.add(strings("qwe"), tokens("qq/ww/ee"), orig, merge);
-    map.add(strings("qwe"), tokens("xx"), orig, merge);
-    map.add(strings("qwe"), tokens("yy"), orig, merge);
-    map.add(strings("qwe"), tokens("zz"), orig, merge);
-    assertTokenizesTo(map, "$", new String[] { "$" });
-    assertTokenizesTo(map, "qwe",
-        new String[] { "qq", "ww", "ee", "xx", "yy", "zz" },
-        new int[] { 1, 0, 0, 0, 0, 0 });
-
-    // test merging within the map
-
-    map.add(strings("a"), tokens("a5,5 a8,3 a10,2"), orig, merge);
-    map.add(strings("a"), tokens("a3,3 a7,4 a9,2 a11,2 a111,100"), orig, merge);
-    assertTokenizesTo(map, "a",
-        new String[] { "a3", "a5", "a7", "a8", "a9", "a10", "a11", "a111" },
-        new int[] { 1, 2, 2, 1, 1, 1, 1, 100 });
-  }
-
-  public void testPositionIncrements() throws IOException {
-    SlowSynonymMap map = new SlowSynonymMap();
-
-    boolean orig = false;
-    boolean merge = true;
-
-    // test that generated tokens start at the same posInc as the original
-    map.add(strings("a"), tokens("aa"), orig, merge);
-    assertTokenizesTo(map, tokens("a,5"), 
-        new String[] { "aa" },
-        new int[] { 5 });
-    assertTokenizesTo(map, tokens("b,1 a,0"),
-        new String[] { "b", "aa" },
-        new int[] { 1, 0 });
-
-    // test that offset of first replacement is ignored (always takes the orig offset)
-    map.add(strings("b"), tokens("bb,100"), orig, merge);
-    assertTokenizesTo(map, tokens("b,5"),
-        new String[] { "bb" },
-        new int[] { 5 });
-    assertTokenizesTo(map, tokens("c,1 b,0"),
-        new String[] { "c", "bb" },
-        new int[] { 1, 0 });
-
-    // test that subsequent tokens are adjusted accordingly
-    map.add(strings("c"), tokens("cc,100 c2,2"), orig, merge);
-    assertTokenizesTo(map, tokens("c,5"),
-        new String[] { "cc", "c2" },
-        new int[] { 5, 2 });
-    assertTokenizesTo(map, tokens("d,1 c,0"),
-        new String[] { "d", "cc", "c2" },
-        new int[] { 1, 0, 2 });
-  }
-
-
-  public void testPositionIncrementsWithOrig() throws IOException {
-    SlowSynonymMap map = new SlowSynonymMap();
-
-    boolean orig = true;
-    boolean merge = true;
-
-    // test that generated tokens start at the same offset as the original
-    map.add(strings("a"), tokens("aa"), orig, merge);
-    assertTokenizesTo(map, tokens("a,5"),
-        new String[] { "a", "aa" },
-        new int[] { 5, 0 });
-    assertTokenizesTo(map, tokens("b,1 a,0"),
-        new String[] { "b", "a", "aa" },
-        new int[] { 1, 0, 0 });
-
-    // test that offset of first replacement is ignored (always takes the orig offset)
-    map.add(strings("b"), tokens("bb,100"), orig, merge);
-    assertTokenizesTo(map, tokens("b,5"),
-        new String[] { "b", "bb" },
-        new int[] { 5, 0 });
-    assertTokenizesTo(map, tokens("c,1 b,0"),
-        new String[] { "c", "b", "bb" },
-        new int[] { 1, 0, 0 });
-
-    // test that subsequent tokens are adjusted accordingly
-    map.add(strings("c"), tokens("cc,100 c2,2"), orig, merge);
-    assertTokenizesTo(map, tokens("c,5"),
-        new String[] { "c", "cc", "c2" },
-        new int[] { 5, 0, 2 });
-    assertTokenizesTo(map, tokens("d,1 c,0"),
-        new String[] { "d", "c", "cc", "c2" },
-        new int[] { 1, 0, 0, 2 });
-  }
-
-
-  public void testOffsetBug() throws IOException {
-    // With the following rules:
-    // a a=>b
-    // x=>y
-    // analysing "a x" causes "y" to have a bad offset (end less than start)
-    // SOLR-167
-    SlowSynonymMap map = new SlowSynonymMap();
-
-    boolean orig = false;
-    boolean merge = true;
-
-    map.add(strings("a a"), tokens("b"), orig, merge);
-    map.add(strings("x"), tokens("y"), orig, merge);
-
-    // "a a x" => "b y"
-    assertTokenizesTo(map, tokens("a,1,0,1 a,1,2,3 x,1,4,5"),
-        new String[] { "b", "y" },
-        new int[] { 0, 4 },
-        new int[] { 3, 5 },
-        new int[] { 1, 1 });
-  }
-
-  
-  /***
-   * Return a list of tokens according to a test string format:
-   * a b c  =>  returns List<Token> [a,b,c]
-   * a/b   => tokens a and b share the same spot (b.positionIncrement=0)
-   * a,3/b/c => a,b,c all share same position (a.positionIncrement=3, b.positionIncrement=0, c.positionIncrement=0)
-   * a,1,10,11  => "a" with positionIncrement=1, startOffset=10, endOffset=11
-   * @deprecated (3.0) does not support attributes api
-   */
-  @Deprecated
-  private List<Token> tokens(String str) {
-    String[] arr = str.split(" ");
-    List<Token> result = new ArrayList<Token>();
-    for (int i=0; i<arr.length; i++) {
-      String[] toks = arr[i].split("/");
-      String[] params = toks[0].split(",");
-
-      int posInc;
-      int start;
-      int end;
-
-      if (params.length > 1) {
-        posInc = Integer.parseInt(params[1]);
-      } else {
-        posInc = 1;
-      }
-
-      if (params.length > 2) {
-        start = Integer.parseInt(params[2]);
-      } else {
-        start = 0;
-      }
-
-      if (params.length > 3) {
-        end = Integer.parseInt(params[3]);
-      } else {
-        end = start + params[0].length();
-      }
-
-      Token t = new Token(params[0],start,end,"TEST");
-      t.setPositionIncrement(posInc);
-      
-      result.add(t);
-      for (int j=1; j<toks.length; j++) {
-        t = new Token(toks[j],0,0,"TEST");
-        t.setPositionIncrement(0);
-        result.add(t);
-      }
-    }
-    return result;
-  }
-  
-  /**
-   * @deprecated (3.0) does not support custom attributes
-   */
-  @Deprecated
-  private static class IterTokenStream extends TokenStream {
-    final Token tokens[];
-    int index = 0;
-    CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-    OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
-    PositionIncrementAttribute posIncAtt = addAttribute(PositionIncrementAttribute.class);
-    FlagsAttribute flagsAtt = addAttribute(FlagsAttribute.class);
-    TypeAttribute typeAtt = addAttribute(TypeAttribute.class);
-    PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);
-    
-    public IterTokenStream(Token... tokens) {
-      super();
-      this.tokens = tokens;
-    }
-    
-    public IterTokenStream(Collection<Token> tokens) {
-      this(tokens.toArray(new Token[tokens.size()]));
-    }
-    
-    @Override
-    public boolean incrementToken() throws IOException {
-      if (index >= tokens.length)
-        return false;
-      else {
-        clearAttributes();
-        Token token = tokens[index++];
-        termAtt.setEmpty().append(token);
-        offsetAtt.setOffset(token.startOffset(), token.endOffset());
-        posIncAtt.setPositionIncrement(token.getPositionIncrement());
-        flagsAtt.setFlags(token.getFlags());
-        typeAtt.setType(token.type());
-        payloadAtt.setPayload(token.getPayload());
-        return true;
-      }
-    }
-  }
-}
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestSynonymFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestSynonymFilterFactory.java
index 7b24b67..7eb228d 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestSynonymFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestSynonymFilterFactory.java
@@ -30,7 +30,6 @@ import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.synonym.SynonymFilter;
-import org.apache.lucene.util.Version;
 import org.apache.lucene.analysis.util.ResourceLoader;
 import org.apache.solr.core.SolrResourceLoader;
 
@@ -50,42 +49,6 @@ public class TestSynonymFilterFactory extends BaseTokenStreamTestCase {
         new int[] { 1, 0, 0, 0 });
   }
   
-  /** test that we can parse and use the solr syn file, with the old impl
-   * @deprecated Remove this test in Lucene 5.0 */
-  @Deprecated
-  public void testSynonymsOld() throws Exception {
-    SynonymFilterFactory factory = new SynonymFilterFactory();
-    Map<String,String> args = new HashMap<String,String>();
-    args.put("synonyms", "synonyms.txt");
-    factory.setLuceneMatchVersion(Version.LUCENE_33);
-    factory.init(args);
-    factory.inform(new SolrResourceLoader(null, null));
-    TokenStream ts = factory.create(new MockTokenizer(new StringReader("GB"), MockTokenizer.WHITESPACE, false));
-    assertTrue(ts instanceof SlowSynonymFilter);
-    assertTokenStreamContents(ts, 
-        new String[] { "GB", "gib", "gigabyte", "gigabytes" },
-        new int[] { 1, 0, 0, 0 });
-  }
-  
-  /** test multiword offsets with the old impl
-   * @deprecated Remove this test in Lucene 5.0 */
-  @Deprecated
-  public void testMultiwordOffsetsOld() throws Exception {
-    SynonymFilterFactory factory = new SynonymFilterFactory();
-    Map<String,String> args = new HashMap<String,String>();
-    args.put("synonyms", "synonyms.txt");
-    factory.setLuceneMatchVersion(Version.LUCENE_33);
-    factory.init(args);
-    factory.inform(new StringMockSolrResourceLoader("national hockey league, nhl"));
-    TokenStream ts = factory.create(new MockTokenizer(new StringReader("national hockey league"), MockTokenizer.WHITESPACE, false));
-    // WTF?
-    assertTokenStreamContents(ts, 
-        new String[] { "national", "nhl", "hockey", "league" },
-        new int[] { 0, 0, 0, 0 },
-        new int[] { 22, 22, 22, 22 },
-        new int[] { 1, 0, 1, 1 });
-  }
-  
   /** if the synonyms are completely empty, test that we still analyze correctly */
   public void testEmptySynonyms() throws Exception {
     SynonymFilterFactory factory = new SynonymFilterFactory();
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestSynonymMap.java b/solr/core/src/test/org/apache/solr/analysis/TestSynonymMap.java
deleted file mode 100644
index 2449b43..0000000
--- a/solr/core/src/test/org/apache/solr/analysis/TestSynonymMap.java
+++ /dev/null
@@ -1,319 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.solr.analysis;
-
-import java.io.IOException;
-import java.io.InputStream;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.lucene.analysis.Token;
-import org.apache.lucene.analysis.util.InitializationException;
-import org.apache.lucene.analysis.util.TokenizerFactory;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.analysis.util.ResourceLoader;
-
-
-/**
- * @deprecated Remove this test in Lucene 5.0
- */
-@Deprecated
-public class TestSynonymMap extends LuceneTestCase {
-
-  public void testInvalidMappingRules() throws Exception {
-    SlowSynonymMap synMap = new SlowSynonymMap( true );
-    List<String> rules = new ArrayList<String>( 1 );
-    rules.add( "a=>b=>c" );
-    try{
-        SlowSynonymFilterFactory.parseRules( rules, synMap, "=>", ",", true, null);
-        fail( "InitializationException must be thrown." );
-    }
-    catch(InitializationException expected) {}
-  }
-  
-  public void testReadMappingRules() throws Exception {
-	SlowSynonymMap synMap;
-
-    // (a)->[b]
-    List<String> rules = new ArrayList<String>();
-    rules.add( "a=>b" );
-    synMap = new SlowSynonymMap( true );
-    SlowSynonymFilterFactory.parseRules( rules, synMap, "=>", ",", true, null);
-    assertEquals( 1, synMap.submap.size() );
-    assertTokIncludes( synMap, "a", "b" );
-
-    // (a)->[c]
-    // (b)->[c]
-    rules.clear();
-    rules.add( "a,b=>c" );
-    synMap = new SlowSynonymMap( true );
-    SlowSynonymFilterFactory.parseRules( rules, synMap, "=>", ",", true, null);
-    assertEquals( 2, synMap.submap.size() );
-    assertTokIncludes( synMap, "a", "c" );
-    assertTokIncludes( synMap, "b", "c" );
-
-    // (a)->[b][c]
-    rules.clear();
-    rules.add( "a=>b,c" );
-    synMap = new SlowSynonymMap( true );
-    SlowSynonymFilterFactory.parseRules( rules, synMap, "=>", ",", true, null);
-    assertEquals( 1, synMap.submap.size() );
-    assertTokIncludes( synMap, "a", "b" );
-    assertTokIncludes( synMap, "a", "c" );
-
-    // (a)->(b)->[a2]
-    //      [a1]
-    rules.clear();
-    rules.add( "a=>a1" );
-    rules.add( "a b=>a2" );
-    synMap = new SlowSynonymMap( true );
-    SlowSynonymFilterFactory.parseRules( rules, synMap, "=>", ",", true, null);
-    assertEquals( 1, synMap.submap.size() );
-    assertTokIncludes( synMap, "a", "a1" );
-    assertEquals( 1, getSubSynonymMap( synMap, "a" ).submap.size() );
-    assertTokIncludes( getSubSynonymMap( synMap, "a" ), "b", "a2" );
-
-    // (a)->(b)->[a2]
-    //      (c)->[a3]
-    //      [a1]
-    rules.clear();
-    rules.add( "a=>a1" );
-    rules.add( "a b=>a2" );
-    rules.add( "a c=>a3" );
-    synMap = new SlowSynonymMap( true );
-    SlowSynonymFilterFactory.parseRules( rules, synMap, "=>", ",", true, null);
-    assertEquals( 1, synMap.submap.size() );
-    assertTokIncludes( synMap, "a", "a1" );
-    assertEquals( 2, getSubSynonymMap( synMap, "a" ).submap.size() );
-    assertTokIncludes( getSubSynonymMap( synMap, "a" ), "b", "a2" );
-    assertTokIncludes( getSubSynonymMap( synMap, "a" ), "c", "a3" );
-
-    // (a)->(b)->[a2]
-    //      [a1]
-    // (b)->(c)->[b2]
-    //      [b1]
-    rules.clear();
-    rules.add( "a=>a1" );
-    rules.add( "a b=>a2" );
-    rules.add( "b=>b1" );
-    rules.add( "b c=>b2" );
-    synMap = new SlowSynonymMap( true );
-    SlowSynonymFilterFactory.parseRules( rules, synMap, "=>", ",", true, null);
-    assertEquals( 2, synMap.submap.size() );
-    assertTokIncludes( synMap, "a", "a1" );
-    assertEquals( 1, getSubSynonymMap( synMap, "a" ).submap.size() );
-    assertTokIncludes( getSubSynonymMap( synMap, "a" ), "b", "a2" );
-    assertTokIncludes( synMap, "b", "b1" );
-    assertEquals( 1, getSubSynonymMap( synMap, "b" ).submap.size() );
-    assertTokIncludes( getSubSynonymMap( synMap, "b" ), "c", "b2" );
-  }
-  
-  public void testRead1waySynonymRules() throws Exception {
-    SlowSynonymMap synMap;
-
-    // (a)->[a]
-    // (b)->[a]
-    List<String> rules = new ArrayList<String>();
-    rules.add( "a,b" );
-    synMap = new SlowSynonymMap( true );
-    SlowSynonymFilterFactory.parseRules( rules, synMap, "=>", ",", false, null);
-    assertEquals( 2, synMap.submap.size() );
-    assertTokIncludes( synMap, "a", "a" );
-    assertTokIncludes( synMap, "b", "a" );
-
-    // (a)->[a]
-    // (b)->[a]
-    // (c)->[a]
-    rules.clear();
-    rules.add( "a,b,c" );
-    synMap = new SlowSynonymMap( true );
-    SlowSynonymFilterFactory.parseRules( rules, synMap, "=>", ",", false, null);
-    assertEquals( 3, synMap.submap.size() );
-    assertTokIncludes( synMap, "a", "a" );
-    assertTokIncludes( synMap, "b", "a" );
-    assertTokIncludes( synMap, "c", "a" );
-
-    // (a)->[a]
-    // (b1)->(b2)->[a]
-    rules.clear();
-    rules.add( "a,b1 b2" );
-    synMap = new SlowSynonymMap( true );
-    SlowSynonymFilterFactory.parseRules( rules, synMap, "=>", ",", false, null);
-    assertEquals( 2, synMap.submap.size() );
-    assertTokIncludes( synMap, "a", "a" );
-    assertEquals( 1, getSubSynonymMap( synMap, "b1" ).submap.size() );
-    assertTokIncludes( getSubSynonymMap( synMap, "b1" ), "b2", "a" );
-
-    // (a1)->(a2)->[a1][a2]
-    // (b)->[a1][a2]
-    rules.clear();
-    rules.add( "a1 a2,b" );
-    synMap = new SlowSynonymMap( true );
-    SlowSynonymFilterFactory.parseRules( rules, synMap, "=>", ",", false, null);
-    assertEquals( 2, synMap.submap.size() );
-    assertEquals( 1, getSubSynonymMap( synMap, "a1" ).submap.size() );
-    assertTokIncludes( getSubSynonymMap( synMap, "a1" ), "a2", "a1" );
-    assertTokIncludes( getSubSynonymMap( synMap, "a1" ), "a2", "a2" );
-    assertTokIncludes( synMap, "b", "a1" );
-    assertTokIncludes( synMap, "b", "a2" );
-  }
-  
-  public void testRead2waySynonymRules() throws Exception {
-    SlowSynonymMap synMap;
-
-    // (a)->[a][b]
-    // (b)->[a][b]
-    List<String> rules = new ArrayList<String>();
-    rules.add( "a,b" );
-    synMap = new SlowSynonymMap( true );
-    SlowSynonymFilterFactory.parseRules( rules, synMap, "=>", ",", true, null);
-    assertEquals( 2, synMap.submap.size() );
-    assertTokIncludes( synMap, "a", "a" );
-    assertTokIncludes( synMap, "a", "b" );
-    assertTokIncludes( synMap, "b", "a" );
-    assertTokIncludes( synMap, "b", "b" );
-
-    // (a)->[a][b][c]
-    // (b)->[a][b][c]
-    // (c)->[a][b][c]
-    rules.clear();
-    rules.add( "a,b,c" );
-    synMap = new SlowSynonymMap( true );
-    SlowSynonymFilterFactory.parseRules( rules, synMap, "=>", ",", true, null);
-    assertEquals( 3, synMap.submap.size() );
-    assertTokIncludes( synMap, "a", "a" );
-    assertTokIncludes( synMap, "a", "b" );
-    assertTokIncludes( synMap, "a", "c" );
-    assertTokIncludes( synMap, "b", "a" );
-    assertTokIncludes( synMap, "b", "b" );
-    assertTokIncludes( synMap, "b", "c" );
-    assertTokIncludes( synMap, "c", "a" );
-    assertTokIncludes( synMap, "c", "b" );
-    assertTokIncludes( synMap, "c", "c" );
-
-    // (a)->[a]
-    //      [b1][b2]
-    // (b1)->(b2)->[a]
-    //             [b1][b2]
-    rules.clear();
-    rules.add( "a,b1 b2" );
-    synMap = new SlowSynonymMap( true );
-    SlowSynonymFilterFactory.parseRules( rules, synMap, "=>", ",", true, null);
-    assertEquals( 2, synMap.submap.size() );
-    assertTokIncludes( synMap, "a", "a" );
-    assertTokIncludes( synMap, "a", "b1" );
-    assertTokIncludes( synMap, "a", "b2" );
-    assertEquals( 1, getSubSynonymMap( synMap, "b1" ).submap.size() );
-    assertTokIncludes( getSubSynonymMap( synMap, "b1" ), "b2", "a" );
-    assertTokIncludes( getSubSynonymMap( synMap, "b1" ), "b2", "b1" );
-    assertTokIncludes( getSubSynonymMap( synMap, "b1" ), "b2", "b2" );
-
-    // (a1)->(a2)->[a1][a2]
-    //             [b]
-    // (b)->[a1][a2]
-    //      [b]
-    rules.clear();
-    rules.add( "a1 a2,b" );
-    synMap = new SlowSynonymMap( true );
-    SlowSynonymFilterFactory.parseRules( rules, synMap, "=>", ",", true, null);
-    assertEquals( 2, synMap.submap.size() );
-    assertEquals( 1, getSubSynonymMap( synMap, "a1" ).submap.size() );
-    assertTokIncludes( getSubSynonymMap( synMap, "a1" ), "a2", "a1" );
-    assertTokIncludes( getSubSynonymMap( synMap, "a1" ), "a2", "a2" );
-    assertTokIncludes( getSubSynonymMap( synMap, "a1" ), "a2", "b" );
-    assertTokIncludes( synMap, "b", "a1" );
-    assertTokIncludes( synMap, "b", "a2" );
-    assertTokIncludes( synMap, "b", "b" );
-  }
-  
-  public void testBigramTokenizer() throws Exception {
-	SlowSynonymMap synMap;
-	
-	// prepare bi-gram tokenizer factory
-	TokenizerFactory tf = new NGramTokenizerFactory();
-	Map<String, String> args = new HashMap<String, String>();
-	args.put("minGramSize","2");
-	args.put("maxGramSize","2");
-	tf.init( args );
-
-    // (ab)->(bc)->(cd)->[ef][fg][gh]
-    List<String> rules = new ArrayList<String>();
-    rules.add( "abcd=>efgh" );
-    synMap = new SlowSynonymMap( true );
-    SlowSynonymFilterFactory.parseRules( rules, synMap, "=>", ",", true, tf);
-    assertEquals( 1, synMap.submap.size() );
-    assertEquals( 1, getSubSynonymMap( synMap, "ab" ).submap.size() );
-    assertEquals( 1, getSubSynonymMap( getSubSynonymMap( synMap, "ab" ), "bc" ).submap.size() );
-    assertTokIncludes( getSubSynonymMap( getSubSynonymMap( synMap, "ab" ), "bc" ), "cd", "ef" );
-    assertTokIncludes( getSubSynonymMap( getSubSynonymMap( synMap, "ab" ), "bc" ), "cd", "fg" );
-    assertTokIncludes( getSubSynonymMap( getSubSynonymMap( synMap, "ab" ), "bc" ), "cd", "gh" );
-  }
-  
-
-  public void testLoadRules() throws Exception {
-    Map<String, String> args = new HashMap<String, String>();
-    args.put( "synonyms", "something.txt" );
-    SlowSynonymFilterFactory ff = new SlowSynonymFilterFactory();
-    ff.init(args);
-    ff.inform( new ResourceLoader() {
-      @Override
-      public List<String> getLines(String resource) throws IOException {
-        if( !"something.txt".equals(resource) ) {
-          throw new RuntimeException( "should not get a differnt resource" );
-        }
-        List<String> rules = new ArrayList<String>();
-        rules.add( "a,b" );
-        return rules;
-      }
-
-      @Override
-      public <T> T newInstance(String cname, Class<T> expectedType, String... subpackages) {
-        throw new RuntimeException("stub");
-      }
-
-      @Override
-      public InputStream openResource(String resource) throws IOException {
-        throw new RuntimeException("stub");
-      }
-    });
-    
-    SlowSynonymMap synMap = ff.getSynonymMap();
-    assertEquals( 2, synMap.submap.size() );
-    assertTokIncludes( synMap, "a", "a" );
-    assertTokIncludes( synMap, "a", "b" );
-    assertTokIncludes( synMap, "b", "a" );
-    assertTokIncludes( synMap, "b", "b" );
-  }
-  
-  
-  private void assertTokIncludes( SlowSynonymMap map, String src, String exp ) throws Exception {
-    Token[] tokens = map.submap.get( src ).synonyms;
-    boolean inc = false;
-    for( Token token : tokens ){
-      if( exp.equals( new String(token.buffer(), 0, token.length()) ) )
-        inc = true;
-    }
-    assertTrue( inc );
-  }
-  
-  private SlowSynonymMap getSubSynonymMap( SlowSynonymMap map, String src ){
-    return map.submap.get( src );
-  }
-}
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestUAX29URLEmailTokenizerFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestUAX29URLEmailTokenizerFactory.java
index bd59fb2..1031e86 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestUAX29URLEmailTokenizerFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestUAX29URLEmailTokenizerFactory.java
@@ -190,25 +190,4 @@ public class TestUAX29URLEmailTokenizerFactory extends BaseTokenStreamTestCase {
     assertTokenStreamContents(stream, 
         new String[] {"one", "two", "three", longWord, "four", "five", "six" });
   }
-  
-  /** @deprecated nuke this test in lucene 5.0 */
-  @Deprecated
-  public void testMatchVersion() throws Exception {
-    Reader reader = new StringReader("???");
-    UAX29URLEmailTokenizerFactory factory = new UAX29URLEmailTokenizerFactory();
-    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
-    Map<String, String> args = Collections.emptyMap();
-    factory.init(args);
-    Tokenizer stream = factory.create(reader);
-    assertTokenStreamContents(stream, 
-        new String[] {"???"});
-    
-    reader = new StringReader("???");
-    factory = new UAX29URLEmailTokenizerFactory();
-    factory.setLuceneMatchVersion(Version.LUCENE_31);
-    factory.init(args);
-    stream = factory.create(reader);
-    assertTokenStreamContents(stream, 
-        new String[] {"??"}); // old broken behavior
-  }
 }
diff --git a/solr/core/src/test/org/apache/solr/core/TestConfig.java b/solr/core/src/test/org/apache/solr/core/TestConfig.java
index d1556a6..e699b4d 100644
--- a/solr/core/src/test/org/apache/solr/core/TestConfig.java
+++ b/solr/core/src/test/org/apache/solr/core/TestConfig.java
@@ -137,15 +137,6 @@ public class TestConfig extends SolrTestCaseJ4 {
     assertTrue("default LockType should be native", sic.lockType.equals(SolrIndexConfig.LOCK_TYPE_NATIVE));
   }
 
-  @Test
-  public void testDefaults31() throws Exception {
-    SolrConfig sc = new SolrConfig("solrconfig-basic-luceneVersion31.xml");
-    SolrIndexConfig sic = sc.indexConfig;
-    assertTrue("default ramBufferSizeMB should be 16", sic.ramBufferSizeMB == 16);
-    assertTrue("default useCompoundFile should be true", sic.useCompoundFile == true);
-    assertTrue("default LockType should be simple", sic.lockType.equals(SolrIndexConfig.LOCK_TYPE_SIMPLE));
-  }
-
 }
 
 
diff --git a/solr/core/src/test/org/apache/solr/handler/TestReplicationHandler.java b/solr/core/src/test/org/apache/solr/handler/TestReplicationHandler.java
index 05d79dc..89d4d67 100644
--- a/solr/core/src/test/org/apache/solr/handler/TestReplicationHandler.java
+++ b/solr/core/src/test/org/apache/solr/handler/TestReplicationHandler.java
@@ -29,6 +29,7 @@ import java.util.regex.Matcher;
 import java.util.regex.Pattern;
 
 import org.apache.commons.io.IOUtils;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.MatchAllDocsQuery;
@@ -906,7 +907,7 @@ public class TestReplicationHandler extends SolrTestCaseJ4 {
       assertEquals(1, files.length);
       snapDir[i] = files[0];
       Directory dir = new SimpleFSDirectory(snapDir[i].getAbsoluteFile());
-      IndexReader reader = IndexReader.open(dir);
+      IndexReader reader = DirectoryReader.open(dir);
       IndexSearcher searcher = new IndexSearcher(reader);
       TopDocs hits = searcher.search(new MatchAllDocsQuery(), 1);
       assertEquals(nDocs, hits.totalHits);
diff --git a/solr/example/example-DIH/solr/db/conf/solrconfig.xml b/solr/example/example-DIH/solr/db/conf/solrconfig.xml
index c9fd9fa..095944b 100644
--- a/solr/example/example-DIH/solr/db/conf/solrconfig.xml
+++ b/solr/example/example-DIH/solr/db/conf/solrconfig.xml
@@ -24,7 +24,7 @@
     that you fully re-index after changing this setting as it can affect both how text is indexed
     and queried.
   -->
-  <luceneMatchVersion>LUCENE_40</luceneMatchVersion>
+  <luceneMatchVersion>LUCENE_50</luceneMatchVersion>
 
   <jmx />
 
diff --git a/solr/example/example-DIH/solr/mail/conf/solrconfig.xml b/solr/example/example-DIH/solr/mail/conf/solrconfig.xml
index dc78641..0b0b718 100644
--- a/solr/example/example-DIH/solr/mail/conf/solrconfig.xml
+++ b/solr/example/example-DIH/solr/mail/conf/solrconfig.xml
@@ -24,7 +24,7 @@
     that you fully re-index after changing this setting as it can affect both how text is indexed
     and queried.
   -->
-  <luceneMatchVersion>LUCENE_40</luceneMatchVersion>
+  <luceneMatchVersion>LUCENE_50</luceneMatchVersion>
 
   <lib dir="../../../../contrib/dataimporthandler/lib/" regex=".*jar$" />
   <lib dir="../../../../dist/" regex="apache-solr-dataimporthandler-.*\.jar" />
diff --git a/solr/example/example-DIH/solr/rss/conf/solrconfig.xml b/solr/example/example-DIH/solr/rss/conf/solrconfig.xml
index 08d8628..529928d 100644
--- a/solr/example/example-DIH/solr/rss/conf/solrconfig.xml
+++ b/solr/example/example-DIH/solr/rss/conf/solrconfig.xml
@@ -24,7 +24,7 @@
     that you fully re-index after changing this setting as it can affect both how text is indexed
     and queried.
   -->
-  <luceneMatchVersion>LUCENE_40</luceneMatchVersion>
+  <luceneMatchVersion>LUCENE_50</luceneMatchVersion>
 
   <jmx />
 
diff --git a/solr/example/example-DIH/solr/solr/conf/solrconfig.xml b/solr/example/example-DIH/solr/solr/conf/solrconfig.xml
index 3e213cd..429dccb 100644
--- a/solr/example/example-DIH/solr/solr/conf/solrconfig.xml
+++ b/solr/example/example-DIH/solr/solr/conf/solrconfig.xml
@@ -24,7 +24,7 @@
     that you fully re-index after changing this setting as it can affect both how text is indexed
     and queried.
   -->
-  <luceneMatchVersion>LUCENE_40</luceneMatchVersion>
+  <luceneMatchVersion>LUCENE_50</luceneMatchVersion>
 
   <jmx />
 
diff --git a/solr/example/example-DIH/solr/tika/conf/solrconfig.xml b/solr/example/example-DIH/solr/tika/conf/solrconfig.xml
index 72140eb..a2e339f 100644
--- a/solr/example/example-DIH/solr/tika/conf/solrconfig.xml
+++ b/solr/example/example-DIH/solr/tika/conf/solrconfig.xml
@@ -24,7 +24,7 @@
     that you fully re-index after changing this setting as it can affect both how text is indexed
     and queried.
   -->
-  <luceneMatchVersion>LUCENE_40</luceneMatchVersion>
+  <luceneMatchVersion>LUCENE_50</luceneMatchVersion>
 
   <lib dir="../../../../contrib/extraction/lib" />
   <lib dir="../../../../dist/" regex="apache-solr-dataimporthandler-.*\.jar" />
diff --git a/solr/example/multicore/core0/conf/solrconfig.xml b/solr/example/multicore/core0/conf/solrconfig.xml
index d29101c..c822bb7 100644
--- a/solr/example/multicore/core0/conf/solrconfig.xml
+++ b/solr/example/multicore/core0/conf/solrconfig.xml
@@ -21,7 +21,7 @@
  It is *not* a good example to work from. 
 -->
 <config>
-  <luceneMatchVersion>LUCENE_40</luceneMatchVersion>
+  <luceneMatchVersion>LUCENE_50</luceneMatchVersion>
   <!--  The DirectoryFactory to use for indexes.
         solr.StandardDirectoryFactory, the default, is filesystem based.
         solr.RAMDirectoryFactory is memory based, not persistent, and doesn't work with replication. -->
diff --git a/solr/example/multicore/core1/conf/solrconfig.xml b/solr/example/multicore/core1/conf/solrconfig.xml
index 13c59fb..2cda4d0 100644
--- a/solr/example/multicore/core1/conf/solrconfig.xml
+++ b/solr/example/multicore/core1/conf/solrconfig.xml
@@ -21,7 +21,7 @@
  It is *not* a good example to work from. 
 -->
 <config>
-  <luceneMatchVersion>LUCENE_40</luceneMatchVersion>
+  <luceneMatchVersion>LUCENE_50</luceneMatchVersion>
   <!--  The DirectoryFactory to use for indexes.
         solr.StandardDirectoryFactory, the default, is filesystem based.
         solr.RAMDirectoryFactory is memory based, not persistent, and doesn't work with replication. -->
diff --git a/solr/example/solr/conf/solrconfig.xml b/solr/example/solr/conf/solrconfig.xml
index 618d352..cdfde5d 100755
--- a/solr/example/solr/conf/solrconfig.xml
+++ b/solr/example/solr/conf/solrconfig.xml
@@ -35,7 +35,7 @@
        that you fully re-index after changing this setting as it can
        affect both how text is indexed and queried.
   -->
-  <luceneMatchVersion>LUCENE_40</luceneMatchVersion>
+  <luceneMatchVersion>LUCENE_50</luceneMatchVersion>
 
   <!-- lib directives can be used to instruct Solr to load an Jars
        identified and use them to resolve any "plugins" specified in

